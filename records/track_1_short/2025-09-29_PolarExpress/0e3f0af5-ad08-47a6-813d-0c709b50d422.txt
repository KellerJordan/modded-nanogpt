import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)


@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)


@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[
    Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)


@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)


def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None


def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)


mm_op.register_autograd(backward, setup_context=setup_context)


# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
        pid,
        M,
        BLOCK_SIZE_M: tl.constexpr,
        BLOCK_SIZE_N: tl.constexpr,
        GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
coeffs_list = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in coeffs_list:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """

    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size() == 8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1,  # 1 param
            'attn_gate': 2,  # 10 params
            'attn': 3,  # 10 params
            'mlp': 4,  # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx + size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                    (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                    group["lr"]
                    * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                    * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                    group["lr"]
                    * group["weight_decay"]
                    * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[module_idx], 'module', 'none') == 'attn':
                for p in params[module_idx:module_idx + chunk_size]:
                    assert getattr(params[module_idx], 'module', 'none') == 'attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8,
                 weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))


class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5)  # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim // 4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim // 4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int = 1, beta: int = 32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1


def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)


@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim * 4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module = 'attn'
        with torch.no_grad():
            self.qkvo_w.view(4, self.hdim, self.dim)[:3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w.view(4, self.hdim, self.dim)[3].zero_()  # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1)  # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T,
                                                                                                               3 * self.num_heads,
                                                                                                               self.head_dim).chunk(
            3, dim=-2)
        q, k = norm(q), norm(k)  # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v)  # @ KoszarskyB & @Grad62304977
        else:  # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len,
                                   max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)  # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module = 'mlp'
        self.c_proj.module = 'mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_()  # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(
            x).square()  # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x


# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)


class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int,
                 max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim ** 0.5) / 448, w_s=2 ** -9,
                                    grad_s=1 / 448)
        self.lm_head.weight.detach().zero_()  # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers),  # extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm,
                    long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1, len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i < 11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss


# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32)  # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2])  # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True)  # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy())  # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens


BOS_ID = 50256


class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens = tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter == 5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter += 1
        return starts, ends


class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data


def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1,
                               align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens

        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin"  # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin"  # input .bin to eval validation loss on
    val_tokens: int = 10485760  # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1630  # number of iterations to run
    iteration_extension = 40  # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5  # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13  # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20  # extend long windows out even further


args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0)  # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)


def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)


# begin by printing this file (the Python code)
print0(code)
print0("=" * 100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")


def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout


print0(nvidia_smi())
print0("=" * 100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if
                        p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]


# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr


def get_ws(step: int):
    if step == args.num_iterations + args.iteration_extension:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers])  # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len,
                                          grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[
        step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long < ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long // 2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len,
                                          grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1,
                                                grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(
            f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms",
            console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(),
                       optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1)  # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(
        f"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms",
        console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Mon Sep 29 06:26:49 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   40C    P0            130W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   33C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1670 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1670 train_time:143ms step_avg:143.06ms
step:2/1670 train_time:164ms step_avg:81.87ms
step:3/1670 train_time:226ms step_avg:75.49ms
step:4/1670 train_time:312ms step_avg:77.97ms
step:5/1670 train_time:398ms step_avg:79.61ms
step:6/1670 train_time:485ms step_avg:80.83ms
step:7/1670 train_time:572ms step_avg:81.66ms
step:8/1670 train_time:658ms step_avg:82.27ms
step:9/1670 train_time:745ms step_avg:82.82ms
step:10/1670 train_time:832ms step_avg:83.20ms
step:11/1670 train_time:919ms step_avg:83.55ms
step:12/1670 train_time:1008ms step_avg:83.98ms
step:13/1670 train_time:1101ms step_avg:84.67ms
step:14/1670 train_time:1194ms step_avg:85.27ms
step:15/1670 train_time:1282ms step_avg:85.48ms
step:16/1670 train_time:1369ms step_avg:85.57ms
step:17/1670 train_time:1456ms step_avg:85.67ms
step:18/1670 train_time:1544ms step_avg:85.78ms
step:19/1670 train_time:1631ms step_avg:85.83ms
step:20/1670 train_time:1718ms step_avg:85.88ms
step:21/1670 train_time:1805ms step_avg:85.94ms
step:22/1670 train_time:1892ms step_avg:85.98ms
step:23/1670 train_time:1980ms step_avg:86.10ms
step:24/1670 train_time:2069ms step_avg:86.23ms
step:25/1670 train_time:2160ms step_avg:86.38ms
step:26/1670 train_time:2248ms step_avg:86.46ms
step:27/1670 train_time:2336ms step_avg:86.53ms
step:28/1670 train_time:2424ms step_avg:86.58ms
step:29/1670 train_time:2512ms step_avg:86.60ms
step:30/1670 train_time:2599ms step_avg:86.63ms
step:31/1670 train_time:2686ms step_avg:86.64ms
step:32/1670 train_time:2774ms step_avg:86.68ms
step:33/1670 train_time:2862ms step_avg:86.72ms
step:34/1670 train_time:2950ms step_avg:86.76ms
step:35/1670 train_time:3040ms step_avg:86.84ms
step:36/1670 train_time:3129ms step_avg:86.92ms
step:37/1670 train_time:3218ms step_avg:86.98ms
step:38/1670 train_time:3306ms step_avg:87.01ms
step:39/1670 train_time:3395ms step_avg:87.04ms
step:40/1670 train_time:3483ms step_avg:87.08ms
step:41/1670 train_time:3571ms step_avg:87.09ms
step:42/1670 train_time:3658ms step_avg:87.09ms
step:43/1670 train_time:3745ms step_avg:87.10ms
step:44/1670 train_time:3833ms step_avg:87.11ms
step:45/1670 train_time:3921ms step_avg:87.12ms
step:46/1670 train_time:4008ms step_avg:87.14ms
step:47/1670 train_time:4098ms step_avg:87.18ms
step:48/1670 train_time:4186ms step_avg:87.21ms
step:49/1670 train_time:4276ms step_avg:87.26ms
step:50/1670 train_time:4364ms step_avg:87.28ms
step:51/1670 train_time:4452ms step_avg:87.29ms
step:52/1670 train_time:4539ms step_avg:87.29ms
step:53/1670 train_time:4627ms step_avg:87.30ms
step:54/1670 train_time:4714ms step_avg:87.30ms
step:55/1670 train_time:4802ms step_avg:87.31ms
step:56/1670 train_time:4889ms step_avg:87.31ms
step:57/1670 train_time:4977ms step_avg:87.31ms
step:58/1670 train_time:5065ms step_avg:87.33ms
step:59/1670 train_time:5153ms step_avg:87.34ms
step:60/1670 train_time:5242ms step_avg:87.37ms
step:61/1670 train_time:5330ms step_avg:87.38ms
step:62/1670 train_time:5419ms step_avg:87.40ms
step:63/1670 train_time:5506ms step_avg:87.40ms
step:64/1670 train_time:5593ms step_avg:87.39ms
step:65/1670 train_time:5681ms step_avg:87.41ms
step:66/1670 train_time:5769ms step_avg:87.41ms
step:67/1670 train_time:5857ms step_avg:87.42ms
step:68/1670 train_time:5944ms step_avg:87.41ms
step:69/1670 train_time:6031ms step_avg:87.41ms
step:70/1670 train_time:6119ms step_avg:87.41ms
step:71/1670 train_time:6207ms step_avg:87.42ms
step:72/1670 train_time:6295ms step_avg:87.43ms
step:73/1670 train_time:6384ms step_avg:87.45ms
step:74/1670 train_time:6471ms step_avg:87.45ms
step:75/1670 train_time:6559ms step_avg:87.45ms
step:76/1670 train_time:6647ms step_avg:87.45ms
step:77/1670 train_time:6734ms step_avg:87.46ms
step:78/1670 train_time:6823ms step_avg:87.47ms
step:79/1670 train_time:6911ms step_avg:87.48ms
step:80/1670 train_time:7000ms step_avg:87.50ms
step:81/1670 train_time:7088ms step_avg:87.50ms
step:82/1670 train_time:7176ms step_avg:87.51ms
step:83/1670 train_time:7264ms step_avg:87.52ms
step:84/1670 train_time:7352ms step_avg:87.52ms
step:85/1670 train_time:7440ms step_avg:87.52ms
step:86/1670 train_time:7527ms step_avg:87.52ms
step:87/1670 train_time:7615ms step_avg:87.53ms
step:88/1670 train_time:7703ms step_avg:87.53ms
step:89/1670 train_time:7790ms step_avg:87.53ms
step:90/1670 train_time:7877ms step_avg:87.53ms
step:91/1670 train_time:7965ms step_avg:87.53ms
step:92/1670 train_time:8053ms step_avg:87.53ms
step:93/1670 train_time:8142ms step_avg:87.54ms
step:94/1670 train_time:8229ms step_avg:87.55ms
step:95/1670 train_time:8318ms step_avg:87.56ms
step:96/1670 train_time:8406ms step_avg:87.56ms
step:97/1670 train_time:8493ms step_avg:87.56ms
step:98/1670 train_time:8581ms step_avg:87.56ms
step:99/1670 train_time:8669ms step_avg:87.56ms
step:100/1670 train_time:8757ms step_avg:87.57ms
step:101/1670 train_time:8844ms step_avg:87.57ms
step:102/1670 train_time:8932ms step_avg:87.57ms
step:103/1670 train_time:9021ms step_avg:87.58ms
step:104/1670 train_time:9108ms step_avg:87.58ms
step:105/1670 train_time:9196ms step_avg:87.58ms
step:106/1670 train_time:9284ms step_avg:87.59ms
step:107/1670 train_time:9371ms step_avg:87.58ms
step:108/1670 train_time:9459ms step_avg:87.58ms
step:109/1670 train_time:9546ms step_avg:87.58ms
step:110/1670 train_time:9634ms step_avg:87.58ms
step:111/1670 train_time:9723ms step_avg:87.59ms
step:112/1670 train_time:9810ms step_avg:87.59ms
step:113/1670 train_time:9897ms step_avg:87.59ms
step:114/1670 train_time:9986ms step_avg:87.59ms
step:115/1670 train_time:10073ms step_avg:87.59ms
step:116/1670 train_time:10161ms step_avg:87.59ms
step:117/1670 train_time:10249ms step_avg:87.60ms
step:118/1670 train_time:10337ms step_avg:87.60ms
step:119/1670 train_time:10425ms step_avg:87.60ms
step:120/1670 train_time:10512ms step_avg:87.60ms
step:121/1670 train_time:10599ms step_avg:87.59ms
step:122/1670 train_time:10687ms step_avg:87.59ms
step:123/1670 train_time:10774ms step_avg:87.59ms
step:124/1670 train_time:10862ms step_avg:87.60ms
step:125/1670 train_time:10949ms step_avg:87.59ms
step:125/1670 val_loss:4.3341 train_time:11038ms step_avg:88.30ms
step:126/1670 train_time:11057ms step_avg:87.76ms
step:127/1670 train_time:11128ms step_avg:87.62ms
step:128/1670 train_time:11223ms step_avg:87.68ms
step:129/1670 train_time:11313ms step_avg:87.70ms
step:130/1670 train_time:11400ms step_avg:87.69ms
step:131/1670 train_time:11487ms step_avg:87.68ms
step:132/1670 train_time:11574ms step_avg:87.68ms
step:133/1670 train_time:11660ms step_avg:87.67ms
step:134/1670 train_time:11747ms step_avg:87.67ms
step:135/1670 train_time:11834ms step_avg:87.66ms
step:136/1670 train_time:11922ms step_avg:87.66ms
step:137/1670 train_time:12010ms step_avg:87.66ms
step:138/1670 train_time:12099ms step_avg:87.67ms
step:139/1670 train_time:12190ms step_avg:87.69ms
step:140/1670 train_time:12278ms step_avg:87.70ms
step:141/1670 train_time:12367ms step_avg:87.71ms
step:142/1670 train_time:12454ms step_avg:87.71ms
step:143/1670 train_time:12541ms step_avg:87.70ms
step:144/1670 train_time:12628ms step_avg:87.70ms
step:145/1670 train_time:12715ms step_avg:87.69ms
step:146/1670 train_time:12803ms step_avg:87.69ms
step:147/1670 train_time:12890ms step_avg:87.69ms
step:148/1670 train_time:12978ms step_avg:87.69ms
step:149/1670 train_time:13067ms step_avg:87.70ms
step:150/1670 train_time:13156ms step_avg:87.70ms
step:151/1670 train_time:13245ms step_avg:87.72ms
step:152/1670 train_time:13333ms step_avg:87.72ms
step:153/1670 train_time:13421ms step_avg:87.72ms
step:154/1670 train_time:13510ms step_avg:87.72ms
step:155/1670 train_time:13596ms step_avg:87.72ms
step:156/1670 train_time:13683ms step_avg:87.71ms
step:157/1670 train_time:13771ms step_avg:87.71ms
step:158/1670 train_time:13857ms step_avg:87.71ms
step:159/1670 train_time:13945ms step_avg:87.70ms
step:160/1670 train_time:14033ms step_avg:87.70ms
step:161/1670 train_time:14122ms step_avg:87.71ms
step:162/1670 train_time:14211ms step_avg:87.72ms
step:163/1670 train_time:14300ms step_avg:87.73ms
step:164/1670 train_time:14388ms step_avg:87.73ms
step:165/1670 train_time:14476ms step_avg:87.73ms
step:166/1670 train_time:14563ms step_avg:87.73ms
step:167/1670 train_time:14651ms step_avg:87.73ms
step:168/1670 train_time:14738ms step_avg:87.73ms
step:169/1670 train_time:14826ms step_avg:87.73ms
step:170/1670 train_time:14913ms step_avg:87.72ms
step:171/1670 train_time:15001ms step_avg:87.72ms
step:172/1670 train_time:15090ms step_avg:87.73ms
step:173/1670 train_time:15177ms step_avg:87.73ms
step:174/1670 train_time:15266ms step_avg:87.73ms
step:175/1670 train_time:15354ms step_avg:87.73ms
step:176/1670 train_time:15443ms step_avg:87.74ms
step:177/1670 train_time:15531ms step_avg:87.75ms
step:178/1670 train_time:15618ms step_avg:87.74ms
step:179/1670 train_time:15706ms step_avg:87.74ms
step:180/1670 train_time:15792ms step_avg:87.74ms
step:181/1670 train_time:15879ms step_avg:87.73ms
step:182/1670 train_time:15968ms step_avg:87.74ms
step:183/1670 train_time:16056ms step_avg:87.74ms
step:184/1670 train_time:16144ms step_avg:87.74ms
step:185/1670 train_time:16233ms step_avg:87.74ms
step:186/1670 train_time:16320ms step_avg:87.74ms
step:187/1670 train_time:16409ms step_avg:87.75ms
step:188/1670 train_time:16497ms step_avg:87.75ms
step:189/1670 train_time:16585ms step_avg:87.75ms
step:190/1670 train_time:16672ms step_avg:87.75ms
step:191/1670 train_time:16761ms step_avg:87.75ms
step:192/1670 train_time:16849ms step_avg:87.75ms
step:193/1670 train_time:16936ms step_avg:87.75ms
step:194/1670 train_time:17023ms step_avg:87.75ms
step:195/1670 train_time:17112ms step_avg:87.75ms
step:196/1670 train_time:17200ms step_avg:87.75ms
step:197/1670 train_time:17288ms step_avg:87.76ms
step:198/1670 train_time:17376ms step_avg:87.76ms
step:199/1670 train_time:17464ms step_avg:87.76ms
step:200/1670 train_time:17552ms step_avg:87.76ms
step:201/1670 train_time:17640ms step_avg:87.76ms
step:202/1670 train_time:17729ms step_avg:87.77ms
step:203/1670 train_time:17816ms step_avg:87.76ms
step:204/1670 train_time:17904ms step_avg:87.76ms
step:205/1670 train_time:17991ms step_avg:87.76ms
step:206/1670 train_time:18078ms step_avg:87.76ms
step:207/1670 train_time:18166ms step_avg:87.76ms
step:208/1670 train_time:18254ms step_avg:87.76ms
step:209/1670 train_time:18342ms step_avg:87.76ms
step:210/1670 train_time:18429ms step_avg:87.76ms
step:211/1670 train_time:18516ms step_avg:87.75ms
step:212/1670 train_time:18604ms step_avg:87.75ms
step:213/1670 train_time:18692ms step_avg:87.75ms
step:214/1670 train_time:18779ms step_avg:87.75ms
step:215/1670 train_time:18867ms step_avg:87.75ms
step:216/1670 train_time:18953ms step_avg:87.75ms
step:217/1670 train_time:19041ms step_avg:87.75ms
step:218/1670 train_time:19128ms step_avg:87.74ms
step:219/1670 train_time:19216ms step_avg:87.74ms
step:220/1670 train_time:19303ms step_avg:87.74ms
step:221/1670 train_time:19391ms step_avg:87.74ms
step:222/1670 train_time:19479ms step_avg:87.74ms
step:223/1670 train_time:19567ms step_avg:87.74ms
step:224/1670 train_time:19654ms step_avg:87.74ms
step:225/1670 train_time:19742ms step_avg:87.74ms
step:226/1670 train_time:19830ms step_avg:87.74ms
step:227/1670 train_time:19917ms step_avg:87.74ms
step:228/1670 train_time:20005ms step_avg:87.74ms
step:229/1670 train_time:20092ms step_avg:87.74ms
step:230/1670 train_time:20180ms step_avg:87.74ms
step:231/1670 train_time:20268ms step_avg:87.74ms
step:232/1670 train_time:20355ms step_avg:87.74ms
step:233/1670 train_time:20443ms step_avg:87.74ms
step:234/1670 train_time:20530ms step_avg:87.74ms
step:235/1670 train_time:20618ms step_avg:87.74ms
step:236/1670 train_time:20706ms step_avg:87.74ms
step:237/1670 train_time:20793ms step_avg:87.73ms
step:238/1670 train_time:20880ms step_avg:87.73ms
step:239/1670 train_time:20969ms step_avg:87.74ms
step:240/1670 train_time:21056ms step_avg:87.73ms
step:241/1670 train_time:21144ms step_avg:87.73ms
step:242/1670 train_time:21232ms step_avg:87.74ms
step:243/1670 train_time:21320ms step_avg:87.74ms
step:244/1670 train_time:21407ms step_avg:87.74ms
step:245/1670 train_time:21495ms step_avg:87.73ms
step:246/1670 train_time:21582ms step_avg:87.73ms
step:247/1670 train_time:21670ms step_avg:87.73ms
step:248/1670 train_time:21758ms step_avg:87.73ms
step:249/1670 train_time:21846ms step_avg:87.74ms
step:250/1670 train_time:21933ms step_avg:87.73ms
step:250/1670 val_loss:3.9771 train_time:22022ms step_avg:88.09ms
step:251/1670 train_time:22042ms step_avg:87.82ms
step:252/1670 train_time:22112ms step_avg:87.75ms
step:253/1670 train_time:22204ms step_avg:87.76ms
step:254/1670 train_time:22291ms step_avg:87.76ms
step:255/1670 train_time:22379ms step_avg:87.76ms
step:256/1670 train_time:22465ms step_avg:87.76ms
step:257/1670 train_time:22552ms step_avg:87.75ms
step:258/1670 train_time:22639ms step_avg:87.75ms
step:259/1670 train_time:22725ms step_avg:87.74ms
step:260/1670 train_time:22813ms step_avg:87.74ms
step:261/1670 train_time:22899ms step_avg:87.74ms
step:262/1670 train_time:22988ms step_avg:87.74ms
step:263/1670 train_time:23079ms step_avg:87.75ms
step:264/1670 train_time:23169ms step_avg:87.76ms
step:265/1670 train_time:23257ms step_avg:87.76ms
step:266/1670 train_time:23345ms step_avg:87.76ms
step:267/1670 train_time:23432ms step_avg:87.76ms
step:268/1670 train_time:23518ms step_avg:87.76ms
step:269/1670 train_time:23605ms step_avg:87.75ms
step:270/1670 train_time:23692ms step_avg:87.75ms
step:271/1670 train_time:23779ms step_avg:87.75ms
step:272/1670 train_time:23866ms step_avg:87.74ms
step:273/1670 train_time:23954ms step_avg:87.74ms
step:274/1670 train_time:24042ms step_avg:87.75ms
step:275/1670 train_time:24130ms step_avg:87.75ms
step:276/1670 train_time:24219ms step_avg:87.75ms
step:277/1670 train_time:24307ms step_avg:87.75ms
step:278/1670 train_time:24395ms step_avg:87.75ms
step:279/1670 train_time:24482ms step_avg:87.75ms
step:280/1670 train_time:24570ms step_avg:87.75ms
step:281/1670 train_time:24657ms step_avg:87.75ms
step:282/1670 train_time:24744ms step_avg:87.75ms
step:283/1670 train_time:24832ms step_avg:87.74ms
step:284/1670 train_time:24919ms step_avg:87.74ms
step:285/1670 train_time:25007ms step_avg:87.74ms
step:286/1670 train_time:25096ms step_avg:87.75ms
step:287/1670 train_time:25186ms step_avg:87.75ms
step:288/1670 train_time:25273ms step_avg:87.75ms
step:289/1670 train_time:25361ms step_avg:87.75ms
step:290/1670 train_time:25448ms step_avg:87.75ms
step:291/1670 train_time:25536ms step_avg:87.75ms
step:292/1670 train_time:25623ms step_avg:87.75ms
step:293/1670 train_time:25710ms step_avg:87.75ms
step:294/1670 train_time:25798ms step_avg:87.75ms
step:295/1670 train_time:25885ms step_avg:87.75ms
step:296/1670 train_time:25974ms step_avg:87.75ms
step:297/1670 train_time:26061ms step_avg:87.75ms
step:298/1670 train_time:26149ms step_avg:87.75ms
step:299/1670 train_time:26238ms step_avg:87.75ms
step:300/1670 train_time:26325ms step_avg:87.75ms
step:301/1670 train_time:26413ms step_avg:87.75ms
step:302/1670 train_time:26500ms step_avg:87.75ms
step:303/1670 train_time:26588ms step_avg:87.75ms
step:304/1670 train_time:26676ms step_avg:87.75ms
step:305/1670 train_time:26762ms step_avg:87.75ms
step:306/1670 train_time:26850ms step_avg:87.74ms
step:307/1670 train_time:26937ms step_avg:87.74ms
step:308/1670 train_time:27025ms step_avg:87.74ms
step:309/1670 train_time:27113ms step_avg:87.74ms
step:310/1670 train_time:27201ms step_avg:87.74ms
step:311/1670 train_time:27289ms step_avg:87.75ms
step:312/1670 train_time:27377ms step_avg:87.75ms
step:313/1670 train_time:27463ms step_avg:87.74ms
step:314/1670 train_time:27551ms step_avg:87.74ms
step:315/1670 train_time:27638ms step_avg:87.74ms
step:316/1670 train_time:27725ms step_avg:87.74ms
step:317/1670 train_time:27813ms step_avg:87.74ms
step:318/1670 train_time:27900ms step_avg:87.74ms
step:319/1670 train_time:27988ms step_avg:87.74ms
step:320/1670 train_time:28077ms step_avg:87.74ms
step:321/1670 train_time:28164ms step_avg:87.74ms
step:322/1670 train_time:28252ms step_avg:87.74ms
step:323/1670 train_time:28340ms step_avg:87.74ms
step:324/1670 train_time:28428ms step_avg:87.74ms
step:325/1670 train_time:28516ms step_avg:87.74ms
step:326/1670 train_time:28603ms step_avg:87.74ms
step:327/1670 train_time:28690ms step_avg:87.74ms
step:328/1670 train_time:28778ms step_avg:87.74ms
step:329/1670 train_time:28865ms step_avg:87.74ms
step:330/1670 train_time:28953ms step_avg:87.74ms
step:331/1670 train_time:29040ms step_avg:87.74ms
step:332/1670 train_time:29128ms step_avg:87.74ms
step:333/1670 train_time:29216ms step_avg:87.74ms
step:334/1670 train_time:29304ms step_avg:87.74ms
step:335/1670 train_time:29391ms step_avg:87.73ms
step:336/1670 train_time:29479ms step_avg:87.73ms
step:337/1670 train_time:29567ms step_avg:87.73ms
step:338/1670 train_time:29655ms step_avg:87.74ms
step:339/1670 train_time:29743ms step_avg:87.74ms
step:340/1670 train_time:29830ms step_avg:87.74ms
step:341/1670 train_time:29918ms step_avg:87.74ms
step:342/1670 train_time:30006ms step_avg:87.74ms
step:343/1670 train_time:30094ms step_avg:87.74ms
step:344/1670 train_time:30181ms step_avg:87.74ms
step:345/1670 train_time:30270ms step_avg:87.74ms
step:346/1670 train_time:30358ms step_avg:87.74ms
step:347/1670 train_time:30445ms step_avg:87.74ms
step:348/1670 train_time:30533ms step_avg:87.74ms
step:349/1670 train_time:30621ms step_avg:87.74ms
step:350/1670 train_time:30708ms step_avg:87.74ms
step:351/1670 train_time:30797ms step_avg:87.74ms
step:352/1670 train_time:30884ms step_avg:87.74ms
step:353/1670 train_time:30972ms step_avg:87.74ms
step:354/1670 train_time:31059ms step_avg:87.74ms
step:355/1670 train_time:31147ms step_avg:87.74ms
step:356/1670 train_time:31235ms step_avg:87.74ms
step:357/1670 train_time:31322ms step_avg:87.74ms
step:358/1670 train_time:31410ms step_avg:87.74ms
step:359/1670 train_time:31498ms step_avg:87.74ms
step:360/1670 train_time:31585ms step_avg:87.74ms
step:361/1670 train_time:31673ms step_avg:87.74ms
step:362/1670 train_time:31760ms step_avg:87.74ms
step:363/1670 train_time:31848ms step_avg:87.73ms
step:364/1670 train_time:31936ms step_avg:87.74ms
step:365/1670 train_time:32023ms step_avg:87.73ms
step:366/1670 train_time:32113ms step_avg:87.74ms
step:367/1670 train_time:32200ms step_avg:87.74ms
step:368/1670 train_time:32287ms step_avg:87.74ms
step:369/1670 train_time:32375ms step_avg:87.74ms
step:370/1670 train_time:32462ms step_avg:87.74ms
step:371/1670 train_time:32550ms step_avg:87.74ms
step:372/1670 train_time:32638ms step_avg:87.74ms
step:373/1670 train_time:32726ms step_avg:87.74ms
step:374/1670 train_time:32814ms step_avg:87.74ms
step:375/1670 train_time:32901ms step_avg:87.74ms
step:375/1670 val_loss:3.8204 train_time:32989ms step_avg:87.97ms
step:376/1670 train_time:33010ms step_avg:87.79ms
step:377/1670 train_time:33082ms step_avg:87.75ms
step:378/1670 train_time:33173ms step_avg:87.76ms
step:379/1670 train_time:33262ms step_avg:87.76ms
step:380/1670 train_time:33349ms step_avg:87.76ms
step:381/1670 train_time:33436ms step_avg:87.76ms
step:382/1670 train_time:33524ms step_avg:87.76ms
step:383/1670 train_time:33609ms step_avg:87.75ms
step:384/1670 train_time:33696ms step_avg:87.75ms
step:385/1670 train_time:33784ms step_avg:87.75ms
step:386/1670 train_time:33870ms step_avg:87.75ms
step:387/1670 train_time:33959ms step_avg:87.75ms
step:388/1670 train_time:34050ms step_avg:87.76ms
step:389/1670 train_time:34140ms step_avg:87.76ms
step:390/1670 train_time:34230ms step_avg:87.77ms
step:391/1670 train_time:34317ms step_avg:87.77ms
step:392/1670 train_time:34405ms step_avg:87.77ms
step:393/1670 train_time:34492ms step_avg:87.77ms
step:394/1670 train_time:34579ms step_avg:87.76ms
step:395/1670 train_time:34665ms step_avg:87.76ms
step:396/1670 train_time:34752ms step_avg:87.76ms
step:397/1670 train_time:34839ms step_avg:87.75ms
step:398/1670 train_time:34926ms step_avg:87.75ms
step:399/1670 train_time:35014ms step_avg:87.75ms
step:400/1670 train_time:35104ms step_avg:87.76ms
step:401/1670 train_time:35192ms step_avg:87.76ms
step:402/1670 train_time:35280ms step_avg:87.76ms
step:403/1670 train_time:35368ms step_avg:87.76ms
step:404/1670 train_time:35456ms step_avg:87.76ms
step:405/1670 train_time:35543ms step_avg:87.76ms
step:406/1670 train_time:35630ms step_avg:87.76ms
step:407/1670 train_time:35717ms step_avg:87.76ms
step:408/1670 train_time:35804ms step_avg:87.76ms
step:409/1670 train_time:35891ms step_avg:87.75ms
step:410/1670 train_time:35979ms step_avg:87.75ms
step:411/1670 train_time:36068ms step_avg:87.76ms
step:412/1670 train_time:36156ms step_avg:87.76ms
step:413/1670 train_time:36244ms step_avg:87.76ms
step:414/1670 train_time:36332ms step_avg:87.76ms
step:415/1670 train_time:36419ms step_avg:87.76ms
step:416/1670 train_time:36507ms step_avg:87.76ms
step:417/1670 train_time:36594ms step_avg:87.76ms
step:418/1670 train_time:36681ms step_avg:87.75ms
step:419/1670 train_time:36768ms step_avg:87.75ms
step:420/1670 train_time:36855ms step_avg:87.75ms
step:421/1670 train_time:36943ms step_avg:87.75ms
step:422/1670 train_time:37030ms step_avg:87.75ms
step:423/1670 train_time:37119ms step_avg:87.75ms
step:424/1670 train_time:37207ms step_avg:87.75ms
step:425/1670 train_time:37295ms step_avg:87.75ms
step:426/1670 train_time:37383ms step_avg:87.75ms
step:427/1670 train_time:37470ms step_avg:87.75ms
step:428/1670 train_time:37558ms step_avg:87.75ms
step:429/1670 train_time:37645ms step_avg:87.75ms
step:430/1670 train_time:37733ms step_avg:87.75ms
step:431/1670 train_time:37820ms step_avg:87.75ms
step:432/1670 train_time:37907ms step_avg:87.75ms
step:433/1670 train_time:37995ms step_avg:87.75ms
step:434/1670 train_time:38083ms step_avg:87.75ms
step:435/1670 train_time:38170ms step_avg:87.75ms
step:436/1670 train_time:38258ms step_avg:87.75ms
step:437/1670 train_time:38346ms step_avg:87.75ms
step:438/1670 train_time:38434ms step_avg:87.75ms
step:439/1670 train_time:38523ms step_avg:87.75ms
step:440/1670 train_time:38610ms step_avg:87.75ms
step:441/1670 train_time:38697ms step_avg:87.75ms
step:442/1670 train_time:38785ms step_avg:87.75ms
step:443/1670 train_time:38872ms step_avg:87.75ms
step:444/1670 train_time:38960ms step_avg:87.75ms
step:445/1670 train_time:39047ms step_avg:87.75ms
step:446/1670 train_time:39135ms step_avg:87.75ms
step:447/1670 train_time:39224ms step_avg:87.75ms
step:448/1670 train_time:39311ms step_avg:87.75ms
step:449/1670 train_time:39399ms step_avg:87.75ms
step:450/1670 train_time:39487ms step_avg:87.75ms
step:451/1670 train_time:39574ms step_avg:87.75ms
step:452/1670 train_time:39662ms step_avg:87.75ms
step:453/1670 train_time:39749ms step_avg:87.75ms
step:454/1670 train_time:39836ms step_avg:87.74ms
step:455/1670 train_time:39924ms step_avg:87.75ms
step:456/1670 train_time:40011ms step_avg:87.74ms
step:457/1670 train_time:40099ms step_avg:87.74ms
step:458/1670 train_time:40187ms step_avg:87.74ms
step:459/1670 train_time:40274ms step_avg:87.74ms
step:460/1670 train_time:40363ms step_avg:87.75ms
step:461/1670 train_time:40450ms step_avg:87.74ms
step:462/1670 train_time:40537ms step_avg:87.74ms
step:463/1670 train_time:40626ms step_avg:87.75ms
step:464/1670 train_time:40714ms step_avg:87.75ms
step:465/1670 train_time:40801ms step_avg:87.74ms
step:466/1670 train_time:40889ms step_avg:87.74ms
step:467/1670 train_time:40976ms step_avg:87.74ms
step:468/1670 train_time:41064ms step_avg:87.74ms
step:469/1670 train_time:41151ms step_avg:87.74ms
step:470/1670 train_time:41240ms step_avg:87.74ms
step:471/1670 train_time:41328ms step_avg:87.75ms
step:472/1670 train_time:41416ms step_avg:87.75ms
step:473/1670 train_time:41504ms step_avg:87.75ms
step:474/1670 train_time:41591ms step_avg:87.75ms
step:475/1670 train_time:41679ms step_avg:87.74ms
step:476/1670 train_time:41767ms step_avg:87.75ms
step:477/1670 train_time:41855ms step_avg:87.75ms
step:478/1670 train_time:41942ms step_avg:87.74ms
step:479/1670 train_time:42029ms step_avg:87.74ms
step:480/1670 train_time:42117ms step_avg:87.74ms
step:481/1670 train_time:42205ms step_avg:87.74ms
step:482/1670 train_time:42293ms step_avg:87.74ms
step:483/1670 train_time:42380ms step_avg:87.74ms
step:484/1670 train_time:42468ms step_avg:87.74ms
step:485/1670 train_time:42555ms step_avg:87.74ms
step:486/1670 train_time:42642ms step_avg:87.74ms
step:487/1670 train_time:42730ms step_avg:87.74ms
step:488/1670 train_time:42817ms step_avg:87.74ms
step:489/1670 train_time:42905ms step_avg:87.74ms
step:490/1670 train_time:42993ms step_avg:87.74ms
step:491/1670 train_time:43081ms step_avg:87.74ms
step:492/1670 train_time:43168ms step_avg:87.74ms
step:493/1670 train_time:43256ms step_avg:87.74ms
step:494/1670 train_time:43344ms step_avg:87.74ms
step:495/1670 train_time:43431ms step_avg:87.74ms
step:496/1670 train_time:43519ms step_avg:87.74ms
step:497/1670 train_time:43607ms step_avg:87.74ms
step:498/1670 train_time:43694ms step_avg:87.74ms
step:499/1670 train_time:43783ms step_avg:87.74ms
step:500/1670 train_time:43870ms step_avg:87.74ms
step:500/1670 val_loss:3.7193 train_time:43960ms step_avg:87.92ms
step:501/1670 train_time:43981ms step_avg:87.79ms
step:502/1670 train_time:44049ms step_avg:87.75ms
step:503/1670 train_time:44140ms step_avg:87.75ms
step:504/1670 train_time:44227ms step_avg:87.75ms
step:505/1670 train_time:44314ms step_avg:87.75ms
step:506/1670 train_time:44402ms step_avg:87.75ms
step:507/1670 train_time:44489ms step_avg:87.75ms
step:508/1670 train_time:44576ms step_avg:87.75ms
step:509/1670 train_time:44663ms step_avg:87.75ms
step:510/1670 train_time:44751ms step_avg:87.75ms
step:511/1670 train_time:44837ms step_avg:87.74ms
step:512/1670 train_time:44926ms step_avg:87.75ms
step:513/1670 train_time:45016ms step_avg:87.75ms
step:514/1670 train_time:45104ms step_avg:87.75ms
step:515/1670 train_time:45192ms step_avg:87.75ms
step:516/1670 train_time:45280ms step_avg:87.75ms
step:517/1670 train_time:45367ms step_avg:87.75ms
step:518/1670 train_time:45455ms step_avg:87.75ms
step:519/1670 train_time:45542ms step_avg:87.75ms
step:520/1670 train_time:45630ms step_avg:87.75ms
step:521/1670 train_time:45717ms step_avg:87.75ms
step:522/1670 train_time:45803ms step_avg:87.75ms
step:523/1670 train_time:45891ms step_avg:87.75ms
step:524/1670 train_time:45979ms step_avg:87.75ms
step:525/1670 train_time:46068ms step_avg:87.75ms
step:526/1670 train_time:46158ms step_avg:87.75ms
step:527/1670 train_time:46245ms step_avg:87.75ms
step:528/1670 train_time:46332ms step_avg:87.75ms
step:529/1670 train_time:46421ms step_avg:87.75ms
step:530/1670 train_time:46507ms step_avg:87.75ms
step:531/1670 train_time:46595ms step_avg:87.75ms
step:532/1670 train_time:46683ms step_avg:87.75ms
step:533/1670 train_time:46770ms step_avg:87.75ms
step:534/1670 train_time:46859ms step_avg:87.75ms
step:535/1670 train_time:46946ms step_avg:87.75ms
step:536/1670 train_time:47034ms step_avg:87.75ms
step:537/1670 train_time:47123ms step_avg:87.75ms
step:538/1670 train_time:47211ms step_avg:87.75ms
step:539/1670 train_time:47300ms step_avg:87.76ms
step:540/1670 train_time:47388ms step_avg:87.75ms
step:541/1670 train_time:47476ms step_avg:87.76ms
step:542/1670 train_time:47563ms step_avg:87.75ms
step:543/1670 train_time:47650ms step_avg:87.75ms
step:544/1670 train_time:47738ms step_avg:87.75ms
step:545/1670 train_time:47826ms step_avg:87.75ms
step:546/1670 train_time:47916ms step_avg:87.76ms
step:547/1670 train_time:48004ms step_avg:87.76ms
step:548/1670 train_time:48094ms step_avg:87.76ms
step:549/1670 train_time:48183ms step_avg:87.77ms
step:550/1670 train_time:48273ms step_avg:87.77ms
step:551/1670 train_time:48361ms step_avg:87.77ms
step:552/1670 train_time:48450ms step_avg:87.77ms
step:553/1670 train_time:48539ms step_avg:87.77ms
step:554/1670 train_time:48628ms step_avg:87.78ms
step:555/1670 train_time:48717ms step_avg:87.78ms
step:556/1670 train_time:48806ms step_avg:87.78ms
step:557/1670 train_time:48895ms step_avg:87.78ms
step:558/1670 train_time:48984ms step_avg:87.78ms
step:559/1670 train_time:49073ms step_avg:87.79ms
step:560/1670 train_time:49163ms step_avg:87.79ms
step:561/1670 train_time:49252ms step_avg:87.79ms
step:562/1670 train_time:49341ms step_avg:87.80ms
step:563/1670 train_time:49430ms step_avg:87.80ms
step:564/1670 train_time:49520ms step_avg:87.80ms
step:565/1670 train_time:49608ms step_avg:87.80ms
step:566/1670 train_time:49697ms step_avg:87.80ms
step:567/1670 train_time:49786ms step_avg:87.81ms
step:568/1670 train_time:49875ms step_avg:87.81ms
step:569/1670 train_time:49964ms step_avg:87.81ms
step:570/1670 train_time:50052ms step_avg:87.81ms
step:571/1670 train_time:50141ms step_avg:87.81ms
step:572/1670 train_time:50231ms step_avg:87.82ms
step:573/1670 train_time:50320ms step_avg:87.82ms
step:574/1670 train_time:50409ms step_avg:87.82ms
step:575/1670 train_time:50498ms step_avg:87.82ms
step:576/1670 train_time:50586ms step_avg:87.82ms
step:577/1670 train_time:50675ms step_avg:87.83ms
step:578/1670 train_time:50764ms step_avg:87.83ms
step:579/1670 train_time:50853ms step_avg:87.83ms
step:580/1670 train_time:50941ms step_avg:87.83ms
step:581/1670 train_time:51030ms step_avg:87.83ms
step:582/1670 train_time:51119ms step_avg:87.83ms
step:583/1670 train_time:51208ms step_avg:87.84ms
step:584/1670 train_time:51298ms step_avg:87.84ms
step:585/1670 train_time:51387ms step_avg:87.84ms
step:586/1670 train_time:51477ms step_avg:87.84ms
step:587/1670 train_time:51565ms step_avg:87.84ms
step:588/1670 train_time:51654ms step_avg:87.85ms
step:589/1670 train_time:51742ms step_avg:87.85ms
step:590/1670 train_time:51831ms step_avg:87.85ms
step:591/1670 train_time:51920ms step_avg:87.85ms
step:592/1670 train_time:52009ms step_avg:87.85ms
step:593/1670 train_time:52099ms step_avg:87.86ms
step:594/1670 train_time:52188ms step_avg:87.86ms
step:595/1670 train_time:52278ms step_avg:87.86ms
step:596/1670 train_time:52366ms step_avg:87.86ms
step:597/1670 train_time:52455ms step_avg:87.86ms
step:598/1670 train_time:52544ms step_avg:87.87ms
step:599/1670 train_time:52633ms step_avg:87.87ms
step:600/1670 train_time:52722ms step_avg:87.87ms
step:601/1670 train_time:52811ms step_avg:87.87ms
step:602/1670 train_time:52899ms step_avg:87.87ms
step:603/1670 train_time:52988ms step_avg:87.87ms
step:604/1670 train_time:53076ms step_avg:87.87ms
step:605/1670 train_time:53165ms step_avg:87.88ms
step:606/1670 train_time:53254ms step_avg:87.88ms
step:607/1670 train_time:53343ms step_avg:87.88ms
step:608/1670 train_time:53432ms step_avg:87.88ms
step:609/1670 train_time:53522ms step_avg:87.88ms
step:610/1670 train_time:53612ms step_avg:87.89ms
step:611/1670 train_time:53702ms step_avg:87.89ms
step:612/1670 train_time:53791ms step_avg:87.89ms
step:613/1670 train_time:53882ms step_avg:87.90ms
step:614/1670 train_time:53971ms step_avg:87.90ms
step:615/1670 train_time:54060ms step_avg:87.90ms
step:616/1670 train_time:54148ms step_avg:87.90ms
step:617/1670 train_time:54237ms step_avg:87.90ms
step:618/1670 train_time:54325ms step_avg:87.91ms
step:619/1670 train_time:54415ms step_avg:87.91ms
step:620/1670 train_time:54504ms step_avg:87.91ms
step:621/1670 train_time:54593ms step_avg:87.91ms
step:622/1670 train_time:54682ms step_avg:87.91ms
step:623/1670 train_time:54771ms step_avg:87.91ms
step:624/1670 train_time:54860ms step_avg:87.92ms
step:625/1670 train_time:54950ms step_avg:87.92ms
step:625/1670 val_loss:3.6189 train_time:55041ms step_avg:88.07ms
step:626/1670 train_time:55060ms step_avg:87.96ms
step:627/1670 train_time:55131ms step_avg:87.93ms
step:628/1670 train_time:55220ms step_avg:87.93ms
step:629/1670 train_time:55310ms step_avg:87.93ms
step:630/1670 train_time:55398ms step_avg:87.93ms
step:631/1670 train_time:55486ms step_avg:87.93ms
step:632/1670 train_time:55574ms step_avg:87.93ms
step:633/1670 train_time:55661ms step_avg:87.93ms
step:634/1670 train_time:55749ms step_avg:87.93ms
step:635/1670 train_time:55838ms step_avg:87.93ms
step:636/1670 train_time:55926ms step_avg:87.93ms
step:637/1670 train_time:56019ms step_avg:87.94ms
step:638/1670 train_time:56108ms step_avg:87.94ms
step:639/1670 train_time:56199ms step_avg:87.95ms
step:640/1670 train_time:56288ms step_avg:87.95ms
step:641/1670 train_time:56377ms step_avg:87.95ms
step:642/1670 train_time:56465ms step_avg:87.95ms
step:643/1670 train_time:56552ms step_avg:87.95ms
step:644/1670 train_time:56640ms step_avg:87.95ms
step:645/1670 train_time:56728ms step_avg:87.95ms
step:646/1670 train_time:56817ms step_avg:87.95ms
step:647/1670 train_time:56906ms step_avg:87.95ms
step:648/1670 train_time:56996ms step_avg:87.96ms
step:649/1670 train_time:57086ms step_avg:87.96ms
step:650/1670 train_time:57176ms step_avg:87.96ms
step:651/1670 train_time:57265ms step_avg:87.96ms
step:652/1670 train_time:57353ms step_avg:87.96ms
step:653/1670 train_time:57442ms step_avg:87.97ms
step:654/1670 train_time:57530ms step_avg:87.97ms
step:655/1670 train_time:57618ms step_avg:87.97ms
step:656/1670 train_time:57706ms step_avg:87.97ms
step:657/1670 train_time:57795ms step_avg:87.97ms
step:658/1670 train_time:57883ms step_avg:87.97ms
step:659/1670 train_time:57973ms step_avg:87.97ms
step:660/1670 train_time:58063ms step_avg:87.97ms
step:661/1670 train_time:58152ms step_avg:87.98ms
step:662/1670 train_time:58241ms step_avg:87.98ms
step:663/1670 train_time:58330ms step_avg:87.98ms
step:664/1670 train_time:58419ms step_avg:87.98ms
step:665/1670 train_time:58507ms step_avg:87.98ms
step:666/1670 train_time:58596ms step_avg:87.98ms
step:667/1670 train_time:58685ms step_avg:87.98ms
step:668/1670 train_time:58773ms step_avg:87.98ms
step:669/1670 train_time:58863ms step_avg:87.99ms
step:670/1670 train_time:58952ms step_avg:87.99ms
step:671/1670 train_time:59042ms step_avg:87.99ms
step:672/1670 train_time:59130ms step_avg:87.99ms
step:673/1670 train_time:59220ms step_avg:87.99ms
step:674/1670 train_time:59309ms step_avg:87.99ms
step:675/1670 train_time:59398ms step_avg:88.00ms
step:676/1670 train_time:59486ms step_avg:88.00ms
step:677/1670 train_time:59575ms step_avg:88.00ms
step:678/1670 train_time:59664ms step_avg:88.00ms
step:679/1670 train_time:59753ms step_avg:88.00ms
step:680/1670 train_time:59842ms step_avg:88.00ms
step:681/1670 train_time:59930ms step_avg:88.00ms
step:682/1670 train_time:60020ms step_avg:88.01ms
step:683/1670 train_time:60109ms step_avg:88.01ms
step:684/1670 train_time:60199ms step_avg:88.01ms
step:685/1670 train_time:60287ms step_avg:88.01ms
step:686/1670 train_time:60376ms step_avg:88.01ms
step:687/1670 train_time:60465ms step_avg:88.01ms
step:688/1670 train_time:60554ms step_avg:88.01ms
step:689/1670 train_time:60643ms step_avg:88.02ms
step:690/1670 train_time:60731ms step_avg:88.02ms
step:691/1670 train_time:60820ms step_avg:88.02ms
step:692/1670 train_time:60908ms step_avg:88.02ms
step:693/1670 train_time:60999ms step_avg:88.02ms
step:694/1670 train_time:61087ms step_avg:88.02ms
step:695/1670 train_time:61176ms step_avg:88.02ms
step:696/1670 train_time:61266ms step_avg:88.03ms
step:697/1670 train_time:61355ms step_avg:88.03ms
step:698/1670 train_time:61445ms step_avg:88.03ms
step:699/1670 train_time:61534ms step_avg:88.03ms
step:700/1670 train_time:61624ms step_avg:88.03ms
step:701/1670 train_time:61713ms step_avg:88.04ms
step:702/1670 train_time:61802ms step_avg:88.04ms
step:703/1670 train_time:61890ms step_avg:88.04ms
step:704/1670 train_time:61979ms step_avg:88.04ms
step:705/1670 train_time:62068ms step_avg:88.04ms
step:706/1670 train_time:62157ms step_avg:88.04ms
step:707/1670 train_time:62246ms step_avg:88.04ms
step:708/1670 train_time:62334ms step_avg:88.04ms
step:709/1670 train_time:62423ms step_avg:88.04ms
step:710/1670 train_time:62512ms step_avg:88.04ms
step:711/1670 train_time:62602ms step_avg:88.05ms
step:712/1670 train_time:62690ms step_avg:88.05ms
step:713/1670 train_time:62779ms step_avg:88.05ms
step:714/1670 train_time:62867ms step_avg:88.05ms
step:715/1670 train_time:62956ms step_avg:88.05ms
step:716/1670 train_time:63045ms step_avg:88.05ms
step:717/1670 train_time:63134ms step_avg:88.05ms
step:718/1670 train_time:63224ms step_avg:88.06ms
step:719/1670 train_time:63313ms step_avg:88.06ms
step:720/1670 train_time:63402ms step_avg:88.06ms
step:721/1670 train_time:63490ms step_avg:88.06ms
step:722/1670 train_time:63579ms step_avg:88.06ms
step:723/1670 train_time:63668ms step_avg:88.06ms
step:724/1670 train_time:63757ms step_avg:88.06ms
step:725/1670 train_time:63845ms step_avg:88.06ms
step:726/1670 train_time:63934ms step_avg:88.06ms
step:727/1670 train_time:64023ms step_avg:88.06ms
step:728/1670 train_time:64111ms step_avg:88.06ms
step:729/1670 train_time:64200ms step_avg:88.07ms
step:730/1670 train_time:64289ms step_avg:88.07ms
step:731/1670 train_time:64377ms step_avg:88.07ms
step:732/1670 train_time:64466ms step_avg:88.07ms
step:733/1670 train_time:64556ms step_avg:88.07ms
step:734/1670 train_time:64645ms step_avg:88.07ms
step:735/1670 train_time:64734ms step_avg:88.07ms
step:736/1670 train_time:64823ms step_avg:88.08ms
step:737/1670 train_time:64912ms step_avg:88.08ms
step:738/1670 train_time:65001ms step_avg:88.08ms
step:739/1670 train_time:65089ms step_avg:88.08ms
step:740/1670 train_time:65178ms step_avg:88.08ms
step:741/1670 train_time:65267ms step_avg:88.08ms
step:742/1670 train_time:65357ms step_avg:88.08ms
step:743/1670 train_time:65445ms step_avg:88.08ms
step:744/1670 train_time:65534ms step_avg:88.08ms
step:745/1670 train_time:65624ms step_avg:88.09ms
step:746/1670 train_time:65713ms step_avg:88.09ms
step:747/1670 train_time:65802ms step_avg:88.09ms
step:748/1670 train_time:65890ms step_avg:88.09ms
step:749/1670 train_time:65979ms step_avg:88.09ms
step:750/1670 train_time:66068ms step_avg:88.09ms
step:750/1670 val_loss:3.5662 train_time:66159ms step_avg:88.21ms
step:751/1670 train_time:66178ms step_avg:88.12ms
step:752/1670 train_time:66250ms step_avg:88.10ms
step:753/1670 train_time:66346ms step_avg:88.11ms
step:754/1670 train_time:66436ms step_avg:88.11ms
step:755/1670 train_time:66524ms step_avg:88.11ms
step:756/1670 train_time:66612ms step_avg:88.11ms
step:757/1670 train_time:66700ms step_avg:88.11ms
step:758/1670 train_time:66788ms step_avg:88.11ms
step:759/1670 train_time:66876ms step_avg:88.11ms
step:760/1670 train_time:66965ms step_avg:88.11ms
step:761/1670 train_time:67054ms step_avg:88.11ms
step:762/1670 train_time:67143ms step_avg:88.11ms
step:763/1670 train_time:67234ms step_avg:88.12ms
step:764/1670 train_time:67324ms step_avg:88.12ms
step:765/1670 train_time:67415ms step_avg:88.12ms
step:766/1670 train_time:67504ms step_avg:88.13ms
step:767/1670 train_time:67594ms step_avg:88.13ms
step:768/1670 train_time:67682ms step_avg:88.13ms
step:769/1670 train_time:67771ms step_avg:88.13ms
step:770/1670 train_time:67860ms step_avg:88.13ms
step:771/1670 train_time:67948ms step_avg:88.13ms
step:772/1670 train_time:68037ms step_avg:88.13ms
step:773/1670 train_time:68126ms step_avg:88.13ms
step:774/1670 train_time:68215ms step_avg:88.13ms
step:775/1670 train_time:68305ms step_avg:88.14ms
step:776/1670 train_time:68395ms step_avg:88.14ms
step:777/1670 train_time:68485ms step_avg:88.14ms
step:778/1670 train_time:68575ms step_avg:88.14ms
step:779/1670 train_time:68664ms step_avg:88.14ms
step:780/1670 train_time:68753ms step_avg:88.14ms
step:781/1670 train_time:68842ms step_avg:88.15ms
step:782/1670 train_time:68930ms step_avg:88.15ms
step:783/1670 train_time:69019ms step_avg:88.15ms
step:784/1670 train_time:69108ms step_avg:88.15ms
step:785/1670 train_time:69197ms step_avg:88.15ms
step:786/1670 train_time:69286ms step_avg:88.15ms
step:787/1670 train_time:69376ms step_avg:88.15ms
step:788/1670 train_time:69465ms step_avg:88.15ms
step:789/1670 train_time:69555ms step_avg:88.16ms
step:790/1670 train_time:69644ms step_avg:88.16ms
step:791/1670 train_time:69733ms step_avg:88.16ms
step:792/1670 train_time:69822ms step_avg:88.16ms
step:793/1670 train_time:69910ms step_avg:88.16ms
step:794/1670 train_time:70000ms step_avg:88.16ms
step:795/1670 train_time:70088ms step_avg:88.16ms
step:796/1670 train_time:70178ms step_avg:88.16ms
step:797/1670 train_time:70267ms step_avg:88.16ms
step:798/1670 train_time:70357ms step_avg:88.17ms
step:799/1670 train_time:70446ms step_avg:88.17ms
step:800/1670 train_time:70535ms step_avg:88.17ms
step:801/1670 train_time:70624ms step_avg:88.17ms
step:802/1670 train_time:70712ms step_avg:88.17ms
step:803/1670 train_time:70802ms step_avg:88.17ms
step:804/1670 train_time:70890ms step_avg:88.17ms
step:805/1670 train_time:70979ms step_avg:88.17ms
step:806/1670 train_time:71068ms step_avg:88.17ms
step:807/1670 train_time:71158ms step_avg:88.18ms
step:808/1670 train_time:71247ms step_avg:88.18ms
step:809/1670 train_time:71337ms step_avg:88.18ms
step:810/1670 train_time:71425ms step_avg:88.18ms
step:811/1670 train_time:71515ms step_avg:88.18ms
step:812/1670 train_time:71603ms step_avg:88.18ms
step:813/1670 train_time:71692ms step_avg:88.18ms
step:814/1670 train_time:71781ms step_avg:88.18ms
step:815/1670 train_time:71869ms step_avg:88.18ms
step:816/1670 train_time:71959ms step_avg:88.18ms
step:817/1670 train_time:72047ms step_avg:88.18ms
step:818/1670 train_time:72136ms step_avg:88.19ms
step:819/1670 train_time:72224ms step_avg:88.19ms
step:820/1670 train_time:72314ms step_avg:88.19ms
step:821/1670 train_time:72403ms step_avg:88.19ms
step:822/1670 train_time:72493ms step_avg:88.19ms
step:823/1670 train_time:72582ms step_avg:88.19ms
step:824/1670 train_time:72670ms step_avg:88.19ms
step:825/1670 train_time:72759ms step_avg:88.19ms
step:826/1670 train_time:72847ms step_avg:88.19ms
step:827/1670 train_time:72937ms step_avg:88.19ms
step:828/1670 train_time:73025ms step_avg:88.19ms
step:829/1670 train_time:73114ms step_avg:88.20ms
step:830/1670 train_time:73203ms step_avg:88.20ms
step:831/1670 train_time:73292ms step_avg:88.20ms
step:832/1670 train_time:73381ms step_avg:88.20ms
step:833/1670 train_time:73469ms step_avg:88.20ms
step:834/1670 train_time:73558ms step_avg:88.20ms
step:835/1670 train_time:73647ms step_avg:88.20ms
step:836/1670 train_time:73736ms step_avg:88.20ms
step:837/1670 train_time:73825ms step_avg:88.20ms
step:838/1670 train_time:73914ms step_avg:88.20ms
step:839/1670 train_time:74003ms step_avg:88.20ms
step:840/1670 train_time:74093ms step_avg:88.21ms
step:841/1670 train_time:74182ms step_avg:88.21ms
step:842/1670 train_time:74271ms step_avg:88.21ms
step:843/1670 train_time:74360ms step_avg:88.21ms
step:844/1670 train_time:74448ms step_avg:88.21ms
step:845/1670 train_time:74538ms step_avg:88.21ms
step:846/1670 train_time:74626ms step_avg:88.21ms
step:847/1670 train_time:74716ms step_avg:88.21ms
step:848/1670 train_time:74804ms step_avg:88.21ms
step:849/1670 train_time:74894ms step_avg:88.21ms
step:850/1670 train_time:74984ms step_avg:88.22ms
step:851/1670 train_time:75072ms step_avg:88.22ms
step:852/1670 train_time:75162ms step_avg:88.22ms
step:853/1670 train_time:75251ms step_avg:88.22ms
step:854/1670 train_time:75339ms step_avg:88.22ms
step:855/1670 train_time:75429ms step_avg:88.22ms
step:856/1670 train_time:75518ms step_avg:88.22ms
step:857/1670 train_time:75607ms step_avg:88.22ms
step:858/1670 train_time:75696ms step_avg:88.22ms
step:859/1670 train_time:75785ms step_avg:88.22ms
step:860/1670 train_time:75874ms step_avg:88.23ms
step:861/1670 train_time:75964ms step_avg:88.23ms
step:862/1670 train_time:76053ms step_avg:88.23ms
step:863/1670 train_time:76143ms step_avg:88.23ms
step:864/1670 train_time:76232ms step_avg:88.23ms
step:865/1670 train_time:76321ms step_avg:88.23ms
step:866/1670 train_time:76410ms step_avg:88.23ms
step:867/1670 train_time:76499ms step_avg:88.23ms
step:868/1670 train_time:76588ms step_avg:88.23ms
step:869/1670 train_time:76677ms step_avg:88.24ms
step:870/1670 train_time:76766ms step_avg:88.24ms
step:871/1670 train_time:76855ms step_avg:88.24ms
step:872/1670 train_time:76945ms step_avg:88.24ms
step:873/1670 train_time:77034ms step_avg:88.24ms
step:874/1670 train_time:77123ms step_avg:88.24ms
step:875/1670 train_time:77212ms step_avg:88.24ms
step:875/1670 val_loss:3.5193 train_time:77302ms step_avg:88.34ms
step:876/1670 train_time:77321ms step_avg:88.27ms
step:877/1670 train_time:77395ms step_avg:88.25ms
step:878/1670 train_time:77486ms step_avg:88.25ms
step:879/1670 train_time:77576ms step_avg:88.25ms
step:880/1670 train_time:77664ms step_avg:88.25ms
step:881/1670 train_time:77752ms step_avg:88.25ms
step:882/1670 train_time:77839ms step_avg:88.25ms
step:883/1670 train_time:77926ms step_avg:88.25ms
step:884/1670 train_time:78015ms step_avg:88.25ms
step:885/1670 train_time:78103ms step_avg:88.25ms
step:886/1670 train_time:78192ms step_avg:88.25ms
step:887/1670 train_time:78283ms step_avg:88.26ms
step:888/1670 train_time:78376ms step_avg:88.26ms
step:889/1670 train_time:78467ms step_avg:88.26ms
step:890/1670 train_time:78556ms step_avg:88.27ms
step:891/1670 train_time:78644ms step_avg:88.27ms
step:892/1670 train_time:78734ms step_avg:88.27ms
step:893/1670 train_time:78821ms step_avg:88.27ms
step:894/1670 train_time:78908ms step_avg:88.26ms
step:895/1670 train_time:78997ms step_avg:88.26ms
step:896/1670 train_time:79085ms step_avg:88.26ms
step:897/1670 train_time:79173ms step_avg:88.26ms
step:898/1670 train_time:79263ms step_avg:88.27ms
step:899/1670 train_time:79355ms step_avg:88.27ms
step:900/1670 train_time:79446ms step_avg:88.27ms
step:901/1670 train_time:79537ms step_avg:88.28ms
step:902/1670 train_time:79626ms step_avg:88.28ms
step:903/1670 train_time:79715ms step_avg:88.28ms
step:904/1670 train_time:79803ms step_avg:88.28ms
step:905/1670 train_time:79893ms step_avg:88.28ms
step:906/1670 train_time:79981ms step_avg:88.28ms
step:907/1670 train_time:80069ms step_avg:88.28ms
step:908/1670 train_time:80157ms step_avg:88.28ms
step:909/1670 train_time:80246ms step_avg:88.28ms
step:910/1670 train_time:80336ms step_avg:88.28ms
step:911/1670 train_time:80426ms step_avg:88.28ms
step:912/1670 train_time:80517ms step_avg:88.29ms
step:913/1670 train_time:80607ms step_avg:88.29ms
step:914/1670 train_time:80697ms step_avg:88.29ms
step:915/1670 train_time:80786ms step_avg:88.29ms
step:916/1670 train_time:80874ms step_avg:88.29ms
step:917/1670 train_time:80963ms step_avg:88.29ms
step:918/1670 train_time:81051ms step_avg:88.29ms
step:919/1670 train_time:81139ms step_avg:88.29ms
step:920/1670 train_time:81228ms step_avg:88.29ms
step:921/1670 train_time:81318ms step_avg:88.29ms
step:922/1670 train_time:81408ms step_avg:88.30ms
step:923/1670 train_time:81497ms step_avg:88.30ms
step:924/1670 train_time:81587ms step_avg:88.30ms
step:925/1670 train_time:81677ms step_avg:88.30ms
step:926/1670 train_time:81766ms step_avg:88.30ms
step:927/1670 train_time:81856ms step_avg:88.30ms
step:928/1670 train_time:81945ms step_avg:88.30ms
step:929/1670 train_time:82034ms step_avg:88.30ms
step:930/1670 train_time:82121ms step_avg:88.30ms
step:931/1670 train_time:82211ms step_avg:88.30ms
step:932/1670 train_time:82300ms step_avg:88.30ms
step:933/1670 train_time:82389ms step_avg:88.31ms
step:934/1670 train_time:82479ms step_avg:88.31ms
step:935/1670 train_time:82568ms step_avg:88.31ms
step:936/1670 train_time:82657ms step_avg:88.31ms
step:937/1670 train_time:82747ms step_avg:88.31ms
step:938/1670 train_time:82836ms step_avg:88.31ms
step:939/1670 train_time:82926ms step_avg:88.31ms
step:940/1670 train_time:83015ms step_avg:88.31ms
step:941/1670 train_time:83103ms step_avg:88.31ms
step:942/1670 train_time:83192ms step_avg:88.31ms
step:943/1670 train_time:83280ms step_avg:88.31ms
step:944/1670 train_time:83370ms step_avg:88.32ms
step:945/1670 train_time:83459ms step_avg:88.32ms
step:946/1670 train_time:83547ms step_avg:88.32ms
step:947/1670 train_time:83637ms step_avg:88.32ms
step:948/1670 train_time:83726ms step_avg:88.32ms
step:949/1670 train_time:83816ms step_avg:88.32ms
step:950/1670 train_time:83905ms step_avg:88.32ms
step:951/1670 train_time:83996ms step_avg:88.32ms
step:952/1670 train_time:84085ms step_avg:88.32ms
step:953/1670 train_time:84175ms step_avg:88.33ms
step:954/1670 train_time:84263ms step_avg:88.33ms
step:955/1670 train_time:84352ms step_avg:88.33ms
step:956/1670 train_time:84441ms step_avg:88.33ms
step:957/1670 train_time:84530ms step_avg:88.33ms
step:958/1670 train_time:84619ms step_avg:88.33ms
step:959/1670 train_time:84708ms step_avg:88.33ms
step:960/1670 train_time:84797ms step_avg:88.33ms
step:961/1670 train_time:84886ms step_avg:88.33ms
step:962/1670 train_time:84975ms step_avg:88.33ms
step:963/1670 train_time:85063ms step_avg:88.33ms
step:964/1670 train_time:85153ms step_avg:88.33ms
step:965/1670 train_time:85241ms step_avg:88.33ms
step:966/1670 train_time:85330ms step_avg:88.33ms
step:967/1670 train_time:85418ms step_avg:88.33ms
step:968/1670 train_time:85507ms step_avg:88.33ms
step:969/1670 train_time:85597ms step_avg:88.34ms
step:970/1670 train_time:85686ms step_avg:88.34ms
step:971/1670 train_time:85775ms step_avg:88.34ms
step:972/1670 train_time:85864ms step_avg:88.34ms
step:973/1670 train_time:85953ms step_avg:88.34ms
step:974/1670 train_time:86041ms step_avg:88.34ms
step:975/1670 train_time:86131ms step_avg:88.34ms
step:976/1670 train_time:86220ms step_avg:88.34ms
step:977/1670 train_time:86309ms step_avg:88.34ms
step:978/1670 train_time:86397ms step_avg:88.34ms
step:979/1670 train_time:86486ms step_avg:88.34ms
step:980/1670 train_time:86575ms step_avg:88.34ms
step:981/1670 train_time:86664ms step_avg:88.34ms
step:982/1670 train_time:86754ms step_avg:88.34ms
step:983/1670 train_time:86842ms step_avg:88.34ms
step:984/1670 train_time:86932ms step_avg:88.35ms
step:985/1670 train_time:87020ms step_avg:88.35ms
step:986/1670 train_time:87109ms step_avg:88.35ms
step:987/1670 train_time:87198ms step_avg:88.35ms
step:988/1670 train_time:87287ms step_avg:88.35ms
step:989/1670 train_time:87376ms step_avg:88.35ms
step:990/1670 train_time:87466ms step_avg:88.35ms
step:991/1670 train_time:87556ms step_avg:88.35ms
step:992/1670 train_time:87644ms step_avg:88.35ms
step:993/1670 train_time:87733ms step_avg:88.35ms
step:994/1670 train_time:87821ms step_avg:88.35ms
step:995/1670 train_time:87910ms step_avg:88.35ms
step:996/1670 train_time:87999ms step_avg:88.35ms
step:997/1670 train_time:88089ms step_avg:88.35ms
step:998/1670 train_time:88178ms step_avg:88.35ms
step:999/1670 train_time:88267ms step_avg:88.36ms
step:1000/1670 train_time:88355ms step_avg:88.36ms
step:1000/1670 val_loss:3.4681 train_time:88446ms step_avg:88.45ms
step:1001/1670 train_time:88466ms step_avg:88.38ms
step:1002/1670 train_time:88539ms step_avg:88.36ms
step:1003/1670 train_time:88632ms step_avg:88.37ms
step:1004/1670 train_time:88723ms step_avg:88.37ms
step:1005/1670 train_time:88810ms step_avg:88.37ms
step:1006/1670 train_time:88899ms step_avg:88.37ms
step:1007/1670 train_time:88987ms step_avg:88.37ms
step:1008/1670 train_time:89074ms step_avg:88.37ms
step:1009/1670 train_time:89161ms step_avg:88.37ms
step:1010/1670 train_time:89249ms step_avg:88.37ms
step:1011/1670 train_time:89337ms step_avg:88.37ms
step:1012/1670 train_time:89428ms step_avg:88.37ms
step:1013/1670 train_time:89519ms step_avg:88.37ms
step:1014/1670 train_time:89609ms step_avg:88.37ms
step:1015/1670 train_time:89699ms step_avg:88.37ms
step:1016/1670 train_time:89788ms step_avg:88.37ms
step:1017/1670 train_time:89876ms step_avg:88.37ms
step:1018/1670 train_time:89965ms step_avg:88.37ms
step:1019/1670 train_time:90054ms step_avg:88.37ms
step:1020/1670 train_time:90142ms step_avg:88.37ms
step:1021/1670 train_time:90230ms step_avg:88.37ms
step:1022/1670 train_time:90317ms step_avg:88.37ms
step:1023/1670 train_time:90406ms step_avg:88.37ms
step:1024/1670 train_time:90496ms step_avg:88.38ms
step:1025/1670 train_time:90587ms step_avg:88.38ms
step:1026/1670 train_time:90676ms step_avg:88.38ms
step:1027/1670 train_time:90767ms step_avg:88.38ms
step:1028/1670 train_time:90856ms step_avg:88.38ms
step:1029/1670 train_time:90945ms step_avg:88.38ms
step:1030/1670 train_time:91033ms step_avg:88.38ms
step:1031/1670 train_time:91122ms step_avg:88.38ms
step:1032/1670 train_time:91210ms step_avg:88.38ms
step:1033/1670 train_time:91299ms step_avg:88.38ms
step:1034/1670 train_time:91388ms step_avg:88.38ms
step:1035/1670 train_time:91476ms step_avg:88.38ms
step:1036/1670 train_time:91567ms step_avg:88.39ms
step:1037/1670 train_time:91657ms step_avg:88.39ms
step:1038/1670 train_time:91748ms step_avg:88.39ms
step:1039/1670 train_time:91837ms step_avg:88.39ms
step:1040/1670 train_time:91926ms step_avg:88.39ms
step:1041/1670 train_time:92014ms step_avg:88.39ms
step:1042/1670 train_time:92103ms step_avg:88.39ms
step:1043/1670 train_time:92191ms step_avg:88.39ms
step:1044/1670 train_time:92280ms step_avg:88.39ms
step:1045/1670 train_time:92369ms step_avg:88.39ms
step:1046/1670 train_time:92457ms step_avg:88.39ms
step:1047/1670 train_time:92546ms step_avg:88.39ms
step:1048/1670 train_time:92635ms step_avg:88.39ms
step:1049/1670 train_time:92725ms step_avg:88.39ms
step:1050/1670 train_time:92814ms step_avg:88.39ms
step:1051/1670 train_time:92904ms step_avg:88.40ms
step:1052/1670 train_time:92993ms step_avg:88.40ms
step:1053/1670 train_time:93081ms step_avg:88.40ms
step:1054/1670 train_time:93170ms step_avg:88.40ms
step:1055/1670 train_time:93258ms step_avg:88.40ms
step:1056/1670 train_time:93346ms step_avg:88.40ms
step:1057/1670 train_time:93435ms step_avg:88.40ms
step:1058/1670 train_time:93524ms step_avg:88.40ms
step:1059/1670 train_time:93614ms step_avg:88.40ms
step:1060/1670 train_time:93704ms step_avg:88.40ms
step:1061/1670 train_time:93793ms step_avg:88.40ms
step:1062/1670 train_time:93883ms step_avg:88.40ms
step:1063/1670 train_time:93972ms step_avg:88.40ms
step:1064/1670 train_time:94061ms step_avg:88.40ms
step:1065/1670 train_time:94150ms step_avg:88.40ms
step:1066/1670 train_time:94238ms step_avg:88.40ms
step:1067/1670 train_time:94327ms step_avg:88.40ms
step:1068/1670 train_time:94415ms step_avg:88.40ms
step:1069/1670 train_time:94505ms step_avg:88.40ms
step:1070/1670 train_time:94594ms step_avg:88.41ms
step:1071/1670 train_time:94683ms step_avg:88.41ms
step:1072/1670 train_time:94773ms step_avg:88.41ms
step:1073/1670 train_time:94863ms step_avg:88.41ms
step:1074/1670 train_time:94952ms step_avg:88.41ms
step:1075/1670 train_time:95040ms step_avg:88.41ms
step:1076/1670 train_time:95129ms step_avg:88.41ms
step:1077/1670 train_time:95218ms step_avg:88.41ms
step:1078/1670 train_time:95306ms step_avg:88.41ms
step:1079/1670 train_time:95395ms step_avg:88.41ms
step:1080/1670 train_time:95483ms step_avg:88.41ms
step:1081/1670 train_time:95574ms step_avg:88.41ms
step:1082/1670 train_time:95663ms step_avg:88.41ms
step:1083/1670 train_time:95752ms step_avg:88.41ms
step:1084/1670 train_time:95842ms step_avg:88.41ms
step:1085/1670 train_time:95931ms step_avg:88.42ms
step:1086/1670 train_time:96019ms step_avg:88.42ms
step:1087/1670 train_time:96109ms step_avg:88.42ms
step:1088/1670 train_time:96197ms step_avg:88.42ms
step:1089/1670 train_time:96285ms step_avg:88.42ms
step:1090/1670 train_time:96375ms step_avg:88.42ms
step:1091/1670 train_time:96465ms step_avg:88.42ms
step:1092/1670 train_time:96554ms step_avg:88.42ms
step:1093/1670 train_time:96644ms step_avg:88.42ms
step:1094/1670 train_time:96734ms step_avg:88.42ms
step:1095/1670 train_time:96824ms step_avg:88.42ms
step:1096/1670 train_time:96914ms step_avg:88.42ms
step:1097/1670 train_time:97003ms step_avg:88.43ms
step:1098/1670 train_time:97093ms step_avg:88.43ms
step:1099/1670 train_time:97182ms step_avg:88.43ms
step:1100/1670 train_time:97273ms step_avg:88.43ms
step:1101/1670 train_time:97363ms step_avg:88.43ms
step:1102/1670 train_time:97452ms step_avg:88.43ms
step:1103/1670 train_time:97543ms step_avg:88.43ms
step:1104/1670 train_time:97632ms step_avg:88.43ms
step:1105/1670 train_time:97722ms step_avg:88.44ms
step:1106/1670 train_time:97813ms step_avg:88.44ms
step:1107/1670 train_time:97903ms step_avg:88.44ms
step:1108/1670 train_time:97992ms step_avg:88.44ms
step:1109/1670 train_time:98082ms step_avg:88.44ms
step:1110/1670 train_time:98172ms step_avg:88.44ms
step:1111/1670 train_time:98262ms step_avg:88.44ms
step:1112/1670 train_time:98352ms step_avg:88.45ms
step:1113/1670 train_time:98442ms step_avg:88.45ms
step:1114/1670 train_time:98531ms step_avg:88.45ms
step:1115/1670 train_time:98621ms step_avg:88.45ms
step:1116/1670 train_time:98711ms step_avg:88.45ms
step:1117/1670 train_time:98801ms step_avg:88.45ms
step:1118/1670 train_time:98890ms step_avg:88.45ms
step:1119/1670 train_time:98980ms step_avg:88.45ms
step:1120/1670 train_time:99070ms step_avg:88.46ms
step:1121/1670 train_time:99159ms step_avg:88.46ms
step:1122/1670 train_time:99249ms step_avg:88.46ms
step:1123/1670 train_time:99338ms step_avg:88.46ms
step:1124/1670 train_time:99427ms step_avg:88.46ms
step:1125/1670 train_time:99516ms step_avg:88.46ms
step:1125/1670 val_loss:3.4149 train_time:99608ms step_avg:88.54ms
step:1126/1670 train_time:99627ms step_avg:88.48ms
step:1127/1670 train_time:99698ms step_avg:88.46ms
step:1128/1670 train_time:99789ms step_avg:88.47ms
step:1129/1670 train_time:99881ms step_avg:88.47ms
step:1130/1670 train_time:99970ms step_avg:88.47ms
step:1131/1670 train_time:100058ms step_avg:88.47ms
step:1132/1670 train_time:100147ms step_avg:88.47ms
step:1133/1670 train_time:100236ms step_avg:88.47ms
step:1134/1670 train_time:100324ms step_avg:88.47ms
step:1135/1670 train_time:100416ms step_avg:88.47ms
step:1136/1670 train_time:100506ms step_avg:88.47ms
step:1137/1670 train_time:100600ms step_avg:88.48ms
step:1138/1670 train_time:100692ms step_avg:88.48ms
step:1139/1670 train_time:100782ms step_avg:88.48ms
step:1140/1670 train_time:100873ms step_avg:88.48ms
step:1141/1670 train_time:100962ms step_avg:88.49ms
step:1142/1670 train_time:101050ms step_avg:88.49ms
step:1143/1670 train_time:101139ms step_avg:88.49ms
step:1144/1670 train_time:101227ms step_avg:88.49ms
step:1145/1670 train_time:101317ms step_avg:88.49ms
step:1146/1670 train_time:101407ms step_avg:88.49ms
step:1147/1670 train_time:101497ms step_avg:88.49ms
step:1148/1670 train_time:101587ms step_avg:88.49ms
step:1149/1670 train_time:101679ms step_avg:88.49ms
step:1150/1670 train_time:101770ms step_avg:88.50ms
step:1151/1670 train_time:101859ms step_avg:88.50ms
step:1152/1670 train_time:101949ms step_avg:88.50ms
step:1153/1670 train_time:102038ms step_avg:88.50ms
step:1154/1670 train_time:102127ms step_avg:88.50ms
step:1155/1670 train_time:102216ms step_avg:88.50ms
step:1156/1670 train_time:102305ms step_avg:88.50ms
step:1157/1670 train_time:102394ms step_avg:88.50ms
step:1158/1670 train_time:102484ms step_avg:88.50ms
step:1159/1670 train_time:102575ms step_avg:88.50ms
step:1160/1670 train_time:102665ms step_avg:88.50ms
step:1161/1670 train_time:102755ms step_avg:88.51ms
step:1162/1670 train_time:102844ms step_avg:88.51ms
step:1163/1670 train_time:102934ms step_avg:88.51ms
step:1164/1670 train_time:103023ms step_avg:88.51ms
step:1165/1670 train_time:103112ms step_avg:88.51ms
step:1166/1670 train_time:103201ms step_avg:88.51ms
step:1167/1670 train_time:103290ms step_avg:88.51ms
step:1168/1670 train_time:103379ms step_avg:88.51ms
step:1169/1670 train_time:103468ms step_avg:88.51ms
step:1170/1670 train_time:103558ms step_avg:88.51ms
step:1171/1670 train_time:103648ms step_avg:88.51ms
step:1172/1670 train_time:103738ms step_avg:88.51ms
step:1173/1670 train_time:103828ms step_avg:88.52ms
step:1174/1670 train_time:103917ms step_avg:88.52ms
step:1175/1670 train_time:104007ms step_avg:88.52ms
step:1176/1670 train_time:104097ms step_avg:88.52ms
step:1177/1670 train_time:104187ms step_avg:88.52ms
step:1178/1670 train_time:104276ms step_avg:88.52ms
step:1179/1670 train_time:104366ms step_avg:88.52ms
step:1180/1670 train_time:104455ms step_avg:88.52ms
step:1181/1670 train_time:104545ms step_avg:88.52ms
step:1182/1670 train_time:104634ms step_avg:88.52ms
step:1183/1670 train_time:104724ms step_avg:88.52ms
step:1184/1670 train_time:104815ms step_avg:88.53ms
step:1185/1670 train_time:104905ms step_avg:88.53ms
step:1186/1670 train_time:104995ms step_avg:88.53ms
step:1187/1670 train_time:105085ms step_avg:88.53ms
step:1188/1670 train_time:105175ms step_avg:88.53ms
step:1189/1670 train_time:105264ms step_avg:88.53ms
step:1190/1670 train_time:105354ms step_avg:88.53ms
step:1191/1670 train_time:105443ms step_avg:88.53ms
step:1192/1670 train_time:105533ms step_avg:88.53ms
step:1193/1670 train_time:105622ms step_avg:88.54ms
step:1194/1670 train_time:105714ms step_avg:88.54ms
step:1195/1670 train_time:105803ms step_avg:88.54ms
step:1196/1670 train_time:105894ms step_avg:88.54ms
step:1197/1670 train_time:105983ms step_avg:88.54ms
step:1198/1670 train_time:106073ms step_avg:88.54ms
step:1199/1670 train_time:106162ms step_avg:88.54ms
step:1200/1670 train_time:106252ms step_avg:88.54ms
step:1201/1670 train_time:106341ms step_avg:88.54ms
step:1202/1670 train_time:106431ms step_avg:88.54ms
step:1203/1670 train_time:106520ms step_avg:88.55ms
step:1204/1670 train_time:106610ms step_avg:88.55ms
step:1205/1670 train_time:106699ms step_avg:88.55ms
step:1206/1670 train_time:106790ms step_avg:88.55ms
step:1207/1670 train_time:106879ms step_avg:88.55ms
step:1208/1670 train_time:106970ms step_avg:88.55ms
step:1209/1670 train_time:107059ms step_avg:88.55ms
step:1210/1670 train_time:107149ms step_avg:88.55ms
step:1211/1670 train_time:107238ms step_avg:88.55ms
step:1212/1670 train_time:107328ms step_avg:88.55ms
step:1213/1670 train_time:107418ms step_avg:88.56ms
step:1214/1670 train_time:107507ms step_avg:88.56ms
step:1215/1670 train_time:107597ms step_avg:88.56ms
step:1216/1670 train_time:107688ms step_avg:88.56ms
step:1217/1670 train_time:107777ms step_avg:88.56ms
step:1218/1670 train_time:107867ms step_avg:88.56ms
step:1219/1670 train_time:107957ms step_avg:88.56ms
step:1220/1670 train_time:108047ms step_avg:88.56ms
step:1221/1670 train_time:108137ms step_avg:88.56ms
step:1222/1670 train_time:108226ms step_avg:88.56ms
step:1223/1670 train_time:108316ms step_avg:88.57ms
step:1224/1670 train_time:108406ms step_avg:88.57ms
step:1225/1670 train_time:108496ms step_avg:88.57ms
step:1226/1670 train_time:108586ms step_avg:88.57ms
step:1227/1670 train_time:108676ms step_avg:88.57ms
step:1228/1670 train_time:108766ms step_avg:88.57ms
step:1229/1670 train_time:108856ms step_avg:88.57ms
step:1230/1670 train_time:108945ms step_avg:88.57ms
step:1231/1670 train_time:109035ms step_avg:88.57ms
step:1232/1670 train_time:109124ms step_avg:88.57ms
step:1233/1670 train_time:109214ms step_avg:88.58ms
step:1234/1670 train_time:109304ms step_avg:88.58ms
step:1235/1670 train_time:109393ms step_avg:88.58ms
step:1236/1670 train_time:109483ms step_avg:88.58ms
step:1237/1670 train_time:109573ms step_avg:88.58ms
step:1238/1670 train_time:109662ms step_avg:88.58ms
step:1239/1670 train_time:109752ms step_avg:88.58ms
step:1240/1670 train_time:109841ms step_avg:88.58ms
step:1241/1670 train_time:109931ms step_avg:88.58ms
step:1242/1670 train_time:110020ms step_avg:88.58ms
step:1243/1670 train_time:110110ms step_avg:88.58ms
step:1244/1670 train_time:110200ms step_avg:88.58ms
step:1245/1670 train_time:110289ms step_avg:88.59ms
step:1246/1670 train_time:110379ms step_avg:88.59ms
step:1247/1670 train_time:110468ms step_avg:88.59ms
step:1248/1670 train_time:110557ms step_avg:88.59ms
step:1249/1670 train_time:110647ms step_avg:88.59ms
step:1250/1670 train_time:110737ms step_avg:88.59ms
step:1250/1670 val_loss:3.3763 train_time:110829ms step_avg:88.66ms
step:1251/1670 train_time:110849ms step_avg:88.61ms
step:1252/1670 train_time:110923ms step_avg:88.60ms
step:1253/1670 train_time:111014ms step_avg:88.60ms
step:1254/1670 train_time:111103ms step_avg:88.60ms
step:1255/1670 train_time:111191ms step_avg:88.60ms
step:1256/1670 train_time:111281ms step_avg:88.60ms
step:1257/1670 train_time:111369ms step_avg:88.60ms
step:1258/1670 train_time:111458ms step_avg:88.60ms
step:1259/1670 train_time:111547ms step_avg:88.60ms
step:1260/1670 train_time:111635ms step_avg:88.60ms
step:1261/1670 train_time:111724ms step_avg:88.60ms
step:1262/1670 train_time:111816ms step_avg:88.60ms
step:1263/1670 train_time:111910ms step_avg:88.61ms
step:1264/1670 train_time:112003ms step_avg:88.61ms
step:1265/1670 train_time:112092ms step_avg:88.61ms
step:1266/1670 train_time:112182ms step_avg:88.61ms
step:1267/1670 train_time:112271ms step_avg:88.61ms
step:1268/1670 train_time:112360ms step_avg:88.61ms
step:1269/1670 train_time:112449ms step_avg:88.61ms
step:1270/1670 train_time:112538ms step_avg:88.61ms
step:1271/1670 train_time:112627ms step_avg:88.61ms
step:1272/1670 train_time:112717ms step_avg:88.61ms
step:1273/1670 train_time:112808ms step_avg:88.62ms
step:1274/1670 train_time:112900ms step_avg:88.62ms
step:1275/1670 train_time:112990ms step_avg:88.62ms
step:1276/1670 train_time:113080ms step_avg:88.62ms
step:1277/1670 train_time:113170ms step_avg:88.62ms
step:1278/1670 train_time:113259ms step_avg:88.62ms
step:1279/1670 train_time:113348ms step_avg:88.62ms
step:1280/1670 train_time:113437ms step_avg:88.62ms
step:1281/1670 train_time:113527ms step_avg:88.62ms
step:1282/1670 train_time:113617ms step_avg:88.63ms
step:1283/1670 train_time:113707ms step_avg:88.63ms
step:1284/1670 train_time:113797ms step_avg:88.63ms
step:1285/1670 train_time:113888ms step_avg:88.63ms
step:1286/1670 train_time:113978ms step_avg:88.63ms
step:1287/1670 train_time:114068ms step_avg:88.63ms
step:1288/1670 train_time:114158ms step_avg:88.63ms
step:1289/1670 train_time:114248ms step_avg:88.63ms
step:1290/1670 train_time:114337ms step_avg:88.63ms
step:1291/1670 train_time:114427ms step_avg:88.63ms
step:1292/1670 train_time:114516ms step_avg:88.63ms
step:1293/1670 train_time:114607ms step_avg:88.64ms
step:1294/1670 train_time:114695ms step_avg:88.64ms
step:1295/1670 train_time:114785ms step_avg:88.64ms
step:1296/1670 train_time:114875ms step_avg:88.64ms
step:1297/1670 train_time:114965ms step_avg:88.64ms
step:1298/1670 train_time:115054ms step_avg:88.64ms
step:1299/1670 train_time:115144ms step_avg:88.64ms
step:1300/1670 train_time:115234ms step_avg:88.64ms
step:1301/1670 train_time:115324ms step_avg:88.64ms
step:1302/1670 train_time:115413ms step_avg:88.64ms
step:1303/1670 train_time:115503ms step_avg:88.64ms
step:1304/1670 train_time:115593ms step_avg:88.64ms
step:1305/1670 train_time:115681ms step_avg:88.64ms
step:1306/1670 train_time:115771ms step_avg:88.65ms
step:1307/1670 train_time:115861ms step_avg:88.65ms
step:1308/1670 train_time:115951ms step_avg:88.65ms
step:1309/1670 train_time:116043ms step_avg:88.65ms
step:1310/1670 train_time:116133ms step_avg:88.65ms
step:1311/1670 train_time:116223ms step_avg:88.65ms
step:1312/1670 train_time:116313ms step_avg:88.65ms
step:1313/1670 train_time:116402ms step_avg:88.65ms
step:1314/1670 train_time:116492ms step_avg:88.65ms
step:1315/1670 train_time:116582ms step_avg:88.66ms
step:1316/1670 train_time:116671ms step_avg:88.66ms
step:1317/1670 train_time:116760ms step_avg:88.66ms
step:1318/1670 train_time:116850ms step_avg:88.66ms
step:1319/1670 train_time:116940ms step_avg:88.66ms
step:1320/1670 train_time:117032ms step_avg:88.66ms
step:1321/1670 train_time:117123ms step_avg:88.66ms
step:1322/1670 train_time:117213ms step_avg:88.66ms
step:1323/1670 train_time:117304ms step_avg:88.66ms
step:1324/1670 train_time:117392ms step_avg:88.67ms
step:1325/1670 train_time:117482ms step_avg:88.67ms
step:1326/1670 train_time:117572ms step_avg:88.67ms
step:1327/1670 train_time:117661ms step_avg:88.67ms
step:1328/1670 train_time:117751ms step_avg:88.67ms
step:1329/1670 train_time:117840ms step_avg:88.67ms
step:1330/1670 train_time:117930ms step_avg:88.67ms
step:1331/1670 train_time:118021ms step_avg:88.67ms
step:1332/1670 train_time:118112ms step_avg:88.67ms
step:1333/1670 train_time:118203ms step_avg:88.67ms
step:1334/1670 train_time:118292ms step_avg:88.67ms
step:1335/1670 train_time:118383ms step_avg:88.68ms
step:1336/1670 train_time:118472ms step_avg:88.68ms
step:1337/1670 train_time:118561ms step_avg:88.68ms
step:1338/1670 train_time:118651ms step_avg:88.68ms
step:1339/1670 train_time:118741ms step_avg:88.68ms
step:1340/1670 train_time:118830ms step_avg:88.68ms
step:1341/1670 train_time:118920ms step_avg:88.68ms
step:1342/1670 train_time:119011ms step_avg:88.68ms
step:1343/1670 train_time:119100ms step_avg:88.68ms
step:1344/1670 train_time:119191ms step_avg:88.68ms
step:1345/1670 train_time:119281ms step_avg:88.68ms
step:1346/1670 train_time:119370ms step_avg:88.68ms
step:1347/1670 train_time:119459ms step_avg:88.68ms
step:1348/1670 train_time:119548ms step_avg:88.69ms
step:1349/1670 train_time:119638ms step_avg:88.69ms
step:1350/1670 train_time:119729ms step_avg:88.69ms
step:1351/1670 train_time:119819ms step_avg:88.69ms
step:1352/1670 train_time:119910ms step_avg:88.69ms
step:1353/1670 train_time:120000ms step_avg:88.69ms
step:1354/1670 train_time:120090ms step_avg:88.69ms
step:1355/1670 train_time:120181ms step_avg:88.69ms
step:1356/1670 train_time:120271ms step_avg:88.70ms
step:1357/1670 train_time:120360ms step_avg:88.70ms
step:1358/1670 train_time:120450ms step_avg:88.70ms
step:1359/1670 train_time:120539ms step_avg:88.70ms
step:1360/1670 train_time:120629ms step_avg:88.70ms
step:1361/1670 train_time:120718ms step_avg:88.70ms
step:1362/1670 train_time:120808ms step_avg:88.70ms
step:1363/1670 train_time:120898ms step_avg:88.70ms
step:1364/1670 train_time:120987ms step_avg:88.70ms
step:1365/1670 train_time:121077ms step_avg:88.70ms
step:1366/1670 train_time:121167ms step_avg:88.70ms
step:1367/1670 train_time:121256ms step_avg:88.70ms
step:1368/1670 train_time:121347ms step_avg:88.70ms
step:1369/1670 train_time:121436ms step_avg:88.70ms
step:1370/1670 train_time:121526ms step_avg:88.71ms
step:1371/1670 train_time:121615ms step_avg:88.71ms
step:1372/1670 train_time:121705ms step_avg:88.71ms
step:1373/1670 train_time:121794ms step_avg:88.71ms
step:1374/1670 train_time:121884ms step_avg:88.71ms
step:1375/1670 train_time:121973ms step_avg:88.71ms
step:1375/1670 val_loss:3.3415 train_time:122065ms step_avg:88.77ms
step:1376/1670 train_time:122085ms step_avg:88.72ms
step:1377/1670 train_time:122158ms step_avg:88.71ms
step:1378/1670 train_time:122252ms step_avg:88.72ms
step:1379/1670 train_time:122341ms step_avg:88.72ms
step:1380/1670 train_time:122430ms step_avg:88.72ms
step:1381/1670 train_time:122518ms step_avg:88.72ms
step:1382/1670 train_time:122606ms step_avg:88.72ms
step:1383/1670 train_time:122695ms step_avg:88.72ms
step:1384/1670 train_time:122783ms step_avg:88.72ms
step:1385/1670 train_time:122872ms step_avg:88.72ms
step:1386/1670 train_time:122961ms step_avg:88.72ms
step:1387/1670 train_time:123053ms step_avg:88.72ms
step:1388/1670 train_time:123145ms step_avg:88.72ms
step:1389/1670 train_time:123237ms step_avg:88.72ms
step:1390/1670 train_time:123328ms step_avg:88.73ms
step:1391/1670 train_time:123417ms step_avg:88.73ms
step:1392/1670 train_time:123506ms step_avg:88.73ms
step:1393/1670 train_time:123595ms step_avg:88.73ms
step:1394/1670 train_time:123684ms step_avg:88.73ms
step:1395/1670 train_time:123774ms step_avg:88.73ms
step:1396/1670 train_time:123863ms step_avg:88.73ms
step:1397/1670 train_time:123954ms step_avg:88.73ms
step:1398/1670 train_time:124044ms step_avg:88.73ms
step:1399/1670 train_time:124136ms step_avg:88.73ms
step:1400/1670 train_time:124228ms step_avg:88.73ms
step:1401/1670 train_time:124318ms step_avg:88.74ms
step:1402/1670 train_time:124408ms step_avg:88.74ms
step:1403/1670 train_time:124497ms step_avg:88.74ms
step:1404/1670 train_time:124586ms step_avg:88.74ms
step:1405/1670 train_time:124675ms step_avg:88.74ms
step:1406/1670 train_time:124764ms step_avg:88.74ms
step:1407/1670 train_time:124854ms step_avg:88.74ms
step:1408/1670 train_time:124944ms step_avg:88.74ms
step:1409/1670 train_time:125035ms step_avg:88.74ms
step:1410/1670 train_time:125127ms step_avg:88.74ms
step:1411/1670 train_time:125218ms step_avg:88.74ms
step:1412/1670 train_time:125308ms step_avg:88.75ms
step:1413/1670 train_time:125399ms step_avg:88.75ms
step:1414/1670 train_time:125488ms step_avg:88.75ms
step:1415/1670 train_time:125576ms step_avg:88.75ms
step:1416/1670 train_time:125666ms step_avg:88.75ms
step:1417/1670 train_time:125757ms step_avg:88.75ms
step:1418/1670 train_time:125846ms step_avg:88.75ms
step:1419/1670 train_time:125936ms step_avg:88.75ms
step:1420/1670 train_time:126026ms step_avg:88.75ms
step:1421/1670 train_time:126117ms step_avg:88.75ms
step:1422/1670 train_time:126207ms step_avg:88.75ms
step:1423/1670 train_time:126297ms step_avg:88.75ms
step:1424/1670 train_time:126387ms step_avg:88.75ms
step:1425/1670 train_time:126477ms step_avg:88.76ms
step:1426/1670 train_time:126567ms step_avg:88.76ms
step:1427/1670 train_time:126656ms step_avg:88.76ms
step:1428/1670 train_time:126746ms step_avg:88.76ms
step:1429/1670 train_time:126836ms step_avg:88.76ms
step:1430/1670 train_time:126926ms step_avg:88.76ms
step:1431/1670 train_time:127016ms step_avg:88.76ms
step:1432/1670 train_time:127107ms step_avg:88.76ms
step:1433/1670 train_time:127197ms step_avg:88.76ms
step:1434/1670 train_time:127286ms step_avg:88.76ms
step:1435/1670 train_time:127376ms step_avg:88.76ms
step:1436/1670 train_time:127466ms step_avg:88.76ms
step:1437/1670 train_time:127556ms step_avg:88.77ms
step:1438/1670 train_time:127646ms step_avg:88.77ms
step:1439/1670 train_time:127736ms step_avg:88.77ms
step:1440/1670 train_time:127825ms step_avg:88.77ms
step:1441/1670 train_time:127915ms step_avg:88.77ms
step:1442/1670 train_time:128005ms step_avg:88.77ms
step:1443/1670 train_time:128095ms step_avg:88.77ms
step:1444/1670 train_time:128184ms step_avg:88.77ms
step:1445/1670 train_time:128275ms step_avg:88.77ms
step:1446/1670 train_time:128365ms step_avg:88.77ms
step:1447/1670 train_time:128455ms step_avg:88.77ms
step:1448/1670 train_time:128545ms step_avg:88.77ms
step:1449/1670 train_time:128636ms step_avg:88.78ms
step:1450/1670 train_time:128725ms step_avg:88.78ms
step:1451/1670 train_time:128814ms step_avg:88.78ms
step:1452/1670 train_time:128903ms step_avg:88.78ms
step:1453/1670 train_time:128993ms step_avg:88.78ms
step:1454/1670 train_time:129082ms step_avg:88.78ms
step:1455/1670 train_time:129173ms step_avg:88.78ms
step:1456/1670 train_time:129263ms step_avg:88.78ms
step:1457/1670 train_time:129354ms step_avg:88.78ms
step:1458/1670 train_time:129443ms step_avg:88.78ms
step:1459/1670 train_time:129534ms step_avg:88.78ms
step:1460/1670 train_time:129625ms step_avg:88.78ms
step:1461/1670 train_time:129714ms step_avg:88.78ms
step:1462/1670 train_time:129803ms step_avg:88.78ms
step:1463/1670 train_time:129892ms step_avg:88.79ms
step:1464/1670 train_time:129982ms step_avg:88.79ms
step:1465/1670 train_time:130072ms step_avg:88.79ms
step:1466/1670 train_time:130161ms step_avg:88.79ms
step:1467/1670 train_time:130250ms step_avg:88.79ms
step:1468/1670 train_time:130340ms step_avg:88.79ms
step:1469/1670 train_time:130431ms step_avg:88.79ms
step:1470/1670 train_time:130521ms step_avg:88.79ms
step:1471/1670 train_time:130611ms step_avg:88.79ms
step:1472/1670 train_time:130700ms step_avg:88.79ms
step:1473/1670 train_time:130789ms step_avg:88.79ms
step:1474/1670 train_time:130879ms step_avg:88.79ms
step:1475/1670 train_time:130969ms step_avg:88.79ms
step:1476/1670 train_time:131058ms step_avg:88.79ms
step:1477/1670 train_time:131148ms step_avg:88.79ms
step:1478/1670 train_time:131238ms step_avg:88.79ms
step:1479/1670 train_time:131328ms step_avg:88.80ms
step:1480/1670 train_time:131418ms step_avg:88.80ms
step:1481/1670 train_time:131508ms step_avg:88.80ms
step:1482/1670 train_time:131598ms step_avg:88.80ms
step:1483/1670 train_time:131688ms step_avg:88.80ms
step:1484/1670 train_time:131778ms step_avg:88.80ms
step:1485/1670 train_time:131868ms step_avg:88.80ms
step:1486/1670 train_time:131958ms step_avg:88.80ms
step:1487/1670 train_time:132047ms step_avg:88.80ms
step:1488/1670 train_time:132138ms step_avg:88.80ms
step:1489/1670 train_time:132228ms step_avg:88.80ms
step:1490/1670 train_time:132318ms step_avg:88.80ms
step:1491/1670 train_time:132408ms step_avg:88.80ms
step:1492/1670 train_time:132497ms step_avg:88.81ms
step:1493/1670 train_time:132587ms step_avg:88.81ms
step:1494/1670 train_time:132677ms step_avg:88.81ms
step:1495/1670 train_time:132767ms step_avg:88.81ms
step:1496/1670 train_time:132857ms step_avg:88.81ms
step:1497/1670 train_time:132947ms step_avg:88.81ms
step:1498/1670 train_time:133037ms step_avg:88.81ms
step:1499/1670 train_time:133127ms step_avg:88.81ms
step:1500/1670 train_time:133216ms step_avg:88.81ms
step:1500/1670 val_loss:3.3119 train_time:133308ms step_avg:88.87ms
step:1501/1670 train_time:133327ms step_avg:88.83ms
step:1502/1670 train_time:133402ms step_avg:88.82ms
step:1503/1670 train_time:133499ms step_avg:88.82ms
step:1504/1670 train_time:133589ms step_avg:88.82ms
step:1505/1670 train_time:133679ms step_avg:88.82ms
step:1506/1670 train_time:133768ms step_avg:88.82ms
step:1507/1670 train_time:133855ms step_avg:88.82ms
step:1508/1670 train_time:133945ms step_avg:88.82ms
step:1509/1670 train_time:134033ms step_avg:88.82ms
step:1510/1670 train_time:134123ms step_avg:88.82ms
step:1511/1670 train_time:134212ms step_avg:88.82ms
step:1512/1670 train_time:134303ms step_avg:88.82ms
step:1513/1670 train_time:134394ms step_avg:88.83ms
step:1514/1670 train_time:134486ms step_avg:88.83ms
step:1515/1670 train_time:134578ms step_avg:88.83ms
step:1516/1670 train_time:134668ms step_avg:88.83ms
step:1517/1670 train_time:134758ms step_avg:88.83ms
step:1518/1670 train_time:134847ms step_avg:88.83ms
step:1519/1670 train_time:134936ms step_avg:88.83ms
step:1520/1670 train_time:135025ms step_avg:88.83ms
step:1521/1670 train_time:135113ms step_avg:88.83ms
step:1522/1670 train_time:135204ms step_avg:88.83ms
step:1523/1670 train_time:135294ms step_avg:88.83ms
step:1524/1670 train_time:135385ms step_avg:88.84ms
step:1525/1670 train_time:135474ms step_avg:88.84ms
step:1526/1670 train_time:135565ms step_avg:88.84ms
step:1527/1670 train_time:135655ms step_avg:88.84ms
step:1528/1670 train_time:135745ms step_avg:88.84ms
step:1529/1670 train_time:135834ms step_avg:88.84ms
step:1530/1670 train_time:135924ms step_avg:88.84ms
step:1531/1670 train_time:136012ms step_avg:88.84ms
step:1532/1670 train_time:136102ms step_avg:88.84ms
step:1533/1670 train_time:136191ms step_avg:88.84ms
step:1534/1670 train_time:136282ms step_avg:88.84ms
step:1535/1670 train_time:136372ms step_avg:88.84ms
step:1536/1670 train_time:136463ms step_avg:88.84ms
step:1537/1670 train_time:136553ms step_avg:88.84ms
step:1538/1670 train_time:136643ms step_avg:88.84ms
step:1539/1670 train_time:136732ms step_avg:88.84ms
step:1540/1670 train_time:136822ms step_avg:88.85ms
step:1541/1670 train_time:136911ms step_avg:88.85ms
step:1542/1670 train_time:137002ms step_avg:88.85ms
step:1543/1670 train_time:137090ms step_avg:88.85ms
step:1544/1670 train_time:137180ms step_avg:88.85ms
step:1545/1670 train_time:137271ms step_avg:88.85ms
step:1546/1670 train_time:137362ms step_avg:88.85ms
step:1547/1670 train_time:137451ms step_avg:88.85ms
step:1548/1670 train_time:137541ms step_avg:88.85ms
step:1549/1670 train_time:137630ms step_avg:88.85ms
step:1550/1670 train_time:137720ms step_avg:88.85ms
step:1551/1670 train_time:137810ms step_avg:88.85ms
step:1552/1670 train_time:137900ms step_avg:88.85ms
step:1553/1670 train_time:137990ms step_avg:88.85ms
step:1554/1670 train_time:138080ms step_avg:88.85ms
step:1555/1670 train_time:138170ms step_avg:88.86ms
step:1556/1670 train_time:138261ms step_avg:88.86ms
step:1557/1670 train_time:138351ms step_avg:88.86ms
step:1558/1670 train_time:138441ms step_avg:88.86ms
step:1559/1670 train_time:138530ms step_avg:88.86ms
step:1560/1670 train_time:138620ms step_avg:88.86ms
step:1561/1670 train_time:138710ms step_avg:88.86ms
step:1562/1670 train_time:138800ms step_avg:88.86ms
step:1563/1670 train_time:138889ms step_avg:88.86ms
step:1564/1670 train_time:138979ms step_avg:88.86ms
step:1565/1670 train_time:139069ms step_avg:88.86ms
step:1566/1670 train_time:139160ms step_avg:88.86ms
step:1567/1670 train_time:139251ms step_avg:88.86ms
step:1568/1670 train_time:139342ms step_avg:88.87ms
step:1569/1670 train_time:139431ms step_avg:88.87ms
step:1570/1670 train_time:139521ms step_avg:88.87ms
step:1571/1670 train_time:139611ms step_avg:88.87ms
step:1572/1670 train_time:139701ms step_avg:88.87ms
step:1573/1670 train_time:139791ms step_avg:88.87ms
step:1574/1670 train_time:139881ms step_avg:88.87ms
step:1575/1670 train_time:139971ms step_avg:88.87ms
step:1576/1670 train_time:140061ms step_avg:88.87ms
step:1577/1670 train_time:140150ms step_avg:88.87ms
step:1578/1670 train_time:140240ms step_avg:88.87ms
step:1579/1670 train_time:140330ms step_avg:88.87ms
step:1580/1670 train_time:140419ms step_avg:88.87ms
step:1581/1670 train_time:140510ms step_avg:88.87ms
step:1582/1670 train_time:140600ms step_avg:88.87ms
step:1583/1670 train_time:140689ms step_avg:88.88ms
step:1584/1670 train_time:140781ms step_avg:88.88ms
step:1585/1670 train_time:140871ms step_avg:88.88ms
step:1586/1670 train_time:140961ms step_avg:88.88ms
step:1587/1670 train_time:141049ms step_avg:88.88ms
step:1588/1670 train_time:141139ms step_avg:88.88ms
step:1589/1670 train_time:141229ms step_avg:88.88ms
step:1590/1670 train_time:141320ms step_avg:88.88ms
step:1591/1670 train_time:141411ms step_avg:88.88ms
step:1592/1670 train_time:141500ms step_avg:88.88ms
step:1593/1670 train_time:141590ms step_avg:88.88ms
step:1594/1670 train_time:141680ms step_avg:88.88ms
step:1595/1670 train_time:141771ms step_avg:88.88ms
step:1596/1670 train_time:141860ms step_avg:88.88ms
step:1597/1670 train_time:141950ms step_avg:88.89ms
step:1598/1670 train_time:142039ms step_avg:88.89ms
step:1599/1670 train_time:142129ms step_avg:88.89ms
step:1600/1670 train_time:142219ms step_avg:88.89ms
step:1601/1670 train_time:142311ms step_avg:88.89ms
step:1602/1670 train_time:142401ms step_avg:88.89ms
step:1603/1670 train_time:142490ms step_avg:88.89ms
step:1604/1670 train_time:142579ms step_avg:88.89ms
step:1605/1670 train_time:142669ms step_avg:88.89ms
step:1606/1670 train_time:142759ms step_avg:88.89ms
step:1607/1670 train_time:142848ms step_avg:88.89ms
step:1608/1670 train_time:142938ms step_avg:88.89ms
step:1609/1670 train_time:143028ms step_avg:88.89ms
step:1610/1670 train_time:143117ms step_avg:88.89ms
step:1611/1670 train_time:143208ms step_avg:88.89ms
step:1612/1670 train_time:143298ms step_avg:88.89ms
step:1613/1670 train_time:143389ms step_avg:88.90ms
step:1614/1670 train_time:143479ms step_avg:88.90ms
step:1615/1670 train_time:143570ms step_avg:88.90ms
step:1616/1670 train_time:143661ms step_avg:88.90ms
step:1617/1670 train_time:143751ms step_avg:88.90ms
step:1618/1670 train_time:143841ms step_avg:88.90ms
step:1619/1670 train_time:143931ms step_avg:88.90ms
step:1620/1670 train_time:144020ms step_avg:88.90ms
step:1621/1670 train_time:144110ms step_avg:88.90ms
step:1622/1670 train_time:144200ms step_avg:88.90ms
step:1623/1670 train_time:144290ms step_avg:88.90ms
step:1624/1670 train_time:144381ms step_avg:88.90ms
step:1625/1670 train_time:144471ms step_avg:88.91ms
step:1625/1670 val_loss:3.2887 train_time:144562ms step_avg:88.96ms
step:1626/1670 train_time:144583ms step_avg:88.92ms
step:1627/1670 train_time:144655ms step_avg:88.91ms
step:1628/1670 train_time:144748ms step_avg:88.91ms
step:1629/1670 train_time:144839ms step_avg:88.91ms
step:1630/1670 train_time:144929ms step_avg:88.91ms
step:1631/1670 train_time:145018ms step_avg:88.91ms
step:1632/1670 train_time:145106ms step_avg:88.91ms
step:1633/1670 train_time:145195ms step_avg:88.91ms
step:1634/1670 train_time:145284ms step_avg:88.91ms
step:1635/1670 train_time:145373ms step_avg:88.91ms
step:1636/1670 train_time:145463ms step_avg:88.91ms
step:1637/1670 train_time:145554ms step_avg:88.91ms
step:1638/1670 train_time:145647ms step_avg:88.92ms
step:1639/1670 train_time:145738ms step_avg:88.92ms
step:1640/1670 train_time:145829ms step_avg:88.92ms
step:1641/1670 train_time:145918ms step_avg:88.92ms
step:1642/1670 train_time:146007ms step_avg:88.92ms
step:1643/1670 train_time:146097ms step_avg:88.92ms
step:1644/1670 train_time:146186ms step_avg:88.92ms
step:1645/1670 train_time:146275ms step_avg:88.92ms
step:1646/1670 train_time:146364ms step_avg:88.92ms
step:1647/1670 train_time:146453ms step_avg:88.92ms
step:1648/1670 train_time:146543ms step_avg:88.92ms
step:1649/1670 train_time:146634ms step_avg:88.92ms
step:1650/1670 train_time:146726ms step_avg:88.92ms
step:1651/1670 train_time:146817ms step_avg:88.93ms
step:1652/1670 train_time:146908ms step_avg:88.93ms
step:1653/1670 train_time:146998ms step_avg:88.93ms
step:1654/1670 train_time:147088ms step_avg:88.93ms
step:1655/1670 train_time:147176ms step_avg:88.93ms
step:1656/1670 train_time:147265ms step_avg:88.93ms
step:1657/1670 train_time:147354ms step_avg:88.93ms
step:1658/1670 train_time:147443ms step_avg:88.93ms
step:1659/1670 train_time:147533ms step_avg:88.93ms
step:1660/1670 train_time:147623ms step_avg:88.93ms
step:1661/1670 train_time:147715ms step_avg:88.93ms
step:1662/1670 train_time:147806ms step_avg:88.93ms
step:1663/1670 train_time:147898ms step_avg:88.93ms
step:1664/1670 train_time:147988ms step_avg:88.94ms
step:1665/1670 train_time:148078ms step_avg:88.94ms
step:1666/1670 train_time:148167ms step_avg:88.94ms
step:1667/1670 train_time:148256ms step_avg:88.94ms
step:1668/1670 train_time:148345ms step_avg:88.94ms
step:1669/1670 train_time:148435ms step_avg:88.94ms
step:1670/1670 train_time:148525ms step_avg:88.94ms
step:1670/1670 val_loss:3.2789 train_time:148617ms step_avg:88.99ms
peak memory allocated: 30760 MiB reserved: 45574 MiB

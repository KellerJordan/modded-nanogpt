import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)


@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)


@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[
    Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)


@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)


def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None


def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)


mm_op.register_autograd(backward, setup_context=setup_context)


# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
        pid,
        M,
        BLOCK_SIZE_M: tl.constexpr,
        BLOCK_SIZE_N: tl.constexpr,
        GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
coeffs_list = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in coeffs_list:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """

    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size() == 8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1,  # 1 param
            'attn_gate': 2,  # 10 params
            'attn': 3,  # 10 params
            'mlp': 4,  # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx + size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                    (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                    group["lr"]
                    * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                    * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                    group["lr"]
                    * group["weight_decay"]
                    * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[module_idx], 'module', 'none') == 'attn':
                for p in params[module_idx:module_idx + chunk_size]:
                    assert getattr(params[module_idx], 'module', 'none') == 'attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8,
                 weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))


class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5)  # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim // 4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim // 4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int = 1, beta: int = 32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1


def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)


@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim * 4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module = 'attn'
        with torch.no_grad():
            self.qkvo_w.view(4, self.hdim, self.dim)[:3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w.view(4, self.hdim, self.dim)[3].zero_()  # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1)  # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T,
                                                                                                               3 * self.num_heads,
                                                                                                               self.head_dim).chunk(
            3, dim=-2)
        q, k = norm(q), norm(k)  # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v)  # @ KoszarskyB & @Grad62304977
        else:  # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len,
                                   max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)  # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module = 'mlp'
        self.c_proj.module = 'mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_()  # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(
            x).square()  # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x


# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)


class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int,
                 max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim ** 0.5) / 448, w_s=2 ** -9,
                                    grad_s=1 / 448)
        self.lm_head.weight.detach().zero_()  # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers),  # extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm,
                    long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1, len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i < 11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss


# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32)  # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2])  # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True)  # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy())  # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens


BOS_ID = 50256


class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens = tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter == 5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter += 1
        return starts, ends


class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data


def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1,
                               align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens

        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin"  # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin"  # input .bin to eval validation loss on
    val_tokens: int = 10485760  # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1630  # number of iterations to run
    iteration_extension = 40  # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5  # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13  # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20  # extend long windows out even further


args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0)  # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)


def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)


# begin by printing this file (the Python code)
print0(code)
print0("=" * 100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")


def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout


print0(nvidia_smi())
print0("=" * 100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if
                        p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]


# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr


def get_ws(step: int):
    if step == args.num_iterations + args.iteration_extension:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers])  # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len,
                                          grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[
        step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long < ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long // 2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len,
                                          grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1,
                                                grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(
            f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms",
            console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(),
                       optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1)  # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(
        f"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms",
        console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Mon Sep 29 06:18:24 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   36C    P0            126W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   31C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   35C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1670 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1670 train_time:141ms step_avg:141.22ms
step:2/1670 train_time:162ms step_avg:80.79ms
step:3/1670 train_time:225ms step_avg:74.92ms
step:4/1670 train_time:311ms step_avg:77.67ms
step:5/1670 train_time:397ms step_avg:79.47ms
step:6/1670 train_time:484ms step_avg:80.63ms
step:7/1670 train_time:570ms step_avg:81.45ms
step:8/1670 train_time:658ms step_avg:82.20ms
step:9/1670 train_time:744ms step_avg:82.71ms
step:10/1670 train_time:831ms step_avg:83.13ms
step:11/1670 train_time:919ms step_avg:83.51ms
step:12/1670 train_time:1007ms step_avg:83.89ms
step:13/1670 train_time:1099ms step_avg:84.54ms
step:14/1670 train_time:1189ms step_avg:84.94ms
step:15/1670 train_time:1277ms step_avg:85.14ms
step:16/1670 train_time:1364ms step_avg:85.27ms
step:17/1670 train_time:1452ms step_avg:85.42ms
step:18/1670 train_time:1539ms step_avg:85.50ms
step:19/1670 train_time:1626ms step_avg:85.60ms
step:20/1670 train_time:1714ms step_avg:85.70ms
step:21/1670 train_time:1801ms step_avg:85.78ms
step:22/1670 train_time:1888ms step_avg:85.82ms
step:23/1670 train_time:1976ms step_avg:85.93ms
step:24/1670 train_time:2066ms step_avg:86.09ms
step:25/1670 train_time:2155ms step_avg:86.19ms
step:26/1670 train_time:2243ms step_avg:86.27ms
step:27/1670 train_time:2332ms step_avg:86.38ms
step:28/1670 train_time:2421ms step_avg:86.45ms
step:29/1670 train_time:2508ms step_avg:86.47ms
step:30/1670 train_time:2595ms step_avg:86.51ms
step:31/1670 train_time:2682ms step_avg:86.53ms
step:32/1670 train_time:2770ms step_avg:86.55ms
step:33/1670 train_time:2857ms step_avg:86.57ms
step:34/1670 train_time:2944ms step_avg:86.60ms
step:35/1670 train_time:3032ms step_avg:86.64ms
step:36/1670 train_time:3121ms step_avg:86.69ms
step:37/1670 train_time:3208ms step_avg:86.72ms
step:38/1670 train_time:3297ms step_avg:86.77ms
step:39/1670 train_time:3386ms step_avg:86.81ms
step:40/1670 train_time:3473ms step_avg:86.82ms
step:41/1670 train_time:3561ms step_avg:86.86ms
step:42/1670 train_time:3648ms step_avg:86.86ms
step:43/1670 train_time:3736ms step_avg:86.88ms
step:44/1670 train_time:3823ms step_avg:86.89ms
step:45/1670 train_time:3911ms step_avg:86.91ms
step:46/1670 train_time:3999ms step_avg:86.94ms
step:47/1670 train_time:4087ms step_avg:86.95ms
step:48/1670 train_time:4175ms step_avg:86.98ms
step:49/1670 train_time:4264ms step_avg:87.02ms
step:50/1670 train_time:4352ms step_avg:87.04ms
step:51/1670 train_time:4440ms step_avg:87.05ms
step:52/1670 train_time:4528ms step_avg:87.07ms
step:53/1670 train_time:4615ms step_avg:87.07ms
step:54/1670 train_time:4702ms step_avg:87.08ms
step:55/1670 train_time:4789ms step_avg:87.08ms
step:56/1670 train_time:4877ms step_avg:87.09ms
step:57/1670 train_time:4964ms step_avg:87.09ms
step:58/1670 train_time:5052ms step_avg:87.10ms
step:59/1670 train_time:5140ms step_avg:87.12ms
step:60/1670 train_time:5228ms step_avg:87.14ms
step:61/1670 train_time:5317ms step_avg:87.16ms
step:62/1670 train_time:5404ms step_avg:87.17ms
step:63/1670 train_time:5492ms step_avg:87.18ms
step:64/1670 train_time:5580ms step_avg:87.19ms
step:65/1670 train_time:5667ms step_avg:87.19ms
step:66/1670 train_time:5755ms step_avg:87.19ms
step:67/1670 train_time:5842ms step_avg:87.20ms
step:68/1670 train_time:5930ms step_avg:87.20ms
step:69/1670 train_time:6018ms step_avg:87.21ms
step:70/1670 train_time:6105ms step_avg:87.22ms
step:71/1670 train_time:6192ms step_avg:87.22ms
step:72/1670 train_time:6281ms step_avg:87.24ms
step:73/1670 train_time:6369ms step_avg:87.24ms
step:74/1670 train_time:6457ms step_avg:87.26ms
step:75/1670 train_time:6544ms step_avg:87.25ms
step:76/1670 train_time:6632ms step_avg:87.27ms
step:77/1670 train_time:6720ms step_avg:87.27ms
step:78/1670 train_time:6807ms step_avg:87.27ms
step:79/1670 train_time:6895ms step_avg:87.28ms
step:80/1670 train_time:6982ms step_avg:87.28ms
step:81/1670 train_time:7070ms step_avg:87.28ms
step:82/1670 train_time:7157ms step_avg:87.29ms
step:83/1670 train_time:7245ms step_avg:87.29ms
step:84/1670 train_time:7333ms step_avg:87.30ms
step:85/1670 train_time:7422ms step_avg:87.32ms
step:86/1670 train_time:7510ms step_avg:87.32ms
step:87/1670 train_time:7598ms step_avg:87.33ms
step:88/1670 train_time:7685ms step_avg:87.33ms
step:89/1670 train_time:7772ms step_avg:87.33ms
step:90/1670 train_time:7859ms step_avg:87.33ms
step:91/1670 train_time:7946ms step_avg:87.32ms
step:92/1670 train_time:8034ms step_avg:87.33ms
step:93/1670 train_time:8123ms step_avg:87.34ms
step:94/1670 train_time:8210ms step_avg:87.34ms
step:95/1670 train_time:8297ms step_avg:87.34ms
step:96/1670 train_time:8385ms step_avg:87.34ms
step:97/1670 train_time:8473ms step_avg:87.35ms
step:98/1670 train_time:8561ms step_avg:87.36ms
step:99/1670 train_time:8649ms step_avg:87.37ms
step:100/1670 train_time:8737ms step_avg:87.37ms
step:101/1670 train_time:8825ms step_avg:87.37ms
step:102/1670 train_time:8912ms step_avg:87.37ms
step:103/1670 train_time:9000ms step_avg:87.38ms
step:104/1670 train_time:9087ms step_avg:87.38ms
step:105/1670 train_time:9175ms step_avg:87.38ms
step:106/1670 train_time:9263ms step_avg:87.39ms
step:107/1670 train_time:9351ms step_avg:87.39ms
step:108/1670 train_time:9439ms step_avg:87.40ms
step:109/1670 train_time:9527ms step_avg:87.40ms
step:110/1670 train_time:9615ms step_avg:87.41ms
step:111/1670 train_time:9703ms step_avg:87.41ms
step:112/1670 train_time:9790ms step_avg:87.41ms
step:113/1670 train_time:9877ms step_avg:87.41ms
step:114/1670 train_time:9965ms step_avg:87.41ms
step:115/1670 train_time:10053ms step_avg:87.42ms
step:116/1670 train_time:10141ms step_avg:87.42ms
step:117/1670 train_time:10228ms step_avg:87.42ms
step:118/1670 train_time:10316ms step_avg:87.42ms
step:119/1670 train_time:10403ms step_avg:87.42ms
step:120/1670 train_time:10491ms step_avg:87.43ms
step:121/1670 train_time:10579ms step_avg:87.43ms
step:122/1670 train_time:10666ms step_avg:87.43ms
step:123/1670 train_time:10754ms step_avg:87.43ms
step:124/1670 train_time:10842ms step_avg:87.44ms
step:125/1670 train_time:10930ms step_avg:87.44ms
step:125/1670 val_loss:4.3285 train_time:11020ms step_avg:88.16ms
step:126/1670 train_time:11039ms step_avg:87.61ms
step:127/1670 train_time:11109ms step_avg:87.47ms
step:128/1670 train_time:11203ms step_avg:87.52ms
step:129/1670 train_time:11294ms step_avg:87.55ms
step:130/1670 train_time:11382ms step_avg:87.55ms
step:131/1670 train_time:11469ms step_avg:87.55ms
step:132/1670 train_time:11555ms step_avg:87.54ms
step:133/1670 train_time:11641ms step_avg:87.53ms
step:134/1670 train_time:11728ms step_avg:87.52ms
step:135/1670 train_time:11814ms step_avg:87.51ms
step:136/1670 train_time:11900ms step_avg:87.50ms
step:137/1670 train_time:11987ms step_avg:87.49ms
step:138/1670 train_time:12075ms step_avg:87.50ms
step:139/1670 train_time:12165ms step_avg:87.52ms
step:140/1670 train_time:12255ms step_avg:87.53ms
step:141/1670 train_time:12343ms step_avg:87.54ms
step:142/1670 train_time:12431ms step_avg:87.54ms
step:143/1670 train_time:12518ms step_avg:87.54ms
step:144/1670 train_time:12605ms step_avg:87.53ms
step:145/1670 train_time:12692ms step_avg:87.53ms
step:146/1670 train_time:12778ms step_avg:87.52ms
step:147/1670 train_time:12865ms step_avg:87.52ms
step:148/1670 train_time:12952ms step_avg:87.51ms
step:149/1670 train_time:13039ms step_avg:87.51ms
step:150/1670 train_time:13128ms step_avg:87.52ms
step:151/1670 train_time:13217ms step_avg:87.53ms
step:152/1670 train_time:13306ms step_avg:87.54ms
step:153/1670 train_time:13393ms step_avg:87.54ms
step:154/1670 train_time:13482ms step_avg:87.55ms
step:155/1670 train_time:13569ms step_avg:87.54ms
step:156/1670 train_time:13656ms step_avg:87.54ms
step:157/1670 train_time:13742ms step_avg:87.53ms
step:158/1670 train_time:13828ms step_avg:87.52ms
step:159/1670 train_time:13915ms step_avg:87.52ms
step:160/1670 train_time:14003ms step_avg:87.52ms
step:161/1670 train_time:14091ms step_avg:87.52ms
step:162/1670 train_time:14180ms step_avg:87.53ms
step:163/1670 train_time:14269ms step_avg:87.54ms
step:164/1670 train_time:14357ms step_avg:87.54ms
step:165/1670 train_time:14445ms step_avg:87.54ms
step:166/1670 train_time:14532ms step_avg:87.55ms
step:167/1670 train_time:14619ms step_avg:87.54ms
step:168/1670 train_time:14707ms step_avg:87.54ms
step:169/1670 train_time:14793ms step_avg:87.53ms
step:170/1670 train_time:14880ms step_avg:87.53ms
step:171/1670 train_time:14967ms step_avg:87.53ms
step:172/1670 train_time:15054ms step_avg:87.52ms
step:173/1670 train_time:15142ms step_avg:87.53ms
step:174/1670 train_time:15230ms step_avg:87.53ms
step:175/1670 train_time:15318ms step_avg:87.53ms
step:176/1670 train_time:15407ms step_avg:87.54ms
step:177/1670 train_time:15494ms step_avg:87.53ms
step:178/1670 train_time:15582ms step_avg:87.54ms
step:179/1670 train_time:15669ms step_avg:87.54ms
step:180/1670 train_time:15755ms step_avg:87.53ms
step:181/1670 train_time:15842ms step_avg:87.53ms
step:182/1670 train_time:15930ms step_avg:87.53ms
step:183/1670 train_time:16017ms step_avg:87.52ms
step:184/1670 train_time:16105ms step_avg:87.53ms
step:185/1670 train_time:16192ms step_avg:87.53ms
step:186/1670 train_time:16280ms step_avg:87.53ms
step:187/1670 train_time:16368ms step_avg:87.53ms
step:188/1670 train_time:16455ms step_avg:87.53ms
step:189/1670 train_time:16543ms step_avg:87.53ms
step:190/1670 train_time:16630ms step_avg:87.53ms
step:191/1670 train_time:16717ms step_avg:87.52ms
step:192/1670 train_time:16804ms step_avg:87.52ms
step:193/1670 train_time:16891ms step_avg:87.52ms
step:194/1670 train_time:16978ms step_avg:87.52ms
step:195/1670 train_time:17066ms step_avg:87.52ms
step:196/1670 train_time:17153ms step_avg:87.52ms
step:197/1670 train_time:17242ms step_avg:87.52ms
step:198/1670 train_time:17329ms step_avg:87.52ms
step:199/1670 train_time:17417ms step_avg:87.52ms
step:200/1670 train_time:17504ms step_avg:87.52ms
step:201/1670 train_time:17591ms step_avg:87.52ms
step:202/1670 train_time:17679ms step_avg:87.52ms
step:203/1670 train_time:17767ms step_avg:87.52ms
step:204/1670 train_time:17854ms step_avg:87.52ms
step:205/1670 train_time:17941ms step_avg:87.52ms
step:206/1670 train_time:18028ms step_avg:87.51ms
step:207/1670 train_time:18115ms step_avg:87.51ms
step:208/1670 train_time:18202ms step_avg:87.51ms
step:209/1670 train_time:18290ms step_avg:87.51ms
step:210/1670 train_time:18377ms step_avg:87.51ms
step:211/1670 train_time:18465ms step_avg:87.51ms
step:212/1670 train_time:18552ms step_avg:87.51ms
step:213/1670 train_time:18640ms step_avg:87.51ms
step:214/1670 train_time:18727ms step_avg:87.51ms
step:215/1670 train_time:18814ms step_avg:87.51ms
step:216/1670 train_time:18902ms step_avg:87.51ms
step:217/1670 train_time:18989ms step_avg:87.51ms
step:218/1670 train_time:19076ms step_avg:87.51ms
step:219/1670 train_time:19164ms step_avg:87.51ms
step:220/1670 train_time:19251ms step_avg:87.51ms
step:221/1670 train_time:19339ms step_avg:87.51ms
step:222/1670 train_time:19427ms step_avg:87.51ms
step:223/1670 train_time:19514ms step_avg:87.51ms
step:224/1670 train_time:19601ms step_avg:87.51ms
step:225/1670 train_time:19690ms step_avg:87.51ms
step:226/1670 train_time:19777ms step_avg:87.51ms
step:227/1670 train_time:19864ms step_avg:87.51ms
step:228/1670 train_time:19951ms step_avg:87.50ms
step:229/1670 train_time:20038ms step_avg:87.50ms
step:230/1670 train_time:20125ms step_avg:87.50ms
step:231/1670 train_time:20213ms step_avg:87.50ms
step:232/1670 train_time:20300ms step_avg:87.50ms
step:233/1670 train_time:20389ms step_avg:87.50ms
step:234/1670 train_time:20476ms step_avg:87.50ms
step:235/1670 train_time:20564ms step_avg:87.50ms
step:236/1670 train_time:20651ms step_avg:87.51ms
step:237/1670 train_time:20739ms step_avg:87.51ms
step:238/1670 train_time:20827ms step_avg:87.51ms
step:239/1670 train_time:20914ms step_avg:87.51ms
step:240/1670 train_time:21002ms step_avg:87.51ms
step:241/1670 train_time:21089ms step_avg:87.51ms
step:242/1670 train_time:21177ms step_avg:87.51ms
step:243/1670 train_time:21265ms step_avg:87.51ms
step:244/1670 train_time:21352ms step_avg:87.51ms
step:245/1670 train_time:21439ms step_avg:87.51ms
step:246/1670 train_time:21526ms step_avg:87.50ms
step:247/1670 train_time:21614ms step_avg:87.51ms
step:248/1670 train_time:21701ms step_avg:87.51ms
step:249/1670 train_time:21789ms step_avg:87.51ms
step:250/1670 train_time:21877ms step_avg:87.51ms
step:250/1670 val_loss:3.9931 train_time:21966ms step_avg:87.86ms
step:251/1670 train_time:21986ms step_avg:87.59ms
step:252/1670 train_time:22056ms step_avg:87.52ms
step:253/1670 train_time:22148ms step_avg:87.54ms
step:254/1670 train_time:22238ms step_avg:87.55ms
step:255/1670 train_time:22325ms step_avg:87.55ms
step:256/1670 train_time:22411ms step_avg:87.54ms
step:257/1670 train_time:22497ms step_avg:87.54ms
step:258/1670 train_time:22585ms step_avg:87.54ms
step:259/1670 train_time:22672ms step_avg:87.54ms
step:260/1670 train_time:22759ms step_avg:87.53ms
step:261/1670 train_time:22845ms step_avg:87.53ms
step:262/1670 train_time:22932ms step_avg:87.53ms
step:263/1670 train_time:23021ms step_avg:87.53ms
step:264/1670 train_time:23110ms step_avg:87.54ms
step:265/1670 train_time:23200ms step_avg:87.55ms
step:266/1670 train_time:23288ms step_avg:87.55ms
step:267/1670 train_time:23375ms step_avg:87.55ms
step:268/1670 train_time:23462ms step_avg:87.54ms
step:269/1670 train_time:23548ms step_avg:87.54ms
step:270/1670 train_time:23635ms step_avg:87.54ms
step:271/1670 train_time:23723ms step_avg:87.54ms
step:272/1670 train_time:23809ms step_avg:87.53ms
step:273/1670 train_time:23895ms step_avg:87.53ms
step:274/1670 train_time:23983ms step_avg:87.53ms
step:275/1670 train_time:24071ms step_avg:87.53ms
step:276/1670 train_time:24160ms step_avg:87.54ms
step:277/1670 train_time:24248ms step_avg:87.54ms
step:278/1670 train_time:24335ms step_avg:87.54ms
step:279/1670 train_time:24423ms step_avg:87.54ms
step:280/1670 train_time:24510ms step_avg:87.54ms
step:281/1670 train_time:24598ms step_avg:87.54ms
step:282/1670 train_time:24684ms step_avg:87.53ms
step:283/1670 train_time:24771ms step_avg:87.53ms
step:284/1670 train_time:24859ms step_avg:87.53ms
step:285/1670 train_time:24946ms step_avg:87.53ms
step:286/1670 train_time:25035ms step_avg:87.53ms
step:287/1670 train_time:25124ms step_avg:87.54ms
step:288/1670 train_time:25211ms step_avg:87.54ms
step:289/1670 train_time:25298ms step_avg:87.54ms
step:290/1670 train_time:25386ms step_avg:87.54ms
step:291/1670 train_time:25473ms step_avg:87.54ms
step:292/1670 train_time:25560ms step_avg:87.53ms
step:293/1670 train_time:25647ms step_avg:87.53ms
step:294/1670 train_time:25734ms step_avg:87.53ms
step:295/1670 train_time:25821ms step_avg:87.53ms
step:296/1670 train_time:25908ms step_avg:87.53ms
step:297/1670 train_time:25996ms step_avg:87.53ms
step:298/1670 train_time:26084ms step_avg:87.53ms
step:299/1670 train_time:26171ms step_avg:87.53ms
step:300/1670 train_time:26260ms step_avg:87.53ms
step:301/1670 train_time:26347ms step_avg:87.53ms
step:302/1670 train_time:26435ms step_avg:87.53ms
step:303/1670 train_time:26523ms step_avg:87.53ms
step:304/1670 train_time:26610ms step_avg:87.53ms
step:305/1670 train_time:26698ms step_avg:87.53ms
step:306/1670 train_time:26785ms step_avg:87.53ms
step:307/1670 train_time:26872ms step_avg:87.53ms
step:308/1670 train_time:26959ms step_avg:87.53ms
step:309/1670 train_time:27047ms step_avg:87.53ms
step:310/1670 train_time:27136ms step_avg:87.53ms
step:311/1670 train_time:27224ms step_avg:87.54ms
step:312/1670 train_time:27312ms step_avg:87.54ms
step:313/1670 train_time:27399ms step_avg:87.54ms
step:314/1670 train_time:27487ms step_avg:87.54ms
step:315/1670 train_time:27574ms step_avg:87.54ms
step:316/1670 train_time:27662ms step_avg:87.54ms
step:317/1670 train_time:27748ms step_avg:87.53ms
step:318/1670 train_time:27836ms step_avg:87.53ms
step:319/1670 train_time:27924ms step_avg:87.54ms
step:320/1670 train_time:28011ms step_avg:87.53ms
step:321/1670 train_time:28099ms step_avg:87.53ms
step:322/1670 train_time:28186ms step_avg:87.53ms
step:323/1670 train_time:28274ms step_avg:87.54ms
step:324/1670 train_time:28362ms step_avg:87.54ms
step:325/1670 train_time:28449ms step_avg:87.54ms
step:326/1670 train_time:28537ms step_avg:87.54ms
step:327/1670 train_time:28625ms step_avg:87.54ms
step:328/1670 train_time:28712ms step_avg:87.54ms
step:329/1670 train_time:28800ms step_avg:87.54ms
step:330/1670 train_time:28887ms step_avg:87.54ms
step:331/1670 train_time:28975ms step_avg:87.54ms
step:332/1670 train_time:29063ms step_avg:87.54ms
step:333/1670 train_time:29150ms step_avg:87.54ms
step:334/1670 train_time:29238ms step_avg:87.54ms
step:335/1670 train_time:29326ms step_avg:87.54ms
step:336/1670 train_time:29413ms step_avg:87.54ms
step:337/1670 train_time:29501ms step_avg:87.54ms
step:338/1670 train_time:29589ms step_avg:87.54ms
step:339/1670 train_time:29675ms step_avg:87.54ms
step:340/1670 train_time:29763ms step_avg:87.54ms
step:341/1670 train_time:29850ms step_avg:87.54ms
step:342/1670 train_time:29938ms step_avg:87.54ms
step:343/1670 train_time:30026ms step_avg:87.54ms
step:344/1670 train_time:30114ms step_avg:87.54ms
step:345/1670 train_time:30202ms step_avg:87.54ms
step:346/1670 train_time:30289ms step_avg:87.54ms
step:347/1670 train_time:30376ms step_avg:87.54ms
step:348/1670 train_time:30464ms step_avg:87.54ms
step:349/1670 train_time:30550ms step_avg:87.54ms
step:350/1670 train_time:30638ms step_avg:87.54ms
step:351/1670 train_time:30727ms step_avg:87.54ms
step:352/1670 train_time:30814ms step_avg:87.54ms
step:353/1670 train_time:30902ms step_avg:87.54ms
step:354/1670 train_time:30990ms step_avg:87.54ms
step:355/1670 train_time:31077ms step_avg:87.54ms
step:356/1670 train_time:31165ms step_avg:87.54ms
step:357/1670 train_time:31252ms step_avg:87.54ms
step:358/1670 train_time:31339ms step_avg:87.54ms
step:359/1670 train_time:31427ms step_avg:87.54ms
step:360/1670 train_time:31514ms step_avg:87.54ms
step:361/1670 train_time:31602ms step_avg:87.54ms
step:362/1670 train_time:31690ms step_avg:87.54ms
step:363/1670 train_time:31777ms step_avg:87.54ms
step:364/1670 train_time:31865ms step_avg:87.54ms
step:365/1670 train_time:31953ms step_avg:87.54ms
step:366/1670 train_time:32042ms step_avg:87.55ms
step:367/1670 train_time:32129ms step_avg:87.55ms
step:368/1670 train_time:32216ms step_avg:87.54ms
step:369/1670 train_time:32304ms step_avg:87.55ms
step:370/1670 train_time:32392ms step_avg:87.55ms
step:371/1670 train_time:32479ms step_avg:87.54ms
step:372/1670 train_time:32567ms step_avg:87.55ms
step:373/1670 train_time:32655ms step_avg:87.55ms
step:374/1670 train_time:32743ms step_avg:87.55ms
step:375/1670 train_time:32830ms step_avg:87.55ms
step:375/1670 val_loss:3.8258 train_time:32919ms step_avg:87.78ms
step:376/1670 train_time:32938ms step_avg:87.60ms
step:377/1670 train_time:33009ms step_avg:87.56ms
step:378/1670 train_time:33099ms step_avg:87.56ms
step:379/1670 train_time:33190ms step_avg:87.57ms
step:380/1670 train_time:33278ms step_avg:87.57ms
step:381/1670 train_time:33365ms step_avg:87.57ms
step:382/1670 train_time:33451ms step_avg:87.57ms
step:383/1670 train_time:33538ms step_avg:87.57ms
step:384/1670 train_time:33624ms step_avg:87.56ms
step:385/1670 train_time:33711ms step_avg:87.56ms
step:386/1670 train_time:33798ms step_avg:87.56ms
step:387/1670 train_time:33887ms step_avg:87.56ms
step:388/1670 train_time:33976ms step_avg:87.57ms
step:389/1670 train_time:34066ms step_avg:87.57ms
step:390/1670 train_time:34155ms step_avg:87.58ms
step:391/1670 train_time:34244ms step_avg:87.58ms
step:392/1670 train_time:34331ms step_avg:87.58ms
step:393/1670 train_time:34418ms step_avg:87.58ms
step:394/1670 train_time:34506ms step_avg:87.58ms
step:395/1670 train_time:34592ms step_avg:87.58ms
step:396/1670 train_time:34680ms step_avg:87.57ms
step:397/1670 train_time:34768ms step_avg:87.58ms
step:398/1670 train_time:34854ms step_avg:87.57ms
step:399/1670 train_time:34943ms step_avg:87.58ms
step:400/1670 train_time:35031ms step_avg:87.58ms
step:401/1670 train_time:35119ms step_avg:87.58ms
step:402/1670 train_time:35208ms step_avg:87.58ms
step:403/1670 train_time:35296ms step_avg:87.58ms
step:404/1670 train_time:35384ms step_avg:87.58ms
step:405/1670 train_time:35471ms step_avg:87.58ms
step:406/1670 train_time:35558ms step_avg:87.58ms
step:407/1670 train_time:35645ms step_avg:87.58ms
step:408/1670 train_time:35732ms step_avg:87.58ms
step:409/1670 train_time:35820ms step_avg:87.58ms
step:410/1670 train_time:35907ms step_avg:87.58ms
step:411/1670 train_time:35994ms step_avg:87.58ms
step:412/1670 train_time:36083ms step_avg:87.58ms
step:413/1670 train_time:36171ms step_avg:87.58ms
step:414/1670 train_time:36259ms step_avg:87.58ms
step:415/1670 train_time:36347ms step_avg:87.58ms
step:416/1670 train_time:36434ms step_avg:87.58ms
step:417/1670 train_time:36521ms step_avg:87.58ms
step:418/1670 train_time:36609ms step_avg:87.58ms
step:419/1670 train_time:36696ms step_avg:87.58ms
step:420/1670 train_time:36784ms step_avg:87.58ms
step:421/1670 train_time:36872ms step_avg:87.58ms
step:422/1670 train_time:36960ms step_avg:87.58ms
step:423/1670 train_time:37048ms step_avg:87.58ms
step:424/1670 train_time:37136ms step_avg:87.59ms
step:425/1670 train_time:37225ms step_avg:87.59ms
step:426/1670 train_time:37312ms step_avg:87.59ms
step:427/1670 train_time:37401ms step_avg:87.59ms
step:428/1670 train_time:37487ms step_avg:87.59ms
step:429/1670 train_time:37574ms step_avg:87.59ms
step:430/1670 train_time:37662ms step_avg:87.59ms
step:431/1670 train_time:37749ms step_avg:87.58ms
step:432/1670 train_time:37836ms step_avg:87.58ms
step:433/1670 train_time:37923ms step_avg:87.58ms
step:434/1670 train_time:38012ms step_avg:87.58ms
step:435/1670 train_time:38100ms step_avg:87.59ms
step:436/1670 train_time:38188ms step_avg:87.59ms
step:437/1670 train_time:38276ms step_avg:87.59ms
step:438/1670 train_time:38364ms step_avg:87.59ms
step:439/1670 train_time:38451ms step_avg:87.59ms
step:440/1670 train_time:38539ms step_avg:87.59ms
step:441/1670 train_time:38627ms step_avg:87.59ms
step:442/1670 train_time:38714ms step_avg:87.59ms
step:443/1670 train_time:38800ms step_avg:87.59ms
step:444/1670 train_time:38889ms step_avg:87.59ms
step:445/1670 train_time:38978ms step_avg:87.59ms
step:446/1670 train_time:39067ms step_avg:87.59ms
step:447/1670 train_time:39153ms step_avg:87.59ms
step:448/1670 train_time:39241ms step_avg:87.59ms
step:449/1670 train_time:39329ms step_avg:87.59ms
step:450/1670 train_time:39417ms step_avg:87.59ms
step:451/1670 train_time:39506ms step_avg:87.60ms
step:452/1670 train_time:39593ms step_avg:87.59ms
step:453/1670 train_time:39681ms step_avg:87.60ms
step:454/1670 train_time:39768ms step_avg:87.59ms
step:455/1670 train_time:39855ms step_avg:87.59ms
step:456/1670 train_time:39944ms step_avg:87.60ms
step:457/1670 train_time:40031ms step_avg:87.60ms
step:458/1670 train_time:40119ms step_avg:87.60ms
step:459/1670 train_time:40207ms step_avg:87.60ms
step:460/1670 train_time:40294ms step_avg:87.60ms
step:461/1670 train_time:40382ms step_avg:87.60ms
step:462/1670 train_time:40471ms step_avg:87.60ms
step:463/1670 train_time:40558ms step_avg:87.60ms
step:464/1670 train_time:40646ms step_avg:87.60ms
step:465/1670 train_time:40733ms step_avg:87.60ms
step:466/1670 train_time:40821ms step_avg:87.60ms
step:467/1670 train_time:40909ms step_avg:87.60ms
step:468/1670 train_time:40996ms step_avg:87.60ms
step:469/1670 train_time:41084ms step_avg:87.60ms
step:470/1670 train_time:41172ms step_avg:87.60ms
step:471/1670 train_time:41259ms step_avg:87.60ms
step:472/1670 train_time:41347ms step_avg:87.60ms
step:473/1670 train_time:41434ms step_avg:87.60ms
step:474/1670 train_time:41522ms step_avg:87.60ms
step:475/1670 train_time:41611ms step_avg:87.60ms
step:476/1670 train_time:41699ms step_avg:87.60ms
step:477/1670 train_time:41787ms step_avg:87.60ms
step:478/1670 train_time:41874ms step_avg:87.60ms
step:479/1670 train_time:41962ms step_avg:87.60ms
step:480/1670 train_time:42050ms step_avg:87.60ms
step:481/1670 train_time:42138ms step_avg:87.60ms
step:482/1670 train_time:42226ms step_avg:87.61ms
step:483/1670 train_time:42313ms step_avg:87.60ms
step:484/1670 train_time:42400ms step_avg:87.60ms
step:485/1670 train_time:42488ms step_avg:87.60ms
step:486/1670 train_time:42575ms step_avg:87.60ms
step:487/1670 train_time:42664ms step_avg:87.60ms
step:488/1670 train_time:42751ms step_avg:87.60ms
step:489/1670 train_time:42839ms step_avg:87.61ms
step:490/1670 train_time:42928ms step_avg:87.61ms
step:491/1670 train_time:43015ms step_avg:87.61ms
step:492/1670 train_time:43103ms step_avg:87.61ms
step:493/1670 train_time:43191ms step_avg:87.61ms
step:494/1670 train_time:43278ms step_avg:87.61ms
step:495/1670 train_time:43367ms step_avg:87.61ms
step:496/1670 train_time:43454ms step_avg:87.61ms
step:497/1670 train_time:43542ms step_avg:87.61ms
step:498/1670 train_time:43629ms step_avg:87.61ms
step:499/1670 train_time:43717ms step_avg:87.61ms
step:500/1670 train_time:43805ms step_avg:87.61ms
step:500/1670 val_loss:3.7195 train_time:43894ms step_avg:87.79ms
step:501/1670 train_time:43913ms step_avg:87.65ms
step:502/1670 train_time:43985ms step_avg:87.62ms
step:503/1670 train_time:44079ms step_avg:87.63ms
step:504/1670 train_time:44167ms step_avg:87.63ms
step:505/1670 train_time:44255ms step_avg:87.63ms
step:506/1670 train_time:44341ms step_avg:87.63ms
step:507/1670 train_time:44428ms step_avg:87.63ms
step:508/1670 train_time:44515ms step_avg:87.63ms
step:509/1670 train_time:44601ms step_avg:87.62ms
step:510/1670 train_time:44687ms step_avg:87.62ms
step:511/1670 train_time:44774ms step_avg:87.62ms
step:512/1670 train_time:44862ms step_avg:87.62ms
step:513/1670 train_time:44951ms step_avg:87.62ms
step:514/1670 train_time:45041ms step_avg:87.63ms
step:515/1670 train_time:45131ms step_avg:87.63ms
step:516/1670 train_time:45220ms step_avg:87.63ms
step:517/1670 train_time:45307ms step_avg:87.63ms
step:518/1670 train_time:45394ms step_avg:87.63ms
step:519/1670 train_time:45481ms step_avg:87.63ms
step:520/1670 train_time:45568ms step_avg:87.63ms
step:521/1670 train_time:45656ms step_avg:87.63ms
step:522/1670 train_time:45742ms step_avg:87.63ms
step:523/1670 train_time:45830ms step_avg:87.63ms
step:524/1670 train_time:45920ms step_avg:87.63ms
step:525/1670 train_time:46009ms step_avg:87.64ms
step:526/1670 train_time:46098ms step_avg:87.64ms
step:527/1670 train_time:46187ms step_avg:87.64ms
step:528/1670 train_time:46275ms step_avg:87.64ms
step:529/1670 train_time:46362ms step_avg:87.64ms
step:530/1670 train_time:46449ms step_avg:87.64ms
step:531/1670 train_time:46536ms step_avg:87.64ms
step:532/1670 train_time:46623ms step_avg:87.64ms
step:533/1670 train_time:46710ms step_avg:87.64ms
step:534/1670 train_time:46798ms step_avg:87.64ms
step:535/1670 train_time:46884ms step_avg:87.63ms
step:536/1670 train_time:46973ms step_avg:87.64ms
step:537/1670 train_time:47062ms step_avg:87.64ms
step:538/1670 train_time:47151ms step_avg:87.64ms
step:539/1670 train_time:47239ms step_avg:87.64ms
step:540/1670 train_time:47328ms step_avg:87.64ms
step:541/1670 train_time:47416ms step_avg:87.65ms
step:542/1670 train_time:47503ms step_avg:87.64ms
step:543/1670 train_time:47590ms step_avg:87.64ms
step:544/1670 train_time:47677ms step_avg:87.64ms
step:545/1670 train_time:47764ms step_avg:87.64ms
step:546/1670 train_time:47854ms step_avg:87.64ms
step:547/1670 train_time:47943ms step_avg:87.65ms
step:548/1670 train_time:48032ms step_avg:87.65ms
step:549/1670 train_time:48122ms step_avg:87.65ms
step:550/1670 train_time:48211ms step_avg:87.66ms
step:551/1670 train_time:48300ms step_avg:87.66ms
step:552/1670 train_time:48389ms step_avg:87.66ms
step:553/1670 train_time:48478ms step_avg:87.66ms
step:554/1670 train_time:48566ms step_avg:87.66ms
step:555/1670 train_time:48655ms step_avg:87.67ms
step:556/1670 train_time:48743ms step_avg:87.67ms
step:557/1670 train_time:48833ms step_avg:87.67ms
step:558/1670 train_time:48922ms step_avg:87.67ms
step:559/1670 train_time:49012ms step_avg:87.68ms
step:560/1670 train_time:49100ms step_avg:87.68ms
step:561/1670 train_time:49190ms step_avg:87.68ms
step:562/1670 train_time:49279ms step_avg:87.69ms
step:563/1670 train_time:49368ms step_avg:87.69ms
step:564/1670 train_time:49458ms step_avg:87.69ms
step:565/1670 train_time:49546ms step_avg:87.69ms
step:566/1670 train_time:49634ms step_avg:87.69ms
step:567/1670 train_time:49723ms step_avg:87.69ms
step:568/1670 train_time:49812ms step_avg:87.70ms
step:569/1670 train_time:49901ms step_avg:87.70ms
step:570/1670 train_time:49991ms step_avg:87.70ms
step:571/1670 train_time:50079ms step_avg:87.70ms
step:572/1670 train_time:50168ms step_avg:87.71ms
step:573/1670 train_time:50257ms step_avg:87.71ms
step:574/1670 train_time:50346ms step_avg:87.71ms
step:575/1670 train_time:50435ms step_avg:87.71ms
step:576/1670 train_time:50524ms step_avg:87.71ms
step:577/1670 train_time:50613ms step_avg:87.72ms
step:578/1670 train_time:50701ms step_avg:87.72ms
step:579/1670 train_time:50790ms step_avg:87.72ms
step:580/1670 train_time:50879ms step_avg:87.72ms
step:581/1670 train_time:50968ms step_avg:87.73ms
step:582/1670 train_time:51058ms step_avg:87.73ms
step:583/1670 train_time:51147ms step_avg:87.73ms
step:584/1670 train_time:51237ms step_avg:87.73ms
step:585/1670 train_time:51326ms step_avg:87.74ms
step:586/1670 train_time:51415ms step_avg:87.74ms
step:587/1670 train_time:51504ms step_avg:87.74ms
step:588/1670 train_time:51593ms step_avg:87.74ms
step:589/1670 train_time:51681ms step_avg:87.74ms
step:590/1670 train_time:51770ms step_avg:87.75ms
step:591/1670 train_time:51859ms step_avg:87.75ms
step:592/1670 train_time:51949ms step_avg:87.75ms
step:593/1670 train_time:52039ms step_avg:87.76ms
step:594/1670 train_time:52128ms step_avg:87.76ms
step:595/1670 train_time:52217ms step_avg:87.76ms
step:596/1670 train_time:52306ms step_avg:87.76ms
step:597/1670 train_time:52395ms step_avg:87.76ms
step:598/1670 train_time:52483ms step_avg:87.76ms
step:599/1670 train_time:52573ms step_avg:87.77ms
step:600/1670 train_time:52661ms step_avg:87.77ms
step:601/1670 train_time:52750ms step_avg:87.77ms
step:602/1670 train_time:52838ms step_avg:87.77ms
step:603/1670 train_time:52927ms step_avg:87.77ms
step:604/1670 train_time:53017ms step_avg:87.78ms
step:605/1670 train_time:53105ms step_avg:87.78ms
step:606/1670 train_time:53194ms step_avg:87.78ms
step:607/1670 train_time:53283ms step_avg:87.78ms
step:608/1670 train_time:53372ms step_avg:87.78ms
step:609/1670 train_time:53461ms step_avg:87.79ms
step:610/1670 train_time:53551ms step_avg:87.79ms
step:611/1670 train_time:53639ms step_avg:87.79ms
step:612/1670 train_time:53727ms step_avg:87.79ms
step:613/1670 train_time:53818ms step_avg:87.80ms
step:614/1670 train_time:53907ms step_avg:87.80ms
step:615/1670 train_time:53997ms step_avg:87.80ms
step:616/1670 train_time:54085ms step_avg:87.80ms
step:617/1670 train_time:54174ms step_avg:87.80ms
step:618/1670 train_time:54263ms step_avg:87.80ms
step:619/1670 train_time:54352ms step_avg:87.81ms
step:620/1670 train_time:54441ms step_avg:87.81ms
step:621/1670 train_time:54531ms step_avg:87.81ms
step:622/1670 train_time:54619ms step_avg:87.81ms
step:623/1670 train_time:54707ms step_avg:87.81ms
step:624/1670 train_time:54797ms step_avg:87.82ms
step:625/1670 train_time:54886ms step_avg:87.82ms
step:625/1670 val_loss:3.6182 train_time:54977ms step_avg:87.96ms
step:626/1670 train_time:54997ms step_avg:87.85ms
step:627/1670 train_time:55067ms step_avg:87.83ms
step:628/1670 train_time:55156ms step_avg:87.83ms
step:629/1670 train_time:55248ms step_avg:87.83ms
step:630/1670 train_time:55335ms step_avg:87.83ms
step:631/1670 train_time:55423ms step_avg:87.83ms
step:632/1670 train_time:55510ms step_avg:87.83ms
step:633/1670 train_time:55597ms step_avg:87.83ms
step:634/1670 train_time:55686ms step_avg:87.83ms
step:635/1670 train_time:55776ms step_avg:87.84ms
step:636/1670 train_time:55866ms step_avg:87.84ms
step:637/1670 train_time:55956ms step_avg:87.84ms
step:638/1670 train_time:56048ms step_avg:87.85ms
step:639/1670 train_time:56136ms step_avg:87.85ms
step:640/1670 train_time:56225ms step_avg:87.85ms
step:641/1670 train_time:56314ms step_avg:87.85ms
step:642/1670 train_time:56402ms step_avg:87.85ms
step:643/1670 train_time:56490ms step_avg:87.85ms
step:644/1670 train_time:56578ms step_avg:87.85ms
step:645/1670 train_time:56667ms step_avg:87.86ms
step:646/1670 train_time:56755ms step_avg:87.86ms
step:647/1670 train_time:56845ms step_avg:87.86ms
step:648/1670 train_time:56935ms step_avg:87.86ms
step:649/1670 train_time:57025ms step_avg:87.87ms
step:650/1670 train_time:57114ms step_avg:87.87ms
step:651/1670 train_time:57203ms step_avg:87.87ms
step:652/1670 train_time:57292ms step_avg:87.87ms
step:653/1670 train_time:57381ms step_avg:87.87ms
step:654/1670 train_time:57469ms step_avg:87.87ms
step:655/1670 train_time:57557ms step_avg:87.87ms
step:656/1670 train_time:57646ms step_avg:87.88ms
step:657/1670 train_time:57735ms step_avg:87.88ms
step:658/1670 train_time:57824ms step_avg:87.88ms
step:659/1670 train_time:57913ms step_avg:87.88ms
step:660/1670 train_time:58003ms step_avg:87.88ms
step:661/1670 train_time:58092ms step_avg:87.89ms
step:662/1670 train_time:58182ms step_avg:87.89ms
step:663/1670 train_time:58271ms step_avg:87.89ms
step:664/1670 train_time:58359ms step_avg:87.89ms
step:665/1670 train_time:58448ms step_avg:87.89ms
step:666/1670 train_time:58536ms step_avg:87.89ms
step:667/1670 train_time:58624ms step_avg:87.89ms
step:668/1670 train_time:58713ms step_avg:87.89ms
step:669/1670 train_time:58801ms step_avg:87.89ms
step:670/1670 train_time:58891ms step_avg:87.90ms
step:671/1670 train_time:58980ms step_avg:87.90ms
step:672/1670 train_time:59070ms step_avg:87.90ms
step:673/1670 train_time:59159ms step_avg:87.90ms
step:674/1670 train_time:59248ms step_avg:87.90ms
step:675/1670 train_time:59336ms step_avg:87.90ms
step:676/1670 train_time:59424ms step_avg:87.91ms
step:677/1670 train_time:59513ms step_avg:87.91ms
step:678/1670 train_time:59601ms step_avg:87.91ms
step:679/1670 train_time:59690ms step_avg:87.91ms
step:680/1670 train_time:59780ms step_avg:87.91ms
step:681/1670 train_time:59869ms step_avg:87.91ms
step:682/1670 train_time:59959ms step_avg:87.92ms
step:683/1670 train_time:60050ms step_avg:87.92ms
step:684/1670 train_time:60139ms step_avg:87.92ms
step:685/1670 train_time:60228ms step_avg:87.92ms
step:686/1670 train_time:60317ms step_avg:87.93ms
step:687/1670 train_time:60406ms step_avg:87.93ms
step:688/1670 train_time:60494ms step_avg:87.93ms
step:689/1670 train_time:60582ms step_avg:87.93ms
step:690/1670 train_time:60672ms step_avg:87.93ms
step:691/1670 train_time:60761ms step_avg:87.93ms
step:692/1670 train_time:60851ms step_avg:87.93ms
step:693/1670 train_time:60940ms step_avg:87.94ms
step:694/1670 train_time:61029ms step_avg:87.94ms
step:695/1670 train_time:61118ms step_avg:87.94ms
step:696/1670 train_time:61208ms step_avg:87.94ms
step:697/1670 train_time:61296ms step_avg:87.94ms
step:698/1670 train_time:61386ms step_avg:87.95ms
step:699/1670 train_time:61474ms step_avg:87.95ms
step:700/1670 train_time:61563ms step_avg:87.95ms
step:701/1670 train_time:61652ms step_avg:87.95ms
step:702/1670 train_time:61740ms step_avg:87.95ms
step:703/1670 train_time:61829ms step_avg:87.95ms
step:704/1670 train_time:61918ms step_avg:87.95ms
step:705/1670 train_time:62007ms step_avg:87.95ms
step:706/1670 train_time:62095ms step_avg:87.95ms
step:707/1670 train_time:62184ms step_avg:87.95ms
step:708/1670 train_time:62273ms step_avg:87.96ms
step:709/1670 train_time:62362ms step_avg:87.96ms
step:710/1670 train_time:62452ms step_avg:87.96ms
step:711/1670 train_time:62541ms step_avg:87.96ms
step:712/1670 train_time:62629ms step_avg:87.96ms
step:713/1670 train_time:62718ms step_avg:87.96ms
step:714/1670 train_time:62807ms step_avg:87.97ms
step:715/1670 train_time:62896ms step_avg:87.97ms
step:716/1670 train_time:62985ms step_avg:87.97ms
step:717/1670 train_time:63073ms step_avg:87.97ms
step:718/1670 train_time:63162ms step_avg:87.97ms
step:719/1670 train_time:63251ms step_avg:87.97ms
step:720/1670 train_time:63340ms step_avg:87.97ms
step:721/1670 train_time:63430ms step_avg:87.97ms
step:722/1670 train_time:63519ms step_avg:87.98ms
step:723/1670 train_time:63608ms step_avg:87.98ms
step:724/1670 train_time:63696ms step_avg:87.98ms
step:725/1670 train_time:63786ms step_avg:87.98ms
step:726/1670 train_time:63875ms step_avg:87.98ms
step:727/1670 train_time:63964ms step_avg:87.98ms
step:728/1670 train_time:64053ms step_avg:87.99ms
step:729/1670 train_time:64142ms step_avg:87.99ms
step:730/1670 train_time:64231ms step_avg:87.99ms
step:731/1670 train_time:64320ms step_avg:87.99ms
step:732/1670 train_time:64410ms step_avg:87.99ms
step:733/1670 train_time:64498ms step_avg:87.99ms
step:734/1670 train_time:64587ms step_avg:87.99ms
step:735/1670 train_time:64675ms step_avg:87.99ms
step:736/1670 train_time:64765ms step_avg:88.00ms
step:737/1670 train_time:64855ms step_avg:88.00ms
step:738/1670 train_time:64945ms step_avg:88.00ms
step:739/1670 train_time:65034ms step_avg:88.00ms
step:740/1670 train_time:65124ms step_avg:88.01ms
step:741/1670 train_time:65213ms step_avg:88.01ms
step:742/1670 train_time:65301ms step_avg:88.01ms
step:743/1670 train_time:65390ms step_avg:88.01ms
step:744/1670 train_time:65479ms step_avg:88.01ms
step:745/1670 train_time:65568ms step_avg:88.01ms
step:746/1670 train_time:65656ms step_avg:88.01ms
step:747/1670 train_time:65746ms step_avg:88.01ms
step:748/1670 train_time:65834ms step_avg:88.01ms
step:749/1670 train_time:65923ms step_avg:88.02ms
step:750/1670 train_time:66012ms step_avg:88.02ms
step:750/1670 val_loss:3.5684 train_time:66103ms step_avg:88.14ms
step:751/1670 train_time:66123ms step_avg:88.05ms
step:752/1670 train_time:66197ms step_avg:88.03ms
step:753/1670 train_time:66290ms step_avg:88.03ms
step:754/1670 train_time:66379ms step_avg:88.04ms
step:755/1670 train_time:66468ms step_avg:88.04ms
step:756/1670 train_time:66556ms step_avg:88.04ms
step:757/1670 train_time:66644ms step_avg:88.04ms
step:758/1670 train_time:66733ms step_avg:88.04ms
step:759/1670 train_time:66820ms step_avg:88.04ms
step:760/1670 train_time:66908ms step_avg:88.04ms
step:761/1670 train_time:66996ms step_avg:88.04ms
step:762/1670 train_time:67087ms step_avg:88.04ms
step:763/1670 train_time:67177ms step_avg:88.04ms
step:764/1670 train_time:67268ms step_avg:88.05ms
step:765/1670 train_time:67357ms step_avg:88.05ms
step:766/1670 train_time:67447ms step_avg:88.05ms
step:767/1670 train_time:67535ms step_avg:88.05ms
step:768/1670 train_time:67623ms step_avg:88.05ms
step:769/1670 train_time:67711ms step_avg:88.05ms
step:770/1670 train_time:67799ms step_avg:88.05ms
step:771/1670 train_time:67888ms step_avg:88.05ms
step:772/1670 train_time:67976ms step_avg:88.05ms
step:773/1670 train_time:68066ms step_avg:88.05ms
step:774/1670 train_time:68156ms step_avg:88.06ms
step:775/1670 train_time:68247ms step_avg:88.06ms
step:776/1670 train_time:68337ms step_avg:88.06ms
step:777/1670 train_time:68427ms step_avg:88.07ms
step:778/1670 train_time:68515ms step_avg:88.07ms
step:779/1670 train_time:68603ms step_avg:88.07ms
step:780/1670 train_time:68691ms step_avg:88.07ms
step:781/1670 train_time:68780ms step_avg:88.07ms
step:782/1670 train_time:68869ms step_avg:88.07ms
step:783/1670 train_time:68957ms step_avg:88.07ms
step:784/1670 train_time:69046ms step_avg:88.07ms
step:785/1670 train_time:69135ms step_avg:88.07ms
step:786/1670 train_time:69225ms step_avg:88.07ms
step:787/1670 train_time:69314ms step_avg:88.07ms
step:788/1670 train_time:69404ms step_avg:88.08ms
step:789/1670 train_time:69493ms step_avg:88.08ms
step:790/1670 train_time:69582ms step_avg:88.08ms
step:791/1670 train_time:69670ms step_avg:88.08ms
step:792/1670 train_time:69758ms step_avg:88.08ms
step:793/1670 train_time:69847ms step_avg:88.08ms
step:794/1670 train_time:69936ms step_avg:88.08ms
step:795/1670 train_time:70024ms step_avg:88.08ms
step:796/1670 train_time:70114ms step_avg:88.08ms
step:797/1670 train_time:70203ms step_avg:88.08ms
step:798/1670 train_time:70293ms step_avg:88.09ms
step:799/1670 train_time:70382ms step_avg:88.09ms
step:800/1670 train_time:70472ms step_avg:88.09ms
step:801/1670 train_time:70562ms step_avg:88.09ms
step:802/1670 train_time:70650ms step_avg:88.09ms
step:803/1670 train_time:70739ms step_avg:88.09ms
step:804/1670 train_time:70828ms step_avg:88.09ms
step:805/1670 train_time:70916ms step_avg:88.09ms
step:806/1670 train_time:71005ms step_avg:88.10ms
step:807/1670 train_time:71094ms step_avg:88.10ms
step:808/1670 train_time:71183ms step_avg:88.10ms
step:809/1670 train_time:71272ms step_avg:88.10ms
step:810/1670 train_time:71361ms step_avg:88.10ms
step:811/1670 train_time:71451ms step_avg:88.10ms
step:812/1670 train_time:71540ms step_avg:88.10ms
step:813/1670 train_time:71629ms step_avg:88.10ms
step:814/1670 train_time:71718ms step_avg:88.11ms
step:815/1670 train_time:71808ms step_avg:88.11ms
step:816/1670 train_time:71897ms step_avg:88.11ms
step:817/1670 train_time:71986ms step_avg:88.11ms
step:818/1670 train_time:72074ms step_avg:88.11ms
step:819/1670 train_time:72163ms step_avg:88.11ms
step:820/1670 train_time:72252ms step_avg:88.11ms
step:821/1670 train_time:72342ms step_avg:88.11ms
step:822/1670 train_time:72432ms step_avg:88.12ms
step:823/1670 train_time:72521ms step_avg:88.12ms
step:824/1670 train_time:72610ms step_avg:88.12ms
step:825/1670 train_time:72699ms step_avg:88.12ms
step:826/1670 train_time:72790ms step_avg:88.12ms
step:827/1670 train_time:72878ms step_avg:88.12ms
step:828/1670 train_time:72968ms step_avg:88.13ms
step:829/1670 train_time:73056ms step_avg:88.13ms
step:830/1670 train_time:73144ms step_avg:88.13ms
step:831/1670 train_time:73234ms step_avg:88.13ms
step:832/1670 train_time:73323ms step_avg:88.13ms
step:833/1670 train_time:73412ms step_avg:88.13ms
step:834/1670 train_time:73501ms step_avg:88.13ms
step:835/1670 train_time:73590ms step_avg:88.13ms
step:836/1670 train_time:73679ms step_avg:88.13ms
step:837/1670 train_time:73768ms step_avg:88.13ms
step:838/1670 train_time:73857ms step_avg:88.13ms
step:839/1670 train_time:73946ms step_avg:88.14ms
step:840/1670 train_time:74034ms step_avg:88.14ms
step:841/1670 train_time:74123ms step_avg:88.14ms
step:842/1670 train_time:74212ms step_avg:88.14ms
step:843/1670 train_time:74302ms step_avg:88.14ms
step:844/1670 train_time:74391ms step_avg:88.14ms
step:845/1670 train_time:74480ms step_avg:88.14ms
step:846/1670 train_time:74570ms step_avg:88.14ms
step:847/1670 train_time:74658ms step_avg:88.14ms
step:848/1670 train_time:74747ms step_avg:88.14ms
step:849/1670 train_time:74836ms step_avg:88.15ms
step:850/1670 train_time:74925ms step_avg:88.15ms
step:851/1670 train_time:75013ms step_avg:88.15ms
step:852/1670 train_time:75102ms step_avg:88.15ms
step:853/1670 train_time:75192ms step_avg:88.15ms
step:854/1670 train_time:75281ms step_avg:88.15ms
step:855/1670 train_time:75370ms step_avg:88.15ms
step:856/1670 train_time:75459ms step_avg:88.15ms
step:857/1670 train_time:75548ms step_avg:88.15ms
step:858/1670 train_time:75636ms step_avg:88.15ms
step:859/1670 train_time:75726ms step_avg:88.16ms
step:860/1670 train_time:75814ms step_avg:88.16ms
step:861/1670 train_time:75904ms step_avg:88.16ms
step:862/1670 train_time:75992ms step_avg:88.16ms
step:863/1670 train_time:76081ms step_avg:88.16ms
step:864/1670 train_time:76170ms step_avg:88.16ms
step:865/1670 train_time:76259ms step_avg:88.16ms
step:866/1670 train_time:76349ms step_avg:88.16ms
step:867/1670 train_time:76438ms step_avg:88.16ms
step:868/1670 train_time:76526ms step_avg:88.16ms
step:869/1670 train_time:76615ms step_avg:88.16ms
step:870/1670 train_time:76704ms step_avg:88.17ms
step:871/1670 train_time:76793ms step_avg:88.17ms
step:872/1670 train_time:76882ms step_avg:88.17ms
step:873/1670 train_time:76971ms step_avg:88.17ms
step:874/1670 train_time:77059ms step_avg:88.17ms
step:875/1670 train_time:77148ms step_avg:88.17ms
step:875/1670 val_loss:3.5195 train_time:77238ms step_avg:88.27ms
step:876/1670 train_time:77258ms step_avg:88.19ms
step:877/1670 train_time:77332ms step_avg:88.18ms
step:878/1670 train_time:77426ms step_avg:88.18ms
step:879/1670 train_time:77515ms step_avg:88.18ms
step:880/1670 train_time:77603ms step_avg:88.19ms
step:881/1670 train_time:77690ms step_avg:88.18ms
step:882/1670 train_time:77778ms step_avg:88.18ms
step:883/1670 train_time:77867ms step_avg:88.18ms
step:884/1670 train_time:77954ms step_avg:88.18ms
step:885/1670 train_time:78042ms step_avg:88.18ms
step:886/1670 train_time:78130ms step_avg:88.18ms
step:887/1670 train_time:78220ms step_avg:88.18ms
step:888/1670 train_time:78312ms step_avg:88.19ms
step:889/1670 train_time:78404ms step_avg:88.19ms
step:890/1670 train_time:78494ms step_avg:88.20ms
step:891/1670 train_time:78583ms step_avg:88.20ms
step:892/1670 train_time:78671ms step_avg:88.20ms
step:893/1670 train_time:78760ms step_avg:88.20ms
step:894/1670 train_time:78848ms step_avg:88.20ms
step:895/1670 train_time:78935ms step_avg:88.20ms
step:896/1670 train_time:79024ms step_avg:88.20ms
step:897/1670 train_time:79112ms step_avg:88.20ms
step:898/1670 train_time:79201ms step_avg:88.20ms
step:899/1670 train_time:79292ms step_avg:88.20ms
step:900/1670 train_time:79383ms step_avg:88.20ms
step:901/1670 train_time:79473ms step_avg:88.20ms
step:902/1670 train_time:79562ms step_avg:88.21ms
step:903/1670 train_time:79651ms step_avg:88.21ms
step:904/1670 train_time:79739ms step_avg:88.21ms
step:905/1670 train_time:79829ms step_avg:88.21ms
step:906/1670 train_time:79917ms step_avg:88.21ms
step:907/1670 train_time:80006ms step_avg:88.21ms
step:908/1670 train_time:80094ms step_avg:88.21ms
step:909/1670 train_time:80183ms step_avg:88.21ms
step:910/1670 train_time:80273ms step_avg:88.21ms
step:911/1670 train_time:80363ms step_avg:88.21ms
step:912/1670 train_time:80452ms step_avg:88.21ms
step:913/1670 train_time:80541ms step_avg:88.22ms
step:914/1670 train_time:80630ms step_avg:88.22ms
step:915/1670 train_time:80720ms step_avg:88.22ms
step:916/1670 train_time:80808ms step_avg:88.22ms
step:917/1670 train_time:80896ms step_avg:88.22ms
step:918/1670 train_time:80985ms step_avg:88.22ms
step:919/1670 train_time:81073ms step_avg:88.22ms
step:920/1670 train_time:81162ms step_avg:88.22ms
step:921/1670 train_time:81252ms step_avg:88.22ms
step:922/1670 train_time:81342ms step_avg:88.22ms
step:923/1670 train_time:81431ms step_avg:88.22ms
step:924/1670 train_time:81522ms step_avg:88.23ms
step:925/1670 train_time:81611ms step_avg:88.23ms
step:926/1670 train_time:81700ms step_avg:88.23ms
step:927/1670 train_time:81788ms step_avg:88.23ms
step:928/1670 train_time:81877ms step_avg:88.23ms
step:929/1670 train_time:81965ms step_avg:88.23ms
step:930/1670 train_time:82053ms step_avg:88.23ms
step:931/1670 train_time:82142ms step_avg:88.23ms
step:932/1670 train_time:82232ms step_avg:88.23ms
step:933/1670 train_time:82322ms step_avg:88.23ms
step:934/1670 train_time:82411ms step_avg:88.23ms
step:935/1670 train_time:82500ms step_avg:88.24ms
step:936/1670 train_time:82590ms step_avg:88.24ms
step:937/1670 train_time:82678ms step_avg:88.24ms
step:938/1670 train_time:82768ms step_avg:88.24ms
step:939/1670 train_time:82857ms step_avg:88.24ms
step:940/1670 train_time:82946ms step_avg:88.24ms
step:941/1670 train_time:83034ms step_avg:88.24ms
step:942/1670 train_time:83123ms step_avg:88.24ms
step:943/1670 train_time:83212ms step_avg:88.24ms
step:944/1670 train_time:83301ms step_avg:88.24ms
step:945/1670 train_time:83391ms step_avg:88.24ms
step:946/1670 train_time:83479ms step_avg:88.24ms
step:947/1670 train_time:83570ms step_avg:88.25ms
step:948/1670 train_time:83659ms step_avg:88.25ms
step:949/1670 train_time:83749ms step_avg:88.25ms
step:950/1670 train_time:83838ms step_avg:88.25ms
step:951/1670 train_time:83927ms step_avg:88.25ms
step:952/1670 train_time:84016ms step_avg:88.25ms
step:953/1670 train_time:84105ms step_avg:88.25ms
step:954/1670 train_time:84194ms step_avg:88.25ms
step:955/1670 train_time:84282ms step_avg:88.25ms
step:956/1670 train_time:84371ms step_avg:88.25ms
step:957/1670 train_time:84460ms step_avg:88.26ms
step:958/1670 train_time:84550ms step_avg:88.26ms
step:959/1670 train_time:84639ms step_avg:88.26ms
step:960/1670 train_time:84730ms step_avg:88.26ms
step:961/1670 train_time:84819ms step_avg:88.26ms
step:962/1670 train_time:84908ms step_avg:88.26ms
step:963/1670 train_time:84997ms step_avg:88.26ms
step:964/1670 train_time:85087ms step_avg:88.26ms
step:965/1670 train_time:85175ms step_avg:88.26ms
step:966/1670 train_time:85265ms step_avg:88.27ms
step:967/1670 train_time:85353ms step_avg:88.27ms
step:968/1670 train_time:85443ms step_avg:88.27ms
step:969/1670 train_time:85532ms step_avg:88.27ms
step:970/1670 train_time:85621ms step_avg:88.27ms
step:971/1670 train_time:85710ms step_avg:88.27ms
step:972/1670 train_time:85799ms step_avg:88.27ms
step:973/1670 train_time:85888ms step_avg:88.27ms
step:974/1670 train_time:85976ms step_avg:88.27ms
step:975/1670 train_time:86065ms step_avg:88.27ms
step:976/1670 train_time:86153ms step_avg:88.27ms
step:977/1670 train_time:86242ms step_avg:88.27ms
step:978/1670 train_time:86331ms step_avg:88.27ms
step:979/1670 train_time:86420ms step_avg:88.27ms
step:980/1670 train_time:86508ms step_avg:88.27ms
step:981/1670 train_time:86598ms step_avg:88.27ms
step:982/1670 train_time:86688ms step_avg:88.28ms
step:983/1670 train_time:86777ms step_avg:88.28ms
step:984/1670 train_time:86866ms step_avg:88.28ms
step:985/1670 train_time:86954ms step_avg:88.28ms
step:986/1670 train_time:87044ms step_avg:88.28ms
step:987/1670 train_time:87132ms step_avg:88.28ms
step:988/1670 train_time:87222ms step_avg:88.28ms
step:989/1670 train_time:87310ms step_avg:88.28ms
step:990/1670 train_time:87399ms step_avg:88.28ms
step:991/1670 train_time:87488ms step_avg:88.28ms
step:992/1670 train_time:87576ms step_avg:88.28ms
step:993/1670 train_time:87665ms step_avg:88.28ms
step:994/1670 train_time:87753ms step_avg:88.28ms
step:995/1670 train_time:87842ms step_avg:88.28ms
step:996/1670 train_time:87932ms step_avg:88.28ms
step:997/1670 train_time:88021ms step_avg:88.29ms
step:998/1670 train_time:88111ms step_avg:88.29ms
step:999/1670 train_time:88200ms step_avg:88.29ms
step:1000/1670 train_time:88288ms step_avg:88.29ms
step:1000/1670 val_loss:3.4680 train_time:88378ms step_avg:88.38ms
step:1001/1670 train_time:88399ms step_avg:88.31ms
step:1002/1670 train_time:88471ms step_avg:88.29ms
step:1003/1670 train_time:88569ms step_avg:88.30ms
step:1004/1670 train_time:88659ms step_avg:88.31ms
step:1005/1670 train_time:88747ms step_avg:88.31ms
step:1006/1670 train_time:88835ms step_avg:88.30ms
step:1007/1670 train_time:88923ms step_avg:88.30ms
step:1008/1670 train_time:89011ms step_avg:88.31ms
step:1009/1670 train_time:89099ms step_avg:88.30ms
step:1010/1670 train_time:89187ms step_avg:88.30ms
step:1011/1670 train_time:89275ms step_avg:88.30ms
step:1012/1670 train_time:89364ms step_avg:88.30ms
step:1013/1670 train_time:89455ms step_avg:88.31ms
step:1014/1670 train_time:89549ms step_avg:88.31ms
step:1015/1670 train_time:89638ms step_avg:88.31ms
step:1016/1670 train_time:89728ms step_avg:88.32ms
step:1017/1670 train_time:89816ms step_avg:88.32ms
step:1018/1670 train_time:89906ms step_avg:88.32ms
step:1019/1670 train_time:89993ms step_avg:88.32ms
step:1020/1670 train_time:90082ms step_avg:88.32ms
step:1021/1670 train_time:90170ms step_avg:88.32ms
step:1022/1670 train_time:90258ms step_avg:88.32ms
step:1023/1670 train_time:90348ms step_avg:88.32ms
step:1024/1670 train_time:90438ms step_avg:88.32ms
step:1025/1670 train_time:90529ms step_avg:88.32ms
step:1026/1670 train_time:90619ms step_avg:88.32ms
step:1027/1670 train_time:90711ms step_avg:88.33ms
step:1028/1670 train_time:90800ms step_avg:88.33ms
step:1029/1670 train_time:90889ms step_avg:88.33ms
step:1030/1670 train_time:90978ms step_avg:88.33ms
step:1031/1670 train_time:91067ms step_avg:88.33ms
step:1032/1670 train_time:91154ms step_avg:88.33ms
step:1033/1670 train_time:91243ms step_avg:88.33ms
step:1034/1670 train_time:91331ms step_avg:88.33ms
step:1035/1670 train_time:91421ms step_avg:88.33ms
step:1036/1670 train_time:91512ms step_avg:88.33ms
step:1037/1670 train_time:91602ms step_avg:88.33ms
step:1038/1670 train_time:91692ms step_avg:88.33ms
step:1039/1670 train_time:91781ms step_avg:88.34ms
step:1040/1670 train_time:91871ms step_avg:88.34ms
step:1041/1670 train_time:91960ms step_avg:88.34ms
step:1042/1670 train_time:92048ms step_avg:88.34ms
step:1043/1670 train_time:92136ms step_avg:88.34ms
step:1044/1670 train_time:92224ms step_avg:88.34ms
step:1045/1670 train_time:92313ms step_avg:88.34ms
step:1046/1670 train_time:92402ms step_avg:88.34ms
step:1047/1670 train_time:92492ms step_avg:88.34ms
step:1048/1670 train_time:92582ms step_avg:88.34ms
step:1049/1670 train_time:92672ms step_avg:88.34ms
step:1050/1670 train_time:92763ms step_avg:88.35ms
step:1051/1670 train_time:92851ms step_avg:88.35ms
step:1052/1670 train_time:92940ms step_avg:88.35ms
step:1053/1670 train_time:93029ms step_avg:88.35ms
step:1054/1670 train_time:93118ms step_avg:88.35ms
step:1055/1670 train_time:93207ms step_avg:88.35ms
step:1056/1670 train_time:93295ms step_avg:88.35ms
step:1057/1670 train_time:93385ms step_avg:88.35ms
step:1058/1670 train_time:93473ms step_avg:88.35ms
step:1059/1670 train_time:93564ms step_avg:88.35ms
step:1060/1670 train_time:93653ms step_avg:88.35ms
step:1061/1670 train_time:93743ms step_avg:88.35ms
step:1062/1670 train_time:93832ms step_avg:88.35ms
step:1063/1670 train_time:93920ms step_avg:88.35ms
step:1064/1670 train_time:94009ms step_avg:88.35ms
step:1065/1670 train_time:94098ms step_avg:88.35ms
step:1066/1670 train_time:94187ms step_avg:88.36ms
step:1067/1670 train_time:94275ms step_avg:88.36ms
step:1068/1670 train_time:94365ms step_avg:88.36ms
step:1069/1670 train_time:94454ms step_avg:88.36ms
step:1070/1670 train_time:94543ms step_avg:88.36ms
step:1071/1670 train_time:94632ms step_avg:88.36ms
step:1072/1670 train_time:94722ms step_avg:88.36ms
step:1073/1670 train_time:94811ms step_avg:88.36ms
step:1074/1670 train_time:94900ms step_avg:88.36ms
step:1075/1670 train_time:94988ms step_avg:88.36ms
step:1076/1670 train_time:95077ms step_avg:88.36ms
step:1077/1670 train_time:95167ms step_avg:88.36ms
step:1078/1670 train_time:95255ms step_avg:88.36ms
step:1079/1670 train_time:95344ms step_avg:88.36ms
step:1080/1670 train_time:95433ms step_avg:88.36ms
step:1081/1670 train_time:95522ms step_avg:88.36ms
step:1082/1670 train_time:95611ms step_avg:88.36ms
step:1083/1670 train_time:95701ms step_avg:88.37ms
step:1084/1670 train_time:95790ms step_avg:88.37ms
step:1085/1670 train_time:95879ms step_avg:88.37ms
step:1086/1670 train_time:95968ms step_avg:88.37ms
step:1087/1670 train_time:96056ms step_avg:88.37ms
step:1088/1670 train_time:96145ms step_avg:88.37ms
step:1089/1670 train_time:96234ms step_avg:88.37ms
step:1090/1670 train_time:96324ms step_avg:88.37ms
step:1091/1670 train_time:96413ms step_avg:88.37ms
step:1092/1670 train_time:96503ms step_avg:88.37ms
step:1093/1670 train_time:96592ms step_avg:88.37ms
step:1094/1670 train_time:96682ms step_avg:88.37ms
step:1095/1670 train_time:96773ms step_avg:88.38ms
step:1096/1670 train_time:96863ms step_avg:88.38ms
step:1097/1670 train_time:96953ms step_avg:88.38ms
step:1098/1670 train_time:97043ms step_avg:88.38ms
step:1099/1670 train_time:97132ms step_avg:88.38ms
step:1100/1670 train_time:97222ms step_avg:88.38ms
step:1101/1670 train_time:97312ms step_avg:88.39ms
step:1102/1670 train_time:97403ms step_avg:88.39ms
step:1103/1670 train_time:97492ms step_avg:88.39ms
step:1104/1670 train_time:97582ms step_avg:88.39ms
step:1105/1670 train_time:97672ms step_avg:88.39ms
step:1106/1670 train_time:97763ms step_avg:88.39ms
step:1107/1670 train_time:97852ms step_avg:88.39ms
step:1108/1670 train_time:97942ms step_avg:88.40ms
step:1109/1670 train_time:98031ms step_avg:88.40ms
step:1110/1670 train_time:98121ms step_avg:88.40ms
step:1111/1670 train_time:98211ms step_avg:88.40ms
step:1112/1670 train_time:98300ms step_avg:88.40ms
step:1113/1670 train_time:98391ms step_avg:88.40ms
step:1114/1670 train_time:98480ms step_avg:88.40ms
step:1115/1670 train_time:98571ms step_avg:88.40ms
step:1116/1670 train_time:98660ms step_avg:88.40ms
step:1117/1670 train_time:98750ms step_avg:88.41ms
step:1118/1670 train_time:98839ms step_avg:88.41ms
step:1119/1670 train_time:98929ms step_avg:88.41ms
step:1120/1670 train_time:99019ms step_avg:88.41ms
step:1121/1670 train_time:99108ms step_avg:88.41ms
step:1122/1670 train_time:99198ms step_avg:88.41ms
step:1123/1670 train_time:99288ms step_avg:88.41ms
step:1124/1670 train_time:99378ms step_avg:88.41ms
step:1125/1670 train_time:99470ms step_avg:88.42ms
step:1125/1670 val_loss:3.4143 train_time:99561ms step_avg:88.50ms
step:1126/1670 train_time:99581ms step_avg:88.44ms
step:1127/1670 train_time:99654ms step_avg:88.42ms
step:1128/1670 train_time:99746ms step_avg:88.43ms
step:1129/1670 train_time:99835ms step_avg:88.43ms
step:1130/1670 train_time:99924ms step_avg:88.43ms
step:1131/1670 train_time:100013ms step_avg:88.43ms
step:1132/1670 train_time:100102ms step_avg:88.43ms
step:1133/1670 train_time:100191ms step_avg:88.43ms
step:1134/1670 train_time:100279ms step_avg:88.43ms
step:1135/1670 train_time:100370ms step_avg:88.43ms
step:1136/1670 train_time:100460ms step_avg:88.43ms
step:1137/1670 train_time:100551ms step_avg:88.44ms
step:1138/1670 train_time:100643ms step_avg:88.44ms
step:1139/1670 train_time:100735ms step_avg:88.44ms
step:1140/1670 train_time:100826ms step_avg:88.44ms
step:1141/1670 train_time:100916ms step_avg:88.45ms
step:1142/1670 train_time:101005ms step_avg:88.45ms
step:1143/1670 train_time:101093ms step_avg:88.45ms
step:1144/1670 train_time:101181ms step_avg:88.45ms
step:1145/1670 train_time:101270ms step_avg:88.45ms
step:1146/1670 train_time:101359ms step_avg:88.45ms
step:1147/1670 train_time:101449ms step_avg:88.45ms
step:1148/1670 train_time:101539ms step_avg:88.45ms
step:1149/1670 train_time:101630ms step_avg:88.45ms
step:1150/1670 train_time:101721ms step_avg:88.45ms
step:1151/1670 train_time:101812ms step_avg:88.45ms
step:1152/1670 train_time:101902ms step_avg:88.46ms
step:1153/1670 train_time:101992ms step_avg:88.46ms
step:1154/1670 train_time:102081ms step_avg:88.46ms
step:1155/1670 train_time:102170ms step_avg:88.46ms
step:1156/1670 train_time:102259ms step_avg:88.46ms
step:1157/1670 train_time:102349ms step_avg:88.46ms
step:1158/1670 train_time:102438ms step_avg:88.46ms
step:1159/1670 train_time:102528ms step_avg:88.46ms
step:1160/1670 train_time:102617ms step_avg:88.46ms
step:1161/1670 train_time:102708ms step_avg:88.46ms
step:1162/1670 train_time:102798ms step_avg:88.47ms
step:1163/1670 train_time:102888ms step_avg:88.47ms
step:1164/1670 train_time:102978ms step_avg:88.47ms
step:1165/1670 train_time:103068ms step_avg:88.47ms
step:1166/1670 train_time:103156ms step_avg:88.47ms
step:1167/1670 train_time:103245ms step_avg:88.47ms
step:1168/1670 train_time:103334ms step_avg:88.47ms
step:1169/1670 train_time:103424ms step_avg:88.47ms
step:1170/1670 train_time:103513ms step_avg:88.47ms
step:1171/1670 train_time:103603ms step_avg:88.47ms
step:1172/1670 train_time:103692ms step_avg:88.47ms
step:1173/1670 train_time:103782ms step_avg:88.48ms
step:1174/1670 train_time:103873ms step_avg:88.48ms
step:1175/1670 train_time:103962ms step_avg:88.48ms
step:1176/1670 train_time:104053ms step_avg:88.48ms
step:1177/1670 train_time:104143ms step_avg:88.48ms
step:1178/1670 train_time:104231ms step_avg:88.48ms
step:1179/1670 train_time:104320ms step_avg:88.48ms
step:1180/1670 train_time:104410ms step_avg:88.48ms
step:1181/1670 train_time:104500ms step_avg:88.48ms
step:1182/1670 train_time:104589ms step_avg:88.49ms
step:1183/1670 train_time:104678ms step_avg:88.49ms
step:1184/1670 train_time:104769ms step_avg:88.49ms
step:1185/1670 train_time:104859ms step_avg:88.49ms
step:1186/1670 train_time:104949ms step_avg:88.49ms
step:1187/1670 train_time:105039ms step_avg:88.49ms
step:1188/1670 train_time:105130ms step_avg:88.49ms
step:1189/1670 train_time:105220ms step_avg:88.49ms
step:1190/1670 train_time:105310ms step_avg:88.50ms
step:1191/1670 train_time:105399ms step_avg:88.50ms
step:1192/1670 train_time:105489ms step_avg:88.50ms
step:1193/1670 train_time:105579ms step_avg:88.50ms
step:1194/1670 train_time:105669ms step_avg:88.50ms
step:1195/1670 train_time:105758ms step_avg:88.50ms
step:1196/1670 train_time:105849ms step_avg:88.50ms
step:1197/1670 train_time:105938ms step_avg:88.50ms
step:1198/1670 train_time:106028ms step_avg:88.50ms
step:1199/1670 train_time:106118ms step_avg:88.51ms
step:1200/1670 train_time:106208ms step_avg:88.51ms
step:1201/1670 train_time:106297ms step_avg:88.51ms
step:1202/1670 train_time:106387ms step_avg:88.51ms
step:1203/1670 train_time:106476ms step_avg:88.51ms
step:1204/1670 train_time:106565ms step_avg:88.51ms
step:1205/1670 train_time:106655ms step_avg:88.51ms
step:1206/1670 train_time:106744ms step_avg:88.51ms
step:1207/1670 train_time:106834ms step_avg:88.51ms
step:1208/1670 train_time:106924ms step_avg:88.51ms
step:1209/1670 train_time:107013ms step_avg:88.51ms
step:1210/1670 train_time:107103ms step_avg:88.52ms
step:1211/1670 train_time:107193ms step_avg:88.52ms
step:1212/1670 train_time:107283ms step_avg:88.52ms
step:1213/1670 train_time:107372ms step_avg:88.52ms
step:1214/1670 train_time:107462ms step_avg:88.52ms
step:1215/1670 train_time:107553ms step_avg:88.52ms
step:1216/1670 train_time:107644ms step_avg:88.52ms
step:1217/1670 train_time:107733ms step_avg:88.52ms
step:1218/1670 train_time:107822ms step_avg:88.52ms
step:1219/1670 train_time:107912ms step_avg:88.52ms
step:1220/1670 train_time:108002ms step_avg:88.53ms
step:1221/1670 train_time:108091ms step_avg:88.53ms
step:1222/1670 train_time:108181ms step_avg:88.53ms
step:1223/1670 train_time:108271ms step_avg:88.53ms
step:1224/1670 train_time:108360ms step_avg:88.53ms
step:1225/1670 train_time:108451ms step_avg:88.53ms
step:1226/1670 train_time:108541ms step_avg:88.53ms
step:1227/1670 train_time:108631ms step_avg:88.53ms
step:1228/1670 train_time:108721ms step_avg:88.53ms
step:1229/1670 train_time:108811ms step_avg:88.54ms
step:1230/1670 train_time:108901ms step_avg:88.54ms
step:1231/1670 train_time:108990ms step_avg:88.54ms
step:1232/1670 train_time:109080ms step_avg:88.54ms
step:1233/1670 train_time:109170ms step_avg:88.54ms
step:1234/1670 train_time:109259ms step_avg:88.54ms
step:1235/1670 train_time:109350ms step_avg:88.54ms
step:1236/1670 train_time:109440ms step_avg:88.54ms
step:1237/1670 train_time:109530ms step_avg:88.55ms
step:1238/1670 train_time:109620ms step_avg:88.55ms
step:1239/1670 train_time:109710ms step_avg:88.55ms
step:1240/1670 train_time:109799ms step_avg:88.55ms
step:1241/1670 train_time:109889ms step_avg:88.55ms
step:1242/1670 train_time:109979ms step_avg:88.55ms
step:1243/1670 train_time:110069ms step_avg:88.55ms
step:1244/1670 train_time:110159ms step_avg:88.55ms
step:1245/1670 train_time:110248ms step_avg:88.55ms
step:1246/1670 train_time:110338ms step_avg:88.55ms
step:1247/1670 train_time:110428ms step_avg:88.56ms
step:1248/1670 train_time:110518ms step_avg:88.56ms
step:1249/1670 train_time:110609ms step_avg:88.56ms
step:1250/1670 train_time:110700ms step_avg:88.56ms
step:1250/1670 val_loss:3.3758 train_time:110791ms step_avg:88.63ms
step:1251/1670 train_time:110810ms step_avg:88.58ms
step:1252/1670 train_time:110884ms step_avg:88.57ms
step:1253/1670 train_time:110980ms step_avg:88.57ms
step:1254/1670 train_time:111070ms step_avg:88.57ms
step:1255/1670 train_time:111159ms step_avg:88.57ms
step:1256/1670 train_time:111247ms step_avg:88.57ms
step:1257/1670 train_time:111335ms step_avg:88.57ms
step:1258/1670 train_time:111423ms step_avg:88.57ms
step:1259/1670 train_time:111512ms step_avg:88.57ms
step:1260/1670 train_time:111602ms step_avg:88.57ms
step:1261/1670 train_time:111691ms step_avg:88.57ms
step:1262/1670 train_time:111785ms step_avg:88.58ms
step:1263/1670 train_time:111879ms step_avg:88.58ms
step:1264/1670 train_time:111970ms step_avg:88.58ms
step:1265/1670 train_time:112062ms step_avg:88.59ms
step:1266/1670 train_time:112151ms step_avg:88.59ms
step:1267/1670 train_time:112241ms step_avg:88.59ms
step:1268/1670 train_time:112330ms step_avg:88.59ms
step:1269/1670 train_time:112418ms step_avg:88.59ms
step:1270/1670 train_time:112507ms step_avg:88.59ms
step:1271/1670 train_time:112595ms step_avg:88.59ms
step:1272/1670 train_time:112684ms step_avg:88.59ms
step:1273/1670 train_time:112776ms step_avg:88.59ms
step:1274/1670 train_time:112866ms step_avg:88.59ms
step:1275/1670 train_time:112957ms step_avg:88.59ms
step:1276/1670 train_time:113047ms step_avg:88.59ms
step:1277/1670 train_time:113137ms step_avg:88.60ms
step:1278/1670 train_time:113225ms step_avg:88.60ms
step:1279/1670 train_time:113315ms step_avg:88.60ms
step:1280/1670 train_time:113404ms step_avg:88.60ms
step:1281/1670 train_time:113492ms step_avg:88.60ms
step:1282/1670 train_time:113583ms step_avg:88.60ms
step:1283/1670 train_time:113673ms step_avg:88.60ms
step:1284/1670 train_time:113763ms step_avg:88.60ms
step:1285/1670 train_time:113854ms step_avg:88.60ms
step:1286/1670 train_time:113945ms step_avg:88.60ms
step:1287/1670 train_time:114036ms step_avg:88.61ms
step:1288/1670 train_time:114125ms step_avg:88.61ms
step:1289/1670 train_time:114215ms step_avg:88.61ms
step:1290/1670 train_time:114304ms step_avg:88.61ms
step:1291/1670 train_time:114393ms step_avg:88.61ms
step:1292/1670 train_time:114482ms step_avg:88.61ms
step:1293/1670 train_time:114572ms step_avg:88.61ms
step:1294/1670 train_time:114662ms step_avg:88.61ms
step:1295/1670 train_time:114753ms step_avg:88.61ms
step:1296/1670 train_time:114843ms step_avg:88.61ms
step:1297/1670 train_time:114934ms step_avg:88.62ms
step:1298/1670 train_time:115024ms step_avg:88.62ms
step:1299/1670 train_time:115114ms step_avg:88.62ms
step:1300/1670 train_time:115203ms step_avg:88.62ms
step:1301/1670 train_time:115292ms step_avg:88.62ms
step:1302/1670 train_time:115382ms step_avg:88.62ms
step:1303/1670 train_time:115472ms step_avg:88.62ms
step:1304/1670 train_time:115562ms step_avg:88.62ms
step:1305/1670 train_time:115652ms step_avg:88.62ms
step:1306/1670 train_time:115742ms step_avg:88.62ms
step:1307/1670 train_time:115832ms step_avg:88.62ms
step:1308/1670 train_time:115922ms step_avg:88.63ms
step:1309/1670 train_time:116012ms step_avg:88.63ms
step:1310/1670 train_time:116103ms step_avg:88.63ms
step:1311/1670 train_time:116193ms step_avg:88.63ms
step:1312/1670 train_time:116283ms step_avg:88.63ms
step:1313/1670 train_time:116373ms step_avg:88.63ms
step:1314/1670 train_time:116462ms step_avg:88.63ms
step:1315/1670 train_time:116552ms step_avg:88.63ms
step:1316/1670 train_time:116642ms step_avg:88.63ms
step:1317/1670 train_time:116733ms step_avg:88.64ms
step:1318/1670 train_time:116823ms step_avg:88.64ms
step:1319/1670 train_time:116913ms step_avg:88.64ms
step:1320/1670 train_time:117002ms step_avg:88.64ms
step:1321/1670 train_time:117091ms step_avg:88.64ms
step:1322/1670 train_time:117181ms step_avg:88.64ms
step:1323/1670 train_time:117272ms step_avg:88.64ms
step:1324/1670 train_time:117361ms step_avg:88.64ms
step:1325/1670 train_time:117451ms step_avg:88.64ms
step:1326/1670 train_time:117541ms step_avg:88.64ms
step:1327/1670 train_time:117631ms step_avg:88.64ms
step:1328/1670 train_time:117721ms step_avg:88.65ms
step:1329/1670 train_time:117811ms step_avg:88.65ms
step:1330/1670 train_time:117901ms step_avg:88.65ms
step:1331/1670 train_time:117991ms step_avg:88.65ms
step:1332/1670 train_time:118080ms step_avg:88.65ms
step:1333/1670 train_time:118170ms step_avg:88.65ms
step:1334/1670 train_time:118260ms step_avg:88.65ms
step:1335/1670 train_time:118349ms step_avg:88.65ms
step:1336/1670 train_time:118440ms step_avg:88.65ms
step:1337/1670 train_time:118531ms step_avg:88.65ms
step:1338/1670 train_time:118621ms step_avg:88.66ms
step:1339/1670 train_time:118712ms step_avg:88.66ms
step:1340/1670 train_time:118801ms step_avg:88.66ms
step:1341/1670 train_time:118891ms step_avg:88.66ms
step:1342/1670 train_time:118980ms step_avg:88.66ms
step:1343/1670 train_time:119070ms step_avg:88.66ms
step:1344/1670 train_time:119159ms step_avg:88.66ms
step:1345/1670 train_time:119249ms step_avg:88.66ms
step:1346/1670 train_time:119339ms step_avg:88.66ms
step:1347/1670 train_time:119429ms step_avg:88.66ms
step:1348/1670 train_time:119519ms step_avg:88.66ms
step:1349/1670 train_time:119609ms step_avg:88.66ms
step:1350/1670 train_time:119699ms step_avg:88.67ms
step:1351/1670 train_time:119788ms step_avg:88.67ms
step:1352/1670 train_time:119878ms step_avg:88.67ms
step:1353/1670 train_time:119967ms step_avg:88.67ms
step:1354/1670 train_time:120057ms step_avg:88.67ms
step:1355/1670 train_time:120147ms step_avg:88.67ms
step:1356/1670 train_time:120236ms step_avg:88.67ms
step:1357/1670 train_time:120325ms step_avg:88.67ms
step:1358/1670 train_time:120415ms step_avg:88.67ms
step:1359/1670 train_time:120505ms step_avg:88.67ms
step:1360/1670 train_time:120595ms step_avg:88.67ms
step:1361/1670 train_time:120683ms step_avg:88.67ms
step:1362/1670 train_time:120774ms step_avg:88.67ms
step:1363/1670 train_time:120864ms step_avg:88.67ms
step:1364/1670 train_time:120954ms step_avg:88.68ms
step:1365/1670 train_time:121043ms step_avg:88.68ms
step:1366/1670 train_time:121133ms step_avg:88.68ms
step:1367/1670 train_time:121223ms step_avg:88.68ms
step:1368/1670 train_time:121312ms step_avg:88.68ms
step:1369/1670 train_time:121402ms step_avg:88.68ms
step:1370/1670 train_time:121492ms step_avg:88.68ms
step:1371/1670 train_time:121582ms step_avg:88.68ms
step:1372/1670 train_time:121672ms step_avg:88.68ms
step:1373/1670 train_time:121762ms step_avg:88.68ms
step:1374/1670 train_time:121853ms step_avg:88.68ms
step:1375/1670 train_time:121943ms step_avg:88.69ms
step:1375/1670 val_loss:3.3410 train_time:122035ms step_avg:88.75ms
step:1376/1670 train_time:122054ms step_avg:88.70ms
step:1377/1670 train_time:122129ms step_avg:88.69ms
step:1378/1670 train_time:122223ms step_avg:88.70ms
step:1379/1670 train_time:122312ms step_avg:88.70ms
step:1380/1670 train_time:122401ms step_avg:88.70ms
step:1381/1670 train_time:122489ms step_avg:88.70ms
step:1382/1670 train_time:122578ms step_avg:88.70ms
step:1383/1670 train_time:122667ms step_avg:88.70ms
step:1384/1670 train_time:122755ms step_avg:88.70ms
step:1385/1670 train_time:122845ms step_avg:88.70ms
step:1386/1670 train_time:122934ms step_avg:88.70ms
step:1387/1670 train_time:123026ms step_avg:88.70ms
step:1388/1670 train_time:123118ms step_avg:88.70ms
step:1389/1670 train_time:123212ms step_avg:88.71ms
step:1390/1670 train_time:123304ms step_avg:88.71ms
step:1391/1670 train_time:123392ms step_avg:88.71ms
step:1392/1670 train_time:123482ms step_avg:88.71ms
step:1393/1670 train_time:123571ms step_avg:88.71ms
step:1394/1670 train_time:123661ms step_avg:88.71ms
step:1395/1670 train_time:123750ms step_avg:88.71ms
step:1396/1670 train_time:123839ms step_avg:88.71ms
step:1397/1670 train_time:123927ms step_avg:88.71ms
step:1398/1670 train_time:124016ms step_avg:88.71ms
step:1399/1670 train_time:124108ms step_avg:88.71ms
step:1400/1670 train_time:124200ms step_avg:88.71ms
step:1401/1670 train_time:124290ms step_avg:88.72ms
step:1402/1670 train_time:124379ms step_avg:88.72ms
step:1403/1670 train_time:124469ms step_avg:88.72ms
step:1404/1670 train_time:124559ms step_avg:88.72ms
step:1405/1670 train_time:124647ms step_avg:88.72ms
step:1406/1670 train_time:124736ms step_avg:88.72ms
step:1407/1670 train_time:124826ms step_avg:88.72ms
step:1408/1670 train_time:124915ms step_avg:88.72ms
step:1409/1670 train_time:125006ms step_avg:88.72ms
step:1410/1670 train_time:125096ms step_avg:88.72ms
step:1411/1670 train_time:125187ms step_avg:88.72ms
step:1412/1670 train_time:125278ms step_avg:88.72ms
step:1413/1670 train_time:125369ms step_avg:88.73ms
step:1414/1670 train_time:125459ms step_avg:88.73ms
step:1415/1670 train_time:125548ms step_avg:88.73ms
step:1416/1670 train_time:125638ms step_avg:88.73ms
step:1417/1670 train_time:125727ms step_avg:88.73ms
step:1418/1670 train_time:125817ms step_avg:88.73ms
step:1419/1670 train_time:125908ms step_avg:88.73ms
step:1420/1670 train_time:125997ms step_avg:88.73ms
step:1421/1670 train_time:126089ms step_avg:88.73ms
step:1422/1670 train_time:126179ms step_avg:88.73ms
step:1423/1670 train_time:126271ms step_avg:88.74ms
step:1424/1670 train_time:126361ms step_avg:88.74ms
step:1425/1670 train_time:126450ms step_avg:88.74ms
step:1426/1670 train_time:126540ms step_avg:88.74ms
step:1427/1670 train_time:126629ms step_avg:88.74ms
step:1428/1670 train_time:126719ms step_avg:88.74ms
step:1429/1670 train_time:126810ms step_avg:88.74ms
step:1430/1670 train_time:126900ms step_avg:88.74ms
step:1431/1670 train_time:126990ms step_avg:88.74ms
step:1432/1670 train_time:127080ms step_avg:88.74ms
step:1433/1670 train_time:127171ms step_avg:88.74ms
step:1434/1670 train_time:127262ms step_avg:88.75ms
step:1435/1670 train_time:127352ms step_avg:88.75ms
step:1436/1670 train_time:127442ms step_avg:88.75ms
step:1437/1670 train_time:127530ms step_avg:88.75ms
step:1438/1670 train_time:127619ms step_avg:88.75ms
step:1439/1670 train_time:127708ms step_avg:88.75ms
step:1440/1670 train_time:127798ms step_avg:88.75ms
step:1441/1670 train_time:127888ms step_avg:88.75ms
step:1442/1670 train_time:127979ms step_avg:88.75ms
step:1443/1670 train_time:128069ms step_avg:88.75ms
step:1444/1670 train_time:128159ms step_avg:88.75ms
step:1445/1670 train_time:128250ms step_avg:88.75ms
step:1446/1670 train_time:128341ms step_avg:88.76ms
step:1447/1670 train_time:128430ms step_avg:88.76ms
step:1448/1670 train_time:128520ms step_avg:88.76ms
step:1449/1670 train_time:128610ms step_avg:88.76ms
step:1450/1670 train_time:128700ms step_avg:88.76ms
step:1451/1670 train_time:128789ms step_avg:88.76ms
step:1452/1670 train_time:128879ms step_avg:88.76ms
step:1453/1670 train_time:128969ms step_avg:88.76ms
step:1454/1670 train_time:129059ms step_avg:88.76ms
step:1455/1670 train_time:129149ms step_avg:88.76ms
step:1456/1670 train_time:129240ms step_avg:88.76ms
step:1457/1670 train_time:129330ms step_avg:88.76ms
step:1458/1670 train_time:129419ms step_avg:88.76ms
step:1459/1670 train_time:129509ms step_avg:88.77ms
step:1460/1670 train_time:129600ms step_avg:88.77ms
step:1461/1670 train_time:129689ms step_avg:88.77ms
step:1462/1670 train_time:129780ms step_avg:88.77ms
step:1463/1670 train_time:129870ms step_avg:88.77ms
step:1464/1670 train_time:129961ms step_avg:88.77ms
step:1465/1670 train_time:130050ms step_avg:88.77ms
step:1466/1670 train_time:130139ms step_avg:88.77ms
step:1467/1670 train_time:130229ms step_avg:88.77ms
step:1468/1670 train_time:130319ms step_avg:88.77ms
step:1469/1670 train_time:130409ms step_avg:88.77ms
step:1470/1670 train_time:130499ms step_avg:88.77ms
step:1471/1670 train_time:130589ms step_avg:88.78ms
step:1472/1670 train_time:130678ms step_avg:88.78ms
step:1473/1670 train_time:130769ms step_avg:88.78ms
step:1474/1670 train_time:130860ms step_avg:88.78ms
step:1475/1670 train_time:130950ms step_avg:88.78ms
step:1476/1670 train_time:131039ms step_avg:88.78ms
step:1477/1670 train_time:131129ms step_avg:88.78ms
step:1478/1670 train_time:131218ms step_avg:88.78ms
step:1479/1670 train_time:131308ms step_avg:88.78ms
step:1480/1670 train_time:131399ms step_avg:88.78ms
step:1481/1670 train_time:131488ms step_avg:88.78ms
step:1482/1670 train_time:131578ms step_avg:88.78ms
step:1483/1670 train_time:131669ms step_avg:88.79ms
step:1484/1670 train_time:131759ms step_avg:88.79ms
step:1485/1670 train_time:131849ms step_avg:88.79ms
step:1486/1670 train_time:131939ms step_avg:88.79ms
step:1487/1670 train_time:132029ms step_avg:88.79ms
step:1488/1670 train_time:132119ms step_avg:88.79ms
step:1489/1670 train_time:132210ms step_avg:88.79ms
step:1490/1670 train_time:132300ms step_avg:88.79ms
step:1491/1670 train_time:132390ms step_avg:88.79ms
step:1492/1670 train_time:132479ms step_avg:88.79ms
step:1493/1670 train_time:132569ms step_avg:88.79ms
step:1494/1670 train_time:132659ms step_avg:88.79ms
step:1495/1670 train_time:132749ms step_avg:88.80ms
step:1496/1670 train_time:132839ms step_avg:88.80ms
step:1497/1670 train_time:132928ms step_avg:88.80ms
step:1498/1670 train_time:133018ms step_avg:88.80ms
step:1499/1670 train_time:133107ms step_avg:88.80ms
step:1500/1670 train_time:133197ms step_avg:88.80ms
step:1500/1670 val_loss:3.3114 train_time:133288ms step_avg:88.86ms
step:1501/1670 train_time:133307ms step_avg:88.81ms
step:1502/1670 train_time:133381ms step_avg:88.80ms
step:1503/1670 train_time:133473ms step_avg:88.80ms
step:1504/1670 train_time:133563ms step_avg:88.80ms
step:1505/1670 train_time:133651ms step_avg:88.80ms
step:1506/1670 train_time:133739ms step_avg:88.80ms
step:1507/1670 train_time:133827ms step_avg:88.80ms
step:1508/1670 train_time:133916ms step_avg:88.80ms
step:1509/1670 train_time:134005ms step_avg:88.80ms
step:1510/1670 train_time:134096ms step_avg:88.81ms
step:1511/1670 train_time:134186ms step_avg:88.81ms
step:1512/1670 train_time:134279ms step_avg:88.81ms
step:1513/1670 train_time:134372ms step_avg:88.81ms
step:1514/1670 train_time:134463ms step_avg:88.81ms
step:1515/1670 train_time:134554ms step_avg:88.81ms
step:1516/1670 train_time:134643ms step_avg:88.81ms
step:1517/1670 train_time:134732ms step_avg:88.82ms
step:1518/1670 train_time:134821ms step_avg:88.81ms
step:1519/1670 train_time:134909ms step_avg:88.81ms
step:1520/1670 train_time:134998ms step_avg:88.81ms
step:1521/1670 train_time:135087ms step_avg:88.81ms
step:1522/1670 train_time:135177ms step_avg:88.82ms
step:1523/1670 train_time:135268ms step_avg:88.82ms
step:1524/1670 train_time:135358ms step_avg:88.82ms
step:1525/1670 train_time:135449ms step_avg:88.82ms
step:1526/1670 train_time:135539ms step_avg:88.82ms
step:1527/1670 train_time:135629ms step_avg:88.82ms
step:1528/1670 train_time:135718ms step_avg:88.82ms
step:1529/1670 train_time:135807ms step_avg:88.82ms
step:1530/1670 train_time:135896ms step_avg:88.82ms
step:1531/1670 train_time:135986ms step_avg:88.82ms
step:1532/1670 train_time:136076ms step_avg:88.82ms
step:1533/1670 train_time:136166ms step_avg:88.82ms
step:1534/1670 train_time:136256ms step_avg:88.82ms
step:1535/1670 train_time:136346ms step_avg:88.82ms
step:1536/1670 train_time:136438ms step_avg:88.83ms
step:1537/1670 train_time:136528ms step_avg:88.83ms
step:1538/1670 train_time:136618ms step_avg:88.83ms
step:1539/1670 train_time:136708ms step_avg:88.83ms
step:1540/1670 train_time:136797ms step_avg:88.83ms
step:1541/1670 train_time:136886ms step_avg:88.83ms
step:1542/1670 train_time:136976ms step_avg:88.83ms
step:1543/1670 train_time:137065ms step_avg:88.83ms
step:1544/1670 train_time:137155ms step_avg:88.83ms
step:1545/1670 train_time:137245ms step_avg:88.83ms
step:1546/1670 train_time:137337ms step_avg:88.83ms
step:1547/1670 train_time:137428ms step_avg:88.83ms
step:1548/1670 train_time:137518ms step_avg:88.84ms
step:1549/1670 train_time:137608ms step_avg:88.84ms
step:1550/1670 train_time:137698ms step_avg:88.84ms
step:1551/1670 train_time:137787ms step_avg:88.84ms
step:1552/1670 train_time:137876ms step_avg:88.84ms
step:1553/1670 train_time:137965ms step_avg:88.84ms
step:1554/1670 train_time:138055ms step_avg:88.84ms
step:1555/1670 train_time:138144ms step_avg:88.84ms
step:1556/1670 train_time:138234ms step_avg:88.84ms
step:1557/1670 train_time:138325ms step_avg:88.84ms
step:1558/1670 train_time:138416ms step_avg:88.84ms
step:1559/1670 train_time:138506ms step_avg:88.84ms
step:1560/1670 train_time:138597ms step_avg:88.84ms
step:1561/1670 train_time:138687ms step_avg:88.84ms
step:1562/1670 train_time:138777ms step_avg:88.85ms
step:1563/1670 train_time:138866ms step_avg:88.85ms
step:1564/1670 train_time:138954ms step_avg:88.85ms
step:1565/1670 train_time:139044ms step_avg:88.85ms
step:1566/1670 train_time:139134ms step_avg:88.85ms
step:1567/1670 train_time:139223ms step_avg:88.85ms
step:1568/1670 train_time:139314ms step_avg:88.85ms
step:1569/1670 train_time:139405ms step_avg:88.85ms
step:1570/1670 train_time:139497ms step_avg:88.85ms
step:1571/1670 train_time:139588ms step_avg:88.85ms
step:1572/1670 train_time:139678ms step_avg:88.85ms
step:1573/1670 train_time:139769ms step_avg:88.85ms
step:1574/1670 train_time:139858ms step_avg:88.86ms
step:1575/1670 train_time:139947ms step_avg:88.86ms
step:1576/1670 train_time:140036ms step_avg:88.86ms
step:1577/1670 train_time:140126ms step_avg:88.86ms
step:1578/1670 train_time:140215ms step_avg:88.86ms
step:1579/1670 train_time:140305ms step_avg:88.86ms
step:1580/1670 train_time:140396ms step_avg:88.86ms
step:1581/1670 train_time:140487ms step_avg:88.86ms
step:1582/1670 train_time:140578ms step_avg:88.86ms
step:1583/1670 train_time:140668ms step_avg:88.86ms
step:1584/1670 train_time:140757ms step_avg:88.86ms
step:1585/1670 train_time:140847ms step_avg:88.86ms
step:1586/1670 train_time:140937ms step_avg:88.86ms
step:1587/1670 train_time:141027ms step_avg:88.86ms
step:1588/1670 train_time:141117ms step_avg:88.86ms
step:1589/1670 train_time:141207ms step_avg:88.87ms
step:1590/1670 train_time:141298ms step_avg:88.87ms
step:1591/1670 train_time:141388ms step_avg:88.87ms
step:1592/1670 train_time:141477ms step_avg:88.87ms
step:1593/1670 train_time:141567ms step_avg:88.87ms
step:1594/1670 train_time:141657ms step_avg:88.87ms
step:1595/1670 train_time:141746ms step_avg:88.87ms
step:1596/1670 train_time:141835ms step_avg:88.87ms
step:1597/1670 train_time:141926ms step_avg:88.87ms
step:1598/1670 train_time:142015ms step_avg:88.87ms
step:1599/1670 train_time:142105ms step_avg:88.87ms
step:1600/1670 train_time:142195ms step_avg:88.87ms
step:1601/1670 train_time:142285ms step_avg:88.87ms
step:1602/1670 train_time:142375ms step_avg:88.87ms
step:1603/1670 train_time:142465ms step_avg:88.87ms
step:1604/1670 train_time:142555ms step_avg:88.87ms
step:1605/1670 train_time:142646ms step_avg:88.88ms
step:1606/1670 train_time:142735ms step_avg:88.88ms
step:1607/1670 train_time:142826ms step_avg:88.88ms
step:1608/1670 train_time:142916ms step_avg:88.88ms
step:1609/1670 train_time:143007ms step_avg:88.88ms
step:1610/1670 train_time:143096ms step_avg:88.88ms
step:1611/1670 train_time:143186ms step_avg:88.88ms
step:1612/1670 train_time:143276ms step_avg:88.88ms
step:1613/1670 train_time:143366ms step_avg:88.88ms
step:1614/1670 train_time:143456ms step_avg:88.88ms
step:1615/1670 train_time:143546ms step_avg:88.88ms
step:1616/1670 train_time:143637ms step_avg:88.88ms
step:1617/1670 train_time:143727ms step_avg:88.88ms
step:1618/1670 train_time:143816ms step_avg:88.89ms
step:1619/1670 train_time:143907ms step_avg:88.89ms
step:1620/1670 train_time:143997ms step_avg:88.89ms
step:1621/1670 train_time:144087ms step_avg:88.89ms
step:1622/1670 train_time:144177ms step_avg:88.89ms
step:1623/1670 train_time:144267ms step_avg:88.89ms
step:1624/1670 train_time:144357ms step_avg:88.89ms
step:1625/1670 train_time:144448ms step_avg:88.89ms
step:1625/1670 val_loss:3.2884 train_time:144538ms step_avg:88.95ms
step:1626/1670 train_time:144559ms step_avg:88.90ms
step:1627/1670 train_time:144631ms step_avg:88.89ms
step:1628/1670 train_time:144725ms step_avg:88.90ms
step:1629/1670 train_time:144816ms step_avg:88.90ms
step:1630/1670 train_time:144904ms step_avg:88.90ms
step:1631/1670 train_time:144993ms step_avg:88.90ms
step:1632/1670 train_time:145082ms step_avg:88.90ms
step:1633/1670 train_time:145170ms step_avg:88.90ms
step:1634/1670 train_time:145259ms step_avg:88.90ms
step:1635/1670 train_time:145347ms step_avg:88.90ms
step:1636/1670 train_time:145436ms step_avg:88.90ms
step:1637/1670 train_time:145528ms step_avg:88.90ms
step:1638/1670 train_time:145621ms step_avg:88.90ms
step:1639/1670 train_time:145712ms step_avg:88.90ms
step:1640/1670 train_time:145803ms step_avg:88.90ms
step:1641/1670 train_time:145893ms step_avg:88.90ms
step:1642/1670 train_time:145982ms step_avg:88.90ms
step:1643/1670 train_time:146071ms step_avg:88.90ms
step:1644/1670 train_time:146160ms step_avg:88.91ms
step:1645/1670 train_time:146249ms step_avg:88.91ms
step:1646/1670 train_time:146339ms step_avg:88.91ms
step:1647/1670 train_time:146428ms step_avg:88.91ms
step:1648/1670 train_time:146520ms step_avg:88.91ms
step:1649/1670 train_time:146611ms step_avg:88.91ms
step:1650/1670 train_time:146701ms step_avg:88.91ms
step:1651/1670 train_time:146791ms step_avg:88.91ms
step:1652/1670 train_time:146881ms step_avg:88.91ms
step:1653/1670 train_time:146970ms step_avg:88.91ms
step:1654/1670 train_time:147060ms step_avg:88.91ms
step:1655/1670 train_time:147149ms step_avg:88.91ms
step:1656/1670 train_time:147239ms step_avg:88.91ms
step:1657/1670 train_time:147327ms step_avg:88.91ms
step:1658/1670 train_time:147418ms step_avg:88.91ms
step:1659/1670 train_time:147507ms step_avg:88.91ms
step:1660/1670 train_time:147597ms step_avg:88.91ms
step:1661/1670 train_time:147687ms step_avg:88.91ms
step:1662/1670 train_time:147778ms step_avg:88.92ms
step:1663/1670 train_time:147868ms step_avg:88.92ms
step:1664/1670 train_time:147958ms step_avg:88.92ms
step:1665/1670 train_time:148047ms step_avg:88.92ms
step:1666/1670 train_time:148137ms step_avg:88.92ms
step:1667/1670 train_time:148226ms step_avg:88.92ms
step:1668/1670 train_time:148316ms step_avg:88.92ms
step:1669/1670 train_time:148405ms step_avg:88.92ms
step:1670/1670 train_time:148496ms step_avg:88.92ms
step:1670/1670 val_loss:3.2792 train_time:148587ms step_avg:88.97ms
peak memory allocated: 30511 MiB reserved: 45934 MiB

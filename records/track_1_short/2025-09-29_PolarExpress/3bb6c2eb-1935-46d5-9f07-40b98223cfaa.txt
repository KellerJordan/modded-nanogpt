import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)


@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)


@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[
    Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)


@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)


def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None


def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)


mm_op.register_autograd(backward, setup_context=setup_context)


# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
        pid,
        M,
        BLOCK_SIZE_M: tl.constexpr,
        BLOCK_SIZE_N: tl.constexpr,
        GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
coeffs_list = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in coeffs_list:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """

    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size() == 8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1,  # 1 param
            'attn_gate': 2,  # 10 params
            'attn': 3,  # 10 params
            'mlp': 4,  # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx + size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                    (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                    group["lr"]
                    * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                    * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                    group["lr"]
                    * group["weight_decay"]
                    * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[module_idx], 'module', 'none') == 'attn':
                for p in params[module_idx:module_idx + chunk_size]:
                    assert getattr(params[module_idx], 'module', 'none') == 'attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8,
                 weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))


class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5)  # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim // 4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim // 4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int = 1, beta: int = 32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1


def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)


@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim * 4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module = 'attn'
        with torch.no_grad():
            self.qkvo_w.view(4, self.hdim, self.dim)[:3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w.view(4, self.hdim, self.dim)[3].zero_()  # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1)  # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T,
                                                                                                               3 * self.num_heads,
                                                                                                               self.head_dim).chunk(
            3, dim=-2)
        q, k = norm(q), norm(k)  # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v)  # @ KoszarskyB & @Grad62304977
        else:  # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len,
                                   max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)  # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module = 'mlp'
        self.c_proj.module = 'mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_()  # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(
            x).square()  # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x


# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)


class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int,
                 max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim ** 0.5) / 448, w_s=2 ** -9,
                                    grad_s=1 / 448)
        self.lm_head.weight.detach().zero_()  # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers),  # extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm,
                    long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1, len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i < 11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss


# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32)  # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2])  # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True)  # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy())  # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens


BOS_ID = 50256


class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens = tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter == 5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter += 1
        return starts, ends


class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data


def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1,
                               align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens

        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin"  # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin"  # input .bin to eval validation loss on
    val_tokens: int = 10485760  # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1630  # number of iterations to run
    iteration_extension = 40  # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5  # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13  # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20  # extend long windows out even further


args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0)  # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)


def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)


# begin by printing this file (the Python code)
print0(code)
print0("=" * 100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")


def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout


print0(nvidia_smi())
print0("=" * 100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if
                        p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]


# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr


def get_ws(step: int):
    if step == args.num_iterations + args.iteration_extension:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers])  # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len,
                                          grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[
        step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long < ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long // 2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len,
                                          grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1,
                                                grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(
            f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms",
            console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(),
                       optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1)  # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(
        f"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms",
        console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Mon Sep 29 06:43:39 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   40C    P0            129W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   33C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1670 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1670 train_time:158ms step_avg:157.66ms
step:2/1670 train_time:179ms step_avg:89.40ms
step:3/1670 train_time:242ms step_avg:80.66ms
step:4/1670 train_time:327ms step_avg:81.86ms
step:5/1670 train_time:414ms step_avg:82.75ms
step:6/1670 train_time:500ms step_avg:83.40ms
step:7/1670 train_time:587ms step_avg:83.88ms
step:8/1670 train_time:674ms step_avg:84.23ms
step:9/1670 train_time:761ms step_avg:84.53ms
step:10/1670 train_time:847ms step_avg:84.73ms
step:11/1670 train_time:934ms step_avg:84.90ms
step:12/1670 train_time:1025ms step_avg:85.40ms
step:13/1670 train_time:1116ms step_avg:85.85ms
step:14/1670 train_time:1209ms step_avg:86.37ms
step:15/1670 train_time:1297ms step_avg:86.48ms
step:16/1670 train_time:1384ms step_avg:86.52ms
step:17/1670 train_time:1472ms step_avg:86.59ms
step:18/1670 train_time:1559ms step_avg:86.60ms
step:19/1670 train_time:1646ms step_avg:86.65ms
step:20/1670 train_time:1734ms step_avg:86.68ms
step:21/1670 train_time:1821ms step_avg:86.69ms
step:22/1670 train_time:1909ms step_avg:86.76ms
step:23/1670 train_time:1997ms step_avg:86.81ms
step:24/1670 train_time:2086ms step_avg:86.92ms
step:25/1670 train_time:2177ms step_avg:87.07ms
step:26/1670 train_time:2267ms step_avg:87.19ms
step:27/1670 train_time:2355ms step_avg:87.22ms
step:28/1670 train_time:2442ms step_avg:87.22ms
step:29/1670 train_time:2530ms step_avg:87.23ms
step:30/1670 train_time:2617ms step_avg:87.22ms
step:31/1670 train_time:2704ms step_avg:87.23ms
step:32/1670 train_time:2791ms step_avg:87.22ms
step:33/1670 train_time:2878ms step_avg:87.22ms
step:34/1670 train_time:2966ms step_avg:87.23ms
step:35/1670 train_time:3054ms step_avg:87.26ms
step:36/1670 train_time:3144ms step_avg:87.32ms
step:37/1670 train_time:3232ms step_avg:87.36ms
step:38/1670 train_time:3321ms step_avg:87.41ms
step:39/1670 train_time:3409ms step_avg:87.42ms
step:40/1670 train_time:3497ms step_avg:87.42ms
step:41/1670 train_time:3585ms step_avg:87.44ms
step:42/1670 train_time:3673ms step_avg:87.45ms
step:43/1670 train_time:3760ms step_avg:87.44ms
step:44/1670 train_time:3848ms step_avg:87.44ms
step:45/1670 train_time:3935ms step_avg:87.44ms
step:46/1670 train_time:4022ms step_avg:87.44ms
step:47/1670 train_time:4110ms step_avg:87.46ms
step:48/1670 train_time:4199ms step_avg:87.48ms
step:49/1670 train_time:4288ms step_avg:87.50ms
step:50/1670 train_time:4376ms step_avg:87.52ms
step:51/1670 train_time:4463ms step_avg:87.52ms
step:52/1670 train_time:4551ms step_avg:87.53ms
step:53/1670 train_time:4639ms step_avg:87.52ms
step:54/1670 train_time:4726ms step_avg:87.52ms
step:55/1670 train_time:4813ms step_avg:87.51ms
step:56/1670 train_time:4901ms step_avg:87.52ms
step:57/1670 train_time:4989ms step_avg:87.52ms
step:58/1670 train_time:5076ms step_avg:87.52ms
step:59/1670 train_time:5164ms step_avg:87.53ms
step:60/1670 train_time:5252ms step_avg:87.53ms
step:61/1670 train_time:5340ms step_avg:87.54ms
step:62/1670 train_time:5428ms step_avg:87.55ms
step:63/1670 train_time:5516ms step_avg:87.56ms
step:64/1670 train_time:5603ms step_avg:87.55ms
step:65/1670 train_time:5691ms step_avg:87.55ms
step:66/1670 train_time:5778ms step_avg:87.54ms
step:67/1670 train_time:5866ms step_avg:87.55ms
step:68/1670 train_time:5953ms step_avg:87.54ms
step:69/1670 train_time:6041ms step_avg:87.55ms
step:70/1670 train_time:6130ms step_avg:87.57ms
step:71/1670 train_time:6217ms step_avg:87.57ms
step:72/1670 train_time:6305ms step_avg:87.58ms
step:73/1670 train_time:6393ms step_avg:87.58ms
step:74/1670 train_time:6481ms step_avg:87.58ms
step:75/1670 train_time:6569ms step_avg:87.58ms
step:76/1670 train_time:6656ms step_avg:87.58ms
step:77/1670 train_time:6744ms step_avg:87.58ms
step:78/1670 train_time:6832ms step_avg:87.59ms
step:79/1670 train_time:6920ms step_avg:87.59ms
step:80/1670 train_time:7009ms step_avg:87.61ms
step:81/1670 train_time:7096ms step_avg:87.61ms
step:82/1670 train_time:7184ms step_avg:87.61ms
step:83/1670 train_time:7272ms step_avg:87.62ms
step:84/1670 train_time:7360ms step_avg:87.62ms
step:85/1670 train_time:7448ms step_avg:87.62ms
step:86/1670 train_time:7535ms step_avg:87.61ms
step:87/1670 train_time:7622ms step_avg:87.61ms
step:88/1670 train_time:7710ms step_avg:87.62ms
step:89/1670 train_time:7798ms step_avg:87.62ms
step:90/1670 train_time:7886ms step_avg:87.62ms
step:91/1670 train_time:7973ms step_avg:87.62ms
step:92/1670 train_time:8061ms step_avg:87.62ms
step:93/1670 train_time:8150ms step_avg:87.63ms
step:94/1670 train_time:8238ms step_avg:87.63ms
step:95/1670 train_time:8326ms step_avg:87.64ms
step:96/1670 train_time:8414ms step_avg:87.64ms
step:97/1670 train_time:8502ms step_avg:87.65ms
step:98/1670 train_time:8591ms step_avg:87.66ms
step:99/1670 train_time:8678ms step_avg:87.66ms
step:100/1670 train_time:8765ms step_avg:87.65ms
step:101/1670 train_time:8853ms step_avg:87.65ms
step:102/1670 train_time:8941ms step_avg:87.65ms
step:103/1670 train_time:9029ms step_avg:87.66ms
step:104/1670 train_time:9117ms step_avg:87.66ms
step:105/1670 train_time:9205ms step_avg:87.67ms
step:106/1670 train_time:9292ms step_avg:87.66ms
step:107/1670 train_time:9381ms step_avg:87.67ms
step:108/1670 train_time:9469ms step_avg:87.67ms
step:109/1670 train_time:9556ms step_avg:87.67ms
step:110/1670 train_time:9644ms step_avg:87.67ms
step:111/1670 train_time:9731ms step_avg:87.67ms
step:112/1670 train_time:9819ms step_avg:87.67ms
step:113/1670 train_time:9907ms step_avg:87.68ms
step:114/1670 train_time:9994ms step_avg:87.67ms
step:115/1670 train_time:10082ms step_avg:87.67ms
step:116/1670 train_time:10169ms step_avg:87.67ms
step:117/1670 train_time:10257ms step_avg:87.67ms
step:118/1670 train_time:10345ms step_avg:87.67ms
step:119/1670 train_time:10433ms step_avg:87.67ms
step:120/1670 train_time:10522ms step_avg:87.68ms
step:121/1670 train_time:10609ms step_avg:87.68ms
step:122/1670 train_time:10696ms step_avg:87.68ms
step:123/1670 train_time:10785ms step_avg:87.68ms
step:124/1670 train_time:10872ms step_avg:87.68ms
step:125/1670 train_time:10960ms step_avg:87.68ms
step:125/1670 val_loss:4.3301 train_time:11050ms step_avg:88.40ms
step:126/1670 train_time:11070ms step_avg:87.86ms
step:127/1670 train_time:11142ms step_avg:87.74ms
step:128/1670 train_time:11238ms step_avg:87.79ms
step:129/1670 train_time:11327ms step_avg:87.81ms
step:130/1670 train_time:11414ms step_avg:87.80ms
step:131/1670 train_time:11501ms step_avg:87.79ms
step:132/1670 train_time:11587ms step_avg:87.78ms
step:133/1670 train_time:11673ms step_avg:87.77ms
step:134/1670 train_time:11760ms step_avg:87.76ms
step:135/1670 train_time:11846ms step_avg:87.75ms
step:136/1670 train_time:11933ms step_avg:87.74ms
step:137/1670 train_time:12020ms step_avg:87.74ms
step:138/1670 train_time:12109ms step_avg:87.75ms
step:139/1670 train_time:12201ms step_avg:87.78ms
step:140/1670 train_time:12291ms step_avg:87.79ms
step:141/1670 train_time:12381ms step_avg:87.81ms
step:142/1670 train_time:12468ms step_avg:87.80ms
step:143/1670 train_time:12555ms step_avg:87.80ms
step:144/1670 train_time:12642ms step_avg:87.79ms
step:145/1670 train_time:12729ms step_avg:87.79ms
step:146/1670 train_time:12815ms step_avg:87.78ms
step:147/1670 train_time:12903ms step_avg:87.77ms
step:148/1670 train_time:12989ms step_avg:87.76ms
step:149/1670 train_time:13077ms step_avg:87.77ms
step:150/1670 train_time:13166ms step_avg:87.77ms
step:151/1670 train_time:13255ms step_avg:87.78ms
step:152/1670 train_time:13343ms step_avg:87.79ms
step:153/1670 train_time:13431ms step_avg:87.79ms
step:154/1670 train_time:13520ms step_avg:87.79ms
step:155/1670 train_time:13607ms step_avg:87.78ms
step:156/1670 train_time:13694ms step_avg:87.78ms
step:157/1670 train_time:13782ms step_avg:87.78ms
step:158/1670 train_time:13868ms step_avg:87.78ms
step:159/1670 train_time:13956ms step_avg:87.78ms
step:160/1670 train_time:14044ms step_avg:87.77ms
step:161/1670 train_time:14132ms step_avg:87.78ms
step:162/1670 train_time:14221ms step_avg:87.78ms
step:163/1670 train_time:14309ms step_avg:87.78ms
step:164/1670 train_time:14397ms step_avg:87.79ms
step:165/1670 train_time:14485ms step_avg:87.79ms
step:166/1670 train_time:14573ms step_avg:87.79ms
step:167/1670 train_time:14661ms step_avg:87.79ms
step:168/1670 train_time:14748ms step_avg:87.79ms
step:169/1670 train_time:14836ms step_avg:87.78ms
step:170/1670 train_time:14922ms step_avg:87.78ms
step:171/1670 train_time:15010ms step_avg:87.78ms
step:172/1670 train_time:15098ms step_avg:87.78ms
step:173/1670 train_time:15186ms step_avg:87.78ms
step:174/1670 train_time:15275ms step_avg:87.79ms
step:175/1670 train_time:15364ms step_avg:87.79ms
step:176/1670 train_time:15452ms step_avg:87.80ms
step:177/1670 train_time:15541ms step_avg:87.80ms
step:178/1670 train_time:15629ms step_avg:87.80ms
step:179/1670 train_time:15716ms step_avg:87.80ms
step:180/1670 train_time:15803ms step_avg:87.79ms
step:181/1670 train_time:15890ms step_avg:87.79ms
step:182/1670 train_time:15978ms step_avg:87.79ms
step:183/1670 train_time:16065ms step_avg:87.79ms
step:184/1670 train_time:16154ms step_avg:87.79ms
step:185/1670 train_time:16242ms step_avg:87.79ms
step:186/1670 train_time:16330ms step_avg:87.80ms
step:187/1670 train_time:16418ms step_avg:87.80ms
step:188/1670 train_time:16505ms step_avg:87.79ms
step:189/1670 train_time:16593ms step_avg:87.79ms
step:190/1670 train_time:16681ms step_avg:87.79ms
step:191/1670 train_time:16767ms step_avg:87.79ms
step:192/1670 train_time:16855ms step_avg:87.79ms
step:193/1670 train_time:16943ms step_avg:87.79ms
step:194/1670 train_time:17030ms step_avg:87.78ms
step:195/1670 train_time:17117ms step_avg:87.78ms
step:196/1670 train_time:17205ms step_avg:87.78ms
step:197/1670 train_time:17293ms step_avg:87.78ms
step:198/1670 train_time:17381ms step_avg:87.78ms
step:199/1670 train_time:17469ms step_avg:87.78ms
step:200/1670 train_time:17556ms step_avg:87.78ms
step:201/1670 train_time:17644ms step_avg:87.78ms
step:202/1670 train_time:17731ms step_avg:87.78ms
step:203/1670 train_time:17819ms step_avg:87.78ms
step:204/1670 train_time:17906ms step_avg:87.78ms
step:205/1670 train_time:17994ms step_avg:87.77ms
step:206/1670 train_time:18081ms step_avg:87.77ms
step:207/1670 train_time:18168ms step_avg:87.77ms
step:208/1670 train_time:18255ms step_avg:87.77ms
step:209/1670 train_time:18343ms step_avg:87.77ms
step:210/1670 train_time:18430ms step_avg:87.76ms
step:211/1670 train_time:18518ms step_avg:87.76ms
step:212/1670 train_time:18606ms step_avg:87.76ms
step:213/1670 train_time:18693ms step_avg:87.76ms
step:214/1670 train_time:18781ms step_avg:87.76ms
step:215/1670 train_time:18868ms step_avg:87.76ms
step:216/1670 train_time:18955ms step_avg:87.76ms
step:217/1670 train_time:19043ms step_avg:87.75ms
step:218/1670 train_time:19130ms step_avg:87.75ms
step:219/1670 train_time:19218ms step_avg:87.75ms
step:220/1670 train_time:19306ms step_avg:87.75ms
step:221/1670 train_time:19393ms step_avg:87.75ms
step:222/1670 train_time:19482ms step_avg:87.76ms
step:223/1670 train_time:19569ms step_avg:87.76ms
step:224/1670 train_time:19658ms step_avg:87.76ms
step:225/1670 train_time:19745ms step_avg:87.76ms
step:226/1670 train_time:19832ms step_avg:87.75ms
step:227/1670 train_time:19919ms step_avg:87.75ms
step:228/1670 train_time:20007ms step_avg:87.75ms
step:229/1670 train_time:20094ms step_avg:87.75ms
step:230/1670 train_time:20182ms step_avg:87.75ms
step:231/1670 train_time:20269ms step_avg:87.74ms
step:232/1670 train_time:20357ms step_avg:87.75ms
step:233/1670 train_time:20444ms step_avg:87.74ms
step:234/1670 train_time:20532ms step_avg:87.74ms
step:235/1670 train_time:20620ms step_avg:87.75ms
step:236/1670 train_time:20707ms step_avg:87.74ms
step:237/1670 train_time:20795ms step_avg:87.74ms
step:238/1670 train_time:20883ms step_avg:87.74ms
step:239/1670 train_time:20970ms step_avg:87.74ms
step:240/1670 train_time:21058ms step_avg:87.74ms
step:241/1670 train_time:21144ms step_avg:87.74ms
step:242/1670 train_time:21232ms step_avg:87.74ms
step:243/1670 train_time:21320ms step_avg:87.74ms
step:244/1670 train_time:21407ms step_avg:87.73ms
step:245/1670 train_time:21495ms step_avg:87.74ms
step:246/1670 train_time:21582ms step_avg:87.73ms
step:247/1670 train_time:21669ms step_avg:87.73ms
step:248/1670 train_time:21758ms step_avg:87.73ms
step:249/1670 train_time:21845ms step_avg:87.73ms
step:250/1670 train_time:21933ms step_avg:87.73ms
step:250/1670 val_loss:3.9793 train_time:22022ms step_avg:88.09ms
step:251/1670 train_time:22041ms step_avg:87.81ms
step:252/1670 train_time:22113ms step_avg:87.75ms
step:253/1670 train_time:22205ms step_avg:87.77ms
step:254/1670 train_time:22292ms step_avg:87.76ms
step:255/1670 train_time:22378ms step_avg:87.76ms
step:256/1670 train_time:22465ms step_avg:87.75ms
step:257/1670 train_time:22552ms step_avg:87.75ms
step:258/1670 train_time:22638ms step_avg:87.74ms
step:259/1670 train_time:22724ms step_avg:87.74ms
step:260/1670 train_time:22812ms step_avg:87.74ms
step:261/1670 train_time:22898ms step_avg:87.73ms
step:262/1670 train_time:22987ms step_avg:87.74ms
step:263/1670 train_time:23077ms step_avg:87.75ms
step:264/1670 train_time:23167ms step_avg:87.75ms
step:265/1670 train_time:23256ms step_avg:87.76ms
step:266/1670 train_time:23343ms step_avg:87.75ms
step:267/1670 train_time:23429ms step_avg:87.75ms
step:268/1670 train_time:23517ms step_avg:87.75ms
step:269/1670 train_time:23603ms step_avg:87.74ms
step:270/1670 train_time:23689ms step_avg:87.74ms
step:271/1670 train_time:23776ms step_avg:87.73ms
step:272/1670 train_time:23863ms step_avg:87.73ms
step:273/1670 train_time:23950ms step_avg:87.73ms
step:274/1670 train_time:24039ms step_avg:87.73ms
step:275/1670 train_time:24127ms step_avg:87.74ms
step:276/1670 train_time:24216ms step_avg:87.74ms
step:277/1670 train_time:24304ms step_avg:87.74ms
step:278/1670 train_time:24392ms step_avg:87.74ms
step:279/1670 train_time:24479ms step_avg:87.74ms
step:280/1670 train_time:24566ms step_avg:87.73ms
step:281/1670 train_time:24653ms step_avg:87.73ms
step:282/1670 train_time:24740ms step_avg:87.73ms
step:283/1670 train_time:24826ms step_avg:87.72ms
step:284/1670 train_time:24914ms step_avg:87.72ms
step:285/1670 train_time:25001ms step_avg:87.72ms
step:286/1670 train_time:25090ms step_avg:87.73ms
step:287/1670 train_time:25179ms step_avg:87.73ms
step:288/1670 train_time:25267ms step_avg:87.73ms
step:289/1670 train_time:25355ms step_avg:87.74ms
step:290/1670 train_time:25443ms step_avg:87.73ms
step:291/1670 train_time:25530ms step_avg:87.73ms
step:292/1670 train_time:25617ms step_avg:87.73ms
step:293/1670 train_time:25704ms step_avg:87.73ms
step:294/1670 train_time:25791ms step_avg:87.73ms
step:295/1670 train_time:25879ms step_avg:87.72ms
step:296/1670 train_time:25966ms step_avg:87.72ms
step:297/1670 train_time:26055ms step_avg:87.73ms
step:298/1670 train_time:26142ms step_avg:87.72ms
step:299/1670 train_time:26230ms step_avg:87.72ms
step:300/1670 train_time:26318ms step_avg:87.73ms
step:301/1670 train_time:26405ms step_avg:87.73ms
step:302/1670 train_time:26493ms step_avg:87.73ms
step:303/1670 train_time:26581ms step_avg:87.73ms
step:304/1670 train_time:26668ms step_avg:87.72ms
step:305/1670 train_time:26755ms step_avg:87.72ms
step:306/1670 train_time:26842ms step_avg:87.72ms
step:307/1670 train_time:26929ms step_avg:87.72ms
step:308/1670 train_time:27017ms step_avg:87.72ms
step:309/1670 train_time:27105ms step_avg:87.72ms
step:310/1670 train_time:27194ms step_avg:87.72ms
step:311/1670 train_time:27280ms step_avg:87.72ms
step:312/1670 train_time:27369ms step_avg:87.72ms
step:313/1670 train_time:27456ms step_avg:87.72ms
step:314/1670 train_time:27543ms step_avg:87.72ms
step:315/1670 train_time:27630ms step_avg:87.72ms
step:316/1670 train_time:27718ms step_avg:87.72ms
step:317/1670 train_time:27805ms step_avg:87.71ms
step:318/1670 train_time:27893ms step_avg:87.71ms
step:319/1670 train_time:27981ms step_avg:87.71ms
step:320/1670 train_time:28068ms step_avg:87.71ms
step:321/1670 train_time:28156ms step_avg:87.71ms
step:322/1670 train_time:28244ms step_avg:87.71ms
step:323/1670 train_time:28332ms step_avg:87.72ms
step:324/1670 train_time:28420ms step_avg:87.72ms
step:325/1670 train_time:28508ms step_avg:87.72ms
step:326/1670 train_time:28596ms step_avg:87.72ms
step:327/1670 train_time:28683ms step_avg:87.71ms
step:328/1670 train_time:28770ms step_avg:87.71ms
step:329/1670 train_time:28857ms step_avg:87.71ms
step:330/1670 train_time:28945ms step_avg:87.71ms
step:331/1670 train_time:29032ms step_avg:87.71ms
step:332/1670 train_time:29119ms step_avg:87.71ms
step:333/1670 train_time:29207ms step_avg:87.71ms
step:334/1670 train_time:29295ms step_avg:87.71ms
step:335/1670 train_time:29382ms step_avg:87.71ms
step:336/1670 train_time:29470ms step_avg:87.71ms
step:337/1670 train_time:29558ms step_avg:87.71ms
step:338/1670 train_time:29645ms step_avg:87.71ms
step:339/1670 train_time:29733ms step_avg:87.71ms
step:340/1670 train_time:29820ms step_avg:87.71ms
step:341/1670 train_time:29907ms step_avg:87.71ms
step:342/1670 train_time:29995ms step_avg:87.70ms
step:343/1670 train_time:30082ms step_avg:87.70ms
step:344/1670 train_time:30170ms step_avg:87.70ms
step:345/1670 train_time:30258ms step_avg:87.70ms
step:346/1670 train_time:30344ms step_avg:87.70ms
step:347/1670 train_time:30432ms step_avg:87.70ms
step:348/1670 train_time:30519ms step_avg:87.70ms
step:349/1670 train_time:30607ms step_avg:87.70ms
step:350/1670 train_time:30694ms step_avg:87.70ms
step:351/1670 train_time:30781ms step_avg:87.70ms
step:352/1670 train_time:30869ms step_avg:87.70ms
step:353/1670 train_time:30958ms step_avg:87.70ms
step:354/1670 train_time:31045ms step_avg:87.70ms
step:355/1670 train_time:31133ms step_avg:87.70ms
step:356/1670 train_time:31220ms step_avg:87.70ms
step:357/1670 train_time:31308ms step_avg:87.70ms
step:358/1670 train_time:31396ms step_avg:87.70ms
step:359/1670 train_time:31484ms step_avg:87.70ms
step:360/1670 train_time:31573ms step_avg:87.70ms
step:361/1670 train_time:31660ms step_avg:87.70ms
step:362/1670 train_time:31747ms step_avg:87.70ms
step:363/1670 train_time:31835ms step_avg:87.70ms
step:364/1670 train_time:31922ms step_avg:87.70ms
step:365/1670 train_time:32010ms step_avg:87.70ms
step:366/1670 train_time:32097ms step_avg:87.70ms
step:367/1670 train_time:32185ms step_avg:87.70ms
step:368/1670 train_time:32272ms step_avg:87.70ms
step:369/1670 train_time:32360ms step_avg:87.70ms
step:370/1670 train_time:32447ms step_avg:87.70ms
step:371/1670 train_time:32535ms step_avg:87.70ms
step:372/1670 train_time:32623ms step_avg:87.70ms
step:373/1670 train_time:32711ms step_avg:87.70ms
step:374/1670 train_time:32798ms step_avg:87.70ms
step:375/1670 train_time:32885ms step_avg:87.69ms
step:375/1670 val_loss:3.8259 train_time:32975ms step_avg:87.93ms
step:376/1670 train_time:32995ms step_avg:87.75ms
step:377/1670 train_time:33065ms step_avg:87.71ms
step:378/1670 train_time:33155ms step_avg:87.71ms
step:379/1670 train_time:33243ms step_avg:87.71ms
step:380/1670 train_time:33330ms step_avg:87.71ms
step:381/1670 train_time:33417ms step_avg:87.71ms
step:382/1670 train_time:33503ms step_avg:87.70ms
step:383/1670 train_time:33590ms step_avg:87.70ms
step:384/1670 train_time:33677ms step_avg:87.70ms
step:385/1670 train_time:33765ms step_avg:87.70ms
step:386/1670 train_time:33852ms step_avg:87.70ms
step:387/1670 train_time:33940ms step_avg:87.70ms
step:388/1670 train_time:34031ms step_avg:87.71ms
step:389/1670 train_time:34121ms step_avg:87.72ms
step:390/1670 train_time:34209ms step_avg:87.72ms
step:391/1670 train_time:34297ms step_avg:87.72ms
step:392/1670 train_time:34385ms step_avg:87.72ms
step:393/1670 train_time:34471ms step_avg:87.71ms
step:394/1670 train_time:34558ms step_avg:87.71ms
step:395/1670 train_time:34645ms step_avg:87.71ms
step:396/1670 train_time:34732ms step_avg:87.71ms
step:397/1670 train_time:34819ms step_avg:87.71ms
step:398/1670 train_time:34908ms step_avg:87.71ms
step:399/1670 train_time:34997ms step_avg:87.71ms
step:400/1670 train_time:35087ms step_avg:87.72ms
step:401/1670 train_time:35175ms step_avg:87.72ms
step:402/1670 train_time:35263ms step_avg:87.72ms
step:403/1670 train_time:35351ms step_avg:87.72ms
step:404/1670 train_time:35438ms step_avg:87.72ms
step:405/1670 train_time:35525ms step_avg:87.72ms
step:406/1670 train_time:35612ms step_avg:87.72ms
step:407/1670 train_time:35700ms step_avg:87.72ms
step:408/1670 train_time:35787ms step_avg:87.71ms
step:409/1670 train_time:35875ms step_avg:87.71ms
step:410/1670 train_time:35962ms step_avg:87.71ms
step:411/1670 train_time:36050ms step_avg:87.71ms
step:412/1670 train_time:36139ms step_avg:87.72ms
step:413/1670 train_time:36227ms step_avg:87.72ms
step:414/1670 train_time:36315ms step_avg:87.72ms
step:415/1670 train_time:36402ms step_avg:87.72ms
step:416/1670 train_time:36489ms step_avg:87.71ms
step:417/1670 train_time:36576ms step_avg:87.71ms
step:418/1670 train_time:36663ms step_avg:87.71ms
step:419/1670 train_time:36751ms step_avg:87.71ms
step:420/1670 train_time:36839ms step_avg:87.71ms
step:421/1670 train_time:36928ms step_avg:87.71ms
step:422/1670 train_time:37015ms step_avg:87.71ms
step:423/1670 train_time:37104ms step_avg:87.72ms
step:424/1670 train_time:37192ms step_avg:87.72ms
step:425/1670 train_time:37280ms step_avg:87.72ms
step:426/1670 train_time:37369ms step_avg:87.72ms
step:427/1670 train_time:37455ms step_avg:87.72ms
step:428/1670 train_time:37542ms step_avg:87.71ms
step:429/1670 train_time:37629ms step_avg:87.71ms
step:430/1670 train_time:37716ms step_avg:87.71ms
step:431/1670 train_time:37805ms step_avg:87.71ms
step:432/1670 train_time:37892ms step_avg:87.71ms
step:433/1670 train_time:37981ms step_avg:87.72ms
step:434/1670 train_time:38069ms step_avg:87.72ms
step:435/1670 train_time:38157ms step_avg:87.72ms
step:436/1670 train_time:38246ms step_avg:87.72ms
step:437/1670 train_time:38333ms step_avg:87.72ms
step:438/1670 train_time:38421ms step_avg:87.72ms
step:439/1670 train_time:38509ms step_avg:87.72ms
step:440/1670 train_time:38596ms step_avg:87.72ms
step:441/1670 train_time:38684ms step_avg:87.72ms
step:442/1670 train_time:38771ms step_avg:87.72ms
step:443/1670 train_time:38859ms step_avg:87.72ms
step:444/1670 train_time:38947ms step_avg:87.72ms
step:445/1670 train_time:39035ms step_avg:87.72ms
step:446/1670 train_time:39123ms step_avg:87.72ms
step:447/1670 train_time:39211ms step_avg:87.72ms
step:448/1670 train_time:39298ms step_avg:87.72ms
step:449/1670 train_time:39386ms step_avg:87.72ms
step:450/1670 train_time:39473ms step_avg:87.72ms
step:451/1670 train_time:39560ms step_avg:87.72ms
step:452/1670 train_time:39647ms step_avg:87.72ms
step:453/1670 train_time:39734ms step_avg:87.71ms
step:454/1670 train_time:39821ms step_avg:87.71ms
step:455/1670 train_time:39909ms step_avg:87.71ms
step:456/1670 train_time:39997ms step_avg:87.71ms
step:457/1670 train_time:40085ms step_avg:87.71ms
step:458/1670 train_time:40173ms step_avg:87.71ms
step:459/1670 train_time:40260ms step_avg:87.71ms
step:460/1670 train_time:40348ms step_avg:87.71ms
step:461/1670 train_time:40436ms step_avg:87.71ms
step:462/1670 train_time:40523ms step_avg:87.71ms
step:463/1670 train_time:40611ms step_avg:87.71ms
step:464/1670 train_time:40698ms step_avg:87.71ms
step:465/1670 train_time:40786ms step_avg:87.71ms
step:466/1670 train_time:40874ms step_avg:87.71ms
step:467/1670 train_time:40962ms step_avg:87.71ms
step:468/1670 train_time:41050ms step_avg:87.71ms
step:469/1670 train_time:41138ms step_avg:87.72ms
step:470/1670 train_time:41226ms step_avg:87.72ms
step:471/1670 train_time:41314ms step_avg:87.71ms
step:472/1670 train_time:41401ms step_avg:87.71ms
step:473/1670 train_time:41489ms step_avg:87.71ms
step:474/1670 train_time:41576ms step_avg:87.71ms
step:475/1670 train_time:41664ms step_avg:87.71ms
step:476/1670 train_time:41751ms step_avg:87.71ms
step:477/1670 train_time:41839ms step_avg:87.71ms
step:478/1670 train_time:41927ms step_avg:87.71ms
step:479/1670 train_time:42014ms step_avg:87.71ms
step:480/1670 train_time:42102ms step_avg:87.71ms
step:481/1670 train_time:42190ms step_avg:87.71ms
step:482/1670 train_time:42278ms step_avg:87.71ms
step:483/1670 train_time:42366ms step_avg:87.71ms
step:484/1670 train_time:42454ms step_avg:87.71ms
step:485/1670 train_time:42541ms step_avg:87.71ms
step:486/1670 train_time:42628ms step_avg:87.71ms
step:487/1670 train_time:42716ms step_avg:87.71ms
step:488/1670 train_time:42804ms step_avg:87.71ms
step:489/1670 train_time:42892ms step_avg:87.71ms
step:490/1670 train_time:42979ms step_avg:87.71ms
step:491/1670 train_time:43068ms step_avg:87.71ms
step:492/1670 train_time:43156ms step_avg:87.71ms
step:493/1670 train_time:43244ms step_avg:87.72ms
step:494/1670 train_time:43331ms step_avg:87.72ms
step:495/1670 train_time:43419ms step_avg:87.71ms
step:496/1670 train_time:43507ms step_avg:87.72ms
step:497/1670 train_time:43594ms step_avg:87.71ms
step:498/1670 train_time:43681ms step_avg:87.71ms
step:499/1670 train_time:43769ms step_avg:87.71ms
step:500/1670 train_time:43857ms step_avg:87.71ms
step:500/1670 val_loss:3.7194 train_time:43946ms step_avg:87.89ms
step:501/1670 train_time:43968ms step_avg:87.76ms
step:502/1670 train_time:44038ms step_avg:87.72ms
step:503/1670 train_time:44131ms step_avg:87.74ms
step:504/1670 train_time:44219ms step_avg:87.74ms
step:505/1670 train_time:44307ms step_avg:87.74ms
step:506/1670 train_time:44393ms step_avg:87.73ms
step:507/1670 train_time:44479ms step_avg:87.73ms
step:508/1670 train_time:44566ms step_avg:87.73ms
step:509/1670 train_time:44653ms step_avg:87.73ms
step:510/1670 train_time:44739ms step_avg:87.72ms
step:511/1670 train_time:44826ms step_avg:87.72ms
step:512/1670 train_time:44915ms step_avg:87.72ms
step:513/1670 train_time:45004ms step_avg:87.73ms
step:514/1670 train_time:45096ms step_avg:87.73ms
step:515/1670 train_time:45185ms step_avg:87.74ms
step:516/1670 train_time:45273ms step_avg:87.74ms
step:517/1670 train_time:45360ms step_avg:87.74ms
step:518/1670 train_time:45446ms step_avg:87.73ms
step:519/1670 train_time:45534ms step_avg:87.73ms
step:520/1670 train_time:45620ms step_avg:87.73ms
step:521/1670 train_time:45707ms step_avg:87.73ms
step:522/1670 train_time:45794ms step_avg:87.73ms
step:523/1670 train_time:45882ms step_avg:87.73ms
step:524/1670 train_time:45970ms step_avg:87.73ms
step:525/1670 train_time:46059ms step_avg:87.73ms
step:526/1670 train_time:46148ms step_avg:87.73ms
step:527/1670 train_time:46237ms step_avg:87.74ms
step:528/1670 train_time:46325ms step_avg:87.74ms
step:529/1670 train_time:46412ms step_avg:87.74ms
step:530/1670 train_time:46499ms step_avg:87.73ms
step:531/1670 train_time:46586ms step_avg:87.73ms
step:532/1670 train_time:46674ms step_avg:87.73ms
step:533/1670 train_time:46760ms step_avg:87.73ms
step:534/1670 train_time:46849ms step_avg:87.73ms
step:535/1670 train_time:46937ms step_avg:87.73ms
step:536/1670 train_time:47026ms step_avg:87.74ms
step:537/1670 train_time:47116ms step_avg:87.74ms
step:538/1670 train_time:47204ms step_avg:87.74ms
step:539/1670 train_time:47291ms step_avg:87.74ms
step:540/1670 train_time:47378ms step_avg:87.74ms
step:541/1670 train_time:47465ms step_avg:87.74ms
step:542/1670 train_time:47553ms step_avg:87.74ms
step:543/1670 train_time:47640ms step_avg:87.73ms
step:544/1670 train_time:47727ms step_avg:87.73ms
step:545/1670 train_time:47816ms step_avg:87.74ms
step:546/1670 train_time:47905ms step_avg:87.74ms
step:547/1670 train_time:47994ms step_avg:87.74ms
step:548/1670 train_time:48082ms step_avg:87.74ms
step:549/1670 train_time:48172ms step_avg:87.74ms
step:550/1670 train_time:48261ms step_avg:87.75ms
step:551/1670 train_time:48350ms step_avg:87.75ms
step:552/1670 train_time:48439ms step_avg:87.75ms
step:553/1670 train_time:48527ms step_avg:87.75ms
step:554/1670 train_time:48615ms step_avg:87.75ms
step:555/1670 train_time:48704ms step_avg:87.75ms
step:556/1670 train_time:48793ms step_avg:87.76ms
step:557/1670 train_time:48882ms step_avg:87.76ms
step:558/1670 train_time:48971ms step_avg:87.76ms
step:559/1670 train_time:49059ms step_avg:87.76ms
step:560/1670 train_time:49148ms step_avg:87.76ms
step:561/1670 train_time:49238ms step_avg:87.77ms
step:562/1670 train_time:49328ms step_avg:87.77ms
step:563/1670 train_time:49418ms step_avg:87.78ms
step:564/1670 train_time:49507ms step_avg:87.78ms
step:565/1670 train_time:49596ms step_avg:87.78ms
step:566/1670 train_time:49683ms step_avg:87.78ms
step:567/1670 train_time:49772ms step_avg:87.78ms
step:568/1670 train_time:49861ms step_avg:87.78ms
step:569/1670 train_time:49951ms step_avg:87.79ms
step:570/1670 train_time:50039ms step_avg:87.79ms
step:571/1670 train_time:50129ms step_avg:87.79ms
step:572/1670 train_time:50218ms step_avg:87.79ms
step:573/1670 train_time:50308ms step_avg:87.80ms
step:574/1670 train_time:50397ms step_avg:87.80ms
step:575/1670 train_time:50485ms step_avg:87.80ms
step:576/1670 train_time:50574ms step_avg:87.80ms
step:577/1670 train_time:50662ms step_avg:87.80ms
step:578/1670 train_time:50750ms step_avg:87.80ms
step:579/1670 train_time:50839ms step_avg:87.81ms
step:580/1670 train_time:50928ms step_avg:87.81ms
step:581/1670 train_time:51017ms step_avg:87.81ms
step:582/1670 train_time:51106ms step_avg:87.81ms
step:583/1670 train_time:51196ms step_avg:87.81ms
step:584/1670 train_time:51286ms step_avg:87.82ms
step:585/1670 train_time:51375ms step_avg:87.82ms
step:586/1670 train_time:51464ms step_avg:87.82ms
step:587/1670 train_time:51553ms step_avg:87.82ms
step:588/1670 train_time:51641ms step_avg:87.82ms
step:589/1670 train_time:51730ms step_avg:87.83ms
step:590/1670 train_time:51819ms step_avg:87.83ms
step:591/1670 train_time:51909ms step_avg:87.83ms
step:592/1670 train_time:51998ms step_avg:87.83ms
step:593/1670 train_time:52087ms step_avg:87.84ms
step:594/1670 train_time:52176ms step_avg:87.84ms
step:595/1670 train_time:52265ms step_avg:87.84ms
step:596/1670 train_time:52355ms step_avg:87.84ms
step:597/1670 train_time:52442ms step_avg:87.84ms
step:598/1670 train_time:52531ms step_avg:87.84ms
step:599/1670 train_time:52620ms step_avg:87.85ms
step:600/1670 train_time:52709ms step_avg:87.85ms
step:601/1670 train_time:52797ms step_avg:87.85ms
step:602/1670 train_time:52886ms step_avg:87.85ms
step:603/1670 train_time:52976ms step_avg:87.85ms
step:604/1670 train_time:53065ms step_avg:87.86ms
step:605/1670 train_time:53155ms step_avg:87.86ms
step:606/1670 train_time:53244ms step_avg:87.86ms
step:607/1670 train_time:53334ms step_avg:87.86ms
step:608/1670 train_time:53422ms step_avg:87.86ms
step:609/1670 train_time:53511ms step_avg:87.87ms
step:610/1670 train_time:53600ms step_avg:87.87ms
step:611/1670 train_time:53689ms step_avg:87.87ms
step:612/1670 train_time:53778ms step_avg:87.87ms
step:613/1670 train_time:53866ms step_avg:87.87ms
step:614/1670 train_time:53955ms step_avg:87.87ms
step:615/1670 train_time:54043ms step_avg:87.88ms
step:616/1670 train_time:54132ms step_avg:87.88ms
step:617/1670 train_time:54221ms step_avg:87.88ms
step:618/1670 train_time:54309ms step_avg:87.88ms
step:619/1670 train_time:54398ms step_avg:87.88ms
step:620/1670 train_time:54487ms step_avg:87.88ms
step:621/1670 train_time:54576ms step_avg:87.88ms
step:622/1670 train_time:54664ms step_avg:87.88ms
step:623/1670 train_time:54754ms step_avg:87.89ms
step:624/1670 train_time:54842ms step_avg:87.89ms
step:625/1670 train_time:54931ms step_avg:87.89ms
step:625/1670 val_loss:3.6188 train_time:55020ms step_avg:88.03ms
step:626/1670 train_time:55040ms step_avg:87.92ms
step:627/1670 train_time:55110ms step_avg:87.89ms
step:628/1670 train_time:55200ms step_avg:87.90ms
step:629/1670 train_time:55290ms step_avg:87.90ms
step:630/1670 train_time:55378ms step_avg:87.90ms
step:631/1670 train_time:55465ms step_avg:87.90ms
step:632/1670 train_time:55552ms step_avg:87.90ms
step:633/1670 train_time:55640ms step_avg:87.90ms
step:634/1670 train_time:55728ms step_avg:87.90ms
step:635/1670 train_time:55819ms step_avg:87.90ms
step:636/1670 train_time:55907ms step_avg:87.90ms
step:637/1670 train_time:55999ms step_avg:87.91ms
step:638/1670 train_time:56090ms step_avg:87.92ms
step:639/1670 train_time:56180ms step_avg:87.92ms
step:640/1670 train_time:56270ms step_avg:87.92ms
step:641/1670 train_time:56359ms step_avg:87.92ms
step:642/1670 train_time:56446ms step_avg:87.92ms
step:643/1670 train_time:56535ms step_avg:87.92ms
step:644/1670 train_time:56622ms step_avg:87.92ms
step:645/1670 train_time:56711ms step_avg:87.92ms
step:646/1670 train_time:56800ms step_avg:87.93ms
step:647/1670 train_time:56889ms step_avg:87.93ms
step:648/1670 train_time:56979ms step_avg:87.93ms
step:649/1670 train_time:57069ms step_avg:87.93ms
step:650/1670 train_time:57159ms step_avg:87.94ms
step:651/1670 train_time:57249ms step_avg:87.94ms
step:652/1670 train_time:57338ms step_avg:87.94ms
step:653/1670 train_time:57426ms step_avg:87.94ms
step:654/1670 train_time:57515ms step_avg:87.94ms
step:655/1670 train_time:57602ms step_avg:87.94ms
step:656/1670 train_time:57691ms step_avg:87.94ms
step:657/1670 train_time:57778ms step_avg:87.94ms
step:658/1670 train_time:57868ms step_avg:87.94ms
step:659/1670 train_time:57958ms step_avg:87.95ms
step:660/1670 train_time:58048ms step_avg:87.95ms
step:661/1670 train_time:58138ms step_avg:87.96ms
step:662/1670 train_time:58228ms step_avg:87.96ms
step:663/1670 train_time:58317ms step_avg:87.96ms
step:664/1670 train_time:58405ms step_avg:87.96ms
step:665/1670 train_time:58494ms step_avg:87.96ms
step:666/1670 train_time:58582ms step_avg:87.96ms
step:667/1670 train_time:58671ms step_avg:87.96ms
step:668/1670 train_time:58760ms step_avg:87.96ms
step:669/1670 train_time:58848ms step_avg:87.96ms
step:670/1670 train_time:58938ms step_avg:87.97ms
step:671/1670 train_time:59027ms step_avg:87.97ms
step:672/1670 train_time:59118ms step_avg:87.97ms
step:673/1670 train_time:59206ms step_avg:87.97ms
step:674/1670 train_time:59295ms step_avg:87.97ms
step:675/1670 train_time:59383ms step_avg:87.98ms
step:676/1670 train_time:59473ms step_avg:87.98ms
step:677/1670 train_time:59562ms step_avg:87.98ms
step:678/1670 train_time:59650ms step_avg:87.98ms
step:679/1670 train_time:59739ms step_avg:87.98ms
step:680/1670 train_time:59827ms step_avg:87.98ms
step:681/1670 train_time:59916ms step_avg:87.98ms
step:682/1670 train_time:60004ms step_avg:87.98ms
step:683/1670 train_time:60095ms step_avg:87.99ms
step:684/1670 train_time:60182ms step_avg:87.99ms
step:685/1670 train_time:60272ms step_avg:87.99ms
step:686/1670 train_time:60360ms step_avg:87.99ms
step:687/1670 train_time:60449ms step_avg:87.99ms
step:688/1670 train_time:60537ms step_avg:87.99ms
step:689/1670 train_time:60626ms step_avg:87.99ms
step:690/1670 train_time:60714ms step_avg:87.99ms
step:691/1670 train_time:60802ms step_avg:87.99ms
step:692/1670 train_time:60892ms step_avg:87.99ms
step:693/1670 train_time:60981ms step_avg:88.00ms
step:694/1670 train_time:61070ms step_avg:88.00ms
step:695/1670 train_time:61159ms step_avg:88.00ms
step:696/1670 train_time:61248ms step_avg:88.00ms
step:697/1670 train_time:61337ms step_avg:88.00ms
step:698/1670 train_time:61426ms step_avg:88.00ms
step:699/1670 train_time:61515ms step_avg:88.00ms
step:700/1670 train_time:61603ms step_avg:88.00ms
step:701/1670 train_time:61692ms step_avg:88.01ms
step:702/1670 train_time:61780ms step_avg:88.01ms
step:703/1670 train_time:61869ms step_avg:88.01ms
step:704/1670 train_time:61959ms step_avg:88.01ms
step:705/1670 train_time:62048ms step_avg:88.01ms
step:706/1670 train_time:62137ms step_avg:88.01ms
step:707/1670 train_time:62226ms step_avg:88.01ms
step:708/1670 train_time:62315ms step_avg:88.02ms
step:709/1670 train_time:62403ms step_avg:88.02ms
step:710/1670 train_time:62493ms step_avg:88.02ms
step:711/1670 train_time:62582ms step_avg:88.02ms
step:712/1670 train_time:62671ms step_avg:88.02ms
step:713/1670 train_time:62759ms step_avg:88.02ms
step:714/1670 train_time:62849ms step_avg:88.02ms
step:715/1670 train_time:62938ms step_avg:88.02ms
step:716/1670 train_time:63027ms step_avg:88.03ms
step:717/1670 train_time:63116ms step_avg:88.03ms
step:718/1670 train_time:63204ms step_avg:88.03ms
step:719/1670 train_time:63294ms step_avg:88.03ms
step:720/1670 train_time:63382ms step_avg:88.03ms
step:721/1670 train_time:63471ms step_avg:88.03ms
step:722/1670 train_time:63560ms step_avg:88.03ms
step:723/1670 train_time:63649ms step_avg:88.03ms
step:724/1670 train_time:63737ms step_avg:88.03ms
step:725/1670 train_time:63825ms step_avg:88.04ms
step:726/1670 train_time:63914ms step_avg:88.04ms
step:727/1670 train_time:64003ms step_avg:88.04ms
step:728/1670 train_time:64092ms step_avg:88.04ms
step:729/1670 train_time:64180ms step_avg:88.04ms
step:730/1670 train_time:64270ms step_avg:88.04ms
step:731/1670 train_time:64359ms step_avg:88.04ms
step:732/1670 train_time:64449ms step_avg:88.05ms
step:733/1670 train_time:64538ms step_avg:88.05ms
step:734/1670 train_time:64627ms step_avg:88.05ms
step:735/1670 train_time:64716ms step_avg:88.05ms
step:736/1670 train_time:64804ms step_avg:88.05ms
step:737/1670 train_time:64892ms step_avg:88.05ms
step:738/1670 train_time:64981ms step_avg:88.05ms
step:739/1670 train_time:65070ms step_avg:88.05ms
step:740/1670 train_time:65159ms step_avg:88.05ms
step:741/1670 train_time:65249ms step_avg:88.05ms
step:742/1670 train_time:65338ms step_avg:88.06ms
step:743/1670 train_time:65426ms step_avg:88.06ms
step:744/1670 train_time:65515ms step_avg:88.06ms
step:745/1670 train_time:65603ms step_avg:88.06ms
step:746/1670 train_time:65693ms step_avg:88.06ms
step:747/1670 train_time:65781ms step_avg:88.06ms
step:748/1670 train_time:65870ms step_avg:88.06ms
step:749/1670 train_time:65960ms step_avg:88.06ms
step:750/1670 train_time:66049ms step_avg:88.07ms
step:750/1670 val_loss:3.5666 train_time:66140ms step_avg:88.19ms
step:751/1670 train_time:66161ms step_avg:88.10ms
step:752/1670 train_time:66231ms step_avg:88.07ms
step:753/1670 train_time:66320ms step_avg:88.07ms
step:754/1670 train_time:66410ms step_avg:88.08ms
step:755/1670 train_time:66498ms step_avg:88.08ms
step:756/1670 train_time:66587ms step_avg:88.08ms
step:757/1670 train_time:66676ms step_avg:88.08ms
step:758/1670 train_time:66763ms step_avg:88.08ms
step:759/1670 train_time:66852ms step_avg:88.08ms
step:760/1670 train_time:66940ms step_avg:88.08ms
step:761/1670 train_time:67028ms step_avg:88.08ms
step:762/1670 train_time:67118ms step_avg:88.08ms
step:763/1670 train_time:67207ms step_avg:88.08ms
step:764/1670 train_time:67299ms step_avg:88.09ms
step:765/1670 train_time:67388ms step_avg:88.09ms
step:766/1670 train_time:67476ms step_avg:88.09ms
step:767/1670 train_time:67565ms step_avg:88.09ms
step:768/1670 train_time:67653ms step_avg:88.09ms
step:769/1670 train_time:67741ms step_avg:88.09ms
step:770/1670 train_time:67830ms step_avg:88.09ms
step:771/1670 train_time:67919ms step_avg:88.09ms
step:772/1670 train_time:68008ms step_avg:88.09ms
step:773/1670 train_time:68098ms step_avg:88.10ms
step:774/1670 train_time:68187ms step_avg:88.10ms
step:775/1670 train_time:68278ms step_avg:88.10ms
step:776/1670 train_time:68367ms step_avg:88.10ms
step:777/1670 train_time:68456ms step_avg:88.10ms
step:778/1670 train_time:68544ms step_avg:88.10ms
step:779/1670 train_time:68633ms step_avg:88.10ms
step:780/1670 train_time:68721ms step_avg:88.10ms
step:781/1670 train_time:68811ms step_avg:88.11ms
step:782/1670 train_time:68899ms step_avg:88.11ms
step:783/1670 train_time:68989ms step_avg:88.11ms
step:784/1670 train_time:69078ms step_avg:88.11ms
step:785/1670 train_time:69166ms step_avg:88.11ms
step:786/1670 train_time:69255ms step_avg:88.11ms
step:787/1670 train_time:69344ms step_avg:88.11ms
step:788/1670 train_time:69434ms step_avg:88.11ms
step:789/1670 train_time:69522ms step_avg:88.11ms
step:790/1670 train_time:69612ms step_avg:88.12ms
step:791/1670 train_time:69700ms step_avg:88.12ms
step:792/1670 train_time:69789ms step_avg:88.12ms
step:793/1670 train_time:69878ms step_avg:88.12ms
step:794/1670 train_time:69966ms step_avg:88.12ms
step:795/1670 train_time:70054ms step_avg:88.12ms
step:796/1670 train_time:70143ms step_avg:88.12ms
step:797/1670 train_time:70232ms step_avg:88.12ms
step:798/1670 train_time:70321ms step_avg:88.12ms
step:799/1670 train_time:70410ms step_avg:88.12ms
step:800/1670 train_time:70501ms step_avg:88.13ms
step:801/1670 train_time:70589ms step_avg:88.13ms
step:802/1670 train_time:70678ms step_avg:88.13ms
step:803/1670 train_time:70766ms step_avg:88.13ms
step:804/1670 train_time:70856ms step_avg:88.13ms
step:805/1670 train_time:70943ms step_avg:88.13ms
step:806/1670 train_time:71033ms step_avg:88.13ms
step:807/1670 train_time:71122ms step_avg:88.13ms
step:808/1670 train_time:71211ms step_avg:88.13ms
step:809/1670 train_time:71301ms step_avg:88.13ms
step:810/1670 train_time:71390ms step_avg:88.14ms
step:811/1670 train_time:71480ms step_avg:88.14ms
step:812/1670 train_time:71568ms step_avg:88.14ms
step:813/1670 train_time:71657ms step_avg:88.14ms
step:814/1670 train_time:71745ms step_avg:88.14ms
step:815/1670 train_time:71835ms step_avg:88.14ms
step:816/1670 train_time:71923ms step_avg:88.14ms
step:817/1670 train_time:72012ms step_avg:88.14ms
step:818/1670 train_time:72100ms step_avg:88.14ms
step:819/1670 train_time:72190ms step_avg:88.14ms
step:820/1670 train_time:72279ms step_avg:88.14ms
step:821/1670 train_time:72368ms step_avg:88.15ms
step:822/1670 train_time:72457ms step_avg:88.15ms
step:823/1670 train_time:72545ms step_avg:88.15ms
step:824/1670 train_time:72634ms step_avg:88.15ms
step:825/1670 train_time:72722ms step_avg:88.15ms
step:826/1670 train_time:72811ms step_avg:88.15ms
step:827/1670 train_time:72900ms step_avg:88.15ms
step:828/1670 train_time:72989ms step_avg:88.15ms
step:829/1670 train_time:73078ms step_avg:88.15ms
step:830/1670 train_time:73166ms step_avg:88.15ms
step:831/1670 train_time:73255ms step_avg:88.15ms
step:832/1670 train_time:73343ms step_avg:88.15ms
step:833/1670 train_time:73433ms step_avg:88.15ms
step:834/1670 train_time:73521ms step_avg:88.16ms
step:835/1670 train_time:73611ms step_avg:88.16ms
step:836/1670 train_time:73699ms step_avg:88.16ms
step:837/1670 train_time:73789ms step_avg:88.16ms
step:838/1670 train_time:73878ms step_avg:88.16ms
step:839/1670 train_time:73968ms step_avg:88.16ms
step:840/1670 train_time:74056ms step_avg:88.16ms
step:841/1670 train_time:74145ms step_avg:88.16ms
step:842/1670 train_time:74234ms step_avg:88.16ms
step:843/1670 train_time:74322ms step_avg:88.16ms
step:844/1670 train_time:74412ms step_avg:88.17ms
step:845/1670 train_time:74502ms step_avg:88.17ms
step:846/1670 train_time:74590ms step_avg:88.17ms
step:847/1670 train_time:74679ms step_avg:88.17ms
step:848/1670 train_time:74767ms step_avg:88.17ms
step:849/1670 train_time:74856ms step_avg:88.17ms
step:850/1670 train_time:74945ms step_avg:88.17ms
step:851/1670 train_time:75034ms step_avg:88.17ms
step:852/1670 train_time:75123ms step_avg:88.17ms
step:853/1670 train_time:75211ms step_avg:88.17ms
step:854/1670 train_time:75301ms step_avg:88.17ms
step:855/1670 train_time:75390ms step_avg:88.18ms
step:856/1670 train_time:75479ms step_avg:88.18ms
step:857/1670 train_time:75567ms step_avg:88.18ms
step:858/1670 train_time:75656ms step_avg:88.18ms
step:859/1670 train_time:75744ms step_avg:88.18ms
step:860/1670 train_time:75834ms step_avg:88.18ms
step:861/1670 train_time:75922ms step_avg:88.18ms
step:862/1670 train_time:76011ms step_avg:88.18ms
step:863/1670 train_time:76100ms step_avg:88.18ms
step:864/1670 train_time:76190ms step_avg:88.18ms
step:865/1670 train_time:76279ms step_avg:88.18ms
step:866/1670 train_time:76368ms step_avg:88.19ms
step:867/1670 train_time:76458ms step_avg:88.19ms
step:868/1670 train_time:76547ms step_avg:88.19ms
step:869/1670 train_time:76636ms step_avg:88.19ms
step:870/1670 train_time:76725ms step_avg:88.19ms
step:871/1670 train_time:76814ms step_avg:88.19ms
step:872/1670 train_time:76903ms step_avg:88.19ms
step:873/1670 train_time:76992ms step_avg:88.19ms
step:874/1670 train_time:77081ms step_avg:88.19ms
step:875/1670 train_time:77170ms step_avg:88.19ms
step:875/1670 val_loss:3.5199 train_time:77260ms step_avg:88.30ms
step:876/1670 train_time:77289ms step_avg:88.23ms
step:877/1670 train_time:77355ms step_avg:88.20ms
step:878/1670 train_time:77449ms step_avg:88.21ms
step:879/1670 train_time:77538ms step_avg:88.21ms
step:880/1670 train_time:77627ms step_avg:88.21ms
step:881/1670 train_time:77713ms step_avg:88.21ms
step:882/1670 train_time:77801ms step_avg:88.21ms
step:883/1670 train_time:77889ms step_avg:88.21ms
step:884/1670 train_time:77976ms step_avg:88.21ms
step:885/1670 train_time:78064ms step_avg:88.21ms
step:886/1670 train_time:78153ms step_avg:88.21ms
step:887/1670 train_time:78245ms step_avg:88.21ms
step:888/1670 train_time:78335ms step_avg:88.22ms
step:889/1670 train_time:78428ms step_avg:88.22ms
step:890/1670 train_time:78517ms step_avg:88.22ms
step:891/1670 train_time:78606ms step_avg:88.22ms
step:892/1670 train_time:78694ms step_avg:88.22ms
step:893/1670 train_time:78783ms step_avg:88.22ms
step:894/1670 train_time:78871ms step_avg:88.22ms
step:895/1670 train_time:78959ms step_avg:88.22ms
step:896/1670 train_time:79047ms step_avg:88.22ms
step:897/1670 train_time:79135ms step_avg:88.22ms
step:898/1670 train_time:79226ms step_avg:88.22ms
step:899/1670 train_time:79316ms step_avg:88.23ms
step:900/1670 train_time:79408ms step_avg:88.23ms
step:901/1670 train_time:79498ms step_avg:88.23ms
step:902/1670 train_time:79588ms step_avg:88.23ms
step:903/1670 train_time:79677ms step_avg:88.24ms
step:904/1670 train_time:79766ms step_avg:88.24ms
step:905/1670 train_time:79854ms step_avg:88.24ms
step:906/1670 train_time:79942ms step_avg:88.24ms
step:907/1670 train_time:80030ms step_avg:88.24ms
step:908/1670 train_time:80118ms step_avg:88.24ms
step:909/1670 train_time:80208ms step_avg:88.24ms
step:910/1670 train_time:80298ms step_avg:88.24ms
step:911/1670 train_time:80388ms step_avg:88.24ms
step:912/1670 train_time:80478ms step_avg:88.24ms
step:913/1670 train_time:80567ms step_avg:88.24ms
step:914/1670 train_time:80657ms step_avg:88.25ms
step:915/1670 train_time:80746ms step_avg:88.25ms
step:916/1670 train_time:80835ms step_avg:88.25ms
step:917/1670 train_time:80925ms step_avg:88.25ms
step:918/1670 train_time:81013ms step_avg:88.25ms
step:919/1670 train_time:81102ms step_avg:88.25ms
step:920/1670 train_time:81191ms step_avg:88.25ms
step:921/1670 train_time:81279ms step_avg:88.25ms
step:922/1670 train_time:81369ms step_avg:88.25ms
step:923/1670 train_time:81458ms step_avg:88.25ms
step:924/1670 train_time:81548ms step_avg:88.26ms
step:925/1670 train_time:81637ms step_avg:88.26ms
step:926/1670 train_time:81727ms step_avg:88.26ms
step:927/1670 train_time:81816ms step_avg:88.26ms
step:928/1670 train_time:81906ms step_avg:88.26ms
step:929/1670 train_time:81995ms step_avg:88.26ms
step:930/1670 train_time:82084ms step_avg:88.26ms
step:931/1670 train_time:82172ms step_avg:88.26ms
step:932/1670 train_time:82261ms step_avg:88.26ms
step:933/1670 train_time:82349ms step_avg:88.26ms
step:934/1670 train_time:82439ms step_avg:88.26ms
step:935/1670 train_time:82529ms step_avg:88.27ms
step:936/1670 train_time:82619ms step_avg:88.27ms
step:937/1670 train_time:82707ms step_avg:88.27ms
step:938/1670 train_time:82796ms step_avg:88.27ms
step:939/1670 train_time:82885ms step_avg:88.27ms
step:940/1670 train_time:82973ms step_avg:88.27ms
step:941/1670 train_time:83062ms step_avg:88.27ms
step:942/1670 train_time:83151ms step_avg:88.27ms
step:943/1670 train_time:83240ms step_avg:88.27ms
step:944/1670 train_time:83328ms step_avg:88.27ms
step:945/1670 train_time:83417ms step_avg:88.27ms
step:946/1670 train_time:83507ms step_avg:88.27ms
step:947/1670 train_time:83597ms step_avg:88.28ms
step:948/1670 train_time:83686ms step_avg:88.28ms
step:949/1670 train_time:83774ms step_avg:88.28ms
step:950/1670 train_time:83864ms step_avg:88.28ms
step:951/1670 train_time:83951ms step_avg:88.28ms
step:952/1670 train_time:84040ms step_avg:88.28ms
step:953/1670 train_time:84129ms step_avg:88.28ms
step:954/1670 train_time:84218ms step_avg:88.28ms
step:955/1670 train_time:84307ms step_avg:88.28ms
step:956/1670 train_time:84395ms step_avg:88.28ms
step:957/1670 train_time:84485ms step_avg:88.28ms
step:958/1670 train_time:84573ms step_avg:88.28ms
step:959/1670 train_time:84662ms step_avg:88.28ms
step:960/1670 train_time:84751ms step_avg:88.28ms
step:961/1670 train_time:84841ms step_avg:88.28ms
step:962/1670 train_time:84929ms step_avg:88.28ms
step:963/1670 train_time:85018ms step_avg:88.28ms
step:964/1670 train_time:85107ms step_avg:88.29ms
step:965/1670 train_time:85195ms step_avg:88.29ms
step:966/1670 train_time:85285ms step_avg:88.29ms
step:967/1670 train_time:85374ms step_avg:88.29ms
step:968/1670 train_time:85463ms step_avg:88.29ms
step:969/1670 train_time:85552ms step_avg:88.29ms
step:970/1670 train_time:85641ms step_avg:88.29ms
step:971/1670 train_time:85729ms step_avg:88.29ms
step:972/1670 train_time:85818ms step_avg:88.29ms
step:973/1670 train_time:85907ms step_avg:88.29ms
step:974/1670 train_time:85996ms step_avg:88.29ms
step:975/1670 train_time:86085ms step_avg:88.29ms
step:976/1670 train_time:86173ms step_avg:88.29ms
step:977/1670 train_time:86262ms step_avg:88.29ms
step:978/1670 train_time:86351ms step_avg:88.29ms
step:979/1670 train_time:86440ms step_avg:88.29ms
step:980/1670 train_time:86529ms step_avg:88.29ms
step:981/1670 train_time:86618ms step_avg:88.30ms
step:982/1670 train_time:86708ms step_avg:88.30ms
step:983/1670 train_time:86798ms step_avg:88.30ms
step:984/1670 train_time:86887ms step_avg:88.30ms
step:985/1670 train_time:86975ms step_avg:88.30ms
step:986/1670 train_time:87065ms step_avg:88.30ms
step:987/1670 train_time:87152ms step_avg:88.30ms
step:988/1670 train_time:87241ms step_avg:88.30ms
step:989/1670 train_time:87331ms step_avg:88.30ms
step:990/1670 train_time:87419ms step_avg:88.30ms
step:991/1670 train_time:87509ms step_avg:88.30ms
step:992/1670 train_time:87599ms step_avg:88.31ms
step:993/1670 train_time:87688ms step_avg:88.31ms
step:994/1670 train_time:87777ms step_avg:88.31ms
step:995/1670 train_time:87867ms step_avg:88.31ms
step:996/1670 train_time:87955ms step_avg:88.31ms
step:997/1670 train_time:88045ms step_avg:88.31ms
step:998/1670 train_time:88133ms step_avg:88.31ms
step:999/1670 train_time:88222ms step_avg:88.31ms
step:1000/1670 train_time:88311ms step_avg:88.31ms
step:1000/1670 val_loss:3.4692 train_time:88401ms step_avg:88.40ms
step:1001/1670 train_time:88424ms step_avg:88.34ms
step:1002/1670 train_time:88493ms step_avg:88.32ms
step:1003/1670 train_time:88585ms step_avg:88.32ms
step:1004/1670 train_time:88673ms step_avg:88.32ms
step:1005/1670 train_time:88762ms step_avg:88.32ms
step:1006/1670 train_time:88849ms step_avg:88.32ms
step:1007/1670 train_time:88937ms step_avg:88.32ms
step:1008/1670 train_time:89024ms step_avg:88.32ms
step:1009/1670 train_time:89112ms step_avg:88.32ms
step:1010/1670 train_time:89201ms step_avg:88.32ms
step:1011/1670 train_time:89289ms step_avg:88.32ms
step:1012/1670 train_time:89379ms step_avg:88.32ms
step:1013/1670 train_time:89472ms step_avg:88.32ms
step:1014/1670 train_time:89564ms step_avg:88.33ms
step:1015/1670 train_time:89653ms step_avg:88.33ms
step:1016/1670 train_time:89744ms step_avg:88.33ms
step:1017/1670 train_time:89832ms step_avg:88.33ms
step:1018/1670 train_time:89920ms step_avg:88.33ms
step:1019/1670 train_time:90009ms step_avg:88.33ms
step:1020/1670 train_time:90096ms step_avg:88.33ms
step:1021/1670 train_time:90185ms step_avg:88.33ms
step:1022/1670 train_time:90272ms step_avg:88.33ms
step:1023/1670 train_time:90362ms step_avg:88.33ms
step:1024/1670 train_time:90452ms step_avg:88.33ms
step:1025/1670 train_time:90543ms step_avg:88.33ms
step:1026/1670 train_time:90633ms step_avg:88.34ms
step:1027/1670 train_time:90722ms step_avg:88.34ms
step:1028/1670 train_time:90811ms step_avg:88.34ms
step:1029/1670 train_time:90899ms step_avg:88.34ms
step:1030/1670 train_time:90988ms step_avg:88.34ms
step:1031/1670 train_time:91076ms step_avg:88.34ms
step:1032/1670 train_time:91166ms step_avg:88.34ms
step:1033/1670 train_time:91254ms step_avg:88.34ms
step:1034/1670 train_time:91343ms step_avg:88.34ms
step:1035/1670 train_time:91432ms step_avg:88.34ms
step:1036/1670 train_time:91522ms step_avg:88.34ms
step:1037/1670 train_time:91613ms step_avg:88.34ms
step:1038/1670 train_time:91702ms step_avg:88.34ms
step:1039/1670 train_time:91791ms step_avg:88.35ms
step:1040/1670 train_time:91879ms step_avg:88.35ms
step:1041/1670 train_time:91968ms step_avg:88.35ms
step:1042/1670 train_time:92056ms step_avg:88.35ms
step:1043/1670 train_time:92145ms step_avg:88.35ms
step:1044/1670 train_time:92234ms step_avg:88.35ms
step:1045/1670 train_time:92322ms step_avg:88.35ms
step:1046/1670 train_time:92411ms step_avg:88.35ms
step:1047/1670 train_time:92501ms step_avg:88.35ms
step:1048/1670 train_time:92591ms step_avg:88.35ms
step:1049/1670 train_time:92681ms step_avg:88.35ms
step:1050/1670 train_time:92770ms step_avg:88.35ms
step:1051/1670 train_time:92859ms step_avg:88.35ms
step:1052/1670 train_time:92947ms step_avg:88.35ms
step:1053/1670 train_time:93035ms step_avg:88.35ms
step:1054/1670 train_time:93123ms step_avg:88.35ms
step:1055/1670 train_time:93212ms step_avg:88.35ms
step:1056/1670 train_time:93301ms step_avg:88.35ms
step:1057/1670 train_time:93391ms step_avg:88.35ms
step:1058/1670 train_time:93480ms step_avg:88.36ms
step:1059/1670 train_time:93571ms step_avg:88.36ms
step:1060/1670 train_time:93660ms step_avg:88.36ms
step:1061/1670 train_time:93749ms step_avg:88.36ms
step:1062/1670 train_time:93837ms step_avg:88.36ms
step:1063/1670 train_time:93927ms step_avg:88.36ms
step:1064/1670 train_time:94015ms step_avg:88.36ms
step:1065/1670 train_time:94103ms step_avg:88.36ms
step:1066/1670 train_time:94192ms step_avg:88.36ms
step:1067/1670 train_time:94280ms step_avg:88.36ms
step:1068/1670 train_time:94369ms step_avg:88.36ms
step:1069/1670 train_time:94459ms step_avg:88.36ms
step:1070/1670 train_time:94549ms step_avg:88.36ms
step:1071/1670 train_time:94638ms step_avg:88.36ms
step:1072/1670 train_time:94727ms step_avg:88.37ms
step:1073/1670 train_time:94816ms step_avg:88.37ms
step:1074/1670 train_time:94906ms step_avg:88.37ms
step:1075/1670 train_time:94994ms step_avg:88.37ms
step:1076/1670 train_time:95083ms step_avg:88.37ms
step:1077/1670 train_time:95172ms step_avg:88.37ms
step:1078/1670 train_time:95261ms step_avg:88.37ms
step:1079/1670 train_time:95350ms step_avg:88.37ms
step:1080/1670 train_time:95439ms step_avg:88.37ms
step:1081/1670 train_time:95528ms step_avg:88.37ms
step:1082/1670 train_time:95617ms step_avg:88.37ms
step:1083/1670 train_time:95706ms step_avg:88.37ms
step:1084/1670 train_time:95795ms step_avg:88.37ms
step:1085/1670 train_time:95883ms step_avg:88.37ms
step:1086/1670 train_time:95971ms step_avg:88.37ms
step:1087/1670 train_time:96060ms step_avg:88.37ms
step:1088/1670 train_time:96150ms step_avg:88.37ms
step:1089/1670 train_time:96238ms step_avg:88.37ms
step:1090/1670 train_time:96328ms step_avg:88.37ms
step:1091/1670 train_time:96418ms step_avg:88.38ms
step:1092/1670 train_time:96508ms step_avg:88.38ms
step:1093/1670 train_time:96597ms step_avg:88.38ms
step:1094/1670 train_time:96687ms step_avg:88.38ms
step:1095/1670 train_time:96776ms step_avg:88.38ms
step:1096/1670 train_time:96867ms step_avg:88.38ms
step:1097/1670 train_time:96955ms step_avg:88.38ms
step:1098/1670 train_time:97045ms step_avg:88.38ms
step:1099/1670 train_time:97134ms step_avg:88.38ms
step:1100/1670 train_time:97224ms step_avg:88.39ms
step:1101/1670 train_time:97313ms step_avg:88.39ms
step:1102/1670 train_time:97403ms step_avg:88.39ms
step:1103/1670 train_time:97491ms step_avg:88.39ms
step:1104/1670 train_time:97581ms step_avg:88.39ms
step:1105/1670 train_time:97671ms step_avg:88.39ms
step:1106/1670 train_time:97762ms step_avg:88.39ms
step:1107/1670 train_time:97851ms step_avg:88.39ms
step:1108/1670 train_time:97940ms step_avg:88.39ms
step:1109/1670 train_time:98030ms step_avg:88.40ms
step:1110/1670 train_time:98120ms step_avg:88.40ms
step:1111/1670 train_time:98210ms step_avg:88.40ms
step:1112/1670 train_time:98300ms step_avg:88.40ms
step:1113/1670 train_time:98390ms step_avg:88.40ms
step:1114/1670 train_time:98480ms step_avg:88.40ms
step:1115/1670 train_time:98570ms step_avg:88.40ms
step:1116/1670 train_time:98660ms step_avg:88.40ms
step:1117/1670 train_time:98750ms step_avg:88.41ms
step:1118/1670 train_time:98840ms step_avg:88.41ms
step:1119/1670 train_time:98930ms step_avg:88.41ms
step:1120/1670 train_time:99020ms step_avg:88.41ms
step:1121/1670 train_time:99109ms step_avg:88.41ms
step:1122/1670 train_time:99199ms step_avg:88.41ms
step:1123/1670 train_time:99290ms step_avg:88.41ms
step:1124/1670 train_time:99380ms step_avg:88.42ms
step:1125/1670 train_time:99470ms step_avg:88.42ms
step:1125/1670 val_loss:3.4150 train_time:99561ms step_avg:88.50ms
step:1126/1670 train_time:99585ms step_avg:88.44ms
step:1127/1670 train_time:99652ms step_avg:88.42ms
step:1128/1670 train_time:99742ms step_avg:88.42ms
step:1129/1670 train_time:99833ms step_avg:88.43ms
step:1130/1670 train_time:99922ms step_avg:88.43ms
step:1131/1670 train_time:100010ms step_avg:88.43ms
step:1132/1670 train_time:100099ms step_avg:88.43ms
step:1133/1670 train_time:100188ms step_avg:88.43ms
step:1134/1670 train_time:100276ms step_avg:88.43ms
step:1135/1670 train_time:100366ms step_avg:88.43ms
step:1136/1670 train_time:100457ms step_avg:88.43ms
step:1137/1670 train_time:100551ms step_avg:88.44ms
step:1138/1670 train_time:100641ms step_avg:88.44ms
step:1139/1670 train_time:100733ms step_avg:88.44ms
step:1140/1670 train_time:100823ms step_avg:88.44ms
step:1141/1670 train_time:100912ms step_avg:88.44ms
step:1142/1670 train_time:101001ms step_avg:88.44ms
step:1143/1670 train_time:101090ms step_avg:88.44ms
step:1144/1670 train_time:101178ms step_avg:88.44ms
step:1145/1670 train_time:101267ms step_avg:88.44ms
step:1146/1670 train_time:101355ms step_avg:88.44ms
step:1147/1670 train_time:101447ms step_avg:88.45ms
step:1148/1670 train_time:101538ms step_avg:88.45ms
step:1149/1670 train_time:101629ms step_avg:88.45ms
step:1150/1670 train_time:101721ms step_avg:88.45ms
step:1151/1670 train_time:101812ms step_avg:88.46ms
step:1152/1670 train_time:101902ms step_avg:88.46ms
step:1153/1670 train_time:101991ms step_avg:88.46ms
step:1154/1670 train_time:102080ms step_avg:88.46ms
step:1155/1670 train_time:102169ms step_avg:88.46ms
step:1156/1670 train_time:102258ms step_avg:88.46ms
step:1157/1670 train_time:102347ms step_avg:88.46ms
step:1158/1670 train_time:102437ms step_avg:88.46ms
step:1159/1670 train_time:102528ms step_avg:88.46ms
step:1160/1670 train_time:102617ms step_avg:88.46ms
step:1161/1670 train_time:102708ms step_avg:88.47ms
step:1162/1670 train_time:102798ms step_avg:88.47ms
step:1163/1670 train_time:102889ms step_avg:88.47ms
step:1164/1670 train_time:102978ms step_avg:88.47ms
step:1165/1670 train_time:103067ms step_avg:88.47ms
step:1166/1670 train_time:103156ms step_avg:88.47ms
step:1167/1670 train_time:103244ms step_avg:88.47ms
step:1168/1670 train_time:103333ms step_avg:88.47ms
step:1169/1670 train_time:103423ms step_avg:88.47ms
step:1170/1670 train_time:103513ms step_avg:88.47ms
step:1171/1670 train_time:103603ms step_avg:88.47ms
step:1172/1670 train_time:103693ms step_avg:88.48ms
step:1173/1670 train_time:103784ms step_avg:88.48ms
step:1174/1670 train_time:103873ms step_avg:88.48ms
step:1175/1670 train_time:103963ms step_avg:88.48ms
step:1176/1670 train_time:104053ms step_avg:88.48ms
step:1177/1670 train_time:104142ms step_avg:88.48ms
step:1178/1670 train_time:104231ms step_avg:88.48ms
step:1179/1670 train_time:104320ms step_avg:88.48ms
step:1180/1670 train_time:104410ms step_avg:88.48ms
step:1181/1670 train_time:104500ms step_avg:88.48ms
step:1182/1670 train_time:104590ms step_avg:88.49ms
step:1183/1670 train_time:104680ms step_avg:88.49ms
step:1184/1670 train_time:104771ms step_avg:88.49ms
step:1185/1670 train_time:104862ms step_avg:88.49ms
step:1186/1670 train_time:104951ms step_avg:88.49ms
step:1187/1670 train_time:105040ms step_avg:88.49ms
step:1188/1670 train_time:105131ms step_avg:88.49ms
step:1189/1670 train_time:105220ms step_avg:88.49ms
step:1190/1670 train_time:105309ms step_avg:88.50ms
step:1191/1670 train_time:105399ms step_avg:88.50ms
step:1192/1670 train_time:105490ms step_avg:88.50ms
step:1193/1670 train_time:105581ms step_avg:88.50ms
step:1194/1670 train_time:105671ms step_avg:88.50ms
step:1195/1670 train_time:105761ms step_avg:88.50ms
step:1196/1670 train_time:105851ms step_avg:88.50ms
step:1197/1670 train_time:105940ms step_avg:88.50ms
step:1198/1670 train_time:106030ms step_avg:88.51ms
step:1199/1670 train_time:106119ms step_avg:88.51ms
step:1200/1670 train_time:106209ms step_avg:88.51ms
step:1201/1670 train_time:106298ms step_avg:88.51ms
step:1202/1670 train_time:106389ms step_avg:88.51ms
step:1203/1670 train_time:106478ms step_avg:88.51ms
step:1204/1670 train_time:106567ms step_avg:88.51ms
step:1205/1670 train_time:106657ms step_avg:88.51ms
step:1206/1670 train_time:106747ms step_avg:88.51ms
step:1207/1670 train_time:106837ms step_avg:88.51ms
step:1208/1670 train_time:106929ms step_avg:88.52ms
step:1209/1670 train_time:107019ms step_avg:88.52ms
step:1210/1670 train_time:107108ms step_avg:88.52ms
step:1211/1670 train_time:107198ms step_avg:88.52ms
step:1212/1670 train_time:107288ms step_avg:88.52ms
step:1213/1670 train_time:107377ms step_avg:88.52ms
step:1214/1670 train_time:107467ms step_avg:88.52ms
step:1215/1670 train_time:107556ms step_avg:88.52ms
step:1216/1670 train_time:107646ms step_avg:88.52ms
step:1217/1670 train_time:107735ms step_avg:88.53ms
step:1218/1670 train_time:107825ms step_avg:88.53ms
step:1219/1670 train_time:107915ms step_avg:88.53ms
step:1220/1670 train_time:108004ms step_avg:88.53ms
step:1221/1670 train_time:108093ms step_avg:88.53ms
step:1222/1670 train_time:108183ms step_avg:88.53ms
step:1223/1670 train_time:108272ms step_avg:88.53ms
step:1224/1670 train_time:108362ms step_avg:88.53ms
step:1225/1670 train_time:108452ms step_avg:88.53ms
step:1226/1670 train_time:108541ms step_avg:88.53ms
step:1227/1670 train_time:108630ms step_avg:88.53ms
step:1228/1670 train_time:108720ms step_avg:88.53ms
step:1229/1670 train_time:108811ms step_avg:88.54ms
step:1230/1670 train_time:108901ms step_avg:88.54ms
step:1231/1670 train_time:108990ms step_avg:88.54ms
step:1232/1670 train_time:109080ms step_avg:88.54ms
step:1233/1670 train_time:109170ms step_avg:88.54ms
step:1234/1670 train_time:109260ms step_avg:88.54ms
step:1235/1670 train_time:109350ms step_avg:88.54ms
step:1236/1670 train_time:109439ms step_avg:88.54ms
step:1237/1670 train_time:109530ms step_avg:88.54ms
step:1238/1670 train_time:109619ms step_avg:88.55ms
step:1239/1670 train_time:109709ms step_avg:88.55ms
step:1240/1670 train_time:109798ms step_avg:88.55ms
step:1241/1670 train_time:109889ms step_avg:88.55ms
step:1242/1670 train_time:109978ms step_avg:88.55ms
step:1243/1670 train_time:110068ms step_avg:88.55ms
step:1244/1670 train_time:110157ms step_avg:88.55ms
step:1245/1670 train_time:110247ms step_avg:88.55ms
step:1246/1670 train_time:110336ms step_avg:88.55ms
step:1247/1670 train_time:110426ms step_avg:88.55ms
step:1248/1670 train_time:110515ms step_avg:88.55ms
step:1249/1670 train_time:110604ms step_avg:88.55ms
step:1250/1670 train_time:110694ms step_avg:88.56ms
step:1250/1670 val_loss:3.3760 train_time:110785ms step_avg:88.63ms
step:1251/1670 train_time:110805ms step_avg:88.57ms
step:1252/1670 train_time:110880ms step_avg:88.56ms
step:1253/1670 train_time:110970ms step_avg:88.56ms
step:1254/1670 train_time:111062ms step_avg:88.57ms
step:1255/1670 train_time:111152ms step_avg:88.57ms
step:1256/1670 train_time:111240ms step_avg:88.57ms
step:1257/1670 train_time:111329ms step_avg:88.57ms
step:1258/1670 train_time:111420ms step_avg:88.57ms
step:1259/1670 train_time:111509ms step_avg:88.57ms
step:1260/1670 train_time:111599ms step_avg:88.57ms
step:1261/1670 train_time:111688ms step_avg:88.57ms
step:1262/1670 train_time:111779ms step_avg:88.57ms
step:1263/1670 train_time:111870ms step_avg:88.57ms
step:1264/1670 train_time:111961ms step_avg:88.58ms
step:1265/1670 train_time:112052ms step_avg:88.58ms
step:1266/1670 train_time:112142ms step_avg:88.58ms
step:1267/1670 train_time:112232ms step_avg:88.58ms
step:1268/1670 train_time:112321ms step_avg:88.58ms
step:1269/1670 train_time:112410ms step_avg:88.58ms
step:1270/1670 train_time:112499ms step_avg:88.58ms
step:1271/1670 train_time:112588ms step_avg:88.58ms
step:1272/1670 train_time:112678ms step_avg:88.58ms
step:1273/1670 train_time:112768ms step_avg:88.58ms
step:1274/1670 train_time:112860ms step_avg:88.59ms
step:1275/1670 train_time:112951ms step_avg:88.59ms
step:1276/1670 train_time:113041ms step_avg:88.59ms
step:1277/1670 train_time:113132ms step_avg:88.59ms
step:1278/1670 train_time:113222ms step_avg:88.59ms
step:1279/1670 train_time:113312ms step_avg:88.59ms
step:1280/1670 train_time:113402ms step_avg:88.59ms
step:1281/1670 train_time:113491ms step_avg:88.60ms
step:1282/1670 train_time:113581ms step_avg:88.60ms
step:1283/1670 train_time:113671ms step_avg:88.60ms
step:1284/1670 train_time:113760ms step_avg:88.60ms
step:1285/1670 train_time:113851ms step_avg:88.60ms
step:1286/1670 train_time:113940ms step_avg:88.60ms
step:1287/1670 train_time:114031ms step_avg:88.60ms
step:1288/1670 train_time:114121ms step_avg:88.60ms
step:1289/1670 train_time:114212ms step_avg:88.61ms
step:1290/1670 train_time:114301ms step_avg:88.61ms
step:1291/1670 train_time:114391ms step_avg:88.61ms
step:1292/1670 train_time:114480ms step_avg:88.61ms
step:1293/1670 train_time:114570ms step_avg:88.61ms
step:1294/1670 train_time:114660ms step_avg:88.61ms
step:1295/1670 train_time:114750ms step_avg:88.61ms
step:1296/1670 train_time:114841ms step_avg:88.61ms
step:1297/1670 train_time:114931ms step_avg:88.61ms
step:1298/1670 train_time:115021ms step_avg:88.61ms
step:1299/1670 train_time:115111ms step_avg:88.61ms
step:1300/1670 train_time:115201ms step_avg:88.62ms
step:1301/1670 train_time:115292ms step_avg:88.62ms
step:1302/1670 train_time:115381ms step_avg:88.62ms
step:1303/1670 train_time:115470ms step_avg:88.62ms
step:1304/1670 train_time:115559ms step_avg:88.62ms
step:1305/1670 train_time:115649ms step_avg:88.62ms
step:1306/1670 train_time:115739ms step_avg:88.62ms
step:1307/1670 train_time:115829ms step_avg:88.62ms
step:1308/1670 train_time:115920ms step_avg:88.62ms
step:1309/1670 train_time:116009ms step_avg:88.62ms
step:1310/1670 train_time:116100ms step_avg:88.63ms
step:1311/1670 train_time:116191ms step_avg:88.63ms
step:1312/1670 train_time:116281ms step_avg:88.63ms
step:1313/1670 train_time:116371ms step_avg:88.63ms
step:1314/1670 train_time:116461ms step_avg:88.63ms
step:1315/1670 train_time:116551ms step_avg:88.63ms
step:1316/1670 train_time:116640ms step_avg:88.63ms
step:1317/1670 train_time:116730ms step_avg:88.63ms
step:1318/1670 train_time:116821ms step_avg:88.64ms
step:1319/1670 train_time:116912ms step_avg:88.64ms
step:1320/1670 train_time:117002ms step_avg:88.64ms
step:1321/1670 train_time:117092ms step_avg:88.64ms
step:1322/1670 train_time:117181ms step_avg:88.64ms
step:1323/1670 train_time:117271ms step_avg:88.64ms
step:1324/1670 train_time:117361ms step_avg:88.64ms
step:1325/1670 train_time:117451ms step_avg:88.64ms
step:1326/1670 train_time:117541ms step_avg:88.64ms
step:1327/1670 train_time:117631ms step_avg:88.64ms
step:1328/1670 train_time:117721ms step_avg:88.65ms
step:1329/1670 train_time:117811ms step_avg:88.65ms
step:1330/1670 train_time:117901ms step_avg:88.65ms
step:1331/1670 train_time:117990ms step_avg:88.65ms
step:1332/1670 train_time:118080ms step_avg:88.65ms
step:1333/1670 train_time:118170ms step_avg:88.65ms
step:1334/1670 train_time:118260ms step_avg:88.65ms
step:1335/1670 train_time:118350ms step_avg:88.65ms
step:1336/1670 train_time:118442ms step_avg:88.65ms
step:1337/1670 train_time:118531ms step_avg:88.65ms
step:1338/1670 train_time:118621ms step_avg:88.66ms
step:1339/1670 train_time:118711ms step_avg:88.66ms
step:1340/1670 train_time:118800ms step_avg:88.66ms
step:1341/1670 train_time:118889ms step_avg:88.66ms
step:1342/1670 train_time:118979ms step_avg:88.66ms
step:1343/1670 train_time:119069ms step_avg:88.66ms
step:1344/1670 train_time:119159ms step_avg:88.66ms
step:1345/1670 train_time:119249ms step_avg:88.66ms
step:1346/1670 train_time:119339ms step_avg:88.66ms
step:1347/1670 train_time:119430ms step_avg:88.66ms
step:1348/1670 train_time:119522ms step_avg:88.67ms
step:1349/1670 train_time:119612ms step_avg:88.67ms
step:1350/1670 train_time:119702ms step_avg:88.67ms
step:1351/1670 train_time:119792ms step_avg:88.67ms
step:1352/1670 train_time:119881ms step_avg:88.67ms
step:1353/1670 train_time:119971ms step_avg:88.67ms
step:1354/1670 train_time:120061ms step_avg:88.67ms
step:1355/1670 train_time:120150ms step_avg:88.67ms
step:1356/1670 train_time:120240ms step_avg:88.67ms
step:1357/1670 train_time:120330ms step_avg:88.67ms
step:1358/1670 train_time:120421ms step_avg:88.68ms
step:1359/1670 train_time:120511ms step_avg:88.68ms
step:1360/1670 train_time:120600ms step_avg:88.68ms
step:1361/1670 train_time:120690ms step_avg:88.68ms
step:1362/1670 train_time:120779ms step_avg:88.68ms
step:1363/1670 train_time:120869ms step_avg:88.68ms
step:1364/1670 train_time:120958ms step_avg:88.68ms
step:1365/1670 train_time:121048ms step_avg:88.68ms
step:1366/1670 train_time:121138ms step_avg:88.68ms
step:1367/1670 train_time:121227ms step_avg:88.68ms
step:1368/1670 train_time:121317ms step_avg:88.68ms
step:1369/1670 train_time:121407ms step_avg:88.68ms
step:1370/1670 train_time:121498ms step_avg:88.68ms
step:1371/1670 train_time:121589ms step_avg:88.69ms
step:1372/1670 train_time:121678ms step_avg:88.69ms
step:1373/1670 train_time:121768ms step_avg:88.69ms
step:1374/1670 train_time:121859ms step_avg:88.69ms
step:1375/1670 train_time:121948ms step_avg:88.69ms
step:1375/1670 val_loss:3.3413 train_time:122040ms step_avg:88.76ms
step:1376/1670 train_time:122059ms step_avg:88.71ms
step:1377/1670 train_time:122133ms step_avg:88.69ms
step:1378/1670 train_time:122225ms step_avg:88.70ms
step:1379/1670 train_time:122314ms step_avg:88.70ms
step:1380/1670 train_time:122403ms step_avg:88.70ms
step:1381/1670 train_time:122492ms step_avg:88.70ms
step:1382/1670 train_time:122580ms step_avg:88.70ms
step:1383/1670 train_time:122669ms step_avg:88.70ms
step:1384/1670 train_time:122758ms step_avg:88.70ms
step:1385/1670 train_time:122847ms step_avg:88.70ms
step:1386/1670 train_time:122936ms step_avg:88.70ms
step:1387/1670 train_time:123028ms step_avg:88.70ms
step:1388/1670 train_time:123119ms step_avg:88.70ms
step:1389/1670 train_time:123210ms step_avg:88.70ms
step:1390/1670 train_time:123299ms step_avg:88.70ms
step:1391/1670 train_time:123389ms step_avg:88.71ms
step:1392/1670 train_time:123479ms step_avg:88.71ms
step:1393/1670 train_time:123568ms step_avg:88.71ms
step:1394/1670 train_time:123656ms step_avg:88.71ms
step:1395/1670 train_time:123746ms step_avg:88.71ms
step:1396/1670 train_time:123835ms step_avg:88.71ms
step:1397/1670 train_time:123925ms step_avg:88.71ms
step:1398/1670 train_time:124016ms step_avg:88.71ms
step:1399/1670 train_time:124109ms step_avg:88.71ms
step:1400/1670 train_time:124198ms step_avg:88.71ms
step:1401/1670 train_time:124289ms step_avg:88.71ms
step:1402/1670 train_time:124378ms step_avg:88.71ms
step:1403/1670 train_time:124468ms step_avg:88.72ms
step:1404/1670 train_time:124557ms step_avg:88.72ms
step:1405/1670 train_time:124646ms step_avg:88.72ms
step:1406/1670 train_time:124735ms step_avg:88.72ms
step:1407/1670 train_time:124824ms step_avg:88.72ms
step:1408/1670 train_time:124915ms step_avg:88.72ms
step:1409/1670 train_time:125005ms step_avg:88.72ms
step:1410/1670 train_time:125096ms step_avg:88.72ms
step:1411/1670 train_time:125187ms step_avg:88.72ms
step:1412/1670 train_time:125277ms step_avg:88.72ms
step:1413/1670 train_time:125368ms step_avg:88.72ms
step:1414/1670 train_time:125458ms step_avg:88.73ms
step:1415/1670 train_time:125548ms step_avg:88.73ms
step:1416/1670 train_time:125637ms step_avg:88.73ms
step:1417/1670 train_time:125728ms step_avg:88.73ms
step:1418/1670 train_time:125816ms step_avg:88.73ms
step:1419/1670 train_time:125905ms step_avg:88.73ms
step:1420/1670 train_time:125996ms step_avg:88.73ms
step:1421/1670 train_time:126086ms step_avg:88.73ms
step:1422/1670 train_time:126176ms step_avg:88.73ms
step:1423/1670 train_time:126267ms step_avg:88.73ms
step:1424/1670 train_time:126358ms step_avg:88.73ms
step:1425/1670 train_time:126448ms step_avg:88.74ms
step:1426/1670 train_time:126538ms step_avg:88.74ms
step:1427/1670 train_time:126627ms step_avg:88.74ms
step:1428/1670 train_time:126716ms step_avg:88.74ms
step:1429/1670 train_time:126806ms step_avg:88.74ms
step:1430/1670 train_time:126895ms step_avg:88.74ms
step:1431/1670 train_time:126985ms step_avg:88.74ms
step:1432/1670 train_time:127074ms step_avg:88.74ms
step:1433/1670 train_time:127165ms step_avg:88.74ms
step:1434/1670 train_time:127255ms step_avg:88.74ms
step:1435/1670 train_time:127346ms step_avg:88.74ms
step:1436/1670 train_time:127436ms step_avg:88.74ms
step:1437/1670 train_time:127527ms step_avg:88.75ms
step:1438/1670 train_time:127616ms step_avg:88.75ms
step:1439/1670 train_time:127706ms step_avg:88.75ms
step:1440/1670 train_time:127795ms step_avg:88.75ms
step:1441/1670 train_time:127884ms step_avg:88.75ms
step:1442/1670 train_time:127975ms step_avg:88.75ms
step:1443/1670 train_time:128065ms step_avg:88.75ms
step:1444/1670 train_time:128156ms step_avg:88.75ms
step:1445/1670 train_time:128246ms step_avg:88.75ms
step:1446/1670 train_time:128336ms step_avg:88.75ms
step:1447/1670 train_time:128427ms step_avg:88.75ms
step:1448/1670 train_time:128517ms step_avg:88.75ms
step:1449/1670 train_time:128606ms step_avg:88.76ms
step:1450/1670 train_time:128696ms step_avg:88.76ms
step:1451/1670 train_time:128785ms step_avg:88.76ms
step:1452/1670 train_time:128874ms step_avg:88.76ms
step:1453/1670 train_time:128964ms step_avg:88.76ms
step:1454/1670 train_time:129054ms step_avg:88.76ms
step:1455/1670 train_time:129145ms step_avg:88.76ms
step:1456/1670 train_time:129234ms step_avg:88.76ms
step:1457/1670 train_time:129324ms step_avg:88.76ms
step:1458/1670 train_time:129415ms step_avg:88.76ms
step:1459/1670 train_time:129505ms step_avg:88.76ms
step:1460/1670 train_time:129595ms step_avg:88.76ms
step:1461/1670 train_time:129685ms step_avg:88.76ms
step:1462/1670 train_time:129774ms step_avg:88.76ms
step:1463/1670 train_time:129864ms step_avg:88.77ms
step:1464/1670 train_time:129954ms step_avg:88.77ms
step:1465/1670 train_time:130044ms step_avg:88.77ms
step:1466/1670 train_time:130133ms step_avg:88.77ms
step:1467/1670 train_time:130223ms step_avg:88.77ms
step:1468/1670 train_time:130315ms step_avg:88.77ms
step:1469/1670 train_time:130405ms step_avg:88.77ms
step:1470/1670 train_time:130495ms step_avg:88.77ms
step:1471/1670 train_time:130586ms step_avg:88.77ms
step:1472/1670 train_time:130675ms step_avg:88.77ms
step:1473/1670 train_time:130765ms step_avg:88.77ms
step:1474/1670 train_time:130855ms step_avg:88.78ms
step:1475/1670 train_time:130944ms step_avg:88.78ms
step:1476/1670 train_time:131034ms step_avg:88.78ms
step:1477/1670 train_time:131124ms step_avg:88.78ms
step:1478/1670 train_time:131214ms step_avg:88.78ms
step:1479/1670 train_time:131304ms step_avg:88.78ms
step:1480/1670 train_time:131395ms step_avg:88.78ms
step:1481/1670 train_time:131485ms step_avg:88.78ms
step:1482/1670 train_time:131576ms step_avg:88.78ms
step:1483/1670 train_time:131666ms step_avg:88.78ms
step:1484/1670 train_time:131757ms step_avg:88.79ms
step:1485/1670 train_time:131847ms step_avg:88.79ms
step:1486/1670 train_time:131937ms step_avg:88.79ms
step:1487/1670 train_time:132027ms step_avg:88.79ms
step:1488/1670 train_time:132115ms step_avg:88.79ms
step:1489/1670 train_time:132205ms step_avg:88.79ms
step:1490/1670 train_time:132295ms step_avg:88.79ms
step:1491/1670 train_time:132385ms step_avg:88.79ms
step:1492/1670 train_time:132474ms step_avg:88.79ms
step:1493/1670 train_time:132564ms step_avg:88.79ms
step:1494/1670 train_time:132654ms step_avg:88.79ms
step:1495/1670 train_time:132744ms step_avg:88.79ms
step:1496/1670 train_time:132834ms step_avg:88.79ms
step:1497/1670 train_time:132924ms step_avg:88.79ms
step:1498/1670 train_time:133014ms step_avg:88.79ms
step:1499/1670 train_time:133103ms step_avg:88.79ms
step:1500/1670 train_time:133193ms step_avg:88.80ms
step:1500/1670 val_loss:3.3118 train_time:133284ms step_avg:88.86ms
step:1501/1670 train_time:133304ms step_avg:88.81ms
step:1502/1670 train_time:133376ms step_avg:88.80ms
step:1503/1670 train_time:133469ms step_avg:88.80ms
step:1504/1670 train_time:133558ms step_avg:88.80ms
step:1505/1670 train_time:133647ms step_avg:88.80ms
step:1506/1670 train_time:133735ms step_avg:88.80ms
step:1507/1670 train_time:133824ms step_avg:88.80ms
step:1508/1670 train_time:133913ms step_avg:88.80ms
step:1509/1670 train_time:134002ms step_avg:88.80ms
step:1510/1670 train_time:134091ms step_avg:88.80ms
step:1511/1670 train_time:134181ms step_avg:88.80ms
step:1512/1670 train_time:134273ms step_avg:88.80ms
step:1513/1670 train_time:134365ms step_avg:88.81ms
step:1514/1670 train_time:134456ms step_avg:88.81ms
step:1515/1670 train_time:134547ms step_avg:88.81ms
step:1516/1670 train_time:134636ms step_avg:88.81ms
step:1517/1670 train_time:134725ms step_avg:88.81ms
step:1518/1670 train_time:134813ms step_avg:88.81ms
step:1519/1670 train_time:134903ms step_avg:88.81ms
step:1520/1670 train_time:134992ms step_avg:88.81ms
step:1521/1670 train_time:135083ms step_avg:88.81ms
step:1522/1670 train_time:135172ms step_avg:88.81ms
step:1523/1670 train_time:135263ms step_avg:88.81ms
step:1524/1670 train_time:135353ms step_avg:88.81ms
step:1525/1670 train_time:135443ms step_avg:88.82ms
step:1526/1670 train_time:135533ms step_avg:88.82ms
step:1527/1670 train_time:135624ms step_avg:88.82ms
step:1528/1670 train_time:135713ms step_avg:88.82ms
step:1529/1670 train_time:135803ms step_avg:88.82ms
step:1530/1670 train_time:135892ms step_avg:88.82ms
step:1531/1670 train_time:135982ms step_avg:88.82ms
step:1532/1670 train_time:136072ms step_avg:88.82ms
step:1533/1670 train_time:136162ms step_avg:88.82ms
step:1534/1670 train_time:136252ms step_avg:88.82ms
step:1535/1670 train_time:136342ms step_avg:88.82ms
step:1536/1670 train_time:136431ms step_avg:88.82ms
step:1537/1670 train_time:136522ms step_avg:88.82ms
step:1538/1670 train_time:136611ms step_avg:88.82ms
step:1539/1670 train_time:136702ms step_avg:88.82ms
step:1540/1670 train_time:136791ms step_avg:88.83ms
step:1541/1670 train_time:136880ms step_avg:88.83ms
step:1542/1670 train_time:136970ms step_avg:88.83ms
step:1543/1670 train_time:137060ms step_avg:88.83ms
step:1544/1670 train_time:137149ms step_avg:88.83ms
step:1545/1670 train_time:137239ms step_avg:88.83ms
step:1546/1670 train_time:137329ms step_avg:88.83ms
step:1547/1670 train_time:137419ms step_avg:88.83ms
step:1548/1670 train_time:137510ms step_avg:88.83ms
step:1549/1670 train_time:137600ms step_avg:88.83ms
step:1550/1670 train_time:137690ms step_avg:88.83ms
step:1551/1670 train_time:137780ms step_avg:88.83ms
step:1552/1670 train_time:137869ms step_avg:88.83ms
step:1553/1670 train_time:137958ms step_avg:88.83ms
step:1554/1670 train_time:138048ms step_avg:88.83ms
step:1555/1670 train_time:138138ms step_avg:88.83ms
step:1556/1670 train_time:138228ms step_avg:88.84ms
step:1557/1670 train_time:138318ms step_avg:88.84ms
step:1558/1670 train_time:138409ms step_avg:88.84ms
step:1559/1670 train_time:138499ms step_avg:88.84ms
step:1560/1670 train_time:138589ms step_avg:88.84ms
step:1561/1670 train_time:138679ms step_avg:88.84ms
step:1562/1670 train_time:138769ms step_avg:88.84ms
step:1563/1670 train_time:138859ms step_avg:88.84ms
step:1564/1670 train_time:138948ms step_avg:88.84ms
step:1565/1670 train_time:139038ms step_avg:88.84ms
step:1566/1670 train_time:139127ms step_avg:88.84ms
step:1567/1670 train_time:139217ms step_avg:88.84ms
step:1568/1670 train_time:139308ms step_avg:88.84ms
step:1569/1670 train_time:139399ms step_avg:88.85ms
step:1570/1670 train_time:139489ms step_avg:88.85ms
step:1571/1670 train_time:139579ms step_avg:88.85ms
step:1572/1670 train_time:139669ms step_avg:88.85ms
step:1573/1670 train_time:139759ms step_avg:88.85ms
step:1574/1670 train_time:139848ms step_avg:88.85ms
step:1575/1670 train_time:139937ms step_avg:88.85ms
step:1576/1670 train_time:140028ms step_avg:88.85ms
step:1577/1670 train_time:140118ms step_avg:88.85ms
step:1578/1670 train_time:140208ms step_avg:88.85ms
step:1579/1670 train_time:140298ms step_avg:88.85ms
step:1580/1670 train_time:140388ms step_avg:88.85ms
step:1581/1670 train_time:140478ms step_avg:88.85ms
step:1582/1670 train_time:140569ms step_avg:88.86ms
step:1583/1670 train_time:140659ms step_avg:88.86ms
step:1584/1670 train_time:140749ms step_avg:88.86ms
step:1585/1670 train_time:140838ms step_avg:88.86ms
step:1586/1670 train_time:140928ms step_avg:88.86ms
step:1587/1670 train_time:141017ms step_avg:88.86ms
step:1588/1670 train_time:141108ms step_avg:88.86ms
step:1589/1670 train_time:141199ms step_avg:88.86ms
step:1590/1670 train_time:141290ms step_avg:88.86ms
step:1591/1670 train_time:141381ms step_avg:88.86ms
step:1592/1670 train_time:141469ms step_avg:88.86ms
step:1593/1670 train_time:141559ms step_avg:88.86ms
step:1594/1670 train_time:141649ms step_avg:88.86ms
step:1595/1670 train_time:141739ms step_avg:88.86ms
step:1596/1670 train_time:141829ms step_avg:88.87ms
step:1597/1670 train_time:141918ms step_avg:88.87ms
step:1598/1670 train_time:142007ms step_avg:88.87ms
step:1599/1670 train_time:142097ms step_avg:88.87ms
step:1600/1670 train_time:142188ms step_avg:88.87ms
step:1601/1670 train_time:142279ms step_avg:88.87ms
step:1602/1670 train_time:142367ms step_avg:88.87ms
step:1603/1670 train_time:142457ms step_avg:88.87ms
step:1604/1670 train_time:142546ms step_avg:88.87ms
step:1605/1670 train_time:142635ms step_avg:88.87ms
step:1606/1670 train_time:142725ms step_avg:88.87ms
step:1607/1670 train_time:142815ms step_avg:88.87ms
step:1608/1670 train_time:142905ms step_avg:88.87ms
step:1609/1670 train_time:142994ms step_avg:88.87ms
step:1610/1670 train_time:143085ms step_avg:88.87ms
step:1611/1670 train_time:143174ms step_avg:88.87ms
step:1612/1670 train_time:143264ms step_avg:88.87ms
step:1613/1670 train_time:143353ms step_avg:88.87ms
step:1614/1670 train_time:143444ms step_avg:88.87ms
step:1615/1670 train_time:143533ms step_avg:88.87ms
step:1616/1670 train_time:143623ms step_avg:88.88ms
step:1617/1670 train_time:143712ms step_avg:88.88ms
step:1618/1670 train_time:143803ms step_avg:88.88ms
step:1619/1670 train_time:143892ms step_avg:88.88ms
step:1620/1670 train_time:143982ms step_avg:88.88ms
step:1621/1670 train_time:144072ms step_avg:88.88ms
step:1622/1670 train_time:144162ms step_avg:88.88ms
step:1623/1670 train_time:144251ms step_avg:88.88ms
step:1624/1670 train_time:144341ms step_avg:88.88ms
step:1625/1670 train_time:144430ms step_avg:88.88ms
step:1625/1670 val_loss:3.2885 train_time:144522ms step_avg:88.94ms
step:1626/1670 train_time:144543ms step_avg:88.89ms
step:1627/1670 train_time:144622ms step_avg:88.89ms
step:1628/1670 train_time:144714ms step_avg:88.89ms
step:1629/1670 train_time:144803ms step_avg:88.89ms
step:1630/1670 train_time:144891ms step_avg:88.89ms
step:1631/1670 train_time:144979ms step_avg:88.89ms
step:1632/1670 train_time:145067ms step_avg:88.89ms
step:1633/1670 train_time:145157ms step_avg:88.89ms
step:1634/1670 train_time:145246ms step_avg:88.89ms
step:1635/1670 train_time:145337ms step_avg:88.89ms
step:1636/1670 train_time:145427ms step_avg:88.89ms
step:1637/1670 train_time:145521ms step_avg:88.89ms
step:1638/1670 train_time:145615ms step_avg:88.90ms
step:1639/1670 train_time:145704ms step_avg:88.90ms
step:1640/1670 train_time:145794ms step_avg:88.90ms
step:1641/1670 train_time:145883ms step_avg:88.90ms
step:1642/1670 train_time:145972ms step_avg:88.90ms
step:1643/1670 train_time:146060ms step_avg:88.90ms
step:1644/1670 train_time:146149ms step_avg:88.90ms
step:1645/1670 train_time:146238ms step_avg:88.90ms
step:1646/1670 train_time:146328ms step_avg:88.90ms
step:1647/1670 train_time:146419ms step_avg:88.90ms
step:1648/1670 train_time:146512ms step_avg:88.90ms
step:1649/1670 train_time:146602ms step_avg:88.90ms
step:1650/1670 train_time:146692ms step_avg:88.90ms
step:1651/1670 train_time:146782ms step_avg:88.90ms
step:1652/1670 train_time:146871ms step_avg:88.91ms
step:1653/1670 train_time:146960ms step_avg:88.91ms
step:1654/1670 train_time:147049ms step_avg:88.91ms
step:1655/1670 train_time:147138ms step_avg:88.91ms
step:1656/1670 train_time:147227ms step_avg:88.91ms
step:1657/1670 train_time:147317ms step_avg:88.91ms
step:1658/1670 train_time:147408ms step_avg:88.91ms
step:1659/1670 train_time:147500ms step_avg:88.91ms
step:1660/1670 train_time:147590ms step_avg:88.91ms
step:1661/1670 train_time:147681ms step_avg:88.91ms
step:1662/1670 train_time:147770ms step_avg:88.91ms
step:1663/1670 train_time:147860ms step_avg:88.91ms
step:1664/1670 train_time:147950ms step_avg:88.91ms
step:1665/1670 train_time:148039ms step_avg:88.91ms
step:1666/1670 train_time:148128ms step_avg:88.91ms
step:1667/1670 train_time:148218ms step_avg:88.91ms
step:1668/1670 train_time:148307ms step_avg:88.91ms
step:1669/1670 train_time:148397ms step_avg:88.91ms
step:1670/1670 train_time:148488ms step_avg:88.91ms
step:1670/1670 val_loss:3.2792 train_time:148580ms step_avg:88.97ms
peak memory allocated: 30760 MiB reserved: 45434 MiB

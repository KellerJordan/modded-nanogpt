import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)


@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)


@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[
    Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)


@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)


def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None


def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)


mm_op.register_autograd(backward, setup_context=setup_context)


# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
        pid,
        M,
        BLOCK_SIZE_M: tl.constexpr,
        BLOCK_SIZE_N: tl.constexpr,
        GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
coeffs_list = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in coeffs_list:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """

    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size() == 8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1,  # 1 param
            'attn_gate': 2,  # 10 params
            'attn': 3,  # 10 params
            'mlp': 4,  # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx + size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                    (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                    group["lr"]
                    * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                    * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                    group["lr"]
                    * group["weight_decay"]
                    * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[module_idx], 'module', 'none') == 'attn':
                for p in params[module_idx:module_idx + chunk_size]:
                    assert getattr(params[module_idx], 'module', 'none') == 'attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8,
                 weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))


class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5)  # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim // 4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim // 4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int = 1, beta: int = 32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1


def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)


@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim * 4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module = 'attn'
        with torch.no_grad():
            self.qkvo_w.view(4, self.hdim, self.dim)[:3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w.view(4, self.hdim, self.dim)[3].zero_()  # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1)  # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T,
                                                                                                               3 * self.num_heads,
                                                                                                               self.head_dim).chunk(
            3, dim=-2)
        q, k = norm(q), norm(k)  # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v)  # @ KoszarskyB & @Grad62304977
        else:  # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len,
                                   max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)  # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module = 'mlp'
        self.c_proj.module = 'mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_()  # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(
            x).square()  # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x


# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)


class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int,
                 max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim ** 0.5) / 448, w_s=2 ** -9,
                                    grad_s=1 / 448)
        self.lm_head.weight.detach().zero_()  # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers),  # extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm,
                    long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1, len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i < 11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss


# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32)  # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2])  # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True)  # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy())  # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens


BOS_ID = 50256


class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens = tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter == 5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter += 1
        return starts, ends


class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data


def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1,
                               align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens

        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin"  # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin"  # input .bin to eval validation loss on
    val_tokens: int = 10485760  # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1630  # number of iterations to run
    iteration_extension = 40  # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5  # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13  # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20  # extend long windows out even further


args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0)  # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)


def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)


# begin by printing this file (the Python code)
print0(code)
print0("=" * 100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")


def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout


print0(nvidia_smi())
print0("=" * 100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if
                        p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]


# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr


def get_ws(step: int):
    if step == args.num_iterations + args.iteration_extension:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers])  # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len,
                                          grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[
        step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long < ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long // 2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len,
                                          grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1,
                                                grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(
            f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms",
            console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(),
                       optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1)  # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(
        f"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms",
        console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Mon Sep 29 06:39:26 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   40C    P0            130W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   33C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1670 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1670 train_time:149ms step_avg:149.45ms
step:2/1670 train_time:171ms step_avg:85.25ms
step:3/1670 train_time:234ms step_avg:78.14ms
step:4/1670 train_time:321ms step_avg:80.28ms
step:5/1670 train_time:405ms step_avg:81.09ms
step:6/1670 train_time:492ms step_avg:81.97ms
step:7/1670 train_time:578ms step_avg:82.61ms
step:8/1670 train_time:665ms step_avg:83.15ms
step:9/1670 train_time:752ms step_avg:83.59ms
step:10/1670 train_time:839ms step_avg:83.89ms
step:11/1670 train_time:926ms step_avg:84.15ms
step:12/1670 train_time:1016ms step_avg:84.70ms
step:13/1670 train_time:1111ms step_avg:85.46ms
step:14/1670 train_time:1200ms step_avg:85.69ms
step:15/1670 train_time:1288ms step_avg:85.84ms
step:16/1670 train_time:1375ms step_avg:85.94ms
step:17/1670 train_time:1462ms step_avg:86.02ms
step:18/1670 train_time:1550ms step_avg:86.13ms
step:19/1670 train_time:1638ms step_avg:86.20ms
step:20/1670 train_time:1724ms step_avg:86.22ms
step:21/1670 train_time:1811ms step_avg:86.25ms
step:22/1670 train_time:1898ms step_avg:86.29ms
step:23/1670 train_time:1988ms step_avg:86.42ms
step:24/1670 train_time:2077ms step_avg:86.53ms
step:25/1670 train_time:2165ms step_avg:86.62ms
step:26/1670 train_time:2255ms step_avg:86.72ms
step:27/1670 train_time:2344ms step_avg:86.81ms
step:28/1670 train_time:2433ms step_avg:86.89ms
step:29/1670 train_time:2521ms step_avg:86.92ms
step:30/1670 train_time:2608ms step_avg:86.92ms
step:31/1670 train_time:2694ms step_avg:86.91ms
step:32/1670 train_time:2781ms step_avg:86.91ms
step:33/1670 train_time:2868ms step_avg:86.92ms
step:34/1670 train_time:2956ms step_avg:86.94ms
step:35/1670 train_time:3044ms step_avg:86.98ms
step:36/1670 train_time:3133ms step_avg:87.03ms
step:37/1670 train_time:3222ms step_avg:87.07ms
step:38/1670 train_time:3312ms step_avg:87.15ms
step:39/1670 train_time:3400ms step_avg:87.18ms
step:40/1670 train_time:3488ms step_avg:87.20ms
step:41/1670 train_time:3575ms step_avg:87.20ms
step:42/1670 train_time:3663ms step_avg:87.21ms
step:43/1670 train_time:3750ms step_avg:87.22ms
step:44/1670 train_time:3838ms step_avg:87.22ms
step:45/1670 train_time:3925ms step_avg:87.22ms
step:46/1670 train_time:4013ms step_avg:87.25ms
step:47/1670 train_time:4101ms step_avg:87.26ms
step:48/1670 train_time:4190ms step_avg:87.30ms
step:49/1670 train_time:4277ms step_avg:87.28ms
step:50/1670 train_time:4366ms step_avg:87.31ms
step:51/1670 train_time:4454ms step_avg:87.34ms
step:52/1670 train_time:4542ms step_avg:87.34ms
step:53/1670 train_time:4629ms step_avg:87.34ms
step:54/1670 train_time:4716ms step_avg:87.33ms
step:55/1670 train_time:4803ms step_avg:87.33ms
step:56/1670 train_time:4891ms step_avg:87.33ms
step:57/1670 train_time:4978ms step_avg:87.34ms
step:58/1670 train_time:5066ms step_avg:87.35ms
step:59/1670 train_time:5155ms step_avg:87.37ms
step:60/1670 train_time:5244ms step_avg:87.40ms
step:61/1670 train_time:5333ms step_avg:87.43ms
step:62/1670 train_time:5421ms step_avg:87.43ms
step:63/1670 train_time:5508ms step_avg:87.44ms
step:64/1670 train_time:5596ms step_avg:87.43ms
step:65/1670 train_time:5683ms step_avg:87.43ms
step:66/1670 train_time:5771ms step_avg:87.44ms
step:67/1670 train_time:5859ms step_avg:87.44ms
step:68/1670 train_time:5947ms step_avg:87.45ms
step:69/1670 train_time:6035ms step_avg:87.46ms
step:70/1670 train_time:6122ms step_avg:87.46ms
step:71/1670 train_time:6210ms step_avg:87.46ms
step:72/1670 train_time:6298ms step_avg:87.47ms
step:73/1670 train_time:6387ms step_avg:87.49ms
step:74/1670 train_time:6475ms step_avg:87.50ms
step:75/1670 train_time:6563ms step_avg:87.51ms
step:76/1670 train_time:6651ms step_avg:87.51ms
step:77/1670 train_time:6739ms step_avg:87.52ms
step:78/1670 train_time:6827ms step_avg:87.52ms
step:79/1670 train_time:6914ms step_avg:87.52ms
step:80/1670 train_time:7001ms step_avg:87.51ms
step:81/1670 train_time:7089ms step_avg:87.52ms
step:82/1670 train_time:7176ms step_avg:87.52ms
step:83/1670 train_time:7264ms step_avg:87.52ms
step:84/1670 train_time:7353ms step_avg:87.53ms
step:85/1670 train_time:7440ms step_avg:87.53ms
step:86/1670 train_time:7528ms step_avg:87.53ms
step:87/1670 train_time:7615ms step_avg:87.53ms
step:88/1670 train_time:7703ms step_avg:87.53ms
step:89/1670 train_time:7791ms step_avg:87.54ms
step:90/1670 train_time:7878ms step_avg:87.54ms
step:91/1670 train_time:7966ms step_avg:87.54ms
step:92/1670 train_time:8054ms step_avg:87.54ms
step:93/1670 train_time:8142ms step_avg:87.55ms
step:94/1670 train_time:8230ms step_avg:87.55ms
step:95/1670 train_time:8318ms step_avg:87.55ms
step:96/1670 train_time:8405ms step_avg:87.55ms
step:97/1670 train_time:8494ms step_avg:87.56ms
step:98/1670 train_time:8581ms step_avg:87.56ms
step:99/1670 train_time:8669ms step_avg:87.57ms
step:100/1670 train_time:8756ms step_avg:87.56ms
step:101/1670 train_time:8844ms step_avg:87.56ms
step:102/1670 train_time:8932ms step_avg:87.57ms
step:103/1670 train_time:9019ms step_avg:87.56ms
step:104/1670 train_time:9106ms step_avg:87.56ms
step:105/1670 train_time:9194ms step_avg:87.56ms
step:106/1670 train_time:9282ms step_avg:87.56ms
step:107/1670 train_time:9370ms step_avg:87.57ms
step:108/1670 train_time:9458ms step_avg:87.57ms
step:109/1670 train_time:9545ms step_avg:87.57ms
step:110/1670 train_time:9633ms step_avg:87.58ms
step:111/1670 train_time:9721ms step_avg:87.58ms
step:112/1670 train_time:9809ms step_avg:87.58ms
step:113/1670 train_time:9896ms step_avg:87.58ms
step:114/1670 train_time:9983ms step_avg:87.57ms
step:115/1670 train_time:10071ms step_avg:87.58ms
step:116/1670 train_time:10159ms step_avg:87.58ms
step:117/1670 train_time:10246ms step_avg:87.58ms
step:118/1670 train_time:10334ms step_avg:87.58ms
step:119/1670 train_time:10422ms step_avg:87.58ms
step:120/1670 train_time:10510ms step_avg:87.58ms
step:121/1670 train_time:10597ms step_avg:87.58ms
step:122/1670 train_time:10685ms step_avg:87.58ms
step:123/1670 train_time:10773ms step_avg:87.58ms
step:124/1670 train_time:10860ms step_avg:87.58ms
step:125/1670 train_time:10947ms step_avg:87.58ms
step:125/1670 val_loss:4.3268 train_time:11036ms step_avg:88.29ms
step:126/1670 train_time:11056ms step_avg:87.74ms
step:127/1670 train_time:11123ms step_avg:87.59ms
step:128/1670 train_time:11210ms step_avg:87.58ms
step:129/1670 train_time:11300ms step_avg:87.60ms
step:130/1670 train_time:11390ms step_avg:87.62ms
step:131/1670 train_time:11478ms step_avg:87.62ms
step:132/1670 train_time:11565ms step_avg:87.61ms
step:133/1670 train_time:11651ms step_avg:87.60ms
step:134/1670 train_time:11737ms step_avg:87.59ms
step:135/1670 train_time:11823ms step_avg:87.58ms
step:136/1670 train_time:11910ms step_avg:87.57ms
step:137/1670 train_time:12002ms step_avg:87.61ms
step:138/1670 train_time:12093ms step_avg:87.63ms
step:139/1670 train_time:12181ms step_avg:87.63ms
step:140/1670 train_time:12268ms step_avg:87.63ms
step:141/1670 train_time:12356ms step_avg:87.63ms
step:142/1670 train_time:12443ms step_avg:87.63ms
step:143/1670 train_time:12530ms step_avg:87.62ms
step:144/1670 train_time:12617ms step_avg:87.62ms
step:145/1670 train_time:12704ms step_avg:87.62ms
step:146/1670 train_time:12791ms step_avg:87.61ms
step:147/1670 train_time:12878ms step_avg:87.61ms
step:148/1670 train_time:12967ms step_avg:87.61ms
step:149/1670 train_time:13055ms step_avg:87.62ms
step:150/1670 train_time:13144ms step_avg:87.63ms
step:151/1670 train_time:13232ms step_avg:87.63ms
step:152/1670 train_time:13319ms step_avg:87.62ms
step:153/1670 train_time:13406ms step_avg:87.62ms
step:154/1670 train_time:13493ms step_avg:87.62ms
step:155/1670 train_time:13580ms step_avg:87.61ms
step:156/1670 train_time:13668ms step_avg:87.61ms
step:157/1670 train_time:13755ms step_avg:87.61ms
step:158/1670 train_time:13842ms step_avg:87.61ms
step:159/1670 train_time:13930ms step_avg:87.61ms
step:160/1670 train_time:14018ms step_avg:87.61ms
step:161/1670 train_time:14106ms step_avg:87.61ms
step:162/1670 train_time:14194ms step_avg:87.62ms
step:163/1670 train_time:14281ms step_avg:87.62ms
step:164/1670 train_time:14369ms step_avg:87.62ms
step:165/1670 train_time:14456ms step_avg:87.61ms
step:166/1670 train_time:14544ms step_avg:87.61ms
step:167/1670 train_time:14631ms step_avg:87.61ms
step:168/1670 train_time:14719ms step_avg:87.61ms
step:169/1670 train_time:14805ms step_avg:87.60ms
step:170/1670 train_time:14893ms step_avg:87.61ms
step:171/1670 train_time:14980ms step_avg:87.60ms
step:172/1670 train_time:15069ms step_avg:87.61ms
step:173/1670 train_time:15157ms step_avg:87.61ms
step:174/1670 train_time:15245ms step_avg:87.61ms
step:175/1670 train_time:15332ms step_avg:87.61ms
step:176/1670 train_time:15419ms step_avg:87.61ms
step:177/1670 train_time:15506ms step_avg:87.61ms
step:178/1670 train_time:15594ms step_avg:87.61ms
step:179/1670 train_time:15680ms step_avg:87.60ms
step:180/1670 train_time:15768ms step_avg:87.60ms
step:181/1670 train_time:15855ms step_avg:87.60ms
step:182/1670 train_time:15943ms step_avg:87.60ms
step:183/1670 train_time:16031ms step_avg:87.60ms
step:184/1670 train_time:16120ms step_avg:87.61ms
step:185/1670 train_time:16208ms step_avg:87.61ms
step:186/1670 train_time:16296ms step_avg:87.61ms
step:187/1670 train_time:16383ms step_avg:87.61ms
step:188/1670 train_time:16470ms step_avg:87.61ms
step:189/1670 train_time:16558ms step_avg:87.61ms
step:190/1670 train_time:16645ms step_avg:87.61ms
step:191/1670 train_time:16733ms step_avg:87.61ms
step:192/1670 train_time:16820ms step_avg:87.60ms
step:193/1670 train_time:16907ms step_avg:87.60ms
step:194/1670 train_time:16995ms step_avg:87.61ms
step:195/1670 train_time:17083ms step_avg:87.61ms
step:196/1670 train_time:17171ms step_avg:87.61ms
step:197/1670 train_time:17259ms step_avg:87.61ms
step:198/1670 train_time:17346ms step_avg:87.61ms
step:199/1670 train_time:17433ms step_avg:87.60ms
step:200/1670 train_time:17520ms step_avg:87.60ms
step:201/1670 train_time:17608ms step_avg:87.60ms
step:202/1670 train_time:17695ms step_avg:87.60ms
step:203/1670 train_time:17782ms step_avg:87.60ms
step:204/1670 train_time:17870ms step_avg:87.60ms
step:205/1670 train_time:17957ms step_avg:87.59ms
step:206/1670 train_time:18045ms step_avg:87.59ms
step:207/1670 train_time:18133ms step_avg:87.60ms
step:208/1670 train_time:18220ms step_avg:87.60ms
step:209/1670 train_time:18308ms step_avg:87.60ms
step:210/1670 train_time:18396ms step_avg:87.60ms
step:211/1670 train_time:18483ms step_avg:87.60ms
step:212/1670 train_time:18571ms step_avg:87.60ms
step:213/1670 train_time:18659ms step_avg:87.60ms
step:214/1670 train_time:18746ms step_avg:87.60ms
step:215/1670 train_time:18833ms step_avg:87.60ms
step:216/1670 train_time:18920ms step_avg:87.59ms
step:217/1670 train_time:19007ms step_avg:87.59ms
step:218/1670 train_time:19095ms step_avg:87.59ms
step:219/1670 train_time:19182ms step_avg:87.59ms
step:220/1670 train_time:19270ms step_avg:87.59ms
step:221/1670 train_time:19358ms step_avg:87.59ms
step:222/1670 train_time:19445ms step_avg:87.59ms
step:223/1670 train_time:19533ms step_avg:87.59ms
step:224/1670 train_time:19620ms step_avg:87.59ms
step:225/1670 train_time:19707ms step_avg:87.59ms
step:226/1670 train_time:19795ms step_avg:87.59ms
step:227/1670 train_time:19883ms step_avg:87.59ms
step:228/1670 train_time:19971ms step_avg:87.59ms
step:229/1670 train_time:20058ms step_avg:87.59ms
step:230/1670 train_time:20146ms step_avg:87.59ms
step:231/1670 train_time:20235ms step_avg:87.60ms
step:232/1670 train_time:20321ms step_avg:87.59ms
step:233/1670 train_time:20410ms step_avg:87.60ms
step:234/1670 train_time:20497ms step_avg:87.59ms
step:235/1670 train_time:20585ms step_avg:87.59ms
step:236/1670 train_time:20672ms step_avg:87.59ms
step:237/1670 train_time:20759ms step_avg:87.59ms
step:238/1670 train_time:20847ms step_avg:87.59ms
step:239/1670 train_time:20934ms step_avg:87.59ms
step:240/1670 train_time:21022ms step_avg:87.59ms
step:241/1670 train_time:21111ms step_avg:87.60ms
step:242/1670 train_time:21198ms step_avg:87.60ms
step:243/1670 train_time:21286ms step_avg:87.60ms
step:244/1670 train_time:21373ms step_avg:87.60ms
step:245/1670 train_time:21461ms step_avg:87.60ms
step:246/1670 train_time:21549ms step_avg:87.60ms
step:247/1670 train_time:21636ms step_avg:87.59ms
step:248/1670 train_time:21723ms step_avg:87.59ms
step:249/1670 train_time:21811ms step_avg:87.59ms
step:250/1670 train_time:21898ms step_avg:87.59ms
step:250/1670 val_loss:3.9745 train_time:21987ms step_avg:87.95ms
step:251/1670 train_time:22009ms step_avg:87.68ms
step:252/1670 train_time:22079ms step_avg:87.61ms
step:253/1670 train_time:22172ms step_avg:87.64ms
step:254/1670 train_time:22259ms step_avg:87.64ms
step:255/1670 train_time:22346ms step_avg:87.63ms
step:256/1670 train_time:22433ms step_avg:87.63ms
step:257/1670 train_time:22519ms step_avg:87.62ms
step:258/1670 train_time:22606ms step_avg:87.62ms
step:259/1670 train_time:22692ms step_avg:87.62ms
step:260/1670 train_time:22779ms step_avg:87.61ms
step:261/1670 train_time:22866ms step_avg:87.61ms
step:262/1670 train_time:22954ms step_avg:87.61ms
step:263/1670 train_time:23044ms step_avg:87.62ms
step:264/1670 train_time:23134ms step_avg:87.63ms
step:265/1670 train_time:23222ms step_avg:87.63ms
step:266/1670 train_time:23310ms step_avg:87.63ms
step:267/1670 train_time:23398ms step_avg:87.63ms
step:268/1670 train_time:23485ms step_avg:87.63ms
step:269/1670 train_time:23572ms step_avg:87.63ms
step:270/1670 train_time:23658ms step_avg:87.62ms
step:271/1670 train_time:23745ms step_avg:87.62ms
step:272/1670 train_time:23832ms step_avg:87.62ms
step:273/1670 train_time:23919ms step_avg:87.62ms
step:274/1670 train_time:24007ms step_avg:87.62ms
step:275/1670 train_time:24096ms step_avg:87.62ms
step:276/1670 train_time:24184ms step_avg:87.62ms
step:277/1670 train_time:24272ms step_avg:87.63ms
step:278/1670 train_time:24360ms step_avg:87.63ms
step:279/1670 train_time:24448ms step_avg:87.63ms
step:280/1670 train_time:24535ms step_avg:87.62ms
step:281/1670 train_time:24621ms step_avg:87.62ms
step:282/1670 train_time:24708ms step_avg:87.62ms
step:283/1670 train_time:24795ms step_avg:87.62ms
step:284/1670 train_time:24883ms step_avg:87.62ms
step:285/1670 train_time:24971ms step_avg:87.62ms
step:286/1670 train_time:25059ms step_avg:87.62ms
step:287/1670 train_time:25146ms step_avg:87.62ms
step:288/1670 train_time:25234ms step_avg:87.62ms
step:289/1670 train_time:25322ms step_avg:87.62ms
step:290/1670 train_time:25410ms step_avg:87.62ms
step:291/1670 train_time:25497ms step_avg:87.62ms
step:292/1670 train_time:25585ms step_avg:87.62ms
step:293/1670 train_time:25672ms step_avg:87.62ms
step:294/1670 train_time:25760ms step_avg:87.62ms
step:295/1670 train_time:25847ms step_avg:87.62ms
step:296/1670 train_time:25934ms step_avg:87.62ms
step:297/1670 train_time:26022ms step_avg:87.61ms
step:298/1670 train_time:26110ms step_avg:87.62ms
step:299/1670 train_time:26197ms step_avg:87.62ms
step:300/1670 train_time:26285ms step_avg:87.62ms
step:301/1670 train_time:26373ms step_avg:87.62ms
step:302/1670 train_time:26461ms step_avg:87.62ms
step:303/1670 train_time:26549ms step_avg:87.62ms
step:304/1670 train_time:26636ms step_avg:87.62ms
step:305/1670 train_time:26723ms step_avg:87.62ms
step:306/1670 train_time:26811ms step_avg:87.62ms
step:307/1670 train_time:26898ms step_avg:87.61ms
step:308/1670 train_time:26985ms step_avg:87.61ms
step:309/1670 train_time:27073ms step_avg:87.61ms
step:310/1670 train_time:27160ms step_avg:87.61ms
step:311/1670 train_time:27248ms step_avg:87.61ms
step:312/1670 train_time:27335ms step_avg:87.61ms
step:313/1670 train_time:27423ms step_avg:87.61ms
step:314/1670 train_time:27513ms step_avg:87.62ms
step:315/1670 train_time:27600ms step_avg:87.62ms
step:316/1670 train_time:27687ms step_avg:87.62ms
step:317/1670 train_time:27774ms step_avg:87.61ms
step:318/1670 train_time:27861ms step_avg:87.61ms
step:319/1670 train_time:27949ms step_avg:87.61ms
step:320/1670 train_time:28036ms step_avg:87.61ms
step:321/1670 train_time:28124ms step_avg:87.61ms
step:322/1670 train_time:28212ms step_avg:87.62ms
step:323/1670 train_time:28299ms step_avg:87.61ms
step:324/1670 train_time:28389ms step_avg:87.62ms
step:325/1670 train_time:28477ms step_avg:87.62ms
step:326/1670 train_time:28565ms step_avg:87.62ms
step:327/1670 train_time:28653ms step_avg:87.62ms
step:328/1670 train_time:28740ms step_avg:87.62ms
step:329/1670 train_time:28826ms step_avg:87.62ms
step:330/1670 train_time:28914ms step_avg:87.62ms
step:331/1670 train_time:29001ms step_avg:87.62ms
step:332/1670 train_time:29089ms step_avg:87.62ms
step:333/1670 train_time:29177ms step_avg:87.62ms
step:334/1670 train_time:29264ms step_avg:87.62ms
step:335/1670 train_time:29353ms step_avg:87.62ms
step:336/1670 train_time:29442ms step_avg:87.63ms
step:337/1670 train_time:29531ms step_avg:87.63ms
step:338/1670 train_time:29618ms step_avg:87.63ms
step:339/1670 train_time:29704ms step_avg:87.62ms
step:340/1670 train_time:29793ms step_avg:87.63ms
step:341/1670 train_time:29880ms step_avg:87.63ms
step:342/1670 train_time:29968ms step_avg:87.62ms
step:343/1670 train_time:30055ms step_avg:87.62ms
step:344/1670 train_time:30142ms step_avg:87.62ms
step:345/1670 train_time:30229ms step_avg:87.62ms
step:346/1670 train_time:30317ms step_avg:87.62ms
step:347/1670 train_time:30405ms step_avg:87.62ms
step:348/1670 train_time:30493ms step_avg:87.62ms
step:349/1670 train_time:30580ms step_avg:87.62ms
step:350/1670 train_time:30668ms step_avg:87.62ms
step:351/1670 train_time:30755ms step_avg:87.62ms
step:352/1670 train_time:30842ms step_avg:87.62ms
step:353/1670 train_time:30930ms step_avg:87.62ms
step:354/1670 train_time:31018ms step_avg:87.62ms
step:355/1670 train_time:31105ms step_avg:87.62ms
step:356/1670 train_time:31193ms step_avg:87.62ms
step:357/1670 train_time:31280ms step_avg:87.62ms
step:358/1670 train_time:31368ms step_avg:87.62ms
step:359/1670 train_time:31456ms step_avg:87.62ms
step:360/1670 train_time:31543ms step_avg:87.62ms
step:361/1670 train_time:31631ms step_avg:87.62ms
step:362/1670 train_time:31719ms step_avg:87.62ms
step:363/1670 train_time:31807ms step_avg:87.62ms
step:364/1670 train_time:31894ms step_avg:87.62ms
step:365/1670 train_time:31982ms step_avg:87.62ms
step:366/1670 train_time:32070ms step_avg:87.62ms
step:367/1670 train_time:32157ms step_avg:87.62ms
step:368/1670 train_time:32244ms step_avg:87.62ms
step:369/1670 train_time:32333ms step_avg:87.62ms
step:370/1670 train_time:32420ms step_avg:87.62ms
step:371/1670 train_time:32508ms step_avg:87.62ms
step:372/1670 train_time:32596ms step_avg:87.62ms
step:373/1670 train_time:32683ms step_avg:87.62ms
step:374/1670 train_time:32771ms step_avg:87.62ms
step:375/1670 train_time:32859ms step_avg:87.62ms
step:375/1670 val_loss:3.8208 train_time:32948ms step_avg:87.86ms
step:376/1670 train_time:32967ms step_avg:87.68ms
step:377/1670 train_time:33038ms step_avg:87.63ms
step:378/1670 train_time:33132ms step_avg:87.65ms
step:379/1670 train_time:33220ms step_avg:87.65ms
step:380/1670 train_time:33309ms step_avg:87.65ms
step:381/1670 train_time:33395ms step_avg:87.65ms
step:382/1670 train_time:33481ms step_avg:87.65ms
step:383/1670 train_time:33568ms step_avg:87.65ms
step:384/1670 train_time:33655ms step_avg:87.64ms
step:385/1670 train_time:33741ms step_avg:87.64ms
step:386/1670 train_time:33828ms step_avg:87.64ms
step:387/1670 train_time:33915ms step_avg:87.64ms
step:388/1670 train_time:34005ms step_avg:87.64ms
step:389/1670 train_time:34097ms step_avg:87.65ms
step:390/1670 train_time:34186ms step_avg:87.66ms
step:391/1670 train_time:34275ms step_avg:87.66ms
step:392/1670 train_time:34362ms step_avg:87.66ms
step:393/1670 train_time:34448ms step_avg:87.65ms
step:394/1670 train_time:34535ms step_avg:87.65ms
step:395/1670 train_time:34621ms step_avg:87.65ms
step:396/1670 train_time:34709ms step_avg:87.65ms
step:397/1670 train_time:34796ms step_avg:87.65ms
step:398/1670 train_time:34883ms step_avg:87.65ms
step:399/1670 train_time:34971ms step_avg:87.65ms
step:400/1670 train_time:35060ms step_avg:87.65ms
step:401/1670 train_time:35149ms step_avg:87.65ms
step:402/1670 train_time:35238ms step_avg:87.66ms
step:403/1670 train_time:35326ms step_avg:87.66ms
step:404/1670 train_time:35415ms step_avg:87.66ms
step:405/1670 train_time:35501ms step_avg:87.66ms
step:406/1670 train_time:35588ms step_avg:87.65ms
step:407/1670 train_time:35675ms step_avg:87.65ms
step:408/1670 train_time:35762ms step_avg:87.65ms
step:409/1670 train_time:35849ms step_avg:87.65ms
step:410/1670 train_time:35936ms step_avg:87.65ms
step:411/1670 train_time:36024ms step_avg:87.65ms
step:412/1670 train_time:36114ms step_avg:87.65ms
step:413/1670 train_time:36202ms step_avg:87.66ms
step:414/1670 train_time:36291ms step_avg:87.66ms
step:415/1670 train_time:36378ms step_avg:87.66ms
step:416/1670 train_time:36466ms step_avg:87.66ms
step:417/1670 train_time:36553ms step_avg:87.66ms
step:418/1670 train_time:36640ms step_avg:87.66ms
step:419/1670 train_time:36727ms step_avg:87.65ms
step:420/1670 train_time:36816ms step_avg:87.66ms
step:421/1670 train_time:36903ms step_avg:87.66ms
step:422/1670 train_time:36991ms step_avg:87.66ms
step:423/1670 train_time:37079ms step_avg:87.66ms
step:424/1670 train_time:37167ms step_avg:87.66ms
step:425/1670 train_time:37256ms step_avg:87.66ms
step:426/1670 train_time:37344ms step_avg:87.66ms
step:427/1670 train_time:37432ms step_avg:87.66ms
step:428/1670 train_time:37519ms step_avg:87.66ms
step:429/1670 train_time:37605ms step_avg:87.66ms
step:430/1670 train_time:37693ms step_avg:87.66ms
step:431/1670 train_time:37780ms step_avg:87.66ms
step:432/1670 train_time:37868ms step_avg:87.66ms
step:433/1670 train_time:37956ms step_avg:87.66ms
step:434/1670 train_time:38044ms step_avg:87.66ms
step:435/1670 train_time:38132ms step_avg:87.66ms
step:436/1670 train_time:38220ms step_avg:87.66ms
step:437/1670 train_time:38308ms step_avg:87.66ms
step:438/1670 train_time:38396ms step_avg:87.66ms
step:439/1670 train_time:38484ms step_avg:87.66ms
step:440/1670 train_time:38571ms step_avg:87.66ms
step:441/1670 train_time:38658ms step_avg:87.66ms
step:442/1670 train_time:38746ms step_avg:87.66ms
step:443/1670 train_time:38834ms step_avg:87.66ms
step:444/1670 train_time:38921ms step_avg:87.66ms
step:445/1670 train_time:39009ms step_avg:87.66ms
step:446/1670 train_time:39097ms step_avg:87.66ms
step:447/1670 train_time:39185ms step_avg:87.66ms
step:448/1670 train_time:39273ms step_avg:87.66ms
step:449/1670 train_time:39361ms step_avg:87.66ms
step:450/1670 train_time:39448ms step_avg:87.66ms
step:451/1670 train_time:39536ms step_avg:87.66ms
step:452/1670 train_time:39623ms step_avg:87.66ms
step:453/1670 train_time:39711ms step_avg:87.66ms
step:454/1670 train_time:39798ms step_avg:87.66ms
step:455/1670 train_time:39884ms step_avg:87.66ms
step:456/1670 train_time:39972ms step_avg:87.66ms
step:457/1670 train_time:40060ms step_avg:87.66ms
step:458/1670 train_time:40147ms step_avg:87.66ms
step:459/1670 train_time:40235ms step_avg:87.66ms
step:460/1670 train_time:40323ms step_avg:87.66ms
step:461/1670 train_time:40410ms step_avg:87.66ms
step:462/1670 train_time:40498ms step_avg:87.66ms
step:463/1670 train_time:40585ms step_avg:87.66ms
step:464/1670 train_time:40673ms step_avg:87.66ms
step:465/1670 train_time:40760ms step_avg:87.66ms
step:466/1670 train_time:40848ms step_avg:87.66ms
step:467/1670 train_time:40936ms step_avg:87.66ms
step:468/1670 train_time:41022ms step_avg:87.65ms
step:469/1670 train_time:41110ms step_avg:87.65ms
step:470/1670 train_time:41198ms step_avg:87.65ms
step:471/1670 train_time:41286ms step_avg:87.66ms
step:472/1670 train_time:41374ms step_avg:87.66ms
step:473/1670 train_time:41461ms step_avg:87.66ms
step:474/1670 train_time:41549ms step_avg:87.66ms
step:475/1670 train_time:41637ms step_avg:87.66ms
step:476/1670 train_time:41724ms step_avg:87.66ms
step:477/1670 train_time:41812ms step_avg:87.66ms
step:478/1670 train_time:41899ms step_avg:87.66ms
step:479/1670 train_time:41987ms step_avg:87.66ms
step:480/1670 train_time:42076ms step_avg:87.66ms
step:481/1670 train_time:42164ms step_avg:87.66ms
step:482/1670 train_time:42251ms step_avg:87.66ms
step:483/1670 train_time:42339ms step_avg:87.66ms
step:484/1670 train_time:42426ms step_avg:87.66ms
step:485/1670 train_time:42514ms step_avg:87.66ms
step:486/1670 train_time:42601ms step_avg:87.66ms
step:487/1670 train_time:42688ms step_avg:87.66ms
step:488/1670 train_time:42776ms step_avg:87.66ms
step:489/1670 train_time:42863ms step_avg:87.65ms
step:490/1670 train_time:42951ms step_avg:87.66ms
step:491/1670 train_time:43039ms step_avg:87.66ms
step:492/1670 train_time:43127ms step_avg:87.66ms
step:493/1670 train_time:43215ms step_avg:87.66ms
step:494/1670 train_time:43302ms step_avg:87.66ms
step:495/1670 train_time:43390ms step_avg:87.66ms
step:496/1670 train_time:43478ms step_avg:87.66ms
step:497/1670 train_time:43565ms step_avg:87.66ms
step:498/1670 train_time:43653ms step_avg:87.66ms
step:499/1670 train_time:43740ms step_avg:87.66ms
step:500/1670 train_time:43828ms step_avg:87.66ms
step:500/1670 val_loss:3.7187 train_time:43917ms step_avg:87.83ms
step:501/1670 train_time:43937ms step_avg:87.70ms
step:502/1670 train_time:44010ms step_avg:87.67ms
step:503/1670 train_time:44105ms step_avg:87.68ms
step:504/1670 train_time:44195ms step_avg:87.69ms
step:505/1670 train_time:44283ms step_avg:87.69ms
step:506/1670 train_time:44370ms step_avg:87.69ms
step:507/1670 train_time:44456ms step_avg:87.68ms
step:508/1670 train_time:44543ms step_avg:87.68ms
step:509/1670 train_time:44628ms step_avg:87.68ms
step:510/1670 train_time:44715ms step_avg:87.68ms
step:511/1670 train_time:44802ms step_avg:87.68ms
step:512/1670 train_time:44890ms step_avg:87.67ms
step:513/1670 train_time:44980ms step_avg:87.68ms
step:514/1670 train_time:45069ms step_avg:87.68ms
step:515/1670 train_time:45159ms step_avg:87.69ms
step:516/1670 train_time:45247ms step_avg:87.69ms
step:517/1670 train_time:45335ms step_avg:87.69ms
step:518/1670 train_time:45423ms step_avg:87.69ms
step:519/1670 train_time:45509ms step_avg:87.69ms
step:520/1670 train_time:45596ms step_avg:87.68ms
step:521/1670 train_time:45682ms step_avg:87.68ms
step:522/1670 train_time:45768ms step_avg:87.68ms
step:523/1670 train_time:45856ms step_avg:87.68ms
step:524/1670 train_time:45946ms step_avg:87.68ms
step:525/1670 train_time:46035ms step_avg:87.69ms
step:526/1670 train_time:46125ms step_avg:87.69ms
step:527/1670 train_time:46214ms step_avg:87.69ms
step:528/1670 train_time:46301ms step_avg:87.69ms
step:529/1670 train_time:46388ms step_avg:87.69ms
step:530/1670 train_time:46476ms step_avg:87.69ms
step:531/1670 train_time:46563ms step_avg:87.69ms
step:532/1670 train_time:46649ms step_avg:87.69ms
step:533/1670 train_time:46736ms step_avg:87.69ms
step:534/1670 train_time:46824ms step_avg:87.69ms
step:535/1670 train_time:46911ms step_avg:87.68ms
step:536/1670 train_time:47000ms step_avg:87.69ms
step:537/1670 train_time:47088ms step_avg:87.69ms
step:538/1670 train_time:47177ms step_avg:87.69ms
step:539/1670 train_time:47265ms step_avg:87.69ms
step:540/1670 train_time:47353ms step_avg:87.69ms
step:541/1670 train_time:47441ms step_avg:87.69ms
step:542/1670 train_time:47528ms step_avg:87.69ms
step:543/1670 train_time:47615ms step_avg:87.69ms
step:544/1670 train_time:47702ms step_avg:87.69ms
step:545/1670 train_time:47789ms step_avg:87.69ms
step:546/1670 train_time:47878ms step_avg:87.69ms
step:547/1670 train_time:47967ms step_avg:87.69ms
step:548/1670 train_time:48057ms step_avg:87.69ms
step:549/1670 train_time:48147ms step_avg:87.70ms
step:550/1670 train_time:48238ms step_avg:87.70ms
step:551/1670 train_time:48327ms step_avg:87.71ms
step:552/1670 train_time:48416ms step_avg:87.71ms
step:553/1670 train_time:48505ms step_avg:87.71ms
step:554/1670 train_time:48593ms step_avg:87.71ms
step:555/1670 train_time:48682ms step_avg:87.72ms
step:556/1670 train_time:48770ms step_avg:87.72ms
step:557/1670 train_time:48859ms step_avg:87.72ms
step:558/1670 train_time:48948ms step_avg:87.72ms
step:559/1670 train_time:49038ms step_avg:87.72ms
step:560/1670 train_time:49127ms step_avg:87.73ms
step:561/1670 train_time:49216ms step_avg:87.73ms
step:562/1670 train_time:49305ms step_avg:87.73ms
step:563/1670 train_time:49394ms step_avg:87.73ms
step:564/1670 train_time:49485ms step_avg:87.74ms
step:565/1670 train_time:49574ms step_avg:87.74ms
step:566/1670 train_time:49663ms step_avg:87.74ms
step:567/1670 train_time:49751ms step_avg:87.74ms
step:568/1670 train_time:49840ms step_avg:87.75ms
step:569/1670 train_time:49928ms step_avg:87.75ms
step:570/1670 train_time:50017ms step_avg:87.75ms
step:571/1670 train_time:50106ms step_avg:87.75ms
step:572/1670 train_time:50194ms step_avg:87.75ms
step:573/1670 train_time:50284ms step_avg:87.76ms
step:574/1670 train_time:50373ms step_avg:87.76ms
step:575/1670 train_time:50463ms step_avg:87.76ms
step:576/1670 train_time:50552ms step_avg:87.76ms
step:577/1670 train_time:50642ms step_avg:87.77ms
step:578/1670 train_time:50730ms step_avg:87.77ms
step:579/1670 train_time:50819ms step_avg:87.77ms
step:580/1670 train_time:50907ms step_avg:87.77ms
step:581/1670 train_time:50996ms step_avg:87.77ms
step:582/1670 train_time:51085ms step_avg:87.78ms
step:583/1670 train_time:51174ms step_avg:87.78ms
step:584/1670 train_time:51263ms step_avg:87.78ms
step:585/1670 train_time:51352ms step_avg:87.78ms
step:586/1670 train_time:51441ms step_avg:87.78ms
step:587/1670 train_time:51529ms step_avg:87.78ms
step:588/1670 train_time:51618ms step_avg:87.79ms
step:589/1670 train_time:51707ms step_avg:87.79ms
step:590/1670 train_time:51795ms step_avg:87.79ms
step:591/1670 train_time:51884ms step_avg:87.79ms
step:592/1670 train_time:51973ms step_avg:87.79ms
step:593/1670 train_time:52062ms step_avg:87.79ms
step:594/1670 train_time:52151ms step_avg:87.80ms
step:595/1670 train_time:52240ms step_avg:87.80ms
step:596/1670 train_time:52328ms step_avg:87.80ms
step:597/1670 train_time:52417ms step_avg:87.80ms
step:598/1670 train_time:52506ms step_avg:87.80ms
step:599/1670 train_time:52596ms step_avg:87.81ms
step:600/1670 train_time:52685ms step_avg:87.81ms
step:601/1670 train_time:52774ms step_avg:87.81ms
step:602/1670 train_time:52863ms step_avg:87.81ms
step:603/1670 train_time:52951ms step_avg:87.81ms
step:604/1670 train_time:53041ms step_avg:87.82ms
step:605/1670 train_time:53129ms step_avg:87.82ms
step:606/1670 train_time:53218ms step_avg:87.82ms
step:607/1670 train_time:53306ms step_avg:87.82ms
step:608/1670 train_time:53395ms step_avg:87.82ms
step:609/1670 train_time:53486ms step_avg:87.83ms
step:610/1670 train_time:53575ms step_avg:87.83ms
step:611/1670 train_time:53665ms step_avg:87.83ms
step:612/1670 train_time:53754ms step_avg:87.83ms
step:613/1670 train_time:53844ms step_avg:87.84ms
step:614/1670 train_time:53933ms step_avg:87.84ms
step:615/1670 train_time:54021ms step_avg:87.84ms
step:616/1670 train_time:54109ms step_avg:87.84ms
step:617/1670 train_time:54197ms step_avg:87.84ms
step:618/1670 train_time:54286ms step_avg:87.84ms
step:619/1670 train_time:54375ms step_avg:87.84ms
step:620/1670 train_time:54464ms step_avg:87.84ms
step:621/1670 train_time:54553ms step_avg:87.85ms
step:622/1670 train_time:54643ms step_avg:87.85ms
step:623/1670 train_time:54731ms step_avg:87.85ms
step:624/1670 train_time:54819ms step_avg:87.85ms
step:625/1670 train_time:54908ms step_avg:87.85ms
step:625/1670 val_loss:3.6148 train_time:54998ms step_avg:88.00ms
step:626/1670 train_time:55017ms step_avg:87.89ms
step:627/1670 train_time:55088ms step_avg:87.86ms
step:628/1670 train_time:55176ms step_avg:87.86ms
step:629/1670 train_time:55265ms step_avg:87.86ms
step:630/1670 train_time:55354ms step_avg:87.86ms
step:631/1670 train_time:55442ms step_avg:87.86ms
step:632/1670 train_time:55529ms step_avg:87.86ms
step:633/1670 train_time:55617ms step_avg:87.86ms
step:634/1670 train_time:55705ms step_avg:87.86ms
step:635/1670 train_time:55794ms step_avg:87.86ms
step:636/1670 train_time:55884ms step_avg:87.87ms
step:637/1670 train_time:55975ms step_avg:87.87ms
step:638/1670 train_time:56065ms step_avg:87.88ms
step:639/1670 train_time:56154ms step_avg:87.88ms
step:640/1670 train_time:56243ms step_avg:87.88ms
step:641/1670 train_time:56331ms step_avg:87.88ms
step:642/1670 train_time:56419ms step_avg:87.88ms
step:643/1670 train_time:56507ms step_avg:87.88ms
step:644/1670 train_time:56595ms step_avg:87.88ms
step:645/1670 train_time:56683ms step_avg:87.88ms
step:646/1670 train_time:56772ms step_avg:87.88ms
step:647/1670 train_time:56862ms step_avg:87.89ms
step:648/1670 train_time:56952ms step_avg:87.89ms
step:649/1670 train_time:57042ms step_avg:87.89ms
step:650/1670 train_time:57131ms step_avg:87.89ms
step:651/1670 train_time:57220ms step_avg:87.90ms
step:652/1670 train_time:57308ms step_avg:87.90ms
step:653/1670 train_time:57398ms step_avg:87.90ms
step:654/1670 train_time:57486ms step_avg:87.90ms
step:655/1670 train_time:57574ms step_avg:87.90ms
step:656/1670 train_time:57663ms step_avg:87.90ms
step:657/1670 train_time:57751ms step_avg:87.90ms
step:658/1670 train_time:57841ms step_avg:87.90ms
step:659/1670 train_time:57930ms step_avg:87.91ms
step:660/1670 train_time:58020ms step_avg:87.91ms
step:661/1670 train_time:58110ms step_avg:87.91ms
step:662/1670 train_time:58199ms step_avg:87.91ms
step:663/1670 train_time:58288ms step_avg:87.92ms
step:664/1670 train_time:58376ms step_avg:87.92ms
step:665/1670 train_time:58465ms step_avg:87.92ms
step:666/1670 train_time:58553ms step_avg:87.92ms
step:667/1670 train_time:58641ms step_avg:87.92ms
step:668/1670 train_time:58730ms step_avg:87.92ms
step:669/1670 train_time:58819ms step_avg:87.92ms
step:670/1670 train_time:58907ms step_avg:87.92ms
step:671/1670 train_time:58996ms step_avg:87.92ms
step:672/1670 train_time:59087ms step_avg:87.93ms
step:673/1670 train_time:59176ms step_avg:87.93ms
step:674/1670 train_time:59266ms step_avg:87.93ms
step:675/1670 train_time:59355ms step_avg:87.93ms
step:676/1670 train_time:59444ms step_avg:87.93ms
step:677/1670 train_time:59533ms step_avg:87.94ms
step:678/1670 train_time:59622ms step_avg:87.94ms
step:679/1670 train_time:59710ms step_avg:87.94ms
step:680/1670 train_time:59799ms step_avg:87.94ms
step:681/1670 train_time:59888ms step_avg:87.94ms
step:682/1670 train_time:59978ms step_avg:87.94ms
step:683/1670 train_time:60067ms step_avg:87.95ms
step:684/1670 train_time:60157ms step_avg:87.95ms
step:685/1670 train_time:60246ms step_avg:87.95ms
step:686/1670 train_time:60335ms step_avg:87.95ms
step:687/1670 train_time:60424ms step_avg:87.95ms
step:688/1670 train_time:60512ms step_avg:87.95ms
step:689/1670 train_time:60601ms step_avg:87.95ms
step:690/1670 train_time:60689ms step_avg:87.95ms
step:691/1670 train_time:60778ms step_avg:87.96ms
step:692/1670 train_time:60867ms step_avg:87.96ms
step:693/1670 train_time:60957ms step_avg:87.96ms
step:694/1670 train_time:61046ms step_avg:87.96ms
step:695/1670 train_time:61135ms step_avg:87.96ms
step:696/1670 train_time:61225ms step_avg:87.97ms
step:697/1670 train_time:61314ms step_avg:87.97ms
step:698/1670 train_time:61403ms step_avg:87.97ms
step:699/1670 train_time:61491ms step_avg:87.97ms
step:700/1670 train_time:61580ms step_avg:87.97ms
step:701/1670 train_time:61668ms step_avg:87.97ms
step:702/1670 train_time:61757ms step_avg:87.97ms
step:703/1670 train_time:61847ms step_avg:87.98ms
step:704/1670 train_time:61936ms step_avg:87.98ms
step:705/1670 train_time:62025ms step_avg:87.98ms
step:706/1670 train_time:62114ms step_avg:87.98ms
step:707/1670 train_time:62202ms step_avg:87.98ms
step:708/1670 train_time:62291ms step_avg:87.98ms
step:709/1670 train_time:62380ms step_avg:87.98ms
step:710/1670 train_time:62468ms step_avg:87.98ms
step:711/1670 train_time:62556ms step_avg:87.98ms
step:712/1670 train_time:62645ms step_avg:87.98ms
step:713/1670 train_time:62734ms step_avg:87.99ms
step:714/1670 train_time:62823ms step_avg:87.99ms
step:715/1670 train_time:62912ms step_avg:87.99ms
step:716/1670 train_time:63001ms step_avg:87.99ms
step:717/1670 train_time:63090ms step_avg:87.99ms
step:718/1670 train_time:63179ms step_avg:87.99ms
step:719/1670 train_time:63268ms step_avg:87.99ms
step:720/1670 train_time:63357ms step_avg:88.00ms
step:721/1670 train_time:63446ms step_avg:88.00ms
step:722/1670 train_time:63535ms step_avg:88.00ms
step:723/1670 train_time:63624ms step_avg:88.00ms
step:724/1670 train_time:63712ms step_avg:88.00ms
step:725/1670 train_time:63802ms step_avg:88.00ms
step:726/1670 train_time:63891ms step_avg:88.00ms
step:727/1670 train_time:63980ms step_avg:88.01ms
step:728/1670 train_time:64069ms step_avg:88.01ms
step:729/1670 train_time:64158ms step_avg:88.01ms
step:730/1670 train_time:64248ms step_avg:88.01ms
step:731/1670 train_time:64338ms step_avg:88.01ms
step:732/1670 train_time:64426ms step_avg:88.01ms
step:733/1670 train_time:64515ms step_avg:88.01ms
step:734/1670 train_time:64604ms step_avg:88.02ms
step:735/1670 train_time:64692ms step_avg:88.02ms
step:736/1670 train_time:64781ms step_avg:88.02ms
step:737/1670 train_time:64870ms step_avg:88.02ms
step:738/1670 train_time:64959ms step_avg:88.02ms
step:739/1670 train_time:65048ms step_avg:88.02ms
step:740/1670 train_time:65137ms step_avg:88.02ms
step:741/1670 train_time:65226ms step_avg:88.02ms
step:742/1670 train_time:65315ms step_avg:88.03ms
step:743/1670 train_time:65405ms step_avg:88.03ms
step:744/1670 train_time:65494ms step_avg:88.03ms
step:745/1670 train_time:65583ms step_avg:88.03ms
step:746/1670 train_time:65671ms step_avg:88.03ms
step:747/1670 train_time:65760ms step_avg:88.03ms
step:748/1670 train_time:65849ms step_avg:88.03ms
step:749/1670 train_time:65939ms step_avg:88.04ms
step:750/1670 train_time:66028ms step_avg:88.04ms
step:750/1670 val_loss:3.5672 train_time:66119ms step_avg:88.16ms
step:751/1670 train_time:66138ms step_avg:88.07ms
step:752/1670 train_time:66213ms step_avg:88.05ms
step:753/1670 train_time:66305ms step_avg:88.05ms
step:754/1670 train_time:66394ms step_avg:88.06ms
step:755/1670 train_time:66482ms step_avg:88.06ms
step:756/1670 train_time:66570ms step_avg:88.06ms
step:757/1670 train_time:66657ms step_avg:88.05ms
step:758/1670 train_time:66745ms step_avg:88.05ms
step:759/1670 train_time:66833ms step_avg:88.05ms
step:760/1670 train_time:66921ms step_avg:88.05ms
step:761/1670 train_time:67010ms step_avg:88.05ms
step:762/1670 train_time:67101ms step_avg:88.06ms
step:763/1670 train_time:67193ms step_avg:88.06ms
step:764/1670 train_time:67284ms step_avg:88.07ms
step:765/1670 train_time:67373ms step_avg:88.07ms
step:766/1670 train_time:67463ms step_avg:88.07ms
step:767/1670 train_time:67552ms step_avg:88.07ms
step:768/1670 train_time:67639ms step_avg:88.07ms
step:769/1670 train_time:67728ms step_avg:88.07ms
step:770/1670 train_time:67816ms step_avg:88.07ms
step:771/1670 train_time:67903ms step_avg:88.07ms
step:772/1670 train_time:67992ms step_avg:88.07ms
step:773/1670 train_time:68081ms step_avg:88.07ms
step:774/1670 train_time:68171ms step_avg:88.08ms
step:775/1670 train_time:68262ms step_avg:88.08ms
step:776/1670 train_time:68353ms step_avg:88.08ms
step:777/1670 train_time:68442ms step_avg:88.08ms
step:778/1670 train_time:68530ms step_avg:88.08ms
step:779/1670 train_time:68618ms step_avg:88.09ms
step:780/1670 train_time:68707ms step_avg:88.09ms
step:781/1670 train_time:68796ms step_avg:88.09ms
step:782/1670 train_time:68883ms step_avg:88.09ms
step:783/1670 train_time:68973ms step_avg:88.09ms
step:784/1670 train_time:69061ms step_avg:88.09ms
step:785/1670 train_time:69150ms step_avg:88.09ms
step:786/1670 train_time:69241ms step_avg:88.09ms
step:787/1670 train_time:69331ms step_avg:88.10ms
step:788/1670 train_time:69420ms step_avg:88.10ms
step:789/1670 train_time:69509ms step_avg:88.10ms
step:790/1670 train_time:69598ms step_avg:88.10ms
step:791/1670 train_time:69686ms step_avg:88.10ms
step:792/1670 train_time:69774ms step_avg:88.10ms
step:793/1670 train_time:69862ms step_avg:88.10ms
step:794/1670 train_time:69951ms step_avg:88.10ms
step:795/1670 train_time:70039ms step_avg:88.10ms
step:796/1670 train_time:70128ms step_avg:88.10ms
step:797/1670 train_time:70218ms step_avg:88.10ms
step:798/1670 train_time:70307ms step_avg:88.10ms
step:799/1670 train_time:70397ms step_avg:88.11ms
step:800/1670 train_time:70486ms step_avg:88.11ms
step:801/1670 train_time:70575ms step_avg:88.11ms
step:802/1670 train_time:70663ms step_avg:88.11ms
step:803/1670 train_time:70751ms step_avg:88.11ms
step:804/1670 train_time:70839ms step_avg:88.11ms
step:805/1670 train_time:70928ms step_avg:88.11ms
step:806/1670 train_time:71017ms step_avg:88.11ms
step:807/1670 train_time:71107ms step_avg:88.11ms
step:808/1670 train_time:71197ms step_avg:88.11ms
step:809/1670 train_time:71286ms step_avg:88.12ms
step:810/1670 train_time:71375ms step_avg:88.12ms
step:811/1670 train_time:71464ms step_avg:88.12ms
step:812/1670 train_time:71553ms step_avg:88.12ms
step:813/1670 train_time:71641ms step_avg:88.12ms
step:814/1670 train_time:71730ms step_avg:88.12ms
step:815/1670 train_time:71819ms step_avg:88.12ms
step:816/1670 train_time:71908ms step_avg:88.12ms
step:817/1670 train_time:71997ms step_avg:88.12ms
step:818/1670 train_time:72086ms step_avg:88.13ms
step:819/1670 train_time:72176ms step_avg:88.13ms
step:820/1670 train_time:72264ms step_avg:88.13ms
step:821/1670 train_time:72353ms step_avg:88.13ms
step:822/1670 train_time:72443ms step_avg:88.13ms
step:823/1670 train_time:72532ms step_avg:88.13ms
step:824/1670 train_time:72620ms step_avg:88.13ms
step:825/1670 train_time:72710ms step_avg:88.13ms
step:826/1670 train_time:72799ms step_avg:88.13ms
step:827/1670 train_time:72887ms step_avg:88.13ms
step:828/1670 train_time:72976ms step_avg:88.13ms
step:829/1670 train_time:73064ms step_avg:88.14ms
step:830/1670 train_time:73154ms step_avg:88.14ms
step:831/1670 train_time:73242ms step_avg:88.14ms
step:832/1670 train_time:73331ms step_avg:88.14ms
step:833/1670 train_time:73420ms step_avg:88.14ms
step:834/1670 train_time:73509ms step_avg:88.14ms
step:835/1670 train_time:73599ms step_avg:88.14ms
step:836/1670 train_time:73688ms step_avg:88.14ms
step:837/1670 train_time:73776ms step_avg:88.14ms
step:838/1670 train_time:73865ms step_avg:88.14ms
step:839/1670 train_time:73955ms step_avg:88.15ms
step:840/1670 train_time:74043ms step_avg:88.15ms
step:841/1670 train_time:74132ms step_avg:88.15ms
step:842/1670 train_time:74220ms step_avg:88.15ms
step:843/1670 train_time:74309ms step_avg:88.15ms
step:844/1670 train_time:74398ms step_avg:88.15ms
step:845/1670 train_time:74488ms step_avg:88.15ms
step:846/1670 train_time:74577ms step_avg:88.15ms
step:847/1670 train_time:74668ms step_avg:88.16ms
step:848/1670 train_time:74757ms step_avg:88.16ms
step:849/1670 train_time:74845ms step_avg:88.16ms
step:850/1670 train_time:74935ms step_avg:88.16ms
step:851/1670 train_time:75023ms step_avg:88.16ms
step:852/1670 train_time:75113ms step_avg:88.16ms
step:853/1670 train_time:75202ms step_avg:88.16ms
step:854/1670 train_time:75291ms step_avg:88.16ms
step:855/1670 train_time:75381ms step_avg:88.16ms
step:856/1670 train_time:75469ms step_avg:88.16ms
step:857/1670 train_time:75558ms step_avg:88.17ms
step:858/1670 train_time:75647ms step_avg:88.17ms
step:859/1670 train_time:75736ms step_avg:88.17ms
step:860/1670 train_time:75825ms step_avg:88.17ms
step:861/1670 train_time:75915ms step_avg:88.17ms
step:862/1670 train_time:76004ms step_avg:88.17ms
step:863/1670 train_time:76094ms step_avg:88.17ms
step:864/1670 train_time:76182ms step_avg:88.17ms
step:865/1670 train_time:76271ms step_avg:88.17ms
step:866/1670 train_time:76360ms step_avg:88.18ms
step:867/1670 train_time:76449ms step_avg:88.18ms
step:868/1670 train_time:76538ms step_avg:88.18ms
step:869/1670 train_time:76627ms step_avg:88.18ms
step:870/1670 train_time:76716ms step_avg:88.18ms
step:871/1670 train_time:76805ms step_avg:88.18ms
step:872/1670 train_time:76894ms step_avg:88.18ms
step:873/1670 train_time:76982ms step_avg:88.18ms
step:874/1670 train_time:77072ms step_avg:88.18ms
step:875/1670 train_time:77160ms step_avg:88.18ms
step:875/1670 val_loss:3.5183 train_time:77250ms step_avg:88.29ms
step:876/1670 train_time:77269ms step_avg:88.21ms
step:877/1670 train_time:77342ms step_avg:88.19ms
step:878/1670 train_time:77437ms step_avg:88.20ms
step:879/1670 train_time:77527ms step_avg:88.20ms
step:880/1670 train_time:77614ms step_avg:88.20ms
step:881/1670 train_time:77703ms step_avg:88.20ms
step:882/1670 train_time:77791ms step_avg:88.20ms
step:883/1670 train_time:77878ms step_avg:88.20ms
step:884/1670 train_time:77966ms step_avg:88.20ms
step:885/1670 train_time:78054ms step_avg:88.20ms
step:886/1670 train_time:78141ms step_avg:88.20ms
step:887/1670 train_time:78231ms step_avg:88.20ms
step:888/1670 train_time:78323ms step_avg:88.20ms
step:889/1670 train_time:78414ms step_avg:88.20ms
step:890/1670 train_time:78505ms step_avg:88.21ms
step:891/1670 train_time:78593ms step_avg:88.21ms
step:892/1670 train_time:78682ms step_avg:88.21ms
step:893/1670 train_time:78771ms step_avg:88.21ms
step:894/1670 train_time:78858ms step_avg:88.21ms
step:895/1670 train_time:78946ms step_avg:88.21ms
step:896/1670 train_time:79034ms step_avg:88.21ms
step:897/1670 train_time:79122ms step_avg:88.21ms
step:898/1670 train_time:79211ms step_avg:88.21ms
step:899/1670 train_time:79301ms step_avg:88.21ms
step:900/1670 train_time:79393ms step_avg:88.21ms
step:901/1670 train_time:79484ms step_avg:88.22ms
step:902/1670 train_time:79574ms step_avg:88.22ms
step:903/1670 train_time:79663ms step_avg:88.22ms
step:904/1670 train_time:79752ms step_avg:88.22ms
step:905/1670 train_time:79840ms step_avg:88.22ms
step:906/1670 train_time:79928ms step_avg:88.22ms
step:907/1670 train_time:80016ms step_avg:88.22ms
step:908/1670 train_time:80104ms step_avg:88.22ms
step:909/1670 train_time:80193ms step_avg:88.22ms
step:910/1670 train_time:80282ms step_avg:88.22ms
step:911/1670 train_time:80374ms step_avg:88.23ms
step:912/1670 train_time:80464ms step_avg:88.23ms
step:913/1670 train_time:80554ms step_avg:88.23ms
step:914/1670 train_time:80643ms step_avg:88.23ms
step:915/1670 train_time:80732ms step_avg:88.23ms
step:916/1670 train_time:80821ms step_avg:88.23ms
step:917/1670 train_time:80909ms step_avg:88.23ms
step:918/1670 train_time:80997ms step_avg:88.23ms
step:919/1670 train_time:81085ms step_avg:88.23ms
step:920/1670 train_time:81174ms step_avg:88.23ms
step:921/1670 train_time:81263ms step_avg:88.23ms
step:922/1670 train_time:81353ms step_avg:88.24ms
step:923/1670 train_time:81443ms step_avg:88.24ms
step:924/1670 train_time:81533ms step_avg:88.24ms
step:925/1670 train_time:81622ms step_avg:88.24ms
step:926/1670 train_time:81711ms step_avg:88.24ms
step:927/1670 train_time:81799ms step_avg:88.24ms
step:928/1670 train_time:81888ms step_avg:88.24ms
step:929/1670 train_time:81976ms step_avg:88.24ms
step:930/1670 train_time:82065ms step_avg:88.24ms
step:931/1670 train_time:82154ms step_avg:88.24ms
step:932/1670 train_time:82243ms step_avg:88.24ms
step:933/1670 train_time:82333ms step_avg:88.24ms
step:934/1670 train_time:82423ms step_avg:88.25ms
step:935/1670 train_time:82513ms step_avg:88.25ms
step:936/1670 train_time:82602ms step_avg:88.25ms
step:937/1670 train_time:82692ms step_avg:88.25ms
step:938/1670 train_time:82780ms step_avg:88.25ms
step:939/1670 train_time:82870ms step_avg:88.25ms
step:940/1670 train_time:82958ms step_avg:88.25ms
step:941/1670 train_time:83047ms step_avg:88.25ms
step:942/1670 train_time:83136ms step_avg:88.25ms
step:943/1670 train_time:83225ms step_avg:88.26ms
step:944/1670 train_time:83315ms step_avg:88.26ms
step:945/1670 train_time:83404ms step_avg:88.26ms
step:946/1670 train_time:83494ms step_avg:88.26ms
step:947/1670 train_time:83583ms step_avg:88.26ms
step:948/1670 train_time:83673ms step_avg:88.26ms
step:949/1670 train_time:83763ms step_avg:88.26ms
step:950/1670 train_time:83851ms step_avg:88.26ms
step:951/1670 train_time:83941ms step_avg:88.27ms
step:952/1670 train_time:84030ms step_avg:88.27ms
step:953/1670 train_time:84118ms step_avg:88.27ms
step:954/1670 train_time:84207ms step_avg:88.27ms
step:955/1670 train_time:84296ms step_avg:88.27ms
step:956/1670 train_time:84384ms step_avg:88.27ms
step:957/1670 train_time:84474ms step_avg:88.27ms
step:958/1670 train_time:84563ms step_avg:88.27ms
step:959/1670 train_time:84652ms step_avg:88.27ms
step:960/1670 train_time:84741ms step_avg:88.27ms
step:961/1670 train_time:84831ms step_avg:88.27ms
step:962/1670 train_time:84919ms step_avg:88.27ms
step:963/1670 train_time:85008ms step_avg:88.27ms
step:964/1670 train_time:85097ms step_avg:88.28ms
step:965/1670 train_time:85187ms step_avg:88.28ms
step:966/1670 train_time:85275ms step_avg:88.28ms
step:967/1670 train_time:85365ms step_avg:88.28ms
step:968/1670 train_time:85453ms step_avg:88.28ms
step:969/1670 train_time:85543ms step_avg:88.28ms
step:970/1670 train_time:85632ms step_avg:88.28ms
step:971/1670 train_time:85722ms step_avg:88.28ms
step:972/1670 train_time:85810ms step_avg:88.28ms
step:973/1670 train_time:85899ms step_avg:88.28ms
step:974/1670 train_time:85988ms step_avg:88.28ms
step:975/1670 train_time:86076ms step_avg:88.28ms
step:976/1670 train_time:86166ms step_avg:88.28ms
step:977/1670 train_time:86255ms step_avg:88.29ms
step:978/1670 train_time:86343ms step_avg:88.29ms
step:979/1670 train_time:86433ms step_avg:88.29ms
step:980/1670 train_time:86522ms step_avg:88.29ms
step:981/1670 train_time:86611ms step_avg:88.29ms
step:982/1670 train_time:86700ms step_avg:88.29ms
step:983/1670 train_time:86789ms step_avg:88.29ms
step:984/1670 train_time:86878ms step_avg:88.29ms
step:985/1670 train_time:86966ms step_avg:88.29ms
step:986/1670 train_time:87055ms step_avg:88.29ms
step:987/1670 train_time:87145ms step_avg:88.29ms
step:988/1670 train_time:87234ms step_avg:88.29ms
step:989/1670 train_time:87323ms step_avg:88.29ms
step:990/1670 train_time:87412ms step_avg:88.29ms
step:991/1670 train_time:87500ms step_avg:88.29ms
step:992/1670 train_time:87590ms step_avg:88.30ms
step:993/1670 train_time:87679ms step_avg:88.30ms
step:994/1670 train_time:87768ms step_avg:88.30ms
step:995/1670 train_time:87856ms step_avg:88.30ms
step:996/1670 train_time:87945ms step_avg:88.30ms
step:997/1670 train_time:88034ms step_avg:88.30ms
step:998/1670 train_time:88123ms step_avg:88.30ms
step:999/1670 train_time:88212ms step_avg:88.30ms
step:1000/1670 train_time:88301ms step_avg:88.30ms
step:1000/1670 val_loss:3.4677 train_time:88391ms step_avg:88.39ms
step:1001/1670 train_time:88412ms step_avg:88.32ms
step:1002/1670 train_time:88485ms step_avg:88.31ms
step:1003/1670 train_time:88578ms step_avg:88.31ms
step:1004/1670 train_time:88667ms step_avg:88.31ms
step:1005/1670 train_time:88755ms step_avg:88.31ms
step:1006/1670 train_time:88843ms step_avg:88.31ms
step:1007/1670 train_time:88931ms step_avg:88.31ms
step:1008/1670 train_time:89019ms step_avg:88.31ms
step:1009/1670 train_time:89106ms step_avg:88.31ms
step:1010/1670 train_time:89194ms step_avg:88.31ms
step:1011/1670 train_time:89282ms step_avg:88.31ms
step:1012/1670 train_time:89372ms step_avg:88.31ms
step:1013/1670 train_time:89465ms step_avg:88.32ms
step:1014/1670 train_time:89557ms step_avg:88.32ms
step:1015/1670 train_time:89645ms step_avg:88.32ms
step:1016/1670 train_time:89734ms step_avg:88.32ms
step:1017/1670 train_time:89823ms step_avg:88.32ms
step:1018/1670 train_time:89911ms step_avg:88.32ms
step:1019/1670 train_time:90000ms step_avg:88.32ms
step:1020/1670 train_time:90087ms step_avg:88.32ms
step:1021/1670 train_time:90175ms step_avg:88.32ms
step:1022/1670 train_time:90264ms step_avg:88.32ms
step:1023/1670 train_time:90353ms step_avg:88.32ms
step:1024/1670 train_time:90444ms step_avg:88.32ms
step:1025/1670 train_time:90534ms step_avg:88.33ms
step:1026/1670 train_time:90624ms step_avg:88.33ms
step:1027/1670 train_time:90714ms step_avg:88.33ms
step:1028/1670 train_time:90804ms step_avg:88.33ms
step:1029/1670 train_time:90892ms step_avg:88.33ms
step:1030/1670 train_time:90980ms step_avg:88.33ms
step:1031/1670 train_time:91069ms step_avg:88.33ms
step:1032/1670 train_time:91157ms step_avg:88.33ms
step:1033/1670 train_time:91244ms step_avg:88.33ms
step:1034/1670 train_time:91334ms step_avg:88.33ms
step:1035/1670 train_time:91424ms step_avg:88.33ms
step:1036/1670 train_time:91514ms step_avg:88.33ms
step:1037/1670 train_time:91603ms step_avg:88.33ms
step:1038/1670 train_time:91693ms step_avg:88.34ms
step:1039/1670 train_time:91783ms step_avg:88.34ms
step:1040/1670 train_time:91872ms step_avg:88.34ms
step:1041/1670 train_time:91961ms step_avg:88.34ms
step:1042/1670 train_time:92050ms step_avg:88.34ms
step:1043/1670 train_time:92139ms step_avg:88.34ms
step:1044/1670 train_time:92227ms step_avg:88.34ms
step:1045/1670 train_time:92316ms step_avg:88.34ms
step:1046/1670 train_time:92405ms step_avg:88.34ms
step:1047/1670 train_time:92495ms step_avg:88.34ms
step:1048/1670 train_time:92584ms step_avg:88.34ms
step:1049/1670 train_time:92673ms step_avg:88.34ms
step:1050/1670 train_time:92763ms step_avg:88.35ms
step:1051/1670 train_time:92852ms step_avg:88.35ms
step:1052/1670 train_time:92941ms step_avg:88.35ms
step:1053/1670 train_time:93029ms step_avg:88.35ms
step:1054/1670 train_time:93117ms step_avg:88.35ms
step:1055/1670 train_time:93205ms step_avg:88.35ms
step:1056/1670 train_time:93294ms step_avg:88.35ms
step:1057/1670 train_time:93383ms step_avg:88.35ms
step:1058/1670 train_time:93473ms step_avg:88.35ms
step:1059/1670 train_time:93562ms step_avg:88.35ms
step:1060/1670 train_time:93652ms step_avg:88.35ms
step:1061/1670 train_time:93741ms step_avg:88.35ms
step:1062/1670 train_time:93830ms step_avg:88.35ms
step:1063/1670 train_time:93919ms step_avg:88.35ms
step:1064/1670 train_time:94007ms step_avg:88.35ms
step:1065/1670 train_time:94096ms step_avg:88.35ms
step:1066/1670 train_time:94184ms step_avg:88.35ms
step:1067/1670 train_time:94273ms step_avg:88.35ms
step:1068/1670 train_time:94363ms step_avg:88.35ms
step:1069/1670 train_time:94453ms step_avg:88.36ms
step:1070/1670 train_time:94542ms step_avg:88.36ms
step:1071/1670 train_time:94630ms step_avg:88.36ms
step:1072/1670 train_time:94719ms step_avg:88.36ms
step:1073/1670 train_time:94808ms step_avg:88.36ms
step:1074/1670 train_time:94897ms step_avg:88.36ms
step:1075/1670 train_time:94986ms step_avg:88.36ms
step:1076/1670 train_time:95075ms step_avg:88.36ms
step:1077/1670 train_time:95163ms step_avg:88.36ms
step:1078/1670 train_time:95252ms step_avg:88.36ms
step:1079/1670 train_time:95340ms step_avg:88.36ms
step:1080/1670 train_time:95429ms step_avg:88.36ms
step:1081/1670 train_time:95519ms step_avg:88.36ms
step:1082/1670 train_time:95608ms step_avg:88.36ms
step:1083/1670 train_time:95697ms step_avg:88.36ms
step:1084/1670 train_time:95785ms step_avg:88.36ms
step:1085/1670 train_time:95874ms step_avg:88.36ms
step:1086/1670 train_time:95963ms step_avg:88.36ms
step:1087/1670 train_time:96052ms step_avg:88.36ms
step:1088/1670 train_time:96141ms step_avg:88.36ms
step:1089/1670 train_time:96229ms step_avg:88.36ms
step:1090/1670 train_time:96319ms step_avg:88.37ms
step:1091/1670 train_time:96408ms step_avg:88.37ms
step:1092/1670 train_time:96497ms step_avg:88.37ms
step:1093/1670 train_time:96586ms step_avg:88.37ms
step:1094/1670 train_time:96676ms step_avg:88.37ms
step:1095/1670 train_time:96765ms step_avg:88.37ms
step:1096/1670 train_time:96855ms step_avg:88.37ms
step:1097/1670 train_time:96944ms step_avg:88.37ms
step:1098/1670 train_time:97035ms step_avg:88.37ms
step:1099/1670 train_time:97125ms step_avg:88.38ms
step:1100/1670 train_time:97214ms step_avg:88.38ms
step:1101/1670 train_time:97304ms step_avg:88.38ms
step:1102/1670 train_time:97394ms step_avg:88.38ms
step:1103/1670 train_time:97483ms step_avg:88.38ms
step:1104/1670 train_time:97573ms step_avg:88.38ms
step:1105/1670 train_time:97664ms step_avg:88.38ms
step:1106/1670 train_time:97754ms step_avg:88.39ms
step:1107/1670 train_time:97844ms step_avg:88.39ms
step:1108/1670 train_time:97934ms step_avg:88.39ms
step:1109/1670 train_time:98023ms step_avg:88.39ms
step:1110/1670 train_time:98112ms step_avg:88.39ms
step:1111/1670 train_time:98202ms step_avg:88.39ms
step:1112/1670 train_time:98293ms step_avg:88.39ms
step:1113/1670 train_time:98383ms step_avg:88.39ms
step:1114/1670 train_time:98473ms step_avg:88.40ms
step:1115/1670 train_time:98562ms step_avg:88.40ms
step:1116/1670 train_time:98653ms step_avg:88.40ms
step:1117/1670 train_time:98743ms step_avg:88.40ms
step:1118/1670 train_time:98833ms step_avg:88.40ms
step:1119/1670 train_time:98923ms step_avg:88.40ms
step:1120/1670 train_time:99012ms step_avg:88.40ms
step:1121/1670 train_time:99101ms step_avg:88.40ms
step:1122/1670 train_time:99191ms step_avg:88.41ms
step:1123/1670 train_time:99281ms step_avg:88.41ms
step:1124/1670 train_time:99370ms step_avg:88.41ms
step:1125/1670 train_time:99460ms step_avg:88.41ms
step:1125/1670 val_loss:3.4135 train_time:99551ms step_avg:88.49ms
step:1126/1670 train_time:99571ms step_avg:88.43ms
step:1127/1670 train_time:99642ms step_avg:88.41ms
step:1128/1670 train_time:99731ms step_avg:88.41ms
step:1129/1670 train_time:99822ms step_avg:88.42ms
step:1130/1670 train_time:99910ms step_avg:88.42ms
step:1131/1670 train_time:99999ms step_avg:88.42ms
step:1132/1670 train_time:100087ms step_avg:88.42ms
step:1133/1670 train_time:100176ms step_avg:88.42ms
step:1134/1670 train_time:100266ms step_avg:88.42ms
step:1135/1670 train_time:100354ms step_avg:88.42ms
step:1136/1670 train_time:100445ms step_avg:88.42ms
step:1137/1670 train_time:100536ms step_avg:88.42ms
step:1138/1670 train_time:100628ms step_avg:88.43ms
step:1139/1670 train_time:100721ms step_avg:88.43ms
step:1140/1670 train_time:100811ms step_avg:88.43ms
step:1141/1670 train_time:100900ms step_avg:88.43ms
step:1142/1670 train_time:100990ms step_avg:88.43ms
step:1143/1670 train_time:101079ms step_avg:88.43ms
step:1144/1670 train_time:101167ms step_avg:88.43ms
step:1145/1670 train_time:101256ms step_avg:88.43ms
step:1146/1670 train_time:101345ms step_avg:88.43ms
step:1147/1670 train_time:101434ms step_avg:88.43ms
step:1148/1670 train_time:101525ms step_avg:88.44ms
step:1149/1670 train_time:101616ms step_avg:88.44ms
step:1150/1670 train_time:101707ms step_avg:88.44ms
step:1151/1670 train_time:101797ms step_avg:88.44ms
step:1152/1670 train_time:101887ms step_avg:88.44ms
step:1153/1670 train_time:101977ms step_avg:88.44ms
step:1154/1670 train_time:102066ms step_avg:88.45ms
step:1155/1670 train_time:102155ms step_avg:88.45ms
step:1156/1670 train_time:102244ms step_avg:88.45ms
step:1157/1670 train_time:102332ms step_avg:88.45ms
step:1158/1670 train_time:102423ms step_avg:88.45ms
step:1159/1670 train_time:102512ms step_avg:88.45ms
step:1160/1670 train_time:102604ms step_avg:88.45ms
step:1161/1670 train_time:102695ms step_avg:88.45ms
step:1162/1670 train_time:102785ms step_avg:88.46ms
step:1163/1670 train_time:102875ms step_avg:88.46ms
step:1164/1670 train_time:102965ms step_avg:88.46ms
step:1165/1670 train_time:103053ms step_avg:88.46ms
step:1166/1670 train_time:103143ms step_avg:88.46ms
step:1167/1670 train_time:103231ms step_avg:88.46ms
step:1168/1670 train_time:103321ms step_avg:88.46ms
step:1169/1670 train_time:103410ms step_avg:88.46ms
step:1170/1670 train_time:103499ms step_avg:88.46ms
step:1171/1670 train_time:103591ms step_avg:88.46ms
step:1172/1670 train_time:103682ms step_avg:88.47ms
step:1173/1670 train_time:103771ms step_avg:88.47ms
step:1174/1670 train_time:103862ms step_avg:88.47ms
step:1175/1670 train_time:103951ms step_avg:88.47ms
step:1176/1670 train_time:104041ms step_avg:88.47ms
step:1177/1670 train_time:104130ms step_avg:88.47ms
step:1178/1670 train_time:104219ms step_avg:88.47ms
step:1179/1670 train_time:104309ms step_avg:88.47ms
step:1180/1670 train_time:104398ms step_avg:88.47ms
step:1181/1670 train_time:104488ms step_avg:88.47ms
step:1182/1670 train_time:104578ms step_avg:88.48ms
step:1183/1670 train_time:104668ms step_avg:88.48ms
step:1184/1670 train_time:104758ms step_avg:88.48ms
step:1185/1670 train_time:104848ms step_avg:88.48ms
step:1186/1670 train_time:104937ms step_avg:88.48ms
step:1187/1670 train_time:105028ms step_avg:88.48ms
step:1188/1670 train_time:105117ms step_avg:88.48ms
step:1189/1670 train_time:105206ms step_avg:88.48ms
step:1190/1670 train_time:105296ms step_avg:88.48ms
step:1191/1670 train_time:105386ms step_avg:88.49ms
step:1192/1670 train_time:105475ms step_avg:88.49ms
step:1193/1670 train_time:105566ms step_avg:88.49ms
step:1194/1670 train_time:105656ms step_avg:88.49ms
step:1195/1670 train_time:105747ms step_avg:88.49ms
step:1196/1670 train_time:105837ms step_avg:88.49ms
step:1197/1670 train_time:105927ms step_avg:88.49ms
step:1198/1670 train_time:106016ms step_avg:88.49ms
step:1199/1670 train_time:106106ms step_avg:88.50ms
step:1200/1670 train_time:106196ms step_avg:88.50ms
step:1201/1670 train_time:106286ms step_avg:88.50ms
step:1202/1670 train_time:106375ms step_avg:88.50ms
step:1203/1670 train_time:106464ms step_avg:88.50ms
step:1204/1670 train_time:106554ms step_avg:88.50ms
step:1205/1670 train_time:106644ms step_avg:88.50ms
step:1206/1670 train_time:106734ms step_avg:88.50ms
step:1207/1670 train_time:106825ms step_avg:88.50ms
step:1208/1670 train_time:106914ms step_avg:88.50ms
step:1209/1670 train_time:107003ms step_avg:88.51ms
step:1210/1670 train_time:107093ms step_avg:88.51ms
step:1211/1670 train_time:107182ms step_avg:88.51ms
step:1212/1670 train_time:107272ms step_avg:88.51ms
step:1213/1670 train_time:107361ms step_avg:88.51ms
step:1214/1670 train_time:107450ms step_avg:88.51ms
step:1215/1670 train_time:107540ms step_avg:88.51ms
step:1216/1670 train_time:107630ms step_avg:88.51ms
step:1217/1670 train_time:107721ms step_avg:88.51ms
step:1218/1670 train_time:107810ms step_avg:88.51ms
step:1219/1670 train_time:107900ms step_avg:88.52ms
step:1220/1670 train_time:107990ms step_avg:88.52ms
step:1221/1670 train_time:108080ms step_avg:88.52ms
step:1222/1670 train_time:108170ms step_avg:88.52ms
step:1223/1670 train_time:108259ms step_avg:88.52ms
step:1224/1670 train_time:108349ms step_avg:88.52ms
step:1225/1670 train_time:108438ms step_avg:88.52ms
step:1226/1670 train_time:108527ms step_avg:88.52ms
step:1227/1670 train_time:108617ms step_avg:88.52ms
step:1228/1670 train_time:108706ms step_avg:88.52ms
step:1229/1670 train_time:108796ms step_avg:88.52ms
step:1230/1670 train_time:108886ms step_avg:88.53ms
step:1231/1670 train_time:108975ms step_avg:88.53ms
step:1232/1670 train_time:109065ms step_avg:88.53ms
step:1233/1670 train_time:109155ms step_avg:88.53ms
step:1234/1670 train_time:109246ms step_avg:88.53ms
step:1235/1670 train_time:109335ms step_avg:88.53ms
step:1236/1670 train_time:109424ms step_avg:88.53ms
step:1237/1670 train_time:109513ms step_avg:88.53ms
step:1238/1670 train_time:109604ms step_avg:88.53ms
step:1239/1670 train_time:109693ms step_avg:88.53ms
step:1240/1670 train_time:109783ms step_avg:88.53ms
step:1241/1670 train_time:109873ms step_avg:88.54ms
step:1242/1670 train_time:109963ms step_avg:88.54ms
step:1243/1670 train_time:110052ms step_avg:88.54ms
step:1244/1670 train_time:110141ms step_avg:88.54ms
step:1245/1670 train_time:110230ms step_avg:88.54ms
step:1246/1670 train_time:110321ms step_avg:88.54ms
step:1247/1670 train_time:110409ms step_avg:88.54ms
step:1248/1670 train_time:110499ms step_avg:88.54ms
step:1249/1670 train_time:110589ms step_avg:88.54ms
step:1250/1670 train_time:110679ms step_avg:88.54ms
step:1250/1670 val_loss:3.3755 train_time:110770ms step_avg:88.62ms
step:1251/1670 train_time:110790ms step_avg:88.56ms
step:1252/1670 train_time:110867ms step_avg:88.55ms
step:1253/1670 train_time:110960ms step_avg:88.56ms
step:1254/1670 train_time:111051ms step_avg:88.56ms
step:1255/1670 train_time:111140ms step_avg:88.56ms
step:1256/1670 train_time:111228ms step_avg:88.56ms
step:1257/1670 train_time:111316ms step_avg:88.56ms
step:1258/1670 train_time:111405ms step_avg:88.56ms
step:1259/1670 train_time:111493ms step_avg:88.56ms
step:1260/1670 train_time:111582ms step_avg:88.56ms
step:1261/1670 train_time:111671ms step_avg:88.56ms
step:1262/1670 train_time:111762ms step_avg:88.56ms
step:1263/1670 train_time:111856ms step_avg:88.56ms
step:1264/1670 train_time:111949ms step_avg:88.57ms
step:1265/1670 train_time:112039ms step_avg:88.57ms
step:1266/1670 train_time:112129ms step_avg:88.57ms
step:1267/1670 train_time:112218ms step_avg:88.57ms
step:1268/1670 train_time:112307ms step_avg:88.57ms
step:1269/1670 train_time:112395ms step_avg:88.57ms
step:1270/1670 train_time:112484ms step_avg:88.57ms
step:1271/1670 train_time:112573ms step_avg:88.57ms
step:1272/1670 train_time:112661ms step_avg:88.57ms
step:1273/1670 train_time:112753ms step_avg:88.57ms
step:1274/1670 train_time:112844ms step_avg:88.57ms
step:1275/1670 train_time:112936ms step_avg:88.58ms
step:1276/1670 train_time:113027ms step_avg:88.58ms
step:1277/1670 train_time:113117ms step_avg:88.58ms
step:1278/1670 train_time:113206ms step_avg:88.58ms
step:1279/1670 train_time:113294ms step_avg:88.58ms
step:1280/1670 train_time:113383ms step_avg:88.58ms
step:1281/1670 train_time:113472ms step_avg:88.58ms
step:1282/1670 train_time:113561ms step_avg:88.58ms
step:1283/1670 train_time:113651ms step_avg:88.58ms
step:1284/1670 train_time:113740ms step_avg:88.58ms
step:1285/1670 train_time:113832ms step_avg:88.59ms
step:1286/1670 train_time:113923ms step_avg:88.59ms
step:1287/1670 train_time:114015ms step_avg:88.59ms
step:1288/1670 train_time:114107ms step_avg:88.59ms
step:1289/1670 train_time:114196ms step_avg:88.59ms
step:1290/1670 train_time:114286ms step_avg:88.59ms
step:1291/1670 train_time:114375ms step_avg:88.59ms
step:1292/1670 train_time:114464ms step_avg:88.59ms
step:1293/1670 train_time:114553ms step_avg:88.59ms
step:1294/1670 train_time:114642ms step_avg:88.59ms
step:1295/1670 train_time:114732ms step_avg:88.60ms
step:1296/1670 train_time:114823ms step_avg:88.60ms
step:1297/1670 train_time:114914ms step_avg:88.60ms
step:1298/1670 train_time:115005ms step_avg:88.60ms
step:1299/1670 train_time:115095ms step_avg:88.60ms
step:1300/1670 train_time:115185ms step_avg:88.60ms
step:1301/1670 train_time:115274ms step_avg:88.60ms
step:1302/1670 train_time:115363ms step_avg:88.60ms
step:1303/1670 train_time:115453ms step_avg:88.61ms
step:1304/1670 train_time:115542ms step_avg:88.61ms
step:1305/1670 train_time:115631ms step_avg:88.61ms
step:1306/1670 train_time:115720ms step_avg:88.61ms
step:1307/1670 train_time:115812ms step_avg:88.61ms
step:1308/1670 train_time:115903ms step_avg:88.61ms
step:1309/1670 train_time:115993ms step_avg:88.61ms
step:1310/1670 train_time:116084ms step_avg:88.61ms
step:1311/1670 train_time:116173ms step_avg:88.61ms
step:1312/1670 train_time:116263ms step_avg:88.61ms
step:1313/1670 train_time:116352ms step_avg:88.62ms
step:1314/1670 train_time:116442ms step_avg:88.62ms
step:1315/1670 train_time:116532ms step_avg:88.62ms
step:1316/1670 train_time:116622ms step_avg:88.62ms
step:1317/1670 train_time:116712ms step_avg:88.62ms
step:1318/1670 train_time:116801ms step_avg:88.62ms
step:1319/1670 train_time:116892ms step_avg:88.62ms
step:1320/1670 train_time:116984ms step_avg:88.62ms
step:1321/1670 train_time:117074ms step_avg:88.62ms
step:1322/1670 train_time:117163ms step_avg:88.63ms
step:1323/1670 train_time:117254ms step_avg:88.63ms
step:1324/1670 train_time:117343ms step_avg:88.63ms
step:1325/1670 train_time:117432ms step_avg:88.63ms
step:1326/1670 train_time:117523ms step_avg:88.63ms
step:1327/1670 train_time:117612ms step_avg:88.63ms
step:1328/1670 train_time:117702ms step_avg:88.63ms
step:1329/1670 train_time:117792ms step_avg:88.63ms
step:1330/1670 train_time:117881ms step_avg:88.63ms
step:1331/1670 train_time:117972ms step_avg:88.63ms
step:1332/1670 train_time:118062ms step_avg:88.63ms
step:1333/1670 train_time:118152ms step_avg:88.64ms
step:1334/1670 train_time:118242ms step_avg:88.64ms
step:1335/1670 train_time:118331ms step_avg:88.64ms
step:1336/1670 train_time:118420ms step_avg:88.64ms
step:1337/1670 train_time:118510ms step_avg:88.64ms
step:1338/1670 train_time:118599ms step_avg:88.64ms
step:1339/1670 train_time:118690ms step_avg:88.64ms
step:1340/1670 train_time:118779ms step_avg:88.64ms
step:1341/1670 train_time:118869ms step_avg:88.64ms
step:1342/1670 train_time:118958ms step_avg:88.64ms
step:1343/1670 train_time:119049ms step_avg:88.64ms
step:1344/1670 train_time:119138ms step_avg:88.64ms
step:1345/1670 train_time:119228ms step_avg:88.65ms
step:1346/1670 train_time:119317ms step_avg:88.65ms
step:1347/1670 train_time:119407ms step_avg:88.65ms
step:1348/1670 train_time:119496ms step_avg:88.65ms
step:1349/1670 train_time:119586ms step_avg:88.65ms
step:1350/1670 train_time:119676ms step_avg:88.65ms
step:1351/1670 train_time:119766ms step_avg:88.65ms
step:1352/1670 train_time:119855ms step_avg:88.65ms
step:1353/1670 train_time:119946ms step_avg:88.65ms
step:1354/1670 train_time:120037ms step_avg:88.65ms
step:1355/1670 train_time:120128ms step_avg:88.66ms
step:1356/1670 train_time:120217ms step_avg:88.66ms
step:1357/1670 train_time:120307ms step_avg:88.66ms
step:1358/1670 train_time:120396ms step_avg:88.66ms
step:1359/1670 train_time:120486ms step_avg:88.66ms
step:1360/1670 train_time:120575ms step_avg:88.66ms
step:1361/1670 train_time:120664ms step_avg:88.66ms
step:1362/1670 train_time:120755ms step_avg:88.66ms
step:1363/1670 train_time:120845ms step_avg:88.66ms
step:1364/1670 train_time:120935ms step_avg:88.66ms
step:1365/1670 train_time:121025ms step_avg:88.66ms
step:1366/1670 train_time:121114ms step_avg:88.66ms
step:1367/1670 train_time:121204ms step_avg:88.66ms
step:1368/1670 train_time:121294ms step_avg:88.66ms
step:1369/1670 train_time:121383ms step_avg:88.67ms
step:1370/1670 train_time:121472ms step_avg:88.67ms
step:1371/1670 train_time:121562ms step_avg:88.67ms
step:1372/1670 train_time:121653ms step_avg:88.67ms
step:1373/1670 train_time:121744ms step_avg:88.67ms
step:1374/1670 train_time:121833ms step_avg:88.67ms
step:1375/1670 train_time:121923ms step_avg:88.67ms
step:1375/1670 val_loss:3.3409 train_time:122014ms step_avg:88.74ms
step:1376/1670 train_time:122033ms step_avg:88.69ms
step:1377/1670 train_time:122108ms step_avg:88.68ms
step:1378/1670 train_time:122199ms step_avg:88.68ms
step:1379/1670 train_time:122289ms step_avg:88.68ms
step:1380/1670 train_time:122376ms step_avg:88.68ms
step:1381/1670 train_time:122465ms step_avg:88.68ms
step:1382/1670 train_time:122553ms step_avg:88.68ms
step:1383/1670 train_time:122641ms step_avg:88.68ms
step:1384/1670 train_time:122731ms step_avg:88.68ms
step:1385/1670 train_time:122821ms step_avg:88.68ms
step:1386/1670 train_time:122910ms step_avg:88.68ms
step:1387/1670 train_time:123002ms step_avg:88.68ms
step:1388/1670 train_time:123094ms step_avg:88.68ms
step:1389/1670 train_time:123186ms step_avg:88.69ms
step:1390/1670 train_time:123276ms step_avg:88.69ms
step:1391/1670 train_time:123366ms step_avg:88.69ms
step:1392/1670 train_time:123454ms step_avg:88.69ms
step:1393/1670 train_time:123544ms step_avg:88.69ms
step:1394/1670 train_time:123632ms step_avg:88.69ms
step:1395/1670 train_time:123722ms step_avg:88.69ms
step:1396/1670 train_time:123812ms step_avg:88.69ms
step:1397/1670 train_time:123901ms step_avg:88.69ms
step:1398/1670 train_time:123992ms step_avg:88.69ms
step:1399/1670 train_time:124084ms step_avg:88.69ms
step:1400/1670 train_time:124174ms step_avg:88.70ms
step:1401/1670 train_time:124265ms step_avg:88.70ms
step:1402/1670 train_time:124355ms step_avg:88.70ms
step:1403/1670 train_time:124445ms step_avg:88.70ms
step:1404/1670 train_time:124534ms step_avg:88.70ms
step:1405/1670 train_time:124624ms step_avg:88.70ms
step:1406/1670 train_time:124712ms step_avg:88.70ms
step:1407/1670 train_time:124801ms step_avg:88.70ms
step:1408/1670 train_time:124891ms step_avg:88.70ms
step:1409/1670 train_time:124981ms step_avg:88.70ms
step:1410/1670 train_time:125071ms step_avg:88.70ms
step:1411/1670 train_time:125161ms step_avg:88.70ms
step:1412/1670 train_time:125251ms step_avg:88.70ms
step:1413/1670 train_time:125342ms step_avg:88.71ms
step:1414/1670 train_time:125431ms step_avg:88.71ms
step:1415/1670 train_time:125521ms step_avg:88.71ms
step:1416/1670 train_time:125610ms step_avg:88.71ms
step:1417/1670 train_time:125699ms step_avg:88.71ms
step:1418/1670 train_time:125788ms step_avg:88.71ms
step:1419/1670 train_time:125878ms step_avg:88.71ms
step:1420/1670 train_time:125968ms step_avg:88.71ms
step:1421/1670 train_time:126058ms step_avg:88.71ms
step:1422/1670 train_time:126149ms step_avg:88.71ms
step:1423/1670 train_time:126239ms step_avg:88.71ms
step:1424/1670 train_time:126330ms step_avg:88.71ms
step:1425/1670 train_time:126419ms step_avg:88.71ms
step:1426/1670 train_time:126509ms step_avg:88.72ms
step:1427/1670 train_time:126598ms step_avg:88.72ms
step:1428/1670 train_time:126688ms step_avg:88.72ms
step:1429/1670 train_time:126777ms step_avg:88.72ms
step:1430/1670 train_time:126868ms step_avg:88.72ms
step:1431/1670 train_time:126956ms step_avg:88.72ms
step:1432/1670 train_time:127046ms step_avg:88.72ms
step:1433/1670 train_time:127136ms step_avg:88.72ms
step:1434/1670 train_time:127227ms step_avg:88.72ms
step:1435/1670 train_time:127317ms step_avg:88.72ms
step:1436/1670 train_time:127407ms step_avg:88.72ms
step:1437/1670 train_time:127496ms step_avg:88.72ms
step:1438/1670 train_time:127586ms step_avg:88.72ms
step:1439/1670 train_time:127675ms step_avg:88.73ms
step:1440/1670 train_time:127766ms step_avg:88.73ms
step:1441/1670 train_time:127855ms step_avg:88.73ms
step:1442/1670 train_time:127945ms step_avg:88.73ms
step:1443/1670 train_time:128034ms step_avg:88.73ms
step:1444/1670 train_time:128124ms step_avg:88.73ms
step:1445/1670 train_time:128215ms step_avg:88.73ms
step:1446/1670 train_time:128305ms step_avg:88.73ms
step:1447/1670 train_time:128394ms step_avg:88.73ms
step:1448/1670 train_time:128484ms step_avg:88.73ms
step:1449/1670 train_time:128573ms step_avg:88.73ms
step:1450/1670 train_time:128663ms step_avg:88.73ms
step:1451/1670 train_time:128752ms step_avg:88.73ms
step:1452/1670 train_time:128842ms step_avg:88.73ms
step:1453/1670 train_time:128932ms step_avg:88.73ms
step:1454/1670 train_time:129022ms step_avg:88.74ms
step:1455/1670 train_time:129112ms step_avg:88.74ms
step:1456/1670 train_time:129202ms step_avg:88.74ms
step:1457/1670 train_time:129292ms step_avg:88.74ms
step:1458/1670 train_time:129382ms step_avg:88.74ms
step:1459/1670 train_time:129471ms step_avg:88.74ms
step:1460/1670 train_time:129562ms step_avg:88.74ms
step:1461/1670 train_time:129651ms step_avg:88.74ms
step:1462/1670 train_time:129742ms step_avg:88.74ms
step:1463/1670 train_time:129833ms step_avg:88.74ms
step:1464/1670 train_time:129923ms step_avg:88.75ms
step:1465/1670 train_time:130013ms step_avg:88.75ms
step:1466/1670 train_time:130102ms step_avg:88.75ms
step:1467/1670 train_time:130191ms step_avg:88.75ms
step:1468/1670 train_time:130282ms step_avg:88.75ms
step:1469/1670 train_time:130372ms step_avg:88.75ms
step:1470/1670 train_time:130462ms step_avg:88.75ms
step:1471/1670 train_time:130552ms step_avg:88.75ms
step:1472/1670 train_time:130641ms step_avg:88.75ms
step:1473/1670 train_time:130731ms step_avg:88.75ms
step:1474/1670 train_time:130820ms step_avg:88.75ms
step:1475/1670 train_time:130910ms step_avg:88.75ms
step:1476/1670 train_time:130999ms step_avg:88.75ms
step:1477/1670 train_time:131089ms step_avg:88.75ms
step:1478/1670 train_time:131178ms step_avg:88.75ms
step:1479/1670 train_time:131269ms step_avg:88.75ms
step:1480/1670 train_time:131358ms step_avg:88.76ms
step:1481/1670 train_time:131448ms step_avg:88.76ms
step:1482/1670 train_time:131537ms step_avg:88.76ms
step:1483/1670 train_time:131627ms step_avg:88.76ms
step:1484/1670 train_time:131717ms step_avg:88.76ms
step:1485/1670 train_time:131806ms step_avg:88.76ms
step:1486/1670 train_time:131895ms step_avg:88.76ms
step:1487/1670 train_time:131984ms step_avg:88.76ms
step:1488/1670 train_time:132074ms step_avg:88.76ms
step:1489/1670 train_time:132163ms step_avg:88.76ms
step:1490/1670 train_time:132253ms step_avg:88.76ms
step:1491/1670 train_time:132343ms step_avg:88.76ms
step:1492/1670 train_time:132433ms step_avg:88.76ms
step:1493/1670 train_time:132523ms step_avg:88.76ms
step:1494/1670 train_time:132613ms step_avg:88.76ms
step:1495/1670 train_time:132704ms step_avg:88.77ms
step:1496/1670 train_time:132793ms step_avg:88.77ms
step:1497/1670 train_time:132882ms step_avg:88.77ms
step:1498/1670 train_time:132972ms step_avg:88.77ms
step:1499/1670 train_time:133062ms step_avg:88.77ms
step:1500/1670 train_time:133152ms step_avg:88.77ms
step:1500/1670 val_loss:3.3110 train_time:133243ms step_avg:88.83ms
step:1501/1670 train_time:133264ms step_avg:88.78ms
step:1502/1670 train_time:133340ms step_avg:88.77ms
step:1503/1670 train_time:133433ms step_avg:88.78ms
step:1504/1670 train_time:133523ms step_avg:88.78ms
step:1505/1670 train_time:133612ms step_avg:88.78ms
step:1506/1670 train_time:133699ms step_avg:88.78ms
step:1507/1670 train_time:133788ms step_avg:88.78ms
step:1508/1670 train_time:133876ms step_avg:88.78ms
step:1509/1670 train_time:133965ms step_avg:88.78ms
step:1510/1670 train_time:134055ms step_avg:88.78ms
step:1511/1670 train_time:134144ms step_avg:88.78ms
step:1512/1670 train_time:134235ms step_avg:88.78ms
step:1513/1670 train_time:134328ms step_avg:88.78ms
step:1514/1670 train_time:134420ms step_avg:88.78ms
step:1515/1670 train_time:134511ms step_avg:88.79ms
step:1516/1670 train_time:134600ms step_avg:88.79ms
step:1517/1670 train_time:134690ms step_avg:88.79ms
step:1518/1670 train_time:134779ms step_avg:88.79ms
step:1519/1670 train_time:134868ms step_avg:88.79ms
step:1520/1670 train_time:134957ms step_avg:88.79ms
step:1521/1670 train_time:135046ms step_avg:88.79ms
step:1522/1670 train_time:135135ms step_avg:88.79ms
step:1523/1670 train_time:135226ms step_avg:88.79ms
step:1524/1670 train_time:135318ms step_avg:88.79ms
step:1525/1670 train_time:135409ms step_avg:88.79ms
step:1526/1670 train_time:135498ms step_avg:88.79ms
step:1527/1670 train_time:135588ms step_avg:88.79ms
step:1528/1670 train_time:135678ms step_avg:88.79ms
step:1529/1670 train_time:135767ms step_avg:88.79ms
step:1530/1670 train_time:135856ms step_avg:88.80ms
step:1531/1670 train_time:135945ms step_avg:88.80ms
step:1532/1670 train_time:136034ms step_avg:88.80ms
step:1533/1670 train_time:136123ms step_avg:88.80ms
step:1534/1670 train_time:136213ms step_avg:88.80ms
step:1535/1670 train_time:136303ms step_avg:88.80ms
step:1536/1670 train_time:136394ms step_avg:88.80ms
step:1537/1670 train_time:136485ms step_avg:88.80ms
step:1538/1670 train_time:136575ms step_avg:88.80ms
step:1539/1670 train_time:136665ms step_avg:88.80ms
step:1540/1670 train_time:136754ms step_avg:88.80ms
step:1541/1670 train_time:136843ms step_avg:88.80ms
step:1542/1670 train_time:136932ms step_avg:88.80ms
step:1543/1670 train_time:137020ms step_avg:88.80ms
step:1544/1670 train_time:137109ms step_avg:88.80ms
step:1545/1670 train_time:137199ms step_avg:88.80ms
step:1546/1670 train_time:137289ms step_avg:88.80ms
step:1547/1670 train_time:137379ms step_avg:88.80ms
step:1548/1670 train_time:137469ms step_avg:88.80ms
step:1549/1670 train_time:137559ms step_avg:88.81ms
step:1550/1670 train_time:137649ms step_avg:88.81ms
step:1551/1670 train_time:137739ms step_avg:88.81ms
step:1552/1670 train_time:137828ms step_avg:88.81ms
step:1553/1670 train_time:137917ms step_avg:88.81ms
step:1554/1670 train_time:138006ms step_avg:88.81ms
step:1555/1670 train_time:138096ms step_avg:88.81ms
step:1556/1670 train_time:138187ms step_avg:88.81ms
step:1557/1670 train_time:138277ms step_avg:88.81ms
step:1558/1670 train_time:138366ms step_avg:88.81ms
step:1559/1670 train_time:138456ms step_avg:88.81ms
step:1560/1670 train_time:138547ms step_avg:88.81ms
step:1561/1670 train_time:138637ms step_avg:88.81ms
step:1562/1670 train_time:138727ms step_avg:88.81ms
step:1563/1670 train_time:138816ms step_avg:88.81ms
step:1564/1670 train_time:138905ms step_avg:88.81ms
step:1565/1670 train_time:138995ms step_avg:88.81ms
step:1566/1670 train_time:139086ms step_avg:88.82ms
step:1567/1670 train_time:139176ms step_avg:88.82ms
step:1568/1670 train_time:139267ms step_avg:88.82ms
step:1569/1670 train_time:139359ms step_avg:88.82ms
step:1570/1670 train_time:139450ms step_avg:88.82ms
step:1571/1670 train_time:139540ms step_avg:88.82ms
step:1572/1670 train_time:139630ms step_avg:88.82ms
step:1573/1670 train_time:139719ms step_avg:88.82ms
step:1574/1670 train_time:139809ms step_avg:88.82ms
step:1575/1670 train_time:139899ms step_avg:88.82ms
step:1576/1670 train_time:139988ms step_avg:88.83ms
step:1577/1670 train_time:140078ms step_avg:88.83ms
step:1578/1670 train_time:140168ms step_avg:88.83ms
step:1579/1670 train_time:140258ms step_avg:88.83ms
step:1580/1670 train_time:140349ms step_avg:88.83ms
step:1581/1670 train_time:140438ms step_avg:88.83ms
step:1582/1670 train_time:140528ms step_avg:88.83ms
step:1583/1670 train_time:140618ms step_avg:88.83ms
step:1584/1670 train_time:140707ms step_avg:88.83ms
step:1585/1670 train_time:140797ms step_avg:88.83ms
step:1586/1670 train_time:140887ms step_avg:88.83ms
step:1587/1670 train_time:140978ms step_avg:88.83ms
step:1588/1670 train_time:141069ms step_avg:88.83ms
step:1589/1670 train_time:141158ms step_avg:88.83ms
step:1590/1670 train_time:141248ms step_avg:88.84ms
step:1591/1670 train_time:141337ms step_avg:88.84ms
step:1592/1670 train_time:141427ms step_avg:88.84ms
step:1593/1670 train_time:141516ms step_avg:88.84ms
step:1594/1670 train_time:141607ms step_avg:88.84ms
step:1595/1670 train_time:141697ms step_avg:88.84ms
step:1596/1670 train_time:141786ms step_avg:88.84ms
step:1597/1670 train_time:141875ms step_avg:88.84ms
step:1598/1670 train_time:141965ms step_avg:88.84ms
step:1599/1670 train_time:142055ms step_avg:88.84ms
step:1600/1670 train_time:142145ms step_avg:88.84ms
step:1601/1670 train_time:142236ms step_avg:88.84ms
step:1602/1670 train_time:142326ms step_avg:88.84ms
step:1603/1670 train_time:142415ms step_avg:88.84ms
step:1604/1670 train_time:142505ms step_avg:88.84ms
step:1605/1670 train_time:142596ms step_avg:88.84ms
step:1606/1670 train_time:142686ms step_avg:88.85ms
step:1607/1670 train_time:142775ms step_avg:88.85ms
step:1608/1670 train_time:142866ms step_avg:88.85ms
step:1609/1670 train_time:142956ms step_avg:88.85ms
step:1610/1670 train_time:143047ms step_avg:88.85ms
step:1611/1670 train_time:143136ms step_avg:88.85ms
step:1612/1670 train_time:143226ms step_avg:88.85ms
step:1613/1670 train_time:143316ms step_avg:88.85ms
step:1614/1670 train_time:143406ms step_avg:88.85ms
step:1615/1670 train_time:143496ms step_avg:88.85ms
step:1616/1670 train_time:143586ms step_avg:88.85ms
step:1617/1670 train_time:143677ms step_avg:88.85ms
step:1618/1670 train_time:143767ms step_avg:88.85ms
step:1619/1670 train_time:143858ms step_avg:88.86ms
step:1620/1670 train_time:143948ms step_avg:88.86ms
step:1621/1670 train_time:144038ms step_avg:88.86ms
step:1622/1670 train_time:144127ms step_avg:88.86ms
step:1623/1670 train_time:144217ms step_avg:88.86ms
step:1624/1670 train_time:144306ms step_avg:88.86ms
step:1625/1670 train_time:144397ms step_avg:88.86ms
step:1625/1670 val_loss:3.2879 train_time:144489ms step_avg:88.92ms
step:1626/1670 train_time:144508ms step_avg:88.87ms
step:1627/1670 train_time:144584ms step_avg:88.87ms
step:1628/1670 train_time:144679ms step_avg:88.87ms
step:1629/1670 train_time:144771ms step_avg:88.87ms
step:1630/1670 train_time:144860ms step_avg:88.87ms
step:1631/1670 train_time:144949ms step_avg:88.87ms
step:1632/1670 train_time:145037ms step_avg:88.87ms
step:1633/1670 train_time:145126ms step_avg:88.87ms
step:1634/1670 train_time:145214ms step_avg:88.87ms
step:1635/1670 train_time:145302ms step_avg:88.87ms
step:1636/1670 train_time:145391ms step_avg:88.87ms
step:1637/1670 train_time:145482ms step_avg:88.87ms
step:1638/1670 train_time:145574ms step_avg:88.87ms
step:1639/1670 train_time:145665ms step_avg:88.87ms
step:1640/1670 train_time:145757ms step_avg:88.88ms
step:1641/1670 train_time:145847ms step_avg:88.88ms
step:1642/1670 train_time:145937ms step_avg:88.88ms
step:1643/1670 train_time:146026ms step_avg:88.88ms
step:1644/1670 train_time:146116ms step_avg:88.88ms
step:1645/1670 train_time:146204ms step_avg:88.88ms
step:1646/1670 train_time:146293ms step_avg:88.88ms
step:1647/1670 train_time:146381ms step_avg:88.88ms
step:1648/1670 train_time:146472ms step_avg:88.88ms
step:1649/1670 train_time:146562ms step_avg:88.88ms
step:1650/1670 train_time:146654ms step_avg:88.88ms
step:1651/1670 train_time:146744ms step_avg:88.88ms
step:1652/1670 train_time:146835ms step_avg:88.88ms
step:1653/1670 train_time:146924ms step_avg:88.88ms
step:1654/1670 train_time:147014ms step_avg:88.88ms
step:1655/1670 train_time:147103ms step_avg:88.88ms
step:1656/1670 train_time:147193ms step_avg:88.88ms
step:1657/1670 train_time:147281ms step_avg:88.88ms
step:1658/1670 train_time:147370ms step_avg:88.88ms
step:1659/1670 train_time:147461ms step_avg:88.89ms
step:1660/1670 train_time:147550ms step_avg:88.89ms
step:1661/1670 train_time:147641ms step_avg:88.89ms
step:1662/1670 train_time:147731ms step_avg:88.89ms
step:1663/1670 train_time:147821ms step_avg:88.89ms
step:1664/1670 train_time:147911ms step_avg:88.89ms
step:1665/1670 train_time:148001ms step_avg:88.89ms
step:1666/1670 train_time:148091ms step_avg:88.89ms
step:1667/1670 train_time:148180ms step_avg:88.89ms
step:1668/1670 train_time:148270ms step_avg:88.89ms
step:1669/1670 train_time:148360ms step_avg:88.89ms
step:1670/1670 train_time:148450ms step_avg:88.89ms
step:1670/1670 val_loss:3.2787 train_time:148542ms step_avg:88.95ms
peak memory allocated: 30760 MiB reserved: 45514 MiB

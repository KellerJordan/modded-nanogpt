import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)


@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)


@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[
    Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)


@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)


def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None


def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)


mm_op.register_autograd(backward, setup_context=setup_context)


# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
        pid,
        M,
        BLOCK_SIZE_M: tl.constexpr,
        BLOCK_SIZE_N: tl.constexpr,
        GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
coeffs_list = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in coeffs_list:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """

    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size() == 8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1,  # 1 param
            'attn_gate': 2,  # 10 params
            'attn': 3,  # 10 params
            'mlp': 4,  # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx + size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                    (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                    group["lr"]
                    * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                    * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                    group["lr"]
                    * group["weight_decay"]
                    * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[module_idx], 'module', 'none') == 'attn':
                for p in params[module_idx:module_idx + chunk_size]:
                    assert getattr(params[module_idx], 'module', 'none') == 'attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8,
                 weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))


class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5)  # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim // 4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim // 4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int = 1, beta: int = 32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1


def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)


@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim * 4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module = 'attn'
        with torch.no_grad():
            self.qkvo_w.view(4, self.hdim, self.dim)[:3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w.view(4, self.hdim, self.dim)[3].zero_()  # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1)  # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T,
                                                                                                               3 * self.num_heads,
                                                                                                               self.head_dim).chunk(
            3, dim=-2)
        q, k = norm(q), norm(k)  # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v)  # @ KoszarskyB & @Grad62304977
        else:  # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len,
                                   max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)  # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module = 'mlp'
        self.c_proj.module = 'mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std  # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_()  # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(
            x).square()  # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x


# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)


class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int,
                 max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim ** 0.5) / 448, w_s=2 ** -9,
                                    grad_s=1 / 448)
        self.lm_head.weight.detach().zero_()  # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers),  # extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm,
                    long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1, len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i < 11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss


# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32)  # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2])  # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True)  # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy())  # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens


BOS_ID = 50256


class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens = tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter == 5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter += 1
        return starts, ends


class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data


def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1,
                               align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens

        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin"  # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin"  # input .bin to eval validation loss on
    val_tokens: int = 10485760  # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1630  # number of iterations to run
    iteration_extension = 40  # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5  # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13  # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20  # extend long windows out even further


args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0)  # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)


def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)


# begin by printing this file (the Python code)
print0(code)
print0("=" * 100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")


def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout


print0(nvidia_smi())
print0("=" * 100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if
                        p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]


# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr


def get_ws(step: int):
    if step == args.num_iterations + args.iteration_extension:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers])  # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len,
                                          grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[
        step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long < ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long // 2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len,
                                          grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1,
                                                grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(
            f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms",
            console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(),
                       optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1)  # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(
        f"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms",
        console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Mon Sep 29 06:22:36 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   40C    P0            129W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   33C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1670 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1670 train_time:135ms step_avg:135.10ms
step:2/1670 train_time:156ms step_avg:78.05ms
step:3/1670 train_time:222ms step_avg:74.08ms
step:4/1670 train_time:308ms step_avg:77.00ms
step:5/1670 train_time:395ms step_avg:78.99ms
step:6/1670 train_time:481ms step_avg:80.21ms
step:7/1670 train_time:568ms step_avg:81.12ms
step:8/1670 train_time:655ms step_avg:81.84ms
step:9/1670 train_time:742ms step_avg:82.42ms
step:10/1670 train_time:829ms step_avg:82.88ms
step:11/1670 train_time:915ms step_avg:83.21ms
step:12/1670 train_time:1005ms step_avg:83.75ms
step:13/1670 train_time:1096ms step_avg:84.30ms
step:14/1670 train_time:1188ms step_avg:84.84ms
step:15/1670 train_time:1277ms step_avg:85.11ms
step:16/1670 train_time:1365ms step_avg:85.29ms
step:17/1670 train_time:1452ms step_avg:85.41ms
step:18/1670 train_time:1539ms step_avg:85.52ms
step:19/1670 train_time:1627ms step_avg:85.62ms
step:20/1670 train_time:1714ms step_avg:85.68ms
step:21/1670 train_time:1801ms step_avg:85.75ms
step:22/1670 train_time:1888ms step_avg:85.83ms
step:23/1670 train_time:1976ms step_avg:85.89ms
step:24/1670 train_time:2067ms step_avg:86.11ms
step:25/1670 train_time:2155ms step_avg:86.20ms
step:26/1670 train_time:2244ms step_avg:86.32ms
step:27/1670 train_time:2332ms step_avg:86.36ms
step:28/1670 train_time:2421ms step_avg:86.45ms
step:29/1670 train_time:2508ms step_avg:86.49ms
step:30/1670 train_time:2596ms step_avg:86.53ms
step:31/1670 train_time:2683ms step_avg:86.54ms
step:32/1670 train_time:2770ms step_avg:86.55ms
step:33/1670 train_time:2857ms step_avg:86.57ms
step:34/1670 train_time:2944ms step_avg:86.60ms
step:35/1670 train_time:3032ms step_avg:86.64ms
step:36/1670 train_time:3121ms step_avg:86.68ms
step:37/1670 train_time:3210ms step_avg:86.76ms
step:38/1670 train_time:3299ms step_avg:86.82ms
step:39/1670 train_time:3388ms step_avg:86.88ms
step:40/1670 train_time:3477ms step_avg:86.92ms
step:41/1670 train_time:3566ms step_avg:86.97ms
step:42/1670 train_time:3653ms step_avg:86.98ms
step:43/1670 train_time:3740ms step_avg:86.99ms
step:44/1670 train_time:3828ms step_avg:86.99ms
step:45/1670 train_time:3915ms step_avg:87.00ms
step:46/1670 train_time:4003ms step_avg:87.01ms
step:47/1670 train_time:4092ms step_avg:87.05ms
step:48/1670 train_time:4179ms step_avg:87.07ms
step:49/1670 train_time:4268ms step_avg:87.11ms
step:50/1670 train_time:4357ms step_avg:87.13ms
step:51/1670 train_time:4445ms step_avg:87.16ms
step:52/1670 train_time:4533ms step_avg:87.18ms
step:53/1670 train_time:4622ms step_avg:87.20ms
step:54/1670 train_time:4710ms step_avg:87.22ms
step:55/1670 train_time:4798ms step_avg:87.23ms
step:56/1670 train_time:4886ms step_avg:87.25ms
step:57/1670 train_time:4973ms step_avg:87.25ms
step:58/1670 train_time:5061ms step_avg:87.27ms
step:59/1670 train_time:5149ms step_avg:87.27ms
step:60/1670 train_time:5238ms step_avg:87.29ms
step:61/1670 train_time:5327ms step_avg:87.33ms
step:62/1670 train_time:5415ms step_avg:87.34ms
step:63/1670 train_time:5503ms step_avg:87.35ms
step:64/1670 train_time:5591ms step_avg:87.36ms
step:65/1670 train_time:5679ms step_avg:87.36ms
step:66/1670 train_time:5768ms step_avg:87.39ms
step:67/1670 train_time:5856ms step_avg:87.41ms
step:68/1670 train_time:5944ms step_avg:87.42ms
step:69/1670 train_time:6031ms step_avg:87.41ms
step:70/1670 train_time:6119ms step_avg:87.41ms
step:71/1670 train_time:6206ms step_avg:87.41ms
step:72/1670 train_time:6295ms step_avg:87.43ms
step:73/1670 train_time:6383ms step_avg:87.43ms
step:74/1670 train_time:6471ms step_avg:87.44ms
step:75/1670 train_time:6559ms step_avg:87.45ms
step:76/1670 train_time:6647ms step_avg:87.47ms
step:77/1670 train_time:6736ms step_avg:87.48ms
step:78/1670 train_time:6825ms step_avg:87.50ms
step:79/1670 train_time:6912ms step_avg:87.49ms
step:80/1670 train_time:7000ms step_avg:87.49ms
step:81/1670 train_time:7087ms step_avg:87.49ms
step:82/1670 train_time:7174ms step_avg:87.48ms
step:83/1670 train_time:7261ms step_avg:87.48ms
step:84/1670 train_time:7349ms step_avg:87.49ms
step:85/1670 train_time:7437ms step_avg:87.50ms
step:86/1670 train_time:7526ms step_avg:87.51ms
step:87/1670 train_time:7614ms step_avg:87.51ms
step:88/1670 train_time:7702ms step_avg:87.52ms
step:89/1670 train_time:7790ms step_avg:87.53ms
step:90/1670 train_time:7878ms step_avg:87.53ms
step:91/1670 train_time:7966ms step_avg:87.54ms
step:92/1670 train_time:8052ms step_avg:87.53ms
step:93/1670 train_time:8139ms step_avg:87.52ms
step:94/1670 train_time:8228ms step_avg:87.53ms
step:95/1670 train_time:8315ms step_avg:87.53ms
step:96/1670 train_time:8403ms step_avg:87.53ms
step:97/1670 train_time:8491ms step_avg:87.53ms
step:98/1670 train_time:8579ms step_avg:87.54ms
step:99/1670 train_time:8668ms step_avg:87.56ms
step:100/1670 train_time:8756ms step_avg:87.56ms
step:101/1670 train_time:8843ms step_avg:87.56ms
step:102/1670 train_time:8931ms step_avg:87.56ms
step:103/1670 train_time:9019ms step_avg:87.57ms
step:104/1670 train_time:9107ms step_avg:87.57ms
step:105/1670 train_time:9195ms step_avg:87.57ms
step:106/1670 train_time:9282ms step_avg:87.57ms
step:107/1670 train_time:9370ms step_avg:87.57ms
step:108/1670 train_time:9459ms step_avg:87.58ms
step:109/1670 train_time:9546ms step_avg:87.58ms
step:110/1670 train_time:9634ms step_avg:87.58ms
step:111/1670 train_time:9722ms step_avg:87.58ms
step:112/1670 train_time:9809ms step_avg:87.58ms
step:113/1670 train_time:9898ms step_avg:87.59ms
step:114/1670 train_time:9986ms step_avg:87.60ms
step:115/1670 train_time:10074ms step_avg:87.60ms
step:116/1670 train_time:10162ms step_avg:87.61ms
step:117/1670 train_time:10249ms step_avg:87.60ms
step:118/1670 train_time:10337ms step_avg:87.60ms
step:119/1670 train_time:10426ms step_avg:87.61ms
step:120/1670 train_time:10513ms step_avg:87.61ms
step:121/1670 train_time:10601ms step_avg:87.61ms
step:122/1670 train_time:10689ms step_avg:87.61ms
step:123/1670 train_time:10776ms step_avg:87.61ms
step:124/1670 train_time:10864ms step_avg:87.61ms
step:125/1670 train_time:10951ms step_avg:87.61ms
step:125/1670 val_loss:4.3456 train_time:11041ms step_avg:88.32ms
step:126/1670 train_time:11061ms step_avg:87.78ms
step:127/1670 train_time:11133ms step_avg:87.66ms
step:128/1670 train_time:11229ms step_avg:87.73ms
step:129/1670 train_time:11319ms step_avg:87.74ms
step:130/1670 train_time:11407ms step_avg:87.75ms
step:131/1670 train_time:11494ms step_avg:87.74ms
step:132/1670 train_time:11581ms step_avg:87.73ms
step:133/1670 train_time:11667ms step_avg:87.72ms
step:134/1670 train_time:11754ms step_avg:87.72ms
step:135/1670 train_time:11841ms step_avg:87.71ms
step:136/1670 train_time:11927ms step_avg:87.70ms
step:137/1670 train_time:12014ms step_avg:87.69ms
step:138/1670 train_time:12104ms step_avg:87.71ms
step:139/1670 train_time:12195ms step_avg:87.73ms
step:140/1670 train_time:12283ms step_avg:87.74ms
step:141/1670 train_time:12372ms step_avg:87.74ms
step:142/1670 train_time:12460ms step_avg:87.74ms
step:143/1670 train_time:12547ms step_avg:87.74ms
step:144/1670 train_time:12634ms step_avg:87.74ms
step:145/1670 train_time:12720ms step_avg:87.73ms
step:146/1670 train_time:12807ms step_avg:87.72ms
step:147/1670 train_time:12894ms step_avg:87.71ms
step:148/1670 train_time:12981ms step_avg:87.71ms
step:149/1670 train_time:13069ms step_avg:87.71ms
step:150/1670 train_time:13159ms step_avg:87.73ms
step:151/1670 train_time:13247ms step_avg:87.73ms
step:152/1670 train_time:13335ms step_avg:87.73ms
step:153/1670 train_time:13423ms step_avg:87.73ms
step:154/1670 train_time:13510ms step_avg:87.73ms
step:155/1670 train_time:13598ms step_avg:87.73ms
step:156/1670 train_time:13684ms step_avg:87.72ms
step:157/1670 train_time:13772ms step_avg:87.72ms
step:158/1670 train_time:13859ms step_avg:87.71ms
step:159/1670 train_time:13945ms step_avg:87.70ms
step:160/1670 train_time:14032ms step_avg:87.70ms
step:161/1670 train_time:14120ms step_avg:87.70ms
step:162/1670 train_time:14208ms step_avg:87.70ms
step:163/1670 train_time:14296ms step_avg:87.71ms
step:164/1670 train_time:14384ms step_avg:87.71ms
step:165/1670 train_time:14472ms step_avg:87.71ms
step:166/1670 train_time:14560ms step_avg:87.71ms
step:167/1670 train_time:14647ms step_avg:87.71ms
step:168/1670 train_time:14733ms step_avg:87.70ms
step:169/1670 train_time:14821ms step_avg:87.70ms
step:170/1670 train_time:14907ms step_avg:87.69ms
step:171/1670 train_time:14995ms step_avg:87.69ms
step:172/1670 train_time:15083ms step_avg:87.69ms
step:173/1670 train_time:15171ms step_avg:87.69ms
step:174/1670 train_time:15259ms step_avg:87.69ms
step:175/1670 train_time:15346ms step_avg:87.69ms
step:176/1670 train_time:15434ms step_avg:87.69ms
step:177/1670 train_time:15522ms step_avg:87.70ms
step:178/1670 train_time:15609ms step_avg:87.69ms
step:179/1670 train_time:15697ms step_avg:87.69ms
step:180/1670 train_time:15783ms step_avg:87.69ms
step:181/1670 train_time:15870ms step_avg:87.68ms
step:182/1670 train_time:15957ms step_avg:87.68ms
step:183/1670 train_time:16045ms step_avg:87.68ms
step:184/1670 train_time:16133ms step_avg:87.68ms
step:185/1670 train_time:16220ms step_avg:87.68ms
step:186/1670 train_time:16308ms step_avg:87.68ms
step:187/1670 train_time:16396ms step_avg:87.68ms
step:188/1670 train_time:16483ms step_avg:87.68ms
step:189/1670 train_time:16571ms step_avg:87.68ms
step:190/1670 train_time:16659ms step_avg:87.68ms
step:191/1670 train_time:16746ms step_avg:87.68ms
step:192/1670 train_time:16834ms step_avg:87.67ms
step:193/1670 train_time:16921ms step_avg:87.67ms
step:194/1670 train_time:17008ms step_avg:87.67ms
step:195/1670 train_time:17097ms step_avg:87.67ms
step:196/1670 train_time:17184ms step_avg:87.67ms
step:197/1670 train_time:17272ms step_avg:87.68ms
step:198/1670 train_time:17360ms step_avg:87.68ms
step:199/1670 train_time:17447ms step_avg:87.67ms
step:200/1670 train_time:17535ms step_avg:87.67ms
step:201/1670 train_time:17622ms step_avg:87.67ms
step:202/1670 train_time:17709ms step_avg:87.67ms
step:203/1670 train_time:17798ms step_avg:87.68ms
step:204/1670 train_time:17885ms step_avg:87.67ms
step:205/1670 train_time:17973ms step_avg:87.67ms
step:206/1670 train_time:18061ms step_avg:87.67ms
step:207/1670 train_time:18149ms step_avg:87.67ms
step:208/1670 train_time:18236ms step_avg:87.67ms
step:209/1670 train_time:18323ms step_avg:87.67ms
step:210/1670 train_time:18411ms step_avg:87.67ms
step:211/1670 train_time:18498ms step_avg:87.67ms
step:212/1670 train_time:18585ms step_avg:87.67ms
step:213/1670 train_time:18673ms step_avg:87.67ms
step:214/1670 train_time:18761ms step_avg:87.67ms
step:215/1670 train_time:18848ms step_avg:87.66ms
step:216/1670 train_time:18935ms step_avg:87.66ms
step:217/1670 train_time:19022ms step_avg:87.66ms
step:218/1670 train_time:19110ms step_avg:87.66ms
step:219/1670 train_time:19198ms step_avg:87.66ms
step:220/1670 train_time:19285ms step_avg:87.66ms
step:221/1670 train_time:19372ms step_avg:87.66ms
step:222/1670 train_time:19460ms step_avg:87.66ms
step:223/1670 train_time:19547ms step_avg:87.65ms
step:224/1670 train_time:19635ms step_avg:87.66ms
step:225/1670 train_time:19722ms step_avg:87.65ms
step:226/1670 train_time:19809ms step_avg:87.65ms
step:227/1670 train_time:19897ms step_avg:87.65ms
step:228/1670 train_time:19984ms step_avg:87.65ms
step:229/1670 train_time:20072ms step_avg:87.65ms
step:230/1670 train_time:20161ms step_avg:87.66ms
step:231/1670 train_time:20249ms step_avg:87.66ms
step:232/1670 train_time:20336ms step_avg:87.66ms
step:233/1670 train_time:20423ms step_avg:87.65ms
step:234/1670 train_time:20510ms step_avg:87.65ms
step:235/1670 train_time:20599ms step_avg:87.66ms
step:236/1670 train_time:20686ms step_avg:87.65ms
step:237/1670 train_time:20774ms step_avg:87.66ms
step:238/1670 train_time:20862ms step_avg:87.65ms
step:239/1670 train_time:20949ms step_avg:87.65ms
step:240/1670 train_time:21037ms step_avg:87.65ms
step:241/1670 train_time:21125ms step_avg:87.65ms
step:242/1670 train_time:21212ms step_avg:87.65ms
step:243/1670 train_time:21301ms step_avg:87.66ms
step:244/1670 train_time:21388ms step_avg:87.66ms
step:245/1670 train_time:21475ms step_avg:87.65ms
step:246/1670 train_time:21563ms step_avg:87.65ms
step:247/1670 train_time:21650ms step_avg:87.65ms
step:248/1670 train_time:21738ms step_avg:87.65ms
step:249/1670 train_time:21826ms step_avg:87.65ms
step:250/1670 train_time:21914ms step_avg:87.65ms
step:250/1670 val_loss:3.9771 train_time:22003ms step_avg:88.01ms
step:251/1670 train_time:22024ms step_avg:87.74ms
step:252/1670 train_time:22094ms step_avg:87.67ms
step:253/1670 train_time:22183ms step_avg:87.68ms
step:254/1670 train_time:22271ms step_avg:87.68ms
step:255/1670 train_time:22357ms step_avg:87.67ms
step:256/1670 train_time:22444ms step_avg:87.67ms
step:257/1670 train_time:22531ms step_avg:87.67ms
step:258/1670 train_time:22617ms step_avg:87.66ms
step:259/1670 train_time:22703ms step_avg:87.66ms
step:260/1670 train_time:22791ms step_avg:87.66ms
step:261/1670 train_time:22877ms step_avg:87.65ms
step:262/1670 train_time:22965ms step_avg:87.65ms
step:263/1670 train_time:23057ms step_avg:87.67ms
step:264/1670 train_time:23146ms step_avg:87.68ms
step:265/1670 train_time:23236ms step_avg:87.68ms
step:266/1670 train_time:23323ms step_avg:87.68ms
step:267/1670 train_time:23411ms step_avg:87.68ms
step:268/1670 train_time:23498ms step_avg:87.68ms
step:269/1670 train_time:23585ms step_avg:87.67ms
step:270/1670 train_time:23671ms step_avg:87.67ms
step:271/1670 train_time:23758ms step_avg:87.67ms
step:272/1670 train_time:23845ms step_avg:87.67ms
step:273/1670 train_time:23933ms step_avg:87.67ms
step:274/1670 train_time:24022ms step_avg:87.67ms
step:275/1670 train_time:24111ms step_avg:87.67ms
step:276/1670 train_time:24199ms step_avg:87.68ms
step:277/1670 train_time:24287ms step_avg:87.68ms
step:278/1670 train_time:24374ms step_avg:87.68ms
step:279/1670 train_time:24461ms step_avg:87.67ms
step:280/1670 train_time:24548ms step_avg:87.67ms
step:281/1670 train_time:24635ms step_avg:87.67ms
step:282/1670 train_time:24722ms step_avg:87.67ms
step:283/1670 train_time:24809ms step_avg:87.66ms
step:284/1670 train_time:24896ms step_avg:87.66ms
step:285/1670 train_time:24984ms step_avg:87.66ms
step:286/1670 train_time:25074ms step_avg:87.67ms
step:287/1670 train_time:25161ms step_avg:87.67ms
step:288/1670 train_time:25249ms step_avg:87.67ms
step:289/1670 train_time:25336ms step_avg:87.67ms
step:290/1670 train_time:25423ms step_avg:87.67ms
step:291/1670 train_time:25511ms step_avg:87.67ms
step:292/1670 train_time:25598ms step_avg:87.66ms
step:293/1670 train_time:25685ms step_avg:87.66ms
step:294/1670 train_time:25773ms step_avg:87.66ms
step:295/1670 train_time:25860ms step_avg:87.66ms
step:296/1670 train_time:25947ms step_avg:87.66ms
step:297/1670 train_time:26035ms step_avg:87.66ms
step:298/1670 train_time:26125ms step_avg:87.67ms
step:299/1670 train_time:26214ms step_avg:87.67ms
step:300/1670 train_time:26301ms step_avg:87.67ms
step:301/1670 train_time:26388ms step_avg:87.67ms
step:302/1670 train_time:26475ms step_avg:87.67ms
step:303/1670 train_time:26563ms step_avg:87.67ms
step:304/1670 train_time:26651ms step_avg:87.67ms
step:305/1670 train_time:26738ms step_avg:87.66ms
step:306/1670 train_time:26825ms step_avg:87.66ms
step:307/1670 train_time:26913ms step_avg:87.67ms
step:308/1670 train_time:27000ms step_avg:87.66ms
step:309/1670 train_time:27088ms step_avg:87.66ms
step:310/1670 train_time:27176ms step_avg:87.66ms
step:311/1670 train_time:27264ms step_avg:87.66ms
step:312/1670 train_time:27353ms step_avg:87.67ms
step:313/1670 train_time:27440ms step_avg:87.67ms
step:314/1670 train_time:27528ms step_avg:87.67ms
step:315/1670 train_time:27616ms step_avg:87.67ms
step:316/1670 train_time:27703ms step_avg:87.67ms
step:317/1670 train_time:27790ms step_avg:87.67ms
step:318/1670 train_time:27877ms step_avg:87.66ms
step:319/1670 train_time:27965ms step_avg:87.66ms
step:320/1670 train_time:28054ms step_avg:87.67ms
step:321/1670 train_time:28141ms step_avg:87.67ms
step:322/1670 train_time:28229ms step_avg:87.67ms
step:323/1670 train_time:28317ms step_avg:87.67ms
step:324/1670 train_time:28404ms step_avg:87.67ms
step:325/1670 train_time:28492ms step_avg:87.67ms
step:326/1670 train_time:28579ms step_avg:87.66ms
step:327/1670 train_time:28666ms step_avg:87.66ms
step:328/1670 train_time:28754ms step_avg:87.66ms
step:329/1670 train_time:28841ms step_avg:87.66ms
step:330/1670 train_time:28928ms step_avg:87.66ms
step:331/1670 train_time:29017ms step_avg:87.66ms
step:332/1670 train_time:29104ms step_avg:87.66ms
step:333/1670 train_time:29192ms step_avg:87.66ms
step:334/1670 train_time:29280ms step_avg:87.67ms
step:335/1670 train_time:29368ms step_avg:87.66ms
step:336/1670 train_time:29456ms step_avg:87.67ms
step:337/1670 train_time:29543ms step_avg:87.67ms
step:338/1670 train_time:29631ms step_avg:87.67ms
step:339/1670 train_time:29718ms step_avg:87.66ms
step:340/1670 train_time:29805ms step_avg:87.66ms
step:341/1670 train_time:29893ms step_avg:87.66ms
step:342/1670 train_time:29981ms step_avg:87.66ms
step:343/1670 train_time:30069ms step_avg:87.67ms
step:344/1670 train_time:30157ms step_avg:87.67ms
step:345/1670 train_time:30245ms step_avg:87.67ms
step:346/1670 train_time:30332ms step_avg:87.67ms
step:347/1670 train_time:30419ms step_avg:87.66ms
step:348/1670 train_time:30507ms step_avg:87.66ms
step:349/1670 train_time:30595ms step_avg:87.66ms
step:350/1670 train_time:30682ms step_avg:87.66ms
step:351/1670 train_time:30770ms step_avg:87.66ms
step:352/1670 train_time:30857ms step_avg:87.66ms
step:353/1670 train_time:30945ms step_avg:87.66ms
step:354/1670 train_time:31033ms step_avg:87.66ms
step:355/1670 train_time:31120ms step_avg:87.66ms
step:356/1670 train_time:31208ms step_avg:87.66ms
step:357/1670 train_time:31296ms step_avg:87.66ms
step:358/1670 train_time:31383ms step_avg:87.66ms
step:359/1670 train_time:31471ms step_avg:87.66ms
step:360/1670 train_time:31558ms step_avg:87.66ms
step:361/1670 train_time:31645ms step_avg:87.66ms
step:362/1670 train_time:31733ms step_avg:87.66ms
step:363/1670 train_time:31820ms step_avg:87.66ms
step:364/1670 train_time:31908ms step_avg:87.66ms
step:365/1670 train_time:31996ms step_avg:87.66ms
step:366/1670 train_time:32084ms step_avg:87.66ms
step:367/1670 train_time:32172ms step_avg:87.66ms
step:368/1670 train_time:32259ms step_avg:87.66ms
step:369/1670 train_time:32347ms step_avg:87.66ms
step:370/1670 train_time:32435ms step_avg:87.66ms
step:371/1670 train_time:32523ms step_avg:87.66ms
step:372/1670 train_time:32610ms step_avg:87.66ms
step:373/1670 train_time:32698ms step_avg:87.66ms
step:374/1670 train_time:32785ms step_avg:87.66ms
step:375/1670 train_time:32873ms step_avg:87.66ms
step:375/1670 val_loss:3.8215 train_time:32961ms step_avg:87.90ms
step:376/1670 train_time:32983ms step_avg:87.72ms
step:377/1670 train_time:33051ms step_avg:87.67ms
step:378/1670 train_time:33142ms step_avg:87.68ms
step:379/1670 train_time:33229ms step_avg:87.67ms
step:380/1670 train_time:33315ms step_avg:87.67ms
step:381/1670 train_time:33403ms step_avg:87.67ms
step:382/1670 train_time:33489ms step_avg:87.67ms
step:383/1670 train_time:33576ms step_avg:87.67ms
step:384/1670 train_time:33662ms step_avg:87.66ms
step:385/1670 train_time:33750ms step_avg:87.66ms
step:386/1670 train_time:33837ms step_avg:87.66ms
step:387/1670 train_time:33927ms step_avg:87.67ms
step:388/1670 train_time:34016ms step_avg:87.67ms
step:389/1670 train_time:34107ms step_avg:87.68ms
step:390/1670 train_time:34195ms step_avg:87.68ms
step:391/1670 train_time:34282ms step_avg:87.68ms
step:392/1670 train_time:34369ms step_avg:87.68ms
step:393/1670 train_time:34456ms step_avg:87.68ms
step:394/1670 train_time:34543ms step_avg:87.67ms
step:395/1670 train_time:34630ms step_avg:87.67ms
step:396/1670 train_time:34716ms step_avg:87.67ms
step:397/1670 train_time:34803ms step_avg:87.67ms
step:398/1670 train_time:34890ms step_avg:87.66ms
step:399/1670 train_time:34979ms step_avg:87.67ms
step:400/1670 train_time:35068ms step_avg:87.67ms
step:401/1670 train_time:35156ms step_avg:87.67ms
step:402/1670 train_time:35245ms step_avg:87.68ms
step:403/1670 train_time:35333ms step_avg:87.67ms
step:404/1670 train_time:35420ms step_avg:87.67ms
step:405/1670 train_time:35508ms step_avg:87.67ms
step:406/1670 train_time:35595ms step_avg:87.67ms
step:407/1670 train_time:35682ms step_avg:87.67ms
step:408/1670 train_time:35769ms step_avg:87.67ms
step:409/1670 train_time:35857ms step_avg:87.67ms
step:410/1670 train_time:35945ms step_avg:87.67ms
step:411/1670 train_time:36033ms step_avg:87.67ms
step:412/1670 train_time:36122ms step_avg:87.67ms
step:413/1670 train_time:36210ms step_avg:87.67ms
step:414/1670 train_time:36298ms step_avg:87.68ms
step:415/1670 train_time:36386ms step_avg:87.68ms
step:416/1670 train_time:36473ms step_avg:87.68ms
step:417/1670 train_time:36561ms step_avg:87.68ms
step:418/1670 train_time:36648ms step_avg:87.67ms
step:419/1670 train_time:36735ms step_avg:87.67ms
step:420/1670 train_time:36822ms step_avg:87.67ms
step:421/1670 train_time:36911ms step_avg:87.67ms
step:422/1670 train_time:36998ms step_avg:87.67ms
step:423/1670 train_time:37087ms step_avg:87.68ms
step:424/1670 train_time:37175ms step_avg:87.68ms
step:425/1670 train_time:37263ms step_avg:87.68ms
step:426/1670 train_time:37350ms step_avg:87.68ms
step:427/1670 train_time:37438ms step_avg:87.68ms
step:428/1670 train_time:37525ms step_avg:87.68ms
step:429/1670 train_time:37613ms step_avg:87.68ms
step:430/1670 train_time:37701ms step_avg:87.68ms
step:431/1670 train_time:37788ms step_avg:87.68ms
step:432/1670 train_time:37876ms step_avg:87.68ms
step:433/1670 train_time:37965ms step_avg:87.68ms
step:434/1670 train_time:38052ms step_avg:87.68ms
step:435/1670 train_time:38140ms step_avg:87.68ms
step:436/1670 train_time:38227ms step_avg:87.68ms
step:437/1670 train_time:38315ms step_avg:87.68ms
step:438/1670 train_time:38403ms step_avg:87.68ms
step:439/1670 train_time:38490ms step_avg:87.68ms
step:440/1670 train_time:38577ms step_avg:87.68ms
step:441/1670 train_time:38665ms step_avg:87.68ms
step:442/1670 train_time:38752ms step_avg:87.67ms
step:443/1670 train_time:38840ms step_avg:87.68ms
step:444/1670 train_time:38928ms step_avg:87.68ms
step:445/1670 train_time:39016ms step_avg:87.68ms
step:446/1670 train_time:39104ms step_avg:87.68ms
step:447/1670 train_time:39191ms step_avg:87.68ms
step:448/1670 train_time:39279ms step_avg:87.68ms
step:449/1670 train_time:39367ms step_avg:87.68ms
step:450/1670 train_time:39454ms step_avg:87.68ms
step:451/1670 train_time:39542ms step_avg:87.68ms
step:452/1670 train_time:39629ms step_avg:87.68ms
step:453/1670 train_time:39717ms step_avg:87.67ms
step:454/1670 train_time:39805ms step_avg:87.68ms
step:455/1670 train_time:39893ms step_avg:87.68ms
step:456/1670 train_time:39980ms step_avg:87.68ms
step:457/1670 train_time:40068ms step_avg:87.68ms
step:458/1670 train_time:40155ms step_avg:87.67ms
step:459/1670 train_time:40243ms step_avg:87.68ms
step:460/1670 train_time:40330ms step_avg:87.67ms
step:461/1670 train_time:40418ms step_avg:87.67ms
step:462/1670 train_time:40506ms step_avg:87.68ms
step:463/1670 train_time:40594ms step_avg:87.68ms
step:464/1670 train_time:40682ms step_avg:87.68ms
step:465/1670 train_time:40769ms step_avg:87.68ms
step:466/1670 train_time:40857ms step_avg:87.68ms
step:467/1670 train_time:40945ms step_avg:87.68ms
step:468/1670 train_time:41032ms step_avg:87.68ms
step:469/1670 train_time:41119ms step_avg:87.67ms
step:470/1670 train_time:41207ms step_avg:87.67ms
step:471/1670 train_time:41294ms step_avg:87.67ms
step:472/1670 train_time:41381ms step_avg:87.67ms
step:473/1670 train_time:41468ms step_avg:87.67ms
step:474/1670 train_time:41556ms step_avg:87.67ms
step:475/1670 train_time:41646ms step_avg:87.68ms
step:476/1670 train_time:41733ms step_avg:87.68ms
step:477/1670 train_time:41821ms step_avg:87.68ms
step:478/1670 train_time:41909ms step_avg:87.68ms
step:479/1670 train_time:41997ms step_avg:87.68ms
step:480/1670 train_time:42086ms step_avg:87.68ms
step:481/1670 train_time:42174ms step_avg:87.68ms
step:482/1670 train_time:42261ms step_avg:87.68ms
step:483/1670 train_time:42348ms step_avg:87.68ms
step:484/1670 train_time:42436ms step_avg:87.68ms
step:485/1670 train_time:42524ms step_avg:87.68ms
step:486/1670 train_time:42611ms step_avg:87.68ms
step:487/1670 train_time:42700ms step_avg:87.68ms
step:488/1670 train_time:42787ms step_avg:87.68ms
step:489/1670 train_time:42875ms step_avg:87.68ms
step:490/1670 train_time:42962ms step_avg:87.68ms
step:491/1670 train_time:43049ms step_avg:87.68ms
step:492/1670 train_time:43138ms step_avg:87.68ms
step:493/1670 train_time:43225ms step_avg:87.68ms
step:494/1670 train_time:43312ms step_avg:87.68ms
step:495/1670 train_time:43400ms step_avg:87.68ms
step:496/1670 train_time:43488ms step_avg:87.68ms
step:497/1670 train_time:43576ms step_avg:87.68ms
step:498/1670 train_time:43665ms step_avg:87.68ms
step:499/1670 train_time:43752ms step_avg:87.68ms
step:500/1670 train_time:43840ms step_avg:87.68ms
step:500/1670 val_loss:3.7174 train_time:43929ms step_avg:87.86ms
step:501/1670 train_time:43948ms step_avg:87.72ms
step:502/1670 train_time:44021ms step_avg:87.69ms
step:503/1670 train_time:44114ms step_avg:87.70ms
step:504/1670 train_time:44201ms step_avg:87.70ms
step:505/1670 train_time:44288ms step_avg:87.70ms
step:506/1670 train_time:44375ms step_avg:87.70ms
step:507/1670 train_time:44461ms step_avg:87.69ms
step:508/1670 train_time:44548ms step_avg:87.69ms
step:509/1670 train_time:44635ms step_avg:87.69ms
step:510/1670 train_time:44722ms step_avg:87.69ms
step:511/1670 train_time:44809ms step_avg:87.69ms
step:512/1670 train_time:44899ms step_avg:87.69ms
step:513/1670 train_time:44988ms step_avg:87.70ms
step:514/1670 train_time:45078ms step_avg:87.70ms
step:515/1670 train_time:45167ms step_avg:87.70ms
step:516/1670 train_time:45255ms step_avg:87.70ms
step:517/1670 train_time:45342ms step_avg:87.70ms
step:518/1670 train_time:45429ms step_avg:87.70ms
step:519/1670 train_time:45517ms step_avg:87.70ms
step:520/1670 train_time:45603ms step_avg:87.70ms
step:521/1670 train_time:45690ms step_avg:87.70ms
step:522/1670 train_time:45777ms step_avg:87.70ms
step:523/1670 train_time:45865ms step_avg:87.70ms
step:524/1670 train_time:45955ms step_avg:87.70ms
step:525/1670 train_time:46043ms step_avg:87.70ms
step:526/1670 train_time:46132ms step_avg:87.70ms
step:527/1670 train_time:46220ms step_avg:87.70ms
step:528/1670 train_time:46307ms step_avg:87.70ms
step:529/1670 train_time:46395ms step_avg:87.70ms
step:530/1670 train_time:46481ms step_avg:87.70ms
step:531/1670 train_time:46568ms step_avg:87.70ms
step:532/1670 train_time:46656ms step_avg:87.70ms
step:533/1670 train_time:46742ms step_avg:87.70ms
step:534/1670 train_time:46830ms step_avg:87.70ms
step:535/1670 train_time:46919ms step_avg:87.70ms
step:536/1670 train_time:47007ms step_avg:87.70ms
step:537/1670 train_time:47096ms step_avg:87.70ms
step:538/1670 train_time:47183ms step_avg:87.70ms
step:539/1670 train_time:47271ms step_avg:87.70ms
step:540/1670 train_time:47359ms step_avg:87.70ms
step:541/1670 train_time:47447ms step_avg:87.70ms
step:542/1670 train_time:47534ms step_avg:87.70ms
step:543/1670 train_time:47621ms step_avg:87.70ms
step:544/1670 train_time:47708ms step_avg:87.70ms
step:545/1670 train_time:47796ms step_avg:87.70ms
step:546/1670 train_time:47885ms step_avg:87.70ms
step:547/1670 train_time:47974ms step_avg:87.70ms
step:548/1670 train_time:48063ms step_avg:87.71ms
step:549/1670 train_time:48152ms step_avg:87.71ms
step:550/1670 train_time:48241ms step_avg:87.71ms
step:551/1670 train_time:48330ms step_avg:87.71ms
step:552/1670 train_time:48419ms step_avg:87.72ms
step:553/1670 train_time:48508ms step_avg:87.72ms
step:554/1670 train_time:48597ms step_avg:87.72ms
step:555/1670 train_time:48687ms step_avg:87.72ms
step:556/1670 train_time:48775ms step_avg:87.73ms
step:557/1670 train_time:48863ms step_avg:87.73ms
step:558/1670 train_time:48953ms step_avg:87.73ms
step:559/1670 train_time:49041ms step_avg:87.73ms
step:560/1670 train_time:49131ms step_avg:87.73ms
step:561/1670 train_time:49220ms step_avg:87.74ms
step:562/1670 train_time:49310ms step_avg:87.74ms
step:563/1670 train_time:49399ms step_avg:87.74ms
step:564/1670 train_time:49488ms step_avg:87.74ms
step:565/1670 train_time:49576ms step_avg:87.75ms
step:566/1670 train_time:49666ms step_avg:87.75ms
step:567/1670 train_time:49755ms step_avg:87.75ms
step:568/1670 train_time:49844ms step_avg:87.75ms
step:569/1670 train_time:49933ms step_avg:87.76ms
step:570/1670 train_time:50021ms step_avg:87.76ms
step:571/1670 train_time:50111ms step_avg:87.76ms
step:572/1670 train_time:50200ms step_avg:87.76ms
step:573/1670 train_time:50290ms step_avg:87.77ms
step:574/1670 train_time:50378ms step_avg:87.77ms
step:575/1670 train_time:50467ms step_avg:87.77ms
step:576/1670 train_time:50557ms step_avg:87.77ms
step:577/1670 train_time:50645ms step_avg:87.77ms
step:578/1670 train_time:50734ms step_avg:87.77ms
step:579/1670 train_time:50822ms step_avg:87.78ms
step:580/1670 train_time:50911ms step_avg:87.78ms
step:581/1670 train_time:51001ms step_avg:87.78ms
step:582/1670 train_time:51090ms step_avg:87.78ms
step:583/1670 train_time:51179ms step_avg:87.78ms
step:584/1670 train_time:51268ms step_avg:87.79ms
step:585/1670 train_time:51357ms step_avg:87.79ms
step:586/1670 train_time:51445ms step_avg:87.79ms
step:587/1670 train_time:51535ms step_avg:87.79ms
step:588/1670 train_time:51623ms step_avg:87.79ms
step:589/1670 train_time:51713ms step_avg:87.80ms
step:590/1670 train_time:51801ms step_avg:87.80ms
step:591/1670 train_time:51891ms step_avg:87.80ms
step:592/1670 train_time:51980ms step_avg:87.80ms
step:593/1670 train_time:52069ms step_avg:87.81ms
step:594/1670 train_time:52158ms step_avg:87.81ms
step:595/1670 train_time:52247ms step_avg:87.81ms
step:596/1670 train_time:52336ms step_avg:87.81ms
step:597/1670 train_time:52426ms step_avg:87.82ms
step:598/1670 train_time:52516ms step_avg:87.82ms
step:599/1670 train_time:52605ms step_avg:87.82ms
step:600/1670 train_time:52694ms step_avg:87.82ms
step:601/1670 train_time:52783ms step_avg:87.82ms
step:602/1670 train_time:52872ms step_avg:87.83ms
step:603/1670 train_time:52960ms step_avg:87.83ms
step:604/1670 train_time:53049ms step_avg:87.83ms
step:605/1670 train_time:53138ms step_avg:87.83ms
step:606/1670 train_time:53227ms step_avg:87.83ms
step:607/1670 train_time:53316ms step_avg:87.83ms
step:608/1670 train_time:53404ms step_avg:87.84ms
step:609/1670 train_time:53495ms step_avg:87.84ms
step:610/1670 train_time:53584ms step_avg:87.84ms
step:611/1670 train_time:53673ms step_avg:87.84ms
step:612/1670 train_time:53761ms step_avg:87.85ms
step:613/1670 train_time:53851ms step_avg:87.85ms
step:614/1670 train_time:53939ms step_avg:87.85ms
step:615/1670 train_time:54028ms step_avg:87.85ms
step:616/1670 train_time:54117ms step_avg:87.85ms
step:617/1670 train_time:54206ms step_avg:87.85ms
step:618/1670 train_time:54295ms step_avg:87.86ms
step:619/1670 train_time:54384ms step_avg:87.86ms
step:620/1670 train_time:54474ms step_avg:87.86ms
step:621/1670 train_time:54562ms step_avg:87.86ms
step:622/1670 train_time:54651ms step_avg:87.86ms
step:623/1670 train_time:54740ms step_avg:87.86ms
step:624/1670 train_time:54829ms step_avg:87.87ms
step:625/1670 train_time:54918ms step_avg:87.87ms
step:625/1670 val_loss:3.6176 train_time:55009ms step_avg:88.01ms
step:626/1670 train_time:55029ms step_avg:87.91ms
step:627/1670 train_time:55099ms step_avg:87.88ms
step:628/1670 train_time:55188ms step_avg:87.88ms
step:629/1670 train_time:55277ms step_avg:87.88ms
step:630/1670 train_time:55366ms step_avg:87.88ms
step:631/1670 train_time:55454ms step_avg:87.88ms
step:632/1670 train_time:55541ms step_avg:87.88ms
step:633/1670 train_time:55629ms step_avg:87.88ms
step:634/1670 train_time:55716ms step_avg:87.88ms
step:635/1670 train_time:55804ms step_avg:87.88ms
step:636/1670 train_time:55896ms step_avg:87.89ms
step:637/1670 train_time:55991ms step_avg:87.90ms
step:638/1670 train_time:56081ms step_avg:87.90ms
step:639/1670 train_time:56172ms step_avg:87.91ms
step:640/1670 train_time:56260ms step_avg:87.91ms
step:641/1670 train_time:56350ms step_avg:87.91ms
step:642/1670 train_time:56438ms step_avg:87.91ms
step:643/1670 train_time:56525ms step_avg:87.91ms
step:644/1670 train_time:56613ms step_avg:87.91ms
step:645/1670 train_time:56701ms step_avg:87.91ms
step:646/1670 train_time:56790ms step_avg:87.91ms
step:647/1670 train_time:56880ms step_avg:87.91ms
step:648/1670 train_time:56972ms step_avg:87.92ms
step:649/1670 train_time:57063ms step_avg:87.92ms
step:650/1670 train_time:57153ms step_avg:87.93ms
step:651/1670 train_time:57243ms step_avg:87.93ms
step:652/1670 train_time:57332ms step_avg:87.93ms
step:653/1670 train_time:57421ms step_avg:87.93ms
step:654/1670 train_time:57509ms step_avg:87.93ms
step:655/1670 train_time:57597ms step_avg:87.93ms
step:656/1670 train_time:57685ms step_avg:87.93ms
step:657/1670 train_time:57773ms step_avg:87.93ms
step:658/1670 train_time:57863ms step_avg:87.94ms
step:659/1670 train_time:57953ms step_avg:87.94ms
step:660/1670 train_time:58042ms step_avg:87.94ms
step:661/1670 train_time:58132ms step_avg:87.95ms
step:662/1670 train_time:58222ms step_avg:87.95ms
step:663/1670 train_time:58310ms step_avg:87.95ms
step:664/1670 train_time:58399ms step_avg:87.95ms
step:665/1670 train_time:58487ms step_avg:87.95ms
step:666/1670 train_time:58575ms step_avg:87.95ms
step:667/1670 train_time:58664ms step_avg:87.95ms
step:668/1670 train_time:58753ms step_avg:87.95ms
step:669/1670 train_time:58842ms step_avg:87.96ms
step:670/1670 train_time:58932ms step_avg:87.96ms
step:671/1670 train_time:59020ms step_avg:87.96ms
step:672/1670 train_time:59110ms step_avg:87.96ms
step:673/1670 train_time:59199ms step_avg:87.96ms
step:674/1670 train_time:59287ms step_avg:87.96ms
step:675/1670 train_time:59375ms step_avg:87.96ms
step:676/1670 train_time:59464ms step_avg:87.97ms
step:677/1670 train_time:59553ms step_avg:87.97ms
step:678/1670 train_time:59641ms step_avg:87.97ms
step:679/1670 train_time:59729ms step_avg:87.97ms
step:680/1670 train_time:59818ms step_avg:87.97ms
step:681/1670 train_time:59907ms step_avg:87.97ms
step:682/1670 train_time:59996ms step_avg:87.97ms
step:683/1670 train_time:60085ms step_avg:87.97ms
step:684/1670 train_time:60173ms step_avg:87.97ms
step:685/1670 train_time:60262ms step_avg:87.97ms
step:686/1670 train_time:60352ms step_avg:87.98ms
step:687/1670 train_time:60442ms step_avg:87.98ms
step:688/1670 train_time:60531ms step_avg:87.98ms
step:689/1670 train_time:60619ms step_avg:87.98ms
step:690/1670 train_time:60709ms step_avg:87.98ms
step:691/1670 train_time:60797ms step_avg:87.98ms
step:692/1670 train_time:60886ms step_avg:87.99ms
step:693/1670 train_time:60975ms step_avg:87.99ms
step:694/1670 train_time:61065ms step_avg:87.99ms
step:695/1670 train_time:61155ms step_avg:87.99ms
step:696/1670 train_time:61244ms step_avg:87.99ms
step:697/1670 train_time:61333ms step_avg:88.00ms
step:698/1670 train_time:61422ms step_avg:88.00ms
step:699/1670 train_time:61511ms step_avg:88.00ms
step:700/1670 train_time:61600ms step_avg:88.00ms
step:701/1670 train_time:61689ms step_avg:88.00ms
step:702/1670 train_time:61777ms step_avg:88.00ms
step:703/1670 train_time:61867ms step_avg:88.00ms
step:704/1670 train_time:61956ms step_avg:88.01ms
step:705/1670 train_time:62045ms step_avg:88.01ms
step:706/1670 train_time:62134ms step_avg:88.01ms
step:707/1670 train_time:62223ms step_avg:88.01ms
step:708/1670 train_time:62312ms step_avg:88.01ms
step:709/1670 train_time:62401ms step_avg:88.01ms
step:710/1670 train_time:62491ms step_avg:88.01ms
step:711/1670 train_time:62579ms step_avg:88.02ms
step:712/1670 train_time:62668ms step_avg:88.02ms
step:713/1670 train_time:62757ms step_avg:88.02ms
step:714/1670 train_time:62846ms step_avg:88.02ms
step:715/1670 train_time:62934ms step_avg:88.02ms
step:716/1670 train_time:63023ms step_avg:88.02ms
step:717/1670 train_time:63111ms step_avg:88.02ms
step:718/1670 train_time:63200ms step_avg:88.02ms
step:719/1670 train_time:63290ms step_avg:88.02ms
step:720/1670 train_time:63378ms step_avg:88.02ms
step:721/1670 train_time:63467ms step_avg:88.03ms
step:722/1670 train_time:63555ms step_avg:88.03ms
step:723/1670 train_time:63644ms step_avg:88.03ms
step:724/1670 train_time:63733ms step_avg:88.03ms
step:725/1670 train_time:63822ms step_avg:88.03ms
step:726/1670 train_time:63912ms step_avg:88.03ms
step:727/1670 train_time:64001ms step_avg:88.03ms
step:728/1670 train_time:64090ms step_avg:88.04ms
step:729/1670 train_time:64178ms step_avg:88.04ms
step:730/1670 train_time:64268ms step_avg:88.04ms
step:731/1670 train_time:64356ms step_avg:88.04ms
step:732/1670 train_time:64445ms step_avg:88.04ms
step:733/1670 train_time:64534ms step_avg:88.04ms
step:734/1670 train_time:64623ms step_avg:88.04ms
step:735/1670 train_time:64711ms step_avg:88.04ms
step:736/1670 train_time:64800ms step_avg:88.04ms
step:737/1670 train_time:64889ms step_avg:88.05ms
step:738/1670 train_time:64978ms step_avg:88.05ms
step:739/1670 train_time:65068ms step_avg:88.05ms
step:740/1670 train_time:65157ms step_avg:88.05ms
step:741/1670 train_time:65247ms step_avg:88.05ms
step:742/1670 train_time:65335ms step_avg:88.05ms
step:743/1670 train_time:65424ms step_avg:88.05ms
step:744/1670 train_time:65513ms step_avg:88.06ms
step:745/1670 train_time:65602ms step_avg:88.06ms
step:746/1670 train_time:65693ms step_avg:88.06ms
step:747/1670 train_time:65782ms step_avg:88.06ms
step:748/1670 train_time:65871ms step_avg:88.06ms
step:749/1670 train_time:65959ms step_avg:88.06ms
step:750/1670 train_time:66049ms step_avg:88.07ms
step:750/1670 val_loss:3.5665 train_time:66139ms step_avg:88.19ms
step:751/1670 train_time:66159ms step_avg:88.09ms
step:752/1670 train_time:66233ms step_avg:88.08ms
step:753/1670 train_time:66323ms step_avg:88.08ms
step:754/1670 train_time:66414ms step_avg:88.08ms
step:755/1670 train_time:66502ms step_avg:88.08ms
step:756/1670 train_time:66591ms step_avg:88.08ms
step:757/1670 train_time:66680ms step_avg:88.08ms
step:758/1670 train_time:66768ms step_avg:88.08ms
step:759/1670 train_time:66856ms step_avg:88.08ms
step:760/1670 train_time:66943ms step_avg:88.08ms
step:761/1670 train_time:67032ms step_avg:88.08ms
step:762/1670 train_time:67122ms step_avg:88.09ms
step:763/1670 train_time:67213ms step_avg:88.09ms
step:764/1670 train_time:67304ms step_avg:88.09ms
step:765/1670 train_time:67393ms step_avg:88.10ms
step:766/1670 train_time:67482ms step_avg:88.10ms
step:767/1670 train_time:67573ms step_avg:88.10ms
step:768/1670 train_time:67660ms step_avg:88.10ms
step:769/1670 train_time:67749ms step_avg:88.10ms
step:770/1670 train_time:67837ms step_avg:88.10ms
step:771/1670 train_time:67925ms step_avg:88.10ms
step:772/1670 train_time:68014ms step_avg:88.10ms
step:773/1670 train_time:68103ms step_avg:88.10ms
step:774/1670 train_time:68194ms step_avg:88.11ms
step:775/1670 train_time:68283ms step_avg:88.11ms
step:776/1670 train_time:68374ms step_avg:88.11ms
step:777/1670 train_time:68463ms step_avg:88.11ms
step:778/1670 train_time:68553ms step_avg:88.11ms
step:779/1670 train_time:68642ms step_avg:88.12ms
step:780/1670 train_time:68730ms step_avg:88.12ms
step:781/1670 train_time:68818ms step_avg:88.12ms
step:782/1670 train_time:68906ms step_avg:88.12ms
step:783/1670 train_time:68996ms step_avg:88.12ms
step:784/1670 train_time:69084ms step_avg:88.12ms
step:785/1670 train_time:69174ms step_avg:88.12ms
step:786/1670 train_time:69263ms step_avg:88.12ms
step:787/1670 train_time:69353ms step_avg:88.12ms
step:788/1670 train_time:69441ms step_avg:88.12ms
step:789/1670 train_time:69531ms step_avg:88.13ms
step:790/1670 train_time:69619ms step_avg:88.13ms
step:791/1670 train_time:69708ms step_avg:88.13ms
step:792/1670 train_time:69796ms step_avg:88.13ms
step:793/1670 train_time:69884ms step_avg:88.13ms
step:794/1670 train_time:69974ms step_avg:88.13ms
step:795/1670 train_time:70063ms step_avg:88.13ms
step:796/1670 train_time:70153ms step_avg:88.13ms
step:797/1670 train_time:70241ms step_avg:88.13ms
step:798/1670 train_time:70330ms step_avg:88.13ms
step:799/1670 train_time:70419ms step_avg:88.13ms
step:800/1670 train_time:70510ms step_avg:88.14ms
step:801/1670 train_time:70598ms step_avg:88.14ms
step:802/1670 train_time:70687ms step_avg:88.14ms
step:803/1670 train_time:70775ms step_avg:88.14ms
step:804/1670 train_time:70863ms step_avg:88.14ms
step:805/1670 train_time:70952ms step_avg:88.14ms
step:806/1670 train_time:71041ms step_avg:88.14ms
step:807/1670 train_time:71131ms step_avg:88.14ms
step:808/1670 train_time:71219ms step_avg:88.14ms
step:809/1670 train_time:71308ms step_avg:88.14ms
step:810/1670 train_time:71398ms step_avg:88.15ms
step:811/1670 train_time:71487ms step_avg:88.15ms
step:812/1670 train_time:71576ms step_avg:88.15ms
step:813/1670 train_time:71665ms step_avg:88.15ms
step:814/1670 train_time:71754ms step_avg:88.15ms
step:815/1670 train_time:71842ms step_avg:88.15ms
step:816/1670 train_time:71931ms step_avg:88.15ms
step:817/1670 train_time:72019ms step_avg:88.15ms
step:818/1670 train_time:72108ms step_avg:88.15ms
step:819/1670 train_time:72197ms step_avg:88.15ms
step:820/1670 train_time:72286ms step_avg:88.15ms
step:821/1670 train_time:72375ms step_avg:88.15ms
step:822/1670 train_time:72464ms step_avg:88.16ms
step:823/1670 train_time:72553ms step_avg:88.16ms
step:824/1670 train_time:72641ms step_avg:88.16ms
step:825/1670 train_time:72731ms step_avg:88.16ms
step:826/1670 train_time:72819ms step_avg:88.16ms
step:827/1670 train_time:72908ms step_avg:88.16ms
step:828/1670 train_time:72997ms step_avg:88.16ms
step:829/1670 train_time:73086ms step_avg:88.16ms
step:830/1670 train_time:73174ms step_avg:88.16ms
step:831/1670 train_time:73263ms step_avg:88.16ms
step:832/1670 train_time:73352ms step_avg:88.16ms
step:833/1670 train_time:73440ms step_avg:88.16ms
step:834/1670 train_time:73530ms step_avg:88.17ms
step:835/1670 train_time:73618ms step_avg:88.17ms
step:836/1670 train_time:73708ms step_avg:88.17ms
step:837/1670 train_time:73797ms step_avg:88.17ms
step:838/1670 train_time:73886ms step_avg:88.17ms
step:839/1670 train_time:73975ms step_avg:88.17ms
step:840/1670 train_time:74063ms step_avg:88.17ms
step:841/1670 train_time:74153ms step_avg:88.17ms
step:842/1670 train_time:74241ms step_avg:88.17ms
step:843/1670 train_time:74331ms step_avg:88.17ms
step:844/1670 train_time:74419ms step_avg:88.17ms
step:845/1670 train_time:74508ms step_avg:88.18ms
step:846/1670 train_time:74597ms step_avg:88.18ms
step:847/1670 train_time:74686ms step_avg:88.18ms
step:848/1670 train_time:74775ms step_avg:88.18ms
step:849/1670 train_time:74864ms step_avg:88.18ms
step:850/1670 train_time:74953ms step_avg:88.18ms
step:851/1670 train_time:75042ms step_avg:88.18ms
step:852/1670 train_time:75131ms step_avg:88.18ms
step:853/1670 train_time:75220ms step_avg:88.18ms
step:854/1670 train_time:75309ms step_avg:88.18ms
step:855/1670 train_time:75397ms step_avg:88.18ms
step:856/1670 train_time:75486ms step_avg:88.18ms
step:857/1670 train_time:75576ms step_avg:88.19ms
step:858/1670 train_time:75666ms step_avg:88.19ms
step:859/1670 train_time:75754ms step_avg:88.19ms
step:860/1670 train_time:75843ms step_avg:88.19ms
step:861/1670 train_time:75932ms step_avg:88.19ms
step:862/1670 train_time:76021ms step_avg:88.19ms
step:863/1670 train_time:76111ms step_avg:88.19ms
step:864/1670 train_time:76200ms step_avg:88.19ms
step:865/1670 train_time:76290ms step_avg:88.20ms
step:866/1670 train_time:76379ms step_avg:88.20ms
step:867/1670 train_time:76468ms step_avg:88.20ms
step:868/1670 train_time:76556ms step_avg:88.20ms
step:869/1670 train_time:76645ms step_avg:88.20ms
step:870/1670 train_time:76735ms step_avg:88.20ms
step:871/1670 train_time:76823ms step_avg:88.20ms
step:872/1670 train_time:76912ms step_avg:88.20ms
step:873/1670 train_time:77000ms step_avg:88.20ms
step:874/1670 train_time:77090ms step_avg:88.20ms
step:875/1670 train_time:77178ms step_avg:88.20ms
step:875/1670 val_loss:3.5204 train_time:77269ms step_avg:88.31ms
step:876/1670 train_time:77288ms step_avg:88.23ms
step:877/1670 train_time:77359ms step_avg:88.21ms
step:878/1670 train_time:77451ms step_avg:88.21ms
step:879/1670 train_time:77541ms step_avg:88.21ms
step:880/1670 train_time:77629ms step_avg:88.21ms
step:881/1670 train_time:77716ms step_avg:88.21ms
step:882/1670 train_time:77804ms step_avg:88.21ms
step:883/1670 train_time:77892ms step_avg:88.21ms
step:884/1670 train_time:77980ms step_avg:88.21ms
step:885/1670 train_time:78068ms step_avg:88.21ms
step:886/1670 train_time:78156ms step_avg:88.21ms
step:887/1670 train_time:78247ms step_avg:88.21ms
step:888/1670 train_time:78338ms step_avg:88.22ms
step:889/1670 train_time:78430ms step_avg:88.22ms
step:890/1670 train_time:78520ms step_avg:88.22ms
step:891/1670 train_time:78608ms step_avg:88.22ms
step:892/1670 train_time:78697ms step_avg:88.23ms
step:893/1670 train_time:78785ms step_avg:88.23ms
step:894/1670 train_time:78874ms step_avg:88.23ms
step:895/1670 train_time:78962ms step_avg:88.23ms
step:896/1670 train_time:79050ms step_avg:88.23ms
step:897/1670 train_time:79138ms step_avg:88.22ms
step:898/1670 train_time:79227ms step_avg:88.23ms
step:899/1670 train_time:79316ms step_avg:88.23ms
step:900/1670 train_time:79406ms step_avg:88.23ms
step:901/1670 train_time:79496ms step_avg:88.23ms
step:902/1670 train_time:79585ms step_avg:88.23ms
step:903/1670 train_time:79674ms step_avg:88.23ms
step:904/1670 train_time:79762ms step_avg:88.23ms
step:905/1670 train_time:79851ms step_avg:88.23ms
step:906/1670 train_time:79940ms step_avg:88.23ms
step:907/1670 train_time:80029ms step_avg:88.23ms
step:908/1670 train_time:80117ms step_avg:88.23ms
step:909/1670 train_time:80206ms step_avg:88.24ms
step:910/1670 train_time:80296ms step_avg:88.24ms
step:911/1670 train_time:80385ms step_avg:88.24ms
step:912/1670 train_time:80475ms step_avg:88.24ms
step:913/1670 train_time:80564ms step_avg:88.24ms
step:914/1670 train_time:80654ms step_avg:88.24ms
step:915/1670 train_time:80743ms step_avg:88.24ms
step:916/1670 train_time:80832ms step_avg:88.24ms
step:917/1670 train_time:80920ms step_avg:88.24ms
step:918/1670 train_time:81009ms step_avg:88.24ms
step:919/1670 train_time:81097ms step_avg:88.25ms
step:920/1670 train_time:81186ms step_avg:88.25ms
step:921/1670 train_time:81276ms step_avg:88.25ms
step:922/1670 train_time:81366ms step_avg:88.25ms
step:923/1670 train_time:81456ms step_avg:88.25ms
step:924/1670 train_time:81546ms step_avg:88.25ms
step:925/1670 train_time:81636ms step_avg:88.25ms
step:926/1670 train_time:81724ms step_avg:88.25ms
step:927/1670 train_time:81813ms step_avg:88.26ms
step:928/1670 train_time:81901ms step_avg:88.26ms
step:929/1670 train_time:81990ms step_avg:88.26ms
step:930/1670 train_time:82078ms step_avg:88.26ms
step:931/1670 train_time:82167ms step_avg:88.26ms
step:932/1670 train_time:82256ms step_avg:88.26ms
step:933/1670 train_time:82345ms step_avg:88.26ms
step:934/1670 train_time:82435ms step_avg:88.26ms
step:935/1670 train_time:82525ms step_avg:88.26ms
step:936/1670 train_time:82614ms step_avg:88.26ms
step:937/1670 train_time:82702ms step_avg:88.26ms
step:938/1670 train_time:82792ms step_avg:88.26ms
step:939/1670 train_time:82880ms step_avg:88.26ms
step:940/1670 train_time:82969ms step_avg:88.27ms
step:941/1670 train_time:83058ms step_avg:88.27ms
step:942/1670 train_time:83147ms step_avg:88.27ms
step:943/1670 train_time:83237ms step_avg:88.27ms
step:944/1670 train_time:83326ms step_avg:88.27ms
step:945/1670 train_time:83415ms step_avg:88.27ms
step:946/1670 train_time:83504ms step_avg:88.27ms
step:947/1670 train_time:83594ms step_avg:88.27ms
step:948/1670 train_time:83682ms step_avg:88.27ms
step:949/1670 train_time:83771ms step_avg:88.27ms
step:950/1670 train_time:83860ms step_avg:88.27ms
step:951/1670 train_time:83950ms step_avg:88.28ms
step:952/1670 train_time:84039ms step_avg:88.28ms
step:953/1670 train_time:84128ms step_avg:88.28ms
step:954/1670 train_time:84217ms step_avg:88.28ms
step:955/1670 train_time:84306ms step_avg:88.28ms
step:956/1670 train_time:84395ms step_avg:88.28ms
step:957/1670 train_time:84484ms step_avg:88.28ms
step:958/1670 train_time:84573ms step_avg:88.28ms
step:959/1670 train_time:84661ms step_avg:88.28ms
step:960/1670 train_time:84750ms step_avg:88.28ms
step:961/1670 train_time:84839ms step_avg:88.28ms
step:962/1670 train_time:84929ms step_avg:88.28ms
step:963/1670 train_time:85017ms step_avg:88.28ms
step:964/1670 train_time:85106ms step_avg:88.28ms
step:965/1670 train_time:85196ms step_avg:88.29ms
step:966/1670 train_time:85284ms step_avg:88.29ms
step:967/1670 train_time:85373ms step_avg:88.29ms
step:968/1670 train_time:85462ms step_avg:88.29ms
step:969/1670 train_time:85551ms step_avg:88.29ms
step:970/1670 train_time:85640ms step_avg:88.29ms
step:971/1670 train_time:85729ms step_avg:88.29ms
step:972/1670 train_time:85818ms step_avg:88.29ms
step:973/1670 train_time:85907ms step_avg:88.29ms
step:974/1670 train_time:85996ms step_avg:88.29ms
step:975/1670 train_time:86084ms step_avg:88.29ms
step:976/1670 train_time:86173ms step_avg:88.29ms
step:977/1670 train_time:86261ms step_avg:88.29ms
step:978/1670 train_time:86351ms step_avg:88.29ms
step:979/1670 train_time:86440ms step_avg:88.29ms
step:980/1670 train_time:86529ms step_avg:88.29ms
step:981/1670 train_time:86618ms step_avg:88.30ms
step:982/1670 train_time:86707ms step_avg:88.30ms
step:983/1670 train_time:86796ms step_avg:88.30ms
step:984/1670 train_time:86884ms step_avg:88.30ms
step:985/1670 train_time:86974ms step_avg:88.30ms
step:986/1670 train_time:87062ms step_avg:88.30ms
step:987/1670 train_time:87151ms step_avg:88.30ms
step:988/1670 train_time:87240ms step_avg:88.30ms
step:989/1670 train_time:87329ms step_avg:88.30ms
step:990/1670 train_time:87419ms step_avg:88.30ms
step:991/1670 train_time:87508ms step_avg:88.30ms
step:992/1670 train_time:87597ms step_avg:88.30ms
step:993/1670 train_time:87686ms step_avg:88.30ms
step:994/1670 train_time:87775ms step_avg:88.31ms
step:995/1670 train_time:87864ms step_avg:88.31ms
step:996/1670 train_time:87953ms step_avg:88.31ms
step:997/1670 train_time:88041ms step_avg:88.31ms
step:998/1670 train_time:88130ms step_avg:88.31ms
step:999/1670 train_time:88219ms step_avg:88.31ms
step:1000/1670 train_time:88308ms step_avg:88.31ms
step:1000/1670 val_loss:3.4692 train_time:88399ms step_avg:88.40ms
step:1001/1670 train_time:88420ms step_avg:88.33ms
step:1002/1670 train_time:88492ms step_avg:88.32ms
step:1003/1670 train_time:88583ms step_avg:88.32ms
step:1004/1670 train_time:88672ms step_avg:88.32ms
step:1005/1670 train_time:88760ms step_avg:88.32ms
step:1006/1670 train_time:88847ms step_avg:88.32ms
step:1007/1670 train_time:88935ms step_avg:88.32ms
step:1008/1670 train_time:89024ms step_avg:88.32ms
step:1009/1670 train_time:89111ms step_avg:88.32ms
step:1010/1670 train_time:89200ms step_avg:88.32ms
step:1011/1670 train_time:89288ms step_avg:88.32ms
step:1012/1670 train_time:89379ms step_avg:88.32ms
step:1013/1670 train_time:89472ms step_avg:88.32ms
step:1014/1670 train_time:89562ms step_avg:88.33ms
step:1015/1670 train_time:89651ms step_avg:88.33ms
step:1016/1670 train_time:89740ms step_avg:88.33ms
step:1017/1670 train_time:89829ms step_avg:88.33ms
step:1018/1670 train_time:89916ms step_avg:88.33ms
step:1019/1670 train_time:90004ms step_avg:88.33ms
step:1020/1670 train_time:90093ms step_avg:88.33ms
step:1021/1670 train_time:90182ms step_avg:88.33ms
step:1022/1670 train_time:90271ms step_avg:88.33ms
step:1023/1670 train_time:90361ms step_avg:88.33ms
step:1024/1670 train_time:90452ms step_avg:88.33ms
step:1025/1670 train_time:90542ms step_avg:88.33ms
step:1026/1670 train_time:90631ms step_avg:88.33ms
step:1027/1670 train_time:90721ms step_avg:88.34ms
step:1028/1670 train_time:90809ms step_avg:88.34ms
step:1029/1670 train_time:90898ms step_avg:88.34ms
step:1030/1670 train_time:90986ms step_avg:88.34ms
step:1031/1670 train_time:91074ms step_avg:88.34ms
step:1032/1670 train_time:91162ms step_avg:88.34ms
step:1033/1670 train_time:91251ms step_avg:88.34ms
step:1034/1670 train_time:91341ms step_avg:88.34ms
step:1035/1670 train_time:91431ms step_avg:88.34ms
step:1036/1670 train_time:91521ms step_avg:88.34ms
step:1037/1670 train_time:91610ms step_avg:88.34ms
step:1038/1670 train_time:91700ms step_avg:88.34ms
step:1039/1670 train_time:91788ms step_avg:88.34ms
step:1040/1670 train_time:91878ms step_avg:88.34ms
step:1041/1670 train_time:91965ms step_avg:88.34ms
step:1042/1670 train_time:92054ms step_avg:88.34ms
step:1043/1670 train_time:92142ms step_avg:88.34ms
step:1044/1670 train_time:92231ms step_avg:88.34ms
step:1045/1670 train_time:92320ms step_avg:88.34ms
step:1046/1670 train_time:92409ms step_avg:88.35ms
step:1047/1670 train_time:92500ms step_avg:88.35ms
step:1048/1670 train_time:92589ms step_avg:88.35ms
step:1049/1670 train_time:92679ms step_avg:88.35ms
step:1050/1670 train_time:92767ms step_avg:88.35ms
step:1051/1670 train_time:92856ms step_avg:88.35ms
step:1052/1670 train_time:92944ms step_avg:88.35ms
step:1053/1670 train_time:93033ms step_avg:88.35ms
step:1054/1670 train_time:93122ms step_avg:88.35ms
step:1055/1670 train_time:93211ms step_avg:88.35ms
step:1056/1670 train_time:93300ms step_avg:88.35ms
step:1057/1670 train_time:93389ms step_avg:88.35ms
step:1058/1670 train_time:93478ms step_avg:88.35ms
step:1059/1670 train_time:93567ms step_avg:88.35ms
step:1060/1670 train_time:93656ms step_avg:88.36ms
step:1061/1670 train_time:93745ms step_avg:88.36ms
step:1062/1670 train_time:93833ms step_avg:88.36ms
step:1063/1670 train_time:93923ms step_avg:88.36ms
step:1064/1670 train_time:94013ms step_avg:88.36ms
step:1065/1670 train_time:94102ms step_avg:88.36ms
step:1066/1670 train_time:94191ms step_avg:88.36ms
step:1067/1670 train_time:94281ms step_avg:88.36ms
step:1068/1670 train_time:94370ms step_avg:88.36ms
step:1069/1670 train_time:94459ms step_avg:88.36ms
step:1070/1670 train_time:94548ms step_avg:88.36ms
step:1071/1670 train_time:94637ms step_avg:88.36ms
step:1072/1670 train_time:94726ms step_avg:88.36ms
step:1073/1670 train_time:94815ms step_avg:88.36ms
step:1074/1670 train_time:94904ms step_avg:88.36ms
step:1075/1670 train_time:94993ms step_avg:88.37ms
step:1076/1670 train_time:95083ms step_avg:88.37ms
step:1077/1670 train_time:95171ms step_avg:88.37ms
step:1078/1670 train_time:95261ms step_avg:88.37ms
step:1079/1670 train_time:95350ms step_avg:88.37ms
step:1080/1670 train_time:95439ms step_avg:88.37ms
step:1081/1670 train_time:95527ms step_avg:88.37ms
step:1082/1670 train_time:95616ms step_avg:88.37ms
step:1083/1670 train_time:95705ms step_avg:88.37ms
step:1084/1670 train_time:95794ms step_avg:88.37ms
step:1085/1670 train_time:95883ms step_avg:88.37ms
step:1086/1670 train_time:95972ms step_avg:88.37ms
step:1087/1670 train_time:96061ms step_avg:88.37ms
step:1088/1670 train_time:96150ms step_avg:88.37ms
step:1089/1670 train_time:96240ms step_avg:88.37ms
step:1090/1670 train_time:96330ms step_avg:88.38ms
step:1091/1670 train_time:96421ms step_avg:88.38ms
step:1092/1670 train_time:96511ms step_avg:88.38ms
step:1093/1670 train_time:96601ms step_avg:88.38ms
step:1094/1670 train_time:96691ms step_avg:88.38ms
step:1095/1670 train_time:96781ms step_avg:88.38ms
step:1096/1670 train_time:96871ms step_avg:88.39ms
step:1097/1670 train_time:96960ms step_avg:88.39ms
step:1098/1670 train_time:97049ms step_avg:88.39ms
step:1099/1670 train_time:97139ms step_avg:88.39ms
step:1100/1670 train_time:97228ms step_avg:88.39ms
step:1101/1670 train_time:97318ms step_avg:88.39ms
step:1102/1670 train_time:97407ms step_avg:88.39ms
step:1103/1670 train_time:97498ms step_avg:88.39ms
step:1104/1670 train_time:97588ms step_avg:88.39ms
step:1105/1670 train_time:97677ms step_avg:88.40ms
step:1106/1670 train_time:97767ms step_avg:88.40ms
step:1107/1670 train_time:97856ms step_avg:88.40ms
step:1108/1670 train_time:97946ms step_avg:88.40ms
step:1109/1670 train_time:98036ms step_avg:88.40ms
step:1110/1670 train_time:98125ms step_avg:88.40ms
step:1111/1670 train_time:98215ms step_avg:88.40ms
step:1112/1670 train_time:98304ms step_avg:88.40ms
step:1113/1670 train_time:98394ms step_avg:88.40ms
step:1114/1670 train_time:98484ms step_avg:88.41ms
step:1115/1670 train_time:98574ms step_avg:88.41ms
step:1116/1670 train_time:98663ms step_avg:88.41ms
step:1117/1670 train_time:98753ms step_avg:88.41ms
step:1118/1670 train_time:98842ms step_avg:88.41ms
step:1119/1670 train_time:98931ms step_avg:88.41ms
step:1120/1670 train_time:99021ms step_avg:88.41ms
step:1121/1670 train_time:99111ms step_avg:88.41ms
step:1122/1670 train_time:99201ms step_avg:88.41ms
step:1123/1670 train_time:99291ms step_avg:88.42ms
step:1124/1670 train_time:99381ms step_avg:88.42ms
step:1125/1670 train_time:99471ms step_avg:88.42ms
step:1125/1670 val_loss:3.4156 train_time:99562ms step_avg:88.50ms
step:1126/1670 train_time:99582ms step_avg:88.44ms
step:1127/1670 train_time:99656ms step_avg:88.43ms
step:1128/1670 train_time:99746ms step_avg:88.43ms
step:1129/1670 train_time:99838ms step_avg:88.43ms
step:1130/1670 train_time:99927ms step_avg:88.43ms
step:1131/1670 train_time:100015ms step_avg:88.43ms
step:1132/1670 train_time:100104ms step_avg:88.43ms
step:1133/1670 train_time:100191ms step_avg:88.43ms
step:1134/1670 train_time:100279ms step_avg:88.43ms
step:1135/1670 train_time:100368ms step_avg:88.43ms
step:1136/1670 train_time:100463ms step_avg:88.44ms
step:1137/1670 train_time:100556ms step_avg:88.44ms
step:1138/1670 train_time:100648ms step_avg:88.44ms
step:1139/1670 train_time:100738ms step_avg:88.44ms
step:1140/1670 train_time:100828ms step_avg:88.45ms
step:1141/1670 train_time:100917ms step_avg:88.45ms
step:1142/1670 train_time:101005ms step_avg:88.45ms
step:1143/1670 train_time:101094ms step_avg:88.45ms
step:1144/1670 train_time:101182ms step_avg:88.45ms
step:1145/1670 train_time:101270ms step_avg:88.45ms
step:1146/1670 train_time:101360ms step_avg:88.45ms
step:1147/1670 train_time:101452ms step_avg:88.45ms
step:1148/1670 train_time:101544ms step_avg:88.45ms
step:1149/1670 train_time:101634ms step_avg:88.45ms
step:1150/1670 train_time:101725ms step_avg:88.46ms
step:1151/1670 train_time:101815ms step_avg:88.46ms
step:1152/1670 train_time:101905ms step_avg:88.46ms
step:1153/1670 train_time:101993ms step_avg:88.46ms
step:1154/1670 train_time:102082ms step_avg:88.46ms
step:1155/1670 train_time:102170ms step_avg:88.46ms
step:1156/1670 train_time:102258ms step_avg:88.46ms
step:1157/1670 train_time:102348ms step_avg:88.46ms
step:1158/1670 train_time:102440ms step_avg:88.46ms
step:1159/1670 train_time:102529ms step_avg:88.46ms
step:1160/1670 train_time:102620ms step_avg:88.47ms
step:1161/1670 train_time:102711ms step_avg:88.47ms
step:1162/1670 train_time:102802ms step_avg:88.47ms
step:1163/1670 train_time:102890ms step_avg:88.47ms
step:1164/1670 train_time:102979ms step_avg:88.47ms
step:1165/1670 train_time:103069ms step_avg:88.47ms
step:1166/1670 train_time:103158ms step_avg:88.47ms
step:1167/1670 train_time:103246ms step_avg:88.47ms
step:1168/1670 train_time:103335ms step_avg:88.47ms
step:1169/1670 train_time:103426ms step_avg:88.47ms
step:1170/1670 train_time:103515ms step_avg:88.47ms
step:1171/1670 train_time:103607ms step_avg:88.48ms
step:1172/1670 train_time:103698ms step_avg:88.48ms
step:1173/1670 train_time:103788ms step_avg:88.48ms
step:1174/1670 train_time:103878ms step_avg:88.48ms
step:1175/1670 train_time:103968ms step_avg:88.48ms
step:1176/1670 train_time:104059ms step_avg:88.49ms
step:1177/1670 train_time:104147ms step_avg:88.49ms
step:1178/1670 train_time:104236ms step_avg:88.49ms
step:1179/1670 train_time:104325ms step_avg:88.49ms
step:1180/1670 train_time:104415ms step_avg:88.49ms
step:1181/1670 train_time:104506ms step_avg:88.49ms
step:1182/1670 train_time:104596ms step_avg:88.49ms
step:1183/1670 train_time:104687ms step_avg:88.49ms
step:1184/1670 train_time:104777ms step_avg:88.49ms
step:1185/1670 train_time:104867ms step_avg:88.50ms
step:1186/1670 train_time:104956ms step_avg:88.50ms
step:1187/1670 train_time:105046ms step_avg:88.50ms
step:1188/1670 train_time:105135ms step_avg:88.50ms
step:1189/1670 train_time:105225ms step_avg:88.50ms
step:1190/1670 train_time:105314ms step_avg:88.50ms
step:1191/1670 train_time:105404ms step_avg:88.50ms
step:1192/1670 train_time:105494ms step_avg:88.50ms
step:1193/1670 train_time:105585ms step_avg:88.50ms
step:1194/1670 train_time:105676ms step_avg:88.51ms
step:1195/1670 train_time:105766ms step_avg:88.51ms
step:1196/1670 train_time:105855ms step_avg:88.51ms
step:1197/1670 train_time:105945ms step_avg:88.51ms
step:1198/1670 train_time:106035ms step_avg:88.51ms
step:1199/1670 train_time:106124ms step_avg:88.51ms
step:1200/1670 train_time:106213ms step_avg:88.51ms
step:1201/1670 train_time:106303ms step_avg:88.51ms
step:1202/1670 train_time:106392ms step_avg:88.51ms
step:1203/1670 train_time:106481ms step_avg:88.51ms
step:1204/1670 train_time:106570ms step_avg:88.51ms
step:1205/1670 train_time:106661ms step_avg:88.51ms
step:1206/1670 train_time:106751ms step_avg:88.52ms
step:1207/1670 train_time:106840ms step_avg:88.52ms
step:1208/1670 train_time:106930ms step_avg:88.52ms
step:1209/1670 train_time:107020ms step_avg:88.52ms
step:1210/1670 train_time:107110ms step_avg:88.52ms
step:1211/1670 train_time:107199ms step_avg:88.52ms
step:1212/1670 train_time:107289ms step_avg:88.52ms
step:1213/1670 train_time:107379ms step_avg:88.52ms
step:1214/1670 train_time:107469ms step_avg:88.52ms
step:1215/1670 train_time:107559ms step_avg:88.53ms
step:1216/1670 train_time:107649ms step_avg:88.53ms
step:1217/1670 train_time:107739ms step_avg:88.53ms
step:1218/1670 train_time:107829ms step_avg:88.53ms
step:1219/1670 train_time:107918ms step_avg:88.53ms
step:1220/1670 train_time:108008ms step_avg:88.53ms
step:1221/1670 train_time:108098ms step_avg:88.53ms
step:1222/1670 train_time:108188ms step_avg:88.53ms
step:1223/1670 train_time:108278ms step_avg:88.53ms
step:1224/1670 train_time:108367ms step_avg:88.54ms
step:1225/1670 train_time:108457ms step_avg:88.54ms
step:1226/1670 train_time:108547ms step_avg:88.54ms
step:1227/1670 train_time:108637ms step_avg:88.54ms
step:1228/1670 train_time:108727ms step_avg:88.54ms
step:1229/1670 train_time:108816ms step_avg:88.54ms
step:1230/1670 train_time:108906ms step_avg:88.54ms
step:1231/1670 train_time:108995ms step_avg:88.54ms
step:1232/1670 train_time:109085ms step_avg:88.54ms
step:1233/1670 train_time:109174ms step_avg:88.54ms
step:1234/1670 train_time:109264ms step_avg:88.54ms
step:1235/1670 train_time:109353ms step_avg:88.54ms
step:1236/1670 train_time:109443ms step_avg:88.55ms
step:1237/1670 train_time:109533ms step_avg:88.55ms
step:1238/1670 train_time:109622ms step_avg:88.55ms
step:1239/1670 train_time:109711ms step_avg:88.55ms
step:1240/1670 train_time:109802ms step_avg:88.55ms
step:1241/1670 train_time:109891ms step_avg:88.55ms
step:1242/1670 train_time:109981ms step_avg:88.55ms
step:1243/1670 train_time:110071ms step_avg:88.55ms
step:1244/1670 train_time:110161ms step_avg:88.55ms
step:1245/1670 train_time:110251ms step_avg:88.55ms
step:1246/1670 train_time:110340ms step_avg:88.56ms
step:1247/1670 train_time:110429ms step_avg:88.56ms
step:1248/1670 train_time:110520ms step_avg:88.56ms
step:1249/1670 train_time:110609ms step_avg:88.56ms
step:1250/1670 train_time:110700ms step_avg:88.56ms
step:1250/1670 val_loss:3.3769 train_time:110790ms step_avg:88.63ms
step:1251/1670 train_time:110810ms step_avg:88.58ms
step:1252/1670 train_time:110883ms step_avg:88.56ms
step:1253/1670 train_time:110975ms step_avg:88.57ms
step:1254/1670 train_time:111064ms step_avg:88.57ms
step:1255/1670 train_time:111152ms step_avg:88.57ms
step:1256/1670 train_time:111241ms step_avg:88.57ms
step:1257/1670 train_time:111329ms step_avg:88.57ms
step:1258/1670 train_time:111417ms step_avg:88.57ms
step:1259/1670 train_time:111508ms step_avg:88.57ms
step:1260/1670 train_time:111597ms step_avg:88.57ms
step:1261/1670 train_time:111686ms step_avg:88.57ms
step:1262/1670 train_time:111780ms step_avg:88.57ms
step:1263/1670 train_time:111872ms step_avg:88.58ms
step:1264/1670 train_time:111963ms step_avg:88.58ms
step:1265/1670 train_time:112053ms step_avg:88.58ms
step:1266/1670 train_time:112142ms step_avg:88.58ms
step:1267/1670 train_time:112232ms step_avg:88.58ms
step:1268/1670 train_time:112320ms step_avg:88.58ms
step:1269/1670 train_time:112409ms step_avg:88.58ms
step:1270/1670 train_time:112497ms step_avg:88.58ms
step:1271/1670 train_time:112586ms step_avg:88.58ms
step:1272/1670 train_time:112676ms step_avg:88.58ms
step:1273/1670 train_time:112767ms step_avg:88.58ms
step:1274/1670 train_time:112858ms step_avg:88.59ms
step:1275/1670 train_time:112949ms step_avg:88.59ms
step:1276/1670 train_time:113039ms step_avg:88.59ms
step:1277/1670 train_time:113128ms step_avg:88.59ms
step:1278/1670 train_time:113217ms step_avg:88.59ms
step:1279/1670 train_time:113306ms step_avg:88.59ms
step:1280/1670 train_time:113396ms step_avg:88.59ms
step:1281/1670 train_time:113486ms step_avg:88.59ms
step:1282/1670 train_time:113575ms step_avg:88.59ms
step:1283/1670 train_time:113665ms step_avg:88.59ms
step:1284/1670 train_time:113755ms step_avg:88.59ms
step:1285/1670 train_time:113846ms step_avg:88.60ms
step:1286/1670 train_time:113937ms step_avg:88.60ms
step:1287/1670 train_time:114028ms step_avg:88.60ms
step:1288/1670 train_time:114117ms step_avg:88.60ms
step:1289/1670 train_time:114207ms step_avg:88.60ms
step:1290/1670 train_time:114297ms step_avg:88.60ms
step:1291/1670 train_time:114386ms step_avg:88.60ms
step:1292/1670 train_time:114476ms step_avg:88.60ms
step:1293/1670 train_time:114565ms step_avg:88.60ms
step:1294/1670 train_time:114655ms step_avg:88.60ms
step:1295/1670 train_time:114745ms step_avg:88.61ms
step:1296/1670 train_time:114836ms step_avg:88.61ms
step:1297/1670 train_time:114928ms step_avg:88.61ms
step:1298/1670 train_time:115017ms step_avg:88.61ms
step:1299/1670 train_time:115107ms step_avg:88.61ms
step:1300/1670 train_time:115197ms step_avg:88.61ms
step:1301/1670 train_time:115288ms step_avg:88.61ms
step:1302/1670 train_time:115376ms step_avg:88.61ms
step:1303/1670 train_time:115466ms step_avg:88.62ms
step:1304/1670 train_time:115555ms step_avg:88.62ms
step:1305/1670 train_time:115644ms step_avg:88.62ms
step:1306/1670 train_time:115734ms step_avg:88.62ms
step:1307/1670 train_time:115824ms step_avg:88.62ms
step:1308/1670 train_time:115915ms step_avg:88.62ms
step:1309/1670 train_time:116006ms step_avg:88.62ms
step:1310/1670 train_time:116096ms step_avg:88.62ms
step:1311/1670 train_time:116186ms step_avg:88.62ms
step:1312/1670 train_time:116275ms step_avg:88.62ms
step:1313/1670 train_time:116365ms step_avg:88.63ms
step:1314/1670 train_time:116454ms step_avg:88.63ms
step:1315/1670 train_time:116544ms step_avg:88.63ms
step:1316/1670 train_time:116634ms step_avg:88.63ms
step:1317/1670 train_time:116723ms step_avg:88.63ms
step:1318/1670 train_time:116813ms step_avg:88.63ms
step:1319/1670 train_time:116904ms step_avg:88.63ms
step:1320/1670 train_time:116996ms step_avg:88.63ms
step:1321/1670 train_time:117087ms step_avg:88.63ms
step:1322/1670 train_time:117176ms step_avg:88.64ms
step:1323/1670 train_time:117266ms step_avg:88.64ms
step:1324/1670 train_time:117355ms step_avg:88.64ms
step:1325/1670 train_time:117444ms step_avg:88.64ms
step:1326/1670 train_time:117533ms step_avg:88.64ms
step:1327/1670 train_time:117622ms step_avg:88.64ms
step:1328/1670 train_time:117712ms step_avg:88.64ms
step:1329/1670 train_time:117802ms step_avg:88.64ms
step:1330/1670 train_time:117893ms step_avg:88.64ms
step:1331/1670 train_time:117984ms step_avg:88.64ms
step:1332/1670 train_time:118074ms step_avg:88.64ms
step:1333/1670 train_time:118164ms step_avg:88.65ms
step:1334/1670 train_time:118253ms step_avg:88.65ms
step:1335/1670 train_time:118343ms step_avg:88.65ms
step:1336/1670 train_time:118432ms step_avg:88.65ms
step:1337/1670 train_time:118522ms step_avg:88.65ms
step:1338/1670 train_time:118612ms step_avg:88.65ms
step:1339/1670 train_time:118702ms step_avg:88.65ms
step:1340/1670 train_time:118792ms step_avg:88.65ms
step:1341/1670 train_time:118881ms step_avg:88.65ms
step:1342/1670 train_time:118971ms step_avg:88.65ms
step:1343/1670 train_time:119062ms step_avg:88.65ms
step:1344/1670 train_time:119152ms step_avg:88.65ms
step:1345/1670 train_time:119242ms step_avg:88.66ms
step:1346/1670 train_time:119332ms step_avg:88.66ms
step:1347/1670 train_time:119421ms step_avg:88.66ms
step:1348/1670 train_time:119511ms step_avg:88.66ms
step:1349/1670 train_time:119600ms step_avg:88.66ms
step:1350/1670 train_time:119690ms step_avg:88.66ms
step:1351/1670 train_time:119779ms step_avg:88.66ms
step:1352/1670 train_time:119870ms step_avg:88.66ms
step:1353/1670 train_time:119961ms step_avg:88.66ms
step:1354/1670 train_time:120051ms step_avg:88.66ms
step:1355/1670 train_time:120140ms step_avg:88.66ms
step:1356/1670 train_time:120230ms step_avg:88.67ms
step:1357/1670 train_time:120319ms step_avg:88.67ms
step:1358/1670 train_time:120408ms step_avg:88.67ms
step:1359/1670 train_time:120497ms step_avg:88.67ms
step:1360/1670 train_time:120586ms step_avg:88.67ms
step:1361/1670 train_time:120675ms step_avg:88.67ms
step:1362/1670 train_time:120765ms step_avg:88.67ms
step:1363/1670 train_time:120855ms step_avg:88.67ms
step:1364/1670 train_time:120945ms step_avg:88.67ms
step:1365/1670 train_time:121035ms step_avg:88.67ms
step:1366/1670 train_time:121125ms step_avg:88.67ms
step:1367/1670 train_time:121216ms step_avg:88.67ms
step:1368/1670 train_time:121306ms step_avg:88.67ms
step:1369/1670 train_time:121396ms step_avg:88.67ms
step:1370/1670 train_time:121485ms step_avg:88.68ms
step:1371/1670 train_time:121574ms step_avg:88.68ms
step:1372/1670 train_time:121663ms step_avg:88.68ms
step:1373/1670 train_time:121753ms step_avg:88.68ms
step:1374/1670 train_time:121843ms step_avg:88.68ms
step:1375/1670 train_time:121935ms step_avg:88.68ms
step:1375/1670 val_loss:3.3421 train_time:122026ms step_avg:88.75ms
step:1376/1670 train_time:122046ms step_avg:88.70ms
step:1377/1670 train_time:122118ms step_avg:88.68ms
step:1378/1670 train_time:122209ms step_avg:88.69ms
step:1379/1670 train_time:122298ms step_avg:88.69ms
step:1380/1670 train_time:122386ms step_avg:88.69ms
step:1381/1670 train_time:122475ms step_avg:88.69ms
step:1382/1670 train_time:122563ms step_avg:88.69ms
step:1383/1670 train_time:122654ms step_avg:88.69ms
step:1384/1670 train_time:122743ms step_avg:88.69ms
step:1385/1670 train_time:122833ms step_avg:88.69ms
step:1386/1670 train_time:122923ms step_avg:88.69ms
step:1387/1670 train_time:123016ms step_avg:88.69ms
step:1388/1670 train_time:123108ms step_avg:88.69ms
step:1389/1670 train_time:123200ms step_avg:88.70ms
step:1390/1670 train_time:123290ms step_avg:88.70ms
step:1391/1670 train_time:123379ms step_avg:88.70ms
step:1392/1670 train_time:123468ms step_avg:88.70ms
step:1393/1670 train_time:123557ms step_avg:88.70ms
step:1394/1670 train_time:123645ms step_avg:88.70ms
step:1395/1670 train_time:123735ms step_avg:88.70ms
step:1396/1670 train_time:123824ms step_avg:88.70ms
step:1397/1670 train_time:123914ms step_avg:88.70ms
step:1398/1670 train_time:124005ms step_avg:88.70ms
step:1399/1670 train_time:124096ms step_avg:88.70ms
step:1400/1670 train_time:124187ms step_avg:88.71ms
step:1401/1670 train_time:124277ms step_avg:88.71ms
step:1402/1670 train_time:124366ms step_avg:88.71ms
step:1403/1670 train_time:124456ms step_avg:88.71ms
step:1404/1670 train_time:124546ms step_avg:88.71ms
step:1405/1670 train_time:124635ms step_avg:88.71ms
step:1406/1670 train_time:124723ms step_avg:88.71ms
step:1407/1670 train_time:124814ms step_avg:88.71ms
step:1408/1670 train_time:124903ms step_avg:88.71ms
step:1409/1670 train_time:124995ms step_avg:88.71ms
step:1410/1670 train_time:125085ms step_avg:88.71ms
step:1411/1670 train_time:125175ms step_avg:88.71ms
step:1412/1670 train_time:125265ms step_avg:88.71ms
step:1413/1670 train_time:125356ms step_avg:88.72ms
step:1414/1670 train_time:125445ms step_avg:88.72ms
step:1415/1670 train_time:125535ms step_avg:88.72ms
step:1416/1670 train_time:125624ms step_avg:88.72ms
step:1417/1670 train_time:125714ms step_avg:88.72ms
step:1418/1670 train_time:125803ms step_avg:88.72ms
step:1419/1670 train_time:125894ms step_avg:88.72ms
step:1420/1670 train_time:125983ms step_avg:88.72ms
step:1421/1670 train_time:126073ms step_avg:88.72ms
step:1422/1670 train_time:126164ms step_avg:88.72ms
step:1423/1670 train_time:126255ms step_avg:88.72ms
step:1424/1670 train_time:126344ms step_avg:88.72ms
step:1425/1670 train_time:126434ms step_avg:88.73ms
step:1426/1670 train_time:126524ms step_avg:88.73ms
step:1427/1670 train_time:126614ms step_avg:88.73ms
step:1428/1670 train_time:126703ms step_avg:88.73ms
step:1429/1670 train_time:126793ms step_avg:88.73ms
step:1430/1670 train_time:126882ms step_avg:88.73ms
step:1431/1670 train_time:126972ms step_avg:88.73ms
step:1432/1670 train_time:127062ms step_avg:88.73ms
step:1433/1670 train_time:127154ms step_avg:88.73ms
step:1434/1670 train_time:127244ms step_avg:88.73ms
step:1435/1670 train_time:127335ms step_avg:88.74ms
step:1436/1670 train_time:127425ms step_avg:88.74ms
step:1437/1670 train_time:127515ms step_avg:88.74ms
step:1438/1670 train_time:127603ms step_avg:88.74ms
step:1439/1670 train_time:127692ms step_avg:88.74ms
step:1440/1670 train_time:127782ms step_avg:88.74ms
step:1441/1670 train_time:127871ms step_avg:88.74ms
step:1442/1670 train_time:127962ms step_avg:88.74ms
step:1443/1670 train_time:128051ms step_avg:88.74ms
step:1444/1670 train_time:128141ms step_avg:88.74ms
step:1445/1670 train_time:128232ms step_avg:88.74ms
step:1446/1670 train_time:128323ms step_avg:88.74ms
step:1447/1670 train_time:128414ms step_avg:88.74ms
step:1448/1670 train_time:128503ms step_avg:88.75ms
step:1449/1670 train_time:128593ms step_avg:88.75ms
step:1450/1670 train_time:128682ms step_avg:88.75ms
step:1451/1670 train_time:128772ms step_avg:88.75ms
step:1452/1670 train_time:128862ms step_avg:88.75ms
step:1453/1670 train_time:128952ms step_avg:88.75ms
step:1454/1670 train_time:129042ms step_avg:88.75ms
step:1455/1670 train_time:129133ms step_avg:88.75ms
step:1456/1670 train_time:129225ms step_avg:88.75ms
step:1457/1670 train_time:129316ms step_avg:88.75ms
step:1458/1670 train_time:129405ms step_avg:88.76ms
step:1459/1670 train_time:129495ms step_avg:88.76ms
step:1460/1670 train_time:129585ms step_avg:88.76ms
step:1461/1670 train_time:129674ms step_avg:88.76ms
step:1462/1670 train_time:129764ms step_avg:88.76ms
step:1463/1670 train_time:129854ms step_avg:88.76ms
step:1464/1670 train_time:129944ms step_avg:88.76ms
step:1465/1670 train_time:130034ms step_avg:88.76ms
step:1466/1670 train_time:130124ms step_avg:88.76ms
step:1467/1670 train_time:130215ms step_avg:88.76ms
step:1468/1670 train_time:130305ms step_avg:88.76ms
step:1469/1670 train_time:130395ms step_avg:88.76ms
step:1470/1670 train_time:130484ms step_avg:88.76ms
step:1471/1670 train_time:130573ms step_avg:88.77ms
step:1472/1670 train_time:130664ms step_avg:88.77ms
step:1473/1670 train_time:130754ms step_avg:88.77ms
step:1474/1670 train_time:130844ms step_avg:88.77ms
step:1475/1670 train_time:130934ms step_avg:88.77ms
step:1476/1670 train_time:131024ms step_avg:88.77ms
step:1477/1670 train_time:131114ms step_avg:88.77ms
step:1478/1670 train_time:131204ms step_avg:88.77ms
step:1479/1670 train_time:131295ms step_avg:88.77ms
step:1480/1670 train_time:131384ms step_avg:88.77ms
step:1481/1670 train_time:131473ms step_avg:88.77ms
step:1482/1670 train_time:131564ms step_avg:88.77ms
step:1483/1670 train_time:131653ms step_avg:88.78ms
step:1484/1670 train_time:131743ms step_avg:88.78ms
step:1485/1670 train_time:131833ms step_avg:88.78ms
step:1486/1670 train_time:131923ms step_avg:88.78ms
step:1487/1670 train_time:132013ms step_avg:88.78ms
step:1488/1670 train_time:132103ms step_avg:88.78ms
step:1489/1670 train_time:132194ms step_avg:88.78ms
step:1490/1670 train_time:132285ms step_avg:88.78ms
step:1491/1670 train_time:132374ms step_avg:88.78ms
step:1492/1670 train_time:132463ms step_avg:88.78ms
step:1493/1670 train_time:132554ms step_avg:88.78ms
step:1494/1670 train_time:132643ms step_avg:88.78ms
step:1495/1670 train_time:132733ms step_avg:88.78ms
step:1496/1670 train_time:132823ms step_avg:88.79ms
step:1497/1670 train_time:132913ms step_avg:88.79ms
step:1498/1670 train_time:133002ms step_avg:88.79ms
step:1499/1670 train_time:133092ms step_avg:88.79ms
step:1500/1670 train_time:133182ms step_avg:88.79ms
step:1500/1670 val_loss:3.3124 train_time:133273ms step_avg:88.85ms
step:1501/1670 train_time:133292ms step_avg:88.80ms
step:1502/1670 train_time:133367ms step_avg:88.79ms
step:1503/1670 train_time:133459ms step_avg:88.80ms
step:1504/1670 train_time:133550ms step_avg:88.80ms
step:1505/1670 train_time:133639ms step_avg:88.80ms
step:1506/1670 train_time:133728ms step_avg:88.80ms
step:1507/1670 train_time:133816ms step_avg:88.80ms
step:1508/1670 train_time:133904ms step_avg:88.80ms
step:1509/1670 train_time:133993ms step_avg:88.80ms
step:1510/1670 train_time:134082ms step_avg:88.80ms
step:1511/1670 train_time:134171ms step_avg:88.80ms
step:1512/1670 train_time:134263ms step_avg:88.80ms
step:1513/1670 train_time:134356ms step_avg:88.80ms
step:1514/1670 train_time:134450ms step_avg:88.80ms
step:1515/1670 train_time:134539ms step_avg:88.80ms
step:1516/1670 train_time:134629ms step_avg:88.81ms
step:1517/1670 train_time:134719ms step_avg:88.81ms
step:1518/1670 train_time:134808ms step_avg:88.81ms
step:1519/1670 train_time:134896ms step_avg:88.81ms
step:1520/1670 train_time:134985ms step_avg:88.81ms
step:1521/1670 train_time:135074ms step_avg:88.81ms
step:1522/1670 train_time:135164ms step_avg:88.81ms
step:1523/1670 train_time:135256ms step_avg:88.81ms
step:1524/1670 train_time:135348ms step_avg:88.81ms
step:1525/1670 train_time:135439ms step_avg:88.81ms
step:1526/1670 train_time:135530ms step_avg:88.81ms
step:1527/1670 train_time:135620ms step_avg:88.81ms
step:1528/1670 train_time:135710ms step_avg:88.82ms
step:1529/1670 train_time:135799ms step_avg:88.82ms
step:1530/1670 train_time:135888ms step_avg:88.82ms
step:1531/1670 train_time:135977ms step_avg:88.82ms
step:1532/1670 train_time:136066ms step_avg:88.82ms
step:1533/1670 train_time:136156ms step_avg:88.82ms
step:1534/1670 train_time:136246ms step_avg:88.82ms
step:1535/1670 train_time:136336ms step_avg:88.82ms
step:1536/1670 train_time:136427ms step_avg:88.82ms
step:1537/1670 train_time:136517ms step_avg:88.82ms
step:1538/1670 train_time:136607ms step_avg:88.82ms
step:1539/1670 train_time:136696ms step_avg:88.82ms
step:1540/1670 train_time:136786ms step_avg:88.82ms
step:1541/1670 train_time:136875ms step_avg:88.82ms
step:1542/1670 train_time:136965ms step_avg:88.82ms
step:1543/1670 train_time:137053ms step_avg:88.82ms
step:1544/1670 train_time:137143ms step_avg:88.82ms
step:1545/1670 train_time:137233ms step_avg:88.82ms
step:1546/1670 train_time:137323ms step_avg:88.82ms
step:1547/1670 train_time:137414ms step_avg:88.83ms
step:1548/1670 train_time:137505ms step_avg:88.83ms
step:1549/1670 train_time:137594ms step_avg:88.83ms
step:1550/1670 train_time:137685ms step_avg:88.83ms
step:1551/1670 train_time:137774ms step_avg:88.83ms
step:1552/1670 train_time:137864ms step_avg:88.83ms
step:1553/1670 train_time:137953ms step_avg:88.83ms
step:1554/1670 train_time:138043ms step_avg:88.83ms
step:1555/1670 train_time:138132ms step_avg:88.83ms
step:1556/1670 train_time:138222ms step_avg:88.83ms
step:1557/1670 train_time:138312ms step_avg:88.83ms
step:1558/1670 train_time:138403ms step_avg:88.83ms
step:1559/1670 train_time:138493ms step_avg:88.83ms
step:1560/1670 train_time:138584ms step_avg:88.84ms
step:1561/1670 train_time:138673ms step_avg:88.84ms
step:1562/1670 train_time:138763ms step_avg:88.84ms
step:1563/1670 train_time:138853ms step_avg:88.84ms
step:1564/1670 train_time:138942ms step_avg:88.84ms
step:1565/1670 train_time:139032ms step_avg:88.84ms
step:1566/1670 train_time:139121ms step_avg:88.84ms
step:1567/1670 train_time:139212ms step_avg:88.84ms
step:1568/1670 train_time:139302ms step_avg:88.84ms
step:1569/1670 train_time:139392ms step_avg:88.84ms
step:1570/1670 train_time:139484ms step_avg:88.84ms
step:1571/1670 train_time:139573ms step_avg:88.84ms
step:1572/1670 train_time:139663ms step_avg:88.84ms
step:1573/1670 train_time:139752ms step_avg:88.84ms
step:1574/1670 train_time:139843ms step_avg:88.85ms
step:1575/1670 train_time:139932ms step_avg:88.85ms
step:1576/1670 train_time:140022ms step_avg:88.85ms
step:1577/1670 train_time:140112ms step_avg:88.85ms
step:1578/1670 train_time:140201ms step_avg:88.85ms
step:1579/1670 train_time:140292ms step_avg:88.85ms
step:1580/1670 train_time:140382ms step_avg:88.85ms
step:1581/1670 train_time:140473ms step_avg:88.85ms
step:1582/1670 train_time:140564ms step_avg:88.85ms
step:1583/1670 train_time:140654ms step_avg:88.85ms
step:1584/1670 train_time:140744ms step_avg:88.85ms
step:1585/1670 train_time:140833ms step_avg:88.85ms
step:1586/1670 train_time:140923ms step_avg:88.85ms
step:1587/1670 train_time:141011ms step_avg:88.85ms
step:1588/1670 train_time:141101ms step_avg:88.85ms
step:1589/1670 train_time:141191ms step_avg:88.86ms
step:1590/1670 train_time:141280ms step_avg:88.86ms
step:1591/1670 train_time:141371ms step_avg:88.86ms
step:1592/1670 train_time:141462ms step_avg:88.86ms
step:1593/1670 train_time:141552ms step_avg:88.86ms
step:1594/1670 train_time:141642ms step_avg:88.86ms
step:1595/1670 train_time:141732ms step_avg:88.86ms
step:1596/1670 train_time:141822ms step_avg:88.86ms
step:1597/1670 train_time:141912ms step_avg:88.86ms
step:1598/1670 train_time:142001ms step_avg:88.86ms
step:1599/1670 train_time:142091ms step_avg:88.86ms
step:1600/1670 train_time:142180ms step_avg:88.86ms
step:1601/1670 train_time:142271ms step_avg:88.86ms
step:1602/1670 train_time:142361ms step_avg:88.86ms
step:1603/1670 train_time:142451ms step_avg:88.87ms
step:1604/1670 train_time:142541ms step_avg:88.87ms
step:1605/1670 train_time:142631ms step_avg:88.87ms
step:1606/1670 train_time:142721ms step_avg:88.87ms
step:1607/1670 train_time:142811ms step_avg:88.87ms
step:1608/1670 train_time:142901ms step_avg:88.87ms
step:1609/1670 train_time:142991ms step_avg:88.87ms
step:1610/1670 train_time:143081ms step_avg:88.87ms
step:1611/1670 train_time:143171ms step_avg:88.87ms
step:1612/1670 train_time:143261ms step_avg:88.87ms
step:1613/1670 train_time:143351ms step_avg:88.87ms
step:1614/1670 train_time:143442ms step_avg:88.87ms
step:1615/1670 train_time:143531ms step_avg:88.87ms
step:1616/1670 train_time:143622ms step_avg:88.87ms
step:1617/1670 train_time:143712ms step_avg:88.88ms
step:1618/1670 train_time:143802ms step_avg:88.88ms
step:1619/1670 train_time:143892ms step_avg:88.88ms
step:1620/1670 train_time:143981ms step_avg:88.88ms
step:1621/1670 train_time:144072ms step_avg:88.88ms
step:1622/1670 train_time:144161ms step_avg:88.88ms
step:1623/1670 train_time:144251ms step_avg:88.88ms
step:1624/1670 train_time:144342ms step_avg:88.88ms
step:1625/1670 train_time:144431ms step_avg:88.88ms
step:1625/1670 val_loss:3.2891 train_time:144522ms step_avg:88.94ms
step:1626/1670 train_time:144542ms step_avg:88.89ms
step:1627/1670 train_time:144617ms step_avg:88.89ms
step:1628/1670 train_time:144711ms step_avg:88.89ms
step:1629/1670 train_time:144801ms step_avg:88.89ms
step:1630/1670 train_time:144890ms step_avg:88.89ms
step:1631/1670 train_time:144979ms step_avg:88.89ms
step:1632/1670 train_time:145067ms step_avg:88.89ms
step:1633/1670 train_time:145156ms step_avg:88.89ms
step:1634/1670 train_time:145245ms step_avg:88.89ms
step:1635/1670 train_time:145335ms step_avg:88.89ms
step:1636/1670 train_time:145423ms step_avg:88.89ms
step:1637/1670 train_time:145515ms step_avg:88.89ms
step:1638/1670 train_time:145607ms step_avg:88.89ms
step:1639/1670 train_time:145699ms step_avg:88.89ms
step:1640/1670 train_time:145790ms step_avg:88.90ms
step:1641/1670 train_time:145880ms step_avg:88.90ms
step:1642/1670 train_time:145969ms step_avg:88.90ms
step:1643/1670 train_time:146058ms step_avg:88.90ms
step:1644/1670 train_time:146147ms step_avg:88.90ms
step:1645/1670 train_time:146236ms step_avg:88.90ms
step:1646/1670 train_time:146325ms step_avg:88.90ms
step:1647/1670 train_time:146415ms step_avg:88.90ms
step:1648/1670 train_time:146505ms step_avg:88.90ms
step:1649/1670 train_time:146597ms step_avg:88.90ms
step:1650/1670 train_time:146688ms step_avg:88.90ms
step:1651/1670 train_time:146780ms step_avg:88.90ms
step:1652/1670 train_time:146871ms step_avg:88.90ms
step:1653/1670 train_time:146960ms step_avg:88.90ms
step:1654/1670 train_time:147049ms step_avg:88.91ms
step:1655/1670 train_time:147138ms step_avg:88.91ms
step:1656/1670 train_time:147227ms step_avg:88.91ms
step:1657/1670 train_time:147316ms step_avg:88.91ms
step:1658/1670 train_time:147405ms step_avg:88.91ms
step:1659/1670 train_time:147496ms step_avg:88.91ms
step:1660/1670 train_time:147586ms step_avg:88.91ms
step:1661/1670 train_time:147679ms step_avg:88.91ms
step:1662/1670 train_time:147770ms step_avg:88.91ms
step:1663/1670 train_time:147860ms step_avg:88.91ms
step:1664/1670 train_time:147950ms step_avg:88.91ms
step:1665/1670 train_time:148039ms step_avg:88.91ms
step:1666/1670 train_time:148128ms step_avg:88.91ms
step:1667/1670 train_time:148217ms step_avg:88.91ms
step:1668/1670 train_time:148306ms step_avg:88.91ms
step:1669/1670 train_time:148395ms step_avg:88.91ms
step:1670/1670 train_time:148485ms step_avg:88.91ms
step:1670/1670 val_loss:3.2797 train_time:148578ms step_avg:88.97ms
peak memory allocated: 30760 MiB reserved: 45694 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 10 14:05:58 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                  Off |
| N/A   32C    P0            150W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                  Off |
| N/A   28C    P0            139W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                  Off |
| N/A   24C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                  Off |
| N/A   29C    P0            134W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                  Off |
| N/A   30C    P0            138W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                  Off |
| N/A   26C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                  Off |
| N/A   30C    P0            140W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                  Off |
| N/A   25C    P0            136W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     77636      C   /usr/bin/python3                             1510MiB |
|    0   N/A  N/A     77637      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     77638      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     77639      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     77640      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     77641      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     77642      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     77643      C   /usr/bin/python3                              614MiB |
|    1   N/A  N/A     77637      C   /usr/bin/python3                             1510MiB |
|    2   N/A  N/A     77638      C   /usr/bin/python3                             1510MiB |
|    3   N/A  N/A     77639      C   /usr/bin/python3                             1510MiB |
|    4   N/A  N/A     77640      C   /usr/bin/python3                             1510MiB |
|    5   N/A  N/A     77641      C   /usr/bin/python3                             1510MiB |
|    6   N/A  N/A     77642      C   /usr/bin/python3                             1510MiB |
|    7   N/A  N/A     77643      C   /usr/bin/python3                             1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2160 train_time:108ms step_avg:107.51ms
step:2/2160 train_time:133ms step_avg:66.29ms
step:3/2160 train_time:154ms step_avg:51.27ms
step:4/2160 train_time:176ms step_avg:43.96ms
step:5/2160 train_time:208ms step_avg:41.53ms
step:6/2160 train_time:269ms step_avg:44.89ms
step:7/2160 train_time:296ms step_avg:42.35ms
step:8/2160 train_time:330ms step_avg:41.19ms
step:9/2160 train_time:363ms step_avg:40.34ms
step:10/2160 train_time:396ms step_avg:39.61ms
step:11/2160 train_time:430ms step_avg:39.07ms
step:12/2160 train_time:463ms step_avg:38.56ms
step:13/2160 train_time:497ms step_avg:38.21ms
step:14/2160 train_time:530ms step_avg:37.85ms
step:15/2160 train_time:564ms step_avg:37.58ms
step:16/2160 train_time:597ms step_avg:37.29ms
step:17/2160 train_time:630ms step_avg:37.09ms
step:18/2160 train_time:663ms step_avg:36.86ms
step:19/2160 train_time:697ms step_avg:36.69ms
step:20/2160 train_time:730ms step_avg:36.51ms
step:21/2160 train_time:764ms step_avg:36.39ms
step:22/2160 train_time:797ms step_avg:36.23ms
step:23/2160 train_time:831ms step_avg:36.12ms
step:24/2160 train_time:864ms step_avg:35.99ms
step:25/2160 train_time:897ms step_avg:35.90ms
step:26/2160 train_time:930ms step_avg:35.79ms
step:27/2160 train_time:964ms step_avg:35.71ms
step:28/2160 train_time:997ms step_avg:35.62ms
step:29/2160 train_time:1031ms step_avg:35.56ms
step:30/2160 train_time:1064ms step_avg:35.47ms
step:31/2160 train_time:1099ms step_avg:35.44ms
step:32/2160 train_time:1132ms step_avg:35.37ms
step:33/2160 train_time:1165ms step_avg:35.32ms
step:34/2160 train_time:1198ms step_avg:35.25ms
step:35/2160 train_time:1232ms step_avg:35.20ms
step:36/2160 train_time:1265ms step_avg:35.14ms
step:37/2160 train_time:1299ms step_avg:35.12ms
step:38/2160 train_time:1332ms step_avg:35.06ms
step:39/2160 train_time:1367ms step_avg:35.04ms
step:40/2160 train_time:1399ms step_avg:34.99ms
step:41/2160 train_time:1433ms step_avg:34.96ms
step:42/2160 train_time:1466ms step_avg:34.92ms
step:43/2160 train_time:1500ms step_avg:34.89ms
step:44/2160 train_time:1533ms step_avg:34.85ms
step:45/2160 train_time:1567ms step_avg:34.83ms
step:46/2160 train_time:1600ms step_avg:34.79ms
step:47/2160 train_time:1634ms step_avg:34.76ms
step:48/2160 train_time:1667ms step_avg:34.73ms
step:49/2160 train_time:1701ms step_avg:34.71ms
step:50/2160 train_time:1734ms step_avg:34.67ms
step:51/2160 train_time:1767ms step_avg:34.65ms
step:52/2160 train_time:1800ms step_avg:34.62ms
step:53/2160 train_time:1834ms step_avg:34.61ms
step:54/2160 train_time:1867ms step_avg:34.58ms
step:55/2160 train_time:1901ms step_avg:34.57ms
step:56/2160 train_time:1934ms step_avg:34.54ms
step:57/2160 train_time:1968ms step_avg:34.52ms
step:58/2160 train_time:2001ms step_avg:34.50ms
step:59/2160 train_time:2035ms step_avg:34.49ms
step:60/2160 train_time:2068ms step_avg:34.46ms
step:61/2160 train_time:2101ms step_avg:34.45ms
step:62/2160 train_time:2134ms step_avg:34.43ms
step:63/2160 train_time:2169ms step_avg:34.42ms
step:64/2160 train_time:2202ms step_avg:34.40ms
step:65/2160 train_time:2235ms step_avg:34.39ms
step:66/2160 train_time:2269ms step_avg:34.37ms
step:67/2160 train_time:2302ms step_avg:34.36ms
step:68/2160 train_time:2335ms step_avg:34.34ms
step:69/2160 train_time:2369ms step_avg:34.33ms
step:70/2160 train_time:2402ms step_avg:34.31ms
step:71/2160 train_time:2437ms step_avg:34.32ms
step:72/2160 train_time:2469ms step_avg:34.30ms
step:73/2160 train_time:2503ms step_avg:34.29ms
step:74/2160 train_time:2536ms step_avg:34.28ms
step:75/2160 train_time:2570ms step_avg:34.27ms
step:76/2160 train_time:2603ms step_avg:34.25ms
step:77/2160 train_time:2637ms step_avg:34.25ms
step:78/2160 train_time:2670ms step_avg:34.23ms
step:79/2160 train_time:2704ms step_avg:34.23ms
step:80/2160 train_time:2737ms step_avg:34.22ms
step:81/2160 train_time:2771ms step_avg:34.21ms
step:82/2160 train_time:2804ms step_avg:34.20ms
step:83/2160 train_time:2838ms step_avg:34.20ms
step:84/2160 train_time:2871ms step_avg:34.18ms
step:85/2160 train_time:2905ms step_avg:34.18ms
step:86/2160 train_time:2938ms step_avg:34.17ms
step:87/2160 train_time:2972ms step_avg:34.16ms
step:88/2160 train_time:3005ms step_avg:34.15ms
step:89/2160 train_time:3039ms step_avg:34.15ms
step:90/2160 train_time:3072ms step_avg:34.14ms
step:91/2160 train_time:3106ms step_avg:34.13ms
step:92/2160 train_time:3139ms step_avg:34.12ms
step:93/2160 train_time:3173ms step_avg:34.12ms
step:94/2160 train_time:3206ms step_avg:34.11ms
step:95/2160 train_time:3240ms step_avg:34.10ms
step:96/2160 train_time:3273ms step_avg:34.09ms
step:97/2160 train_time:3306ms step_avg:34.09ms
step:98/2160 train_time:3339ms step_avg:34.07ms
step:99/2160 train_time:3373ms step_avg:34.07ms
step:100/2160 train_time:3406ms step_avg:34.06ms
step:101/2160 train_time:3439ms step_avg:34.05ms
step:102/2160 train_time:3472ms step_avg:34.04ms
step:103/2160 train_time:3506ms step_avg:34.04ms
step:104/2160 train_time:3539ms step_avg:34.03ms
step:105/2160 train_time:3573ms step_avg:34.03ms
step:106/2160 train_time:3606ms step_avg:34.01ms
step:107/2160 train_time:3640ms step_avg:34.02ms
step:108/2160 train_time:3673ms step_avg:34.01ms
step:109/2160 train_time:3706ms step_avg:34.00ms
step:110/2160 train_time:3739ms step_avg:33.99ms
step:111/2160 train_time:3773ms step_avg:33.99ms
step:112/2160 train_time:3806ms step_avg:33.98ms
step:113/2160 train_time:3840ms step_avg:33.98ms
step:114/2160 train_time:3873ms step_avg:33.97ms
step:115/2160 train_time:3906ms step_avg:33.97ms
step:116/2160 train_time:3939ms step_avg:33.96ms
step:117/2160 train_time:3973ms step_avg:33.96ms
step:118/2160 train_time:4007ms step_avg:33.95ms
step:119/2160 train_time:4040ms step_avg:33.95ms
step:120/2160 train_time:4073ms step_avg:33.94ms
step:121/2160 train_time:4106ms step_avg:33.94ms
step:122/2160 train_time:4139ms step_avg:33.93ms
step:123/2160 train_time:4173ms step_avg:33.93ms
step:124/2160 train_time:4206ms step_avg:33.92ms
step:125/2160 train_time:4240ms step_avg:33.92ms
step:126/2160 train_time:4273ms step_avg:33.91ms
step:127/2160 train_time:4306ms step_avg:33.91ms
step:128/2160 train_time:4339ms step_avg:33.90ms
step:129/2160 train_time:4373ms step_avg:33.90ms
step:130/2160 train_time:4406ms step_avg:33.89ms
step:131/2160 train_time:4440ms step_avg:33.89ms
step:132/2160 train_time:4473ms step_avg:33.88ms
step:133/2160 train_time:4506ms step_avg:33.88ms
step:134/2160 train_time:4539ms step_avg:33.88ms
step:135/2160 train_time:4573ms step_avg:33.87ms
step:136/2160 train_time:4606ms step_avg:33.87ms
step:137/2160 train_time:4639ms step_avg:33.86ms
step:138/2160 train_time:4672ms step_avg:33.86ms
step:139/2160 train_time:4706ms step_avg:33.86ms
step:140/2160 train_time:4739ms step_avg:33.85ms
step:141/2160 train_time:4773ms step_avg:33.85ms
step:142/2160 train_time:4806ms step_avg:33.84ms
step:143/2160 train_time:4840ms step_avg:33.84ms
step:144/2160 train_time:4872ms step_avg:33.84ms
step:145/2160 train_time:4906ms step_avg:33.84ms
step:146/2160 train_time:4939ms step_avg:33.83ms
step:147/2160 train_time:4973ms step_avg:33.83ms
step:148/2160 train_time:5006ms step_avg:33.82ms
step:149/2160 train_time:5040ms step_avg:33.82ms
step:150/2160 train_time:5073ms step_avg:33.82ms
step:151/2160 train_time:5106ms step_avg:33.82ms
step:152/2160 train_time:5139ms step_avg:33.81ms
step:153/2160 train_time:5173ms step_avg:33.81ms
step:154/2160 train_time:5205ms step_avg:33.80ms
step:155/2160 train_time:5239ms step_avg:33.80ms
step:156/2160 train_time:5272ms step_avg:33.80ms
step:157/2160 train_time:5306ms step_avg:33.79ms
step:158/2160 train_time:5339ms step_avg:33.79ms
step:159/2160 train_time:5373ms step_avg:33.79ms
step:160/2160 train_time:5405ms step_avg:33.78ms
step:161/2160 train_time:5439ms step_avg:33.78ms
step:162/2160 train_time:5472ms step_avg:33.78ms
step:163/2160 train_time:5505ms step_avg:33.77ms
step:164/2160 train_time:5539ms step_avg:33.77ms
step:165/2160 train_time:5572ms step_avg:33.77ms
step:166/2160 train_time:5605ms step_avg:33.76ms
step:167/2160 train_time:5639ms step_avg:33.76ms
step:168/2160 train_time:5672ms step_avg:33.76ms
step:169/2160 train_time:5705ms step_avg:33.76ms
step:170/2160 train_time:5738ms step_avg:33.75ms
step:171/2160 train_time:5772ms step_avg:33.75ms
step:172/2160 train_time:5805ms step_avg:33.75ms
step:173/2160 train_time:5838ms step_avg:33.75ms
step:174/2160 train_time:5871ms step_avg:33.74ms
step:175/2160 train_time:5905ms step_avg:33.74ms
step:176/2160 train_time:5938ms step_avg:33.74ms
step:177/2160 train_time:5971ms step_avg:33.73ms
step:178/2160 train_time:6004ms step_avg:33.73ms
step:179/2160 train_time:6037ms step_avg:33.73ms
step:180/2160 train_time:6070ms step_avg:33.72ms
step:181/2160 train_time:6104ms step_avg:33.72ms
step:182/2160 train_time:6137ms step_avg:33.72ms
step:183/2160 train_time:6171ms step_avg:33.72ms
step:184/2160 train_time:6203ms step_avg:33.71ms
step:185/2160 train_time:6237ms step_avg:33.71ms
step:186/2160 train_time:6270ms step_avg:33.71ms
step:187/2160 train_time:6304ms step_avg:33.71ms
step:188/2160 train_time:6337ms step_avg:33.71ms
step:189/2160 train_time:6370ms step_avg:33.70ms
step:190/2160 train_time:6403ms step_avg:33.70ms
step:191/2160 train_time:6437ms step_avg:33.70ms
step:192/2160 train_time:6470ms step_avg:33.70ms
step:193/2160 train_time:6504ms step_avg:33.70ms
step:194/2160 train_time:6537ms step_avg:33.69ms
step:195/2160 train_time:6570ms step_avg:33.69ms
step:196/2160 train_time:6603ms step_avg:33.69ms
step:197/2160 train_time:6637ms step_avg:33.69ms
step:198/2160 train_time:6669ms step_avg:33.68ms
step:199/2160 train_time:6703ms step_avg:33.68ms
step:200/2160 train_time:6736ms step_avg:33.68ms
step:201/2160 train_time:6770ms step_avg:33.68ms
step:202/2160 train_time:6803ms step_avg:33.68ms
step:203/2160 train_time:6837ms step_avg:33.68ms
step:204/2160 train_time:6870ms step_avg:33.68ms
step:205/2160 train_time:6903ms step_avg:33.68ms
step:206/2160 train_time:6938ms step_avg:33.68ms
step:207/2160 train_time:6970ms step_avg:33.67ms
step:208/2160 train_time:7003ms step_avg:33.67ms
step:209/2160 train_time:7037ms step_avg:33.67ms
step:210/2160 train_time:7070ms step_avg:33.66ms
step:211/2160 train_time:7103ms step_avg:33.67ms
step:212/2160 train_time:7136ms step_avg:33.66ms
step:213/2160 train_time:7170ms step_avg:33.66ms
step:214/2160 train_time:7203ms step_avg:33.66ms
step:215/2160 train_time:7236ms step_avg:33.66ms
step:216/2160 train_time:7269ms step_avg:33.65ms
step:217/2160 train_time:7303ms step_avg:33.65ms
step:218/2160 train_time:7336ms step_avg:33.65ms
step:219/2160 train_time:7369ms step_avg:33.65ms
step:220/2160 train_time:7402ms step_avg:33.65ms
step:221/2160 train_time:7436ms step_avg:33.65ms
step:222/2160 train_time:7469ms step_avg:33.65ms
step:223/2160 train_time:7503ms step_avg:33.64ms
step:224/2160 train_time:7536ms step_avg:33.64ms
step:225/2160 train_time:7569ms step_avg:33.64ms
step:226/2160 train_time:7602ms step_avg:33.64ms
step:227/2160 train_time:7636ms step_avg:33.64ms
step:228/2160 train_time:7669ms step_avg:33.64ms
step:229/2160 train_time:7703ms step_avg:33.64ms
step:230/2160 train_time:7736ms step_avg:33.63ms
step:231/2160 train_time:7770ms step_avg:33.63ms
step:232/2160 train_time:7803ms step_avg:33.63ms
step:233/2160 train_time:7837ms step_avg:33.63ms
step:234/2160 train_time:7870ms step_avg:33.63ms
step:235/2160 train_time:7904ms step_avg:33.63ms
step:236/2160 train_time:7937ms step_avg:33.63ms
step:237/2160 train_time:7971ms step_avg:33.63ms
step:238/2160 train_time:8003ms step_avg:33.63ms
step:239/2160 train_time:8038ms step_avg:33.63ms
step:240/2160 train_time:8071ms step_avg:33.63ms
step:241/2160 train_time:8104ms step_avg:33.63ms
step:242/2160 train_time:8137ms step_avg:33.63ms
step:243/2160 train_time:8171ms step_avg:33.63ms
step:244/2160 train_time:8205ms step_avg:33.63ms
step:245/2160 train_time:8238ms step_avg:33.62ms
step:246/2160 train_time:8271ms step_avg:33.62ms
step:247/2160 train_time:8304ms step_avg:33.62ms
step:248/2160 train_time:8337ms step_avg:33.62ms
step:249/2160 train_time:8371ms step_avg:33.62ms
step:250/2160 train_time:8404ms step_avg:33.61ms
step:250/2160 val_loss:4.3151 train_time:8439ms step_avg:33.76ms
step:251/2160 train_time:8460ms step_avg:33.71ms
step:252/2160 train_time:8481ms step_avg:33.66ms
step:253/2160 train_time:8511ms step_avg:33.64ms
step:254/2160 train_time:8544ms step_avg:33.64ms
step:255/2160 train_time:8579ms step_avg:33.64ms
step:256/2160 train_time:8613ms step_avg:33.64ms
step:257/2160 train_time:8648ms step_avg:33.65ms
step:258/2160 train_time:8681ms step_avg:33.65ms
step:259/2160 train_time:8715ms step_avg:33.65ms
step:260/2160 train_time:8749ms step_avg:33.65ms
step:261/2160 train_time:8783ms step_avg:33.65ms
step:262/2160 train_time:8816ms step_avg:33.65ms
step:263/2160 train_time:8849ms step_avg:33.65ms
step:264/2160 train_time:8882ms step_avg:33.64ms
step:265/2160 train_time:8916ms step_avg:33.64ms
step:266/2160 train_time:8949ms step_avg:33.64ms
step:267/2160 train_time:8982ms step_avg:33.64ms
step:268/2160 train_time:9015ms step_avg:33.64ms
step:269/2160 train_time:9049ms step_avg:33.64ms
step:270/2160 train_time:9081ms step_avg:33.63ms
step:271/2160 train_time:9115ms step_avg:33.63ms
step:272/2160 train_time:9148ms step_avg:33.63ms
step:273/2160 train_time:9182ms step_avg:33.63ms
step:274/2160 train_time:9214ms step_avg:33.63ms
step:275/2160 train_time:9248ms step_avg:33.63ms
step:276/2160 train_time:9281ms step_avg:33.63ms
step:277/2160 train_time:9314ms step_avg:33.63ms
step:278/2160 train_time:9347ms step_avg:33.62ms
step:279/2160 train_time:9381ms step_avg:33.62ms
step:280/2160 train_time:9414ms step_avg:33.62ms
step:281/2160 train_time:9447ms step_avg:33.62ms
step:282/2160 train_time:9480ms step_avg:33.62ms
step:283/2160 train_time:9514ms step_avg:33.62ms
step:284/2160 train_time:9547ms step_avg:33.62ms
step:285/2160 train_time:9580ms step_avg:33.62ms
step:286/2160 train_time:9613ms step_avg:33.61ms
step:287/2160 train_time:9647ms step_avg:33.61ms
step:288/2160 train_time:9680ms step_avg:33.61ms
step:289/2160 train_time:9714ms step_avg:33.61ms
step:290/2160 train_time:9746ms step_avg:33.61ms
step:291/2160 train_time:9780ms step_avg:33.61ms
step:292/2160 train_time:9813ms step_avg:33.61ms
step:293/2160 train_time:9847ms step_avg:33.61ms
step:294/2160 train_time:9879ms step_avg:33.60ms
step:295/2160 train_time:9913ms step_avg:33.60ms
step:296/2160 train_time:9946ms step_avg:33.60ms
step:297/2160 train_time:9979ms step_avg:33.60ms
step:298/2160 train_time:10012ms step_avg:33.60ms
step:299/2160 train_time:10046ms step_avg:33.60ms
step:300/2160 train_time:10079ms step_avg:33.60ms
step:301/2160 train_time:10112ms step_avg:33.60ms
step:302/2160 train_time:10145ms step_avg:33.59ms
step:303/2160 train_time:10179ms step_avg:33.59ms
step:304/2160 train_time:10212ms step_avg:33.59ms
step:305/2160 train_time:10246ms step_avg:33.59ms
step:306/2160 train_time:10279ms step_avg:33.59ms
step:307/2160 train_time:10312ms step_avg:33.59ms
step:308/2160 train_time:10345ms step_avg:33.59ms
step:309/2160 train_time:10379ms step_avg:33.59ms
step:310/2160 train_time:10412ms step_avg:33.59ms
step:311/2160 train_time:10445ms step_avg:33.59ms
step:312/2160 train_time:10478ms step_avg:33.58ms
step:313/2160 train_time:10512ms step_avg:33.58ms
step:314/2160 train_time:10545ms step_avg:33.58ms
step:315/2160 train_time:10579ms step_avg:33.58ms
step:316/2160 train_time:10612ms step_avg:33.58ms
step:317/2160 train_time:10645ms step_avg:33.58ms
step:318/2160 train_time:10678ms step_avg:33.58ms
step:319/2160 train_time:10712ms step_avg:33.58ms
step:320/2160 train_time:10745ms step_avg:33.58ms
step:321/2160 train_time:10779ms step_avg:33.58ms
step:322/2160 train_time:10812ms step_avg:33.58ms
step:323/2160 train_time:10845ms step_avg:33.58ms
step:324/2160 train_time:10878ms step_avg:33.58ms
step:325/2160 train_time:10912ms step_avg:33.57ms
step:326/2160 train_time:10945ms step_avg:33.57ms
step:327/2160 train_time:10978ms step_avg:33.57ms
step:328/2160 train_time:11011ms step_avg:33.57ms
step:329/2160 train_time:11045ms step_avg:33.57ms
step:330/2160 train_time:11078ms step_avg:33.57ms
step:331/2160 train_time:11111ms step_avg:33.57ms
step:332/2160 train_time:11144ms step_avg:33.57ms
step:333/2160 train_time:11178ms step_avg:33.57ms
step:334/2160 train_time:11211ms step_avg:33.57ms
step:335/2160 train_time:11244ms step_avg:33.56ms
step:336/2160 train_time:11277ms step_avg:33.56ms
step:337/2160 train_time:11311ms step_avg:33.56ms
step:338/2160 train_time:11343ms step_avg:33.56ms
step:339/2160 train_time:11377ms step_avg:33.56ms
step:340/2160 train_time:11410ms step_avg:33.56ms
step:341/2160 train_time:11444ms step_avg:33.56ms
step:342/2160 train_time:11477ms step_avg:33.56ms
step:343/2160 train_time:11510ms step_avg:33.56ms
step:344/2160 train_time:11543ms step_avg:33.56ms
step:345/2160 train_time:11576ms step_avg:33.55ms
step:346/2160 train_time:11609ms step_avg:33.55ms
step:347/2160 train_time:11643ms step_avg:33.55ms
step:348/2160 train_time:11676ms step_avg:33.55ms
step:349/2160 train_time:11709ms step_avg:33.55ms
step:350/2160 train_time:11742ms step_avg:33.55ms
step:351/2160 train_time:11776ms step_avg:33.55ms
step:352/2160 train_time:11809ms step_avg:33.55ms
step:353/2160 train_time:11843ms step_avg:33.55ms
step:354/2160 train_time:11876ms step_avg:33.55ms
step:355/2160 train_time:11909ms step_avg:33.55ms
step:356/2160 train_time:11942ms step_avg:33.54ms
step:357/2160 train_time:11976ms step_avg:33.54ms
step:358/2160 train_time:12009ms step_avg:33.54ms
step:359/2160 train_time:12042ms step_avg:33.54ms
step:360/2160 train_time:12075ms step_avg:33.54ms
step:361/2160 train_time:12109ms step_avg:33.54ms
step:362/2160 train_time:12142ms step_avg:33.54ms
step:363/2160 train_time:12175ms step_avg:33.54ms
step:364/2160 train_time:12208ms step_avg:33.54ms
step:365/2160 train_time:12241ms step_avg:33.54ms
step:366/2160 train_time:12274ms step_avg:33.54ms
step:367/2160 train_time:12308ms step_avg:33.54ms
step:368/2160 train_time:12341ms step_avg:33.54ms
step:369/2160 train_time:12374ms step_avg:33.53ms
step:370/2160 train_time:12407ms step_avg:33.53ms
step:371/2160 train_time:12441ms step_avg:33.53ms
step:372/2160 train_time:12474ms step_avg:33.53ms
step:373/2160 train_time:12507ms step_avg:33.53ms
step:374/2160 train_time:12541ms step_avg:33.53ms
step:375/2160 train_time:12574ms step_avg:33.53ms
step:376/2160 train_time:12607ms step_avg:33.53ms
step:377/2160 train_time:12640ms step_avg:33.53ms
step:378/2160 train_time:12673ms step_avg:33.53ms
step:379/2160 train_time:12706ms step_avg:33.53ms
step:380/2160 train_time:12739ms step_avg:33.52ms
step:381/2160 train_time:12773ms step_avg:33.52ms
step:382/2160 train_time:12806ms step_avg:33.52ms
step:383/2160 train_time:12839ms step_avg:33.52ms
step:384/2160 train_time:12872ms step_avg:33.52ms
step:385/2160 train_time:12906ms step_avg:33.52ms
step:386/2160 train_time:12939ms step_avg:33.52ms
step:387/2160 train_time:12972ms step_avg:33.52ms
step:388/2160 train_time:13005ms step_avg:33.52ms
step:389/2160 train_time:13039ms step_avg:33.52ms
step:390/2160 train_time:13072ms step_avg:33.52ms
step:391/2160 train_time:13106ms step_avg:33.52ms
step:392/2160 train_time:13139ms step_avg:33.52ms
step:393/2160 train_time:13172ms step_avg:33.52ms
step:394/2160 train_time:13205ms step_avg:33.52ms
step:395/2160 train_time:13239ms step_avg:33.52ms
step:396/2160 train_time:13272ms step_avg:33.52ms
step:397/2160 train_time:13306ms step_avg:33.52ms
step:398/2160 train_time:13338ms step_avg:33.51ms
step:399/2160 train_time:13372ms step_avg:33.51ms
step:400/2160 train_time:13405ms step_avg:33.51ms
step:401/2160 train_time:13439ms step_avg:33.51ms
step:402/2160 train_time:13472ms step_avg:33.51ms
step:403/2160 train_time:13506ms step_avg:33.51ms
step:404/2160 train_time:13539ms step_avg:33.51ms
step:405/2160 train_time:13572ms step_avg:33.51ms
step:406/2160 train_time:13605ms step_avg:33.51ms
step:407/2160 train_time:13639ms step_avg:33.51ms
step:408/2160 train_time:13672ms step_avg:33.51ms
step:409/2160 train_time:13706ms step_avg:33.51ms
step:410/2160 train_time:13739ms step_avg:33.51ms
step:411/2160 train_time:13772ms step_avg:33.51ms
step:412/2160 train_time:13805ms step_avg:33.51ms
step:413/2160 train_time:13838ms step_avg:33.51ms
step:414/2160 train_time:13872ms step_avg:33.51ms
step:415/2160 train_time:13905ms step_avg:33.51ms
step:416/2160 train_time:13938ms step_avg:33.51ms
step:417/2160 train_time:13972ms step_avg:33.50ms
step:418/2160 train_time:14005ms step_avg:33.50ms
step:419/2160 train_time:14038ms step_avg:33.50ms
step:420/2160 train_time:14071ms step_avg:33.50ms
step:421/2160 train_time:14105ms step_avg:33.50ms
step:422/2160 train_time:14138ms step_avg:33.50ms
step:423/2160 train_time:14171ms step_avg:33.50ms
step:424/2160 train_time:14204ms step_avg:33.50ms
step:425/2160 train_time:14238ms step_avg:33.50ms
step:426/2160 train_time:14271ms step_avg:33.50ms
step:427/2160 train_time:14305ms step_avg:33.50ms
step:428/2160 train_time:14338ms step_avg:33.50ms
step:429/2160 train_time:14371ms step_avg:33.50ms
step:430/2160 train_time:14404ms step_avg:33.50ms
step:431/2160 train_time:14438ms step_avg:33.50ms
step:432/2160 train_time:14471ms step_avg:33.50ms
step:433/2160 train_time:14504ms step_avg:33.50ms
step:434/2160 train_time:14537ms step_avg:33.50ms
step:435/2160 train_time:14571ms step_avg:33.50ms
step:436/2160 train_time:14604ms step_avg:33.50ms
step:437/2160 train_time:14638ms step_avg:33.50ms
step:438/2160 train_time:14671ms step_avg:33.49ms
step:439/2160 train_time:14704ms step_avg:33.50ms
step:440/2160 train_time:14737ms step_avg:33.49ms
step:441/2160 train_time:14771ms step_avg:33.49ms
step:442/2160 train_time:14804ms step_avg:33.49ms
step:443/2160 train_time:14838ms step_avg:33.49ms
step:444/2160 train_time:14871ms step_avg:33.49ms
step:445/2160 train_time:14904ms step_avg:33.49ms
step:446/2160 train_time:14937ms step_avg:33.49ms
step:447/2160 train_time:14971ms step_avg:33.49ms
step:448/2160 train_time:15003ms step_avg:33.49ms
step:449/2160 train_time:15037ms step_avg:33.49ms
step:450/2160 train_time:15070ms step_avg:33.49ms
step:451/2160 train_time:15103ms step_avg:33.49ms
step:452/2160 train_time:15136ms step_avg:33.49ms
step:453/2160 train_time:15170ms step_avg:33.49ms
step:454/2160 train_time:15203ms step_avg:33.49ms
step:455/2160 train_time:15236ms step_avg:33.49ms
step:456/2160 train_time:15269ms step_avg:33.49ms
step:457/2160 train_time:15303ms step_avg:33.49ms
step:458/2160 train_time:15336ms step_avg:33.48ms
step:459/2160 train_time:15369ms step_avg:33.48ms
step:460/2160 train_time:15402ms step_avg:33.48ms
step:461/2160 train_time:15436ms step_avg:33.48ms
step:462/2160 train_time:15469ms step_avg:33.48ms
step:463/2160 train_time:15502ms step_avg:33.48ms
step:464/2160 train_time:15535ms step_avg:33.48ms
step:465/2160 train_time:15568ms step_avg:33.48ms
step:466/2160 train_time:15601ms step_avg:33.48ms
step:467/2160 train_time:15635ms step_avg:33.48ms
step:468/2160 train_time:15668ms step_avg:33.48ms
step:469/2160 train_time:15702ms step_avg:33.48ms
step:470/2160 train_time:15734ms step_avg:33.48ms
step:471/2160 train_time:15768ms step_avg:33.48ms
step:472/2160 train_time:15802ms step_avg:33.48ms
step:473/2160 train_time:15835ms step_avg:33.48ms
step:474/2160 train_time:15868ms step_avg:33.48ms
step:475/2160 train_time:15901ms step_avg:33.48ms
step:476/2160 train_time:15934ms step_avg:33.48ms
step:477/2160 train_time:15968ms step_avg:33.48ms
step:478/2160 train_time:16001ms step_avg:33.47ms
step:479/2160 train_time:16034ms step_avg:33.47ms
step:480/2160 train_time:16067ms step_avg:33.47ms
step:481/2160 train_time:16100ms step_avg:33.47ms
step:482/2160 train_time:16134ms step_avg:33.47ms
step:483/2160 train_time:16167ms step_avg:33.47ms
step:484/2160 train_time:16200ms step_avg:33.47ms
step:485/2160 train_time:16234ms step_avg:33.47ms
step:486/2160 train_time:16267ms step_avg:33.47ms
step:487/2160 train_time:16300ms step_avg:33.47ms
step:488/2160 train_time:16333ms step_avg:33.47ms
step:489/2160 train_time:16367ms step_avg:33.47ms
step:490/2160 train_time:16400ms step_avg:33.47ms
step:491/2160 train_time:16434ms step_avg:33.47ms
step:492/2160 train_time:16467ms step_avg:33.47ms
step:493/2160 train_time:16500ms step_avg:33.47ms
step:494/2160 train_time:16533ms step_avg:33.47ms
step:495/2160 train_time:16567ms step_avg:33.47ms
step:496/2160 train_time:16600ms step_avg:33.47ms
step:497/2160 train_time:16635ms step_avg:33.47ms
step:498/2160 train_time:16667ms step_avg:33.47ms
step:499/2160 train_time:16701ms step_avg:33.47ms
step:500/2160 train_time:16734ms step_avg:33.47ms
step:500/2160 val_loss:4.0191 train_time:16769ms step_avg:33.54ms
step:501/2160 train_time:16792ms step_avg:33.52ms
step:502/2160 train_time:16813ms step_avg:33.49ms
step:503/2160 train_time:16838ms step_avg:33.48ms
step:504/2160 train_time:16871ms step_avg:33.48ms
step:505/2160 train_time:16907ms step_avg:33.48ms
step:506/2160 train_time:16942ms step_avg:33.48ms
step:507/2160 train_time:16977ms step_avg:33.48ms
step:508/2160 train_time:17010ms step_avg:33.48ms
step:509/2160 train_time:17044ms step_avg:33.49ms
step:510/2160 train_time:17077ms step_avg:33.48ms
step:511/2160 train_time:17111ms step_avg:33.48ms
step:512/2160 train_time:17144ms step_avg:33.48ms
step:513/2160 train_time:17177ms step_avg:33.48ms
step:514/2160 train_time:17210ms step_avg:33.48ms
step:515/2160 train_time:17244ms step_avg:33.48ms
step:516/2160 train_time:17277ms step_avg:33.48ms
step:517/2160 train_time:17310ms step_avg:33.48ms
step:518/2160 train_time:17343ms step_avg:33.48ms
step:519/2160 train_time:17377ms step_avg:33.48ms
step:520/2160 train_time:17410ms step_avg:33.48ms
step:521/2160 train_time:17443ms step_avg:33.48ms
step:522/2160 train_time:17476ms step_avg:33.48ms
step:523/2160 train_time:17510ms step_avg:33.48ms
step:524/2160 train_time:17542ms step_avg:33.48ms
step:525/2160 train_time:17576ms step_avg:33.48ms
step:526/2160 train_time:17609ms step_avg:33.48ms
step:527/2160 train_time:17643ms step_avg:33.48ms
step:528/2160 train_time:17675ms step_avg:33.48ms
step:529/2160 train_time:17709ms step_avg:33.48ms
step:530/2160 train_time:17742ms step_avg:33.48ms
step:531/2160 train_time:17776ms step_avg:33.48ms
step:532/2160 train_time:17809ms step_avg:33.47ms
step:533/2160 train_time:17842ms step_avg:33.47ms
step:534/2160 train_time:17875ms step_avg:33.47ms
step:535/2160 train_time:17909ms step_avg:33.48ms
step:536/2160 train_time:17942ms step_avg:33.47ms
step:537/2160 train_time:17975ms step_avg:33.47ms
step:538/2160 train_time:18008ms step_avg:33.47ms
step:539/2160 train_time:18042ms step_avg:33.47ms
step:540/2160 train_time:18075ms step_avg:33.47ms
step:541/2160 train_time:18108ms step_avg:33.47ms
step:542/2160 train_time:18141ms step_avg:33.47ms
step:543/2160 train_time:18175ms step_avg:33.47ms
step:544/2160 train_time:18208ms step_avg:33.47ms
step:545/2160 train_time:18242ms step_avg:33.47ms
step:546/2160 train_time:18274ms step_avg:33.47ms
step:547/2160 train_time:18308ms step_avg:33.47ms
step:548/2160 train_time:18341ms step_avg:33.47ms
step:549/2160 train_time:18375ms step_avg:33.47ms
step:550/2160 train_time:18407ms step_avg:33.47ms
step:551/2160 train_time:18441ms step_avg:33.47ms
step:552/2160 train_time:18474ms step_avg:33.47ms
step:553/2160 train_time:18508ms step_avg:33.47ms
step:554/2160 train_time:18541ms step_avg:33.47ms
step:555/2160 train_time:18574ms step_avg:33.47ms
step:556/2160 train_time:18607ms step_avg:33.47ms
step:557/2160 train_time:18641ms step_avg:33.47ms
step:558/2160 train_time:18674ms step_avg:33.47ms
step:559/2160 train_time:18707ms step_avg:33.47ms
step:560/2160 train_time:18740ms step_avg:33.46ms
step:561/2160 train_time:18774ms step_avg:33.47ms
step:562/2160 train_time:18807ms step_avg:33.46ms
step:563/2160 train_time:18841ms step_avg:33.46ms
step:564/2160 train_time:18874ms step_avg:33.46ms
step:565/2160 train_time:18907ms step_avg:33.46ms
step:566/2160 train_time:18940ms step_avg:33.46ms
step:567/2160 train_time:18974ms step_avg:33.46ms
step:568/2160 train_time:19007ms step_avg:33.46ms
step:569/2160 train_time:19041ms step_avg:33.46ms
step:570/2160 train_time:19074ms step_avg:33.46ms
step:571/2160 train_time:19107ms step_avg:33.46ms
step:572/2160 train_time:19140ms step_avg:33.46ms
step:573/2160 train_time:19174ms step_avg:33.46ms
step:574/2160 train_time:19207ms step_avg:33.46ms
step:575/2160 train_time:19241ms step_avg:33.46ms
step:576/2160 train_time:19274ms step_avg:33.46ms
step:577/2160 train_time:19308ms step_avg:33.46ms
step:578/2160 train_time:19341ms step_avg:33.46ms
step:579/2160 train_time:19374ms step_avg:33.46ms
step:580/2160 train_time:19407ms step_avg:33.46ms
step:581/2160 train_time:19441ms step_avg:33.46ms
step:582/2160 train_time:19474ms step_avg:33.46ms
step:583/2160 train_time:19508ms step_avg:33.46ms
step:584/2160 train_time:19541ms step_avg:33.46ms
step:585/2160 train_time:19574ms step_avg:33.46ms
step:586/2160 train_time:19607ms step_avg:33.46ms
step:587/2160 train_time:19641ms step_avg:33.46ms
step:588/2160 train_time:19674ms step_avg:33.46ms
step:589/2160 train_time:19708ms step_avg:33.46ms
step:590/2160 train_time:19740ms step_avg:33.46ms
step:591/2160 train_time:19775ms step_avg:33.46ms
step:592/2160 train_time:19807ms step_avg:33.46ms
step:593/2160 train_time:19841ms step_avg:33.46ms
step:594/2160 train_time:19874ms step_avg:33.46ms
step:595/2160 train_time:19908ms step_avg:33.46ms
step:596/2160 train_time:19941ms step_avg:33.46ms
step:597/2160 train_time:19974ms step_avg:33.46ms
step:598/2160 train_time:20007ms step_avg:33.46ms
step:599/2160 train_time:20041ms step_avg:33.46ms
step:600/2160 train_time:20074ms step_avg:33.46ms
step:601/2160 train_time:20107ms step_avg:33.46ms
step:602/2160 train_time:20140ms step_avg:33.46ms
step:603/2160 train_time:20174ms step_avg:33.46ms
step:604/2160 train_time:20207ms step_avg:33.46ms
step:605/2160 train_time:20241ms step_avg:33.46ms
step:606/2160 train_time:20274ms step_avg:33.46ms
step:607/2160 train_time:20308ms step_avg:33.46ms
step:608/2160 train_time:20341ms step_avg:33.45ms
step:609/2160 train_time:20374ms step_avg:33.46ms
step:610/2160 train_time:20407ms step_avg:33.45ms
step:611/2160 train_time:20441ms step_avg:33.45ms
step:612/2160 train_time:20474ms step_avg:33.45ms
step:613/2160 train_time:20508ms step_avg:33.45ms
step:614/2160 train_time:20541ms step_avg:33.45ms
step:615/2160 train_time:20574ms step_avg:33.45ms
step:616/2160 train_time:20608ms step_avg:33.45ms
step:617/2160 train_time:20641ms step_avg:33.45ms
step:618/2160 train_time:20674ms step_avg:33.45ms
step:619/2160 train_time:20708ms step_avg:33.45ms
step:620/2160 train_time:20740ms step_avg:33.45ms
step:621/2160 train_time:20774ms step_avg:33.45ms
step:622/2160 train_time:20807ms step_avg:33.45ms
step:623/2160 train_time:20841ms step_avg:33.45ms
step:624/2160 train_time:20874ms step_avg:33.45ms
step:625/2160 train_time:20907ms step_avg:33.45ms
step:626/2160 train_time:20940ms step_avg:33.45ms
step:627/2160 train_time:20974ms step_avg:33.45ms
step:628/2160 train_time:21007ms step_avg:33.45ms
step:629/2160 train_time:21041ms step_avg:33.45ms
step:630/2160 train_time:21074ms step_avg:33.45ms
step:631/2160 train_time:21107ms step_avg:33.45ms
step:632/2160 train_time:21140ms step_avg:33.45ms
step:633/2160 train_time:21174ms step_avg:33.45ms
step:634/2160 train_time:21207ms step_avg:33.45ms
step:635/2160 train_time:21241ms step_avg:33.45ms
step:636/2160 train_time:21273ms step_avg:33.45ms
step:637/2160 train_time:21307ms step_avg:33.45ms
step:638/2160 train_time:21340ms step_avg:33.45ms
step:639/2160 train_time:21374ms step_avg:33.45ms
step:640/2160 train_time:21407ms step_avg:33.45ms
step:641/2160 train_time:21440ms step_avg:33.45ms
step:642/2160 train_time:21473ms step_avg:33.45ms
step:643/2160 train_time:21507ms step_avg:33.45ms
step:644/2160 train_time:21540ms step_avg:33.45ms
step:645/2160 train_time:21574ms step_avg:33.45ms
step:646/2160 train_time:21607ms step_avg:33.45ms
step:647/2160 train_time:21641ms step_avg:33.45ms
step:648/2160 train_time:21674ms step_avg:33.45ms
step:649/2160 train_time:21708ms step_avg:33.45ms
step:650/2160 train_time:21741ms step_avg:33.45ms
step:651/2160 train_time:21774ms step_avg:33.45ms
step:652/2160 train_time:21807ms step_avg:33.45ms
step:653/2160 train_time:21841ms step_avg:33.45ms
step:654/2160 train_time:21874ms step_avg:33.45ms
step:655/2160 train_time:21907ms step_avg:33.45ms
step:656/2160 train_time:21940ms step_avg:33.45ms
step:657/2160 train_time:21974ms step_avg:33.45ms
step:658/2160 train_time:22007ms step_avg:33.45ms
step:659/2160 train_time:22041ms step_avg:33.45ms
step:660/2160 train_time:22074ms step_avg:33.44ms
step:661/2160 train_time:22107ms step_avg:33.45ms
step:662/2160 train_time:22140ms step_avg:33.44ms
step:663/2160 train_time:22174ms step_avg:33.44ms
step:664/2160 train_time:22207ms step_avg:33.44ms
step:665/2160 train_time:22240ms step_avg:33.44ms
step:666/2160 train_time:22273ms step_avg:33.44ms
step:667/2160 train_time:22307ms step_avg:33.44ms
step:668/2160 train_time:22340ms step_avg:33.44ms
step:669/2160 train_time:22374ms step_avg:33.44ms
step:670/2160 train_time:22407ms step_avg:33.44ms
step:671/2160 train_time:22441ms step_avg:33.44ms
step:672/2160 train_time:22474ms step_avg:33.44ms
step:673/2160 train_time:22507ms step_avg:33.44ms
step:674/2160 train_time:22540ms step_avg:33.44ms
step:675/2160 train_time:22574ms step_avg:33.44ms
step:676/2160 train_time:22607ms step_avg:33.44ms
step:677/2160 train_time:22640ms step_avg:33.44ms
step:678/2160 train_time:22673ms step_avg:33.44ms
step:679/2160 train_time:22707ms step_avg:33.44ms
step:680/2160 train_time:22740ms step_avg:33.44ms
step:681/2160 train_time:22774ms step_avg:33.44ms
step:682/2160 train_time:22807ms step_avg:33.44ms
step:683/2160 train_time:22840ms step_avg:33.44ms
step:684/2160 train_time:22873ms step_avg:33.44ms
step:685/2160 train_time:22907ms step_avg:33.44ms
step:686/2160 train_time:22940ms step_avg:33.44ms
step:687/2160 train_time:22974ms step_avg:33.44ms
step:688/2160 train_time:23007ms step_avg:33.44ms
step:689/2160 train_time:23041ms step_avg:33.44ms
step:690/2160 train_time:23074ms step_avg:33.44ms
step:691/2160 train_time:23107ms step_avg:33.44ms
step:692/2160 train_time:23140ms step_avg:33.44ms
step:693/2160 train_time:23174ms step_avg:33.44ms
step:694/2160 train_time:23207ms step_avg:33.44ms
step:695/2160 train_time:23241ms step_avg:33.44ms
step:696/2160 train_time:23274ms step_avg:33.44ms
step:697/2160 train_time:23308ms step_avg:33.44ms
step:698/2160 train_time:23341ms step_avg:33.44ms
step:699/2160 train_time:23374ms step_avg:33.44ms
step:700/2160 train_time:23407ms step_avg:33.44ms
step:701/2160 train_time:23441ms step_avg:33.44ms
step:702/2160 train_time:23474ms step_avg:33.44ms
step:703/2160 train_time:23508ms step_avg:33.44ms
step:704/2160 train_time:23575ms step_avg:33.49ms
step:705/2160 train_time:23594ms step_avg:33.47ms
step:706/2160 train_time:23622ms step_avg:33.46ms
step:707/2160 train_time:23655ms step_avg:33.46ms
step:708/2160 train_time:23689ms step_avg:33.46ms
step:709/2160 train_time:23748ms step_avg:33.50ms
step:710/2160 train_time:23806ms step_avg:33.53ms
step:711/2160 train_time:23866ms step_avg:33.57ms
step:712/2160 train_time:23924ms step_avg:33.60ms
step:713/2160 train_time:23984ms step_avg:33.64ms
step:714/2160 train_time:24044ms step_avg:33.68ms
step:715/2160 train_time:24104ms step_avg:33.71ms
step:716/2160 train_time:24163ms step_avg:33.75ms
step:717/2160 train_time:24224ms step_avg:33.79ms
step:718/2160 train_time:24283ms step_avg:33.82ms
step:719/2160 train_time:24344ms step_avg:33.86ms
step:720/2160 train_time:24404ms step_avg:33.89ms
step:721/2160 train_time:24464ms step_avg:33.93ms
step:722/2160 train_time:24524ms step_avg:33.97ms
step:723/2160 train_time:24585ms step_avg:34.00ms
step:724/2160 train_time:24644ms step_avg:34.04ms
step:725/2160 train_time:24705ms step_avg:34.08ms
step:726/2160 train_time:24764ms step_avg:34.11ms
step:727/2160 train_time:24824ms step_avg:34.15ms
step:728/2160 train_time:24882ms step_avg:34.18ms
step:729/2160 train_time:24943ms step_avg:34.21ms
step:730/2160 train_time:25001ms step_avg:34.25ms
step:731/2160 train_time:25062ms step_avg:34.28ms
step:732/2160 train_time:25121ms step_avg:34.32ms
step:733/2160 train_time:25181ms step_avg:34.35ms
step:734/2160 train_time:25240ms step_avg:34.39ms
step:735/2160 train_time:25301ms step_avg:34.42ms
step:736/2160 train_time:25360ms step_avg:34.46ms
step:737/2160 train_time:25421ms step_avg:34.49ms
step:738/2160 train_time:25480ms step_avg:34.53ms
step:739/2160 train_time:25541ms step_avg:34.56ms
step:740/2160 train_time:25600ms step_avg:34.60ms
step:741/2160 train_time:25661ms step_avg:34.63ms
step:742/2160 train_time:25719ms step_avg:34.66ms
step:743/2160 train_time:25780ms step_avg:34.70ms
step:744/2160 train_time:25838ms step_avg:34.73ms
step:745/2160 train_time:25899ms step_avg:34.76ms
step:746/2160 train_time:25957ms step_avg:34.79ms
step:747/2160 train_time:26017ms step_avg:34.83ms
step:748/2160 train_time:26076ms step_avg:34.86ms
step:749/2160 train_time:26136ms step_avg:34.89ms
step:750/2160 train_time:26195ms step_avg:34.93ms
step:750/2160 val_loss:3.8551 train_time:26256ms step_avg:35.01ms
step:751/2160 train_time:26284ms step_avg:35.00ms
step:752/2160 train_time:26317ms step_avg:35.00ms
step:753/2160 train_time:26383ms step_avg:35.04ms
step:754/2160 train_time:26448ms step_avg:35.08ms
step:755/2160 train_time:26509ms step_avg:35.11ms
step:756/2160 train_time:26569ms step_avg:35.14ms
step:757/2160 train_time:26629ms step_avg:35.18ms
step:758/2160 train_time:26687ms step_avg:35.21ms
step:759/2160 train_time:26747ms step_avg:35.24ms
step:760/2160 train_time:26805ms step_avg:35.27ms
step:761/2160 train_time:26864ms step_avg:35.30ms
step:762/2160 train_time:26922ms step_avg:35.33ms
step:763/2160 train_time:26982ms step_avg:35.36ms
step:764/2160 train_time:27039ms step_avg:35.39ms
step:765/2160 train_time:27099ms step_avg:35.42ms
step:766/2160 train_time:27158ms step_avg:35.45ms
step:767/2160 train_time:27218ms step_avg:35.49ms
step:768/2160 train_time:27277ms step_avg:35.52ms
step:769/2160 train_time:27339ms step_avg:35.55ms
step:770/2160 train_time:27399ms step_avg:35.58ms
step:771/2160 train_time:27461ms step_avg:35.62ms
step:772/2160 train_time:27521ms step_avg:35.65ms
step:773/2160 train_time:27582ms step_avg:35.68ms
step:774/2160 train_time:27641ms step_avg:35.71ms
step:775/2160 train_time:27701ms step_avg:35.74ms
step:776/2160 train_time:27759ms step_avg:35.77ms
step:777/2160 train_time:27872ms step_avg:35.87ms
step:778/2160 train_time:27909ms step_avg:35.87ms
step:779/2160 train_time:27969ms step_avg:35.90ms
step:780/2160 train_time:28026ms step_avg:35.93ms
step:781/2160 train_time:28086ms step_avg:35.96ms
step:782/2160 train_time:28144ms step_avg:35.99ms
step:783/2160 train_time:28204ms step_avg:36.02ms
step:784/2160 train_time:28261ms step_avg:36.05ms
step:785/2160 train_time:28321ms step_avg:36.08ms
step:786/2160 train_time:28379ms step_avg:36.11ms
step:787/2160 train_time:28439ms step_avg:36.14ms
step:788/2160 train_time:28497ms step_avg:36.16ms
step:789/2160 train_time:28559ms step_avg:36.20ms
step:790/2160 train_time:28617ms step_avg:36.22ms
step:791/2160 train_time:28678ms step_avg:36.26ms
step:792/2160 train_time:28737ms step_avg:36.28ms
step:793/2160 train_time:28799ms step_avg:36.32ms
step:794/2160 train_time:28861ms step_avg:36.35ms
step:795/2160 train_time:28924ms step_avg:36.38ms
step:796/2160 train_time:28983ms step_avg:36.41ms
step:797/2160 train_time:29044ms step_avg:36.44ms
step:798/2160 train_time:29101ms step_avg:36.47ms
step:799/2160 train_time:29161ms step_avg:36.50ms
step:800/2160 train_time:29219ms step_avg:36.52ms
step:801/2160 train_time:29279ms step_avg:36.55ms
step:802/2160 train_time:29337ms step_avg:36.58ms
step:803/2160 train_time:29397ms step_avg:36.61ms
step:804/2160 train_time:29455ms step_avg:36.64ms
step:805/2160 train_time:29515ms step_avg:36.66ms
step:806/2160 train_time:29573ms step_avg:36.69ms
step:807/2160 train_time:29633ms step_avg:36.72ms
step:808/2160 train_time:29692ms step_avg:36.75ms
step:809/2160 train_time:29753ms step_avg:36.78ms
step:810/2160 train_time:29813ms step_avg:36.81ms
step:811/2160 train_time:29876ms step_avg:36.84ms
step:812/2160 train_time:29936ms step_avg:36.87ms
step:813/2160 train_time:29997ms step_avg:36.90ms
step:814/2160 train_time:30057ms step_avg:36.92ms
step:815/2160 train_time:30117ms step_avg:36.95ms
step:816/2160 train_time:30176ms step_avg:36.98ms
step:817/2160 train_time:30237ms step_avg:37.01ms
step:818/2160 train_time:30295ms step_avg:37.04ms
step:819/2160 train_time:30355ms step_avg:37.06ms
step:820/2160 train_time:30413ms step_avg:37.09ms
step:821/2160 train_time:30473ms step_avg:37.12ms
step:822/2160 train_time:30531ms step_avg:37.14ms
step:823/2160 train_time:30590ms step_avg:37.17ms
step:824/2160 train_time:30649ms step_avg:37.20ms
step:825/2160 train_time:30710ms step_avg:37.22ms
step:826/2160 train_time:30769ms step_avg:37.25ms
step:827/2160 train_time:30830ms step_avg:37.28ms
step:828/2160 train_time:30890ms step_avg:37.31ms
step:829/2160 train_time:30951ms step_avg:37.34ms
step:830/2160 train_time:31011ms step_avg:37.36ms
step:831/2160 train_time:31072ms step_avg:37.39ms
step:832/2160 train_time:31130ms step_avg:37.42ms
step:833/2160 train_time:31191ms step_avg:37.44ms
step:834/2160 train_time:31249ms step_avg:37.47ms
step:835/2160 train_time:31310ms step_avg:37.50ms
step:836/2160 train_time:31368ms step_avg:37.52ms
step:837/2160 train_time:31428ms step_avg:37.55ms
step:838/2160 train_time:31487ms step_avg:37.57ms
step:839/2160 train_time:31547ms step_avg:37.60ms
step:840/2160 train_time:31605ms step_avg:37.63ms
step:841/2160 train_time:31666ms step_avg:37.65ms
step:842/2160 train_time:31725ms step_avg:37.68ms
step:843/2160 train_time:31786ms step_avg:37.71ms
step:844/2160 train_time:31845ms step_avg:37.73ms
step:845/2160 train_time:31906ms step_avg:37.76ms
step:846/2160 train_time:31965ms step_avg:37.78ms
step:847/2160 train_time:32026ms step_avg:37.81ms
step:848/2160 train_time:32085ms step_avg:37.84ms
step:849/2160 train_time:32146ms step_avg:37.86ms
step:850/2160 train_time:32205ms step_avg:37.89ms
step:851/2160 train_time:32266ms step_avg:37.92ms
step:852/2160 train_time:32325ms step_avg:37.94ms
step:853/2160 train_time:32385ms step_avg:37.97ms
step:854/2160 train_time:32444ms step_avg:37.99ms
step:855/2160 train_time:32504ms step_avg:38.02ms
step:856/2160 train_time:32562ms step_avg:38.04ms
step:857/2160 train_time:32623ms step_avg:38.07ms
step:858/2160 train_time:32682ms step_avg:38.09ms
step:859/2160 train_time:32742ms step_avg:38.12ms
step:860/2160 train_time:32801ms step_avg:38.14ms
step:861/2160 train_time:32862ms step_avg:38.17ms
step:862/2160 train_time:32920ms step_avg:38.19ms
step:863/2160 train_time:32982ms step_avg:38.22ms
step:864/2160 train_time:33041ms step_avg:38.24ms
step:865/2160 train_time:33101ms step_avg:38.27ms
step:866/2160 train_time:33160ms step_avg:38.29ms
step:867/2160 train_time:33221ms step_avg:38.32ms
step:868/2160 train_time:33281ms step_avg:38.34ms
step:869/2160 train_time:33341ms step_avg:38.37ms
step:870/2160 train_time:33400ms step_avg:38.39ms
step:871/2160 train_time:33460ms step_avg:38.42ms
step:872/2160 train_time:33518ms step_avg:38.44ms
step:873/2160 train_time:33578ms step_avg:38.46ms
step:874/2160 train_time:33637ms step_avg:38.49ms
step:875/2160 train_time:33697ms step_avg:38.51ms
step:876/2160 train_time:33755ms step_avg:38.53ms
step:877/2160 train_time:33817ms step_avg:38.56ms
step:878/2160 train_time:33875ms step_avg:38.58ms
step:879/2160 train_time:33937ms step_avg:38.61ms
step:880/2160 train_time:33996ms step_avg:38.63ms
step:881/2160 train_time:34058ms step_avg:38.66ms
step:882/2160 train_time:34117ms step_avg:38.68ms
step:883/2160 train_time:34177ms step_avg:38.71ms
step:884/2160 train_time:34236ms step_avg:38.73ms
step:885/2160 train_time:34298ms step_avg:38.75ms
step:886/2160 train_time:34356ms step_avg:38.78ms
step:887/2160 train_time:34417ms step_avg:38.80ms
step:888/2160 train_time:34476ms step_avg:38.82ms
step:889/2160 train_time:34536ms step_avg:38.85ms
step:890/2160 train_time:34594ms step_avg:38.87ms
step:891/2160 train_time:34655ms step_avg:38.89ms
step:892/2160 train_time:34713ms step_avg:38.92ms
step:893/2160 train_time:34773ms step_avg:38.94ms
step:894/2160 train_time:34832ms step_avg:38.96ms
step:895/2160 train_time:34892ms step_avg:38.99ms
step:896/2160 train_time:34951ms step_avg:39.01ms
step:897/2160 train_time:35012ms step_avg:39.03ms
step:898/2160 train_time:35072ms step_avg:39.06ms
step:899/2160 train_time:35132ms step_avg:39.08ms
step:900/2160 train_time:35192ms step_avg:39.10ms
step:901/2160 train_time:35252ms step_avg:39.13ms
step:902/2160 train_time:35311ms step_avg:39.15ms
step:903/2160 train_time:35372ms step_avg:39.17ms
step:904/2160 train_time:35431ms step_avg:39.19ms
step:905/2160 train_time:35491ms step_avg:39.22ms
step:906/2160 train_time:35550ms step_avg:39.24ms
step:907/2160 train_time:35611ms step_avg:39.26ms
step:908/2160 train_time:35669ms step_avg:39.28ms
step:909/2160 train_time:35729ms step_avg:39.31ms
step:910/2160 train_time:35788ms step_avg:39.33ms
step:911/2160 train_time:35849ms step_avg:39.35ms
step:912/2160 train_time:35907ms step_avg:39.37ms
step:913/2160 train_time:35968ms step_avg:39.40ms
step:914/2160 train_time:36027ms step_avg:39.42ms
step:915/2160 train_time:36089ms step_avg:39.44ms
step:916/2160 train_time:36147ms step_avg:39.46ms
step:917/2160 train_time:36208ms step_avg:39.49ms
step:918/2160 train_time:36267ms step_avg:39.51ms
step:919/2160 train_time:36327ms step_avg:39.53ms
step:920/2160 train_time:36387ms step_avg:39.55ms
step:921/2160 train_time:36447ms step_avg:39.57ms
step:922/2160 train_time:36506ms step_avg:39.59ms
step:923/2160 train_time:36567ms step_avg:39.62ms
step:924/2160 train_time:36626ms step_avg:39.64ms
step:925/2160 train_time:36686ms step_avg:39.66ms
step:926/2160 train_time:36745ms step_avg:39.68ms
step:927/2160 train_time:36806ms step_avg:39.70ms
step:928/2160 train_time:36864ms step_avg:39.72ms
step:929/2160 train_time:36925ms step_avg:39.75ms
step:930/2160 train_time:36983ms step_avg:39.77ms
step:931/2160 train_time:37045ms step_avg:39.79ms
step:932/2160 train_time:37104ms step_avg:39.81ms
step:933/2160 train_time:37165ms step_avg:39.83ms
step:934/2160 train_time:37224ms step_avg:39.85ms
step:935/2160 train_time:37285ms step_avg:39.88ms
step:936/2160 train_time:37344ms step_avg:39.90ms
step:937/2160 train_time:37405ms step_avg:39.92ms
step:938/2160 train_time:37465ms step_avg:39.94ms
step:939/2160 train_time:37526ms step_avg:39.96ms
step:940/2160 train_time:37585ms step_avg:39.98ms
step:941/2160 train_time:37645ms step_avg:40.01ms
step:942/2160 train_time:37704ms step_avg:40.03ms
step:943/2160 train_time:37764ms step_avg:40.05ms
step:944/2160 train_time:37822ms step_avg:40.07ms
step:945/2160 train_time:37883ms step_avg:40.09ms
step:946/2160 train_time:37942ms step_avg:40.11ms
step:947/2160 train_time:38003ms step_avg:40.13ms
step:948/2160 train_time:38061ms step_avg:40.15ms
step:949/2160 train_time:38122ms step_avg:40.17ms
step:950/2160 train_time:38181ms step_avg:40.19ms
step:951/2160 train_time:38241ms step_avg:40.21ms
step:952/2160 train_time:38299ms step_avg:40.23ms
step:953/2160 train_time:38360ms step_avg:40.25ms
step:954/2160 train_time:38418ms step_avg:40.27ms
step:955/2160 train_time:38480ms step_avg:40.29ms
step:956/2160 train_time:38539ms step_avg:40.31ms
step:957/2160 train_time:38600ms step_avg:40.33ms
step:958/2160 train_time:38658ms step_avg:40.35ms
step:959/2160 train_time:38719ms step_avg:40.37ms
step:960/2160 train_time:38777ms step_avg:40.39ms
step:961/2160 train_time:38838ms step_avg:40.41ms
step:962/2160 train_time:38897ms step_avg:40.43ms
step:963/2160 train_time:38957ms step_avg:40.45ms
step:964/2160 train_time:39016ms step_avg:40.47ms
step:965/2160 train_time:39077ms step_avg:40.49ms
step:966/2160 train_time:39135ms step_avg:40.51ms
step:967/2160 train_time:39196ms step_avg:40.53ms
step:968/2160 train_time:39255ms step_avg:40.55ms
step:969/2160 train_time:39316ms step_avg:40.57ms
step:970/2160 train_time:39375ms step_avg:40.59ms
step:971/2160 train_time:39435ms step_avg:40.61ms
step:972/2160 train_time:39495ms step_avg:40.63ms
step:973/2160 train_time:39555ms step_avg:40.65ms
step:974/2160 train_time:39615ms step_avg:40.67ms
step:975/2160 train_time:39674ms step_avg:40.69ms
step:976/2160 train_time:39733ms step_avg:40.71ms
step:977/2160 train_time:39794ms step_avg:40.73ms
step:978/2160 train_time:39853ms step_avg:40.75ms
step:979/2160 train_time:39913ms step_avg:40.77ms
step:980/2160 train_time:39972ms step_avg:40.79ms
step:981/2160 train_time:40033ms step_avg:40.81ms
step:982/2160 train_time:40091ms step_avg:40.83ms
step:983/2160 train_time:40151ms step_avg:40.85ms
step:984/2160 train_time:40210ms step_avg:40.86ms
step:985/2160 train_time:40270ms step_avg:40.88ms
step:986/2160 train_time:40330ms step_avg:40.90ms
step:987/2160 train_time:40390ms step_avg:40.92ms
step:988/2160 train_time:40449ms step_avg:40.94ms
step:989/2160 train_time:40510ms step_avg:40.96ms
step:990/2160 train_time:40571ms step_avg:40.98ms
step:991/2160 train_time:40629ms step_avg:41.00ms
step:992/2160 train_time:40688ms step_avg:41.02ms
step:993/2160 train_time:40749ms step_avg:41.04ms
step:994/2160 train_time:40808ms step_avg:41.05ms
step:995/2160 train_time:40868ms step_avg:41.07ms
step:996/2160 train_time:40926ms step_avg:41.09ms
step:997/2160 train_time:40987ms step_avg:41.11ms
step:998/2160 train_time:41046ms step_avg:41.13ms
step:999/2160 train_time:41107ms step_avg:41.15ms
step:1000/2160 train_time:41166ms step_avg:41.17ms
step:1000/2160 val_loss:3.6956 train_time:41228ms step_avg:41.23ms
step:1001/2160 train_time:41251ms step_avg:41.21ms
step:1002/2160 train_time:41290ms step_avg:41.21ms
step:1003/2160 train_time:41354ms step_avg:41.23ms
step:1004/2160 train_time:41417ms step_avg:41.25ms
step:1005/2160 train_time:41477ms step_avg:41.27ms
step:1006/2160 train_time:41536ms step_avg:41.29ms
step:1007/2160 train_time:41596ms step_avg:41.31ms
step:1008/2160 train_time:41654ms step_avg:41.32ms
step:1009/2160 train_time:41714ms step_avg:41.34ms
step:1010/2160 train_time:41772ms step_avg:41.36ms
step:1011/2160 train_time:41831ms step_avg:41.38ms
step:1012/2160 train_time:41889ms step_avg:41.39ms
step:1013/2160 train_time:41949ms step_avg:41.41ms
step:1014/2160 train_time:42007ms step_avg:41.43ms
step:1015/2160 train_time:42066ms step_avg:41.44ms
step:1016/2160 train_time:42124ms step_avg:41.46ms
step:1017/2160 train_time:42187ms step_avg:41.48ms
step:1018/2160 train_time:42245ms step_avg:41.50ms
step:1019/2160 train_time:42307ms step_avg:41.52ms
step:1020/2160 train_time:42368ms step_avg:41.54ms
step:1021/2160 train_time:42430ms step_avg:41.56ms
step:1022/2160 train_time:42491ms step_avg:41.58ms
step:1023/2160 train_time:42552ms step_avg:41.60ms
step:1024/2160 train_time:42610ms step_avg:41.61ms
step:1025/2160 train_time:42670ms step_avg:41.63ms
step:1026/2160 train_time:42729ms step_avg:41.65ms
step:1027/2160 train_time:42788ms step_avg:41.66ms
step:1028/2160 train_time:42846ms step_avg:41.68ms
step:1029/2160 train_time:42906ms step_avg:41.70ms
step:1030/2160 train_time:42964ms step_avg:41.71ms
step:1031/2160 train_time:43023ms step_avg:41.73ms
step:1032/2160 train_time:43082ms step_avg:41.75ms
step:1033/2160 train_time:43142ms step_avg:41.76ms
step:1034/2160 train_time:43200ms step_avg:41.78ms
step:1035/2160 train_time:43261ms step_avg:41.80ms
step:1036/2160 train_time:43321ms step_avg:41.82ms
step:1037/2160 train_time:43383ms step_avg:41.84ms
step:1038/2160 train_time:43443ms step_avg:41.85ms
step:1039/2160 train_time:43505ms step_avg:41.87ms
step:1040/2160 train_time:43563ms step_avg:41.89ms
step:1041/2160 train_time:43624ms step_avg:41.91ms
step:1042/2160 train_time:43682ms step_avg:41.92ms
step:1043/2160 train_time:43742ms step_avg:41.94ms
step:1044/2160 train_time:43801ms step_avg:41.95ms
step:1045/2160 train_time:43861ms step_avg:41.97ms
step:1046/2160 train_time:43919ms step_avg:41.99ms
step:1047/2160 train_time:43978ms step_avg:42.00ms
step:1048/2160 train_time:44037ms step_avg:42.02ms
step:1049/2160 train_time:44097ms step_avg:42.04ms
step:1050/2160 train_time:44155ms step_avg:42.05ms
step:1051/2160 train_time:44216ms step_avg:42.07ms
step:1052/2160 train_time:44274ms step_avg:42.09ms
step:1053/2160 train_time:44336ms step_avg:42.10ms
step:1054/2160 train_time:44395ms step_avg:42.12ms
step:1055/2160 train_time:44457ms step_avg:42.14ms
step:1056/2160 train_time:44517ms step_avg:42.16ms
step:1057/2160 train_time:44577ms step_avg:42.17ms
step:1058/2160 train_time:44636ms step_avg:42.19ms
step:1059/2160 train_time:44697ms step_avg:42.21ms
step:1060/2160 train_time:44755ms step_avg:42.22ms
step:1061/2160 train_time:44815ms step_avg:42.24ms
step:1062/2160 train_time:44874ms step_avg:42.25ms
step:1063/2160 train_time:44934ms step_avg:42.27ms
step:1064/2160 train_time:44993ms step_avg:42.29ms
step:1065/2160 train_time:45054ms step_avg:42.30ms
step:1066/2160 train_time:45112ms step_avg:42.32ms
step:1067/2160 train_time:45172ms step_avg:42.34ms
step:1068/2160 train_time:45231ms step_avg:42.35ms
step:1069/2160 train_time:45292ms step_avg:42.37ms
step:1070/2160 train_time:45351ms step_avg:42.38ms
step:1071/2160 train_time:45411ms step_avg:42.40ms
step:1072/2160 train_time:45471ms step_avg:42.42ms
step:1073/2160 train_time:45532ms step_avg:42.43ms
step:1074/2160 train_time:45591ms step_avg:42.45ms
step:1075/2160 train_time:45652ms step_avg:42.47ms
step:1076/2160 train_time:45710ms step_avg:42.48ms
step:1077/2160 train_time:45770ms step_avg:42.50ms
step:1078/2160 train_time:45829ms step_avg:42.51ms
step:1079/2160 train_time:45889ms step_avg:42.53ms
step:1080/2160 train_time:45948ms step_avg:42.54ms
step:1081/2160 train_time:46009ms step_avg:42.56ms
step:1082/2160 train_time:46067ms step_avg:42.58ms
step:1083/2160 train_time:46128ms step_avg:42.59ms
step:1084/2160 train_time:46186ms step_avg:42.61ms
step:1085/2160 train_time:46247ms step_avg:42.62ms
step:1086/2160 train_time:46306ms step_avg:42.64ms
step:1087/2160 train_time:46367ms step_avg:42.66ms
step:1088/2160 train_time:46426ms step_avg:42.67ms
step:1089/2160 train_time:46487ms step_avg:42.69ms
step:1090/2160 train_time:46545ms step_avg:42.70ms
step:1091/2160 train_time:46607ms step_avg:42.72ms
step:1092/2160 train_time:46665ms step_avg:42.73ms
step:1093/2160 train_time:46726ms step_avg:42.75ms
step:1094/2160 train_time:46785ms step_avg:42.76ms
step:1095/2160 train_time:46845ms step_avg:42.78ms
step:1096/2160 train_time:46904ms step_avg:42.80ms
step:1097/2160 train_time:46965ms step_avg:42.81ms
step:1098/2160 train_time:47024ms step_avg:42.83ms
step:1099/2160 train_time:47084ms step_avg:42.84ms
step:1100/2160 train_time:47143ms step_avg:42.86ms
step:1101/2160 train_time:47204ms step_avg:42.87ms
step:1102/2160 train_time:47262ms step_avg:42.89ms
step:1103/2160 train_time:47323ms step_avg:42.90ms
step:1104/2160 train_time:47382ms step_avg:42.92ms
step:1105/2160 train_time:47443ms step_avg:42.93ms
step:1106/2160 train_time:47501ms step_avg:42.95ms
step:1107/2160 train_time:47562ms step_avg:42.96ms
step:1108/2160 train_time:47621ms step_avg:42.98ms
step:1109/2160 train_time:47681ms step_avg:42.99ms
step:1110/2160 train_time:47740ms step_avg:43.01ms
step:1111/2160 train_time:47801ms step_avg:43.03ms
step:1112/2160 train_time:47860ms step_avg:43.04ms
step:1113/2160 train_time:47920ms step_avg:43.05ms
step:1114/2160 train_time:47979ms step_avg:43.07ms
step:1115/2160 train_time:48039ms step_avg:43.08ms
step:1116/2160 train_time:48098ms step_avg:43.10ms
step:1117/2160 train_time:48159ms step_avg:43.11ms
step:1118/2160 train_time:48217ms step_avg:43.13ms
step:1119/2160 train_time:48278ms step_avg:43.14ms
step:1120/2160 train_time:48337ms step_avg:43.16ms
step:1121/2160 train_time:48398ms step_avg:43.17ms
step:1122/2160 train_time:48457ms step_avg:43.19ms
step:1123/2160 train_time:48518ms step_avg:43.20ms
step:1124/2160 train_time:48577ms step_avg:43.22ms
step:1125/2160 train_time:48637ms step_avg:43.23ms
step:1126/2160 train_time:48696ms step_avg:43.25ms
step:1127/2160 train_time:48757ms step_avg:43.26ms
step:1128/2160 train_time:48816ms step_avg:43.28ms
step:1129/2160 train_time:48876ms step_avg:43.29ms
step:1130/2160 train_time:48936ms step_avg:43.31ms
step:1131/2160 train_time:48996ms step_avg:43.32ms
step:1132/2160 train_time:49055ms step_avg:43.34ms
step:1133/2160 train_time:49116ms step_avg:43.35ms
step:1134/2160 train_time:49175ms step_avg:43.36ms
step:1135/2160 train_time:49235ms step_avg:43.38ms
step:1136/2160 train_time:49294ms step_avg:43.39ms
step:1137/2160 train_time:49355ms step_avg:43.41ms
step:1138/2160 train_time:49414ms step_avg:43.42ms
step:1139/2160 train_time:49474ms step_avg:43.44ms
step:1140/2160 train_time:49533ms step_avg:43.45ms
step:1141/2160 train_time:49594ms step_avg:43.46ms
step:1142/2160 train_time:49652ms step_avg:43.48ms
step:1143/2160 train_time:49712ms step_avg:43.49ms
step:1144/2160 train_time:49772ms step_avg:43.51ms
step:1145/2160 train_time:49832ms step_avg:43.52ms
step:1146/2160 train_time:49891ms step_avg:43.53ms
step:1147/2160 train_time:49952ms step_avg:43.55ms
step:1148/2160 train_time:50011ms step_avg:43.56ms
step:1149/2160 train_time:50072ms step_avg:43.58ms
step:1150/2160 train_time:50131ms step_avg:43.59ms
step:1151/2160 train_time:50191ms step_avg:43.61ms
step:1152/2160 train_time:50250ms step_avg:43.62ms
step:1153/2160 train_time:50310ms step_avg:43.63ms
step:1154/2160 train_time:50369ms step_avg:43.65ms
step:1155/2160 train_time:50430ms step_avg:43.66ms
step:1156/2160 train_time:50489ms step_avg:43.68ms
step:1157/2160 train_time:50550ms step_avg:43.69ms
step:1158/2160 train_time:50609ms step_avg:43.70ms
step:1159/2160 train_time:50669ms step_avg:43.72ms
step:1160/2160 train_time:50729ms step_avg:43.73ms
step:1161/2160 train_time:50790ms step_avg:43.75ms
step:1162/2160 train_time:50848ms step_avg:43.76ms
step:1163/2160 train_time:50909ms step_avg:43.77ms
step:1164/2160 train_time:50968ms step_avg:43.79ms
step:1165/2160 train_time:51029ms step_avg:43.80ms
step:1166/2160 train_time:51088ms step_avg:43.81ms
step:1167/2160 train_time:51149ms step_avg:43.83ms
step:1168/2160 train_time:51207ms step_avg:43.84ms
step:1169/2160 train_time:51268ms step_avg:43.86ms
step:1170/2160 train_time:51327ms step_avg:43.87ms
step:1171/2160 train_time:51388ms step_avg:43.88ms
step:1172/2160 train_time:51447ms step_avg:43.90ms
step:1173/2160 train_time:51508ms step_avg:43.91ms
step:1174/2160 train_time:51566ms step_avg:43.92ms
step:1175/2160 train_time:51627ms step_avg:43.94ms
step:1176/2160 train_time:51686ms step_avg:43.95ms
step:1177/2160 train_time:51746ms step_avg:43.96ms
step:1178/2160 train_time:51805ms step_avg:43.98ms
step:1179/2160 train_time:51866ms step_avg:43.99ms
step:1180/2160 train_time:51925ms step_avg:44.00ms
step:1181/2160 train_time:51985ms step_avg:44.02ms
step:1182/2160 train_time:52044ms step_avg:44.03ms
step:1183/2160 train_time:52105ms step_avg:44.05ms
step:1184/2160 train_time:52164ms step_avg:44.06ms
step:1185/2160 train_time:52225ms step_avg:44.07ms
step:1186/2160 train_time:52283ms step_avg:44.08ms
step:1187/2160 train_time:52344ms step_avg:44.10ms
step:1188/2160 train_time:52402ms step_avg:44.11ms
step:1189/2160 train_time:52463ms step_avg:44.12ms
step:1190/2160 train_time:52521ms step_avg:44.14ms
step:1191/2160 train_time:52582ms step_avg:44.15ms
step:1192/2160 train_time:52640ms step_avg:44.16ms
step:1193/2160 train_time:52701ms step_avg:44.17ms
step:1194/2160 train_time:52759ms step_avg:44.19ms
step:1195/2160 train_time:52820ms step_avg:44.20ms
step:1196/2160 train_time:52879ms step_avg:44.21ms
step:1197/2160 train_time:52939ms step_avg:44.23ms
step:1198/2160 train_time:52999ms step_avg:44.24ms
step:1199/2160 train_time:53059ms step_avg:44.25ms
step:1200/2160 train_time:53118ms step_avg:44.27ms
step:1201/2160 train_time:53178ms step_avg:44.28ms
step:1202/2160 train_time:53237ms step_avg:44.29ms
step:1203/2160 train_time:53297ms step_avg:44.30ms
step:1204/2160 train_time:53356ms step_avg:44.32ms
step:1205/2160 train_time:53416ms step_avg:44.33ms
step:1206/2160 train_time:53475ms step_avg:44.34ms
step:1207/2160 train_time:53536ms step_avg:44.35ms
step:1208/2160 train_time:53596ms step_avg:44.37ms
step:1209/2160 train_time:53657ms step_avg:44.38ms
step:1210/2160 train_time:53715ms step_avg:44.39ms
step:1211/2160 train_time:53776ms step_avg:44.41ms
step:1212/2160 train_time:53836ms step_avg:44.42ms
step:1213/2160 train_time:53897ms step_avg:44.43ms
step:1214/2160 train_time:53955ms step_avg:44.44ms
step:1215/2160 train_time:54016ms step_avg:44.46ms
step:1216/2160 train_time:54075ms step_avg:44.47ms
step:1217/2160 train_time:54135ms step_avg:44.48ms
step:1218/2160 train_time:54194ms step_avg:44.49ms
step:1219/2160 train_time:54255ms step_avg:44.51ms
step:1220/2160 train_time:54314ms step_avg:44.52ms
step:1221/2160 train_time:54374ms step_avg:44.53ms
step:1222/2160 train_time:54433ms step_avg:44.54ms
step:1223/2160 train_time:54493ms step_avg:44.56ms
step:1224/2160 train_time:54552ms step_avg:44.57ms
step:1225/2160 train_time:54612ms step_avg:44.58ms
step:1226/2160 train_time:54671ms step_avg:44.59ms
step:1227/2160 train_time:54732ms step_avg:44.61ms
step:1228/2160 train_time:54791ms step_avg:44.62ms
step:1229/2160 train_time:54852ms step_avg:44.63ms
step:1230/2160 train_time:54911ms step_avg:44.64ms
step:1231/2160 train_time:54971ms step_avg:44.66ms
step:1232/2160 train_time:55030ms step_avg:44.67ms
step:1233/2160 train_time:55091ms step_avg:44.68ms
step:1234/2160 train_time:55150ms step_avg:44.69ms
step:1235/2160 train_time:55210ms step_avg:44.70ms
step:1236/2160 train_time:55269ms step_avg:44.72ms
step:1237/2160 train_time:55329ms step_avg:44.73ms
step:1238/2160 train_time:55388ms step_avg:44.74ms
step:1239/2160 train_time:55448ms step_avg:44.75ms
step:1240/2160 train_time:55507ms step_avg:44.76ms
step:1241/2160 train_time:55567ms step_avg:44.78ms
step:1242/2160 train_time:55626ms step_avg:44.79ms
step:1243/2160 train_time:55687ms step_avg:44.80ms
step:1244/2160 train_time:55746ms step_avg:44.81ms
step:1245/2160 train_time:55806ms step_avg:44.82ms
step:1246/2160 train_time:55865ms step_avg:44.84ms
step:1247/2160 train_time:55927ms step_avg:44.85ms
step:1248/2160 train_time:55986ms step_avg:44.86ms
step:1249/2160 train_time:56046ms step_avg:44.87ms
step:1250/2160 train_time:56105ms step_avg:44.88ms
step:1250/2160 val_loss:3.5761 train_time:56167ms step_avg:44.93ms
step:1251/2160 train_time:56193ms step_avg:44.92ms
step:1252/2160 train_time:56231ms step_avg:44.91ms
step:1253/2160 train_time:56293ms step_avg:44.93ms
step:1254/2160 train_time:56355ms step_avg:44.94ms
step:1255/2160 train_time:56417ms step_avg:44.95ms
step:1256/2160 train_time:56476ms step_avg:44.96ms
step:1257/2160 train_time:56536ms step_avg:44.98ms
step:1258/2160 train_time:56594ms step_avg:44.99ms
step:1259/2160 train_time:56654ms step_avg:45.00ms
step:1260/2160 train_time:56712ms step_avg:45.01ms
step:1261/2160 train_time:56772ms step_avg:45.02ms
step:1262/2160 train_time:56830ms step_avg:45.03ms
step:1263/2160 train_time:56890ms step_avg:45.04ms
step:1264/2160 train_time:56948ms step_avg:45.05ms
step:1265/2160 train_time:57008ms step_avg:45.07ms
step:1266/2160 train_time:57066ms step_avg:45.08ms
step:1267/2160 train_time:57127ms step_avg:45.09ms
step:1268/2160 train_time:57186ms step_avg:45.10ms
step:1269/2160 train_time:57249ms step_avg:45.11ms
step:1270/2160 train_time:57309ms step_avg:45.13ms
step:1271/2160 train_time:57371ms step_avg:45.14ms
step:1272/2160 train_time:57432ms step_avg:45.15ms
step:1273/2160 train_time:57493ms step_avg:45.16ms
step:1274/2160 train_time:57552ms step_avg:45.17ms
step:1275/2160 train_time:57612ms step_avg:45.19ms
step:1276/2160 train_time:57671ms step_avg:45.20ms
step:1277/2160 train_time:57731ms step_avg:45.21ms
step:1278/2160 train_time:57789ms step_avg:45.22ms
step:1279/2160 train_time:57848ms step_avg:45.23ms
step:1280/2160 train_time:57906ms step_avg:45.24ms
step:1281/2160 train_time:57965ms step_avg:45.25ms
step:1282/2160 train_time:58023ms step_avg:45.26ms
step:1283/2160 train_time:58083ms step_avg:45.27ms
step:1284/2160 train_time:58142ms step_avg:45.28ms
step:1285/2160 train_time:58203ms step_avg:45.29ms
step:1286/2160 train_time:58262ms step_avg:45.30ms
step:1287/2160 train_time:58324ms step_avg:45.32ms
step:1288/2160 train_time:58383ms step_avg:45.33ms
step:1289/2160 train_time:58445ms step_avg:45.34ms
step:1290/2160 train_time:58503ms step_avg:45.35ms
step:1291/2160 train_time:58565ms step_avg:45.36ms
step:1292/2160 train_time:58624ms step_avg:45.37ms
step:1293/2160 train_time:58684ms step_avg:45.39ms
step:1294/2160 train_time:58742ms step_avg:45.40ms
step:1295/2160 train_time:58803ms step_avg:45.41ms
step:1296/2160 train_time:58860ms step_avg:45.42ms
step:1297/2160 train_time:58920ms step_avg:45.43ms
step:1298/2160 train_time:58979ms step_avg:45.44ms
step:1299/2160 train_time:59038ms step_avg:45.45ms
step:1300/2160 train_time:59097ms step_avg:45.46ms
step:1301/2160 train_time:59158ms step_avg:45.47ms
step:1302/2160 train_time:59217ms step_avg:45.48ms
step:1303/2160 train_time:59279ms step_avg:45.49ms
step:1304/2160 train_time:59338ms step_avg:45.50ms
step:1305/2160 train_time:59399ms step_avg:45.52ms
step:1306/2160 train_time:59459ms step_avg:45.53ms
step:1307/2160 train_time:59521ms step_avg:45.54ms
step:1308/2160 train_time:59580ms step_avg:45.55ms
step:1309/2160 train_time:59641ms step_avg:45.56ms
step:1310/2160 train_time:59700ms step_avg:45.57ms
step:1311/2160 train_time:59760ms step_avg:45.58ms
step:1312/2160 train_time:59818ms step_avg:45.59ms
step:1313/2160 train_time:59879ms step_avg:45.60ms
step:1314/2160 train_time:59937ms step_avg:45.61ms
step:1315/2160 train_time:59998ms step_avg:45.63ms
step:1316/2160 train_time:60056ms step_avg:45.64ms
step:1317/2160 train_time:60117ms step_avg:45.65ms
step:1318/2160 train_time:60175ms step_avg:45.66ms
step:1319/2160 train_time:60236ms step_avg:45.67ms
step:1320/2160 train_time:60295ms step_avg:45.68ms
step:1321/2160 train_time:60356ms step_avg:45.69ms
step:1322/2160 train_time:60415ms step_avg:45.70ms
step:1323/2160 train_time:60476ms step_avg:45.71ms
step:1324/2160 train_time:60535ms step_avg:45.72ms
step:1325/2160 train_time:60596ms step_avg:45.73ms
step:1326/2160 train_time:60655ms step_avg:45.74ms
step:1327/2160 train_time:60715ms step_avg:45.75ms
step:1328/2160 train_time:60774ms step_avg:45.76ms
step:1329/2160 train_time:60835ms step_avg:45.77ms
step:1330/2160 train_time:60893ms step_avg:45.78ms
step:1331/2160 train_time:60953ms step_avg:45.80ms
step:1332/2160 train_time:61012ms step_avg:45.80ms
step:1333/2160 train_time:61072ms step_avg:45.82ms
step:1334/2160 train_time:61131ms step_avg:45.83ms
step:1335/2160 train_time:61191ms step_avg:45.84ms
step:1336/2160 train_time:61249ms step_avg:45.85ms
step:1337/2160 train_time:61310ms step_avg:45.86ms
step:1338/2160 train_time:61369ms step_avg:45.87ms
step:1339/2160 train_time:61430ms step_avg:45.88ms
step:1340/2160 train_time:61489ms step_avg:45.89ms
step:1341/2160 train_time:61550ms step_avg:45.90ms
step:1342/2160 train_time:61609ms step_avg:45.91ms
step:1343/2160 train_time:61670ms step_avg:45.92ms
step:1344/2160 train_time:61731ms step_avg:45.93ms
step:1345/2160 train_time:61790ms step_avg:45.94ms
step:1346/2160 train_time:61848ms step_avg:45.95ms
step:1347/2160 train_time:61909ms step_avg:45.96ms
step:1348/2160 train_time:61967ms step_avg:45.97ms
step:1349/2160 train_time:62028ms step_avg:45.98ms
step:1350/2160 train_time:62087ms step_avg:45.99ms
step:1351/2160 train_time:62147ms step_avg:46.00ms
step:1352/2160 train_time:62206ms step_avg:46.01ms
step:1353/2160 train_time:62267ms step_avg:46.02ms
step:1354/2160 train_time:62326ms step_avg:46.03ms
step:1355/2160 train_time:62386ms step_avg:46.04ms
step:1356/2160 train_time:62445ms step_avg:46.05ms
step:1357/2160 train_time:62506ms step_avg:46.06ms
step:1358/2160 train_time:62564ms step_avg:46.07ms
step:1359/2160 train_time:62625ms step_avg:46.08ms
step:1360/2160 train_time:62684ms step_avg:46.09ms
step:1361/2160 train_time:62745ms step_avg:46.10ms
step:1362/2160 train_time:62803ms step_avg:46.11ms
step:1363/2160 train_time:62864ms step_avg:46.12ms
step:1364/2160 train_time:62923ms step_avg:46.13ms
step:1365/2160 train_time:62983ms step_avg:46.14ms
step:1366/2160 train_time:63042ms step_avg:46.15ms
step:1367/2160 train_time:63102ms step_avg:46.16ms
step:1368/2160 train_time:63160ms step_avg:46.17ms
step:1369/2160 train_time:63222ms step_avg:46.18ms
step:1370/2160 train_time:63279ms step_avg:46.19ms
step:1371/2160 train_time:63340ms step_avg:46.20ms
step:1372/2160 train_time:63399ms step_avg:46.21ms
step:1373/2160 train_time:63460ms step_avg:46.22ms
step:1374/2160 train_time:63519ms step_avg:46.23ms
step:1375/2160 train_time:63579ms step_avg:46.24ms
step:1376/2160 train_time:63638ms step_avg:46.25ms
step:1377/2160 train_time:63699ms step_avg:46.26ms
step:1378/2160 train_time:63759ms step_avg:46.27ms
step:1379/2160 train_time:63819ms step_avg:46.28ms
step:1380/2160 train_time:63877ms step_avg:46.29ms
step:1381/2160 train_time:63938ms step_avg:46.30ms
step:1382/2160 train_time:63997ms step_avg:46.31ms
step:1383/2160 train_time:64057ms step_avg:46.32ms
step:1384/2160 train_time:64116ms step_avg:46.33ms
step:1385/2160 train_time:64177ms step_avg:46.34ms
step:1386/2160 train_time:64235ms step_avg:46.35ms
step:1387/2160 train_time:64296ms step_avg:46.36ms
step:1388/2160 train_time:64355ms step_avg:46.37ms
step:1389/2160 train_time:64415ms step_avg:46.38ms
step:1390/2160 train_time:64474ms step_avg:46.38ms
step:1391/2160 train_time:64535ms step_avg:46.39ms
step:1392/2160 train_time:64594ms step_avg:46.40ms
step:1393/2160 train_time:64655ms step_avg:46.41ms
step:1394/2160 train_time:64714ms step_avg:46.42ms
step:1395/2160 train_time:64775ms step_avg:46.43ms
step:1396/2160 train_time:64834ms step_avg:46.44ms
step:1397/2160 train_time:64894ms step_avg:46.45ms
step:1398/2160 train_time:64953ms step_avg:46.46ms
step:1399/2160 train_time:65013ms step_avg:46.47ms
step:1400/2160 train_time:65072ms step_avg:46.48ms
step:1401/2160 train_time:65133ms step_avg:46.49ms
step:1402/2160 train_time:65191ms step_avg:46.50ms
step:1403/2160 train_time:65252ms step_avg:46.51ms
step:1404/2160 train_time:65310ms step_avg:46.52ms
step:1405/2160 train_time:65371ms step_avg:46.53ms
step:1406/2160 train_time:65430ms step_avg:46.54ms
step:1407/2160 train_time:65491ms step_avg:46.55ms
step:1408/2160 train_time:65550ms step_avg:46.56ms
step:1409/2160 train_time:65610ms step_avg:46.57ms
step:1410/2160 train_time:65669ms step_avg:46.57ms
step:1411/2160 train_time:65730ms step_avg:46.58ms
step:1412/2160 train_time:65788ms step_avg:46.59ms
step:1413/2160 train_time:65849ms step_avg:46.60ms
step:1414/2160 train_time:65908ms step_avg:46.61ms
step:1415/2160 train_time:65969ms step_avg:46.62ms
step:1416/2160 train_time:66056ms step_avg:46.65ms
step:1417/2160 train_time:66144ms step_avg:46.68ms
step:1418/2160 train_time:66231ms step_avg:46.71ms
step:1419/2160 train_time:66318ms step_avg:46.74ms
step:1420/2160 train_time:66404ms step_avg:46.76ms
step:1421/2160 train_time:66492ms step_avg:46.79ms
step:1422/2160 train_time:66579ms step_avg:46.82ms
step:1423/2160 train_time:66667ms step_avg:46.85ms
step:1424/2160 train_time:66754ms step_avg:46.88ms
step:1425/2160 train_time:66843ms step_avg:46.91ms
step:1426/2160 train_time:66930ms step_avg:46.94ms
step:1427/2160 train_time:67017ms step_avg:46.96ms
step:1428/2160 train_time:67103ms step_avg:46.99ms
step:1429/2160 train_time:67190ms step_avg:47.02ms
step:1430/2160 train_time:67278ms step_avg:47.05ms
step:1431/2160 train_time:67367ms step_avg:47.08ms
step:1432/2160 train_time:67453ms step_avg:47.10ms
step:1433/2160 train_time:67541ms step_avg:47.13ms
step:1434/2160 train_time:67628ms step_avg:47.16ms
step:1435/2160 train_time:67716ms step_avg:47.19ms
step:1436/2160 train_time:67803ms step_avg:47.22ms
step:1437/2160 train_time:67892ms step_avg:47.25ms
step:1438/2160 train_time:67978ms step_avg:47.27ms
step:1439/2160 train_time:68066ms step_avg:47.30ms
step:1440/2160 train_time:68152ms step_avg:47.33ms
step:1441/2160 train_time:68240ms step_avg:47.36ms
step:1442/2160 train_time:68326ms step_avg:47.38ms
step:1443/2160 train_time:68413ms step_avg:47.41ms
step:1444/2160 train_time:68500ms step_avg:47.44ms
step:1445/2160 train_time:68588ms step_avg:47.47ms
step:1446/2160 train_time:68674ms step_avg:47.49ms
step:1447/2160 train_time:68763ms step_avg:47.52ms
step:1448/2160 train_time:68849ms step_avg:47.55ms
step:1449/2160 train_time:68938ms step_avg:47.58ms
step:1450/2160 train_time:69024ms step_avg:47.60ms
step:1451/2160 train_time:69113ms step_avg:47.63ms
step:1452/2160 train_time:69198ms step_avg:47.66ms
step:1453/2160 train_time:69286ms step_avg:47.68ms
step:1454/2160 train_time:69372ms step_avg:47.71ms
step:1455/2160 train_time:69461ms step_avg:47.74ms
step:1456/2160 train_time:69548ms step_avg:47.77ms
step:1457/2160 train_time:69635ms step_avg:47.79ms
step:1458/2160 train_time:69723ms step_avg:47.82ms
step:1459/2160 train_time:69811ms step_avg:47.85ms
step:1460/2160 train_time:69897ms step_avg:47.87ms
step:1461/2160 train_time:69986ms step_avg:47.90ms
step:1462/2160 train_time:70073ms step_avg:47.93ms
step:1463/2160 train_time:70161ms step_avg:47.96ms
step:1464/2160 train_time:70246ms step_avg:47.98ms
step:1465/2160 train_time:70335ms step_avg:48.01ms
step:1466/2160 train_time:70421ms step_avg:48.04ms
step:1467/2160 train_time:70510ms step_avg:48.06ms
step:1468/2160 train_time:70597ms step_avg:48.09ms
step:1469/2160 train_time:70686ms step_avg:48.12ms
step:1470/2160 train_time:70772ms step_avg:48.14ms
step:1471/2160 train_time:70861ms step_avg:48.17ms
step:1472/2160 train_time:70947ms step_avg:48.20ms
step:1473/2160 train_time:71035ms step_avg:48.22ms
step:1474/2160 train_time:71121ms step_avg:48.25ms
step:1475/2160 train_time:71209ms step_avg:48.28ms
step:1476/2160 train_time:71295ms step_avg:48.30ms
step:1477/2160 train_time:71383ms step_avg:48.33ms
step:1478/2160 train_time:71470ms step_avg:48.36ms
step:1479/2160 train_time:71558ms step_avg:48.38ms
step:1480/2160 train_time:71644ms step_avg:48.41ms
step:1481/2160 train_time:71732ms step_avg:48.44ms
step:1482/2160 train_time:71819ms step_avg:48.46ms
step:1483/2160 train_time:71908ms step_avg:48.49ms
step:1484/2160 train_time:71994ms step_avg:48.51ms
step:1485/2160 train_time:72083ms step_avg:48.54ms
step:1486/2160 train_time:72169ms step_avg:48.57ms
step:1487/2160 train_time:72257ms step_avg:48.59ms
step:1488/2160 train_time:72343ms step_avg:48.62ms
step:1489/2160 train_time:72431ms step_avg:48.64ms
step:1490/2160 train_time:72517ms step_avg:48.67ms
step:1491/2160 train_time:72606ms step_avg:48.70ms
step:1492/2160 train_time:72692ms step_avg:48.72ms
step:1493/2160 train_time:72781ms step_avg:48.75ms
step:1494/2160 train_time:72868ms step_avg:48.77ms
step:1495/2160 train_time:72955ms step_avg:48.80ms
step:1496/2160 train_time:73042ms step_avg:48.82ms
step:1497/2160 train_time:73130ms step_avg:48.85ms
step:1498/2160 train_time:73216ms step_avg:48.88ms
step:1499/2160 train_time:73305ms step_avg:48.90ms
step:1500/2160 train_time:73391ms step_avg:48.93ms
step:1500/2160 val_loss:3.4748 train_time:73479ms step_avg:48.99ms
step:1501/2160 train_time:73503ms step_avg:48.97ms
step:1502/2160 train_time:73571ms step_avg:48.98ms
step:1503/2160 train_time:73664ms step_avg:49.01ms
step:1504/2160 train_time:73752ms step_avg:49.04ms
step:1505/2160 train_time:73841ms step_avg:49.06ms
step:1506/2160 train_time:73927ms step_avg:49.09ms
step:1507/2160 train_time:74013ms step_avg:49.11ms
step:1508/2160 train_time:74098ms step_avg:49.14ms
step:1509/2160 train_time:74185ms step_avg:49.16ms
step:1510/2160 train_time:74269ms step_avg:49.19ms
step:1511/2160 train_time:74356ms step_avg:49.21ms
step:1512/2160 train_time:74442ms step_avg:49.23ms
step:1513/2160 train_time:74533ms step_avg:49.26ms
step:1514/2160 train_time:74622ms step_avg:49.29ms
step:1515/2160 train_time:74712ms step_avg:49.31ms
step:1516/2160 train_time:74799ms step_avg:49.34ms
step:1517/2160 train_time:74887ms step_avg:49.37ms
step:1518/2160 train_time:74973ms step_avg:49.39ms
step:1519/2160 train_time:75060ms step_avg:49.41ms
step:1520/2160 train_time:75146ms step_avg:49.44ms
step:1521/2160 train_time:75233ms step_avg:49.46ms
step:1522/2160 train_time:75318ms step_avg:49.49ms
step:1523/2160 train_time:75406ms step_avg:49.51ms
step:1524/2160 train_time:75493ms step_avg:49.54ms
step:1525/2160 train_time:75582ms step_avg:49.56ms
step:1526/2160 train_time:75671ms step_avg:49.59ms
step:1527/2160 train_time:75760ms step_avg:49.61ms
step:1528/2160 train_time:75847ms step_avg:49.64ms
step:1529/2160 train_time:75935ms step_avg:49.66ms
step:1530/2160 train_time:76021ms step_avg:49.69ms
step:1531/2160 train_time:76109ms step_avg:49.71ms
step:1532/2160 train_time:76194ms step_avg:49.73ms
step:1533/2160 train_time:76282ms step_avg:49.76ms
step:1534/2160 train_time:76368ms step_avg:49.78ms
step:1535/2160 train_time:76456ms step_avg:49.81ms
step:1536/2160 train_time:76542ms step_avg:49.83ms
step:1537/2160 train_time:76633ms step_avg:49.86ms
step:1538/2160 train_time:76720ms step_avg:49.88ms
step:1539/2160 train_time:76810ms step_avg:49.91ms
step:1540/2160 train_time:76895ms step_avg:49.93ms
step:1541/2160 train_time:76984ms step_avg:49.96ms
step:1542/2160 train_time:77070ms step_avg:49.98ms
step:1543/2160 train_time:77158ms step_avg:50.00ms
step:1544/2160 train_time:77243ms step_avg:50.03ms
step:1545/2160 train_time:77331ms step_avg:50.05ms
step:1546/2160 train_time:77418ms step_avg:50.08ms
step:1547/2160 train_time:77507ms step_avg:50.10ms
step:1548/2160 train_time:77593ms step_avg:50.12ms
step:1549/2160 train_time:77682ms step_avg:50.15ms
step:1550/2160 train_time:77769ms step_avg:50.17ms
step:1551/2160 train_time:77857ms step_avg:50.20ms
step:1552/2160 train_time:77943ms step_avg:50.22ms
step:1553/2160 train_time:78031ms step_avg:50.25ms
step:1554/2160 train_time:78117ms step_avg:50.27ms
step:1555/2160 train_time:78205ms step_avg:50.29ms
step:1556/2160 train_time:78291ms step_avg:50.32ms
step:1557/2160 train_time:78380ms step_avg:50.34ms
step:1558/2160 train_time:78467ms step_avg:50.36ms
step:1559/2160 train_time:78555ms step_avg:50.39ms
step:1560/2160 train_time:78642ms step_avg:50.41ms
step:1561/2160 train_time:78731ms step_avg:50.44ms
step:1562/2160 train_time:78818ms step_avg:50.46ms
step:1563/2160 train_time:78906ms step_avg:50.48ms
step:1564/2160 train_time:78992ms step_avg:50.51ms
step:1565/2160 train_time:79080ms step_avg:50.53ms
step:1566/2160 train_time:79166ms step_avg:50.55ms
step:1567/2160 train_time:79254ms step_avg:50.58ms
step:1568/2160 train_time:79340ms step_avg:50.60ms
step:1569/2160 train_time:79428ms step_avg:50.62ms
step:1570/2160 train_time:79514ms step_avg:50.65ms
step:1571/2160 train_time:79603ms step_avg:50.67ms
step:1572/2160 train_time:79690ms step_avg:50.69ms
step:1573/2160 train_time:79778ms step_avg:50.72ms
step:1574/2160 train_time:79864ms step_avg:50.74ms
step:1575/2160 train_time:79952ms step_avg:50.76ms
step:1576/2160 train_time:80038ms step_avg:50.79ms
step:1577/2160 train_time:80126ms step_avg:50.81ms
step:1578/2160 train_time:80212ms step_avg:50.83ms
step:1579/2160 train_time:80300ms step_avg:50.86ms
step:1580/2160 train_time:80386ms step_avg:50.88ms
step:1581/2160 train_time:80474ms step_avg:50.90ms
step:1582/2160 train_time:80561ms step_avg:50.92ms
step:1583/2160 train_time:80650ms step_avg:50.95ms
step:1584/2160 train_time:80737ms step_avg:50.97ms
step:1585/2160 train_time:80825ms step_avg:50.99ms
step:1586/2160 train_time:80912ms step_avg:51.02ms
step:1587/2160 train_time:81001ms step_avg:51.04ms
step:1588/2160 train_time:81087ms step_avg:51.06ms
step:1589/2160 train_time:81175ms step_avg:51.09ms
step:1590/2160 train_time:81261ms step_avg:51.11ms
step:1591/2160 train_time:81350ms step_avg:51.13ms
step:1592/2160 train_time:81435ms step_avg:51.15ms
step:1593/2160 train_time:81524ms step_avg:51.18ms
step:1594/2160 train_time:81611ms step_avg:51.20ms
step:1595/2160 train_time:81698ms step_avg:51.22ms
step:1596/2160 train_time:81786ms step_avg:51.24ms
step:1597/2160 train_time:81874ms step_avg:51.27ms
step:1598/2160 train_time:81960ms step_avg:51.29ms
step:1599/2160 train_time:82048ms step_avg:51.31ms
step:1600/2160 train_time:82134ms step_avg:51.33ms
step:1601/2160 train_time:82222ms step_avg:51.36ms
step:1602/2160 train_time:82308ms step_avg:51.38ms
step:1603/2160 train_time:82396ms step_avg:51.40ms
step:1604/2160 train_time:82483ms step_avg:51.42ms
step:1605/2160 train_time:82571ms step_avg:51.45ms
step:1606/2160 train_time:82657ms step_avg:51.47ms
step:1607/2160 train_time:82745ms step_avg:51.49ms
step:1608/2160 train_time:82831ms step_avg:51.51ms
step:1609/2160 train_time:82919ms step_avg:51.53ms
step:1610/2160 train_time:83006ms step_avg:51.56ms
step:1611/2160 train_time:83094ms step_avg:51.58ms
step:1612/2160 train_time:83180ms step_avg:51.60ms
step:1613/2160 train_time:83268ms step_avg:51.62ms
step:1614/2160 train_time:83354ms step_avg:51.64ms
step:1615/2160 train_time:83443ms step_avg:51.67ms
step:1616/2160 train_time:83530ms step_avg:51.69ms
step:1617/2160 train_time:83619ms step_avg:51.71ms
step:1618/2160 train_time:83706ms step_avg:51.73ms
step:1619/2160 train_time:83794ms step_avg:51.76ms
step:1620/2160 train_time:83879ms step_avg:51.78ms
step:1621/2160 train_time:83967ms step_avg:51.80ms
step:1622/2160 train_time:84053ms step_avg:51.82ms
step:1623/2160 train_time:84143ms step_avg:51.84ms
step:1624/2160 train_time:84229ms step_avg:51.87ms
step:1625/2160 train_time:84317ms step_avg:51.89ms
step:1626/2160 train_time:84404ms step_avg:51.91ms
step:1627/2160 train_time:84492ms step_avg:51.93ms
step:1628/2160 train_time:84578ms step_avg:51.95ms
step:1629/2160 train_time:84667ms step_avg:51.97ms
step:1630/2160 train_time:84753ms step_avg:52.00ms
step:1631/2160 train_time:84841ms step_avg:52.02ms
step:1632/2160 train_time:84928ms step_avg:52.04ms
step:1633/2160 train_time:85016ms step_avg:52.06ms
step:1634/2160 train_time:85102ms step_avg:52.08ms
step:1635/2160 train_time:85190ms step_avg:52.10ms
step:1636/2160 train_time:85277ms step_avg:52.13ms
step:1637/2160 train_time:85365ms step_avg:52.15ms
step:1638/2160 train_time:85451ms step_avg:52.17ms
step:1639/2160 train_time:85539ms step_avg:52.19ms
step:1640/2160 train_time:85625ms step_avg:52.21ms
step:1641/2160 train_time:85714ms step_avg:52.23ms
step:1642/2160 train_time:85800ms step_avg:52.25ms
step:1643/2160 train_time:85888ms step_avg:52.28ms
step:1644/2160 train_time:85974ms step_avg:52.30ms
step:1645/2160 train_time:86063ms step_avg:52.32ms
step:1646/2160 train_time:86150ms step_avg:52.34ms
step:1647/2160 train_time:86237ms step_avg:52.36ms
step:1648/2160 train_time:86326ms step_avg:52.38ms
step:1649/2160 train_time:86415ms step_avg:52.40ms
step:1650/2160 train_time:86501ms step_avg:52.43ms
step:1651/2160 train_time:86590ms step_avg:52.45ms
step:1652/2160 train_time:86675ms step_avg:52.47ms
step:1653/2160 train_time:86764ms step_avg:52.49ms
step:1654/2160 train_time:86850ms step_avg:52.51ms
step:1655/2160 train_time:86939ms step_avg:52.53ms
step:1656/2160 train_time:87026ms step_avg:52.55ms
step:1657/2160 train_time:87113ms step_avg:52.57ms
step:1658/2160 train_time:87200ms step_avg:52.59ms
step:1659/2160 train_time:87288ms step_avg:52.61ms
step:1660/2160 train_time:87374ms step_avg:52.64ms
step:1661/2160 train_time:87462ms step_avg:52.66ms
step:1662/2160 train_time:87549ms step_avg:52.68ms
step:1663/2160 train_time:87637ms step_avg:52.70ms
step:1664/2160 train_time:87724ms step_avg:52.72ms
step:1665/2160 train_time:87812ms step_avg:52.74ms
step:1666/2160 train_time:87899ms step_avg:52.76ms
step:1667/2160 train_time:87987ms step_avg:52.78ms
step:1668/2160 train_time:88073ms step_avg:52.80ms
step:1669/2160 train_time:88162ms step_avg:52.82ms
step:1670/2160 train_time:88249ms step_avg:52.84ms
step:1671/2160 train_time:88337ms step_avg:52.86ms
step:1672/2160 train_time:88423ms step_avg:52.88ms
step:1673/2160 train_time:88512ms step_avg:52.91ms
step:1674/2160 train_time:88598ms step_avg:52.93ms
step:1675/2160 train_time:88686ms step_avg:52.95ms
step:1676/2160 train_time:88773ms step_avg:52.97ms
step:1677/2160 train_time:88861ms step_avg:52.99ms
step:1678/2160 train_time:88947ms step_avg:53.01ms
step:1679/2160 train_time:89036ms step_avg:53.03ms
step:1680/2160 train_time:89123ms step_avg:53.05ms
step:1681/2160 train_time:89212ms step_avg:53.07ms
step:1682/2160 train_time:89299ms step_avg:53.09ms
step:1683/2160 train_time:89386ms step_avg:53.11ms
step:1684/2160 train_time:89473ms step_avg:53.13ms
step:1685/2160 train_time:89561ms step_avg:53.15ms
step:1686/2160 train_time:89647ms step_avg:53.17ms
step:1687/2160 train_time:89735ms step_avg:53.19ms
step:1688/2160 train_time:89822ms step_avg:53.21ms
step:1689/2160 train_time:89910ms step_avg:53.23ms
step:1690/2160 train_time:89996ms step_avg:53.25ms
step:1691/2160 train_time:90085ms step_avg:53.27ms
step:1692/2160 train_time:90171ms step_avg:53.29ms
step:1693/2160 train_time:90259ms step_avg:53.31ms
step:1694/2160 train_time:90346ms step_avg:53.33ms
step:1695/2160 train_time:90435ms step_avg:53.35ms
step:1696/2160 train_time:90522ms step_avg:53.37ms
step:1697/2160 train_time:90610ms step_avg:53.39ms
step:1698/2160 train_time:90697ms step_avg:53.41ms
step:1699/2160 train_time:90784ms step_avg:53.43ms
step:1700/2160 train_time:90871ms step_avg:53.45ms
step:1701/2160 train_time:90958ms step_avg:53.47ms
step:1702/2160 train_time:91045ms step_avg:53.49ms
step:1703/2160 train_time:91134ms step_avg:53.51ms
step:1704/2160 train_time:91220ms step_avg:53.53ms
step:1705/2160 train_time:91308ms step_avg:53.55ms
step:1706/2160 train_time:91396ms step_avg:53.57ms
step:1707/2160 train_time:91485ms step_avg:53.59ms
step:1708/2160 train_time:91571ms step_avg:53.61ms
step:1709/2160 train_time:91659ms step_avg:53.63ms
step:1710/2160 train_time:91746ms step_avg:53.65ms
step:1711/2160 train_time:91834ms step_avg:53.67ms
step:1712/2160 train_time:91919ms step_avg:53.69ms
step:1713/2160 train_time:92007ms step_avg:53.71ms
step:1714/2160 train_time:92094ms step_avg:53.73ms
step:1715/2160 train_time:92182ms step_avg:53.75ms
step:1716/2160 train_time:92269ms step_avg:53.77ms
step:1717/2160 train_time:92357ms step_avg:53.79ms
step:1718/2160 train_time:92443ms step_avg:53.81ms
step:1719/2160 train_time:92532ms step_avg:53.83ms
step:1720/2160 train_time:92618ms step_avg:53.85ms
step:1721/2160 train_time:92706ms step_avg:53.87ms
step:1722/2160 train_time:92793ms step_avg:53.89ms
step:1723/2160 train_time:92882ms step_avg:53.91ms
step:1724/2160 train_time:92968ms step_avg:53.93ms
step:1725/2160 train_time:93056ms step_avg:53.95ms
step:1726/2160 train_time:93142ms step_avg:53.96ms
step:1727/2160 train_time:93231ms step_avg:53.98ms
step:1728/2160 train_time:93317ms step_avg:54.00ms
step:1729/2160 train_time:93407ms step_avg:54.02ms
step:1730/2160 train_time:93493ms step_avg:54.04ms
step:1731/2160 train_time:93582ms step_avg:54.06ms
step:1732/2160 train_time:93668ms step_avg:54.08ms
step:1733/2160 train_time:93756ms step_avg:54.10ms
step:1734/2160 train_time:93844ms step_avg:54.12ms
step:1735/2160 train_time:93932ms step_avg:54.14ms
step:1736/2160 train_time:94018ms step_avg:54.16ms
step:1737/2160 train_time:94107ms step_avg:54.18ms
step:1738/2160 train_time:94193ms step_avg:54.20ms
step:1739/2160 train_time:94282ms step_avg:54.22ms
step:1740/2160 train_time:94369ms step_avg:54.23ms
step:1741/2160 train_time:94458ms step_avg:54.25ms
step:1742/2160 train_time:94544ms step_avg:54.27ms
step:1743/2160 train_time:94633ms step_avg:54.29ms
step:1744/2160 train_time:94719ms step_avg:54.31ms
step:1745/2160 train_time:94807ms step_avg:54.33ms
step:1746/2160 train_time:94892ms step_avg:54.35ms
step:1747/2160 train_time:94980ms step_avg:54.37ms
step:1748/2160 train_time:95067ms step_avg:54.39ms
step:1749/2160 train_time:95155ms step_avg:54.41ms
step:1750/2160 train_time:95241ms step_avg:54.42ms
step:1750/2160 val_loss:3.3838 train_time:95330ms step_avg:54.47ms
step:1751/2160 train_time:95353ms step_avg:54.46ms
step:1752/2160 train_time:95421ms step_avg:54.46ms
step:1753/2160 train_time:95517ms step_avg:54.49ms
step:1754/2160 train_time:95604ms step_avg:54.51ms
step:1755/2160 train_time:95692ms step_avg:54.53ms
step:1756/2160 train_time:95778ms step_avg:54.54ms
step:1757/2160 train_time:95865ms step_avg:54.56ms
step:1758/2160 train_time:95950ms step_avg:54.58ms
step:1759/2160 train_time:96037ms step_avg:54.60ms
step:1760/2160 train_time:96122ms step_avg:54.61ms
step:1761/2160 train_time:96210ms step_avg:54.63ms
step:1762/2160 train_time:96296ms step_avg:54.65ms
step:1763/2160 train_time:96386ms step_avg:54.67ms
step:1764/2160 train_time:96474ms step_avg:54.69ms
step:1765/2160 train_time:96567ms step_avg:54.71ms
step:1766/2160 train_time:96655ms step_avg:54.73ms
step:1767/2160 train_time:96742ms step_avg:54.75ms
step:1768/2160 train_time:96828ms step_avg:54.77ms
step:1769/2160 train_time:96915ms step_avg:54.79ms
step:1770/2160 train_time:97000ms step_avg:54.80ms
step:1771/2160 train_time:97087ms step_avg:54.82ms
step:1772/2160 train_time:97173ms step_avg:54.84ms
step:1773/2160 train_time:97261ms step_avg:54.86ms
step:1774/2160 train_time:97348ms step_avg:54.87ms
step:1775/2160 train_time:97438ms step_avg:54.89ms
step:1776/2160 train_time:97525ms step_avg:54.91ms
step:1777/2160 train_time:97614ms step_avg:54.93ms
step:1778/2160 train_time:97701ms step_avg:54.95ms
step:1779/2160 train_time:97789ms step_avg:54.97ms
step:1780/2160 train_time:97875ms step_avg:54.99ms
step:1781/2160 train_time:97963ms step_avg:55.00ms
step:1782/2160 train_time:98048ms step_avg:55.02ms
step:1783/2160 train_time:98136ms step_avg:55.04ms
step:1784/2160 train_time:98221ms step_avg:55.06ms
step:1785/2160 train_time:98311ms step_avg:55.08ms
step:1786/2160 train_time:98398ms step_avg:55.09ms
step:1787/2160 train_time:98488ms step_avg:55.11ms
step:1788/2160 train_time:98574ms step_avg:55.13ms
step:1789/2160 train_time:98664ms step_avg:55.15ms
step:1790/2160 train_time:98750ms step_avg:55.17ms
step:1791/2160 train_time:98838ms step_avg:55.19ms
step:1792/2160 train_time:98924ms step_avg:55.20ms
step:1793/2160 train_time:99012ms step_avg:55.22ms
step:1794/2160 train_time:99098ms step_avg:55.24ms
step:1795/2160 train_time:99186ms step_avg:55.26ms
step:1796/2160 train_time:99272ms step_avg:55.27ms
step:1797/2160 train_time:99361ms step_avg:55.29ms
step:1798/2160 train_time:99449ms step_avg:55.31ms
step:1799/2160 train_time:99537ms step_avg:55.33ms
step:1800/2160 train_time:99623ms step_avg:55.35ms
step:1801/2160 train_time:99713ms step_avg:55.37ms
step:1802/2160 train_time:99799ms step_avg:55.38ms
step:1803/2160 train_time:99888ms step_avg:55.40ms
step:1804/2160 train_time:99974ms step_avg:55.42ms
step:1805/2160 train_time:100062ms step_avg:55.44ms
step:1806/2160 train_time:100148ms step_avg:55.45ms
step:1807/2160 train_time:100235ms step_avg:55.47ms
step:1808/2160 train_time:100322ms step_avg:55.49ms
step:1809/2160 train_time:100412ms step_avg:55.51ms
step:1810/2160 train_time:100498ms step_avg:55.52ms
step:1811/2160 train_time:100587ms step_avg:55.54ms
step:1812/2160 train_time:100674ms step_avg:55.56ms
step:1813/2160 train_time:100763ms step_avg:55.58ms
step:1814/2160 train_time:100849ms step_avg:55.59ms
step:1815/2160 train_time:100937ms step_avg:55.61ms
step:1816/2160 train_time:101023ms step_avg:55.63ms
step:1817/2160 train_time:101111ms step_avg:55.65ms
step:1818/2160 train_time:101197ms step_avg:55.66ms
step:1819/2160 train_time:101286ms step_avg:55.68ms
step:1820/2160 train_time:101373ms step_avg:55.70ms
step:1821/2160 train_time:101462ms step_avg:55.72ms
step:1822/2160 train_time:101549ms step_avg:55.73ms
step:1823/2160 train_time:101638ms step_avg:55.75ms
step:1824/2160 train_time:101724ms step_avg:55.77ms
step:1825/2160 train_time:101812ms step_avg:55.79ms
step:1826/2160 train_time:101898ms step_avg:55.80ms
step:1827/2160 train_time:101987ms step_avg:55.82ms
step:1828/2160 train_time:102074ms step_avg:55.84ms
step:1829/2160 train_time:102161ms step_avg:55.86ms
step:1830/2160 train_time:102248ms step_avg:55.87ms
step:1831/2160 train_time:102337ms step_avg:55.89ms
step:1832/2160 train_time:102423ms step_avg:55.91ms
step:1833/2160 train_time:102512ms step_avg:55.93ms
step:1834/2160 train_time:102599ms step_avg:55.94ms
step:1835/2160 train_time:102689ms step_avg:55.96ms
step:1836/2160 train_time:102775ms step_avg:55.98ms
step:1837/2160 train_time:102863ms step_avg:55.99ms
step:1838/2160 train_time:102949ms step_avg:56.01ms
step:1839/2160 train_time:103038ms step_avg:56.03ms
step:1840/2160 train_time:103124ms step_avg:56.05ms
step:1841/2160 train_time:103213ms step_avg:56.06ms
step:1842/2160 train_time:103299ms step_avg:56.08ms
step:1843/2160 train_time:103388ms step_avg:56.10ms
step:1844/2160 train_time:103474ms step_avg:56.11ms
step:1845/2160 train_time:103563ms step_avg:56.13ms
step:1846/2160 train_time:103649ms step_avg:56.15ms
step:1847/2160 train_time:103738ms step_avg:56.17ms
step:1848/2160 train_time:103824ms step_avg:56.18ms
step:1849/2160 train_time:103912ms step_avg:56.20ms
step:1850/2160 train_time:103999ms step_avg:56.22ms
step:1851/2160 train_time:104086ms step_avg:56.23ms
step:1852/2160 train_time:104174ms step_avg:56.25ms
step:1853/2160 train_time:104262ms step_avg:56.27ms
step:1854/2160 train_time:104348ms step_avg:56.28ms
step:1855/2160 train_time:104437ms step_avg:56.30ms
step:1856/2160 train_time:104524ms step_avg:56.32ms
step:1857/2160 train_time:104613ms step_avg:56.33ms
step:1858/2160 train_time:104700ms step_avg:56.35ms
step:1859/2160 train_time:104789ms step_avg:56.37ms
step:1860/2160 train_time:104876ms step_avg:56.38ms
step:1861/2160 train_time:104964ms step_avg:56.40ms
step:1862/2160 train_time:105051ms step_avg:56.42ms
step:1863/2160 train_time:105139ms step_avg:56.44ms
step:1864/2160 train_time:105225ms step_avg:56.45ms
step:1865/2160 train_time:105313ms step_avg:56.47ms
step:1866/2160 train_time:105400ms step_avg:56.48ms
step:1867/2160 train_time:105489ms step_avg:56.50ms
step:1868/2160 train_time:105576ms step_avg:56.52ms
step:1869/2160 train_time:105664ms step_avg:56.54ms
step:1870/2160 train_time:105750ms step_avg:56.55ms
step:1871/2160 train_time:105838ms step_avg:56.57ms
step:1872/2160 train_time:105925ms step_avg:56.58ms
step:1873/2160 train_time:106013ms step_avg:56.60ms
step:1874/2160 train_time:106099ms step_avg:56.62ms
step:1875/2160 train_time:106188ms step_avg:56.63ms
step:1876/2160 train_time:106275ms step_avg:56.65ms
step:1877/2160 train_time:106363ms step_avg:56.67ms
step:1878/2160 train_time:106450ms step_avg:56.68ms
step:1879/2160 train_time:106538ms step_avg:56.70ms
step:1880/2160 train_time:106624ms step_avg:56.72ms
step:1881/2160 train_time:106713ms step_avg:56.73ms
step:1882/2160 train_time:106800ms step_avg:56.75ms
step:1883/2160 train_time:106889ms step_avg:56.77ms
step:1884/2160 train_time:106976ms step_avg:56.78ms
step:1885/2160 train_time:107064ms step_avg:56.80ms
step:1886/2160 train_time:107150ms step_avg:56.81ms
step:1887/2160 train_time:107239ms step_avg:56.83ms
step:1888/2160 train_time:107325ms step_avg:56.85ms
step:1889/2160 train_time:107414ms step_avg:56.86ms
step:1890/2160 train_time:107501ms step_avg:56.88ms
step:1891/2160 train_time:107590ms step_avg:56.90ms
step:1892/2160 train_time:107676ms step_avg:56.91ms
step:1893/2160 train_time:107764ms step_avg:56.93ms
step:1894/2160 train_time:107850ms step_avg:56.94ms
step:1895/2160 train_time:107939ms step_avg:56.96ms
step:1896/2160 train_time:108025ms step_avg:56.98ms
step:1897/2160 train_time:108113ms step_avg:56.99ms
step:1898/2160 train_time:108200ms step_avg:57.01ms
step:1899/2160 train_time:108288ms step_avg:57.02ms
step:1900/2160 train_time:108374ms step_avg:57.04ms
step:1901/2160 train_time:108464ms step_avg:57.06ms
step:1902/2160 train_time:108550ms step_avg:57.07ms
step:1903/2160 train_time:108639ms step_avg:57.09ms
step:1904/2160 train_time:108726ms step_avg:57.10ms
step:1905/2160 train_time:108813ms step_avg:57.12ms
step:1906/2160 train_time:108900ms step_avg:57.14ms
step:1907/2160 train_time:108988ms step_avg:57.15ms
step:1908/2160 train_time:109074ms step_avg:57.17ms
step:1909/2160 train_time:109163ms step_avg:57.18ms
step:1910/2160 train_time:109250ms step_avg:57.20ms
step:1911/2160 train_time:109338ms step_avg:57.21ms
step:1912/2160 train_time:109425ms step_avg:57.23ms
step:1913/2160 train_time:109513ms step_avg:57.25ms
step:1914/2160 train_time:109600ms step_avg:57.26ms
step:1915/2160 train_time:109689ms step_avg:57.28ms
step:1916/2160 train_time:109776ms step_avg:57.29ms
step:1917/2160 train_time:109865ms step_avg:57.31ms
step:1918/2160 train_time:109951ms step_avg:57.33ms
step:1919/2160 train_time:110039ms step_avg:57.34ms
step:1920/2160 train_time:110126ms step_avg:57.36ms
step:1921/2160 train_time:110214ms step_avg:57.37ms
step:1922/2160 train_time:110299ms step_avg:57.39ms
step:1923/2160 train_time:110387ms step_avg:57.40ms
step:1924/2160 train_time:110474ms step_avg:57.42ms
step:1925/2160 train_time:110562ms step_avg:57.43ms
step:1926/2160 train_time:110649ms step_avg:57.45ms
step:1927/2160 train_time:110737ms step_avg:57.47ms
step:1928/2160 train_time:110823ms step_avg:57.48ms
step:1929/2160 train_time:110912ms step_avg:57.50ms
step:1930/2160 train_time:110999ms step_avg:57.51ms
step:1931/2160 train_time:111087ms step_avg:57.53ms
step:1932/2160 train_time:111174ms step_avg:57.54ms
step:1933/2160 train_time:111263ms step_avg:57.56ms
step:1934/2160 train_time:111349ms step_avg:57.57ms
step:1935/2160 train_time:111437ms step_avg:57.59ms
step:1936/2160 train_time:111523ms step_avg:57.61ms
step:1937/2160 train_time:111613ms step_avg:57.62ms
step:1938/2160 train_time:111700ms step_avg:57.64ms
step:1939/2160 train_time:111789ms step_avg:57.65ms
step:1940/2160 train_time:111876ms step_avg:57.67ms
step:1941/2160 train_time:111964ms step_avg:57.68ms
step:1942/2160 train_time:112051ms step_avg:57.70ms
step:1943/2160 train_time:112139ms step_avg:57.71ms
step:1944/2160 train_time:112227ms step_avg:57.73ms
step:1945/2160 train_time:112315ms step_avg:57.75ms
step:1946/2160 train_time:112402ms step_avg:57.76ms
step:1947/2160 train_time:112490ms step_avg:57.78ms
step:1948/2160 train_time:112577ms step_avg:57.79ms
step:1949/2160 train_time:112665ms step_avg:57.81ms
step:1950/2160 train_time:112752ms step_avg:57.82ms
step:1951/2160 train_time:112840ms step_avg:57.84ms
step:1952/2160 train_time:112927ms step_avg:57.85ms
step:1953/2160 train_time:113014ms step_avg:57.87ms
step:1954/2160 train_time:113100ms step_avg:57.88ms
step:1955/2160 train_time:113189ms step_avg:57.90ms
step:1956/2160 train_time:113276ms step_avg:57.91ms
step:1957/2160 train_time:113365ms step_avg:57.93ms
step:1958/2160 train_time:113451ms step_avg:57.94ms
step:1959/2160 train_time:113539ms step_avg:57.96ms
step:1960/2160 train_time:113626ms step_avg:57.97ms
step:1961/2160 train_time:113715ms step_avg:57.99ms
step:1962/2160 train_time:113801ms step_avg:58.00ms
step:1963/2160 train_time:113890ms step_avg:58.02ms
step:1964/2160 train_time:113976ms step_avg:58.03ms
step:1965/2160 train_time:114064ms step_avg:58.05ms
step:1966/2160 train_time:114151ms step_avg:58.06ms
step:1967/2160 train_time:114240ms step_avg:58.08ms
step:1968/2160 train_time:114326ms step_avg:58.09ms
step:1969/2160 train_time:114414ms step_avg:58.11ms
step:1970/2160 train_time:114501ms step_avg:58.12ms
step:1971/2160 train_time:114590ms step_avg:58.14ms
step:1972/2160 train_time:114677ms step_avg:58.15ms
step:1973/2160 train_time:114765ms step_avg:58.17ms
step:1974/2160 train_time:114851ms step_avg:58.18ms
step:1975/2160 train_time:114940ms step_avg:58.20ms
step:1976/2160 train_time:115027ms step_avg:58.21ms
step:1977/2160 train_time:115115ms step_avg:58.23ms
step:1978/2160 train_time:115201ms step_avg:58.24ms
step:1979/2160 train_time:115290ms step_avg:58.26ms
step:1980/2160 train_time:115376ms step_avg:58.27ms
step:1981/2160 train_time:115464ms step_avg:58.29ms
step:1982/2160 train_time:115551ms step_avg:58.30ms
step:1983/2160 train_time:115640ms step_avg:58.32ms
step:1984/2160 train_time:115726ms step_avg:58.33ms
step:1985/2160 train_time:115814ms step_avg:58.34ms
step:1986/2160 train_time:115900ms step_avg:58.36ms
step:1987/2160 train_time:115988ms step_avg:58.37ms
step:1988/2160 train_time:116075ms step_avg:58.39ms
step:1989/2160 train_time:116163ms step_avg:58.40ms
step:1990/2160 train_time:116257ms step_avg:58.42ms
step:1991/2160 train_time:116338ms step_avg:58.43ms
step:1992/2160 train_time:116425ms step_avg:58.45ms
step:1993/2160 train_time:116513ms step_avg:58.46ms
step:1994/2160 train_time:116599ms step_avg:58.48ms
step:1995/2160 train_time:116688ms step_avg:58.49ms
step:1996/2160 train_time:116774ms step_avg:58.50ms
step:1997/2160 train_time:116864ms step_avg:58.52ms
step:1998/2160 train_time:116950ms step_avg:58.53ms
step:1999/2160 train_time:117038ms step_avg:58.55ms
step:2000/2160 train_time:117125ms step_avg:58.56ms
step:2000/2160 val_loss:3.3135 train_time:117213ms step_avg:58.61ms
step:2001/2160 train_time:117236ms step_avg:58.59ms
step:2002/2160 train_time:117305ms step_avg:58.59ms
step:2003/2160 train_time:117399ms step_avg:58.61ms
step:2004/2160 train_time:117485ms step_avg:58.63ms
step:2005/2160 train_time:117573ms step_avg:58.64ms
step:2006/2160 train_time:117658ms step_avg:58.65ms
step:2007/2160 train_time:117745ms step_avg:58.67ms
step:2008/2160 train_time:117830ms step_avg:58.68ms
step:2009/2160 train_time:117917ms step_avg:58.69ms
step:2010/2160 train_time:118003ms step_avg:58.71ms
step:2011/2160 train_time:118091ms step_avg:58.72ms
step:2012/2160 train_time:118178ms step_avg:58.74ms
step:2013/2160 train_time:118269ms step_avg:58.75ms
step:2014/2160 train_time:118358ms step_avg:58.77ms
step:2015/2160 train_time:118448ms step_avg:58.78ms
step:2016/2160 train_time:118535ms step_avg:58.80ms
step:2017/2160 train_time:118623ms step_avg:58.81ms
step:2018/2160 train_time:118709ms step_avg:58.83ms
step:2019/2160 train_time:118796ms step_avg:58.84ms
step:2020/2160 train_time:118881ms step_avg:58.85ms
step:2021/2160 train_time:118969ms step_avg:58.87ms
step:2022/2160 train_time:119054ms step_avg:58.88ms
step:2023/2160 train_time:119143ms step_avg:58.89ms
step:2024/2160 train_time:119231ms step_avg:58.91ms
step:2025/2160 train_time:119321ms step_avg:58.92ms
step:2026/2160 train_time:119409ms step_avg:58.94ms
step:2027/2160 train_time:119498ms step_avg:58.95ms
step:2028/2160 train_time:119584ms step_avg:58.97ms
step:2029/2160 train_time:119673ms step_avg:58.98ms
step:2030/2160 train_time:119759ms step_avg:58.99ms
step:2031/2160 train_time:119846ms step_avg:59.01ms
step:2032/2160 train_time:119931ms step_avg:59.02ms
step:2033/2160 train_time:120019ms step_avg:59.04ms
step:2034/2160 train_time:120106ms step_avg:59.05ms
step:2035/2160 train_time:120196ms step_avg:59.06ms
step:2036/2160 train_time:120284ms step_avg:59.08ms
step:2037/2160 train_time:120372ms step_avg:59.09ms
step:2038/2160 train_time:120459ms step_avg:59.11ms
step:2039/2160 train_time:120548ms step_avg:59.12ms
step:2040/2160 train_time:120635ms step_avg:59.13ms
step:2041/2160 train_time:120723ms step_avg:59.15ms
step:2042/2160 train_time:120808ms step_avg:59.16ms
step:2043/2160 train_time:120896ms step_avg:59.18ms
step:2044/2160 train_time:120982ms step_avg:59.19ms
step:2045/2160 train_time:121070ms step_avg:59.20ms
step:2046/2160 train_time:121157ms step_avg:59.22ms
step:2047/2160 train_time:121248ms step_avg:59.23ms
step:2048/2160 train_time:121334ms step_avg:59.25ms
step:2049/2160 train_time:121424ms step_avg:59.26ms
step:2050/2160 train_time:121510ms step_avg:59.27ms
step:2051/2160 train_time:121600ms step_avg:59.29ms
step:2052/2160 train_time:121686ms step_avg:59.30ms
step:2053/2160 train_time:121774ms step_avg:59.32ms
step:2054/2160 train_time:121860ms step_avg:59.33ms
step:2055/2160 train_time:121948ms step_avg:59.34ms
step:2056/2160 train_time:122033ms step_avg:59.35ms
step:2057/2160 train_time:122121ms step_avg:59.37ms
step:2058/2160 train_time:122209ms step_avg:59.38ms
step:2059/2160 train_time:122298ms step_avg:59.40ms
step:2060/2160 train_time:122385ms step_avg:59.41ms
step:2061/2160 train_time:122473ms step_avg:59.42ms
step:2062/2160 train_time:122560ms step_avg:59.44ms
step:2063/2160 train_time:122648ms step_avg:59.45ms
step:2064/2160 train_time:122734ms step_avg:59.46ms
step:2065/2160 train_time:122822ms step_avg:59.48ms
step:2066/2160 train_time:122908ms step_avg:59.49ms
step:2067/2160 train_time:122997ms step_avg:59.50ms
step:2068/2160 train_time:123083ms step_avg:59.52ms
step:2069/2160 train_time:123171ms step_avg:59.53ms
step:2070/2160 train_time:123258ms step_avg:59.54ms
step:2071/2160 train_time:123346ms step_avg:59.56ms
step:2072/2160 train_time:123433ms step_avg:59.57ms
step:2073/2160 train_time:123522ms step_avg:59.59ms
step:2074/2160 train_time:123609ms step_avg:59.60ms
step:2075/2160 train_time:123698ms step_avg:59.61ms
step:2076/2160 train_time:123785ms step_avg:59.63ms
step:2077/2160 train_time:123873ms step_avg:59.64ms
step:2078/2160 train_time:123959ms step_avg:59.65ms
step:2079/2160 train_time:124048ms step_avg:59.67ms
step:2080/2160 train_time:124134ms step_avg:59.68ms
step:2081/2160 train_time:124222ms step_avg:59.69ms
step:2082/2160 train_time:124309ms step_avg:59.71ms
step:2083/2160 train_time:124399ms step_avg:59.72ms
step:2084/2160 train_time:124486ms step_avg:59.73ms
step:2085/2160 train_time:124575ms step_avg:59.75ms
step:2086/2160 train_time:124661ms step_avg:59.76ms
step:2087/2160 train_time:124750ms step_avg:59.77ms
step:2088/2160 train_time:124836ms step_avg:59.79ms
step:2089/2160 train_time:124924ms step_avg:59.80ms
step:2090/2160 train_time:125010ms step_avg:59.81ms
step:2091/2160 train_time:125099ms step_avg:59.83ms
step:2092/2160 train_time:125185ms step_avg:59.84ms
step:2093/2160 train_time:125273ms step_avg:59.85ms
step:2094/2160 train_time:125360ms step_avg:59.87ms
step:2095/2160 train_time:125449ms step_avg:59.88ms
step:2096/2160 train_time:125537ms step_avg:59.89ms
step:2097/2160 train_time:125625ms step_avg:59.91ms
step:2098/2160 train_time:125713ms step_avg:59.92ms
step:2099/2160 train_time:125799ms step_avg:59.93ms
step:2100/2160 train_time:125887ms step_avg:59.95ms
step:2101/2160 train_time:125974ms step_avg:59.96ms
step:2102/2160 train_time:126060ms step_avg:59.97ms
step:2103/2160 train_time:126149ms step_avg:59.99ms
step:2104/2160 train_time:126235ms step_avg:60.00ms
step:2105/2160 train_time:126323ms step_avg:60.01ms
step:2106/2160 train_time:126410ms step_avg:60.02ms
step:2107/2160 train_time:126499ms step_avg:60.04ms
step:2108/2160 train_time:126585ms step_avg:60.05ms
step:2109/2160 train_time:126674ms step_avg:60.06ms
step:2110/2160 train_time:126761ms step_avg:60.08ms
step:2111/2160 train_time:126849ms step_avg:60.09ms
step:2112/2160 train_time:126935ms step_avg:60.10ms
step:2113/2160 train_time:127024ms step_avg:60.12ms
step:2114/2160 train_time:127111ms step_avg:60.13ms
step:2115/2160 train_time:127199ms step_avg:60.14ms
step:2116/2160 train_time:127286ms step_avg:60.15ms
step:2117/2160 train_time:127375ms step_avg:60.17ms
step:2118/2160 train_time:127463ms step_avg:60.18ms
step:2119/2160 train_time:127550ms step_avg:60.19ms
step:2120/2160 train_time:127635ms step_avg:60.21ms
step:2121/2160 train_time:127724ms step_avg:60.22ms
step:2122/2160 train_time:127811ms step_avg:60.23ms
step:2123/2160 train_time:127900ms step_avg:60.24ms
step:2124/2160 train_time:127986ms step_avg:60.26ms
step:2125/2160 train_time:128075ms step_avg:60.27ms
step:2126/2160 train_time:128162ms step_avg:60.28ms
step:2127/2160 train_time:128250ms step_avg:60.30ms
step:2128/2160 train_time:128337ms step_avg:60.31ms
step:2129/2160 train_time:128426ms step_avg:60.32ms
step:2130/2160 train_time:128513ms step_avg:60.33ms
step:2131/2160 train_time:128601ms step_avg:60.35ms
step:2132/2160 train_time:128689ms step_avg:60.36ms
step:2133/2160 train_time:128777ms step_avg:60.37ms
step:2134/2160 train_time:128863ms step_avg:60.39ms
step:2135/2160 train_time:128952ms step_avg:60.40ms
step:2136/2160 train_time:129039ms step_avg:60.41ms
step:2137/2160 train_time:129128ms step_avg:60.42ms
step:2138/2160 train_time:129214ms step_avg:60.44ms
step:2139/2160 train_time:129302ms step_avg:60.45ms
step:2140/2160 train_time:129390ms step_avg:60.46ms
step:2141/2160 train_time:129478ms step_avg:60.48ms
step:2142/2160 train_time:129565ms step_avg:60.49ms
step:2143/2160 train_time:129654ms step_avg:60.50ms
step:2144/2160 train_time:129741ms step_avg:60.51ms
step:2145/2160 train_time:129829ms step_avg:60.53ms
step:2146/2160 train_time:129916ms step_avg:60.54ms
step:2147/2160 train_time:130005ms step_avg:60.55ms
step:2148/2160 train_time:130092ms step_avg:60.56ms
step:2149/2160 train_time:130182ms step_avg:60.58ms
step:2150/2160 train_time:130268ms step_avg:60.59ms
step:2151/2160 train_time:130356ms step_avg:60.60ms
step:2152/2160 train_time:130443ms step_avg:60.61ms
step:2153/2160 train_time:130531ms step_avg:60.63ms
step:2154/2160 train_time:130618ms step_avg:60.64ms
step:2155/2160 train_time:130708ms step_avg:60.65ms
step:2156/2160 train_time:130795ms step_avg:60.67ms
step:2157/2160 train_time:130884ms step_avg:60.68ms
step:2158/2160 train_time:130971ms step_avg:60.69ms
step:2159/2160 train_time:131060ms step_avg:60.70ms
step:2160/2160 train_time:131147ms step_avg:60.72ms
step:2160/2160 val_loss:3.2808 train_time:131236ms step_avg:60.76ms
peak memory allocated: 30078 MiB reserved: 45036 MiB

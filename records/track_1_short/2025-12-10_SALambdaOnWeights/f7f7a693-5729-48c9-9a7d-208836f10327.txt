import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 10 13:50:34 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                  Off |
| N/A   32C    P0            151W /  700W |    5858MiB /  81559MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                  Off |
| N/A   27C    P0            139W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                  Off |
| N/A   24C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                  Off |
| N/A   29C    P0            134W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                  Off |
| N/A   30C    P0            136W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                  Off |
| N/A   26C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                  Off |
| N/A   30C    P0            142W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                  Off |
| N/A   25C    P0            132W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     69792      C   /usr/bin/python3                             1510MiB |
|    0   N/A  N/A     69793      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     69794      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     69795      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     69796      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     69797      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     69798      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     69799      C   /usr/bin/python3                              614MiB |
|    1   N/A  N/A     69793      C   /usr/bin/python3                             1510MiB |
|    2   N/A  N/A     69794      C   /usr/bin/python3                             1510MiB |
|    3   N/A  N/A     69795      C   /usr/bin/python3                             1510MiB |
|    4   N/A  N/A     69796      C   /usr/bin/python3                             1510MiB |
|    5   N/A  N/A     69797      C   /usr/bin/python3                             1510MiB |
|    6   N/A  N/A     69798      C   /usr/bin/python3                             1510MiB |
|    7   N/A  N/A     69799      C   /usr/bin/python3                             1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2160 train_time:87ms step_avg:86.73ms
step:2/2160 train_time:108ms step_avg:54.16ms
step:3/2160 train_time:127ms step_avg:42.25ms
step:4/2160 train_time:153ms step_avg:38.20ms
step:5/2160 train_time:186ms step_avg:37.25ms
step:6/2160 train_time:247ms step_avg:41.21ms
step:7/2160 train_time:275ms step_avg:39.31ms
step:8/2160 train_time:308ms step_avg:38.49ms
step:9/2160 train_time:342ms step_avg:37.95ms
step:10/2160 train_time:374ms step_avg:37.42ms
step:11/2160 train_time:408ms step_avg:37.09ms
step:12/2160 train_time:441ms step_avg:36.74ms
step:13/2160 train_time:475ms step_avg:36.51ms
step:14/2160 train_time:508ms step_avg:36.27ms
step:15/2160 train_time:541ms step_avg:36.10ms
step:16/2160 train_time:574ms step_avg:35.90ms
step:17/2160 train_time:608ms step_avg:35.77ms
step:18/2160 train_time:641ms step_avg:35.61ms
step:19/2160 train_time:675ms step_avg:35.54ms
step:20/2160 train_time:708ms step_avg:35.42ms
step:21/2160 train_time:742ms step_avg:35.35ms
step:22/2160 train_time:775ms step_avg:35.24ms
step:23/2160 train_time:809ms step_avg:35.18ms
step:24/2160 train_time:842ms step_avg:35.09ms
step:25/2160 train_time:876ms step_avg:35.06ms
step:26/2160 train_time:909ms step_avg:34.97ms
step:27/2160 train_time:943ms step_avg:34.94ms
step:28/2160 train_time:976ms step_avg:34.87ms
step:29/2160 train_time:1010ms step_avg:34.83ms
step:30/2160 train_time:1043ms step_avg:34.76ms
step:31/2160 train_time:1077ms step_avg:34.75ms
step:32/2160 train_time:1111ms step_avg:34.71ms
step:33/2160 train_time:1144ms step_avg:34.67ms
step:34/2160 train_time:1177ms step_avg:34.62ms
step:35/2160 train_time:1211ms step_avg:34.60ms
step:36/2160 train_time:1244ms step_avg:34.56ms
step:37/2160 train_time:1278ms step_avg:34.55ms
step:38/2160 train_time:1311ms step_avg:34.50ms
step:39/2160 train_time:1345ms step_avg:34.49ms
step:40/2160 train_time:1378ms step_avg:34.45ms
step:41/2160 train_time:1412ms step_avg:34.44ms
step:42/2160 train_time:1445ms step_avg:34.40ms
step:43/2160 train_time:1479ms step_avg:34.39ms
step:44/2160 train_time:1512ms step_avg:34.36ms
step:45/2160 train_time:1546ms step_avg:34.35ms
step:46/2160 train_time:1578ms step_avg:34.31ms
step:47/2160 train_time:1612ms step_avg:34.30ms
step:48/2160 train_time:1645ms step_avg:34.28ms
step:49/2160 train_time:1679ms step_avg:34.27ms
step:50/2160 train_time:1712ms step_avg:34.24ms
step:51/2160 train_time:1746ms step_avg:34.23ms
step:52/2160 train_time:1779ms step_avg:34.21ms
step:53/2160 train_time:1813ms step_avg:34.21ms
step:54/2160 train_time:1846ms step_avg:34.18ms
step:55/2160 train_time:1880ms step_avg:34.17ms
step:56/2160 train_time:1912ms step_avg:34.15ms
step:57/2160 train_time:1947ms step_avg:34.15ms
step:58/2160 train_time:1979ms step_avg:34.13ms
step:59/2160 train_time:2013ms step_avg:34.12ms
step:60/2160 train_time:2046ms step_avg:34.10ms
step:61/2160 train_time:2080ms step_avg:34.10ms
step:62/2160 train_time:2113ms step_avg:34.08ms
step:63/2160 train_time:2147ms step_avg:34.07ms
step:64/2160 train_time:2179ms step_avg:34.05ms
step:65/2160 train_time:2213ms step_avg:34.05ms
step:66/2160 train_time:2246ms step_avg:34.04ms
step:67/2160 train_time:2281ms step_avg:34.04ms
step:68/2160 train_time:2314ms step_avg:34.03ms
step:69/2160 train_time:2348ms step_avg:34.03ms
step:70/2160 train_time:2381ms step_avg:34.01ms
step:71/2160 train_time:2415ms step_avg:34.01ms
step:72/2160 train_time:2448ms step_avg:34.00ms
step:73/2160 train_time:2482ms step_avg:34.00ms
step:74/2160 train_time:2515ms step_avg:33.99ms
step:75/2160 train_time:2549ms step_avg:33.98ms
step:76/2160 train_time:2581ms step_avg:33.97ms
step:77/2160 train_time:2616ms step_avg:33.97ms
step:78/2160 train_time:2648ms step_avg:33.95ms
step:79/2160 train_time:2683ms step_avg:33.96ms
step:80/2160 train_time:2716ms step_avg:33.95ms
step:81/2160 train_time:2750ms step_avg:33.94ms
step:82/2160 train_time:2782ms step_avg:33.93ms
step:83/2160 train_time:2816ms step_avg:33.93ms
step:84/2160 train_time:2849ms step_avg:33.92ms
step:85/2160 train_time:2883ms step_avg:33.92ms
step:86/2160 train_time:2916ms step_avg:33.90ms
step:87/2160 train_time:2950ms step_avg:33.90ms
step:88/2160 train_time:2982ms step_avg:33.89ms
step:89/2160 train_time:3016ms step_avg:33.89ms
step:90/2160 train_time:3049ms step_avg:33.88ms
step:91/2160 train_time:3084ms step_avg:33.89ms
step:92/2160 train_time:3116ms step_avg:33.87ms
step:93/2160 train_time:3150ms step_avg:33.87ms
step:94/2160 train_time:3182ms step_avg:33.86ms
step:95/2160 train_time:3216ms step_avg:33.86ms
step:96/2160 train_time:3249ms step_avg:33.85ms
step:97/2160 train_time:3283ms step_avg:33.85ms
step:98/2160 train_time:3316ms step_avg:33.84ms
step:99/2160 train_time:3350ms step_avg:33.84ms
step:100/2160 train_time:3383ms step_avg:33.83ms
step:101/2160 train_time:3416ms step_avg:33.83ms
step:102/2160 train_time:3449ms step_avg:33.82ms
step:103/2160 train_time:3483ms step_avg:33.82ms
step:104/2160 train_time:3516ms step_avg:33.81ms
step:105/2160 train_time:3550ms step_avg:33.81ms
step:106/2160 train_time:3583ms step_avg:33.80ms
step:107/2160 train_time:3617ms step_avg:33.80ms
step:108/2160 train_time:3650ms step_avg:33.79ms
step:109/2160 train_time:3684ms step_avg:33.80ms
step:110/2160 train_time:3717ms step_avg:33.79ms
step:111/2160 train_time:3750ms step_avg:33.79ms
step:112/2160 train_time:3783ms step_avg:33.78ms
step:113/2160 train_time:3817ms step_avg:33.78ms
step:114/2160 train_time:3850ms step_avg:33.77ms
step:115/2160 train_time:3884ms step_avg:33.77ms
step:116/2160 train_time:3917ms step_avg:33.76ms
step:117/2160 train_time:3950ms step_avg:33.76ms
step:118/2160 train_time:3983ms step_avg:33.76ms
step:119/2160 train_time:4017ms step_avg:33.76ms
step:120/2160 train_time:4050ms step_avg:33.75ms
step:121/2160 train_time:4084ms step_avg:33.76ms
step:122/2160 train_time:4117ms step_avg:33.74ms
step:123/2160 train_time:4151ms step_avg:33.74ms
step:124/2160 train_time:4183ms step_avg:33.74ms
step:125/2160 train_time:4217ms step_avg:33.74ms
step:126/2160 train_time:4250ms step_avg:33.73ms
step:127/2160 train_time:4284ms step_avg:33.73ms
step:128/2160 train_time:4317ms step_avg:33.72ms
step:129/2160 train_time:4351ms step_avg:33.73ms
step:130/2160 train_time:4383ms step_avg:33.72ms
step:131/2160 train_time:4417ms step_avg:33.72ms
step:132/2160 train_time:4450ms step_avg:33.71ms
step:133/2160 train_time:4484ms step_avg:33.71ms
step:134/2160 train_time:4517ms step_avg:33.71ms
step:135/2160 train_time:4550ms step_avg:33.71ms
step:136/2160 train_time:4583ms step_avg:33.70ms
step:137/2160 train_time:4617ms step_avg:33.70ms
step:138/2160 train_time:4650ms step_avg:33.70ms
step:139/2160 train_time:4684ms step_avg:33.70ms
step:140/2160 train_time:4717ms step_avg:33.69ms
step:141/2160 train_time:4751ms step_avg:33.69ms
step:142/2160 train_time:4783ms step_avg:33.69ms
step:143/2160 train_time:4817ms step_avg:33.69ms
step:144/2160 train_time:4850ms step_avg:33.68ms
step:145/2160 train_time:4884ms step_avg:33.68ms
step:146/2160 train_time:4916ms step_avg:33.67ms
step:147/2160 train_time:4951ms step_avg:33.68ms
step:148/2160 train_time:4983ms step_avg:33.67ms
step:149/2160 train_time:5017ms step_avg:33.67ms
step:150/2160 train_time:5050ms step_avg:33.67ms
step:151/2160 train_time:5084ms step_avg:33.67ms
step:152/2160 train_time:5117ms step_avg:33.66ms
step:153/2160 train_time:5150ms step_avg:33.66ms
step:154/2160 train_time:5183ms step_avg:33.66ms
step:155/2160 train_time:5217ms step_avg:33.66ms
step:156/2160 train_time:5250ms step_avg:33.65ms
step:157/2160 train_time:5284ms step_avg:33.65ms
step:158/2160 train_time:5317ms step_avg:33.65ms
step:159/2160 train_time:5351ms step_avg:33.65ms
step:160/2160 train_time:5384ms step_avg:33.65ms
step:161/2160 train_time:5417ms step_avg:33.65ms
step:162/2160 train_time:5450ms step_avg:33.64ms
step:163/2160 train_time:5484ms step_avg:33.64ms
step:164/2160 train_time:5517ms step_avg:33.64ms
step:165/2160 train_time:5551ms step_avg:33.64ms
step:166/2160 train_time:5583ms step_avg:33.63ms
step:167/2160 train_time:5617ms step_avg:33.64ms
step:168/2160 train_time:5650ms step_avg:33.63ms
step:169/2160 train_time:5684ms step_avg:33.63ms
step:170/2160 train_time:5717ms step_avg:33.63ms
step:171/2160 train_time:5751ms step_avg:33.63ms
step:172/2160 train_time:5783ms step_avg:33.62ms
step:173/2160 train_time:5817ms step_avg:33.63ms
step:174/2160 train_time:5850ms step_avg:33.62ms
step:175/2160 train_time:5884ms step_avg:33.62ms
step:176/2160 train_time:5917ms step_avg:33.62ms
step:177/2160 train_time:5951ms step_avg:33.62ms
step:178/2160 train_time:5983ms step_avg:33.61ms
step:179/2160 train_time:6018ms step_avg:33.62ms
step:180/2160 train_time:6050ms step_avg:33.61ms
step:181/2160 train_time:6084ms step_avg:33.61ms
step:182/2160 train_time:6117ms step_avg:33.61ms
step:183/2160 train_time:6151ms step_avg:33.61ms
step:184/2160 train_time:6183ms step_avg:33.61ms
step:185/2160 train_time:6217ms step_avg:33.61ms
step:186/2160 train_time:6250ms step_avg:33.60ms
step:187/2160 train_time:6284ms step_avg:33.60ms
step:188/2160 train_time:6317ms step_avg:33.60ms
step:189/2160 train_time:6350ms step_avg:33.60ms
step:190/2160 train_time:6383ms step_avg:33.60ms
step:191/2160 train_time:6417ms step_avg:33.60ms
step:192/2160 train_time:6450ms step_avg:33.59ms
step:193/2160 train_time:6484ms step_avg:33.59ms
step:194/2160 train_time:6516ms step_avg:33.59ms
step:195/2160 train_time:6550ms step_avg:33.59ms
step:196/2160 train_time:6583ms step_avg:33.59ms
step:197/2160 train_time:6617ms step_avg:33.59ms
step:198/2160 train_time:6650ms step_avg:33.58ms
step:199/2160 train_time:6683ms step_avg:33.58ms
step:200/2160 train_time:6716ms step_avg:33.58ms
step:201/2160 train_time:6750ms step_avg:33.58ms
step:202/2160 train_time:6782ms step_avg:33.58ms
step:203/2160 train_time:6816ms step_avg:33.58ms
step:204/2160 train_time:6849ms step_avg:33.57ms
step:205/2160 train_time:6883ms step_avg:33.57ms
step:206/2160 train_time:6916ms step_avg:33.57ms
step:207/2160 train_time:6949ms step_avg:33.57ms
step:208/2160 train_time:6982ms step_avg:33.57ms
step:209/2160 train_time:7016ms step_avg:33.57ms
step:210/2160 train_time:7049ms step_avg:33.56ms
step:211/2160 train_time:7082ms step_avg:33.57ms
step:212/2160 train_time:7115ms step_avg:33.56ms
step:213/2160 train_time:7149ms step_avg:33.56ms
step:214/2160 train_time:7181ms step_avg:33.56ms
step:215/2160 train_time:7215ms step_avg:33.56ms
step:216/2160 train_time:7248ms step_avg:33.56ms
step:217/2160 train_time:7282ms step_avg:33.56ms
step:218/2160 train_time:7315ms step_avg:33.55ms
step:219/2160 train_time:7348ms step_avg:33.55ms
step:220/2160 train_time:7381ms step_avg:33.55ms
step:221/2160 train_time:7415ms step_avg:33.55ms
step:222/2160 train_time:7448ms step_avg:33.55ms
step:223/2160 train_time:7481ms step_avg:33.55ms
step:224/2160 train_time:7514ms step_avg:33.55ms
step:225/2160 train_time:7548ms step_avg:33.55ms
step:226/2160 train_time:7581ms step_avg:33.54ms
step:227/2160 train_time:7614ms step_avg:33.54ms
step:228/2160 train_time:7647ms step_avg:33.54ms
step:229/2160 train_time:7681ms step_avg:33.54ms
step:230/2160 train_time:7714ms step_avg:33.54ms
step:231/2160 train_time:7748ms step_avg:33.54ms
step:232/2160 train_time:7780ms step_avg:33.54ms
step:233/2160 train_time:7815ms step_avg:33.54ms
step:234/2160 train_time:7847ms step_avg:33.54ms
step:235/2160 train_time:7881ms step_avg:33.54ms
step:236/2160 train_time:7914ms step_avg:33.53ms
step:237/2160 train_time:7948ms step_avg:33.53ms
step:238/2160 train_time:7980ms step_avg:33.53ms
step:239/2160 train_time:8014ms step_avg:33.53ms
step:240/2160 train_time:8047ms step_avg:33.53ms
step:241/2160 train_time:8082ms step_avg:33.53ms
step:242/2160 train_time:8114ms step_avg:33.53ms
step:243/2160 train_time:8147ms step_avg:33.53ms
step:244/2160 train_time:8180ms step_avg:33.52ms
step:245/2160 train_time:8214ms step_avg:33.53ms
step:246/2160 train_time:8247ms step_avg:33.52ms
step:247/2160 train_time:8281ms step_avg:33.52ms
step:248/2160 train_time:8313ms step_avg:33.52ms
step:249/2160 train_time:8347ms step_avg:33.52ms
step:250/2160 train_time:8380ms step_avg:33.52ms
step:250/2160 val_loss:4.3013 train_time:8415ms step_avg:33.66ms
step:251/2160 train_time:8435ms step_avg:33.61ms
step:252/2160 train_time:8454ms step_avg:33.55ms
step:253/2160 train_time:8487ms step_avg:33.55ms
step:254/2160 train_time:8521ms step_avg:33.55ms
step:255/2160 train_time:8558ms step_avg:33.56ms
step:256/2160 train_time:8593ms step_avg:33.57ms
step:257/2160 train_time:8628ms step_avg:33.57ms
step:258/2160 train_time:8661ms step_avg:33.57ms
step:259/2160 train_time:8695ms step_avg:33.57ms
step:260/2160 train_time:8728ms step_avg:33.57ms
step:261/2160 train_time:8762ms step_avg:33.57ms
step:262/2160 train_time:8795ms step_avg:33.57ms
step:263/2160 train_time:8828ms step_avg:33.57ms
step:264/2160 train_time:8861ms step_avg:33.56ms
step:265/2160 train_time:8895ms step_avg:33.56ms
step:266/2160 train_time:8927ms step_avg:33.56ms
step:267/2160 train_time:8961ms step_avg:33.56ms
step:268/2160 train_time:8994ms step_avg:33.56ms
step:269/2160 train_time:9028ms step_avg:33.56ms
step:270/2160 train_time:9061ms step_avg:33.56ms
step:271/2160 train_time:9094ms step_avg:33.56ms
step:272/2160 train_time:9127ms step_avg:33.56ms
step:273/2160 train_time:9160ms step_avg:33.55ms
step:274/2160 train_time:9193ms step_avg:33.55ms
step:275/2160 train_time:9227ms step_avg:33.55ms
step:276/2160 train_time:9260ms step_avg:33.55ms
step:277/2160 train_time:9293ms step_avg:33.55ms
step:278/2160 train_time:9326ms step_avg:33.55ms
step:279/2160 train_time:9360ms step_avg:33.55ms
step:280/2160 train_time:9392ms step_avg:33.54ms
step:281/2160 train_time:9426ms step_avg:33.54ms
step:282/2160 train_time:9459ms step_avg:33.54ms
step:283/2160 train_time:9493ms step_avg:33.54ms
step:284/2160 train_time:9526ms step_avg:33.54ms
step:285/2160 train_time:9559ms step_avg:33.54ms
step:286/2160 train_time:9592ms step_avg:33.54ms
step:287/2160 train_time:9626ms step_avg:33.54ms
step:288/2160 train_time:9659ms step_avg:33.54ms
step:289/2160 train_time:9693ms step_avg:33.54ms
step:290/2160 train_time:9726ms step_avg:33.54ms
step:291/2160 train_time:9760ms step_avg:33.54ms
step:292/2160 train_time:9792ms step_avg:33.54ms
step:293/2160 train_time:9826ms step_avg:33.54ms
step:294/2160 train_time:9859ms step_avg:33.53ms
step:295/2160 train_time:9893ms step_avg:33.54ms
step:296/2160 train_time:9926ms step_avg:33.53ms
step:297/2160 train_time:9960ms step_avg:33.53ms
step:298/2160 train_time:9992ms step_avg:33.53ms
step:299/2160 train_time:10026ms step_avg:33.53ms
step:300/2160 train_time:10059ms step_avg:33.53ms
step:301/2160 train_time:10093ms step_avg:33.53ms
step:302/2160 train_time:10126ms step_avg:33.53ms
step:303/2160 train_time:10159ms step_avg:33.53ms
step:304/2160 train_time:10192ms step_avg:33.53ms
step:305/2160 train_time:10226ms step_avg:33.53ms
step:306/2160 train_time:10259ms step_avg:33.53ms
step:307/2160 train_time:10293ms step_avg:33.53ms
step:308/2160 train_time:10326ms step_avg:33.52ms
step:309/2160 train_time:10359ms step_avg:33.52ms
step:310/2160 train_time:10392ms step_avg:33.52ms
step:311/2160 train_time:10426ms step_avg:33.52ms
step:312/2160 train_time:10458ms step_avg:33.52ms
step:313/2160 train_time:10492ms step_avg:33.52ms
step:314/2160 train_time:10525ms step_avg:33.52ms
step:315/2160 train_time:10559ms step_avg:33.52ms
step:316/2160 train_time:10592ms step_avg:33.52ms
step:317/2160 train_time:10626ms step_avg:33.52ms
step:318/2160 train_time:10658ms step_avg:33.52ms
step:319/2160 train_time:10692ms step_avg:33.52ms
step:320/2160 train_time:10725ms step_avg:33.52ms
step:321/2160 train_time:10759ms step_avg:33.52ms
step:322/2160 train_time:10791ms step_avg:33.51ms
step:323/2160 train_time:10825ms step_avg:33.51ms
step:324/2160 train_time:10858ms step_avg:33.51ms
step:325/2160 train_time:10892ms step_avg:33.51ms
step:326/2160 train_time:10925ms step_avg:33.51ms
step:327/2160 train_time:10958ms step_avg:33.51ms
step:328/2160 train_time:10991ms step_avg:33.51ms
step:329/2160 train_time:11025ms step_avg:33.51ms
step:330/2160 train_time:11057ms step_avg:33.51ms
step:331/2160 train_time:11091ms step_avg:33.51ms
step:332/2160 train_time:11124ms step_avg:33.51ms
step:333/2160 train_time:11158ms step_avg:33.51ms
step:334/2160 train_time:11190ms step_avg:33.50ms
step:335/2160 train_time:11224ms step_avg:33.50ms
step:336/2160 train_time:11257ms step_avg:33.50ms
step:337/2160 train_time:11291ms step_avg:33.50ms
step:338/2160 train_time:11324ms step_avg:33.50ms
step:339/2160 train_time:11357ms step_avg:33.50ms
step:340/2160 train_time:11390ms step_avg:33.50ms
step:341/2160 train_time:11424ms step_avg:33.50ms
step:342/2160 train_time:11456ms step_avg:33.50ms
step:343/2160 train_time:11490ms step_avg:33.50ms
step:344/2160 train_time:11523ms step_avg:33.50ms
step:345/2160 train_time:11557ms step_avg:33.50ms
step:346/2160 train_time:11590ms step_avg:33.50ms
step:347/2160 train_time:11623ms step_avg:33.50ms
step:348/2160 train_time:11656ms step_avg:33.49ms
step:349/2160 train_time:11690ms step_avg:33.50ms
step:350/2160 train_time:11723ms step_avg:33.49ms
step:351/2160 train_time:11757ms step_avg:33.49ms
step:352/2160 train_time:11789ms step_avg:33.49ms
step:353/2160 train_time:11823ms step_avg:33.49ms
step:354/2160 train_time:11856ms step_avg:33.49ms
step:355/2160 train_time:11890ms step_avg:33.49ms
step:356/2160 train_time:11923ms step_avg:33.49ms
step:357/2160 train_time:11956ms step_avg:33.49ms
step:358/2160 train_time:11989ms step_avg:33.49ms
step:359/2160 train_time:12023ms step_avg:33.49ms
step:360/2160 train_time:12056ms step_avg:33.49ms
step:361/2160 train_time:12090ms step_avg:33.49ms
step:362/2160 train_time:12123ms step_avg:33.49ms
step:363/2160 train_time:12156ms step_avg:33.49ms
step:364/2160 train_time:12189ms step_avg:33.49ms
step:365/2160 train_time:12223ms step_avg:33.49ms
step:366/2160 train_time:12255ms step_avg:33.48ms
step:367/2160 train_time:12289ms step_avg:33.49ms
step:368/2160 train_time:12322ms step_avg:33.48ms
step:369/2160 train_time:12356ms step_avg:33.48ms
step:370/2160 train_time:12389ms step_avg:33.48ms
step:371/2160 train_time:12422ms step_avg:33.48ms
step:372/2160 train_time:12455ms step_avg:33.48ms
step:373/2160 train_time:12489ms step_avg:33.48ms
step:374/2160 train_time:12522ms step_avg:33.48ms
step:375/2160 train_time:12556ms step_avg:33.48ms
step:376/2160 train_time:12588ms step_avg:33.48ms
step:377/2160 train_time:12622ms step_avg:33.48ms
step:378/2160 train_time:12655ms step_avg:33.48ms
step:379/2160 train_time:12689ms step_avg:33.48ms
step:380/2160 train_time:12722ms step_avg:33.48ms
step:381/2160 train_time:12756ms step_avg:33.48ms
step:382/2160 train_time:12788ms step_avg:33.48ms
step:383/2160 train_time:12822ms step_avg:33.48ms
step:384/2160 train_time:12855ms step_avg:33.48ms
step:385/2160 train_time:12889ms step_avg:33.48ms
step:386/2160 train_time:12922ms step_avg:33.48ms
step:387/2160 train_time:12955ms step_avg:33.48ms
step:388/2160 train_time:12988ms step_avg:33.47ms
step:389/2160 train_time:13022ms step_avg:33.47ms
step:390/2160 train_time:13054ms step_avg:33.47ms
step:391/2160 train_time:13088ms step_avg:33.47ms
step:392/2160 train_time:13121ms step_avg:33.47ms
step:393/2160 train_time:13155ms step_avg:33.47ms
step:394/2160 train_time:13188ms step_avg:33.47ms
step:395/2160 train_time:13222ms step_avg:33.47ms
step:396/2160 train_time:13254ms step_avg:33.47ms
step:397/2160 train_time:13288ms step_avg:33.47ms
step:398/2160 train_time:13321ms step_avg:33.47ms
step:399/2160 train_time:13355ms step_avg:33.47ms
step:400/2160 train_time:13387ms step_avg:33.47ms
step:401/2160 train_time:13421ms step_avg:33.47ms
step:402/2160 train_time:13454ms step_avg:33.47ms
step:403/2160 train_time:13488ms step_avg:33.47ms
step:404/2160 train_time:13520ms step_avg:33.47ms
step:405/2160 train_time:13554ms step_avg:33.47ms
step:406/2160 train_time:13587ms step_avg:33.47ms
step:407/2160 train_time:13621ms step_avg:33.47ms
step:408/2160 train_time:13654ms step_avg:33.47ms
step:409/2160 train_time:13688ms step_avg:33.47ms
step:410/2160 train_time:13721ms step_avg:33.47ms
step:411/2160 train_time:13755ms step_avg:33.47ms
step:412/2160 train_time:13788ms step_avg:33.47ms
step:413/2160 train_time:13822ms step_avg:33.47ms
step:414/2160 train_time:13854ms step_avg:33.46ms
step:415/2160 train_time:13889ms step_avg:33.47ms
step:416/2160 train_time:13921ms step_avg:33.46ms
step:417/2160 train_time:13955ms step_avg:33.47ms
step:418/2160 train_time:13988ms step_avg:33.46ms
step:419/2160 train_time:14022ms step_avg:33.46ms
step:420/2160 train_time:14054ms step_avg:33.46ms
step:421/2160 train_time:14088ms step_avg:33.46ms
step:422/2160 train_time:14121ms step_avg:33.46ms
step:423/2160 train_time:14155ms step_avg:33.46ms
step:424/2160 train_time:14188ms step_avg:33.46ms
step:425/2160 train_time:14221ms step_avg:33.46ms
step:426/2160 train_time:14254ms step_avg:33.46ms
step:427/2160 train_time:14288ms step_avg:33.46ms
step:428/2160 train_time:14321ms step_avg:33.46ms
step:429/2160 train_time:14355ms step_avg:33.46ms
step:430/2160 train_time:14388ms step_avg:33.46ms
step:431/2160 train_time:14421ms step_avg:33.46ms
step:432/2160 train_time:14454ms step_avg:33.46ms
step:433/2160 train_time:14488ms step_avg:33.46ms
step:434/2160 train_time:14521ms step_avg:33.46ms
step:435/2160 train_time:14555ms step_avg:33.46ms
step:436/2160 train_time:14588ms step_avg:33.46ms
step:437/2160 train_time:14621ms step_avg:33.46ms
step:438/2160 train_time:14654ms step_avg:33.46ms
step:439/2160 train_time:14689ms step_avg:33.46ms
step:440/2160 train_time:14721ms step_avg:33.46ms
step:441/2160 train_time:14755ms step_avg:33.46ms
step:442/2160 train_time:14788ms step_avg:33.46ms
step:443/2160 train_time:14821ms step_avg:33.46ms
step:444/2160 train_time:14854ms step_avg:33.46ms
step:445/2160 train_time:14888ms step_avg:33.46ms
step:446/2160 train_time:14921ms step_avg:33.46ms
step:447/2160 train_time:14955ms step_avg:33.46ms
step:448/2160 train_time:14988ms step_avg:33.46ms
step:449/2160 train_time:15022ms step_avg:33.46ms
step:450/2160 train_time:15054ms step_avg:33.45ms
step:451/2160 train_time:15088ms step_avg:33.46ms
step:452/2160 train_time:15121ms step_avg:33.45ms
step:453/2160 train_time:15155ms step_avg:33.46ms
step:454/2160 train_time:15188ms step_avg:33.45ms
step:455/2160 train_time:15222ms step_avg:33.45ms
step:456/2160 train_time:15255ms step_avg:33.45ms
step:457/2160 train_time:15289ms step_avg:33.45ms
step:458/2160 train_time:15321ms step_avg:33.45ms
step:459/2160 train_time:15355ms step_avg:33.45ms
step:460/2160 train_time:15388ms step_avg:33.45ms
step:461/2160 train_time:15422ms step_avg:33.45ms
step:462/2160 train_time:15455ms step_avg:33.45ms
step:463/2160 train_time:15489ms step_avg:33.45ms
step:464/2160 train_time:15521ms step_avg:33.45ms
step:465/2160 train_time:15555ms step_avg:33.45ms
step:466/2160 train_time:15588ms step_avg:33.45ms
step:467/2160 train_time:15621ms step_avg:33.45ms
step:468/2160 train_time:15654ms step_avg:33.45ms
step:469/2160 train_time:15689ms step_avg:33.45ms
step:470/2160 train_time:15721ms step_avg:33.45ms
step:471/2160 train_time:15755ms step_avg:33.45ms
step:472/2160 train_time:15788ms step_avg:33.45ms
step:473/2160 train_time:15822ms step_avg:33.45ms
step:474/2160 train_time:15854ms step_avg:33.45ms
step:475/2160 train_time:15889ms step_avg:33.45ms
step:476/2160 train_time:15922ms step_avg:33.45ms
step:477/2160 train_time:15955ms step_avg:33.45ms
step:478/2160 train_time:15988ms step_avg:33.45ms
step:479/2160 train_time:16022ms step_avg:33.45ms
step:480/2160 train_time:16054ms step_avg:33.45ms
step:481/2160 train_time:16089ms step_avg:33.45ms
step:482/2160 train_time:16121ms step_avg:33.45ms
step:483/2160 train_time:16155ms step_avg:33.45ms
step:484/2160 train_time:16188ms step_avg:33.45ms
step:485/2160 train_time:16222ms step_avg:33.45ms
step:486/2160 train_time:16254ms step_avg:33.45ms
step:487/2160 train_time:16288ms step_avg:33.45ms
step:488/2160 train_time:16321ms step_avg:33.45ms
step:489/2160 train_time:16355ms step_avg:33.45ms
step:490/2160 train_time:16388ms step_avg:33.44ms
step:491/2160 train_time:16422ms step_avg:33.45ms
step:492/2160 train_time:16455ms step_avg:33.44ms
step:493/2160 train_time:16488ms step_avg:33.44ms
step:494/2160 train_time:16521ms step_avg:33.44ms
step:495/2160 train_time:16555ms step_avg:33.44ms
step:496/2160 train_time:16588ms step_avg:33.44ms
step:497/2160 train_time:16621ms step_avg:33.44ms
step:498/2160 train_time:16654ms step_avg:33.44ms
step:499/2160 train_time:16688ms step_avg:33.44ms
step:500/2160 train_time:16721ms step_avg:33.44ms
step:500/2160 val_loss:4.0109 train_time:16756ms step_avg:33.51ms
step:501/2160 train_time:16778ms step_avg:33.49ms
step:502/2160 train_time:16797ms step_avg:33.46ms
step:503/2160 train_time:16828ms step_avg:33.46ms
step:504/2160 train_time:16861ms step_avg:33.46ms
step:505/2160 train_time:16897ms step_avg:33.46ms
step:506/2160 train_time:16931ms step_avg:33.46ms
step:507/2160 train_time:16966ms step_avg:33.46ms
step:508/2160 train_time:16999ms step_avg:33.46ms
step:509/2160 train_time:17033ms step_avg:33.46ms
step:510/2160 train_time:17066ms step_avg:33.46ms
step:511/2160 train_time:17100ms step_avg:33.46ms
step:512/2160 train_time:17133ms step_avg:33.46ms
step:513/2160 train_time:17167ms step_avg:33.46ms
step:514/2160 train_time:17200ms step_avg:33.46ms
step:515/2160 train_time:17233ms step_avg:33.46ms
step:516/2160 train_time:17266ms step_avg:33.46ms
step:517/2160 train_time:17300ms step_avg:33.46ms
step:518/2160 train_time:17332ms step_avg:33.46ms
step:519/2160 train_time:17366ms step_avg:33.46ms
step:520/2160 train_time:17399ms step_avg:33.46ms
step:521/2160 train_time:17433ms step_avg:33.46ms
step:522/2160 train_time:17465ms step_avg:33.46ms
step:523/2160 train_time:17499ms step_avg:33.46ms
step:524/2160 train_time:17532ms step_avg:33.46ms
step:525/2160 train_time:17565ms step_avg:33.46ms
step:526/2160 train_time:17598ms step_avg:33.46ms
step:527/2160 train_time:17632ms step_avg:33.46ms
step:528/2160 train_time:17664ms step_avg:33.46ms
step:529/2160 train_time:17698ms step_avg:33.46ms
step:530/2160 train_time:17731ms step_avg:33.45ms
step:531/2160 train_time:17764ms step_avg:33.45ms
step:532/2160 train_time:17797ms step_avg:33.45ms
step:533/2160 train_time:17831ms step_avg:33.45ms
step:534/2160 train_time:17864ms step_avg:33.45ms
step:535/2160 train_time:17898ms step_avg:33.45ms
step:536/2160 train_time:17931ms step_avg:33.45ms
step:537/2160 train_time:17965ms step_avg:33.45ms
step:538/2160 train_time:17998ms step_avg:33.45ms
step:539/2160 train_time:18031ms step_avg:33.45ms
step:540/2160 train_time:18064ms step_avg:33.45ms
step:541/2160 train_time:18098ms step_avg:33.45ms
step:542/2160 train_time:18131ms step_avg:33.45ms
step:543/2160 train_time:18165ms step_avg:33.45ms
step:544/2160 train_time:18198ms step_avg:33.45ms
step:545/2160 train_time:18231ms step_avg:33.45ms
step:546/2160 train_time:18264ms step_avg:33.45ms
step:547/2160 train_time:18298ms step_avg:33.45ms
step:548/2160 train_time:18330ms step_avg:33.45ms
step:549/2160 train_time:18364ms step_avg:33.45ms
step:550/2160 train_time:18397ms step_avg:33.45ms
step:551/2160 train_time:18431ms step_avg:33.45ms
step:552/2160 train_time:18464ms step_avg:33.45ms
step:553/2160 train_time:18497ms step_avg:33.45ms
step:554/2160 train_time:18530ms step_avg:33.45ms
step:555/2160 train_time:18564ms step_avg:33.45ms
step:556/2160 train_time:18597ms step_avg:33.45ms
step:557/2160 train_time:18631ms step_avg:33.45ms
step:558/2160 train_time:18664ms step_avg:33.45ms
step:559/2160 train_time:18697ms step_avg:33.45ms
step:560/2160 train_time:18730ms step_avg:33.45ms
step:561/2160 train_time:18764ms step_avg:33.45ms
step:562/2160 train_time:18797ms step_avg:33.45ms
step:563/2160 train_time:18830ms step_avg:33.45ms
step:564/2160 train_time:18863ms step_avg:33.45ms
step:565/2160 train_time:18897ms step_avg:33.45ms
step:566/2160 train_time:18930ms step_avg:33.44ms
step:567/2160 train_time:18964ms step_avg:33.45ms
step:568/2160 train_time:18997ms step_avg:33.45ms
step:569/2160 train_time:19031ms step_avg:33.45ms
step:570/2160 train_time:19063ms step_avg:33.44ms
step:571/2160 train_time:19097ms step_avg:33.45ms
step:572/2160 train_time:19130ms step_avg:33.44ms
step:573/2160 train_time:19164ms step_avg:33.45ms
step:574/2160 train_time:19197ms step_avg:33.44ms
step:575/2160 train_time:19231ms step_avg:33.45ms
step:576/2160 train_time:19264ms step_avg:33.44ms
step:577/2160 train_time:19298ms step_avg:33.45ms
step:578/2160 train_time:19331ms step_avg:33.44ms
step:579/2160 train_time:19365ms step_avg:33.45ms
step:580/2160 train_time:19398ms step_avg:33.45ms
step:581/2160 train_time:19432ms step_avg:33.45ms
step:582/2160 train_time:19464ms step_avg:33.44ms
step:583/2160 train_time:19498ms step_avg:33.44ms
step:584/2160 train_time:19531ms step_avg:33.44ms
step:585/2160 train_time:19565ms step_avg:33.44ms
step:586/2160 train_time:19598ms step_avg:33.44ms
step:587/2160 train_time:19632ms step_avg:33.44ms
step:588/2160 train_time:19664ms step_avg:33.44ms
step:589/2160 train_time:19698ms step_avg:33.44ms
step:590/2160 train_time:19731ms step_avg:33.44ms
step:591/2160 train_time:19765ms step_avg:33.44ms
step:592/2160 train_time:19798ms step_avg:33.44ms
step:593/2160 train_time:19831ms step_avg:33.44ms
step:594/2160 train_time:19864ms step_avg:33.44ms
step:595/2160 train_time:19898ms step_avg:33.44ms
step:596/2160 train_time:19930ms step_avg:33.44ms
step:597/2160 train_time:19964ms step_avg:33.44ms
step:598/2160 train_time:19997ms step_avg:33.44ms
step:599/2160 train_time:20031ms step_avg:33.44ms
step:600/2160 train_time:20064ms step_avg:33.44ms
step:601/2160 train_time:20097ms step_avg:33.44ms
step:602/2160 train_time:20130ms step_avg:33.44ms
step:603/2160 train_time:20164ms step_avg:33.44ms
step:604/2160 train_time:20197ms step_avg:33.44ms
step:605/2160 train_time:20231ms step_avg:33.44ms
step:606/2160 train_time:20264ms step_avg:33.44ms
step:607/2160 train_time:20297ms step_avg:33.44ms
step:608/2160 train_time:20330ms step_avg:33.44ms
step:609/2160 train_time:20364ms step_avg:33.44ms
step:610/2160 train_time:20397ms step_avg:33.44ms
step:611/2160 train_time:20431ms step_avg:33.44ms
step:612/2160 train_time:20463ms step_avg:33.44ms
step:613/2160 train_time:20497ms step_avg:33.44ms
step:614/2160 train_time:20530ms step_avg:33.44ms
step:615/2160 train_time:20564ms step_avg:33.44ms
step:616/2160 train_time:20596ms step_avg:33.44ms
step:617/2160 train_time:20630ms step_avg:33.44ms
step:618/2160 train_time:20663ms step_avg:33.44ms
step:619/2160 train_time:20697ms step_avg:33.44ms
step:620/2160 train_time:20729ms step_avg:33.43ms
step:621/2160 train_time:20763ms step_avg:33.44ms
step:622/2160 train_time:20796ms step_avg:33.43ms
step:623/2160 train_time:20830ms step_avg:33.44ms
step:624/2160 train_time:20863ms step_avg:33.43ms
step:625/2160 train_time:20897ms step_avg:33.43ms
step:626/2160 train_time:20930ms step_avg:33.43ms
step:627/2160 train_time:20964ms step_avg:33.43ms
step:628/2160 train_time:20997ms step_avg:33.43ms
step:629/2160 train_time:21030ms step_avg:33.43ms
step:630/2160 train_time:21063ms step_avg:33.43ms
step:631/2160 train_time:21097ms step_avg:33.43ms
step:632/2160 train_time:21130ms step_avg:33.43ms
step:633/2160 train_time:21164ms step_avg:33.43ms
step:634/2160 train_time:21196ms step_avg:33.43ms
step:635/2160 train_time:21230ms step_avg:33.43ms
step:636/2160 train_time:21263ms step_avg:33.43ms
step:637/2160 train_time:21297ms step_avg:33.43ms
step:638/2160 train_time:21330ms step_avg:33.43ms
step:639/2160 train_time:21363ms step_avg:33.43ms
step:640/2160 train_time:21396ms step_avg:33.43ms
step:641/2160 train_time:21430ms step_avg:33.43ms
step:642/2160 train_time:21463ms step_avg:33.43ms
step:643/2160 train_time:21496ms step_avg:33.43ms
step:644/2160 train_time:21529ms step_avg:33.43ms
step:645/2160 train_time:21563ms step_avg:33.43ms
step:646/2160 train_time:21596ms step_avg:33.43ms
step:647/2160 train_time:21630ms step_avg:33.43ms
step:648/2160 train_time:21663ms step_avg:33.43ms
step:649/2160 train_time:21696ms step_avg:33.43ms
step:650/2160 train_time:21729ms step_avg:33.43ms
step:651/2160 train_time:21763ms step_avg:33.43ms
step:652/2160 train_time:21795ms step_avg:33.43ms
step:653/2160 train_time:21829ms step_avg:33.43ms
step:654/2160 train_time:21862ms step_avg:33.43ms
step:655/2160 train_time:21896ms step_avg:33.43ms
step:656/2160 train_time:21928ms step_avg:33.43ms
step:657/2160 train_time:21962ms step_avg:33.43ms
step:658/2160 train_time:21995ms step_avg:33.43ms
step:659/2160 train_time:22029ms step_avg:33.43ms
step:660/2160 train_time:22062ms step_avg:33.43ms
step:661/2160 train_time:22095ms step_avg:33.43ms
step:662/2160 train_time:22128ms step_avg:33.43ms
step:663/2160 train_time:22162ms step_avg:33.43ms
step:664/2160 train_time:22195ms step_avg:33.43ms
step:665/2160 train_time:22229ms step_avg:33.43ms
step:666/2160 train_time:22262ms step_avg:33.43ms
step:667/2160 train_time:22295ms step_avg:33.43ms
step:668/2160 train_time:22328ms step_avg:33.43ms
step:669/2160 train_time:22362ms step_avg:33.43ms
step:670/2160 train_time:22395ms step_avg:33.42ms
step:671/2160 train_time:22428ms step_avg:33.43ms
step:672/2160 train_time:22461ms step_avg:33.42ms
step:673/2160 train_time:22495ms step_avg:33.42ms
step:674/2160 train_time:22528ms step_avg:33.42ms
step:675/2160 train_time:22561ms step_avg:33.42ms
step:676/2160 train_time:22594ms step_avg:33.42ms
step:677/2160 train_time:22628ms step_avg:33.42ms
step:678/2160 train_time:22661ms step_avg:33.42ms
step:679/2160 train_time:22694ms step_avg:33.42ms
step:680/2160 train_time:22727ms step_avg:33.42ms
step:681/2160 train_time:22761ms step_avg:33.42ms
step:682/2160 train_time:22794ms step_avg:33.42ms
step:683/2160 train_time:22827ms step_avg:33.42ms
step:684/2160 train_time:22860ms step_avg:33.42ms
step:685/2160 train_time:22894ms step_avg:33.42ms
step:686/2160 train_time:22927ms step_avg:33.42ms
step:687/2160 train_time:22960ms step_avg:33.42ms
step:688/2160 train_time:22993ms step_avg:33.42ms
step:689/2160 train_time:23027ms step_avg:33.42ms
step:690/2160 train_time:23060ms step_avg:33.42ms
step:691/2160 train_time:23093ms step_avg:33.42ms
step:692/2160 train_time:23126ms step_avg:33.42ms
step:693/2160 train_time:23160ms step_avg:33.42ms
step:694/2160 train_time:23193ms step_avg:33.42ms
step:695/2160 train_time:23227ms step_avg:33.42ms
step:696/2160 train_time:23260ms step_avg:33.42ms
step:697/2160 train_time:23293ms step_avg:33.42ms
step:698/2160 train_time:23326ms step_avg:33.42ms
step:699/2160 train_time:23360ms step_avg:33.42ms
step:700/2160 train_time:23393ms step_avg:33.42ms
step:701/2160 train_time:23426ms step_avg:33.42ms
step:702/2160 train_time:23459ms step_avg:33.42ms
step:703/2160 train_time:23493ms step_avg:33.42ms
step:704/2160 train_time:23526ms step_avg:33.42ms
step:705/2160 train_time:23560ms step_avg:33.42ms
step:706/2160 train_time:23592ms step_avg:33.42ms
step:707/2160 train_time:23626ms step_avg:33.42ms
step:708/2160 train_time:23660ms step_avg:33.42ms
step:709/2160 train_time:23718ms step_avg:33.45ms
step:710/2160 train_time:23777ms step_avg:33.49ms
step:711/2160 train_time:23837ms step_avg:33.53ms
step:712/2160 train_time:23896ms step_avg:33.56ms
step:713/2160 train_time:23957ms step_avg:33.60ms
step:714/2160 train_time:24015ms step_avg:33.63ms
step:715/2160 train_time:24076ms step_avg:33.67ms
step:716/2160 train_time:24135ms step_avg:33.71ms
step:717/2160 train_time:24196ms step_avg:33.75ms
step:718/2160 train_time:24255ms step_avg:33.78ms
step:719/2160 train_time:24315ms step_avg:33.82ms
step:720/2160 train_time:24374ms step_avg:33.85ms
step:721/2160 train_time:24435ms step_avg:33.89ms
step:722/2160 train_time:24495ms step_avg:33.93ms
step:723/2160 train_time:24556ms step_avg:33.96ms
step:724/2160 train_time:24615ms step_avg:34.00ms
step:725/2160 train_time:24676ms step_avg:34.04ms
step:726/2160 train_time:24734ms step_avg:34.07ms
step:727/2160 train_time:24795ms step_avg:34.11ms
step:728/2160 train_time:24853ms step_avg:34.14ms
step:729/2160 train_time:24914ms step_avg:34.18ms
step:730/2160 train_time:24973ms step_avg:34.21ms
step:731/2160 train_time:25034ms step_avg:34.25ms
step:732/2160 train_time:25092ms step_avg:34.28ms
step:733/2160 train_time:25153ms step_avg:34.32ms
step:734/2160 train_time:25211ms step_avg:34.35ms
step:735/2160 train_time:25272ms step_avg:34.38ms
step:736/2160 train_time:25330ms step_avg:34.42ms
step:737/2160 train_time:25391ms step_avg:34.45ms
step:738/2160 train_time:25451ms step_avg:34.49ms
step:739/2160 train_time:25512ms step_avg:34.52ms
step:740/2160 train_time:25571ms step_avg:34.56ms
step:741/2160 train_time:25632ms step_avg:34.59ms
step:742/2160 train_time:25691ms step_avg:34.62ms
step:743/2160 train_time:25751ms step_avg:34.66ms
step:744/2160 train_time:25810ms step_avg:34.69ms
step:745/2160 train_time:25871ms step_avg:34.73ms
step:746/2160 train_time:25930ms step_avg:34.76ms
step:747/2160 train_time:25991ms step_avg:34.79ms
step:748/2160 train_time:26050ms step_avg:34.83ms
step:749/2160 train_time:26110ms step_avg:34.86ms
step:750/2160 train_time:26169ms step_avg:34.89ms
step:750/2160 val_loss:3.8524 train_time:26231ms step_avg:34.97ms
step:751/2160 train_time:26253ms step_avg:34.96ms
step:752/2160 train_time:26294ms step_avg:34.96ms
step:753/2160 train_time:26357ms step_avg:35.00ms
step:754/2160 train_time:26420ms step_avg:35.04ms
step:755/2160 train_time:26481ms step_avg:35.07ms
step:756/2160 train_time:26540ms step_avg:35.11ms
step:757/2160 train_time:26600ms step_avg:35.14ms
step:758/2160 train_time:26659ms step_avg:35.17ms
step:759/2160 train_time:26719ms step_avg:35.20ms
step:760/2160 train_time:26777ms step_avg:35.23ms
step:761/2160 train_time:26837ms step_avg:35.27ms
step:762/2160 train_time:26895ms step_avg:35.30ms
step:763/2160 train_time:26955ms step_avg:35.33ms
step:764/2160 train_time:27013ms step_avg:35.36ms
step:765/2160 train_time:27073ms step_avg:35.39ms
step:766/2160 train_time:27132ms step_avg:35.42ms
step:767/2160 train_time:27193ms step_avg:35.45ms
step:768/2160 train_time:27252ms step_avg:35.48ms
step:769/2160 train_time:27315ms step_avg:35.52ms
step:770/2160 train_time:27375ms step_avg:35.55ms
step:771/2160 train_time:27437ms step_avg:35.59ms
step:772/2160 train_time:27497ms step_avg:35.62ms
step:773/2160 train_time:27558ms step_avg:35.65ms
step:774/2160 train_time:27617ms step_avg:35.68ms
step:775/2160 train_time:27677ms step_avg:35.71ms
step:776/2160 train_time:27736ms step_avg:35.74ms
step:777/2160 train_time:27796ms step_avg:35.77ms
step:778/2160 train_time:27854ms step_avg:35.80ms
step:779/2160 train_time:27915ms step_avg:35.83ms
step:780/2160 train_time:27973ms step_avg:35.86ms
step:781/2160 train_time:28033ms step_avg:35.89ms
step:782/2160 train_time:28092ms step_avg:35.92ms
step:783/2160 train_time:28152ms step_avg:35.95ms
step:784/2160 train_time:28211ms step_avg:35.98ms
step:785/2160 train_time:28272ms step_avg:36.01ms
step:786/2160 train_time:28331ms step_avg:36.04ms
step:787/2160 train_time:28393ms step_avg:36.08ms
step:788/2160 train_time:28453ms step_avg:36.11ms
step:789/2160 train_time:28514ms step_avg:36.14ms
step:790/2160 train_time:28574ms step_avg:36.17ms
step:791/2160 train_time:28635ms step_avg:36.20ms
step:792/2160 train_time:28693ms step_avg:36.23ms
step:793/2160 train_time:28754ms step_avg:36.26ms
step:794/2160 train_time:28813ms step_avg:36.29ms
step:795/2160 train_time:28873ms step_avg:36.32ms
step:796/2160 train_time:28932ms step_avg:36.35ms
step:797/2160 train_time:28992ms step_avg:36.38ms
step:798/2160 train_time:29051ms step_avg:36.40ms
step:799/2160 train_time:29111ms step_avg:36.43ms
step:800/2160 train_time:29170ms step_avg:36.46ms
step:801/2160 train_time:29230ms step_avg:36.49ms
step:802/2160 train_time:29289ms step_avg:36.52ms
step:803/2160 train_time:29350ms step_avg:36.55ms
step:804/2160 train_time:29409ms step_avg:36.58ms
step:805/2160 train_time:29470ms step_avg:36.61ms
step:806/2160 train_time:29530ms step_avg:36.64ms
step:807/2160 train_time:29591ms step_avg:36.67ms
step:808/2160 train_time:29650ms step_avg:36.70ms
step:809/2160 train_time:29710ms step_avg:36.72ms
step:810/2160 train_time:29769ms step_avg:36.75ms
step:811/2160 train_time:29829ms step_avg:36.78ms
step:812/2160 train_time:29888ms step_avg:36.81ms
step:813/2160 train_time:29948ms step_avg:36.84ms
step:814/2160 train_time:30007ms step_avg:36.86ms
step:815/2160 train_time:30066ms step_avg:36.89ms
step:816/2160 train_time:30124ms step_avg:36.92ms
step:817/2160 train_time:30184ms step_avg:36.95ms
step:818/2160 train_time:30243ms step_avg:36.97ms
step:819/2160 train_time:30303ms step_avg:37.00ms
step:820/2160 train_time:30362ms step_avg:37.03ms
step:821/2160 train_time:30424ms step_avg:37.06ms
step:822/2160 train_time:30483ms step_avg:37.08ms
step:823/2160 train_time:30544ms step_avg:37.11ms
step:824/2160 train_time:30604ms step_avg:37.14ms
step:825/2160 train_time:30665ms step_avg:37.17ms
step:826/2160 train_time:30724ms step_avg:37.20ms
step:827/2160 train_time:30784ms step_avg:37.22ms
step:828/2160 train_time:30843ms step_avg:37.25ms
step:829/2160 train_time:30904ms step_avg:37.28ms
step:830/2160 train_time:30962ms step_avg:37.30ms
step:831/2160 train_time:31022ms step_avg:37.33ms
step:832/2160 train_time:31080ms step_avg:37.36ms
step:833/2160 train_time:31141ms step_avg:37.38ms
step:834/2160 train_time:31199ms step_avg:37.41ms
step:835/2160 train_time:31260ms step_avg:37.44ms
step:836/2160 train_time:31318ms step_avg:37.46ms
step:837/2160 train_time:31379ms step_avg:37.49ms
step:838/2160 train_time:31438ms step_avg:37.52ms
step:839/2160 train_time:31499ms step_avg:37.54ms
step:840/2160 train_time:31558ms step_avg:37.57ms
step:841/2160 train_time:31619ms step_avg:37.60ms
step:842/2160 train_time:31678ms step_avg:37.62ms
step:843/2160 train_time:31739ms step_avg:37.65ms
step:844/2160 train_time:31798ms step_avg:37.67ms
step:845/2160 train_time:31858ms step_avg:37.70ms
step:846/2160 train_time:31917ms step_avg:37.73ms
step:847/2160 train_time:31978ms step_avg:37.75ms
step:848/2160 train_time:32036ms step_avg:37.78ms
step:849/2160 train_time:32097ms step_avg:37.81ms
step:850/2160 train_time:32155ms step_avg:37.83ms
step:851/2160 train_time:32216ms step_avg:37.86ms
step:852/2160 train_time:32274ms step_avg:37.88ms
step:853/2160 train_time:32335ms step_avg:37.91ms
step:854/2160 train_time:32394ms step_avg:37.93ms
step:855/2160 train_time:32454ms step_avg:37.96ms
step:856/2160 train_time:32514ms step_avg:37.98ms
step:857/2160 train_time:32574ms step_avg:38.01ms
step:858/2160 train_time:32633ms step_avg:38.03ms
step:859/2160 train_time:32695ms step_avg:38.06ms
step:860/2160 train_time:32753ms step_avg:38.09ms
step:861/2160 train_time:32814ms step_avg:38.11ms
step:862/2160 train_time:32873ms step_avg:38.14ms
step:863/2160 train_time:32934ms step_avg:38.16ms
step:864/2160 train_time:32993ms step_avg:38.19ms
step:865/2160 train_time:33054ms step_avg:38.21ms
step:866/2160 train_time:33112ms step_avg:38.24ms
step:867/2160 train_time:33173ms step_avg:38.26ms
step:868/2160 train_time:33232ms step_avg:38.29ms
step:869/2160 train_time:33293ms step_avg:38.31ms
step:870/2160 train_time:33352ms step_avg:38.34ms
step:871/2160 train_time:33412ms step_avg:38.36ms
step:872/2160 train_time:33470ms step_avg:38.38ms
step:873/2160 train_time:33531ms step_avg:38.41ms
step:874/2160 train_time:33590ms step_avg:38.43ms
step:875/2160 train_time:33651ms step_avg:38.46ms
step:876/2160 train_time:33710ms step_avg:38.48ms
step:877/2160 train_time:33770ms step_avg:38.51ms
step:878/2160 train_time:33829ms step_avg:38.53ms
step:879/2160 train_time:33890ms step_avg:38.55ms
step:880/2160 train_time:33948ms step_avg:38.58ms
step:881/2160 train_time:34009ms step_avg:38.60ms
step:882/2160 train_time:34067ms step_avg:38.63ms
step:883/2160 train_time:34128ms step_avg:38.65ms
step:884/2160 train_time:34187ms step_avg:38.67ms
step:885/2160 train_time:34247ms step_avg:38.70ms
step:886/2160 train_time:34306ms step_avg:38.72ms
step:887/2160 train_time:34366ms step_avg:38.74ms
step:888/2160 train_time:34425ms step_avg:38.77ms
step:889/2160 train_time:34486ms step_avg:38.79ms
step:890/2160 train_time:34545ms step_avg:38.81ms
step:891/2160 train_time:34605ms step_avg:38.84ms
step:892/2160 train_time:34664ms step_avg:38.86ms
step:893/2160 train_time:34725ms step_avg:38.89ms
step:894/2160 train_time:34784ms step_avg:38.91ms
step:895/2160 train_time:34845ms step_avg:38.93ms
step:896/2160 train_time:34904ms step_avg:38.96ms
step:897/2160 train_time:34964ms step_avg:38.98ms
step:898/2160 train_time:35023ms step_avg:39.00ms
step:899/2160 train_time:35085ms step_avg:39.03ms
step:900/2160 train_time:35143ms step_avg:39.05ms
step:901/2160 train_time:35203ms step_avg:39.07ms
step:902/2160 train_time:35262ms step_avg:39.09ms
step:903/2160 train_time:35322ms step_avg:39.12ms
step:904/2160 train_time:35381ms step_avg:39.14ms
step:905/2160 train_time:35442ms step_avg:39.16ms
step:906/2160 train_time:35500ms step_avg:39.18ms
step:907/2160 train_time:35560ms step_avg:39.21ms
step:908/2160 train_time:35619ms step_avg:39.23ms
step:909/2160 train_time:35680ms step_avg:39.25ms
step:910/2160 train_time:35738ms step_avg:39.27ms
step:911/2160 train_time:35800ms step_avg:39.30ms
step:912/2160 train_time:35859ms step_avg:39.32ms
step:913/2160 train_time:35919ms step_avg:39.34ms
step:914/2160 train_time:35978ms step_avg:39.36ms
step:915/2160 train_time:36039ms step_avg:39.39ms
step:916/2160 train_time:36098ms step_avg:39.41ms
step:917/2160 train_time:36159ms step_avg:39.43ms
step:918/2160 train_time:36218ms step_avg:39.45ms
step:919/2160 train_time:36279ms step_avg:39.48ms
step:920/2160 train_time:36338ms step_avg:39.50ms
step:921/2160 train_time:36398ms step_avg:39.52ms
step:922/2160 train_time:36457ms step_avg:39.54ms
step:923/2160 train_time:36517ms step_avg:39.56ms
step:924/2160 train_time:36576ms step_avg:39.58ms
step:925/2160 train_time:36637ms step_avg:39.61ms
step:926/2160 train_time:36695ms step_avg:39.63ms
step:927/2160 train_time:36756ms step_avg:39.65ms
step:928/2160 train_time:36815ms step_avg:39.67ms
step:929/2160 train_time:36876ms step_avg:39.69ms
step:930/2160 train_time:36934ms step_avg:39.71ms
step:931/2160 train_time:36996ms step_avg:39.74ms
step:932/2160 train_time:37055ms step_avg:39.76ms
step:933/2160 train_time:37116ms step_avg:39.78ms
step:934/2160 train_time:37176ms step_avg:39.80ms
step:935/2160 train_time:37237ms step_avg:39.83ms
step:936/2160 train_time:37295ms step_avg:39.85ms
step:937/2160 train_time:37356ms step_avg:39.87ms
step:938/2160 train_time:37415ms step_avg:39.89ms
step:939/2160 train_time:37475ms step_avg:39.91ms
step:940/2160 train_time:37533ms step_avg:39.93ms
step:941/2160 train_time:37594ms step_avg:39.95ms
step:942/2160 train_time:37652ms step_avg:39.97ms
step:943/2160 train_time:37713ms step_avg:39.99ms
step:944/2160 train_time:37772ms step_avg:40.01ms
step:945/2160 train_time:37833ms step_avg:40.04ms
step:946/2160 train_time:37892ms step_avg:40.05ms
step:947/2160 train_time:37952ms step_avg:40.08ms
step:948/2160 train_time:38011ms step_avg:40.10ms
step:949/2160 train_time:38072ms step_avg:40.12ms
step:950/2160 train_time:38131ms step_avg:40.14ms
step:951/2160 train_time:38192ms step_avg:40.16ms
step:952/2160 train_time:38251ms step_avg:40.18ms
step:953/2160 train_time:38312ms step_avg:40.20ms
step:954/2160 train_time:38371ms step_avg:40.22ms
step:955/2160 train_time:38432ms step_avg:40.24ms
step:956/2160 train_time:38490ms step_avg:40.26ms
step:957/2160 train_time:38551ms step_avg:40.28ms
step:958/2160 train_time:38609ms step_avg:40.30ms
step:959/2160 train_time:38670ms step_avg:40.32ms
step:960/2160 train_time:38728ms step_avg:40.34ms
step:961/2160 train_time:38789ms step_avg:40.36ms
step:962/2160 train_time:38847ms step_avg:40.38ms
step:963/2160 train_time:38908ms step_avg:40.40ms
step:964/2160 train_time:38967ms step_avg:40.42ms
step:965/2160 train_time:39027ms step_avg:40.44ms
step:966/2160 train_time:39086ms step_avg:40.46ms
step:967/2160 train_time:39147ms step_avg:40.48ms
step:968/2160 train_time:39207ms step_avg:40.50ms
step:969/2160 train_time:39268ms step_avg:40.52ms
step:970/2160 train_time:39326ms step_avg:40.54ms
step:971/2160 train_time:39387ms step_avg:40.56ms
step:972/2160 train_time:39445ms step_avg:40.58ms
step:973/2160 train_time:39506ms step_avg:40.60ms
step:974/2160 train_time:39565ms step_avg:40.62ms
step:975/2160 train_time:39625ms step_avg:40.64ms
step:976/2160 train_time:39684ms step_avg:40.66ms
step:977/2160 train_time:39744ms step_avg:40.68ms
step:978/2160 train_time:39803ms step_avg:40.70ms
step:979/2160 train_time:39863ms step_avg:40.72ms
step:980/2160 train_time:39922ms step_avg:40.74ms
step:981/2160 train_time:39983ms step_avg:40.76ms
step:982/2160 train_time:40042ms step_avg:40.78ms
step:983/2160 train_time:40102ms step_avg:40.80ms
step:984/2160 train_time:40161ms step_avg:40.81ms
step:985/2160 train_time:40221ms step_avg:40.83ms
step:986/2160 train_time:40280ms step_avg:40.85ms
step:987/2160 train_time:40341ms step_avg:40.87ms
step:988/2160 train_time:40399ms step_avg:40.89ms
step:989/2160 train_time:40460ms step_avg:40.91ms
step:990/2160 train_time:40518ms step_avg:40.93ms
step:991/2160 train_time:40580ms step_avg:40.95ms
step:992/2160 train_time:40639ms step_avg:40.97ms
step:993/2160 train_time:40699ms step_avg:40.99ms
step:994/2160 train_time:40757ms step_avg:41.00ms
step:995/2160 train_time:40817ms step_avg:41.02ms
step:996/2160 train_time:40877ms step_avg:41.04ms
step:997/2160 train_time:40938ms step_avg:41.06ms
step:998/2160 train_time:40997ms step_avg:41.08ms
step:999/2160 train_time:41057ms step_avg:41.10ms
step:1000/2160 train_time:41116ms step_avg:41.12ms
step:1000/2160 val_loss:3.6886 train_time:41178ms step_avg:41.18ms
step:1001/2160 train_time:41199ms step_avg:41.16ms
step:1002/2160 train_time:41239ms step_avg:41.16ms
step:1003/2160 train_time:41303ms step_avg:41.18ms
step:1004/2160 train_time:41366ms step_avg:41.20ms
step:1005/2160 train_time:41427ms step_avg:41.22ms
step:1006/2160 train_time:41486ms step_avg:41.24ms
step:1007/2160 train_time:41546ms step_avg:41.26ms
step:1008/2160 train_time:41604ms step_avg:41.27ms
step:1009/2160 train_time:41664ms step_avg:41.29ms
step:1010/2160 train_time:41722ms step_avg:41.31ms
step:1011/2160 train_time:41781ms step_avg:41.33ms
step:1012/2160 train_time:41839ms step_avg:41.34ms
step:1013/2160 train_time:41899ms step_avg:41.36ms
step:1014/2160 train_time:41957ms step_avg:41.38ms
step:1015/2160 train_time:42016ms step_avg:41.40ms
step:1016/2160 train_time:42074ms step_avg:41.41ms
step:1017/2160 train_time:42135ms step_avg:41.43ms
step:1018/2160 train_time:42195ms step_avg:41.45ms
step:1019/2160 train_time:42256ms step_avg:41.47ms
step:1020/2160 train_time:42318ms step_avg:41.49ms
step:1021/2160 train_time:42381ms step_avg:41.51ms
step:1022/2160 train_time:42440ms step_avg:41.53ms
step:1023/2160 train_time:42501ms step_avg:41.55ms
step:1024/2160 train_time:42559ms step_avg:41.56ms
step:1025/2160 train_time:42621ms step_avg:41.58ms
step:1026/2160 train_time:42679ms step_avg:41.60ms
step:1027/2160 train_time:42739ms step_avg:41.62ms
step:1028/2160 train_time:42797ms step_avg:41.63ms
step:1029/2160 train_time:42857ms step_avg:41.65ms
step:1030/2160 train_time:42915ms step_avg:41.66ms
step:1031/2160 train_time:42975ms step_avg:41.68ms
step:1032/2160 train_time:43033ms step_avg:41.70ms
step:1033/2160 train_time:43093ms step_avg:41.72ms
step:1034/2160 train_time:43152ms step_avg:41.73ms
step:1035/2160 train_time:43212ms step_avg:41.75ms
step:1036/2160 train_time:43271ms step_avg:41.77ms
step:1037/2160 train_time:43333ms step_avg:41.79ms
step:1038/2160 train_time:43392ms step_avg:41.80ms
step:1039/2160 train_time:43454ms step_avg:41.82ms
step:1040/2160 train_time:43513ms step_avg:41.84ms
step:1041/2160 train_time:43574ms step_avg:41.86ms
step:1042/2160 train_time:43633ms step_avg:41.87ms
step:1043/2160 train_time:43693ms step_avg:41.89ms
step:1044/2160 train_time:43752ms step_avg:41.91ms
step:1045/2160 train_time:43813ms step_avg:41.93ms
step:1046/2160 train_time:43871ms step_avg:41.94ms
step:1047/2160 train_time:43931ms step_avg:41.96ms
step:1048/2160 train_time:43989ms step_avg:41.97ms
step:1049/2160 train_time:44050ms step_avg:41.99ms
step:1050/2160 train_time:44108ms step_avg:42.01ms
step:1051/2160 train_time:44169ms step_avg:42.03ms
step:1052/2160 train_time:44228ms step_avg:42.04ms
step:1053/2160 train_time:44289ms step_avg:42.06ms
step:1054/2160 train_time:44349ms step_avg:42.08ms
step:1055/2160 train_time:44410ms step_avg:42.09ms
step:1056/2160 train_time:44469ms step_avg:42.11ms
step:1057/2160 train_time:44530ms step_avg:42.13ms
step:1058/2160 train_time:44589ms step_avg:42.14ms
step:1059/2160 train_time:44650ms step_avg:42.16ms
step:1060/2160 train_time:44709ms step_avg:42.18ms
step:1061/2160 train_time:44769ms step_avg:42.20ms
step:1062/2160 train_time:44828ms step_avg:42.21ms
step:1063/2160 train_time:44888ms step_avg:42.23ms
step:1064/2160 train_time:44946ms step_avg:42.24ms
step:1065/2160 train_time:45006ms step_avg:42.26ms
step:1066/2160 train_time:45065ms step_avg:42.27ms
step:1067/2160 train_time:45125ms step_avg:42.29ms
step:1068/2160 train_time:45183ms step_avg:42.31ms
step:1069/2160 train_time:45244ms step_avg:42.32ms
step:1070/2160 train_time:45303ms step_avg:42.34ms
step:1071/2160 train_time:45364ms step_avg:42.36ms
step:1072/2160 train_time:45422ms step_avg:42.37ms
step:1073/2160 train_time:45483ms step_avg:42.39ms
step:1074/2160 train_time:45543ms step_avg:42.40ms
step:1075/2160 train_time:45604ms step_avg:42.42ms
step:1076/2160 train_time:45663ms step_avg:42.44ms
step:1077/2160 train_time:45724ms step_avg:42.45ms
step:1078/2160 train_time:45782ms step_avg:42.47ms
step:1079/2160 train_time:45842ms step_avg:42.49ms
step:1080/2160 train_time:45902ms step_avg:42.50ms
step:1081/2160 train_time:45961ms step_avg:42.52ms
step:1082/2160 train_time:46019ms step_avg:42.53ms
step:1083/2160 train_time:46079ms step_avg:42.55ms
step:1084/2160 train_time:46138ms step_avg:42.56ms
step:1085/2160 train_time:46199ms step_avg:42.58ms
step:1086/2160 train_time:46257ms step_avg:42.59ms
step:1087/2160 train_time:46318ms step_avg:42.61ms
step:1088/2160 train_time:46377ms step_avg:42.63ms
step:1089/2160 train_time:46438ms step_avg:42.64ms
step:1090/2160 train_time:46497ms step_avg:42.66ms
step:1091/2160 train_time:46557ms step_avg:42.67ms
step:1092/2160 train_time:46616ms step_avg:42.69ms
step:1093/2160 train_time:46677ms step_avg:42.71ms
step:1094/2160 train_time:46736ms step_avg:42.72ms
step:1095/2160 train_time:46796ms step_avg:42.74ms
step:1096/2160 train_time:46855ms step_avg:42.75ms
step:1097/2160 train_time:46916ms step_avg:42.77ms
step:1098/2160 train_time:46974ms step_avg:42.78ms
step:1099/2160 train_time:47035ms step_avg:42.80ms
step:1100/2160 train_time:47093ms step_avg:42.81ms
step:1101/2160 train_time:47154ms step_avg:42.83ms
step:1102/2160 train_time:47213ms step_avg:42.84ms
step:1103/2160 train_time:47274ms step_avg:42.86ms
step:1104/2160 train_time:47332ms step_avg:42.87ms
step:1105/2160 train_time:47394ms step_avg:42.89ms
step:1106/2160 train_time:47452ms step_avg:42.90ms
step:1107/2160 train_time:47513ms step_avg:42.92ms
step:1108/2160 train_time:47572ms step_avg:42.93ms
step:1109/2160 train_time:47632ms step_avg:42.95ms
step:1110/2160 train_time:47691ms step_avg:42.96ms
step:1111/2160 train_time:47753ms step_avg:42.98ms
step:1112/2160 train_time:47811ms step_avg:43.00ms
step:1113/2160 train_time:47871ms step_avg:43.01ms
step:1114/2160 train_time:47930ms step_avg:43.03ms
step:1115/2160 train_time:47991ms step_avg:43.04ms
step:1116/2160 train_time:48050ms step_avg:43.06ms
step:1117/2160 train_time:48111ms step_avg:43.07ms
step:1118/2160 train_time:48170ms step_avg:43.09ms
step:1119/2160 train_time:48231ms step_avg:43.10ms
step:1120/2160 train_time:48289ms step_avg:43.12ms
step:1121/2160 train_time:48350ms step_avg:43.13ms
step:1122/2160 train_time:48409ms step_avg:43.14ms
step:1123/2160 train_time:48470ms step_avg:43.16ms
step:1124/2160 train_time:48528ms step_avg:43.17ms
step:1125/2160 train_time:48589ms step_avg:43.19ms
step:1126/2160 train_time:48648ms step_avg:43.20ms
step:1127/2160 train_time:48709ms step_avg:43.22ms
step:1128/2160 train_time:48768ms step_avg:43.23ms
step:1129/2160 train_time:48828ms step_avg:43.25ms
step:1130/2160 train_time:48887ms step_avg:43.26ms
step:1131/2160 train_time:48947ms step_avg:43.28ms
step:1132/2160 train_time:49006ms step_avg:43.29ms
step:1133/2160 train_time:49066ms step_avg:43.31ms
step:1134/2160 train_time:49125ms step_avg:43.32ms
step:1135/2160 train_time:49185ms step_avg:43.34ms
step:1136/2160 train_time:49244ms step_avg:43.35ms
step:1137/2160 train_time:49304ms step_avg:43.36ms
step:1138/2160 train_time:49363ms step_avg:43.38ms
step:1139/2160 train_time:49423ms step_avg:43.39ms
step:1140/2160 train_time:49483ms step_avg:43.41ms
step:1141/2160 train_time:49543ms step_avg:43.42ms
step:1142/2160 train_time:49601ms step_avg:43.43ms
step:1143/2160 train_time:49661ms step_avg:43.45ms
step:1144/2160 train_time:49720ms step_avg:43.46ms
step:1145/2160 train_time:49781ms step_avg:43.48ms
step:1146/2160 train_time:49839ms step_avg:43.49ms
step:1147/2160 train_time:49901ms step_avg:43.51ms
step:1148/2160 train_time:49959ms step_avg:43.52ms
step:1149/2160 train_time:50020ms step_avg:43.53ms
step:1150/2160 train_time:50078ms step_avg:43.55ms
step:1151/2160 train_time:50139ms step_avg:43.56ms
step:1152/2160 train_time:50198ms step_avg:43.57ms
step:1153/2160 train_time:50260ms step_avg:43.59ms
step:1154/2160 train_time:50319ms step_avg:43.60ms
step:1155/2160 train_time:50379ms step_avg:43.62ms
step:1156/2160 train_time:50438ms step_avg:43.63ms
step:1157/2160 train_time:50499ms step_avg:43.65ms
step:1158/2160 train_time:50558ms step_avg:43.66ms
step:1159/2160 train_time:50618ms step_avg:43.67ms
step:1160/2160 train_time:50677ms step_avg:43.69ms
step:1161/2160 train_time:50738ms step_avg:43.70ms
step:1162/2160 train_time:50796ms step_avg:43.71ms
step:1163/2160 train_time:50857ms step_avg:43.73ms
step:1164/2160 train_time:50916ms step_avg:43.74ms
step:1165/2160 train_time:50976ms step_avg:43.76ms
step:1166/2160 train_time:51035ms step_avg:43.77ms
step:1167/2160 train_time:51096ms step_avg:43.78ms
step:1168/2160 train_time:51155ms step_avg:43.80ms
step:1169/2160 train_time:51215ms step_avg:43.81ms
step:1170/2160 train_time:51274ms step_avg:43.82ms
step:1171/2160 train_time:51336ms step_avg:43.84ms
step:1172/2160 train_time:51395ms step_avg:43.85ms
step:1173/2160 train_time:51455ms step_avg:43.87ms
step:1174/2160 train_time:51514ms step_avg:43.88ms
step:1175/2160 train_time:51575ms step_avg:43.89ms
step:1176/2160 train_time:51634ms step_avg:43.91ms
step:1177/2160 train_time:51695ms step_avg:43.92ms
step:1178/2160 train_time:51753ms step_avg:43.93ms
step:1179/2160 train_time:51814ms step_avg:43.95ms
step:1180/2160 train_time:51872ms step_avg:43.96ms
step:1181/2160 train_time:51933ms step_avg:43.97ms
step:1182/2160 train_time:51991ms step_avg:43.99ms
step:1183/2160 train_time:52052ms step_avg:44.00ms
step:1184/2160 train_time:52111ms step_avg:44.01ms
step:1185/2160 train_time:52172ms step_avg:44.03ms
step:1186/2160 train_time:52231ms step_avg:44.04ms
step:1187/2160 train_time:52292ms step_avg:44.05ms
step:1188/2160 train_time:52351ms step_avg:44.07ms
step:1189/2160 train_time:52412ms step_avg:44.08ms
step:1190/2160 train_time:52471ms step_avg:44.09ms
step:1191/2160 train_time:52532ms step_avg:44.11ms
step:1192/2160 train_time:52590ms step_avg:44.12ms
step:1193/2160 train_time:52651ms step_avg:44.13ms
step:1194/2160 train_time:52710ms step_avg:44.15ms
step:1195/2160 train_time:52772ms step_avg:44.16ms
step:1196/2160 train_time:52830ms step_avg:44.17ms
step:1197/2160 train_time:52890ms step_avg:44.19ms
step:1198/2160 train_time:52949ms step_avg:44.20ms
step:1199/2160 train_time:53010ms step_avg:44.21ms
step:1200/2160 train_time:53068ms step_avg:44.22ms
step:1201/2160 train_time:53129ms step_avg:44.24ms
step:1202/2160 train_time:53188ms step_avg:44.25ms
step:1203/2160 train_time:53249ms step_avg:44.26ms
step:1204/2160 train_time:53308ms step_avg:44.28ms
step:1205/2160 train_time:53369ms step_avg:44.29ms
step:1206/2160 train_time:53428ms step_avg:44.30ms
step:1207/2160 train_time:53489ms step_avg:44.32ms
step:1208/2160 train_time:53547ms step_avg:44.33ms
step:1209/2160 train_time:53609ms step_avg:44.34ms
step:1210/2160 train_time:53668ms step_avg:44.35ms
step:1211/2160 train_time:53729ms step_avg:44.37ms
step:1212/2160 train_time:53787ms step_avg:44.38ms
step:1213/2160 train_time:53848ms step_avg:44.39ms
step:1214/2160 train_time:53907ms step_avg:44.40ms
step:1215/2160 train_time:53968ms step_avg:44.42ms
step:1216/2160 train_time:54027ms step_avg:44.43ms
step:1217/2160 train_time:54088ms step_avg:44.44ms
step:1218/2160 train_time:54147ms step_avg:44.46ms
step:1219/2160 train_time:54207ms step_avg:44.47ms
step:1220/2160 train_time:54266ms step_avg:44.48ms
step:1221/2160 train_time:54327ms step_avg:44.49ms
step:1222/2160 train_time:54386ms step_avg:44.51ms
step:1223/2160 train_time:54446ms step_avg:44.52ms
step:1224/2160 train_time:54505ms step_avg:44.53ms
step:1225/2160 train_time:54566ms step_avg:44.54ms
step:1226/2160 train_time:54625ms step_avg:44.56ms
step:1227/2160 train_time:54685ms step_avg:44.57ms
step:1228/2160 train_time:54744ms step_avg:44.58ms
step:1229/2160 train_time:54804ms step_avg:44.59ms
step:1230/2160 train_time:54863ms step_avg:44.60ms
step:1231/2160 train_time:54923ms step_avg:44.62ms
step:1232/2160 train_time:54982ms step_avg:44.63ms
step:1233/2160 train_time:55042ms step_avg:44.64ms
step:1234/2160 train_time:55101ms step_avg:44.65ms
step:1235/2160 train_time:55161ms step_avg:44.67ms
step:1236/2160 train_time:55220ms step_avg:44.68ms
step:1237/2160 train_time:55282ms step_avg:44.69ms
step:1238/2160 train_time:55342ms step_avg:44.70ms
step:1239/2160 train_time:55401ms step_avg:44.71ms
step:1240/2160 train_time:55460ms step_avg:44.73ms
step:1241/2160 train_time:55521ms step_avg:44.74ms
step:1242/2160 train_time:55580ms step_avg:44.75ms
step:1243/2160 train_time:55640ms step_avg:44.76ms
step:1244/2160 train_time:55699ms step_avg:44.77ms
step:1245/2160 train_time:55760ms step_avg:44.79ms
step:1246/2160 train_time:55819ms step_avg:44.80ms
step:1247/2160 train_time:55879ms step_avg:44.81ms
step:1248/2160 train_time:55938ms step_avg:44.82ms
step:1249/2160 train_time:55998ms step_avg:44.83ms
step:1250/2160 train_time:56057ms step_avg:44.85ms
step:1250/2160 val_loss:3.5704 train_time:56119ms step_avg:44.89ms
step:1251/2160 train_time:56141ms step_avg:44.88ms
step:1252/2160 train_time:56179ms step_avg:44.87ms
step:1253/2160 train_time:56243ms step_avg:44.89ms
step:1254/2160 train_time:56304ms step_avg:44.90ms
step:1255/2160 train_time:56365ms step_avg:44.91ms
step:1256/2160 train_time:56423ms step_avg:44.92ms
step:1257/2160 train_time:56483ms step_avg:44.93ms
step:1258/2160 train_time:56540ms step_avg:44.94ms
step:1259/2160 train_time:56601ms step_avg:44.96ms
step:1260/2160 train_time:56659ms step_avg:44.97ms
step:1261/2160 train_time:56719ms step_avg:44.98ms
step:1262/2160 train_time:56777ms step_avg:44.99ms
step:1263/2160 train_time:56836ms step_avg:45.00ms
step:1264/2160 train_time:56895ms step_avg:45.01ms
step:1265/2160 train_time:56955ms step_avg:45.02ms
step:1266/2160 train_time:57014ms step_avg:45.03ms
step:1267/2160 train_time:57075ms step_avg:45.05ms
step:1268/2160 train_time:57135ms step_avg:45.06ms
step:1269/2160 train_time:57197ms step_avg:45.07ms
step:1270/2160 train_time:57257ms step_avg:45.08ms
step:1271/2160 train_time:57318ms step_avg:45.10ms
step:1272/2160 train_time:57376ms step_avg:45.11ms
step:1273/2160 train_time:57437ms step_avg:45.12ms
step:1274/2160 train_time:57496ms step_avg:45.13ms
step:1275/2160 train_time:57556ms step_avg:45.14ms
step:1276/2160 train_time:57615ms step_avg:45.15ms
step:1277/2160 train_time:57675ms step_avg:45.16ms
step:1278/2160 train_time:57733ms step_avg:45.17ms
step:1279/2160 train_time:57793ms step_avg:45.19ms
step:1280/2160 train_time:57851ms step_avg:45.20ms
step:1281/2160 train_time:57911ms step_avg:45.21ms
step:1282/2160 train_time:57969ms step_avg:45.22ms
step:1283/2160 train_time:58030ms step_avg:45.23ms
step:1284/2160 train_time:58089ms step_avg:45.24ms
step:1285/2160 train_time:58150ms step_avg:45.25ms
step:1286/2160 train_time:58209ms step_avg:45.26ms
step:1287/2160 train_time:58271ms step_avg:45.28ms
step:1288/2160 train_time:58330ms step_avg:45.29ms
step:1289/2160 train_time:58392ms step_avg:45.30ms
step:1290/2160 train_time:58451ms step_avg:45.31ms
step:1291/2160 train_time:58512ms step_avg:45.32ms
step:1292/2160 train_time:58572ms step_avg:45.33ms
step:1293/2160 train_time:58632ms step_avg:45.35ms
step:1294/2160 train_time:58691ms step_avg:45.36ms
step:1295/2160 train_time:58751ms step_avg:45.37ms
step:1296/2160 train_time:58809ms step_avg:45.38ms
step:1297/2160 train_time:58870ms step_avg:45.39ms
step:1298/2160 train_time:58928ms step_avg:45.40ms
step:1299/2160 train_time:58988ms step_avg:45.41ms
step:1300/2160 train_time:59047ms step_avg:45.42ms
step:1301/2160 train_time:59108ms step_avg:45.43ms
step:1302/2160 train_time:59167ms step_avg:45.44ms
step:1303/2160 train_time:59229ms step_avg:45.46ms
step:1304/2160 train_time:59288ms step_avg:45.47ms
step:1305/2160 train_time:59350ms step_avg:45.48ms
step:1306/2160 train_time:59409ms step_avg:45.49ms
step:1307/2160 train_time:59471ms step_avg:45.50ms
step:1308/2160 train_time:59530ms step_avg:45.51ms
step:1309/2160 train_time:59591ms step_avg:45.52ms
step:1310/2160 train_time:59649ms step_avg:45.53ms
step:1311/2160 train_time:59710ms step_avg:45.55ms
step:1312/2160 train_time:59770ms step_avg:45.56ms
step:1313/2160 train_time:59830ms step_avg:45.57ms
step:1314/2160 train_time:59888ms step_avg:45.58ms
step:1315/2160 train_time:59948ms step_avg:45.59ms
step:1316/2160 train_time:60007ms step_avg:45.60ms
step:1317/2160 train_time:60067ms step_avg:45.61ms
step:1318/2160 train_time:60126ms step_avg:45.62ms
step:1319/2160 train_time:60187ms step_avg:45.63ms
step:1320/2160 train_time:60246ms step_avg:45.64ms
step:1321/2160 train_time:60307ms step_avg:45.65ms
step:1322/2160 train_time:60366ms step_avg:45.66ms
step:1323/2160 train_time:60427ms step_avg:45.67ms
step:1324/2160 train_time:60486ms step_avg:45.68ms
step:1325/2160 train_time:60547ms step_avg:45.70ms
step:1326/2160 train_time:60606ms step_avg:45.71ms
step:1327/2160 train_time:60667ms step_avg:45.72ms
step:1328/2160 train_time:60726ms step_avg:45.73ms
step:1329/2160 train_time:60786ms step_avg:45.74ms
step:1330/2160 train_time:60845ms step_avg:45.75ms
step:1331/2160 train_time:60905ms step_avg:45.76ms
step:1332/2160 train_time:60964ms step_avg:45.77ms
step:1333/2160 train_time:61025ms step_avg:45.78ms
step:1334/2160 train_time:61083ms step_avg:45.79ms
step:1335/2160 train_time:61144ms step_avg:45.80ms
step:1336/2160 train_time:61203ms step_avg:45.81ms
step:1337/2160 train_time:61264ms step_avg:45.82ms
step:1338/2160 train_time:61323ms step_avg:45.83ms
step:1339/2160 train_time:61384ms step_avg:45.84ms
step:1340/2160 train_time:61442ms step_avg:45.85ms
step:1341/2160 train_time:61504ms step_avg:45.86ms
step:1342/2160 train_time:61563ms step_avg:45.87ms
step:1343/2160 train_time:61624ms step_avg:45.89ms
step:1344/2160 train_time:61683ms step_avg:45.90ms
step:1345/2160 train_time:61743ms step_avg:45.91ms
step:1346/2160 train_time:61802ms step_avg:45.92ms
step:1347/2160 train_time:61863ms step_avg:45.93ms
step:1348/2160 train_time:61921ms step_avg:45.94ms
step:1349/2160 train_time:61982ms step_avg:45.95ms
step:1350/2160 train_time:62040ms step_avg:45.96ms
step:1351/2160 train_time:62100ms step_avg:45.97ms
step:1352/2160 train_time:62158ms step_avg:45.98ms
step:1353/2160 train_time:62219ms step_avg:45.99ms
step:1354/2160 train_time:62278ms step_avg:46.00ms
step:1355/2160 train_time:62339ms step_avg:46.01ms
step:1356/2160 train_time:62398ms step_avg:46.02ms
step:1357/2160 train_time:62459ms step_avg:46.03ms
step:1358/2160 train_time:62517ms step_avg:46.04ms
step:1359/2160 train_time:62578ms step_avg:46.05ms
step:1360/2160 train_time:62637ms step_avg:46.06ms
step:1361/2160 train_time:62697ms step_avg:46.07ms
step:1362/2160 train_time:62756ms step_avg:46.08ms
step:1363/2160 train_time:62817ms step_avg:46.09ms
step:1364/2160 train_time:62876ms step_avg:46.10ms
step:1365/2160 train_time:62936ms step_avg:46.11ms
step:1366/2160 train_time:62995ms step_avg:46.12ms
step:1367/2160 train_time:63055ms step_avg:46.13ms
step:1368/2160 train_time:63113ms step_avg:46.14ms
step:1369/2160 train_time:63174ms step_avg:46.15ms
step:1370/2160 train_time:63233ms step_avg:46.16ms
step:1371/2160 train_time:63293ms step_avg:46.17ms
step:1372/2160 train_time:63352ms step_avg:46.18ms
step:1373/2160 train_time:63413ms step_avg:46.19ms
step:1374/2160 train_time:63472ms step_avg:46.20ms
step:1375/2160 train_time:63532ms step_avg:46.21ms
step:1376/2160 train_time:63591ms step_avg:46.21ms
step:1377/2160 train_time:63651ms step_avg:46.22ms
step:1378/2160 train_time:63710ms step_avg:46.23ms
step:1379/2160 train_time:63771ms step_avg:46.24ms
step:1380/2160 train_time:63831ms step_avg:46.25ms
step:1381/2160 train_time:63891ms step_avg:46.26ms
step:1382/2160 train_time:63950ms step_avg:46.27ms
step:1383/2160 train_time:64010ms step_avg:46.28ms
step:1384/2160 train_time:64069ms step_avg:46.29ms
step:1385/2160 train_time:64130ms step_avg:46.30ms
step:1386/2160 train_time:64188ms step_avg:46.31ms
step:1387/2160 train_time:64249ms step_avg:46.32ms
step:1388/2160 train_time:64308ms step_avg:46.33ms
step:1389/2160 train_time:64369ms step_avg:46.34ms
step:1390/2160 train_time:64428ms step_avg:46.35ms
step:1391/2160 train_time:64489ms step_avg:46.36ms
step:1392/2160 train_time:64548ms step_avg:46.37ms
step:1393/2160 train_time:64609ms step_avg:46.38ms
step:1394/2160 train_time:64668ms step_avg:46.39ms
step:1395/2160 train_time:64728ms step_avg:46.40ms
step:1396/2160 train_time:64787ms step_avg:46.41ms
step:1397/2160 train_time:64848ms step_avg:46.42ms
step:1398/2160 train_time:64906ms step_avg:46.43ms
step:1399/2160 train_time:64967ms step_avg:46.44ms
step:1400/2160 train_time:65026ms step_avg:46.45ms
step:1401/2160 train_time:65086ms step_avg:46.46ms
step:1402/2160 train_time:65145ms step_avg:46.47ms
step:1403/2160 train_time:65205ms step_avg:46.48ms
step:1404/2160 train_time:65264ms step_avg:46.48ms
step:1405/2160 train_time:65325ms step_avg:46.49ms
step:1406/2160 train_time:65385ms step_avg:46.50ms
step:1407/2160 train_time:65446ms step_avg:46.51ms
step:1408/2160 train_time:65504ms step_avg:46.52ms
step:1409/2160 train_time:65565ms step_avg:46.53ms
step:1410/2160 train_time:65624ms step_avg:46.54ms
step:1411/2160 train_time:65685ms step_avg:46.55ms
step:1412/2160 train_time:65743ms step_avg:46.56ms
step:1413/2160 train_time:65804ms step_avg:46.57ms
step:1414/2160 train_time:65863ms step_avg:46.58ms
step:1415/2160 train_time:65925ms step_avg:46.59ms
step:1416/2160 train_time:66012ms step_avg:46.62ms
step:1417/2160 train_time:66099ms step_avg:46.65ms
step:1418/2160 train_time:66185ms step_avg:46.67ms
step:1419/2160 train_time:66274ms step_avg:46.70ms
step:1420/2160 train_time:66360ms step_avg:46.73ms
step:1421/2160 train_time:66448ms step_avg:46.76ms
step:1422/2160 train_time:66534ms step_avg:46.79ms
step:1423/2160 train_time:66623ms step_avg:46.82ms
step:1424/2160 train_time:66709ms step_avg:46.85ms
step:1425/2160 train_time:66798ms step_avg:46.88ms
step:1426/2160 train_time:66885ms step_avg:46.90ms
step:1427/2160 train_time:66973ms step_avg:46.93ms
step:1428/2160 train_time:67059ms step_avg:46.96ms
step:1429/2160 train_time:67147ms step_avg:46.99ms
step:1430/2160 train_time:67233ms step_avg:47.02ms
step:1431/2160 train_time:67322ms step_avg:47.05ms
step:1432/2160 train_time:67408ms step_avg:47.07ms
step:1433/2160 train_time:67496ms step_avg:47.10ms
step:1434/2160 train_time:67582ms step_avg:47.13ms
step:1435/2160 train_time:67671ms step_avg:47.16ms
step:1436/2160 train_time:67758ms step_avg:47.19ms
step:1437/2160 train_time:67846ms step_avg:47.21ms
step:1438/2160 train_time:67933ms step_avg:47.24ms
step:1439/2160 train_time:68020ms step_avg:47.27ms
step:1440/2160 train_time:68107ms step_avg:47.30ms
step:1441/2160 train_time:68194ms step_avg:47.32ms
step:1442/2160 train_time:68281ms step_avg:47.35ms
step:1443/2160 train_time:68370ms step_avg:47.38ms
step:1444/2160 train_time:68456ms step_avg:47.41ms
step:1445/2160 train_time:68544ms step_avg:47.44ms
step:1446/2160 train_time:68630ms step_avg:47.46ms
step:1447/2160 train_time:68718ms step_avg:47.49ms
step:1448/2160 train_time:68805ms step_avg:47.52ms
step:1449/2160 train_time:68893ms step_avg:47.55ms
step:1450/2160 train_time:68980ms step_avg:47.57ms
step:1451/2160 train_time:69068ms step_avg:47.60ms
step:1452/2160 train_time:69155ms step_avg:47.63ms
step:1453/2160 train_time:69243ms step_avg:47.66ms
step:1454/2160 train_time:69330ms step_avg:47.68ms
step:1455/2160 train_time:69418ms step_avg:47.71ms
step:1456/2160 train_time:69505ms step_avg:47.74ms
step:1457/2160 train_time:69593ms step_avg:47.76ms
step:1458/2160 train_time:69680ms step_avg:47.79ms
step:1459/2160 train_time:69768ms step_avg:47.82ms
step:1460/2160 train_time:69854ms step_avg:47.85ms
step:1461/2160 train_time:69943ms step_avg:47.87ms
step:1462/2160 train_time:70029ms step_avg:47.90ms
step:1463/2160 train_time:70117ms step_avg:47.93ms
step:1464/2160 train_time:70204ms step_avg:47.95ms
step:1465/2160 train_time:70292ms step_avg:47.98ms
step:1466/2160 train_time:70379ms step_avg:48.01ms
step:1467/2160 train_time:70467ms step_avg:48.03ms
step:1468/2160 train_time:70553ms step_avg:48.06ms
step:1469/2160 train_time:70642ms step_avg:48.09ms
step:1470/2160 train_time:70728ms step_avg:48.11ms
step:1471/2160 train_time:70817ms step_avg:48.14ms
step:1472/2160 train_time:70902ms step_avg:48.17ms
step:1473/2160 train_time:70990ms step_avg:48.19ms
step:1474/2160 train_time:71077ms step_avg:48.22ms
step:1475/2160 train_time:71166ms step_avg:48.25ms
step:1476/2160 train_time:71253ms step_avg:48.27ms
step:1477/2160 train_time:71341ms step_avg:48.30ms
step:1478/2160 train_time:71428ms step_avg:48.33ms
step:1479/2160 train_time:71516ms step_avg:48.35ms
step:1480/2160 train_time:71603ms step_avg:48.38ms
step:1481/2160 train_time:71691ms step_avg:48.41ms
step:1482/2160 train_time:71777ms step_avg:48.43ms
step:1483/2160 train_time:71865ms step_avg:48.46ms
step:1484/2160 train_time:71951ms step_avg:48.48ms
step:1485/2160 train_time:72040ms step_avg:48.51ms
step:1486/2160 train_time:72127ms step_avg:48.54ms
step:1487/2160 train_time:72215ms step_avg:48.56ms
step:1488/2160 train_time:72301ms step_avg:48.59ms
step:1489/2160 train_time:72389ms step_avg:48.62ms
step:1490/2160 train_time:72476ms step_avg:48.64ms
step:1491/2160 train_time:72564ms step_avg:48.67ms
step:1492/2160 train_time:72650ms step_avg:48.69ms
step:1493/2160 train_time:72739ms step_avg:48.72ms
step:1494/2160 train_time:72824ms step_avg:48.74ms
step:1495/2160 train_time:72913ms step_avg:48.77ms
step:1496/2160 train_time:73000ms step_avg:48.80ms
step:1497/2160 train_time:73088ms step_avg:48.82ms
step:1498/2160 train_time:73174ms step_avg:48.85ms
step:1499/2160 train_time:73263ms step_avg:48.87ms
step:1500/2160 train_time:73348ms step_avg:48.90ms
step:1500/2160 val_loss:3.4699 train_time:73438ms step_avg:48.96ms
step:1501/2160 train_time:73460ms step_avg:48.94ms
step:1502/2160 train_time:73529ms step_avg:48.95ms
step:1503/2160 train_time:73626ms step_avg:48.99ms
step:1504/2160 train_time:73713ms step_avg:49.01ms
step:1505/2160 train_time:73802ms step_avg:49.04ms
step:1506/2160 train_time:73888ms step_avg:49.06ms
step:1507/2160 train_time:73975ms step_avg:49.09ms
step:1508/2160 train_time:74060ms step_avg:49.11ms
step:1509/2160 train_time:74147ms step_avg:49.14ms
step:1510/2160 train_time:74232ms step_avg:49.16ms
step:1511/2160 train_time:74319ms step_avg:49.19ms
step:1512/2160 train_time:74405ms step_avg:49.21ms
step:1513/2160 train_time:74494ms step_avg:49.24ms
step:1514/2160 train_time:74584ms step_avg:49.26ms
step:1515/2160 train_time:74675ms step_avg:49.29ms
step:1516/2160 train_time:74762ms step_avg:49.32ms
step:1517/2160 train_time:74850ms step_avg:49.34ms
step:1518/2160 train_time:74936ms step_avg:49.37ms
step:1519/2160 train_time:75023ms step_avg:49.39ms
step:1520/2160 train_time:75109ms step_avg:49.41ms
step:1521/2160 train_time:75196ms step_avg:49.44ms
step:1522/2160 train_time:75281ms step_avg:49.46ms
step:1523/2160 train_time:75370ms step_avg:49.49ms
step:1524/2160 train_time:75456ms step_avg:49.51ms
step:1525/2160 train_time:75546ms step_avg:49.54ms
step:1526/2160 train_time:75634ms step_avg:49.56ms
step:1527/2160 train_time:75724ms step_avg:49.59ms
step:1528/2160 train_time:75810ms step_avg:49.61ms
step:1529/2160 train_time:75899ms step_avg:49.64ms
step:1530/2160 train_time:75984ms step_avg:49.66ms
step:1531/2160 train_time:76072ms step_avg:49.69ms
step:1532/2160 train_time:76158ms step_avg:49.71ms
step:1533/2160 train_time:76245ms step_avg:49.74ms
step:1534/2160 train_time:76331ms step_avg:49.76ms
step:1535/2160 train_time:76418ms step_avg:49.78ms
step:1536/2160 train_time:76505ms step_avg:49.81ms
step:1537/2160 train_time:76595ms step_avg:49.83ms
step:1538/2160 train_time:76683ms step_avg:49.86ms
step:1539/2160 train_time:76772ms step_avg:49.88ms
step:1540/2160 train_time:76858ms step_avg:49.91ms
step:1541/2160 train_time:76946ms step_avg:49.93ms
step:1542/2160 train_time:77033ms step_avg:49.96ms
step:1543/2160 train_time:77120ms step_avg:49.98ms
step:1544/2160 train_time:77206ms step_avg:50.00ms
step:1545/2160 train_time:77294ms step_avg:50.03ms
step:1546/2160 train_time:77379ms step_avg:50.05ms
step:1547/2160 train_time:77469ms step_avg:50.08ms
step:1548/2160 train_time:77556ms step_avg:50.10ms
step:1549/2160 train_time:77645ms step_avg:50.13ms
step:1550/2160 train_time:77733ms step_avg:50.15ms
step:1551/2160 train_time:77822ms step_avg:50.18ms
step:1552/2160 train_time:77908ms step_avg:50.20ms
step:1553/2160 train_time:77997ms step_avg:50.22ms
step:1554/2160 train_time:78082ms step_avg:50.25ms
step:1555/2160 train_time:78170ms step_avg:50.27ms
step:1556/2160 train_time:78256ms step_avg:50.29ms
step:1557/2160 train_time:78344ms step_avg:50.32ms
step:1558/2160 train_time:78430ms step_avg:50.34ms
step:1559/2160 train_time:78519ms step_avg:50.36ms
step:1560/2160 train_time:78606ms step_avg:50.39ms
step:1561/2160 train_time:78695ms step_avg:50.41ms
step:1562/2160 train_time:78782ms step_avg:50.44ms
step:1563/2160 train_time:78870ms step_avg:50.46ms
step:1564/2160 train_time:78955ms step_avg:50.48ms
step:1565/2160 train_time:79044ms step_avg:50.51ms
step:1566/2160 train_time:79129ms step_avg:50.53ms
step:1567/2160 train_time:79217ms step_avg:50.55ms
step:1568/2160 train_time:79303ms step_avg:50.58ms
step:1569/2160 train_time:79391ms step_avg:50.60ms
step:1570/2160 train_time:79477ms step_avg:50.62ms
step:1571/2160 train_time:79566ms step_avg:50.65ms
step:1572/2160 train_time:79653ms step_avg:50.67ms
step:1573/2160 train_time:79742ms step_avg:50.69ms
step:1574/2160 train_time:79829ms step_avg:50.72ms
step:1575/2160 train_time:79917ms step_avg:50.74ms
step:1576/2160 train_time:80003ms step_avg:50.76ms
step:1577/2160 train_time:80091ms step_avg:50.79ms
step:1578/2160 train_time:80177ms step_avg:50.81ms
step:1579/2160 train_time:80264ms step_avg:50.83ms
step:1580/2160 train_time:80351ms step_avg:50.85ms
step:1581/2160 train_time:80439ms step_avg:50.88ms
step:1582/2160 train_time:80526ms step_avg:50.90ms
step:1583/2160 train_time:80614ms step_avg:50.92ms
step:1584/2160 train_time:80701ms step_avg:50.95ms
step:1585/2160 train_time:80790ms step_avg:50.97ms
step:1586/2160 train_time:80876ms step_avg:50.99ms
step:1587/2160 train_time:80965ms step_avg:51.02ms
step:1588/2160 train_time:81051ms step_avg:51.04ms
step:1589/2160 train_time:81139ms step_avg:51.06ms
step:1590/2160 train_time:81225ms step_avg:51.08ms
step:1591/2160 train_time:81314ms step_avg:51.11ms
step:1592/2160 train_time:81400ms step_avg:51.13ms
step:1593/2160 train_time:81488ms step_avg:51.15ms
step:1594/2160 train_time:81575ms step_avg:51.18ms
step:1595/2160 train_time:81664ms step_avg:51.20ms
step:1596/2160 train_time:81749ms step_avg:51.22ms
step:1597/2160 train_time:81838ms step_avg:51.24ms
step:1598/2160 train_time:81925ms step_avg:51.27ms
step:1599/2160 train_time:82014ms step_avg:51.29ms
step:1600/2160 train_time:82100ms step_avg:51.31ms
step:1601/2160 train_time:82188ms step_avg:51.34ms
step:1602/2160 train_time:82275ms step_avg:51.36ms
step:1603/2160 train_time:82363ms step_avg:51.38ms
step:1604/2160 train_time:82450ms step_avg:51.40ms
step:1605/2160 train_time:82538ms step_avg:51.43ms
step:1606/2160 train_time:82625ms step_avg:51.45ms
step:1607/2160 train_time:82714ms step_avg:51.47ms
step:1608/2160 train_time:82800ms step_avg:51.49ms
step:1609/2160 train_time:82888ms step_avg:51.52ms
step:1610/2160 train_time:82974ms step_avg:51.54ms
step:1611/2160 train_time:83062ms step_avg:51.56ms
step:1612/2160 train_time:83149ms step_avg:51.58ms
step:1613/2160 train_time:83238ms step_avg:51.60ms
step:1614/2160 train_time:83324ms step_avg:51.63ms
step:1615/2160 train_time:83412ms step_avg:51.65ms
step:1616/2160 train_time:83498ms step_avg:51.67ms
step:1617/2160 train_time:83586ms step_avg:51.69ms
step:1618/2160 train_time:83672ms step_avg:51.71ms
step:1619/2160 train_time:83760ms step_avg:51.74ms
step:1620/2160 train_time:83847ms step_avg:51.76ms
step:1621/2160 train_time:83936ms step_avg:51.78ms
step:1622/2160 train_time:84022ms step_avg:51.80ms
step:1623/2160 train_time:84110ms step_avg:51.82ms
step:1624/2160 train_time:84196ms step_avg:51.84ms
step:1625/2160 train_time:84285ms step_avg:51.87ms
step:1626/2160 train_time:84371ms step_avg:51.89ms
step:1627/2160 train_time:84459ms step_avg:51.91ms
step:1628/2160 train_time:84545ms step_avg:51.93ms
step:1629/2160 train_time:84635ms step_avg:51.96ms
step:1630/2160 train_time:84721ms step_avg:51.98ms
step:1631/2160 train_time:84809ms step_avg:52.00ms
step:1632/2160 train_time:84895ms step_avg:52.02ms
step:1633/2160 train_time:84983ms step_avg:52.04ms
step:1634/2160 train_time:85070ms step_avg:52.06ms
step:1635/2160 train_time:85158ms step_avg:52.08ms
step:1636/2160 train_time:85244ms step_avg:52.11ms
step:1637/2160 train_time:85332ms step_avg:52.13ms
step:1638/2160 train_time:85418ms step_avg:52.15ms
step:1639/2160 train_time:85507ms step_avg:52.17ms
step:1640/2160 train_time:85594ms step_avg:52.19ms
step:1641/2160 train_time:85683ms step_avg:52.21ms
step:1642/2160 train_time:85770ms step_avg:52.23ms
step:1643/2160 train_time:85858ms step_avg:52.26ms
step:1644/2160 train_time:85945ms step_avg:52.28ms
step:1645/2160 train_time:86034ms step_avg:52.30ms
step:1646/2160 train_time:86120ms step_avg:52.32ms
step:1647/2160 train_time:86209ms step_avg:52.34ms
step:1648/2160 train_time:86295ms step_avg:52.36ms
step:1649/2160 train_time:86383ms step_avg:52.39ms
step:1650/2160 train_time:86470ms step_avg:52.41ms
step:1651/2160 train_time:86558ms step_avg:52.43ms
step:1652/2160 train_time:86644ms step_avg:52.45ms
step:1653/2160 train_time:86732ms step_avg:52.47ms
step:1654/2160 train_time:86819ms step_avg:52.49ms
step:1655/2160 train_time:86907ms step_avg:52.51ms
step:1656/2160 train_time:86994ms step_avg:52.53ms
step:1657/2160 train_time:87083ms step_avg:52.55ms
step:1658/2160 train_time:87169ms step_avg:52.57ms
step:1659/2160 train_time:87257ms step_avg:52.60ms
step:1660/2160 train_time:87343ms step_avg:52.62ms
step:1661/2160 train_time:87431ms step_avg:52.64ms
step:1662/2160 train_time:87516ms step_avg:52.66ms
step:1663/2160 train_time:87605ms step_avg:52.68ms
step:1664/2160 train_time:87691ms step_avg:52.70ms
step:1665/2160 train_time:87779ms step_avg:52.72ms
step:1666/2160 train_time:87866ms step_avg:52.74ms
step:1667/2160 train_time:87954ms step_avg:52.76ms
step:1668/2160 train_time:88041ms step_avg:52.78ms
step:1669/2160 train_time:88129ms step_avg:52.80ms
step:1670/2160 train_time:88216ms step_avg:52.82ms
step:1671/2160 train_time:88304ms step_avg:52.85ms
step:1672/2160 train_time:88391ms step_avg:52.87ms
step:1673/2160 train_time:88480ms step_avg:52.89ms
step:1674/2160 train_time:88566ms step_avg:52.91ms
step:1675/2160 train_time:88654ms step_avg:52.93ms
step:1676/2160 train_time:88741ms step_avg:52.95ms
step:1677/2160 train_time:88830ms step_avg:52.97ms
step:1678/2160 train_time:88917ms step_avg:52.99ms
step:1679/2160 train_time:89005ms step_avg:53.01ms
step:1680/2160 train_time:89092ms step_avg:53.03ms
step:1681/2160 train_time:89181ms step_avg:53.05ms
step:1682/2160 train_time:89267ms step_avg:53.07ms
step:1683/2160 train_time:89354ms step_avg:53.09ms
step:1684/2160 train_time:89441ms step_avg:53.11ms
step:1685/2160 train_time:89528ms step_avg:53.13ms
step:1686/2160 train_time:89614ms step_avg:53.15ms
step:1687/2160 train_time:89703ms step_avg:53.17ms
step:1688/2160 train_time:89789ms step_avg:53.19ms
step:1689/2160 train_time:89878ms step_avg:53.21ms
step:1690/2160 train_time:89965ms step_avg:53.23ms
step:1691/2160 train_time:90052ms step_avg:53.25ms
step:1692/2160 train_time:90139ms step_avg:53.27ms
step:1693/2160 train_time:90227ms step_avg:53.29ms
step:1694/2160 train_time:90313ms step_avg:53.31ms
step:1695/2160 train_time:90401ms step_avg:53.33ms
step:1696/2160 train_time:90488ms step_avg:53.35ms
step:1697/2160 train_time:90576ms step_avg:53.37ms
step:1698/2160 train_time:90663ms step_avg:53.39ms
step:1699/2160 train_time:90751ms step_avg:53.41ms
step:1700/2160 train_time:90838ms step_avg:53.43ms
step:1701/2160 train_time:90926ms step_avg:53.45ms
step:1702/2160 train_time:91013ms step_avg:53.47ms
step:1703/2160 train_time:91101ms step_avg:53.49ms
step:1704/2160 train_time:91188ms step_avg:53.51ms
step:1705/2160 train_time:91275ms step_avg:53.53ms
step:1706/2160 train_time:91362ms step_avg:53.55ms
step:1707/2160 train_time:91450ms step_avg:53.57ms
step:1708/2160 train_time:91537ms step_avg:53.59ms
step:1709/2160 train_time:91625ms step_avg:53.61ms
step:1710/2160 train_time:91711ms step_avg:53.63ms
step:1711/2160 train_time:91799ms step_avg:53.65ms
step:1712/2160 train_time:91885ms step_avg:53.67ms
step:1713/2160 train_time:91974ms step_avg:53.69ms
step:1714/2160 train_time:92060ms step_avg:53.71ms
step:1715/2160 train_time:92149ms step_avg:53.73ms
step:1716/2160 train_time:92236ms step_avg:53.75ms
step:1717/2160 train_time:92324ms step_avg:53.77ms
step:1718/2160 train_time:92410ms step_avg:53.79ms
step:1719/2160 train_time:92499ms step_avg:53.81ms
step:1720/2160 train_time:92585ms step_avg:53.83ms
step:1721/2160 train_time:92672ms step_avg:53.85ms
step:1722/2160 train_time:92759ms step_avg:53.87ms
step:1723/2160 train_time:92847ms step_avg:53.89ms
step:1724/2160 train_time:92934ms step_avg:53.91ms
step:1725/2160 train_time:93022ms step_avg:53.93ms
step:1726/2160 train_time:93108ms step_avg:53.94ms
step:1727/2160 train_time:93197ms step_avg:53.96ms
step:1728/2160 train_time:93283ms step_avg:53.98ms
step:1729/2160 train_time:93371ms step_avg:54.00ms
step:1730/2160 train_time:93457ms step_avg:54.02ms
step:1731/2160 train_time:93545ms step_avg:54.04ms
step:1732/2160 train_time:93631ms step_avg:54.06ms
step:1733/2160 train_time:93720ms step_avg:54.08ms
step:1734/2160 train_time:93807ms step_avg:54.10ms
step:1735/2160 train_time:93895ms step_avg:54.12ms
step:1736/2160 train_time:93981ms step_avg:54.14ms
step:1737/2160 train_time:94071ms step_avg:54.16ms
step:1738/2160 train_time:94157ms step_avg:54.18ms
step:1739/2160 train_time:94245ms step_avg:54.19ms
step:1740/2160 train_time:94331ms step_avg:54.21ms
step:1741/2160 train_time:94420ms step_avg:54.23ms
step:1742/2160 train_time:94505ms step_avg:54.25ms
step:1743/2160 train_time:94593ms step_avg:54.27ms
step:1744/2160 train_time:94680ms step_avg:54.29ms
step:1745/2160 train_time:94768ms step_avg:54.31ms
step:1746/2160 train_time:94854ms step_avg:54.33ms
step:1747/2160 train_time:94943ms step_avg:54.35ms
step:1748/2160 train_time:95029ms step_avg:54.36ms
step:1749/2160 train_time:95117ms step_avg:54.38ms
step:1750/2160 train_time:95204ms step_avg:54.40ms
step:1750/2160 val_loss:3.3815 train_time:95292ms step_avg:54.45ms
step:1751/2160 train_time:95314ms step_avg:54.43ms
step:1752/2160 train_time:95382ms step_avg:54.44ms
step:1753/2160 train_time:95472ms step_avg:54.46ms
step:1754/2160 train_time:95558ms step_avg:54.48ms
step:1755/2160 train_time:95647ms step_avg:54.50ms
step:1756/2160 train_time:95732ms step_avg:54.52ms
step:1757/2160 train_time:95820ms step_avg:54.54ms
step:1758/2160 train_time:95906ms step_avg:54.55ms
step:1759/2160 train_time:95993ms step_avg:54.57ms
step:1760/2160 train_time:96079ms step_avg:54.59ms
step:1761/2160 train_time:96167ms step_avg:54.61ms
step:1762/2160 train_time:96253ms step_avg:54.63ms
step:1763/2160 train_time:96342ms step_avg:54.65ms
step:1764/2160 train_time:96430ms step_avg:54.67ms
step:1765/2160 train_time:96520ms step_avg:54.69ms
step:1766/2160 train_time:96607ms step_avg:54.70ms
step:1767/2160 train_time:96695ms step_avg:54.72ms
step:1768/2160 train_time:96781ms step_avg:54.74ms
step:1769/2160 train_time:96868ms step_avg:54.76ms
step:1770/2160 train_time:96954ms step_avg:54.78ms
step:1771/2160 train_time:97042ms step_avg:54.80ms
step:1772/2160 train_time:97128ms step_avg:54.81ms
step:1773/2160 train_time:97216ms step_avg:54.83ms
step:1774/2160 train_time:97304ms step_avg:54.85ms
step:1775/2160 train_time:97393ms step_avg:54.87ms
step:1776/2160 train_time:97480ms step_avg:54.89ms
step:1777/2160 train_time:97568ms step_avg:54.91ms
step:1778/2160 train_time:97654ms step_avg:54.92ms
step:1779/2160 train_time:97743ms step_avg:54.94ms
step:1780/2160 train_time:97828ms step_avg:54.96ms
step:1781/2160 train_time:97917ms step_avg:54.98ms
step:1782/2160 train_time:98003ms step_avg:55.00ms
step:1783/2160 train_time:98091ms step_avg:55.01ms
step:1784/2160 train_time:98177ms step_avg:55.03ms
step:1785/2160 train_time:98266ms step_avg:55.05ms
step:1786/2160 train_time:98352ms step_avg:55.07ms
step:1787/2160 train_time:98440ms step_avg:55.09ms
step:1788/2160 train_time:98527ms step_avg:55.10ms
step:1789/2160 train_time:98616ms step_avg:55.12ms
step:1790/2160 train_time:98703ms step_avg:55.14ms
step:1791/2160 train_time:98791ms step_avg:55.16ms
step:1792/2160 train_time:98876ms step_avg:55.18ms
step:1793/2160 train_time:98964ms step_avg:55.19ms
step:1794/2160 train_time:99051ms step_avg:55.21ms
step:1795/2160 train_time:99139ms step_avg:55.23ms
step:1796/2160 train_time:99225ms step_avg:55.25ms
step:1797/2160 train_time:99313ms step_avg:55.27ms
step:1798/2160 train_time:99400ms step_avg:55.28ms
step:1799/2160 train_time:99491ms step_avg:55.30ms
step:1800/2160 train_time:99576ms step_avg:55.32ms
step:1801/2160 train_time:99664ms step_avg:55.34ms
step:1802/2160 train_time:99750ms step_avg:55.36ms
step:1803/2160 train_time:99837ms step_avg:55.37ms
step:1804/2160 train_time:99925ms step_avg:55.39ms
step:1805/2160 train_time:100012ms step_avg:55.41ms
step:1806/2160 train_time:100098ms step_avg:55.43ms
step:1807/2160 train_time:100187ms step_avg:55.44ms
step:1808/2160 train_time:100273ms step_avg:55.46ms
step:1809/2160 train_time:100361ms step_avg:55.48ms
step:1810/2160 train_time:100448ms step_avg:55.50ms
step:1811/2160 train_time:100536ms step_avg:55.51ms
step:1812/2160 train_time:100622ms step_avg:55.53ms
step:1813/2160 train_time:100711ms step_avg:55.55ms
step:1814/2160 train_time:100797ms step_avg:55.57ms
step:1815/2160 train_time:100886ms step_avg:55.58ms
step:1816/2160 train_time:100972ms step_avg:55.60ms
step:1817/2160 train_time:101060ms step_avg:55.62ms
step:1818/2160 train_time:101147ms step_avg:55.64ms
step:1819/2160 train_time:101235ms step_avg:55.65ms
step:1820/2160 train_time:101321ms step_avg:55.67ms
step:1821/2160 train_time:101410ms step_avg:55.69ms
step:1822/2160 train_time:101496ms step_avg:55.71ms
step:1823/2160 train_time:101584ms step_avg:55.72ms
step:1824/2160 train_time:101671ms step_avg:55.74ms
step:1825/2160 train_time:101759ms step_avg:55.76ms
step:1826/2160 train_time:101845ms step_avg:55.77ms
step:1827/2160 train_time:101933ms step_avg:55.79ms
step:1828/2160 train_time:102019ms step_avg:55.81ms
step:1829/2160 train_time:102106ms step_avg:55.83ms
step:1830/2160 train_time:102192ms step_avg:55.84ms
step:1831/2160 train_time:102281ms step_avg:55.86ms
step:1832/2160 train_time:102368ms step_avg:55.88ms
step:1833/2160 train_time:102456ms step_avg:55.90ms
step:1834/2160 train_time:102543ms step_avg:55.91ms
step:1835/2160 train_time:102632ms step_avg:55.93ms
step:1836/2160 train_time:102718ms step_avg:55.95ms
step:1837/2160 train_time:102806ms step_avg:55.96ms
step:1838/2160 train_time:102893ms step_avg:55.98ms
step:1839/2160 train_time:102981ms step_avg:56.00ms
step:1840/2160 train_time:103066ms step_avg:56.01ms
step:1841/2160 train_time:103154ms step_avg:56.03ms
step:1842/2160 train_time:103241ms step_avg:56.05ms
step:1843/2160 train_time:103329ms step_avg:56.07ms
step:1844/2160 train_time:103416ms step_avg:56.08ms
step:1845/2160 train_time:103504ms step_avg:56.10ms
step:1846/2160 train_time:103592ms step_avg:56.12ms
step:1847/2160 train_time:103681ms step_avg:56.13ms
step:1848/2160 train_time:103767ms step_avg:56.15ms
step:1849/2160 train_time:103856ms step_avg:56.17ms
step:1850/2160 train_time:103942ms step_avg:56.18ms
step:1851/2160 train_time:104031ms step_avg:56.20ms
step:1852/2160 train_time:104116ms step_avg:56.22ms
step:1853/2160 train_time:104205ms step_avg:56.24ms
step:1854/2160 train_time:104291ms step_avg:56.25ms
step:1855/2160 train_time:104379ms step_avg:56.27ms
step:1856/2160 train_time:104466ms step_avg:56.29ms
step:1857/2160 train_time:104555ms step_avg:56.30ms
step:1858/2160 train_time:104642ms step_avg:56.32ms
step:1859/2160 train_time:104731ms step_avg:56.34ms
step:1860/2160 train_time:104817ms step_avg:56.35ms
step:1861/2160 train_time:104905ms step_avg:56.37ms
step:1862/2160 train_time:104991ms step_avg:56.39ms
step:1863/2160 train_time:105080ms step_avg:56.40ms
step:1864/2160 train_time:105165ms step_avg:56.42ms
step:1865/2160 train_time:105254ms step_avg:56.44ms
step:1866/2160 train_time:105340ms step_avg:56.45ms
step:1867/2160 train_time:105429ms step_avg:56.47ms
step:1868/2160 train_time:105515ms step_avg:56.49ms
step:1869/2160 train_time:105604ms step_avg:56.50ms
step:1870/2160 train_time:105691ms step_avg:56.52ms
step:1871/2160 train_time:105779ms step_avg:56.54ms
step:1872/2160 train_time:105865ms step_avg:56.55ms
step:1873/2160 train_time:105954ms step_avg:56.57ms
step:1874/2160 train_time:106039ms step_avg:56.58ms
step:1875/2160 train_time:106128ms step_avg:56.60ms
step:1876/2160 train_time:106214ms step_avg:56.62ms
step:1877/2160 train_time:106302ms step_avg:56.63ms
step:1878/2160 train_time:106389ms step_avg:56.65ms
step:1879/2160 train_time:106477ms step_avg:56.67ms
step:1880/2160 train_time:106564ms step_avg:56.68ms
step:1881/2160 train_time:106652ms step_avg:56.70ms
step:1882/2160 train_time:106739ms step_avg:56.72ms
step:1883/2160 train_time:106828ms step_avg:56.73ms
step:1884/2160 train_time:106914ms step_avg:56.75ms
step:1885/2160 train_time:107003ms step_avg:56.77ms
step:1886/2160 train_time:107090ms step_avg:56.78ms
step:1887/2160 train_time:107177ms step_avg:56.80ms
step:1888/2160 train_time:107263ms step_avg:56.81ms
step:1889/2160 train_time:107352ms step_avg:56.83ms
step:1890/2160 train_time:107438ms step_avg:56.85ms
step:1891/2160 train_time:107527ms step_avg:56.86ms
step:1892/2160 train_time:107614ms step_avg:56.88ms
step:1893/2160 train_time:107701ms step_avg:56.89ms
step:1894/2160 train_time:107787ms step_avg:56.91ms
step:1895/2160 train_time:107875ms step_avg:56.93ms
step:1896/2160 train_time:107962ms step_avg:56.94ms
step:1897/2160 train_time:108051ms step_avg:56.96ms
step:1898/2160 train_time:108137ms step_avg:56.97ms
step:1899/2160 train_time:108226ms step_avg:56.99ms
step:1900/2160 train_time:108312ms step_avg:57.01ms
step:1901/2160 train_time:108401ms step_avg:57.02ms
step:1902/2160 train_time:108487ms step_avg:57.04ms
step:1903/2160 train_time:108575ms step_avg:57.05ms
step:1904/2160 train_time:108662ms step_avg:57.07ms
step:1905/2160 train_time:108750ms step_avg:57.09ms
step:1906/2160 train_time:108837ms step_avg:57.10ms
step:1907/2160 train_time:108927ms step_avg:57.12ms
step:1908/2160 train_time:109013ms step_avg:57.13ms
step:1909/2160 train_time:109101ms step_avg:57.15ms
step:1910/2160 train_time:109188ms step_avg:57.17ms
step:1911/2160 train_time:109276ms step_avg:57.18ms
step:1912/2160 train_time:109362ms step_avg:57.20ms
step:1913/2160 train_time:109450ms step_avg:57.21ms
step:1914/2160 train_time:109537ms step_avg:57.23ms
step:1915/2160 train_time:109625ms step_avg:57.25ms
step:1916/2160 train_time:109712ms step_avg:57.26ms
step:1917/2160 train_time:109800ms step_avg:57.28ms
step:1918/2160 train_time:109887ms step_avg:57.29ms
step:1919/2160 train_time:109975ms step_avg:57.31ms
step:1920/2160 train_time:110061ms step_avg:57.32ms
step:1921/2160 train_time:110150ms step_avg:57.34ms
step:1922/2160 train_time:110235ms step_avg:57.35ms
step:1923/2160 train_time:110324ms step_avg:57.37ms
step:1924/2160 train_time:110410ms step_avg:57.39ms
step:1925/2160 train_time:110499ms step_avg:57.40ms
step:1926/2160 train_time:110586ms step_avg:57.42ms
step:1927/2160 train_time:110674ms step_avg:57.43ms
step:1928/2160 train_time:110760ms step_avg:57.45ms
step:1929/2160 train_time:110849ms step_avg:57.46ms
step:1930/2160 train_time:110935ms step_avg:57.48ms
step:1931/2160 train_time:111024ms step_avg:57.50ms
step:1932/2160 train_time:111111ms step_avg:57.51ms
step:1933/2160 train_time:111198ms step_avg:57.53ms
step:1934/2160 train_time:111285ms step_avg:57.54ms
step:1935/2160 train_time:111373ms step_avg:57.56ms
step:1936/2160 train_time:111460ms step_avg:57.57ms
step:1937/2160 train_time:111548ms step_avg:57.59ms
step:1938/2160 train_time:111635ms step_avg:57.60ms
step:1939/2160 train_time:111724ms step_avg:57.62ms
step:1940/2160 train_time:111810ms step_avg:57.63ms
step:1941/2160 train_time:111898ms step_avg:57.65ms
step:1942/2160 train_time:111984ms step_avg:57.66ms
step:1943/2160 train_time:112072ms step_avg:57.68ms
step:1944/2160 train_time:112159ms step_avg:57.69ms
step:1945/2160 train_time:112247ms step_avg:57.71ms
step:1946/2160 train_time:112333ms step_avg:57.73ms
step:1947/2160 train_time:112422ms step_avg:57.74ms
step:1948/2160 train_time:112509ms step_avg:57.76ms
step:1949/2160 train_time:112597ms step_avg:57.77ms
step:1950/2160 train_time:112684ms step_avg:57.79ms
step:1951/2160 train_time:112772ms step_avg:57.80ms
step:1952/2160 train_time:112859ms step_avg:57.82ms
step:1953/2160 train_time:112947ms step_avg:57.83ms
step:1954/2160 train_time:113034ms step_avg:57.85ms
step:1955/2160 train_time:113122ms step_avg:57.86ms
step:1956/2160 train_time:113209ms step_avg:57.88ms
step:1957/2160 train_time:113297ms step_avg:57.89ms
step:1958/2160 train_time:113384ms step_avg:57.91ms
step:1959/2160 train_time:113472ms step_avg:57.92ms
step:1960/2160 train_time:113559ms step_avg:57.94ms
step:1961/2160 train_time:113647ms step_avg:57.95ms
step:1962/2160 train_time:113734ms step_avg:57.97ms
step:1963/2160 train_time:113822ms step_avg:57.98ms
step:1964/2160 train_time:113909ms step_avg:58.00ms
step:1965/2160 train_time:113996ms step_avg:58.01ms
step:1966/2160 train_time:114083ms step_avg:58.03ms
step:1967/2160 train_time:114172ms step_avg:58.04ms
step:1968/2160 train_time:114258ms step_avg:58.06ms
step:1969/2160 train_time:114347ms step_avg:58.07ms
step:1970/2160 train_time:114434ms step_avg:58.09ms
step:1971/2160 train_time:114522ms step_avg:58.10ms
step:1972/2160 train_time:114608ms step_avg:58.12ms
step:1973/2160 train_time:114697ms step_avg:58.13ms
step:1974/2160 train_time:114783ms step_avg:58.15ms
step:1975/2160 train_time:114871ms step_avg:58.16ms
step:1976/2160 train_time:114957ms step_avg:58.18ms
step:1977/2160 train_time:115045ms step_avg:58.19ms
step:1978/2160 train_time:115132ms step_avg:58.21ms
step:1979/2160 train_time:115220ms step_avg:58.22ms
step:1980/2160 train_time:115307ms step_avg:58.24ms
step:1981/2160 train_time:115395ms step_avg:58.25ms
step:1982/2160 train_time:115481ms step_avg:58.27ms
step:1983/2160 train_time:115570ms step_avg:58.28ms
step:1984/2160 train_time:115657ms step_avg:58.29ms
step:1985/2160 train_time:115745ms step_avg:58.31ms
step:1986/2160 train_time:115831ms step_avg:58.32ms
step:1987/2160 train_time:115919ms step_avg:58.34ms
step:1988/2160 train_time:116007ms step_avg:58.35ms
step:1989/2160 train_time:116095ms step_avg:58.37ms
step:1990/2160 train_time:116182ms step_avg:58.38ms
step:1991/2160 train_time:116270ms step_avg:58.40ms
step:1992/2160 train_time:116356ms step_avg:58.41ms
step:1993/2160 train_time:116446ms step_avg:58.43ms
step:1994/2160 train_time:116531ms step_avg:58.44ms
step:1995/2160 train_time:116620ms step_avg:58.46ms
step:1996/2160 train_time:116707ms step_avg:58.47ms
step:1997/2160 train_time:116795ms step_avg:58.49ms
step:1998/2160 train_time:116881ms step_avg:58.50ms
step:1999/2160 train_time:116970ms step_avg:58.51ms
step:2000/2160 train_time:117056ms step_avg:58.53ms
step:2000/2160 val_loss:3.3119 train_time:117145ms step_avg:58.57ms
step:2001/2160 train_time:117165ms step_avg:58.55ms
step:2002/2160 train_time:117234ms step_avg:58.56ms
step:2003/2160 train_time:117327ms step_avg:58.58ms
step:2004/2160 train_time:117413ms step_avg:58.59ms
step:2005/2160 train_time:117501ms step_avg:58.60ms
step:2006/2160 train_time:117586ms step_avg:58.62ms
step:2007/2160 train_time:117673ms step_avg:58.63ms
step:2008/2160 train_time:117759ms step_avg:58.64ms
step:2009/2160 train_time:117845ms step_avg:58.66ms
step:2010/2160 train_time:117931ms step_avg:58.67ms
step:2011/2160 train_time:118018ms step_avg:58.69ms
step:2012/2160 train_time:118105ms step_avg:58.70ms
step:2013/2160 train_time:118196ms step_avg:58.72ms
step:2014/2160 train_time:118284ms step_avg:58.73ms
step:2015/2160 train_time:118374ms step_avg:58.75ms
step:2016/2160 train_time:118460ms step_avg:58.76ms
step:2017/2160 train_time:118549ms step_avg:58.77ms
step:2018/2160 train_time:118634ms step_avg:58.79ms
step:2019/2160 train_time:118722ms step_avg:58.80ms
step:2020/2160 train_time:118807ms step_avg:58.82ms
step:2021/2160 train_time:118895ms step_avg:58.83ms
step:2022/2160 train_time:118981ms step_avg:58.84ms
step:2023/2160 train_time:119069ms step_avg:58.86ms
step:2024/2160 train_time:119156ms step_avg:58.87ms
step:2025/2160 train_time:119246ms step_avg:58.89ms
step:2026/2160 train_time:119334ms step_avg:58.90ms
step:2027/2160 train_time:119423ms step_avg:58.92ms
step:2028/2160 train_time:119508ms step_avg:58.93ms
step:2029/2160 train_time:119596ms step_avg:58.94ms
step:2030/2160 train_time:119682ms step_avg:58.96ms
step:2031/2160 train_time:119771ms step_avg:58.97ms
step:2032/2160 train_time:119856ms step_avg:58.98ms
step:2033/2160 train_time:119943ms step_avg:59.00ms
step:2034/2160 train_time:120029ms step_avg:59.01ms
step:2035/2160 train_time:120119ms step_avg:59.03ms
step:2036/2160 train_time:120205ms step_avg:59.04ms
step:2037/2160 train_time:120298ms step_avg:59.06ms
step:2038/2160 train_time:120382ms step_avg:59.07ms
step:2039/2160 train_time:120471ms step_avg:59.08ms
step:2040/2160 train_time:120557ms step_avg:59.10ms
step:2041/2160 train_time:120645ms step_avg:59.11ms
step:2042/2160 train_time:120731ms step_avg:59.12ms
step:2043/2160 train_time:120818ms step_avg:59.14ms
step:2044/2160 train_time:120904ms step_avg:59.15ms
step:2045/2160 train_time:120992ms step_avg:59.17ms
step:2046/2160 train_time:121079ms step_avg:59.18ms
step:2047/2160 train_time:121168ms step_avg:59.19ms
step:2048/2160 train_time:121255ms step_avg:59.21ms
step:2049/2160 train_time:121345ms step_avg:59.22ms
step:2050/2160 train_time:121431ms step_avg:59.23ms
step:2051/2160 train_time:121519ms step_avg:59.25ms
step:2052/2160 train_time:121606ms step_avg:59.26ms
step:2053/2160 train_time:121693ms step_avg:59.28ms
step:2054/2160 train_time:121779ms step_avg:59.29ms
step:2055/2160 train_time:121866ms step_avg:59.30ms
step:2056/2160 train_time:121952ms step_avg:59.32ms
step:2057/2160 train_time:122041ms step_avg:59.33ms
step:2058/2160 train_time:122127ms step_avg:59.34ms
step:2059/2160 train_time:122215ms step_avg:59.36ms
step:2060/2160 train_time:122302ms step_avg:59.37ms
step:2061/2160 train_time:122390ms step_avg:59.38ms
step:2062/2160 train_time:122477ms step_avg:59.40ms
step:2063/2160 train_time:122565ms step_avg:59.41ms
step:2064/2160 train_time:122652ms step_avg:59.42ms
step:2065/2160 train_time:122740ms step_avg:59.44ms
step:2066/2160 train_time:122826ms step_avg:59.45ms
step:2067/2160 train_time:122914ms step_avg:59.47ms
step:2068/2160 train_time:123000ms step_avg:59.48ms
step:2069/2160 train_time:123088ms step_avg:59.49ms
step:2070/2160 train_time:123176ms step_avg:59.51ms
step:2071/2160 train_time:123264ms step_avg:59.52ms
step:2072/2160 train_time:123350ms step_avg:59.53ms
step:2073/2160 train_time:123439ms step_avg:59.55ms
step:2074/2160 train_time:123526ms step_avg:59.56ms
step:2075/2160 train_time:123614ms step_avg:59.57ms
step:2076/2160 train_time:123701ms step_avg:59.59ms
step:2077/2160 train_time:123789ms step_avg:59.60ms
step:2078/2160 train_time:123875ms step_avg:59.61ms
step:2079/2160 train_time:123964ms step_avg:59.63ms
step:2080/2160 train_time:124050ms step_avg:59.64ms
step:2081/2160 train_time:124139ms step_avg:59.65ms
step:2082/2160 train_time:124225ms step_avg:59.67ms
step:2083/2160 train_time:124314ms step_avg:59.68ms
step:2084/2160 train_time:124400ms step_avg:59.69ms
step:2085/2160 train_time:124488ms step_avg:59.71ms
step:2086/2160 train_time:124574ms step_avg:59.72ms
step:2087/2160 train_time:124663ms step_avg:59.73ms
step:2088/2160 train_time:124750ms step_avg:59.75ms
step:2089/2160 train_time:124838ms step_avg:59.76ms
step:2090/2160 train_time:124924ms step_avg:59.77ms
step:2091/2160 train_time:125014ms step_avg:59.79ms
step:2092/2160 train_time:125100ms step_avg:59.80ms
step:2093/2160 train_time:125188ms step_avg:59.81ms
step:2094/2160 train_time:125275ms step_avg:59.83ms
step:2095/2160 train_time:125364ms step_avg:59.84ms
step:2096/2160 train_time:125450ms step_avg:59.85ms
step:2097/2160 train_time:125539ms step_avg:59.87ms
step:2098/2160 train_time:125625ms step_avg:59.88ms
step:2099/2160 train_time:125713ms step_avg:59.89ms
step:2100/2160 train_time:125799ms step_avg:59.90ms
step:2101/2160 train_time:125888ms step_avg:59.92ms
step:2102/2160 train_time:125974ms step_avg:59.93ms
step:2103/2160 train_time:126062ms step_avg:59.94ms
step:2104/2160 train_time:126148ms step_avg:59.96ms
step:2105/2160 train_time:126237ms step_avg:59.97ms
step:2106/2160 train_time:126323ms step_avg:59.98ms
step:2107/2160 train_time:126412ms step_avg:60.00ms
step:2108/2160 train_time:126499ms step_avg:60.01ms
step:2109/2160 train_time:126587ms step_avg:60.02ms
step:2110/2160 train_time:126673ms step_avg:60.03ms
step:2111/2160 train_time:126761ms step_avg:60.05ms
step:2112/2160 train_time:126848ms step_avg:60.06ms
step:2113/2160 train_time:126937ms step_avg:60.07ms
step:2114/2160 train_time:127023ms step_avg:60.09ms
step:2115/2160 train_time:127111ms step_avg:60.10ms
step:2116/2160 train_time:127199ms step_avg:60.11ms
step:2117/2160 train_time:127287ms step_avg:60.13ms
step:2118/2160 train_time:127373ms step_avg:60.14ms
step:2119/2160 train_time:127462ms step_avg:60.15ms
step:2120/2160 train_time:127548ms step_avg:60.16ms
step:2121/2160 train_time:127637ms step_avg:60.18ms
step:2122/2160 train_time:127724ms step_avg:60.19ms
step:2123/2160 train_time:127813ms step_avg:60.20ms
step:2124/2160 train_time:127899ms step_avg:60.22ms
step:2125/2160 train_time:127987ms step_avg:60.23ms
step:2126/2160 train_time:128074ms step_avg:60.24ms
step:2127/2160 train_time:128163ms step_avg:60.26ms
step:2128/2160 train_time:128250ms step_avg:60.27ms
step:2129/2160 train_time:128339ms step_avg:60.28ms
step:2130/2160 train_time:128425ms step_avg:60.29ms
step:2131/2160 train_time:128513ms step_avg:60.31ms
step:2132/2160 train_time:128601ms step_avg:60.32ms
step:2133/2160 train_time:128689ms step_avg:60.33ms
step:2134/2160 train_time:128776ms step_avg:60.34ms
step:2135/2160 train_time:128865ms step_avg:60.36ms
step:2136/2160 train_time:128951ms step_avg:60.37ms
step:2137/2160 train_time:129039ms step_avg:60.38ms
step:2138/2160 train_time:129126ms step_avg:60.40ms
step:2139/2160 train_time:129214ms step_avg:60.41ms
step:2140/2160 train_time:129301ms step_avg:60.42ms
step:2141/2160 train_time:129390ms step_avg:60.43ms
step:2142/2160 train_time:129477ms step_avg:60.45ms
step:2143/2160 train_time:129565ms step_avg:60.46ms
step:2144/2160 train_time:129652ms step_avg:60.47ms
step:2145/2160 train_time:129741ms step_avg:60.49ms
step:2146/2160 train_time:129828ms step_avg:60.50ms
step:2147/2160 train_time:129916ms step_avg:60.51ms
step:2148/2160 train_time:130004ms step_avg:60.52ms
step:2149/2160 train_time:130092ms step_avg:60.54ms
step:2150/2160 train_time:130179ms step_avg:60.55ms
step:2151/2160 train_time:130268ms step_avg:60.56ms
step:2152/2160 train_time:130354ms step_avg:60.57ms
step:2153/2160 train_time:130443ms step_avg:60.59ms
step:2154/2160 train_time:130530ms step_avg:60.60ms
step:2155/2160 train_time:130618ms step_avg:60.61ms
step:2156/2160 train_time:130705ms step_avg:60.62ms
step:2157/2160 train_time:130794ms step_avg:60.64ms
step:2158/2160 train_time:130880ms step_avg:60.65ms
step:2159/2160 train_time:130969ms step_avg:60.66ms
step:2160/2160 train_time:131056ms step_avg:60.67ms
step:2160/2160 val_loss:3.2797 train_time:131145ms step_avg:60.72ms
peak memory allocated: 30078 MiB reserved: 45076 MiB

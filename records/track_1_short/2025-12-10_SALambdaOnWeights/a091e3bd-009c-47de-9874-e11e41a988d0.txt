import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 10 13:43:14 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                  Off |
| N/A   30C    P0            150W /  700W |    5858MiB /  81559MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                  Off |
| N/A   26C    P0            139W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                  Off |
| N/A   23C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                  Off |
| N/A   28C    P0            134W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                  Off |
| N/A   28C    P0            135W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                  Off |
| N/A   25C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                  Off |
| N/A   28C    P0            140W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                  Off |
| N/A   24C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     65898      C   /usr/bin/python3                             1510MiB |
|    0   N/A  N/A     65899      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     65900      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     65901      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     65902      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     65903      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     65904      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     65905      C   /usr/bin/python3                              614MiB |
|    1   N/A  N/A     65899      C   /usr/bin/python3                             1510MiB |
|    2   N/A  N/A     65900      C   /usr/bin/python3                             1510MiB |
|    3   N/A  N/A     65901      C   /usr/bin/python3                             1510MiB |
|    4   N/A  N/A     65902      C   /usr/bin/python3                             1510MiB |
|    5   N/A  N/A     65903      C   /usr/bin/python3                             1510MiB |
|    6   N/A  N/A     65904      C   /usr/bin/python3                             1510MiB |
|    7   N/A  N/A     65905      C   /usr/bin/python3                             1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2160 train_time:96ms step_avg:96.03ms
step:2/2160 train_time:117ms step_avg:58.31ms
step:3/2160 train_time:135ms step_avg:44.84ms
step:4/2160 train_time:162ms step_avg:40.42ms
step:5/2160 train_time:196ms step_avg:39.11ms
step:6/2160 train_time:253ms step_avg:42.22ms
step:7/2160 train_time:281ms step_avg:40.11ms
step:8/2160 train_time:313ms step_avg:39.19ms
step:9/2160 train_time:347ms step_avg:38.59ms
step:10/2160 train_time:380ms step_avg:38.00ms
step:11/2160 train_time:414ms step_avg:37.64ms
step:12/2160 train_time:447ms step_avg:37.23ms
step:13/2160 train_time:481ms step_avg:36.97ms
step:14/2160 train_time:513ms step_avg:36.68ms
step:15/2160 train_time:547ms step_avg:36.49ms
step:16/2160 train_time:580ms step_avg:36.26ms
step:17/2160 train_time:614ms step_avg:36.12ms
step:18/2160 train_time:647ms step_avg:35.93ms
step:19/2160 train_time:681ms step_avg:35.83ms
step:20/2160 train_time:713ms step_avg:35.67ms
step:21/2160 train_time:748ms step_avg:35.60ms
step:22/2160 train_time:780ms step_avg:35.48ms
step:23/2160 train_time:814ms step_avg:35.40ms
step:24/2160 train_time:847ms step_avg:35.29ms
step:25/2160 train_time:881ms step_avg:35.23ms
step:26/2160 train_time:913ms step_avg:35.13ms
step:27/2160 train_time:948ms step_avg:35.09ms
step:28/2160 train_time:980ms step_avg:35.01ms
step:29/2160 train_time:1014ms step_avg:34.97ms
step:30/2160 train_time:1047ms step_avg:34.89ms
step:31/2160 train_time:1081ms step_avg:34.87ms
step:32/2160 train_time:1114ms step_avg:34.80ms
step:33/2160 train_time:1148ms step_avg:34.77ms
step:34/2160 train_time:1180ms step_avg:34.72ms
step:35/2160 train_time:1214ms step_avg:34.69ms
step:36/2160 train_time:1247ms step_avg:34.64ms
step:37/2160 train_time:1281ms step_avg:34.63ms
step:38/2160 train_time:1314ms step_avg:34.58ms
step:39/2160 train_time:1349ms step_avg:34.58ms
step:40/2160 train_time:1381ms step_avg:34.53ms
step:41/2160 train_time:1416ms step_avg:34.53ms
step:42/2160 train_time:1448ms step_avg:34.49ms
step:43/2160 train_time:1483ms step_avg:34.49ms
step:44/2160 train_time:1516ms step_avg:34.45ms
step:45/2160 train_time:1550ms step_avg:34.45ms
step:46/2160 train_time:1583ms step_avg:34.41ms
step:47/2160 train_time:1617ms step_avg:34.41ms
step:48/2160 train_time:1650ms step_avg:34.38ms
step:49/2160 train_time:1684ms step_avg:34.37ms
step:50/2160 train_time:1717ms step_avg:34.34ms
step:51/2160 train_time:1751ms step_avg:34.33ms
step:52/2160 train_time:1784ms step_avg:34.30ms
step:53/2160 train_time:1818ms step_avg:34.30ms
step:54/2160 train_time:1850ms step_avg:34.27ms
step:55/2160 train_time:1885ms step_avg:34.27ms
step:56/2160 train_time:1918ms step_avg:34.25ms
step:57/2160 train_time:1952ms step_avg:34.24ms
step:58/2160 train_time:1984ms step_avg:34.21ms
step:59/2160 train_time:2018ms step_avg:34.21ms
step:60/2160 train_time:2051ms step_avg:34.18ms
step:61/2160 train_time:2085ms step_avg:34.18ms
step:62/2160 train_time:2118ms step_avg:34.16ms
step:63/2160 train_time:2152ms step_avg:34.15ms
step:64/2160 train_time:2184ms step_avg:34.13ms
step:65/2160 train_time:2219ms step_avg:34.14ms
step:66/2160 train_time:2253ms step_avg:34.14ms
step:67/2160 train_time:2286ms step_avg:34.11ms
step:68/2160 train_time:2318ms step_avg:34.09ms
step:69/2160 train_time:2352ms step_avg:34.09ms
step:70/2160 train_time:2385ms step_avg:34.07ms
step:71/2160 train_time:2419ms step_avg:34.07ms
step:72/2160 train_time:2452ms step_avg:34.06ms
step:73/2160 train_time:2486ms step_avg:34.06ms
step:74/2160 train_time:2519ms step_avg:34.04ms
step:75/2160 train_time:2553ms step_avg:34.04ms
step:76/2160 train_time:2586ms step_avg:34.02ms
step:77/2160 train_time:2620ms step_avg:34.02ms
step:78/2160 train_time:2652ms step_avg:34.01ms
step:79/2160 train_time:2687ms step_avg:34.01ms
step:80/2160 train_time:2720ms step_avg:34.00ms
step:81/2160 train_time:2754ms step_avg:34.00ms
step:82/2160 train_time:2786ms step_avg:33.98ms
step:83/2160 train_time:2820ms step_avg:33.98ms
step:84/2160 train_time:2853ms step_avg:33.96ms
step:85/2160 train_time:2887ms step_avg:33.97ms
step:86/2160 train_time:2920ms step_avg:33.95ms
step:87/2160 train_time:2954ms step_avg:33.95ms
step:88/2160 train_time:2987ms step_avg:33.94ms
step:89/2160 train_time:3021ms step_avg:33.94ms
step:90/2160 train_time:3054ms step_avg:33.93ms
step:91/2160 train_time:3088ms step_avg:33.94ms
step:92/2160 train_time:3121ms step_avg:33.92ms
step:93/2160 train_time:3155ms step_avg:33.92ms
step:94/2160 train_time:3188ms step_avg:33.91ms
step:95/2160 train_time:3222ms step_avg:33.92ms
step:96/2160 train_time:3255ms step_avg:33.90ms
step:97/2160 train_time:3289ms step_avg:33.91ms
step:98/2160 train_time:3322ms step_avg:33.89ms
step:99/2160 train_time:3356ms step_avg:33.89ms
step:100/2160 train_time:3388ms step_avg:33.88ms
step:101/2160 train_time:3422ms step_avg:33.89ms
step:102/2160 train_time:3455ms step_avg:33.88ms
step:103/2160 train_time:3489ms step_avg:33.88ms
step:104/2160 train_time:3522ms step_avg:33.86ms
step:105/2160 train_time:3556ms step_avg:33.87ms
step:106/2160 train_time:3589ms step_avg:33.86ms
step:107/2160 train_time:3623ms step_avg:33.86ms
step:108/2160 train_time:3656ms step_avg:33.85ms
step:109/2160 train_time:3690ms step_avg:33.85ms
step:110/2160 train_time:3723ms step_avg:33.84ms
step:111/2160 train_time:3757ms step_avg:33.84ms
step:112/2160 train_time:3789ms step_avg:33.83ms
step:113/2160 train_time:3823ms step_avg:33.83ms
step:114/2160 train_time:3856ms step_avg:33.83ms
step:115/2160 train_time:3890ms step_avg:33.82ms
step:116/2160 train_time:3922ms step_avg:33.81ms
step:117/2160 train_time:3957ms step_avg:33.82ms
step:118/2160 train_time:3989ms step_avg:33.81ms
step:119/2160 train_time:4023ms step_avg:33.81ms
step:120/2160 train_time:4056ms step_avg:33.80ms
step:121/2160 train_time:4090ms step_avg:33.80ms
step:122/2160 train_time:4122ms step_avg:33.79ms
step:123/2160 train_time:4156ms step_avg:33.79ms
step:124/2160 train_time:4189ms step_avg:33.78ms
step:125/2160 train_time:4223ms step_avg:33.78ms
step:126/2160 train_time:4256ms step_avg:33.78ms
step:127/2160 train_time:4290ms step_avg:33.78ms
step:128/2160 train_time:4322ms step_avg:33.77ms
step:129/2160 train_time:4356ms step_avg:33.77ms
step:130/2160 train_time:4389ms step_avg:33.76ms
step:131/2160 train_time:4424ms step_avg:33.77ms
step:132/2160 train_time:4456ms step_avg:33.76ms
step:133/2160 train_time:4490ms step_avg:33.76ms
step:134/2160 train_time:4523ms step_avg:33.76ms
step:135/2160 train_time:4557ms step_avg:33.76ms
step:136/2160 train_time:4590ms step_avg:33.75ms
step:137/2160 train_time:4624ms step_avg:33.75ms
step:138/2160 train_time:4657ms step_avg:33.75ms
step:139/2160 train_time:4691ms step_avg:33.75ms
step:140/2160 train_time:4724ms step_avg:33.74ms
step:141/2160 train_time:4758ms step_avg:33.74ms
step:142/2160 train_time:4790ms step_avg:33.73ms
step:143/2160 train_time:4824ms step_avg:33.74ms
step:144/2160 train_time:4857ms step_avg:33.73ms
step:145/2160 train_time:4891ms step_avg:33.73ms
step:146/2160 train_time:4924ms step_avg:33.72ms
step:147/2160 train_time:4957ms step_avg:33.72ms
step:148/2160 train_time:4990ms step_avg:33.72ms
step:149/2160 train_time:5024ms step_avg:33.72ms
step:150/2160 train_time:5057ms step_avg:33.71ms
step:151/2160 train_time:5200ms step_avg:34.44ms
step:152/2160 train_time:5219ms step_avg:34.33ms
step:153/2160 train_time:5248ms step_avg:34.30ms
step:154/2160 train_time:5281ms step_avg:34.29ms
step:155/2160 train_time:5314ms step_avg:34.29ms
step:156/2160 train_time:5347ms step_avg:34.28ms
step:157/2160 train_time:5381ms step_avg:34.27ms
step:158/2160 train_time:5413ms step_avg:34.26ms
step:159/2160 train_time:5447ms step_avg:34.26ms
step:160/2160 train_time:5480ms step_avg:34.25ms
step:161/2160 train_time:5514ms step_avg:34.25ms
step:162/2160 train_time:5546ms step_avg:34.24ms
step:163/2160 train_time:5580ms step_avg:34.23ms
step:164/2160 train_time:5613ms step_avg:34.22ms
step:165/2160 train_time:5647ms step_avg:34.22ms
step:166/2160 train_time:5679ms step_avg:34.21ms
step:167/2160 train_time:5713ms step_avg:34.21ms
step:168/2160 train_time:5746ms step_avg:34.20ms
step:169/2160 train_time:5780ms step_avg:34.20ms
step:170/2160 train_time:5813ms step_avg:34.19ms
step:171/2160 train_time:5846ms step_avg:34.19ms
step:172/2160 train_time:5879ms step_avg:34.18ms
step:173/2160 train_time:5913ms step_avg:34.18ms
step:174/2160 train_time:5945ms step_avg:34.17ms
step:175/2160 train_time:5979ms step_avg:34.17ms
step:176/2160 train_time:6012ms step_avg:34.16ms
step:177/2160 train_time:6045ms step_avg:34.16ms
step:178/2160 train_time:6078ms step_avg:34.15ms
step:179/2160 train_time:6112ms step_avg:34.15ms
step:180/2160 train_time:6145ms step_avg:34.14ms
step:181/2160 train_time:6179ms step_avg:34.14ms
step:182/2160 train_time:6211ms step_avg:34.13ms
step:183/2160 train_time:6245ms step_avg:34.13ms
step:184/2160 train_time:6278ms step_avg:34.12ms
step:185/2160 train_time:6312ms step_avg:34.12ms
step:186/2160 train_time:6344ms step_avg:34.11ms
step:187/2160 train_time:6378ms step_avg:34.11ms
step:188/2160 train_time:6411ms step_avg:34.10ms
step:189/2160 train_time:6445ms step_avg:34.10ms
step:190/2160 train_time:6477ms step_avg:34.09ms
step:191/2160 train_time:6511ms step_avg:34.09ms
step:192/2160 train_time:6544ms step_avg:34.08ms
step:193/2160 train_time:6578ms step_avg:34.08ms
step:194/2160 train_time:6611ms step_avg:34.08ms
step:195/2160 train_time:6645ms step_avg:34.08ms
step:196/2160 train_time:6677ms step_avg:34.07ms
step:197/2160 train_time:6711ms step_avg:34.07ms
step:198/2160 train_time:6744ms step_avg:34.06ms
step:199/2160 train_time:6777ms step_avg:34.06ms
step:200/2160 train_time:6810ms step_avg:34.05ms
step:201/2160 train_time:6844ms step_avg:34.05ms
step:202/2160 train_time:6876ms step_avg:34.04ms
step:203/2160 train_time:6910ms step_avg:34.04ms
step:204/2160 train_time:6943ms step_avg:34.03ms
step:205/2160 train_time:6977ms step_avg:34.03ms
step:206/2160 train_time:7009ms step_avg:34.03ms
step:207/2160 train_time:7043ms step_avg:34.02ms
step:208/2160 train_time:7076ms step_avg:34.02ms
step:209/2160 train_time:7110ms step_avg:34.02ms
step:210/2160 train_time:7142ms step_avg:34.01ms
step:211/2160 train_time:7176ms step_avg:34.01ms
step:212/2160 train_time:7209ms step_avg:34.00ms
step:213/2160 train_time:7243ms step_avg:34.00ms
step:214/2160 train_time:7276ms step_avg:34.00ms
step:215/2160 train_time:7310ms step_avg:34.00ms
step:216/2160 train_time:7342ms step_avg:33.99ms
step:217/2160 train_time:7376ms step_avg:33.99ms
step:218/2160 train_time:7409ms step_avg:33.99ms
step:219/2160 train_time:7443ms step_avg:33.99ms
step:220/2160 train_time:7476ms step_avg:33.98ms
step:221/2160 train_time:7509ms step_avg:33.98ms
step:222/2160 train_time:7542ms step_avg:33.97ms
step:223/2160 train_time:7575ms step_avg:33.97ms
step:224/2160 train_time:7608ms step_avg:33.96ms
step:225/2160 train_time:7642ms step_avg:33.96ms
step:226/2160 train_time:7675ms step_avg:33.96ms
step:227/2160 train_time:7709ms step_avg:33.96ms
step:228/2160 train_time:7742ms step_avg:33.95ms
step:229/2160 train_time:7775ms step_avg:33.95ms
step:230/2160 train_time:7808ms step_avg:33.95ms
step:231/2160 train_time:7842ms step_avg:33.95ms
step:232/2160 train_time:7875ms step_avg:33.94ms
step:233/2160 train_time:7909ms step_avg:33.94ms
step:234/2160 train_time:7942ms step_avg:33.94ms
step:235/2160 train_time:7975ms step_avg:33.94ms
step:236/2160 train_time:8008ms step_avg:33.93ms
step:237/2160 train_time:8042ms step_avg:33.93ms
step:238/2160 train_time:8075ms step_avg:33.93ms
step:239/2160 train_time:8109ms step_avg:33.93ms
step:240/2160 train_time:8142ms step_avg:33.92ms
step:241/2160 train_time:8176ms step_avg:33.92ms
step:242/2160 train_time:8208ms step_avg:33.92ms
step:243/2160 train_time:8242ms step_avg:33.92ms
step:244/2160 train_time:8275ms step_avg:33.91ms
step:245/2160 train_time:8309ms step_avg:33.91ms
step:246/2160 train_time:8341ms step_avg:33.91ms
step:247/2160 train_time:8375ms step_avg:33.91ms
step:248/2160 train_time:8408ms step_avg:33.90ms
step:249/2160 train_time:8442ms step_avg:33.90ms
step:250/2160 train_time:8474ms step_avg:33.90ms
step:250/2160 val_loss:4.3045 train_time:8510ms step_avg:34.04ms
step:251/2160 train_time:8529ms step_avg:33.98ms
step:252/2160 train_time:8548ms step_avg:33.92ms
step:253/2160 train_time:8578ms step_avg:33.91ms
step:254/2160 train_time:8612ms step_avg:33.90ms
step:255/2160 train_time:8647ms step_avg:33.91ms
step:256/2160 train_time:8680ms step_avg:33.91ms
step:257/2160 train_time:8714ms step_avg:33.91ms
step:258/2160 train_time:8747ms step_avg:33.90ms
step:259/2160 train_time:8781ms step_avg:33.90ms
step:260/2160 train_time:8814ms step_avg:33.90ms
step:261/2160 train_time:8848ms step_avg:33.90ms
step:262/2160 train_time:8881ms step_avg:33.90ms
step:263/2160 train_time:8915ms step_avg:33.90ms
step:264/2160 train_time:8947ms step_avg:33.89ms
step:265/2160 train_time:8981ms step_avg:33.89ms
step:266/2160 train_time:9014ms step_avg:33.89ms
step:267/2160 train_time:9047ms step_avg:33.89ms
step:268/2160 train_time:9080ms step_avg:33.88ms
step:269/2160 train_time:9114ms step_avg:33.88ms
step:270/2160 train_time:9146ms step_avg:33.88ms
step:271/2160 train_time:9180ms step_avg:33.87ms
step:272/2160 train_time:9213ms step_avg:33.87ms
step:273/2160 train_time:9247ms step_avg:33.87ms
step:274/2160 train_time:9340ms step_avg:34.09ms
step:275/2160 train_time:9358ms step_avg:34.03ms
step:276/2160 train_time:9388ms step_avg:34.01ms
step:277/2160 train_time:9421ms step_avg:34.01ms
step:278/2160 train_time:9454ms step_avg:34.01ms
step:279/2160 train_time:9488ms step_avg:34.01ms
step:280/2160 train_time:9521ms step_avg:34.00ms
step:281/2160 train_time:9554ms step_avg:34.00ms
step:282/2160 train_time:9587ms step_avg:34.00ms
step:283/2160 train_time:9621ms step_avg:33.99ms
step:284/2160 train_time:9653ms step_avg:33.99ms
step:285/2160 train_time:9687ms step_avg:33.99ms
step:286/2160 train_time:9720ms step_avg:33.98ms
step:287/2160 train_time:9753ms step_avg:33.98ms
step:288/2160 train_time:9786ms step_avg:33.98ms
step:289/2160 train_time:9820ms step_avg:33.98ms
step:290/2160 train_time:9852ms step_avg:33.97ms
step:291/2160 train_time:9886ms step_avg:33.97ms
step:292/2160 train_time:9918ms step_avg:33.97ms
step:293/2160 train_time:9953ms step_avg:33.97ms
step:294/2160 train_time:9985ms step_avg:33.96ms
step:295/2160 train_time:10019ms step_avg:33.96ms
step:296/2160 train_time:10052ms step_avg:33.96ms
step:297/2160 train_time:10085ms step_avg:33.96ms
step:298/2160 train_time:10118ms step_avg:33.95ms
step:299/2160 train_time:10152ms step_avg:33.95ms
step:300/2160 train_time:10184ms step_avg:33.95ms
step:301/2160 train_time:10218ms step_avg:33.95ms
step:302/2160 train_time:10251ms step_avg:33.94ms
step:303/2160 train_time:10285ms step_avg:33.94ms
step:304/2160 train_time:10317ms step_avg:33.94ms
step:305/2160 train_time:10351ms step_avg:33.94ms
step:306/2160 train_time:10384ms step_avg:33.93ms
step:307/2160 train_time:10418ms step_avg:33.93ms
step:308/2160 train_time:10450ms step_avg:33.93ms
step:309/2160 train_time:10484ms step_avg:33.93ms
step:310/2160 train_time:10517ms step_avg:33.92ms
step:311/2160 train_time:10551ms step_avg:33.92ms
step:312/2160 train_time:10583ms step_avg:33.92ms
step:313/2160 train_time:10617ms step_avg:33.92ms
step:314/2160 train_time:10650ms step_avg:33.92ms
step:315/2160 train_time:10683ms step_avg:33.92ms
step:316/2160 train_time:10716ms step_avg:33.91ms
step:317/2160 train_time:10750ms step_avg:33.91ms
step:318/2160 train_time:10782ms step_avg:33.91ms
step:319/2160 train_time:10816ms step_avg:33.91ms
step:320/2160 train_time:10849ms step_avg:33.90ms
step:321/2160 train_time:10882ms step_avg:33.90ms
step:322/2160 train_time:10915ms step_avg:33.90ms
step:323/2160 train_time:10949ms step_avg:33.90ms
step:324/2160 train_time:10981ms step_avg:33.89ms
step:325/2160 train_time:11015ms step_avg:33.89ms
step:326/2160 train_time:11048ms step_avg:33.89ms
step:327/2160 train_time:11082ms step_avg:33.89ms
step:328/2160 train_time:11114ms step_avg:33.89ms
step:329/2160 train_time:11148ms step_avg:33.88ms
step:330/2160 train_time:11181ms step_avg:33.88ms
step:331/2160 train_time:11215ms step_avg:33.88ms
step:332/2160 train_time:11247ms step_avg:33.88ms
step:333/2160 train_time:11281ms step_avg:33.88ms
step:334/2160 train_time:11314ms step_avg:33.87ms
step:335/2160 train_time:11348ms step_avg:33.87ms
step:336/2160 train_time:11380ms step_avg:33.87ms
step:337/2160 train_time:11414ms step_avg:33.87ms
step:338/2160 train_time:11447ms step_avg:33.87ms
step:339/2160 train_time:11481ms step_avg:33.87ms
step:340/2160 train_time:11513ms step_avg:33.86ms
step:341/2160 train_time:11547ms step_avg:33.86ms
step:342/2160 train_time:11580ms step_avg:33.86ms
step:343/2160 train_time:11614ms step_avg:33.86ms
step:344/2160 train_time:11646ms step_avg:33.86ms
step:345/2160 train_time:11680ms step_avg:33.85ms
step:346/2160 train_time:11713ms step_avg:33.85ms
step:347/2160 train_time:11746ms step_avg:33.85ms
step:348/2160 train_time:11779ms step_avg:33.85ms
step:349/2160 train_time:11813ms step_avg:33.85ms
step:350/2160 train_time:11846ms step_avg:33.84ms
step:351/2160 train_time:11879ms step_avg:33.84ms
step:352/2160 train_time:11912ms step_avg:33.84ms
step:353/2160 train_time:11946ms step_avg:33.84ms
step:354/2160 train_time:11978ms step_avg:33.84ms
step:355/2160 train_time:12012ms step_avg:33.84ms
step:356/2160 train_time:12045ms step_avg:33.83ms
step:357/2160 train_time:12079ms step_avg:33.83ms
step:358/2160 train_time:12111ms step_avg:33.83ms
step:359/2160 train_time:12145ms step_avg:33.83ms
step:360/2160 train_time:12178ms step_avg:33.83ms
step:361/2160 train_time:12212ms step_avg:33.83ms
step:362/2160 train_time:12245ms step_avg:33.83ms
step:363/2160 train_time:12279ms step_avg:33.83ms
step:364/2160 train_time:12311ms step_avg:33.82ms
step:365/2160 train_time:12345ms step_avg:33.82ms
step:366/2160 train_time:12378ms step_avg:33.82ms
step:367/2160 train_time:12412ms step_avg:33.82ms
step:368/2160 train_time:12444ms step_avg:33.82ms
step:369/2160 train_time:12478ms step_avg:33.82ms
step:370/2160 train_time:12511ms step_avg:33.81ms
step:371/2160 train_time:12545ms step_avg:33.81ms
step:372/2160 train_time:12577ms step_avg:33.81ms
step:373/2160 train_time:12611ms step_avg:33.81ms
step:374/2160 train_time:12644ms step_avg:33.81ms
step:375/2160 train_time:12677ms step_avg:33.81ms
step:376/2160 train_time:12710ms step_avg:33.80ms
step:377/2160 train_time:12744ms step_avg:33.80ms
step:378/2160 train_time:12776ms step_avg:33.80ms
step:379/2160 train_time:12810ms step_avg:33.80ms
step:380/2160 train_time:12843ms step_avg:33.80ms
step:381/2160 train_time:12877ms step_avg:33.80ms
step:382/2160 train_time:12909ms step_avg:33.79ms
step:383/2160 train_time:12943ms step_avg:33.79ms
step:384/2160 train_time:12976ms step_avg:33.79ms
step:385/2160 train_time:13010ms step_avg:33.79ms
step:386/2160 train_time:13043ms step_avg:33.79ms
step:387/2160 train_time:13077ms step_avg:33.79ms
step:388/2160 train_time:13109ms step_avg:33.79ms
step:389/2160 train_time:13143ms step_avg:33.79ms
step:390/2160 train_time:13176ms step_avg:33.78ms
step:391/2160 train_time:13210ms step_avg:33.79ms
step:392/2160 train_time:13243ms step_avg:33.78ms
step:393/2160 train_time:13276ms step_avg:33.78ms
step:394/2160 train_time:13309ms step_avg:33.78ms
step:395/2160 train_time:13343ms step_avg:33.78ms
step:396/2160 train_time:13375ms step_avg:33.78ms
step:397/2160 train_time:13409ms step_avg:33.78ms
step:398/2160 train_time:13442ms step_avg:33.77ms
step:399/2160 train_time:13476ms step_avg:33.77ms
step:400/2160 train_time:13509ms step_avg:33.77ms
step:401/2160 train_time:13542ms step_avg:33.77ms
step:402/2160 train_time:13575ms step_avg:33.77ms
step:403/2160 train_time:13609ms step_avg:33.77ms
step:404/2160 train_time:13642ms step_avg:33.77ms
step:405/2160 train_time:13675ms step_avg:33.77ms
step:406/2160 train_time:13708ms step_avg:33.76ms
step:407/2160 train_time:13742ms step_avg:33.76ms
step:408/2160 train_time:13774ms step_avg:33.76ms
step:409/2160 train_time:13809ms step_avg:33.76ms
step:410/2160 train_time:13841ms step_avg:33.76ms
step:411/2160 train_time:13875ms step_avg:33.76ms
step:412/2160 train_time:13908ms step_avg:33.76ms
step:413/2160 train_time:13941ms step_avg:33.76ms
step:414/2160 train_time:13974ms step_avg:33.75ms
step:415/2160 train_time:14008ms step_avg:33.75ms
step:416/2160 train_time:14041ms step_avg:33.75ms
step:417/2160 train_time:14074ms step_avg:33.75ms
step:418/2160 train_time:14107ms step_avg:33.75ms
step:419/2160 train_time:14141ms step_avg:33.75ms
step:420/2160 train_time:14174ms step_avg:33.75ms
step:421/2160 train_time:14208ms step_avg:33.75ms
step:422/2160 train_time:14240ms step_avg:33.75ms
step:423/2160 train_time:14274ms step_avg:33.75ms
step:424/2160 train_time:14307ms step_avg:33.74ms
step:425/2160 train_time:14341ms step_avg:33.74ms
step:426/2160 train_time:14373ms step_avg:33.74ms
step:427/2160 train_time:14407ms step_avg:33.74ms
step:428/2160 train_time:14440ms step_avg:33.74ms
step:429/2160 train_time:14474ms step_avg:33.74ms
step:430/2160 train_time:14507ms step_avg:33.74ms
step:431/2160 train_time:14540ms step_avg:33.74ms
step:432/2160 train_time:14573ms step_avg:33.73ms
step:433/2160 train_time:14607ms step_avg:33.74ms
step:434/2160 train_time:14640ms step_avg:33.73ms
step:435/2160 train_time:14674ms step_avg:33.73ms
step:436/2160 train_time:14707ms step_avg:33.73ms
step:437/2160 train_time:14740ms step_avg:33.73ms
step:438/2160 train_time:14773ms step_avg:33.73ms
step:439/2160 train_time:14807ms step_avg:33.73ms
step:440/2160 train_time:14839ms step_avg:33.73ms
step:441/2160 train_time:14873ms step_avg:33.73ms
step:442/2160 train_time:14906ms step_avg:33.72ms
step:443/2160 train_time:14940ms step_avg:33.72ms
step:444/2160 train_time:14972ms step_avg:33.72ms
step:445/2160 train_time:15006ms step_avg:33.72ms
step:446/2160 train_time:15039ms step_avg:33.72ms
step:447/2160 train_time:15072ms step_avg:33.72ms
step:448/2160 train_time:15105ms step_avg:33.72ms
step:449/2160 train_time:15139ms step_avg:33.72ms
step:450/2160 train_time:15172ms step_avg:33.71ms
step:451/2160 train_time:15205ms step_avg:33.71ms
step:452/2160 train_time:15238ms step_avg:33.71ms
step:453/2160 train_time:15272ms step_avg:33.71ms
step:454/2160 train_time:15304ms step_avg:33.71ms
step:455/2160 train_time:15338ms step_avg:33.71ms
step:456/2160 train_time:15371ms step_avg:33.71ms
step:457/2160 train_time:15405ms step_avg:33.71ms
step:458/2160 train_time:15437ms step_avg:33.71ms
step:459/2160 train_time:15472ms step_avg:33.71ms
step:460/2160 train_time:15504ms step_avg:33.70ms
step:461/2160 train_time:15538ms step_avg:33.70ms
step:462/2160 train_time:15571ms step_avg:33.70ms
step:463/2160 train_time:15604ms step_avg:33.70ms
step:464/2160 train_time:15637ms step_avg:33.70ms
step:465/2160 train_time:15671ms step_avg:33.70ms
step:466/2160 train_time:15703ms step_avg:33.70ms
step:467/2160 train_time:15737ms step_avg:33.70ms
step:468/2160 train_time:15770ms step_avg:33.70ms
step:469/2160 train_time:15804ms step_avg:33.70ms
step:470/2160 train_time:15837ms step_avg:33.70ms
step:471/2160 train_time:15871ms step_avg:33.70ms
step:472/2160 train_time:15903ms step_avg:33.69ms
step:473/2160 train_time:15937ms step_avg:33.69ms
step:474/2160 train_time:15970ms step_avg:33.69ms
step:475/2160 train_time:16004ms step_avg:33.69ms
step:476/2160 train_time:16036ms step_avg:33.69ms
step:477/2160 train_time:16071ms step_avg:33.69ms
step:478/2160 train_time:16103ms step_avg:33.69ms
step:479/2160 train_time:16137ms step_avg:33.69ms
step:480/2160 train_time:16170ms step_avg:33.69ms
step:481/2160 train_time:16204ms step_avg:33.69ms
step:482/2160 train_time:16237ms step_avg:33.69ms
step:483/2160 train_time:16271ms step_avg:33.69ms
step:484/2160 train_time:16303ms step_avg:33.68ms
step:485/2160 train_time:16337ms step_avg:33.68ms
step:486/2160 train_time:16370ms step_avg:33.68ms
step:487/2160 train_time:16404ms step_avg:33.68ms
step:488/2160 train_time:16436ms step_avg:33.68ms
step:489/2160 train_time:16470ms step_avg:33.68ms
step:490/2160 train_time:16503ms step_avg:33.68ms
step:491/2160 train_time:16537ms step_avg:33.68ms
step:492/2160 train_time:16570ms step_avg:33.68ms
step:493/2160 train_time:16604ms step_avg:33.68ms
step:494/2160 train_time:16636ms step_avg:33.68ms
step:495/2160 train_time:16670ms step_avg:33.68ms
step:496/2160 train_time:16703ms step_avg:33.68ms
step:497/2160 train_time:16737ms step_avg:33.68ms
step:498/2160 train_time:16770ms step_avg:33.67ms
step:499/2160 train_time:16804ms step_avg:33.68ms
step:500/2160 train_time:16837ms step_avg:33.67ms
step:500/2160 val_loss:4.0492 train_time:16872ms step_avg:33.74ms
step:501/2160 train_time:16892ms step_avg:33.72ms
step:502/2160 train_time:16911ms step_avg:33.69ms
step:503/2160 train_time:16941ms step_avg:33.68ms
step:504/2160 train_time:16974ms step_avg:33.68ms
step:505/2160 train_time:17010ms step_avg:33.68ms
step:506/2160 train_time:17046ms step_avg:33.69ms
step:507/2160 train_time:17082ms step_avg:33.69ms
step:508/2160 train_time:17114ms step_avg:33.69ms
step:509/2160 train_time:17149ms step_avg:33.69ms
step:510/2160 train_time:17183ms step_avg:33.69ms
step:511/2160 train_time:17216ms step_avg:33.69ms
step:512/2160 train_time:17249ms step_avg:33.69ms
step:513/2160 train_time:17282ms step_avg:33.69ms
step:514/2160 train_time:17315ms step_avg:33.69ms
step:515/2160 train_time:17349ms step_avg:33.69ms
step:516/2160 train_time:17381ms step_avg:33.68ms
step:517/2160 train_time:17415ms step_avg:33.69ms
step:518/2160 train_time:17448ms step_avg:33.68ms
step:519/2160 train_time:17482ms step_avg:33.68ms
step:520/2160 train_time:17514ms step_avg:33.68ms
step:521/2160 train_time:17548ms step_avg:33.68ms
step:522/2160 train_time:17581ms step_avg:33.68ms
step:523/2160 train_time:17615ms step_avg:33.68ms
step:524/2160 train_time:17647ms step_avg:33.68ms
step:525/2160 train_time:17681ms step_avg:33.68ms
step:526/2160 train_time:17714ms step_avg:33.68ms
step:527/2160 train_time:17747ms step_avg:33.68ms
step:528/2160 train_time:17780ms step_avg:33.67ms
step:529/2160 train_time:17814ms step_avg:33.67ms
step:530/2160 train_time:17846ms step_avg:33.67ms
step:531/2160 train_time:17880ms step_avg:33.67ms
step:532/2160 train_time:17913ms step_avg:33.67ms
step:533/2160 train_time:17947ms step_avg:33.67ms
step:534/2160 train_time:17979ms step_avg:33.67ms
step:535/2160 train_time:18013ms step_avg:33.67ms
step:536/2160 train_time:18046ms step_avg:33.67ms
step:537/2160 train_time:18080ms step_avg:33.67ms
step:538/2160 train_time:18112ms step_avg:33.67ms
step:539/2160 train_time:18147ms step_avg:33.67ms
step:540/2160 train_time:18179ms step_avg:33.67ms
step:541/2160 train_time:18213ms step_avg:33.67ms
step:542/2160 train_time:18246ms step_avg:33.66ms
step:543/2160 train_time:18280ms step_avg:33.67ms
step:544/2160 train_time:18313ms step_avg:33.66ms
step:545/2160 train_time:18347ms step_avg:33.66ms
step:546/2160 train_time:18380ms step_avg:33.66ms
step:547/2160 train_time:18414ms step_avg:33.66ms
step:548/2160 train_time:18446ms step_avg:33.66ms
step:549/2160 train_time:18480ms step_avg:33.66ms
step:550/2160 train_time:18513ms step_avg:33.66ms
step:551/2160 train_time:18576ms step_avg:33.71ms
step:552/2160 train_time:18595ms step_avg:33.69ms
step:553/2160 train_time:18625ms step_avg:33.68ms
step:554/2160 train_time:18657ms step_avg:33.68ms
step:555/2160 train_time:18691ms step_avg:33.68ms
step:556/2160 train_time:18724ms step_avg:33.68ms
step:557/2160 train_time:18757ms step_avg:33.68ms
step:558/2160 train_time:18790ms step_avg:33.67ms
step:559/2160 train_time:18824ms step_avg:33.67ms
step:560/2160 train_time:18856ms step_avg:33.67ms
step:561/2160 train_time:18890ms step_avg:33.67ms
step:562/2160 train_time:18923ms step_avg:33.67ms
step:563/2160 train_time:18956ms step_avg:33.67ms
step:564/2160 train_time:18989ms step_avg:33.67ms
step:565/2160 train_time:19023ms step_avg:33.67ms
step:566/2160 train_time:19056ms step_avg:33.67ms
step:567/2160 train_time:19089ms step_avg:33.67ms
step:568/2160 train_time:19122ms step_avg:33.67ms
step:569/2160 train_time:19156ms step_avg:33.67ms
step:570/2160 train_time:19188ms step_avg:33.66ms
step:571/2160 train_time:19222ms step_avg:33.66ms
step:572/2160 train_time:19255ms step_avg:33.66ms
step:573/2160 train_time:19289ms step_avg:33.66ms
step:574/2160 train_time:19321ms step_avg:33.66ms
step:575/2160 train_time:19355ms step_avg:33.66ms
step:576/2160 train_time:19388ms step_avg:33.66ms
step:577/2160 train_time:19422ms step_avg:33.66ms
step:578/2160 train_time:19454ms step_avg:33.66ms
step:579/2160 train_time:19488ms step_avg:33.66ms
step:580/2160 train_time:19521ms step_avg:33.66ms
step:581/2160 train_time:19555ms step_avg:33.66ms
step:582/2160 train_time:19588ms step_avg:33.66ms
step:583/2160 train_time:19622ms step_avg:33.66ms
step:584/2160 train_time:19655ms step_avg:33.66ms
step:585/2160 train_time:19689ms step_avg:33.66ms
step:586/2160 train_time:19722ms step_avg:33.65ms
step:587/2160 train_time:19755ms step_avg:33.65ms
step:588/2160 train_time:19788ms step_avg:33.65ms
step:589/2160 train_time:19822ms step_avg:33.65ms
step:590/2160 train_time:19854ms step_avg:33.65ms
step:591/2160 train_time:19888ms step_avg:33.65ms
step:592/2160 train_time:19921ms step_avg:33.65ms
step:593/2160 train_time:19955ms step_avg:33.65ms
step:594/2160 train_time:19988ms step_avg:33.65ms
step:595/2160 train_time:20021ms step_avg:33.65ms
step:596/2160 train_time:20054ms step_avg:33.65ms
step:597/2160 train_time:20088ms step_avg:33.65ms
step:598/2160 train_time:20120ms step_avg:33.65ms
step:599/2160 train_time:20154ms step_avg:33.65ms
step:600/2160 train_time:20187ms step_avg:33.64ms
step:601/2160 train_time:20221ms step_avg:33.64ms
step:602/2160 train_time:20253ms step_avg:33.64ms
step:603/2160 train_time:20287ms step_avg:33.64ms
step:604/2160 train_time:20320ms step_avg:33.64ms
step:605/2160 train_time:20354ms step_avg:33.64ms
step:606/2160 train_time:20387ms step_avg:33.64ms
step:607/2160 train_time:20420ms step_avg:33.64ms
step:608/2160 train_time:20453ms step_avg:33.64ms
step:609/2160 train_time:20487ms step_avg:33.64ms
step:610/2160 train_time:20519ms step_avg:33.64ms
step:611/2160 train_time:20553ms step_avg:33.64ms
step:612/2160 train_time:20586ms step_avg:33.64ms
step:613/2160 train_time:20620ms step_avg:33.64ms
step:614/2160 train_time:20653ms step_avg:33.64ms
step:615/2160 train_time:20687ms step_avg:33.64ms
step:616/2160 train_time:20719ms step_avg:33.64ms
step:617/2160 train_time:20753ms step_avg:33.64ms
step:618/2160 train_time:20786ms step_avg:33.63ms
step:619/2160 train_time:20820ms step_avg:33.63ms
step:620/2160 train_time:20853ms step_avg:33.63ms
step:621/2160 train_time:20887ms step_avg:33.63ms
step:622/2160 train_time:20919ms step_avg:33.63ms
step:623/2160 train_time:20954ms step_avg:33.63ms
step:624/2160 train_time:20986ms step_avg:33.63ms
step:625/2160 train_time:21019ms step_avg:33.63ms
step:626/2160 train_time:21052ms step_avg:33.63ms
step:627/2160 train_time:21086ms step_avg:33.63ms
step:628/2160 train_time:21119ms step_avg:33.63ms
step:629/2160 train_time:21153ms step_avg:33.63ms
step:630/2160 train_time:21186ms step_avg:33.63ms
step:631/2160 train_time:21219ms step_avg:33.63ms
step:632/2160 train_time:21252ms step_avg:33.63ms
step:633/2160 train_time:21286ms step_avg:33.63ms
step:634/2160 train_time:21319ms step_avg:33.63ms
step:635/2160 train_time:21352ms step_avg:33.63ms
step:636/2160 train_time:21385ms step_avg:33.62ms
step:637/2160 train_time:21419ms step_avg:33.62ms
step:638/2160 train_time:21451ms step_avg:33.62ms
step:639/2160 train_time:21485ms step_avg:33.62ms
step:640/2160 train_time:21518ms step_avg:33.62ms
step:641/2160 train_time:21552ms step_avg:33.62ms
step:642/2160 train_time:21585ms step_avg:33.62ms
step:643/2160 train_time:21618ms step_avg:33.62ms
step:644/2160 train_time:21651ms step_avg:33.62ms
step:645/2160 train_time:21685ms step_avg:33.62ms
step:646/2160 train_time:21718ms step_avg:33.62ms
step:647/2160 train_time:21752ms step_avg:33.62ms
step:648/2160 train_time:21785ms step_avg:33.62ms
step:649/2160 train_time:21818ms step_avg:33.62ms
step:650/2160 train_time:21851ms step_avg:33.62ms
step:651/2160 train_time:21885ms step_avg:33.62ms
step:652/2160 train_time:21918ms step_avg:33.62ms
step:653/2160 train_time:21952ms step_avg:33.62ms
step:654/2160 train_time:21985ms step_avg:33.62ms
step:655/2160 train_time:22018ms step_avg:33.62ms
step:656/2160 train_time:22051ms step_avg:33.61ms
step:657/2160 train_time:22085ms step_avg:33.61ms
step:658/2160 train_time:22118ms step_avg:33.61ms
step:659/2160 train_time:22151ms step_avg:33.61ms
step:660/2160 train_time:22184ms step_avg:33.61ms
step:661/2160 train_time:22218ms step_avg:33.61ms
step:662/2160 train_time:22251ms step_avg:33.61ms
step:663/2160 train_time:22285ms step_avg:33.61ms
step:664/2160 train_time:22317ms step_avg:33.61ms
step:665/2160 train_time:22351ms step_avg:33.61ms
step:666/2160 train_time:22384ms step_avg:33.61ms
step:667/2160 train_time:22418ms step_avg:33.61ms
step:668/2160 train_time:22450ms step_avg:33.61ms
step:669/2160 train_time:22484ms step_avg:33.61ms
step:670/2160 train_time:22517ms step_avg:33.61ms
step:671/2160 train_time:22551ms step_avg:33.61ms
step:672/2160 train_time:22584ms step_avg:33.61ms
step:673/2160 train_time:22618ms step_avg:33.61ms
step:674/2160 train_time:22650ms step_avg:33.61ms
step:675/2160 train_time:22684ms step_avg:33.61ms
step:676/2160 train_time:22717ms step_avg:33.60ms
step:677/2160 train_time:22751ms step_avg:33.61ms
step:678/2160 train_time:22784ms step_avg:33.60ms
step:679/2160 train_time:22818ms step_avg:33.61ms
step:680/2160 train_time:22851ms step_avg:33.60ms
step:681/2160 train_time:22885ms step_avg:33.60ms
step:682/2160 train_time:22917ms step_avg:33.60ms
step:683/2160 train_time:22951ms step_avg:33.60ms
step:684/2160 train_time:22984ms step_avg:33.60ms
step:685/2160 train_time:23018ms step_avg:33.60ms
step:686/2160 train_time:23051ms step_avg:33.60ms
step:687/2160 train_time:23085ms step_avg:33.60ms
step:688/2160 train_time:23117ms step_avg:33.60ms
step:689/2160 train_time:23151ms step_avg:33.60ms
step:690/2160 train_time:23184ms step_avg:33.60ms
step:691/2160 train_time:23218ms step_avg:33.60ms
step:692/2160 train_time:23251ms step_avg:33.60ms
step:693/2160 train_time:23285ms step_avg:33.60ms
step:694/2160 train_time:23317ms step_avg:33.60ms
step:695/2160 train_time:23351ms step_avg:33.60ms
step:696/2160 train_time:23384ms step_avg:33.60ms
step:697/2160 train_time:23418ms step_avg:33.60ms
step:698/2160 train_time:23450ms step_avg:33.60ms
step:699/2160 train_time:23484ms step_avg:33.60ms
step:700/2160 train_time:23517ms step_avg:33.60ms
step:701/2160 train_time:23551ms step_avg:33.60ms
step:702/2160 train_time:23584ms step_avg:33.59ms
step:703/2160 train_time:23618ms step_avg:33.60ms
step:704/2160 train_time:23650ms step_avg:33.59ms
step:705/2160 train_time:23685ms step_avg:33.60ms
step:706/2160 train_time:23717ms step_avg:33.59ms
step:707/2160 train_time:23751ms step_avg:33.59ms
step:708/2160 train_time:23785ms step_avg:33.59ms
step:709/2160 train_time:23844ms step_avg:33.63ms
step:710/2160 train_time:23902ms step_avg:33.67ms
step:711/2160 train_time:23962ms step_avg:33.70ms
step:712/2160 train_time:24021ms step_avg:33.74ms
step:713/2160 train_time:24081ms step_avg:33.77ms
step:714/2160 train_time:24140ms step_avg:33.81ms
step:715/2160 train_time:24201ms step_avg:33.85ms
step:716/2160 train_time:24259ms step_avg:33.88ms
step:717/2160 train_time:24320ms step_avg:33.92ms
step:718/2160 train_time:24379ms step_avg:33.95ms
step:719/2160 train_time:24440ms step_avg:33.99ms
step:720/2160 train_time:24500ms step_avg:34.03ms
step:721/2160 train_time:24560ms step_avg:34.06ms
step:722/2160 train_time:24619ms step_avg:34.10ms
step:723/2160 train_time:24680ms step_avg:34.14ms
step:724/2160 train_time:24739ms step_avg:34.17ms
step:725/2160 train_time:24800ms step_avg:34.21ms
step:726/2160 train_time:24858ms step_avg:34.24ms
step:727/2160 train_time:24919ms step_avg:34.28ms
step:728/2160 train_time:24977ms step_avg:34.31ms
step:729/2160 train_time:25038ms step_avg:34.35ms
step:730/2160 train_time:25096ms step_avg:34.38ms
step:731/2160 train_time:25157ms step_avg:34.41ms
step:732/2160 train_time:25216ms step_avg:34.45ms
step:733/2160 train_time:25277ms step_avg:34.48ms
step:734/2160 train_time:25336ms step_avg:34.52ms
step:735/2160 train_time:25397ms step_avg:34.55ms
step:736/2160 train_time:25456ms step_avg:34.59ms
step:737/2160 train_time:25516ms step_avg:34.62ms
step:738/2160 train_time:25576ms step_avg:34.66ms
step:739/2160 train_time:25637ms step_avg:34.69ms
step:740/2160 train_time:25696ms step_avg:34.72ms
step:741/2160 train_time:25757ms step_avg:34.76ms
step:742/2160 train_time:25815ms step_avg:34.79ms
step:743/2160 train_time:25877ms step_avg:34.83ms
step:744/2160 train_time:25936ms step_avg:34.86ms
step:745/2160 train_time:25996ms step_avg:34.89ms
step:746/2160 train_time:26054ms step_avg:34.93ms
step:747/2160 train_time:26115ms step_avg:34.96ms
step:748/2160 train_time:26174ms step_avg:34.99ms
step:749/2160 train_time:26235ms step_avg:35.03ms
step:750/2160 train_time:26294ms step_avg:35.06ms
step:750/2160 val_loss:3.8508 train_time:26356ms step_avg:35.14ms
step:751/2160 train_time:26376ms step_avg:35.12ms
step:752/2160 train_time:26418ms step_avg:35.13ms
step:753/2160 train_time:26481ms step_avg:35.17ms
step:754/2160 train_time:26543ms step_avg:35.20ms
step:755/2160 train_time:26604ms step_avg:35.24ms
step:756/2160 train_time:26665ms step_avg:35.27ms
step:757/2160 train_time:26725ms step_avg:35.30ms
step:758/2160 train_time:26783ms step_avg:35.33ms
step:759/2160 train_time:26843ms step_avg:35.37ms
step:760/2160 train_time:26901ms step_avg:35.40ms
step:761/2160 train_time:26961ms step_avg:35.43ms
step:762/2160 train_time:27019ms step_avg:35.46ms
step:763/2160 train_time:27079ms step_avg:35.49ms
step:764/2160 train_time:27136ms step_avg:35.52ms
step:765/2160 train_time:27196ms step_avg:35.55ms
step:766/2160 train_time:27254ms step_avg:35.58ms
step:767/2160 train_time:27316ms step_avg:35.61ms
step:768/2160 train_time:27375ms step_avg:35.64ms
step:769/2160 train_time:27436ms step_avg:35.68ms
step:770/2160 train_time:27496ms step_avg:35.71ms
step:771/2160 train_time:27559ms step_avg:35.74ms
step:772/2160 train_time:27618ms step_avg:35.77ms
step:773/2160 train_time:27679ms step_avg:35.81ms
step:774/2160 train_time:27738ms step_avg:35.84ms
step:775/2160 train_time:27798ms step_avg:35.87ms
step:776/2160 train_time:27858ms step_avg:35.90ms
step:777/2160 train_time:27918ms step_avg:35.93ms
step:778/2160 train_time:27976ms step_avg:35.96ms
step:779/2160 train_time:28035ms step_avg:35.99ms
step:780/2160 train_time:28093ms step_avg:36.02ms
step:781/2160 train_time:28153ms step_avg:36.05ms
step:782/2160 train_time:28212ms step_avg:36.08ms
step:783/2160 train_time:28272ms step_avg:36.11ms
step:784/2160 train_time:28331ms step_avg:36.14ms
step:785/2160 train_time:28392ms step_avg:36.17ms
step:786/2160 train_time:28451ms step_avg:36.20ms
step:787/2160 train_time:28513ms step_avg:36.23ms
step:788/2160 train_time:28573ms step_avg:36.26ms
step:789/2160 train_time:28634ms step_avg:36.29ms
step:790/2160 train_time:28693ms step_avg:36.32ms
step:791/2160 train_time:28753ms step_avg:36.35ms
step:792/2160 train_time:28812ms step_avg:36.38ms
step:793/2160 train_time:28873ms step_avg:36.41ms
step:794/2160 train_time:28931ms step_avg:36.44ms
step:795/2160 train_time:28991ms step_avg:36.47ms
step:796/2160 train_time:29049ms step_avg:36.49ms
step:797/2160 train_time:29109ms step_avg:36.52ms
step:798/2160 train_time:29168ms step_avg:36.55ms
step:799/2160 train_time:29228ms step_avg:36.58ms
step:800/2160 train_time:29286ms step_avg:36.61ms
step:801/2160 train_time:29347ms step_avg:36.64ms
step:802/2160 train_time:29405ms step_avg:36.67ms
step:803/2160 train_time:29466ms step_avg:36.70ms
step:804/2160 train_time:29526ms step_avg:36.72ms
step:805/2160 train_time:29588ms step_avg:36.75ms
step:806/2160 train_time:29647ms step_avg:36.78ms
step:807/2160 train_time:29708ms step_avg:36.81ms
step:808/2160 train_time:29767ms step_avg:36.84ms
step:809/2160 train_time:29828ms step_avg:36.87ms
step:810/2160 train_time:29887ms step_avg:36.90ms
step:811/2160 train_time:29948ms step_avg:36.93ms
step:812/2160 train_time:30006ms step_avg:36.95ms
step:813/2160 train_time:30066ms step_avg:36.98ms
step:814/2160 train_time:30125ms step_avg:37.01ms
step:815/2160 train_time:30186ms step_avg:37.04ms
step:816/2160 train_time:30244ms step_avg:37.06ms
step:817/2160 train_time:30305ms step_avg:37.09ms
step:818/2160 train_time:30364ms step_avg:37.12ms
step:819/2160 train_time:30425ms step_avg:37.15ms
step:820/2160 train_time:30483ms step_avg:37.17ms
step:821/2160 train_time:30544ms step_avg:37.20ms
step:822/2160 train_time:30603ms step_avg:37.23ms
step:823/2160 train_time:30664ms step_avg:37.26ms
step:824/2160 train_time:30723ms step_avg:37.29ms
step:825/2160 train_time:30784ms step_avg:37.31ms
step:826/2160 train_time:30843ms step_avg:37.34ms
step:827/2160 train_time:30904ms step_avg:37.37ms
step:828/2160 train_time:30963ms step_avg:37.39ms
step:829/2160 train_time:31023ms step_avg:37.42ms
step:830/2160 train_time:31081ms step_avg:37.45ms
step:831/2160 train_time:31142ms step_avg:37.47ms
step:832/2160 train_time:31200ms step_avg:37.50ms
step:833/2160 train_time:31261ms step_avg:37.53ms
step:834/2160 train_time:31319ms step_avg:37.55ms
step:835/2160 train_time:31379ms step_avg:37.58ms
step:836/2160 train_time:31438ms step_avg:37.61ms
step:837/2160 train_time:31499ms step_avg:37.63ms
step:838/2160 train_time:31558ms step_avg:37.66ms
step:839/2160 train_time:31619ms step_avg:37.69ms
step:840/2160 train_time:31678ms step_avg:37.71ms
step:841/2160 train_time:31738ms step_avg:37.74ms
step:842/2160 train_time:31797ms step_avg:37.76ms
step:843/2160 train_time:31858ms step_avg:37.79ms
step:844/2160 train_time:31917ms step_avg:37.82ms
step:845/2160 train_time:31978ms step_avg:37.84ms
step:846/2160 train_time:32036ms step_avg:37.87ms
step:847/2160 train_time:32097ms step_avg:37.89ms
step:848/2160 train_time:32156ms step_avg:37.92ms
step:849/2160 train_time:32216ms step_avg:37.95ms
step:850/2160 train_time:32275ms step_avg:37.97ms
step:851/2160 train_time:32336ms step_avg:38.00ms
step:852/2160 train_time:32394ms step_avg:38.02ms
step:853/2160 train_time:32455ms step_avg:38.05ms
step:854/2160 train_time:32514ms step_avg:38.07ms
step:855/2160 train_time:32574ms step_avg:38.10ms
step:856/2160 train_time:32633ms step_avg:38.12ms
step:857/2160 train_time:32694ms step_avg:38.15ms
step:858/2160 train_time:32753ms step_avg:38.17ms
step:859/2160 train_time:32814ms step_avg:38.20ms
step:860/2160 train_time:32872ms step_avg:38.22ms
step:861/2160 train_time:32933ms step_avg:38.25ms
step:862/2160 train_time:32991ms step_avg:38.27ms
step:863/2160 train_time:33051ms step_avg:38.30ms
step:864/2160 train_time:33110ms step_avg:38.32ms
step:865/2160 train_time:33171ms step_avg:38.35ms
step:866/2160 train_time:33230ms step_avg:38.37ms
step:867/2160 train_time:33291ms step_avg:38.40ms
step:868/2160 train_time:33350ms step_avg:38.42ms
step:869/2160 train_time:33411ms step_avg:38.45ms
step:870/2160 train_time:33469ms step_avg:38.47ms
step:871/2160 train_time:33531ms step_avg:38.50ms
step:872/2160 train_time:33590ms step_avg:38.52ms
step:873/2160 train_time:33650ms step_avg:38.55ms
step:874/2160 train_time:33709ms step_avg:38.57ms
step:875/2160 train_time:33769ms step_avg:38.59ms
step:876/2160 train_time:33829ms step_avg:38.62ms
step:877/2160 train_time:33890ms step_avg:38.64ms
step:878/2160 train_time:33948ms step_avg:38.67ms
step:879/2160 train_time:34008ms step_avg:38.69ms
step:880/2160 train_time:34067ms step_avg:38.71ms
step:881/2160 train_time:34127ms step_avg:38.74ms
step:882/2160 train_time:34186ms step_avg:38.76ms
step:883/2160 train_time:34246ms step_avg:38.78ms
step:884/2160 train_time:34305ms step_avg:38.81ms
step:885/2160 train_time:34365ms step_avg:38.83ms
step:886/2160 train_time:34424ms step_avg:38.85ms
step:887/2160 train_time:34486ms step_avg:38.88ms
step:888/2160 train_time:34545ms step_avg:38.90ms
step:889/2160 train_time:34606ms step_avg:38.93ms
step:890/2160 train_time:34665ms step_avg:38.95ms
step:891/2160 train_time:34726ms step_avg:38.97ms
step:892/2160 train_time:34785ms step_avg:39.00ms
step:893/2160 train_time:34846ms step_avg:39.02ms
step:894/2160 train_time:34904ms step_avg:39.04ms
step:895/2160 train_time:34965ms step_avg:39.07ms
step:896/2160 train_time:35025ms step_avg:39.09ms
step:897/2160 train_time:35085ms step_avg:39.11ms
step:898/2160 train_time:35144ms step_avg:39.14ms
step:899/2160 train_time:35204ms step_avg:39.16ms
step:900/2160 train_time:35263ms step_avg:39.18ms
step:901/2160 train_time:35323ms step_avg:39.20ms
step:902/2160 train_time:35383ms step_avg:39.23ms
step:903/2160 train_time:35444ms step_avg:39.25ms
step:904/2160 train_time:35502ms step_avg:39.27ms
step:905/2160 train_time:35564ms step_avg:39.30ms
step:906/2160 train_time:35623ms step_avg:39.32ms
step:907/2160 train_time:35684ms step_avg:39.34ms
step:908/2160 train_time:35743ms step_avg:39.36ms
step:909/2160 train_time:35803ms step_avg:39.39ms
step:910/2160 train_time:35862ms step_avg:39.41ms
step:911/2160 train_time:35923ms step_avg:39.43ms
step:912/2160 train_time:35981ms step_avg:39.45ms
step:913/2160 train_time:36042ms step_avg:39.48ms
step:914/2160 train_time:36100ms step_avg:39.50ms
step:915/2160 train_time:36161ms step_avg:39.52ms
step:916/2160 train_time:36219ms step_avg:39.54ms
step:917/2160 train_time:36280ms step_avg:39.56ms
step:918/2160 train_time:36339ms step_avg:39.59ms
step:919/2160 train_time:36400ms step_avg:39.61ms
step:920/2160 train_time:36459ms step_avg:39.63ms
step:921/2160 train_time:36520ms step_avg:39.65ms
step:922/2160 train_time:36578ms step_avg:39.67ms
step:923/2160 train_time:36639ms step_avg:39.70ms
step:924/2160 train_time:36698ms step_avg:39.72ms
step:925/2160 train_time:36758ms step_avg:39.74ms
step:926/2160 train_time:36817ms step_avg:39.76ms
step:927/2160 train_time:36877ms step_avg:39.78ms
step:928/2160 train_time:36935ms step_avg:39.80ms
step:929/2160 train_time:36996ms step_avg:39.82ms
step:930/2160 train_time:37055ms step_avg:39.84ms
step:931/2160 train_time:37115ms step_avg:39.87ms
step:932/2160 train_time:37174ms step_avg:39.89ms
step:933/2160 train_time:37234ms step_avg:39.91ms
step:934/2160 train_time:37293ms step_avg:39.93ms
step:935/2160 train_time:37354ms step_avg:39.95ms
step:936/2160 train_time:37413ms step_avg:39.97ms
step:937/2160 train_time:37473ms step_avg:39.99ms
step:938/2160 train_time:37532ms step_avg:40.01ms
step:939/2160 train_time:37592ms step_avg:40.03ms
step:940/2160 train_time:37651ms step_avg:40.05ms
step:941/2160 train_time:37711ms step_avg:40.08ms
step:942/2160 train_time:37770ms step_avg:40.10ms
step:943/2160 train_time:37831ms step_avg:40.12ms
step:944/2160 train_time:37889ms step_avg:40.14ms
step:945/2160 train_time:37950ms step_avg:40.16ms
step:946/2160 train_time:38008ms step_avg:40.18ms
step:947/2160 train_time:38069ms step_avg:40.20ms
step:948/2160 train_time:38128ms step_avg:40.22ms
step:949/2160 train_time:38189ms step_avg:40.24ms
step:950/2160 train_time:38248ms step_avg:40.26ms
step:951/2160 train_time:38308ms step_avg:40.28ms
step:952/2160 train_time:38367ms step_avg:40.30ms
step:953/2160 train_time:38428ms step_avg:40.32ms
step:954/2160 train_time:38487ms step_avg:40.34ms
step:955/2160 train_time:38547ms step_avg:40.36ms
step:956/2160 train_time:38606ms step_avg:40.38ms
step:957/2160 train_time:38667ms step_avg:40.40ms
step:958/2160 train_time:38726ms step_avg:40.42ms
step:959/2160 train_time:38787ms step_avg:40.45ms
step:960/2160 train_time:38846ms step_avg:40.46ms
step:961/2160 train_time:38906ms step_avg:40.48ms
step:962/2160 train_time:38965ms step_avg:40.50ms
step:963/2160 train_time:39026ms step_avg:40.52ms
step:964/2160 train_time:39084ms step_avg:40.54ms
step:965/2160 train_time:39145ms step_avg:40.56ms
step:966/2160 train_time:39204ms step_avg:40.58ms
step:967/2160 train_time:39264ms step_avg:40.60ms
step:968/2160 train_time:39323ms step_avg:40.62ms
step:969/2160 train_time:39384ms step_avg:40.64ms
step:970/2160 train_time:39443ms step_avg:40.66ms
step:971/2160 train_time:39504ms step_avg:40.68ms
step:972/2160 train_time:39563ms step_avg:40.70ms
step:973/2160 train_time:39624ms step_avg:40.72ms
step:974/2160 train_time:39683ms step_avg:40.74ms
step:975/2160 train_time:39744ms step_avg:40.76ms
step:976/2160 train_time:39802ms step_avg:40.78ms
step:977/2160 train_time:39863ms step_avg:40.80ms
step:978/2160 train_time:39922ms step_avg:40.82ms
step:979/2160 train_time:39983ms step_avg:40.84ms
step:980/2160 train_time:40042ms step_avg:40.86ms
step:981/2160 train_time:40103ms step_avg:40.88ms
step:982/2160 train_time:40162ms step_avg:40.90ms
step:983/2160 train_time:40223ms step_avg:40.92ms
step:984/2160 train_time:40282ms step_avg:40.94ms
step:985/2160 train_time:40343ms step_avg:40.96ms
step:986/2160 train_time:40402ms step_avg:40.98ms
step:987/2160 train_time:40463ms step_avg:41.00ms
step:988/2160 train_time:40522ms step_avg:41.01ms
step:989/2160 train_time:40583ms step_avg:41.03ms
step:990/2160 train_time:40642ms step_avg:41.05ms
step:991/2160 train_time:40703ms step_avg:41.07ms
step:992/2160 train_time:40762ms step_avg:41.09ms
step:993/2160 train_time:40823ms step_avg:41.11ms
step:994/2160 train_time:40881ms step_avg:41.13ms
step:995/2160 train_time:40942ms step_avg:41.15ms
step:996/2160 train_time:41001ms step_avg:41.17ms
step:997/2160 train_time:41062ms step_avg:41.19ms
step:998/2160 train_time:41121ms step_avg:41.20ms
step:999/2160 train_time:41182ms step_avg:41.22ms
step:1000/2160 train_time:41241ms step_avg:41.24ms
step:1000/2160 val_loss:3.6843 train_time:41303ms step_avg:41.30ms
step:1001/2160 train_time:41323ms step_avg:41.28ms
step:1002/2160 train_time:41363ms step_avg:41.28ms
step:1003/2160 train_time:41426ms step_avg:41.30ms
step:1004/2160 train_time:41491ms step_avg:41.33ms
step:1005/2160 train_time:41552ms step_avg:41.34ms
step:1006/2160 train_time:41610ms step_avg:41.36ms
step:1007/2160 train_time:41670ms step_avg:41.38ms
step:1008/2160 train_time:41728ms step_avg:41.40ms
step:1009/2160 train_time:41788ms step_avg:41.42ms
step:1010/2160 train_time:41846ms step_avg:41.43ms
step:1011/2160 train_time:41906ms step_avg:41.45ms
step:1012/2160 train_time:41964ms step_avg:41.47ms
step:1013/2160 train_time:42024ms step_avg:41.48ms
step:1014/2160 train_time:42082ms step_avg:41.50ms
step:1015/2160 train_time:42141ms step_avg:41.52ms
step:1016/2160 train_time:42199ms step_avg:41.53ms
step:1017/2160 train_time:42260ms step_avg:41.55ms
step:1018/2160 train_time:42320ms step_avg:41.57ms
step:1019/2160 train_time:42382ms step_avg:41.59ms
step:1020/2160 train_time:42442ms step_avg:41.61ms
step:1021/2160 train_time:42504ms step_avg:41.63ms
step:1022/2160 train_time:42563ms step_avg:41.65ms
step:1023/2160 train_time:42624ms step_avg:41.67ms
step:1024/2160 train_time:42683ms step_avg:41.68ms
step:1025/2160 train_time:42743ms step_avg:41.70ms
step:1026/2160 train_time:42801ms step_avg:41.72ms
step:1027/2160 train_time:42862ms step_avg:41.73ms
step:1028/2160 train_time:42920ms step_avg:41.75ms
step:1029/2160 train_time:42980ms step_avg:41.77ms
step:1030/2160 train_time:43038ms step_avg:41.78ms
step:1031/2160 train_time:43098ms step_avg:41.80ms
step:1032/2160 train_time:43156ms step_avg:41.82ms
step:1033/2160 train_time:43216ms step_avg:41.84ms
step:1034/2160 train_time:43276ms step_avg:41.85ms
step:1035/2160 train_time:43337ms step_avg:41.87ms
step:1036/2160 train_time:43396ms step_avg:41.89ms
step:1037/2160 train_time:43458ms step_avg:41.91ms
step:1038/2160 train_time:43517ms step_avg:41.92ms
step:1039/2160 train_time:43579ms step_avg:41.94ms
step:1040/2160 train_time:43638ms step_avg:41.96ms
step:1041/2160 train_time:43699ms step_avg:41.98ms
step:1042/2160 train_time:43757ms step_avg:41.99ms
step:1043/2160 train_time:43818ms step_avg:42.01ms
step:1044/2160 train_time:43877ms step_avg:42.03ms
step:1045/2160 train_time:43937ms step_avg:42.04ms
step:1046/2160 train_time:43996ms step_avg:42.06ms
step:1047/2160 train_time:44055ms step_avg:42.08ms
step:1048/2160 train_time:44114ms step_avg:42.09ms
step:1049/2160 train_time:44174ms step_avg:42.11ms
step:1050/2160 train_time:44233ms step_avg:42.13ms
step:1051/2160 train_time:44294ms step_avg:42.14ms
step:1052/2160 train_time:44353ms step_avg:42.16ms
step:1053/2160 train_time:44414ms step_avg:42.18ms
step:1054/2160 train_time:44474ms step_avg:42.20ms
step:1055/2160 train_time:44535ms step_avg:42.21ms
step:1056/2160 train_time:44595ms step_avg:42.23ms
step:1057/2160 train_time:44656ms step_avg:42.25ms
step:1058/2160 train_time:44714ms step_avg:42.26ms
step:1059/2160 train_time:44775ms step_avg:42.28ms
step:1060/2160 train_time:44834ms step_avg:42.30ms
step:1061/2160 train_time:44895ms step_avg:42.31ms
step:1062/2160 train_time:44953ms step_avg:42.33ms
step:1063/2160 train_time:45014ms step_avg:42.35ms
step:1064/2160 train_time:45072ms step_avg:42.36ms
step:1065/2160 train_time:45132ms step_avg:42.38ms
step:1066/2160 train_time:45191ms step_avg:42.39ms
step:1067/2160 train_time:45251ms step_avg:42.41ms
step:1068/2160 train_time:45310ms step_avg:42.43ms
step:1069/2160 train_time:45371ms step_avg:42.44ms
step:1070/2160 train_time:45430ms step_avg:42.46ms
step:1071/2160 train_time:45491ms step_avg:42.48ms
step:1072/2160 train_time:45550ms step_avg:42.49ms
step:1073/2160 train_time:45611ms step_avg:42.51ms
step:1074/2160 train_time:45671ms step_avg:42.52ms
step:1075/2160 train_time:45732ms step_avg:42.54ms
step:1076/2160 train_time:45791ms step_avg:42.56ms
step:1077/2160 train_time:45851ms step_avg:42.57ms
step:1078/2160 train_time:45910ms step_avg:42.59ms
step:1079/2160 train_time:45970ms step_avg:42.60ms
step:1080/2160 train_time:46028ms step_avg:42.62ms
step:1081/2160 train_time:46089ms step_avg:42.64ms
step:1082/2160 train_time:46147ms step_avg:42.65ms
step:1083/2160 train_time:46207ms step_avg:42.67ms
step:1084/2160 train_time:46266ms step_avg:42.68ms
step:1085/2160 train_time:46327ms step_avg:42.70ms
step:1086/2160 train_time:46385ms step_avg:42.71ms
step:1087/2160 train_time:46446ms step_avg:42.73ms
step:1088/2160 train_time:46505ms step_avg:42.74ms
step:1089/2160 train_time:46566ms step_avg:42.76ms
step:1090/2160 train_time:46626ms step_avg:42.78ms
step:1091/2160 train_time:46687ms step_avg:42.79ms
step:1092/2160 train_time:46746ms step_avg:42.81ms
step:1093/2160 train_time:46808ms step_avg:42.82ms
step:1094/2160 train_time:46866ms step_avg:42.84ms
step:1095/2160 train_time:46927ms step_avg:42.86ms
step:1096/2160 train_time:46986ms step_avg:42.87ms
step:1097/2160 train_time:47046ms step_avg:42.89ms
step:1098/2160 train_time:47105ms step_avg:42.90ms
step:1099/2160 train_time:47165ms step_avg:42.92ms
step:1100/2160 train_time:47224ms step_avg:42.93ms
step:1101/2160 train_time:47285ms step_avg:42.95ms
step:1102/2160 train_time:47344ms step_avg:42.96ms
step:1103/2160 train_time:47405ms step_avg:42.98ms
step:1104/2160 train_time:47463ms step_avg:42.99ms
step:1105/2160 train_time:47524ms step_avg:43.01ms
step:1106/2160 train_time:47583ms step_avg:43.02ms
step:1107/2160 train_time:47644ms step_avg:43.04ms
step:1108/2160 train_time:47703ms step_avg:43.05ms
step:1109/2160 train_time:47764ms step_avg:43.07ms
step:1110/2160 train_time:47823ms step_avg:43.08ms
step:1111/2160 train_time:47884ms step_avg:43.10ms
step:1112/2160 train_time:47942ms step_avg:43.11ms
step:1113/2160 train_time:48003ms step_avg:43.13ms
step:1114/2160 train_time:48062ms step_avg:43.14ms
step:1115/2160 train_time:48122ms step_avg:43.16ms
step:1116/2160 train_time:48181ms step_avg:43.17ms
step:1117/2160 train_time:48241ms step_avg:43.19ms
step:1118/2160 train_time:48299ms step_avg:43.20ms
step:1119/2160 train_time:48359ms step_avg:43.22ms
step:1120/2160 train_time:48418ms step_avg:43.23ms
step:1121/2160 train_time:48478ms step_avg:43.25ms
step:1122/2160 train_time:48537ms step_avg:43.26ms
step:1123/2160 train_time:48598ms step_avg:43.28ms
step:1124/2160 train_time:48656ms step_avg:43.29ms
step:1125/2160 train_time:48717ms step_avg:43.30ms
step:1126/2160 train_time:48776ms step_avg:43.32ms
step:1127/2160 train_time:48837ms step_avg:43.33ms
step:1128/2160 train_time:48896ms step_avg:43.35ms
step:1129/2160 train_time:48957ms step_avg:43.36ms
step:1130/2160 train_time:49016ms step_avg:43.38ms
step:1131/2160 train_time:49076ms step_avg:43.39ms
step:1132/2160 train_time:49135ms step_avg:43.41ms
step:1133/2160 train_time:49196ms step_avg:43.42ms
step:1134/2160 train_time:49254ms step_avg:43.43ms
step:1135/2160 train_time:49314ms step_avg:43.45ms
step:1136/2160 train_time:49373ms step_avg:43.46ms
step:1137/2160 train_time:49434ms step_avg:43.48ms
step:1138/2160 train_time:49493ms step_avg:43.49ms
step:1139/2160 train_time:49553ms step_avg:43.51ms
step:1140/2160 train_time:49612ms step_avg:43.52ms
step:1141/2160 train_time:49673ms step_avg:43.53ms
step:1142/2160 train_time:49732ms step_avg:43.55ms
step:1143/2160 train_time:49793ms step_avg:43.56ms
step:1144/2160 train_time:49852ms step_avg:43.58ms
step:1145/2160 train_time:49913ms step_avg:43.59ms
step:1146/2160 train_time:49973ms step_avg:43.61ms
step:1147/2160 train_time:50033ms step_avg:43.62ms
step:1148/2160 train_time:50092ms step_avg:43.63ms
step:1149/2160 train_time:50153ms step_avg:43.65ms
step:1150/2160 train_time:50211ms step_avg:43.66ms
step:1151/2160 train_time:50272ms step_avg:43.68ms
step:1152/2160 train_time:50330ms step_avg:43.69ms
step:1153/2160 train_time:50391ms step_avg:43.70ms
step:1154/2160 train_time:50449ms step_avg:43.72ms
step:1155/2160 train_time:50510ms step_avg:43.73ms
step:1156/2160 train_time:50569ms step_avg:43.74ms
step:1157/2160 train_time:50631ms step_avg:43.76ms
step:1158/2160 train_time:50689ms step_avg:43.77ms
step:1159/2160 train_time:50750ms step_avg:43.79ms
step:1160/2160 train_time:50809ms step_avg:43.80ms
step:1161/2160 train_time:50871ms step_avg:43.82ms
step:1162/2160 train_time:50930ms step_avg:43.83ms
step:1163/2160 train_time:50991ms step_avg:43.84ms
step:1164/2160 train_time:51049ms step_avg:43.86ms
step:1165/2160 train_time:51110ms step_avg:43.87ms
step:1166/2160 train_time:51169ms step_avg:43.88ms
step:1167/2160 train_time:51229ms step_avg:43.90ms
step:1168/2160 train_time:51288ms step_avg:43.91ms
step:1169/2160 train_time:51349ms step_avg:43.93ms
step:1170/2160 train_time:51407ms step_avg:43.94ms
step:1171/2160 train_time:51467ms step_avg:43.95ms
step:1172/2160 train_time:51526ms step_avg:43.96ms
step:1173/2160 train_time:51586ms step_avg:43.98ms
step:1174/2160 train_time:51645ms step_avg:43.99ms
step:1175/2160 train_time:51706ms step_avg:44.00ms
step:1176/2160 train_time:51764ms step_avg:44.02ms
step:1177/2160 train_time:51825ms step_avg:44.03ms
step:1178/2160 train_time:51884ms step_avg:44.04ms
step:1179/2160 train_time:51945ms step_avg:44.06ms
step:1180/2160 train_time:52005ms step_avg:44.07ms
step:1181/2160 train_time:52066ms step_avg:44.09ms
step:1182/2160 train_time:52125ms step_avg:44.10ms
step:1183/2160 train_time:52186ms step_avg:44.11ms
step:1184/2160 train_time:52245ms step_avg:44.13ms
step:1185/2160 train_time:52306ms step_avg:44.14ms
step:1186/2160 train_time:52365ms step_avg:44.15ms
step:1187/2160 train_time:52426ms step_avg:44.17ms
step:1188/2160 train_time:52484ms step_avg:44.18ms
step:1189/2160 train_time:52544ms step_avg:44.19ms
step:1190/2160 train_time:52603ms step_avg:44.20ms
step:1191/2160 train_time:52664ms step_avg:44.22ms
step:1192/2160 train_time:52723ms step_avg:44.23ms
step:1193/2160 train_time:52784ms step_avg:44.24ms
step:1194/2160 train_time:52843ms step_avg:44.26ms
step:1195/2160 train_time:52903ms step_avg:44.27ms
step:1196/2160 train_time:52962ms step_avg:44.28ms
step:1197/2160 train_time:53023ms step_avg:44.30ms
step:1198/2160 train_time:53082ms step_avg:44.31ms
step:1199/2160 train_time:53143ms step_avg:44.32ms
step:1200/2160 train_time:53201ms step_avg:44.33ms
step:1201/2160 train_time:53262ms step_avg:44.35ms
step:1202/2160 train_time:53321ms step_avg:44.36ms
step:1203/2160 train_time:53381ms step_avg:44.37ms
step:1204/2160 train_time:53440ms step_avg:44.39ms
step:1205/2160 train_time:53501ms step_avg:44.40ms
step:1206/2160 train_time:53559ms step_avg:44.41ms
step:1207/2160 train_time:53620ms step_avg:44.42ms
step:1208/2160 train_time:53680ms step_avg:44.44ms
step:1209/2160 train_time:53740ms step_avg:44.45ms
step:1210/2160 train_time:53799ms step_avg:44.46ms
step:1211/2160 train_time:53860ms step_avg:44.48ms
step:1212/2160 train_time:53918ms step_avg:44.49ms
step:1213/2160 train_time:53979ms step_avg:44.50ms
step:1214/2160 train_time:54037ms step_avg:44.51ms
step:1215/2160 train_time:54098ms step_avg:44.53ms
step:1216/2160 train_time:54156ms step_avg:44.54ms
step:1217/2160 train_time:54217ms step_avg:44.55ms
step:1218/2160 train_time:54277ms step_avg:44.56ms
step:1219/2160 train_time:54337ms step_avg:44.58ms
step:1220/2160 train_time:54396ms step_avg:44.59ms
step:1221/2160 train_time:54456ms step_avg:44.60ms
step:1222/2160 train_time:54515ms step_avg:44.61ms
step:1223/2160 train_time:54576ms step_avg:44.62ms
step:1224/2160 train_time:54635ms step_avg:44.64ms
step:1225/2160 train_time:54695ms step_avg:44.65ms
step:1226/2160 train_time:54754ms step_avg:44.66ms
step:1227/2160 train_time:54815ms step_avg:44.67ms
step:1228/2160 train_time:54874ms step_avg:44.69ms
step:1229/2160 train_time:54935ms step_avg:44.70ms
step:1230/2160 train_time:54993ms step_avg:44.71ms
step:1231/2160 train_time:55054ms step_avg:44.72ms
step:1232/2160 train_time:55113ms step_avg:44.73ms
step:1233/2160 train_time:55174ms step_avg:44.75ms
step:1234/2160 train_time:55233ms step_avg:44.76ms
step:1235/2160 train_time:55293ms step_avg:44.77ms
step:1236/2160 train_time:55352ms step_avg:44.78ms
step:1237/2160 train_time:55413ms step_avg:44.80ms
step:1238/2160 train_time:55471ms step_avg:44.81ms
step:1239/2160 train_time:55532ms step_avg:44.82ms
step:1240/2160 train_time:55591ms step_avg:44.83ms
step:1241/2160 train_time:55652ms step_avg:44.84ms
step:1242/2160 train_time:55711ms step_avg:44.86ms
step:1243/2160 train_time:55772ms step_avg:44.87ms
step:1244/2160 train_time:55830ms step_avg:44.88ms
step:1245/2160 train_time:55891ms step_avg:44.89ms
step:1246/2160 train_time:55950ms step_avg:44.90ms
step:1247/2160 train_time:56010ms step_avg:44.92ms
step:1248/2160 train_time:56069ms step_avg:44.93ms
step:1249/2160 train_time:56130ms step_avg:44.94ms
step:1250/2160 train_time:56189ms step_avg:44.95ms
step:1250/2160 val_loss:3.5707 train_time:56251ms step_avg:45.00ms
step:1251/2160 train_time:56271ms step_avg:44.98ms
step:1252/2160 train_time:56311ms step_avg:44.98ms
step:1253/2160 train_time:56376ms step_avg:44.99ms
step:1254/2160 train_time:56440ms step_avg:45.01ms
step:1255/2160 train_time:56501ms step_avg:45.02ms
step:1256/2160 train_time:56560ms step_avg:45.03ms
step:1257/2160 train_time:56620ms step_avg:45.04ms
step:1258/2160 train_time:56678ms step_avg:45.05ms
step:1259/2160 train_time:56739ms step_avg:45.07ms
step:1260/2160 train_time:56797ms step_avg:45.08ms
step:1261/2160 train_time:56857ms step_avg:45.09ms
step:1262/2160 train_time:56915ms step_avg:45.10ms
step:1263/2160 train_time:56975ms step_avg:45.11ms
step:1264/2160 train_time:57034ms step_avg:45.12ms
step:1265/2160 train_time:57094ms step_avg:45.13ms
step:1266/2160 train_time:57152ms step_avg:45.14ms
step:1267/2160 train_time:57213ms step_avg:45.16ms
step:1268/2160 train_time:57273ms step_avg:45.17ms
step:1269/2160 train_time:57335ms step_avg:45.18ms
step:1270/2160 train_time:57395ms step_avg:45.19ms
step:1271/2160 train_time:57457ms step_avg:45.21ms
step:1272/2160 train_time:57518ms step_avg:45.22ms
step:1273/2160 train_time:57579ms step_avg:45.23ms
step:1274/2160 train_time:57637ms step_avg:45.24ms
step:1275/2160 train_time:57698ms step_avg:45.25ms
step:1276/2160 train_time:57755ms step_avg:45.26ms
step:1277/2160 train_time:57815ms step_avg:45.27ms
step:1278/2160 train_time:57874ms step_avg:45.28ms
step:1279/2160 train_time:57933ms step_avg:45.30ms
step:1280/2160 train_time:57992ms step_avg:45.31ms
step:1281/2160 train_time:58052ms step_avg:45.32ms
step:1282/2160 train_time:58110ms step_avg:45.33ms
step:1283/2160 train_time:58170ms step_avg:45.34ms
step:1284/2160 train_time:58229ms step_avg:45.35ms
step:1285/2160 train_time:58290ms step_avg:45.36ms
step:1286/2160 train_time:58350ms step_avg:45.37ms
step:1287/2160 train_time:58411ms step_avg:45.39ms
step:1288/2160 train_time:58470ms step_avg:45.40ms
step:1289/2160 train_time:58532ms step_avg:45.41ms
step:1290/2160 train_time:58591ms step_avg:45.42ms
step:1291/2160 train_time:58652ms step_avg:45.43ms
step:1292/2160 train_time:58711ms step_avg:45.44ms
step:1293/2160 train_time:58771ms step_avg:45.45ms
step:1294/2160 train_time:58829ms step_avg:45.46ms
step:1295/2160 train_time:58890ms step_avg:45.47ms
step:1296/2160 train_time:58949ms step_avg:45.49ms
step:1297/2160 train_time:59009ms step_avg:45.50ms
step:1298/2160 train_time:59067ms step_avg:45.51ms
step:1299/2160 train_time:59127ms step_avg:45.52ms
step:1300/2160 train_time:59185ms step_avg:45.53ms
step:1301/2160 train_time:59246ms step_avg:45.54ms
step:1302/2160 train_time:59304ms step_avg:45.55ms
step:1303/2160 train_time:59366ms step_avg:45.56ms
step:1304/2160 train_time:59424ms step_avg:45.57ms
step:1305/2160 train_time:59485ms step_avg:45.58ms
step:1306/2160 train_time:59545ms step_avg:45.59ms
step:1307/2160 train_time:59607ms step_avg:45.61ms
step:1308/2160 train_time:59665ms step_avg:45.62ms
step:1309/2160 train_time:59726ms step_avg:45.63ms
step:1310/2160 train_time:59784ms step_avg:45.64ms
step:1311/2160 train_time:59845ms step_avg:45.65ms
step:1312/2160 train_time:59903ms step_avg:45.66ms
step:1313/2160 train_time:59963ms step_avg:45.67ms
step:1314/2160 train_time:60022ms step_avg:45.68ms
step:1315/2160 train_time:60082ms step_avg:45.69ms
step:1316/2160 train_time:60141ms step_avg:45.70ms
step:1317/2160 train_time:60202ms step_avg:45.71ms
step:1318/2160 train_time:60261ms step_avg:45.72ms
step:1319/2160 train_time:60322ms step_avg:45.73ms
step:1320/2160 train_time:60381ms step_avg:45.74ms
step:1321/2160 train_time:60442ms step_avg:45.75ms
step:1322/2160 train_time:60501ms step_avg:45.76ms
step:1323/2160 train_time:60563ms step_avg:45.78ms
step:1324/2160 train_time:60622ms step_avg:45.79ms
step:1325/2160 train_time:60682ms step_avg:45.80ms
step:1326/2160 train_time:60741ms step_avg:45.81ms
step:1327/2160 train_time:60802ms step_avg:45.82ms
step:1328/2160 train_time:60860ms step_avg:45.83ms
step:1329/2160 train_time:60921ms step_avg:45.84ms
step:1330/2160 train_time:60980ms step_avg:45.85ms
step:1331/2160 train_time:61040ms step_avg:45.86ms
step:1332/2160 train_time:61098ms step_avg:45.87ms
step:1333/2160 train_time:61159ms step_avg:45.88ms
step:1334/2160 train_time:61218ms step_avg:45.89ms
step:1335/2160 train_time:61279ms step_avg:45.90ms
step:1336/2160 train_time:61338ms step_avg:45.91ms
step:1337/2160 train_time:61399ms step_avg:45.92ms
step:1338/2160 train_time:61457ms step_avg:45.93ms
step:1339/2160 train_time:61519ms step_avg:45.94ms
step:1340/2160 train_time:61578ms step_avg:45.95ms
step:1341/2160 train_time:61639ms step_avg:45.96ms
step:1342/2160 train_time:61697ms step_avg:45.97ms
step:1343/2160 train_time:61758ms step_avg:45.98ms
step:1344/2160 train_time:61817ms step_avg:45.99ms
step:1345/2160 train_time:61878ms step_avg:46.01ms
step:1346/2160 train_time:61936ms step_avg:46.01ms
step:1347/2160 train_time:61996ms step_avg:46.03ms
step:1348/2160 train_time:62054ms step_avg:46.03ms
step:1349/2160 train_time:62115ms step_avg:46.05ms
step:1350/2160 train_time:62174ms step_avg:46.05ms
step:1351/2160 train_time:62234ms step_avg:46.07ms
step:1352/2160 train_time:62293ms step_avg:46.07ms
step:1353/2160 train_time:62354ms step_avg:46.09ms
step:1354/2160 train_time:62413ms step_avg:46.10ms
step:1355/2160 train_time:62475ms step_avg:46.11ms
step:1356/2160 train_time:62534ms step_avg:46.12ms
step:1357/2160 train_time:62595ms step_avg:46.13ms
step:1358/2160 train_time:62654ms step_avg:46.14ms
step:1359/2160 train_time:62715ms step_avg:46.15ms
step:1360/2160 train_time:62775ms step_avg:46.16ms
step:1361/2160 train_time:62835ms step_avg:46.17ms
step:1362/2160 train_time:62893ms step_avg:46.18ms
step:1363/2160 train_time:62954ms step_avg:46.19ms
step:1364/2160 train_time:63013ms step_avg:46.20ms
step:1365/2160 train_time:63074ms step_avg:46.21ms
step:1366/2160 train_time:63133ms step_avg:46.22ms
step:1367/2160 train_time:63193ms step_avg:46.23ms
step:1368/2160 train_time:63251ms step_avg:46.24ms
step:1369/2160 train_time:63312ms step_avg:46.25ms
step:1370/2160 train_time:63371ms step_avg:46.26ms
step:1371/2160 train_time:63432ms step_avg:46.27ms
step:1372/2160 train_time:63490ms step_avg:46.28ms
step:1373/2160 train_time:63552ms step_avg:46.29ms
step:1374/2160 train_time:63610ms step_avg:46.30ms
step:1375/2160 train_time:63671ms step_avg:46.31ms
step:1376/2160 train_time:63730ms step_avg:46.32ms
step:1377/2160 train_time:63791ms step_avg:46.33ms
step:1378/2160 train_time:63850ms step_avg:46.34ms
step:1379/2160 train_time:63911ms step_avg:46.35ms
step:1380/2160 train_time:63969ms step_avg:46.35ms
step:1381/2160 train_time:64030ms step_avg:46.37ms
step:1382/2160 train_time:64089ms step_avg:46.37ms
step:1383/2160 train_time:64150ms step_avg:46.38ms
step:1384/2160 train_time:64209ms step_avg:46.39ms
step:1385/2160 train_time:64269ms step_avg:46.40ms
step:1386/2160 train_time:64327ms step_avg:46.41ms
step:1387/2160 train_time:64388ms step_avg:46.42ms
step:1388/2160 train_time:64447ms step_avg:46.43ms
step:1389/2160 train_time:64508ms step_avg:46.44ms
step:1390/2160 train_time:64567ms step_avg:46.45ms
step:1391/2160 train_time:64627ms step_avg:46.46ms
step:1392/2160 train_time:64686ms step_avg:46.47ms
step:1393/2160 train_time:64747ms step_avg:46.48ms
step:1394/2160 train_time:64806ms step_avg:46.49ms
step:1395/2160 train_time:64867ms step_avg:46.50ms
step:1396/2160 train_time:64925ms step_avg:46.51ms
step:1397/2160 train_time:64986ms step_avg:46.52ms
step:1398/2160 train_time:65045ms step_avg:46.53ms
step:1399/2160 train_time:65105ms step_avg:46.54ms
step:1400/2160 train_time:65163ms step_avg:46.55ms
step:1401/2160 train_time:65224ms step_avg:46.56ms
step:1402/2160 train_time:65282ms step_avg:46.56ms
step:1403/2160 train_time:65342ms step_avg:46.57ms
step:1404/2160 train_time:65401ms step_avg:46.58ms
step:1405/2160 train_time:65462ms step_avg:46.59ms
step:1406/2160 train_time:65521ms step_avg:46.60ms
step:1407/2160 train_time:65581ms step_avg:46.61ms
step:1408/2160 train_time:65640ms step_avg:46.62ms
step:1409/2160 train_time:65701ms step_avg:46.63ms
step:1410/2160 train_time:65760ms step_avg:46.64ms
step:1411/2160 train_time:65820ms step_avg:46.65ms
step:1412/2160 train_time:65879ms step_avg:46.66ms
step:1413/2160 train_time:65940ms step_avg:46.67ms
step:1414/2160 train_time:65999ms step_avg:46.68ms
step:1415/2160 train_time:66059ms step_avg:46.68ms
step:1416/2160 train_time:66145ms step_avg:46.71ms
step:1417/2160 train_time:66233ms step_avg:46.74ms
step:1418/2160 train_time:66319ms step_avg:46.77ms
step:1419/2160 train_time:66407ms step_avg:46.80ms
step:1420/2160 train_time:66494ms step_avg:46.83ms
step:1421/2160 train_time:66582ms step_avg:46.86ms
step:1422/2160 train_time:66669ms step_avg:46.88ms
step:1423/2160 train_time:66757ms step_avg:46.91ms
step:1424/2160 train_time:66844ms step_avg:46.94ms
step:1425/2160 train_time:66933ms step_avg:46.97ms
step:1426/2160 train_time:67019ms step_avg:47.00ms
step:1427/2160 train_time:67108ms step_avg:47.03ms
step:1428/2160 train_time:67194ms step_avg:47.05ms
step:1429/2160 train_time:67282ms step_avg:47.08ms
step:1430/2160 train_time:67368ms step_avg:47.11ms
step:1431/2160 train_time:67456ms step_avg:47.14ms
step:1432/2160 train_time:67543ms step_avg:47.17ms
step:1433/2160 train_time:67631ms step_avg:47.20ms
step:1434/2160 train_time:67718ms step_avg:47.22ms
step:1435/2160 train_time:67807ms step_avg:47.25ms
step:1436/2160 train_time:67893ms step_avg:47.28ms
step:1437/2160 train_time:67982ms step_avg:47.31ms
step:1438/2160 train_time:68069ms step_avg:47.34ms
step:1439/2160 train_time:68157ms step_avg:47.36ms
step:1440/2160 train_time:68243ms step_avg:47.39ms
step:1441/2160 train_time:68331ms step_avg:47.42ms
step:1442/2160 train_time:68417ms step_avg:47.45ms
step:1443/2160 train_time:68506ms step_avg:47.47ms
step:1444/2160 train_time:68593ms step_avg:47.50ms
step:1445/2160 train_time:68681ms step_avg:47.53ms
step:1446/2160 train_time:68767ms step_avg:47.56ms
step:1447/2160 train_time:68856ms step_avg:47.59ms
step:1448/2160 train_time:68943ms step_avg:47.61ms
step:1449/2160 train_time:69031ms step_avg:47.64ms
step:1450/2160 train_time:69118ms step_avg:47.67ms
step:1451/2160 train_time:69207ms step_avg:47.70ms
step:1452/2160 train_time:69292ms step_avg:47.72ms
step:1453/2160 train_time:69380ms step_avg:47.75ms
step:1454/2160 train_time:69467ms step_avg:47.78ms
step:1455/2160 train_time:69555ms step_avg:47.80ms
step:1456/2160 train_time:69642ms step_avg:47.83ms
step:1457/2160 train_time:69730ms step_avg:47.86ms
step:1458/2160 train_time:69816ms step_avg:47.88ms
step:1459/2160 train_time:69905ms step_avg:47.91ms
step:1460/2160 train_time:69991ms step_avg:47.94ms
step:1461/2160 train_time:70080ms step_avg:47.97ms
step:1462/2160 train_time:70166ms step_avg:47.99ms
step:1463/2160 train_time:70255ms step_avg:48.02ms
step:1464/2160 train_time:70340ms step_avg:48.05ms
step:1465/2160 train_time:70429ms step_avg:48.07ms
step:1466/2160 train_time:70515ms step_avg:48.10ms
step:1467/2160 train_time:70604ms step_avg:48.13ms
step:1468/2160 train_time:70690ms step_avg:48.15ms
step:1469/2160 train_time:70778ms step_avg:48.18ms
step:1470/2160 train_time:70865ms step_avg:48.21ms
step:1471/2160 train_time:70955ms step_avg:48.24ms
step:1472/2160 train_time:71041ms step_avg:48.26ms
step:1473/2160 train_time:71129ms step_avg:48.29ms
step:1474/2160 train_time:71216ms step_avg:48.31ms
step:1475/2160 train_time:71305ms step_avg:48.34ms
step:1476/2160 train_time:71391ms step_avg:48.37ms
step:1477/2160 train_time:71478ms step_avg:48.39ms
step:1478/2160 train_time:71564ms step_avg:48.42ms
step:1479/2160 train_time:71653ms step_avg:48.45ms
step:1480/2160 train_time:71740ms step_avg:48.47ms
step:1481/2160 train_time:71828ms step_avg:48.50ms
step:1482/2160 train_time:71915ms step_avg:48.53ms
step:1483/2160 train_time:72003ms step_avg:48.55ms
step:1484/2160 train_time:72089ms step_avg:48.58ms
step:1485/2160 train_time:72177ms step_avg:48.60ms
step:1486/2160 train_time:72263ms step_avg:48.63ms
step:1487/2160 train_time:72352ms step_avg:48.66ms
step:1488/2160 train_time:72438ms step_avg:48.68ms
step:1489/2160 train_time:72526ms step_avg:48.71ms
step:1490/2160 train_time:72613ms step_avg:48.73ms
step:1491/2160 train_time:72702ms step_avg:48.76ms
step:1492/2160 train_time:72788ms step_avg:48.79ms
step:1493/2160 train_time:72877ms step_avg:48.81ms
step:1494/2160 train_time:72962ms step_avg:48.84ms
step:1495/2160 train_time:73051ms step_avg:48.86ms
step:1496/2160 train_time:73137ms step_avg:48.89ms
step:1497/2160 train_time:73224ms step_avg:48.91ms
step:1498/2160 train_time:73311ms step_avg:48.94ms
step:1499/2160 train_time:73398ms step_avg:48.96ms
step:1500/2160 train_time:73486ms step_avg:48.99ms
step:1500/2160 val_loss:3.4688 train_time:73575ms step_avg:49.05ms
step:1501/2160 train_time:73595ms step_avg:49.03ms
step:1502/2160 train_time:73663ms step_avg:49.04ms
step:1503/2160 train_time:73757ms step_avg:49.07ms
step:1504/2160 train_time:73845ms step_avg:49.10ms
step:1505/2160 train_time:73933ms step_avg:49.12ms
step:1506/2160 train_time:74018ms step_avg:49.15ms
step:1507/2160 train_time:74106ms step_avg:49.17ms
step:1508/2160 train_time:74191ms step_avg:49.20ms
step:1509/2160 train_time:74278ms step_avg:49.22ms
step:1510/2160 train_time:74364ms step_avg:49.25ms
step:1511/2160 train_time:74453ms step_avg:49.27ms
step:1512/2160 train_time:74540ms step_avg:49.30ms
step:1513/2160 train_time:74630ms step_avg:49.33ms
step:1514/2160 train_time:74719ms step_avg:49.35ms
step:1515/2160 train_time:74808ms step_avg:49.38ms
step:1516/2160 train_time:74895ms step_avg:49.40ms
step:1517/2160 train_time:74983ms step_avg:49.43ms
step:1518/2160 train_time:75069ms step_avg:49.45ms
step:1519/2160 train_time:75157ms step_avg:49.48ms
step:1520/2160 train_time:75242ms step_avg:49.50ms
step:1521/2160 train_time:75330ms step_avg:49.53ms
step:1522/2160 train_time:75416ms step_avg:49.55ms
step:1523/2160 train_time:75504ms step_avg:49.58ms
step:1524/2160 train_time:75591ms step_avg:49.60ms
step:1525/2160 train_time:75680ms step_avg:49.63ms
step:1526/2160 train_time:75768ms step_avg:49.65ms
step:1527/2160 train_time:75857ms step_avg:49.68ms
step:1528/2160 train_time:75943ms step_avg:49.70ms
step:1529/2160 train_time:76031ms step_avg:49.73ms
step:1530/2160 train_time:76116ms step_avg:49.75ms
step:1531/2160 train_time:76204ms step_avg:49.77ms
step:1532/2160 train_time:76290ms step_avg:49.80ms
step:1533/2160 train_time:76377ms step_avg:49.82ms
step:1534/2160 train_time:76464ms step_avg:49.85ms
step:1535/2160 train_time:76552ms step_avg:49.87ms
step:1536/2160 train_time:76638ms step_avg:49.89ms
step:1537/2160 train_time:76728ms step_avg:49.92ms
step:1538/2160 train_time:76815ms step_avg:49.94ms
step:1539/2160 train_time:76903ms step_avg:49.97ms
step:1540/2160 train_time:76990ms step_avg:49.99ms
step:1541/2160 train_time:77078ms step_avg:50.02ms
step:1542/2160 train_time:77164ms step_avg:50.04ms
step:1543/2160 train_time:77251ms step_avg:50.07ms
step:1544/2160 train_time:77337ms step_avg:50.09ms
step:1545/2160 train_time:77425ms step_avg:50.11ms
step:1546/2160 train_time:77511ms step_avg:50.14ms
step:1547/2160 train_time:77600ms step_avg:50.16ms
step:1548/2160 train_time:77687ms step_avg:50.19ms
step:1549/2160 train_time:77776ms step_avg:50.21ms
step:1550/2160 train_time:77863ms step_avg:50.23ms
step:1551/2160 train_time:77950ms step_avg:50.26ms
step:1552/2160 train_time:78037ms step_avg:50.28ms
step:1553/2160 train_time:78125ms step_avg:50.31ms
step:1554/2160 train_time:78211ms step_avg:50.33ms
step:1555/2160 train_time:78299ms step_avg:50.35ms
step:1556/2160 train_time:78385ms step_avg:50.38ms
step:1557/2160 train_time:78473ms step_avg:50.40ms
step:1558/2160 train_time:78560ms step_avg:50.42ms
step:1559/2160 train_time:78649ms step_avg:50.45ms
step:1560/2160 train_time:78736ms step_avg:50.47ms
step:1561/2160 train_time:78825ms step_avg:50.50ms
step:1562/2160 train_time:78911ms step_avg:50.52ms
step:1563/2160 train_time:79000ms step_avg:50.54ms
step:1564/2160 train_time:79087ms step_avg:50.57ms
step:1565/2160 train_time:79174ms step_avg:50.59ms
step:1566/2160 train_time:79260ms step_avg:50.61ms
step:1567/2160 train_time:79348ms step_avg:50.64ms
step:1568/2160 train_time:79435ms step_avg:50.66ms
step:1569/2160 train_time:79523ms step_avg:50.68ms
step:1570/2160 train_time:79609ms step_avg:50.71ms
step:1571/2160 train_time:79697ms step_avg:50.73ms
step:1572/2160 train_time:79784ms step_avg:50.75ms
step:1573/2160 train_time:79874ms step_avg:50.78ms
step:1574/2160 train_time:79960ms step_avg:50.80ms
step:1575/2160 train_time:80048ms step_avg:50.82ms
step:1576/2160 train_time:80134ms step_avg:50.85ms
step:1577/2160 train_time:80222ms step_avg:50.87ms
step:1578/2160 train_time:80308ms step_avg:50.89ms
step:1579/2160 train_time:80397ms step_avg:50.92ms
step:1580/2160 train_time:80483ms step_avg:50.94ms
step:1581/2160 train_time:80571ms step_avg:50.96ms
step:1582/2160 train_time:80657ms step_avg:50.98ms
step:1583/2160 train_time:80746ms step_avg:51.01ms
step:1584/2160 train_time:80833ms step_avg:51.03ms
step:1585/2160 train_time:80922ms step_avg:51.05ms
step:1586/2160 train_time:81007ms step_avg:51.08ms
step:1587/2160 train_time:81096ms step_avg:51.10ms
step:1588/2160 train_time:81182ms step_avg:51.12ms
step:1589/2160 train_time:81270ms step_avg:51.15ms
step:1590/2160 train_time:81357ms step_avg:51.17ms
step:1591/2160 train_time:81445ms step_avg:51.19ms
step:1592/2160 train_time:81532ms step_avg:51.21ms
step:1593/2160 train_time:81620ms step_avg:51.24ms
step:1594/2160 train_time:81707ms step_avg:51.26ms
step:1595/2160 train_time:81795ms step_avg:51.28ms
step:1596/2160 train_time:81881ms step_avg:51.30ms
step:1597/2160 train_time:81970ms step_avg:51.33ms
step:1598/2160 train_time:82057ms step_avg:51.35ms
step:1599/2160 train_time:82147ms step_avg:51.37ms
step:1600/2160 train_time:82233ms step_avg:51.40ms
step:1601/2160 train_time:82321ms step_avg:51.42ms
step:1602/2160 train_time:82406ms step_avg:51.44ms
step:1603/2160 train_time:82495ms step_avg:51.46ms
step:1604/2160 train_time:82581ms step_avg:51.48ms
step:1605/2160 train_time:82671ms step_avg:51.51ms
step:1606/2160 train_time:82757ms step_avg:51.53ms
step:1607/2160 train_time:82845ms step_avg:51.55ms
step:1608/2160 train_time:82931ms step_avg:51.57ms
step:1609/2160 train_time:83021ms step_avg:51.60ms
step:1610/2160 train_time:83107ms step_avg:51.62ms
step:1611/2160 train_time:83195ms step_avg:51.64ms
step:1612/2160 train_time:83281ms step_avg:51.66ms
step:1613/2160 train_time:83370ms step_avg:51.69ms
step:1614/2160 train_time:83456ms step_avg:51.71ms
step:1615/2160 train_time:83545ms step_avg:51.73ms
step:1616/2160 train_time:83631ms step_avg:51.75ms
step:1617/2160 train_time:83719ms step_avg:51.77ms
step:1618/2160 train_time:83806ms step_avg:51.80ms
step:1619/2160 train_time:83895ms step_avg:51.82ms
step:1620/2160 train_time:83982ms step_avg:51.84ms
step:1621/2160 train_time:84070ms step_avg:51.86ms
step:1622/2160 train_time:84156ms step_avg:51.88ms
step:1623/2160 train_time:84245ms step_avg:51.91ms
step:1624/2160 train_time:84331ms step_avg:51.93ms
step:1625/2160 train_time:84420ms step_avg:51.95ms
step:1626/2160 train_time:84507ms step_avg:51.97ms
step:1627/2160 train_time:84596ms step_avg:52.00ms
step:1628/2160 train_time:84683ms step_avg:52.02ms
step:1629/2160 train_time:84771ms step_avg:52.04ms
step:1630/2160 train_time:84857ms step_avg:52.06ms
step:1631/2160 train_time:84946ms step_avg:52.08ms
step:1632/2160 train_time:85032ms step_avg:52.10ms
step:1633/2160 train_time:85120ms step_avg:52.13ms
step:1634/2160 train_time:85206ms step_avg:52.15ms
step:1635/2160 train_time:85295ms step_avg:52.17ms
step:1636/2160 train_time:85380ms step_avg:52.19ms
step:1637/2160 train_time:85469ms step_avg:52.21ms
step:1638/2160 train_time:85554ms step_avg:52.23ms
step:1639/2160 train_time:85643ms step_avg:52.25ms
step:1640/2160 train_time:85730ms step_avg:52.27ms
step:1641/2160 train_time:85819ms step_avg:52.30ms
step:1642/2160 train_time:85905ms step_avg:52.32ms
step:1643/2160 train_time:85993ms step_avg:52.34ms
step:1644/2160 train_time:86080ms step_avg:52.36ms
step:1645/2160 train_time:86168ms step_avg:52.38ms
step:1646/2160 train_time:86255ms step_avg:52.40ms
step:1647/2160 train_time:86343ms step_avg:52.42ms
step:1648/2160 train_time:86429ms step_avg:52.45ms
step:1649/2160 train_time:86518ms step_avg:52.47ms
step:1650/2160 train_time:86605ms step_avg:52.49ms
step:1651/2160 train_time:86693ms step_avg:52.51ms
step:1652/2160 train_time:86780ms step_avg:52.53ms
step:1653/2160 train_time:86868ms step_avg:52.55ms
step:1654/2160 train_time:86953ms step_avg:52.57ms
step:1655/2160 train_time:87042ms step_avg:52.59ms
step:1656/2160 train_time:87128ms step_avg:52.61ms
step:1657/2160 train_time:87217ms step_avg:52.64ms
step:1658/2160 train_time:87304ms step_avg:52.66ms
step:1659/2160 train_time:87393ms step_avg:52.68ms
step:1660/2160 train_time:87479ms step_avg:52.70ms
step:1661/2160 train_time:87568ms step_avg:52.72ms
step:1662/2160 train_time:87654ms step_avg:52.74ms
step:1663/2160 train_time:87742ms step_avg:52.76ms
step:1664/2160 train_time:87829ms step_avg:52.78ms
step:1665/2160 train_time:87918ms step_avg:52.80ms
step:1666/2160 train_time:88004ms step_avg:52.82ms
step:1667/2160 train_time:88093ms step_avg:52.85ms
step:1668/2160 train_time:88178ms step_avg:52.86ms
step:1669/2160 train_time:88267ms step_avg:52.89ms
step:1670/2160 train_time:88354ms step_avg:52.91ms
step:1671/2160 train_time:88442ms step_avg:52.93ms
step:1672/2160 train_time:88528ms step_avg:52.95ms
step:1673/2160 train_time:88617ms step_avg:52.97ms
step:1674/2160 train_time:88703ms step_avg:52.99ms
step:1675/2160 train_time:88792ms step_avg:53.01ms
step:1676/2160 train_time:88878ms step_avg:53.03ms
step:1677/2160 train_time:88967ms step_avg:53.05ms
step:1678/2160 train_time:89053ms step_avg:53.07ms
step:1679/2160 train_time:89142ms step_avg:53.09ms
step:1680/2160 train_time:89229ms step_avg:53.11ms
step:1681/2160 train_time:89318ms step_avg:53.13ms
step:1682/2160 train_time:89404ms step_avg:53.15ms
step:1683/2160 train_time:89492ms step_avg:53.17ms
step:1684/2160 train_time:89578ms step_avg:53.19ms
step:1685/2160 train_time:89666ms step_avg:53.21ms
step:1686/2160 train_time:89752ms step_avg:53.23ms
step:1687/2160 train_time:89840ms step_avg:53.25ms
step:1688/2160 train_time:89927ms step_avg:53.27ms
step:1689/2160 train_time:90016ms step_avg:53.30ms
step:1690/2160 train_time:90103ms step_avg:53.32ms
step:1691/2160 train_time:90192ms step_avg:53.34ms
step:1692/2160 train_time:90278ms step_avg:53.36ms
step:1693/2160 train_time:90367ms step_avg:53.38ms
step:1694/2160 train_time:90453ms step_avg:53.40ms
step:1695/2160 train_time:90541ms step_avg:53.42ms
step:1696/2160 train_time:90628ms step_avg:53.44ms
step:1697/2160 train_time:90716ms step_avg:53.46ms
step:1698/2160 train_time:90802ms step_avg:53.48ms
step:1699/2160 train_time:90891ms step_avg:53.50ms
step:1700/2160 train_time:90977ms step_avg:53.52ms
step:1701/2160 train_time:91066ms step_avg:53.54ms
step:1702/2160 train_time:91152ms step_avg:53.56ms
step:1703/2160 train_time:91241ms step_avg:53.58ms
step:1704/2160 train_time:91327ms step_avg:53.60ms
step:1705/2160 train_time:91415ms step_avg:53.62ms
step:1706/2160 train_time:91501ms step_avg:53.63ms
step:1707/2160 train_time:91589ms step_avg:53.66ms
step:1708/2160 train_time:91676ms step_avg:53.67ms
step:1709/2160 train_time:91765ms step_avg:53.69ms
step:1710/2160 train_time:91851ms step_avg:53.71ms
step:1711/2160 train_time:91939ms step_avg:53.73ms
step:1712/2160 train_time:92026ms step_avg:53.75ms
step:1713/2160 train_time:92115ms step_avg:53.77ms
step:1714/2160 train_time:92201ms step_avg:53.79ms
step:1715/2160 train_time:92290ms step_avg:53.81ms
step:1716/2160 train_time:92376ms step_avg:53.83ms
step:1717/2160 train_time:92464ms step_avg:53.85ms
step:1718/2160 train_time:92549ms step_avg:53.87ms
step:1719/2160 train_time:92638ms step_avg:53.89ms
step:1720/2160 train_time:92725ms step_avg:53.91ms
step:1721/2160 train_time:92813ms step_avg:53.93ms
step:1722/2160 train_time:92900ms step_avg:53.95ms
step:1723/2160 train_time:92989ms step_avg:53.97ms
step:1724/2160 train_time:93074ms step_avg:53.99ms
step:1725/2160 train_time:93162ms step_avg:54.01ms
step:1726/2160 train_time:93249ms step_avg:54.03ms
step:1727/2160 train_time:93338ms step_avg:54.05ms
step:1728/2160 train_time:93424ms step_avg:54.06ms
step:1729/2160 train_time:93512ms step_avg:54.08ms
step:1730/2160 train_time:93599ms step_avg:54.10ms
step:1731/2160 train_time:93687ms step_avg:54.12ms
step:1732/2160 train_time:93773ms step_avg:54.14ms
step:1733/2160 train_time:93861ms step_avg:54.16ms
step:1734/2160 train_time:93947ms step_avg:54.18ms
step:1735/2160 train_time:94036ms step_avg:54.20ms
step:1736/2160 train_time:94122ms step_avg:54.22ms
step:1737/2160 train_time:94211ms step_avg:54.24ms
step:1738/2160 train_time:94298ms step_avg:54.26ms
step:1739/2160 train_time:94386ms step_avg:54.28ms
step:1740/2160 train_time:94472ms step_avg:54.29ms
step:1741/2160 train_time:94561ms step_avg:54.31ms
step:1742/2160 train_time:94647ms step_avg:54.33ms
step:1743/2160 train_time:94736ms step_avg:54.35ms
step:1744/2160 train_time:94821ms step_avg:54.37ms
step:1745/2160 train_time:94910ms step_avg:54.39ms
step:1746/2160 train_time:94995ms step_avg:54.41ms
step:1747/2160 train_time:95084ms step_avg:54.43ms
step:1748/2160 train_time:95170ms step_avg:54.45ms
step:1749/2160 train_time:95259ms step_avg:54.46ms
step:1750/2160 train_time:95345ms step_avg:54.48ms
step:1750/2160 val_loss:3.3764 train_time:95435ms step_avg:54.53ms
step:1751/2160 train_time:95454ms step_avg:54.51ms
step:1752/2160 train_time:95525ms step_avg:54.52ms
step:1753/2160 train_time:95620ms step_avg:54.55ms
step:1754/2160 train_time:95707ms step_avg:54.56ms
step:1755/2160 train_time:95795ms step_avg:54.58ms
step:1756/2160 train_time:95879ms step_avg:54.60ms
step:1757/2160 train_time:96008ms step_avg:54.64ms
step:1758/2160 train_time:96075ms step_avg:54.65ms
step:1759/2160 train_time:96161ms step_avg:54.67ms
step:1760/2160 train_time:96246ms step_avg:54.69ms
step:1761/2160 train_time:96333ms step_avg:54.70ms
step:1762/2160 train_time:96418ms step_avg:54.72ms
step:1763/2160 train_time:96508ms step_avg:54.74ms
step:1764/2160 train_time:96596ms step_avg:54.76ms
step:1765/2160 train_time:96685ms step_avg:54.78ms
step:1766/2160 train_time:96771ms step_avg:54.80ms
step:1767/2160 train_time:96859ms step_avg:54.82ms
step:1768/2160 train_time:96947ms step_avg:54.83ms
step:1769/2160 train_time:97036ms step_avg:54.85ms
step:1770/2160 train_time:97122ms step_avg:54.87ms
step:1771/2160 train_time:97211ms step_avg:54.89ms
step:1772/2160 train_time:97296ms step_avg:54.91ms
step:1773/2160 train_time:97384ms step_avg:54.93ms
step:1774/2160 train_time:97470ms step_avg:54.94ms
step:1775/2160 train_time:97559ms step_avg:54.96ms
step:1776/2160 train_time:97646ms step_avg:54.98ms
step:1777/2160 train_time:97735ms step_avg:55.00ms
step:1778/2160 train_time:97821ms step_avg:55.02ms
step:1779/2160 train_time:97910ms step_avg:55.04ms
step:1780/2160 train_time:97996ms step_avg:55.05ms
step:1781/2160 train_time:98084ms step_avg:55.07ms
step:1782/2160 train_time:98170ms step_avg:55.09ms
step:1783/2160 train_time:98259ms step_avg:55.11ms
step:1784/2160 train_time:98345ms step_avg:55.13ms
step:1785/2160 train_time:98434ms step_avg:55.14ms
step:1786/2160 train_time:98520ms step_avg:55.16ms
step:1787/2160 train_time:98608ms step_avg:55.18ms
step:1788/2160 train_time:98694ms step_avg:55.20ms
step:1789/2160 train_time:98783ms step_avg:55.22ms
step:1790/2160 train_time:98871ms step_avg:55.24ms
step:1791/2160 train_time:98959ms step_avg:55.25ms
step:1792/2160 train_time:99046ms step_avg:55.27ms
step:1793/2160 train_time:99134ms step_avg:55.29ms
step:1794/2160 train_time:99221ms step_avg:55.31ms
step:1795/2160 train_time:99309ms step_avg:55.33ms
step:1796/2160 train_time:99395ms step_avg:55.34ms
step:1797/2160 train_time:99483ms step_avg:55.36ms
step:1798/2160 train_time:99570ms step_avg:55.38ms
step:1799/2160 train_time:99658ms step_avg:55.40ms
step:1800/2160 train_time:99743ms step_avg:55.41ms
step:1801/2160 train_time:99833ms step_avg:55.43ms
step:1802/2160 train_time:99918ms step_avg:55.45ms
step:1803/2160 train_time:100007ms step_avg:55.47ms
step:1804/2160 train_time:100093ms step_avg:55.48ms
step:1805/2160 train_time:100181ms step_avg:55.50ms
step:1806/2160 train_time:100268ms step_avg:55.52ms
step:1807/2160 train_time:100357ms step_avg:55.54ms
step:1808/2160 train_time:100443ms step_avg:55.55ms
step:1809/2160 train_time:100532ms step_avg:55.57ms
step:1810/2160 train_time:100618ms step_avg:55.59ms
step:1811/2160 train_time:100707ms step_avg:55.61ms
step:1812/2160 train_time:100793ms step_avg:55.63ms
step:1813/2160 train_time:100882ms step_avg:55.64ms
step:1814/2160 train_time:100968ms step_avg:55.66ms
step:1815/2160 train_time:101056ms step_avg:55.68ms
step:1816/2160 train_time:101142ms step_avg:55.69ms
step:1817/2160 train_time:101230ms step_avg:55.71ms
step:1818/2160 train_time:101316ms step_avg:55.73ms
step:1819/2160 train_time:101405ms step_avg:55.75ms
step:1820/2160 train_time:101491ms step_avg:55.76ms
step:1821/2160 train_time:101580ms step_avg:55.78ms
step:1822/2160 train_time:101667ms step_avg:55.80ms
step:1823/2160 train_time:101755ms step_avg:55.82ms
step:1824/2160 train_time:101842ms step_avg:55.83ms
step:1825/2160 train_time:101930ms step_avg:55.85ms
step:1826/2160 train_time:102016ms step_avg:55.87ms
step:1827/2160 train_time:102105ms step_avg:55.89ms
step:1828/2160 train_time:102191ms step_avg:55.90ms
step:1829/2160 train_time:102280ms step_avg:55.92ms
step:1830/2160 train_time:102366ms step_avg:55.94ms
step:1831/2160 train_time:102455ms step_avg:55.96ms
step:1832/2160 train_time:102542ms step_avg:55.97ms
step:1833/2160 train_time:102630ms step_avg:55.99ms
step:1834/2160 train_time:102717ms step_avg:56.01ms
step:1835/2160 train_time:102806ms step_avg:56.02ms
step:1836/2160 train_time:102892ms step_avg:56.04ms
step:1837/2160 train_time:102980ms step_avg:56.06ms
step:1838/2160 train_time:103067ms step_avg:56.08ms
step:1839/2160 train_time:103156ms step_avg:56.09ms
step:1840/2160 train_time:103242ms step_avg:56.11ms
step:1841/2160 train_time:103330ms step_avg:56.13ms
step:1842/2160 train_time:103417ms step_avg:56.14ms
step:1843/2160 train_time:103505ms step_avg:56.16ms
step:1844/2160 train_time:103591ms step_avg:56.18ms
step:1845/2160 train_time:103680ms step_avg:56.20ms
step:1846/2160 train_time:103767ms step_avg:56.21ms
step:1847/2160 train_time:103856ms step_avg:56.23ms
step:1848/2160 train_time:103942ms step_avg:56.25ms
step:1849/2160 train_time:104030ms step_avg:56.26ms
step:1850/2160 train_time:104116ms step_avg:56.28ms
step:1851/2160 train_time:104204ms step_avg:56.30ms
step:1852/2160 train_time:104290ms step_avg:56.31ms
step:1853/2160 train_time:104379ms step_avg:56.33ms
step:1854/2160 train_time:104466ms step_avg:56.35ms
step:1855/2160 train_time:104555ms step_avg:56.36ms
step:1856/2160 train_time:104641ms step_avg:56.38ms
step:1857/2160 train_time:104730ms step_avg:56.40ms
step:1858/2160 train_time:104816ms step_avg:56.41ms
step:1859/2160 train_time:104905ms step_avg:56.43ms
step:1860/2160 train_time:104991ms step_avg:56.45ms
step:1861/2160 train_time:105079ms step_avg:56.46ms
step:1862/2160 train_time:105165ms step_avg:56.48ms
step:1863/2160 train_time:105254ms step_avg:56.50ms
step:1864/2160 train_time:105340ms step_avg:56.51ms
step:1865/2160 train_time:105430ms step_avg:56.53ms
step:1866/2160 train_time:105515ms step_avg:56.55ms
step:1867/2160 train_time:105604ms step_avg:56.56ms
step:1868/2160 train_time:105691ms step_avg:56.58ms
step:1869/2160 train_time:105780ms step_avg:56.60ms
step:1870/2160 train_time:105867ms step_avg:56.61ms
step:1871/2160 train_time:105955ms step_avg:56.63ms
step:1872/2160 train_time:106041ms step_avg:56.65ms
step:1873/2160 train_time:106130ms step_avg:56.66ms
step:1874/2160 train_time:106217ms step_avg:56.68ms
step:1875/2160 train_time:106305ms step_avg:56.70ms
step:1876/2160 train_time:106391ms step_avg:56.71ms
step:1877/2160 train_time:106480ms step_avg:56.73ms
step:1878/2160 train_time:106567ms step_avg:56.74ms
step:1879/2160 train_time:106655ms step_avg:56.76ms
step:1880/2160 train_time:106741ms step_avg:56.78ms
step:1881/2160 train_time:106830ms step_avg:56.79ms
step:1882/2160 train_time:106916ms step_avg:56.81ms
step:1883/2160 train_time:107005ms step_avg:56.83ms
step:1884/2160 train_time:107090ms step_avg:56.84ms
step:1885/2160 train_time:107179ms step_avg:56.86ms
step:1886/2160 train_time:107265ms step_avg:56.87ms
step:1887/2160 train_time:107353ms step_avg:56.89ms
step:1888/2160 train_time:107440ms step_avg:56.91ms
step:1889/2160 train_time:107529ms step_avg:56.92ms
step:1890/2160 train_time:107615ms step_avg:56.94ms
step:1891/2160 train_time:107704ms step_avg:56.96ms
step:1892/2160 train_time:107791ms step_avg:56.97ms
step:1893/2160 train_time:107878ms step_avg:56.99ms
step:1894/2160 train_time:107965ms step_avg:57.00ms
step:1895/2160 train_time:108053ms step_avg:57.02ms
step:1896/2160 train_time:108140ms step_avg:57.04ms
step:1897/2160 train_time:108228ms step_avg:57.05ms
step:1898/2160 train_time:108314ms step_avg:57.07ms
step:1899/2160 train_time:108402ms step_avg:57.08ms
step:1900/2160 train_time:108489ms step_avg:57.10ms
step:1901/2160 train_time:108578ms step_avg:57.12ms
step:1902/2160 train_time:108664ms step_avg:57.13ms
step:1903/2160 train_time:108753ms step_avg:57.15ms
step:1904/2160 train_time:108839ms step_avg:57.16ms
step:1905/2160 train_time:108927ms step_avg:57.18ms
step:1906/2160 train_time:109013ms step_avg:57.19ms
step:1907/2160 train_time:109102ms step_avg:57.21ms
step:1908/2160 train_time:109189ms step_avg:57.23ms
step:1909/2160 train_time:109277ms step_avg:57.24ms
step:1910/2160 train_time:109364ms step_avg:57.26ms
step:1911/2160 train_time:109453ms step_avg:57.28ms
step:1912/2160 train_time:109539ms step_avg:57.29ms
step:1913/2160 train_time:109628ms step_avg:57.31ms
step:1914/2160 train_time:109714ms step_avg:57.32ms
step:1915/2160 train_time:109802ms step_avg:57.34ms
step:1916/2160 train_time:109888ms step_avg:57.35ms
step:1917/2160 train_time:109977ms step_avg:57.37ms
step:1918/2160 train_time:110064ms step_avg:57.38ms
step:1919/2160 train_time:110152ms step_avg:57.40ms
step:1920/2160 train_time:110239ms step_avg:57.42ms
step:1921/2160 train_time:110327ms step_avg:57.43ms
step:1922/2160 train_time:110413ms step_avg:57.45ms
step:1923/2160 train_time:110502ms step_avg:57.46ms
step:1924/2160 train_time:110588ms step_avg:57.48ms
step:1925/2160 train_time:110677ms step_avg:57.49ms
step:1926/2160 train_time:110763ms step_avg:57.51ms
step:1927/2160 train_time:110851ms step_avg:57.53ms
step:1928/2160 train_time:110937ms step_avg:57.54ms
step:1929/2160 train_time:111025ms step_avg:57.56ms
step:1930/2160 train_time:111111ms step_avg:57.57ms
step:1931/2160 train_time:111199ms step_avg:57.59ms
step:1932/2160 train_time:111286ms step_avg:57.60ms
step:1933/2160 train_time:111375ms step_avg:57.62ms
step:1934/2160 train_time:111461ms step_avg:57.63ms
step:1935/2160 train_time:111550ms step_avg:57.65ms
step:1936/2160 train_time:111637ms step_avg:57.66ms
step:1937/2160 train_time:111725ms step_avg:57.68ms
step:1938/2160 train_time:111812ms step_avg:57.69ms
step:1939/2160 train_time:111900ms step_avg:57.71ms
step:1940/2160 train_time:111986ms step_avg:57.72ms
step:1941/2160 train_time:112075ms step_avg:57.74ms
step:1942/2160 train_time:112161ms step_avg:57.76ms
step:1943/2160 train_time:112249ms step_avg:57.77ms
step:1944/2160 train_time:112336ms step_avg:57.79ms
step:1945/2160 train_time:112424ms step_avg:57.80ms
step:1946/2160 train_time:112512ms step_avg:57.82ms
step:1947/2160 train_time:112600ms step_avg:57.83ms
step:1948/2160 train_time:112685ms step_avg:57.85ms
step:1949/2160 train_time:112774ms step_avg:57.86ms
step:1950/2160 train_time:112860ms step_avg:57.88ms
step:1951/2160 train_time:112948ms step_avg:57.89ms
step:1952/2160 train_time:113035ms step_avg:57.91ms
step:1953/2160 train_time:113123ms step_avg:57.92ms
step:1954/2160 train_time:113209ms step_avg:57.94ms
step:1955/2160 train_time:113298ms step_avg:57.95ms
step:1956/2160 train_time:113384ms step_avg:57.97ms
step:1957/2160 train_time:113472ms step_avg:57.98ms
step:1958/2160 train_time:113558ms step_avg:58.00ms
step:1959/2160 train_time:113647ms step_avg:58.01ms
step:1960/2160 train_time:113733ms step_avg:58.03ms
step:1961/2160 train_time:113822ms step_avg:58.04ms
step:1962/2160 train_time:113908ms step_avg:58.06ms
step:1963/2160 train_time:113996ms step_avg:58.07ms
step:1964/2160 train_time:114083ms step_avg:58.09ms
step:1965/2160 train_time:114171ms step_avg:58.10ms
step:1966/2160 train_time:114258ms step_avg:58.12ms
step:1967/2160 train_time:114346ms step_avg:58.13ms
step:1968/2160 train_time:114434ms step_avg:58.15ms
step:1969/2160 train_time:114522ms step_avg:58.16ms
step:1970/2160 train_time:114608ms step_avg:58.18ms
step:1971/2160 train_time:114697ms step_avg:58.19ms
step:1972/2160 train_time:114783ms step_avg:58.21ms
step:1973/2160 train_time:114873ms step_avg:58.22ms
step:1974/2160 train_time:114959ms step_avg:58.24ms
step:1975/2160 train_time:115047ms step_avg:58.25ms
step:1976/2160 train_time:115133ms step_avg:58.27ms
step:1977/2160 train_time:115221ms step_avg:58.28ms
step:1978/2160 train_time:115307ms step_avg:58.29ms
step:1979/2160 train_time:115396ms step_avg:58.31ms
step:1980/2160 train_time:115481ms step_avg:58.32ms
step:1981/2160 train_time:115570ms step_avg:58.34ms
step:1982/2160 train_time:115656ms step_avg:58.35ms
step:1983/2160 train_time:115744ms step_avg:58.37ms
step:1984/2160 train_time:115830ms step_avg:58.38ms
step:1985/2160 train_time:115918ms step_avg:58.40ms
step:1986/2160 train_time:116005ms step_avg:58.41ms
step:1987/2160 train_time:116093ms step_avg:58.43ms
step:1988/2160 train_time:116180ms step_avg:58.44ms
step:1989/2160 train_time:116268ms step_avg:58.46ms
step:1990/2160 train_time:116354ms step_avg:58.47ms
step:1991/2160 train_time:116443ms step_avg:58.48ms
step:1992/2160 train_time:116529ms step_avg:58.50ms
step:1993/2160 train_time:116617ms step_avg:58.51ms
step:1994/2160 train_time:116704ms step_avg:58.53ms
step:1995/2160 train_time:116791ms step_avg:58.54ms
step:1996/2160 train_time:116877ms step_avg:58.56ms
step:1997/2160 train_time:116966ms step_avg:58.57ms
step:1998/2160 train_time:117052ms step_avg:58.58ms
step:1999/2160 train_time:117141ms step_avg:58.60ms
step:2000/2160 train_time:117228ms step_avg:58.61ms
step:2000/2160 val_loss:3.3075 train_time:117317ms step_avg:58.66ms
step:2001/2160 train_time:117336ms step_avg:58.64ms
step:2002/2160 train_time:117406ms step_avg:58.64ms
step:2003/2160 train_time:117498ms step_avg:58.66ms
step:2004/2160 train_time:117585ms step_avg:58.68ms
step:2005/2160 train_time:117673ms step_avg:58.69ms
step:2006/2160 train_time:117758ms step_avg:58.70ms
step:2007/2160 train_time:117845ms step_avg:58.72ms
step:2008/2160 train_time:117930ms step_avg:58.73ms
step:2009/2160 train_time:118017ms step_avg:58.74ms
step:2010/2160 train_time:118103ms step_avg:58.76ms
step:2011/2160 train_time:118190ms step_avg:58.77ms
step:2012/2160 train_time:118278ms step_avg:58.79ms
step:2013/2160 train_time:118367ms step_avg:58.80ms
step:2014/2160 train_time:118457ms step_avg:58.82ms
step:2015/2160 train_time:118546ms step_avg:58.83ms
step:2016/2160 train_time:118632ms step_avg:58.85ms
step:2017/2160 train_time:118721ms step_avg:58.86ms
step:2018/2160 train_time:118806ms step_avg:58.87ms
step:2019/2160 train_time:118894ms step_avg:58.89ms
step:2020/2160 train_time:118979ms step_avg:58.90ms
step:2021/2160 train_time:119067ms step_avg:58.91ms
step:2022/2160 train_time:119153ms step_avg:58.93ms
step:2023/2160 train_time:119242ms step_avg:58.94ms
step:2024/2160 train_time:119329ms step_avg:58.96ms
step:2025/2160 train_time:119420ms step_avg:58.97ms
step:2026/2160 train_time:119507ms step_avg:58.99ms
step:2027/2160 train_time:119595ms step_avg:59.00ms
step:2028/2160 train_time:119682ms step_avg:59.01ms
step:2029/2160 train_time:119771ms step_avg:59.03ms
step:2030/2160 train_time:119856ms step_avg:59.04ms
step:2031/2160 train_time:119944ms step_avg:59.06ms
step:2032/2160 train_time:120029ms step_avg:59.07ms
step:2033/2160 train_time:120118ms step_avg:59.08ms
step:2034/2160 train_time:120204ms step_avg:59.10ms
step:2035/2160 train_time:120293ms step_avg:59.11ms
step:2036/2160 train_time:120380ms step_avg:59.13ms
step:2037/2160 train_time:120469ms step_avg:59.14ms
step:2038/2160 train_time:120556ms step_avg:59.15ms
step:2039/2160 train_time:120645ms step_avg:59.17ms
step:2040/2160 train_time:120731ms step_avg:59.18ms
step:2041/2160 train_time:120820ms step_avg:59.20ms
step:2042/2160 train_time:120906ms step_avg:59.21ms
step:2043/2160 train_time:120994ms step_avg:59.22ms
step:2044/2160 train_time:121080ms step_avg:59.24ms
step:2045/2160 train_time:121168ms step_avg:59.25ms
step:2046/2160 train_time:121254ms step_avg:59.26ms
step:2047/2160 train_time:121343ms step_avg:59.28ms
step:2048/2160 train_time:121431ms step_avg:59.29ms
step:2049/2160 train_time:121519ms step_avg:59.31ms
step:2050/2160 train_time:121606ms step_avg:59.32ms
step:2051/2160 train_time:121694ms step_avg:59.33ms
step:2052/2160 train_time:121781ms step_avg:59.35ms
step:2053/2160 train_time:121868ms step_avg:59.36ms
step:2054/2160 train_time:121954ms step_avg:59.37ms
step:2055/2160 train_time:122042ms step_avg:59.39ms
step:2056/2160 train_time:122127ms step_avg:59.40ms
step:2057/2160 train_time:122216ms step_avg:59.41ms
step:2058/2160 train_time:122303ms step_avg:59.43ms
step:2059/2160 train_time:122391ms step_avg:59.44ms
step:2060/2160 train_time:122478ms step_avg:59.46ms
step:2061/2160 train_time:122567ms step_avg:59.47ms
step:2062/2160 train_time:122654ms step_avg:59.48ms
step:2063/2160 train_time:122742ms step_avg:59.50ms
step:2064/2160 train_time:122829ms step_avg:59.51ms
step:2065/2160 train_time:122917ms step_avg:59.52ms
step:2066/2160 train_time:123003ms step_avg:59.54ms
step:2067/2160 train_time:123091ms step_avg:59.55ms
step:2068/2160 train_time:123177ms step_avg:59.56ms
step:2069/2160 train_time:123266ms step_avg:59.58ms
step:2070/2160 train_time:123354ms step_avg:59.59ms
step:2071/2160 train_time:123442ms step_avg:59.61ms
step:2072/2160 train_time:123529ms step_avg:59.62ms
step:2073/2160 train_time:123618ms step_avg:59.63ms
step:2074/2160 train_time:123705ms step_avg:59.65ms
step:2075/2160 train_time:123793ms step_avg:59.66ms
step:2076/2160 train_time:123880ms step_avg:59.67ms
step:2077/2160 train_time:123968ms step_avg:59.69ms
step:2078/2160 train_time:124054ms step_avg:59.70ms
step:2079/2160 train_time:124142ms step_avg:59.71ms
step:2080/2160 train_time:124229ms step_avg:59.73ms
step:2081/2160 train_time:124318ms step_avg:59.74ms
step:2082/2160 train_time:124404ms step_avg:59.75ms
step:2083/2160 train_time:124494ms step_avg:59.77ms
step:2084/2160 train_time:124581ms step_avg:59.78ms
step:2085/2160 train_time:124670ms step_avg:59.79ms
step:2086/2160 train_time:124756ms step_avg:59.81ms
step:2087/2160 train_time:124844ms step_avg:59.82ms
step:2088/2160 train_time:124930ms step_avg:59.83ms
step:2089/2160 train_time:125017ms step_avg:59.85ms
step:2090/2160 train_time:125104ms step_avg:59.86ms
step:2091/2160 train_time:125192ms step_avg:59.87ms
step:2092/2160 train_time:125278ms step_avg:59.88ms
step:2093/2160 train_time:125367ms step_avg:59.90ms
step:2094/2160 train_time:125454ms step_avg:59.91ms
step:2095/2160 train_time:125544ms step_avg:59.93ms
step:2096/2160 train_time:125630ms step_avg:59.94ms
step:2097/2160 train_time:125719ms step_avg:59.95ms
step:2098/2160 train_time:125806ms step_avg:59.96ms
step:2099/2160 train_time:125894ms step_avg:59.98ms
step:2100/2160 train_time:125980ms step_avg:59.99ms
step:2101/2160 train_time:126068ms step_avg:60.00ms
step:2102/2160 train_time:126154ms step_avg:60.02ms
step:2103/2160 train_time:126242ms step_avg:60.03ms
step:2104/2160 train_time:126327ms step_avg:60.04ms
step:2105/2160 train_time:126417ms step_avg:60.06ms
step:2106/2160 train_time:126504ms step_avg:60.07ms
step:2107/2160 train_time:126593ms step_avg:60.08ms
step:2108/2160 train_time:126679ms step_avg:60.09ms
step:2109/2160 train_time:126768ms step_avg:60.11ms
step:2110/2160 train_time:126854ms step_avg:60.12ms
step:2111/2160 train_time:126943ms step_avg:60.13ms
step:2112/2160 train_time:127029ms step_avg:60.15ms
step:2113/2160 train_time:127117ms step_avg:60.16ms
step:2114/2160 train_time:127203ms step_avg:60.17ms
step:2115/2160 train_time:127291ms step_avg:60.18ms
step:2116/2160 train_time:127377ms step_avg:60.20ms
step:2117/2160 train_time:127466ms step_avg:60.21ms
step:2118/2160 train_time:127553ms step_avg:60.22ms
step:2119/2160 train_time:127641ms step_avg:60.24ms
step:2120/2160 train_time:127728ms step_avg:60.25ms
step:2121/2160 train_time:127817ms step_avg:60.26ms
step:2122/2160 train_time:127904ms step_avg:60.28ms
step:2123/2160 train_time:127993ms step_avg:60.29ms
step:2124/2160 train_time:128080ms step_avg:60.30ms
step:2125/2160 train_time:128169ms step_avg:60.31ms
step:2126/2160 train_time:128255ms step_avg:60.33ms
step:2127/2160 train_time:128343ms step_avg:60.34ms
step:2128/2160 train_time:128430ms step_avg:60.35ms
step:2129/2160 train_time:128519ms step_avg:60.37ms
step:2130/2160 train_time:128606ms step_avg:60.38ms
step:2131/2160 train_time:128695ms step_avg:60.39ms
step:2132/2160 train_time:128782ms step_avg:60.40ms
step:2133/2160 train_time:128870ms step_avg:60.42ms
step:2134/2160 train_time:128957ms step_avg:60.43ms
step:2135/2160 train_time:129046ms step_avg:60.44ms
step:2136/2160 train_time:129132ms step_avg:60.46ms
step:2137/2160 train_time:129220ms step_avg:60.47ms
step:2138/2160 train_time:129306ms step_avg:60.48ms
step:2139/2160 train_time:129395ms step_avg:60.49ms
step:2140/2160 train_time:129482ms step_avg:60.51ms
step:2141/2160 train_time:129570ms step_avg:60.52ms
step:2142/2160 train_time:129658ms step_avg:60.53ms
step:2143/2160 train_time:129747ms step_avg:60.54ms
step:2144/2160 train_time:129833ms step_avg:60.56ms
step:2145/2160 train_time:129922ms step_avg:60.57ms
step:2146/2160 train_time:130009ms step_avg:60.58ms
step:2147/2160 train_time:130097ms step_avg:60.59ms
step:2148/2160 train_time:130184ms step_avg:60.61ms
step:2149/2160 train_time:130271ms step_avg:60.62ms
step:2150/2160 train_time:130358ms step_avg:60.63ms
step:2151/2160 train_time:130446ms step_avg:60.64ms
step:2152/2160 train_time:130532ms step_avg:60.66ms
step:2153/2160 train_time:130621ms step_avg:60.67ms
step:2154/2160 train_time:130709ms step_avg:60.68ms
step:2155/2160 train_time:130798ms step_avg:60.70ms
step:2156/2160 train_time:130886ms step_avg:60.71ms
step:2157/2160 train_time:130974ms step_avg:60.72ms
step:2158/2160 train_time:131060ms step_avg:60.73ms
step:2159/2160 train_time:131149ms step_avg:60.75ms
step:2160/2160 train_time:131236ms step_avg:60.76ms
step:2160/2160 val_loss:3.2750 train_time:131325ms step_avg:60.80ms
peak memory allocated: 30032 MiB reserved: 44956 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 10 13:24:22 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                  Off |
| N/A   30C    P0            146W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                  Off |
| N/A   26C    P0            137W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                  Off |
| N/A   23C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                  Off |
| N/A   27C    P0            133W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                  Off |
| N/A   28C    P0            134W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                  Off |
| N/A   24C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                  Off |
| N/A   27C    P0            138W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                  Off |
| N/A   23C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     56108      C   /usr/bin/python3                             1510MiB |
|    0   N/A  N/A     56109      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     56110      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     56111      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     56112      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     56113      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     56114      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     56115      C   /usr/bin/python3                              614MiB |
|    1   N/A  N/A     56109      C   /usr/bin/python3                             1510MiB |
|    2   N/A  N/A     56110      C   /usr/bin/python3                             1510MiB |
|    3   N/A  N/A     56111      C   /usr/bin/python3                             1510MiB |
|    4   N/A  N/A     56112      C   /usr/bin/python3                             1510MiB |
|    5   N/A  N/A     56113      C   /usr/bin/python3                             1510MiB |
|    6   N/A  N/A     56114      C   /usr/bin/python3                             1510MiB |
|    7   N/A  N/A     56115      C   /usr/bin/python3                             1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2160 train_time:90ms step_avg:90.14ms
step:2/2160 train_time:112ms step_avg:55.82ms
step:3/2160 train_time:130ms step_avg:43.43ms
step:4/2160 train_time:157ms step_avg:39.15ms
step:5/2160 train_time:190ms step_avg:37.99ms
step:6/2160 train_time:258ms step_avg:42.97ms
step:7/2160 train_time:280ms step_avg:40.06ms
step:8/2160 train_time:313ms step_avg:39.16ms
step:9/2160 train_time:347ms step_avg:38.54ms
step:10/2160 train_time:380ms step_avg:37.97ms
step:11/2160 train_time:413ms step_avg:37.58ms
step:12/2160 train_time:446ms step_avg:37.18ms
step:13/2160 train_time:480ms step_avg:36.95ms
step:14/2160 train_time:514ms step_avg:36.69ms
step:15/2160 train_time:547ms step_avg:36.49ms
step:16/2160 train_time:580ms step_avg:36.27ms
step:17/2160 train_time:614ms step_avg:36.12ms
step:18/2160 train_time:647ms step_avg:35.95ms
step:19/2160 train_time:681ms step_avg:35.83ms
step:20/2160 train_time:714ms step_avg:35.69ms
step:21/2160 train_time:748ms step_avg:35.60ms
step:22/2160 train_time:781ms step_avg:35.48ms
step:23/2160 train_time:814ms step_avg:35.40ms
step:24/2160 train_time:847ms step_avg:35.30ms
step:25/2160 train_time:881ms step_avg:35.25ms
step:26/2160 train_time:914ms step_avg:35.16ms
step:27/2160 train_time:948ms step_avg:35.11ms
step:28/2160 train_time:981ms step_avg:35.04ms
step:29/2160 train_time:1015ms step_avg:34.99ms
step:30/2160 train_time:1048ms step_avg:34.92ms
step:31/2160 train_time:1082ms step_avg:34.90ms
step:32/2160 train_time:1115ms step_avg:34.85ms
step:33/2160 train_time:1149ms step_avg:34.82ms
step:34/2160 train_time:1182ms step_avg:34.76ms
step:35/2160 train_time:1216ms step_avg:34.74ms
step:36/2160 train_time:1249ms step_avg:34.69ms
step:37/2160 train_time:1283ms step_avg:34.69ms
step:38/2160 train_time:1316ms step_avg:34.64ms
step:39/2160 train_time:1350ms step_avg:34.63ms
step:40/2160 train_time:1383ms step_avg:34.58ms
step:41/2160 train_time:1418ms step_avg:34.58ms
step:42/2160 train_time:1451ms step_avg:34.54ms
step:43/2160 train_time:1485ms step_avg:34.53ms
step:44/2160 train_time:1518ms step_avg:34.50ms
step:45/2160 train_time:1552ms step_avg:34.49ms
step:46/2160 train_time:1585ms step_avg:34.45ms
step:47/2160 train_time:1619ms step_avg:34.44ms
step:48/2160 train_time:1652ms step_avg:34.41ms
step:49/2160 train_time:1686ms step_avg:34.40ms
step:50/2160 train_time:1719ms step_avg:34.37ms
step:51/2160 train_time:1752ms step_avg:34.36ms
step:52/2160 train_time:1785ms step_avg:34.33ms
step:53/2160 train_time:1820ms step_avg:34.33ms
step:54/2160 train_time:1853ms step_avg:34.31ms
step:55/2160 train_time:1886ms step_avg:34.30ms
step:56/2160 train_time:1919ms step_avg:34.28ms
step:57/2160 train_time:1953ms step_avg:34.27ms
step:58/2160 train_time:1986ms step_avg:34.24ms
step:59/2160 train_time:2020ms step_avg:34.24ms
step:60/2160 train_time:2053ms step_avg:34.22ms
step:61/2160 train_time:2087ms step_avg:34.21ms
step:62/2160 train_time:2120ms step_avg:34.20ms
step:63/2160 train_time:2154ms step_avg:34.19ms
step:64/2160 train_time:2187ms step_avg:34.17ms
step:65/2160 train_time:2221ms step_avg:34.16ms
step:66/2160 train_time:2254ms step_avg:34.15ms
step:67/2160 train_time:2288ms step_avg:34.15ms
step:68/2160 train_time:2321ms step_avg:34.13ms
step:69/2160 train_time:2355ms step_avg:34.12ms
step:70/2160 train_time:2387ms step_avg:34.11ms
step:71/2160 train_time:2422ms step_avg:34.11ms
step:72/2160 train_time:2455ms step_avg:34.09ms
step:73/2160 train_time:2489ms step_avg:34.09ms
step:74/2160 train_time:2522ms step_avg:34.08ms
step:75/2160 train_time:2555ms step_avg:34.07ms
step:76/2160 train_time:2588ms step_avg:34.06ms
step:77/2160 train_time:2622ms step_avg:34.06ms
step:78/2160 train_time:2655ms step_avg:34.04ms
step:79/2160 train_time:2689ms step_avg:34.04ms
step:80/2160 train_time:2722ms step_avg:34.02ms
step:81/2160 train_time:2756ms step_avg:34.02ms
step:82/2160 train_time:2788ms step_avg:34.01ms
step:83/2160 train_time:2822ms step_avg:34.01ms
step:84/2160 train_time:2855ms step_avg:33.99ms
step:85/2160 train_time:2889ms step_avg:33.99ms
step:86/2160 train_time:2922ms step_avg:33.98ms
step:87/2160 train_time:2956ms step_avg:33.97ms
step:88/2160 train_time:2989ms step_avg:33.96ms
step:89/2160 train_time:3023ms step_avg:33.96ms
step:90/2160 train_time:3055ms step_avg:33.95ms
step:91/2160 train_time:3089ms step_avg:33.95ms
step:92/2160 train_time:3122ms step_avg:33.94ms
step:93/2160 train_time:3156ms step_avg:33.94ms
step:94/2160 train_time:3189ms step_avg:33.93ms
step:95/2160 train_time:3223ms step_avg:33.93ms
step:96/2160 train_time:3256ms step_avg:33.92ms
step:97/2160 train_time:3290ms step_avg:33.92ms
step:98/2160 train_time:3323ms step_avg:33.91ms
step:99/2160 train_time:3357ms step_avg:33.90ms
step:100/2160 train_time:3390ms step_avg:33.90ms
step:101/2160 train_time:3423ms step_avg:33.90ms
step:102/2160 train_time:3456ms step_avg:33.89ms
step:103/2160 train_time:3491ms step_avg:33.89ms
step:104/2160 train_time:3524ms step_avg:33.88ms
step:105/2160 train_time:3557ms step_avg:33.88ms
step:106/2160 train_time:3590ms step_avg:33.87ms
step:107/2160 train_time:3624ms step_avg:33.87ms
step:108/2160 train_time:3657ms step_avg:33.86ms
step:109/2160 train_time:3691ms step_avg:33.86ms
step:110/2160 train_time:3724ms step_avg:33.85ms
step:111/2160 train_time:3758ms step_avg:33.85ms
step:112/2160 train_time:3791ms step_avg:33.84ms
step:113/2160 train_time:3824ms step_avg:33.84ms
step:114/2160 train_time:3857ms step_avg:33.83ms
step:115/2160 train_time:3891ms step_avg:33.83ms
step:116/2160 train_time:3924ms step_avg:33.82ms
step:117/2160 train_time:3958ms step_avg:33.82ms
step:118/2160 train_time:3990ms step_avg:33.82ms
step:119/2160 train_time:4024ms step_avg:33.82ms
step:120/2160 train_time:4057ms step_avg:33.81ms
step:121/2160 train_time:4091ms step_avg:33.81ms
step:122/2160 train_time:4124ms step_avg:33.80ms
step:123/2160 train_time:4157ms step_avg:33.80ms
step:124/2160 train_time:4190ms step_avg:33.79ms
step:125/2160 train_time:4224ms step_avg:33.79ms
step:126/2160 train_time:4257ms step_avg:33.79ms
step:127/2160 train_time:4291ms step_avg:33.79ms
step:128/2160 train_time:4324ms step_avg:33.78ms
step:129/2160 train_time:4357ms step_avg:33.78ms
step:130/2160 train_time:4390ms step_avg:33.77ms
step:131/2160 train_time:4424ms step_avg:33.77ms
step:132/2160 train_time:4457ms step_avg:33.76ms
step:133/2160 train_time:4491ms step_avg:33.76ms
step:134/2160 train_time:4524ms step_avg:33.76ms
step:135/2160 train_time:4557ms step_avg:33.76ms
step:136/2160 train_time:4590ms step_avg:33.75ms
step:137/2160 train_time:4624ms step_avg:33.75ms
step:138/2160 train_time:4657ms step_avg:33.75ms
step:139/2160 train_time:4691ms step_avg:33.75ms
step:140/2160 train_time:4724ms step_avg:33.74ms
step:141/2160 train_time:4758ms step_avg:33.74ms
step:142/2160 train_time:4790ms step_avg:33.74ms
step:143/2160 train_time:4824ms step_avg:33.74ms
step:144/2160 train_time:4857ms step_avg:33.73ms
step:145/2160 train_time:4891ms step_avg:33.73ms
step:146/2160 train_time:4924ms step_avg:33.72ms
step:147/2160 train_time:4958ms step_avg:33.72ms
step:148/2160 train_time:4990ms step_avg:33.72ms
step:149/2160 train_time:5024ms step_avg:33.72ms
step:150/2160 train_time:5057ms step_avg:33.72ms
step:151/2160 train_time:5091ms step_avg:33.72ms
step:152/2160 train_time:5124ms step_avg:33.71ms
step:153/2160 train_time:5158ms step_avg:33.71ms
step:154/2160 train_time:5190ms step_avg:33.70ms
step:155/2160 train_time:5224ms step_avg:33.70ms
step:156/2160 train_time:5257ms step_avg:33.70ms
step:157/2160 train_time:5291ms step_avg:33.70ms
step:158/2160 train_time:5323ms step_avg:33.69ms
step:159/2160 train_time:5357ms step_avg:33.69ms
step:160/2160 train_time:5390ms step_avg:33.69ms
step:161/2160 train_time:5424ms step_avg:33.69ms
step:162/2160 train_time:5457ms step_avg:33.68ms
step:163/2160 train_time:5490ms step_avg:33.68ms
step:164/2160 train_time:5523ms step_avg:33.68ms
step:165/2160 train_time:5557ms step_avg:33.68ms
step:166/2160 train_time:5590ms step_avg:33.67ms
step:167/2160 train_time:5624ms step_avg:33.68ms
step:168/2160 train_time:5657ms step_avg:33.67ms
step:169/2160 train_time:5690ms step_avg:33.67ms
step:170/2160 train_time:5723ms step_avg:33.67ms
step:171/2160 train_time:5757ms step_avg:33.67ms
step:172/2160 train_time:5790ms step_avg:33.66ms
step:173/2160 train_time:5824ms step_avg:33.66ms
step:174/2160 train_time:5857ms step_avg:33.66ms
step:175/2160 train_time:5890ms step_avg:33.66ms
step:176/2160 train_time:5923ms step_avg:33.65ms
step:177/2160 train_time:5957ms step_avg:33.65ms
step:178/2160 train_time:5989ms step_avg:33.65ms
step:179/2160 train_time:6023ms step_avg:33.65ms
step:180/2160 train_time:6056ms step_avg:33.64ms
step:181/2160 train_time:6089ms step_avg:33.64ms
step:182/2160 train_time:6122ms step_avg:33.64ms
step:183/2160 train_time:6156ms step_avg:33.64ms
step:184/2160 train_time:6189ms step_avg:33.63ms
step:185/2160 train_time:6222ms step_avg:33.63ms
step:186/2160 train_time:6255ms step_avg:33.63ms
step:187/2160 train_time:6289ms step_avg:33.63ms
step:188/2160 train_time:6321ms step_avg:33.62ms
step:189/2160 train_time:6355ms step_avg:33.63ms
step:190/2160 train_time:6388ms step_avg:33.62ms
step:191/2160 train_time:6422ms step_avg:33.62ms
step:192/2160 train_time:6455ms step_avg:33.62ms
step:193/2160 train_time:6488ms step_avg:33.62ms
step:194/2160 train_time:6521ms step_avg:33.62ms
step:195/2160 train_time:6555ms step_avg:33.62ms
step:196/2160 train_time:6588ms step_avg:33.61ms
step:197/2160 train_time:6622ms step_avg:33.61ms
step:198/2160 train_time:6655ms step_avg:33.61ms
step:199/2160 train_time:6688ms step_avg:33.61ms
step:200/2160 train_time:6721ms step_avg:33.60ms
step:201/2160 train_time:6755ms step_avg:33.61ms
step:202/2160 train_time:6788ms step_avg:33.60ms
step:203/2160 train_time:6821ms step_avg:33.60ms
step:204/2160 train_time:6854ms step_avg:33.60ms
step:205/2160 train_time:6888ms step_avg:33.60ms
step:206/2160 train_time:6920ms step_avg:33.59ms
step:207/2160 train_time:6954ms step_avg:33.59ms
step:208/2160 train_time:6987ms step_avg:33.59ms
step:209/2160 train_time:7021ms step_avg:33.59ms
step:210/2160 train_time:7053ms step_avg:33.59ms
step:211/2160 train_time:7087ms step_avg:33.59ms
step:212/2160 train_time:7120ms step_avg:33.59ms
step:213/2160 train_time:7153ms step_avg:33.58ms
step:214/2160 train_time:7186ms step_avg:33.58ms
step:215/2160 train_time:7220ms step_avg:33.58ms
step:216/2160 train_time:7253ms step_avg:33.58ms
step:217/2160 train_time:7287ms step_avg:33.58ms
step:218/2160 train_time:7320ms step_avg:33.58ms
step:219/2160 train_time:7353ms step_avg:33.58ms
step:220/2160 train_time:7386ms step_avg:33.57ms
step:221/2160 train_time:7420ms step_avg:33.57ms
step:222/2160 train_time:7453ms step_avg:33.57ms
step:223/2160 train_time:7486ms step_avg:33.57ms
step:224/2160 train_time:7519ms step_avg:33.57ms
step:225/2160 train_time:7553ms step_avg:33.57ms
step:226/2160 train_time:7586ms step_avg:33.56ms
step:227/2160 train_time:7620ms step_avg:33.57ms
step:228/2160 train_time:7652ms step_avg:33.56ms
step:229/2160 train_time:7686ms step_avg:33.56ms
step:230/2160 train_time:7719ms step_avg:33.56ms
step:231/2160 train_time:7753ms step_avg:33.56ms
step:232/2160 train_time:7786ms step_avg:33.56ms
step:233/2160 train_time:7819ms step_avg:33.56ms
step:234/2160 train_time:7852ms step_avg:33.56ms
step:235/2160 train_time:7886ms step_avg:33.56ms
step:236/2160 train_time:7919ms step_avg:33.55ms
step:237/2160 train_time:7952ms step_avg:33.55ms
step:238/2160 train_time:7985ms step_avg:33.55ms
step:239/2160 train_time:8019ms step_avg:33.55ms
step:240/2160 train_time:8051ms step_avg:33.55ms
step:241/2160 train_time:8085ms step_avg:33.55ms
step:242/2160 train_time:8118ms step_avg:33.55ms
step:243/2160 train_time:8152ms step_avg:33.55ms
step:244/2160 train_time:8184ms step_avg:33.54ms
step:245/2160 train_time:8218ms step_avg:33.54ms
step:246/2160 train_time:8251ms step_avg:33.54ms
step:247/2160 train_time:8285ms step_avg:33.54ms
step:248/2160 train_time:8318ms step_avg:33.54ms
step:249/2160 train_time:8352ms step_avg:33.54ms
step:250/2160 train_time:8384ms step_avg:33.54ms
step:250/2160 val_loss:4.3129 train_time:8420ms step_avg:33.68ms
step:251/2160 train_time:8440ms step_avg:33.62ms
step:252/2160 train_time:8459ms step_avg:33.57ms
step:253/2160 train_time:8489ms step_avg:33.55ms
step:254/2160 train_time:8522ms step_avg:33.55ms
step:255/2160 train_time:8558ms step_avg:33.56ms
step:256/2160 train_time:8592ms step_avg:33.56ms
step:257/2160 train_time:8627ms step_avg:33.57ms
step:258/2160 train_time:8660ms step_avg:33.57ms
step:259/2160 train_time:8694ms step_avg:33.57ms
step:260/2160 train_time:8727ms step_avg:33.57ms
step:261/2160 train_time:8761ms step_avg:33.57ms
step:262/2160 train_time:8794ms step_avg:33.56ms
step:263/2160 train_time:8827ms step_avg:33.56ms
step:264/2160 train_time:8860ms step_avg:33.56ms
step:265/2160 train_time:8894ms step_avg:33.56ms
step:266/2160 train_time:8927ms step_avg:33.56ms
step:267/2160 train_time:8961ms step_avg:33.56ms
step:268/2160 train_time:8993ms step_avg:33.56ms
step:269/2160 train_time:9027ms step_avg:33.56ms
step:270/2160 train_time:9060ms step_avg:33.55ms
step:271/2160 train_time:9093ms step_avg:33.56ms
step:272/2160 train_time:9126ms step_avg:33.55ms
step:273/2160 train_time:9160ms step_avg:33.55ms
step:274/2160 train_time:9192ms step_avg:33.55ms
step:275/2160 train_time:9226ms step_avg:33.55ms
step:276/2160 train_time:9259ms step_avg:33.55ms
step:277/2160 train_time:9292ms step_avg:33.55ms
step:278/2160 train_time:9325ms step_avg:33.54ms
step:279/2160 train_time:9359ms step_avg:33.54ms
step:280/2160 train_time:9391ms step_avg:33.54ms
step:281/2160 train_time:9425ms step_avg:33.54ms
step:282/2160 train_time:9458ms step_avg:33.54ms
step:283/2160 train_time:9491ms step_avg:33.54ms
step:284/2160 train_time:9524ms step_avg:33.54ms
step:285/2160 train_time:9558ms step_avg:33.54ms
step:286/2160 train_time:9591ms step_avg:33.53ms
step:287/2160 train_time:9624ms step_avg:33.53ms
step:288/2160 train_time:9657ms step_avg:33.53ms
step:289/2160 train_time:9691ms step_avg:33.53ms
step:290/2160 train_time:9724ms step_avg:33.53ms
step:291/2160 train_time:9757ms step_avg:33.53ms
step:292/2160 train_time:9790ms step_avg:33.53ms
step:293/2160 train_time:9824ms step_avg:33.53ms
step:294/2160 train_time:9857ms step_avg:33.53ms
step:295/2160 train_time:9891ms step_avg:33.53ms
step:296/2160 train_time:9923ms step_avg:33.52ms
step:297/2160 train_time:9957ms step_avg:33.53ms
step:298/2160 train_time:9990ms step_avg:33.52ms
step:299/2160 train_time:10024ms step_avg:33.52ms
step:300/2160 train_time:10057ms step_avg:33.52ms
step:301/2160 train_time:10090ms step_avg:33.52ms
step:302/2160 train_time:10123ms step_avg:33.52ms
step:303/2160 train_time:10157ms step_avg:33.52ms
step:304/2160 train_time:10190ms step_avg:33.52ms
step:305/2160 train_time:10223ms step_avg:33.52ms
step:306/2160 train_time:10256ms step_avg:33.52ms
step:307/2160 train_time:10290ms step_avg:33.52ms
step:308/2160 train_time:10322ms step_avg:33.51ms
step:309/2160 train_time:10356ms step_avg:33.51ms
step:310/2160 train_time:10389ms step_avg:33.51ms
step:311/2160 train_time:10423ms step_avg:33.51ms
step:312/2160 train_time:10455ms step_avg:33.51ms
step:313/2160 train_time:10489ms step_avg:33.51ms
step:314/2160 train_time:10522ms step_avg:33.51ms
step:315/2160 train_time:10555ms step_avg:33.51ms
step:316/2160 train_time:10588ms step_avg:33.51ms
step:317/2160 train_time:10622ms step_avg:33.51ms
step:318/2160 train_time:10655ms step_avg:33.51ms
step:319/2160 train_time:10688ms step_avg:33.51ms
step:320/2160 train_time:10721ms step_avg:33.50ms
step:321/2160 train_time:10755ms step_avg:33.50ms
step:322/2160 train_time:10787ms step_avg:33.50ms
step:323/2160 train_time:10821ms step_avg:33.50ms
step:324/2160 train_time:10854ms step_avg:33.50ms
step:325/2160 train_time:10888ms step_avg:33.50ms
step:326/2160 train_time:10921ms step_avg:33.50ms
step:327/2160 train_time:10954ms step_avg:33.50ms
step:328/2160 train_time:10987ms step_avg:33.50ms
step:329/2160 train_time:11020ms step_avg:33.50ms
step:330/2160 train_time:11053ms step_avg:33.50ms
step:331/2160 train_time:11087ms step_avg:33.49ms
step:332/2160 train_time:11120ms step_avg:33.49ms
step:333/2160 train_time:11153ms step_avg:33.49ms
step:334/2160 train_time:11186ms step_avg:33.49ms
step:335/2160 train_time:11220ms step_avg:33.49ms
step:336/2160 train_time:11253ms step_avg:33.49ms
step:337/2160 train_time:11286ms step_avg:33.49ms
step:338/2160 train_time:11319ms step_avg:33.49ms
step:339/2160 train_time:11353ms step_avg:33.49ms
step:340/2160 train_time:11385ms step_avg:33.49ms
step:341/2160 train_time:11419ms step_avg:33.49ms
step:342/2160 train_time:11452ms step_avg:33.48ms
step:343/2160 train_time:11486ms step_avg:33.49ms
step:344/2160 train_time:11518ms step_avg:33.48ms
step:345/2160 train_time:11552ms step_avg:33.48ms
step:346/2160 train_time:11585ms step_avg:33.48ms
step:347/2160 train_time:11618ms step_avg:33.48ms
step:348/2160 train_time:11651ms step_avg:33.48ms
step:349/2160 train_time:11685ms step_avg:33.48ms
step:350/2160 train_time:11717ms step_avg:33.48ms
step:351/2160 train_time:11751ms step_avg:33.48ms
step:352/2160 train_time:11784ms step_avg:33.48ms
step:353/2160 train_time:11818ms step_avg:33.48ms
step:354/2160 train_time:11850ms step_avg:33.48ms
step:355/2160 train_time:11884ms step_avg:33.48ms
step:356/2160 train_time:11917ms step_avg:33.47ms
step:357/2160 train_time:11951ms step_avg:33.48ms
step:358/2160 train_time:11983ms step_avg:33.47ms
step:359/2160 train_time:12017ms step_avg:33.47ms
step:360/2160 train_time:12050ms step_avg:33.47ms
step:361/2160 train_time:12084ms step_avg:33.47ms
step:362/2160 train_time:12117ms step_avg:33.47ms
step:363/2160 train_time:12150ms step_avg:33.47ms
step:364/2160 train_time:12183ms step_avg:33.47ms
step:365/2160 train_time:12217ms step_avg:33.47ms
step:366/2160 train_time:12250ms step_avg:33.47ms
step:367/2160 train_time:12284ms step_avg:33.47ms
step:368/2160 train_time:12316ms step_avg:33.47ms
step:369/2160 train_time:12350ms step_avg:33.47ms
step:370/2160 train_time:12383ms step_avg:33.47ms
step:371/2160 train_time:12417ms step_avg:33.47ms
step:372/2160 train_time:12450ms step_avg:33.47ms
step:373/2160 train_time:12484ms step_avg:33.47ms
step:374/2160 train_time:12516ms step_avg:33.47ms
step:375/2160 train_time:12550ms step_avg:33.47ms
step:376/2160 train_time:12583ms step_avg:33.47ms
step:377/2160 train_time:12617ms step_avg:33.47ms
step:378/2160 train_time:12649ms step_avg:33.46ms
step:379/2160 train_time:12683ms step_avg:33.46ms
step:380/2160 train_time:12716ms step_avg:33.46ms
step:381/2160 train_time:12749ms step_avg:33.46ms
step:382/2160 train_time:12782ms step_avg:33.46ms
step:383/2160 train_time:12816ms step_avg:33.46ms
step:384/2160 train_time:12849ms step_avg:33.46ms
step:385/2160 train_time:12882ms step_avg:33.46ms
step:386/2160 train_time:12915ms step_avg:33.46ms
step:387/2160 train_time:12949ms step_avg:33.46ms
step:388/2160 train_time:12981ms step_avg:33.46ms
step:389/2160 train_time:13015ms step_avg:33.46ms
step:390/2160 train_time:13048ms step_avg:33.46ms
step:391/2160 train_time:13082ms step_avg:33.46ms
step:392/2160 train_time:13114ms step_avg:33.46ms
step:393/2160 train_time:13148ms step_avg:33.46ms
step:394/2160 train_time:13181ms step_avg:33.45ms
step:395/2160 train_time:13215ms step_avg:33.46ms
step:396/2160 train_time:13248ms step_avg:33.45ms
step:397/2160 train_time:13281ms step_avg:33.45ms
step:398/2160 train_time:13315ms step_avg:33.45ms
step:399/2160 train_time:13348ms step_avg:33.45ms
step:400/2160 train_time:13381ms step_avg:33.45ms
step:401/2160 train_time:13414ms step_avg:33.45ms
step:402/2160 train_time:13447ms step_avg:33.45ms
step:403/2160 train_time:13481ms step_avg:33.45ms
step:404/2160 train_time:13513ms step_avg:33.45ms
step:405/2160 train_time:13547ms step_avg:33.45ms
step:406/2160 train_time:13580ms step_avg:33.45ms
step:407/2160 train_time:13614ms step_avg:33.45ms
step:408/2160 train_time:13646ms step_avg:33.45ms
step:409/2160 train_time:13680ms step_avg:33.45ms
step:410/2160 train_time:13713ms step_avg:33.45ms
step:411/2160 train_time:13747ms step_avg:33.45ms
step:412/2160 train_time:13780ms step_avg:33.45ms
step:413/2160 train_time:13813ms step_avg:33.45ms
step:414/2160 train_time:13846ms step_avg:33.44ms
step:415/2160 train_time:13880ms step_avg:33.45ms
step:416/2160 train_time:13913ms step_avg:33.44ms
step:417/2160 train_time:13946ms step_avg:33.44ms
step:418/2160 train_time:13979ms step_avg:33.44ms
step:419/2160 train_time:14013ms step_avg:33.44ms
step:420/2160 train_time:14046ms step_avg:33.44ms
step:421/2160 train_time:14079ms step_avg:33.44ms
step:422/2160 train_time:14112ms step_avg:33.44ms
step:423/2160 train_time:14146ms step_avg:33.44ms
step:424/2160 train_time:14179ms step_avg:33.44ms
step:425/2160 train_time:14212ms step_avg:33.44ms
step:426/2160 train_time:14245ms step_avg:33.44ms
step:427/2160 train_time:14279ms step_avg:33.44ms
step:428/2160 train_time:14312ms step_avg:33.44ms
step:429/2160 train_time:14423ms step_avg:33.62ms
step:430/2160 train_time:14442ms step_avg:33.59ms
step:431/2160 train_time:14470ms step_avg:33.57ms
step:432/2160 train_time:14503ms step_avg:33.57ms
step:433/2160 train_time:14537ms step_avg:33.57ms
step:434/2160 train_time:14569ms step_avg:33.57ms
step:435/2160 train_time:14603ms step_avg:33.57ms
step:436/2160 train_time:14636ms step_avg:33.57ms
step:437/2160 train_time:14670ms step_avg:33.57ms
step:438/2160 train_time:14702ms step_avg:33.57ms
step:439/2160 train_time:14736ms step_avg:33.57ms
step:440/2160 train_time:14769ms step_avg:33.57ms
step:441/2160 train_time:14802ms step_avg:33.57ms
step:442/2160 train_time:14835ms step_avg:33.56ms
step:443/2160 train_time:14869ms step_avg:33.56ms
step:444/2160 train_time:14902ms step_avg:33.56ms
step:445/2160 train_time:14935ms step_avg:33.56ms
step:446/2160 train_time:14968ms step_avg:33.56ms
step:447/2160 train_time:15002ms step_avg:33.56ms
step:448/2160 train_time:15034ms step_avg:33.56ms
step:449/2160 train_time:15068ms step_avg:33.56ms
step:450/2160 train_time:15101ms step_avg:33.56ms
step:451/2160 train_time:15135ms step_avg:33.56ms
step:452/2160 train_time:15168ms step_avg:33.56ms
step:453/2160 train_time:15201ms step_avg:33.56ms
step:454/2160 train_time:15234ms step_avg:33.56ms
step:455/2160 train_time:15268ms step_avg:33.56ms
step:456/2160 train_time:15300ms step_avg:33.55ms
step:457/2160 train_time:15334ms step_avg:33.55ms
step:458/2160 train_time:15367ms step_avg:33.55ms
step:459/2160 train_time:15401ms step_avg:33.55ms
step:460/2160 train_time:15434ms step_avg:33.55ms
step:461/2160 train_time:15467ms step_avg:33.55ms
step:462/2160 train_time:15500ms step_avg:33.55ms
step:463/2160 train_time:15534ms step_avg:33.55ms
step:464/2160 train_time:15567ms step_avg:33.55ms
step:465/2160 train_time:15601ms step_avg:33.55ms
step:466/2160 train_time:15634ms step_avg:33.55ms
step:467/2160 train_time:15667ms step_avg:33.55ms
step:468/2160 train_time:15700ms step_avg:33.55ms
step:469/2160 train_time:15734ms step_avg:33.55ms
step:470/2160 train_time:15767ms step_avg:33.55ms
step:471/2160 train_time:15800ms step_avg:33.55ms
step:472/2160 train_time:15833ms step_avg:33.54ms
step:473/2160 train_time:15867ms step_avg:33.55ms
step:474/2160 train_time:15900ms step_avg:33.54ms
step:475/2160 train_time:15934ms step_avg:33.54ms
step:476/2160 train_time:15966ms step_avg:33.54ms
step:477/2160 train_time:16000ms step_avg:33.54ms
step:478/2160 train_time:16033ms step_avg:33.54ms
step:479/2160 train_time:16067ms step_avg:33.54ms
step:480/2160 train_time:16100ms step_avg:33.54ms
step:481/2160 train_time:16133ms step_avg:33.54ms
step:482/2160 train_time:16166ms step_avg:33.54ms
step:483/2160 train_time:16200ms step_avg:33.54ms
step:484/2160 train_time:16232ms step_avg:33.54ms
step:485/2160 train_time:16266ms step_avg:33.54ms
step:486/2160 train_time:16299ms step_avg:33.54ms
step:487/2160 train_time:16332ms step_avg:33.54ms
step:488/2160 train_time:16365ms step_avg:33.54ms
step:489/2160 train_time:16399ms step_avg:33.54ms
step:490/2160 train_time:16432ms step_avg:33.53ms
step:491/2160 train_time:16465ms step_avg:33.53ms
step:492/2160 train_time:16498ms step_avg:33.53ms
step:493/2160 train_time:16532ms step_avg:33.53ms
step:494/2160 train_time:16565ms step_avg:33.53ms
step:495/2160 train_time:16598ms step_avg:33.53ms
step:496/2160 train_time:16631ms step_avg:33.53ms
step:497/2160 train_time:16665ms step_avg:33.53ms
step:498/2160 train_time:16698ms step_avg:33.53ms
step:499/2160 train_time:16731ms step_avg:33.53ms
step:500/2160 train_time:16764ms step_avg:33.53ms
step:500/2160 val_loss:4.0132 train_time:16799ms step_avg:33.60ms
step:501/2160 train_time:16819ms step_avg:33.57ms
step:502/2160 train_time:16839ms step_avg:33.54ms
step:503/2160 train_time:16871ms step_avg:33.54ms
step:504/2160 train_time:16905ms step_avg:33.54ms
step:505/2160 train_time:16941ms step_avg:33.55ms
step:506/2160 train_time:16975ms step_avg:33.55ms
step:507/2160 train_time:17010ms step_avg:33.55ms
step:508/2160 train_time:17043ms step_avg:33.55ms
step:509/2160 train_time:17077ms step_avg:33.55ms
step:510/2160 train_time:17110ms step_avg:33.55ms
step:511/2160 train_time:17144ms step_avg:33.55ms
step:512/2160 train_time:17177ms step_avg:33.55ms
step:513/2160 train_time:17210ms step_avg:33.55ms
step:514/2160 train_time:17243ms step_avg:33.55ms
step:515/2160 train_time:17277ms step_avg:33.55ms
step:516/2160 train_time:17310ms step_avg:33.55ms
step:517/2160 train_time:17343ms step_avg:33.55ms
step:518/2160 train_time:17376ms step_avg:33.54ms
step:519/2160 train_time:17410ms step_avg:33.54ms
step:520/2160 train_time:17442ms step_avg:33.54ms
step:521/2160 train_time:17477ms step_avg:33.54ms
step:522/2160 train_time:17509ms step_avg:33.54ms
step:523/2160 train_time:17543ms step_avg:33.54ms
step:524/2160 train_time:17575ms step_avg:33.54ms
step:525/2160 train_time:17609ms step_avg:33.54ms
step:526/2160 train_time:17642ms step_avg:33.54ms
step:527/2160 train_time:17675ms step_avg:33.54ms
step:528/2160 train_time:17708ms step_avg:33.54ms
step:529/2160 train_time:17742ms step_avg:33.54ms
step:530/2160 train_time:17774ms step_avg:33.54ms
step:531/2160 train_time:17808ms step_avg:33.54ms
step:532/2160 train_time:17841ms step_avg:33.54ms
step:533/2160 train_time:17875ms step_avg:33.54ms
step:534/2160 train_time:17908ms step_avg:33.53ms
step:535/2160 train_time:17942ms step_avg:33.54ms
step:536/2160 train_time:17975ms step_avg:33.54ms
step:537/2160 train_time:18009ms step_avg:33.54ms
step:538/2160 train_time:18041ms step_avg:33.53ms
step:539/2160 train_time:18076ms step_avg:33.54ms
step:540/2160 train_time:18109ms step_avg:33.53ms
step:541/2160 train_time:18143ms step_avg:33.54ms
step:542/2160 train_time:18175ms step_avg:33.53ms
step:543/2160 train_time:18210ms step_avg:33.54ms
step:544/2160 train_time:18243ms step_avg:33.53ms
step:545/2160 train_time:18276ms step_avg:33.53ms
step:546/2160 train_time:18309ms step_avg:33.53ms
step:547/2160 train_time:18343ms step_avg:33.53ms
step:548/2160 train_time:18376ms step_avg:33.53ms
step:549/2160 train_time:18410ms step_avg:33.53ms
step:550/2160 train_time:18442ms step_avg:33.53ms
step:551/2160 train_time:18476ms step_avg:33.53ms
step:552/2160 train_time:18509ms step_avg:33.53ms
step:553/2160 train_time:18542ms step_avg:33.53ms
step:554/2160 train_time:18575ms step_avg:33.53ms
step:555/2160 train_time:18609ms step_avg:33.53ms
step:556/2160 train_time:18642ms step_avg:33.53ms
step:557/2160 train_time:18676ms step_avg:33.53ms
step:558/2160 train_time:18708ms step_avg:33.53ms
step:559/2160 train_time:18742ms step_avg:33.53ms
step:560/2160 train_time:18775ms step_avg:33.53ms
step:561/2160 train_time:18808ms step_avg:33.53ms
step:562/2160 train_time:18841ms step_avg:33.53ms
step:563/2160 train_time:18875ms step_avg:33.53ms
step:564/2160 train_time:18908ms step_avg:33.52ms
step:565/2160 train_time:18942ms step_avg:33.52ms
step:566/2160 train_time:18974ms step_avg:33.52ms
step:567/2160 train_time:19008ms step_avg:33.52ms
step:568/2160 train_time:19041ms step_avg:33.52ms
step:569/2160 train_time:19075ms step_avg:33.52ms
step:570/2160 train_time:19108ms step_avg:33.52ms
step:571/2160 train_time:19141ms step_avg:33.52ms
step:572/2160 train_time:19174ms step_avg:33.52ms
step:573/2160 train_time:19208ms step_avg:33.52ms
step:574/2160 train_time:19241ms step_avg:33.52ms
step:575/2160 train_time:19275ms step_avg:33.52ms
step:576/2160 train_time:19308ms step_avg:33.52ms
step:577/2160 train_time:19342ms step_avg:33.52ms
step:578/2160 train_time:19374ms step_avg:33.52ms
step:579/2160 train_time:19408ms step_avg:33.52ms
step:580/2160 train_time:19441ms step_avg:33.52ms
step:581/2160 train_time:19475ms step_avg:33.52ms
step:582/2160 train_time:19508ms step_avg:33.52ms
step:583/2160 train_time:19541ms step_avg:33.52ms
step:584/2160 train_time:19574ms step_avg:33.52ms
step:585/2160 train_time:19608ms step_avg:33.52ms
step:586/2160 train_time:19640ms step_avg:33.52ms
step:587/2160 train_time:19674ms step_avg:33.52ms
step:588/2160 train_time:19707ms step_avg:33.52ms
step:589/2160 train_time:19740ms step_avg:33.52ms
step:590/2160 train_time:19773ms step_avg:33.51ms
step:591/2160 train_time:19807ms step_avg:33.51ms
step:592/2160 train_time:19840ms step_avg:33.51ms
step:593/2160 train_time:19874ms step_avg:33.51ms
step:594/2160 train_time:19906ms step_avg:33.51ms
step:595/2160 train_time:19940ms step_avg:33.51ms
step:596/2160 train_time:19973ms step_avg:33.51ms
step:597/2160 train_time:20007ms step_avg:33.51ms
step:598/2160 train_time:20040ms step_avg:33.51ms
step:599/2160 train_time:20074ms step_avg:33.51ms
step:600/2160 train_time:20107ms step_avg:33.51ms
step:601/2160 train_time:20140ms step_avg:33.51ms
step:602/2160 train_time:20173ms step_avg:33.51ms
step:603/2160 train_time:20207ms step_avg:33.51ms
step:604/2160 train_time:20240ms step_avg:33.51ms
step:605/2160 train_time:20274ms step_avg:33.51ms
step:606/2160 train_time:20307ms step_avg:33.51ms
step:607/2160 train_time:20340ms step_avg:33.51ms
step:608/2160 train_time:20373ms step_avg:33.51ms
step:609/2160 train_time:20407ms step_avg:33.51ms
step:610/2160 train_time:20440ms step_avg:33.51ms
step:611/2160 train_time:20474ms step_avg:33.51ms
step:612/2160 train_time:20507ms step_avg:33.51ms
step:613/2160 train_time:20540ms step_avg:33.51ms
step:614/2160 train_time:20573ms step_avg:33.51ms
step:615/2160 train_time:20607ms step_avg:33.51ms
step:616/2160 train_time:20640ms step_avg:33.51ms
step:617/2160 train_time:20674ms step_avg:33.51ms
step:618/2160 train_time:20706ms step_avg:33.51ms
step:619/2160 train_time:20740ms step_avg:33.51ms
step:620/2160 train_time:20773ms step_avg:33.50ms
step:621/2160 train_time:20806ms step_avg:33.50ms
step:622/2160 train_time:20839ms step_avg:33.50ms
step:623/2160 train_time:20874ms step_avg:33.50ms
step:624/2160 train_time:20906ms step_avg:33.50ms
step:625/2160 train_time:20940ms step_avg:33.50ms
step:626/2160 train_time:20973ms step_avg:33.50ms
step:627/2160 train_time:21007ms step_avg:33.50ms
step:628/2160 train_time:21040ms step_avg:33.50ms
step:629/2160 train_time:21073ms step_avg:33.50ms
step:630/2160 train_time:21106ms step_avg:33.50ms
step:631/2160 train_time:21140ms step_avg:33.50ms
step:632/2160 train_time:21172ms step_avg:33.50ms
step:633/2160 train_time:21206ms step_avg:33.50ms
step:634/2160 train_time:21239ms step_avg:33.50ms
step:635/2160 train_time:21273ms step_avg:33.50ms
step:636/2160 train_time:21305ms step_avg:33.50ms
step:637/2160 train_time:21339ms step_avg:33.50ms
step:638/2160 train_time:21372ms step_avg:33.50ms
step:639/2160 train_time:21406ms step_avg:33.50ms
step:640/2160 train_time:21439ms step_avg:33.50ms
step:641/2160 train_time:21473ms step_avg:33.50ms
step:642/2160 train_time:21506ms step_avg:33.50ms
step:643/2160 train_time:21540ms step_avg:33.50ms
step:644/2160 train_time:21572ms step_avg:33.50ms
step:645/2160 train_time:21606ms step_avg:33.50ms
step:646/2160 train_time:21639ms step_avg:33.50ms
step:647/2160 train_time:21672ms step_avg:33.50ms
step:648/2160 train_time:21705ms step_avg:33.50ms
step:649/2160 train_time:21739ms step_avg:33.50ms
step:650/2160 train_time:21772ms step_avg:33.50ms
step:651/2160 train_time:21806ms step_avg:33.50ms
step:652/2160 train_time:21838ms step_avg:33.49ms
step:653/2160 train_time:21872ms step_avg:33.50ms
step:654/2160 train_time:21906ms step_avg:33.49ms
step:655/2160 train_time:21939ms step_avg:33.49ms
step:656/2160 train_time:21972ms step_avg:33.49ms
step:657/2160 train_time:22006ms step_avg:33.49ms
step:658/2160 train_time:22039ms step_avg:33.49ms
step:659/2160 train_time:22072ms step_avg:33.49ms
step:660/2160 train_time:22106ms step_avg:33.49ms
step:661/2160 train_time:22139ms step_avg:33.49ms
step:662/2160 train_time:22172ms step_avg:33.49ms
step:663/2160 train_time:22206ms step_avg:33.49ms
step:664/2160 train_time:22239ms step_avg:33.49ms
step:665/2160 train_time:22272ms step_avg:33.49ms
step:666/2160 train_time:22305ms step_avg:33.49ms
step:667/2160 train_time:22339ms step_avg:33.49ms
step:668/2160 train_time:22372ms step_avg:33.49ms
step:669/2160 train_time:22405ms step_avg:33.49ms
step:670/2160 train_time:22438ms step_avg:33.49ms
step:671/2160 train_time:22472ms step_avg:33.49ms
step:672/2160 train_time:22505ms step_avg:33.49ms
step:673/2160 train_time:22539ms step_avg:33.49ms
step:674/2160 train_time:22572ms step_avg:33.49ms
step:675/2160 train_time:22605ms step_avg:33.49ms
step:676/2160 train_time:22638ms step_avg:33.49ms
step:677/2160 train_time:22672ms step_avg:33.49ms
step:678/2160 train_time:22705ms step_avg:33.49ms
step:679/2160 train_time:22739ms step_avg:33.49ms
step:680/2160 train_time:22771ms step_avg:33.49ms
step:681/2160 train_time:22805ms step_avg:33.49ms
step:682/2160 train_time:22838ms step_avg:33.49ms
step:683/2160 train_time:22872ms step_avg:33.49ms
step:684/2160 train_time:22904ms step_avg:33.49ms
step:685/2160 train_time:22938ms step_avg:33.49ms
step:686/2160 train_time:22971ms step_avg:33.49ms
step:687/2160 train_time:23005ms step_avg:33.49ms
step:688/2160 train_time:23038ms step_avg:33.48ms
step:689/2160 train_time:23071ms step_avg:33.49ms
step:690/2160 train_time:23104ms step_avg:33.48ms
step:691/2160 train_time:23138ms step_avg:33.48ms
step:692/2160 train_time:23171ms step_avg:33.48ms
step:693/2160 train_time:23204ms step_avg:33.48ms
step:694/2160 train_time:23237ms step_avg:33.48ms
step:695/2160 train_time:23271ms step_avg:33.48ms
step:696/2160 train_time:23304ms step_avg:33.48ms
step:697/2160 train_time:23338ms step_avg:33.48ms
step:698/2160 train_time:23371ms step_avg:33.48ms
step:699/2160 train_time:23404ms step_avg:33.48ms
step:700/2160 train_time:23437ms step_avg:33.48ms
step:701/2160 train_time:23471ms step_avg:33.48ms
step:702/2160 train_time:23503ms step_avg:33.48ms
step:703/2160 train_time:23537ms step_avg:33.48ms
step:704/2160 train_time:23570ms step_avg:33.48ms
step:705/2160 train_time:23604ms step_avg:33.48ms
step:706/2160 train_time:23637ms step_avg:33.48ms
step:707/2160 train_time:23671ms step_avg:33.48ms
step:708/2160 train_time:23704ms step_avg:33.48ms
step:709/2160 train_time:23764ms step_avg:33.52ms
step:710/2160 train_time:23822ms step_avg:33.55ms
step:711/2160 train_time:23883ms step_avg:33.59ms
step:712/2160 train_time:23942ms step_avg:33.63ms
step:713/2160 train_time:24002ms step_avg:33.66ms
step:714/2160 train_time:24061ms step_avg:33.70ms
step:715/2160 train_time:24122ms step_avg:33.74ms
step:716/2160 train_time:24182ms step_avg:33.77ms
step:717/2160 train_time:24243ms step_avg:33.81ms
step:718/2160 train_time:24301ms step_avg:33.85ms
step:719/2160 train_time:24362ms step_avg:33.88ms
step:720/2160 train_time:24422ms step_avg:33.92ms
step:721/2160 train_time:24483ms step_avg:33.96ms
step:722/2160 train_time:24542ms step_avg:33.99ms
step:723/2160 train_time:24603ms step_avg:34.03ms
step:724/2160 train_time:24662ms step_avg:34.06ms
step:725/2160 train_time:24723ms step_avg:34.10ms
step:726/2160 train_time:24782ms step_avg:34.13ms
step:727/2160 train_time:24842ms step_avg:34.17ms
step:728/2160 train_time:24900ms step_avg:34.20ms
step:729/2160 train_time:24961ms step_avg:34.24ms
step:730/2160 train_time:25020ms step_avg:34.27ms
step:731/2160 train_time:25081ms step_avg:34.31ms
step:732/2160 train_time:25140ms step_avg:34.34ms
step:733/2160 train_time:25201ms step_avg:34.38ms
step:734/2160 train_time:25259ms step_avg:34.41ms
step:735/2160 train_time:25320ms step_avg:34.45ms
step:736/2160 train_time:25379ms step_avg:34.48ms
step:737/2160 train_time:25440ms step_avg:34.52ms
step:738/2160 train_time:25499ms step_avg:34.55ms
step:739/2160 train_time:25560ms step_avg:34.59ms
step:740/2160 train_time:25619ms step_avg:34.62ms
step:741/2160 train_time:25680ms step_avg:34.66ms
step:742/2160 train_time:25738ms step_avg:34.69ms
step:743/2160 train_time:25799ms step_avg:34.72ms
step:744/2160 train_time:25858ms step_avg:34.76ms
step:745/2160 train_time:25919ms step_avg:34.79ms
step:746/2160 train_time:25978ms step_avg:34.82ms
step:747/2160 train_time:26039ms step_avg:34.86ms
step:748/2160 train_time:26098ms step_avg:34.89ms
step:749/2160 train_time:26159ms step_avg:34.93ms
step:750/2160 train_time:26218ms step_avg:34.96ms
step:750/2160 val_loss:3.8493 train_time:26280ms step_avg:35.04ms
step:751/2160 train_time:26300ms step_avg:35.02ms
step:752/2160 train_time:26342ms step_avg:35.03ms
step:753/2160 train_time:26404ms step_avg:35.06ms
step:754/2160 train_time:26465ms step_avg:35.10ms
step:755/2160 train_time:26526ms step_avg:35.13ms
step:756/2160 train_time:26586ms step_avg:35.17ms
step:757/2160 train_time:26645ms step_avg:35.20ms
step:758/2160 train_time:26704ms step_avg:35.23ms
step:759/2160 train_time:26763ms step_avg:35.26ms
step:760/2160 train_time:26821ms step_avg:35.29ms
step:761/2160 train_time:26881ms step_avg:35.32ms
step:762/2160 train_time:26939ms step_avg:35.35ms
step:763/2160 train_time:26999ms step_avg:35.39ms
step:764/2160 train_time:27057ms step_avg:35.42ms
step:765/2160 train_time:27117ms step_avg:35.45ms
step:766/2160 train_time:27176ms step_avg:35.48ms
step:767/2160 train_time:27236ms step_avg:35.51ms
step:768/2160 train_time:27296ms step_avg:35.54ms
step:769/2160 train_time:27357ms step_avg:35.57ms
step:770/2160 train_time:27416ms step_avg:35.61ms
step:771/2160 train_time:27478ms step_avg:35.64ms
step:772/2160 train_time:27537ms step_avg:35.67ms
step:773/2160 train_time:27598ms step_avg:35.70ms
step:774/2160 train_time:27657ms step_avg:35.73ms
step:775/2160 train_time:27718ms step_avg:35.77ms
step:776/2160 train_time:27777ms step_avg:35.80ms
step:777/2160 train_time:27838ms step_avg:35.83ms
step:778/2160 train_time:27896ms step_avg:35.86ms
step:779/2160 train_time:27956ms step_avg:35.89ms
step:780/2160 train_time:28015ms step_avg:35.92ms
step:781/2160 train_time:28075ms step_avg:35.95ms
step:782/2160 train_time:28133ms step_avg:35.98ms
step:783/2160 train_time:28193ms step_avg:36.01ms
step:784/2160 train_time:28252ms step_avg:36.04ms
step:785/2160 train_time:28313ms step_avg:36.07ms
step:786/2160 train_time:28373ms step_avg:36.10ms
step:787/2160 train_time:28434ms step_avg:36.13ms
step:788/2160 train_time:28494ms step_avg:36.16ms
step:789/2160 train_time:28555ms step_avg:36.19ms
step:790/2160 train_time:28615ms step_avg:36.22ms
step:791/2160 train_time:28676ms step_avg:36.25ms
step:792/2160 train_time:28735ms step_avg:36.28ms
step:793/2160 train_time:28796ms step_avg:36.31ms
step:794/2160 train_time:28855ms step_avg:36.34ms
step:795/2160 train_time:28915ms step_avg:36.37ms
step:796/2160 train_time:28974ms step_avg:36.40ms
step:797/2160 train_time:29035ms step_avg:36.43ms
step:798/2160 train_time:29093ms step_avg:36.46ms
step:799/2160 train_time:29154ms step_avg:36.49ms
step:800/2160 train_time:29212ms step_avg:36.51ms
step:801/2160 train_time:29272ms step_avg:36.54ms
step:802/2160 train_time:29331ms step_avg:36.57ms
step:803/2160 train_time:29392ms step_avg:36.60ms
step:804/2160 train_time:29451ms step_avg:36.63ms
step:805/2160 train_time:29512ms step_avg:36.66ms
step:806/2160 train_time:29572ms step_avg:36.69ms
step:807/2160 train_time:29634ms step_avg:36.72ms
step:808/2160 train_time:29693ms step_avg:36.75ms
step:809/2160 train_time:29754ms step_avg:36.78ms
step:810/2160 train_time:29812ms step_avg:36.81ms
step:811/2160 train_time:29873ms step_avg:36.83ms
step:812/2160 train_time:29932ms step_avg:36.86ms
step:813/2160 train_time:29993ms step_avg:36.89ms
step:814/2160 train_time:30052ms step_avg:36.92ms
step:815/2160 train_time:30113ms step_avg:36.95ms
step:816/2160 train_time:30171ms step_avg:36.97ms
step:817/2160 train_time:30232ms step_avg:37.00ms
step:818/2160 train_time:30291ms step_avg:37.03ms
step:819/2160 train_time:30351ms step_avg:37.06ms
step:820/2160 train_time:30410ms step_avg:37.09ms
step:821/2160 train_time:30471ms step_avg:37.11ms
step:822/2160 train_time:30530ms step_avg:37.14ms
step:823/2160 train_time:30592ms step_avg:37.17ms
step:824/2160 train_time:30651ms step_avg:37.20ms
step:825/2160 train_time:30712ms step_avg:37.23ms
step:826/2160 train_time:30771ms step_avg:37.25ms
step:827/2160 train_time:30831ms step_avg:37.28ms
step:828/2160 train_time:30890ms step_avg:37.31ms
step:829/2160 train_time:30951ms step_avg:37.34ms
step:830/2160 train_time:31010ms step_avg:37.36ms
step:831/2160 train_time:31070ms step_avg:37.39ms
step:832/2160 train_time:31129ms step_avg:37.41ms
step:833/2160 train_time:31190ms step_avg:37.44ms
step:834/2160 train_time:31248ms step_avg:37.47ms
step:835/2160 train_time:31309ms step_avg:37.50ms
step:836/2160 train_time:31367ms step_avg:37.52ms
step:837/2160 train_time:31428ms step_avg:37.55ms
step:838/2160 train_time:31486ms step_avg:37.57ms
step:839/2160 train_time:31547ms step_avg:37.60ms
step:840/2160 train_time:31607ms step_avg:37.63ms
step:841/2160 train_time:31668ms step_avg:37.66ms
step:842/2160 train_time:31727ms step_avg:37.68ms
step:843/2160 train_time:31788ms step_avg:37.71ms
step:844/2160 train_time:31846ms step_avg:37.73ms
step:845/2160 train_time:31907ms step_avg:37.76ms
step:846/2160 train_time:31966ms step_avg:37.79ms
step:847/2160 train_time:32027ms step_avg:37.81ms
step:848/2160 train_time:32086ms step_avg:37.84ms
step:849/2160 train_time:32146ms step_avg:37.86ms
step:850/2160 train_time:32205ms step_avg:37.89ms
step:851/2160 train_time:32266ms step_avg:37.91ms
step:852/2160 train_time:32325ms step_avg:37.94ms
step:853/2160 train_time:32384ms step_avg:37.97ms
step:854/2160 train_time:32443ms step_avg:37.99ms
step:855/2160 train_time:32503ms step_avg:38.02ms
step:856/2160 train_time:32562ms step_avg:38.04ms
step:857/2160 train_time:32623ms step_avg:38.07ms
step:858/2160 train_time:32682ms step_avg:38.09ms
step:859/2160 train_time:32743ms step_avg:38.12ms
step:860/2160 train_time:32802ms step_avg:38.14ms
step:861/2160 train_time:32863ms step_avg:38.17ms
step:862/2160 train_time:32921ms step_avg:38.19ms
step:863/2160 train_time:32982ms step_avg:38.22ms
step:864/2160 train_time:33041ms step_avg:38.24ms
step:865/2160 train_time:33102ms step_avg:38.27ms
step:866/2160 train_time:33160ms step_avg:38.29ms
step:867/2160 train_time:33221ms step_avg:38.32ms
step:868/2160 train_time:33279ms step_avg:38.34ms
step:869/2160 train_time:33340ms step_avg:38.37ms
step:870/2160 train_time:33398ms step_avg:38.39ms
step:871/2160 train_time:33458ms step_avg:38.41ms
step:872/2160 train_time:33517ms step_avg:38.44ms
step:873/2160 train_time:33578ms step_avg:38.46ms
step:874/2160 train_time:33636ms step_avg:38.49ms
step:875/2160 train_time:33697ms step_avg:38.51ms
step:876/2160 train_time:33756ms step_avg:38.53ms
step:877/2160 train_time:33817ms step_avg:38.56ms
step:878/2160 train_time:33876ms step_avg:38.58ms
step:879/2160 train_time:33937ms step_avg:38.61ms
step:880/2160 train_time:33996ms step_avg:38.63ms
step:881/2160 train_time:34057ms step_avg:38.66ms
step:882/2160 train_time:34116ms step_avg:38.68ms
step:883/2160 train_time:34177ms step_avg:38.71ms
step:884/2160 train_time:34236ms step_avg:38.73ms
step:885/2160 train_time:34296ms step_avg:38.75ms
step:886/2160 train_time:34355ms step_avg:38.78ms
step:887/2160 train_time:34416ms step_avg:38.80ms
step:888/2160 train_time:34474ms step_avg:38.82ms
step:889/2160 train_time:34536ms step_avg:38.85ms
step:890/2160 train_time:34595ms step_avg:38.87ms
step:891/2160 train_time:34655ms step_avg:38.89ms
step:892/2160 train_time:34714ms step_avg:38.92ms
step:893/2160 train_time:34775ms step_avg:38.94ms
step:894/2160 train_time:34834ms step_avg:38.96ms
step:895/2160 train_time:34895ms step_avg:38.99ms
step:896/2160 train_time:34954ms step_avg:39.01ms
step:897/2160 train_time:35015ms step_avg:39.04ms
step:898/2160 train_time:35074ms step_avg:39.06ms
step:899/2160 train_time:35135ms step_avg:39.08ms
step:900/2160 train_time:35194ms step_avg:39.10ms
step:901/2160 train_time:35254ms step_avg:39.13ms
step:902/2160 train_time:35313ms step_avg:39.15ms
step:903/2160 train_time:35374ms step_avg:39.17ms
step:904/2160 train_time:35433ms step_avg:39.20ms
step:905/2160 train_time:35494ms step_avg:39.22ms
step:906/2160 train_time:35553ms step_avg:39.24ms
step:907/2160 train_time:35613ms step_avg:39.26ms
step:908/2160 train_time:35673ms step_avg:39.29ms
step:909/2160 train_time:35734ms step_avg:39.31ms
step:910/2160 train_time:35793ms step_avg:39.33ms
step:911/2160 train_time:35853ms step_avg:39.36ms
step:912/2160 train_time:35912ms step_avg:39.38ms
step:913/2160 train_time:35974ms step_avg:39.40ms
step:914/2160 train_time:36033ms step_avg:39.42ms
step:915/2160 train_time:36094ms step_avg:39.45ms
step:916/2160 train_time:36153ms step_avg:39.47ms
step:917/2160 train_time:36213ms step_avg:39.49ms
step:918/2160 train_time:36273ms step_avg:39.51ms
step:919/2160 train_time:36334ms step_avg:39.54ms
step:920/2160 train_time:36393ms step_avg:39.56ms
step:921/2160 train_time:36453ms step_avg:39.58ms
step:922/2160 train_time:36512ms step_avg:39.60ms
step:923/2160 train_time:36573ms step_avg:39.62ms
step:924/2160 train_time:36632ms step_avg:39.64ms
step:925/2160 train_time:36692ms step_avg:39.67ms
step:926/2160 train_time:36751ms step_avg:39.69ms
step:927/2160 train_time:36812ms step_avg:39.71ms
step:928/2160 train_time:36871ms step_avg:39.73ms
step:929/2160 train_time:36932ms step_avg:39.75ms
step:930/2160 train_time:36990ms step_avg:39.77ms
step:931/2160 train_time:37051ms step_avg:39.80ms
step:932/2160 train_time:37109ms step_avg:39.82ms
step:933/2160 train_time:37170ms step_avg:39.84ms
step:934/2160 train_time:37229ms step_avg:39.86ms
step:935/2160 train_time:37291ms step_avg:39.88ms
step:936/2160 train_time:37350ms step_avg:39.90ms
step:937/2160 train_time:37411ms step_avg:39.93ms
step:938/2160 train_time:37470ms step_avg:39.95ms
step:939/2160 train_time:37530ms step_avg:39.97ms
step:940/2160 train_time:37589ms step_avg:39.99ms
step:941/2160 train_time:37650ms step_avg:40.01ms
step:942/2160 train_time:37709ms step_avg:40.03ms
step:943/2160 train_time:37769ms step_avg:40.05ms
step:944/2160 train_time:37828ms step_avg:40.07ms
step:945/2160 train_time:37889ms step_avg:40.09ms
step:946/2160 train_time:37947ms step_avg:40.11ms
step:947/2160 train_time:38008ms step_avg:40.13ms
step:948/2160 train_time:38066ms step_avg:40.15ms
step:949/2160 train_time:38127ms step_avg:40.18ms
step:950/2160 train_time:38186ms step_avg:40.20ms
step:951/2160 train_time:38246ms step_avg:40.22ms
step:952/2160 train_time:38306ms step_avg:40.24ms
step:953/2160 train_time:38367ms step_avg:40.26ms
step:954/2160 train_time:38425ms step_avg:40.28ms
step:955/2160 train_time:38486ms step_avg:40.30ms
step:956/2160 train_time:38545ms step_avg:40.32ms
step:957/2160 train_time:38606ms step_avg:40.34ms
step:958/2160 train_time:38665ms step_avg:40.36ms
step:959/2160 train_time:38726ms step_avg:40.38ms
step:960/2160 train_time:38785ms step_avg:40.40ms
step:961/2160 train_time:38845ms step_avg:40.42ms
step:962/2160 train_time:38905ms step_avg:40.44ms
step:963/2160 train_time:38965ms step_avg:40.46ms
step:964/2160 train_time:39023ms step_avg:40.48ms
step:965/2160 train_time:39083ms step_avg:40.50ms
step:966/2160 train_time:39142ms step_avg:40.52ms
step:967/2160 train_time:39202ms step_avg:40.54ms
step:968/2160 train_time:39261ms step_avg:40.56ms
step:969/2160 train_time:39322ms step_avg:40.58ms
step:970/2160 train_time:39381ms step_avg:40.60ms
step:971/2160 train_time:39442ms step_avg:40.62ms
step:972/2160 train_time:39500ms step_avg:40.64ms
step:973/2160 train_time:39561ms step_avg:40.66ms
step:974/2160 train_time:39620ms step_avg:40.68ms
step:975/2160 train_time:39681ms step_avg:40.70ms
step:976/2160 train_time:39740ms step_avg:40.72ms
step:977/2160 train_time:39801ms step_avg:40.74ms
step:978/2160 train_time:39859ms step_avg:40.76ms
step:979/2160 train_time:39920ms step_avg:40.78ms
step:980/2160 train_time:39978ms step_avg:40.79ms
step:981/2160 train_time:40039ms step_avg:40.81ms
step:982/2160 train_time:40097ms step_avg:40.83ms
step:983/2160 train_time:40158ms step_avg:40.85ms
step:984/2160 train_time:40217ms step_avg:40.87ms
step:985/2160 train_time:40278ms step_avg:40.89ms
step:986/2160 train_time:40337ms step_avg:40.91ms
step:987/2160 train_time:40398ms step_avg:40.93ms
step:988/2160 train_time:40456ms step_avg:40.95ms
step:989/2160 train_time:40517ms step_avg:40.97ms
step:990/2160 train_time:40576ms step_avg:40.99ms
step:991/2160 train_time:40637ms step_avg:41.01ms
step:992/2160 train_time:40696ms step_avg:41.02ms
step:993/2160 train_time:40756ms step_avg:41.04ms
step:994/2160 train_time:40815ms step_avg:41.06ms
step:995/2160 train_time:40876ms step_avg:41.08ms
step:996/2160 train_time:40935ms step_avg:41.10ms
step:997/2160 train_time:40996ms step_avg:41.12ms
step:998/2160 train_time:41055ms step_avg:41.14ms
step:999/2160 train_time:41115ms step_avg:41.16ms
step:1000/2160 train_time:41174ms step_avg:41.17ms
step:1000/2160 val_loss:3.6903 train_time:41235ms step_avg:41.24ms
step:1001/2160 train_time:41255ms step_avg:41.21ms
step:1002/2160 train_time:41297ms step_avg:41.21ms
step:1003/2160 train_time:41359ms step_avg:41.24ms
step:1004/2160 train_time:41422ms step_avg:41.26ms
step:1005/2160 train_time:41483ms step_avg:41.28ms
step:1006/2160 train_time:41541ms step_avg:41.29ms
step:1007/2160 train_time:41602ms step_avg:41.31ms
step:1008/2160 train_time:41660ms step_avg:41.33ms
step:1009/2160 train_time:41720ms step_avg:41.35ms
step:1010/2160 train_time:41778ms step_avg:41.36ms
step:1011/2160 train_time:41838ms step_avg:41.38ms
step:1012/2160 train_time:41896ms step_avg:41.40ms
step:1013/2160 train_time:41955ms step_avg:41.42ms
step:1014/2160 train_time:42014ms step_avg:41.43ms
step:1015/2160 train_time:42074ms step_avg:41.45ms
step:1016/2160 train_time:42132ms step_avg:41.47ms
step:1017/2160 train_time:42194ms step_avg:41.49ms
step:1018/2160 train_time:42254ms step_avg:41.51ms
step:1019/2160 train_time:42316ms step_avg:41.53ms
step:1020/2160 train_time:42376ms step_avg:41.55ms
step:1021/2160 train_time:42438ms step_avg:41.57ms
step:1022/2160 train_time:42497ms step_avg:41.58ms
step:1023/2160 train_time:42559ms step_avg:41.60ms
step:1024/2160 train_time:42617ms step_avg:41.62ms
step:1025/2160 train_time:42678ms step_avg:41.64ms
step:1026/2160 train_time:42736ms step_avg:41.65ms
step:1027/2160 train_time:42796ms step_avg:41.67ms
step:1028/2160 train_time:42854ms step_avg:41.69ms
step:1029/2160 train_time:42914ms step_avg:41.70ms
step:1030/2160 train_time:42972ms step_avg:41.72ms
step:1031/2160 train_time:43032ms step_avg:41.74ms
step:1032/2160 train_time:43090ms step_avg:41.75ms
step:1033/2160 train_time:43151ms step_avg:41.77ms
step:1034/2160 train_time:43210ms step_avg:41.79ms
step:1035/2160 train_time:43271ms step_avg:41.81ms
step:1036/2160 train_time:43330ms step_avg:41.82ms
step:1037/2160 train_time:43391ms step_avg:41.84ms
step:1038/2160 train_time:43451ms step_avg:41.86ms
step:1039/2160 train_time:43512ms step_avg:41.88ms
step:1040/2160 train_time:43570ms step_avg:41.89ms
step:1041/2160 train_time:43631ms step_avg:41.91ms
step:1042/2160 train_time:43690ms step_avg:41.93ms
step:1043/2160 train_time:43750ms step_avg:41.95ms
step:1044/2160 train_time:43809ms step_avg:41.96ms
step:1045/2160 train_time:43869ms step_avg:41.98ms
step:1046/2160 train_time:43928ms step_avg:42.00ms
step:1047/2160 train_time:43988ms step_avg:42.01ms
step:1048/2160 train_time:44046ms step_avg:42.03ms
step:1049/2160 train_time:44106ms step_avg:42.05ms
step:1050/2160 train_time:44165ms step_avg:42.06ms
step:1051/2160 train_time:44226ms step_avg:42.08ms
step:1052/2160 train_time:44285ms step_avg:42.10ms
step:1053/2160 train_time:44346ms step_avg:42.11ms
step:1054/2160 train_time:44405ms step_avg:42.13ms
step:1055/2160 train_time:44467ms step_avg:42.15ms
step:1056/2160 train_time:44526ms step_avg:42.16ms
step:1057/2160 train_time:44587ms step_avg:42.18ms
step:1058/2160 train_time:44646ms step_avg:42.20ms
step:1059/2160 train_time:44706ms step_avg:42.22ms
step:1060/2160 train_time:44765ms step_avg:42.23ms
step:1061/2160 train_time:44826ms step_avg:42.25ms
step:1062/2160 train_time:44884ms step_avg:42.26ms
step:1063/2160 train_time:44945ms step_avg:42.28ms
step:1064/2160 train_time:45003ms step_avg:42.30ms
step:1065/2160 train_time:45063ms step_avg:42.31ms
step:1066/2160 train_time:45122ms step_avg:42.33ms
step:1067/2160 train_time:45183ms step_avg:42.35ms
step:1068/2160 train_time:45242ms step_avg:42.36ms
step:1069/2160 train_time:45303ms step_avg:42.38ms
step:1070/2160 train_time:45362ms step_avg:42.39ms
step:1071/2160 train_time:45423ms step_avg:42.41ms
step:1072/2160 train_time:45482ms step_avg:42.43ms
step:1073/2160 train_time:45543ms step_avg:42.44ms
step:1074/2160 train_time:45602ms step_avg:42.46ms
step:1075/2160 train_time:45663ms step_avg:42.48ms
step:1076/2160 train_time:45721ms step_avg:42.49ms
step:1077/2160 train_time:45782ms step_avg:42.51ms
step:1078/2160 train_time:45841ms step_avg:42.52ms
step:1079/2160 train_time:45901ms step_avg:42.54ms
step:1080/2160 train_time:45961ms step_avg:42.56ms
step:1081/2160 train_time:46021ms step_avg:42.57ms
step:1082/2160 train_time:46080ms step_avg:42.59ms
step:1083/2160 train_time:46141ms step_avg:42.60ms
step:1084/2160 train_time:46199ms step_avg:42.62ms
step:1085/2160 train_time:46260ms step_avg:42.64ms
step:1086/2160 train_time:46319ms step_avg:42.65ms
step:1087/2160 train_time:46380ms step_avg:42.67ms
step:1088/2160 train_time:46439ms step_avg:42.68ms
step:1089/2160 train_time:46499ms step_avg:42.70ms
step:1090/2160 train_time:46558ms step_avg:42.71ms
step:1091/2160 train_time:46619ms step_avg:42.73ms
step:1092/2160 train_time:46678ms step_avg:42.75ms
step:1093/2160 train_time:46739ms step_avg:42.76ms
step:1094/2160 train_time:46798ms step_avg:42.78ms
step:1095/2160 train_time:46858ms step_avg:42.79ms
step:1096/2160 train_time:46917ms step_avg:42.81ms
step:1097/2160 train_time:46978ms step_avg:42.82ms
step:1098/2160 train_time:47037ms step_avg:42.84ms
step:1099/2160 train_time:47097ms step_avg:42.85ms
step:1100/2160 train_time:47156ms step_avg:42.87ms
step:1101/2160 train_time:47217ms step_avg:42.89ms
step:1102/2160 train_time:47275ms step_avg:42.90ms
step:1103/2160 train_time:47336ms step_avg:42.92ms
step:1104/2160 train_time:47395ms step_avg:42.93ms
step:1105/2160 train_time:47457ms step_avg:42.95ms
step:1106/2160 train_time:47516ms step_avg:42.96ms
step:1107/2160 train_time:47576ms step_avg:42.98ms
step:1108/2160 train_time:47635ms step_avg:42.99ms
step:1109/2160 train_time:47695ms step_avg:43.01ms
step:1110/2160 train_time:47754ms step_avg:43.02ms
step:1111/2160 train_time:47815ms step_avg:43.04ms
step:1112/2160 train_time:47873ms step_avg:43.05ms
step:1113/2160 train_time:47934ms step_avg:43.07ms
step:1114/2160 train_time:47993ms step_avg:43.08ms
step:1115/2160 train_time:48053ms step_avg:43.10ms
step:1116/2160 train_time:48112ms step_avg:43.11ms
step:1117/2160 train_time:48173ms step_avg:43.13ms
step:1118/2160 train_time:48231ms step_avg:43.14ms
step:1119/2160 train_time:48292ms step_avg:43.16ms
step:1120/2160 train_time:48350ms step_avg:43.17ms
step:1121/2160 train_time:48411ms step_avg:43.19ms
step:1122/2160 train_time:48470ms step_avg:43.20ms
step:1123/2160 train_time:48530ms step_avg:43.21ms
step:1124/2160 train_time:48589ms step_avg:43.23ms
step:1125/2160 train_time:48650ms step_avg:43.24ms
step:1126/2160 train_time:48708ms step_avg:43.26ms
step:1127/2160 train_time:48769ms step_avg:43.27ms
step:1128/2160 train_time:48828ms step_avg:43.29ms
step:1129/2160 train_time:48888ms step_avg:43.30ms
step:1130/2160 train_time:48947ms step_avg:43.32ms
step:1131/2160 train_time:49007ms step_avg:43.33ms
step:1132/2160 train_time:49066ms step_avg:43.34ms
step:1133/2160 train_time:49127ms step_avg:43.36ms
step:1134/2160 train_time:49186ms step_avg:43.37ms
step:1135/2160 train_time:49246ms step_avg:43.39ms
step:1136/2160 train_time:49304ms step_avg:43.40ms
step:1137/2160 train_time:49365ms step_avg:43.42ms
step:1138/2160 train_time:49424ms step_avg:43.43ms
step:1139/2160 train_time:49485ms step_avg:43.45ms
step:1140/2160 train_time:49544ms step_avg:43.46ms
step:1141/2160 train_time:49604ms step_avg:43.47ms
step:1142/2160 train_time:49664ms step_avg:43.49ms
step:1143/2160 train_time:49724ms step_avg:43.50ms
step:1144/2160 train_time:49783ms step_avg:43.52ms
step:1145/2160 train_time:49844ms step_avg:43.53ms
step:1146/2160 train_time:49902ms step_avg:43.54ms
step:1147/2160 train_time:49964ms step_avg:43.56ms
step:1148/2160 train_time:50022ms step_avg:43.57ms
step:1149/2160 train_time:50084ms step_avg:43.59ms
step:1150/2160 train_time:50142ms step_avg:43.60ms
step:1151/2160 train_time:50203ms step_avg:43.62ms
step:1152/2160 train_time:50262ms step_avg:43.63ms
step:1153/2160 train_time:50322ms step_avg:43.64ms
step:1154/2160 train_time:50381ms step_avg:43.66ms
step:1155/2160 train_time:50442ms step_avg:43.67ms
step:1156/2160 train_time:50500ms step_avg:43.69ms
step:1157/2160 train_time:50561ms step_avg:43.70ms
step:1158/2160 train_time:50620ms step_avg:43.71ms
step:1159/2160 train_time:50681ms step_avg:43.73ms
step:1160/2160 train_time:50739ms step_avg:43.74ms
step:1161/2160 train_time:50800ms step_avg:43.76ms
step:1162/2160 train_time:50860ms step_avg:43.77ms
step:1163/2160 train_time:50921ms step_avg:43.78ms
step:1164/2160 train_time:50980ms step_avg:43.80ms
step:1165/2160 train_time:51041ms step_avg:43.81ms
step:1166/2160 train_time:51100ms step_avg:43.82ms
step:1167/2160 train_time:51161ms step_avg:43.84ms
step:1168/2160 train_time:51220ms step_avg:43.85ms
step:1169/2160 train_time:51280ms step_avg:43.87ms
step:1170/2160 train_time:51339ms step_avg:43.88ms
step:1171/2160 train_time:51399ms step_avg:43.89ms
step:1172/2160 train_time:51458ms step_avg:43.91ms
step:1173/2160 train_time:51518ms step_avg:43.92ms
step:1174/2160 train_time:51578ms step_avg:43.93ms
step:1175/2160 train_time:51638ms step_avg:43.95ms
step:1176/2160 train_time:51697ms step_avg:43.96ms
step:1177/2160 train_time:51758ms step_avg:43.97ms
step:1178/2160 train_time:51817ms step_avg:43.99ms
step:1179/2160 train_time:51878ms step_avg:44.00ms
step:1180/2160 train_time:51937ms step_avg:44.01ms
step:1181/2160 train_time:51998ms step_avg:44.03ms
step:1182/2160 train_time:52057ms step_avg:44.04ms
step:1183/2160 train_time:52118ms step_avg:44.06ms
step:1184/2160 train_time:52177ms step_avg:44.07ms
step:1185/2160 train_time:52238ms step_avg:44.08ms
step:1186/2160 train_time:52297ms step_avg:44.10ms
step:1187/2160 train_time:52357ms step_avg:44.11ms
step:1188/2160 train_time:52416ms step_avg:44.12ms
step:1189/2160 train_time:52476ms step_avg:44.13ms
step:1190/2160 train_time:52535ms step_avg:44.15ms
step:1191/2160 train_time:52596ms step_avg:44.16ms
step:1192/2160 train_time:52655ms step_avg:44.17ms
step:1193/2160 train_time:52716ms step_avg:44.19ms
step:1194/2160 train_time:52775ms step_avg:44.20ms
step:1195/2160 train_time:52835ms step_avg:44.21ms
step:1196/2160 train_time:52894ms step_avg:44.23ms
step:1197/2160 train_time:52955ms step_avg:44.24ms
step:1198/2160 train_time:53014ms step_avg:44.25ms
step:1199/2160 train_time:53075ms step_avg:44.27ms
step:1200/2160 train_time:53134ms step_avg:44.28ms
step:1201/2160 train_time:53195ms step_avg:44.29ms
step:1202/2160 train_time:53253ms step_avg:44.30ms
step:1203/2160 train_time:53314ms step_avg:44.32ms
step:1204/2160 train_time:53373ms step_avg:44.33ms
step:1205/2160 train_time:53433ms step_avg:44.34ms
step:1206/2160 train_time:53492ms step_avg:44.35ms
step:1207/2160 train_time:53552ms step_avg:44.37ms
step:1208/2160 train_time:53611ms step_avg:44.38ms
step:1209/2160 train_time:53672ms step_avg:44.39ms
step:1210/2160 train_time:53730ms step_avg:44.41ms
step:1211/2160 train_time:53791ms step_avg:44.42ms
step:1212/2160 train_time:53850ms step_avg:44.43ms
step:1213/2160 train_time:53910ms step_avg:44.44ms
step:1214/2160 train_time:53969ms step_avg:44.46ms
step:1215/2160 train_time:54031ms step_avg:44.47ms
step:1216/2160 train_time:54089ms step_avg:44.48ms
step:1217/2160 train_time:54150ms step_avg:44.49ms
step:1218/2160 train_time:54208ms step_avg:44.51ms
step:1219/2160 train_time:54269ms step_avg:44.52ms
step:1220/2160 train_time:54328ms step_avg:44.53ms
step:1221/2160 train_time:54389ms step_avg:44.54ms
step:1222/2160 train_time:54448ms step_avg:44.56ms
step:1223/2160 train_time:54508ms step_avg:44.57ms
step:1224/2160 train_time:54566ms step_avg:44.58ms
step:1225/2160 train_time:54627ms step_avg:44.59ms
step:1226/2160 train_time:54686ms step_avg:44.61ms
step:1227/2160 train_time:54747ms step_avg:44.62ms
step:1228/2160 train_time:54805ms step_avg:44.63ms
step:1229/2160 train_time:54866ms step_avg:44.64ms
step:1230/2160 train_time:54925ms step_avg:44.65ms
step:1231/2160 train_time:54986ms step_avg:44.67ms
step:1232/2160 train_time:55045ms step_avg:44.68ms
step:1233/2160 train_time:55107ms step_avg:44.69ms
step:1234/2160 train_time:55166ms step_avg:44.70ms
step:1235/2160 train_time:55227ms step_avg:44.72ms
step:1236/2160 train_time:55285ms step_avg:44.73ms
step:1237/2160 train_time:55346ms step_avg:44.74ms
step:1238/2160 train_time:55405ms step_avg:44.75ms
step:1239/2160 train_time:55465ms step_avg:44.77ms
step:1240/2160 train_time:55524ms step_avg:44.78ms
step:1241/2160 train_time:55586ms step_avg:44.79ms
step:1242/2160 train_time:55645ms step_avg:44.80ms
step:1243/2160 train_time:55705ms step_avg:44.81ms
step:1244/2160 train_time:55764ms step_avg:44.83ms
step:1245/2160 train_time:55824ms step_avg:44.84ms
step:1246/2160 train_time:55883ms step_avg:44.85ms
step:1247/2160 train_time:55944ms step_avg:44.86ms
step:1248/2160 train_time:56003ms step_avg:44.87ms
step:1249/2160 train_time:56064ms step_avg:44.89ms
step:1250/2160 train_time:56123ms step_avg:44.90ms
step:1250/2160 val_loss:3.5729 train_time:56185ms step_avg:44.95ms
step:1251/2160 train_time:56205ms step_avg:44.93ms
step:1252/2160 train_time:56247ms step_avg:44.93ms
step:1253/2160 train_time:56310ms step_avg:44.94ms
step:1254/2160 train_time:56370ms step_avg:44.95ms
step:1255/2160 train_time:56431ms step_avg:44.96ms
step:1256/2160 train_time:56489ms step_avg:44.98ms
step:1257/2160 train_time:56550ms step_avg:44.99ms
step:1258/2160 train_time:56608ms step_avg:45.00ms
step:1259/2160 train_time:56668ms step_avg:45.01ms
step:1260/2160 train_time:56725ms step_avg:45.02ms
step:1261/2160 train_time:56786ms step_avg:45.03ms
step:1262/2160 train_time:56844ms step_avg:45.04ms
step:1263/2160 train_time:56904ms step_avg:45.05ms
step:1264/2160 train_time:56962ms step_avg:45.06ms
step:1265/2160 train_time:57022ms step_avg:45.08ms
step:1266/2160 train_time:57080ms step_avg:45.09ms
step:1267/2160 train_time:57142ms step_avg:45.10ms
step:1268/2160 train_time:57202ms step_avg:45.11ms
step:1269/2160 train_time:57264ms step_avg:45.12ms
step:1270/2160 train_time:57323ms step_avg:45.14ms
step:1271/2160 train_time:57386ms step_avg:45.15ms
step:1272/2160 train_time:57445ms step_avg:45.16ms
step:1273/2160 train_time:57505ms step_avg:45.17ms
step:1274/2160 train_time:57563ms step_avg:45.18ms
step:1275/2160 train_time:57624ms step_avg:45.20ms
step:1276/2160 train_time:57682ms step_avg:45.21ms
step:1277/2160 train_time:57743ms step_avg:45.22ms
step:1278/2160 train_time:57801ms step_avg:45.23ms
step:1279/2160 train_time:57861ms step_avg:45.24ms
step:1280/2160 train_time:57919ms step_avg:45.25ms
step:1281/2160 train_time:57980ms step_avg:45.26ms
step:1282/2160 train_time:58038ms step_avg:45.27ms
step:1283/2160 train_time:58098ms step_avg:45.28ms
step:1284/2160 train_time:58157ms step_avg:45.29ms
step:1285/2160 train_time:58218ms step_avg:45.31ms
step:1286/2160 train_time:58278ms step_avg:45.32ms
step:1287/2160 train_time:58339ms step_avg:45.33ms
step:1288/2160 train_time:58399ms step_avg:45.34ms
step:1289/2160 train_time:58460ms step_avg:45.35ms
step:1290/2160 train_time:58519ms step_avg:45.36ms
step:1291/2160 train_time:58580ms step_avg:45.38ms
step:1292/2160 train_time:58639ms step_avg:45.39ms
step:1293/2160 train_time:58700ms step_avg:45.40ms
step:1294/2160 train_time:58759ms step_avg:45.41ms
step:1295/2160 train_time:58819ms step_avg:45.42ms
step:1296/2160 train_time:58878ms step_avg:45.43ms
step:1297/2160 train_time:58938ms step_avg:45.44ms
step:1298/2160 train_time:58996ms step_avg:45.45ms
step:1299/2160 train_time:59057ms step_avg:45.46ms
step:1300/2160 train_time:59116ms step_avg:45.47ms
step:1301/2160 train_time:59177ms step_avg:45.49ms
step:1302/2160 train_time:59236ms step_avg:45.50ms
step:1303/2160 train_time:59297ms step_avg:45.51ms
step:1304/2160 train_time:59356ms step_avg:45.52ms
step:1305/2160 train_time:59418ms step_avg:45.53ms
step:1306/2160 train_time:59478ms step_avg:45.54ms
step:1307/2160 train_time:59538ms step_avg:45.55ms
step:1308/2160 train_time:59597ms step_avg:45.56ms
step:1309/2160 train_time:59658ms step_avg:45.58ms
step:1310/2160 train_time:59717ms step_avg:45.59ms
step:1311/2160 train_time:59778ms step_avg:45.60ms
step:1312/2160 train_time:59837ms step_avg:45.61ms
step:1313/2160 train_time:59897ms step_avg:45.62ms
step:1314/2160 train_time:59955ms step_avg:45.63ms
step:1315/2160 train_time:60016ms step_avg:45.64ms
step:1316/2160 train_time:60075ms step_avg:45.65ms
step:1317/2160 train_time:60136ms step_avg:45.66ms
step:1318/2160 train_time:60195ms step_avg:45.67ms
step:1319/2160 train_time:60256ms step_avg:45.68ms
step:1320/2160 train_time:60315ms step_avg:45.69ms
step:1321/2160 train_time:60377ms step_avg:45.71ms
step:1322/2160 train_time:60436ms step_avg:45.72ms
step:1323/2160 train_time:60498ms step_avg:45.73ms
step:1324/2160 train_time:60556ms step_avg:45.74ms
step:1325/2160 train_time:60617ms step_avg:45.75ms
step:1326/2160 train_time:60676ms step_avg:45.76ms
step:1327/2160 train_time:60737ms step_avg:45.77ms
step:1328/2160 train_time:60796ms step_avg:45.78ms
step:1329/2160 train_time:60857ms step_avg:45.79ms
step:1330/2160 train_time:60915ms step_avg:45.80ms
step:1331/2160 train_time:60976ms step_avg:45.81ms
step:1332/2160 train_time:61035ms step_avg:45.82ms
step:1333/2160 train_time:61095ms step_avg:45.83ms
step:1334/2160 train_time:61154ms step_avg:45.84ms
step:1335/2160 train_time:61214ms step_avg:45.85ms
step:1336/2160 train_time:61273ms step_avg:45.86ms
step:1337/2160 train_time:61334ms step_avg:45.87ms
step:1338/2160 train_time:61393ms step_avg:45.88ms
step:1339/2160 train_time:61454ms step_avg:45.90ms
step:1340/2160 train_time:61513ms step_avg:45.91ms
step:1341/2160 train_time:61575ms step_avg:45.92ms
step:1342/2160 train_time:61634ms step_avg:45.93ms
step:1343/2160 train_time:61695ms step_avg:45.94ms
step:1344/2160 train_time:61753ms step_avg:45.95ms
step:1345/2160 train_time:61814ms step_avg:45.96ms
step:1346/2160 train_time:61872ms step_avg:45.97ms
step:1347/2160 train_time:61933ms step_avg:45.98ms
step:1348/2160 train_time:61992ms step_avg:45.99ms
step:1349/2160 train_time:62052ms step_avg:46.00ms
step:1350/2160 train_time:62111ms step_avg:46.01ms
step:1351/2160 train_time:62171ms step_avg:46.02ms
step:1352/2160 train_time:62230ms step_avg:46.03ms
step:1353/2160 train_time:62290ms step_avg:46.04ms
step:1354/2160 train_time:62349ms step_avg:46.05ms
step:1355/2160 train_time:62410ms step_avg:46.06ms
step:1356/2160 train_time:62469ms step_avg:46.07ms
step:1357/2160 train_time:62530ms step_avg:46.08ms
step:1358/2160 train_time:62589ms step_avg:46.09ms
step:1359/2160 train_time:62650ms step_avg:46.10ms
step:1360/2160 train_time:62710ms step_avg:46.11ms
step:1361/2160 train_time:62770ms step_avg:46.12ms
step:1362/2160 train_time:62829ms step_avg:46.13ms
step:1363/2160 train_time:62890ms step_avg:46.14ms
step:1364/2160 train_time:62949ms step_avg:46.15ms
step:1365/2160 train_time:63009ms step_avg:46.16ms
step:1366/2160 train_time:63069ms step_avg:46.17ms
step:1367/2160 train_time:63128ms step_avg:46.18ms
step:1368/2160 train_time:63187ms step_avg:46.19ms
step:1369/2160 train_time:63247ms step_avg:46.20ms
step:1370/2160 train_time:63306ms step_avg:46.21ms
step:1371/2160 train_time:63366ms step_avg:46.22ms
step:1372/2160 train_time:63425ms step_avg:46.23ms
step:1373/2160 train_time:63486ms step_avg:46.24ms
step:1374/2160 train_time:63545ms step_avg:46.25ms
step:1375/2160 train_time:63606ms step_avg:46.26ms
step:1376/2160 train_time:63666ms step_avg:46.27ms
step:1377/2160 train_time:63726ms step_avg:46.28ms
step:1378/2160 train_time:63785ms step_avg:46.29ms
step:1379/2160 train_time:63846ms step_avg:46.30ms
step:1380/2160 train_time:63905ms step_avg:46.31ms
step:1381/2160 train_time:63965ms step_avg:46.32ms
step:1382/2160 train_time:64024ms step_avg:46.33ms
step:1383/2160 train_time:64085ms step_avg:46.34ms
step:1384/2160 train_time:64143ms step_avg:46.35ms
step:1385/2160 train_time:64204ms step_avg:46.36ms
step:1386/2160 train_time:64263ms step_avg:46.37ms
step:1387/2160 train_time:64324ms step_avg:46.38ms
step:1388/2160 train_time:64382ms step_avg:46.39ms
step:1389/2160 train_time:64443ms step_avg:46.40ms
step:1390/2160 train_time:64501ms step_avg:46.40ms
step:1391/2160 train_time:64562ms step_avg:46.41ms
step:1392/2160 train_time:64621ms step_avg:46.42ms
step:1393/2160 train_time:64682ms step_avg:46.43ms
step:1394/2160 train_time:64740ms step_avg:46.44ms
step:1395/2160 train_time:64801ms step_avg:46.45ms
step:1396/2160 train_time:64860ms step_avg:46.46ms
step:1397/2160 train_time:64921ms step_avg:46.47ms
step:1398/2160 train_time:64980ms step_avg:46.48ms
step:1399/2160 train_time:65041ms step_avg:46.49ms
step:1400/2160 train_time:65100ms step_avg:46.50ms
step:1401/2160 train_time:65160ms step_avg:46.51ms
step:1402/2160 train_time:65219ms step_avg:46.52ms
step:1403/2160 train_time:65280ms step_avg:46.53ms
step:1404/2160 train_time:65339ms step_avg:46.54ms
step:1405/2160 train_time:65400ms step_avg:46.55ms
step:1406/2160 train_time:65458ms step_avg:46.56ms
step:1407/2160 train_time:65519ms step_avg:46.57ms
step:1408/2160 train_time:65579ms step_avg:46.58ms
step:1409/2160 train_time:65640ms step_avg:46.59ms
step:1410/2160 train_time:65699ms step_avg:46.59ms
step:1411/2160 train_time:65759ms step_avg:46.60ms
step:1412/2160 train_time:65818ms step_avg:46.61ms
step:1413/2160 train_time:65879ms step_avg:46.62ms
step:1414/2160 train_time:65938ms step_avg:46.63ms
step:1415/2160 train_time:66000ms step_avg:46.64ms
step:1416/2160 train_time:66086ms step_avg:46.67ms
step:1417/2160 train_time:66174ms step_avg:46.70ms
step:1418/2160 train_time:66260ms step_avg:46.73ms
step:1419/2160 train_time:66349ms step_avg:46.76ms
step:1420/2160 train_time:66435ms step_avg:46.79ms
step:1421/2160 train_time:66523ms step_avg:46.81ms
step:1422/2160 train_time:66610ms step_avg:46.84ms
step:1423/2160 train_time:66698ms step_avg:46.87ms
step:1424/2160 train_time:66785ms step_avg:46.90ms
step:1425/2160 train_time:66873ms step_avg:46.93ms
step:1426/2160 train_time:66960ms step_avg:46.96ms
step:1427/2160 train_time:67048ms step_avg:46.99ms
step:1428/2160 train_time:67135ms step_avg:47.01ms
step:1429/2160 train_time:67223ms step_avg:47.04ms
step:1430/2160 train_time:67309ms step_avg:47.07ms
step:1431/2160 train_time:67397ms step_avg:47.10ms
step:1432/2160 train_time:67484ms step_avg:47.13ms
step:1433/2160 train_time:67572ms step_avg:47.15ms
step:1434/2160 train_time:67659ms step_avg:47.18ms
step:1435/2160 train_time:67747ms step_avg:47.21ms
step:1436/2160 train_time:67834ms step_avg:47.24ms
step:1437/2160 train_time:67922ms step_avg:47.27ms
step:1438/2160 train_time:68009ms step_avg:47.29ms
step:1439/2160 train_time:68097ms step_avg:47.32ms
step:1440/2160 train_time:68183ms step_avg:47.35ms
step:1441/2160 train_time:68271ms step_avg:47.38ms
step:1442/2160 train_time:68357ms step_avg:47.40ms
step:1443/2160 train_time:68446ms step_avg:47.43ms
step:1444/2160 train_time:68532ms step_avg:47.46ms
step:1445/2160 train_time:68620ms step_avg:47.49ms
step:1446/2160 train_time:68707ms step_avg:47.52ms
step:1447/2160 train_time:68795ms step_avg:47.54ms
step:1448/2160 train_time:68883ms step_avg:47.57ms
step:1449/2160 train_time:68970ms step_avg:47.60ms
step:1450/2160 train_time:69056ms step_avg:47.62ms
step:1451/2160 train_time:69145ms step_avg:47.65ms
step:1452/2160 train_time:69231ms step_avg:47.68ms
step:1453/2160 train_time:69319ms step_avg:47.71ms
step:1454/2160 train_time:69405ms step_avg:47.73ms
step:1455/2160 train_time:69494ms step_avg:47.76ms
step:1456/2160 train_time:69580ms step_avg:47.79ms
step:1457/2160 train_time:69669ms step_avg:47.82ms
step:1458/2160 train_time:69755ms step_avg:47.84ms
step:1459/2160 train_time:69844ms step_avg:47.87ms
step:1460/2160 train_time:69930ms step_avg:47.90ms
step:1461/2160 train_time:70018ms step_avg:47.92ms
step:1462/2160 train_time:70104ms step_avg:47.95ms
step:1463/2160 train_time:70193ms step_avg:47.98ms
step:1464/2160 train_time:70279ms step_avg:48.00ms
step:1465/2160 train_time:70367ms step_avg:48.03ms
step:1466/2160 train_time:70453ms step_avg:48.06ms
step:1467/2160 train_time:70542ms step_avg:48.09ms
step:1468/2160 train_time:70628ms step_avg:48.11ms
step:1469/2160 train_time:70717ms step_avg:48.14ms
step:1470/2160 train_time:70803ms step_avg:48.17ms
step:1471/2160 train_time:70892ms step_avg:48.19ms
step:1472/2160 train_time:70978ms step_avg:48.22ms
step:1473/2160 train_time:71067ms step_avg:48.25ms
step:1474/2160 train_time:71153ms step_avg:48.27ms
step:1475/2160 train_time:71242ms step_avg:48.30ms
step:1476/2160 train_time:71328ms step_avg:48.33ms
step:1477/2160 train_time:71416ms step_avg:48.35ms
step:1478/2160 train_time:71504ms step_avg:48.38ms
step:1479/2160 train_time:71592ms step_avg:48.41ms
step:1480/2160 train_time:71679ms step_avg:48.43ms
step:1481/2160 train_time:71768ms step_avg:48.46ms
step:1482/2160 train_time:71854ms step_avg:48.48ms
step:1483/2160 train_time:71943ms step_avg:48.51ms
step:1484/2160 train_time:72028ms step_avg:48.54ms
step:1485/2160 train_time:72116ms step_avg:48.56ms
step:1486/2160 train_time:72203ms step_avg:48.59ms
step:1487/2160 train_time:72291ms step_avg:48.62ms
step:1488/2160 train_time:72379ms step_avg:48.64ms
step:1489/2160 train_time:72467ms step_avg:48.67ms
step:1490/2160 train_time:72554ms step_avg:48.69ms
step:1491/2160 train_time:72643ms step_avg:48.72ms
step:1492/2160 train_time:72730ms step_avg:48.75ms
step:1493/2160 train_time:72818ms step_avg:48.77ms
step:1494/2160 train_time:72904ms step_avg:48.80ms
step:1495/2160 train_time:72992ms step_avg:48.82ms
step:1496/2160 train_time:73078ms step_avg:48.85ms
step:1497/2160 train_time:73167ms step_avg:48.88ms
step:1498/2160 train_time:73253ms step_avg:48.90ms
step:1499/2160 train_time:73341ms step_avg:48.93ms
step:1500/2160 train_time:73427ms step_avg:48.95ms
step:1500/2160 val_loss:3.4693 train_time:73516ms step_avg:49.01ms
step:1501/2160 train_time:73536ms step_avg:48.99ms
step:1502/2160 train_time:73607ms step_avg:49.01ms
step:1503/2160 train_time:73700ms step_avg:49.04ms
step:1504/2160 train_time:73788ms step_avg:49.06ms
step:1505/2160 train_time:73876ms step_avg:49.09ms
step:1506/2160 train_time:73962ms step_avg:49.11ms
step:1507/2160 train_time:74048ms step_avg:49.14ms
step:1508/2160 train_time:74134ms step_avg:49.16ms
step:1509/2160 train_time:74221ms step_avg:49.19ms
step:1510/2160 train_time:74307ms step_avg:49.21ms
step:1511/2160 train_time:74394ms step_avg:49.23ms
step:1512/2160 train_time:74482ms step_avg:49.26ms
step:1513/2160 train_time:74572ms step_avg:49.29ms
step:1514/2160 train_time:74661ms step_avg:49.31ms
step:1515/2160 train_time:74751ms step_avg:49.34ms
step:1516/2160 train_time:74838ms step_avg:49.37ms
step:1517/2160 train_time:74927ms step_avg:49.39ms
step:1518/2160 train_time:75012ms step_avg:49.42ms
step:1519/2160 train_time:75100ms step_avg:49.44ms
step:1520/2160 train_time:75185ms step_avg:49.46ms
step:1521/2160 train_time:75272ms step_avg:49.49ms
step:1522/2160 train_time:75358ms step_avg:49.51ms
step:1523/2160 train_time:75447ms step_avg:49.54ms
step:1524/2160 train_time:75535ms step_avg:49.56ms
step:1525/2160 train_time:75625ms step_avg:49.59ms
step:1526/2160 train_time:75713ms step_avg:49.62ms
step:1527/2160 train_time:75801ms step_avg:49.64ms
step:1528/2160 train_time:75887ms step_avg:49.66ms
step:1529/2160 train_time:75975ms step_avg:49.69ms
step:1530/2160 train_time:76061ms step_avg:49.71ms
step:1531/2160 train_time:76149ms step_avg:49.74ms
step:1532/2160 train_time:76234ms step_avg:49.76ms
step:1533/2160 train_time:76321ms step_avg:49.79ms
step:1534/2160 train_time:76408ms step_avg:49.81ms
step:1535/2160 train_time:76498ms step_avg:49.84ms
step:1536/2160 train_time:76585ms step_avg:49.86ms
step:1537/2160 train_time:76675ms step_avg:49.89ms
step:1538/2160 train_time:76761ms step_avg:49.91ms
step:1539/2160 train_time:76850ms step_avg:49.93ms
step:1540/2160 train_time:76937ms step_avg:49.96ms
step:1541/2160 train_time:77025ms step_avg:49.98ms
step:1542/2160 train_time:77111ms step_avg:50.01ms
step:1543/2160 train_time:77198ms step_avg:50.03ms
step:1544/2160 train_time:77284ms step_avg:50.05ms
step:1545/2160 train_time:77372ms step_avg:50.08ms
step:1546/2160 train_time:77458ms step_avg:50.10ms
step:1547/2160 train_time:77547ms step_avg:50.13ms
step:1548/2160 train_time:77634ms step_avg:50.15ms
step:1549/2160 train_time:77722ms step_avg:50.18ms
step:1550/2160 train_time:77808ms step_avg:50.20ms
step:1551/2160 train_time:77897ms step_avg:50.22ms
step:1552/2160 train_time:77983ms step_avg:50.25ms
step:1553/2160 train_time:78071ms step_avg:50.27ms
step:1554/2160 train_time:78158ms step_avg:50.29ms
step:1555/2160 train_time:78246ms step_avg:50.32ms
step:1556/2160 train_time:78332ms step_avg:50.34ms
step:1557/2160 train_time:78420ms step_avg:50.37ms
step:1558/2160 train_time:78507ms step_avg:50.39ms
step:1559/2160 train_time:78596ms step_avg:50.41ms
step:1560/2160 train_time:78683ms step_avg:50.44ms
step:1561/2160 train_time:78772ms step_avg:50.46ms
step:1562/2160 train_time:78858ms step_avg:50.49ms
step:1563/2160 train_time:78946ms step_avg:50.51ms
step:1564/2160 train_time:79033ms step_avg:50.53ms
step:1565/2160 train_time:79121ms step_avg:50.56ms
step:1566/2160 train_time:79206ms step_avg:50.58ms
step:1567/2160 train_time:79294ms step_avg:50.60ms
step:1568/2160 train_time:79380ms step_avg:50.63ms
step:1569/2160 train_time:79469ms step_avg:50.65ms
step:1570/2160 train_time:79556ms step_avg:50.67ms
step:1571/2160 train_time:79645ms step_avg:50.70ms
step:1572/2160 train_time:79731ms step_avg:50.72ms
step:1573/2160 train_time:79820ms step_avg:50.74ms
step:1574/2160 train_time:79907ms step_avg:50.77ms
step:1575/2160 train_time:79995ms step_avg:50.79ms
step:1576/2160 train_time:80081ms step_avg:50.81ms
step:1577/2160 train_time:80169ms step_avg:50.84ms
step:1578/2160 train_time:80255ms step_avg:50.86ms
step:1579/2160 train_time:80343ms step_avg:50.88ms
step:1580/2160 train_time:80429ms step_avg:50.90ms
step:1581/2160 train_time:80518ms step_avg:50.93ms
step:1582/2160 train_time:80605ms step_avg:50.95ms
step:1583/2160 train_time:80693ms step_avg:50.97ms
step:1584/2160 train_time:80779ms step_avg:51.00ms
step:1585/2160 train_time:80868ms step_avg:51.02ms
step:1586/2160 train_time:80955ms step_avg:51.04ms
step:1587/2160 train_time:81043ms step_avg:51.07ms
step:1588/2160 train_time:81129ms step_avg:51.09ms
step:1589/2160 train_time:81217ms step_avg:51.11ms
step:1590/2160 train_time:81303ms step_avg:51.13ms
step:1591/2160 train_time:81391ms step_avg:51.16ms
step:1592/2160 train_time:81477ms step_avg:51.18ms
step:1593/2160 train_time:81566ms step_avg:51.20ms
step:1594/2160 train_time:81653ms step_avg:51.23ms
step:1595/2160 train_time:81741ms step_avg:51.25ms
step:1596/2160 train_time:81827ms step_avg:51.27ms
step:1597/2160 train_time:81915ms step_avg:51.29ms
step:1598/2160 train_time:82001ms step_avg:51.31ms
step:1599/2160 train_time:82089ms step_avg:51.34ms
step:1600/2160 train_time:82176ms step_avg:51.36ms
step:1601/2160 train_time:82264ms step_avg:51.38ms
step:1602/2160 train_time:82351ms step_avg:51.41ms
step:1603/2160 train_time:82439ms step_avg:51.43ms
step:1604/2160 train_time:82526ms step_avg:51.45ms
step:1605/2160 train_time:82615ms step_avg:51.47ms
step:1606/2160 train_time:82701ms step_avg:51.49ms
step:1607/2160 train_time:82789ms step_avg:51.52ms
step:1608/2160 train_time:82876ms step_avg:51.54ms
step:1609/2160 train_time:82965ms step_avg:51.56ms
step:1610/2160 train_time:83051ms step_avg:51.58ms
step:1611/2160 train_time:83139ms step_avg:51.61ms
step:1612/2160 train_time:83225ms step_avg:51.63ms
step:1613/2160 train_time:83314ms step_avg:51.65ms
step:1614/2160 train_time:83401ms step_avg:51.67ms
step:1615/2160 train_time:83489ms step_avg:51.70ms
step:1616/2160 train_time:83576ms step_avg:51.72ms
step:1617/2160 train_time:83665ms step_avg:51.74ms
step:1618/2160 train_time:83751ms step_avg:51.76ms
step:1619/2160 train_time:83839ms step_avg:51.78ms
step:1620/2160 train_time:83926ms step_avg:51.81ms
step:1621/2160 train_time:84015ms step_avg:51.83ms
step:1622/2160 train_time:84102ms step_avg:51.85ms
step:1623/2160 train_time:84191ms step_avg:51.87ms
step:1624/2160 train_time:84277ms step_avg:51.89ms
step:1625/2160 train_time:84366ms step_avg:51.92ms
step:1626/2160 train_time:84452ms step_avg:51.94ms
step:1627/2160 train_time:84540ms step_avg:51.96ms
step:1628/2160 train_time:84627ms step_avg:51.98ms
step:1629/2160 train_time:84715ms step_avg:52.00ms
step:1630/2160 train_time:84801ms step_avg:52.03ms
step:1631/2160 train_time:84889ms step_avg:52.05ms
step:1632/2160 train_time:84977ms step_avg:52.07ms
step:1633/2160 train_time:85066ms step_avg:52.09ms
step:1634/2160 train_time:85152ms step_avg:52.11ms
step:1635/2160 train_time:85241ms step_avg:52.13ms
step:1636/2160 train_time:85327ms step_avg:52.16ms
step:1637/2160 train_time:85415ms step_avg:52.18ms
step:1638/2160 train_time:85501ms step_avg:52.20ms
step:1639/2160 train_time:85589ms step_avg:52.22ms
step:1640/2160 train_time:85676ms step_avg:52.24ms
step:1641/2160 train_time:85765ms step_avg:52.26ms
step:1642/2160 train_time:85851ms step_avg:52.28ms
step:1643/2160 train_time:85939ms step_avg:52.31ms
step:1644/2160 train_time:86025ms step_avg:52.33ms
step:1645/2160 train_time:86114ms step_avg:52.35ms
step:1646/2160 train_time:86200ms step_avg:52.37ms
step:1647/2160 train_time:86289ms step_avg:52.39ms
step:1648/2160 train_time:86376ms step_avg:52.41ms
step:1649/2160 train_time:86465ms step_avg:52.43ms
step:1650/2160 train_time:86551ms step_avg:52.45ms
step:1651/2160 train_time:86639ms step_avg:52.48ms
step:1652/2160 train_time:86725ms step_avg:52.50ms
step:1653/2160 train_time:86813ms step_avg:52.52ms
step:1654/2160 train_time:86900ms step_avg:52.54ms
step:1655/2160 train_time:86988ms step_avg:52.56ms
step:1656/2160 train_time:87075ms step_avg:52.58ms
step:1657/2160 train_time:87163ms step_avg:52.60ms
step:1658/2160 train_time:87250ms step_avg:52.62ms
step:1659/2160 train_time:87338ms step_avg:52.65ms
step:1660/2160 train_time:87424ms step_avg:52.67ms
step:1661/2160 train_time:87513ms step_avg:52.69ms
step:1662/2160 train_time:87599ms step_avg:52.71ms
step:1663/2160 train_time:87687ms step_avg:52.73ms
step:1664/2160 train_time:87774ms step_avg:52.75ms
step:1665/2160 train_time:87862ms step_avg:52.77ms
step:1666/2160 train_time:87948ms step_avg:52.79ms
step:1667/2160 train_time:88038ms step_avg:52.81ms
step:1668/2160 train_time:88124ms step_avg:52.83ms
step:1669/2160 train_time:88212ms step_avg:52.85ms
step:1670/2160 train_time:88298ms step_avg:52.87ms
step:1671/2160 train_time:88387ms step_avg:52.89ms
step:1672/2160 train_time:88473ms step_avg:52.91ms
step:1673/2160 train_time:88561ms step_avg:52.94ms
step:1674/2160 train_time:88648ms step_avg:52.96ms
step:1675/2160 train_time:88737ms step_avg:52.98ms
step:1676/2160 train_time:88822ms step_avg:53.00ms
step:1677/2160 train_time:88911ms step_avg:53.02ms
step:1678/2160 train_time:88997ms step_avg:53.04ms
step:1679/2160 train_time:89085ms step_avg:53.06ms
step:1680/2160 train_time:89173ms step_avg:53.08ms
step:1681/2160 train_time:89261ms step_avg:53.10ms
step:1682/2160 train_time:89347ms step_avg:53.12ms
step:1683/2160 train_time:89436ms step_avg:53.14ms
step:1684/2160 train_time:89522ms step_avg:53.16ms
step:1685/2160 train_time:89610ms step_avg:53.18ms
step:1686/2160 train_time:89697ms step_avg:53.20ms
step:1687/2160 train_time:89785ms step_avg:53.22ms
step:1688/2160 train_time:89872ms step_avg:53.24ms
step:1689/2160 train_time:89960ms step_avg:53.26ms
step:1690/2160 train_time:90046ms step_avg:53.28ms
step:1691/2160 train_time:90135ms step_avg:53.30ms
step:1692/2160 train_time:90221ms step_avg:53.32ms
step:1693/2160 train_time:90309ms step_avg:53.34ms
step:1694/2160 train_time:90396ms step_avg:53.36ms
step:1695/2160 train_time:90485ms step_avg:53.38ms
step:1696/2160 train_time:90571ms step_avg:53.40ms
step:1697/2160 train_time:90659ms step_avg:53.42ms
step:1698/2160 train_time:90745ms step_avg:53.44ms
step:1699/2160 train_time:90833ms step_avg:53.46ms
step:1700/2160 train_time:90919ms step_avg:53.48ms
step:1701/2160 train_time:91008ms step_avg:53.50ms
step:1702/2160 train_time:91095ms step_avg:53.52ms
step:1703/2160 train_time:91183ms step_avg:53.54ms
step:1704/2160 train_time:91269ms step_avg:53.56ms
step:1705/2160 train_time:91358ms step_avg:53.58ms
step:1706/2160 train_time:91445ms step_avg:53.60ms
step:1707/2160 train_time:91533ms step_avg:53.62ms
step:1708/2160 train_time:91619ms step_avg:53.64ms
step:1709/2160 train_time:91707ms step_avg:53.66ms
step:1710/2160 train_time:91793ms step_avg:53.68ms
step:1711/2160 train_time:91881ms step_avg:53.70ms
step:1712/2160 train_time:91968ms step_avg:53.72ms
step:1713/2160 train_time:92057ms step_avg:53.74ms
step:1714/2160 train_time:92144ms step_avg:53.76ms
step:1715/2160 train_time:92233ms step_avg:53.78ms
step:1716/2160 train_time:92319ms step_avg:53.80ms
step:1717/2160 train_time:92408ms step_avg:53.82ms
step:1718/2160 train_time:92494ms step_avg:53.84ms
step:1719/2160 train_time:92583ms step_avg:53.86ms
step:1720/2160 train_time:92669ms step_avg:53.88ms
step:1721/2160 train_time:92758ms step_avg:53.90ms
step:1722/2160 train_time:92844ms step_avg:53.92ms
step:1723/2160 train_time:92933ms step_avg:53.94ms
step:1724/2160 train_time:93019ms step_avg:53.96ms
step:1725/2160 train_time:93108ms step_avg:53.98ms
step:1726/2160 train_time:93195ms step_avg:53.99ms
step:1727/2160 train_time:93283ms step_avg:54.01ms
step:1728/2160 train_time:93369ms step_avg:54.03ms
step:1729/2160 train_time:93458ms step_avg:54.05ms
step:1730/2160 train_time:93545ms step_avg:54.07ms
step:1731/2160 train_time:93632ms step_avg:54.09ms
step:1732/2160 train_time:93719ms step_avg:54.11ms
step:1733/2160 train_time:93807ms step_avg:54.13ms
step:1734/2160 train_time:93894ms step_avg:54.15ms
step:1735/2160 train_time:93982ms step_avg:54.17ms
step:1736/2160 train_time:94069ms step_avg:54.19ms
step:1737/2160 train_time:94157ms step_avg:54.21ms
step:1738/2160 train_time:94243ms step_avg:54.23ms
step:1739/2160 train_time:94331ms step_avg:54.24ms
step:1740/2160 train_time:94417ms step_avg:54.26ms
step:1741/2160 train_time:94506ms step_avg:54.28ms
step:1742/2160 train_time:94592ms step_avg:54.30ms
step:1743/2160 train_time:94681ms step_avg:54.32ms
step:1744/2160 train_time:94768ms step_avg:54.34ms
step:1745/2160 train_time:94857ms step_avg:54.36ms
step:1746/2160 train_time:94943ms step_avg:54.38ms
step:1747/2160 train_time:95031ms step_avg:54.40ms
step:1748/2160 train_time:95117ms step_avg:54.41ms
step:1749/2160 train_time:95206ms step_avg:54.43ms
step:1750/2160 train_time:95292ms step_avg:54.45ms
step:1750/2160 val_loss:3.3787 train_time:95381ms step_avg:54.50ms
step:1751/2160 train_time:95403ms step_avg:54.48ms
step:1752/2160 train_time:95472ms step_avg:54.49ms
step:1753/2160 train_time:95567ms step_avg:54.52ms
step:1754/2160 train_time:95657ms step_avg:54.54ms
step:1755/2160 train_time:95745ms step_avg:54.56ms
step:1756/2160 train_time:95830ms step_avg:54.57ms
step:1757/2160 train_time:95918ms step_avg:54.59ms
step:1758/2160 train_time:96004ms step_avg:54.61ms
step:1759/2160 train_time:96090ms step_avg:54.63ms
step:1760/2160 train_time:96175ms step_avg:54.65ms
step:1761/2160 train_time:96262ms step_avg:54.66ms
step:1762/2160 train_time:96352ms step_avg:54.68ms
step:1763/2160 train_time:96441ms step_avg:54.70ms
step:1764/2160 train_time:96530ms step_avg:54.72ms
step:1765/2160 train_time:96620ms step_avg:54.74ms
step:1766/2160 train_time:96707ms step_avg:54.76ms
step:1767/2160 train_time:96796ms step_avg:54.78ms
step:1768/2160 train_time:96882ms step_avg:54.80ms
step:1769/2160 train_time:96970ms step_avg:54.82ms
step:1770/2160 train_time:97055ms step_avg:54.83ms
step:1771/2160 train_time:97142ms step_avg:54.85ms
step:1772/2160 train_time:97229ms step_avg:54.87ms
step:1773/2160 train_time:97317ms step_avg:54.89ms
step:1774/2160 train_time:97404ms step_avg:54.91ms
step:1775/2160 train_time:97493ms step_avg:54.93ms
step:1776/2160 train_time:97581ms step_avg:54.94ms
step:1777/2160 train_time:97670ms step_avg:54.96ms
step:1778/2160 train_time:97758ms step_avg:54.98ms
step:1779/2160 train_time:97846ms step_avg:55.00ms
step:1780/2160 train_time:97932ms step_avg:55.02ms
step:1781/2160 train_time:98019ms step_avg:55.04ms
step:1782/2160 train_time:98105ms step_avg:55.05ms
step:1783/2160 train_time:98193ms step_avg:55.07ms
step:1784/2160 train_time:98279ms step_avg:55.09ms
step:1785/2160 train_time:98367ms step_avg:55.11ms
step:1786/2160 train_time:98454ms step_avg:55.13ms
step:1787/2160 train_time:98544ms step_avg:55.14ms
step:1788/2160 train_time:98631ms step_avg:55.16ms
step:1789/2160 train_time:98720ms step_avg:55.18ms
step:1790/2160 train_time:98806ms step_avg:55.20ms
step:1791/2160 train_time:98894ms step_avg:55.22ms
step:1792/2160 train_time:98979ms step_avg:55.23ms
step:1793/2160 train_time:99067ms step_avg:55.25ms
step:1794/2160 train_time:99152ms step_avg:55.27ms
step:1795/2160 train_time:99241ms step_avg:55.29ms
step:1796/2160 train_time:99327ms step_avg:55.30ms
step:1797/2160 train_time:99416ms step_avg:55.32ms
step:1798/2160 train_time:99502ms step_avg:55.34ms
step:1799/2160 train_time:99591ms step_avg:55.36ms
step:1800/2160 train_time:99679ms step_avg:55.38ms
step:1801/2160 train_time:99767ms step_avg:55.40ms
step:1802/2160 train_time:99853ms step_avg:55.41ms
step:1803/2160 train_time:99942ms step_avg:55.43ms
step:1804/2160 train_time:100027ms step_avg:55.45ms
step:1805/2160 train_time:100115ms step_avg:55.47ms
step:1806/2160 train_time:100200ms step_avg:55.48ms
step:1807/2160 train_time:100289ms step_avg:55.50ms
step:1808/2160 train_time:100376ms step_avg:55.52ms
step:1809/2160 train_time:100465ms step_avg:55.54ms
step:1810/2160 train_time:100552ms step_avg:55.55ms
step:1811/2160 train_time:100641ms step_avg:55.57ms
step:1812/2160 train_time:100727ms step_avg:55.59ms
step:1813/2160 train_time:100816ms step_avg:55.61ms
step:1814/2160 train_time:100902ms step_avg:55.62ms
step:1815/2160 train_time:100991ms step_avg:55.64ms
step:1816/2160 train_time:101077ms step_avg:55.66ms
step:1817/2160 train_time:101165ms step_avg:55.68ms
step:1818/2160 train_time:101251ms step_avg:55.69ms
step:1819/2160 train_time:101340ms step_avg:55.71ms
step:1820/2160 train_time:101426ms step_avg:55.73ms
step:1821/2160 train_time:101515ms step_avg:55.75ms
step:1822/2160 train_time:101602ms step_avg:55.76ms
step:1823/2160 train_time:101690ms step_avg:55.78ms
step:1824/2160 train_time:101777ms step_avg:55.80ms
step:1825/2160 train_time:101865ms step_avg:55.82ms
step:1826/2160 train_time:101951ms step_avg:55.83ms
step:1827/2160 train_time:102039ms step_avg:55.85ms
step:1828/2160 train_time:102125ms step_avg:55.87ms
step:1829/2160 train_time:102213ms step_avg:55.88ms
step:1830/2160 train_time:102299ms step_avg:55.90ms
step:1831/2160 train_time:102387ms step_avg:55.92ms
step:1832/2160 train_time:102474ms step_avg:55.94ms
step:1833/2160 train_time:102563ms step_avg:55.95ms
step:1834/2160 train_time:102650ms step_avg:55.97ms
step:1835/2160 train_time:102738ms step_avg:55.99ms
step:1836/2160 train_time:102824ms step_avg:56.00ms
step:1837/2160 train_time:102913ms step_avg:56.02ms
step:1838/2160 train_time:102999ms step_avg:56.04ms
step:1839/2160 train_time:103087ms step_avg:56.06ms
step:1840/2160 train_time:103173ms step_avg:56.07ms
step:1841/2160 train_time:103261ms step_avg:56.09ms
step:1842/2160 train_time:103349ms step_avg:56.11ms
step:1843/2160 train_time:103438ms step_avg:56.12ms
step:1844/2160 train_time:103525ms step_avg:56.14ms
step:1845/2160 train_time:103613ms step_avg:56.16ms
step:1846/2160 train_time:103700ms step_avg:56.18ms
step:1847/2160 train_time:103788ms step_avg:56.19ms
step:1848/2160 train_time:103874ms step_avg:56.21ms
step:1849/2160 train_time:103962ms step_avg:56.23ms
step:1850/2160 train_time:104049ms step_avg:56.24ms
step:1851/2160 train_time:104136ms step_avg:56.26ms
step:1852/2160 train_time:104222ms step_avg:56.28ms
step:1853/2160 train_time:104311ms step_avg:56.29ms
step:1854/2160 train_time:104398ms step_avg:56.31ms
step:1855/2160 train_time:104486ms step_avg:56.33ms
step:1856/2160 train_time:104573ms step_avg:56.34ms
step:1857/2160 train_time:104661ms step_avg:56.36ms
step:1858/2160 train_time:104748ms step_avg:56.38ms
step:1859/2160 train_time:104836ms step_avg:56.39ms
step:1860/2160 train_time:104922ms step_avg:56.41ms
step:1861/2160 train_time:105011ms step_avg:56.43ms
step:1862/2160 train_time:105097ms step_avg:56.44ms
step:1863/2160 train_time:105185ms step_avg:56.46ms
step:1864/2160 train_time:105271ms step_avg:56.48ms
step:1865/2160 train_time:105360ms step_avg:56.49ms
step:1866/2160 train_time:105447ms step_avg:56.51ms
step:1867/2160 train_time:105535ms step_avg:56.53ms
step:1868/2160 train_time:105623ms step_avg:56.54ms
step:1869/2160 train_time:105712ms step_avg:56.56ms
step:1870/2160 train_time:105799ms step_avg:56.58ms
step:1871/2160 train_time:105888ms step_avg:56.59ms
step:1872/2160 train_time:105974ms step_avg:56.61ms
step:1873/2160 train_time:106062ms step_avg:56.63ms
step:1874/2160 train_time:106147ms step_avg:56.64ms
step:1875/2160 train_time:106235ms step_avg:56.66ms
step:1876/2160 train_time:106322ms step_avg:56.67ms
step:1877/2160 train_time:106411ms step_avg:56.69ms
step:1878/2160 train_time:106497ms step_avg:56.71ms
step:1879/2160 train_time:106586ms step_avg:56.72ms
step:1880/2160 train_time:106672ms step_avg:56.74ms
step:1881/2160 train_time:106760ms step_avg:56.76ms
step:1882/2160 train_time:106846ms step_avg:56.77ms
step:1883/2160 train_time:106935ms step_avg:56.79ms
step:1884/2160 train_time:107021ms step_avg:56.81ms
step:1885/2160 train_time:107110ms step_avg:56.82ms
step:1886/2160 train_time:107196ms step_avg:56.84ms
step:1887/2160 train_time:107284ms step_avg:56.85ms
step:1888/2160 train_time:107371ms step_avg:56.87ms
step:1889/2160 train_time:107460ms step_avg:56.89ms
step:1890/2160 train_time:107547ms step_avg:56.90ms
step:1891/2160 train_time:107635ms step_avg:56.92ms
step:1892/2160 train_time:107722ms step_avg:56.94ms
step:1893/2160 train_time:107811ms step_avg:56.95ms
step:1894/2160 train_time:107897ms step_avg:56.97ms
step:1895/2160 train_time:107985ms step_avg:56.98ms
step:1896/2160 train_time:108072ms step_avg:57.00ms
step:1897/2160 train_time:108160ms step_avg:57.02ms
step:1898/2160 train_time:108246ms step_avg:57.03ms
step:1899/2160 train_time:108335ms step_avg:57.05ms
step:1900/2160 train_time:108421ms step_avg:57.06ms
step:1901/2160 train_time:108510ms step_avg:57.08ms
step:1902/2160 train_time:108596ms step_avg:57.10ms
step:1903/2160 train_time:108685ms step_avg:57.11ms
step:1904/2160 train_time:108771ms step_avg:57.13ms
step:1905/2160 train_time:108860ms step_avg:57.14ms
step:1906/2160 train_time:108946ms step_avg:57.16ms
step:1907/2160 train_time:109035ms step_avg:57.18ms
step:1908/2160 train_time:109121ms step_avg:57.19ms
step:1909/2160 train_time:109208ms step_avg:57.21ms
step:1910/2160 train_time:109295ms step_avg:57.22ms
step:1911/2160 train_time:109384ms step_avg:57.24ms
step:1912/2160 train_time:109471ms step_avg:57.25ms
step:1913/2160 train_time:109560ms step_avg:57.27ms
step:1914/2160 train_time:109647ms step_avg:57.29ms
step:1915/2160 train_time:109735ms step_avg:57.30ms
step:1916/2160 train_time:109821ms step_avg:57.32ms
step:1917/2160 train_time:109910ms step_avg:57.33ms
step:1918/2160 train_time:109997ms step_avg:57.35ms
step:1919/2160 train_time:110085ms step_avg:57.37ms
step:1920/2160 train_time:110171ms step_avg:57.38ms
step:1921/2160 train_time:110259ms step_avg:57.40ms
step:1922/2160 train_time:110346ms step_avg:57.41ms
step:1923/2160 train_time:110435ms step_avg:57.43ms
step:1924/2160 train_time:110521ms step_avg:57.44ms
step:1925/2160 train_time:110610ms step_avg:57.46ms
step:1926/2160 train_time:110696ms step_avg:57.47ms
step:1927/2160 train_time:110785ms step_avg:57.49ms
step:1928/2160 train_time:110872ms step_avg:57.51ms
step:1929/2160 train_time:110960ms step_avg:57.52ms
step:1930/2160 train_time:111046ms step_avg:57.54ms
step:1931/2160 train_time:111134ms step_avg:57.55ms
step:1932/2160 train_time:111220ms step_avg:57.57ms
step:1933/2160 train_time:111309ms step_avg:57.58ms
step:1934/2160 train_time:111395ms step_avg:57.60ms
step:1935/2160 train_time:111484ms step_avg:57.61ms
step:1936/2160 train_time:111570ms step_avg:57.63ms
step:1937/2160 train_time:111658ms step_avg:57.64ms
step:1938/2160 train_time:111745ms step_avg:57.66ms
step:1939/2160 train_time:111833ms step_avg:57.68ms
step:1940/2160 train_time:111919ms step_avg:57.69ms
step:1941/2160 train_time:112008ms step_avg:57.71ms
step:1942/2160 train_time:112094ms step_avg:57.72ms
step:1943/2160 train_time:112183ms step_avg:57.74ms
step:1944/2160 train_time:112269ms step_avg:57.75ms
step:1945/2160 train_time:112358ms step_avg:57.77ms
step:1946/2160 train_time:112444ms step_avg:57.78ms
step:1947/2160 train_time:112533ms step_avg:57.80ms
step:1948/2160 train_time:112619ms step_avg:57.81ms
step:1949/2160 train_time:112708ms step_avg:57.83ms
step:1950/2160 train_time:112795ms step_avg:57.84ms
step:1951/2160 train_time:112883ms step_avg:57.86ms
step:1952/2160 train_time:112970ms step_avg:57.87ms
step:1953/2160 train_time:113058ms step_avg:57.89ms
step:1954/2160 train_time:113145ms step_avg:57.90ms
step:1955/2160 train_time:113233ms step_avg:57.92ms
step:1956/2160 train_time:113320ms step_avg:57.93ms
step:1957/2160 train_time:113408ms step_avg:57.95ms
step:1958/2160 train_time:113494ms step_avg:57.96ms
step:1959/2160 train_time:113582ms step_avg:57.98ms
step:1960/2160 train_time:113669ms step_avg:57.99ms
step:1961/2160 train_time:113757ms step_avg:58.01ms
step:1962/2160 train_time:113843ms step_avg:58.02ms
step:1963/2160 train_time:113933ms step_avg:58.04ms
step:1964/2160 train_time:114019ms step_avg:58.05ms
step:1965/2160 train_time:114108ms step_avg:58.07ms
step:1966/2160 train_time:114195ms step_avg:58.09ms
step:1967/2160 train_time:114285ms step_avg:58.10ms
step:1968/2160 train_time:114371ms step_avg:58.12ms
step:1969/2160 train_time:114460ms step_avg:58.13ms
step:1970/2160 train_time:114547ms step_avg:58.15ms
step:1971/2160 train_time:114634ms step_avg:58.16ms
step:1972/2160 train_time:114720ms step_avg:58.17ms
step:1973/2160 train_time:114809ms step_avg:58.19ms
step:1974/2160 train_time:114895ms step_avg:58.20ms
step:1975/2160 train_time:114983ms step_avg:58.22ms
step:1976/2160 train_time:115070ms step_avg:58.23ms
step:1977/2160 train_time:115157ms step_avg:58.25ms
step:1978/2160 train_time:115244ms step_avg:58.26ms
step:1979/2160 train_time:115332ms step_avg:58.28ms
step:1980/2160 train_time:115418ms step_avg:58.29ms
step:1981/2160 train_time:115507ms step_avg:58.31ms
step:1982/2160 train_time:115594ms step_avg:58.32ms
step:1983/2160 train_time:115682ms step_avg:58.34ms
step:1984/2160 train_time:115769ms step_avg:58.35ms
step:1985/2160 train_time:115858ms step_avg:58.37ms
step:1986/2160 train_time:115944ms step_avg:58.38ms
step:1987/2160 train_time:116032ms step_avg:58.40ms
step:1988/2160 train_time:116119ms step_avg:58.41ms
step:1989/2160 train_time:116207ms step_avg:58.42ms
step:1990/2160 train_time:116294ms step_avg:58.44ms
step:1991/2160 train_time:116382ms step_avg:58.45ms
step:1992/2160 train_time:116469ms step_avg:58.47ms
step:1993/2160 train_time:116558ms step_avg:58.48ms
step:1994/2160 train_time:116645ms step_avg:58.50ms
step:1995/2160 train_time:116733ms step_avg:58.51ms
step:1996/2160 train_time:116819ms step_avg:58.53ms
step:1997/2160 train_time:116908ms step_avg:58.54ms
step:1998/2160 train_time:116994ms step_avg:58.56ms
step:1999/2160 train_time:117083ms step_avg:58.57ms
step:2000/2160 train_time:117170ms step_avg:58.59ms
step:2000/2160 val_loss:3.3094 train_time:117260ms step_avg:58.63ms
step:2001/2160 train_time:117281ms step_avg:58.61ms
step:2002/2160 train_time:117352ms step_avg:58.62ms
step:2003/2160 train_time:117443ms step_avg:58.63ms
step:2004/2160 train_time:117530ms step_avg:58.65ms
step:2005/2160 train_time:117617ms step_avg:58.66ms
step:2006/2160 train_time:117703ms step_avg:58.68ms
step:2007/2160 train_time:117790ms step_avg:58.69ms
step:2008/2160 train_time:117876ms step_avg:58.70ms
step:2009/2160 train_time:117963ms step_avg:58.72ms
step:2010/2160 train_time:118049ms step_avg:58.73ms
step:2011/2160 train_time:118137ms step_avg:58.75ms
step:2012/2160 train_time:118225ms step_avg:58.76ms
step:2013/2160 train_time:118315ms step_avg:58.78ms
step:2014/2160 train_time:118403ms step_avg:58.79ms
step:2015/2160 train_time:118492ms step_avg:58.81ms
step:2016/2160 train_time:118579ms step_avg:58.82ms
step:2017/2160 train_time:118667ms step_avg:58.83ms
step:2018/2160 train_time:118753ms step_avg:58.85ms
step:2019/2160 train_time:118840ms step_avg:58.86ms
step:2020/2160 train_time:118926ms step_avg:58.87ms
step:2021/2160 train_time:119014ms step_avg:58.89ms
step:2022/2160 train_time:119100ms step_avg:58.90ms
step:2023/2160 train_time:119188ms step_avg:58.92ms
step:2024/2160 train_time:119276ms step_avg:58.93ms
step:2025/2160 train_time:119366ms step_avg:58.95ms
step:2026/2160 train_time:119453ms step_avg:58.96ms
step:2027/2160 train_time:119541ms step_avg:58.97ms
step:2028/2160 train_time:119628ms step_avg:58.99ms
step:2029/2160 train_time:119716ms step_avg:59.00ms
step:2030/2160 train_time:119801ms step_avg:59.02ms
step:2031/2160 train_time:119889ms step_avg:59.03ms
step:2032/2160 train_time:119974ms step_avg:59.04ms
step:2033/2160 train_time:120062ms step_avg:59.06ms
step:2034/2160 train_time:120148ms step_avg:59.07ms
step:2035/2160 train_time:120237ms step_avg:59.08ms
step:2036/2160 train_time:120324ms step_avg:59.10ms
step:2037/2160 train_time:120413ms step_avg:59.11ms
step:2038/2160 train_time:120499ms step_avg:59.13ms
step:2039/2160 train_time:120589ms step_avg:59.14ms
step:2040/2160 train_time:120675ms step_avg:59.15ms
step:2041/2160 train_time:120763ms step_avg:59.17ms
step:2042/2160 train_time:120848ms step_avg:59.18ms
step:2043/2160 train_time:120936ms step_avg:59.20ms
step:2044/2160 train_time:121022ms step_avg:59.21ms
step:2045/2160 train_time:121110ms step_avg:59.22ms
step:2046/2160 train_time:121197ms step_avg:59.24ms
step:2047/2160 train_time:121287ms step_avg:59.25ms
step:2048/2160 train_time:121374ms step_avg:59.26ms
step:2049/2160 train_time:121462ms step_avg:59.28ms
step:2050/2160 train_time:121550ms step_avg:59.29ms
step:2051/2160 train_time:121639ms step_avg:59.31ms
step:2052/2160 train_time:121726ms step_avg:59.32ms
step:2053/2160 train_time:121814ms step_avg:59.33ms
step:2054/2160 train_time:121899ms step_avg:59.35ms
step:2055/2160 train_time:121987ms step_avg:59.36ms
step:2056/2160 train_time:122074ms step_avg:59.37ms
step:2057/2160 train_time:122162ms step_avg:59.39ms
step:2058/2160 train_time:122249ms step_avg:59.40ms
step:2059/2160 train_time:122338ms step_avg:59.42ms
step:2060/2160 train_time:122424ms step_avg:59.43ms
step:2061/2160 train_time:122512ms step_avg:59.44ms
step:2062/2160 train_time:122599ms step_avg:59.46ms
step:2063/2160 train_time:122689ms step_avg:59.47ms
step:2064/2160 train_time:122775ms step_avg:59.48ms
step:2065/2160 train_time:122862ms step_avg:59.50ms
step:2066/2160 train_time:122948ms step_avg:59.51ms
step:2067/2160 train_time:123037ms step_avg:59.52ms
step:2068/2160 train_time:123123ms step_avg:59.54ms
step:2069/2160 train_time:123211ms step_avg:59.55ms
step:2070/2160 train_time:123298ms step_avg:59.56ms
step:2071/2160 train_time:123387ms step_avg:59.58ms
step:2072/2160 train_time:123474ms step_avg:59.59ms
step:2073/2160 train_time:123562ms step_avg:59.61ms
step:2074/2160 train_time:123649ms step_avg:59.62ms
step:2075/2160 train_time:123738ms step_avg:59.63ms
step:2076/2160 train_time:123824ms step_avg:59.65ms
step:2077/2160 train_time:123912ms step_avg:59.66ms
step:2078/2160 train_time:123998ms step_avg:59.67ms
step:2079/2160 train_time:124087ms step_avg:59.69ms
step:2080/2160 train_time:124174ms step_avg:59.70ms
step:2081/2160 train_time:124262ms step_avg:59.71ms
step:2082/2160 train_time:124349ms step_avg:59.73ms
step:2083/2160 train_time:124437ms step_avg:59.74ms
step:2084/2160 train_time:124524ms step_avg:59.75ms
step:2085/2160 train_time:124613ms step_avg:59.77ms
step:2086/2160 train_time:124700ms step_avg:59.78ms
step:2087/2160 train_time:124789ms step_avg:59.79ms
step:2088/2160 train_time:124876ms step_avg:59.81ms
step:2089/2160 train_time:124964ms step_avg:59.82ms
step:2090/2160 train_time:125050ms step_avg:59.83ms
step:2091/2160 train_time:125138ms step_avg:59.85ms
step:2092/2160 train_time:125224ms step_avg:59.86ms
step:2093/2160 train_time:125313ms step_avg:59.87ms
step:2094/2160 train_time:125401ms step_avg:59.89ms
step:2095/2160 train_time:125489ms step_avg:59.90ms
step:2096/2160 train_time:125576ms step_avg:59.91ms
step:2097/2160 train_time:125664ms step_avg:59.93ms
step:2098/2160 train_time:125750ms step_avg:59.94ms
step:2099/2160 train_time:125840ms step_avg:59.95ms
step:2100/2160 train_time:125926ms step_avg:59.96ms
step:2101/2160 train_time:126014ms step_avg:59.98ms
step:2102/2160 train_time:126099ms step_avg:59.99ms
step:2103/2160 train_time:126188ms step_avg:60.00ms
step:2104/2160 train_time:126275ms step_avg:60.02ms
step:2105/2160 train_time:126363ms step_avg:60.03ms
step:2106/2160 train_time:126450ms step_avg:60.04ms
step:2107/2160 train_time:126539ms step_avg:60.06ms
step:2108/2160 train_time:126625ms step_avg:60.07ms
step:2109/2160 train_time:126713ms step_avg:60.08ms
step:2110/2160 train_time:126799ms step_avg:60.09ms
step:2111/2160 train_time:126888ms step_avg:60.11ms
step:2112/2160 train_time:126975ms step_avg:60.12ms
step:2113/2160 train_time:127062ms step_avg:60.13ms
step:2114/2160 train_time:127148ms step_avg:60.15ms
step:2115/2160 train_time:127236ms step_avg:60.16ms
step:2116/2160 train_time:127323ms step_avg:60.17ms
step:2117/2160 train_time:127411ms step_avg:60.18ms
step:2118/2160 train_time:127497ms step_avg:60.20ms
step:2119/2160 train_time:127586ms step_avg:60.21ms
step:2120/2160 train_time:127673ms step_avg:60.22ms
step:2121/2160 train_time:127761ms step_avg:60.24ms
step:2122/2160 train_time:127848ms step_avg:60.25ms
step:2123/2160 train_time:127936ms step_avg:60.26ms
step:2124/2160 train_time:128023ms step_avg:60.27ms
step:2125/2160 train_time:128113ms step_avg:60.29ms
step:2126/2160 train_time:128199ms step_avg:60.30ms
step:2127/2160 train_time:128288ms step_avg:60.31ms
step:2128/2160 train_time:128376ms step_avg:60.33ms
step:2129/2160 train_time:128465ms step_avg:60.34ms
step:2130/2160 train_time:128552ms step_avg:60.35ms
step:2131/2160 train_time:128641ms step_avg:60.37ms
step:2132/2160 train_time:128727ms step_avg:60.38ms
step:2133/2160 train_time:128816ms step_avg:60.39ms
step:2134/2160 train_time:128903ms step_avg:60.40ms
step:2135/2160 train_time:128991ms step_avg:60.42ms
step:2136/2160 train_time:129077ms step_avg:60.43ms
step:2137/2160 train_time:129166ms step_avg:60.44ms
step:2138/2160 train_time:129253ms step_avg:60.46ms
step:2139/2160 train_time:129341ms step_avg:60.47ms
step:2140/2160 train_time:129428ms step_avg:60.48ms
step:2141/2160 train_time:129518ms step_avg:60.49ms
step:2142/2160 train_time:129605ms step_avg:60.51ms
step:2143/2160 train_time:129693ms step_avg:60.52ms
step:2144/2160 train_time:129780ms step_avg:60.53ms
step:2145/2160 train_time:129869ms step_avg:60.54ms
step:2146/2160 train_time:129956ms step_avg:60.56ms
step:2147/2160 train_time:130044ms step_avg:60.57ms
step:2148/2160 train_time:130131ms step_avg:60.58ms
step:2149/2160 train_time:130220ms step_avg:60.60ms
step:2150/2160 train_time:130306ms step_avg:60.61ms
step:2151/2160 train_time:130394ms step_avg:60.62ms
step:2152/2160 train_time:130481ms step_avg:60.63ms
step:2153/2160 train_time:130570ms step_avg:60.65ms
step:2154/2160 train_time:130657ms step_avg:60.66ms
step:2155/2160 train_time:130746ms step_avg:60.67ms
step:2156/2160 train_time:130831ms step_avg:60.68ms
step:2157/2160 train_time:130921ms step_avg:60.70ms
step:2158/2160 train_time:131007ms step_avg:60.71ms
step:2159/2160 train_time:131096ms step_avg:60.72ms
step:2160/2160 train_time:131183ms step_avg:60.73ms
step:2160/2160 val_loss:3.2769 train_time:131273ms step_avg:60.77ms
peak memory allocated: 30078 MiB reserved: 44556 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 10 13:28:10 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                  Off |
| N/A   30C    P0            147W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                  Off |
| N/A   26C    P0            137W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                  Off |
| N/A   23C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                  Off |
| N/A   28C    P0            133W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                  Off |
| N/A   28C    P0            132W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                  Off |
| N/A   25C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                  Off |
| N/A   27C    P0            138W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                  Off |
| N/A   24C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     58057      C   /usr/bin/python3                             1510MiB |
|    0   N/A  N/A     58058      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     58059      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     58060      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     58061      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     58062      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     58063      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     58064      C   /usr/bin/python3                              614MiB |
|    1   N/A  N/A     58058      C   /usr/bin/python3                             1510MiB |
|    2   N/A  N/A     58059      C   /usr/bin/python3                             1510MiB |
|    3   N/A  N/A     58060      C   /usr/bin/python3                             1510MiB |
|    4   N/A  N/A     58061      C   /usr/bin/python3                             1510MiB |
|    5   N/A  N/A     58062      C   /usr/bin/python3                             1510MiB |
|    6   N/A  N/A     58063      C   /usr/bin/python3                             1510MiB |
|    7   N/A  N/A     58064      C   /usr/bin/python3                             1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2160 train_time:103ms step_avg:102.69ms
step:2/2160 train_time:124ms step_avg:61.85ms
step:3/2160 train_time:142ms step_avg:47.31ms
step:4/2160 train_time:171ms step_avg:42.63ms
step:5/2160 train_time:204ms step_avg:40.81ms
step:6/2160 train_time:275ms step_avg:45.79ms
step:7/2160 train_time:292ms step_avg:41.76ms
step:8/2160 train_time:324ms step_avg:40.45ms
step:9/2160 train_time:357ms step_avg:39.68ms
step:10/2160 train_time:390ms step_avg:38.99ms
step:11/2160 train_time:424ms step_avg:38.53ms
step:12/2160 train_time:457ms step_avg:38.06ms
step:13/2160 train_time:491ms step_avg:37.74ms
step:14/2160 train_time:524ms step_avg:37.41ms
step:15/2160 train_time:557ms step_avg:37.16ms
step:16/2160 train_time:590ms step_avg:36.89ms
step:17/2160 train_time:624ms step_avg:36.72ms
step:18/2160 train_time:657ms step_avg:36.51ms
step:19/2160 train_time:691ms step_avg:36.36ms
step:20/2160 train_time:724ms step_avg:36.20ms
step:21/2160 train_time:758ms step_avg:36.09ms
step:22/2160 train_time:791ms step_avg:35.94ms
step:23/2160 train_time:825ms step_avg:35.85ms
step:24/2160 train_time:858ms step_avg:35.73ms
step:25/2160 train_time:891ms step_avg:35.66ms
step:26/2160 train_time:924ms step_avg:35.55ms
step:27/2160 train_time:958ms step_avg:35.48ms
step:28/2160 train_time:991ms step_avg:35.39ms
step:29/2160 train_time:1025ms step_avg:35.34ms
step:30/2160 train_time:1058ms step_avg:35.26ms
step:31/2160 train_time:1091ms step_avg:35.21ms
step:32/2160 train_time:1124ms step_avg:35.13ms
step:33/2160 train_time:1158ms step_avg:35.10ms
step:34/2160 train_time:1191ms step_avg:35.03ms
step:35/2160 train_time:1225ms step_avg:35.00ms
step:36/2160 train_time:1258ms step_avg:34.95ms
step:37/2160 train_time:1292ms step_avg:34.92ms
step:38/2160 train_time:1325ms step_avg:34.87ms
step:39/2160 train_time:1359ms step_avg:34.84ms
step:40/2160 train_time:1392ms step_avg:34.79ms
step:41/2160 train_time:1426ms step_avg:34.78ms
step:42/2160 train_time:1459ms step_avg:34.74ms
step:43/2160 train_time:1493ms step_avg:34.72ms
step:44/2160 train_time:1526ms step_avg:34.68ms
step:45/2160 train_time:1560ms step_avg:34.66ms
step:46/2160 train_time:1593ms step_avg:34.62ms
step:47/2160 train_time:1627ms step_avg:34.61ms
step:48/2160 train_time:1660ms step_avg:34.58ms
step:49/2160 train_time:1694ms step_avg:34.57ms
step:50/2160 train_time:1727ms step_avg:34.54ms
step:51/2160 train_time:1761ms step_avg:34.52ms
step:52/2160 train_time:1793ms step_avg:34.49ms
step:53/2160 train_time:1828ms step_avg:34.48ms
step:54/2160 train_time:1861ms step_avg:34.46ms
step:55/2160 train_time:1894ms step_avg:34.44ms
step:56/2160 train_time:1927ms step_avg:34.41ms
step:57/2160 train_time:1961ms step_avg:34.40ms
step:58/2160 train_time:1994ms step_avg:34.37ms
step:59/2160 train_time:2028ms step_avg:34.37ms
step:60/2160 train_time:2061ms step_avg:34.35ms
step:61/2160 train_time:2095ms step_avg:34.34ms
step:62/2160 train_time:2128ms step_avg:34.31ms
step:63/2160 train_time:2161ms step_avg:34.30ms
step:64/2160 train_time:2194ms step_avg:34.28ms
step:65/2160 train_time:2228ms step_avg:34.28ms
step:66/2160 train_time:2261ms step_avg:34.26ms
step:67/2160 train_time:2295ms step_avg:34.25ms
step:68/2160 train_time:2328ms step_avg:34.23ms
step:69/2160 train_time:2362ms step_avg:34.23ms
step:70/2160 train_time:2395ms step_avg:34.21ms
step:71/2160 train_time:2429ms step_avg:34.21ms
step:72/2160 train_time:2462ms step_avg:34.19ms
step:73/2160 train_time:2496ms step_avg:34.19ms
step:74/2160 train_time:2529ms step_avg:34.17ms
step:75/2160 train_time:2563ms step_avg:34.17ms
step:76/2160 train_time:2595ms step_avg:34.15ms
step:77/2160 train_time:2629ms step_avg:34.15ms
step:78/2160 train_time:2662ms step_avg:34.13ms
step:79/2160 train_time:2697ms step_avg:34.13ms
step:80/2160 train_time:2729ms step_avg:34.12ms
step:81/2160 train_time:2763ms step_avg:34.11ms
step:82/2160 train_time:2796ms step_avg:34.10ms
step:83/2160 train_time:2830ms step_avg:34.09ms
step:84/2160 train_time:2863ms step_avg:34.08ms
step:85/2160 train_time:2896ms step_avg:34.08ms
step:86/2160 train_time:2929ms step_avg:34.06ms
step:87/2160 train_time:2963ms step_avg:34.06ms
step:88/2160 train_time:2996ms step_avg:34.04ms
step:89/2160 train_time:3029ms step_avg:34.04ms
step:90/2160 train_time:3062ms step_avg:34.03ms
step:91/2160 train_time:3096ms step_avg:34.02ms
step:92/2160 train_time:3129ms step_avg:34.01ms
step:93/2160 train_time:3163ms step_avg:34.01ms
step:94/2160 train_time:3196ms step_avg:34.00ms
step:95/2160 train_time:3230ms step_avg:34.00ms
step:96/2160 train_time:3263ms step_avg:33.99ms
step:97/2160 train_time:3297ms step_avg:33.99ms
step:98/2160 train_time:3330ms step_avg:33.97ms
step:99/2160 train_time:3363ms step_avg:33.97ms
step:100/2160 train_time:3396ms step_avg:33.96ms
step:101/2160 train_time:3430ms step_avg:33.96ms
step:102/2160 train_time:3463ms step_avg:33.95ms
step:103/2160 train_time:3497ms step_avg:33.95ms
step:104/2160 train_time:3529ms step_avg:33.94ms
step:105/2160 train_time:3563ms step_avg:33.94ms
step:106/2160 train_time:3596ms step_avg:33.93ms
step:107/2160 train_time:3630ms step_avg:33.92ms
step:108/2160 train_time:3663ms step_avg:33.91ms
step:109/2160 train_time:3697ms step_avg:33.91ms
step:110/2160 train_time:3729ms step_avg:33.90ms
step:111/2160 train_time:3763ms step_avg:33.90ms
step:112/2160 train_time:3796ms step_avg:33.89ms
step:113/2160 train_time:3830ms step_avg:33.89ms
step:114/2160 train_time:3863ms step_avg:33.89ms
step:115/2160 train_time:3896ms step_avg:33.88ms
step:116/2160 train_time:3929ms step_avg:33.87ms
step:117/2160 train_time:3963ms step_avg:33.87ms
step:118/2160 train_time:3996ms step_avg:33.87ms
step:119/2160 train_time:4030ms step_avg:33.86ms
step:120/2160 train_time:4063ms step_avg:33.86ms
step:121/2160 train_time:4096ms step_avg:33.85ms
step:122/2160 train_time:4129ms step_avg:33.85ms
step:123/2160 train_time:4163ms step_avg:33.84ms
step:124/2160 train_time:4196ms step_avg:33.84ms
step:125/2160 train_time:4229ms step_avg:33.84ms
step:126/2160 train_time:4262ms step_avg:33.83ms
step:127/2160 train_time:4296ms step_avg:33.83ms
step:128/2160 train_time:4329ms step_avg:33.82ms
step:129/2160 train_time:4363ms step_avg:33.82ms
step:130/2160 train_time:4396ms step_avg:33.81ms
step:131/2160 train_time:4429ms step_avg:33.81ms
step:132/2160 train_time:4462ms step_avg:33.80ms
step:133/2160 train_time:4496ms step_avg:33.81ms
step:134/2160 train_time:4529ms step_avg:33.80ms
step:135/2160 train_time:4563ms step_avg:33.80ms
step:136/2160 train_time:4596ms step_avg:33.79ms
step:137/2160 train_time:4630ms step_avg:33.79ms
step:138/2160 train_time:4662ms step_avg:33.79ms
step:139/2160 train_time:4696ms step_avg:33.79ms
step:140/2160 train_time:4729ms step_avg:33.78ms
step:141/2160 train_time:4763ms step_avg:33.78ms
step:142/2160 train_time:4795ms step_avg:33.77ms
step:143/2160 train_time:4829ms step_avg:33.77ms
step:144/2160 train_time:4862ms step_avg:33.76ms
step:145/2160 train_time:4896ms step_avg:33.76ms
step:146/2160 train_time:4929ms step_avg:33.76ms
step:147/2160 train_time:4963ms step_avg:33.76ms
step:148/2160 train_time:4995ms step_avg:33.75ms
step:149/2160 train_time:5029ms step_avg:33.75ms
step:150/2160 train_time:5062ms step_avg:33.75ms
step:151/2160 train_time:5148ms step_avg:34.09ms
step:152/2160 train_time:5166ms step_avg:33.99ms
step:153/2160 train_time:5197ms step_avg:33.97ms
step:154/2160 train_time:5229ms step_avg:33.96ms
step:155/2160 train_time:5263ms step_avg:33.95ms
step:156/2160 train_time:5296ms step_avg:33.95ms
step:157/2160 train_time:5329ms step_avg:33.95ms
step:158/2160 train_time:5362ms step_avg:33.94ms
step:159/2160 train_time:5396ms step_avg:33.94ms
step:160/2160 train_time:5429ms step_avg:33.93ms
step:161/2160 train_time:5462ms step_avg:33.93ms
step:162/2160 train_time:5495ms step_avg:33.92ms
step:163/2160 train_time:5529ms step_avg:33.92ms
step:164/2160 train_time:5562ms step_avg:33.92ms
step:165/2160 train_time:5595ms step_avg:33.91ms
step:166/2160 train_time:5628ms step_avg:33.90ms
step:167/2160 train_time:5662ms step_avg:33.90ms
step:168/2160 train_time:5694ms step_avg:33.90ms
step:169/2160 train_time:5728ms step_avg:33.89ms
step:170/2160 train_time:5761ms step_avg:33.89ms
step:171/2160 train_time:5795ms step_avg:33.89ms
step:172/2160 train_time:5827ms step_avg:33.88ms
step:173/2160 train_time:5861ms step_avg:33.88ms
step:174/2160 train_time:5894ms step_avg:33.87ms
step:175/2160 train_time:5927ms step_avg:33.87ms
step:176/2160 train_time:5960ms step_avg:33.87ms
step:177/2160 train_time:5994ms step_avg:33.86ms
step:178/2160 train_time:6026ms step_avg:33.86ms
step:179/2160 train_time:6060ms step_avg:33.86ms
step:180/2160 train_time:6093ms step_avg:33.85ms
step:181/2160 train_time:6127ms step_avg:33.85ms
step:182/2160 train_time:6160ms step_avg:33.85ms
step:183/2160 train_time:6193ms step_avg:33.84ms
step:184/2160 train_time:6226ms step_avg:33.84ms
step:185/2160 train_time:6260ms step_avg:33.84ms
step:186/2160 train_time:6293ms step_avg:33.83ms
step:187/2160 train_time:6327ms step_avg:33.83ms
step:188/2160 train_time:6359ms step_avg:33.83ms
step:189/2160 train_time:6394ms step_avg:33.83ms
step:190/2160 train_time:6426ms step_avg:33.82ms
step:191/2160 train_time:6460ms step_avg:33.82ms
step:192/2160 train_time:6493ms step_avg:33.82ms
step:193/2160 train_time:6526ms step_avg:33.81ms
step:194/2160 train_time:6559ms step_avg:33.81ms
step:195/2160 train_time:6593ms step_avg:33.81ms
step:196/2160 train_time:6626ms step_avg:33.81ms
step:197/2160 train_time:6659ms step_avg:33.80ms
step:198/2160 train_time:6692ms step_avg:33.80ms
step:199/2160 train_time:6726ms step_avg:33.80ms
step:200/2160 train_time:6758ms step_avg:33.79ms
step:201/2160 train_time:6792ms step_avg:33.79ms
step:202/2160 train_time:6825ms step_avg:33.79ms
step:203/2160 train_time:6858ms step_avg:33.78ms
step:204/2160 train_time:6891ms step_avg:33.78ms
step:205/2160 train_time:6925ms step_avg:33.78ms
step:206/2160 train_time:6957ms step_avg:33.77ms
step:207/2160 train_time:6991ms step_avg:33.77ms
step:208/2160 train_time:7024ms step_avg:33.77ms
step:209/2160 train_time:7058ms step_avg:33.77ms
step:210/2160 train_time:7091ms step_avg:33.76ms
step:211/2160 train_time:7124ms step_avg:33.76ms
step:212/2160 train_time:7157ms step_avg:33.76ms
step:213/2160 train_time:7191ms step_avg:33.76ms
step:214/2160 train_time:7223ms step_avg:33.75ms
step:215/2160 train_time:7257ms step_avg:33.75ms
step:216/2160 train_time:7290ms step_avg:33.75ms
step:217/2160 train_time:7323ms step_avg:33.75ms
step:218/2160 train_time:7356ms step_avg:33.74ms
step:219/2160 train_time:7390ms step_avg:33.74ms
step:220/2160 train_time:7423ms step_avg:33.74ms
step:221/2160 train_time:7457ms step_avg:33.74ms
step:222/2160 train_time:7489ms step_avg:33.74ms
step:223/2160 train_time:7523ms step_avg:33.74ms
step:224/2160 train_time:7556ms step_avg:33.73ms
step:225/2160 train_time:7590ms step_avg:33.73ms
step:226/2160 train_time:7623ms step_avg:33.73ms
step:227/2160 train_time:7656ms step_avg:33.73ms
step:228/2160 train_time:7689ms step_avg:33.72ms
step:229/2160 train_time:7723ms step_avg:33.72ms
step:230/2160 train_time:7756ms step_avg:33.72ms
step:231/2160 train_time:7789ms step_avg:33.72ms
step:232/2160 train_time:7822ms step_avg:33.72ms
step:233/2160 train_time:7856ms step_avg:33.72ms
step:234/2160 train_time:7889ms step_avg:33.71ms
step:235/2160 train_time:7923ms step_avg:33.71ms
step:236/2160 train_time:7955ms step_avg:33.71ms
step:237/2160 train_time:7989ms step_avg:33.71ms
step:238/2160 train_time:8022ms step_avg:33.71ms
step:239/2160 train_time:8056ms step_avg:33.71ms
step:240/2160 train_time:8089ms step_avg:33.70ms
step:241/2160 train_time:8123ms step_avg:33.70ms
step:242/2160 train_time:8155ms step_avg:33.70ms
step:243/2160 train_time:8189ms step_avg:33.70ms
step:244/2160 train_time:8222ms step_avg:33.70ms
step:245/2160 train_time:8256ms step_avg:33.70ms
step:246/2160 train_time:8289ms step_avg:33.69ms
step:247/2160 train_time:8322ms step_avg:33.69ms
step:248/2160 train_time:8355ms step_avg:33.69ms
step:249/2160 train_time:8389ms step_avg:33.69ms
step:250/2160 train_time:8422ms step_avg:33.69ms
step:250/2160 val_loss:4.3813 train_time:8457ms step_avg:33.83ms
step:251/2160 train_time:8476ms step_avg:33.77ms
step:252/2160 train_time:8495ms step_avg:33.71ms
step:253/2160 train_time:8528ms step_avg:33.71ms
step:254/2160 train_time:8562ms step_avg:33.71ms
step:255/2160 train_time:8597ms step_avg:33.71ms
step:256/2160 train_time:8630ms step_avg:33.71ms
step:257/2160 train_time:8665ms step_avg:33.72ms
step:258/2160 train_time:8698ms step_avg:33.71ms
step:259/2160 train_time:8732ms step_avg:33.71ms
step:260/2160 train_time:8765ms step_avg:33.71ms
step:261/2160 train_time:8799ms step_avg:33.71ms
step:262/2160 train_time:8831ms step_avg:33.71ms
step:263/2160 train_time:8865ms step_avg:33.71ms
step:264/2160 train_time:8898ms step_avg:33.70ms
step:265/2160 train_time:8931ms step_avg:33.70ms
step:266/2160 train_time:8964ms step_avg:33.70ms
step:267/2160 train_time:8998ms step_avg:33.70ms
step:268/2160 train_time:9031ms step_avg:33.70ms
step:269/2160 train_time:9064ms step_avg:33.70ms
step:270/2160 train_time:9097ms step_avg:33.69ms
step:271/2160 train_time:9131ms step_avg:33.69ms
step:272/2160 train_time:9164ms step_avg:33.69ms
step:273/2160 train_time:9197ms step_avg:33.69ms
step:274/2160 train_time:9230ms step_avg:33.69ms
step:275/2160 train_time:9264ms step_avg:33.69ms
step:276/2160 train_time:9303ms step_avg:33.71ms
step:277/2160 train_time:9330ms step_avg:33.68ms
step:278/2160 train_time:9363ms step_avg:33.68ms
step:279/2160 train_time:9397ms step_avg:33.68ms
step:280/2160 train_time:9430ms step_avg:33.68ms
step:281/2160 train_time:9463ms step_avg:33.68ms
step:282/2160 train_time:9496ms step_avg:33.67ms
step:283/2160 train_time:9530ms step_avg:33.67ms
step:284/2160 train_time:9562ms step_avg:33.67ms
step:285/2160 train_time:9596ms step_avg:33.67ms
step:286/2160 train_time:9629ms step_avg:33.67ms
step:287/2160 train_time:9662ms step_avg:33.67ms
step:288/2160 train_time:9695ms step_avg:33.66ms
step:289/2160 train_time:9729ms step_avg:33.66ms
step:290/2160 train_time:9761ms step_avg:33.66ms
step:291/2160 train_time:9795ms step_avg:33.66ms
step:292/2160 train_time:9828ms step_avg:33.66ms
step:293/2160 train_time:9861ms step_avg:33.66ms
step:294/2160 train_time:9894ms step_avg:33.65ms
step:295/2160 train_time:9928ms step_avg:33.65ms
step:296/2160 train_time:9960ms step_avg:33.65ms
step:297/2160 train_time:9994ms step_avg:33.65ms
step:298/2160 train_time:10027ms step_avg:33.65ms
step:299/2160 train_time:10061ms step_avg:33.65ms
step:300/2160 train_time:10093ms step_avg:33.64ms
step:301/2160 train_time:10127ms step_avg:33.64ms
step:302/2160 train_time:10160ms step_avg:33.64ms
step:303/2160 train_time:10193ms step_avg:33.64ms
step:304/2160 train_time:10226ms step_avg:33.64ms
step:305/2160 train_time:10260ms step_avg:33.64ms
step:306/2160 train_time:10292ms step_avg:33.64ms
step:307/2160 train_time:10326ms step_avg:33.64ms
step:308/2160 train_time:10359ms step_avg:33.63ms
step:309/2160 train_time:10393ms step_avg:33.63ms
step:310/2160 train_time:10425ms step_avg:33.63ms
step:311/2160 train_time:10459ms step_avg:33.63ms
step:312/2160 train_time:10492ms step_avg:33.63ms
step:313/2160 train_time:10526ms step_avg:33.63ms
step:314/2160 train_time:10558ms step_avg:33.63ms
step:315/2160 train_time:10592ms step_avg:33.63ms
step:316/2160 train_time:10625ms step_avg:33.62ms
step:317/2160 train_time:10659ms step_avg:33.62ms
step:318/2160 train_time:10692ms step_avg:33.62ms
step:319/2160 train_time:10725ms step_avg:33.62ms
step:320/2160 train_time:10758ms step_avg:33.62ms
step:321/2160 train_time:10792ms step_avg:33.62ms
step:322/2160 train_time:10825ms step_avg:33.62ms
step:323/2160 train_time:10858ms step_avg:33.62ms
step:324/2160 train_time:10891ms step_avg:33.61ms
step:325/2160 train_time:10924ms step_avg:33.61ms
step:326/2160 train_time:10957ms step_avg:33.61ms
step:327/2160 train_time:10991ms step_avg:33.61ms
step:328/2160 train_time:11024ms step_avg:33.61ms
step:329/2160 train_time:11057ms step_avg:33.61ms
step:330/2160 train_time:11090ms step_avg:33.61ms
step:331/2160 train_time:11124ms step_avg:33.61ms
step:332/2160 train_time:11157ms step_avg:33.60ms
step:333/2160 train_time:11190ms step_avg:33.60ms
step:334/2160 train_time:11223ms step_avg:33.60ms
step:335/2160 train_time:11257ms step_avg:33.60ms
step:336/2160 train_time:11289ms step_avg:33.60ms
step:337/2160 train_time:11323ms step_avg:33.60ms
step:338/2160 train_time:11356ms step_avg:33.60ms
step:339/2160 train_time:11389ms step_avg:33.60ms
step:340/2160 train_time:11422ms step_avg:33.59ms
step:341/2160 train_time:11456ms step_avg:33.59ms
step:342/2160 train_time:11488ms step_avg:33.59ms
step:343/2160 train_time:11522ms step_avg:33.59ms
step:344/2160 train_time:11555ms step_avg:33.59ms
step:345/2160 train_time:11588ms step_avg:33.59ms
step:346/2160 train_time:11621ms step_avg:33.59ms
step:347/2160 train_time:11655ms step_avg:33.59ms
step:348/2160 train_time:11687ms step_avg:33.58ms
step:349/2160 train_time:11721ms step_avg:33.59ms
step:350/2160 train_time:11754ms step_avg:33.58ms
step:351/2160 train_time:11788ms step_avg:33.58ms
step:352/2160 train_time:11820ms step_avg:33.58ms
step:353/2160 train_time:11854ms step_avg:33.58ms
step:354/2160 train_time:11887ms step_avg:33.58ms
step:355/2160 train_time:11920ms step_avg:33.58ms
step:356/2160 train_time:11953ms step_avg:33.58ms
step:357/2160 train_time:11986ms step_avg:33.58ms
step:358/2160 train_time:12019ms step_avg:33.57ms
step:359/2160 train_time:12053ms step_avg:33.57ms
step:360/2160 train_time:12086ms step_avg:33.57ms
step:361/2160 train_time:12119ms step_avg:33.57ms
step:362/2160 train_time:12152ms step_avg:33.57ms
step:363/2160 train_time:12186ms step_avg:33.57ms
step:364/2160 train_time:12218ms step_avg:33.57ms
step:365/2160 train_time:12252ms step_avg:33.57ms
step:366/2160 train_time:12285ms step_avg:33.57ms
step:367/2160 train_time:12319ms step_avg:33.57ms
step:368/2160 train_time:12352ms step_avg:33.56ms
step:369/2160 train_time:12385ms step_avg:33.56ms
step:370/2160 train_time:12418ms step_avg:33.56ms
step:371/2160 train_time:12452ms step_avg:33.56ms
step:372/2160 train_time:12485ms step_avg:33.56ms
step:373/2160 train_time:12519ms step_avg:33.56ms
step:374/2160 train_time:12551ms step_avg:33.56ms
step:375/2160 train_time:12585ms step_avg:33.56ms
step:376/2160 train_time:12618ms step_avg:33.56ms
step:377/2160 train_time:12651ms step_avg:33.56ms
step:378/2160 train_time:12684ms step_avg:33.56ms
step:379/2160 train_time:12718ms step_avg:33.56ms
step:380/2160 train_time:12751ms step_avg:33.55ms
step:381/2160 train_time:12785ms step_avg:33.56ms
step:382/2160 train_time:12817ms step_avg:33.55ms
step:383/2160 train_time:12851ms step_avg:33.55ms
step:384/2160 train_time:12884ms step_avg:33.55ms
step:385/2160 train_time:12917ms step_avg:33.55ms
step:386/2160 train_time:12950ms step_avg:33.55ms
step:387/2160 train_time:12984ms step_avg:33.55ms
step:388/2160 train_time:13017ms step_avg:33.55ms
step:389/2160 train_time:13050ms step_avg:33.55ms
step:390/2160 train_time:13083ms step_avg:33.55ms
step:391/2160 train_time:13117ms step_avg:33.55ms
step:392/2160 train_time:13150ms step_avg:33.54ms
step:393/2160 train_time:13183ms step_avg:33.55ms
step:394/2160 train_time:13216ms step_avg:33.54ms
step:395/2160 train_time:13250ms step_avg:33.54ms
step:396/2160 train_time:13283ms step_avg:33.54ms
step:397/2160 train_time:13316ms step_avg:33.54ms
step:398/2160 train_time:13349ms step_avg:33.54ms
step:399/2160 train_time:13383ms step_avg:33.54ms
step:400/2160 train_time:13415ms step_avg:33.54ms
step:401/2160 train_time:13449ms step_avg:33.54ms
step:402/2160 train_time:13481ms step_avg:33.54ms
step:403/2160 train_time:13515ms step_avg:33.54ms
step:404/2160 train_time:13548ms step_avg:33.53ms
step:405/2160 train_time:13582ms step_avg:33.53ms
step:406/2160 train_time:13615ms step_avg:33.53ms
step:407/2160 train_time:13648ms step_avg:33.53ms
step:408/2160 train_time:13681ms step_avg:33.53ms
step:409/2160 train_time:13715ms step_avg:33.53ms
step:410/2160 train_time:13747ms step_avg:33.53ms
step:411/2160 train_time:13781ms step_avg:33.53ms
step:412/2160 train_time:13814ms step_avg:33.53ms
step:413/2160 train_time:13848ms step_avg:33.53ms
step:414/2160 train_time:13880ms step_avg:33.53ms
step:415/2160 train_time:13914ms step_avg:33.53ms
step:416/2160 train_time:13947ms step_avg:33.53ms
step:417/2160 train_time:13980ms step_avg:33.53ms
step:418/2160 train_time:14013ms step_avg:33.52ms
step:419/2160 train_time:14047ms step_avg:33.53ms
step:420/2160 train_time:14080ms step_avg:33.52ms
step:421/2160 train_time:14114ms step_avg:33.53ms
step:422/2160 train_time:14147ms step_avg:33.52ms
step:423/2160 train_time:14181ms step_avg:33.52ms
step:424/2160 train_time:14214ms step_avg:33.52ms
step:425/2160 train_time:14248ms step_avg:33.52ms
step:426/2160 train_time:14280ms step_avg:33.52ms
step:427/2160 train_time:14314ms step_avg:33.52ms
step:428/2160 train_time:14347ms step_avg:33.52ms
step:429/2160 train_time:14381ms step_avg:33.52ms
step:430/2160 train_time:14414ms step_avg:33.52ms
step:431/2160 train_time:14448ms step_avg:33.52ms
step:432/2160 train_time:14481ms step_avg:33.52ms
step:433/2160 train_time:14514ms step_avg:33.52ms
step:434/2160 train_time:14547ms step_avg:33.52ms
step:435/2160 train_time:14581ms step_avg:33.52ms
step:436/2160 train_time:14614ms step_avg:33.52ms
step:437/2160 train_time:14647ms step_avg:33.52ms
step:438/2160 train_time:14680ms step_avg:33.52ms
step:439/2160 train_time:14714ms step_avg:33.52ms
step:440/2160 train_time:14747ms step_avg:33.52ms
step:441/2160 train_time:14780ms step_avg:33.52ms
step:442/2160 train_time:14813ms step_avg:33.51ms
step:443/2160 train_time:14846ms step_avg:33.51ms
step:444/2160 train_time:14879ms step_avg:33.51ms
step:445/2160 train_time:14913ms step_avg:33.51ms
step:446/2160 train_time:14946ms step_avg:33.51ms
step:447/2160 train_time:14979ms step_avg:33.51ms
step:448/2160 train_time:15012ms step_avg:33.51ms
step:449/2160 train_time:15046ms step_avg:33.51ms
step:450/2160 train_time:15078ms step_avg:33.51ms
step:451/2160 train_time:15112ms step_avg:33.51ms
step:452/2160 train_time:15145ms step_avg:33.51ms
step:453/2160 train_time:15179ms step_avg:33.51ms
step:454/2160 train_time:15211ms step_avg:33.51ms
step:455/2160 train_time:15245ms step_avg:33.51ms
step:456/2160 train_time:15278ms step_avg:33.50ms
step:457/2160 train_time:15312ms step_avg:33.50ms
step:458/2160 train_time:15344ms step_avg:33.50ms
step:459/2160 train_time:15378ms step_avg:33.50ms
step:460/2160 train_time:15411ms step_avg:33.50ms
step:461/2160 train_time:15445ms step_avg:33.50ms
step:462/2160 train_time:15478ms step_avg:33.50ms
step:463/2160 train_time:15512ms step_avg:33.50ms
step:464/2160 train_time:15545ms step_avg:33.50ms
step:465/2160 train_time:15578ms step_avg:33.50ms
step:466/2160 train_time:15611ms step_avg:33.50ms
step:467/2160 train_time:15645ms step_avg:33.50ms
step:468/2160 train_time:15678ms step_avg:33.50ms
step:469/2160 train_time:15712ms step_avg:33.50ms
step:470/2160 train_time:15745ms step_avg:33.50ms
step:471/2160 train_time:15778ms step_avg:33.50ms
step:472/2160 train_time:15811ms step_avg:33.50ms
step:473/2160 train_time:15845ms step_avg:33.50ms
step:474/2160 train_time:15878ms step_avg:33.50ms
step:475/2160 train_time:15912ms step_avg:33.50ms
step:476/2160 train_time:15944ms step_avg:33.50ms
step:477/2160 train_time:15978ms step_avg:33.50ms
step:478/2160 train_time:16011ms step_avg:33.50ms
step:479/2160 train_time:16045ms step_avg:33.50ms
step:480/2160 train_time:16077ms step_avg:33.49ms
step:481/2160 train_time:16111ms step_avg:33.49ms
step:482/2160 train_time:16144ms step_avg:33.49ms
step:483/2160 train_time:16178ms step_avg:33.49ms
step:484/2160 train_time:16210ms step_avg:33.49ms
step:485/2160 train_time:16244ms step_avg:33.49ms
step:486/2160 train_time:16277ms step_avg:33.49ms
step:487/2160 train_time:16311ms step_avg:33.49ms
step:488/2160 train_time:16343ms step_avg:33.49ms
step:489/2160 train_time:16377ms step_avg:33.49ms
step:490/2160 train_time:16410ms step_avg:33.49ms
step:491/2160 train_time:16444ms step_avg:33.49ms
step:492/2160 train_time:16477ms step_avg:33.49ms
step:493/2160 train_time:16510ms step_avg:33.49ms
step:494/2160 train_time:16543ms step_avg:33.49ms
step:495/2160 train_time:16577ms step_avg:33.49ms
step:496/2160 train_time:16610ms step_avg:33.49ms
step:497/2160 train_time:16643ms step_avg:33.49ms
step:498/2160 train_time:16676ms step_avg:33.49ms
step:499/2160 train_time:16710ms step_avg:33.49ms
step:500/2160 train_time:16743ms step_avg:33.49ms
step:500/2160 val_loss:4.0101 train_time:16778ms step_avg:33.56ms
step:501/2160 train_time:16797ms step_avg:33.53ms
step:502/2160 train_time:16815ms step_avg:33.50ms
step:503/2160 train_time:16849ms step_avg:33.50ms
step:504/2160 train_time:16882ms step_avg:33.50ms
step:505/2160 train_time:16918ms step_avg:33.50ms
step:506/2160 train_time:16952ms step_avg:33.50ms
step:507/2160 train_time:16986ms step_avg:33.50ms
step:508/2160 train_time:17019ms step_avg:33.50ms
step:509/2160 train_time:17053ms step_avg:33.50ms
step:510/2160 train_time:17086ms step_avg:33.50ms
step:511/2160 train_time:17119ms step_avg:33.50ms
step:512/2160 train_time:17152ms step_avg:33.50ms
step:513/2160 train_time:17185ms step_avg:33.50ms
step:514/2160 train_time:17218ms step_avg:33.50ms
step:515/2160 train_time:17252ms step_avg:33.50ms
step:516/2160 train_time:17285ms step_avg:33.50ms
step:517/2160 train_time:17318ms step_avg:33.50ms
step:518/2160 train_time:17351ms step_avg:33.50ms
step:519/2160 train_time:17385ms step_avg:33.50ms
step:520/2160 train_time:17417ms step_avg:33.49ms
step:521/2160 train_time:17451ms step_avg:33.50ms
step:522/2160 train_time:17484ms step_avg:33.49ms
step:523/2160 train_time:17517ms step_avg:33.49ms
step:524/2160 train_time:17550ms step_avg:33.49ms
step:525/2160 train_time:17584ms step_avg:33.49ms
step:526/2160 train_time:17617ms step_avg:33.49ms
step:527/2160 train_time:17650ms step_avg:33.49ms
step:528/2160 train_time:17683ms step_avg:33.49ms
step:529/2160 train_time:17717ms step_avg:33.49ms
step:530/2160 train_time:17749ms step_avg:33.49ms
step:531/2160 train_time:17783ms step_avg:33.49ms
step:532/2160 train_time:17816ms step_avg:33.49ms
step:533/2160 train_time:17849ms step_avg:33.49ms
step:534/2160 train_time:17882ms step_avg:33.49ms
step:535/2160 train_time:17916ms step_avg:33.49ms
step:536/2160 train_time:17949ms step_avg:33.49ms
step:537/2160 train_time:17983ms step_avg:33.49ms
step:538/2160 train_time:18016ms step_avg:33.49ms
step:539/2160 train_time:18050ms step_avg:33.49ms
step:540/2160 train_time:18083ms step_avg:33.49ms
step:541/2160 train_time:18117ms step_avg:33.49ms
step:542/2160 train_time:18150ms step_avg:33.49ms
step:543/2160 train_time:18183ms step_avg:33.49ms
step:544/2160 train_time:18216ms step_avg:33.49ms
step:545/2160 train_time:18250ms step_avg:33.49ms
step:546/2160 train_time:18283ms step_avg:33.48ms
step:547/2160 train_time:18317ms step_avg:33.49ms
step:548/2160 train_time:18349ms step_avg:33.48ms
step:549/2160 train_time:18383ms step_avg:33.48ms
step:550/2160 train_time:18417ms step_avg:33.49ms
step:551/2160 train_time:18450ms step_avg:33.48ms
step:552/2160 train_time:18483ms step_avg:33.48ms
step:553/2160 train_time:18517ms step_avg:33.48ms
step:554/2160 train_time:18599ms step_avg:33.57ms
step:555/2160 train_time:18616ms step_avg:33.54ms
step:556/2160 train_time:18647ms step_avg:33.54ms
step:557/2160 train_time:18681ms step_avg:33.54ms
step:558/2160 train_time:18713ms step_avg:33.54ms
step:559/2160 train_time:18747ms step_avg:33.54ms
step:560/2160 train_time:18780ms step_avg:33.53ms
step:561/2160 train_time:18813ms step_avg:33.54ms
step:562/2160 train_time:18846ms step_avg:33.53ms
step:563/2160 train_time:18880ms step_avg:33.53ms
step:564/2160 train_time:18913ms step_avg:33.53ms
step:565/2160 train_time:18946ms step_avg:33.53ms
step:566/2160 train_time:18979ms step_avg:33.53ms
step:567/2160 train_time:19012ms step_avg:33.53ms
step:568/2160 train_time:19045ms step_avg:33.53ms
step:569/2160 train_time:19079ms step_avg:33.53ms
step:570/2160 train_time:19111ms step_avg:33.53ms
step:571/2160 train_time:19145ms step_avg:33.53ms
step:572/2160 train_time:19178ms step_avg:33.53ms
step:573/2160 train_time:19211ms step_avg:33.53ms
step:574/2160 train_time:19244ms step_avg:33.53ms
step:575/2160 train_time:19278ms step_avg:33.53ms
step:576/2160 train_time:19310ms step_avg:33.53ms
step:577/2160 train_time:19344ms step_avg:33.52ms
step:578/2160 train_time:19377ms step_avg:33.52ms
step:579/2160 train_time:19410ms step_avg:33.52ms
step:580/2160 train_time:19443ms step_avg:33.52ms
step:581/2160 train_time:19477ms step_avg:33.52ms
step:582/2160 train_time:19509ms step_avg:33.52ms
step:583/2160 train_time:19543ms step_avg:33.52ms
step:584/2160 train_time:19576ms step_avg:33.52ms
step:585/2160 train_time:19610ms step_avg:33.52ms
step:586/2160 train_time:19643ms step_avg:33.52ms
step:587/2160 train_time:19677ms step_avg:33.52ms
step:588/2160 train_time:19710ms step_avg:33.52ms
step:589/2160 train_time:19743ms step_avg:33.52ms
step:590/2160 train_time:19776ms step_avg:33.52ms
step:591/2160 train_time:19810ms step_avg:33.52ms
step:592/2160 train_time:19843ms step_avg:33.52ms
step:593/2160 train_time:19876ms step_avg:33.52ms
step:594/2160 train_time:19909ms step_avg:33.52ms
step:595/2160 train_time:19943ms step_avg:33.52ms
step:596/2160 train_time:19976ms step_avg:33.52ms
step:597/2160 train_time:20010ms step_avg:33.52ms
step:598/2160 train_time:20042ms step_avg:33.52ms
step:599/2160 train_time:20076ms step_avg:33.52ms
step:600/2160 train_time:20109ms step_avg:33.52ms
step:601/2160 train_time:20143ms step_avg:33.52ms
step:602/2160 train_time:20176ms step_avg:33.51ms
step:603/2160 train_time:20210ms step_avg:33.51ms
step:604/2160 train_time:20242ms step_avg:33.51ms
step:605/2160 train_time:20276ms step_avg:33.51ms
step:606/2160 train_time:20309ms step_avg:33.51ms
step:607/2160 train_time:20342ms step_avg:33.51ms
step:608/2160 train_time:20375ms step_avg:33.51ms
step:609/2160 train_time:20409ms step_avg:33.51ms
step:610/2160 train_time:20442ms step_avg:33.51ms
step:611/2160 train_time:20476ms step_avg:33.51ms
step:612/2160 train_time:20509ms step_avg:33.51ms
step:613/2160 train_time:20542ms step_avg:33.51ms
step:614/2160 train_time:20575ms step_avg:33.51ms
step:615/2160 train_time:20609ms step_avg:33.51ms
step:616/2160 train_time:20642ms step_avg:33.51ms
step:617/2160 train_time:20676ms step_avg:33.51ms
step:618/2160 train_time:20709ms step_avg:33.51ms
step:619/2160 train_time:20743ms step_avg:33.51ms
step:620/2160 train_time:20775ms step_avg:33.51ms
step:621/2160 train_time:20809ms step_avg:33.51ms
step:622/2160 train_time:20841ms step_avg:33.51ms
step:623/2160 train_time:20875ms step_avg:33.51ms
step:624/2160 train_time:20908ms step_avg:33.51ms
step:625/2160 train_time:20942ms step_avg:33.51ms
step:626/2160 train_time:20975ms step_avg:33.51ms
step:627/2160 train_time:21008ms step_avg:33.51ms
step:628/2160 train_time:21041ms step_avg:33.50ms
step:629/2160 train_time:21075ms step_avg:33.51ms
step:630/2160 train_time:21107ms step_avg:33.50ms
step:631/2160 train_time:21141ms step_avg:33.50ms
step:632/2160 train_time:21174ms step_avg:33.50ms
step:633/2160 train_time:21208ms step_avg:33.50ms
step:634/2160 train_time:21241ms step_avg:33.50ms
step:635/2160 train_time:21275ms step_avg:33.50ms
step:636/2160 train_time:21307ms step_avg:33.50ms
step:637/2160 train_time:21341ms step_avg:33.50ms
step:638/2160 train_time:21374ms step_avg:33.50ms
step:639/2160 train_time:21407ms step_avg:33.50ms
step:640/2160 train_time:21440ms step_avg:33.50ms
step:641/2160 train_time:21474ms step_avg:33.50ms
step:642/2160 train_time:21507ms step_avg:33.50ms
step:643/2160 train_time:21540ms step_avg:33.50ms
step:644/2160 train_time:21573ms step_avg:33.50ms
step:645/2160 train_time:21607ms step_avg:33.50ms
step:646/2160 train_time:21640ms step_avg:33.50ms
step:647/2160 train_time:21673ms step_avg:33.50ms
step:648/2160 train_time:21706ms step_avg:33.50ms
step:649/2160 train_time:21740ms step_avg:33.50ms
step:650/2160 train_time:21773ms step_avg:33.50ms
step:651/2160 train_time:21806ms step_avg:33.50ms
step:652/2160 train_time:21839ms step_avg:33.50ms
step:653/2160 train_time:21873ms step_avg:33.50ms
step:654/2160 train_time:21906ms step_avg:33.49ms
step:655/2160 train_time:21939ms step_avg:33.49ms
step:656/2160 train_time:21972ms step_avg:33.49ms
step:657/2160 train_time:22006ms step_avg:33.49ms
step:658/2160 train_time:22039ms step_avg:33.49ms
step:659/2160 train_time:22072ms step_avg:33.49ms
step:660/2160 train_time:22105ms step_avg:33.49ms
step:661/2160 train_time:22139ms step_avg:33.49ms
step:662/2160 train_time:22171ms step_avg:33.49ms
step:663/2160 train_time:22205ms step_avg:33.49ms
step:664/2160 train_time:22238ms step_avg:33.49ms
step:665/2160 train_time:22272ms step_avg:33.49ms
step:666/2160 train_time:22304ms step_avg:33.49ms
step:667/2160 train_time:22338ms step_avg:33.49ms
step:668/2160 train_time:22371ms step_avg:33.49ms
step:669/2160 train_time:22404ms step_avg:33.49ms
step:670/2160 train_time:22437ms step_avg:33.49ms
step:671/2160 train_time:22471ms step_avg:33.49ms
step:672/2160 train_time:22504ms step_avg:33.49ms
step:673/2160 train_time:22537ms step_avg:33.49ms
step:674/2160 train_time:22570ms step_avg:33.49ms
step:675/2160 train_time:22604ms step_avg:33.49ms
step:676/2160 train_time:22637ms step_avg:33.49ms
step:677/2160 train_time:22670ms step_avg:33.49ms
step:678/2160 train_time:22703ms step_avg:33.49ms
step:679/2160 train_time:22737ms step_avg:33.49ms
step:680/2160 train_time:22770ms step_avg:33.49ms
step:681/2160 train_time:22804ms step_avg:33.49ms
step:682/2160 train_time:22837ms step_avg:33.48ms
step:683/2160 train_time:22870ms step_avg:33.48ms
step:684/2160 train_time:22903ms step_avg:33.48ms
step:685/2160 train_time:22937ms step_avg:33.48ms
step:686/2160 train_time:22970ms step_avg:33.48ms
step:687/2160 train_time:23004ms step_avg:33.48ms
step:688/2160 train_time:23036ms step_avg:33.48ms
step:689/2160 train_time:23070ms step_avg:33.48ms
step:690/2160 train_time:23103ms step_avg:33.48ms
step:691/2160 train_time:23137ms step_avg:33.48ms
step:692/2160 train_time:23170ms step_avg:33.48ms
step:693/2160 train_time:23203ms step_avg:33.48ms
step:694/2160 train_time:23236ms step_avg:33.48ms
step:695/2160 train_time:23270ms step_avg:33.48ms
step:696/2160 train_time:23302ms step_avg:33.48ms
step:697/2160 train_time:23336ms step_avg:33.48ms
step:698/2160 train_time:23369ms step_avg:33.48ms
step:699/2160 train_time:23403ms step_avg:33.48ms
step:700/2160 train_time:23436ms step_avg:33.48ms
step:701/2160 train_time:23469ms step_avg:33.48ms
step:702/2160 train_time:23502ms step_avg:33.48ms
step:703/2160 train_time:23536ms step_avg:33.48ms
step:704/2160 train_time:23569ms step_avg:33.48ms
step:705/2160 train_time:23602ms step_avg:33.48ms
step:706/2160 train_time:23635ms step_avg:33.48ms
step:707/2160 train_time:23669ms step_avg:33.48ms
step:708/2160 train_time:23702ms step_avg:33.48ms
step:709/2160 train_time:23761ms step_avg:33.51ms
step:710/2160 train_time:23820ms step_avg:33.55ms
step:711/2160 train_time:23881ms step_avg:33.59ms
step:712/2160 train_time:23940ms step_avg:33.62ms
step:713/2160 train_time:24001ms step_avg:33.66ms
step:714/2160 train_time:24060ms step_avg:33.70ms
step:715/2160 train_time:24120ms step_avg:33.73ms
step:716/2160 train_time:24178ms step_avg:33.77ms
step:717/2160 train_time:24239ms step_avg:33.81ms
step:718/2160 train_time:24298ms step_avg:33.84ms
step:719/2160 train_time:24359ms step_avg:33.88ms
step:720/2160 train_time:24418ms step_avg:33.91ms
step:721/2160 train_time:24478ms step_avg:33.95ms
step:722/2160 train_time:24538ms step_avg:33.99ms
step:723/2160 train_time:24598ms step_avg:34.02ms
step:724/2160 train_time:24657ms step_avg:34.06ms
step:725/2160 train_time:24716ms step_avg:34.09ms
step:726/2160 train_time:24775ms step_avg:34.13ms
step:727/2160 train_time:24835ms step_avg:34.16ms
step:728/2160 train_time:24894ms step_avg:34.20ms
step:729/2160 train_time:24956ms step_avg:34.23ms
step:730/2160 train_time:25015ms step_avg:34.27ms
step:731/2160 train_time:25075ms step_avg:34.30ms
step:732/2160 train_time:25134ms step_avg:34.34ms
step:733/2160 train_time:25195ms step_avg:34.37ms
step:734/2160 train_time:25253ms step_avg:34.40ms
step:735/2160 train_time:25314ms step_avg:34.44ms
step:736/2160 train_time:25372ms step_avg:34.47ms
step:737/2160 train_time:25433ms step_avg:34.51ms
step:738/2160 train_time:25492ms step_avg:34.54ms
step:739/2160 train_time:25553ms step_avg:34.58ms
step:740/2160 train_time:25612ms step_avg:34.61ms
step:741/2160 train_time:25672ms step_avg:34.64ms
step:742/2160 train_time:25730ms step_avg:34.68ms
step:743/2160 train_time:25791ms step_avg:34.71ms
step:744/2160 train_time:25849ms step_avg:34.74ms
step:745/2160 train_time:25910ms step_avg:34.78ms
step:746/2160 train_time:25969ms step_avg:34.81ms
step:747/2160 train_time:26029ms step_avg:34.85ms
step:748/2160 train_time:26088ms step_avg:34.88ms
step:749/2160 train_time:26148ms step_avg:34.91ms
step:750/2160 train_time:26207ms step_avg:34.94ms
step:750/2160 val_loss:3.8463 train_time:26268ms step_avg:35.02ms
step:751/2160 train_time:26288ms step_avg:35.00ms
step:752/2160 train_time:26329ms step_avg:35.01ms
step:753/2160 train_time:26392ms step_avg:35.05ms
step:754/2160 train_time:26454ms step_avg:35.09ms
step:755/2160 train_time:26516ms step_avg:35.12ms
step:756/2160 train_time:26575ms step_avg:35.15ms
step:757/2160 train_time:26635ms step_avg:35.18ms
step:758/2160 train_time:26693ms step_avg:35.22ms
step:759/2160 train_time:26754ms step_avg:35.25ms
step:760/2160 train_time:26812ms step_avg:35.28ms
step:761/2160 train_time:26872ms step_avg:35.31ms
step:762/2160 train_time:26929ms step_avg:35.34ms
step:763/2160 train_time:26989ms step_avg:35.37ms
step:764/2160 train_time:27048ms step_avg:35.40ms
step:765/2160 train_time:27108ms step_avg:35.44ms
step:766/2160 train_time:27166ms step_avg:35.46ms
step:767/2160 train_time:27227ms step_avg:35.50ms
step:768/2160 train_time:27286ms step_avg:35.53ms
step:769/2160 train_time:27347ms step_avg:35.56ms
step:770/2160 train_time:27407ms step_avg:35.59ms
step:771/2160 train_time:27469ms step_avg:35.63ms
step:772/2160 train_time:27528ms step_avg:35.66ms
step:773/2160 train_time:27590ms step_avg:35.69ms
step:774/2160 train_time:27648ms step_avg:35.72ms
step:775/2160 train_time:27710ms step_avg:35.75ms
step:776/2160 train_time:27769ms step_avg:35.78ms
step:777/2160 train_time:27829ms step_avg:35.82ms
step:778/2160 train_time:27887ms step_avg:35.84ms
step:779/2160 train_time:27947ms step_avg:35.87ms
step:780/2160 train_time:28005ms step_avg:35.90ms
step:781/2160 train_time:28064ms step_avg:35.93ms
step:782/2160 train_time:28122ms step_avg:35.96ms
step:783/2160 train_time:28182ms step_avg:35.99ms
step:784/2160 train_time:28241ms step_avg:36.02ms
step:785/2160 train_time:28301ms step_avg:36.05ms
step:786/2160 train_time:28360ms step_avg:36.08ms
step:787/2160 train_time:28421ms step_avg:36.11ms
step:788/2160 train_time:28481ms step_avg:36.14ms
step:789/2160 train_time:28542ms step_avg:36.18ms
step:790/2160 train_time:28602ms step_avg:36.20ms
step:791/2160 train_time:28663ms step_avg:36.24ms
step:792/2160 train_time:28722ms step_avg:36.27ms
step:793/2160 train_time:28783ms step_avg:36.30ms
step:794/2160 train_time:28841ms step_avg:36.32ms
step:795/2160 train_time:28902ms step_avg:36.35ms
step:796/2160 train_time:28960ms step_avg:36.38ms
step:797/2160 train_time:29020ms step_avg:36.41ms
step:798/2160 train_time:29079ms step_avg:36.44ms
step:799/2160 train_time:29138ms step_avg:36.47ms
step:800/2160 train_time:29197ms step_avg:36.50ms
step:801/2160 train_time:29257ms step_avg:36.53ms
step:802/2160 train_time:29316ms step_avg:36.55ms
step:803/2160 train_time:29377ms step_avg:36.58ms
step:804/2160 train_time:29436ms step_avg:36.61ms
step:805/2160 train_time:29497ms step_avg:36.64ms
step:806/2160 train_time:29557ms step_avg:36.67ms
step:807/2160 train_time:29618ms step_avg:36.70ms
step:808/2160 train_time:29677ms step_avg:36.73ms
step:809/2160 train_time:29737ms step_avg:36.76ms
step:810/2160 train_time:29796ms step_avg:36.79ms
step:811/2160 train_time:29857ms step_avg:36.81ms
step:812/2160 train_time:29915ms step_avg:36.84ms
step:813/2160 train_time:29976ms step_avg:36.87ms
step:814/2160 train_time:30034ms step_avg:36.90ms
step:815/2160 train_time:30095ms step_avg:36.93ms
step:816/2160 train_time:30153ms step_avg:36.95ms
step:817/2160 train_time:30213ms step_avg:36.98ms
step:818/2160 train_time:30272ms step_avg:37.01ms
step:819/2160 train_time:30332ms step_avg:37.04ms
step:820/2160 train_time:30391ms step_avg:37.06ms
step:821/2160 train_time:30451ms step_avg:37.09ms
step:822/2160 train_time:30511ms step_avg:37.12ms
step:823/2160 train_time:30572ms step_avg:37.15ms
step:824/2160 train_time:30631ms step_avg:37.17ms
step:825/2160 train_time:30692ms step_avg:37.20ms
step:826/2160 train_time:30751ms step_avg:37.23ms
step:827/2160 train_time:30811ms step_avg:37.26ms
step:828/2160 train_time:30870ms step_avg:37.28ms
step:829/2160 train_time:30930ms step_avg:37.31ms
step:830/2160 train_time:30989ms step_avg:37.34ms
step:831/2160 train_time:31050ms step_avg:37.36ms
step:832/2160 train_time:31108ms step_avg:37.39ms
step:833/2160 train_time:31169ms step_avg:37.42ms
step:834/2160 train_time:31227ms step_avg:37.44ms
step:835/2160 train_time:31288ms step_avg:37.47ms
step:836/2160 train_time:31346ms step_avg:37.49ms
step:837/2160 train_time:31406ms step_avg:37.52ms
step:838/2160 train_time:31464ms step_avg:37.55ms
step:839/2160 train_time:31525ms step_avg:37.58ms
step:840/2160 train_time:31585ms step_avg:37.60ms
step:841/2160 train_time:31646ms step_avg:37.63ms
step:842/2160 train_time:31705ms step_avg:37.65ms
step:843/2160 train_time:31766ms step_avg:37.68ms
step:844/2160 train_time:31825ms step_avg:37.71ms
step:845/2160 train_time:31887ms step_avg:37.74ms
step:846/2160 train_time:31945ms step_avg:37.76ms
step:847/2160 train_time:32005ms step_avg:37.79ms
step:848/2160 train_time:32063ms step_avg:37.81ms
step:849/2160 train_time:32123ms step_avg:37.84ms
step:850/2160 train_time:32183ms step_avg:37.86ms
step:851/2160 train_time:32243ms step_avg:37.89ms
step:852/2160 train_time:32302ms step_avg:37.91ms
step:853/2160 train_time:32362ms step_avg:37.94ms
step:854/2160 train_time:32421ms step_avg:37.96ms
step:855/2160 train_time:32481ms step_avg:37.99ms
step:856/2160 train_time:32541ms step_avg:38.01ms
step:857/2160 train_time:32601ms step_avg:38.04ms
step:858/2160 train_time:32660ms step_avg:38.07ms
step:859/2160 train_time:32721ms step_avg:38.09ms
step:860/2160 train_time:32780ms step_avg:38.12ms
step:861/2160 train_time:32842ms step_avg:38.14ms
step:862/2160 train_time:32901ms step_avg:38.17ms
step:863/2160 train_time:32961ms step_avg:38.19ms
step:864/2160 train_time:33020ms step_avg:38.22ms
step:865/2160 train_time:33080ms step_avg:38.24ms
step:866/2160 train_time:33139ms step_avg:38.27ms
step:867/2160 train_time:33200ms step_avg:38.29ms
step:868/2160 train_time:33258ms step_avg:38.32ms
step:869/2160 train_time:33319ms step_avg:38.34ms
step:870/2160 train_time:33377ms step_avg:38.36ms
step:871/2160 train_time:33438ms step_avg:38.39ms
step:872/2160 train_time:33497ms step_avg:38.41ms
step:873/2160 train_time:33557ms step_avg:38.44ms
step:874/2160 train_time:33616ms step_avg:38.46ms
step:875/2160 train_time:33677ms step_avg:38.49ms
step:876/2160 train_time:33735ms step_avg:38.51ms
step:877/2160 train_time:33796ms step_avg:38.54ms
step:878/2160 train_time:33856ms step_avg:38.56ms
step:879/2160 train_time:33916ms step_avg:38.59ms
step:880/2160 train_time:33975ms step_avg:38.61ms
step:881/2160 train_time:34035ms step_avg:38.63ms
step:882/2160 train_time:34094ms step_avg:38.66ms
step:883/2160 train_time:34154ms step_avg:38.68ms
step:884/2160 train_time:34213ms step_avg:38.70ms
step:885/2160 train_time:34273ms step_avg:38.73ms
step:886/2160 train_time:34331ms step_avg:38.75ms
step:887/2160 train_time:34392ms step_avg:38.77ms
step:888/2160 train_time:34451ms step_avg:38.80ms
step:889/2160 train_time:34511ms step_avg:38.82ms
step:890/2160 train_time:34570ms step_avg:38.84ms
step:891/2160 train_time:34631ms step_avg:38.87ms
step:892/2160 train_time:34689ms step_avg:38.89ms
step:893/2160 train_time:34750ms step_avg:38.91ms
step:894/2160 train_time:34809ms step_avg:38.94ms
step:895/2160 train_time:34870ms step_avg:38.96ms
step:896/2160 train_time:34929ms step_avg:38.98ms
step:897/2160 train_time:34989ms step_avg:39.01ms
step:898/2160 train_time:35048ms step_avg:39.03ms
step:899/2160 train_time:35109ms step_avg:39.05ms
step:900/2160 train_time:35168ms step_avg:39.08ms
step:901/2160 train_time:35229ms step_avg:39.10ms
step:902/2160 train_time:35287ms step_avg:39.12ms
step:903/2160 train_time:35348ms step_avg:39.15ms
step:904/2160 train_time:35407ms step_avg:39.17ms
step:905/2160 train_time:35468ms step_avg:39.19ms
step:906/2160 train_time:35527ms step_avg:39.21ms
step:907/2160 train_time:35587ms step_avg:39.24ms
step:908/2160 train_time:35646ms step_avg:39.26ms
step:909/2160 train_time:35707ms step_avg:39.28ms
step:910/2160 train_time:35766ms step_avg:39.30ms
step:911/2160 train_time:35826ms step_avg:39.33ms
step:912/2160 train_time:35886ms step_avg:39.35ms
step:913/2160 train_time:35946ms step_avg:39.37ms
step:914/2160 train_time:36005ms step_avg:39.39ms
step:915/2160 train_time:36066ms step_avg:39.42ms
step:916/2160 train_time:36125ms step_avg:39.44ms
step:917/2160 train_time:36186ms step_avg:39.46ms
step:918/2160 train_time:36244ms step_avg:39.48ms
step:919/2160 train_time:36305ms step_avg:39.50ms
step:920/2160 train_time:36363ms step_avg:39.53ms
step:921/2160 train_time:36424ms step_avg:39.55ms
step:922/2160 train_time:36483ms step_avg:39.57ms
step:923/2160 train_time:36544ms step_avg:39.59ms
step:924/2160 train_time:36602ms step_avg:39.61ms
step:925/2160 train_time:36662ms step_avg:39.63ms
step:926/2160 train_time:36721ms step_avg:39.66ms
step:927/2160 train_time:36782ms step_avg:39.68ms
step:928/2160 train_time:36841ms step_avg:39.70ms
step:929/2160 train_time:36901ms step_avg:39.72ms
step:930/2160 train_time:36960ms step_avg:39.74ms
step:931/2160 train_time:37021ms step_avg:39.76ms
step:932/2160 train_time:37080ms step_avg:39.78ms
step:933/2160 train_time:37141ms step_avg:39.81ms
step:934/2160 train_time:37200ms step_avg:39.83ms
step:935/2160 train_time:37260ms step_avg:39.85ms
step:936/2160 train_time:37319ms step_avg:39.87ms
step:937/2160 train_time:37379ms step_avg:39.89ms
step:938/2160 train_time:37438ms step_avg:39.91ms
step:939/2160 train_time:37499ms step_avg:39.94ms
step:940/2160 train_time:37558ms step_avg:39.96ms
step:941/2160 train_time:37618ms step_avg:39.98ms
step:942/2160 train_time:37677ms step_avg:40.00ms
step:943/2160 train_time:37737ms step_avg:40.02ms
step:944/2160 train_time:37796ms step_avg:40.04ms
step:945/2160 train_time:37856ms step_avg:40.06ms
step:946/2160 train_time:37915ms step_avg:40.08ms
step:947/2160 train_time:38048ms step_avg:40.18ms
step:948/2160 train_time:38088ms step_avg:40.18ms
step:949/2160 train_time:38147ms step_avg:40.20ms
step:950/2160 train_time:38205ms step_avg:40.22ms
step:951/2160 train_time:38264ms step_avg:40.24ms
step:952/2160 train_time:38322ms step_avg:40.25ms
step:953/2160 train_time:38381ms step_avg:40.27ms
step:954/2160 train_time:38439ms step_avg:40.29ms
step:955/2160 train_time:38499ms step_avg:40.31ms
step:956/2160 train_time:38556ms step_avg:40.33ms
step:957/2160 train_time:38616ms step_avg:40.35ms
step:958/2160 train_time:38674ms step_avg:40.37ms
step:959/2160 train_time:38734ms step_avg:40.39ms
step:960/2160 train_time:38792ms step_avg:40.41ms
step:961/2160 train_time:38852ms step_avg:40.43ms
step:962/2160 train_time:38911ms step_avg:40.45ms
step:963/2160 train_time:38975ms step_avg:40.47ms
step:964/2160 train_time:39039ms step_avg:40.50ms
step:965/2160 train_time:39101ms step_avg:40.52ms
step:966/2160 train_time:39160ms step_avg:40.54ms
step:967/2160 train_time:39220ms step_avg:40.56ms
step:968/2160 train_time:39278ms step_avg:40.58ms
step:969/2160 train_time:39339ms step_avg:40.60ms
step:970/2160 train_time:39397ms step_avg:40.62ms
step:971/2160 train_time:39457ms step_avg:40.64ms
step:972/2160 train_time:39515ms step_avg:40.65ms
step:973/2160 train_time:39574ms step_avg:40.67ms
step:974/2160 train_time:39632ms step_avg:40.69ms
step:975/2160 train_time:39692ms step_avg:40.71ms
step:976/2160 train_time:39750ms step_avg:40.73ms
step:977/2160 train_time:39810ms step_avg:40.75ms
step:978/2160 train_time:39868ms step_avg:40.76ms
step:979/2160 train_time:39929ms step_avg:40.79ms
step:980/2160 train_time:39989ms step_avg:40.81ms
step:981/2160 train_time:40052ms step_avg:40.83ms
step:982/2160 train_time:40112ms step_avg:40.85ms
step:983/2160 train_time:40174ms step_avg:40.87ms
step:984/2160 train_time:40233ms step_avg:40.89ms
step:985/2160 train_time:40294ms step_avg:40.91ms
step:986/2160 train_time:40353ms step_avg:40.93ms
step:987/2160 train_time:40414ms step_avg:40.95ms
step:988/2160 train_time:40472ms step_avg:40.96ms
step:989/2160 train_time:40532ms step_avg:40.98ms
step:990/2160 train_time:40590ms step_avg:41.00ms
step:991/2160 train_time:40651ms step_avg:41.02ms
step:992/2160 train_time:40709ms step_avg:41.04ms
step:993/2160 train_time:40769ms step_avg:41.06ms
step:994/2160 train_time:40827ms step_avg:41.07ms
step:995/2160 train_time:40888ms step_avg:41.09ms
step:996/2160 train_time:40947ms step_avg:41.11ms
step:997/2160 train_time:41008ms step_avg:41.13ms
step:998/2160 train_time:41069ms step_avg:41.15ms
step:999/2160 train_time:41130ms step_avg:41.17ms
step:1000/2160 train_time:41189ms step_avg:41.19ms
step:1000/2160 val_loss:3.6952 train_time:41252ms step_avg:41.25ms
step:1001/2160 train_time:41273ms step_avg:41.23ms
step:1002/2160 train_time:41312ms step_avg:41.23ms
step:1003/2160 train_time:41376ms step_avg:41.25ms
step:1004/2160 train_time:41441ms step_avg:41.28ms
step:1005/2160 train_time:41502ms step_avg:41.30ms
step:1006/2160 train_time:41560ms step_avg:41.31ms
step:1007/2160 train_time:41620ms step_avg:41.33ms
step:1008/2160 train_time:41678ms step_avg:41.35ms
step:1009/2160 train_time:41738ms step_avg:41.37ms
step:1010/2160 train_time:41796ms step_avg:41.38ms
step:1011/2160 train_time:41856ms step_avg:41.40ms
step:1012/2160 train_time:41913ms step_avg:41.42ms
step:1013/2160 train_time:41973ms step_avg:41.43ms
step:1014/2160 train_time:42031ms step_avg:41.45ms
step:1015/2160 train_time:42091ms step_avg:41.47ms
step:1016/2160 train_time:42152ms step_avg:41.49ms
step:1017/2160 train_time:42211ms step_avg:41.50ms
step:1018/2160 train_time:42269ms step_avg:41.52ms
step:1019/2160 train_time:42332ms step_avg:41.54ms
step:1020/2160 train_time:42393ms step_avg:41.56ms
step:1021/2160 train_time:42455ms step_avg:41.58ms
step:1022/2160 train_time:42514ms step_avg:41.60ms
step:1023/2160 train_time:42576ms step_avg:41.62ms
step:1024/2160 train_time:42635ms step_avg:41.64ms
step:1025/2160 train_time:42696ms step_avg:41.65ms
step:1026/2160 train_time:42754ms step_avg:41.67ms
step:1027/2160 train_time:42814ms step_avg:41.69ms
step:1028/2160 train_time:42872ms step_avg:41.70ms
step:1029/2160 train_time:42931ms step_avg:41.72ms
step:1030/2160 train_time:42989ms step_avg:41.74ms
step:1031/2160 train_time:43049ms step_avg:41.75ms
step:1032/2160 train_time:43107ms step_avg:41.77ms
step:1033/2160 train_time:43167ms step_avg:41.79ms
step:1034/2160 train_time:43225ms step_avg:41.80ms
step:1035/2160 train_time:43286ms step_avg:41.82ms
step:1036/2160 train_time:43345ms step_avg:41.84ms
step:1037/2160 train_time:43407ms step_avg:41.86ms
step:1038/2160 train_time:43467ms step_avg:41.88ms
step:1039/2160 train_time:43528ms step_avg:41.89ms
step:1040/2160 train_time:43587ms step_avg:41.91ms
step:1041/2160 train_time:43648ms step_avg:41.93ms
step:1042/2160 train_time:43706ms step_avg:41.94ms
step:1043/2160 train_time:43767ms step_avg:41.96ms
step:1044/2160 train_time:43825ms step_avg:41.98ms
step:1045/2160 train_time:43886ms step_avg:42.00ms
step:1046/2160 train_time:43944ms step_avg:42.01ms
step:1047/2160 train_time:44004ms step_avg:42.03ms
step:1048/2160 train_time:44062ms step_avg:42.04ms
step:1049/2160 train_time:44122ms step_avg:42.06ms
step:1050/2160 train_time:44180ms step_avg:42.08ms
step:1051/2160 train_time:44241ms step_avg:42.09ms
step:1052/2160 train_time:44300ms step_avg:42.11ms
step:1053/2160 train_time:44361ms step_avg:42.13ms
step:1054/2160 train_time:44420ms step_avg:42.14ms
step:1055/2160 train_time:44482ms step_avg:42.16ms
step:1056/2160 train_time:44541ms step_avg:42.18ms
step:1057/2160 train_time:44602ms step_avg:42.20ms
step:1058/2160 train_time:44661ms step_avg:42.21ms
step:1059/2160 train_time:44723ms step_avg:42.23ms
step:1060/2160 train_time:44781ms step_avg:42.25ms
step:1061/2160 train_time:44842ms step_avg:42.26ms
step:1062/2160 train_time:44900ms step_avg:42.28ms
step:1063/2160 train_time:44961ms step_avg:42.30ms
step:1064/2160 train_time:45019ms step_avg:42.31ms
step:1065/2160 train_time:45079ms step_avg:42.33ms
step:1066/2160 train_time:45137ms step_avg:42.34ms
step:1067/2160 train_time:45198ms step_avg:42.36ms
step:1068/2160 train_time:45257ms step_avg:42.38ms
step:1069/2160 train_time:45317ms step_avg:42.39ms
step:1070/2160 train_time:45376ms step_avg:42.41ms
step:1071/2160 train_time:45436ms step_avg:42.42ms
step:1072/2160 train_time:45495ms step_avg:42.44ms
step:1073/2160 train_time:45556ms step_avg:42.46ms
step:1074/2160 train_time:45615ms step_avg:42.47ms
step:1075/2160 train_time:45676ms step_avg:42.49ms
step:1076/2160 train_time:45736ms step_avg:42.51ms
step:1077/2160 train_time:45797ms step_avg:42.52ms
step:1078/2160 train_time:45856ms step_avg:42.54ms
step:1079/2160 train_time:45917ms step_avg:42.56ms
step:1080/2160 train_time:45976ms step_avg:42.57ms
step:1081/2160 train_time:46037ms step_avg:42.59ms
step:1082/2160 train_time:46095ms step_avg:42.60ms
step:1083/2160 train_time:46155ms step_avg:42.62ms
step:1084/2160 train_time:46214ms step_avg:42.63ms
step:1085/2160 train_time:46274ms step_avg:42.65ms
step:1086/2160 train_time:46333ms step_avg:42.66ms
step:1087/2160 train_time:46393ms step_avg:42.68ms
step:1088/2160 train_time:46452ms step_avg:42.69ms
step:1089/2160 train_time:46513ms step_avg:42.71ms
step:1090/2160 train_time:46572ms step_avg:42.73ms
step:1091/2160 train_time:46634ms step_avg:42.74ms
step:1092/2160 train_time:46693ms step_avg:42.76ms
step:1093/2160 train_time:46754ms step_avg:42.78ms
step:1094/2160 train_time:46812ms step_avg:42.79ms
step:1095/2160 train_time:46873ms step_avg:42.81ms
step:1096/2160 train_time:46932ms step_avg:42.82ms
step:1097/2160 train_time:46993ms step_avg:42.84ms
step:1098/2160 train_time:47052ms step_avg:42.85ms
step:1099/2160 train_time:47112ms step_avg:42.87ms
step:1100/2160 train_time:47171ms step_avg:42.88ms
step:1101/2160 train_time:47231ms step_avg:42.90ms
step:1102/2160 train_time:47290ms step_avg:42.91ms
step:1103/2160 train_time:47351ms step_avg:42.93ms
step:1104/2160 train_time:47410ms step_avg:42.94ms
step:1105/2160 train_time:47471ms step_avg:42.96ms
step:1106/2160 train_time:47529ms step_avg:42.97ms
step:1107/2160 train_time:47590ms step_avg:42.99ms
step:1108/2160 train_time:47649ms step_avg:43.00ms
step:1109/2160 train_time:47710ms step_avg:43.02ms
step:1110/2160 train_time:47768ms step_avg:43.03ms
step:1111/2160 train_time:47829ms step_avg:43.05ms
step:1112/2160 train_time:47888ms step_avg:43.06ms
step:1113/2160 train_time:47949ms step_avg:43.08ms
step:1114/2160 train_time:48008ms step_avg:43.09ms
step:1115/2160 train_time:48068ms step_avg:43.11ms
step:1116/2160 train_time:48126ms step_avg:43.12ms
step:1117/2160 train_time:48187ms step_avg:43.14ms
step:1118/2160 train_time:48245ms step_avg:43.15ms
step:1119/2160 train_time:48305ms step_avg:43.17ms
step:1120/2160 train_time:48364ms step_avg:43.18ms
step:1121/2160 train_time:48425ms step_avg:43.20ms
step:1122/2160 train_time:48483ms step_avg:43.21ms
step:1123/2160 train_time:48544ms step_avg:43.23ms
step:1124/2160 train_time:48603ms step_avg:43.24ms
step:1125/2160 train_time:48664ms step_avg:43.26ms
step:1126/2160 train_time:48723ms step_avg:43.27ms
step:1127/2160 train_time:48783ms step_avg:43.29ms
step:1128/2160 train_time:48841ms step_avg:43.30ms
step:1129/2160 train_time:48902ms step_avg:43.31ms
step:1130/2160 train_time:48961ms step_avg:43.33ms
step:1131/2160 train_time:49022ms step_avg:43.34ms
step:1132/2160 train_time:49080ms step_avg:43.36ms
step:1133/2160 train_time:49140ms step_avg:43.37ms
step:1134/2160 train_time:49199ms step_avg:43.38ms
step:1135/2160 train_time:49259ms step_avg:43.40ms
step:1136/2160 train_time:49318ms step_avg:43.41ms
step:1137/2160 train_time:49378ms step_avg:43.43ms
step:1138/2160 train_time:49437ms step_avg:43.44ms
step:1139/2160 train_time:49498ms step_avg:43.46ms
step:1140/2160 train_time:49557ms step_avg:43.47ms
step:1141/2160 train_time:49618ms step_avg:43.49ms
step:1142/2160 train_time:49676ms step_avg:43.50ms
step:1143/2160 train_time:49737ms step_avg:43.51ms
step:1144/2160 train_time:49796ms step_avg:43.53ms
step:1145/2160 train_time:49857ms step_avg:43.54ms
step:1146/2160 train_time:49915ms step_avg:43.56ms
step:1147/2160 train_time:49976ms step_avg:43.57ms
step:1148/2160 train_time:50036ms step_avg:43.59ms
step:1149/2160 train_time:50096ms step_avg:43.60ms
step:1150/2160 train_time:50155ms step_avg:43.61ms
step:1151/2160 train_time:50215ms step_avg:43.63ms
step:1152/2160 train_time:50274ms step_avg:43.64ms
step:1153/2160 train_time:50334ms step_avg:43.65ms
step:1154/2160 train_time:50393ms step_avg:43.67ms
step:1155/2160 train_time:50454ms step_avg:43.68ms
step:1156/2160 train_time:50513ms step_avg:43.70ms
step:1157/2160 train_time:50574ms step_avg:43.71ms
step:1158/2160 train_time:50633ms step_avg:43.72ms
step:1159/2160 train_time:50694ms step_avg:43.74ms
step:1160/2160 train_time:50753ms step_avg:43.75ms
step:1161/2160 train_time:50814ms step_avg:43.77ms
step:1162/2160 train_time:50873ms step_avg:43.78ms
step:1163/2160 train_time:50933ms step_avg:43.79ms
step:1164/2160 train_time:50992ms step_avg:43.81ms
step:1165/2160 train_time:51053ms step_avg:43.82ms
step:1166/2160 train_time:51112ms step_avg:43.84ms
step:1167/2160 train_time:51173ms step_avg:43.85ms
step:1168/2160 train_time:51231ms step_avg:43.86ms
step:1169/2160 train_time:51292ms step_avg:43.88ms
step:1170/2160 train_time:51351ms step_avg:43.89ms
step:1171/2160 train_time:51411ms step_avg:43.90ms
step:1172/2160 train_time:51469ms step_avg:43.92ms
step:1173/2160 train_time:51529ms step_avg:43.93ms
step:1174/2160 train_time:51588ms step_avg:43.94ms
step:1175/2160 train_time:51648ms step_avg:43.96ms
step:1176/2160 train_time:51707ms step_avg:43.97ms
step:1177/2160 train_time:51767ms step_avg:43.98ms
step:1178/2160 train_time:51826ms step_avg:43.99ms
step:1179/2160 train_time:51886ms step_avg:44.01ms
step:1180/2160 train_time:51945ms step_avg:44.02ms
step:1181/2160 train_time:52006ms step_avg:44.04ms
step:1182/2160 train_time:52065ms step_avg:44.05ms
step:1183/2160 train_time:52126ms step_avg:44.06ms
step:1184/2160 train_time:52185ms step_avg:44.08ms
step:1185/2160 train_time:52245ms step_avg:44.09ms
step:1186/2160 train_time:52304ms step_avg:44.10ms
step:1187/2160 train_time:52364ms step_avg:44.11ms
step:1188/2160 train_time:52423ms step_avg:44.13ms
step:1189/2160 train_time:52484ms step_avg:44.14ms
step:1190/2160 train_time:52543ms step_avg:44.15ms
step:1191/2160 train_time:52603ms step_avg:44.17ms
step:1192/2160 train_time:52662ms step_avg:44.18ms
step:1193/2160 train_time:52723ms step_avg:44.19ms
step:1194/2160 train_time:52782ms step_avg:44.21ms
step:1195/2160 train_time:52842ms step_avg:44.22ms
step:1196/2160 train_time:52902ms step_avg:44.23ms
step:1197/2160 train_time:52962ms step_avg:44.25ms
step:1198/2160 train_time:53021ms step_avg:44.26ms
step:1199/2160 train_time:53082ms step_avg:44.27ms
step:1200/2160 train_time:53140ms step_avg:44.28ms
step:1201/2160 train_time:53202ms step_avg:44.30ms
step:1202/2160 train_time:53261ms step_avg:44.31ms
step:1203/2160 train_time:53321ms step_avg:44.32ms
step:1204/2160 train_time:53380ms step_avg:44.34ms
step:1205/2160 train_time:53440ms step_avg:44.35ms
step:1206/2160 train_time:53499ms step_avg:44.36ms
step:1207/2160 train_time:53560ms step_avg:44.37ms
step:1208/2160 train_time:53618ms step_avg:44.39ms
step:1209/2160 train_time:53679ms step_avg:44.40ms
step:1210/2160 train_time:53738ms step_avg:44.41ms
step:1211/2160 train_time:53799ms step_avg:44.43ms
step:1212/2160 train_time:53857ms step_avg:44.44ms
step:1213/2160 train_time:53918ms step_avg:44.45ms
step:1214/2160 train_time:53977ms step_avg:44.46ms
step:1215/2160 train_time:54038ms step_avg:44.48ms
step:1216/2160 train_time:54097ms step_avg:44.49ms
step:1217/2160 train_time:54157ms step_avg:44.50ms
step:1218/2160 train_time:54216ms step_avg:44.51ms
step:1219/2160 train_time:54277ms step_avg:44.53ms
step:1220/2160 train_time:54336ms step_avg:44.54ms
step:1221/2160 train_time:54397ms step_avg:44.55ms
step:1222/2160 train_time:54456ms step_avg:44.56ms
step:1223/2160 train_time:54516ms step_avg:44.58ms
step:1224/2160 train_time:54575ms step_avg:44.59ms
step:1225/2160 train_time:54635ms step_avg:44.60ms
step:1226/2160 train_time:54694ms step_avg:44.61ms
step:1227/2160 train_time:54755ms step_avg:44.62ms
step:1228/2160 train_time:54814ms step_avg:44.64ms
step:1229/2160 train_time:54874ms step_avg:44.65ms
step:1230/2160 train_time:54933ms step_avg:44.66ms
step:1231/2160 train_time:54994ms step_avg:44.67ms
step:1232/2160 train_time:55053ms step_avg:44.69ms
step:1233/2160 train_time:55113ms step_avg:44.70ms
step:1234/2160 train_time:55172ms step_avg:44.71ms
step:1235/2160 train_time:55234ms step_avg:44.72ms
step:1236/2160 train_time:55293ms step_avg:44.74ms
step:1237/2160 train_time:55353ms step_avg:44.75ms
step:1238/2160 train_time:55412ms step_avg:44.76ms
step:1239/2160 train_time:55473ms step_avg:44.77ms
step:1240/2160 train_time:55532ms step_avg:44.78ms
step:1241/2160 train_time:55592ms step_avg:44.80ms
step:1242/2160 train_time:55651ms step_avg:44.81ms
step:1243/2160 train_time:55712ms step_avg:44.82ms
step:1244/2160 train_time:55771ms step_avg:44.83ms
step:1245/2160 train_time:55832ms step_avg:44.84ms
step:1246/2160 train_time:55891ms step_avg:44.86ms
step:1247/2160 train_time:55952ms step_avg:44.87ms
step:1248/2160 train_time:56010ms step_avg:44.88ms
step:1249/2160 train_time:56071ms step_avg:44.89ms
step:1250/2160 train_time:56130ms step_avg:44.90ms
step:1250/2160 val_loss:3.5682 train_time:56192ms step_avg:44.95ms
step:1251/2160 train_time:56212ms step_avg:44.93ms
step:1252/2160 train_time:56252ms step_avg:44.93ms
step:1253/2160 train_time:56316ms step_avg:44.94ms
step:1254/2160 train_time:56376ms step_avg:44.96ms
step:1255/2160 train_time:56437ms step_avg:44.97ms
step:1256/2160 train_time:56496ms step_avg:44.98ms
step:1257/2160 train_time:56556ms step_avg:44.99ms
step:1258/2160 train_time:56614ms step_avg:45.00ms
step:1259/2160 train_time:56674ms step_avg:45.02ms
step:1260/2160 train_time:56732ms step_avg:45.03ms
step:1261/2160 train_time:56793ms step_avg:45.04ms
step:1262/2160 train_time:56851ms step_avg:45.05ms
step:1263/2160 train_time:56912ms step_avg:45.06ms
step:1264/2160 train_time:56970ms step_avg:45.07ms
step:1265/2160 train_time:57031ms step_avg:45.08ms
step:1266/2160 train_time:57089ms step_avg:45.09ms
step:1267/2160 train_time:57151ms step_avg:45.11ms
step:1268/2160 train_time:57210ms step_avg:45.12ms
step:1269/2160 train_time:57272ms step_avg:45.13ms
step:1270/2160 train_time:57332ms step_avg:45.14ms
step:1271/2160 train_time:57395ms step_avg:45.16ms
step:1272/2160 train_time:57454ms step_avg:45.17ms
step:1273/2160 train_time:57514ms step_avg:45.18ms
step:1274/2160 train_time:57573ms step_avg:45.19ms
step:1275/2160 train_time:57634ms step_avg:45.20ms
step:1276/2160 train_time:57692ms step_avg:45.21ms
step:1277/2160 train_time:57752ms step_avg:45.22ms
step:1278/2160 train_time:57810ms step_avg:45.23ms
step:1279/2160 train_time:57870ms step_avg:45.25ms
step:1280/2160 train_time:57928ms step_avg:45.26ms
step:1281/2160 train_time:57988ms step_avg:45.27ms
step:1282/2160 train_time:58046ms step_avg:45.28ms
step:1283/2160 train_time:58107ms step_avg:45.29ms
step:1284/2160 train_time:58166ms step_avg:45.30ms
step:1285/2160 train_time:58228ms step_avg:45.31ms
step:1286/2160 train_time:58287ms step_avg:45.32ms
step:1287/2160 train_time:58349ms step_avg:45.34ms
step:1288/2160 train_time:58408ms step_avg:45.35ms
step:1289/2160 train_time:58469ms step_avg:45.36ms
step:1290/2160 train_time:58528ms step_avg:45.37ms
step:1291/2160 train_time:58589ms step_avg:45.38ms
step:1292/2160 train_time:58647ms step_avg:45.39ms
step:1293/2160 train_time:58708ms step_avg:45.40ms
step:1294/2160 train_time:58766ms step_avg:45.41ms
step:1295/2160 train_time:58826ms step_avg:45.43ms
step:1296/2160 train_time:58885ms step_avg:45.44ms
step:1297/2160 train_time:58945ms step_avg:45.45ms
step:1298/2160 train_time:59003ms step_avg:45.46ms
step:1299/2160 train_time:59064ms step_avg:45.47ms
step:1300/2160 train_time:59122ms step_avg:45.48ms
step:1301/2160 train_time:59183ms step_avg:45.49ms
step:1302/2160 train_time:59242ms step_avg:45.50ms
step:1303/2160 train_time:59304ms step_avg:45.51ms
step:1304/2160 train_time:59363ms step_avg:45.52ms
step:1305/2160 train_time:59424ms step_avg:45.54ms
step:1306/2160 train_time:59484ms step_avg:45.55ms
step:1307/2160 train_time:59545ms step_avg:45.56ms
step:1308/2160 train_time:59604ms step_avg:45.57ms
step:1309/2160 train_time:59665ms step_avg:45.58ms
step:1310/2160 train_time:59723ms step_avg:45.59ms
step:1311/2160 train_time:59784ms step_avg:45.60ms
step:1312/2160 train_time:59843ms step_avg:45.61ms
step:1313/2160 train_time:59902ms step_avg:45.62ms
step:1314/2160 train_time:59961ms step_avg:45.63ms
step:1315/2160 train_time:60021ms step_avg:45.64ms
step:1316/2160 train_time:60079ms step_avg:45.65ms
step:1317/2160 train_time:60140ms step_avg:45.66ms
step:1318/2160 train_time:60199ms step_avg:45.67ms
step:1319/2160 train_time:60259ms step_avg:45.69ms
step:1320/2160 train_time:60318ms step_avg:45.70ms
step:1321/2160 train_time:60379ms step_avg:45.71ms
step:1322/2160 train_time:60439ms step_avg:45.72ms
step:1323/2160 train_time:60500ms step_avg:45.73ms
step:1324/2160 train_time:60559ms step_avg:45.74ms
step:1325/2160 train_time:60620ms step_avg:45.75ms
step:1326/2160 train_time:60679ms step_avg:45.76ms
step:1327/2160 train_time:60740ms step_avg:45.77ms
step:1328/2160 train_time:60799ms step_avg:45.78ms
step:1329/2160 train_time:60859ms step_avg:45.79ms
step:1330/2160 train_time:60917ms step_avg:45.80ms
step:1331/2160 train_time:60978ms step_avg:45.81ms
step:1332/2160 train_time:61036ms step_avg:45.82ms
step:1333/2160 train_time:61096ms step_avg:45.83ms
step:1334/2160 train_time:61155ms step_avg:45.84ms
step:1335/2160 train_time:61215ms step_avg:45.85ms
step:1336/2160 train_time:61274ms step_avg:45.86ms
step:1337/2160 train_time:61334ms step_avg:45.87ms
step:1338/2160 train_time:61394ms step_avg:45.88ms
step:1339/2160 train_time:61456ms step_avg:45.90ms
step:1340/2160 train_time:61514ms step_avg:45.91ms
step:1341/2160 train_time:61575ms step_avg:45.92ms
step:1342/2160 train_time:61634ms step_avg:45.93ms
step:1343/2160 train_time:61695ms step_avg:45.94ms
step:1344/2160 train_time:61754ms step_avg:45.95ms
step:1345/2160 train_time:61814ms step_avg:45.96ms
step:1346/2160 train_time:61873ms step_avg:45.97ms
step:1347/2160 train_time:61933ms step_avg:45.98ms
step:1348/2160 train_time:61992ms step_avg:45.99ms
step:1349/2160 train_time:62052ms step_avg:46.00ms
step:1350/2160 train_time:62110ms step_avg:46.01ms
step:1351/2160 train_time:62171ms step_avg:46.02ms
step:1352/2160 train_time:62229ms step_avg:46.03ms
step:1353/2160 train_time:62290ms step_avg:46.04ms
step:1354/2160 train_time:62348ms step_avg:46.05ms
step:1355/2160 train_time:62409ms step_avg:46.06ms
step:1356/2160 train_time:62468ms step_avg:46.07ms
step:1357/2160 train_time:62529ms step_avg:46.08ms
step:1358/2160 train_time:62587ms step_avg:46.09ms
step:1359/2160 train_time:62648ms step_avg:46.10ms
step:1360/2160 train_time:62707ms step_avg:46.11ms
step:1361/2160 train_time:62769ms step_avg:46.12ms
step:1362/2160 train_time:62827ms step_avg:46.13ms
step:1363/2160 train_time:62888ms step_avg:46.14ms
step:1364/2160 train_time:62947ms step_avg:46.15ms
step:1365/2160 train_time:63007ms step_avg:46.16ms
step:1366/2160 train_time:63066ms step_avg:46.17ms
step:1367/2160 train_time:63127ms step_avg:46.18ms
step:1368/2160 train_time:63186ms step_avg:46.19ms
step:1369/2160 train_time:63246ms step_avg:46.20ms
step:1370/2160 train_time:63305ms step_avg:46.21ms
step:1371/2160 train_time:63366ms step_avg:46.22ms
step:1372/2160 train_time:63425ms step_avg:46.23ms
step:1373/2160 train_time:63486ms step_avg:46.24ms
step:1374/2160 train_time:63546ms step_avg:46.25ms
step:1375/2160 train_time:63607ms step_avg:46.26ms
step:1376/2160 train_time:63665ms step_avg:46.27ms
step:1377/2160 train_time:63726ms step_avg:46.28ms
step:1378/2160 train_time:63785ms step_avg:46.29ms
step:1379/2160 train_time:63845ms step_avg:46.30ms
step:1380/2160 train_time:63904ms step_avg:46.31ms
step:1381/2160 train_time:63965ms step_avg:46.32ms
step:1382/2160 train_time:64023ms step_avg:46.33ms
step:1383/2160 train_time:64085ms step_avg:46.34ms
step:1384/2160 train_time:64143ms step_avg:46.35ms
step:1385/2160 train_time:64204ms step_avg:46.36ms
step:1386/2160 train_time:64263ms step_avg:46.37ms
step:1387/2160 train_time:64324ms step_avg:46.38ms
step:1388/2160 train_time:64383ms step_avg:46.39ms
step:1389/2160 train_time:64443ms step_avg:46.40ms
step:1390/2160 train_time:64503ms step_avg:46.40ms
step:1391/2160 train_time:64563ms step_avg:46.42ms
step:1392/2160 train_time:64622ms step_avg:46.42ms
step:1393/2160 train_time:64684ms step_avg:46.44ms
step:1394/2160 train_time:64743ms step_avg:46.44ms
step:1395/2160 train_time:64803ms step_avg:46.45ms
step:1396/2160 train_time:64862ms step_avg:46.46ms
step:1397/2160 train_time:64923ms step_avg:46.47ms
step:1398/2160 train_time:64982ms step_avg:46.48ms
step:1399/2160 train_time:65042ms step_avg:46.49ms
step:1400/2160 train_time:65101ms step_avg:46.50ms
step:1401/2160 train_time:65161ms step_avg:46.51ms
step:1402/2160 train_time:65220ms step_avg:46.52ms
step:1403/2160 train_time:65281ms step_avg:46.53ms
step:1404/2160 train_time:65340ms step_avg:46.54ms
step:1405/2160 train_time:65400ms step_avg:46.55ms
step:1406/2160 train_time:65459ms step_avg:46.56ms
step:1407/2160 train_time:65520ms step_avg:46.57ms
step:1408/2160 train_time:65579ms step_avg:46.58ms
step:1409/2160 train_time:65640ms step_avg:46.59ms
step:1410/2160 train_time:65699ms step_avg:46.60ms
step:1411/2160 train_time:65760ms step_avg:46.61ms
step:1412/2160 train_time:65819ms step_avg:46.61ms
step:1413/2160 train_time:65880ms step_avg:46.62ms
step:1414/2160 train_time:65939ms step_avg:46.63ms
step:1415/2160 train_time:66000ms step_avg:46.64ms
step:1416/2160 train_time:66086ms step_avg:46.67ms
step:1417/2160 train_time:66174ms step_avg:46.70ms
step:1418/2160 train_time:66259ms step_avg:46.73ms
step:1419/2160 train_time:66348ms step_avg:46.76ms
step:1420/2160 train_time:66435ms step_avg:46.79ms
step:1421/2160 train_time:66523ms step_avg:46.81ms
step:1422/2160 train_time:66609ms step_avg:46.84ms
step:1423/2160 train_time:66697ms step_avg:46.87ms
step:1424/2160 train_time:66784ms step_avg:46.90ms
step:1425/2160 train_time:66872ms step_avg:46.93ms
step:1426/2160 train_time:66959ms step_avg:46.96ms
step:1427/2160 train_time:67047ms step_avg:46.98ms
step:1428/2160 train_time:67133ms step_avg:47.01ms
step:1429/2160 train_time:67222ms step_avg:47.04ms
step:1430/2160 train_time:67307ms step_avg:47.07ms
step:1431/2160 train_time:67396ms step_avg:47.10ms
step:1432/2160 train_time:67482ms step_avg:47.12ms
step:1433/2160 train_time:67570ms step_avg:47.15ms
step:1434/2160 train_time:67656ms step_avg:47.18ms
step:1435/2160 train_time:67745ms step_avg:47.21ms
step:1436/2160 train_time:67831ms step_avg:47.24ms
step:1437/2160 train_time:67921ms step_avg:47.27ms
step:1438/2160 train_time:68008ms step_avg:47.29ms
step:1439/2160 train_time:68096ms step_avg:47.32ms
step:1440/2160 train_time:68183ms step_avg:47.35ms
step:1441/2160 train_time:68271ms step_avg:47.38ms
step:1442/2160 train_time:68358ms step_avg:47.40ms
step:1443/2160 train_time:68446ms step_avg:47.43ms
step:1444/2160 train_time:68533ms step_avg:47.46ms
step:1445/2160 train_time:68622ms step_avg:47.49ms
step:1446/2160 train_time:68708ms step_avg:47.52ms
step:1447/2160 train_time:68796ms step_avg:47.54ms
step:1448/2160 train_time:68883ms step_avg:47.57ms
step:1449/2160 train_time:68971ms step_avg:47.60ms
step:1450/2160 train_time:69058ms step_avg:47.63ms
step:1451/2160 train_time:69146ms step_avg:47.65ms
step:1452/2160 train_time:69231ms step_avg:47.68ms
step:1453/2160 train_time:69320ms step_avg:47.71ms
step:1454/2160 train_time:69407ms step_avg:47.73ms
step:1455/2160 train_time:69494ms step_avg:47.76ms
step:1456/2160 train_time:69581ms step_avg:47.79ms
step:1457/2160 train_time:69669ms step_avg:47.82ms
step:1458/2160 train_time:69756ms step_avg:47.84ms
step:1459/2160 train_time:69844ms step_avg:47.87ms
step:1460/2160 train_time:69931ms step_avg:47.90ms
step:1461/2160 train_time:70020ms step_avg:47.93ms
step:1462/2160 train_time:70106ms step_avg:47.95ms
step:1463/2160 train_time:70194ms step_avg:47.98ms
step:1464/2160 train_time:70280ms step_avg:48.01ms
step:1465/2160 train_time:70368ms step_avg:48.03ms
step:1466/2160 train_time:70454ms step_avg:48.06ms
step:1467/2160 train_time:70542ms step_avg:48.09ms
step:1468/2160 train_time:70629ms step_avg:48.11ms
step:1469/2160 train_time:70718ms step_avg:48.14ms
step:1470/2160 train_time:70805ms step_avg:48.17ms
step:1471/2160 train_time:70893ms step_avg:48.19ms
step:1472/2160 train_time:70979ms step_avg:48.22ms
step:1473/2160 train_time:71068ms step_avg:48.25ms
step:1474/2160 train_time:71154ms step_avg:48.27ms
step:1475/2160 train_time:71243ms step_avg:48.30ms
step:1476/2160 train_time:71329ms step_avg:48.33ms
step:1477/2160 train_time:71417ms step_avg:48.35ms
step:1478/2160 train_time:71503ms step_avg:48.38ms
step:1479/2160 train_time:71591ms step_avg:48.41ms
step:1480/2160 train_time:71678ms step_avg:48.43ms
step:1481/2160 train_time:71767ms step_avg:48.46ms
step:1482/2160 train_time:71853ms step_avg:48.48ms
step:1483/2160 train_time:71941ms step_avg:48.51ms
step:1484/2160 train_time:72027ms step_avg:48.54ms
step:1485/2160 train_time:72116ms step_avg:48.56ms
step:1486/2160 train_time:72202ms step_avg:48.59ms
step:1487/2160 train_time:72290ms step_avg:48.61ms
step:1488/2160 train_time:72377ms step_avg:48.64ms
step:1489/2160 train_time:72465ms step_avg:48.67ms
step:1490/2160 train_time:72552ms step_avg:48.69ms
step:1491/2160 train_time:72640ms step_avg:48.72ms
step:1492/2160 train_time:72726ms step_avg:48.74ms
step:1493/2160 train_time:72815ms step_avg:48.77ms
step:1494/2160 train_time:72901ms step_avg:48.80ms
step:1495/2160 train_time:72989ms step_avg:48.82ms
step:1496/2160 train_time:73076ms step_avg:48.85ms
step:1497/2160 train_time:73165ms step_avg:48.87ms
step:1498/2160 train_time:73250ms step_avg:48.90ms
step:1499/2160 train_time:73338ms step_avg:48.92ms
step:1500/2160 train_time:73425ms step_avg:48.95ms
step:1500/2160 val_loss:3.4673 train_time:73514ms step_avg:49.01ms
step:1501/2160 train_time:73534ms step_avg:48.99ms
step:1502/2160 train_time:73603ms step_avg:49.00ms
step:1503/2160 train_time:73696ms step_avg:49.03ms
step:1504/2160 train_time:73783ms step_avg:49.06ms
step:1505/2160 train_time:73870ms step_avg:49.08ms
step:1506/2160 train_time:73956ms step_avg:49.11ms
step:1507/2160 train_time:74043ms step_avg:49.13ms
step:1508/2160 train_time:74128ms step_avg:49.16ms
step:1509/2160 train_time:74217ms step_avg:49.18ms
step:1510/2160 train_time:74302ms step_avg:49.21ms
step:1511/2160 train_time:74389ms step_avg:49.23ms
step:1512/2160 train_time:74477ms step_avg:49.26ms
step:1513/2160 train_time:74569ms step_avg:49.29ms
step:1514/2160 train_time:74657ms step_avg:49.31ms
step:1515/2160 train_time:74748ms step_avg:49.34ms
step:1516/2160 train_time:74835ms step_avg:49.36ms
step:1517/2160 train_time:74923ms step_avg:49.39ms
step:1518/2160 train_time:75009ms step_avg:49.41ms
step:1519/2160 train_time:75097ms step_avg:49.44ms
step:1520/2160 train_time:75182ms step_avg:49.46ms
step:1521/2160 train_time:75270ms step_avg:49.49ms
step:1522/2160 train_time:75356ms step_avg:49.51ms
step:1523/2160 train_time:75444ms step_avg:49.54ms
step:1524/2160 train_time:75531ms step_avg:49.56ms
step:1525/2160 train_time:75621ms step_avg:49.59ms
step:1526/2160 train_time:75709ms step_avg:49.61ms
step:1527/2160 train_time:75798ms step_avg:49.64ms
step:1528/2160 train_time:75884ms step_avg:49.66ms
step:1529/2160 train_time:75972ms step_avg:49.69ms
step:1530/2160 train_time:76057ms step_avg:49.71ms
step:1531/2160 train_time:76145ms step_avg:49.74ms
step:1532/2160 train_time:76230ms step_avg:49.76ms
step:1533/2160 train_time:76318ms step_avg:49.78ms
step:1534/2160 train_time:76404ms step_avg:49.81ms
step:1535/2160 train_time:76492ms step_avg:49.83ms
step:1536/2160 train_time:76578ms step_avg:49.86ms
step:1537/2160 train_time:76668ms step_avg:49.88ms
step:1538/2160 train_time:76756ms step_avg:49.91ms
step:1539/2160 train_time:76845ms step_avg:49.93ms
step:1540/2160 train_time:76930ms step_avg:49.95ms
step:1541/2160 train_time:77019ms step_avg:49.98ms
step:1542/2160 train_time:77105ms step_avg:50.00ms
step:1543/2160 train_time:77193ms step_avg:50.03ms
step:1544/2160 train_time:77279ms step_avg:50.05ms
step:1545/2160 train_time:77367ms step_avg:50.08ms
step:1546/2160 train_time:77454ms step_avg:50.10ms
step:1547/2160 train_time:77542ms step_avg:50.12ms
step:1548/2160 train_time:77628ms step_avg:50.15ms
step:1549/2160 train_time:77717ms step_avg:50.17ms
step:1550/2160 train_time:77804ms step_avg:50.20ms
step:1551/2160 train_time:77892ms step_avg:50.22ms
step:1552/2160 train_time:77978ms step_avg:50.24ms
step:1553/2160 train_time:78066ms step_avg:50.27ms
step:1554/2160 train_time:78153ms step_avg:50.29ms
step:1555/2160 train_time:78240ms step_avg:50.32ms
step:1556/2160 train_time:78326ms step_avg:50.34ms
step:1557/2160 train_time:78415ms step_avg:50.36ms
step:1558/2160 train_time:78502ms step_avg:50.39ms
step:1559/2160 train_time:78590ms step_avg:50.41ms
step:1560/2160 train_time:78677ms step_avg:50.43ms
step:1561/2160 train_time:78766ms step_avg:50.46ms
step:1562/2160 train_time:78853ms step_avg:50.48ms
step:1563/2160 train_time:78941ms step_avg:50.51ms
step:1564/2160 train_time:79028ms step_avg:50.53ms
step:1565/2160 train_time:79116ms step_avg:50.55ms
step:1566/2160 train_time:79201ms step_avg:50.58ms
step:1567/2160 train_time:79289ms step_avg:50.60ms
step:1568/2160 train_time:79375ms step_avg:50.62ms
step:1569/2160 train_time:79464ms step_avg:50.65ms
step:1570/2160 train_time:79551ms step_avg:50.67ms
step:1571/2160 train_time:79639ms step_avg:50.69ms
step:1572/2160 train_time:79726ms step_avg:50.72ms
step:1573/2160 train_time:79815ms step_avg:50.74ms
step:1574/2160 train_time:79901ms step_avg:50.76ms
step:1575/2160 train_time:79989ms step_avg:50.79ms
step:1576/2160 train_time:80075ms step_avg:50.81ms
step:1577/2160 train_time:80163ms step_avg:50.83ms
step:1578/2160 train_time:80250ms step_avg:50.86ms
step:1579/2160 train_time:80339ms step_avg:50.88ms
step:1580/2160 train_time:80425ms step_avg:50.90ms
step:1581/2160 train_time:80513ms step_avg:50.93ms
step:1582/2160 train_time:80599ms step_avg:50.95ms
step:1583/2160 train_time:80687ms step_avg:50.97ms
step:1584/2160 train_time:80775ms step_avg:50.99ms
step:1585/2160 train_time:80863ms step_avg:51.02ms
step:1586/2160 train_time:80949ms step_avg:51.04ms
step:1587/2160 train_time:81038ms step_avg:51.06ms
step:1588/2160 train_time:81124ms step_avg:51.09ms
step:1589/2160 train_time:81212ms step_avg:51.11ms
step:1590/2160 train_time:81299ms step_avg:51.13ms
step:1591/2160 train_time:81388ms step_avg:51.16ms
step:1592/2160 train_time:81474ms step_avg:51.18ms
step:1593/2160 train_time:81562ms step_avg:51.20ms
step:1594/2160 train_time:81649ms step_avg:51.22ms
step:1595/2160 train_time:81738ms step_avg:51.25ms
step:1596/2160 train_time:81824ms step_avg:51.27ms
step:1597/2160 train_time:81912ms step_avg:51.29ms
step:1598/2160 train_time:81998ms step_avg:51.31ms
step:1599/2160 train_time:82087ms step_avg:51.34ms
step:1600/2160 train_time:82174ms step_avg:51.36ms
step:1601/2160 train_time:82261ms step_avg:51.38ms
step:1602/2160 train_time:82348ms step_avg:51.40ms
step:1603/2160 train_time:82436ms step_avg:51.43ms
step:1604/2160 train_time:82523ms step_avg:51.45ms
step:1605/2160 train_time:82611ms step_avg:51.47ms
step:1606/2160 train_time:82698ms step_avg:51.49ms
step:1607/2160 train_time:82786ms step_avg:51.52ms
step:1608/2160 train_time:82873ms step_avg:51.54ms
step:1609/2160 train_time:82961ms step_avg:51.56ms
step:1610/2160 train_time:83047ms step_avg:51.58ms
step:1611/2160 train_time:83135ms step_avg:51.60ms
step:1612/2160 train_time:83222ms step_avg:51.63ms
step:1613/2160 train_time:83310ms step_avg:51.65ms
step:1614/2160 train_time:83396ms step_avg:51.67ms
step:1615/2160 train_time:83484ms step_avg:51.69ms
step:1616/2160 train_time:83570ms step_avg:51.71ms
step:1617/2160 train_time:83659ms step_avg:51.74ms
step:1618/2160 train_time:83745ms step_avg:51.76ms
step:1619/2160 train_time:83835ms step_avg:51.78ms
step:1620/2160 train_time:83921ms step_avg:51.80ms
step:1621/2160 train_time:84009ms step_avg:51.83ms
step:1622/2160 train_time:84095ms step_avg:51.85ms
step:1623/2160 train_time:84183ms step_avg:51.87ms
step:1624/2160 train_time:84270ms step_avg:51.89ms
step:1625/2160 train_time:84358ms step_avg:51.91ms
step:1626/2160 train_time:84445ms step_avg:51.93ms
step:1627/2160 train_time:84533ms step_avg:51.96ms
step:1628/2160 train_time:84619ms step_avg:51.98ms
step:1629/2160 train_time:84708ms step_avg:52.00ms
step:1630/2160 train_time:84795ms step_avg:52.02ms
step:1631/2160 train_time:84883ms step_avg:52.04ms
step:1632/2160 train_time:84969ms step_avg:52.06ms
step:1633/2160 train_time:85057ms step_avg:52.09ms
step:1634/2160 train_time:85144ms step_avg:52.11ms
step:1635/2160 train_time:85232ms step_avg:52.13ms
step:1636/2160 train_time:85318ms step_avg:52.15ms
step:1637/2160 train_time:85406ms step_avg:52.17ms
step:1638/2160 train_time:85493ms step_avg:52.19ms
step:1639/2160 train_time:85582ms step_avg:52.22ms
step:1640/2160 train_time:85668ms step_avg:52.24ms
step:1641/2160 train_time:85757ms step_avg:52.26ms
step:1642/2160 train_time:85844ms step_avg:52.28ms
step:1643/2160 train_time:85932ms step_avg:52.30ms
step:1644/2160 train_time:86018ms step_avg:52.32ms
step:1645/2160 train_time:86107ms step_avg:52.34ms
step:1646/2160 train_time:86194ms step_avg:52.37ms
step:1647/2160 train_time:86282ms step_avg:52.39ms
step:1648/2160 train_time:86369ms step_avg:52.41ms
step:1649/2160 train_time:86457ms step_avg:52.43ms
step:1650/2160 train_time:86544ms step_avg:52.45ms
step:1651/2160 train_time:86632ms step_avg:52.47ms
step:1652/2160 train_time:86718ms step_avg:52.49ms
step:1653/2160 train_time:86807ms step_avg:52.51ms
step:1654/2160 train_time:86893ms step_avg:52.54ms
step:1655/2160 train_time:86981ms step_avg:52.56ms
step:1656/2160 train_time:87067ms step_avg:52.58ms
step:1657/2160 train_time:87156ms step_avg:52.60ms
step:1658/2160 train_time:87243ms step_avg:52.62ms
step:1659/2160 train_time:87331ms step_avg:52.64ms
step:1660/2160 train_time:87417ms step_avg:52.66ms
step:1661/2160 train_time:87505ms step_avg:52.68ms
step:1662/2160 train_time:87592ms step_avg:52.70ms
step:1663/2160 train_time:87680ms step_avg:52.72ms
step:1664/2160 train_time:87766ms step_avg:52.74ms
step:1665/2160 train_time:87855ms step_avg:52.77ms
step:1666/2160 train_time:87941ms step_avg:52.79ms
step:1667/2160 train_time:88029ms step_avg:52.81ms
step:1668/2160 train_time:88115ms step_avg:52.83ms
step:1669/2160 train_time:88204ms step_avg:52.85ms
step:1670/2160 train_time:88291ms step_avg:52.87ms
step:1671/2160 train_time:88379ms step_avg:52.89ms
step:1672/2160 train_time:88466ms step_avg:52.91ms
step:1673/2160 train_time:88554ms step_avg:52.93ms
step:1674/2160 train_time:88640ms step_avg:52.95ms
step:1675/2160 train_time:88729ms step_avg:52.97ms
step:1676/2160 train_time:88815ms step_avg:52.99ms
step:1677/2160 train_time:88904ms step_avg:53.01ms
step:1678/2160 train_time:88991ms step_avg:53.03ms
step:1679/2160 train_time:89079ms step_avg:53.05ms
step:1680/2160 train_time:89165ms step_avg:53.07ms
step:1681/2160 train_time:89253ms step_avg:53.10ms
step:1682/2160 train_time:89339ms step_avg:53.12ms
step:1683/2160 train_time:89428ms step_avg:53.14ms
step:1684/2160 train_time:89514ms step_avg:53.16ms
step:1685/2160 train_time:89602ms step_avg:53.18ms
step:1686/2160 train_time:89688ms step_avg:53.20ms
step:1687/2160 train_time:89776ms step_avg:53.22ms
step:1688/2160 train_time:89863ms step_avg:53.24ms
step:1689/2160 train_time:89951ms step_avg:53.26ms
step:1690/2160 train_time:90037ms step_avg:53.28ms
step:1691/2160 train_time:90126ms step_avg:53.30ms
step:1692/2160 train_time:90212ms step_avg:53.32ms
step:1693/2160 train_time:90301ms step_avg:53.34ms
step:1694/2160 train_time:90387ms step_avg:53.36ms
step:1695/2160 train_time:90474ms step_avg:53.38ms
step:1696/2160 train_time:90560ms step_avg:53.40ms
step:1697/2160 train_time:90649ms step_avg:53.42ms
step:1698/2160 train_time:90735ms step_avg:53.44ms
step:1699/2160 train_time:90823ms step_avg:53.46ms
step:1700/2160 train_time:90909ms step_avg:53.48ms
step:1701/2160 train_time:90998ms step_avg:53.50ms
step:1702/2160 train_time:91084ms step_avg:53.52ms
step:1703/2160 train_time:91173ms step_avg:53.54ms
step:1704/2160 train_time:91259ms step_avg:53.56ms
step:1705/2160 train_time:91348ms step_avg:53.58ms
step:1706/2160 train_time:91435ms step_avg:53.60ms
step:1707/2160 train_time:91524ms step_avg:53.62ms
step:1708/2160 train_time:91610ms step_avg:53.64ms
step:1709/2160 train_time:91699ms step_avg:53.66ms
step:1710/2160 train_time:91785ms step_avg:53.68ms
step:1711/2160 train_time:91873ms step_avg:53.70ms
step:1712/2160 train_time:91960ms step_avg:53.71ms
step:1713/2160 train_time:92048ms step_avg:53.74ms
step:1714/2160 train_time:92135ms step_avg:53.75ms
step:1715/2160 train_time:92223ms step_avg:53.77ms
step:1716/2160 train_time:92309ms step_avg:53.79ms
step:1717/2160 train_time:92398ms step_avg:53.81ms
step:1718/2160 train_time:92484ms step_avg:53.83ms
step:1719/2160 train_time:92573ms step_avg:53.85ms
step:1720/2160 train_time:92659ms step_avg:53.87ms
step:1721/2160 train_time:92748ms step_avg:53.89ms
step:1722/2160 train_time:92835ms step_avg:53.91ms
step:1723/2160 train_time:92923ms step_avg:53.93ms
step:1724/2160 train_time:93009ms step_avg:53.95ms
step:1725/2160 train_time:93098ms step_avg:53.97ms
step:1726/2160 train_time:93184ms step_avg:53.99ms
step:1727/2160 train_time:93273ms step_avg:54.01ms
step:1728/2160 train_time:93358ms step_avg:54.03ms
step:1729/2160 train_time:93447ms step_avg:54.05ms
step:1730/2160 train_time:93534ms step_avg:54.07ms
step:1731/2160 train_time:93622ms step_avg:54.09ms
step:1732/2160 train_time:93708ms step_avg:54.10ms
step:1733/2160 train_time:93797ms step_avg:54.12ms
step:1734/2160 train_time:93883ms step_avg:54.14ms
step:1735/2160 train_time:93971ms step_avg:54.16ms
step:1736/2160 train_time:94058ms step_avg:54.18ms
step:1737/2160 train_time:94146ms step_avg:54.20ms
step:1738/2160 train_time:94232ms step_avg:54.22ms
step:1739/2160 train_time:94320ms step_avg:54.24ms
step:1740/2160 train_time:94406ms step_avg:54.26ms
step:1741/2160 train_time:94494ms step_avg:54.28ms
step:1742/2160 train_time:94580ms step_avg:54.29ms
step:1743/2160 train_time:94668ms step_avg:54.31ms
step:1744/2160 train_time:94755ms step_avg:54.33ms
step:1745/2160 train_time:94843ms step_avg:54.35ms
step:1746/2160 train_time:94929ms step_avg:54.37ms
step:1747/2160 train_time:95019ms step_avg:54.39ms
step:1748/2160 train_time:95105ms step_avg:54.41ms
step:1749/2160 train_time:95193ms step_avg:54.43ms
step:1750/2160 train_time:95280ms step_avg:54.45ms
step:1750/2160 val_loss:3.3772 train_time:95369ms step_avg:54.50ms
step:1751/2160 train_time:95388ms step_avg:54.48ms
step:1752/2160 train_time:95459ms step_avg:54.49ms
step:1753/2160 train_time:95554ms step_avg:54.51ms
step:1754/2160 train_time:95640ms step_avg:54.53ms
step:1755/2160 train_time:95728ms step_avg:54.55ms
step:1756/2160 train_time:95813ms step_avg:54.56ms
step:1757/2160 train_time:95901ms step_avg:54.58ms
step:1758/2160 train_time:95986ms step_avg:54.60ms
step:1759/2160 train_time:96073ms step_avg:54.62ms
step:1760/2160 train_time:96159ms step_avg:54.64ms
step:1761/2160 train_time:96246ms step_avg:54.65ms
step:1762/2160 train_time:96333ms step_avg:54.67ms
step:1763/2160 train_time:96424ms step_avg:54.69ms
step:1764/2160 train_time:96514ms step_avg:54.71ms
step:1765/2160 train_time:96604ms step_avg:54.73ms
step:1766/2160 train_time:96691ms step_avg:54.75ms
step:1767/2160 train_time:96779ms step_avg:54.77ms
step:1768/2160 train_time:96865ms step_avg:54.79ms
step:1769/2160 train_time:96952ms step_avg:54.81ms
step:1770/2160 train_time:97037ms step_avg:54.82ms
step:1771/2160 train_time:97124ms step_avg:54.84ms
step:1772/2160 train_time:97210ms step_avg:54.86ms
step:1773/2160 train_time:97298ms step_avg:54.88ms
step:1774/2160 train_time:97385ms step_avg:54.90ms
step:1775/2160 train_time:97476ms step_avg:54.92ms
step:1776/2160 train_time:97563ms step_avg:54.93ms
step:1777/2160 train_time:97652ms step_avg:54.95ms
step:1778/2160 train_time:97738ms step_avg:54.97ms
step:1779/2160 train_time:97826ms step_avg:54.99ms
step:1780/2160 train_time:97912ms step_avg:55.01ms
step:1781/2160 train_time:97999ms step_avg:55.02ms
step:1782/2160 train_time:98084ms step_avg:55.04ms
step:1783/2160 train_time:98172ms step_avg:55.06ms
step:1784/2160 train_time:98259ms step_avg:55.08ms
step:1785/2160 train_time:98347ms step_avg:55.10ms
step:1786/2160 train_time:98435ms step_avg:55.11ms
step:1787/2160 train_time:98524ms step_avg:55.13ms
step:1788/2160 train_time:98611ms step_avg:55.15ms
step:1789/2160 train_time:98700ms step_avg:55.17ms
step:1790/2160 train_time:98787ms step_avg:55.19ms
step:1791/2160 train_time:98875ms step_avg:55.21ms
step:1792/2160 train_time:98961ms step_avg:55.22ms
step:1793/2160 train_time:99049ms step_avg:55.24ms
step:1794/2160 train_time:99134ms step_avg:55.26ms
step:1795/2160 train_time:99223ms step_avg:55.28ms
step:1796/2160 train_time:99310ms step_avg:55.29ms
step:1797/2160 train_time:99398ms step_avg:55.31ms
step:1798/2160 train_time:99486ms step_avg:55.33ms
step:1799/2160 train_time:99576ms step_avg:55.35ms
step:1800/2160 train_time:99662ms step_avg:55.37ms
step:1801/2160 train_time:99751ms step_avg:55.39ms
step:1802/2160 train_time:99836ms step_avg:55.40ms
step:1803/2160 train_time:99925ms step_avg:55.42ms
step:1804/2160 train_time:100011ms step_avg:55.44ms
step:1805/2160 train_time:100098ms step_avg:55.46ms
step:1806/2160 train_time:100184ms step_avg:55.47ms
step:1807/2160 train_time:100272ms step_avg:55.49ms
step:1808/2160 train_time:100359ms step_avg:55.51ms
step:1809/2160 train_time:100447ms step_avg:55.53ms
step:1810/2160 train_time:100534ms step_avg:55.54ms
step:1811/2160 train_time:100623ms step_avg:55.56ms
step:1812/2160 train_time:100709ms step_avg:55.58ms
step:1813/2160 train_time:100797ms step_avg:55.60ms
step:1814/2160 train_time:100883ms step_avg:55.61ms
step:1815/2160 train_time:100972ms step_avg:55.63ms
step:1816/2160 train_time:101058ms step_avg:55.65ms
step:1817/2160 train_time:101145ms step_avg:55.67ms
step:1818/2160 train_time:101231ms step_avg:55.68ms
step:1819/2160 train_time:101319ms step_avg:55.70ms
step:1820/2160 train_time:101406ms step_avg:55.72ms
step:1821/2160 train_time:101495ms step_avg:55.74ms
step:1822/2160 train_time:101582ms step_avg:55.75ms
step:1823/2160 train_time:101671ms step_avg:55.77ms
step:1824/2160 train_time:101757ms step_avg:55.79ms
step:1825/2160 train_time:101845ms step_avg:55.81ms
step:1826/2160 train_time:101931ms step_avg:55.82ms
step:1827/2160 train_time:102019ms step_avg:55.84ms
step:1828/2160 train_time:102105ms step_avg:55.86ms
step:1829/2160 train_time:102193ms step_avg:55.87ms
step:1830/2160 train_time:102279ms step_avg:55.89ms
step:1831/2160 train_time:102367ms step_avg:55.91ms
step:1832/2160 train_time:102454ms step_avg:55.92ms
step:1833/2160 train_time:102542ms step_avg:55.94ms
step:1834/2160 train_time:102629ms step_avg:55.96ms
step:1835/2160 train_time:102718ms step_avg:55.98ms
step:1836/2160 train_time:102804ms step_avg:55.99ms
step:1837/2160 train_time:102894ms step_avg:56.01ms
step:1838/2160 train_time:102980ms step_avg:56.03ms
step:1839/2160 train_time:103068ms step_avg:56.05ms
step:1840/2160 train_time:103154ms step_avg:56.06ms
step:1841/2160 train_time:103243ms step_avg:56.08ms
step:1842/2160 train_time:103329ms step_avg:56.10ms
step:1843/2160 train_time:103418ms step_avg:56.11ms
step:1844/2160 train_time:103504ms step_avg:56.13ms
step:1845/2160 train_time:103593ms step_avg:56.15ms
step:1846/2160 train_time:103680ms step_avg:56.16ms
step:1847/2160 train_time:103768ms step_avg:56.18ms
step:1848/2160 train_time:103855ms step_avg:56.20ms
step:1849/2160 train_time:103943ms step_avg:56.22ms
step:1850/2160 train_time:104028ms step_avg:56.23ms
step:1851/2160 train_time:104116ms step_avg:56.25ms
step:1852/2160 train_time:104203ms step_avg:56.27ms
step:1853/2160 train_time:104291ms step_avg:56.28ms
step:1854/2160 train_time:104377ms step_avg:56.30ms
step:1855/2160 train_time:104466ms step_avg:56.32ms
step:1856/2160 train_time:104552ms step_avg:56.33ms
step:1857/2160 train_time:104640ms step_avg:56.35ms
step:1858/2160 train_time:104727ms step_avg:56.37ms
step:1859/2160 train_time:104816ms step_avg:56.38ms
step:1860/2160 train_time:104903ms step_avg:56.40ms
step:1861/2160 train_time:104991ms step_avg:56.42ms
step:1862/2160 train_time:105077ms step_avg:56.43ms
step:1863/2160 train_time:105165ms step_avg:56.45ms
step:1864/2160 train_time:105252ms step_avg:56.47ms
step:1865/2160 train_time:105339ms step_avg:56.48ms
step:1866/2160 train_time:105427ms step_avg:56.50ms
step:1867/2160 train_time:105515ms step_avg:56.52ms
step:1868/2160 train_time:105602ms step_avg:56.53ms
step:1869/2160 train_time:105690ms step_avg:56.55ms
step:1870/2160 train_time:105776ms step_avg:56.56ms
step:1871/2160 train_time:105864ms step_avg:56.58ms
step:1872/2160 train_time:105951ms step_avg:56.60ms
step:1873/2160 train_time:106039ms step_avg:56.61ms
step:1874/2160 train_time:106125ms step_avg:56.63ms
step:1875/2160 train_time:106213ms step_avg:56.65ms
step:1876/2160 train_time:106300ms step_avg:56.66ms
step:1877/2160 train_time:106388ms step_avg:56.68ms
step:1878/2160 train_time:106475ms step_avg:56.70ms
step:1879/2160 train_time:106565ms step_avg:56.71ms
step:1880/2160 train_time:106652ms step_avg:56.73ms
step:1881/2160 train_time:106740ms step_avg:56.75ms
step:1882/2160 train_time:106826ms step_avg:56.76ms
step:1883/2160 train_time:106915ms step_avg:56.78ms
step:1884/2160 train_time:107002ms step_avg:56.79ms
step:1885/2160 train_time:107090ms step_avg:56.81ms
step:1886/2160 train_time:107176ms step_avg:56.83ms
step:1887/2160 train_time:107265ms step_avg:56.84ms
step:1888/2160 train_time:107351ms step_avg:56.86ms
step:1889/2160 train_time:107439ms step_avg:56.88ms
step:1890/2160 train_time:107525ms step_avg:56.89ms
step:1891/2160 train_time:107614ms step_avg:56.91ms
step:1892/2160 train_time:107701ms step_avg:56.92ms
step:1893/2160 train_time:107789ms step_avg:56.94ms
step:1894/2160 train_time:107875ms step_avg:56.96ms
step:1895/2160 train_time:107964ms step_avg:56.97ms
step:1896/2160 train_time:108050ms step_avg:56.99ms
step:1897/2160 train_time:108138ms step_avg:57.00ms
step:1898/2160 train_time:108225ms step_avg:57.02ms
step:1899/2160 train_time:108313ms step_avg:57.04ms
step:1900/2160 train_time:108399ms step_avg:57.05ms
step:1901/2160 train_time:108487ms step_avg:57.07ms
step:1902/2160 train_time:108573ms step_avg:57.08ms
step:1903/2160 train_time:108662ms step_avg:57.10ms
step:1904/2160 train_time:108748ms step_avg:57.12ms
step:1905/2160 train_time:108836ms step_avg:57.13ms
step:1906/2160 train_time:108923ms step_avg:57.15ms
step:1907/2160 train_time:109011ms step_avg:57.16ms
step:1908/2160 train_time:109097ms step_avg:57.18ms
step:1909/2160 train_time:109186ms step_avg:57.20ms
step:1910/2160 train_time:109273ms step_avg:57.21ms
step:1911/2160 train_time:109362ms step_avg:57.23ms
step:1912/2160 train_time:109448ms step_avg:57.24ms
step:1913/2160 train_time:109535ms step_avg:57.26ms
step:1914/2160 train_time:109621ms step_avg:57.27ms
step:1915/2160 train_time:109710ms step_avg:57.29ms
step:1916/2160 train_time:109796ms step_avg:57.31ms
step:1917/2160 train_time:109885ms step_avg:57.32ms
step:1918/2160 train_time:109972ms step_avg:57.34ms
step:1919/2160 train_time:110061ms step_avg:57.35ms
step:1920/2160 train_time:110147ms step_avg:57.37ms
step:1921/2160 train_time:110236ms step_avg:57.38ms
step:1922/2160 train_time:110322ms step_avg:57.40ms
step:1923/2160 train_time:110410ms step_avg:57.42ms
step:1924/2160 train_time:110496ms step_avg:57.43ms
step:1925/2160 train_time:110585ms step_avg:57.45ms
step:1926/2160 train_time:110671ms step_avg:57.46ms
step:1927/2160 train_time:110759ms step_avg:57.48ms
step:1928/2160 train_time:110845ms step_avg:57.49ms
step:1929/2160 train_time:110933ms step_avg:57.51ms
step:1930/2160 train_time:111020ms step_avg:57.52ms
step:1931/2160 train_time:111108ms step_avg:57.54ms
step:1932/2160 train_time:111194ms step_avg:57.55ms
step:1933/2160 train_time:111282ms step_avg:57.57ms
step:1934/2160 train_time:111368ms step_avg:57.58ms
step:1935/2160 train_time:111457ms step_avg:57.60ms
step:1936/2160 train_time:111543ms step_avg:57.62ms
step:1937/2160 train_time:111631ms step_avg:57.63ms
step:1938/2160 train_time:111717ms step_avg:57.65ms
step:1939/2160 train_time:111806ms step_avg:57.66ms
step:1940/2160 train_time:111894ms step_avg:57.68ms
step:1941/2160 train_time:111981ms step_avg:57.69ms
step:1942/2160 train_time:112067ms step_avg:57.71ms
step:1943/2160 train_time:112156ms step_avg:57.72ms
step:1944/2160 train_time:112243ms step_avg:57.74ms
step:1945/2160 train_time:112332ms step_avg:57.75ms
step:1946/2160 train_time:112417ms step_avg:57.77ms
step:1947/2160 train_time:112506ms step_avg:57.78ms
step:1948/2160 train_time:112592ms step_avg:57.80ms
step:1949/2160 train_time:112680ms step_avg:57.81ms
step:1950/2160 train_time:112766ms step_avg:57.83ms
step:1951/2160 train_time:112854ms step_avg:57.84ms
step:1952/2160 train_time:112941ms step_avg:57.86ms
step:1953/2160 train_time:113029ms step_avg:57.87ms
step:1954/2160 train_time:113115ms step_avg:57.89ms
step:1955/2160 train_time:113204ms step_avg:57.90ms
step:1956/2160 train_time:113291ms step_avg:57.92ms
step:1957/2160 train_time:113378ms step_avg:57.93ms
step:1958/2160 train_time:113464ms step_avg:57.95ms
step:1959/2160 train_time:113553ms step_avg:57.96ms
step:1960/2160 train_time:113639ms step_avg:57.98ms
step:1961/2160 train_time:113728ms step_avg:57.99ms
step:1962/2160 train_time:113814ms step_avg:58.01ms
step:1963/2160 train_time:113903ms step_avg:58.02ms
step:1964/2160 train_time:113990ms step_avg:58.04ms
step:1965/2160 train_time:114078ms step_avg:58.06ms
step:1966/2160 train_time:114165ms step_avg:58.07ms
step:1967/2160 train_time:114253ms step_avg:58.09ms
step:1968/2160 train_time:114339ms step_avg:58.10ms
step:1969/2160 train_time:114428ms step_avg:58.11ms
step:1970/2160 train_time:114514ms step_avg:58.13ms
step:1971/2160 train_time:114602ms step_avg:58.14ms
step:1972/2160 train_time:114690ms step_avg:58.16ms
step:1973/2160 train_time:114777ms step_avg:58.17ms
step:1974/2160 train_time:114863ms step_avg:58.19ms
step:1975/2160 train_time:114951ms step_avg:58.20ms
step:1976/2160 train_time:115038ms step_avg:58.22ms
step:1977/2160 train_time:115127ms step_avg:58.23ms
step:1978/2160 train_time:115213ms step_avg:58.25ms
step:1979/2160 train_time:115302ms step_avg:58.26ms
step:1980/2160 train_time:115389ms step_avg:58.28ms
step:1981/2160 train_time:115477ms step_avg:58.29ms
step:1982/2160 train_time:115563ms step_avg:58.31ms
step:1983/2160 train_time:115651ms step_avg:58.32ms
step:1984/2160 train_time:115737ms step_avg:58.34ms
step:1985/2160 train_time:115825ms step_avg:58.35ms
step:1986/2160 train_time:115912ms step_avg:58.36ms
step:1987/2160 train_time:116000ms step_avg:58.38ms
step:1988/2160 train_time:116087ms step_avg:58.39ms
step:1989/2160 train_time:116175ms step_avg:58.41ms
step:1990/2160 train_time:116262ms step_avg:58.42ms
step:1991/2160 train_time:116351ms step_avg:58.44ms
step:1992/2160 train_time:116437ms step_avg:58.45ms
step:1993/2160 train_time:116525ms step_avg:58.47ms
step:1994/2160 train_time:116611ms step_avg:58.48ms
step:1995/2160 train_time:116700ms step_avg:58.50ms
step:1996/2160 train_time:116786ms step_avg:58.51ms
step:1997/2160 train_time:116875ms step_avg:58.53ms
step:1998/2160 train_time:116961ms step_avg:58.54ms
step:1999/2160 train_time:117049ms step_avg:58.55ms
step:2000/2160 train_time:117135ms step_avg:58.57ms
step:2000/2160 val_loss:3.3080 train_time:117225ms step_avg:58.61ms
step:2001/2160 train_time:117244ms step_avg:58.59ms
step:2002/2160 train_time:117315ms step_avg:58.60ms
step:2003/2160 train_time:117408ms step_avg:58.62ms
step:2004/2160 train_time:117495ms step_avg:58.63ms
step:2005/2160 train_time:117584ms step_avg:58.65ms
step:2006/2160 train_time:117669ms step_avg:58.66ms
step:2007/2160 train_time:117757ms step_avg:58.67ms
step:2008/2160 train_time:117842ms step_avg:58.69ms
step:2009/2160 train_time:117929ms step_avg:58.70ms
step:2010/2160 train_time:118015ms step_avg:58.71ms
step:2011/2160 train_time:118102ms step_avg:58.73ms
step:2012/2160 train_time:118188ms step_avg:58.74ms
step:2013/2160 train_time:118280ms step_avg:58.76ms
step:2014/2160 train_time:118368ms step_avg:58.77ms
step:2015/2160 train_time:118457ms step_avg:58.79ms
step:2016/2160 train_time:118544ms step_avg:58.80ms
step:2017/2160 train_time:118632ms step_avg:58.82ms
step:2018/2160 train_time:118718ms step_avg:58.83ms
step:2019/2160 train_time:118805ms step_avg:58.84ms
step:2020/2160 train_time:118891ms step_avg:58.86ms
step:2021/2160 train_time:118980ms step_avg:58.87ms
step:2022/2160 train_time:119065ms step_avg:58.88ms
step:2023/2160 train_time:119153ms step_avg:58.90ms
step:2024/2160 train_time:119240ms step_avg:58.91ms
step:2025/2160 train_time:119330ms step_avg:58.93ms
step:2026/2160 train_time:119417ms step_avg:58.94ms
step:2027/2160 train_time:119506ms step_avg:58.96ms
step:2028/2160 train_time:119593ms step_avg:58.97ms
step:2029/2160 train_time:119682ms step_avg:58.99ms
step:2030/2160 train_time:119768ms step_avg:59.00ms
step:2031/2160 train_time:119856ms step_avg:59.01ms
step:2032/2160 train_time:119941ms step_avg:59.03ms
step:2033/2160 train_time:120028ms step_avg:59.04ms
step:2034/2160 train_time:120115ms step_avg:59.05ms
step:2035/2160 train_time:120204ms step_avg:59.07ms
step:2036/2160 train_time:120291ms step_avg:59.08ms
step:2037/2160 train_time:120380ms step_avg:59.10ms
step:2038/2160 train_time:120468ms step_avg:59.11ms
step:2039/2160 train_time:120556ms step_avg:59.13ms
step:2040/2160 train_time:120644ms step_avg:59.14ms
step:2041/2160 train_time:120732ms step_avg:59.15ms
step:2042/2160 train_time:120817ms step_avg:59.17ms
step:2043/2160 train_time:120905ms step_avg:59.18ms
step:2044/2160 train_time:120990ms step_avg:59.19ms
step:2045/2160 train_time:121079ms step_avg:59.21ms
step:2046/2160 train_time:121166ms step_avg:59.22ms
step:2047/2160 train_time:121254ms step_avg:59.23ms
step:2048/2160 train_time:121341ms step_avg:59.25ms
step:2049/2160 train_time:121430ms step_avg:59.26ms
step:2050/2160 train_time:121517ms step_avg:59.28ms
step:2051/2160 train_time:121606ms step_avg:59.29ms
step:2052/2160 train_time:121692ms step_avg:59.30ms
step:2053/2160 train_time:121780ms step_avg:59.32ms
step:2054/2160 train_time:121867ms step_avg:59.33ms
step:2055/2160 train_time:121954ms step_avg:59.35ms
step:2056/2160 train_time:122040ms step_avg:59.36ms
step:2057/2160 train_time:122129ms step_avg:59.37ms
step:2058/2160 train_time:122215ms step_avg:59.39ms
step:2059/2160 train_time:122302ms step_avg:59.40ms
step:2060/2160 train_time:122389ms step_avg:59.41ms
step:2061/2160 train_time:122478ms step_avg:59.43ms
step:2062/2160 train_time:122564ms step_avg:59.44ms
step:2063/2160 train_time:122652ms step_avg:59.45ms
step:2064/2160 train_time:122738ms step_avg:59.47ms
step:2065/2160 train_time:122826ms step_avg:59.48ms
step:2066/2160 train_time:122912ms step_avg:59.49ms
step:2067/2160 train_time:123001ms step_avg:59.51ms
step:2068/2160 train_time:123087ms step_avg:59.52ms
step:2069/2160 train_time:123175ms step_avg:59.53ms
step:2070/2160 train_time:123262ms step_avg:59.55ms
step:2071/2160 train_time:123351ms step_avg:59.56ms
step:2072/2160 train_time:123438ms step_avg:59.57ms
step:2073/2160 train_time:123526ms step_avg:59.59ms
step:2074/2160 train_time:123612ms step_avg:59.60ms
step:2075/2160 train_time:123701ms step_avg:59.62ms
step:2076/2160 train_time:123787ms step_avg:59.63ms
step:2077/2160 train_time:123875ms step_avg:59.64ms
step:2078/2160 train_time:123961ms step_avg:59.65ms
step:2079/2160 train_time:124050ms step_avg:59.67ms
step:2080/2160 train_time:124137ms step_avg:59.68ms
step:2081/2160 train_time:124225ms step_avg:59.69ms
step:2082/2160 train_time:124311ms step_avg:59.71ms
step:2083/2160 train_time:124400ms step_avg:59.72ms
step:2084/2160 train_time:124486ms step_avg:59.73ms
step:2085/2160 train_time:124575ms step_avg:59.75ms
step:2086/2160 train_time:124661ms step_avg:59.76ms
step:2087/2160 train_time:124750ms step_avg:59.77ms
step:2088/2160 train_time:124837ms step_avg:59.79ms
step:2089/2160 train_time:124925ms step_avg:59.80ms
step:2090/2160 train_time:125012ms step_avg:59.81ms
step:2091/2160 train_time:125100ms step_avg:59.83ms
step:2092/2160 train_time:125186ms step_avg:59.84ms
step:2093/2160 train_time:125274ms step_avg:59.85ms
step:2094/2160 train_time:125361ms step_avg:59.87ms
step:2095/2160 train_time:125450ms step_avg:59.88ms
step:2096/2160 train_time:125537ms step_avg:59.89ms
step:2097/2160 train_time:125625ms step_avg:59.91ms
step:2098/2160 train_time:125711ms step_avg:59.92ms
step:2099/2160 train_time:125799ms step_avg:59.93ms
step:2100/2160 train_time:125885ms step_avg:59.95ms
step:2101/2160 train_time:125973ms step_avg:59.96ms
step:2102/2160 train_time:126060ms step_avg:59.97ms
step:2103/2160 train_time:126148ms step_avg:59.98ms
step:2104/2160 train_time:126234ms step_avg:60.00ms
step:2105/2160 train_time:126323ms step_avg:60.01ms
step:2106/2160 train_time:126409ms step_avg:60.02ms
step:2107/2160 train_time:126498ms step_avg:60.04ms
step:2108/2160 train_time:126584ms step_avg:60.05ms
step:2109/2160 train_time:126672ms step_avg:60.06ms
step:2110/2160 train_time:126759ms step_avg:60.08ms
step:2111/2160 train_time:126848ms step_avg:60.09ms
step:2112/2160 train_time:126935ms step_avg:60.10ms
step:2113/2160 train_time:127023ms step_avg:60.12ms
step:2114/2160 train_time:127110ms step_avg:60.13ms
step:2115/2160 train_time:127198ms step_avg:60.14ms
step:2116/2160 train_time:127284ms step_avg:60.15ms
step:2117/2160 train_time:127373ms step_avg:60.17ms
step:2118/2160 train_time:127459ms step_avg:60.18ms
step:2119/2160 train_time:127547ms step_avg:60.19ms
step:2120/2160 train_time:127634ms step_avg:60.20ms
step:2121/2160 train_time:127722ms step_avg:60.22ms
step:2122/2160 train_time:127808ms step_avg:60.23ms
step:2123/2160 train_time:127897ms step_avg:60.24ms
step:2124/2160 train_time:127985ms step_avg:60.26ms
step:2125/2160 train_time:128073ms step_avg:60.27ms
step:2126/2160 train_time:128160ms step_avg:60.28ms
step:2127/2160 train_time:128249ms step_avg:60.30ms
step:2128/2160 train_time:128335ms step_avg:60.31ms
step:2129/2160 train_time:128424ms step_avg:60.32ms
step:2130/2160 train_time:128511ms step_avg:60.33ms
step:2131/2160 train_time:128599ms step_avg:60.35ms
step:2132/2160 train_time:128685ms step_avg:60.36ms
step:2133/2160 train_time:128775ms step_avg:60.37ms
step:2134/2160 train_time:128861ms step_avg:60.38ms
step:2135/2160 train_time:128951ms step_avg:60.40ms
step:2136/2160 train_time:129038ms step_avg:60.41ms
step:2137/2160 train_time:129126ms step_avg:60.42ms
step:2138/2160 train_time:129213ms step_avg:60.44ms
step:2139/2160 train_time:129301ms step_avg:60.45ms
step:2140/2160 train_time:129388ms step_avg:60.46ms
step:2141/2160 train_time:129476ms step_avg:60.47ms
step:2142/2160 train_time:129562ms step_avg:60.49ms
step:2143/2160 train_time:129651ms step_avg:60.50ms
step:2144/2160 train_time:129738ms step_avg:60.51ms
step:2145/2160 train_time:129827ms step_avg:60.53ms
step:2146/2160 train_time:129914ms step_avg:60.54ms
step:2147/2160 train_time:130003ms step_avg:60.55ms
step:2148/2160 train_time:130089ms step_avg:60.56ms
step:2149/2160 train_time:130178ms step_avg:60.58ms
step:2150/2160 train_time:130264ms step_avg:60.59ms
step:2151/2160 train_time:130353ms step_avg:60.60ms
step:2152/2160 train_time:130440ms step_avg:60.61ms
step:2153/2160 train_time:130529ms step_avg:60.63ms
step:2154/2160 train_time:130616ms step_avg:60.64ms
step:2155/2160 train_time:130706ms step_avg:60.65ms
step:2156/2160 train_time:130791ms step_avg:60.66ms
step:2157/2160 train_time:130880ms step_avg:60.68ms
step:2158/2160 train_time:130967ms step_avg:60.69ms
step:2159/2160 train_time:131056ms step_avg:60.70ms
step:2160/2160 train_time:131143ms step_avg:60.71ms
step:2160/2160 val_loss:3.2762 train_time:131233ms step_avg:60.76ms
peak memory allocated: 30032 MiB reserved: 45036 MiB

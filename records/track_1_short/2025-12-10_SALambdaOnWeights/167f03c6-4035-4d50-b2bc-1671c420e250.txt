import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 10 13:54:54 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                  Off |
| N/A   28C    P0            148W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                  Off |
| N/A   25C    P0            138W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                  Off |
| N/A   22C    P0            129W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                  Off |
| N/A   25C    P0            132W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                  Off |
| N/A   26C    P0            133W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                  Off |
| N/A   24C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                  Off |
| N/A   25C    P0            138W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                  Off |
| N/A   23C    P0            133W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     71829      C   /usr/bin/python3                             1510MiB |
|    0   N/A  N/A     71830      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     71831      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     71832      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     71833      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     71834      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     71835      C   /usr/bin/python3                              614MiB |
|    0   N/A  N/A     71836      C   /usr/bin/python3                              614MiB |
|    1   N/A  N/A     71830      C   /usr/bin/python3                             1510MiB |
|    2   N/A  N/A     71831      C   /usr/bin/python3                             1510MiB |
|    3   N/A  N/A     71832      C   /usr/bin/python3                             1510MiB |
|    4   N/A  N/A     71833      C   /usr/bin/python3                             1510MiB |
|    5   N/A  N/A     71834      C   /usr/bin/python3                             1510MiB |
|    6   N/A  N/A     71835      C   /usr/bin/python3                             1510MiB |
|    7   N/A  N/A     71836      C   /usr/bin/python3                             1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2160 train_time:89ms step_avg:89.05ms
step:2/2160 train_time:112ms step_avg:55.86ms
step:3/2160 train_time:131ms step_avg:43.79ms
step:4/2160 train_time:156ms step_avg:38.98ms
step:5/2160 train_time:190ms step_avg:37.94ms
step:6/2160 train_time:261ms step_avg:43.46ms
step:7/2160 train_time:279ms step_avg:39.92ms
step:8/2160 train_time:309ms step_avg:38.68ms
step:9/2160 train_time:343ms step_avg:38.12ms
step:10/2160 train_time:376ms step_avg:37.61ms
step:11/2160 train_time:410ms step_avg:37.24ms
step:12/2160 train_time:443ms step_avg:36.88ms
step:13/2160 train_time:477ms step_avg:36.67ms
step:14/2160 train_time:510ms step_avg:36.40ms
step:15/2160 train_time:543ms step_avg:36.23ms
step:16/2160 train_time:576ms step_avg:36.03ms
step:17/2160 train_time:610ms step_avg:35.90ms
step:18/2160 train_time:643ms step_avg:35.73ms
step:19/2160 train_time:677ms step_avg:35.63ms
step:20/2160 train_time:710ms step_avg:35.49ms
step:21/2160 train_time:744ms step_avg:35.43ms
step:22/2160 train_time:777ms step_avg:35.32ms
step:23/2160 train_time:811ms step_avg:35.26ms
step:24/2160 train_time:844ms step_avg:35.16ms
step:25/2160 train_time:878ms step_avg:35.12ms
step:26/2160 train_time:911ms step_avg:35.03ms
step:27/2160 train_time:945ms step_avg:35.01ms
step:28/2160 train_time:978ms step_avg:34.93ms
step:29/2160 train_time:1012ms step_avg:34.90ms
step:30/2160 train_time:1045ms step_avg:34.83ms
step:31/2160 train_time:1079ms step_avg:34.81ms
step:32/2160 train_time:1112ms step_avg:34.75ms
step:33/2160 train_time:1146ms step_avg:34.73ms
step:34/2160 train_time:1179ms step_avg:34.68ms
step:35/2160 train_time:1213ms step_avg:34.66ms
step:36/2160 train_time:1246ms step_avg:34.61ms
step:37/2160 train_time:1280ms step_avg:34.59ms
step:38/2160 train_time:1313ms step_avg:34.55ms
step:39/2160 train_time:1346ms step_avg:34.52ms
step:40/2160 train_time:1379ms step_avg:34.49ms
step:41/2160 train_time:1413ms step_avg:34.47ms
step:42/2160 train_time:1446ms step_avg:34.43ms
step:43/2160 train_time:1480ms step_avg:34.43ms
step:44/2160 train_time:1513ms step_avg:34.39ms
step:45/2160 train_time:1547ms step_avg:34.38ms
step:46/2160 train_time:1580ms step_avg:34.35ms
step:47/2160 train_time:1614ms step_avg:34.35ms
step:48/2160 train_time:1647ms step_avg:34.32ms
step:49/2160 train_time:1681ms step_avg:34.31ms
step:50/2160 train_time:1714ms step_avg:34.28ms
step:51/2160 train_time:1748ms step_avg:34.28ms
step:52/2160 train_time:1781ms step_avg:34.25ms
step:53/2160 train_time:1815ms step_avg:34.24ms
step:54/2160 train_time:1848ms step_avg:34.22ms
step:55/2160 train_time:1882ms step_avg:34.22ms
step:56/2160 train_time:1915ms step_avg:34.19ms
step:57/2160 train_time:1949ms step_avg:34.19ms
step:58/2160 train_time:1982ms step_avg:34.17ms
step:59/2160 train_time:2016ms step_avg:34.17ms
step:60/2160 train_time:2049ms step_avg:34.15ms
step:61/2160 train_time:2083ms step_avg:34.15ms
step:62/2160 train_time:2116ms step_avg:34.13ms
step:63/2160 train_time:2150ms step_avg:34.13ms
step:64/2160 train_time:2183ms step_avg:34.11ms
step:65/2160 train_time:2217ms step_avg:34.10ms
step:66/2160 train_time:2250ms step_avg:34.08ms
step:67/2160 train_time:2284ms step_avg:34.09ms
step:68/2160 train_time:2317ms step_avg:34.07ms
step:69/2160 train_time:2350ms step_avg:34.06ms
step:70/2160 train_time:2383ms step_avg:34.04ms
step:71/2160 train_time:2417ms step_avg:34.04ms
step:72/2160 train_time:2450ms step_avg:34.02ms
step:73/2160 train_time:2484ms step_avg:34.03ms
step:74/2160 train_time:2517ms step_avg:34.01ms
step:75/2160 train_time:2551ms step_avg:34.01ms
step:76/2160 train_time:2583ms step_avg:33.99ms
step:77/2160 train_time:2617ms step_avg:33.99ms
step:78/2160 train_time:2650ms step_avg:33.98ms
step:79/2160 train_time:2684ms step_avg:33.97ms
step:80/2160 train_time:2717ms step_avg:33.96ms
step:81/2160 train_time:2750ms step_avg:33.95ms
step:82/2160 train_time:2783ms step_avg:33.94ms
step:83/2160 train_time:2817ms step_avg:33.94ms
step:84/2160 train_time:2850ms step_avg:33.93ms
step:85/2160 train_time:2883ms step_avg:33.92ms
step:86/2160 train_time:2916ms step_avg:33.91ms
step:87/2160 train_time:2950ms step_avg:33.91ms
step:88/2160 train_time:2983ms step_avg:33.90ms
step:89/2160 train_time:3017ms step_avg:33.89ms
step:90/2160 train_time:3050ms step_avg:33.88ms
step:91/2160 train_time:3084ms step_avg:33.89ms
step:92/2160 train_time:3117ms step_avg:33.88ms
step:93/2160 train_time:3150ms step_avg:33.88ms
step:94/2160 train_time:3183ms step_avg:33.87ms
step:95/2160 train_time:3217ms step_avg:33.87ms
step:96/2160 train_time:3250ms step_avg:33.86ms
step:97/2160 train_time:3284ms step_avg:33.86ms
step:98/2160 train_time:3317ms step_avg:33.85ms
step:99/2160 train_time:3351ms step_avg:33.85ms
step:100/2160 train_time:3384ms step_avg:33.84ms
step:101/2160 train_time:3418ms step_avg:33.84ms
step:102/2160 train_time:3451ms step_avg:33.83ms
step:103/2160 train_time:3485ms step_avg:33.83ms
step:104/2160 train_time:3518ms step_avg:33.82ms
step:105/2160 train_time:3551ms step_avg:33.82ms
step:106/2160 train_time:3584ms step_avg:33.81ms
step:107/2160 train_time:3618ms step_avg:33.82ms
step:108/2160 train_time:3651ms step_avg:33.81ms
step:109/2160 train_time:3685ms step_avg:33.81ms
step:110/2160 train_time:3718ms step_avg:33.80ms
step:111/2160 train_time:3752ms step_avg:33.80ms
step:112/2160 train_time:3784ms step_avg:33.79ms
step:113/2160 train_time:3818ms step_avg:33.79ms
step:114/2160 train_time:3851ms step_avg:33.78ms
step:115/2160 train_time:3885ms step_avg:33.78ms
step:116/2160 train_time:3918ms step_avg:33.77ms
step:117/2160 train_time:3951ms step_avg:33.77ms
step:118/2160 train_time:3984ms step_avg:33.77ms
step:119/2160 train_time:4018ms step_avg:33.76ms
step:120/2160 train_time:4051ms step_avg:33.76ms
step:121/2160 train_time:4085ms step_avg:33.76ms
step:122/2160 train_time:4118ms step_avg:33.75ms
step:123/2160 train_time:4151ms step_avg:33.75ms
step:124/2160 train_time:4184ms step_avg:33.74ms
step:125/2160 train_time:4218ms step_avg:33.74ms
step:126/2160 train_time:4251ms step_avg:33.74ms
step:127/2160 train_time:4284ms step_avg:33.73ms
step:128/2160 train_time:4317ms step_avg:33.73ms
step:129/2160 train_time:4351ms step_avg:33.73ms
step:130/2160 train_time:4384ms step_avg:33.72ms
step:131/2160 train_time:4418ms step_avg:33.72ms
step:132/2160 train_time:4451ms step_avg:33.72ms
step:133/2160 train_time:4484ms step_avg:33.72ms
step:134/2160 train_time:4517ms step_avg:33.71ms
step:135/2160 train_time:4551ms step_avg:33.71ms
step:136/2160 train_time:4584ms step_avg:33.70ms
step:137/2160 train_time:4617ms step_avg:33.70ms
step:138/2160 train_time:4650ms step_avg:33.70ms
step:139/2160 train_time:4684ms step_avg:33.70ms
step:140/2160 train_time:4717ms step_avg:33.69ms
step:141/2160 train_time:4750ms step_avg:33.69ms
step:142/2160 train_time:4783ms step_avg:33.68ms
step:143/2160 train_time:4817ms step_avg:33.69ms
step:144/2160 train_time:4850ms step_avg:33.68ms
step:145/2160 train_time:4884ms step_avg:33.68ms
step:146/2160 train_time:4917ms step_avg:33.68ms
step:147/2160 train_time:4950ms step_avg:33.67ms
step:148/2160 train_time:4983ms step_avg:33.67ms
step:149/2160 train_time:5017ms step_avg:33.67ms
step:150/2160 train_time:5050ms step_avg:33.66ms
step:151/2160 train_time:5083ms step_avg:33.66ms
step:152/2160 train_time:5116ms step_avg:33.66ms
step:153/2160 train_time:5150ms step_avg:33.66ms
step:154/2160 train_time:5183ms step_avg:33.65ms
step:155/2160 train_time:5217ms step_avg:33.66ms
step:156/2160 train_time:5249ms step_avg:33.65ms
step:157/2160 train_time:5283ms step_avg:33.65ms
step:158/2160 train_time:5316ms step_avg:33.65ms
step:159/2160 train_time:5350ms step_avg:33.64ms
step:160/2160 train_time:5382ms step_avg:33.64ms
step:161/2160 train_time:5416ms step_avg:33.64ms
step:162/2160 train_time:5449ms step_avg:33.64ms
step:163/2160 train_time:5483ms step_avg:33.64ms
step:164/2160 train_time:5516ms step_avg:33.64ms
step:165/2160 train_time:5550ms step_avg:33.64ms
step:166/2160 train_time:5583ms step_avg:33.63ms
step:167/2160 train_time:5616ms step_avg:33.63ms
step:168/2160 train_time:5649ms step_avg:33.63ms
step:169/2160 train_time:5683ms step_avg:33.63ms
step:170/2160 train_time:5716ms step_avg:33.62ms
step:171/2160 train_time:5750ms step_avg:33.62ms
step:172/2160 train_time:5783ms step_avg:33.62ms
step:173/2160 train_time:5816ms step_avg:33.62ms
step:174/2160 train_time:5849ms step_avg:33.61ms
step:175/2160 train_time:5883ms step_avg:33.61ms
step:176/2160 train_time:5915ms step_avg:33.61ms
step:177/2160 train_time:5949ms step_avg:33.61ms
step:178/2160 train_time:5982ms step_avg:33.61ms
step:179/2160 train_time:6016ms step_avg:33.61ms
step:180/2160 train_time:6048ms step_avg:33.60ms
step:181/2160 train_time:6082ms step_avg:33.60ms
step:182/2160 train_time:6115ms step_avg:33.60ms
step:183/2160 train_time:6149ms step_avg:33.60ms
step:184/2160 train_time:6181ms step_avg:33.59ms
step:185/2160 train_time:6215ms step_avg:33.59ms
step:186/2160 train_time:6248ms step_avg:33.59ms
step:187/2160 train_time:6281ms step_avg:33.59ms
step:188/2160 train_time:6314ms step_avg:33.59ms
step:189/2160 train_time:6348ms step_avg:33.59ms
step:190/2160 train_time:6381ms step_avg:33.58ms
step:191/2160 train_time:6414ms step_avg:33.58ms
step:192/2160 train_time:6447ms step_avg:33.58ms
step:193/2160 train_time:6481ms step_avg:33.58ms
step:194/2160 train_time:6514ms step_avg:33.58ms
step:195/2160 train_time:6548ms step_avg:33.58ms
step:196/2160 train_time:6581ms step_avg:33.57ms
step:197/2160 train_time:6614ms step_avg:33.57ms
step:198/2160 train_time:6647ms step_avg:33.57ms
step:199/2160 train_time:6681ms step_avg:33.57ms
step:200/2160 train_time:6714ms step_avg:33.57ms
step:201/2160 train_time:6748ms step_avg:33.57ms
step:202/2160 train_time:6780ms step_avg:33.57ms
step:203/2160 train_time:6814ms step_avg:33.57ms
step:204/2160 train_time:6847ms step_avg:33.56ms
step:205/2160 train_time:6881ms step_avg:33.56ms
step:206/2160 train_time:6914ms step_avg:33.56ms
step:207/2160 train_time:6947ms step_avg:33.56ms
step:208/2160 train_time:6980ms step_avg:33.56ms
step:209/2160 train_time:7014ms step_avg:33.56ms
step:210/2160 train_time:7047ms step_avg:33.56ms
step:211/2160 train_time:7081ms step_avg:33.56ms
step:212/2160 train_time:7114ms step_avg:33.55ms
step:213/2160 train_time:7147ms step_avg:33.56ms
step:214/2160 train_time:7182ms step_avg:33.56ms
step:215/2160 train_time:7214ms step_avg:33.55ms
step:216/2160 train_time:7247ms step_avg:33.55ms
step:217/2160 train_time:7280ms step_avg:33.55ms
step:218/2160 train_time:7313ms step_avg:33.55ms
step:219/2160 train_time:7347ms step_avg:33.55ms
step:220/2160 train_time:7380ms step_avg:33.54ms
step:221/2160 train_time:7413ms step_avg:33.55ms
step:222/2160 train_time:7446ms step_avg:33.54ms
step:223/2160 train_time:7480ms step_avg:33.54ms
step:224/2160 train_time:7513ms step_avg:33.54ms
step:225/2160 train_time:7547ms step_avg:33.54ms
step:226/2160 train_time:7580ms step_avg:33.54ms
step:227/2160 train_time:7613ms step_avg:33.54ms
step:228/2160 train_time:7646ms step_avg:33.54ms
step:229/2160 train_time:7680ms step_avg:33.54ms
step:230/2160 train_time:7712ms step_avg:33.53ms
step:231/2160 train_time:7746ms step_avg:33.53ms
step:232/2160 train_time:7779ms step_avg:33.53ms
step:233/2160 train_time:7813ms step_avg:33.53ms
step:234/2160 train_time:7846ms step_avg:33.53ms
step:235/2160 train_time:7880ms step_avg:33.53ms
step:236/2160 train_time:7913ms step_avg:33.53ms
step:237/2160 train_time:7946ms step_avg:33.53ms
step:238/2160 train_time:7979ms step_avg:33.53ms
step:239/2160 train_time:8013ms step_avg:33.53ms
step:240/2160 train_time:8046ms step_avg:33.52ms
step:241/2160 train_time:8080ms step_avg:33.53ms
step:242/2160 train_time:8112ms step_avg:33.52ms
step:243/2160 train_time:8146ms step_avg:33.52ms
step:244/2160 train_time:8179ms step_avg:33.52ms
step:245/2160 train_time:8212ms step_avg:33.52ms
step:246/2160 train_time:8245ms step_avg:33.52ms
step:247/2160 train_time:8279ms step_avg:33.52ms
step:248/2160 train_time:8312ms step_avg:33.51ms
step:249/2160 train_time:8345ms step_avg:33.51ms
step:250/2160 train_time:8378ms step_avg:33.51ms
step:250/2160 val_loss:4.3501 train_time:8413ms step_avg:33.65ms
step:251/2160 train_time:8433ms step_avg:33.60ms
step:252/2160 train_time:8453ms step_avg:33.54ms
step:253/2160 train_time:8483ms step_avg:33.53ms
step:254/2160 train_time:8516ms step_avg:33.53ms
step:255/2160 train_time:8551ms step_avg:33.53ms
step:256/2160 train_time:8585ms step_avg:33.53ms
step:257/2160 train_time:8619ms step_avg:33.54ms
step:258/2160 train_time:8652ms step_avg:33.53ms
step:259/2160 train_time:8685ms step_avg:33.53ms
step:260/2160 train_time:8718ms step_avg:33.53ms
step:261/2160 train_time:8752ms step_avg:33.53ms
step:262/2160 train_time:8785ms step_avg:33.53ms
step:263/2160 train_time:8818ms step_avg:33.53ms
step:264/2160 train_time:8851ms step_avg:33.53ms
step:265/2160 train_time:8885ms step_avg:33.53ms
step:266/2160 train_time:8918ms step_avg:33.53ms
step:267/2160 train_time:8951ms step_avg:33.53ms
step:268/2160 train_time:8984ms step_avg:33.52ms
step:269/2160 train_time:9018ms step_avg:33.52ms
step:270/2160 train_time:9051ms step_avg:33.52ms
step:271/2160 train_time:9084ms step_avg:33.52ms
step:272/2160 train_time:9117ms step_avg:33.52ms
step:273/2160 train_time:9150ms step_avg:33.52ms
step:274/2160 train_time:9183ms step_avg:33.52ms
step:275/2160 train_time:9217ms step_avg:33.52ms
step:276/2160 train_time:9250ms step_avg:33.51ms
step:277/2160 train_time:9284ms step_avg:33.52ms
step:278/2160 train_time:9316ms step_avg:33.51ms
step:279/2160 train_time:9350ms step_avg:33.51ms
step:280/2160 train_time:9383ms step_avg:33.51ms
step:281/2160 train_time:9417ms step_avg:33.51ms
step:282/2160 train_time:9450ms step_avg:33.51ms
step:283/2160 train_time:9484ms step_avg:33.51ms
step:284/2160 train_time:9517ms step_avg:33.51ms
step:285/2160 train_time:9551ms step_avg:33.51ms
step:286/2160 train_time:9583ms step_avg:33.51ms
step:287/2160 train_time:9617ms step_avg:33.51ms
step:288/2160 train_time:9650ms step_avg:33.51ms
step:289/2160 train_time:9684ms step_avg:33.51ms
step:290/2160 train_time:9717ms step_avg:33.51ms
step:291/2160 train_time:9751ms step_avg:33.51ms
step:292/2160 train_time:9784ms step_avg:33.51ms
step:293/2160 train_time:9817ms step_avg:33.51ms
step:294/2160 train_time:9850ms step_avg:33.50ms
step:295/2160 train_time:9884ms step_avg:33.51ms
step:296/2160 train_time:9917ms step_avg:33.50ms
step:297/2160 train_time:9950ms step_avg:33.50ms
step:298/2160 train_time:9983ms step_avg:33.50ms
step:299/2160 train_time:10017ms step_avg:33.50ms
step:300/2160 train_time:10050ms step_avg:33.50ms
step:301/2160 train_time:10083ms step_avg:33.50ms
step:302/2160 train_time:10116ms step_avg:33.50ms
step:303/2160 train_time:10149ms step_avg:33.50ms
step:304/2160 train_time:10182ms step_avg:33.49ms
step:305/2160 train_time:10216ms step_avg:33.49ms
step:306/2160 train_time:10249ms step_avg:33.49ms
step:307/2160 train_time:10282ms step_avg:33.49ms
step:308/2160 train_time:10315ms step_avg:33.49ms
step:309/2160 train_time:10349ms step_avg:33.49ms
step:310/2160 train_time:10382ms step_avg:33.49ms
step:311/2160 train_time:10415ms step_avg:33.49ms
step:312/2160 train_time:10449ms step_avg:33.49ms
step:313/2160 train_time:10482ms step_avg:33.49ms
step:314/2160 train_time:10515ms step_avg:33.49ms
step:315/2160 train_time:10549ms step_avg:33.49ms
step:316/2160 train_time:10581ms step_avg:33.49ms
step:317/2160 train_time:10615ms step_avg:33.49ms
step:318/2160 train_time:10648ms step_avg:33.48ms
step:319/2160 train_time:10682ms step_avg:33.48ms
step:320/2160 train_time:10714ms step_avg:33.48ms
step:321/2160 train_time:10748ms step_avg:33.48ms
step:322/2160 train_time:10781ms step_avg:33.48ms
step:323/2160 train_time:10815ms step_avg:33.48ms
step:324/2160 train_time:10848ms step_avg:33.48ms
step:325/2160 train_time:10881ms step_avg:33.48ms
step:326/2160 train_time:10914ms step_avg:33.48ms
step:327/2160 train_time:10948ms step_avg:33.48ms
step:328/2160 train_time:10981ms step_avg:33.48ms
step:329/2160 train_time:11014ms step_avg:33.48ms
step:330/2160 train_time:11047ms step_avg:33.48ms
step:331/2160 train_time:11081ms step_avg:33.48ms
step:332/2160 train_time:11113ms step_avg:33.47ms
step:333/2160 train_time:11147ms step_avg:33.47ms
step:334/2160 train_time:11180ms step_avg:33.47ms
step:335/2160 train_time:11214ms step_avg:33.47ms
step:336/2160 train_time:11246ms step_avg:33.47ms
step:337/2160 train_time:11280ms step_avg:33.47ms
step:338/2160 train_time:11313ms step_avg:33.47ms
step:339/2160 train_time:11347ms step_avg:33.47ms
step:340/2160 train_time:11380ms step_avg:33.47ms
step:341/2160 train_time:11413ms step_avg:33.47ms
step:342/2160 train_time:11446ms step_avg:33.47ms
step:343/2160 train_time:11480ms step_avg:33.47ms
step:344/2160 train_time:11513ms step_avg:33.47ms
step:345/2160 train_time:11547ms step_avg:33.47ms
step:346/2160 train_time:11580ms step_avg:33.47ms
step:347/2160 train_time:11613ms step_avg:33.47ms
step:348/2160 train_time:11646ms step_avg:33.47ms
step:349/2160 train_time:11680ms step_avg:33.47ms
step:350/2160 train_time:11713ms step_avg:33.47ms
step:351/2160 train_time:11746ms step_avg:33.47ms
step:352/2160 train_time:11779ms step_avg:33.46ms
step:353/2160 train_time:11814ms step_avg:33.47ms
step:354/2160 train_time:11846ms step_avg:33.46ms
step:355/2160 train_time:11880ms step_avg:33.47ms
step:356/2160 train_time:11913ms step_avg:33.46ms
step:357/2160 train_time:11946ms step_avg:33.46ms
step:358/2160 train_time:11979ms step_avg:33.46ms
step:359/2160 train_time:12013ms step_avg:33.46ms
step:360/2160 train_time:12046ms step_avg:33.46ms
step:361/2160 train_time:12080ms step_avg:33.46ms
step:362/2160 train_time:12113ms step_avg:33.46ms
step:363/2160 train_time:12146ms step_avg:33.46ms
step:364/2160 train_time:12179ms step_avg:33.46ms
step:365/2160 train_time:12213ms step_avg:33.46ms
step:366/2160 train_time:12246ms step_avg:33.46ms
step:367/2160 train_time:12279ms step_avg:33.46ms
step:368/2160 train_time:12312ms step_avg:33.46ms
step:369/2160 train_time:12346ms step_avg:33.46ms
step:370/2160 train_time:12379ms step_avg:33.46ms
step:371/2160 train_time:12412ms step_avg:33.46ms
step:372/2160 train_time:12445ms step_avg:33.45ms
step:373/2160 train_time:12478ms step_avg:33.45ms
step:374/2160 train_time:12511ms step_avg:33.45ms
step:375/2160 train_time:12545ms step_avg:33.45ms
step:376/2160 train_time:12578ms step_avg:33.45ms
step:377/2160 train_time:12612ms step_avg:33.45ms
step:378/2160 train_time:12645ms step_avg:33.45ms
step:379/2160 train_time:12678ms step_avg:33.45ms
step:380/2160 train_time:12711ms step_avg:33.45ms
step:381/2160 train_time:12745ms step_avg:33.45ms
step:382/2160 train_time:12778ms step_avg:33.45ms
step:383/2160 train_time:12812ms step_avg:33.45ms
step:384/2160 train_time:12845ms step_avg:33.45ms
step:385/2160 train_time:12878ms step_avg:33.45ms
step:386/2160 train_time:12911ms step_avg:33.45ms
step:387/2160 train_time:12944ms step_avg:33.45ms
step:388/2160 train_time:12977ms step_avg:33.45ms
step:389/2160 train_time:13011ms step_avg:33.45ms
step:390/2160 train_time:13044ms step_avg:33.45ms
step:391/2160 train_time:13078ms step_avg:33.45ms
step:392/2160 train_time:13110ms step_avg:33.45ms
step:393/2160 train_time:13145ms step_avg:33.45ms
step:394/2160 train_time:13177ms step_avg:33.44ms
step:395/2160 train_time:13211ms step_avg:33.45ms
step:396/2160 train_time:13244ms step_avg:33.44ms
step:397/2160 train_time:13277ms step_avg:33.44ms
step:398/2160 train_time:13310ms step_avg:33.44ms
step:399/2160 train_time:13344ms step_avg:33.44ms
step:400/2160 train_time:13377ms step_avg:33.44ms
step:401/2160 train_time:13411ms step_avg:33.44ms
step:402/2160 train_time:13444ms step_avg:33.44ms
step:403/2160 train_time:13477ms step_avg:33.44ms
step:404/2160 train_time:13510ms step_avg:33.44ms
step:405/2160 train_time:13544ms step_avg:33.44ms
step:406/2160 train_time:13577ms step_avg:33.44ms
step:407/2160 train_time:13610ms step_avg:33.44ms
step:408/2160 train_time:13643ms step_avg:33.44ms
step:409/2160 train_time:13677ms step_avg:33.44ms
step:410/2160 train_time:13710ms step_avg:33.44ms
step:411/2160 train_time:13744ms step_avg:33.44ms
step:412/2160 train_time:13777ms step_avg:33.44ms
step:413/2160 train_time:13811ms step_avg:33.44ms
step:414/2160 train_time:13843ms step_avg:33.44ms
step:415/2160 train_time:13877ms step_avg:33.44ms
step:416/2160 train_time:13910ms step_avg:33.44ms
step:417/2160 train_time:13943ms step_avg:33.44ms
step:418/2160 train_time:13976ms step_avg:33.44ms
step:419/2160 train_time:14010ms step_avg:33.44ms
step:420/2160 train_time:14043ms step_avg:33.44ms
step:421/2160 train_time:14077ms step_avg:33.44ms
step:422/2160 train_time:14110ms step_avg:33.44ms
step:423/2160 train_time:14143ms step_avg:33.44ms
step:424/2160 train_time:14176ms step_avg:33.43ms
step:425/2160 train_time:14210ms step_avg:33.43ms
step:426/2160 train_time:14242ms step_avg:33.43ms
step:427/2160 train_time:14276ms step_avg:33.43ms
step:428/2160 train_time:14309ms step_avg:33.43ms
step:429/2160 train_time:14343ms step_avg:33.43ms
step:430/2160 train_time:14375ms step_avg:33.43ms
step:431/2160 train_time:14409ms step_avg:33.43ms
step:432/2160 train_time:14442ms step_avg:33.43ms
step:433/2160 train_time:14476ms step_avg:33.43ms
step:434/2160 train_time:14509ms step_avg:33.43ms
step:435/2160 train_time:14543ms step_avg:33.43ms
step:436/2160 train_time:14576ms step_avg:33.43ms
step:437/2160 train_time:14609ms step_avg:33.43ms
step:438/2160 train_time:14642ms step_avg:33.43ms
step:439/2160 train_time:14676ms step_avg:33.43ms
step:440/2160 train_time:14709ms step_avg:33.43ms
step:441/2160 train_time:14743ms step_avg:33.43ms
step:442/2160 train_time:14776ms step_avg:33.43ms
step:443/2160 train_time:14810ms step_avg:33.43ms
step:444/2160 train_time:14842ms step_avg:33.43ms
step:445/2160 train_time:14877ms step_avg:33.43ms
step:446/2160 train_time:14909ms step_avg:33.43ms
step:447/2160 train_time:14943ms step_avg:33.43ms
step:448/2160 train_time:14976ms step_avg:33.43ms
step:449/2160 train_time:15009ms step_avg:33.43ms
step:450/2160 train_time:15042ms step_avg:33.43ms
step:451/2160 train_time:15075ms step_avg:33.43ms
step:452/2160 train_time:15108ms step_avg:33.43ms
step:453/2160 train_time:15142ms step_avg:33.43ms
step:454/2160 train_time:15175ms step_avg:33.43ms
step:455/2160 train_time:15209ms step_avg:33.43ms
step:456/2160 train_time:15242ms step_avg:33.42ms
step:457/2160 train_time:15275ms step_avg:33.42ms
step:458/2160 train_time:15308ms step_avg:33.42ms
step:459/2160 train_time:15342ms step_avg:33.42ms
step:460/2160 train_time:15375ms step_avg:33.42ms
step:461/2160 train_time:15408ms step_avg:33.42ms
step:462/2160 train_time:15441ms step_avg:33.42ms
step:463/2160 train_time:15475ms step_avg:33.42ms
step:464/2160 train_time:15508ms step_avg:33.42ms
step:465/2160 train_time:15542ms step_avg:33.42ms
step:466/2160 train_time:15575ms step_avg:33.42ms
step:467/2160 train_time:15608ms step_avg:33.42ms
step:468/2160 train_time:15641ms step_avg:33.42ms
step:469/2160 train_time:15675ms step_avg:33.42ms
step:470/2160 train_time:15707ms step_avg:33.42ms
step:471/2160 train_time:15741ms step_avg:33.42ms
step:472/2160 train_time:15774ms step_avg:33.42ms
step:473/2160 train_time:15807ms step_avg:33.42ms
step:474/2160 train_time:15840ms step_avg:33.42ms
step:475/2160 train_time:15874ms step_avg:33.42ms
step:476/2160 train_time:15907ms step_avg:33.42ms
step:477/2160 train_time:15941ms step_avg:33.42ms
step:478/2160 train_time:15974ms step_avg:33.42ms
step:479/2160 train_time:16007ms step_avg:33.42ms
step:480/2160 train_time:16040ms step_avg:33.42ms
step:481/2160 train_time:16074ms step_avg:33.42ms
step:482/2160 train_time:16107ms step_avg:33.42ms
step:483/2160 train_time:16140ms step_avg:33.42ms
step:484/2160 train_time:16174ms step_avg:33.42ms
step:485/2160 train_time:16207ms step_avg:33.42ms
step:486/2160 train_time:16240ms step_avg:33.42ms
step:487/2160 train_time:16274ms step_avg:33.42ms
step:488/2160 train_time:16307ms step_avg:33.42ms
step:489/2160 train_time:16340ms step_avg:33.42ms
step:490/2160 train_time:16373ms step_avg:33.41ms
step:491/2160 train_time:16407ms step_avg:33.42ms
step:492/2160 train_time:16440ms step_avg:33.41ms
step:493/2160 train_time:16473ms step_avg:33.41ms
step:494/2160 train_time:16506ms step_avg:33.41ms
step:495/2160 train_time:16540ms step_avg:33.41ms
step:496/2160 train_time:16573ms step_avg:33.41ms
step:497/2160 train_time:16607ms step_avg:33.41ms
step:498/2160 train_time:16640ms step_avg:33.41ms
step:499/2160 train_time:16674ms step_avg:33.41ms
step:500/2160 train_time:16707ms step_avg:33.41ms
step:500/2160 val_loss:4.0084 train_time:16742ms step_avg:33.48ms
step:501/2160 train_time:16762ms step_avg:33.46ms
step:502/2160 train_time:16781ms step_avg:33.43ms
step:503/2160 train_time:16812ms step_avg:33.42ms
step:504/2160 train_time:16845ms step_avg:33.42ms
step:505/2160 train_time:16882ms step_avg:33.43ms
step:506/2160 train_time:16916ms step_avg:33.43ms
step:507/2160 train_time:16951ms step_avg:33.43ms
step:508/2160 train_time:16984ms step_avg:33.43ms
step:509/2160 train_time:17018ms step_avg:33.43ms
step:510/2160 train_time:17051ms step_avg:33.43ms
step:511/2160 train_time:17085ms step_avg:33.43ms
step:512/2160 train_time:17118ms step_avg:33.43ms
step:513/2160 train_time:17152ms step_avg:33.43ms
step:514/2160 train_time:17184ms step_avg:33.43ms
step:515/2160 train_time:17218ms step_avg:33.43ms
step:516/2160 train_time:17251ms step_avg:33.43ms
step:517/2160 train_time:17284ms step_avg:33.43ms
step:518/2160 train_time:17317ms step_avg:33.43ms
step:519/2160 train_time:17351ms step_avg:33.43ms
step:520/2160 train_time:17384ms step_avg:33.43ms
step:521/2160 train_time:17417ms step_avg:33.43ms
step:522/2160 train_time:17450ms step_avg:33.43ms
step:523/2160 train_time:17484ms step_avg:33.43ms
step:524/2160 train_time:17517ms step_avg:33.43ms
step:525/2160 train_time:17550ms step_avg:33.43ms
step:526/2160 train_time:17583ms step_avg:33.43ms
step:527/2160 train_time:17617ms step_avg:33.43ms
step:528/2160 train_time:17649ms step_avg:33.43ms
step:529/2160 train_time:17683ms step_avg:33.43ms
step:530/2160 train_time:17716ms step_avg:33.43ms
step:531/2160 train_time:17750ms step_avg:33.43ms
step:532/2160 train_time:17783ms step_avg:33.43ms
step:533/2160 train_time:17816ms step_avg:33.43ms
step:534/2160 train_time:17849ms step_avg:33.43ms
step:535/2160 train_time:17884ms step_avg:33.43ms
step:536/2160 train_time:17916ms step_avg:33.43ms
step:537/2160 train_time:17950ms step_avg:33.43ms
step:538/2160 train_time:17983ms step_avg:33.43ms
step:539/2160 train_time:18016ms step_avg:33.43ms
step:540/2160 train_time:18049ms step_avg:33.42ms
step:541/2160 train_time:18083ms step_avg:33.43ms
step:542/2160 train_time:18116ms step_avg:33.42ms
step:543/2160 train_time:18150ms step_avg:33.42ms
step:544/2160 train_time:18183ms step_avg:33.42ms
step:545/2160 train_time:18216ms step_avg:33.42ms
step:546/2160 train_time:18249ms step_avg:33.42ms
step:547/2160 train_time:18282ms step_avg:33.42ms
step:548/2160 train_time:18315ms step_avg:33.42ms
step:549/2160 train_time:18349ms step_avg:33.42ms
step:550/2160 train_time:18382ms step_avg:33.42ms
step:551/2160 train_time:18416ms step_avg:33.42ms
step:552/2160 train_time:18448ms step_avg:33.42ms
step:553/2160 train_time:18482ms step_avg:33.42ms
step:554/2160 train_time:18515ms step_avg:33.42ms
step:555/2160 train_time:18548ms step_avg:33.42ms
step:556/2160 train_time:18581ms step_avg:33.42ms
step:557/2160 train_time:18615ms step_avg:33.42ms
step:558/2160 train_time:18648ms step_avg:33.42ms
step:559/2160 train_time:18681ms step_avg:33.42ms
step:560/2160 train_time:18714ms step_avg:33.42ms
step:561/2160 train_time:18748ms step_avg:33.42ms
step:562/2160 train_time:18781ms step_avg:33.42ms
step:563/2160 train_time:18816ms step_avg:33.42ms
step:564/2160 train_time:18847ms step_avg:33.42ms
step:565/2160 train_time:18881ms step_avg:33.42ms
step:566/2160 train_time:18914ms step_avg:33.42ms
step:567/2160 train_time:18948ms step_avg:33.42ms
step:568/2160 train_time:18981ms step_avg:33.42ms
step:569/2160 train_time:19014ms step_avg:33.42ms
step:570/2160 train_time:19047ms step_avg:33.42ms
step:571/2160 train_time:19081ms step_avg:33.42ms
step:572/2160 train_time:19114ms step_avg:33.42ms
step:573/2160 train_time:19148ms step_avg:33.42ms
step:574/2160 train_time:19182ms step_avg:33.42ms
step:575/2160 train_time:19215ms step_avg:33.42ms
step:576/2160 train_time:19248ms step_avg:33.42ms
step:577/2160 train_time:19282ms step_avg:33.42ms
step:578/2160 train_time:19315ms step_avg:33.42ms
step:579/2160 train_time:19349ms step_avg:33.42ms
step:580/2160 train_time:19382ms step_avg:33.42ms
step:581/2160 train_time:19416ms step_avg:33.42ms
step:582/2160 train_time:19449ms step_avg:33.42ms
step:583/2160 train_time:19482ms step_avg:33.42ms
step:584/2160 train_time:19515ms step_avg:33.42ms
step:585/2160 train_time:19549ms step_avg:33.42ms
step:586/2160 train_time:19582ms step_avg:33.42ms
step:587/2160 train_time:19615ms step_avg:33.42ms
step:588/2160 train_time:19648ms step_avg:33.42ms
step:589/2160 train_time:19682ms step_avg:33.42ms
step:590/2160 train_time:19715ms step_avg:33.41ms
step:591/2160 train_time:19748ms step_avg:33.41ms
step:592/2160 train_time:19781ms step_avg:33.41ms
step:593/2160 train_time:19815ms step_avg:33.41ms
step:594/2160 train_time:19848ms step_avg:33.41ms
step:595/2160 train_time:19881ms step_avg:33.41ms
step:596/2160 train_time:19914ms step_avg:33.41ms
step:597/2160 train_time:19948ms step_avg:33.41ms
step:598/2160 train_time:19981ms step_avg:33.41ms
step:599/2160 train_time:20014ms step_avg:33.41ms
step:600/2160 train_time:20047ms step_avg:33.41ms
step:601/2160 train_time:20081ms step_avg:33.41ms
step:602/2160 train_time:20114ms step_avg:33.41ms
step:603/2160 train_time:20148ms step_avg:33.41ms
step:604/2160 train_time:20180ms step_avg:33.41ms
step:605/2160 train_time:20214ms step_avg:33.41ms
step:606/2160 train_time:20247ms step_avg:33.41ms
step:607/2160 train_time:20281ms step_avg:33.41ms
step:608/2160 train_time:20314ms step_avg:33.41ms
step:609/2160 train_time:20347ms step_avg:33.41ms
step:610/2160 train_time:20381ms step_avg:33.41ms
step:611/2160 train_time:20414ms step_avg:33.41ms
step:612/2160 train_time:20447ms step_avg:33.41ms
step:613/2160 train_time:20481ms step_avg:33.41ms
step:614/2160 train_time:20513ms step_avg:33.41ms
step:615/2160 train_time:20548ms step_avg:33.41ms
step:616/2160 train_time:20580ms step_avg:33.41ms
step:617/2160 train_time:20614ms step_avg:33.41ms
step:618/2160 train_time:20647ms step_avg:33.41ms
step:619/2160 train_time:20681ms step_avg:33.41ms
step:620/2160 train_time:20714ms step_avg:33.41ms
step:621/2160 train_time:20748ms step_avg:33.41ms
step:622/2160 train_time:20780ms step_avg:33.41ms
step:623/2160 train_time:20815ms step_avg:33.41ms
step:624/2160 train_time:20847ms step_avg:33.41ms
step:625/2160 train_time:20881ms step_avg:33.41ms
step:626/2160 train_time:20914ms step_avg:33.41ms
step:627/2160 train_time:20948ms step_avg:33.41ms
step:628/2160 train_time:20981ms step_avg:33.41ms
step:629/2160 train_time:21014ms step_avg:33.41ms
step:630/2160 train_time:21047ms step_avg:33.41ms
step:631/2160 train_time:21081ms step_avg:33.41ms
step:632/2160 train_time:21114ms step_avg:33.41ms
step:633/2160 train_time:21148ms step_avg:33.41ms
step:634/2160 train_time:21181ms step_avg:33.41ms
step:635/2160 train_time:21215ms step_avg:33.41ms
step:636/2160 train_time:21248ms step_avg:33.41ms
step:637/2160 train_time:21281ms step_avg:33.41ms
step:638/2160 train_time:21314ms step_avg:33.41ms
step:639/2160 train_time:21348ms step_avg:33.41ms
step:640/2160 train_time:21381ms step_avg:33.41ms
step:641/2160 train_time:21415ms step_avg:33.41ms
step:642/2160 train_time:21448ms step_avg:33.41ms
step:643/2160 train_time:21482ms step_avg:33.41ms
step:644/2160 train_time:21514ms step_avg:33.41ms
step:645/2160 train_time:21548ms step_avg:33.41ms
step:646/2160 train_time:21581ms step_avg:33.41ms
step:647/2160 train_time:21615ms step_avg:33.41ms
step:648/2160 train_time:21648ms step_avg:33.41ms
step:649/2160 train_time:21681ms step_avg:33.41ms
step:650/2160 train_time:21714ms step_avg:33.41ms
step:651/2160 train_time:21748ms step_avg:33.41ms
step:652/2160 train_time:21781ms step_avg:33.41ms
step:653/2160 train_time:21815ms step_avg:33.41ms
step:654/2160 train_time:21848ms step_avg:33.41ms
step:655/2160 train_time:21881ms step_avg:33.41ms
step:656/2160 train_time:21914ms step_avg:33.41ms
step:657/2160 train_time:21948ms step_avg:33.41ms
step:658/2160 train_time:21981ms step_avg:33.41ms
step:659/2160 train_time:22014ms step_avg:33.41ms
step:660/2160 train_time:22047ms step_avg:33.41ms
step:661/2160 train_time:22081ms step_avg:33.41ms
step:662/2160 train_time:22114ms step_avg:33.40ms
step:663/2160 train_time:22148ms step_avg:33.41ms
step:664/2160 train_time:22181ms step_avg:33.40ms
step:665/2160 train_time:22214ms step_avg:33.40ms
step:666/2160 train_time:22247ms step_avg:33.40ms
step:667/2160 train_time:22281ms step_avg:33.40ms
step:668/2160 train_time:22313ms step_avg:33.40ms
step:669/2160 train_time:22348ms step_avg:33.40ms
step:670/2160 train_time:22380ms step_avg:33.40ms
step:671/2160 train_time:22414ms step_avg:33.40ms
step:672/2160 train_time:22447ms step_avg:33.40ms
step:673/2160 train_time:22480ms step_avg:33.40ms
step:674/2160 train_time:22513ms step_avg:33.40ms
step:675/2160 train_time:22547ms step_avg:33.40ms
step:676/2160 train_time:22580ms step_avg:33.40ms
step:677/2160 train_time:22613ms step_avg:33.40ms
step:678/2160 train_time:22646ms step_avg:33.40ms
step:679/2160 train_time:22680ms step_avg:33.40ms
step:680/2160 train_time:22713ms step_avg:33.40ms
step:681/2160 train_time:22747ms step_avg:33.40ms
step:682/2160 train_time:22780ms step_avg:33.40ms
step:683/2160 train_time:22814ms step_avg:33.40ms
step:684/2160 train_time:22847ms step_avg:33.40ms
step:685/2160 train_time:22880ms step_avg:33.40ms
step:686/2160 train_time:22913ms step_avg:33.40ms
step:687/2160 train_time:22947ms step_avg:33.40ms
step:688/2160 train_time:22980ms step_avg:33.40ms
step:689/2160 train_time:23014ms step_avg:33.40ms
step:690/2160 train_time:23047ms step_avg:33.40ms
step:691/2160 train_time:23080ms step_avg:33.40ms
step:692/2160 train_time:23113ms step_avg:33.40ms
step:693/2160 train_time:23147ms step_avg:33.40ms
step:694/2160 train_time:23180ms step_avg:33.40ms
step:695/2160 train_time:23214ms step_avg:33.40ms
step:696/2160 train_time:23247ms step_avg:33.40ms
step:697/2160 train_time:23281ms step_avg:33.40ms
step:698/2160 train_time:23314ms step_avg:33.40ms
step:699/2160 train_time:23347ms step_avg:33.40ms
step:700/2160 train_time:23380ms step_avg:33.40ms
step:701/2160 train_time:23414ms step_avg:33.40ms
step:702/2160 train_time:23447ms step_avg:33.40ms
step:703/2160 train_time:23481ms step_avg:33.40ms
step:704/2160 train_time:23514ms step_avg:33.40ms
step:705/2160 train_time:23547ms step_avg:33.40ms
step:706/2160 train_time:23580ms step_avg:33.40ms
step:707/2160 train_time:23690ms step_avg:33.51ms
step:708/2160 train_time:23714ms step_avg:33.49ms
step:709/2160 train_time:23764ms step_avg:33.52ms
step:710/2160 train_time:23821ms step_avg:33.55ms
step:711/2160 train_time:23881ms step_avg:33.59ms
step:712/2160 train_time:23939ms step_avg:33.62ms
step:713/2160 train_time:23999ms step_avg:33.66ms
step:714/2160 train_time:24056ms step_avg:33.69ms
step:715/2160 train_time:24116ms step_avg:33.73ms
step:716/2160 train_time:24174ms step_avg:33.76ms
step:717/2160 train_time:24233ms step_avg:33.80ms
step:718/2160 train_time:24291ms step_avg:33.83ms
step:719/2160 train_time:24351ms step_avg:33.87ms
step:720/2160 train_time:24409ms step_avg:33.90ms
step:721/2160 train_time:24468ms step_avg:33.94ms
step:722/2160 train_time:24527ms step_avg:33.97ms
step:723/2160 train_time:24590ms step_avg:34.01ms
step:724/2160 train_time:24653ms step_avg:34.05ms
step:725/2160 train_time:24715ms step_avg:34.09ms
step:726/2160 train_time:24774ms step_avg:34.12ms
step:727/2160 train_time:24835ms step_avg:34.16ms
step:728/2160 train_time:24894ms step_avg:34.20ms
step:729/2160 train_time:24954ms step_avg:34.23ms
step:730/2160 train_time:25013ms step_avg:34.26ms
step:731/2160 train_time:25073ms step_avg:34.30ms
step:732/2160 train_time:25131ms step_avg:34.33ms
step:733/2160 train_time:25191ms step_avg:34.37ms
step:734/2160 train_time:25249ms step_avg:34.40ms
step:735/2160 train_time:25309ms step_avg:34.43ms
step:736/2160 train_time:25367ms step_avg:34.47ms
step:737/2160 train_time:25427ms step_avg:34.50ms
step:738/2160 train_time:25485ms step_avg:34.53ms
step:739/2160 train_time:25546ms step_avg:34.57ms
step:740/2160 train_time:25606ms step_avg:34.60ms
step:741/2160 train_time:25668ms step_avg:34.64ms
step:742/2160 train_time:25727ms step_avg:34.67ms
step:743/2160 train_time:25789ms step_avg:34.71ms
step:744/2160 train_time:25849ms step_avg:34.74ms
step:745/2160 train_time:25910ms step_avg:34.78ms
step:746/2160 train_time:25969ms step_avg:34.81ms
step:747/2160 train_time:26030ms step_avg:34.85ms
step:748/2160 train_time:26088ms step_avg:34.88ms
step:749/2160 train_time:26148ms step_avg:34.91ms
step:750/2160 train_time:26206ms step_avg:34.94ms
step:750/2160 val_loss:3.8504 train_time:26267ms step_avg:35.02ms
step:751/2160 train_time:26290ms step_avg:35.01ms
step:752/2160 train_time:26329ms step_avg:35.01ms
step:753/2160 train_time:26392ms step_avg:35.05ms
step:754/2160 train_time:26453ms step_avg:35.08ms
step:755/2160 train_time:26514ms step_avg:35.12ms
step:756/2160 train_time:26573ms step_avg:35.15ms
step:757/2160 train_time:26633ms step_avg:35.18ms
step:758/2160 train_time:26691ms step_avg:35.21ms
step:759/2160 train_time:26752ms step_avg:35.25ms
step:760/2160 train_time:26810ms step_avg:35.28ms
step:761/2160 train_time:26870ms step_avg:35.31ms
step:762/2160 train_time:26929ms step_avg:35.34ms
step:763/2160 train_time:26988ms step_avg:35.37ms
step:764/2160 train_time:27047ms step_avg:35.40ms
step:765/2160 train_time:27107ms step_avg:35.43ms
step:766/2160 train_time:27165ms step_avg:35.46ms
step:767/2160 train_time:27228ms step_avg:35.50ms
step:768/2160 train_time:27287ms step_avg:35.53ms
step:769/2160 train_time:27350ms step_avg:35.57ms
step:770/2160 train_time:27409ms step_avg:35.60ms
step:771/2160 train_time:27471ms step_avg:35.63ms
step:772/2160 train_time:27530ms step_avg:35.66ms
step:773/2160 train_time:27591ms step_avg:35.69ms
step:774/2160 train_time:27649ms step_avg:35.72ms
step:775/2160 train_time:27710ms step_avg:35.75ms
step:776/2160 train_time:27769ms step_avg:35.78ms
step:777/2160 train_time:27829ms step_avg:35.82ms
step:778/2160 train_time:27887ms step_avg:35.85ms
step:779/2160 train_time:27948ms step_avg:35.88ms
step:780/2160 train_time:28006ms step_avg:35.91ms
step:781/2160 train_time:28067ms step_avg:35.94ms
step:782/2160 train_time:28125ms step_avg:35.97ms
step:783/2160 train_time:28186ms step_avg:36.00ms
step:784/2160 train_time:28245ms step_avg:36.03ms
step:785/2160 train_time:28306ms step_avg:36.06ms
step:786/2160 train_time:28366ms step_avg:36.09ms
step:787/2160 train_time:28427ms step_avg:36.12ms
step:788/2160 train_time:28487ms step_avg:36.15ms
step:789/2160 train_time:28548ms step_avg:36.18ms
step:790/2160 train_time:28607ms step_avg:36.21ms
step:791/2160 train_time:28668ms step_avg:36.24ms
step:792/2160 train_time:28727ms step_avg:36.27ms
step:793/2160 train_time:28787ms step_avg:36.30ms
step:794/2160 train_time:28845ms step_avg:36.33ms
step:795/2160 train_time:28905ms step_avg:36.36ms
step:796/2160 train_time:28964ms step_avg:36.39ms
step:797/2160 train_time:29024ms step_avg:36.42ms
step:798/2160 train_time:29082ms step_avg:36.44ms
step:799/2160 train_time:29142ms step_avg:36.47ms
step:800/2160 train_time:29201ms step_avg:36.50ms
step:801/2160 train_time:29262ms step_avg:36.53ms
step:802/2160 train_time:29320ms step_avg:36.56ms
step:803/2160 train_time:29382ms step_avg:36.59ms
step:804/2160 train_time:29441ms step_avg:36.62ms
step:805/2160 train_time:29503ms step_avg:36.65ms
step:806/2160 train_time:29562ms step_avg:36.68ms
step:807/2160 train_time:29622ms step_avg:36.71ms
step:808/2160 train_time:29681ms step_avg:36.73ms
step:809/2160 train_time:29742ms step_avg:36.76ms
step:810/2160 train_time:29801ms step_avg:36.79ms
step:811/2160 train_time:29861ms step_avg:36.82ms
step:812/2160 train_time:29919ms step_avg:36.85ms
step:813/2160 train_time:29980ms step_avg:36.88ms
step:814/2160 train_time:30038ms step_avg:36.90ms
step:815/2160 train_time:30099ms step_avg:36.93ms
step:816/2160 train_time:30157ms step_avg:36.96ms
step:817/2160 train_time:30218ms step_avg:36.99ms
step:818/2160 train_time:30276ms step_avg:37.01ms
step:819/2160 train_time:30337ms step_avg:37.04ms
step:820/2160 train_time:30396ms step_avg:37.07ms
step:821/2160 train_time:30456ms step_avg:37.10ms
step:822/2160 train_time:30515ms step_avg:37.12ms
step:823/2160 train_time:30577ms step_avg:37.15ms
step:824/2160 train_time:30635ms step_avg:37.18ms
step:825/2160 train_time:30696ms step_avg:37.21ms
step:826/2160 train_time:30755ms step_avg:37.23ms
step:827/2160 train_time:30815ms step_avg:37.26ms
step:828/2160 train_time:30874ms step_avg:37.29ms
step:829/2160 train_time:30934ms step_avg:37.31ms
step:830/2160 train_time:30993ms step_avg:37.34ms
step:831/2160 train_time:31053ms step_avg:37.37ms
step:832/2160 train_time:31112ms step_avg:37.39ms
step:833/2160 train_time:31172ms step_avg:37.42ms
step:834/2160 train_time:31231ms step_avg:37.45ms
step:835/2160 train_time:31291ms step_avg:37.47ms
step:836/2160 train_time:31351ms step_avg:37.50ms
step:837/2160 train_time:31412ms step_avg:37.53ms
step:838/2160 train_time:31471ms step_avg:37.55ms
step:839/2160 train_time:31532ms step_avg:37.58ms
step:840/2160 train_time:31591ms step_avg:37.61ms
step:841/2160 train_time:31651ms step_avg:37.64ms
step:842/2160 train_time:31710ms step_avg:37.66ms
step:843/2160 train_time:31771ms step_avg:37.69ms
step:844/2160 train_time:31830ms step_avg:37.71ms
step:845/2160 train_time:31891ms step_avg:37.74ms
step:846/2160 train_time:31949ms step_avg:37.77ms
step:847/2160 train_time:32010ms step_avg:37.79ms
step:848/2160 train_time:32069ms step_avg:37.82ms
step:849/2160 train_time:32130ms step_avg:37.84ms
step:850/2160 train_time:32188ms step_avg:37.87ms
step:851/2160 train_time:32249ms step_avg:37.90ms
step:852/2160 train_time:32308ms step_avg:37.92ms
step:853/2160 train_time:32369ms step_avg:37.95ms
step:854/2160 train_time:32427ms step_avg:37.97ms
step:855/2160 train_time:32488ms step_avg:38.00ms
step:856/2160 train_time:32548ms step_avg:38.02ms
step:857/2160 train_time:32609ms step_avg:38.05ms
step:858/2160 train_time:32668ms step_avg:38.07ms
step:859/2160 train_time:32729ms step_avg:38.10ms
step:860/2160 train_time:32788ms step_avg:38.13ms
step:861/2160 train_time:32849ms step_avg:38.15ms
step:862/2160 train_time:32907ms step_avg:38.18ms
step:863/2160 train_time:32968ms step_avg:38.20ms
step:864/2160 train_time:33027ms step_avg:38.23ms
step:865/2160 train_time:33087ms step_avg:38.25ms
step:866/2160 train_time:33146ms step_avg:38.27ms
step:867/2160 train_time:33206ms step_avg:38.30ms
step:868/2160 train_time:33265ms step_avg:38.32ms
step:869/2160 train_time:33325ms step_avg:38.35ms
step:870/2160 train_time:33384ms step_avg:38.37ms
step:871/2160 train_time:33444ms step_avg:38.40ms
step:872/2160 train_time:33503ms step_avg:38.42ms
step:873/2160 train_time:33565ms step_avg:38.45ms
step:874/2160 train_time:33624ms step_avg:38.47ms
step:875/2160 train_time:33684ms step_avg:38.50ms
step:876/2160 train_time:33742ms step_avg:38.52ms
step:877/2160 train_time:33803ms step_avg:38.54ms
step:878/2160 train_time:33862ms step_avg:38.57ms
step:879/2160 train_time:33923ms step_avg:38.59ms
step:880/2160 train_time:33981ms step_avg:38.61ms
step:881/2160 train_time:34042ms step_avg:38.64ms
step:882/2160 train_time:34101ms step_avg:38.66ms
step:883/2160 train_time:34161ms step_avg:38.69ms
step:884/2160 train_time:34220ms step_avg:38.71ms
step:885/2160 train_time:34281ms step_avg:38.74ms
step:886/2160 train_time:34339ms step_avg:38.76ms
step:887/2160 train_time:34400ms step_avg:38.78ms
step:888/2160 train_time:34459ms step_avg:38.80ms
step:889/2160 train_time:34519ms step_avg:38.83ms
step:890/2160 train_time:34578ms step_avg:38.85ms
step:891/2160 train_time:34639ms step_avg:38.88ms
step:892/2160 train_time:34698ms step_avg:38.90ms
step:893/2160 train_time:34759ms step_avg:38.92ms
step:894/2160 train_time:34818ms step_avg:38.95ms
step:895/2160 train_time:34879ms step_avg:38.97ms
step:896/2160 train_time:34937ms step_avg:38.99ms
step:897/2160 train_time:34998ms step_avg:39.02ms
step:898/2160 train_time:35057ms step_avg:39.04ms
step:899/2160 train_time:35117ms step_avg:39.06ms
step:900/2160 train_time:35177ms step_avg:39.09ms
step:901/2160 train_time:35237ms step_avg:39.11ms
step:902/2160 train_time:35296ms step_avg:39.13ms
step:903/2160 train_time:35357ms step_avg:39.15ms
step:904/2160 train_time:35416ms step_avg:39.18ms
step:905/2160 train_time:35476ms step_avg:39.20ms
step:906/2160 train_time:35535ms step_avg:39.22ms
step:907/2160 train_time:35595ms step_avg:39.25ms
step:908/2160 train_time:35654ms step_avg:39.27ms
step:909/2160 train_time:35715ms step_avg:39.29ms
step:910/2160 train_time:35774ms step_avg:39.31ms
step:911/2160 train_time:35835ms step_avg:39.34ms
step:912/2160 train_time:35894ms step_avg:39.36ms
step:913/2160 train_time:35954ms step_avg:39.38ms
step:914/2160 train_time:36013ms step_avg:39.40ms
step:915/2160 train_time:36073ms step_avg:39.42ms
step:916/2160 train_time:36133ms step_avg:39.45ms
step:917/2160 train_time:36194ms step_avg:39.47ms
step:918/2160 train_time:36253ms step_avg:39.49ms
step:919/2160 train_time:36313ms step_avg:39.51ms
step:920/2160 train_time:36373ms step_avg:39.54ms
step:921/2160 train_time:36433ms step_avg:39.56ms
step:922/2160 train_time:36492ms step_avg:39.58ms
step:923/2160 train_time:36553ms step_avg:39.60ms
step:924/2160 train_time:36611ms step_avg:39.62ms
step:925/2160 train_time:36671ms step_avg:39.64ms
step:926/2160 train_time:36731ms step_avg:39.67ms
step:927/2160 train_time:36791ms step_avg:39.69ms
step:928/2160 train_time:36850ms step_avg:39.71ms
step:929/2160 train_time:36910ms step_avg:39.73ms
step:930/2160 train_time:36969ms step_avg:39.75ms
step:931/2160 train_time:37030ms step_avg:39.77ms
step:932/2160 train_time:37088ms step_avg:39.79ms
step:933/2160 train_time:37149ms step_avg:39.82ms
step:934/2160 train_time:37208ms step_avg:39.84ms
step:935/2160 train_time:37269ms step_avg:39.86ms
step:936/2160 train_time:37328ms step_avg:39.88ms
step:937/2160 train_time:37389ms step_avg:39.90ms
step:938/2160 train_time:37448ms step_avg:39.92ms
step:939/2160 train_time:37508ms step_avg:39.94ms
step:940/2160 train_time:37567ms step_avg:39.96ms
step:941/2160 train_time:37628ms step_avg:39.99ms
step:942/2160 train_time:37686ms step_avg:40.01ms
step:943/2160 train_time:37748ms step_avg:40.03ms
step:944/2160 train_time:37806ms step_avg:40.05ms
step:945/2160 train_time:37867ms step_avg:40.07ms
step:946/2160 train_time:37925ms step_avg:40.09ms
step:947/2160 train_time:37986ms step_avg:40.11ms
step:948/2160 train_time:38045ms step_avg:40.13ms
step:949/2160 train_time:38106ms step_avg:40.15ms
step:950/2160 train_time:38164ms step_avg:40.17ms
step:951/2160 train_time:38225ms step_avg:40.19ms
step:952/2160 train_time:38284ms step_avg:40.21ms
step:953/2160 train_time:38344ms step_avg:40.24ms
step:954/2160 train_time:38404ms step_avg:40.26ms
step:955/2160 train_time:38465ms step_avg:40.28ms
step:956/2160 train_time:38524ms step_avg:40.30ms
step:957/2160 train_time:38585ms step_avg:40.32ms
step:958/2160 train_time:38643ms step_avg:40.34ms
step:959/2160 train_time:38704ms step_avg:40.36ms
step:960/2160 train_time:38763ms step_avg:40.38ms
step:961/2160 train_time:38823ms step_avg:40.40ms
step:962/2160 train_time:38882ms step_avg:40.42ms
step:963/2160 train_time:38942ms step_avg:40.44ms
step:964/2160 train_time:39001ms step_avg:40.46ms
step:965/2160 train_time:39062ms step_avg:40.48ms
step:966/2160 train_time:39120ms step_avg:40.50ms
step:967/2160 train_time:39181ms step_avg:40.52ms
step:968/2160 train_time:39240ms step_avg:40.54ms
step:969/2160 train_time:39301ms step_avg:40.56ms
step:970/2160 train_time:39360ms step_avg:40.58ms
step:971/2160 train_time:39420ms step_avg:40.60ms
step:972/2160 train_time:39479ms step_avg:40.62ms
step:973/2160 train_time:39541ms step_avg:40.64ms
step:974/2160 train_time:39600ms step_avg:40.66ms
step:975/2160 train_time:39660ms step_avg:40.68ms
step:976/2160 train_time:39719ms step_avg:40.70ms
step:977/2160 train_time:39779ms step_avg:40.72ms
step:978/2160 train_time:39838ms step_avg:40.73ms
step:979/2160 train_time:39898ms step_avg:40.75ms
step:980/2160 train_time:39957ms step_avg:40.77ms
step:981/2160 train_time:40018ms step_avg:40.79ms
step:982/2160 train_time:40076ms step_avg:40.81ms
step:983/2160 train_time:40137ms step_avg:40.83ms
step:984/2160 train_time:40195ms step_avg:40.85ms
step:985/2160 train_time:40256ms step_avg:40.87ms
step:986/2160 train_time:40315ms step_avg:40.89ms
step:987/2160 train_time:40376ms step_avg:40.91ms
step:988/2160 train_time:40435ms step_avg:40.93ms
step:989/2160 train_time:40495ms step_avg:40.95ms
step:990/2160 train_time:40554ms step_avg:40.96ms
step:991/2160 train_time:40615ms step_avg:40.98ms
step:992/2160 train_time:40674ms step_avg:41.00ms
step:993/2160 train_time:40735ms step_avg:41.02ms
step:994/2160 train_time:40794ms step_avg:41.04ms
step:995/2160 train_time:40854ms step_avg:41.06ms
step:996/2160 train_time:40913ms step_avg:41.08ms
step:997/2160 train_time:40973ms step_avg:41.10ms
step:998/2160 train_time:41032ms step_avg:41.11ms
step:999/2160 train_time:41092ms step_avg:41.13ms
step:1000/2160 train_time:41151ms step_avg:41.15ms
step:1000/2160 val_loss:3.6969 train_time:41213ms step_avg:41.21ms
step:1001/2160 train_time:41236ms step_avg:41.20ms
step:1002/2160 train_time:41274ms step_avg:41.19ms
step:1003/2160 train_time:41337ms step_avg:41.21ms
step:1004/2160 train_time:41396ms step_avg:41.23ms
step:1005/2160 train_time:41457ms step_avg:41.25ms
step:1006/2160 train_time:41516ms step_avg:41.27ms
step:1007/2160 train_time:41576ms step_avg:41.29ms
step:1008/2160 train_time:41634ms step_avg:41.30ms
step:1009/2160 train_time:41695ms step_avg:41.32ms
step:1010/2160 train_time:41753ms step_avg:41.34ms
step:1011/2160 train_time:41813ms step_avg:41.36ms
step:1012/2160 train_time:41871ms step_avg:41.37ms
step:1013/2160 train_time:41931ms step_avg:41.39ms
step:1014/2160 train_time:41990ms step_avg:41.41ms
step:1015/2160 train_time:42050ms step_avg:41.43ms
step:1016/2160 train_time:42108ms step_avg:41.44ms
step:1017/2160 train_time:42170ms step_avg:41.46ms
step:1018/2160 train_time:42230ms step_avg:41.48ms
step:1019/2160 train_time:42291ms step_avg:41.50ms
step:1020/2160 train_time:42351ms step_avg:41.52ms
step:1021/2160 train_time:42413ms step_avg:41.54ms
step:1022/2160 train_time:42473ms step_avg:41.56ms
step:1023/2160 train_time:42533ms step_avg:41.58ms
step:1024/2160 train_time:42592ms step_avg:41.59ms
step:1025/2160 train_time:42651ms step_avg:41.61ms
step:1026/2160 train_time:42710ms step_avg:41.63ms
step:1027/2160 train_time:42770ms step_avg:41.65ms
step:1028/2160 train_time:42828ms step_avg:41.66ms
step:1029/2160 train_time:42888ms step_avg:41.68ms
step:1030/2160 train_time:42946ms step_avg:41.70ms
step:1031/2160 train_time:43006ms step_avg:41.71ms
step:1032/2160 train_time:43064ms step_avg:41.73ms
step:1033/2160 train_time:43125ms step_avg:41.75ms
step:1034/2160 train_time:43184ms step_avg:41.76ms
step:1035/2160 train_time:43245ms step_avg:41.78ms
step:1036/2160 train_time:43304ms step_avg:41.80ms
step:1037/2160 train_time:43366ms step_avg:41.82ms
step:1038/2160 train_time:43425ms step_avg:41.84ms
step:1039/2160 train_time:43486ms step_avg:41.85ms
step:1040/2160 train_time:43545ms step_avg:41.87ms
step:1041/2160 train_time:43606ms step_avg:41.89ms
step:1042/2160 train_time:43665ms step_avg:41.91ms
step:1043/2160 train_time:43726ms step_avg:41.92ms
step:1044/2160 train_time:43784ms step_avg:41.94ms
step:1045/2160 train_time:43845ms step_avg:41.96ms
step:1046/2160 train_time:43904ms step_avg:41.97ms
step:1047/2160 train_time:43964ms step_avg:41.99ms
step:1048/2160 train_time:44022ms step_avg:42.01ms
step:1049/2160 train_time:44082ms step_avg:42.02ms
step:1050/2160 train_time:44141ms step_avg:42.04ms
step:1051/2160 train_time:44202ms step_avg:42.06ms
step:1052/2160 train_time:44261ms step_avg:42.07ms
step:1053/2160 train_time:44322ms step_avg:42.09ms
step:1054/2160 train_time:44382ms step_avg:42.11ms
step:1055/2160 train_time:44444ms step_avg:42.13ms
step:1056/2160 train_time:44502ms step_avg:42.14ms
step:1057/2160 train_time:44564ms step_avg:42.16ms
step:1058/2160 train_time:44623ms step_avg:42.18ms
step:1059/2160 train_time:44684ms step_avg:42.19ms
step:1060/2160 train_time:44742ms step_avg:42.21ms
step:1061/2160 train_time:44803ms step_avg:42.23ms
step:1062/2160 train_time:44862ms step_avg:42.24ms
step:1063/2160 train_time:44922ms step_avg:42.26ms
step:1064/2160 train_time:44981ms step_avg:42.27ms
step:1065/2160 train_time:45041ms step_avg:42.29ms
step:1066/2160 train_time:45100ms step_avg:42.31ms
step:1067/2160 train_time:45160ms step_avg:42.32ms
step:1068/2160 train_time:45219ms step_avg:42.34ms
step:1069/2160 train_time:45279ms step_avg:42.36ms
step:1070/2160 train_time:45338ms step_avg:42.37ms
step:1071/2160 train_time:45399ms step_avg:42.39ms
step:1072/2160 train_time:45458ms step_avg:42.41ms
step:1073/2160 train_time:45519ms step_avg:42.42ms
step:1074/2160 train_time:45579ms step_avg:42.44ms
step:1075/2160 train_time:45639ms step_avg:42.46ms
step:1076/2160 train_time:45698ms step_avg:42.47ms
step:1077/2160 train_time:45759ms step_avg:42.49ms
step:1078/2160 train_time:45817ms step_avg:42.50ms
step:1079/2160 train_time:45877ms step_avg:42.52ms
step:1080/2160 train_time:45935ms step_avg:42.53ms
step:1081/2160 train_time:45996ms step_avg:42.55ms
step:1082/2160 train_time:46054ms step_avg:42.56ms
step:1083/2160 train_time:46115ms step_avg:42.58ms
step:1084/2160 train_time:46175ms step_avg:42.60ms
step:1085/2160 train_time:46235ms step_avg:42.61ms
step:1086/2160 train_time:46293ms step_avg:42.63ms
step:1087/2160 train_time:46354ms step_avg:42.64ms
step:1088/2160 train_time:46414ms step_avg:42.66ms
step:1089/2160 train_time:46475ms step_avg:42.68ms
step:1090/2160 train_time:46534ms step_avg:42.69ms
step:1091/2160 train_time:46595ms step_avg:42.71ms
step:1092/2160 train_time:46654ms step_avg:42.72ms
step:1093/2160 train_time:46715ms step_avg:42.74ms
step:1094/2160 train_time:46773ms step_avg:42.75ms
step:1095/2160 train_time:46834ms step_avg:42.77ms
step:1096/2160 train_time:46893ms step_avg:42.79ms
step:1097/2160 train_time:46953ms step_avg:42.80ms
step:1098/2160 train_time:47011ms step_avg:42.82ms
step:1099/2160 train_time:47071ms step_avg:42.83ms
step:1100/2160 train_time:47130ms step_avg:42.85ms
step:1101/2160 train_time:47190ms step_avg:42.86ms
step:1102/2160 train_time:47249ms step_avg:42.88ms
step:1103/2160 train_time:47310ms step_avg:42.89ms
step:1104/2160 train_time:47368ms step_avg:42.91ms
step:1105/2160 train_time:47429ms step_avg:42.92ms
step:1106/2160 train_time:47488ms step_avg:42.94ms
step:1107/2160 train_time:47549ms step_avg:42.95ms
step:1108/2160 train_time:47607ms step_avg:42.97ms
step:1109/2160 train_time:47668ms step_avg:42.98ms
step:1110/2160 train_time:47727ms step_avg:43.00ms
step:1111/2160 train_time:47788ms step_avg:43.01ms
step:1112/2160 train_time:47846ms step_avg:43.03ms
step:1113/2160 train_time:47907ms step_avg:43.04ms
step:1114/2160 train_time:47966ms step_avg:43.06ms
step:1115/2160 train_time:48026ms step_avg:43.07ms
step:1116/2160 train_time:48084ms step_avg:43.09ms
step:1117/2160 train_time:48145ms step_avg:43.10ms
step:1118/2160 train_time:48203ms step_avg:43.12ms
step:1119/2160 train_time:48264ms step_avg:43.13ms
step:1120/2160 train_time:48322ms step_avg:43.14ms
step:1121/2160 train_time:48383ms step_avg:43.16ms
step:1122/2160 train_time:48442ms step_avg:43.17ms
step:1123/2160 train_time:48503ms step_avg:43.19ms
step:1124/2160 train_time:48562ms step_avg:43.20ms
step:1125/2160 train_time:48622ms step_avg:43.22ms
step:1126/2160 train_time:48681ms step_avg:43.23ms
step:1127/2160 train_time:48742ms step_avg:43.25ms
step:1128/2160 train_time:48800ms step_avg:43.26ms
step:1129/2160 train_time:48861ms step_avg:43.28ms
step:1130/2160 train_time:48920ms step_avg:43.29ms
step:1131/2160 train_time:48981ms step_avg:43.31ms
step:1132/2160 train_time:49039ms step_avg:43.32ms
step:1133/2160 train_time:49100ms step_avg:43.34ms
step:1134/2160 train_time:49159ms step_avg:43.35ms
step:1135/2160 train_time:49219ms step_avg:43.37ms
step:1136/2160 train_time:49278ms step_avg:43.38ms
step:1137/2160 train_time:49339ms step_avg:43.39ms
step:1138/2160 train_time:49398ms step_avg:43.41ms
step:1139/2160 train_time:49459ms step_avg:43.42ms
step:1140/2160 train_time:49517ms step_avg:43.44ms
step:1141/2160 train_time:49578ms step_avg:43.45ms
step:1142/2160 train_time:49637ms step_avg:43.46ms
step:1143/2160 train_time:49698ms step_avg:43.48ms
step:1144/2160 train_time:49757ms step_avg:43.49ms
step:1145/2160 train_time:49817ms step_avg:43.51ms
step:1146/2160 train_time:49876ms step_avg:43.52ms
step:1147/2160 train_time:49937ms step_avg:43.54ms
step:1148/2160 train_time:49996ms step_avg:43.55ms
step:1149/2160 train_time:50056ms step_avg:43.57ms
step:1150/2160 train_time:50115ms step_avg:43.58ms
step:1151/2160 train_time:50176ms step_avg:43.59ms
step:1152/2160 train_time:50234ms step_avg:43.61ms
step:1153/2160 train_time:50295ms step_avg:43.62ms
step:1154/2160 train_time:50354ms step_avg:43.63ms
step:1155/2160 train_time:50414ms step_avg:43.65ms
step:1156/2160 train_time:50473ms step_avg:43.66ms
step:1157/2160 train_time:50534ms step_avg:43.68ms
step:1158/2160 train_time:50592ms step_avg:43.69ms
step:1159/2160 train_time:50653ms step_avg:43.70ms
step:1160/2160 train_time:50712ms step_avg:43.72ms
step:1161/2160 train_time:50773ms step_avg:43.73ms
step:1162/2160 train_time:50832ms step_avg:43.75ms
step:1163/2160 train_time:50893ms step_avg:43.76ms
step:1164/2160 train_time:50951ms step_avg:43.77ms
step:1165/2160 train_time:51011ms step_avg:43.79ms
step:1166/2160 train_time:51071ms step_avg:43.80ms
step:1167/2160 train_time:51132ms step_avg:43.81ms
step:1168/2160 train_time:51190ms step_avg:43.83ms
step:1169/2160 train_time:51251ms step_avg:43.84ms
step:1170/2160 train_time:51309ms step_avg:43.85ms
step:1171/2160 train_time:51370ms step_avg:43.87ms
step:1172/2160 train_time:51429ms step_avg:43.88ms
step:1173/2160 train_time:51489ms step_avg:43.90ms
step:1174/2160 train_time:51548ms step_avg:43.91ms
step:1175/2160 train_time:51609ms step_avg:43.92ms
step:1176/2160 train_time:51667ms step_avg:43.93ms
step:1177/2160 train_time:51729ms step_avg:43.95ms
step:1178/2160 train_time:51788ms step_avg:43.96ms
step:1179/2160 train_time:51849ms step_avg:43.98ms
step:1180/2160 train_time:51907ms step_avg:43.99ms
step:1181/2160 train_time:51968ms step_avg:44.00ms
step:1182/2160 train_time:52027ms step_avg:44.02ms
step:1183/2160 train_time:52088ms step_avg:44.03ms
step:1184/2160 train_time:52146ms step_avg:44.04ms
step:1185/2160 train_time:52207ms step_avg:44.06ms
step:1186/2160 train_time:52266ms step_avg:44.07ms
step:1187/2160 train_time:52327ms step_avg:44.08ms
step:1188/2160 train_time:52385ms step_avg:44.10ms
step:1189/2160 train_time:52446ms step_avg:44.11ms
step:1190/2160 train_time:52505ms step_avg:44.12ms
step:1191/2160 train_time:52566ms step_avg:44.14ms
step:1192/2160 train_time:52625ms step_avg:44.15ms
step:1193/2160 train_time:52686ms step_avg:44.16ms
step:1194/2160 train_time:52744ms step_avg:44.17ms
step:1195/2160 train_time:52806ms step_avg:44.19ms
step:1196/2160 train_time:52864ms step_avg:44.20ms
step:1197/2160 train_time:52926ms step_avg:44.22ms
step:1198/2160 train_time:52985ms step_avg:44.23ms
step:1199/2160 train_time:53046ms step_avg:44.24ms
step:1200/2160 train_time:53104ms step_avg:44.25ms
step:1201/2160 train_time:53165ms step_avg:44.27ms
step:1202/2160 train_time:53223ms step_avg:44.28ms
step:1203/2160 train_time:53284ms step_avg:44.29ms
step:1204/2160 train_time:53342ms step_avg:44.30ms
step:1205/2160 train_time:53403ms step_avg:44.32ms
step:1206/2160 train_time:53462ms step_avg:44.33ms
step:1207/2160 train_time:53523ms step_avg:44.34ms
step:1208/2160 train_time:53582ms step_avg:44.36ms
step:1209/2160 train_time:53643ms step_avg:44.37ms
step:1210/2160 train_time:53702ms step_avg:44.38ms
step:1211/2160 train_time:53762ms step_avg:44.39ms
step:1212/2160 train_time:53821ms step_avg:44.41ms
step:1213/2160 train_time:53883ms step_avg:44.42ms
step:1214/2160 train_time:53942ms step_avg:44.43ms
step:1215/2160 train_time:54003ms step_avg:44.45ms
step:1216/2160 train_time:54062ms step_avg:44.46ms
step:1217/2160 train_time:54122ms step_avg:44.47ms
step:1218/2160 train_time:54181ms step_avg:44.48ms
step:1219/2160 train_time:54242ms step_avg:44.50ms
step:1220/2160 train_time:54301ms step_avg:44.51ms
step:1221/2160 train_time:54362ms step_avg:44.52ms
step:1222/2160 train_time:54420ms step_avg:44.53ms
step:1223/2160 train_time:54481ms step_avg:44.55ms
step:1224/2160 train_time:54540ms step_avg:44.56ms
step:1225/2160 train_time:54600ms step_avg:44.57ms
step:1226/2160 train_time:54659ms step_avg:44.58ms
step:1227/2160 train_time:54720ms step_avg:44.60ms
step:1228/2160 train_time:54779ms step_avg:44.61ms
step:1229/2160 train_time:54839ms step_avg:44.62ms
step:1230/2160 train_time:54898ms step_avg:44.63ms
step:1231/2160 train_time:54959ms step_avg:44.65ms
step:1232/2160 train_time:55018ms step_avg:44.66ms
step:1233/2160 train_time:55078ms step_avg:44.67ms
step:1234/2160 train_time:55137ms step_avg:44.68ms
step:1235/2160 train_time:55198ms step_avg:44.69ms
step:1236/2160 train_time:55257ms step_avg:44.71ms
step:1237/2160 train_time:55317ms step_avg:44.72ms
step:1238/2160 train_time:55376ms step_avg:44.73ms
step:1239/2160 train_time:55436ms step_avg:44.74ms
step:1240/2160 train_time:55495ms step_avg:44.75ms
step:1241/2160 train_time:55556ms step_avg:44.77ms
step:1242/2160 train_time:55614ms step_avg:44.78ms
step:1243/2160 train_time:55675ms step_avg:44.79ms
step:1244/2160 train_time:55734ms step_avg:44.80ms
step:1245/2160 train_time:55795ms step_avg:44.82ms
step:1246/2160 train_time:55854ms step_avg:44.83ms
step:1247/2160 train_time:55915ms step_avg:44.84ms
step:1248/2160 train_time:55974ms step_avg:44.85ms
step:1249/2160 train_time:56035ms step_avg:44.86ms
step:1250/2160 train_time:56093ms step_avg:44.87ms
step:1250/2160 val_loss:3.5705 train_time:56155ms step_avg:44.92ms
step:1251/2160 train_time:56178ms step_avg:44.91ms
step:1252/2160 train_time:56217ms step_avg:44.90ms
step:1253/2160 train_time:56280ms step_avg:44.92ms
step:1254/2160 train_time:56343ms step_avg:44.93ms
step:1255/2160 train_time:56404ms step_avg:44.94ms
step:1256/2160 train_time:56464ms step_avg:44.96ms
step:1257/2160 train_time:56524ms step_avg:44.97ms
step:1258/2160 train_time:56582ms step_avg:44.98ms
step:1259/2160 train_time:56643ms step_avg:44.99ms
step:1260/2160 train_time:56702ms step_avg:45.00ms
step:1261/2160 train_time:56762ms step_avg:45.01ms
step:1262/2160 train_time:56820ms step_avg:45.02ms
step:1263/2160 train_time:56880ms step_avg:45.04ms
step:1264/2160 train_time:56938ms step_avg:45.05ms
step:1265/2160 train_time:56997ms step_avg:45.06ms
step:1266/2160 train_time:57056ms step_avg:45.07ms
step:1267/2160 train_time:57117ms step_avg:45.08ms
step:1268/2160 train_time:57176ms step_avg:45.09ms
step:1269/2160 train_time:57238ms step_avg:45.11ms
step:1270/2160 train_time:57298ms step_avg:45.12ms
step:1271/2160 train_time:57360ms step_avg:45.13ms
step:1272/2160 train_time:57419ms step_avg:45.14ms
step:1273/2160 train_time:57479ms step_avg:45.15ms
step:1274/2160 train_time:57538ms step_avg:45.16ms
step:1275/2160 train_time:57599ms step_avg:45.18ms
step:1276/2160 train_time:57657ms step_avg:45.19ms
step:1277/2160 train_time:57718ms step_avg:45.20ms
step:1278/2160 train_time:57776ms step_avg:45.21ms
step:1279/2160 train_time:57835ms step_avg:45.22ms
step:1280/2160 train_time:57893ms step_avg:45.23ms
step:1281/2160 train_time:57953ms step_avg:45.24ms
step:1282/2160 train_time:58011ms step_avg:45.25ms
step:1283/2160 train_time:58072ms step_avg:45.26ms
step:1284/2160 train_time:58131ms step_avg:45.27ms
step:1285/2160 train_time:58193ms step_avg:45.29ms
step:1286/2160 train_time:58252ms step_avg:45.30ms
step:1287/2160 train_time:58313ms step_avg:45.31ms
step:1288/2160 train_time:58373ms step_avg:45.32ms
step:1289/2160 train_time:58434ms step_avg:45.33ms
step:1290/2160 train_time:58494ms step_avg:45.34ms
step:1291/2160 train_time:58555ms step_avg:45.36ms
step:1292/2160 train_time:58613ms step_avg:45.37ms
step:1293/2160 train_time:58674ms step_avg:45.38ms
step:1294/2160 train_time:58733ms step_avg:45.39ms
step:1295/2160 train_time:58793ms step_avg:45.40ms
step:1296/2160 train_time:58852ms step_avg:45.41ms
step:1297/2160 train_time:58912ms step_avg:45.42ms
step:1298/2160 train_time:58970ms step_avg:45.43ms
step:1299/2160 train_time:59030ms step_avg:45.44ms
step:1300/2160 train_time:59090ms step_avg:45.45ms
step:1301/2160 train_time:59149ms step_avg:45.46ms
step:1302/2160 train_time:59208ms step_avg:45.47ms
step:1303/2160 train_time:59270ms step_avg:45.49ms
step:1304/2160 train_time:59329ms step_avg:45.50ms
step:1305/2160 train_time:59390ms step_avg:45.51ms
step:1306/2160 train_time:59449ms step_avg:45.52ms
step:1307/2160 train_time:59510ms step_avg:45.53ms
step:1308/2160 train_time:59569ms step_avg:45.54ms
step:1309/2160 train_time:59630ms step_avg:45.55ms
step:1310/2160 train_time:59689ms step_avg:45.56ms
step:1311/2160 train_time:59749ms step_avg:45.58ms
step:1312/2160 train_time:59809ms step_avg:45.59ms
step:1313/2160 train_time:59869ms step_avg:45.60ms
step:1314/2160 train_time:59927ms step_avg:45.61ms
step:1315/2160 train_time:59987ms step_avg:45.62ms
step:1316/2160 train_time:60045ms step_avg:45.63ms
step:1317/2160 train_time:60106ms step_avg:45.64ms
step:1318/2160 train_time:60164ms step_avg:45.65ms
step:1319/2160 train_time:60225ms step_avg:45.66ms
step:1320/2160 train_time:60284ms step_avg:45.67ms
step:1321/2160 train_time:60345ms step_avg:45.68ms
step:1322/2160 train_time:60404ms step_avg:45.69ms
step:1323/2160 train_time:60465ms step_avg:45.70ms
step:1324/2160 train_time:60524ms step_avg:45.71ms
step:1325/2160 train_time:60585ms step_avg:45.72ms
step:1326/2160 train_time:60644ms step_avg:45.73ms
step:1327/2160 train_time:60705ms step_avg:45.75ms
step:1328/2160 train_time:60764ms step_avg:45.76ms
step:1329/2160 train_time:60825ms step_avg:45.77ms
step:1330/2160 train_time:60883ms step_avg:45.78ms
step:1331/2160 train_time:60944ms step_avg:45.79ms
step:1332/2160 train_time:61002ms step_avg:45.80ms
step:1333/2160 train_time:61063ms step_avg:45.81ms
step:1334/2160 train_time:61122ms step_avg:45.82ms
step:1335/2160 train_time:61183ms step_avg:45.83ms
step:1336/2160 train_time:61241ms step_avg:45.84ms
step:1337/2160 train_time:61302ms step_avg:45.85ms
step:1338/2160 train_time:61361ms step_avg:45.86ms
step:1339/2160 train_time:61422ms step_avg:45.87ms
step:1340/2160 train_time:61481ms step_avg:45.88ms
step:1341/2160 train_time:61542ms step_avg:45.89ms
step:1342/2160 train_time:61601ms step_avg:45.90ms
step:1343/2160 train_time:61662ms step_avg:45.91ms
step:1344/2160 train_time:61721ms step_avg:45.92ms
step:1345/2160 train_time:61782ms step_avg:45.93ms
step:1346/2160 train_time:61841ms step_avg:45.94ms
step:1347/2160 train_time:61901ms step_avg:45.95ms
step:1348/2160 train_time:61960ms step_avg:45.96ms
step:1349/2160 train_time:62020ms step_avg:45.98ms
step:1350/2160 train_time:62079ms step_avg:45.98ms
step:1351/2160 train_time:62140ms step_avg:46.00ms
step:1352/2160 train_time:62199ms step_avg:46.00ms
step:1353/2160 train_time:62259ms step_avg:46.02ms
step:1354/2160 train_time:62318ms step_avg:46.02ms
step:1355/2160 train_time:62378ms step_avg:46.04ms
step:1356/2160 train_time:62437ms step_avg:46.04ms
step:1357/2160 train_time:62498ms step_avg:46.06ms
step:1358/2160 train_time:62557ms step_avg:46.07ms
step:1359/2160 train_time:62618ms step_avg:46.08ms
step:1360/2160 train_time:62677ms step_avg:46.09ms
step:1361/2160 train_time:62738ms step_avg:46.10ms
step:1362/2160 train_time:62796ms step_avg:46.11ms
step:1363/2160 train_time:62857ms step_avg:46.12ms
step:1364/2160 train_time:62916ms step_avg:46.13ms
step:1365/2160 train_time:62976ms step_avg:46.14ms
step:1366/2160 train_time:63035ms step_avg:46.15ms
step:1367/2160 train_time:63096ms step_avg:46.16ms
step:1368/2160 train_time:63155ms step_avg:46.17ms
step:1369/2160 train_time:63215ms step_avg:46.18ms
step:1370/2160 train_time:63273ms step_avg:46.18ms
step:1371/2160 train_time:63334ms step_avg:46.20ms
step:1372/2160 train_time:63393ms step_avg:46.20ms
step:1373/2160 train_time:63454ms step_avg:46.22ms
step:1374/2160 train_time:63512ms step_avg:46.22ms
step:1375/2160 train_time:63573ms step_avg:46.24ms
step:1376/2160 train_time:63632ms step_avg:46.24ms
step:1377/2160 train_time:63694ms step_avg:46.26ms
step:1378/2160 train_time:63752ms step_avg:46.26ms
step:1379/2160 train_time:63813ms step_avg:46.28ms
step:1380/2160 train_time:63873ms step_avg:46.28ms
step:1381/2160 train_time:63932ms step_avg:46.29ms
step:1382/2160 train_time:63992ms step_avg:46.30ms
step:1383/2160 train_time:64052ms step_avg:46.31ms
step:1384/2160 train_time:64111ms step_avg:46.32ms
step:1385/2160 train_time:64172ms step_avg:46.33ms
step:1386/2160 train_time:64231ms step_avg:46.34ms
step:1387/2160 train_time:64291ms step_avg:46.35ms
step:1388/2160 train_time:64350ms step_avg:46.36ms
step:1389/2160 train_time:64410ms step_avg:46.37ms
step:1390/2160 train_time:64469ms step_avg:46.38ms
step:1391/2160 train_time:64530ms step_avg:46.39ms
step:1392/2160 train_time:64588ms step_avg:46.40ms
step:1393/2160 train_time:64649ms step_avg:46.41ms
step:1394/2160 train_time:64708ms step_avg:46.42ms
step:1395/2160 train_time:64770ms step_avg:46.43ms
step:1396/2160 train_time:64828ms step_avg:46.44ms
step:1397/2160 train_time:64889ms step_avg:46.45ms
step:1398/2160 train_time:64948ms step_avg:46.46ms
step:1399/2160 train_time:65009ms step_avg:46.47ms
step:1400/2160 train_time:65068ms step_avg:46.48ms
step:1401/2160 train_time:65129ms step_avg:46.49ms
step:1402/2160 train_time:65187ms step_avg:46.50ms
step:1403/2160 train_time:65248ms step_avg:46.51ms
step:1404/2160 train_time:65307ms step_avg:46.51ms
step:1405/2160 train_time:65368ms step_avg:46.52ms
step:1406/2160 train_time:65426ms step_avg:46.53ms
step:1407/2160 train_time:65487ms step_avg:46.54ms
step:1408/2160 train_time:65546ms step_avg:46.55ms
step:1409/2160 train_time:65606ms step_avg:46.56ms
step:1410/2160 train_time:65665ms step_avg:46.57ms
step:1411/2160 train_time:65726ms step_avg:46.58ms
step:1412/2160 train_time:65785ms step_avg:46.59ms
step:1413/2160 train_time:65846ms step_avg:46.60ms
step:1414/2160 train_time:65904ms step_avg:46.61ms
step:1415/2160 train_time:65966ms step_avg:46.62ms
step:1416/2160 train_time:66052ms step_avg:46.65ms
step:1417/2160 train_time:66140ms step_avg:46.68ms
step:1418/2160 train_time:66226ms step_avg:46.70ms
step:1419/2160 train_time:66315ms step_avg:46.73ms
step:1420/2160 train_time:66401ms step_avg:46.76ms
step:1421/2160 train_time:66490ms step_avg:46.79ms
step:1422/2160 train_time:66576ms step_avg:46.82ms
step:1423/2160 train_time:66664ms step_avg:46.85ms
step:1424/2160 train_time:66750ms step_avg:46.87ms
step:1425/2160 train_time:66838ms step_avg:46.90ms
step:1426/2160 train_time:66925ms step_avg:46.93ms
step:1427/2160 train_time:67013ms step_avg:46.96ms
step:1428/2160 train_time:67099ms step_avg:46.99ms
step:1429/2160 train_time:67187ms step_avg:47.02ms
step:1430/2160 train_time:67273ms step_avg:47.04ms
step:1431/2160 train_time:67361ms step_avg:47.07ms
step:1432/2160 train_time:67447ms step_avg:47.10ms
step:1433/2160 train_time:67535ms step_avg:47.13ms
step:1434/2160 train_time:67622ms step_avg:47.16ms
step:1435/2160 train_time:67710ms step_avg:47.18ms
step:1436/2160 train_time:67797ms step_avg:47.21ms
step:1437/2160 train_time:67886ms step_avg:47.24ms
step:1438/2160 train_time:67974ms step_avg:47.27ms
step:1439/2160 train_time:68062ms step_avg:47.30ms
step:1440/2160 train_time:68148ms step_avg:47.32ms
step:1441/2160 train_time:68236ms step_avg:47.35ms
step:1442/2160 train_time:68322ms step_avg:47.38ms
step:1443/2160 train_time:68411ms step_avg:47.41ms
step:1444/2160 train_time:68497ms step_avg:47.44ms
step:1445/2160 train_time:68585ms step_avg:47.46ms
step:1446/2160 train_time:68671ms step_avg:47.49ms
step:1447/2160 train_time:68759ms step_avg:47.52ms
step:1448/2160 train_time:68846ms step_avg:47.55ms
step:1449/2160 train_time:68934ms step_avg:47.57ms
step:1450/2160 train_time:69021ms step_avg:47.60ms
step:1451/2160 train_time:69109ms step_avg:47.63ms
step:1452/2160 train_time:69196ms step_avg:47.66ms
step:1453/2160 train_time:69283ms step_avg:47.68ms
step:1454/2160 train_time:69370ms step_avg:47.71ms
step:1455/2160 train_time:69458ms step_avg:47.74ms
step:1456/2160 train_time:69545ms step_avg:47.76ms
step:1457/2160 train_time:69633ms step_avg:47.79ms
step:1458/2160 train_time:69719ms step_avg:47.82ms
step:1459/2160 train_time:69808ms step_avg:47.85ms
step:1460/2160 train_time:69895ms step_avg:47.87ms
step:1461/2160 train_time:69983ms step_avg:47.90ms
step:1462/2160 train_time:70070ms step_avg:47.93ms
step:1463/2160 train_time:70159ms step_avg:47.96ms
step:1464/2160 train_time:70245ms step_avg:47.98ms
step:1465/2160 train_time:70333ms step_avg:48.01ms
step:1466/2160 train_time:70419ms step_avg:48.03ms
step:1467/2160 train_time:70508ms step_avg:48.06ms
step:1468/2160 train_time:70594ms step_avg:48.09ms
step:1469/2160 train_time:70682ms step_avg:48.12ms
step:1470/2160 train_time:70769ms step_avg:48.14ms
step:1471/2160 train_time:70858ms step_avg:48.17ms
step:1472/2160 train_time:70945ms step_avg:48.20ms
step:1473/2160 train_time:71034ms step_avg:48.22ms
step:1474/2160 train_time:71121ms step_avg:48.25ms
step:1475/2160 train_time:71209ms step_avg:48.28ms
step:1476/2160 train_time:71295ms step_avg:48.30ms
step:1477/2160 train_time:71383ms step_avg:48.33ms
step:1478/2160 train_time:71470ms step_avg:48.36ms
step:1479/2160 train_time:71558ms step_avg:48.38ms
step:1480/2160 train_time:71645ms step_avg:48.41ms
step:1481/2160 train_time:71733ms step_avg:48.44ms
step:1482/2160 train_time:71820ms step_avg:48.46ms
step:1483/2160 train_time:71908ms step_avg:48.49ms
step:1484/2160 train_time:71995ms step_avg:48.51ms
step:1485/2160 train_time:72083ms step_avg:48.54ms
step:1486/2160 train_time:72170ms step_avg:48.57ms
step:1487/2160 train_time:72258ms step_avg:48.59ms
step:1488/2160 train_time:72344ms step_avg:48.62ms
step:1489/2160 train_time:72433ms step_avg:48.65ms
step:1490/2160 train_time:72519ms step_avg:48.67ms
step:1491/2160 train_time:72607ms step_avg:48.70ms
step:1492/2160 train_time:72695ms step_avg:48.72ms
step:1493/2160 train_time:72783ms step_avg:48.75ms
step:1494/2160 train_time:72869ms step_avg:48.77ms
step:1495/2160 train_time:72957ms step_avg:48.80ms
step:1496/2160 train_time:73044ms step_avg:48.83ms
step:1497/2160 train_time:73133ms step_avg:48.85ms
step:1498/2160 train_time:73219ms step_avg:48.88ms
step:1499/2160 train_time:73307ms step_avg:48.90ms
step:1500/2160 train_time:73394ms step_avg:48.93ms
step:1500/2160 val_loss:3.4720 train_time:73482ms step_avg:48.99ms
step:1501/2160 train_time:73505ms step_avg:48.97ms
step:1502/2160 train_time:73574ms step_avg:48.98ms
step:1503/2160 train_time:73665ms step_avg:49.01ms
step:1504/2160 train_time:73751ms step_avg:49.04ms
step:1505/2160 train_time:73839ms step_avg:49.06ms
step:1506/2160 train_time:73924ms step_avg:49.09ms
step:1507/2160 train_time:74011ms step_avg:49.11ms
step:1508/2160 train_time:74096ms step_avg:49.14ms
step:1509/2160 train_time:74183ms step_avg:49.16ms
step:1510/2160 train_time:74269ms step_avg:49.18ms
step:1511/2160 train_time:74357ms step_avg:49.21ms
step:1512/2160 train_time:74444ms step_avg:49.24ms
step:1513/2160 train_time:74535ms step_avg:49.26ms
step:1514/2160 train_time:74624ms step_avg:49.29ms
step:1515/2160 train_time:74714ms step_avg:49.32ms
step:1516/2160 train_time:74800ms step_avg:49.34ms
step:1517/2160 train_time:74888ms step_avg:49.37ms
step:1518/2160 train_time:74973ms step_avg:49.39ms
step:1519/2160 train_time:75060ms step_avg:49.41ms
step:1520/2160 train_time:75146ms step_avg:49.44ms
step:1521/2160 train_time:75233ms step_avg:49.46ms
step:1522/2160 train_time:75319ms step_avg:49.49ms
step:1523/2160 train_time:75408ms step_avg:49.51ms
step:1524/2160 train_time:75496ms step_avg:49.54ms
step:1525/2160 train_time:75586ms step_avg:49.56ms
step:1526/2160 train_time:75673ms step_avg:49.59ms
step:1527/2160 train_time:75762ms step_avg:49.61ms
step:1528/2160 train_time:75848ms step_avg:49.64ms
step:1529/2160 train_time:75936ms step_avg:49.66ms
step:1530/2160 train_time:76023ms step_avg:49.69ms
step:1531/2160 train_time:76110ms step_avg:49.71ms
step:1532/2160 train_time:76196ms step_avg:49.74ms
step:1533/2160 train_time:76284ms step_avg:49.76ms
step:1534/2160 train_time:76370ms step_avg:49.78ms
step:1535/2160 train_time:76459ms step_avg:49.81ms
step:1536/2160 train_time:76546ms step_avg:49.83ms
step:1537/2160 train_time:76636ms step_avg:49.86ms
step:1538/2160 train_time:76723ms step_avg:49.88ms
step:1539/2160 train_time:76812ms step_avg:49.91ms
step:1540/2160 train_time:76898ms step_avg:49.93ms
step:1541/2160 train_time:76986ms step_avg:49.96ms
step:1542/2160 train_time:77072ms step_avg:49.98ms
step:1543/2160 train_time:77159ms step_avg:50.01ms
step:1544/2160 train_time:77246ms step_avg:50.03ms
step:1545/2160 train_time:77333ms step_avg:50.05ms
step:1546/2160 train_time:77420ms step_avg:50.08ms
step:1547/2160 train_time:77509ms step_avg:50.10ms
step:1548/2160 train_time:77596ms step_avg:50.13ms
step:1549/2160 train_time:77686ms step_avg:50.15ms
step:1550/2160 train_time:77773ms step_avg:50.18ms
step:1551/2160 train_time:77861ms step_avg:50.20ms
step:1552/2160 train_time:77948ms step_avg:50.22ms
step:1553/2160 train_time:78036ms step_avg:50.25ms
step:1554/2160 train_time:78121ms step_avg:50.27ms
step:1555/2160 train_time:78209ms step_avg:50.30ms
step:1556/2160 train_time:78295ms step_avg:50.32ms
step:1557/2160 train_time:78383ms step_avg:50.34ms
step:1558/2160 train_time:78471ms step_avg:50.37ms
step:1559/2160 train_time:78560ms step_avg:50.39ms
step:1560/2160 train_time:78647ms step_avg:50.41ms
step:1561/2160 train_time:78735ms step_avg:50.44ms
step:1562/2160 train_time:78822ms step_avg:50.46ms
step:1563/2160 train_time:78910ms step_avg:50.49ms
step:1564/2160 train_time:78996ms step_avg:50.51ms
step:1565/2160 train_time:79085ms step_avg:50.53ms
step:1566/2160 train_time:79170ms step_avg:50.56ms
step:1567/2160 train_time:79258ms step_avg:50.58ms
step:1568/2160 train_time:79344ms step_avg:50.60ms
step:1569/2160 train_time:79432ms step_avg:50.63ms
step:1570/2160 train_time:79518ms step_avg:50.65ms
step:1571/2160 train_time:79607ms step_avg:50.67ms
step:1572/2160 train_time:79694ms step_avg:50.70ms
step:1573/2160 train_time:79783ms step_avg:50.72ms
step:1574/2160 train_time:79869ms step_avg:50.74ms
step:1575/2160 train_time:79957ms step_avg:50.77ms
step:1576/2160 train_time:80044ms step_avg:50.79ms
step:1577/2160 train_time:80131ms step_avg:50.81ms
step:1578/2160 train_time:80218ms step_avg:50.84ms
step:1579/2160 train_time:80307ms step_avg:50.86ms
step:1580/2160 train_time:80392ms step_avg:50.88ms
step:1581/2160 train_time:80480ms step_avg:50.90ms
step:1582/2160 train_time:80567ms step_avg:50.93ms
step:1583/2160 train_time:80655ms step_avg:50.95ms
step:1584/2160 train_time:80742ms step_avg:50.97ms
step:1585/2160 train_time:80830ms step_avg:51.00ms
step:1586/2160 train_time:80917ms step_avg:51.02ms
step:1587/2160 train_time:81006ms step_avg:51.04ms
step:1588/2160 train_time:81092ms step_avg:51.07ms
step:1589/2160 train_time:81180ms step_avg:51.09ms
step:1590/2160 train_time:81266ms step_avg:51.11ms
step:1591/2160 train_time:81354ms step_avg:51.13ms
step:1592/2160 train_time:81441ms step_avg:51.16ms
step:1593/2160 train_time:81529ms step_avg:51.18ms
step:1594/2160 train_time:81616ms step_avg:51.20ms
step:1595/2160 train_time:81705ms step_avg:51.23ms
step:1596/2160 train_time:81791ms step_avg:51.25ms
step:1597/2160 train_time:81881ms step_avg:51.27ms
step:1598/2160 train_time:81968ms step_avg:51.29ms
step:1599/2160 train_time:82056ms step_avg:51.32ms
step:1600/2160 train_time:82143ms step_avg:51.34ms
step:1601/2160 train_time:82230ms step_avg:51.36ms
step:1602/2160 train_time:82316ms step_avg:51.38ms
step:1603/2160 train_time:82404ms step_avg:51.41ms
step:1604/2160 train_time:82491ms step_avg:51.43ms
step:1605/2160 train_time:82579ms step_avg:51.45ms
step:1606/2160 train_time:82666ms step_avg:51.47ms
step:1607/2160 train_time:82754ms step_avg:51.50ms
step:1608/2160 train_time:82841ms step_avg:51.52ms
step:1609/2160 train_time:82929ms step_avg:51.54ms
step:1610/2160 train_time:83016ms step_avg:51.56ms
step:1611/2160 train_time:83104ms step_avg:51.59ms
step:1612/2160 train_time:83190ms step_avg:51.61ms
step:1613/2160 train_time:83280ms step_avg:51.63ms
step:1614/2160 train_time:83366ms step_avg:51.65ms
step:1615/2160 train_time:83454ms step_avg:51.67ms
step:1616/2160 train_time:83541ms step_avg:51.70ms
step:1617/2160 train_time:83629ms step_avg:51.72ms
step:1618/2160 train_time:83716ms step_avg:51.74ms
step:1619/2160 train_time:83806ms step_avg:51.76ms
step:1620/2160 train_time:83893ms step_avg:51.79ms
step:1621/2160 train_time:83981ms step_avg:51.81ms
step:1622/2160 train_time:84067ms step_avg:51.83ms
step:1623/2160 train_time:84155ms step_avg:51.85ms
step:1624/2160 train_time:84242ms step_avg:51.87ms
step:1625/2160 train_time:84330ms step_avg:51.90ms
step:1626/2160 train_time:84416ms step_avg:51.92ms
step:1627/2160 train_time:84505ms step_avg:51.94ms
step:1628/2160 train_time:84591ms step_avg:51.96ms
step:1629/2160 train_time:84680ms step_avg:51.98ms
step:1630/2160 train_time:84766ms step_avg:52.00ms
step:1631/2160 train_time:84855ms step_avg:52.03ms
step:1632/2160 train_time:84941ms step_avg:52.05ms
step:1633/2160 train_time:85029ms step_avg:52.07ms
step:1634/2160 train_time:85115ms step_avg:52.09ms
step:1635/2160 train_time:85204ms step_avg:52.11ms
step:1636/2160 train_time:85290ms step_avg:52.13ms
step:1637/2160 train_time:85381ms step_avg:52.16ms
step:1638/2160 train_time:85466ms step_avg:52.18ms
step:1639/2160 train_time:85554ms step_avg:52.20ms
step:1640/2160 train_time:85641ms step_avg:52.22ms
step:1641/2160 train_time:85729ms step_avg:52.24ms
step:1642/2160 train_time:85816ms step_avg:52.26ms
step:1643/2160 train_time:85905ms step_avg:52.29ms
step:1644/2160 train_time:85991ms step_avg:52.31ms
step:1645/2160 train_time:86079ms step_avg:52.33ms
step:1646/2160 train_time:86166ms step_avg:52.35ms
step:1647/2160 train_time:86254ms step_avg:52.37ms
step:1648/2160 train_time:86341ms step_avg:52.39ms
step:1649/2160 train_time:86429ms step_avg:52.41ms
step:1650/2160 train_time:86516ms step_avg:52.43ms
step:1651/2160 train_time:86604ms step_avg:52.46ms
step:1652/2160 train_time:86690ms step_avg:52.48ms
step:1653/2160 train_time:86779ms step_avg:52.50ms
step:1654/2160 train_time:86865ms step_avg:52.52ms
step:1655/2160 train_time:86953ms step_avg:52.54ms
step:1656/2160 train_time:87040ms step_avg:52.56ms
step:1657/2160 train_time:87128ms step_avg:52.58ms
step:1658/2160 train_time:87215ms step_avg:52.60ms
step:1659/2160 train_time:87303ms step_avg:52.62ms
step:1660/2160 train_time:87390ms step_avg:52.64ms
step:1661/2160 train_time:87479ms step_avg:52.67ms
step:1662/2160 train_time:87566ms step_avg:52.69ms
step:1663/2160 train_time:87654ms step_avg:52.71ms
step:1664/2160 train_time:87740ms step_avg:52.73ms
step:1665/2160 train_time:87828ms step_avg:52.75ms
step:1666/2160 train_time:87916ms step_avg:52.77ms
step:1667/2160 train_time:88004ms step_avg:52.79ms
step:1668/2160 train_time:88090ms step_avg:52.81ms
step:1669/2160 train_time:88178ms step_avg:52.83ms
step:1670/2160 train_time:88266ms step_avg:52.85ms
step:1671/2160 train_time:88354ms step_avg:52.88ms
step:1672/2160 train_time:88440ms step_avg:52.90ms
step:1673/2160 train_time:88529ms step_avg:52.92ms
step:1674/2160 train_time:88615ms step_avg:52.94ms
step:1675/2160 train_time:88703ms step_avg:52.96ms
step:1676/2160 train_time:88789ms step_avg:52.98ms
step:1677/2160 train_time:88878ms step_avg:53.00ms
step:1678/2160 train_time:88964ms step_avg:53.02ms
step:1679/2160 train_time:89053ms step_avg:53.04ms
step:1680/2160 train_time:89139ms step_avg:53.06ms
step:1681/2160 train_time:89227ms step_avg:53.08ms
step:1682/2160 train_time:89314ms step_avg:53.10ms
step:1683/2160 train_time:89402ms step_avg:53.12ms
step:1684/2160 train_time:89489ms step_avg:53.14ms
step:1685/2160 train_time:89577ms step_avg:53.16ms
step:1686/2160 train_time:89663ms step_avg:53.18ms
step:1687/2160 train_time:89752ms step_avg:53.20ms
step:1688/2160 train_time:89838ms step_avg:53.22ms
step:1689/2160 train_time:89927ms step_avg:53.24ms
step:1690/2160 train_time:90013ms step_avg:53.26ms
step:1691/2160 train_time:90101ms step_avg:53.28ms
step:1692/2160 train_time:90188ms step_avg:53.30ms
step:1693/2160 train_time:90276ms step_avg:53.32ms
step:1694/2160 train_time:90363ms step_avg:53.34ms
step:1695/2160 train_time:90451ms step_avg:53.36ms
step:1696/2160 train_time:90538ms step_avg:53.38ms
step:1697/2160 train_time:90627ms step_avg:53.40ms
step:1698/2160 train_time:90713ms step_avg:53.42ms
step:1699/2160 train_time:90801ms step_avg:53.44ms
step:1700/2160 train_time:90887ms step_avg:53.46ms
step:1701/2160 train_time:90976ms step_avg:53.48ms
step:1702/2160 train_time:91061ms step_avg:53.50ms
step:1703/2160 train_time:91150ms step_avg:53.52ms
step:1704/2160 train_time:91236ms step_avg:53.54ms
step:1705/2160 train_time:91325ms step_avg:53.56ms
step:1706/2160 train_time:91411ms step_avg:53.58ms
step:1707/2160 train_time:91500ms step_avg:53.60ms
step:1708/2160 train_time:91587ms step_avg:53.62ms
step:1709/2160 train_time:91675ms step_avg:53.64ms
step:1710/2160 train_time:91761ms step_avg:53.66ms
step:1711/2160 train_time:91850ms step_avg:53.68ms
step:1712/2160 train_time:91936ms step_avg:53.70ms
step:1713/2160 train_time:92024ms step_avg:53.72ms
step:1714/2160 train_time:92111ms step_avg:53.74ms
step:1715/2160 train_time:92199ms step_avg:53.76ms
step:1716/2160 train_time:92285ms step_avg:53.78ms
step:1717/2160 train_time:92373ms step_avg:53.80ms
step:1718/2160 train_time:92460ms step_avg:53.82ms
step:1719/2160 train_time:92548ms step_avg:53.84ms
step:1720/2160 train_time:92635ms step_avg:53.86ms
step:1721/2160 train_time:92723ms step_avg:53.88ms
step:1722/2160 train_time:92811ms step_avg:53.90ms
step:1723/2160 train_time:92899ms step_avg:53.92ms
step:1724/2160 train_time:92986ms step_avg:53.94ms
step:1725/2160 train_time:93075ms step_avg:53.96ms
step:1726/2160 train_time:93162ms step_avg:53.98ms
step:1727/2160 train_time:93250ms step_avg:54.00ms
step:1728/2160 train_time:93336ms step_avg:54.01ms
step:1729/2160 train_time:93426ms step_avg:54.03ms
step:1730/2160 train_time:93512ms step_avg:54.05ms
step:1731/2160 train_time:93601ms step_avg:54.07ms
step:1732/2160 train_time:93687ms step_avg:54.09ms
step:1733/2160 train_time:93775ms step_avg:54.11ms
step:1734/2160 train_time:93861ms step_avg:54.13ms
step:1735/2160 train_time:93949ms step_avg:54.15ms
step:1736/2160 train_time:94035ms step_avg:54.17ms
step:1737/2160 train_time:94124ms step_avg:54.19ms
step:1738/2160 train_time:94211ms step_avg:54.21ms
step:1739/2160 train_time:94300ms step_avg:54.23ms
step:1740/2160 train_time:94386ms step_avg:54.24ms
step:1741/2160 train_time:94474ms step_avg:54.26ms
step:1742/2160 train_time:94561ms step_avg:54.28ms
step:1743/2160 train_time:94650ms step_avg:54.30ms
step:1744/2160 train_time:94736ms step_avg:54.32ms
step:1745/2160 train_time:94825ms step_avg:54.34ms
step:1746/2160 train_time:94911ms step_avg:54.36ms
step:1747/2160 train_time:94999ms step_avg:54.38ms
step:1748/2160 train_time:95085ms step_avg:54.40ms
step:1749/2160 train_time:95173ms step_avg:54.42ms
step:1750/2160 train_time:95260ms step_avg:54.43ms
step:1750/2160 val_loss:3.3799 train_time:95349ms step_avg:54.49ms
step:1751/2160 train_time:95373ms step_avg:54.47ms
step:1752/2160 train_time:95440ms step_avg:54.47ms
step:1753/2160 train_time:95531ms step_avg:54.50ms
step:1754/2160 train_time:95618ms step_avg:54.51ms
step:1755/2160 train_time:95706ms step_avg:54.53ms
step:1756/2160 train_time:95791ms step_avg:54.55ms
step:1757/2160 train_time:95878ms step_avg:54.57ms
step:1758/2160 train_time:95965ms step_avg:54.59ms
step:1759/2160 train_time:96052ms step_avg:54.61ms
step:1760/2160 train_time:96138ms step_avg:54.62ms
step:1761/2160 train_time:96225ms step_avg:54.64ms
step:1762/2160 train_time:96313ms step_avg:54.66ms
step:1763/2160 train_time:96405ms step_avg:54.68ms
step:1764/2160 train_time:96494ms step_avg:54.70ms
step:1765/2160 train_time:96583ms step_avg:54.72ms
step:1766/2160 train_time:96670ms step_avg:54.74ms
step:1767/2160 train_time:96758ms step_avg:54.76ms
step:1768/2160 train_time:96843ms step_avg:54.78ms
step:1769/2160 train_time:96931ms step_avg:54.79ms
step:1770/2160 train_time:97016ms step_avg:54.81ms
step:1771/2160 train_time:97104ms step_avg:54.83ms
step:1772/2160 train_time:97192ms step_avg:54.85ms
step:1773/2160 train_time:97280ms step_avg:54.87ms
step:1774/2160 train_time:97367ms step_avg:54.89ms
step:1775/2160 train_time:97457ms step_avg:54.91ms
step:1776/2160 train_time:97544ms step_avg:54.92ms
step:1777/2160 train_time:97634ms step_avg:54.94ms
step:1778/2160 train_time:97720ms step_avg:54.96ms
step:1779/2160 train_time:97809ms step_avg:54.98ms
step:1780/2160 train_time:97894ms step_avg:55.00ms
step:1781/2160 train_time:97982ms step_avg:55.02ms
step:1782/2160 train_time:98068ms step_avg:55.03ms
step:1783/2160 train_time:98156ms step_avg:55.05ms
step:1784/2160 train_time:98242ms step_avg:55.07ms
step:1785/2160 train_time:98331ms step_avg:55.09ms
step:1786/2160 train_time:98419ms step_avg:55.11ms
step:1787/2160 train_time:98507ms step_avg:55.12ms
step:1788/2160 train_time:98594ms step_avg:55.14ms
step:1789/2160 train_time:98684ms step_avg:55.16ms
step:1790/2160 train_time:98770ms step_avg:55.18ms
step:1791/2160 train_time:98858ms step_avg:55.20ms
step:1792/2160 train_time:98943ms step_avg:55.21ms
step:1793/2160 train_time:99031ms step_avg:55.23ms
step:1794/2160 train_time:99117ms step_avg:55.25ms
step:1795/2160 train_time:99205ms step_avg:55.27ms
step:1796/2160 train_time:99291ms step_avg:55.28ms
step:1797/2160 train_time:99380ms step_avg:55.30ms
step:1798/2160 train_time:99468ms step_avg:55.32ms
step:1799/2160 train_time:99557ms step_avg:55.34ms
step:1800/2160 train_time:99644ms step_avg:55.36ms
step:1801/2160 train_time:99732ms step_avg:55.38ms
step:1802/2160 train_time:99818ms step_avg:55.39ms
step:1803/2160 train_time:99906ms step_avg:55.41ms
step:1804/2160 train_time:99993ms step_avg:55.43ms
step:1805/2160 train_time:100080ms step_avg:55.45ms
step:1806/2160 train_time:100166ms step_avg:55.46ms
step:1807/2160 train_time:100254ms step_avg:55.48ms
step:1808/2160 train_time:100342ms step_avg:55.50ms
step:1809/2160 train_time:100431ms step_avg:55.52ms
step:1810/2160 train_time:100518ms step_avg:55.53ms
step:1811/2160 train_time:100608ms step_avg:55.55ms
step:1812/2160 train_time:100694ms step_avg:55.57ms
step:1813/2160 train_time:100783ms step_avg:55.59ms
step:1814/2160 train_time:100869ms step_avg:55.61ms
step:1815/2160 train_time:100958ms step_avg:55.62ms
step:1816/2160 train_time:101043ms step_avg:55.64ms
step:1817/2160 train_time:101131ms step_avg:55.66ms
step:1818/2160 train_time:101218ms step_avg:55.68ms
step:1819/2160 train_time:101307ms step_avg:55.69ms
step:1820/2160 train_time:101393ms step_avg:55.71ms
step:1821/2160 train_time:101481ms step_avg:55.73ms
step:1822/2160 train_time:101568ms step_avg:55.75ms
step:1823/2160 train_time:101656ms step_avg:55.76ms
step:1824/2160 train_time:101743ms step_avg:55.78ms
step:1825/2160 train_time:101831ms step_avg:55.80ms
step:1826/2160 train_time:101918ms step_avg:55.81ms
step:1827/2160 train_time:102005ms step_avg:55.83ms
step:1828/2160 train_time:102092ms step_avg:55.85ms
step:1829/2160 train_time:102179ms step_avg:55.87ms
step:1830/2160 train_time:102265ms step_avg:55.88ms
step:1831/2160 train_time:102353ms step_avg:55.90ms
step:1832/2160 train_time:102440ms step_avg:55.92ms
step:1833/2160 train_time:102530ms step_avg:55.94ms
step:1834/2160 train_time:102617ms step_avg:55.95ms
step:1835/2160 train_time:102705ms step_avg:55.97ms
step:1836/2160 train_time:102791ms step_avg:55.99ms
step:1837/2160 train_time:102880ms step_avg:56.00ms
step:1838/2160 train_time:102966ms step_avg:56.02ms
step:1839/2160 train_time:103055ms step_avg:56.04ms
step:1840/2160 train_time:103141ms step_avg:56.06ms
step:1841/2160 train_time:103230ms step_avg:56.07ms
step:1842/2160 train_time:103317ms step_avg:56.09ms
step:1843/2160 train_time:103405ms step_avg:56.11ms
step:1844/2160 train_time:103493ms step_avg:56.12ms
step:1845/2160 train_time:103582ms step_avg:56.14ms
step:1846/2160 train_time:103668ms step_avg:56.16ms
step:1847/2160 train_time:103757ms step_avg:56.18ms
step:1848/2160 train_time:103843ms step_avg:56.19ms
step:1849/2160 train_time:103932ms step_avg:56.21ms
step:1850/2160 train_time:104018ms step_avg:56.23ms
step:1851/2160 train_time:104106ms step_avg:56.24ms
step:1852/2160 train_time:104192ms step_avg:56.26ms
step:1853/2160 train_time:104280ms step_avg:56.28ms
step:1854/2160 train_time:104367ms step_avg:56.29ms
step:1855/2160 train_time:104455ms step_avg:56.31ms
step:1856/2160 train_time:104542ms step_avg:56.33ms
step:1857/2160 train_time:104630ms step_avg:56.34ms
step:1858/2160 train_time:104717ms step_avg:56.36ms
step:1859/2160 train_time:104805ms step_avg:56.38ms
step:1860/2160 train_time:104892ms step_avg:56.39ms
step:1861/2160 train_time:104980ms step_avg:56.41ms
step:1862/2160 train_time:105066ms step_avg:56.43ms
step:1863/2160 train_time:105154ms step_avg:56.44ms
step:1864/2160 train_time:105240ms step_avg:56.46ms
step:1865/2160 train_time:105329ms step_avg:56.48ms
step:1866/2160 train_time:105416ms step_avg:56.49ms
step:1867/2160 train_time:105504ms step_avg:56.51ms
step:1868/2160 train_time:105590ms step_avg:56.53ms
step:1869/2160 train_time:105678ms step_avg:56.54ms
step:1870/2160 train_time:105765ms step_avg:56.56ms
step:1871/2160 train_time:105853ms step_avg:56.58ms
step:1872/2160 train_time:105940ms step_avg:56.59ms
step:1873/2160 train_time:106028ms step_avg:56.61ms
step:1874/2160 train_time:106114ms step_avg:56.62ms
step:1875/2160 train_time:106202ms step_avg:56.64ms
step:1876/2160 train_time:106289ms step_avg:56.66ms
step:1877/2160 train_time:106377ms step_avg:56.67ms
step:1878/2160 train_time:106464ms step_avg:56.69ms
step:1879/2160 train_time:106552ms step_avg:56.71ms
step:1880/2160 train_time:106639ms step_avg:56.72ms
step:1881/2160 train_time:106728ms step_avg:56.74ms
step:1882/2160 train_time:106815ms step_avg:56.76ms
step:1883/2160 train_time:106902ms step_avg:56.77ms
step:1884/2160 train_time:106990ms step_avg:56.79ms
step:1885/2160 train_time:107077ms step_avg:56.81ms
step:1886/2160 train_time:107164ms step_avg:56.82ms
step:1887/2160 train_time:107252ms step_avg:56.84ms
step:1888/2160 train_time:107339ms step_avg:56.85ms
step:1889/2160 train_time:107428ms step_avg:56.87ms
step:1890/2160 train_time:107515ms step_avg:56.89ms
step:1891/2160 train_time:107604ms step_avg:56.90ms
step:1892/2160 train_time:107690ms step_avg:56.92ms
step:1893/2160 train_time:107778ms step_avg:56.94ms
step:1894/2160 train_time:107864ms step_avg:56.95ms
step:1895/2160 train_time:107954ms step_avg:56.97ms
step:1896/2160 train_time:108040ms step_avg:56.98ms
step:1897/2160 train_time:108128ms step_avg:57.00ms
step:1898/2160 train_time:108215ms step_avg:57.02ms
step:1899/2160 train_time:108304ms step_avg:57.03ms
step:1900/2160 train_time:108390ms step_avg:57.05ms
step:1901/2160 train_time:108479ms step_avg:57.06ms
step:1902/2160 train_time:108566ms step_avg:57.08ms
step:1903/2160 train_time:108654ms step_avg:57.10ms
step:1904/2160 train_time:108740ms step_avg:57.11ms
step:1905/2160 train_time:108829ms step_avg:57.13ms
step:1906/2160 train_time:108915ms step_avg:57.14ms
step:1907/2160 train_time:109003ms step_avg:57.16ms
step:1908/2160 train_time:109090ms step_avg:57.17ms
step:1909/2160 train_time:109178ms step_avg:57.19ms
step:1910/2160 train_time:109264ms step_avg:57.21ms
step:1911/2160 train_time:109352ms step_avg:57.22ms
step:1912/2160 train_time:109439ms step_avg:57.24ms
step:1913/2160 train_time:109528ms step_avg:57.25ms
step:1914/2160 train_time:109614ms step_avg:57.27ms
step:1915/2160 train_time:109703ms step_avg:57.29ms
step:1916/2160 train_time:109790ms step_avg:57.30ms
step:1917/2160 train_time:109878ms step_avg:57.32ms
step:1918/2160 train_time:109964ms step_avg:57.33ms
step:1919/2160 train_time:110052ms step_avg:57.35ms
step:1920/2160 train_time:110139ms step_avg:57.36ms
step:1921/2160 train_time:110227ms step_avg:57.38ms
step:1922/2160 train_time:110313ms step_avg:57.39ms
step:1923/2160 train_time:110401ms step_avg:57.41ms
step:1924/2160 train_time:110488ms step_avg:57.43ms
step:1925/2160 train_time:110576ms step_avg:57.44ms
step:1926/2160 train_time:110662ms step_avg:57.46ms
step:1927/2160 train_time:110750ms step_avg:57.47ms
step:1928/2160 train_time:110836ms step_avg:57.49ms
step:1929/2160 train_time:110925ms step_avg:57.50ms
step:1930/2160 train_time:111012ms step_avg:57.52ms
step:1931/2160 train_time:111100ms step_avg:57.53ms
step:1932/2160 train_time:111186ms step_avg:57.55ms
step:1933/2160 train_time:111275ms step_avg:57.57ms
step:1934/2160 train_time:111361ms step_avg:57.58ms
step:1935/2160 train_time:111450ms step_avg:57.60ms
step:1936/2160 train_time:111536ms step_avg:57.61ms
step:1937/2160 train_time:111625ms step_avg:57.63ms
step:1938/2160 train_time:111712ms step_avg:57.64ms
step:1939/2160 train_time:111800ms step_avg:57.66ms
step:1940/2160 train_time:111887ms step_avg:57.67ms
step:1941/2160 train_time:111976ms step_avg:57.69ms
step:1942/2160 train_time:112062ms step_avg:57.70ms
step:1943/2160 train_time:112151ms step_avg:57.72ms
step:1944/2160 train_time:112237ms step_avg:57.74ms
step:1945/2160 train_time:112326ms step_avg:57.75ms
step:1946/2160 train_time:112412ms step_avg:57.77ms
step:1947/2160 train_time:112500ms step_avg:57.78ms
step:1948/2160 train_time:112587ms step_avg:57.80ms
step:1949/2160 train_time:112675ms step_avg:57.81ms
step:1950/2160 train_time:112761ms step_avg:57.83ms
step:1951/2160 train_time:112850ms step_avg:57.84ms
step:1952/2160 train_time:112937ms step_avg:57.86ms
step:1953/2160 train_time:113025ms step_avg:57.87ms
step:1954/2160 train_time:113112ms step_avg:57.89ms
step:1955/2160 train_time:113200ms step_avg:57.90ms
step:1956/2160 train_time:113286ms step_avg:57.92ms
step:1957/2160 train_time:113374ms step_avg:57.93ms
step:1958/2160 train_time:113461ms step_avg:57.95ms
step:1959/2160 train_time:113549ms step_avg:57.96ms
step:1960/2160 train_time:113635ms step_avg:57.98ms
step:1961/2160 train_time:113724ms step_avg:57.99ms
step:1962/2160 train_time:113810ms step_avg:58.01ms
step:1963/2160 train_time:113899ms step_avg:58.02ms
step:1964/2160 train_time:113985ms step_avg:58.04ms
step:1965/2160 train_time:114073ms step_avg:58.05ms
step:1966/2160 train_time:114160ms step_avg:58.07ms
step:1967/2160 train_time:114249ms step_avg:58.08ms
step:1968/2160 train_time:114335ms step_avg:58.10ms
step:1969/2160 train_time:114423ms step_avg:58.11ms
step:1970/2160 train_time:114509ms step_avg:58.13ms
step:1971/2160 train_time:114598ms step_avg:58.14ms
step:1972/2160 train_time:114685ms step_avg:58.16ms
step:1973/2160 train_time:114773ms step_avg:58.17ms
step:1974/2160 train_time:114860ms step_avg:58.19ms
step:1975/2160 train_time:114948ms step_avg:58.20ms
step:1976/2160 train_time:115035ms step_avg:58.22ms
step:1977/2160 train_time:115124ms step_avg:58.23ms
step:1978/2160 train_time:115209ms step_avg:58.25ms
step:1979/2160 train_time:115299ms step_avg:58.26ms
step:1980/2160 train_time:115385ms step_avg:58.28ms
step:1981/2160 train_time:115474ms step_avg:58.29ms
step:1982/2160 train_time:115560ms step_avg:58.30ms
step:1983/2160 train_time:115648ms step_avg:58.32ms
step:1984/2160 train_time:115735ms step_avg:58.33ms
step:1985/2160 train_time:115823ms step_avg:58.35ms
step:1986/2160 train_time:115910ms step_avg:58.36ms
step:1987/2160 train_time:115998ms step_avg:58.38ms
step:1988/2160 train_time:116085ms step_avg:58.39ms
step:1989/2160 train_time:116173ms step_avg:58.41ms
step:1990/2160 train_time:116260ms step_avg:58.42ms
step:1991/2160 train_time:116348ms step_avg:58.44ms
step:1992/2160 train_time:116434ms step_avg:58.45ms
step:1993/2160 train_time:116523ms step_avg:58.47ms
step:1994/2160 train_time:116610ms step_avg:58.48ms
step:1995/2160 train_time:116698ms step_avg:58.50ms
step:1996/2160 train_time:116784ms step_avg:58.51ms
step:1997/2160 train_time:116872ms step_avg:58.52ms
step:1998/2160 train_time:116959ms step_avg:58.54ms
step:1999/2160 train_time:117047ms step_avg:58.55ms
step:2000/2160 train_time:117133ms step_avg:58.57ms
step:2000/2160 val_loss:3.3110 train_time:117223ms step_avg:58.61ms
step:2001/2160 train_time:117247ms step_avg:58.59ms
step:2002/2160 train_time:117312ms step_avg:58.60ms
step:2003/2160 train_time:117405ms step_avg:58.61ms
step:2004/2160 train_time:117492ms step_avg:58.63ms
step:2005/2160 train_time:117580ms step_avg:58.64ms
step:2006/2160 train_time:117665ms step_avg:58.66ms
step:2007/2160 train_time:117753ms step_avg:58.67ms
step:2008/2160 train_time:117838ms step_avg:58.68ms
step:2009/2160 train_time:117925ms step_avg:58.70ms
step:2010/2160 train_time:118011ms step_avg:58.71ms
step:2011/2160 train_time:118098ms step_avg:58.73ms
step:2012/2160 train_time:118185ms step_avg:58.74ms
step:2013/2160 train_time:118275ms step_avg:58.76ms
step:2014/2160 train_time:118362ms step_avg:58.77ms
step:2015/2160 train_time:118452ms step_avg:58.78ms
step:2016/2160 train_time:118538ms step_avg:58.80ms
step:2017/2160 train_time:118627ms step_avg:58.81ms
step:2018/2160 train_time:118712ms step_avg:58.83ms
step:2019/2160 train_time:118800ms step_avg:58.84ms
step:2020/2160 train_time:118885ms step_avg:58.85ms
step:2021/2160 train_time:118973ms step_avg:58.87ms
step:2022/2160 train_time:119059ms step_avg:58.88ms
step:2023/2160 train_time:119147ms step_avg:58.90ms
step:2024/2160 train_time:119235ms step_avg:58.91ms
step:2025/2160 train_time:119324ms step_avg:58.93ms
step:2026/2160 train_time:119412ms step_avg:58.94ms
step:2027/2160 train_time:119501ms step_avg:58.95ms
step:2028/2160 train_time:119589ms step_avg:58.97ms
step:2029/2160 train_time:119677ms step_avg:58.98ms
step:2030/2160 train_time:119764ms step_avg:59.00ms
step:2031/2160 train_time:119853ms step_avg:59.01ms
step:2032/2160 train_time:119937ms step_avg:59.02ms
step:2033/2160 train_time:120024ms step_avg:59.04ms
step:2034/2160 train_time:120109ms step_avg:59.05ms
step:2035/2160 train_time:120199ms step_avg:59.07ms
step:2036/2160 train_time:120286ms step_avg:59.08ms
step:2037/2160 train_time:120376ms step_avg:59.09ms
step:2038/2160 train_time:120463ms step_avg:59.11ms
step:2039/2160 train_time:120551ms step_avg:59.12ms
step:2040/2160 train_time:120638ms step_avg:59.14ms
step:2041/2160 train_time:120727ms step_avg:59.15ms
step:2042/2160 train_time:120813ms step_avg:59.16ms
step:2043/2160 train_time:120901ms step_avg:59.18ms
step:2044/2160 train_time:120987ms step_avg:59.19ms
step:2045/2160 train_time:121074ms step_avg:59.21ms
step:2046/2160 train_time:121161ms step_avg:59.22ms
step:2047/2160 train_time:121250ms step_avg:59.23ms
step:2048/2160 train_time:121336ms step_avg:59.25ms
step:2049/2160 train_time:121426ms step_avg:59.26ms
step:2050/2160 train_time:121512ms step_avg:59.27ms
step:2051/2160 train_time:121601ms step_avg:59.29ms
step:2052/2160 train_time:121688ms step_avg:59.30ms
step:2053/2160 train_time:121777ms step_avg:59.32ms
step:2054/2160 train_time:121864ms step_avg:59.33ms
step:2055/2160 train_time:121952ms step_avg:59.34ms
step:2056/2160 train_time:122038ms step_avg:59.36ms
step:2057/2160 train_time:122126ms step_avg:59.37ms
step:2058/2160 train_time:122212ms step_avg:59.38ms
step:2059/2160 train_time:122300ms step_avg:59.40ms
step:2060/2160 train_time:122387ms step_avg:59.41ms
step:2061/2160 train_time:122476ms step_avg:59.43ms
step:2062/2160 train_time:122563ms step_avg:59.44ms
step:2063/2160 train_time:122651ms step_avg:59.45ms
step:2064/2160 train_time:122738ms step_avg:59.47ms
step:2065/2160 train_time:122825ms step_avg:59.48ms
step:2066/2160 train_time:122911ms step_avg:59.49ms
step:2067/2160 train_time:123000ms step_avg:59.51ms
step:2068/2160 train_time:123086ms step_avg:59.52ms
step:2069/2160 train_time:123174ms step_avg:59.53ms
step:2070/2160 train_time:123261ms step_avg:59.55ms
step:2071/2160 train_time:123350ms step_avg:59.56ms
step:2072/2160 train_time:123437ms step_avg:59.57ms
step:2073/2160 train_time:123525ms step_avg:59.59ms
step:2074/2160 train_time:123613ms step_avg:59.60ms
step:2075/2160 train_time:123701ms step_avg:59.61ms
step:2076/2160 train_time:123787ms step_avg:59.63ms
step:2077/2160 train_time:123876ms step_avg:59.64ms
step:2078/2160 train_time:123962ms step_avg:59.65ms
step:2079/2160 train_time:124050ms step_avg:59.67ms
step:2080/2160 train_time:124137ms step_avg:59.68ms
step:2081/2160 train_time:124225ms step_avg:59.70ms
step:2082/2160 train_time:124312ms step_avg:59.71ms
step:2083/2160 train_time:124400ms step_avg:59.72ms
step:2084/2160 train_time:124487ms step_avg:59.73ms
step:2085/2160 train_time:124576ms step_avg:59.75ms
step:2086/2160 train_time:124663ms step_avg:59.76ms
step:2087/2160 train_time:124752ms step_avg:59.78ms
step:2088/2160 train_time:124838ms step_avg:59.79ms
step:2089/2160 train_time:124926ms step_avg:59.80ms
step:2090/2160 train_time:125012ms step_avg:59.81ms
step:2091/2160 train_time:125101ms step_avg:59.83ms
step:2092/2160 train_time:125188ms step_avg:59.84ms
step:2093/2160 train_time:125276ms step_avg:59.85ms
step:2094/2160 train_time:125362ms step_avg:59.87ms
step:2095/2160 train_time:125451ms step_avg:59.88ms
step:2096/2160 train_time:125539ms step_avg:59.89ms
step:2097/2160 train_time:125628ms step_avg:59.91ms
step:2098/2160 train_time:125714ms step_avg:59.92ms
step:2099/2160 train_time:125803ms step_avg:59.93ms
step:2100/2160 train_time:125890ms step_avg:59.95ms
step:2101/2160 train_time:125978ms step_avg:59.96ms
step:2102/2160 train_time:126065ms step_avg:59.97ms
step:2103/2160 train_time:126153ms step_avg:59.99ms
step:2104/2160 train_time:126239ms step_avg:60.00ms
step:2105/2160 train_time:126328ms step_avg:60.01ms
step:2106/2160 train_time:126415ms step_avg:60.03ms
step:2107/2160 train_time:126505ms step_avg:60.04ms
step:2108/2160 train_time:126591ms step_avg:60.05ms
step:2109/2160 train_time:126680ms step_avg:60.07ms
step:2110/2160 train_time:126766ms step_avg:60.08ms
step:2111/2160 train_time:126855ms step_avg:60.09ms
step:2112/2160 train_time:126942ms step_avg:60.11ms
step:2113/2160 train_time:127030ms step_avg:60.12ms
step:2114/2160 train_time:127116ms step_avg:60.13ms
step:2115/2160 train_time:127204ms step_avg:60.14ms
step:2116/2160 train_time:127290ms step_avg:60.16ms
step:2117/2160 train_time:127380ms step_avg:60.17ms
step:2118/2160 train_time:127467ms step_avg:60.18ms
step:2119/2160 train_time:127554ms step_avg:60.20ms
step:2120/2160 train_time:127640ms step_avg:60.21ms
step:2121/2160 train_time:127729ms step_avg:60.22ms
step:2122/2160 train_time:127816ms step_avg:60.23ms
step:2123/2160 train_time:127905ms step_avg:60.25ms
step:2124/2160 train_time:127991ms step_avg:60.26ms
step:2125/2160 train_time:128080ms step_avg:60.27ms
step:2126/2160 train_time:128167ms step_avg:60.29ms
step:2127/2160 train_time:128255ms step_avg:60.30ms
step:2128/2160 train_time:128343ms step_avg:60.31ms
step:2129/2160 train_time:128431ms step_avg:60.32ms
step:2130/2160 train_time:128518ms step_avg:60.34ms
step:2131/2160 train_time:128606ms step_avg:60.35ms
step:2132/2160 train_time:128693ms step_avg:60.36ms
step:2133/2160 train_time:128781ms step_avg:60.38ms
step:2134/2160 train_time:128868ms step_avg:60.39ms
step:2135/2160 train_time:128957ms step_avg:60.40ms
step:2136/2160 train_time:129044ms step_avg:60.41ms
step:2137/2160 train_time:129132ms step_avg:60.43ms
step:2138/2160 train_time:129218ms step_avg:60.44ms
step:2139/2160 train_time:129308ms step_avg:60.45ms
step:2140/2160 train_time:129394ms step_avg:60.46ms
step:2141/2160 train_time:129482ms step_avg:60.48ms
step:2142/2160 train_time:129569ms step_avg:60.49ms
step:2143/2160 train_time:129658ms step_avg:60.50ms
step:2144/2160 train_time:129745ms step_avg:60.52ms
step:2145/2160 train_time:129833ms step_avg:60.53ms
step:2146/2160 train_time:129920ms step_avg:60.54ms
step:2147/2160 train_time:130009ms step_avg:60.55ms
step:2148/2160 train_time:130095ms step_avg:60.57ms
step:2149/2160 train_time:130184ms step_avg:60.58ms
step:2150/2160 train_time:130271ms step_avg:60.59ms
step:2151/2160 train_time:130360ms step_avg:60.60ms
step:2152/2160 train_time:130446ms step_avg:60.62ms
step:2153/2160 train_time:130535ms step_avg:60.63ms
step:2154/2160 train_time:130622ms step_avg:60.64ms
step:2155/2160 train_time:130710ms step_avg:60.65ms
step:2156/2160 train_time:130796ms step_avg:60.67ms
step:2157/2160 train_time:130886ms step_avg:60.68ms
step:2158/2160 train_time:130972ms step_avg:60.69ms
step:2159/2160 train_time:131061ms step_avg:60.70ms
step:2160/2160 train_time:131149ms step_avg:60.72ms
step:2160/2160 val_loss:3.2785 train_time:131239ms step_avg:60.76ms
peak memory allocated: 30032 MiB reserved: 44716 MiB

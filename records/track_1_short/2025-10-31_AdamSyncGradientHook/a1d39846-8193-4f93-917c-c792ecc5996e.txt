import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)


            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))



            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal
        self.group_to_param_group = {}

        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                self.group_to_param_group[param] = group

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        world_size = dist.get_world_size()
        grad = param.grad
        rank_size = grad.shape[0] // world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(), grad_slice)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']

            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
        # enable sync in the next training step for the adam optimizer
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = next(train_loader)

        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()


====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251031+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 31 16:00:43 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   39C    P0            128W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   32C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2315 train_time:78ms step_avg:78.11ms
step:2/2315 train_time:187ms step_avg:93.45ms
step:3/2315 train_time:206ms step_avg:68.56ms
step:4/2315 train_time:244ms step_avg:61.00ms
step:5/2315 train_time:302ms step_avg:60.31ms
step:6/2315 train_time:361ms step_avg:60.19ms
step:7/2315 train_time:421ms step_avg:60.08ms
step:8/2315 train_time:481ms step_avg:60.07ms
step:9/2315 train_time:540ms step_avg:59.95ms
step:10/2315 train_time:600ms step_avg:59.96ms
step:11/2315 train_time:659ms step_avg:59.88ms
step:12/2315 train_time:719ms step_avg:59.92ms
step:13/2315 train_time:777ms step_avg:59.79ms
step:14/2315 train_time:837ms step_avg:59.78ms
step:15/2315 train_time:896ms step_avg:59.73ms
step:16/2315 train_time:956ms step_avg:59.74ms
step:17/2315 train_time:1015ms step_avg:59.72ms
step:18/2315 train_time:1078ms step_avg:59.91ms
step:19/2315 train_time:1143ms step_avg:60.17ms
step:20/2315 train_time:1207ms step_avg:60.34ms
step:21/2315 train_time:1268ms step_avg:60.38ms
step:22/2315 train_time:1328ms step_avg:60.39ms
step:23/2315 train_time:1388ms step_avg:60.36ms
step:24/2315 train_time:1449ms step_avg:60.38ms
step:25/2315 train_time:1508ms step_avg:60.33ms
step:26/2315 train_time:1569ms step_avg:60.35ms
step:27/2315 train_time:1628ms step_avg:60.31ms
step:28/2315 train_time:1689ms step_avg:60.32ms
step:29/2315 train_time:1749ms step_avg:60.30ms
step:30/2315 train_time:1809ms step_avg:60.31ms
step:31/2315 train_time:1869ms step_avg:60.30ms
step:32/2315 train_time:1930ms step_avg:60.31ms
step:33/2315 train_time:1991ms step_avg:60.33ms
step:34/2315 train_time:2052ms step_avg:60.37ms
step:35/2315 train_time:2114ms step_avg:60.40ms
step:36/2315 train_time:2176ms step_avg:60.44ms
step:37/2315 train_time:2236ms step_avg:60.44ms
step:38/2315 train_time:2297ms step_avg:60.45ms
step:39/2315 train_time:2357ms step_avg:60.42ms
step:40/2315 train_time:2417ms step_avg:60.42ms
step:41/2315 train_time:2476ms step_avg:60.39ms
step:42/2315 train_time:2537ms step_avg:60.39ms
step:43/2315 train_time:2596ms step_avg:60.38ms
step:44/2315 train_time:2657ms step_avg:60.38ms
step:45/2315 train_time:2716ms step_avg:60.37ms
step:46/2315 train_time:2777ms step_avg:60.37ms
step:47/2315 train_time:2837ms step_avg:60.37ms
step:48/2315 train_time:2898ms step_avg:60.37ms
step:49/2315 train_time:2958ms step_avg:60.36ms
step:50/2315 train_time:3019ms step_avg:60.38ms
step:51/2315 train_time:3079ms step_avg:60.36ms
step:52/2315 train_time:3139ms step_avg:60.37ms
step:53/2315 train_time:3201ms step_avg:60.39ms
step:54/2315 train_time:3262ms step_avg:60.41ms
step:55/2315 train_time:3323ms step_avg:60.42ms
step:56/2315 train_time:3384ms step_avg:60.42ms
step:57/2315 train_time:3444ms step_avg:60.42ms
step:58/2315 train_time:3505ms step_avg:60.43ms
step:59/2315 train_time:3566ms step_avg:60.44ms
step:60/2315 train_time:3627ms step_avg:60.45ms
step:61/2315 train_time:3687ms step_avg:60.45ms
step:62/2315 train_time:3748ms step_avg:60.44ms
step:63/2315 train_time:3808ms step_avg:60.44ms
step:64/2315 train_time:3869ms step_avg:60.45ms
step:65/2315 train_time:3928ms step_avg:60.44ms
step:66/2315 train_time:3989ms step_avg:60.44ms
step:67/2315 train_time:4049ms step_avg:60.43ms
step:68/2315 train_time:4110ms step_avg:60.43ms
step:69/2315 train_time:4170ms step_avg:60.43ms
step:70/2315 train_time:4232ms step_avg:60.45ms
step:71/2315 train_time:4292ms step_avg:60.45ms
step:72/2315 train_time:4352ms step_avg:60.45ms
step:73/2315 train_time:4413ms step_avg:60.45ms
step:74/2315 train_time:4474ms step_avg:60.45ms
step:75/2315 train_time:4533ms step_avg:60.44ms
step:76/2315 train_time:4593ms step_avg:60.44ms
step:77/2315 train_time:4652ms step_avg:60.42ms
step:78/2315 train_time:4712ms step_avg:60.41ms
step:79/2315 train_time:4771ms step_avg:60.40ms
step:80/2315 train_time:4831ms step_avg:60.39ms
step:81/2315 train_time:4891ms step_avg:60.38ms
step:82/2315 train_time:4952ms step_avg:60.39ms
step:83/2315 train_time:5012ms step_avg:60.38ms
step:84/2315 train_time:5072ms step_avg:60.38ms
step:85/2315 train_time:5132ms step_avg:60.37ms
step:86/2315 train_time:5192ms step_avg:60.38ms
step:87/2315 train_time:5253ms step_avg:60.37ms
step:88/2315 train_time:5313ms step_avg:60.38ms
step:89/2315 train_time:5374ms step_avg:60.38ms
step:90/2315 train_time:5434ms step_avg:60.38ms
step:91/2315 train_time:5494ms step_avg:60.37ms
step:92/2315 train_time:5554ms step_avg:60.37ms
step:93/2315 train_time:5613ms step_avg:60.36ms
step:94/2315 train_time:5673ms step_avg:60.35ms
step:95/2315 train_time:5732ms step_avg:60.34ms
step:96/2315 train_time:5792ms step_avg:60.33ms
step:97/2315 train_time:5851ms step_avg:60.32ms
step:98/2315 train_time:5912ms step_avg:60.32ms
step:99/2315 train_time:5971ms step_avg:60.32ms
step:100/2315 train_time:6032ms step_avg:60.32ms
step:101/2315 train_time:6092ms step_avg:60.32ms
step:102/2315 train_time:6152ms step_avg:60.31ms
step:103/2315 train_time:6212ms step_avg:60.31ms
step:104/2315 train_time:6273ms step_avg:60.32ms
step:105/2315 train_time:6333ms step_avg:60.31ms
step:106/2315 train_time:6393ms step_avg:60.31ms
step:107/2315 train_time:6453ms step_avg:60.31ms
step:108/2315 train_time:6513ms step_avg:60.31ms
step:109/2315 train_time:6573ms step_avg:60.30ms
step:110/2315 train_time:6633ms step_avg:60.30ms
step:111/2315 train_time:6693ms step_avg:60.29ms
step:112/2315 train_time:6752ms step_avg:60.29ms
step:113/2315 train_time:6812ms step_avg:60.28ms
step:114/2315 train_time:6871ms step_avg:60.28ms
step:115/2315 train_time:6931ms step_avg:60.27ms
step:116/2315 train_time:6992ms step_avg:60.27ms
step:117/2315 train_time:7052ms step_avg:60.27ms
step:118/2315 train_time:7112ms step_avg:60.27ms
step:119/2315 train_time:7172ms step_avg:60.27ms
step:120/2315 train_time:7233ms step_avg:60.27ms
step:121/2315 train_time:7293ms step_avg:60.27ms
step:122/2315 train_time:7354ms step_avg:60.28ms
step:123/2315 train_time:7413ms step_avg:60.27ms
step:124/2315 train_time:7474ms step_avg:60.27ms
step:125/2315 train_time:7533ms step_avg:60.27ms
step:126/2315 train_time:7594ms step_avg:60.27ms
step:127/2315 train_time:7653ms step_avg:60.26ms
step:128/2315 train_time:7713ms step_avg:60.26ms
step:129/2315 train_time:7772ms step_avg:60.25ms
step:130/2315 train_time:7833ms step_avg:60.25ms
step:131/2315 train_time:7892ms step_avg:60.25ms
step:132/2315 train_time:7952ms step_avg:60.25ms
step:133/2315 train_time:8012ms step_avg:60.24ms
step:134/2315 train_time:8073ms step_avg:60.24ms
step:135/2315 train_time:8133ms step_avg:60.24ms
step:136/2315 train_time:8193ms step_avg:60.24ms
step:137/2315 train_time:8253ms step_avg:60.24ms
step:138/2315 train_time:8314ms step_avg:60.25ms
step:139/2315 train_time:8374ms step_avg:60.24ms
step:140/2315 train_time:8434ms step_avg:60.24ms
step:141/2315 train_time:8493ms step_avg:60.24ms
step:142/2315 train_time:8553ms step_avg:60.23ms
step:143/2315 train_time:8612ms step_avg:60.23ms
step:144/2315 train_time:8672ms step_avg:60.23ms
step:145/2315 train_time:8732ms step_avg:60.22ms
step:146/2315 train_time:8792ms step_avg:60.22ms
step:147/2315 train_time:8851ms step_avg:60.21ms
step:148/2315 train_time:8911ms step_avg:60.21ms
step:149/2315 train_time:8970ms step_avg:60.20ms
step:150/2315 train_time:9031ms step_avg:60.21ms
step:151/2315 train_time:9090ms step_avg:60.20ms
step:152/2315 train_time:9150ms step_avg:60.20ms
step:153/2315 train_time:9210ms step_avg:60.20ms
step:154/2315 train_time:9271ms step_avg:60.20ms
step:155/2315 train_time:9331ms step_avg:60.20ms
step:156/2315 train_time:9392ms step_avg:60.20ms
step:157/2315 train_time:9452ms step_avg:60.20ms
step:158/2315 train_time:9512ms step_avg:60.20ms
step:159/2315 train_time:9572ms step_avg:60.20ms
step:160/2315 train_time:9632ms step_avg:60.20ms
step:161/2315 train_time:9691ms step_avg:60.19ms
step:162/2315 train_time:9751ms step_avg:60.19ms
step:163/2315 train_time:9810ms step_avg:60.19ms
step:164/2315 train_time:9870ms step_avg:60.19ms
step:165/2315 train_time:9930ms step_avg:60.18ms
step:166/2315 train_time:9991ms step_avg:60.19ms
step:167/2315 train_time:10051ms step_avg:60.18ms
step:168/2315 train_time:10111ms step_avg:60.19ms
step:169/2315 train_time:10171ms step_avg:60.18ms
step:170/2315 train_time:10232ms step_avg:60.19ms
step:171/2315 train_time:10292ms step_avg:60.19ms
step:172/2315 train_time:10352ms step_avg:60.19ms
step:173/2315 train_time:10412ms step_avg:60.19ms
step:174/2315 train_time:10472ms step_avg:60.18ms
step:175/2315 train_time:10533ms step_avg:60.19ms
step:176/2315 train_time:10593ms step_avg:60.19ms
step:177/2315 train_time:10652ms step_avg:60.18ms
step:178/2315 train_time:10712ms step_avg:60.18ms
step:179/2315 train_time:10772ms step_avg:60.18ms
step:180/2315 train_time:10832ms step_avg:60.18ms
step:181/2315 train_time:10891ms step_avg:60.17ms
step:182/2315 train_time:10952ms step_avg:60.17ms
step:183/2315 train_time:11011ms step_avg:60.17ms
step:184/2315 train_time:11071ms step_avg:60.17ms
step:185/2315 train_time:11131ms step_avg:60.17ms
step:186/2315 train_time:11191ms step_avg:60.17ms
step:187/2315 train_time:11250ms step_avg:60.16ms
step:188/2315 train_time:11311ms step_avg:60.16ms
step:189/2315 train_time:11370ms step_avg:60.16ms
step:190/2315 train_time:11431ms step_avg:60.16ms
step:191/2315 train_time:11491ms step_avg:60.16ms
step:192/2315 train_time:11551ms step_avg:60.16ms
step:193/2315 train_time:11610ms step_avg:60.16ms
step:194/2315 train_time:11671ms step_avg:60.16ms
step:195/2315 train_time:11731ms step_avg:60.16ms
step:196/2315 train_time:11792ms step_avg:60.16ms
step:197/2315 train_time:11851ms step_avg:60.16ms
step:198/2315 train_time:11911ms step_avg:60.16ms
step:199/2315 train_time:11971ms step_avg:60.15ms
step:200/2315 train_time:12031ms step_avg:60.15ms
step:201/2315 train_time:12090ms step_avg:60.15ms
step:202/2315 train_time:12150ms step_avg:60.15ms
step:203/2315 train_time:12211ms step_avg:60.15ms
step:204/2315 train_time:12271ms step_avg:60.15ms
step:205/2315 train_time:12330ms step_avg:60.15ms
step:206/2315 train_time:12391ms step_avg:60.15ms
step:207/2315 train_time:12450ms step_avg:60.15ms
step:208/2315 train_time:12511ms step_avg:60.15ms
step:209/2315 train_time:12571ms step_avg:60.15ms
step:210/2315 train_time:12632ms step_avg:60.15ms
step:211/2315 train_time:12691ms step_avg:60.15ms
step:212/2315 train_time:12752ms step_avg:60.15ms
step:213/2315 train_time:12811ms step_avg:60.14ms
step:214/2315 train_time:12871ms step_avg:60.14ms
step:215/2315 train_time:12930ms step_avg:60.14ms
step:216/2315 train_time:12990ms step_avg:60.14ms
step:217/2315 train_time:13050ms step_avg:60.14ms
step:218/2315 train_time:13110ms step_avg:60.14ms
step:219/2315 train_time:13170ms step_avg:60.14ms
step:220/2315 train_time:13230ms step_avg:60.14ms
step:221/2315 train_time:13290ms step_avg:60.14ms
step:222/2315 train_time:13350ms step_avg:60.14ms
step:223/2315 train_time:13410ms step_avg:60.13ms
step:224/2315 train_time:13470ms step_avg:60.14ms
step:225/2315 train_time:13530ms step_avg:60.13ms
step:226/2315 train_time:13590ms step_avg:60.13ms
step:227/2315 train_time:13650ms step_avg:60.13ms
step:228/2315 train_time:13710ms step_avg:60.13ms
step:229/2315 train_time:13770ms step_avg:60.13ms
step:230/2315 train_time:13830ms step_avg:60.13ms
step:231/2315 train_time:13889ms step_avg:60.13ms
step:232/2315 train_time:13949ms step_avg:60.13ms
step:233/2315 train_time:14010ms step_avg:60.13ms
step:234/2315 train_time:14070ms step_avg:60.13ms
step:235/2315 train_time:14129ms step_avg:60.12ms
step:236/2315 train_time:14189ms step_avg:60.12ms
step:237/2315 train_time:14249ms step_avg:60.12ms
step:238/2315 train_time:14309ms step_avg:60.12ms
step:239/2315 train_time:14368ms step_avg:60.12ms
step:240/2315 train_time:14429ms step_avg:60.12ms
step:241/2315 train_time:14489ms step_avg:60.12ms
step:242/2315 train_time:14549ms step_avg:60.12ms
step:243/2315 train_time:14609ms step_avg:60.12ms
step:244/2315 train_time:14669ms step_avg:60.12ms
step:245/2315 train_time:14729ms step_avg:60.12ms
step:246/2315 train_time:14790ms step_avg:60.12ms
step:247/2315 train_time:14849ms step_avg:60.12ms
step:248/2315 train_time:14910ms step_avg:60.12ms
step:249/2315 train_time:14969ms step_avg:60.12ms
step:250/2315 train_time:15029ms step_avg:60.12ms
step:250/2315 val_loss:4.0660 train_time:15091ms step_avg:60.36ms
step:251/2315 train_time:15111ms step_avg:60.20ms
step:252/2315 train_time:15151ms step_avg:60.12ms
step:253/2315 train_time:15217ms step_avg:60.15ms
step:254/2315 train_time:15281ms step_avg:60.16ms
step:255/2315 train_time:15343ms step_avg:60.17ms
step:256/2315 train_time:15404ms step_avg:60.17ms
step:257/2315 train_time:15463ms step_avg:60.17ms
step:258/2315 train_time:15524ms step_avg:60.17ms
step:259/2315 train_time:15584ms step_avg:60.17ms
step:260/2315 train_time:15644ms step_avg:60.17ms
step:261/2315 train_time:15702ms step_avg:60.16ms
step:262/2315 train_time:15761ms step_avg:60.16ms
step:263/2315 train_time:15820ms step_avg:60.15ms
step:264/2315 train_time:15879ms step_avg:60.15ms
step:265/2315 train_time:15938ms step_avg:60.14ms
step:266/2315 train_time:15997ms step_avg:60.14ms
step:267/2315 train_time:16057ms step_avg:60.14ms
step:268/2315 train_time:16118ms step_avg:60.14ms
step:269/2315 train_time:16180ms step_avg:60.15ms
step:270/2315 train_time:16244ms step_avg:60.16ms
step:271/2315 train_time:16306ms step_avg:60.17ms
step:272/2315 train_time:16367ms step_avg:60.17ms
step:273/2315 train_time:16428ms step_avg:60.17ms
step:274/2315 train_time:16487ms step_avg:60.17ms
step:275/2315 train_time:16547ms step_avg:60.17ms
step:276/2315 train_time:16607ms step_avg:60.17ms
step:277/2315 train_time:16665ms step_avg:60.16ms
step:278/2315 train_time:16725ms step_avg:60.16ms
step:279/2315 train_time:16784ms step_avg:60.16ms
step:280/2315 train_time:16844ms step_avg:60.16ms
step:281/2315 train_time:16903ms step_avg:60.15ms
step:282/2315 train_time:16963ms step_avg:60.15ms
step:283/2315 train_time:17022ms step_avg:60.15ms
step:284/2315 train_time:17083ms step_avg:60.15ms
step:285/2315 train_time:17144ms step_avg:60.15ms
step:286/2315 train_time:17205ms step_avg:60.16ms
step:287/2315 train_time:17267ms step_avg:60.16ms
step:288/2315 train_time:17328ms step_avg:60.17ms
step:289/2315 train_time:17389ms step_avg:60.17ms
step:290/2315 train_time:17449ms step_avg:60.17ms
step:291/2315 train_time:17508ms step_avg:60.16ms
step:292/2315 train_time:17568ms step_avg:60.16ms
step:293/2315 train_time:17627ms step_avg:60.16ms
step:294/2315 train_time:17686ms step_avg:60.16ms
step:295/2315 train_time:17745ms step_avg:60.15ms
step:296/2315 train_time:17805ms step_avg:60.15ms
step:297/2315 train_time:17864ms step_avg:60.15ms
step:298/2315 train_time:17924ms step_avg:60.15ms
step:299/2315 train_time:17984ms step_avg:60.15ms
step:300/2315 train_time:18045ms step_avg:60.15ms
step:301/2315 train_time:18105ms step_avg:60.15ms
step:302/2315 train_time:18166ms step_avg:60.15ms
step:303/2315 train_time:18227ms step_avg:60.15ms
step:304/2315 train_time:18288ms step_avg:60.16ms
step:305/2315 train_time:18348ms step_avg:60.16ms
step:306/2315 train_time:18408ms step_avg:60.16ms
step:307/2315 train_time:18469ms step_avg:60.16ms
step:308/2315 train_time:18529ms step_avg:60.16ms
step:309/2315 train_time:18589ms step_avg:60.16ms
step:310/2315 train_time:18649ms step_avg:60.16ms
step:311/2315 train_time:18708ms step_avg:60.15ms
step:312/2315 train_time:18767ms step_avg:60.15ms
step:313/2315 train_time:18826ms step_avg:60.15ms
step:314/2315 train_time:18887ms step_avg:60.15ms
step:315/2315 train_time:18947ms step_avg:60.15ms
step:316/2315 train_time:19007ms step_avg:60.15ms
step:317/2315 train_time:19067ms step_avg:60.15ms
step:318/2315 train_time:19128ms step_avg:60.15ms
step:319/2315 train_time:19188ms step_avg:60.15ms
step:320/2315 train_time:19249ms step_avg:60.15ms
step:321/2315 train_time:19309ms step_avg:60.15ms
step:322/2315 train_time:19370ms step_avg:60.16ms
step:323/2315 train_time:19430ms step_avg:60.15ms
step:324/2315 train_time:19490ms step_avg:60.16ms
step:325/2315 train_time:19550ms step_avg:60.15ms
step:326/2315 train_time:19610ms step_avg:60.15ms
step:327/2315 train_time:19670ms step_avg:60.15ms
step:328/2315 train_time:19730ms step_avg:60.15ms
step:329/2315 train_time:19789ms step_avg:60.15ms
step:330/2315 train_time:19850ms step_avg:60.15ms
step:331/2315 train_time:19909ms step_avg:60.15ms
step:332/2315 train_time:19969ms step_avg:60.15ms
step:333/2315 train_time:20029ms step_avg:60.15ms
step:334/2315 train_time:20090ms step_avg:60.15ms
step:335/2315 train_time:20150ms step_avg:60.15ms
step:336/2315 train_time:20211ms step_avg:60.15ms
step:337/2315 train_time:20271ms step_avg:60.15ms
step:338/2315 train_time:20331ms step_avg:60.15ms
step:339/2315 train_time:20391ms step_avg:60.15ms
step:340/2315 train_time:20451ms step_avg:60.15ms
step:341/2315 train_time:20510ms step_avg:60.15ms
step:342/2315 train_time:20570ms step_avg:60.15ms
step:343/2315 train_time:20629ms step_avg:60.14ms
step:344/2315 train_time:20689ms step_avg:60.14ms
step:345/2315 train_time:20748ms step_avg:60.14ms
step:346/2315 train_time:20809ms step_avg:60.14ms
step:347/2315 train_time:20868ms step_avg:60.14ms
step:348/2315 train_time:20928ms step_avg:60.14ms
step:349/2315 train_time:20987ms step_avg:60.14ms
step:350/2315 train_time:21048ms step_avg:60.14ms
step:351/2315 train_time:21108ms step_avg:60.14ms
step:352/2315 train_time:21168ms step_avg:60.14ms
step:353/2315 train_time:21228ms step_avg:60.14ms
step:354/2315 train_time:21289ms step_avg:60.14ms
step:355/2315 train_time:21349ms step_avg:60.14ms
step:356/2315 train_time:21410ms step_avg:60.14ms
step:357/2315 train_time:21469ms step_avg:60.14ms
step:358/2315 train_time:21529ms step_avg:60.14ms
step:359/2315 train_time:21588ms step_avg:60.13ms
step:360/2315 train_time:21648ms step_avg:60.13ms
step:361/2315 train_time:21707ms step_avg:60.13ms
step:362/2315 train_time:21768ms step_avg:60.13ms
step:363/2315 train_time:21827ms step_avg:60.13ms
step:364/2315 train_time:21887ms step_avg:60.13ms
step:365/2315 train_time:21947ms step_avg:60.13ms
step:366/2315 train_time:22007ms step_avg:60.13ms
step:367/2315 train_time:22067ms step_avg:60.13ms
step:368/2315 train_time:22127ms step_avg:60.13ms
step:369/2315 train_time:22187ms step_avg:60.13ms
step:370/2315 train_time:22248ms step_avg:60.13ms
step:371/2315 train_time:22308ms step_avg:60.13ms
step:372/2315 train_time:22368ms step_avg:60.13ms
step:373/2315 train_time:22428ms step_avg:60.13ms
step:374/2315 train_time:22488ms step_avg:60.13ms
step:375/2315 train_time:22547ms step_avg:60.13ms
step:376/2315 train_time:22607ms step_avg:60.13ms
step:377/2315 train_time:22666ms step_avg:60.12ms
step:378/2315 train_time:22726ms step_avg:60.12ms
step:379/2315 train_time:22785ms step_avg:60.12ms
step:380/2315 train_time:22845ms step_avg:60.12ms
step:381/2315 train_time:22905ms step_avg:60.12ms
step:382/2315 train_time:22965ms step_avg:60.12ms
step:383/2315 train_time:23025ms step_avg:60.12ms
step:384/2315 train_time:23085ms step_avg:60.12ms
step:385/2315 train_time:23145ms step_avg:60.12ms
step:386/2315 train_time:23205ms step_avg:60.12ms
step:387/2315 train_time:23265ms step_avg:60.12ms
step:388/2315 train_time:23326ms step_avg:60.12ms
step:389/2315 train_time:23386ms step_avg:60.12ms
step:390/2315 train_time:23447ms step_avg:60.12ms
step:391/2315 train_time:23506ms step_avg:60.12ms
step:392/2315 train_time:23567ms step_avg:60.12ms
step:393/2315 train_time:23626ms step_avg:60.12ms
step:394/2315 train_time:23686ms step_avg:60.12ms
step:395/2315 train_time:23746ms step_avg:60.12ms
step:396/2315 train_time:23806ms step_avg:60.12ms
step:397/2315 train_time:23866ms step_avg:60.12ms
step:398/2315 train_time:23926ms step_avg:60.12ms
step:399/2315 train_time:23986ms step_avg:60.11ms
step:400/2315 train_time:24046ms step_avg:60.11ms
step:401/2315 train_time:24105ms step_avg:60.11ms
step:402/2315 train_time:24166ms step_avg:60.11ms
step:403/2315 train_time:24226ms step_avg:60.11ms
step:404/2315 train_time:24287ms step_avg:60.12ms
step:405/2315 train_time:24347ms step_avg:60.12ms
step:406/2315 train_time:24407ms step_avg:60.12ms
step:407/2315 train_time:24467ms step_avg:60.12ms
step:408/2315 train_time:24527ms step_avg:60.12ms
step:409/2315 train_time:24587ms step_avg:60.11ms
step:410/2315 train_time:24647ms step_avg:60.12ms
step:411/2315 train_time:24707ms step_avg:60.11ms
step:412/2315 train_time:24768ms step_avg:60.12ms
step:413/2315 train_time:24827ms step_avg:60.11ms
step:414/2315 train_time:24887ms step_avg:60.11ms
step:415/2315 train_time:24947ms step_avg:60.11ms
step:416/2315 train_time:25007ms step_avg:60.11ms
step:417/2315 train_time:25067ms step_avg:60.11ms
step:418/2315 train_time:25127ms step_avg:60.11ms
step:419/2315 train_time:25187ms step_avg:60.11ms
step:420/2315 train_time:25247ms step_avg:60.11ms
step:421/2315 train_time:25307ms step_avg:60.11ms
step:422/2315 train_time:25368ms step_avg:60.11ms
step:423/2315 train_time:25427ms step_avg:60.11ms
step:424/2315 train_time:25488ms step_avg:60.11ms
step:425/2315 train_time:25548ms step_avg:60.11ms
step:426/2315 train_time:25608ms step_avg:60.11ms
step:427/2315 train_time:25668ms step_avg:60.11ms
step:428/2315 train_time:25728ms step_avg:60.11ms
step:429/2315 train_time:25787ms step_avg:60.11ms
step:430/2315 train_time:25847ms step_avg:60.11ms
step:431/2315 train_time:25907ms step_avg:60.11ms
step:432/2315 train_time:25967ms step_avg:60.11ms
step:433/2315 train_time:26027ms step_avg:60.11ms
step:434/2315 train_time:26087ms step_avg:60.11ms
step:435/2315 train_time:26147ms step_avg:60.11ms
step:436/2315 train_time:26207ms step_avg:60.11ms
step:437/2315 train_time:26267ms step_avg:60.11ms
step:438/2315 train_time:26327ms step_avg:60.11ms
step:439/2315 train_time:26387ms step_avg:60.11ms
step:440/2315 train_time:26448ms step_avg:60.11ms
step:441/2315 train_time:26507ms step_avg:60.11ms
step:442/2315 train_time:26567ms step_avg:60.11ms
step:443/2315 train_time:26627ms step_avg:60.11ms
step:444/2315 train_time:26687ms step_avg:60.11ms
step:445/2315 train_time:26747ms step_avg:60.10ms
step:446/2315 train_time:26807ms step_avg:60.11ms
step:447/2315 train_time:26866ms step_avg:60.10ms
step:448/2315 train_time:26927ms step_avg:60.10ms
step:449/2315 train_time:26987ms step_avg:60.10ms
step:450/2315 train_time:27047ms step_avg:60.10ms
step:451/2315 train_time:27106ms step_avg:60.10ms
step:452/2315 train_time:27166ms step_avg:60.10ms
step:453/2315 train_time:27226ms step_avg:60.10ms
step:454/2315 train_time:27286ms step_avg:60.10ms
step:455/2315 train_time:27346ms step_avg:60.10ms
step:456/2315 train_time:27407ms step_avg:60.10ms
step:457/2315 train_time:27467ms step_avg:60.10ms
step:458/2315 train_time:27527ms step_avg:60.10ms
step:459/2315 train_time:27586ms step_avg:60.10ms
step:460/2315 train_time:27647ms step_avg:60.10ms
step:461/2315 train_time:27706ms step_avg:60.10ms
step:462/2315 train_time:27767ms step_avg:60.10ms
step:463/2315 train_time:27826ms step_avg:60.10ms
step:464/2315 train_time:27886ms step_avg:60.10ms
step:465/2315 train_time:27946ms step_avg:60.10ms
step:466/2315 train_time:28006ms step_avg:60.10ms
step:467/2315 train_time:28066ms step_avg:60.10ms
step:468/2315 train_time:28126ms step_avg:60.10ms
step:469/2315 train_time:28186ms step_avg:60.10ms
step:470/2315 train_time:28247ms step_avg:60.10ms
step:471/2315 train_time:28306ms step_avg:60.10ms
step:472/2315 train_time:28367ms step_avg:60.10ms
step:473/2315 train_time:28426ms step_avg:60.10ms
step:474/2315 train_time:28487ms step_avg:60.10ms
step:475/2315 train_time:28546ms step_avg:60.10ms
step:476/2315 train_time:28607ms step_avg:60.10ms
step:477/2315 train_time:28666ms step_avg:60.10ms
step:478/2315 train_time:28727ms step_avg:60.10ms
step:479/2315 train_time:28786ms step_avg:60.10ms
step:480/2315 train_time:28846ms step_avg:60.10ms
step:481/2315 train_time:28906ms step_avg:60.10ms
step:482/2315 train_time:28966ms step_avg:60.09ms
step:483/2315 train_time:29026ms step_avg:60.09ms
step:484/2315 train_time:29085ms step_avg:60.09ms
step:485/2315 train_time:29145ms step_avg:60.09ms
step:486/2315 train_time:29205ms step_avg:60.09ms
step:487/2315 train_time:29265ms step_avg:60.09ms
step:488/2315 train_time:29326ms step_avg:60.09ms
step:489/2315 train_time:29385ms step_avg:60.09ms
step:490/2315 train_time:29446ms step_avg:60.09ms
step:491/2315 train_time:29506ms step_avg:60.09ms
step:492/2315 train_time:29567ms step_avg:60.09ms
step:493/2315 train_time:29626ms step_avg:60.09ms
step:494/2315 train_time:29687ms step_avg:60.09ms
step:495/2315 train_time:29747ms step_avg:60.09ms
step:496/2315 train_time:29807ms step_avg:60.10ms
step:497/2315 train_time:29867ms step_avg:60.10ms
step:498/2315 train_time:29928ms step_avg:60.10ms
step:499/2315 train_time:29987ms step_avg:60.09ms
step:500/2315 train_time:30047ms step_avg:60.09ms
step:500/2315 val_loss:3.8089 train_time:30109ms step_avg:60.22ms
step:501/2315 train_time:30126ms step_avg:60.13ms
step:502/2315 train_time:30169ms step_avg:60.10ms
step:503/2315 train_time:30233ms step_avg:60.11ms
step:504/2315 train_time:30297ms step_avg:60.11ms
step:505/2315 train_time:30358ms step_avg:60.11ms
step:506/2315 train_time:30418ms step_avg:60.12ms
step:507/2315 train_time:30477ms step_avg:60.11ms
step:508/2315 train_time:30537ms step_avg:60.11ms
step:509/2315 train_time:30597ms step_avg:60.11ms
step:510/2315 train_time:30657ms step_avg:60.11ms
step:511/2315 train_time:30716ms step_avg:60.11ms
step:512/2315 train_time:30776ms step_avg:60.11ms
step:513/2315 train_time:30835ms step_avg:60.11ms
step:514/2315 train_time:30895ms step_avg:60.11ms
step:515/2315 train_time:30953ms step_avg:60.10ms
step:516/2315 train_time:31013ms step_avg:60.10ms
step:517/2315 train_time:31074ms step_avg:60.11ms
step:518/2315 train_time:31136ms step_avg:60.11ms
step:519/2315 train_time:31198ms step_avg:60.11ms
step:520/2315 train_time:31260ms step_avg:60.11ms
step:521/2315 train_time:31320ms step_avg:60.12ms
step:522/2315 train_time:31381ms step_avg:60.12ms
step:523/2315 train_time:31441ms step_avg:60.12ms
step:524/2315 train_time:31501ms step_avg:60.12ms
step:525/2315 train_time:31560ms step_avg:60.11ms
step:526/2315 train_time:31620ms step_avg:60.11ms
step:527/2315 train_time:31680ms step_avg:60.11ms
step:528/2315 train_time:31739ms step_avg:60.11ms
step:529/2315 train_time:31799ms step_avg:60.11ms
step:530/2315 train_time:31859ms step_avg:60.11ms
step:531/2315 train_time:31917ms step_avg:60.11ms
step:532/2315 train_time:31978ms step_avg:60.11ms
step:533/2315 train_time:32038ms step_avg:60.11ms
step:534/2315 train_time:32099ms step_avg:60.11ms
step:535/2315 train_time:32159ms step_avg:60.11ms
step:536/2315 train_time:32220ms step_avg:60.11ms
step:537/2315 train_time:32281ms step_avg:60.11ms
step:538/2315 train_time:32342ms step_avg:60.12ms
step:539/2315 train_time:32402ms step_avg:60.11ms
step:540/2315 train_time:32462ms step_avg:60.11ms
step:541/2315 train_time:32521ms step_avg:60.11ms
step:542/2315 train_time:32581ms step_avg:60.11ms
step:543/2315 train_time:32641ms step_avg:60.11ms
step:544/2315 train_time:32702ms step_avg:60.11ms
step:545/2315 train_time:32761ms step_avg:60.11ms
step:546/2315 train_time:32821ms step_avg:60.11ms
step:547/2315 train_time:32880ms step_avg:60.11ms
step:548/2315 train_time:32940ms step_avg:60.11ms
step:549/2315 train_time:32999ms step_avg:60.11ms
step:550/2315 train_time:33059ms step_avg:60.11ms
step:551/2315 train_time:33119ms step_avg:60.11ms
step:552/2315 train_time:33180ms step_avg:60.11ms
step:553/2315 train_time:33241ms step_avg:60.11ms
step:554/2315 train_time:33302ms step_avg:60.11ms
step:555/2315 train_time:33362ms step_avg:60.11ms
step:556/2315 train_time:33423ms step_avg:60.11ms
step:557/2315 train_time:33482ms step_avg:60.11ms
step:558/2315 train_time:33542ms step_avg:60.11ms
step:559/2315 train_time:33601ms step_avg:60.11ms
step:560/2315 train_time:33661ms step_avg:60.11ms
step:561/2315 train_time:33721ms step_avg:60.11ms
step:562/2315 train_time:33781ms step_avg:60.11ms
step:563/2315 train_time:33841ms step_avg:60.11ms
step:564/2315 train_time:33901ms step_avg:60.11ms
step:565/2315 train_time:33960ms step_avg:60.11ms
step:566/2315 train_time:34020ms step_avg:60.11ms
step:567/2315 train_time:34080ms step_avg:60.11ms
step:568/2315 train_time:34140ms step_avg:60.11ms
step:569/2315 train_time:34200ms step_avg:60.11ms
step:570/2315 train_time:34262ms step_avg:60.11ms
step:571/2315 train_time:34322ms step_avg:60.11ms
step:572/2315 train_time:34383ms step_avg:60.11ms
step:573/2315 train_time:34442ms step_avg:60.11ms
step:574/2315 train_time:34503ms step_avg:60.11ms
step:575/2315 train_time:34562ms step_avg:60.11ms
step:576/2315 train_time:34623ms step_avg:60.11ms
step:577/2315 train_time:34682ms step_avg:60.11ms
step:578/2315 train_time:34742ms step_avg:60.11ms
step:579/2315 train_time:34802ms step_avg:60.11ms
step:580/2315 train_time:34862ms step_avg:60.11ms
step:581/2315 train_time:34921ms step_avg:60.11ms
step:582/2315 train_time:34981ms step_avg:60.10ms
step:583/2315 train_time:35040ms step_avg:60.10ms
step:584/2315 train_time:35100ms step_avg:60.10ms
step:585/2315 train_time:35160ms step_avg:60.10ms
step:586/2315 train_time:35221ms step_avg:60.10ms
step:587/2315 train_time:35281ms step_avg:60.10ms
step:588/2315 train_time:35341ms step_avg:60.10ms
step:589/2315 train_time:35401ms step_avg:60.10ms
step:590/2315 train_time:35462ms step_avg:60.11ms
step:591/2315 train_time:35522ms step_avg:60.10ms
step:592/2315 train_time:35582ms step_avg:60.10ms
step:593/2315 train_time:35641ms step_avg:60.10ms
step:594/2315 train_time:35701ms step_avg:60.10ms
step:595/2315 train_time:35761ms step_avg:60.10ms
step:596/2315 train_time:35821ms step_avg:60.10ms
step:597/2315 train_time:35880ms step_avg:60.10ms
step:598/2315 train_time:35940ms step_avg:60.10ms
step:599/2315 train_time:36000ms step_avg:60.10ms
step:600/2315 train_time:36060ms step_avg:60.10ms
step:601/2315 train_time:36119ms step_avg:60.10ms
step:602/2315 train_time:36180ms step_avg:60.10ms
step:603/2315 train_time:36240ms step_avg:60.10ms
step:604/2315 train_time:36300ms step_avg:60.10ms
step:605/2315 train_time:36360ms step_avg:60.10ms
step:606/2315 train_time:36420ms step_avg:60.10ms
step:607/2315 train_time:36481ms step_avg:60.10ms
step:608/2315 train_time:36541ms step_avg:60.10ms
step:609/2315 train_time:36601ms step_avg:60.10ms
step:610/2315 train_time:36661ms step_avg:60.10ms
step:611/2315 train_time:36722ms step_avg:60.10ms
step:612/2315 train_time:36782ms step_avg:60.10ms
step:613/2315 train_time:36841ms step_avg:60.10ms
step:614/2315 train_time:36902ms step_avg:60.10ms
step:615/2315 train_time:36961ms step_avg:60.10ms
step:616/2315 train_time:37021ms step_avg:60.10ms
step:617/2315 train_time:37080ms step_avg:60.10ms
step:618/2315 train_time:37141ms step_avg:60.10ms
step:619/2315 train_time:37201ms step_avg:60.10ms
step:620/2315 train_time:37261ms step_avg:60.10ms
step:621/2315 train_time:37321ms step_avg:60.10ms
step:622/2315 train_time:37382ms step_avg:60.10ms
step:623/2315 train_time:37442ms step_avg:60.10ms
step:624/2315 train_time:37502ms step_avg:60.10ms
step:625/2315 train_time:37562ms step_avg:60.10ms
step:626/2315 train_time:37623ms step_avg:60.10ms
step:627/2315 train_time:37683ms step_avg:60.10ms
step:628/2315 train_time:37743ms step_avg:60.10ms
step:629/2315 train_time:37802ms step_avg:60.10ms
step:630/2315 train_time:37862ms step_avg:60.10ms
step:631/2315 train_time:37922ms step_avg:60.10ms
step:632/2315 train_time:37982ms step_avg:60.10ms
step:633/2315 train_time:38041ms step_avg:60.10ms
step:634/2315 train_time:38101ms step_avg:60.10ms
step:635/2315 train_time:38161ms step_avg:60.10ms
step:636/2315 train_time:38222ms step_avg:60.10ms
step:637/2315 train_time:38281ms step_avg:60.10ms
step:638/2315 train_time:38342ms step_avg:60.10ms
step:639/2315 train_time:38402ms step_avg:60.10ms
step:640/2315 train_time:38462ms step_avg:60.10ms
step:641/2315 train_time:38522ms step_avg:60.10ms
step:642/2315 train_time:38583ms step_avg:60.10ms
step:643/2315 train_time:38642ms step_avg:60.10ms
step:644/2315 train_time:38702ms step_avg:60.10ms
step:645/2315 train_time:38762ms step_avg:60.10ms
step:646/2315 train_time:38823ms step_avg:60.10ms
step:647/2315 train_time:38882ms step_avg:60.10ms
step:648/2315 train_time:38943ms step_avg:60.10ms
step:649/2315 train_time:39002ms step_avg:60.10ms
step:650/2315 train_time:39062ms step_avg:60.09ms
step:651/2315 train_time:39122ms step_avg:60.09ms
step:652/2315 train_time:39182ms step_avg:60.09ms
step:653/2315 train_time:39242ms step_avg:60.09ms
step:654/2315 train_time:39302ms step_avg:60.10ms
step:655/2315 train_time:39362ms step_avg:60.10ms
step:656/2315 train_time:39423ms step_avg:60.10ms
step:657/2315 train_time:39482ms step_avg:60.09ms
step:658/2315 train_time:39542ms step_avg:60.09ms
step:659/2315 train_time:39602ms step_avg:60.09ms
step:660/2315 train_time:39662ms step_avg:60.09ms
step:661/2315 train_time:39721ms step_avg:60.09ms
step:662/2315 train_time:39782ms step_avg:60.09ms
step:663/2315 train_time:39842ms step_avg:60.09ms
step:664/2315 train_time:39902ms step_avg:60.09ms
step:665/2315 train_time:39961ms step_avg:60.09ms
step:666/2315 train_time:40021ms step_avg:60.09ms
step:667/2315 train_time:40081ms step_avg:60.09ms
step:668/2315 train_time:40140ms step_avg:60.09ms
step:669/2315 train_time:40200ms step_avg:60.09ms
step:670/2315 train_time:40260ms step_avg:60.09ms
step:671/2315 train_time:40321ms step_avg:60.09ms
step:672/2315 train_time:40382ms step_avg:60.09ms
step:673/2315 train_time:40441ms step_avg:60.09ms
step:674/2315 train_time:40502ms step_avg:60.09ms
step:675/2315 train_time:40562ms step_avg:60.09ms
step:676/2315 train_time:40622ms step_avg:60.09ms
step:677/2315 train_time:40681ms step_avg:60.09ms
step:678/2315 train_time:40742ms step_avg:60.09ms
step:679/2315 train_time:40802ms step_avg:60.09ms
step:680/2315 train_time:40862ms step_avg:60.09ms
step:681/2315 train_time:40921ms step_avg:60.09ms
step:682/2315 train_time:40981ms step_avg:60.09ms
step:683/2315 train_time:41040ms step_avg:60.09ms
step:684/2315 train_time:41101ms step_avg:60.09ms
step:685/2315 train_time:41160ms step_avg:60.09ms
step:686/2315 train_time:41220ms step_avg:60.09ms
step:687/2315 train_time:41280ms step_avg:60.09ms
step:688/2315 train_time:41341ms step_avg:60.09ms
step:689/2315 train_time:41401ms step_avg:60.09ms
step:690/2315 train_time:41461ms step_avg:60.09ms
step:691/2315 train_time:41521ms step_avg:60.09ms
step:692/2315 train_time:41582ms step_avg:60.09ms
step:693/2315 train_time:41641ms step_avg:60.09ms
step:694/2315 train_time:41702ms step_avg:60.09ms
step:695/2315 train_time:41761ms step_avg:60.09ms
step:696/2315 train_time:41822ms step_avg:60.09ms
step:697/2315 train_time:41881ms step_avg:60.09ms
step:698/2315 train_time:41941ms step_avg:60.09ms
step:699/2315 train_time:42001ms step_avg:60.09ms
step:700/2315 train_time:42061ms step_avg:60.09ms
step:701/2315 train_time:42120ms step_avg:60.09ms
step:702/2315 train_time:42180ms step_avg:60.09ms
step:703/2315 train_time:42240ms step_avg:60.09ms
step:704/2315 train_time:42300ms step_avg:60.09ms
step:705/2315 train_time:42360ms step_avg:60.09ms
step:706/2315 train_time:42420ms step_avg:60.09ms
step:707/2315 train_time:42480ms step_avg:60.09ms
step:708/2315 train_time:42541ms step_avg:60.09ms
step:709/2315 train_time:42601ms step_avg:60.09ms
step:710/2315 train_time:42661ms step_avg:60.09ms
step:711/2315 train_time:42722ms step_avg:60.09ms
step:712/2315 train_time:42782ms step_avg:60.09ms
step:713/2315 train_time:42841ms step_avg:60.09ms
step:714/2315 train_time:42901ms step_avg:60.09ms
step:715/2315 train_time:42961ms step_avg:60.09ms
step:716/2315 train_time:43022ms step_avg:60.09ms
step:717/2315 train_time:43081ms step_avg:60.08ms
step:718/2315 train_time:43141ms step_avg:60.08ms
step:719/2315 train_time:43200ms step_avg:60.08ms
step:720/2315 train_time:43260ms step_avg:60.08ms
step:721/2315 train_time:43320ms step_avg:60.08ms
step:722/2315 train_time:43380ms step_avg:60.08ms
step:723/2315 train_time:43440ms step_avg:60.08ms
step:724/2315 train_time:43500ms step_avg:60.08ms
step:725/2315 train_time:43560ms step_avg:60.08ms
step:726/2315 train_time:43620ms step_avg:60.08ms
step:727/2315 train_time:43680ms step_avg:60.08ms
step:728/2315 train_time:43740ms step_avg:60.08ms
step:729/2315 train_time:43800ms step_avg:60.08ms
step:730/2315 train_time:43860ms step_avg:60.08ms
step:731/2315 train_time:43920ms step_avg:60.08ms
step:732/2315 train_time:43980ms step_avg:60.08ms
step:733/2315 train_time:44039ms step_avg:60.08ms
step:734/2315 train_time:44100ms step_avg:60.08ms
step:735/2315 train_time:44160ms step_avg:60.08ms
step:736/2315 train_time:44220ms step_avg:60.08ms
step:737/2315 train_time:44280ms step_avg:60.08ms
step:738/2315 train_time:44340ms step_avg:60.08ms
step:739/2315 train_time:44400ms step_avg:60.08ms
step:740/2315 train_time:44460ms step_avg:60.08ms
step:741/2315 train_time:44521ms step_avg:60.08ms
step:742/2315 train_time:44581ms step_avg:60.08ms
step:743/2315 train_time:44640ms step_avg:60.08ms
step:744/2315 train_time:44700ms step_avg:60.08ms
step:745/2315 train_time:44761ms step_avg:60.08ms
step:746/2315 train_time:44821ms step_avg:60.08ms
step:747/2315 train_time:44881ms step_avg:60.08ms
step:748/2315 train_time:44941ms step_avg:60.08ms
step:749/2315 train_time:45001ms step_avg:60.08ms
step:750/2315 train_time:45062ms step_avg:60.08ms
step:750/2315 val_loss:3.6824 train_time:45124ms step_avg:60.16ms
step:751/2315 train_time:45141ms step_avg:60.11ms
step:752/2315 train_time:45185ms step_avg:60.09ms
step:753/2315 train_time:45247ms step_avg:60.09ms
step:754/2315 train_time:45313ms step_avg:60.10ms
step:755/2315 train_time:45372ms step_avg:60.10ms
step:756/2315 train_time:45433ms step_avg:60.10ms
step:757/2315 train_time:45492ms step_avg:60.10ms
step:758/2315 train_time:45552ms step_avg:60.09ms
step:759/2315 train_time:45611ms step_avg:60.09ms
step:760/2315 train_time:45671ms step_avg:60.09ms
step:761/2315 train_time:45731ms step_avg:60.09ms
step:762/2315 train_time:45791ms step_avg:60.09ms
step:763/2315 train_time:45851ms step_avg:60.09ms
step:764/2315 train_time:45912ms step_avg:60.09ms
step:765/2315 train_time:45972ms step_avg:60.09ms
step:766/2315 train_time:46033ms step_avg:60.10ms
step:767/2315 train_time:46096ms step_avg:60.10ms
step:768/2315 train_time:46159ms step_avg:60.10ms
step:769/2315 train_time:46221ms step_avg:60.11ms
step:770/2315 train_time:46283ms step_avg:60.11ms
step:771/2315 train_time:46344ms step_avg:60.11ms
step:772/2315 train_time:46405ms step_avg:60.11ms
step:773/2315 train_time:46465ms step_avg:60.11ms
step:774/2315 train_time:46526ms step_avg:60.11ms
step:775/2315 train_time:46586ms step_avg:60.11ms
step:776/2315 train_time:46646ms step_avg:60.11ms
step:777/2315 train_time:46706ms step_avg:60.11ms
step:778/2315 train_time:46766ms step_avg:60.11ms
step:779/2315 train_time:46826ms step_avg:60.11ms
step:780/2315 train_time:46886ms step_avg:60.11ms
step:781/2315 train_time:46946ms step_avg:60.11ms
step:782/2315 train_time:47007ms step_avg:60.11ms
step:783/2315 train_time:47068ms step_avg:60.11ms
step:784/2315 train_time:47131ms step_avg:60.12ms
step:785/2315 train_time:47192ms step_avg:60.12ms
step:786/2315 train_time:47255ms step_avg:60.12ms
step:787/2315 train_time:47316ms step_avg:60.12ms
step:788/2315 train_time:47378ms step_avg:60.12ms
step:789/2315 train_time:47439ms step_avg:60.13ms
step:790/2315 train_time:47500ms step_avg:60.13ms
step:791/2315 train_time:47560ms step_avg:60.13ms
step:792/2315 train_time:47621ms step_avg:60.13ms
step:793/2315 train_time:47681ms step_avg:60.13ms
step:794/2315 train_time:47742ms step_avg:60.13ms
step:795/2315 train_time:47802ms step_avg:60.13ms
step:796/2315 train_time:47863ms step_avg:60.13ms
step:797/2315 train_time:47923ms step_avg:60.13ms
step:798/2315 train_time:47984ms step_avg:60.13ms
step:799/2315 train_time:48045ms step_avg:60.13ms
step:800/2315 train_time:48105ms step_avg:60.13ms
step:801/2315 train_time:48166ms step_avg:60.13ms
step:802/2315 train_time:48227ms step_avg:60.13ms
step:803/2315 train_time:48288ms step_avg:60.13ms
step:804/2315 train_time:48349ms step_avg:60.14ms
step:805/2315 train_time:48411ms step_avg:60.14ms
step:806/2315 train_time:48472ms step_avg:60.14ms
step:807/2315 train_time:48533ms step_avg:60.14ms
step:808/2315 train_time:48595ms step_avg:60.14ms
step:809/2315 train_time:48656ms step_avg:60.14ms
step:810/2315 train_time:48717ms step_avg:60.14ms
step:811/2315 train_time:48777ms step_avg:60.14ms
step:812/2315 train_time:48838ms step_avg:60.15ms
step:813/2315 train_time:48899ms step_avg:60.15ms
step:814/2315 train_time:48961ms step_avg:60.15ms
step:815/2315 train_time:49021ms step_avg:60.15ms
step:816/2315 train_time:49083ms step_avg:60.15ms
step:817/2315 train_time:49144ms step_avg:60.15ms
step:818/2315 train_time:49205ms step_avg:60.15ms
step:819/2315 train_time:49265ms step_avg:60.15ms
step:820/2315 train_time:49325ms step_avg:60.15ms
step:821/2315 train_time:49386ms step_avg:60.15ms
step:822/2315 train_time:49446ms step_avg:60.15ms
step:823/2315 train_time:49506ms step_avg:60.15ms
step:824/2315 train_time:49568ms step_avg:60.16ms
step:825/2315 train_time:49629ms step_avg:60.16ms
step:826/2315 train_time:49690ms step_avg:60.16ms
step:827/2315 train_time:49750ms step_avg:60.16ms
step:828/2315 train_time:49812ms step_avg:60.16ms
step:829/2315 train_time:49873ms step_avg:60.16ms
step:830/2315 train_time:49934ms step_avg:60.16ms
step:831/2315 train_time:49994ms step_avg:60.16ms
step:832/2315 train_time:50056ms step_avg:60.16ms
step:833/2315 train_time:50117ms step_avg:60.16ms
step:834/2315 train_time:50178ms step_avg:60.16ms
step:835/2315 train_time:50239ms step_avg:60.17ms
step:836/2315 train_time:50300ms step_avg:60.17ms
step:837/2315 train_time:50361ms step_avg:60.17ms
step:838/2315 train_time:50422ms step_avg:60.17ms
step:839/2315 train_time:50483ms step_avg:60.17ms
step:840/2315 train_time:50544ms step_avg:60.17ms
step:841/2315 train_time:50605ms step_avg:60.17ms
step:842/2315 train_time:50665ms step_avg:60.17ms
step:843/2315 train_time:50725ms step_avg:60.17ms
step:844/2315 train_time:50785ms step_avg:60.17ms
step:845/2315 train_time:50845ms step_avg:60.17ms
step:846/2315 train_time:50906ms step_avg:60.17ms
step:847/2315 train_time:50966ms step_avg:60.17ms
step:848/2315 train_time:51027ms step_avg:60.17ms
step:849/2315 train_time:51088ms step_avg:60.17ms
step:850/2315 train_time:51149ms step_avg:60.17ms
step:851/2315 train_time:51210ms step_avg:60.18ms
step:852/2315 train_time:51271ms step_avg:60.18ms
step:853/2315 train_time:51333ms step_avg:60.18ms
step:854/2315 train_time:51395ms step_avg:60.18ms
step:855/2315 train_time:51456ms step_avg:60.18ms
step:856/2315 train_time:51518ms step_avg:60.18ms
step:857/2315 train_time:51578ms step_avg:60.18ms
step:858/2315 train_time:51639ms step_avg:60.19ms
step:859/2315 train_time:51700ms step_avg:60.19ms
step:860/2315 train_time:51761ms step_avg:60.19ms
step:861/2315 train_time:51821ms step_avg:60.19ms
step:862/2315 train_time:51883ms step_avg:60.19ms
step:863/2315 train_time:51943ms step_avg:60.19ms
step:864/2315 train_time:52005ms step_avg:60.19ms
step:865/2315 train_time:52065ms step_avg:60.19ms
step:866/2315 train_time:52125ms step_avg:60.19ms
step:867/2315 train_time:52186ms step_avg:60.19ms
step:868/2315 train_time:52247ms step_avg:60.19ms
step:869/2315 train_time:52306ms step_avg:60.19ms
step:870/2315 train_time:52367ms step_avg:60.19ms
step:871/2315 train_time:52428ms step_avg:60.19ms
step:872/2315 train_time:52489ms step_avg:60.19ms
step:873/2315 train_time:52550ms step_avg:60.19ms
step:874/2315 train_time:52612ms step_avg:60.20ms
step:875/2315 train_time:52673ms step_avg:60.20ms
step:876/2315 train_time:52735ms step_avg:60.20ms
step:877/2315 train_time:52795ms step_avg:60.20ms
step:878/2315 train_time:52857ms step_avg:60.20ms
step:879/2315 train_time:52918ms step_avg:60.20ms
step:880/2315 train_time:52979ms step_avg:60.20ms
step:881/2315 train_time:53040ms step_avg:60.20ms
step:882/2315 train_time:53101ms step_avg:60.20ms
step:883/2315 train_time:53161ms step_avg:60.21ms
step:884/2315 train_time:53223ms step_avg:60.21ms
step:885/2315 train_time:53284ms step_avg:60.21ms
step:886/2315 train_time:53345ms step_avg:60.21ms
step:887/2315 train_time:53406ms step_avg:60.21ms
step:888/2315 train_time:53466ms step_avg:60.21ms
step:889/2315 train_time:53526ms step_avg:60.21ms
step:890/2315 train_time:53587ms step_avg:60.21ms
step:891/2315 train_time:53647ms step_avg:60.21ms
step:892/2315 train_time:53709ms step_avg:60.21ms
step:893/2315 train_time:53769ms step_avg:60.21ms
step:894/2315 train_time:53831ms step_avg:60.21ms
step:895/2315 train_time:53892ms step_avg:60.21ms
step:896/2315 train_time:53953ms step_avg:60.22ms
step:897/2315 train_time:54014ms step_avg:60.22ms
step:898/2315 train_time:54076ms step_avg:60.22ms
step:899/2315 train_time:54136ms step_avg:60.22ms
step:900/2315 train_time:54198ms step_avg:60.22ms
step:901/2315 train_time:54259ms step_avg:60.22ms
step:902/2315 train_time:54320ms step_avg:60.22ms
step:903/2315 train_time:54380ms step_avg:60.22ms
step:904/2315 train_time:54442ms step_avg:60.22ms
step:905/2315 train_time:54503ms step_avg:60.22ms
step:906/2315 train_time:54564ms step_avg:60.23ms
step:907/2315 train_time:54625ms step_avg:60.23ms
step:908/2315 train_time:54685ms step_avg:60.23ms
step:909/2315 train_time:54746ms step_avg:60.23ms
step:910/2315 train_time:54806ms step_avg:60.23ms
step:911/2315 train_time:54866ms step_avg:60.23ms
step:912/2315 train_time:54927ms step_avg:60.23ms
step:913/2315 train_time:54987ms step_avg:60.23ms
step:914/2315 train_time:55048ms step_avg:60.23ms
step:915/2315 train_time:55109ms step_avg:60.23ms
step:916/2315 train_time:55170ms step_avg:60.23ms
step:917/2315 train_time:55232ms step_avg:60.23ms
step:918/2315 train_time:55293ms step_avg:60.23ms
step:919/2315 train_time:55354ms step_avg:60.23ms
step:920/2315 train_time:55416ms step_avg:60.24ms
step:921/2315 train_time:55478ms step_avg:60.24ms
step:922/2315 train_time:55539ms step_avg:60.24ms
step:923/2315 train_time:55599ms step_avg:60.24ms
step:924/2315 train_time:55660ms step_avg:60.24ms
step:925/2315 train_time:55721ms step_avg:60.24ms
step:926/2315 train_time:55783ms step_avg:60.24ms
step:927/2315 train_time:55844ms step_avg:60.24ms
step:928/2315 train_time:55905ms step_avg:60.24ms
step:929/2315 train_time:55964ms step_avg:60.24ms
step:930/2315 train_time:56025ms step_avg:60.24ms
step:931/2315 train_time:56085ms step_avg:60.24ms
step:932/2315 train_time:56146ms step_avg:60.24ms
step:933/2315 train_time:56206ms step_avg:60.24ms
step:934/2315 train_time:56267ms step_avg:60.24ms
step:935/2315 train_time:56329ms step_avg:60.24ms
step:936/2315 train_time:56390ms step_avg:60.25ms
step:937/2315 train_time:56451ms step_avg:60.25ms
step:938/2315 train_time:56513ms step_avg:60.25ms
step:939/2315 train_time:56574ms step_avg:60.25ms
step:940/2315 train_time:56635ms step_avg:60.25ms
step:941/2315 train_time:56697ms step_avg:60.25ms
step:942/2315 train_time:56759ms step_avg:60.25ms
step:943/2315 train_time:56819ms step_avg:60.25ms
step:944/2315 train_time:56880ms step_avg:60.25ms
step:945/2315 train_time:56941ms step_avg:60.26ms
step:946/2315 train_time:57003ms step_avg:60.26ms
step:947/2315 train_time:57064ms step_avg:60.26ms
step:948/2315 train_time:57125ms step_avg:60.26ms
step:949/2315 train_time:57185ms step_avg:60.26ms
step:950/2315 train_time:57245ms step_avg:60.26ms
step:951/2315 train_time:57305ms step_avg:60.26ms
step:952/2315 train_time:57366ms step_avg:60.26ms
step:953/2315 train_time:57426ms step_avg:60.26ms
step:954/2315 train_time:57486ms step_avg:60.26ms
step:955/2315 train_time:57546ms step_avg:60.26ms
step:956/2315 train_time:57607ms step_avg:60.26ms
step:957/2315 train_time:57668ms step_avg:60.26ms
step:958/2315 train_time:57729ms step_avg:60.26ms
step:959/2315 train_time:57790ms step_avg:60.26ms
step:960/2315 train_time:57852ms step_avg:60.26ms
step:961/2315 train_time:57912ms step_avg:60.26ms
step:962/2315 train_time:57974ms step_avg:60.26ms
step:963/2315 train_time:58035ms step_avg:60.26ms
step:964/2315 train_time:58097ms step_avg:60.27ms
step:965/2315 train_time:58157ms step_avg:60.27ms
step:966/2315 train_time:58218ms step_avg:60.27ms
step:967/2315 train_time:58278ms step_avg:60.27ms
step:968/2315 train_time:58340ms step_avg:60.27ms
step:969/2315 train_time:58400ms step_avg:60.27ms
step:970/2315 train_time:58461ms step_avg:60.27ms
step:971/2315 train_time:58522ms step_avg:60.27ms
step:972/2315 train_time:58583ms step_avg:60.27ms
step:973/2315 train_time:58644ms step_avg:60.27ms
step:974/2315 train_time:58704ms step_avg:60.27ms
step:975/2315 train_time:58764ms step_avg:60.27ms
step:976/2315 train_time:58825ms step_avg:60.27ms
step:977/2315 train_time:58885ms step_avg:60.27ms
step:978/2315 train_time:58946ms step_avg:60.27ms
step:979/2315 train_time:59006ms step_avg:60.27ms
step:980/2315 train_time:59067ms step_avg:60.27ms
step:981/2315 train_time:59128ms step_avg:60.27ms
step:982/2315 train_time:59189ms step_avg:60.27ms
step:983/2315 train_time:59250ms step_avg:60.27ms
step:984/2315 train_time:59311ms step_avg:60.28ms
step:985/2315 train_time:59372ms step_avg:60.28ms
step:986/2315 train_time:59433ms step_avg:60.28ms
step:987/2315 train_time:59494ms step_avg:60.28ms
step:988/2315 train_time:59555ms step_avg:60.28ms
step:989/2315 train_time:59616ms step_avg:60.28ms
step:990/2315 train_time:59677ms step_avg:60.28ms
step:991/2315 train_time:59738ms step_avg:60.28ms
step:992/2315 train_time:59799ms step_avg:60.28ms
step:993/2315 train_time:59859ms step_avg:60.28ms
step:994/2315 train_time:59921ms step_avg:60.28ms
step:995/2315 train_time:59982ms step_avg:60.28ms
step:996/2315 train_time:60044ms step_avg:60.28ms
step:997/2315 train_time:60105ms step_avg:60.29ms
step:998/2315 train_time:60165ms step_avg:60.29ms
step:999/2315 train_time:60225ms step_avg:60.29ms
step:1000/2315 train_time:60286ms step_avg:60.29ms
step:1000/2315 val_loss:3.5694 train_time:60348ms step_avg:60.35ms
step:1001/2315 train_time:60366ms step_avg:60.31ms
step:1002/2315 train_time:60409ms step_avg:60.29ms
step:1003/2315 train_time:60474ms step_avg:60.29ms
step:1004/2315 train_time:60539ms step_avg:60.30ms
step:1005/2315 train_time:60600ms step_avg:60.30ms
step:1006/2315 train_time:60661ms step_avg:60.30ms
step:1007/2315 train_time:60721ms step_avg:60.30ms
step:1008/2315 train_time:60781ms step_avg:60.30ms
step:1009/2315 train_time:60841ms step_avg:60.30ms
step:1010/2315 train_time:60901ms step_avg:60.30ms
step:1011/2315 train_time:60961ms step_avg:60.30ms
step:1012/2315 train_time:61021ms step_avg:60.30ms
step:1013/2315 train_time:61080ms step_avg:60.30ms
step:1014/2315 train_time:61142ms step_avg:60.30ms
step:1015/2315 train_time:61201ms step_avg:60.30ms
step:1016/2315 train_time:61262ms step_avg:60.30ms
step:1017/2315 train_time:61324ms step_avg:60.30ms
step:1018/2315 train_time:61387ms step_avg:60.30ms
step:1019/2315 train_time:61450ms step_avg:60.30ms
step:1020/2315 train_time:61512ms step_avg:60.31ms
step:1021/2315 train_time:61573ms step_avg:60.31ms
step:1022/2315 train_time:61635ms step_avg:60.31ms
step:1023/2315 train_time:61696ms step_avg:60.31ms
step:1024/2315 train_time:61757ms step_avg:60.31ms
step:1025/2315 train_time:61818ms step_avg:60.31ms
step:1026/2315 train_time:61879ms step_avg:60.31ms
step:1027/2315 train_time:61938ms step_avg:60.31ms
step:1028/2315 train_time:61999ms step_avg:60.31ms
step:1029/2315 train_time:62058ms step_avg:60.31ms
step:1030/2315 train_time:62119ms step_avg:60.31ms
step:1031/2315 train_time:62179ms step_avg:60.31ms
step:1032/2315 train_time:62240ms step_avg:60.31ms
step:1033/2315 train_time:62302ms step_avg:60.31ms
step:1034/2315 train_time:62365ms step_avg:60.31ms
step:1035/2315 train_time:62427ms step_avg:60.32ms
step:1036/2315 train_time:62489ms step_avg:60.32ms
step:1037/2315 train_time:62549ms step_avg:60.32ms
step:1038/2315 train_time:62610ms step_avg:60.32ms
step:1039/2315 train_time:62671ms step_avg:60.32ms
step:1040/2315 train_time:62732ms step_avg:60.32ms
step:1041/2315 train_time:62792ms step_avg:60.32ms
step:1042/2315 train_time:62854ms step_avg:60.32ms
step:1043/2315 train_time:62914ms step_avg:60.32ms
step:1044/2315 train_time:62976ms step_avg:60.32ms
step:1045/2315 train_time:63036ms step_avg:60.32ms
step:1046/2315 train_time:63098ms step_avg:60.32ms
step:1047/2315 train_time:63158ms step_avg:60.32ms
step:1048/2315 train_time:63219ms step_avg:60.32ms
step:1049/2315 train_time:63280ms step_avg:60.32ms
step:1050/2315 train_time:63342ms step_avg:60.33ms
step:1051/2315 train_time:63404ms step_avg:60.33ms
step:1052/2315 train_time:63465ms step_avg:60.33ms
step:1053/2315 train_time:63526ms step_avg:60.33ms
step:1054/2315 train_time:63587ms step_avg:60.33ms
step:1055/2315 train_time:63647ms step_avg:60.33ms
step:1056/2315 train_time:63708ms step_avg:60.33ms
step:1057/2315 train_time:63767ms step_avg:60.33ms
step:1058/2315 train_time:63828ms step_avg:60.33ms
step:1059/2315 train_time:63889ms step_avg:60.33ms
step:1060/2315 train_time:63950ms step_avg:60.33ms
step:1061/2315 train_time:64011ms step_avg:60.33ms
step:1062/2315 train_time:64072ms step_avg:60.33ms
step:1063/2315 train_time:64133ms step_avg:60.33ms
step:1064/2315 train_time:64194ms step_avg:60.33ms
step:1065/2315 train_time:64255ms step_avg:60.33ms
step:1066/2315 train_time:64317ms step_avg:60.33ms
step:1067/2315 train_time:64378ms step_avg:60.34ms
step:1068/2315 train_time:64440ms step_avg:60.34ms
step:1069/2315 train_time:64501ms step_avg:60.34ms
step:1070/2315 train_time:64562ms step_avg:60.34ms
step:1071/2315 train_time:64624ms step_avg:60.34ms
step:1072/2315 train_time:64685ms step_avg:60.34ms
step:1073/2315 train_time:64745ms step_avg:60.34ms
step:1074/2315 train_time:64806ms step_avg:60.34ms
step:1075/2315 train_time:64866ms step_avg:60.34ms
step:1076/2315 train_time:64927ms step_avg:60.34ms
step:1077/2315 train_time:64987ms step_avg:60.34ms
step:1078/2315 train_time:65047ms step_avg:60.34ms
step:1079/2315 train_time:65107ms step_avg:60.34ms
step:1080/2315 train_time:65168ms step_avg:60.34ms
step:1081/2315 train_time:65229ms step_avg:60.34ms
step:1082/2315 train_time:65290ms step_avg:60.34ms
step:1083/2315 train_time:65352ms step_avg:60.34ms
step:1084/2315 train_time:65415ms step_avg:60.35ms
step:1085/2315 train_time:65476ms step_avg:60.35ms
step:1086/2315 train_time:65538ms step_avg:60.35ms
step:1087/2315 train_time:65599ms step_avg:60.35ms
step:1088/2315 train_time:65660ms step_avg:60.35ms
step:1089/2315 train_time:65721ms step_avg:60.35ms
step:1090/2315 train_time:65783ms step_avg:60.35ms
step:1091/2315 train_time:65844ms step_avg:60.35ms
step:1092/2315 train_time:65906ms step_avg:60.35ms
step:1093/2315 train_time:65965ms step_avg:60.35ms
step:1094/2315 train_time:66026ms step_avg:60.35ms
step:1095/2315 train_time:66086ms step_avg:60.35ms
step:1096/2315 train_time:66147ms step_avg:60.35ms
step:1097/2315 train_time:66206ms step_avg:60.35ms
step:1098/2315 train_time:66267ms step_avg:60.35ms
step:1099/2315 train_time:66327ms step_avg:60.35ms
step:1100/2315 train_time:66388ms step_avg:60.35ms
step:1101/2315 train_time:66449ms step_avg:60.35ms
step:1102/2315 train_time:66511ms step_avg:60.35ms
step:1103/2315 train_time:66572ms step_avg:60.36ms
step:1104/2315 train_time:66634ms step_avg:60.36ms
step:1105/2315 train_time:66695ms step_avg:60.36ms
step:1106/2315 train_time:66757ms step_avg:60.36ms
step:1107/2315 train_time:66818ms step_avg:60.36ms
step:1108/2315 train_time:66879ms step_avg:60.36ms
step:1109/2315 train_time:66940ms step_avg:60.36ms
step:1110/2315 train_time:67001ms step_avg:60.36ms
step:1111/2315 train_time:67061ms step_avg:60.36ms
step:1112/2315 train_time:67122ms step_avg:60.36ms
step:1113/2315 train_time:67183ms step_avg:60.36ms
step:1114/2315 train_time:67244ms step_avg:60.36ms
step:1115/2315 train_time:67305ms step_avg:60.36ms
step:1116/2315 train_time:67366ms step_avg:60.36ms
step:1117/2315 train_time:67426ms step_avg:60.36ms
step:1118/2315 train_time:67487ms step_avg:60.36ms
step:1119/2315 train_time:67547ms step_avg:60.36ms
step:1120/2315 train_time:67608ms step_avg:60.36ms
step:1121/2315 train_time:67669ms step_avg:60.37ms
step:1122/2315 train_time:67730ms step_avg:60.37ms
step:1123/2315 train_time:67791ms step_avg:60.37ms
step:1124/2315 train_time:67853ms step_avg:60.37ms
step:1125/2315 train_time:67913ms step_avg:60.37ms
step:1126/2315 train_time:67975ms step_avg:60.37ms
step:1127/2315 train_time:68036ms step_avg:60.37ms
step:1128/2315 train_time:68097ms step_avg:60.37ms
step:1129/2315 train_time:68157ms step_avg:60.37ms
step:1130/2315 train_time:68218ms step_avg:60.37ms
step:1131/2315 train_time:68279ms step_avg:60.37ms
step:1132/2315 train_time:68340ms step_avg:60.37ms
step:1133/2315 train_time:68401ms step_avg:60.37ms
step:1134/2315 train_time:68462ms step_avg:60.37ms
step:1135/2315 train_time:68523ms step_avg:60.37ms
step:1136/2315 train_time:68584ms step_avg:60.37ms
step:1137/2315 train_time:68645ms step_avg:60.37ms
step:1138/2315 train_time:68706ms step_avg:60.37ms
step:1139/2315 train_time:68766ms step_avg:60.37ms
step:1140/2315 train_time:68827ms step_avg:60.37ms
step:1141/2315 train_time:68887ms step_avg:60.37ms
step:1142/2315 train_time:68948ms step_avg:60.37ms
step:1143/2315 train_time:69009ms step_avg:60.38ms
step:1144/2315 train_time:69070ms step_avg:60.38ms
step:1145/2315 train_time:69131ms step_avg:60.38ms
step:1146/2315 train_time:69192ms step_avg:60.38ms
step:1147/2315 train_time:69254ms step_avg:60.38ms
step:1148/2315 train_time:69317ms step_avg:60.38ms
step:1149/2315 train_time:69378ms step_avg:60.38ms
step:1150/2315 train_time:69439ms step_avg:60.38ms
step:1151/2315 train_time:69500ms step_avg:60.38ms
step:1152/2315 train_time:69561ms step_avg:60.38ms
step:1153/2315 train_time:69621ms step_avg:60.38ms
step:1154/2315 train_time:69682ms step_avg:60.38ms
step:1155/2315 train_time:69743ms step_avg:60.38ms
step:1156/2315 train_time:69805ms step_avg:60.38ms
step:1157/2315 train_time:69866ms step_avg:60.39ms
step:1158/2315 train_time:69926ms step_avg:60.39ms
step:1159/2315 train_time:69987ms step_avg:60.39ms
step:1160/2315 train_time:70047ms step_avg:60.39ms
step:1161/2315 train_time:70107ms step_avg:60.38ms
step:1162/2315 train_time:70168ms step_avg:60.39ms
step:1163/2315 train_time:70228ms step_avg:60.39ms
step:1164/2315 train_time:70290ms step_avg:60.39ms
step:1165/2315 train_time:70351ms step_avg:60.39ms
step:1166/2315 train_time:70413ms step_avg:60.39ms
step:1167/2315 train_time:70474ms step_avg:60.39ms
step:1168/2315 train_time:70535ms step_avg:60.39ms
step:1169/2315 train_time:70596ms step_avg:60.39ms
step:1170/2315 train_time:70658ms step_avg:60.39ms
step:1171/2315 train_time:70719ms step_avg:60.39ms
step:1172/2315 train_time:70780ms step_avg:60.39ms
step:1173/2315 train_time:70840ms step_avg:60.39ms
step:1174/2315 train_time:70903ms step_avg:60.39ms
step:1175/2315 train_time:70963ms step_avg:60.39ms
step:1176/2315 train_time:71025ms step_avg:60.40ms
step:1177/2315 train_time:71085ms step_avg:60.40ms
step:1178/2315 train_time:71146ms step_avg:60.40ms
step:1179/2315 train_time:71206ms step_avg:60.40ms
step:1180/2315 train_time:71267ms step_avg:60.40ms
step:1181/2315 train_time:71327ms step_avg:60.40ms
step:1182/2315 train_time:71388ms step_avg:60.40ms
step:1183/2315 train_time:71448ms step_avg:60.40ms
step:1184/2315 train_time:71509ms step_avg:60.40ms
step:1185/2315 train_time:71569ms step_avg:60.40ms
step:1186/2315 train_time:71631ms step_avg:60.40ms
step:1187/2315 train_time:71692ms step_avg:60.40ms
step:1188/2315 train_time:71754ms step_avg:60.40ms
step:1189/2315 train_time:71815ms step_avg:60.40ms
step:1190/2315 train_time:71878ms step_avg:60.40ms
step:1191/2315 train_time:71938ms step_avg:60.40ms
step:1192/2315 train_time:72000ms step_avg:60.40ms
step:1193/2315 train_time:72060ms step_avg:60.40ms
step:1194/2315 train_time:72122ms step_avg:60.40ms
step:1195/2315 train_time:72182ms step_avg:60.40ms
step:1196/2315 train_time:72243ms step_avg:60.40ms
step:1197/2315 train_time:72304ms step_avg:60.40ms
step:1198/2315 train_time:72365ms step_avg:60.41ms
step:1199/2315 train_time:72426ms step_avg:60.41ms
step:1200/2315 train_time:72487ms step_avg:60.41ms
step:1201/2315 train_time:72547ms step_avg:60.41ms
step:1202/2315 train_time:72607ms step_avg:60.41ms
step:1203/2315 train_time:72668ms step_avg:60.41ms
step:1204/2315 train_time:72729ms step_avg:60.41ms
step:1205/2315 train_time:72791ms step_avg:60.41ms
step:1206/2315 train_time:72853ms step_avg:60.41ms
step:1207/2315 train_time:72914ms step_avg:60.41ms
step:1208/2315 train_time:72976ms step_avg:60.41ms
step:1209/2315 train_time:73037ms step_avg:60.41ms
step:1210/2315 train_time:73098ms step_avg:60.41ms
step:1211/2315 train_time:73159ms step_avg:60.41ms
step:1212/2315 train_time:73221ms step_avg:60.41ms
step:1213/2315 train_time:73281ms step_avg:60.41ms
step:1214/2315 train_time:73343ms step_avg:60.41ms
step:1215/2315 train_time:73404ms step_avg:60.41ms
step:1216/2315 train_time:73465ms step_avg:60.42ms
step:1217/2315 train_time:73525ms step_avg:60.42ms
step:1218/2315 train_time:73586ms step_avg:60.42ms
step:1219/2315 train_time:73646ms step_avg:60.42ms
step:1220/2315 train_time:73707ms step_avg:60.42ms
step:1221/2315 train_time:73767ms step_avg:60.42ms
step:1222/2315 train_time:73828ms step_avg:60.42ms
step:1223/2315 train_time:73889ms step_avg:60.42ms
step:1224/2315 train_time:73951ms step_avg:60.42ms
step:1225/2315 train_time:74012ms step_avg:60.42ms
step:1226/2315 train_time:74074ms step_avg:60.42ms
step:1227/2315 train_time:74135ms step_avg:60.42ms
step:1228/2315 train_time:74197ms step_avg:60.42ms
step:1229/2315 train_time:74258ms step_avg:60.42ms
step:1230/2315 train_time:74319ms step_avg:60.42ms
step:1231/2315 train_time:74380ms step_avg:60.42ms
step:1232/2315 train_time:74441ms step_avg:60.42ms
step:1233/2315 train_time:74503ms step_avg:60.42ms
step:1234/2315 train_time:74564ms step_avg:60.42ms
step:1235/2315 train_time:74624ms step_avg:60.42ms
step:1236/2315 train_time:74685ms step_avg:60.42ms
step:1237/2315 train_time:74745ms step_avg:60.42ms
step:1238/2315 train_time:74806ms step_avg:60.43ms
step:1239/2315 train_time:74867ms step_avg:60.43ms
step:1240/2315 train_time:74927ms step_avg:60.43ms
step:1241/2315 train_time:74988ms step_avg:60.43ms
step:1242/2315 train_time:75049ms step_avg:60.43ms
step:1243/2315 train_time:75110ms step_avg:60.43ms
step:1244/2315 train_time:75171ms step_avg:60.43ms
step:1245/2315 train_time:75233ms step_avg:60.43ms
step:1246/2315 train_time:75295ms step_avg:60.43ms
step:1247/2315 train_time:75356ms step_avg:60.43ms
step:1248/2315 train_time:75418ms step_avg:60.43ms
step:1249/2315 train_time:75478ms step_avg:60.43ms
step:1250/2315 train_time:75539ms step_avg:60.43ms
step:1250/2315 val_loss:3.5153 train_time:75602ms step_avg:60.48ms
step:1251/2315 train_time:75619ms step_avg:60.45ms
step:1252/2315 train_time:75663ms step_avg:60.43ms
step:1253/2315 train_time:75727ms step_avg:60.44ms
step:1254/2315 train_time:75789ms step_avg:60.44ms
step:1255/2315 train_time:75850ms step_avg:60.44ms
step:1256/2315 train_time:75913ms step_avg:60.44ms
step:1257/2315 train_time:75973ms step_avg:60.44ms
step:1258/2315 train_time:76034ms step_avg:60.44ms
step:1259/2315 train_time:76095ms step_avg:60.44ms
step:1260/2315 train_time:76155ms step_avg:60.44ms
step:1261/2315 train_time:76215ms step_avg:60.44ms
step:1262/2315 train_time:76276ms step_avg:60.44ms
step:1263/2315 train_time:76337ms step_avg:60.44ms
step:1264/2315 train_time:76398ms step_avg:60.44ms
step:1265/2315 train_time:76458ms step_avg:60.44ms
step:1266/2315 train_time:76520ms step_avg:60.44ms
step:1267/2315 train_time:76580ms step_avg:60.44ms
step:1268/2315 train_time:76643ms step_avg:60.44ms
step:1269/2315 train_time:76705ms step_avg:60.45ms
step:1270/2315 train_time:76767ms step_avg:60.45ms
step:1271/2315 train_time:76828ms step_avg:60.45ms
step:1272/2315 train_time:76889ms step_avg:60.45ms
step:1273/2315 train_time:76950ms step_avg:60.45ms
step:1274/2315 train_time:77011ms step_avg:60.45ms
step:1275/2315 train_time:77071ms step_avg:60.45ms
step:1276/2315 train_time:77132ms step_avg:60.45ms
step:1277/2315 train_time:77192ms step_avg:60.45ms
step:1278/2315 train_time:77253ms step_avg:60.45ms
step:1279/2315 train_time:77314ms step_avg:60.45ms
step:1280/2315 train_time:77374ms step_avg:60.45ms
step:1281/2315 train_time:77435ms step_avg:60.45ms
step:1282/2315 train_time:77497ms step_avg:60.45ms
step:1283/2315 train_time:77558ms step_avg:60.45ms
step:1284/2315 train_time:77620ms step_avg:60.45ms
step:1285/2315 train_time:77681ms step_avg:60.45ms
step:1286/2315 train_time:77743ms step_avg:60.45ms
step:1287/2315 train_time:77804ms step_avg:60.45ms
step:1288/2315 train_time:77865ms step_avg:60.45ms
step:1289/2315 train_time:77926ms step_avg:60.45ms
step:1290/2315 train_time:77987ms step_avg:60.46ms
step:1291/2315 train_time:78047ms step_avg:60.45ms
step:1292/2315 train_time:78108ms step_avg:60.46ms
step:1293/2315 train_time:78168ms step_avg:60.45ms
step:1294/2315 train_time:78229ms step_avg:60.46ms
step:1295/2315 train_time:78289ms step_avg:60.46ms
step:1296/2315 train_time:78350ms step_avg:60.46ms
step:1297/2315 train_time:78411ms step_avg:60.46ms
step:1298/2315 train_time:78472ms step_avg:60.46ms
step:1299/2315 train_time:78533ms step_avg:60.46ms
step:1300/2315 train_time:78596ms step_avg:60.46ms
step:1301/2315 train_time:78657ms step_avg:60.46ms
step:1302/2315 train_time:78719ms step_avg:60.46ms
step:1303/2315 train_time:78780ms step_avg:60.46ms
step:1304/2315 train_time:78842ms step_avg:60.46ms
step:1305/2315 train_time:78903ms step_avg:60.46ms
step:1306/2315 train_time:78964ms step_avg:60.46ms
step:1307/2315 train_time:79025ms step_avg:60.46ms
step:1308/2315 train_time:79086ms step_avg:60.46ms
step:1309/2315 train_time:79146ms step_avg:60.46ms
step:1310/2315 train_time:79206ms step_avg:60.46ms
step:1311/2315 train_time:79266ms step_avg:60.46ms
step:1312/2315 train_time:79327ms step_avg:60.46ms
step:1313/2315 train_time:79387ms step_avg:60.46ms
step:1314/2315 train_time:79448ms step_avg:60.46ms
step:1315/2315 train_time:79508ms step_avg:60.46ms
step:1316/2315 train_time:79569ms step_avg:60.46ms
step:1317/2315 train_time:79631ms step_avg:60.46ms
step:1318/2315 train_time:79693ms step_avg:60.47ms
step:1319/2315 train_time:79755ms step_avg:60.47ms
step:1320/2315 train_time:79817ms step_avg:60.47ms
step:1321/2315 train_time:79878ms step_avg:60.47ms
step:1322/2315 train_time:79939ms step_avg:60.47ms
step:1323/2315 train_time:80000ms step_avg:60.47ms
step:1324/2315 train_time:80062ms step_avg:60.47ms
step:1325/2315 train_time:80123ms step_avg:60.47ms
step:1326/2315 train_time:80184ms step_avg:60.47ms
step:1327/2315 train_time:80244ms step_avg:60.47ms
step:1328/2315 train_time:80305ms step_avg:60.47ms
step:1329/2315 train_time:80365ms step_avg:60.47ms
step:1330/2315 train_time:80426ms step_avg:60.47ms
step:1331/2315 train_time:80486ms step_avg:60.47ms
step:1332/2315 train_time:80546ms step_avg:60.47ms
step:1333/2315 train_time:80607ms step_avg:60.47ms
step:1334/2315 train_time:80668ms step_avg:60.47ms
step:1335/2315 train_time:80729ms step_avg:60.47ms
step:1336/2315 train_time:80791ms step_avg:60.47ms
step:1337/2315 train_time:80852ms step_avg:60.47ms
step:1338/2315 train_time:80914ms step_avg:60.47ms
step:1339/2315 train_time:80975ms step_avg:60.47ms
step:1340/2315 train_time:81037ms step_avg:60.48ms
step:1341/2315 train_time:81098ms step_avg:60.48ms
step:1342/2315 train_time:81159ms step_avg:60.48ms
step:1343/2315 train_time:81220ms step_avg:60.48ms
step:1344/2315 train_time:81281ms step_avg:60.48ms
step:1345/2315 train_time:81341ms step_avg:60.48ms
step:1346/2315 train_time:81403ms step_avg:60.48ms
step:1347/2315 train_time:81463ms step_avg:60.48ms
step:1348/2315 train_time:81524ms step_avg:60.48ms
step:1349/2315 train_time:81584ms step_avg:60.48ms
step:1350/2315 train_time:81645ms step_avg:60.48ms
step:1351/2315 train_time:81706ms step_avg:60.48ms
step:1352/2315 train_time:81767ms step_avg:60.48ms
step:1353/2315 train_time:81827ms step_avg:60.48ms
step:1354/2315 train_time:81889ms step_avg:60.48ms
step:1355/2315 train_time:81950ms step_avg:60.48ms
step:1356/2315 train_time:82012ms step_avg:60.48ms
step:1357/2315 train_time:82073ms step_avg:60.48ms
step:1358/2315 train_time:82134ms step_avg:60.48ms
step:1359/2315 train_time:82195ms step_avg:60.48ms
step:1360/2315 train_time:82257ms step_avg:60.48ms
step:1361/2315 train_time:82318ms step_avg:60.48ms
step:1362/2315 train_time:82379ms step_avg:60.48ms
step:1363/2315 train_time:82440ms step_avg:60.48ms
step:1364/2315 train_time:82501ms step_avg:60.48ms
step:1365/2315 train_time:82562ms step_avg:60.48ms
step:1366/2315 train_time:82623ms step_avg:60.49ms
step:1367/2315 train_time:82684ms step_avg:60.49ms
step:1368/2315 train_time:82745ms step_avg:60.49ms
step:1369/2315 train_time:82805ms step_avg:60.49ms
step:1370/2315 train_time:82866ms step_avg:60.49ms
step:1371/2315 train_time:82926ms step_avg:60.49ms
step:1372/2315 train_time:82986ms step_avg:60.49ms
step:1373/2315 train_time:83047ms step_avg:60.49ms
step:1374/2315 train_time:83108ms step_avg:60.49ms
step:1375/2315 train_time:83169ms step_avg:60.49ms
step:1376/2315 train_time:83231ms step_avg:60.49ms
step:1377/2315 train_time:83292ms step_avg:60.49ms
step:1378/2315 train_time:83354ms step_avg:60.49ms
step:1379/2315 train_time:83415ms step_avg:60.49ms
step:1380/2315 train_time:83476ms step_avg:60.49ms
step:1381/2315 train_time:83537ms step_avg:60.49ms
step:1382/2315 train_time:83598ms step_avg:60.49ms
step:1383/2315 train_time:83658ms step_avg:60.49ms
step:1384/2315 train_time:83720ms step_avg:60.49ms
step:1385/2315 train_time:83780ms step_avg:60.49ms
step:1386/2315 train_time:83842ms step_avg:60.49ms
step:1387/2315 train_time:83903ms step_avg:60.49ms
step:1388/2315 train_time:83965ms step_avg:60.49ms
step:1389/2315 train_time:84025ms step_avg:60.49ms
step:1390/2315 train_time:84086ms step_avg:60.49ms
step:1391/2315 train_time:84146ms step_avg:60.49ms
step:1392/2315 train_time:84206ms step_avg:60.49ms
step:1393/2315 train_time:84267ms step_avg:60.49ms
step:1394/2315 train_time:84328ms step_avg:60.49ms
step:1395/2315 train_time:84390ms step_avg:60.49ms
step:1396/2315 train_time:84451ms step_avg:60.49ms
step:1397/2315 train_time:84512ms step_avg:60.50ms
step:1398/2315 train_time:84573ms step_avg:60.50ms
step:1399/2315 train_time:84634ms step_avg:60.50ms
step:1400/2315 train_time:84696ms step_avg:60.50ms
step:1401/2315 train_time:84757ms step_avg:60.50ms
step:1402/2315 train_time:84819ms step_avg:60.50ms
step:1403/2315 train_time:84879ms step_avg:60.50ms
step:1404/2315 train_time:84941ms step_avg:60.50ms
step:1405/2315 train_time:85002ms step_avg:60.50ms
step:1406/2315 train_time:85063ms step_avg:60.50ms
step:1407/2315 train_time:85124ms step_avg:60.50ms
step:1408/2315 train_time:85185ms step_avg:60.50ms
step:1409/2315 train_time:85245ms step_avg:60.50ms
step:1410/2315 train_time:85306ms step_avg:60.50ms
step:1411/2315 train_time:85366ms step_avg:60.50ms
step:1412/2315 train_time:85427ms step_avg:60.50ms
step:1413/2315 train_time:85486ms step_avg:60.50ms
step:1414/2315 train_time:85547ms step_avg:60.50ms
step:1415/2315 train_time:85608ms step_avg:60.50ms
step:1416/2315 train_time:85669ms step_avg:60.50ms
step:1417/2315 train_time:85730ms step_avg:60.50ms
step:1418/2315 train_time:85792ms step_avg:60.50ms
step:1419/2315 train_time:85853ms step_avg:60.50ms
step:1420/2315 train_time:85915ms step_avg:60.50ms
step:1421/2315 train_time:85976ms step_avg:60.50ms
step:1422/2315 train_time:86038ms step_avg:60.51ms
step:1423/2315 train_time:86100ms step_avg:60.51ms
step:1424/2315 train_time:86160ms step_avg:60.51ms
step:1425/2315 train_time:86221ms step_avg:60.51ms
step:1426/2315 train_time:86282ms step_avg:60.51ms
step:1427/2315 train_time:86343ms step_avg:60.51ms
step:1428/2315 train_time:86404ms step_avg:60.51ms
step:1429/2315 train_time:86465ms step_avg:60.51ms
step:1430/2315 train_time:86526ms step_avg:60.51ms
step:1431/2315 train_time:86586ms step_avg:60.51ms
step:1432/2315 train_time:86647ms step_avg:60.51ms
step:1433/2315 train_time:86707ms step_avg:60.51ms
step:1434/2315 train_time:86768ms step_avg:60.51ms
step:1435/2315 train_time:86829ms step_avg:60.51ms
step:1436/2315 train_time:86891ms step_avg:60.51ms
step:1437/2315 train_time:86952ms step_avg:60.51ms
step:1438/2315 train_time:87015ms step_avg:60.51ms
step:1439/2315 train_time:87076ms step_avg:60.51ms
step:1440/2315 train_time:87137ms step_avg:60.51ms
step:1441/2315 train_time:87198ms step_avg:60.51ms
step:1442/2315 train_time:87259ms step_avg:60.51ms
step:1443/2315 train_time:87320ms step_avg:60.51ms
step:1444/2315 train_time:87382ms step_avg:60.51ms
step:1445/2315 train_time:87443ms step_avg:60.51ms
step:1446/2315 train_time:87504ms step_avg:60.51ms
step:1447/2315 train_time:87564ms step_avg:60.51ms
step:1448/2315 train_time:87625ms step_avg:60.51ms
step:1449/2315 train_time:87685ms step_avg:60.51ms
step:1450/2315 train_time:87746ms step_avg:60.51ms
step:1451/2315 train_time:87806ms step_avg:60.51ms
step:1452/2315 train_time:87867ms step_avg:60.51ms
step:1453/2315 train_time:87927ms step_avg:60.51ms
step:1454/2315 train_time:87989ms step_avg:60.51ms
step:1455/2315 train_time:88050ms step_avg:60.52ms
step:1456/2315 train_time:88112ms step_avg:60.52ms
step:1457/2315 train_time:88174ms step_avg:60.52ms
step:1458/2315 train_time:88235ms step_avg:60.52ms
step:1459/2315 train_time:88297ms step_avg:60.52ms
step:1460/2315 train_time:88358ms step_avg:60.52ms
step:1461/2315 train_time:88418ms step_avg:60.52ms
step:1462/2315 train_time:88480ms step_avg:60.52ms
step:1463/2315 train_time:88540ms step_avg:60.52ms
step:1464/2315 train_time:88601ms step_avg:60.52ms
step:1465/2315 train_time:88662ms step_avg:60.52ms
step:1466/2315 train_time:88724ms step_avg:60.52ms
step:1467/2315 train_time:88784ms step_avg:60.52ms
step:1468/2315 train_time:88845ms step_avg:60.52ms
step:1469/2315 train_time:88906ms step_avg:60.52ms
step:1470/2315 train_time:88967ms step_avg:60.52ms
step:1471/2315 train_time:89027ms step_avg:60.52ms
step:1472/2315 train_time:89087ms step_avg:60.52ms
step:1473/2315 train_time:89148ms step_avg:60.52ms
step:1474/2315 train_time:89210ms step_avg:60.52ms
step:1475/2315 train_time:89270ms step_avg:60.52ms
step:1476/2315 train_time:89332ms step_avg:60.52ms
step:1477/2315 train_time:89393ms step_avg:60.52ms
step:1478/2315 train_time:89456ms step_avg:60.52ms
step:1479/2315 train_time:89516ms step_avg:60.52ms
step:1480/2315 train_time:89577ms step_avg:60.53ms
step:1481/2315 train_time:89638ms step_avg:60.53ms
step:1482/2315 train_time:89699ms step_avg:60.53ms
step:1483/2315 train_time:89760ms step_avg:60.53ms
step:1484/2315 train_time:89821ms step_avg:60.53ms
step:1485/2315 train_time:89883ms step_avg:60.53ms
step:1486/2315 train_time:89944ms step_avg:60.53ms
step:1487/2315 train_time:90005ms step_avg:60.53ms
step:1488/2315 train_time:90066ms step_avg:60.53ms
step:1489/2315 train_time:90126ms step_avg:60.53ms
step:1490/2315 train_time:90187ms step_avg:60.53ms
step:1491/2315 train_time:90247ms step_avg:60.53ms
step:1492/2315 train_time:90308ms step_avg:60.53ms
step:1493/2315 train_time:90368ms step_avg:60.53ms
step:1494/2315 train_time:90429ms step_avg:60.53ms
step:1495/2315 train_time:90490ms step_avg:60.53ms
step:1496/2315 train_time:90552ms step_avg:60.53ms
step:1497/2315 train_time:90613ms step_avg:60.53ms
step:1498/2315 train_time:90674ms step_avg:60.53ms
step:1499/2315 train_time:90734ms step_avg:60.53ms
step:1500/2315 train_time:90796ms step_avg:60.53ms
step:1500/2315 val_loss:3.4483 train_time:90859ms step_avg:60.57ms
step:1501/2315 train_time:90877ms step_avg:60.54ms
step:1502/2315 train_time:90921ms step_avg:60.53ms
step:1503/2315 train_time:90985ms step_avg:60.54ms
step:1504/2315 train_time:91047ms step_avg:60.54ms
step:1505/2315 train_time:91108ms step_avg:60.54ms
step:1506/2315 train_time:91169ms step_avg:60.54ms
step:1507/2315 train_time:91229ms step_avg:60.54ms
step:1508/2315 train_time:91289ms step_avg:60.54ms
step:1509/2315 train_time:91350ms step_avg:60.54ms
step:1510/2315 train_time:91410ms step_avg:60.54ms
step:1511/2315 train_time:91470ms step_avg:60.54ms
step:1512/2315 train_time:91531ms step_avg:60.54ms
step:1513/2315 train_time:91591ms step_avg:60.54ms
step:1514/2315 train_time:91652ms step_avg:60.54ms
step:1515/2315 train_time:91713ms step_avg:60.54ms
step:1516/2315 train_time:91775ms step_avg:60.54ms
step:1517/2315 train_time:91837ms step_avg:60.54ms
step:1518/2315 train_time:91899ms step_avg:60.54ms
step:1519/2315 train_time:91960ms step_avg:60.54ms
step:1520/2315 train_time:92023ms step_avg:60.54ms
step:1521/2315 train_time:92085ms step_avg:60.54ms
step:1522/2315 train_time:92147ms step_avg:60.54ms
step:1523/2315 train_time:92208ms step_avg:60.54ms
step:1524/2315 train_time:92269ms step_avg:60.54ms
step:1525/2315 train_time:92329ms step_avg:60.54ms
step:1526/2315 train_time:92390ms step_avg:60.54ms
step:1527/2315 train_time:92451ms step_avg:60.54ms
step:1528/2315 train_time:92512ms step_avg:60.54ms
step:1529/2315 train_time:92572ms step_avg:60.54ms
step:1530/2315 train_time:92633ms step_avg:60.54ms
step:1531/2315 train_time:92694ms step_avg:60.54ms
step:1532/2315 train_time:92756ms step_avg:60.55ms
step:1533/2315 train_time:92817ms step_avg:60.55ms
step:1534/2315 train_time:92880ms step_avg:60.55ms
step:1535/2315 train_time:92941ms step_avg:60.55ms
step:1536/2315 train_time:93003ms step_avg:60.55ms
step:1537/2315 train_time:93064ms step_avg:60.55ms
step:1538/2315 train_time:93125ms step_avg:60.55ms
step:1539/2315 train_time:93187ms step_avg:60.55ms
step:1540/2315 train_time:93248ms step_avg:60.55ms
step:1541/2315 train_time:93309ms step_avg:60.55ms
step:1542/2315 train_time:93370ms step_avg:60.55ms
step:1543/2315 train_time:93430ms step_avg:60.55ms
step:1544/2315 train_time:93491ms step_avg:60.55ms
step:1545/2315 train_time:93552ms step_avg:60.55ms
step:1546/2315 train_time:93613ms step_avg:60.55ms
step:1547/2315 train_time:93675ms step_avg:60.55ms
step:1548/2315 train_time:93737ms step_avg:60.55ms
step:1549/2315 train_time:93798ms step_avg:60.55ms
step:1550/2315 train_time:93860ms step_avg:60.55ms
step:1551/2315 train_time:93921ms step_avg:60.56ms
step:1552/2315 train_time:93983ms step_avg:60.56ms
step:1553/2315 train_time:94044ms step_avg:60.56ms
step:1554/2315 train_time:94106ms step_avg:60.56ms
step:1555/2315 train_time:94168ms step_avg:60.56ms
step:1556/2315 train_time:94230ms step_avg:60.56ms
step:1557/2315 train_time:94290ms step_avg:60.56ms
step:1558/2315 train_time:94351ms step_avg:60.56ms
step:1559/2315 train_time:94412ms step_avg:60.56ms
step:1560/2315 train_time:94473ms step_avg:60.56ms
step:1561/2315 train_time:94534ms step_avg:60.56ms
step:1562/2315 train_time:94596ms step_avg:60.56ms
step:1563/2315 train_time:94656ms step_avg:60.56ms
step:1564/2315 train_time:94717ms step_avg:60.56ms
step:1565/2315 train_time:94778ms step_avg:60.56ms
step:1566/2315 train_time:94840ms step_avg:60.56ms
step:1567/2315 train_time:94901ms step_avg:60.56ms
step:1568/2315 train_time:94963ms step_avg:60.56ms
step:1569/2315 train_time:95025ms step_avg:60.56ms
step:1570/2315 train_time:95088ms step_avg:60.57ms
step:1571/2315 train_time:95149ms step_avg:60.57ms
step:1572/2315 train_time:95210ms step_avg:60.57ms
step:1573/2315 train_time:95271ms step_avg:60.57ms
step:1574/2315 train_time:95332ms step_avg:60.57ms
step:1575/2315 train_time:95392ms step_avg:60.57ms
step:1576/2315 train_time:95454ms step_avg:60.57ms
step:1577/2315 train_time:95514ms step_avg:60.57ms
step:1578/2315 train_time:95576ms step_avg:60.57ms
step:1579/2315 train_time:95636ms step_avg:60.57ms
step:1580/2315 train_time:95697ms step_avg:60.57ms
step:1581/2315 train_time:95758ms step_avg:60.57ms
step:1582/2315 train_time:95820ms step_avg:60.57ms
step:1583/2315 train_time:95881ms step_avg:60.57ms
step:1584/2315 train_time:95942ms step_avg:60.57ms
step:1585/2315 train_time:96004ms step_avg:60.57ms
step:1586/2315 train_time:96066ms step_avg:60.57ms
step:1587/2315 train_time:96127ms step_avg:60.57ms
step:1588/2315 train_time:96189ms step_avg:60.57ms
step:1589/2315 train_time:96251ms step_avg:60.57ms
step:1590/2315 train_time:96312ms step_avg:60.57ms
step:1591/2315 train_time:96373ms step_avg:60.57ms
step:1592/2315 train_time:96434ms step_avg:60.57ms
step:1593/2315 train_time:96495ms step_avg:60.57ms
step:1594/2315 train_time:96557ms step_avg:60.58ms
step:1595/2315 train_time:96617ms step_avg:60.58ms
step:1596/2315 train_time:96679ms step_avg:60.58ms
step:1597/2315 train_time:96740ms step_avg:60.58ms
step:1598/2315 train_time:96802ms step_avg:60.58ms
step:1599/2315 train_time:96862ms step_avg:60.58ms
step:1600/2315 train_time:96924ms step_avg:60.58ms
step:1601/2315 train_time:96985ms step_avg:60.58ms
step:1602/2315 train_time:97047ms step_avg:60.58ms
step:1603/2315 train_time:97107ms step_avg:60.58ms
step:1604/2315 train_time:97169ms step_avg:60.58ms
step:1605/2315 train_time:97230ms step_avg:60.58ms
step:1606/2315 train_time:97292ms step_avg:60.58ms
step:1607/2315 train_time:97353ms step_avg:60.58ms
step:1608/2315 train_time:97414ms step_avg:60.58ms
step:1609/2315 train_time:97476ms step_avg:60.58ms
step:1610/2315 train_time:97538ms step_avg:60.58ms
step:1611/2315 train_time:97598ms step_avg:60.58ms
step:1612/2315 train_time:97660ms step_avg:60.58ms
step:1613/2315 train_time:97720ms step_avg:60.58ms
step:1614/2315 train_time:97782ms step_avg:60.58ms
step:1615/2315 train_time:97843ms step_avg:60.58ms
step:1616/2315 train_time:97905ms step_avg:60.58ms
step:1617/2315 train_time:97966ms step_avg:60.59ms
step:1618/2315 train_time:98028ms step_avg:60.59ms
step:1619/2315 train_time:98089ms step_avg:60.59ms
step:1620/2315 train_time:98151ms step_avg:60.59ms
step:1621/2315 train_time:98212ms step_avg:60.59ms
step:1622/2315 train_time:98273ms step_avg:60.59ms
step:1623/2315 train_time:98334ms step_avg:60.59ms
step:1624/2315 train_time:98395ms step_avg:60.59ms
step:1625/2315 train_time:98456ms step_avg:60.59ms
step:1626/2315 train_time:98517ms step_avg:60.59ms
step:1627/2315 train_time:98578ms step_avg:60.59ms
step:1628/2315 train_time:98639ms step_avg:60.59ms
step:1629/2315 train_time:98700ms step_avg:60.59ms
step:1630/2315 train_time:98762ms step_avg:60.59ms
step:1631/2315 train_time:98823ms step_avg:60.59ms
step:1632/2315 train_time:98886ms step_avg:60.59ms
step:1633/2315 train_time:98947ms step_avg:60.59ms
step:1634/2315 train_time:99009ms step_avg:60.59ms
step:1635/2315 train_time:99070ms step_avg:60.59ms
step:1636/2315 train_time:99132ms step_avg:60.59ms
step:1637/2315 train_time:99192ms step_avg:60.59ms
step:1638/2315 train_time:99254ms step_avg:60.59ms
step:1639/2315 train_time:99315ms step_avg:60.59ms
step:1640/2315 train_time:99376ms step_avg:60.60ms
step:1641/2315 train_time:99437ms step_avg:60.60ms
step:1642/2315 train_time:99498ms step_avg:60.60ms
step:1643/2315 train_time:99559ms step_avg:60.60ms
step:1644/2315 train_time:99620ms step_avg:60.60ms
step:1645/2315 train_time:99681ms step_avg:60.60ms
step:1646/2315 train_time:99743ms step_avg:60.60ms
step:1647/2315 train_time:99804ms step_avg:60.60ms
step:1648/2315 train_time:99866ms step_avg:60.60ms
step:1649/2315 train_time:99927ms step_avg:60.60ms
step:1650/2315 train_time:99989ms step_avg:60.60ms
step:1651/2315 train_time:100050ms step_avg:60.60ms
step:1652/2315 train_time:100111ms step_avg:60.60ms
step:1653/2315 train_time:100173ms step_avg:60.60ms
step:1654/2315 train_time:100234ms step_avg:60.60ms
step:1655/2315 train_time:100295ms step_avg:60.60ms
step:1656/2315 train_time:100357ms step_avg:60.60ms
step:1657/2315 train_time:100417ms step_avg:60.60ms
step:1658/2315 train_time:100478ms step_avg:60.60ms
step:1659/2315 train_time:100539ms step_avg:60.60ms
step:1660/2315 train_time:100600ms step_avg:60.60ms
step:1661/2315 train_time:100661ms step_avg:60.60ms
step:1662/2315 train_time:100722ms step_avg:60.60ms
step:1663/2315 train_time:100784ms step_avg:60.60ms
step:1664/2315 train_time:100845ms step_avg:60.60ms
step:1665/2315 train_time:100906ms step_avg:60.60ms
step:1666/2315 train_time:100968ms step_avg:60.61ms
step:1667/2315 train_time:101030ms step_avg:60.61ms
step:1668/2315 train_time:101091ms step_avg:60.61ms
step:1669/2315 train_time:101153ms step_avg:60.61ms
step:1670/2315 train_time:101215ms step_avg:60.61ms
step:1671/2315 train_time:101276ms step_avg:60.61ms
step:1672/2315 train_time:101337ms step_avg:60.61ms
step:1673/2315 train_time:101398ms step_avg:60.61ms
step:1674/2315 train_time:101459ms step_avg:60.61ms
step:1675/2315 train_time:101520ms step_avg:60.61ms
step:1676/2315 train_time:101581ms step_avg:60.61ms
step:1677/2315 train_time:101642ms step_avg:60.61ms
step:1678/2315 train_time:101703ms step_avg:60.61ms
step:1679/2315 train_time:101765ms step_avg:60.61ms
step:1680/2315 train_time:101827ms step_avg:60.61ms
step:1681/2315 train_time:101888ms step_avg:60.61ms
step:1682/2315 train_time:101949ms step_avg:60.61ms
step:1683/2315 train_time:102010ms step_avg:60.61ms
step:1684/2315 train_time:102072ms step_avg:60.61ms
step:1685/2315 train_time:102134ms step_avg:60.61ms
step:1686/2315 train_time:102195ms step_avg:60.61ms
step:1687/2315 train_time:102256ms step_avg:60.61ms
step:1688/2315 train_time:102317ms step_avg:60.61ms
step:1689/2315 train_time:102379ms step_avg:60.61ms
step:1690/2315 train_time:102440ms step_avg:60.62ms
step:1691/2315 train_time:102501ms step_avg:60.62ms
step:1692/2315 train_time:102563ms step_avg:60.62ms
step:1693/2315 train_time:102624ms step_avg:60.62ms
step:1694/2315 train_time:102685ms step_avg:60.62ms
step:1695/2315 train_time:102746ms step_avg:60.62ms
step:1696/2315 train_time:102808ms step_avg:60.62ms
step:1697/2315 train_time:102869ms step_avg:60.62ms
step:1698/2315 train_time:102930ms step_avg:60.62ms
step:1699/2315 train_time:102991ms step_avg:60.62ms
step:1700/2315 train_time:103053ms step_avg:60.62ms
step:1701/2315 train_time:103114ms step_avg:60.62ms
step:1702/2315 train_time:103176ms step_avg:60.62ms
step:1703/2315 train_time:103237ms step_avg:60.62ms
step:1704/2315 train_time:103298ms step_avg:60.62ms
step:1705/2315 train_time:103359ms step_avg:60.62ms
step:1706/2315 train_time:103421ms step_avg:60.62ms
step:1707/2315 train_time:103482ms step_avg:60.62ms
step:1708/2315 train_time:103544ms step_avg:60.62ms
step:1709/2315 train_time:103605ms step_avg:60.62ms
step:1710/2315 train_time:103666ms step_avg:60.62ms
step:1711/2315 train_time:103728ms step_avg:60.62ms
step:1712/2315 train_time:103789ms step_avg:60.62ms
step:1713/2315 train_time:103850ms step_avg:60.62ms
step:1714/2315 train_time:103912ms step_avg:60.63ms
step:1715/2315 train_time:103973ms step_avg:60.63ms
step:1716/2315 train_time:104035ms step_avg:60.63ms
step:1717/2315 train_time:104096ms step_avg:60.63ms
step:1718/2315 train_time:104157ms step_avg:60.63ms
step:1719/2315 train_time:104218ms step_avg:60.63ms
step:1720/2315 train_time:104279ms step_avg:60.63ms
step:1721/2315 train_time:104341ms step_avg:60.63ms
step:1722/2315 train_time:104402ms step_avg:60.63ms
step:1723/2315 train_time:104463ms step_avg:60.63ms
step:1724/2315 train_time:104525ms step_avg:60.63ms
step:1725/2315 train_time:104586ms step_avg:60.63ms
step:1726/2315 train_time:104648ms step_avg:60.63ms
step:1727/2315 train_time:104709ms step_avg:60.63ms
step:1728/2315 train_time:104771ms step_avg:60.63ms
step:1729/2315 train_time:104832ms step_avg:60.63ms
step:1730/2315 train_time:104893ms step_avg:60.63ms
step:1731/2315 train_time:104954ms step_avg:60.63ms
step:1732/2315 train_time:105016ms step_avg:60.63ms
step:1733/2315 train_time:105077ms step_avg:60.63ms
step:1734/2315 train_time:105139ms step_avg:60.63ms
step:1735/2315 train_time:105199ms step_avg:60.63ms
step:1736/2315 train_time:105260ms step_avg:60.63ms
step:1737/2315 train_time:105322ms step_avg:60.63ms
step:1738/2315 train_time:105384ms step_avg:60.64ms
step:1739/2315 train_time:105445ms step_avg:60.64ms
step:1740/2315 train_time:105507ms step_avg:60.64ms
step:1741/2315 train_time:105568ms step_avg:60.64ms
step:1742/2315 train_time:105630ms step_avg:60.64ms
step:1743/2315 train_time:105691ms step_avg:60.64ms
step:1744/2315 train_time:105753ms step_avg:60.64ms
step:1745/2315 train_time:105814ms step_avg:60.64ms
step:1746/2315 train_time:105875ms step_avg:60.64ms
step:1747/2315 train_time:105936ms step_avg:60.64ms
step:1748/2315 train_time:105997ms step_avg:60.64ms
step:1749/2315 train_time:106058ms step_avg:60.64ms
step:1750/2315 train_time:106120ms step_avg:60.64ms
step:1750/2315 val_loss:3.3797 train_time:106183ms step_avg:60.68ms
step:1751/2315 train_time:106201ms step_avg:60.65ms
step:1752/2315 train_time:106245ms step_avg:60.64ms
step:1753/2315 train_time:106313ms step_avg:60.65ms
step:1754/2315 train_time:106377ms step_avg:60.65ms
step:1755/2315 train_time:106439ms step_avg:60.65ms
step:1756/2315 train_time:106500ms step_avg:60.65ms
step:1757/2315 train_time:106560ms step_avg:60.65ms
step:1758/2315 train_time:106621ms step_avg:60.65ms
step:1759/2315 train_time:106681ms step_avg:60.65ms
step:1760/2315 train_time:106741ms step_avg:60.65ms
step:1761/2315 train_time:106801ms step_avg:60.65ms
step:1762/2315 train_time:106861ms step_avg:60.65ms
step:1763/2315 train_time:106921ms step_avg:60.65ms
step:1764/2315 train_time:106982ms step_avg:60.65ms
step:1765/2315 train_time:107042ms step_avg:60.65ms
step:1766/2315 train_time:107107ms step_avg:60.65ms
step:1767/2315 train_time:107169ms step_avg:60.65ms
step:1768/2315 train_time:107232ms step_avg:60.65ms
step:1769/2315 train_time:107294ms step_avg:60.65ms
step:1770/2315 train_time:107357ms step_avg:60.65ms
step:1771/2315 train_time:107419ms step_avg:60.65ms
step:1772/2315 train_time:107481ms step_avg:60.66ms
step:1773/2315 train_time:107542ms step_avg:60.66ms
step:1774/2315 train_time:107603ms step_avg:60.66ms
step:1775/2315 train_time:107663ms step_avg:60.66ms
step:1776/2315 train_time:107725ms step_avg:60.66ms
step:1777/2315 train_time:107785ms step_avg:60.66ms
step:1778/2315 train_time:107847ms step_avg:60.66ms
step:1779/2315 train_time:107908ms step_avg:60.66ms
step:1780/2315 train_time:107969ms step_avg:60.66ms
step:1781/2315 train_time:108029ms step_avg:60.66ms
step:1782/2315 train_time:108090ms step_avg:60.66ms
step:1783/2315 train_time:108152ms step_avg:60.66ms
step:1784/2315 train_time:108214ms step_avg:60.66ms
step:1785/2315 train_time:108276ms step_avg:60.66ms
step:1786/2315 train_time:108338ms step_avg:60.66ms
step:1787/2315 train_time:108399ms step_avg:60.66ms
step:1788/2315 train_time:108462ms step_avg:60.66ms
step:1789/2315 train_time:108523ms step_avg:60.66ms
step:1790/2315 train_time:108584ms step_avg:60.66ms
step:1791/2315 train_time:108645ms step_avg:60.66ms
step:1792/2315 train_time:108705ms step_avg:60.66ms
step:1793/2315 train_time:108766ms step_avg:60.66ms
step:1794/2315 train_time:108827ms step_avg:60.66ms
step:1795/2315 train_time:108887ms step_avg:60.66ms
step:1796/2315 train_time:108948ms step_avg:60.66ms
step:1797/2315 train_time:109009ms step_avg:60.66ms
step:1798/2315 train_time:109070ms step_avg:60.66ms
step:1799/2315 train_time:109131ms step_avg:60.66ms
step:1800/2315 train_time:109194ms step_avg:60.66ms
step:1801/2315 train_time:109255ms step_avg:60.66ms
step:1802/2315 train_time:109317ms step_avg:60.66ms
step:1803/2315 train_time:109378ms step_avg:60.66ms
step:1804/2315 train_time:109441ms step_avg:60.67ms
step:1805/2315 train_time:109502ms step_avg:60.67ms
step:1806/2315 train_time:109563ms step_avg:60.67ms
step:1807/2315 train_time:109624ms step_avg:60.67ms
step:1808/2315 train_time:109686ms step_avg:60.67ms
step:1809/2315 train_time:109746ms step_avg:60.67ms
step:1810/2315 train_time:109808ms step_avg:60.67ms
step:1811/2315 train_time:109868ms step_avg:60.67ms
step:1812/2315 train_time:109930ms step_avg:60.67ms
step:1813/2315 train_time:109990ms step_avg:60.67ms
step:1814/2315 train_time:110052ms step_avg:60.67ms
step:1815/2315 train_time:110113ms step_avg:60.67ms
step:1816/2315 train_time:110175ms step_avg:60.67ms
step:1817/2315 train_time:110236ms step_avg:60.67ms
step:1818/2315 train_time:110298ms step_avg:60.67ms
step:1819/2315 train_time:110359ms step_avg:60.67ms
step:1820/2315 train_time:110421ms step_avg:60.67ms
step:1821/2315 train_time:110482ms step_avg:60.67ms
step:1822/2315 train_time:110545ms step_avg:60.67ms
step:1823/2315 train_time:110606ms step_avg:60.67ms
step:1824/2315 train_time:110667ms step_avg:60.67ms
step:1825/2315 train_time:110728ms step_avg:60.67ms
step:1826/2315 train_time:110789ms step_avg:60.67ms
step:1827/2315 train_time:110850ms step_avg:60.67ms
step:1828/2315 train_time:110911ms step_avg:60.67ms
step:1829/2315 train_time:110972ms step_avg:60.67ms
step:1830/2315 train_time:111033ms step_avg:60.67ms
step:1831/2315 train_time:111094ms step_avg:60.67ms
step:1832/2315 train_time:111155ms step_avg:60.67ms
step:1833/2315 train_time:111216ms step_avg:60.67ms
step:1834/2315 train_time:111278ms step_avg:60.67ms
step:1835/2315 train_time:111339ms step_avg:60.68ms
step:1836/2315 train_time:111401ms step_avg:60.68ms
step:1837/2315 train_time:111461ms step_avg:60.68ms
step:1838/2315 train_time:111524ms step_avg:60.68ms
step:1839/2315 train_time:111585ms step_avg:60.68ms
step:1840/2315 train_time:111647ms step_avg:60.68ms
step:1841/2315 train_time:111708ms step_avg:60.68ms
step:1842/2315 train_time:111769ms step_avg:60.68ms
step:1843/2315 train_time:111830ms step_avg:60.68ms
step:1844/2315 train_time:111892ms step_avg:60.68ms
step:1845/2315 train_time:111952ms step_avg:60.68ms
step:1846/2315 train_time:112014ms step_avg:60.68ms
step:1847/2315 train_time:112074ms step_avg:60.68ms
step:1848/2315 train_time:112135ms step_avg:60.68ms
step:1849/2315 train_time:112196ms step_avg:60.68ms
step:1850/2315 train_time:112258ms step_avg:60.68ms
step:1851/2315 train_time:112319ms step_avg:60.68ms
step:1852/2315 train_time:112381ms step_avg:60.68ms
step:1853/2315 train_time:112442ms step_avg:60.68ms
step:1854/2315 train_time:112504ms step_avg:60.68ms
step:1855/2315 train_time:112566ms step_avg:60.68ms
step:1856/2315 train_time:112628ms step_avg:60.68ms
step:1857/2315 train_time:112689ms step_avg:60.68ms
step:1858/2315 train_time:112751ms step_avg:60.68ms
step:1859/2315 train_time:112811ms step_avg:60.68ms
step:1860/2315 train_time:112872ms step_avg:60.68ms
step:1861/2315 train_time:112933ms step_avg:60.68ms
step:1862/2315 train_time:112994ms step_avg:60.68ms
step:1863/2315 train_time:113055ms step_avg:60.68ms
step:1864/2315 train_time:113115ms step_avg:60.68ms
step:1865/2315 train_time:113176ms step_avg:60.68ms
step:1866/2315 train_time:113238ms step_avg:60.68ms
step:1867/2315 train_time:113298ms step_avg:60.68ms
step:1868/2315 train_time:113360ms step_avg:60.69ms
step:1869/2315 train_time:113422ms step_avg:60.69ms
step:1870/2315 train_time:113483ms step_avg:60.69ms
step:1871/2315 train_time:113545ms step_avg:60.69ms
step:1872/2315 train_time:113607ms step_avg:60.69ms
step:1873/2315 train_time:113668ms step_avg:60.69ms
step:1874/2315 train_time:113729ms step_avg:60.69ms
step:1875/2315 train_time:113790ms step_avg:60.69ms
step:1876/2315 train_time:113853ms step_avg:60.69ms
step:1877/2315 train_time:113913ms step_avg:60.69ms
step:1878/2315 train_time:113975ms step_avg:60.69ms
step:1879/2315 train_time:114036ms step_avg:60.69ms
step:1880/2315 train_time:114098ms step_avg:60.69ms
step:1881/2315 train_time:114159ms step_avg:60.69ms
step:1882/2315 train_time:114221ms step_avg:60.69ms
step:1883/2315 train_time:114281ms step_avg:60.69ms
step:1884/2315 train_time:114343ms step_avg:60.69ms
step:1885/2315 train_time:114404ms step_avg:60.69ms
step:1886/2315 train_time:114465ms step_avg:60.69ms
step:1887/2315 train_time:114527ms step_avg:60.69ms
step:1888/2315 train_time:114588ms step_avg:60.69ms
step:1889/2315 train_time:114650ms step_avg:60.69ms
step:1890/2315 train_time:114712ms step_avg:60.69ms
step:1891/2315 train_time:114772ms step_avg:60.69ms
step:1892/2315 train_time:114834ms step_avg:60.69ms
step:1893/2315 train_time:114894ms step_avg:60.69ms
step:1894/2315 train_time:114956ms step_avg:60.69ms
step:1895/2315 train_time:115017ms step_avg:60.69ms
step:1896/2315 train_time:115078ms step_avg:60.70ms
step:1897/2315 train_time:115139ms step_avg:60.70ms
step:1898/2315 train_time:115200ms step_avg:60.70ms
step:1899/2315 train_time:115262ms step_avg:60.70ms
step:1900/2315 train_time:115324ms step_avg:60.70ms
step:1901/2315 train_time:115385ms step_avg:60.70ms
step:1902/2315 train_time:115447ms step_avg:60.70ms
step:1903/2315 train_time:115507ms step_avg:60.70ms
step:1904/2315 train_time:115569ms step_avg:60.70ms
step:1905/2315 train_time:115630ms step_avg:60.70ms
step:1906/2315 train_time:115691ms step_avg:60.70ms
step:1907/2315 train_time:115752ms step_avg:60.70ms
step:1908/2315 train_time:115814ms step_avg:60.70ms
step:1909/2315 train_time:115874ms step_avg:60.70ms
step:1910/2315 train_time:115936ms step_avg:60.70ms
step:1911/2315 train_time:115996ms step_avg:60.70ms
step:1912/2315 train_time:116058ms step_avg:60.70ms
step:1913/2315 train_time:116119ms step_avg:60.70ms
step:1914/2315 train_time:116180ms step_avg:60.70ms
step:1915/2315 train_time:116241ms step_avg:60.70ms
step:1916/2315 train_time:116302ms step_avg:60.70ms
step:1917/2315 train_time:116364ms step_avg:60.70ms
step:1918/2315 train_time:116425ms step_avg:60.70ms
step:1919/2315 train_time:116487ms step_avg:60.70ms
step:1920/2315 train_time:116549ms step_avg:60.70ms
step:1921/2315 train_time:116610ms step_avg:60.70ms
step:1922/2315 train_time:116672ms step_avg:60.70ms
step:1923/2315 train_time:116734ms step_avg:60.70ms
step:1924/2315 train_time:116795ms step_avg:60.70ms
step:1925/2315 train_time:116855ms step_avg:60.70ms
step:1926/2315 train_time:116917ms step_avg:60.70ms
step:1927/2315 train_time:116977ms step_avg:60.70ms
step:1928/2315 train_time:117039ms step_avg:60.70ms
step:1929/2315 train_time:117100ms step_avg:60.71ms
step:1930/2315 train_time:117161ms step_avg:60.71ms
step:1931/2315 train_time:117222ms step_avg:60.71ms
step:1932/2315 train_time:117284ms step_avg:60.71ms
step:1933/2315 train_time:117344ms step_avg:60.71ms
step:1934/2315 train_time:117406ms step_avg:60.71ms
step:1935/2315 train_time:117468ms step_avg:60.71ms
step:1936/2315 train_time:117530ms step_avg:60.71ms
step:1937/2315 train_time:117591ms step_avg:60.71ms
step:1938/2315 train_time:117652ms step_avg:60.71ms
step:1939/2315 train_time:117713ms step_avg:60.71ms
step:1940/2315 train_time:117774ms step_avg:60.71ms
step:1941/2315 train_time:117835ms step_avg:60.71ms
step:1942/2315 train_time:117896ms step_avg:60.71ms
step:1943/2315 train_time:117957ms step_avg:60.71ms
step:1944/2315 train_time:118019ms step_avg:60.71ms
step:1945/2315 train_time:118080ms step_avg:60.71ms
step:1946/2315 train_time:118141ms step_avg:60.71ms
step:1947/2315 train_time:118202ms step_avg:60.71ms
step:1948/2315 train_time:118264ms step_avg:60.71ms
step:1949/2315 train_time:118326ms step_avg:60.71ms
step:1950/2315 train_time:118387ms step_avg:60.71ms
step:1951/2315 train_time:118448ms step_avg:60.71ms
step:1952/2315 train_time:118510ms step_avg:60.71ms
step:1953/2315 train_time:118571ms step_avg:60.71ms
step:1954/2315 train_time:118632ms step_avg:60.71ms
step:1955/2315 train_time:118693ms step_avg:60.71ms
step:1956/2315 train_time:118755ms step_avg:60.71ms
step:1957/2315 train_time:118815ms step_avg:60.71ms
step:1958/2315 train_time:118877ms step_avg:60.71ms
step:1959/2315 train_time:118938ms step_avg:60.71ms
step:1960/2315 train_time:119000ms step_avg:60.71ms
step:1961/2315 train_time:119061ms step_avg:60.71ms
step:1962/2315 train_time:119123ms step_avg:60.71ms
step:1963/2315 train_time:119184ms step_avg:60.72ms
step:1964/2315 train_time:119246ms step_avg:60.72ms
step:1965/2315 train_time:119307ms step_avg:60.72ms
step:1966/2315 train_time:119369ms step_avg:60.72ms
step:1967/2315 train_time:119430ms step_avg:60.72ms
step:1968/2315 train_time:119491ms step_avg:60.72ms
step:1969/2315 train_time:119552ms step_avg:60.72ms
step:1970/2315 train_time:119614ms step_avg:60.72ms
step:1971/2315 train_time:119675ms step_avg:60.72ms
step:1972/2315 train_time:119736ms step_avg:60.72ms
step:1973/2315 train_time:119797ms step_avg:60.72ms
step:1974/2315 train_time:119859ms step_avg:60.72ms
step:1975/2315 train_time:119920ms step_avg:60.72ms
step:1976/2315 train_time:119981ms step_avg:60.72ms
step:1977/2315 train_time:120042ms step_avg:60.72ms
step:1978/2315 train_time:120103ms step_avg:60.72ms
step:1979/2315 train_time:120165ms step_avg:60.72ms
step:1980/2315 train_time:120227ms step_avg:60.72ms
step:1981/2315 train_time:120288ms step_avg:60.72ms
step:1982/2315 train_time:120349ms step_avg:60.72ms
step:1983/2315 train_time:120411ms step_avg:60.72ms
step:1984/2315 train_time:120472ms step_avg:60.72ms
step:1985/2315 train_time:120533ms step_avg:60.72ms
step:1986/2315 train_time:120594ms step_avg:60.72ms
step:1987/2315 train_time:120655ms step_avg:60.72ms
step:1988/2315 train_time:120716ms step_avg:60.72ms
step:1989/2315 train_time:120776ms step_avg:60.72ms
step:1990/2315 train_time:120838ms step_avg:60.72ms
step:1991/2315 train_time:120899ms step_avg:60.72ms
step:1992/2315 train_time:120961ms step_avg:60.72ms
step:1993/2315 train_time:121022ms step_avg:60.72ms
step:1994/2315 train_time:121084ms step_avg:60.72ms
step:1995/2315 train_time:121146ms step_avg:60.72ms
step:1996/2315 train_time:121207ms step_avg:60.72ms
step:1997/2315 train_time:121268ms step_avg:60.73ms
step:1998/2315 train_time:121330ms step_avg:60.73ms
step:1999/2315 train_time:121391ms step_avg:60.73ms
step:2000/2315 train_time:121452ms step_avg:60.73ms
step:2000/2315 val_loss:3.3298 train_time:121515ms step_avg:60.76ms
step:2001/2315 train_time:121533ms step_avg:60.74ms
step:2002/2315 train_time:121577ms step_avg:60.73ms
step:2003/2315 train_time:121640ms step_avg:60.73ms
step:2004/2315 train_time:121703ms step_avg:60.73ms
step:2005/2315 train_time:121764ms step_avg:60.73ms
step:2006/2315 train_time:121826ms step_avg:60.73ms
step:2007/2315 train_time:121886ms step_avg:60.73ms
step:2008/2315 train_time:121948ms step_avg:60.73ms
step:2009/2315 train_time:122008ms step_avg:60.73ms
step:2010/2315 train_time:122069ms step_avg:60.73ms
step:2011/2315 train_time:122130ms step_avg:60.73ms
step:2012/2315 train_time:122191ms step_avg:60.73ms
step:2013/2315 train_time:122252ms step_avg:60.73ms
step:2014/2315 train_time:122313ms step_avg:60.73ms
step:2015/2315 train_time:122373ms step_avg:60.73ms
step:2016/2315 train_time:122435ms step_avg:60.73ms
step:2017/2315 train_time:122497ms step_avg:60.73ms
step:2018/2315 train_time:122561ms step_avg:60.73ms
step:2019/2315 train_time:122623ms step_avg:60.73ms
step:2020/2315 train_time:122685ms step_avg:60.74ms
step:2021/2315 train_time:122747ms step_avg:60.74ms
step:2022/2315 train_time:122809ms step_avg:60.74ms
step:2023/2315 train_time:122870ms step_avg:60.74ms
step:2024/2315 train_time:122931ms step_avg:60.74ms
step:2025/2315 train_time:122992ms step_avg:60.74ms
step:2026/2315 train_time:123053ms step_avg:60.74ms
step:2027/2315 train_time:123113ms step_avg:60.74ms
step:2028/2315 train_time:123174ms step_avg:60.74ms
step:2029/2315 train_time:123235ms step_avg:60.74ms
step:2030/2315 train_time:123296ms step_avg:60.74ms
step:2031/2315 train_time:123357ms step_avg:60.74ms
step:2032/2315 train_time:123418ms step_avg:60.74ms
step:2033/2315 train_time:123480ms step_avg:60.74ms
step:2034/2315 train_time:123542ms step_avg:60.74ms
step:2035/2315 train_time:123603ms step_avg:60.74ms
step:2036/2315 train_time:123666ms step_avg:60.74ms
step:2037/2315 train_time:123727ms step_avg:60.74ms
step:2038/2315 train_time:123790ms step_avg:60.74ms
step:2039/2315 train_time:123851ms step_avg:60.74ms
step:2040/2315 train_time:123913ms step_avg:60.74ms
step:2041/2315 train_time:123974ms step_avg:60.74ms
step:2042/2315 train_time:124036ms step_avg:60.74ms
step:2043/2315 train_time:124096ms step_avg:60.74ms
step:2044/2315 train_time:124158ms step_avg:60.74ms
step:2045/2315 train_time:124218ms step_avg:60.74ms
step:2046/2315 train_time:124279ms step_avg:60.74ms
step:2047/2315 train_time:124339ms step_avg:60.74ms
step:2048/2315 train_time:124401ms step_avg:60.74ms
step:2049/2315 train_time:124462ms step_avg:60.74ms
step:2050/2315 train_time:124524ms step_avg:60.74ms
step:2051/2315 train_time:124585ms step_avg:60.74ms
step:2052/2315 train_time:124647ms step_avg:60.74ms
step:2053/2315 train_time:124709ms step_avg:60.74ms
step:2054/2315 train_time:124771ms step_avg:60.75ms
step:2055/2315 train_time:124832ms step_avg:60.75ms
step:2056/2315 train_time:124894ms step_avg:60.75ms
step:2057/2315 train_time:124955ms step_avg:60.75ms
step:2058/2315 train_time:125017ms step_avg:60.75ms
step:2059/2315 train_time:125078ms step_avg:60.75ms
step:2060/2315 train_time:125139ms step_avg:60.75ms
step:2061/2315 train_time:125199ms step_avg:60.75ms
step:2062/2315 train_time:125260ms step_avg:60.75ms
step:2063/2315 train_time:125321ms step_avg:60.75ms
step:2064/2315 train_time:125382ms step_avg:60.75ms
step:2065/2315 train_time:125443ms step_avg:60.75ms
step:2066/2315 train_time:125505ms step_avg:60.75ms
step:2067/2315 train_time:125566ms step_avg:60.75ms
step:2068/2315 train_time:125628ms step_avg:60.75ms
step:2069/2315 train_time:125690ms step_avg:60.75ms
step:2070/2315 train_time:125752ms step_avg:60.75ms
step:2071/2315 train_time:125813ms step_avg:60.75ms
step:2072/2315 train_time:125875ms step_avg:60.75ms
step:2073/2315 train_time:125936ms step_avg:60.75ms
step:2074/2315 train_time:125997ms step_avg:60.75ms
step:2075/2315 train_time:126058ms step_avg:60.75ms
step:2076/2315 train_time:126120ms step_avg:60.75ms
step:2077/2315 train_time:126180ms step_avg:60.75ms
step:2078/2315 train_time:126242ms step_avg:60.75ms
step:2079/2315 train_time:126303ms step_avg:60.75ms
step:2080/2315 train_time:126364ms step_avg:60.75ms
step:2081/2315 train_time:126425ms step_avg:60.75ms
step:2082/2315 train_time:126487ms step_avg:60.75ms
step:2083/2315 train_time:126548ms step_avg:60.75ms
step:2084/2315 train_time:126610ms step_avg:60.75ms
step:2085/2315 train_time:126671ms step_avg:60.75ms
step:2086/2315 train_time:126733ms step_avg:60.75ms
step:2087/2315 train_time:126794ms step_avg:60.75ms
step:2088/2315 train_time:126857ms step_avg:60.76ms
step:2089/2315 train_time:126918ms step_avg:60.76ms
step:2090/2315 train_time:126979ms step_avg:60.76ms
step:2091/2315 train_time:127040ms step_avg:60.76ms
step:2092/2315 train_time:127101ms step_avg:60.76ms
step:2093/2315 train_time:127162ms step_avg:60.76ms
step:2094/2315 train_time:127224ms step_avg:60.76ms
step:2095/2315 train_time:127285ms step_avg:60.76ms
step:2096/2315 train_time:127348ms step_avg:60.76ms
step:2097/2315 train_time:127408ms step_avg:60.76ms
step:2098/2315 train_time:127470ms step_avg:60.76ms
step:2099/2315 train_time:127531ms step_avg:60.76ms
step:2100/2315 train_time:127592ms step_avg:60.76ms
step:2101/2315 train_time:127654ms step_avg:60.76ms
step:2102/2315 train_time:127715ms step_avg:60.76ms
step:2103/2315 train_time:127777ms step_avg:60.76ms
step:2104/2315 train_time:127838ms step_avg:60.76ms
step:2105/2315 train_time:127899ms step_avg:60.76ms
step:2106/2315 train_time:127960ms step_avg:60.76ms
step:2107/2315 train_time:128021ms step_avg:60.76ms
step:2108/2315 train_time:128083ms step_avg:60.76ms
step:2109/2315 train_time:128144ms step_avg:60.76ms
step:2110/2315 train_time:128206ms step_avg:60.76ms
step:2111/2315 train_time:128267ms step_avg:60.76ms
step:2112/2315 train_time:128328ms step_avg:60.76ms
step:2113/2315 train_time:128389ms step_avg:60.76ms
step:2114/2315 train_time:128451ms step_avg:60.76ms
step:2115/2315 train_time:128512ms step_avg:60.76ms
step:2116/2315 train_time:128574ms step_avg:60.76ms
step:2117/2315 train_time:128635ms step_avg:60.76ms
step:2118/2315 train_time:128696ms step_avg:60.76ms
step:2119/2315 train_time:128758ms step_avg:60.76ms
step:2120/2315 train_time:128819ms step_avg:60.76ms
step:2121/2315 train_time:128880ms step_avg:60.76ms
step:2122/2315 train_time:128942ms step_avg:60.76ms
step:2123/2315 train_time:129003ms step_avg:60.76ms
step:2124/2315 train_time:129064ms step_avg:60.76ms
step:2125/2315 train_time:129125ms step_avg:60.76ms
step:2126/2315 train_time:129187ms step_avg:60.77ms
step:2127/2315 train_time:129249ms step_avg:60.77ms
step:2128/2315 train_time:129311ms step_avg:60.77ms
step:2129/2315 train_time:129372ms step_avg:60.77ms
step:2130/2315 train_time:129433ms step_avg:60.77ms
step:2131/2315 train_time:129495ms step_avg:60.77ms
step:2132/2315 train_time:129556ms step_avg:60.77ms
step:2133/2315 train_time:129617ms step_avg:60.77ms
step:2134/2315 train_time:129679ms step_avg:60.77ms
step:2135/2315 train_time:129740ms step_avg:60.77ms
step:2136/2315 train_time:129801ms step_avg:60.77ms
step:2137/2315 train_time:129862ms step_avg:60.77ms
step:2138/2315 train_time:129924ms step_avg:60.77ms
step:2139/2315 train_time:129985ms step_avg:60.77ms
step:2140/2315 train_time:130047ms step_avg:60.77ms
step:2141/2315 train_time:130108ms step_avg:60.77ms
step:2142/2315 train_time:130170ms step_avg:60.77ms
step:2143/2315 train_time:130231ms step_avg:60.77ms
step:2144/2315 train_time:130293ms step_avg:60.77ms
step:2145/2315 train_time:130354ms step_avg:60.77ms
step:2146/2315 train_time:130415ms step_avg:60.77ms
step:2147/2315 train_time:130477ms step_avg:60.77ms
step:2148/2315 train_time:130539ms step_avg:60.77ms
step:2149/2315 train_time:130600ms step_avg:60.77ms
step:2150/2315 train_time:130662ms step_avg:60.77ms
step:2151/2315 train_time:130723ms step_avg:60.77ms
step:2152/2315 train_time:130785ms step_avg:60.77ms
step:2153/2315 train_time:130846ms step_avg:60.77ms
step:2154/2315 train_time:130909ms step_avg:60.77ms
step:2155/2315 train_time:130970ms step_avg:60.77ms
step:2156/2315 train_time:131031ms step_avg:60.78ms
step:2157/2315 train_time:131092ms step_avg:60.78ms
step:2158/2315 train_time:131154ms step_avg:60.78ms
step:2159/2315 train_time:131215ms step_avg:60.78ms
step:2160/2315 train_time:131277ms step_avg:60.78ms
step:2161/2315 train_time:131337ms step_avg:60.78ms
step:2162/2315 train_time:131399ms step_avg:60.78ms
step:2163/2315 train_time:131461ms step_avg:60.78ms
step:2164/2315 train_time:131522ms step_avg:60.78ms
step:2165/2315 train_time:131583ms step_avg:60.78ms
step:2166/2315 train_time:131644ms step_avg:60.78ms
step:2167/2315 train_time:131705ms step_avg:60.78ms
step:2168/2315 train_time:131767ms step_avg:60.78ms
step:2169/2315 train_time:131828ms step_avg:60.78ms
step:2170/2315 train_time:131890ms step_avg:60.78ms
step:2171/2315 train_time:131951ms step_avg:60.78ms
step:2172/2315 train_time:132013ms step_avg:60.78ms
step:2173/2315 train_time:132074ms step_avg:60.78ms
step:2174/2315 train_time:132135ms step_avg:60.78ms
step:2175/2315 train_time:132196ms step_avg:60.78ms
step:2176/2315 train_time:132258ms step_avg:60.78ms
step:2177/2315 train_time:132319ms step_avg:60.78ms
step:2178/2315 train_time:132380ms step_avg:60.78ms
step:2179/2315 train_time:132441ms step_avg:60.78ms
step:2180/2315 train_time:132503ms step_avg:60.78ms
step:2181/2315 train_time:132564ms step_avg:60.78ms
step:2182/2315 train_time:132626ms step_avg:60.78ms
step:2183/2315 train_time:132688ms step_avg:60.78ms
step:2184/2315 train_time:132750ms step_avg:60.78ms
step:2185/2315 train_time:132811ms step_avg:60.78ms
step:2186/2315 train_time:132873ms step_avg:60.78ms
step:2187/2315 train_time:132934ms step_avg:60.78ms
step:2188/2315 train_time:132996ms step_avg:60.78ms
step:2189/2315 train_time:133057ms step_avg:60.78ms
step:2190/2315 train_time:133118ms step_avg:60.78ms
step:2191/2315 train_time:133179ms step_avg:60.78ms
step:2192/2315 train_time:133240ms step_avg:60.78ms
step:2193/2315 train_time:133301ms step_avg:60.78ms
step:2194/2315 train_time:133363ms step_avg:60.79ms
step:2195/2315 train_time:133423ms step_avg:60.78ms
step:2196/2315 train_time:133484ms step_avg:60.79ms
step:2197/2315 train_time:133546ms step_avg:60.79ms
step:2198/2315 train_time:133608ms step_avg:60.79ms
step:2199/2315 train_time:133670ms step_avg:60.79ms
step:2200/2315 train_time:133732ms step_avg:60.79ms
step:2201/2315 train_time:133793ms step_avg:60.79ms
step:2202/2315 train_time:133856ms step_avg:60.79ms
step:2203/2315 train_time:133917ms step_avg:60.79ms
step:2204/2315 train_time:133979ms step_avg:60.79ms
step:2205/2315 train_time:134040ms step_avg:60.79ms
step:2206/2315 train_time:134101ms step_avg:60.79ms
step:2207/2315 train_time:134163ms step_avg:60.79ms
step:2208/2315 train_time:134224ms step_avg:60.79ms
step:2209/2315 train_time:134285ms step_avg:60.79ms
step:2210/2315 train_time:134348ms step_avg:60.79ms
step:2211/2315 train_time:134409ms step_avg:60.79ms
step:2212/2315 train_time:134471ms step_avg:60.79ms
step:2213/2315 train_time:134532ms step_avg:60.79ms
step:2214/2315 train_time:134594ms step_avg:60.79ms
step:2215/2315 train_time:134655ms step_avg:60.79ms
step:2216/2315 train_time:134716ms step_avg:60.79ms
step:2217/2315 train_time:134777ms step_avg:60.79ms
step:2218/2315 train_time:134838ms step_avg:60.79ms
step:2219/2315 train_time:134899ms step_avg:60.79ms
step:2220/2315 train_time:134960ms step_avg:60.79ms
step:2221/2315 train_time:135021ms step_avg:60.79ms
step:2222/2315 train_time:135082ms step_avg:60.79ms
step:2223/2315 train_time:135144ms step_avg:60.79ms
step:2224/2315 train_time:135206ms step_avg:60.79ms
step:2225/2315 train_time:135267ms step_avg:60.79ms
step:2226/2315 train_time:135329ms step_avg:60.79ms
step:2227/2315 train_time:135391ms step_avg:60.80ms
step:2228/2315 train_time:135452ms step_avg:60.80ms
step:2229/2315 train_time:135513ms step_avg:60.80ms
step:2230/2315 train_time:135575ms step_avg:60.80ms
step:2231/2315 train_time:135636ms step_avg:60.80ms
step:2232/2315 train_time:135698ms step_avg:60.80ms
step:2233/2315 train_time:135759ms step_avg:60.80ms
step:2234/2315 train_time:135821ms step_avg:60.80ms
step:2235/2315 train_time:135882ms step_avg:60.80ms
step:2236/2315 train_time:135944ms step_avg:60.80ms
step:2237/2315 train_time:136004ms step_avg:60.80ms
step:2238/2315 train_time:136066ms step_avg:60.80ms
step:2239/2315 train_time:136127ms step_avg:60.80ms
step:2240/2315 train_time:136188ms step_avg:60.80ms
step:2241/2315 train_time:136249ms step_avg:60.80ms
step:2242/2315 train_time:136311ms step_avg:60.80ms
step:2243/2315 train_time:136372ms step_avg:60.80ms
step:2244/2315 train_time:136433ms step_avg:60.80ms
step:2245/2315 train_time:136495ms step_avg:60.80ms
step:2246/2315 train_time:136557ms step_avg:60.80ms
step:2247/2315 train_time:136618ms step_avg:60.80ms
step:2248/2315 train_time:136680ms step_avg:60.80ms
step:2249/2315 train_time:136741ms step_avg:60.80ms
step:2250/2315 train_time:136802ms step_avg:60.80ms
step:2250/2315 val_loss:3.2903 train_time:136864ms step_avg:60.83ms
step:2251/2315 train_time:136882ms step_avg:60.81ms
step:2252/2315 train_time:136927ms step_avg:60.80ms
step:2253/2315 train_time:136992ms step_avg:60.80ms
step:2254/2315 train_time:137056ms step_avg:60.81ms
step:2255/2315 train_time:137118ms step_avg:60.81ms
step:2256/2315 train_time:137180ms step_avg:60.81ms
step:2257/2315 train_time:137240ms step_avg:60.81ms
step:2258/2315 train_time:137302ms step_avg:60.81ms
step:2259/2315 train_time:137362ms step_avg:60.81ms
step:2260/2315 train_time:137423ms step_avg:60.81ms
step:2261/2315 train_time:137484ms step_avg:60.81ms
step:2262/2315 train_time:137545ms step_avg:60.81ms
step:2263/2315 train_time:137604ms step_avg:60.81ms
step:2264/2315 train_time:137665ms step_avg:60.81ms
step:2265/2315 train_time:137725ms step_avg:60.81ms
step:2266/2315 train_time:137787ms step_avg:60.81ms
step:2267/2315 train_time:137849ms step_avg:60.81ms
step:2268/2315 train_time:137913ms step_avg:60.81ms
step:2269/2315 train_time:137976ms step_avg:60.81ms
step:2270/2315 train_time:138039ms step_avg:60.81ms
step:2271/2315 train_time:138100ms step_avg:60.81ms
step:2272/2315 train_time:138161ms step_avg:60.81ms
step:2273/2315 train_time:138222ms step_avg:60.81ms
step:2274/2315 train_time:138284ms step_avg:60.81ms
step:2275/2315 train_time:138344ms step_avg:60.81ms
step:2276/2315 train_time:138405ms step_avg:60.81ms
step:2277/2315 train_time:138465ms step_avg:60.81ms
step:2278/2315 train_time:138526ms step_avg:60.81ms
step:2279/2315 train_time:138586ms step_avg:60.81ms
step:2280/2315 train_time:138647ms step_avg:60.81ms
step:2281/2315 train_time:138707ms step_avg:60.81ms
step:2282/2315 train_time:138769ms step_avg:60.81ms
step:2283/2315 train_time:138831ms step_avg:60.81ms
step:2284/2315 train_time:138893ms step_avg:60.81ms
step:2285/2315 train_time:138955ms step_avg:60.81ms
step:2286/2315 train_time:139018ms step_avg:60.81ms
step:2287/2315 train_time:139079ms step_avg:60.81ms
step:2288/2315 train_time:139141ms step_avg:60.81ms
step:2289/2315 train_time:139203ms step_avg:60.81ms
step:2290/2315 train_time:139265ms step_avg:60.81ms
step:2291/2315 train_time:139325ms step_avg:60.81ms
step:2292/2315 train_time:139387ms step_avg:60.81ms
step:2293/2315 train_time:139448ms step_avg:60.81ms
step:2294/2315 train_time:139509ms step_avg:60.81ms
step:2295/2315 train_time:139569ms step_avg:60.81ms
step:2296/2315 train_time:139631ms step_avg:60.81ms
step:2297/2315 train_time:139691ms step_avg:60.81ms
step:2298/2315 train_time:139752ms step_avg:60.81ms
step:2299/2315 train_time:139813ms step_avg:60.81ms
step:2300/2315 train_time:139875ms step_avg:60.82ms
step:2301/2315 train_time:139937ms step_avg:60.82ms
step:2302/2315 train_time:139999ms step_avg:60.82ms
step:2303/2315 train_time:140060ms step_avg:60.82ms
step:2304/2315 train_time:140122ms step_avg:60.82ms
step:2305/2315 train_time:140183ms step_avg:60.82ms
step:2306/2315 train_time:140245ms step_avg:60.82ms
step:2307/2315 train_time:140306ms step_avg:60.82ms
step:2308/2315 train_time:140367ms step_avg:60.82ms
step:2309/2315 train_time:140428ms step_avg:60.82ms
step:2310/2315 train_time:140490ms step_avg:60.82ms
step:2311/2315 train_time:140551ms step_avg:60.82ms
step:2312/2315 train_time:140612ms step_avg:60.82ms
step:2313/2315 train_time:140673ms step_avg:60.82ms
step:2314/2315 train_time:140734ms step_avg:60.82ms
step:2315/2315 train_time:140795ms step_avg:60.82ms
step:2315/2315 val_loss:3.2773 train_time:140857ms step_avg:60.85ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

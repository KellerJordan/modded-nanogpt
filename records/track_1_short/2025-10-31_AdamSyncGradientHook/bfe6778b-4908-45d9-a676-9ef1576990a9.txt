import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)


            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))



            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal
        self.group_to_param_group = {}

        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                self.group_to_param_group[param] = group

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        world_size = dist.get_world_size()
        grad = param.grad
        rank_size = grad.shape[0] // world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(), grad_slice)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']

            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
        # enable sync in the next training step for the adam optimizer
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = next(train_loader)

        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()


====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251031+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 31 16:30:52 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   40C    P0            127W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   32C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   31C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2315 train_time:75ms step_avg:75.15ms
step:2/2315 train_time:189ms step_avg:94.41ms
step:3/2315 train_time:208ms step_avg:69.43ms
step:4/2315 train_time:249ms step_avg:62.13ms
step:5/2315 train_time:306ms step_avg:61.22ms
step:6/2315 train_time:366ms step_avg:61.02ms
step:7/2315 train_time:425ms step_avg:60.67ms
step:8/2315 train_time:484ms step_avg:60.54ms
step:9/2315 train_time:543ms step_avg:60.36ms
step:10/2315 train_time:603ms step_avg:60.30ms
step:11/2315 train_time:662ms step_avg:60.18ms
step:12/2315 train_time:722ms step_avg:60.17ms
step:13/2315 train_time:781ms step_avg:60.07ms
step:14/2315 train_time:841ms step_avg:60.04ms
step:15/2315 train_time:900ms step_avg:59.98ms
step:16/2315 train_time:959ms step_avg:59.96ms
step:17/2315 train_time:1019ms step_avg:59.93ms
step:18/2315 train_time:1081ms step_avg:60.06ms
step:19/2315 train_time:1146ms step_avg:60.33ms
step:20/2315 train_time:1210ms step_avg:60.51ms
step:21/2315 train_time:1271ms step_avg:60.53ms
step:22/2315 train_time:1332ms step_avg:60.53ms
step:23/2315 train_time:1392ms step_avg:60.52ms
step:24/2315 train_time:1453ms step_avg:60.52ms
step:25/2315 train_time:1512ms step_avg:60.50ms
step:26/2315 train_time:1573ms step_avg:60.49ms
step:27/2315 train_time:1633ms step_avg:60.47ms
step:28/2315 train_time:1693ms step_avg:60.46ms
step:29/2315 train_time:1753ms step_avg:60.45ms
step:30/2315 train_time:1814ms step_avg:60.47ms
step:31/2315 train_time:1874ms step_avg:60.44ms
step:32/2315 train_time:1934ms step_avg:60.44ms
step:33/2315 train_time:1995ms step_avg:60.44ms
step:34/2315 train_time:2056ms step_avg:60.47ms
step:35/2315 train_time:2117ms step_avg:60.48ms
step:36/2315 train_time:2178ms step_avg:60.51ms
step:37/2315 train_time:2239ms step_avg:60.51ms
step:38/2315 train_time:2299ms step_avg:60.51ms
step:39/2315 train_time:2359ms step_avg:60.50ms
step:40/2315 train_time:2420ms step_avg:60.50ms
step:41/2315 train_time:2480ms step_avg:60.49ms
step:42/2315 train_time:2540ms step_avg:60.48ms
step:43/2315 train_time:2600ms step_avg:60.47ms
step:44/2315 train_time:2660ms step_avg:60.46ms
step:45/2315 train_time:2719ms step_avg:60.43ms
step:46/2315 train_time:2779ms step_avg:60.42ms
step:47/2315 train_time:2838ms step_avg:60.39ms
step:48/2315 train_time:2899ms step_avg:60.40ms
step:49/2315 train_time:2958ms step_avg:60.37ms
step:50/2315 train_time:3019ms step_avg:60.38ms
step:51/2315 train_time:3079ms step_avg:60.37ms
step:52/2315 train_time:3141ms step_avg:60.40ms
step:53/2315 train_time:3201ms step_avg:60.39ms
step:54/2315 train_time:3261ms step_avg:60.40ms
step:55/2315 train_time:3323ms step_avg:60.42ms
step:56/2315 train_time:3382ms step_avg:60.39ms
step:57/2315 train_time:3442ms step_avg:60.39ms
step:58/2315 train_time:3503ms step_avg:60.40ms
step:59/2315 train_time:3563ms step_avg:60.39ms
step:60/2315 train_time:3623ms step_avg:60.39ms
step:61/2315 train_time:3683ms step_avg:60.37ms
step:62/2315 train_time:3744ms step_avg:60.38ms
step:63/2315 train_time:3802ms step_avg:60.36ms
step:64/2315 train_time:3863ms step_avg:60.36ms
step:65/2315 train_time:3923ms step_avg:60.36ms
step:66/2315 train_time:3984ms step_avg:60.36ms
step:67/2315 train_time:4043ms step_avg:60.35ms
step:68/2315 train_time:4104ms step_avg:60.35ms
step:69/2315 train_time:4164ms step_avg:60.35ms
step:70/2315 train_time:4225ms step_avg:60.36ms
step:71/2315 train_time:4285ms step_avg:60.35ms
step:72/2315 train_time:4346ms step_avg:60.36ms
step:73/2315 train_time:4407ms step_avg:60.37ms
step:74/2315 train_time:4468ms step_avg:60.38ms
step:75/2315 train_time:4527ms step_avg:60.36ms
step:76/2315 train_time:4588ms step_avg:60.36ms
step:77/2315 train_time:4647ms step_avg:60.35ms
step:78/2315 train_time:4708ms step_avg:60.36ms
step:79/2315 train_time:4768ms step_avg:60.35ms
step:80/2315 train_time:4828ms step_avg:60.35ms
step:81/2315 train_time:4889ms step_avg:60.36ms
step:82/2315 train_time:4950ms step_avg:60.36ms
step:83/2315 train_time:5010ms step_avg:60.36ms
step:84/2315 train_time:5071ms step_avg:60.37ms
step:85/2315 train_time:5131ms step_avg:60.37ms
step:86/2315 train_time:5192ms step_avg:60.37ms
step:87/2315 train_time:5252ms step_avg:60.37ms
step:88/2315 train_time:5314ms step_avg:60.38ms
step:89/2315 train_time:5374ms step_avg:60.38ms
step:90/2315 train_time:5434ms step_avg:60.38ms
step:91/2315 train_time:5494ms step_avg:60.38ms
step:92/2315 train_time:5555ms step_avg:60.38ms
step:93/2315 train_time:5614ms step_avg:60.37ms
step:94/2315 train_time:5676ms step_avg:60.38ms
step:95/2315 train_time:5736ms step_avg:60.37ms
step:96/2315 train_time:5796ms step_avg:60.38ms
step:97/2315 train_time:5856ms step_avg:60.37ms
step:98/2315 train_time:5917ms step_avg:60.38ms
step:99/2315 train_time:5977ms step_avg:60.37ms
step:100/2315 train_time:6038ms step_avg:60.38ms
step:101/2315 train_time:6098ms step_avg:60.38ms
step:102/2315 train_time:6159ms step_avg:60.38ms
step:103/2315 train_time:6218ms step_avg:60.37ms
step:104/2315 train_time:6278ms step_avg:60.37ms
step:105/2315 train_time:6338ms step_avg:60.36ms
step:106/2315 train_time:6398ms step_avg:60.36ms
step:107/2315 train_time:6457ms step_avg:60.35ms
step:108/2315 train_time:6517ms step_avg:60.34ms
step:109/2315 train_time:6577ms step_avg:60.34ms
step:110/2315 train_time:6637ms step_avg:60.33ms
step:111/2315 train_time:6696ms step_avg:60.33ms
step:112/2315 train_time:6756ms step_avg:60.32ms
step:113/2315 train_time:6817ms step_avg:60.32ms
step:114/2315 train_time:6877ms step_avg:60.32ms
step:115/2315 train_time:6936ms step_avg:60.32ms
step:116/2315 train_time:6997ms step_avg:60.32ms
step:117/2315 train_time:7057ms step_avg:60.32ms
step:118/2315 train_time:7117ms step_avg:60.31ms
step:119/2315 train_time:7178ms step_avg:60.32ms
step:120/2315 train_time:7238ms step_avg:60.32ms
step:121/2315 train_time:7298ms step_avg:60.31ms
step:122/2315 train_time:7358ms step_avg:60.31ms
step:123/2315 train_time:7418ms step_avg:60.31ms
step:124/2315 train_time:7478ms step_avg:60.31ms
step:125/2315 train_time:7537ms step_avg:60.30ms
step:126/2315 train_time:7598ms step_avg:60.30ms
step:127/2315 train_time:7657ms step_avg:60.29ms
step:128/2315 train_time:7717ms step_avg:60.29ms
step:129/2315 train_time:7777ms step_avg:60.28ms
step:130/2315 train_time:7837ms step_avg:60.28ms
step:131/2315 train_time:7897ms step_avg:60.28ms
step:132/2315 train_time:7957ms step_avg:60.28ms
step:133/2315 train_time:8017ms step_avg:60.28ms
step:134/2315 train_time:8078ms step_avg:60.28ms
step:135/2315 train_time:8137ms step_avg:60.28ms
step:136/2315 train_time:8197ms step_avg:60.27ms
step:137/2315 train_time:8257ms step_avg:60.27ms
step:138/2315 train_time:8317ms step_avg:60.27ms
step:139/2315 train_time:8377ms step_avg:60.26ms
step:140/2315 train_time:8437ms step_avg:60.26ms
step:141/2315 train_time:8497ms step_avg:60.26ms
step:142/2315 train_time:8557ms step_avg:60.26ms
step:143/2315 train_time:8617ms step_avg:60.26ms
step:144/2315 train_time:8677ms step_avg:60.25ms
step:145/2315 train_time:8737ms step_avg:60.25ms
step:146/2315 train_time:8797ms step_avg:60.25ms
step:147/2315 train_time:8857ms step_avg:60.25ms
step:148/2315 train_time:8917ms step_avg:60.25ms
step:149/2315 train_time:8977ms step_avg:60.25ms
step:150/2315 train_time:9038ms step_avg:60.25ms
step:151/2315 train_time:9097ms step_avg:60.25ms
step:152/2315 train_time:9158ms step_avg:60.25ms
step:153/2315 train_time:9217ms step_avg:60.24ms
step:154/2315 train_time:9278ms step_avg:60.24ms
step:155/2315 train_time:9337ms step_avg:60.24ms
step:156/2315 train_time:9397ms step_avg:60.24ms
step:157/2315 train_time:9457ms step_avg:60.24ms
step:158/2315 train_time:9517ms step_avg:60.24ms
step:159/2315 train_time:9577ms step_avg:60.23ms
step:160/2315 train_time:9638ms step_avg:60.24ms
step:161/2315 train_time:9697ms step_avg:60.23ms
step:162/2315 train_time:9757ms step_avg:60.23ms
step:163/2315 train_time:9816ms step_avg:60.22ms
step:164/2315 train_time:9877ms step_avg:60.22ms
step:165/2315 train_time:9936ms step_avg:60.22ms
step:166/2315 train_time:9996ms step_avg:60.22ms
step:167/2315 train_time:10056ms step_avg:60.22ms
step:168/2315 train_time:10117ms step_avg:60.22ms
step:169/2315 train_time:10177ms step_avg:60.22ms
step:170/2315 train_time:10237ms step_avg:60.22ms
step:171/2315 train_time:10297ms step_avg:60.21ms
step:172/2315 train_time:10357ms step_avg:60.21ms
step:173/2315 train_time:10416ms step_avg:60.21ms
step:174/2315 train_time:10476ms step_avg:60.21ms
step:175/2315 train_time:10536ms step_avg:60.20ms
step:176/2315 train_time:10596ms step_avg:60.20ms
step:177/2315 train_time:10655ms step_avg:60.20ms
step:178/2315 train_time:10715ms step_avg:60.20ms
step:179/2315 train_time:10775ms step_avg:60.20ms
step:180/2315 train_time:10836ms step_avg:60.20ms
step:181/2315 train_time:10895ms step_avg:60.19ms
step:182/2315 train_time:10955ms step_avg:60.19ms
step:183/2315 train_time:11015ms step_avg:60.19ms
step:184/2315 train_time:11076ms step_avg:60.19ms
step:185/2315 train_time:11136ms step_avg:60.20ms
step:186/2315 train_time:11196ms step_avg:60.20ms
step:187/2315 train_time:11256ms step_avg:60.19ms
step:188/2315 train_time:11317ms step_avg:60.19ms
step:189/2315 train_time:11376ms step_avg:60.19ms
step:190/2315 train_time:11436ms step_avg:60.19ms
step:191/2315 train_time:11496ms step_avg:60.19ms
step:192/2315 train_time:11557ms step_avg:60.19ms
step:193/2315 train_time:11616ms step_avg:60.19ms
step:194/2315 train_time:11677ms step_avg:60.19ms
step:195/2315 train_time:11737ms step_avg:60.19ms
step:196/2315 train_time:11797ms step_avg:60.19ms
step:197/2315 train_time:11857ms step_avg:60.19ms
step:198/2315 train_time:11917ms step_avg:60.19ms
step:199/2315 train_time:11978ms step_avg:60.19ms
step:200/2315 train_time:12039ms step_avg:60.19ms
step:201/2315 train_time:12099ms step_avg:60.19ms
step:202/2315 train_time:12159ms step_avg:60.19ms
step:203/2315 train_time:12219ms step_avg:60.19ms
step:204/2315 train_time:12278ms step_avg:60.19ms
step:205/2315 train_time:12337ms step_avg:60.18ms
step:206/2315 train_time:12398ms step_avg:60.18ms
step:207/2315 train_time:12458ms step_avg:60.18ms
step:208/2315 train_time:12518ms step_avg:60.18ms
step:209/2315 train_time:12578ms step_avg:60.18ms
step:210/2315 train_time:12638ms step_avg:60.18ms
step:211/2315 train_time:12697ms step_avg:60.18ms
step:212/2315 train_time:12758ms step_avg:60.18ms
step:213/2315 train_time:12817ms step_avg:60.17ms
step:214/2315 train_time:12877ms step_avg:60.17ms
step:215/2315 train_time:12937ms step_avg:60.17ms
step:216/2315 train_time:12997ms step_avg:60.17ms
step:217/2315 train_time:13057ms step_avg:60.17ms
step:218/2315 train_time:13117ms step_avg:60.17ms
step:219/2315 train_time:13177ms step_avg:60.17ms
step:220/2315 train_time:13237ms step_avg:60.17ms
step:221/2315 train_time:13297ms step_avg:60.17ms
step:222/2315 train_time:13357ms step_avg:60.17ms
step:223/2315 train_time:13417ms step_avg:60.17ms
step:224/2315 train_time:13477ms step_avg:60.17ms
step:225/2315 train_time:13536ms step_avg:60.16ms
step:226/2315 train_time:13597ms step_avg:60.16ms
step:227/2315 train_time:13656ms step_avg:60.16ms
step:228/2315 train_time:13717ms step_avg:60.16ms
step:229/2315 train_time:13776ms step_avg:60.16ms
step:230/2315 train_time:13836ms step_avg:60.16ms
step:231/2315 train_time:13896ms step_avg:60.16ms
step:232/2315 train_time:13957ms step_avg:60.16ms
step:233/2315 train_time:14016ms step_avg:60.16ms
step:234/2315 train_time:14077ms step_avg:60.16ms
step:235/2315 train_time:14136ms step_avg:60.16ms
step:236/2315 train_time:14197ms step_avg:60.15ms
step:237/2315 train_time:14256ms step_avg:60.15ms
step:238/2315 train_time:14316ms step_avg:60.15ms
step:239/2315 train_time:14376ms step_avg:60.15ms
step:240/2315 train_time:14436ms step_avg:60.15ms
step:241/2315 train_time:14496ms step_avg:60.15ms
step:242/2315 train_time:14556ms step_avg:60.15ms
step:243/2315 train_time:14616ms step_avg:60.15ms
step:244/2315 train_time:14676ms step_avg:60.15ms
step:245/2315 train_time:14736ms step_avg:60.15ms
step:246/2315 train_time:14796ms step_avg:60.14ms
step:247/2315 train_time:14856ms step_avg:60.14ms
step:248/2315 train_time:14916ms step_avg:60.14ms
step:249/2315 train_time:14975ms step_avg:60.14ms
step:250/2315 train_time:15035ms step_avg:60.14ms
step:250/2315 val_loss:4.0719 train_time:15097ms step_avg:60.39ms
step:251/2315 train_time:15115ms step_avg:60.22ms
step:252/2315 train_time:15157ms step_avg:60.15ms
step:253/2315 train_time:15223ms step_avg:60.17ms
step:254/2315 train_time:15287ms step_avg:60.19ms
step:255/2315 train_time:15348ms step_avg:60.19ms
step:256/2315 train_time:15409ms step_avg:60.19ms
step:257/2315 train_time:15468ms step_avg:60.19ms
step:258/2315 train_time:15527ms step_avg:60.18ms
step:259/2315 train_time:15587ms step_avg:60.18ms
step:260/2315 train_time:15646ms step_avg:60.18ms
step:261/2315 train_time:15705ms step_avg:60.17ms
step:262/2315 train_time:15765ms step_avg:60.17ms
step:263/2315 train_time:15823ms step_avg:60.16ms
step:264/2315 train_time:15882ms step_avg:60.16ms
step:265/2315 train_time:15941ms step_avg:60.15ms
step:266/2315 train_time:16000ms step_avg:60.15ms
step:267/2315 train_time:16059ms step_avg:60.15ms
step:268/2315 train_time:16120ms step_avg:60.15ms
step:269/2315 train_time:16182ms step_avg:60.16ms
step:270/2315 train_time:16243ms step_avg:60.16ms
step:271/2315 train_time:16304ms step_avg:60.16ms
step:272/2315 train_time:16364ms step_avg:60.16ms
step:273/2315 train_time:16423ms step_avg:60.16ms
step:274/2315 train_time:16484ms step_avg:60.16ms
step:275/2315 train_time:16543ms step_avg:60.15ms
step:276/2315 train_time:16602ms step_avg:60.15ms
step:277/2315 train_time:16662ms step_avg:60.15ms
step:278/2315 train_time:16721ms step_avg:60.15ms
step:279/2315 train_time:16781ms step_avg:60.15ms
step:280/2315 train_time:16840ms step_avg:60.14ms
step:281/2315 train_time:16899ms step_avg:60.14ms
step:282/2315 train_time:16959ms step_avg:60.14ms
step:283/2315 train_time:17017ms step_avg:60.13ms
step:284/2315 train_time:17078ms step_avg:60.13ms
step:285/2315 train_time:17138ms step_avg:60.13ms
step:286/2315 train_time:17198ms step_avg:60.13ms
step:287/2315 train_time:17259ms step_avg:60.14ms
step:288/2315 train_time:17320ms step_avg:60.14ms
step:289/2315 train_time:17381ms step_avg:60.14ms
step:290/2315 train_time:17441ms step_avg:60.14ms
step:291/2315 train_time:17501ms step_avg:60.14ms
step:292/2315 train_time:17561ms step_avg:60.14ms
step:293/2315 train_time:17621ms step_avg:60.14ms
step:294/2315 train_time:17681ms step_avg:60.14ms
step:295/2315 train_time:17740ms step_avg:60.14ms
step:296/2315 train_time:17800ms step_avg:60.14ms
step:297/2315 train_time:17859ms step_avg:60.13ms
step:298/2315 train_time:17918ms step_avg:60.13ms
step:299/2315 train_time:17977ms step_avg:60.13ms
step:300/2315 train_time:18038ms step_avg:60.13ms
step:301/2315 train_time:18097ms step_avg:60.12ms
step:302/2315 train_time:18157ms step_avg:60.12ms
step:303/2315 train_time:18219ms step_avg:60.13ms
step:304/2315 train_time:18280ms step_avg:60.13ms
step:305/2315 train_time:18340ms step_avg:60.13ms
step:306/2315 train_time:18400ms step_avg:60.13ms
step:307/2315 train_time:18460ms step_avg:60.13ms
step:308/2315 train_time:18520ms step_avg:60.13ms
step:309/2315 train_time:18581ms step_avg:60.13ms
step:310/2315 train_time:18641ms step_avg:60.13ms
step:311/2315 train_time:18700ms step_avg:60.13ms
step:312/2315 train_time:18760ms step_avg:60.13ms
step:313/2315 train_time:18819ms step_avg:60.13ms
step:314/2315 train_time:18879ms step_avg:60.12ms
step:315/2315 train_time:18938ms step_avg:60.12ms
step:316/2315 train_time:18998ms step_avg:60.12ms
step:317/2315 train_time:19058ms step_avg:60.12ms
step:318/2315 train_time:19119ms step_avg:60.12ms
step:319/2315 train_time:19178ms step_avg:60.12ms
step:320/2315 train_time:19239ms step_avg:60.12ms
step:321/2315 train_time:19298ms step_avg:60.12ms
step:322/2315 train_time:19359ms step_avg:60.12ms
step:323/2315 train_time:19419ms step_avg:60.12ms
step:324/2315 train_time:19480ms step_avg:60.12ms
step:325/2315 train_time:19539ms step_avg:60.12ms
step:326/2315 train_time:19600ms step_avg:60.12ms
step:327/2315 train_time:19660ms step_avg:60.12ms
step:328/2315 train_time:19720ms step_avg:60.12ms
step:329/2315 train_time:19779ms step_avg:60.12ms
step:330/2315 train_time:19839ms step_avg:60.12ms
step:331/2315 train_time:19898ms step_avg:60.11ms
step:332/2315 train_time:19958ms step_avg:60.11ms
step:333/2315 train_time:20018ms step_avg:60.11ms
step:334/2315 train_time:20078ms step_avg:60.11ms
step:335/2315 train_time:20138ms step_avg:60.11ms
step:336/2315 train_time:20198ms step_avg:60.11ms
step:337/2315 train_time:20258ms step_avg:60.11ms
step:338/2315 train_time:20318ms step_avg:60.11ms
step:339/2315 train_time:20378ms step_avg:60.11ms
step:340/2315 train_time:20439ms step_avg:60.11ms
step:341/2315 train_time:20499ms step_avg:60.11ms
step:342/2315 train_time:20559ms step_avg:60.11ms
step:343/2315 train_time:20619ms step_avg:60.12ms
step:344/2315 train_time:20680ms step_avg:60.12ms
step:345/2315 train_time:20740ms step_avg:60.11ms
step:346/2315 train_time:20800ms step_avg:60.12ms
step:347/2315 train_time:20859ms step_avg:60.11ms
step:348/2315 train_time:20920ms step_avg:60.11ms
step:349/2315 train_time:20981ms step_avg:60.12ms
step:350/2315 train_time:21040ms step_avg:60.11ms
step:351/2315 train_time:21099ms step_avg:60.11ms
step:352/2315 train_time:21159ms step_avg:60.11ms
step:353/2315 train_time:21218ms step_avg:60.11ms
step:354/2315 train_time:21279ms step_avg:60.11ms
step:355/2315 train_time:21339ms step_avg:60.11ms
step:356/2315 train_time:21399ms step_avg:60.11ms
step:357/2315 train_time:21458ms step_avg:60.11ms
step:358/2315 train_time:21519ms step_avg:60.11ms
step:359/2315 train_time:21579ms step_avg:60.11ms
step:360/2315 train_time:21639ms step_avg:60.11ms
step:361/2315 train_time:21698ms step_avg:60.11ms
step:362/2315 train_time:21758ms step_avg:60.11ms
step:363/2315 train_time:21817ms step_avg:60.10ms
step:364/2315 train_time:21877ms step_avg:60.10ms
step:365/2315 train_time:21937ms step_avg:60.10ms
step:366/2315 train_time:21998ms step_avg:60.10ms
step:367/2315 train_time:22058ms step_avg:60.10ms
step:368/2315 train_time:22118ms step_avg:60.10ms
step:369/2315 train_time:22178ms step_avg:60.10ms
step:370/2315 train_time:22238ms step_avg:60.10ms
step:371/2315 train_time:22297ms step_avg:60.10ms
step:372/2315 train_time:22357ms step_avg:60.10ms
step:373/2315 train_time:22417ms step_avg:60.10ms
step:374/2315 train_time:22478ms step_avg:60.10ms
step:375/2315 train_time:22538ms step_avg:60.10ms
step:376/2315 train_time:22598ms step_avg:60.10ms
step:377/2315 train_time:22658ms step_avg:60.10ms
step:378/2315 train_time:22718ms step_avg:60.10ms
step:379/2315 train_time:22778ms step_avg:60.10ms
step:380/2315 train_time:22838ms step_avg:60.10ms
step:381/2315 train_time:22897ms step_avg:60.10ms
step:382/2315 train_time:22957ms step_avg:60.10ms
step:383/2315 train_time:23017ms step_avg:60.10ms
step:384/2315 train_time:23077ms step_avg:60.10ms
step:385/2315 train_time:23138ms step_avg:60.10ms
step:386/2315 train_time:23198ms step_avg:60.10ms
step:387/2315 train_time:23258ms step_avg:60.10ms
step:388/2315 train_time:23318ms step_avg:60.10ms
step:389/2315 train_time:23378ms step_avg:60.10ms
step:390/2315 train_time:23438ms step_avg:60.10ms
step:391/2315 train_time:23498ms step_avg:60.10ms
step:392/2315 train_time:23558ms step_avg:60.10ms
step:393/2315 train_time:23617ms step_avg:60.10ms
step:394/2315 train_time:23677ms step_avg:60.10ms
step:395/2315 train_time:23737ms step_avg:60.09ms
step:396/2315 train_time:23797ms step_avg:60.09ms
step:397/2315 train_time:23857ms step_avg:60.09ms
step:398/2315 train_time:23917ms step_avg:60.09ms
step:399/2315 train_time:23976ms step_avg:60.09ms
step:400/2315 train_time:24037ms step_avg:60.09ms
step:401/2315 train_time:24096ms step_avg:60.09ms
step:402/2315 train_time:24156ms step_avg:60.09ms
step:403/2315 train_time:24216ms step_avg:60.09ms
step:404/2315 train_time:24276ms step_avg:60.09ms
step:405/2315 train_time:24336ms step_avg:60.09ms
step:406/2315 train_time:24396ms step_avg:60.09ms
step:407/2315 train_time:24456ms step_avg:60.09ms
step:408/2315 train_time:24516ms step_avg:60.09ms
step:409/2315 train_time:24577ms step_avg:60.09ms
step:410/2315 train_time:24637ms step_avg:60.09ms
step:411/2315 train_time:24697ms step_avg:60.09ms
step:412/2315 train_time:24757ms step_avg:60.09ms
step:413/2315 train_time:24817ms step_avg:60.09ms
step:414/2315 train_time:24878ms step_avg:60.09ms
step:415/2315 train_time:24937ms step_avg:60.09ms
step:416/2315 train_time:24997ms step_avg:60.09ms
step:417/2315 train_time:25056ms step_avg:60.09ms
step:418/2315 train_time:25117ms step_avg:60.09ms
step:419/2315 train_time:25176ms step_avg:60.09ms
step:420/2315 train_time:25236ms step_avg:60.09ms
step:421/2315 train_time:25296ms step_avg:60.09ms
step:422/2315 train_time:25356ms step_avg:60.09ms
step:423/2315 train_time:25416ms step_avg:60.09ms
step:424/2315 train_time:25477ms step_avg:60.09ms
step:425/2315 train_time:25536ms step_avg:60.09ms
step:426/2315 train_time:25596ms step_avg:60.09ms
step:427/2315 train_time:25656ms step_avg:60.08ms
step:428/2315 train_time:25716ms step_avg:60.08ms
step:429/2315 train_time:25775ms step_avg:60.08ms
step:430/2315 train_time:25836ms step_avg:60.08ms
step:431/2315 train_time:25896ms step_avg:60.08ms
step:432/2315 train_time:25956ms step_avg:60.08ms
step:433/2315 train_time:26016ms step_avg:60.08ms
step:434/2315 train_time:26076ms step_avg:60.08ms
step:435/2315 train_time:26136ms step_avg:60.08ms
step:436/2315 train_time:26196ms step_avg:60.08ms
step:437/2315 train_time:26256ms step_avg:60.08ms
step:438/2315 train_time:26317ms step_avg:60.08ms
step:439/2315 train_time:26376ms step_avg:60.08ms
step:440/2315 train_time:26436ms step_avg:60.08ms
step:441/2315 train_time:26496ms step_avg:60.08ms
step:442/2315 train_time:26556ms step_avg:60.08ms
step:443/2315 train_time:26616ms step_avg:60.08ms
step:444/2315 train_time:26676ms step_avg:60.08ms
step:445/2315 train_time:26736ms step_avg:60.08ms
step:446/2315 train_time:26796ms step_avg:60.08ms
step:447/2315 train_time:26856ms step_avg:60.08ms
step:448/2315 train_time:26916ms step_avg:60.08ms
step:449/2315 train_time:26976ms step_avg:60.08ms
step:450/2315 train_time:27037ms step_avg:60.08ms
step:451/2315 train_time:27097ms step_avg:60.08ms
step:452/2315 train_time:27157ms step_avg:60.08ms
step:453/2315 train_time:27216ms step_avg:60.08ms
step:454/2315 train_time:27278ms step_avg:60.08ms
step:455/2315 train_time:27336ms step_avg:60.08ms
step:456/2315 train_time:27396ms step_avg:60.08ms
step:457/2315 train_time:27456ms step_avg:60.08ms
step:458/2315 train_time:27516ms step_avg:60.08ms
step:459/2315 train_time:27576ms step_avg:60.08ms
step:460/2315 train_time:27636ms step_avg:60.08ms
step:461/2315 train_time:27696ms step_avg:60.08ms
step:462/2315 train_time:27756ms step_avg:60.08ms
step:463/2315 train_time:27815ms step_avg:60.08ms
step:464/2315 train_time:27876ms step_avg:60.08ms
step:465/2315 train_time:27936ms step_avg:60.08ms
step:466/2315 train_time:27996ms step_avg:60.08ms
step:467/2315 train_time:28055ms step_avg:60.08ms
step:468/2315 train_time:28115ms step_avg:60.08ms
step:469/2315 train_time:28175ms step_avg:60.07ms
step:470/2315 train_time:28235ms step_avg:60.08ms
step:471/2315 train_time:28295ms step_avg:60.07ms
step:472/2315 train_time:28356ms step_avg:60.08ms
step:473/2315 train_time:28415ms step_avg:60.07ms
step:474/2315 train_time:28475ms step_avg:60.07ms
step:475/2315 train_time:28535ms step_avg:60.07ms
step:476/2315 train_time:28596ms step_avg:60.07ms
step:477/2315 train_time:28655ms step_avg:60.07ms
step:478/2315 train_time:28715ms step_avg:60.07ms
step:479/2315 train_time:28777ms step_avg:60.08ms
step:480/2315 train_time:28835ms step_avg:60.07ms
step:481/2315 train_time:28896ms step_avg:60.07ms
step:482/2315 train_time:28956ms step_avg:60.07ms
step:483/2315 train_time:29016ms step_avg:60.07ms
step:484/2315 train_time:29077ms step_avg:60.08ms
step:485/2315 train_time:29136ms step_avg:60.07ms
step:486/2315 train_time:29196ms step_avg:60.07ms
step:487/2315 train_time:29256ms step_avg:60.07ms
step:488/2315 train_time:29317ms step_avg:60.08ms
step:489/2315 train_time:29377ms step_avg:60.08ms
step:490/2315 train_time:29437ms step_avg:60.07ms
step:491/2315 train_time:29496ms step_avg:60.07ms
step:492/2315 train_time:29557ms step_avg:60.07ms
step:493/2315 train_time:29616ms step_avg:60.07ms
step:494/2315 train_time:29677ms step_avg:60.07ms
step:495/2315 train_time:29737ms step_avg:60.08ms
step:496/2315 train_time:29798ms step_avg:60.08ms
step:497/2315 train_time:29857ms step_avg:60.07ms
step:498/2315 train_time:29917ms step_avg:60.07ms
step:499/2315 train_time:29977ms step_avg:60.07ms
step:500/2315 train_time:30037ms step_avg:60.07ms
step:500/2315 val_loss:3.8188 train_time:30098ms step_avg:60.20ms
step:501/2315 train_time:30117ms step_avg:60.11ms
step:502/2315 train_time:30159ms step_avg:60.08ms
step:503/2315 train_time:30222ms step_avg:60.08ms
step:504/2315 train_time:30285ms step_avg:60.09ms
step:505/2315 train_time:30345ms step_avg:60.09ms
step:506/2315 train_time:30406ms step_avg:60.09ms
step:507/2315 train_time:30465ms step_avg:60.09ms
step:508/2315 train_time:30525ms step_avg:60.09ms
step:509/2315 train_time:30584ms step_avg:60.09ms
step:510/2315 train_time:30643ms step_avg:60.09ms
step:511/2315 train_time:30703ms step_avg:60.08ms
step:512/2315 train_time:30763ms step_avg:60.08ms
step:513/2315 train_time:30825ms step_avg:60.09ms
step:514/2315 train_time:30883ms step_avg:60.08ms
step:515/2315 train_time:30942ms step_avg:60.08ms
step:516/2315 train_time:31002ms step_avg:60.08ms
step:517/2315 train_time:31063ms step_avg:60.08ms
step:518/2315 train_time:31125ms step_avg:60.09ms
step:519/2315 train_time:31186ms step_avg:60.09ms
step:520/2315 train_time:31248ms step_avg:60.09ms
step:521/2315 train_time:31309ms step_avg:60.09ms
step:522/2315 train_time:31369ms step_avg:60.09ms
step:523/2315 train_time:31429ms step_avg:60.09ms
step:524/2315 train_time:31489ms step_avg:60.09ms
step:525/2315 train_time:31548ms step_avg:60.09ms
step:526/2315 train_time:31608ms step_avg:60.09ms
step:527/2315 train_time:31667ms step_avg:60.09ms
step:528/2315 train_time:31727ms step_avg:60.09ms
step:529/2315 train_time:31786ms step_avg:60.09ms
step:530/2315 train_time:31846ms step_avg:60.09ms
step:531/2315 train_time:31906ms step_avg:60.09ms
step:532/2315 train_time:31966ms step_avg:60.09ms
step:533/2315 train_time:32026ms step_avg:60.09ms
step:534/2315 train_time:32087ms step_avg:60.09ms
step:535/2315 train_time:32148ms step_avg:60.09ms
step:536/2315 train_time:32209ms step_avg:60.09ms
step:537/2315 train_time:32270ms step_avg:60.09ms
step:538/2315 train_time:32330ms step_avg:60.09ms
step:539/2315 train_time:32389ms step_avg:60.09ms
step:540/2315 train_time:32449ms step_avg:60.09ms
step:541/2315 train_time:32509ms step_avg:60.09ms
step:542/2315 train_time:32569ms step_avg:60.09ms
step:543/2315 train_time:32628ms step_avg:60.09ms
step:544/2315 train_time:32688ms step_avg:60.09ms
step:545/2315 train_time:32747ms step_avg:60.09ms
step:546/2315 train_time:32807ms step_avg:60.09ms
step:547/2315 train_time:32867ms step_avg:60.09ms
step:548/2315 train_time:32927ms step_avg:60.09ms
step:549/2315 train_time:32987ms step_avg:60.08ms
step:550/2315 train_time:33047ms step_avg:60.09ms
step:551/2315 train_time:33107ms step_avg:60.09ms
step:552/2315 train_time:33169ms step_avg:60.09ms
step:553/2315 train_time:33229ms step_avg:60.09ms
step:554/2315 train_time:33289ms step_avg:60.09ms
step:555/2315 train_time:33349ms step_avg:60.09ms
step:556/2315 train_time:33410ms step_avg:60.09ms
step:557/2315 train_time:33469ms step_avg:60.09ms
step:558/2315 train_time:33528ms step_avg:60.09ms
step:559/2315 train_time:33588ms step_avg:60.09ms
step:560/2315 train_time:33647ms step_avg:60.08ms
step:561/2315 train_time:33707ms step_avg:60.08ms
step:562/2315 train_time:33767ms step_avg:60.08ms
step:563/2315 train_time:33827ms step_avg:60.08ms
step:564/2315 train_time:33887ms step_avg:60.08ms
step:565/2315 train_time:33947ms step_avg:60.08ms
step:566/2315 train_time:34007ms step_avg:60.08ms
step:567/2315 train_time:34067ms step_avg:60.08ms
step:568/2315 train_time:34127ms step_avg:60.08ms
step:569/2315 train_time:34188ms step_avg:60.08ms
step:570/2315 train_time:34249ms step_avg:60.09ms
step:571/2315 train_time:34309ms step_avg:60.09ms
step:572/2315 train_time:34369ms step_avg:60.09ms
step:573/2315 train_time:34429ms step_avg:60.09ms
step:574/2315 train_time:34489ms step_avg:60.09ms
step:575/2315 train_time:34549ms step_avg:60.08ms
step:576/2315 train_time:34608ms step_avg:60.08ms
step:577/2315 train_time:34667ms step_avg:60.08ms
step:578/2315 train_time:34727ms step_avg:60.08ms
step:579/2315 train_time:34787ms step_avg:60.08ms
step:580/2315 train_time:34847ms step_avg:60.08ms
step:581/2315 train_time:34906ms step_avg:60.08ms
step:582/2315 train_time:34966ms step_avg:60.08ms
step:583/2315 train_time:35026ms step_avg:60.08ms
step:584/2315 train_time:35087ms step_avg:60.08ms
step:585/2315 train_time:35148ms step_avg:60.08ms
step:586/2315 train_time:35209ms step_avg:60.08ms
step:587/2315 train_time:35269ms step_avg:60.08ms
step:588/2315 train_time:35330ms step_avg:60.08ms
step:589/2315 train_time:35389ms step_avg:60.08ms
step:590/2315 train_time:35449ms step_avg:60.08ms
step:591/2315 train_time:35509ms step_avg:60.08ms
step:592/2315 train_time:35569ms step_avg:60.08ms
step:593/2315 train_time:35630ms step_avg:60.08ms
step:594/2315 train_time:35689ms step_avg:60.08ms
step:595/2315 train_time:35748ms step_avg:60.08ms
step:596/2315 train_time:35808ms step_avg:60.08ms
step:597/2315 train_time:35868ms step_avg:60.08ms
step:598/2315 train_time:35930ms step_avg:60.08ms
step:599/2315 train_time:35987ms step_avg:60.08ms
step:600/2315 train_time:36047ms step_avg:60.08ms
step:601/2315 train_time:36108ms step_avg:60.08ms
step:602/2315 train_time:36168ms step_avg:60.08ms
step:603/2315 train_time:36229ms step_avg:60.08ms
step:604/2315 train_time:36289ms step_avg:60.08ms
step:605/2315 train_time:36349ms step_avg:60.08ms
step:606/2315 train_time:36410ms step_avg:60.08ms
step:607/2315 train_time:36469ms step_avg:60.08ms
step:608/2315 train_time:36529ms step_avg:60.08ms
step:609/2315 train_time:36588ms step_avg:60.08ms
step:610/2315 train_time:36648ms step_avg:60.08ms
step:611/2315 train_time:36708ms step_avg:60.08ms
step:612/2315 train_time:36768ms step_avg:60.08ms
step:613/2315 train_time:36828ms step_avg:60.08ms
step:614/2315 train_time:36887ms step_avg:60.08ms
step:615/2315 train_time:36947ms step_avg:60.08ms
step:616/2315 train_time:37008ms step_avg:60.08ms
step:617/2315 train_time:37068ms step_avg:60.08ms
step:618/2315 train_time:37128ms step_avg:60.08ms
step:619/2315 train_time:37188ms step_avg:60.08ms
step:620/2315 train_time:37249ms step_avg:60.08ms
step:621/2315 train_time:37309ms step_avg:60.08ms
step:622/2315 train_time:37370ms step_avg:60.08ms
step:623/2315 train_time:37429ms step_avg:60.08ms
step:624/2315 train_time:37490ms step_avg:60.08ms
step:625/2315 train_time:37549ms step_avg:60.08ms
step:626/2315 train_time:37609ms step_avg:60.08ms
step:627/2315 train_time:37669ms step_avg:60.08ms
step:628/2315 train_time:37729ms step_avg:60.08ms
step:629/2315 train_time:37788ms step_avg:60.08ms
step:630/2315 train_time:37848ms step_avg:60.08ms
step:631/2315 train_time:37907ms step_avg:60.07ms
step:632/2315 train_time:37967ms step_avg:60.07ms
step:633/2315 train_time:38027ms step_avg:60.07ms
step:634/2315 train_time:38087ms step_avg:60.07ms
step:635/2315 train_time:38147ms step_avg:60.07ms
step:636/2315 train_time:38208ms step_avg:60.08ms
step:637/2315 train_time:38268ms step_avg:60.08ms
step:638/2315 train_time:38328ms step_avg:60.08ms
step:639/2315 train_time:38388ms step_avg:60.08ms
step:640/2315 train_time:38449ms step_avg:60.08ms
step:641/2315 train_time:38509ms step_avg:60.08ms
step:642/2315 train_time:38570ms step_avg:60.08ms
step:643/2315 train_time:38629ms step_avg:60.08ms
step:644/2315 train_time:38689ms step_avg:60.08ms
step:645/2315 train_time:38749ms step_avg:60.08ms
step:646/2315 train_time:38809ms step_avg:60.08ms
step:647/2315 train_time:38868ms step_avg:60.07ms
step:648/2315 train_time:38929ms step_avg:60.07ms
step:649/2315 train_time:38988ms step_avg:60.07ms
step:650/2315 train_time:39048ms step_avg:60.07ms
step:651/2315 train_time:39108ms step_avg:60.07ms
step:652/2315 train_time:39169ms step_avg:60.08ms
step:653/2315 train_time:39229ms step_avg:60.07ms
step:654/2315 train_time:39289ms step_avg:60.07ms
step:655/2315 train_time:39348ms step_avg:60.07ms
step:656/2315 train_time:39409ms step_avg:60.07ms
step:657/2315 train_time:39470ms step_avg:60.08ms
step:658/2315 train_time:39530ms step_avg:60.08ms
step:659/2315 train_time:39590ms step_avg:60.08ms
step:660/2315 train_time:39650ms step_avg:60.08ms
step:661/2315 train_time:39710ms step_avg:60.08ms
step:662/2315 train_time:39770ms step_avg:60.08ms
step:663/2315 train_time:39830ms step_avg:60.07ms
step:664/2315 train_time:39890ms step_avg:60.07ms
step:665/2315 train_time:39949ms step_avg:60.07ms
step:666/2315 train_time:40009ms step_avg:60.07ms
step:667/2315 train_time:40069ms step_avg:60.07ms
step:668/2315 train_time:40130ms step_avg:60.07ms
step:669/2315 train_time:40189ms step_avg:60.07ms
step:670/2315 train_time:40249ms step_avg:60.07ms
step:671/2315 train_time:40308ms step_avg:60.07ms
step:672/2315 train_time:40369ms step_avg:60.07ms
step:673/2315 train_time:40430ms step_avg:60.07ms
step:674/2315 train_time:40490ms step_avg:60.07ms
step:675/2315 train_time:40550ms step_avg:60.07ms
step:676/2315 train_time:40610ms step_avg:60.07ms
step:677/2315 train_time:40670ms step_avg:60.07ms
step:678/2315 train_time:40730ms step_avg:60.07ms
step:679/2315 train_time:40790ms step_avg:60.07ms
step:680/2315 train_time:40850ms step_avg:60.07ms
step:681/2315 train_time:40910ms step_avg:60.07ms
step:682/2315 train_time:40970ms step_avg:60.07ms
step:683/2315 train_time:41029ms step_avg:60.07ms
step:684/2315 train_time:41089ms step_avg:60.07ms
step:685/2315 train_time:41149ms step_avg:60.07ms
step:686/2315 train_time:41209ms step_avg:60.07ms
step:687/2315 train_time:41269ms step_avg:60.07ms
step:688/2315 train_time:41329ms step_avg:60.07ms
step:689/2315 train_time:41389ms step_avg:60.07ms
step:690/2315 train_time:41449ms step_avg:60.07ms
step:691/2315 train_time:41509ms step_avg:60.07ms
step:692/2315 train_time:41569ms step_avg:60.07ms
step:693/2315 train_time:41629ms step_avg:60.07ms
step:694/2315 train_time:41689ms step_avg:60.07ms
step:695/2315 train_time:41749ms step_avg:60.07ms
step:696/2315 train_time:41809ms step_avg:60.07ms
step:697/2315 train_time:41869ms step_avg:60.07ms
step:698/2315 train_time:41930ms step_avg:60.07ms
step:699/2315 train_time:41989ms step_avg:60.07ms
step:700/2315 train_time:42050ms step_avg:60.07ms
step:701/2315 train_time:42109ms step_avg:60.07ms
step:702/2315 train_time:42169ms step_avg:60.07ms
step:703/2315 train_time:42230ms step_avg:60.07ms
step:704/2315 train_time:42289ms step_avg:60.07ms
step:705/2315 train_time:42349ms step_avg:60.07ms
step:706/2315 train_time:42409ms step_avg:60.07ms
step:707/2315 train_time:42469ms step_avg:60.07ms
step:708/2315 train_time:42529ms step_avg:60.07ms
step:709/2315 train_time:42589ms step_avg:60.07ms
step:710/2315 train_time:42649ms step_avg:60.07ms
step:711/2315 train_time:42709ms step_avg:60.07ms
step:712/2315 train_time:42769ms step_avg:60.07ms
step:713/2315 train_time:42828ms step_avg:60.07ms
step:714/2315 train_time:42888ms step_avg:60.07ms
step:715/2315 train_time:42948ms step_avg:60.07ms
step:716/2315 train_time:43009ms step_avg:60.07ms
step:717/2315 train_time:43068ms step_avg:60.07ms
step:718/2315 train_time:43128ms step_avg:60.07ms
step:719/2315 train_time:43188ms step_avg:60.07ms
step:720/2315 train_time:43249ms step_avg:60.07ms
step:721/2315 train_time:43309ms step_avg:60.07ms
step:722/2315 train_time:43369ms step_avg:60.07ms
step:723/2315 train_time:43429ms step_avg:60.07ms
step:724/2315 train_time:43489ms step_avg:60.07ms
step:725/2315 train_time:43549ms step_avg:60.07ms
step:726/2315 train_time:43610ms step_avg:60.07ms
step:727/2315 train_time:43670ms step_avg:60.07ms
step:728/2315 train_time:43731ms step_avg:60.07ms
step:729/2315 train_time:43791ms step_avg:60.07ms
step:730/2315 train_time:43851ms step_avg:60.07ms
step:731/2315 train_time:43911ms step_avg:60.07ms
step:732/2315 train_time:43970ms step_avg:60.07ms
step:733/2315 train_time:44030ms step_avg:60.07ms
step:734/2315 train_time:44090ms step_avg:60.07ms
step:735/2315 train_time:44149ms step_avg:60.07ms
step:736/2315 train_time:44210ms step_avg:60.07ms
step:737/2315 train_time:44269ms step_avg:60.07ms
step:738/2315 train_time:44329ms step_avg:60.07ms
step:739/2315 train_time:44388ms step_avg:60.07ms
step:740/2315 train_time:44448ms step_avg:60.07ms
step:741/2315 train_time:44509ms step_avg:60.07ms
step:742/2315 train_time:44569ms step_avg:60.07ms
step:743/2315 train_time:44629ms step_avg:60.07ms
step:744/2315 train_time:44689ms step_avg:60.07ms
step:745/2315 train_time:44749ms step_avg:60.07ms
step:746/2315 train_time:44809ms step_avg:60.07ms
step:747/2315 train_time:44869ms step_avg:60.07ms
step:748/2315 train_time:44929ms step_avg:60.07ms
step:749/2315 train_time:44989ms step_avg:60.07ms
step:750/2315 train_time:45050ms step_avg:60.07ms
step:750/2315 val_loss:3.6851 train_time:45111ms step_avg:60.15ms
step:751/2315 train_time:45129ms step_avg:60.09ms
step:752/2315 train_time:45174ms step_avg:60.07ms
step:753/2315 train_time:45236ms step_avg:60.07ms
step:754/2315 train_time:45299ms step_avg:60.08ms
step:755/2315 train_time:45359ms step_avg:60.08ms
step:756/2315 train_time:45421ms step_avg:60.08ms
step:757/2315 train_time:45480ms step_avg:60.08ms
step:758/2315 train_time:45540ms step_avg:60.08ms
step:759/2315 train_time:45599ms step_avg:60.08ms
step:760/2315 train_time:45659ms step_avg:60.08ms
step:761/2315 train_time:45719ms step_avg:60.08ms
step:762/2315 train_time:45780ms step_avg:60.08ms
step:763/2315 train_time:45840ms step_avg:60.08ms
step:764/2315 train_time:45901ms step_avg:60.08ms
step:765/2315 train_time:45961ms step_avg:60.08ms
step:766/2315 train_time:46022ms step_avg:60.08ms
step:767/2315 train_time:46084ms step_avg:60.08ms
step:768/2315 train_time:46147ms step_avg:60.09ms
step:769/2315 train_time:46208ms step_avg:60.09ms
step:770/2315 train_time:46269ms step_avg:60.09ms
step:771/2315 train_time:46330ms step_avg:60.09ms
step:772/2315 train_time:46391ms step_avg:60.09ms
step:773/2315 train_time:46452ms step_avg:60.09ms
step:774/2315 train_time:46513ms step_avg:60.09ms
step:775/2315 train_time:46573ms step_avg:60.09ms
step:776/2315 train_time:46634ms step_avg:60.10ms
step:777/2315 train_time:46694ms step_avg:60.10ms
step:778/2315 train_time:46755ms step_avg:60.10ms
step:779/2315 train_time:46816ms step_avg:60.10ms
step:780/2315 train_time:46877ms step_avg:60.10ms
step:781/2315 train_time:46936ms step_avg:60.10ms
step:782/2315 train_time:46998ms step_avg:60.10ms
step:783/2315 train_time:47060ms step_avg:60.10ms
step:784/2315 train_time:47121ms step_avg:60.10ms
step:785/2315 train_time:47183ms step_avg:60.11ms
step:786/2315 train_time:47244ms step_avg:60.11ms
step:787/2315 train_time:47305ms step_avg:60.11ms
step:788/2315 train_time:47366ms step_avg:60.11ms
step:789/2315 train_time:47427ms step_avg:60.11ms
step:790/2315 train_time:47488ms step_avg:60.11ms
step:791/2315 train_time:47548ms step_avg:60.11ms
step:792/2315 train_time:47608ms step_avg:60.11ms
step:793/2315 train_time:47668ms step_avg:60.11ms
step:794/2315 train_time:47729ms step_avg:60.11ms
step:795/2315 train_time:47789ms step_avg:60.11ms
step:796/2315 train_time:47850ms step_avg:60.11ms
step:797/2315 train_time:47910ms step_avg:60.11ms
step:798/2315 train_time:47972ms step_avg:60.11ms
step:799/2315 train_time:48032ms step_avg:60.12ms
step:800/2315 train_time:48094ms step_avg:60.12ms
step:801/2315 train_time:48156ms step_avg:60.12ms
step:802/2315 train_time:48218ms step_avg:60.12ms
step:803/2315 train_time:48279ms step_avg:60.12ms
step:804/2315 train_time:48341ms step_avg:60.13ms
step:805/2315 train_time:48402ms step_avg:60.13ms
step:806/2315 train_time:48463ms step_avg:60.13ms
step:807/2315 train_time:48523ms step_avg:60.13ms
step:808/2315 train_time:48584ms step_avg:60.13ms
step:809/2315 train_time:48644ms step_avg:60.13ms
step:810/2315 train_time:48705ms step_avg:60.13ms
step:811/2315 train_time:48765ms step_avg:60.13ms
step:812/2315 train_time:48826ms step_avg:60.13ms
step:813/2315 train_time:48887ms step_avg:60.13ms
step:814/2315 train_time:48948ms step_avg:60.13ms
step:815/2315 train_time:49008ms step_avg:60.13ms
step:816/2315 train_time:49069ms step_avg:60.13ms
step:817/2315 train_time:49129ms step_avg:60.13ms
step:818/2315 train_time:49191ms step_avg:60.14ms
step:819/2315 train_time:49251ms step_avg:60.14ms
step:820/2315 train_time:49313ms step_avg:60.14ms
step:821/2315 train_time:49375ms step_avg:60.14ms
step:822/2315 train_time:49437ms step_avg:60.14ms
step:823/2315 train_time:49498ms step_avg:60.14ms
step:824/2315 train_time:49560ms step_avg:60.15ms
step:825/2315 train_time:49620ms step_avg:60.15ms
step:826/2315 train_time:49681ms step_avg:60.15ms
step:827/2315 train_time:49742ms step_avg:60.15ms
step:828/2315 train_time:49803ms step_avg:60.15ms
step:829/2315 train_time:49863ms step_avg:60.15ms
step:830/2315 train_time:49924ms step_avg:60.15ms
step:831/2315 train_time:49985ms step_avg:60.15ms
step:832/2315 train_time:50046ms step_avg:60.15ms
step:833/2315 train_time:50106ms step_avg:60.15ms
step:834/2315 train_time:50168ms step_avg:60.15ms
step:835/2315 train_time:50228ms step_avg:60.15ms
step:836/2315 train_time:50289ms step_avg:60.15ms
step:837/2315 train_time:50350ms step_avg:60.15ms
step:838/2315 train_time:50411ms step_avg:60.16ms
step:839/2315 train_time:50472ms step_avg:60.16ms
step:840/2315 train_time:50533ms step_avg:60.16ms
step:841/2315 train_time:50594ms step_avg:60.16ms
step:842/2315 train_time:50655ms step_avg:60.16ms
step:843/2315 train_time:50716ms step_avg:60.16ms
step:844/2315 train_time:50777ms step_avg:60.16ms
step:845/2315 train_time:50837ms step_avg:60.16ms
step:846/2315 train_time:50899ms step_avg:60.16ms
step:847/2315 train_time:50960ms step_avg:60.17ms
step:848/2315 train_time:51021ms step_avg:60.17ms
step:849/2315 train_time:51082ms step_avg:60.17ms
step:850/2315 train_time:51143ms step_avg:60.17ms
step:851/2315 train_time:51204ms step_avg:60.17ms
step:852/2315 train_time:51265ms step_avg:60.17ms
step:853/2315 train_time:51326ms step_avg:60.17ms
step:854/2315 train_time:51387ms step_avg:60.17ms
step:855/2315 train_time:51448ms step_avg:60.17ms
step:856/2315 train_time:51509ms step_avg:60.17ms
step:857/2315 train_time:51569ms step_avg:60.17ms
step:858/2315 train_time:51630ms step_avg:60.18ms
step:859/2315 train_time:51691ms step_avg:60.18ms
step:860/2315 train_time:51752ms step_avg:60.18ms
step:861/2315 train_time:51812ms step_avg:60.18ms
step:862/2315 train_time:51874ms step_avg:60.18ms
step:863/2315 train_time:51935ms step_avg:60.18ms
step:864/2315 train_time:51996ms step_avg:60.18ms
step:865/2315 train_time:52057ms step_avg:60.18ms
step:866/2315 train_time:52119ms step_avg:60.18ms
step:867/2315 train_time:52180ms step_avg:60.18ms
step:868/2315 train_time:52242ms step_avg:60.19ms
step:869/2315 train_time:52302ms step_avg:60.19ms
step:870/2315 train_time:52363ms step_avg:60.19ms
step:871/2315 train_time:52424ms step_avg:60.19ms
step:872/2315 train_time:52485ms step_avg:60.19ms
step:873/2315 train_time:52546ms step_avg:60.19ms
step:874/2315 train_time:52607ms step_avg:60.19ms
step:875/2315 train_time:52667ms step_avg:60.19ms
step:876/2315 train_time:52728ms step_avg:60.19ms
step:877/2315 train_time:52789ms step_avg:60.19ms
step:878/2315 train_time:52850ms step_avg:60.19ms
step:879/2315 train_time:52910ms step_avg:60.19ms
step:880/2315 train_time:52970ms step_avg:60.19ms
step:881/2315 train_time:53031ms step_avg:60.19ms
step:882/2315 train_time:53092ms step_avg:60.20ms
step:883/2315 train_time:53153ms step_avg:60.20ms
step:884/2315 train_time:53215ms step_avg:60.20ms
step:885/2315 train_time:53275ms step_avg:60.20ms
step:886/2315 train_time:53337ms step_avg:60.20ms
step:887/2315 train_time:53397ms step_avg:60.20ms
step:888/2315 train_time:53459ms step_avg:60.20ms
step:889/2315 train_time:53521ms step_avg:60.20ms
step:890/2315 train_time:53582ms step_avg:60.20ms
step:891/2315 train_time:53643ms step_avg:60.20ms
step:892/2315 train_time:53703ms step_avg:60.21ms
step:893/2315 train_time:53764ms step_avg:60.21ms
step:894/2315 train_time:53825ms step_avg:60.21ms
step:895/2315 train_time:53886ms step_avg:60.21ms
step:896/2315 train_time:53947ms step_avg:60.21ms
step:897/2315 train_time:54008ms step_avg:60.21ms
step:898/2315 train_time:54069ms step_avg:60.21ms
step:899/2315 train_time:54129ms step_avg:60.21ms
step:900/2315 train_time:54190ms step_avg:60.21ms
step:901/2315 train_time:54250ms step_avg:60.21ms
step:902/2315 train_time:54312ms step_avg:60.21ms
step:903/2315 train_time:54372ms step_avg:60.21ms
step:904/2315 train_time:54434ms step_avg:60.21ms
step:905/2315 train_time:54495ms step_avg:60.22ms
step:906/2315 train_time:54557ms step_avg:60.22ms
step:907/2315 train_time:54617ms step_avg:60.22ms
step:908/2315 train_time:54679ms step_avg:60.22ms
step:909/2315 train_time:54740ms step_avg:60.22ms
step:910/2315 train_time:54801ms step_avg:60.22ms
step:911/2315 train_time:54862ms step_avg:60.22ms
step:912/2315 train_time:54923ms step_avg:60.22ms
step:913/2315 train_time:54984ms step_avg:60.22ms
step:914/2315 train_time:55045ms step_avg:60.22ms
step:915/2315 train_time:55105ms step_avg:60.22ms
step:916/2315 train_time:55166ms step_avg:60.22ms
step:917/2315 train_time:55227ms step_avg:60.23ms
step:918/2315 train_time:55291ms step_avg:60.23ms
step:919/2315 train_time:55348ms step_avg:60.23ms
step:920/2315 train_time:55409ms step_avg:60.23ms
step:921/2315 train_time:55469ms step_avg:60.23ms
step:922/2315 train_time:55530ms step_avg:60.23ms
step:923/2315 train_time:55591ms step_avg:60.23ms
step:924/2315 train_time:55652ms step_avg:60.23ms
step:925/2315 train_time:55712ms step_avg:60.23ms
step:926/2315 train_time:55774ms step_avg:60.23ms
step:927/2315 train_time:55835ms step_avg:60.23ms
step:928/2315 train_time:55897ms step_avg:60.23ms
step:929/2315 train_time:55958ms step_avg:60.23ms
step:930/2315 train_time:56020ms step_avg:60.24ms
step:931/2315 train_time:56080ms step_avg:60.24ms
step:932/2315 train_time:56141ms step_avg:60.24ms
step:933/2315 train_time:56202ms step_avg:60.24ms
step:934/2315 train_time:56263ms step_avg:60.24ms
step:935/2315 train_time:56323ms step_avg:60.24ms
step:936/2315 train_time:56385ms step_avg:60.24ms
step:937/2315 train_time:56445ms step_avg:60.24ms
step:938/2315 train_time:56507ms step_avg:60.24ms
step:939/2315 train_time:56567ms step_avg:60.24ms
step:940/2315 train_time:56628ms step_avg:60.24ms
step:941/2315 train_time:56689ms step_avg:60.24ms
step:942/2315 train_time:56749ms step_avg:60.24ms
step:943/2315 train_time:56809ms step_avg:60.24ms
step:944/2315 train_time:56871ms step_avg:60.24ms
step:945/2315 train_time:56931ms step_avg:60.24ms
step:946/2315 train_time:56991ms step_avg:60.24ms
step:947/2315 train_time:57052ms step_avg:60.25ms
step:948/2315 train_time:57114ms step_avg:60.25ms
step:949/2315 train_time:57175ms step_avg:60.25ms
step:950/2315 train_time:57237ms step_avg:60.25ms
step:951/2315 train_time:57298ms step_avg:60.25ms
step:952/2315 train_time:57359ms step_avg:60.25ms
step:953/2315 train_time:57420ms step_avg:60.25ms
step:954/2315 train_time:57482ms step_avg:60.25ms
step:955/2315 train_time:57542ms step_avg:60.25ms
step:956/2315 train_time:57603ms step_avg:60.25ms
step:957/2315 train_time:57664ms step_avg:60.25ms
step:958/2315 train_time:57725ms step_avg:60.26ms
step:959/2315 train_time:57785ms step_avg:60.26ms
step:960/2315 train_time:57847ms step_avg:60.26ms
step:961/2315 train_time:57907ms step_avg:60.26ms
step:962/2315 train_time:57968ms step_avg:60.26ms
step:963/2315 train_time:58028ms step_avg:60.26ms
step:964/2315 train_time:58088ms step_avg:60.26ms
step:965/2315 train_time:58149ms step_avg:60.26ms
step:966/2315 train_time:58211ms step_avg:60.26ms
step:967/2315 train_time:58271ms step_avg:60.26ms
step:968/2315 train_time:58333ms step_avg:60.26ms
step:969/2315 train_time:58394ms step_avg:60.26ms
step:970/2315 train_time:58456ms step_avg:60.26ms
step:971/2315 train_time:58517ms step_avg:60.27ms
step:972/2315 train_time:58579ms step_avg:60.27ms
step:973/2315 train_time:58639ms step_avg:60.27ms
step:974/2315 train_time:58700ms step_avg:60.27ms
step:975/2315 train_time:58762ms step_avg:60.27ms
step:976/2315 train_time:58823ms step_avg:60.27ms
step:977/2315 train_time:58883ms step_avg:60.27ms
step:978/2315 train_time:58944ms step_avg:60.27ms
step:979/2315 train_time:59004ms step_avg:60.27ms
step:980/2315 train_time:59065ms step_avg:60.27ms
step:981/2315 train_time:59126ms step_avg:60.27ms
step:982/2315 train_time:59187ms step_avg:60.27ms
step:983/2315 train_time:59248ms step_avg:60.27ms
step:984/2315 train_time:59309ms step_avg:60.27ms
step:985/2315 train_time:59369ms step_avg:60.27ms
step:986/2315 train_time:59430ms step_avg:60.27ms
step:987/2315 train_time:59490ms step_avg:60.27ms
step:988/2315 train_time:59551ms step_avg:60.27ms
step:989/2315 train_time:59611ms step_avg:60.27ms
step:990/2315 train_time:59673ms step_avg:60.28ms
step:991/2315 train_time:59734ms step_avg:60.28ms
step:992/2315 train_time:59795ms step_avg:60.28ms
step:993/2315 train_time:59856ms step_avg:60.28ms
step:994/2315 train_time:59917ms step_avg:60.28ms
step:995/2315 train_time:59978ms step_avg:60.28ms
step:996/2315 train_time:60039ms step_avg:60.28ms
step:997/2315 train_time:60100ms step_avg:60.28ms
step:998/2315 train_time:60162ms step_avg:60.28ms
step:999/2315 train_time:60222ms step_avg:60.28ms
step:1000/2315 train_time:60284ms step_avg:60.28ms
step:1000/2315 val_loss:3.5697 train_time:60347ms step_avg:60.35ms
step:1001/2315 train_time:60365ms step_avg:60.30ms
step:1002/2315 train_time:60409ms step_avg:60.29ms
step:1003/2315 train_time:60473ms step_avg:60.29ms
step:1004/2315 train_time:60536ms step_avg:60.30ms
step:1005/2315 train_time:60597ms step_avg:60.30ms
step:1006/2315 train_time:60658ms step_avg:60.30ms
step:1007/2315 train_time:60718ms step_avg:60.30ms
step:1008/2315 train_time:60779ms step_avg:60.30ms
step:1009/2315 train_time:60839ms step_avg:60.30ms
step:1010/2315 train_time:60899ms step_avg:60.30ms
step:1011/2315 train_time:60958ms step_avg:60.30ms
step:1012/2315 train_time:61018ms step_avg:60.29ms
step:1013/2315 train_time:61078ms step_avg:60.29ms
step:1014/2315 train_time:61138ms step_avg:60.29ms
step:1015/2315 train_time:61197ms step_avg:60.29ms
step:1016/2315 train_time:61259ms step_avg:60.29ms
step:1017/2315 train_time:61322ms step_avg:60.30ms
step:1018/2315 train_time:61386ms step_avg:60.30ms
step:1019/2315 train_time:61448ms step_avg:60.30ms
step:1020/2315 train_time:61511ms step_avg:60.30ms
step:1021/2315 train_time:61571ms step_avg:60.30ms
step:1022/2315 train_time:61632ms step_avg:60.31ms
step:1023/2315 train_time:61693ms step_avg:60.31ms
step:1024/2315 train_time:61755ms step_avg:60.31ms
step:1025/2315 train_time:61814ms step_avg:60.31ms
step:1026/2315 train_time:61875ms step_avg:60.31ms
step:1027/2315 train_time:61934ms step_avg:60.31ms
step:1028/2315 train_time:61995ms step_avg:60.31ms
step:1029/2315 train_time:62056ms step_avg:60.31ms
step:1030/2315 train_time:62117ms step_avg:60.31ms
step:1031/2315 train_time:62176ms step_avg:60.31ms
step:1032/2315 train_time:62238ms step_avg:60.31ms
step:1033/2315 train_time:62301ms step_avg:60.31ms
step:1034/2315 train_time:62363ms step_avg:60.31ms
step:1035/2315 train_time:62424ms step_avg:60.31ms
step:1036/2315 train_time:62486ms step_avg:60.31ms
step:1037/2315 train_time:62546ms step_avg:60.31ms
step:1038/2315 train_time:62608ms step_avg:60.32ms
step:1039/2315 train_time:62667ms step_avg:60.32ms
step:1040/2315 train_time:62728ms step_avg:60.32ms
step:1041/2315 train_time:62788ms step_avg:60.32ms
step:1042/2315 train_time:62848ms step_avg:60.32ms
step:1043/2315 train_time:62908ms step_avg:60.31ms
step:1044/2315 train_time:62969ms step_avg:60.31ms
step:1045/2315 train_time:63029ms step_avg:60.31ms
step:1046/2315 train_time:63089ms step_avg:60.31ms
step:1047/2315 train_time:63149ms step_avg:60.31ms
step:1048/2315 train_time:63210ms step_avg:60.31ms
step:1049/2315 train_time:63270ms step_avg:60.31ms
step:1050/2315 train_time:63332ms step_avg:60.32ms
step:1051/2315 train_time:63393ms step_avg:60.32ms
step:1052/2315 train_time:63456ms step_avg:60.32ms
step:1053/2315 train_time:63517ms step_avg:60.32ms
step:1054/2315 train_time:63579ms step_avg:60.32ms
step:1055/2315 train_time:63639ms step_avg:60.32ms
step:1056/2315 train_time:63701ms step_avg:60.32ms
step:1057/2315 train_time:63762ms step_avg:60.32ms
step:1058/2315 train_time:63823ms step_avg:60.32ms
step:1059/2315 train_time:63884ms step_avg:60.32ms
step:1060/2315 train_time:63945ms step_avg:60.33ms
step:1061/2315 train_time:64005ms step_avg:60.33ms
step:1062/2315 train_time:64066ms step_avg:60.33ms
step:1063/2315 train_time:64127ms step_avg:60.33ms
step:1064/2315 train_time:64188ms step_avg:60.33ms
step:1065/2315 train_time:64249ms step_avg:60.33ms
step:1066/2315 train_time:64309ms step_avg:60.33ms
step:1067/2315 train_time:64369ms step_avg:60.33ms
step:1068/2315 train_time:64433ms step_avg:60.33ms
step:1069/2315 train_time:64492ms step_avg:60.33ms
step:1070/2315 train_time:64553ms step_avg:60.33ms
step:1071/2315 train_time:64614ms step_avg:60.33ms
step:1072/2315 train_time:64675ms step_avg:60.33ms
step:1073/2315 train_time:64736ms step_avg:60.33ms
step:1074/2315 train_time:64798ms step_avg:60.33ms
step:1075/2315 train_time:64859ms step_avg:60.33ms
step:1076/2315 train_time:64919ms step_avg:60.33ms
step:1077/2315 train_time:64980ms step_avg:60.33ms
step:1078/2315 train_time:65042ms step_avg:60.34ms
step:1079/2315 train_time:65102ms step_avg:60.34ms
step:1080/2315 train_time:65163ms step_avg:60.34ms
step:1081/2315 train_time:65224ms step_avg:60.34ms
step:1082/2315 train_time:65285ms step_avg:60.34ms
step:1083/2315 train_time:65346ms step_avg:60.34ms
step:1084/2315 train_time:65407ms step_avg:60.34ms
step:1085/2315 train_time:65468ms step_avg:60.34ms
step:1086/2315 train_time:65529ms step_avg:60.34ms
step:1087/2315 train_time:65590ms step_avg:60.34ms
step:1088/2315 train_time:65650ms step_avg:60.34ms
step:1089/2315 train_time:65711ms step_avg:60.34ms
step:1090/2315 train_time:65772ms step_avg:60.34ms
step:1091/2315 train_time:65833ms step_avg:60.34ms
step:1092/2315 train_time:65894ms step_avg:60.34ms
step:1093/2315 train_time:65955ms step_avg:60.34ms
step:1094/2315 train_time:66017ms step_avg:60.34ms
step:1095/2315 train_time:66077ms step_avg:60.34ms
step:1096/2315 train_time:66140ms step_avg:60.35ms
step:1097/2315 train_time:66201ms step_avg:60.35ms
step:1098/2315 train_time:66262ms step_avg:60.35ms
step:1099/2315 train_time:66323ms step_avg:60.35ms
step:1100/2315 train_time:66384ms step_avg:60.35ms
step:1101/2315 train_time:66445ms step_avg:60.35ms
step:1102/2315 train_time:66506ms step_avg:60.35ms
step:1103/2315 train_time:66567ms step_avg:60.35ms
step:1104/2315 train_time:66628ms step_avg:60.35ms
step:1105/2315 train_time:66689ms step_avg:60.35ms
step:1106/2315 train_time:66750ms step_avg:60.35ms
step:1107/2315 train_time:66810ms step_avg:60.35ms
step:1108/2315 train_time:66870ms step_avg:60.35ms
step:1109/2315 train_time:66930ms step_avg:60.35ms
step:1110/2315 train_time:66991ms step_avg:60.35ms
step:1111/2315 train_time:67052ms step_avg:60.35ms
step:1112/2315 train_time:67114ms step_avg:60.35ms
step:1113/2315 train_time:67174ms step_avg:60.35ms
step:1114/2315 train_time:67236ms step_avg:60.36ms
step:1115/2315 train_time:67296ms step_avg:60.36ms
step:1116/2315 train_time:67358ms step_avg:60.36ms
step:1117/2315 train_time:67419ms step_avg:60.36ms
step:1118/2315 train_time:67480ms step_avg:60.36ms
step:1119/2315 train_time:67541ms step_avg:60.36ms
step:1120/2315 train_time:67603ms step_avg:60.36ms
step:1121/2315 train_time:67664ms step_avg:60.36ms
step:1122/2315 train_time:67726ms step_avg:60.36ms
step:1123/2315 train_time:67786ms step_avg:60.36ms
step:1124/2315 train_time:67847ms step_avg:60.36ms
step:1125/2315 train_time:67907ms step_avg:60.36ms
step:1126/2315 train_time:67968ms step_avg:60.36ms
step:1127/2315 train_time:68029ms step_avg:60.36ms
step:1128/2315 train_time:68090ms step_avg:60.36ms
step:1129/2315 train_time:68150ms step_avg:60.36ms
step:1130/2315 train_time:68211ms step_avg:60.36ms
step:1131/2315 train_time:68272ms step_avg:60.36ms
step:1132/2315 train_time:68333ms step_avg:60.36ms
step:1133/2315 train_time:68393ms step_avg:60.36ms
step:1134/2315 train_time:68454ms step_avg:60.37ms
step:1135/2315 train_time:68516ms step_avg:60.37ms
step:1136/2315 train_time:68577ms step_avg:60.37ms
step:1137/2315 train_time:68638ms step_avg:60.37ms
step:1138/2315 train_time:68699ms step_avg:60.37ms
step:1139/2315 train_time:68760ms step_avg:60.37ms
step:1140/2315 train_time:68821ms step_avg:60.37ms
step:1141/2315 train_time:68882ms step_avg:60.37ms
step:1142/2315 train_time:68943ms step_avg:60.37ms
step:1143/2315 train_time:69004ms step_avg:60.37ms
step:1144/2315 train_time:69065ms step_avg:60.37ms
step:1145/2315 train_time:69125ms step_avg:60.37ms
step:1146/2315 train_time:69187ms step_avg:60.37ms
step:1147/2315 train_time:69247ms step_avg:60.37ms
step:1148/2315 train_time:69308ms step_avg:60.37ms
step:1149/2315 train_time:69369ms step_avg:60.37ms
step:1150/2315 train_time:69430ms step_avg:60.37ms
step:1151/2315 train_time:69490ms step_avg:60.37ms
step:1152/2315 train_time:69551ms step_avg:60.37ms
step:1153/2315 train_time:69612ms step_avg:60.37ms
step:1154/2315 train_time:69673ms step_avg:60.37ms
step:1155/2315 train_time:69733ms step_avg:60.37ms
step:1156/2315 train_time:69795ms step_avg:60.38ms
step:1157/2315 train_time:69856ms step_avg:60.38ms
step:1158/2315 train_time:69917ms step_avg:60.38ms
step:1159/2315 train_time:69978ms step_avg:60.38ms
step:1160/2315 train_time:70039ms step_avg:60.38ms
step:1161/2315 train_time:70101ms step_avg:60.38ms
step:1162/2315 train_time:70162ms step_avg:60.38ms
step:1163/2315 train_time:70223ms step_avg:60.38ms
step:1164/2315 train_time:70284ms step_avg:60.38ms
step:1165/2315 train_time:70344ms step_avg:60.38ms
step:1166/2315 train_time:70405ms step_avg:60.38ms
step:1167/2315 train_time:70466ms step_avg:60.38ms
step:1168/2315 train_time:70530ms step_avg:60.39ms
step:1169/2315 train_time:70587ms step_avg:60.38ms
step:1170/2315 train_time:70649ms step_avg:60.38ms
step:1171/2315 train_time:70709ms step_avg:60.38ms
step:1172/2315 train_time:70769ms step_avg:60.38ms
step:1173/2315 train_time:70830ms step_avg:60.38ms
step:1174/2315 train_time:70890ms step_avg:60.38ms
step:1175/2315 train_time:70951ms step_avg:60.38ms
step:1176/2315 train_time:71013ms step_avg:60.38ms
step:1177/2315 train_time:71074ms step_avg:60.39ms
step:1178/2315 train_time:71135ms step_avg:60.39ms
step:1179/2315 train_time:71196ms step_avg:60.39ms
step:1180/2315 train_time:71258ms step_avg:60.39ms
step:1181/2315 train_time:71318ms step_avg:60.39ms
step:1182/2315 train_time:71380ms step_avg:60.39ms
step:1183/2315 train_time:71441ms step_avg:60.39ms
step:1184/2315 train_time:71502ms step_avg:60.39ms
step:1185/2315 train_time:71563ms step_avg:60.39ms
step:1186/2315 train_time:71624ms step_avg:60.39ms
step:1187/2315 train_time:71685ms step_avg:60.39ms
step:1188/2315 train_time:71746ms step_avg:60.39ms
step:1189/2315 train_time:71807ms step_avg:60.39ms
step:1190/2315 train_time:71868ms step_avg:60.39ms
step:1191/2315 train_time:71928ms step_avg:60.39ms
step:1192/2315 train_time:71989ms step_avg:60.39ms
step:1193/2315 train_time:72049ms step_avg:60.39ms
step:1194/2315 train_time:72109ms step_avg:60.39ms
step:1195/2315 train_time:72170ms step_avg:60.39ms
step:1196/2315 train_time:72231ms step_avg:60.39ms
step:1197/2315 train_time:72293ms step_avg:60.39ms
step:1198/2315 train_time:72355ms step_avg:60.40ms
step:1199/2315 train_time:72415ms step_avg:60.40ms
step:1200/2315 train_time:72476ms step_avg:60.40ms
step:1201/2315 train_time:72537ms step_avg:60.40ms
step:1202/2315 train_time:72599ms step_avg:60.40ms
step:1203/2315 train_time:72660ms step_avg:60.40ms
step:1204/2315 train_time:72721ms step_avg:60.40ms
step:1205/2315 train_time:72782ms step_avg:60.40ms
step:1206/2315 train_time:72843ms step_avg:60.40ms
step:1207/2315 train_time:72904ms step_avg:60.40ms
step:1208/2315 train_time:72965ms step_avg:60.40ms
step:1209/2315 train_time:73026ms step_avg:60.40ms
step:1210/2315 train_time:73088ms step_avg:60.40ms
step:1211/2315 train_time:73148ms step_avg:60.40ms
step:1212/2315 train_time:73209ms step_avg:60.40ms
step:1213/2315 train_time:73269ms step_avg:60.40ms
step:1214/2315 train_time:73330ms step_avg:60.40ms
step:1215/2315 train_time:73390ms step_avg:60.40ms
step:1216/2315 train_time:73451ms step_avg:60.40ms
step:1217/2315 train_time:73512ms step_avg:60.40ms
step:1218/2315 train_time:73573ms step_avg:60.40ms
step:1219/2315 train_time:73633ms step_avg:60.40ms
step:1220/2315 train_time:73694ms step_avg:60.41ms
step:1221/2315 train_time:73755ms step_avg:60.41ms
step:1222/2315 train_time:73818ms step_avg:60.41ms
step:1223/2315 train_time:73879ms step_avg:60.41ms
step:1224/2315 train_time:73941ms step_avg:60.41ms
step:1225/2315 train_time:74002ms step_avg:60.41ms
step:1226/2315 train_time:74063ms step_avg:60.41ms
step:1227/2315 train_time:74124ms step_avg:60.41ms
step:1228/2315 train_time:74185ms step_avg:60.41ms
step:1229/2315 train_time:74245ms step_avg:60.41ms
step:1230/2315 train_time:74306ms step_avg:60.41ms
step:1231/2315 train_time:74367ms step_avg:60.41ms
step:1232/2315 train_time:74428ms step_avg:60.41ms
step:1233/2315 train_time:74489ms step_avg:60.41ms
step:1234/2315 train_time:74549ms step_avg:60.41ms
step:1235/2315 train_time:74609ms step_avg:60.41ms
step:1236/2315 train_time:74670ms step_avg:60.41ms
step:1237/2315 train_time:74730ms step_avg:60.41ms
step:1238/2315 train_time:74792ms step_avg:60.41ms
step:1239/2315 train_time:74852ms step_avg:60.41ms
step:1240/2315 train_time:74914ms step_avg:60.41ms
step:1241/2315 train_time:74974ms step_avg:60.41ms
step:1242/2315 train_time:75036ms step_avg:60.42ms
step:1243/2315 train_time:75097ms step_avg:60.42ms
step:1244/2315 train_time:75159ms step_avg:60.42ms
step:1245/2315 train_time:75219ms step_avg:60.42ms
step:1246/2315 train_time:75281ms step_avg:60.42ms
step:1247/2315 train_time:75343ms step_avg:60.42ms
step:1248/2315 train_time:75404ms step_avg:60.42ms
step:1249/2315 train_time:75465ms step_avg:60.42ms
step:1250/2315 train_time:75526ms step_avg:60.42ms
step:1250/2315 val_loss:3.5129 train_time:75588ms step_avg:60.47ms
step:1251/2315 train_time:75606ms step_avg:60.44ms
step:1252/2315 train_time:75651ms step_avg:60.42ms
step:1253/2315 train_time:75714ms step_avg:60.43ms
step:1254/2315 train_time:75776ms step_avg:60.43ms
step:1255/2315 train_time:75837ms step_avg:60.43ms
step:1256/2315 train_time:75898ms step_avg:60.43ms
step:1257/2315 train_time:75958ms step_avg:60.43ms
step:1258/2315 train_time:76019ms step_avg:60.43ms
step:1259/2315 train_time:76079ms step_avg:60.43ms
step:1260/2315 train_time:76139ms step_avg:60.43ms
step:1261/2315 train_time:76199ms step_avg:60.43ms
step:1262/2315 train_time:76260ms step_avg:60.43ms
step:1263/2315 train_time:76319ms step_avg:60.43ms
step:1264/2315 train_time:76380ms step_avg:60.43ms
step:1265/2315 train_time:76440ms step_avg:60.43ms
step:1266/2315 train_time:76502ms step_avg:60.43ms
step:1267/2315 train_time:76564ms step_avg:60.43ms
step:1268/2315 train_time:76627ms step_avg:60.43ms
step:1269/2315 train_time:76689ms step_avg:60.43ms
step:1270/2315 train_time:76751ms step_avg:60.43ms
step:1271/2315 train_time:76812ms step_avg:60.43ms
step:1272/2315 train_time:76873ms step_avg:60.44ms
step:1273/2315 train_time:76934ms step_avg:60.43ms
step:1274/2315 train_time:76994ms step_avg:60.43ms
step:1275/2315 train_time:77054ms step_avg:60.43ms
step:1276/2315 train_time:77115ms step_avg:60.43ms
step:1277/2315 train_time:77175ms step_avg:60.43ms
step:1278/2315 train_time:77236ms step_avg:60.43ms
step:1279/2315 train_time:77296ms step_avg:60.43ms
step:1280/2315 train_time:77357ms step_avg:60.44ms
step:1281/2315 train_time:77417ms step_avg:60.43ms
step:1282/2315 train_time:77479ms step_avg:60.44ms
step:1283/2315 train_time:77540ms step_avg:60.44ms
step:1284/2315 train_time:77602ms step_avg:60.44ms
step:1285/2315 train_time:77664ms step_avg:60.44ms
step:1286/2315 train_time:77726ms step_avg:60.44ms
step:1287/2315 train_time:77786ms step_avg:60.44ms
step:1288/2315 train_time:77847ms step_avg:60.44ms
step:1289/2315 train_time:77908ms step_avg:60.44ms
step:1290/2315 train_time:77969ms step_avg:60.44ms
step:1291/2315 train_time:78029ms step_avg:60.44ms
step:1292/2315 train_time:78089ms step_avg:60.44ms
step:1293/2315 train_time:78149ms step_avg:60.44ms
step:1294/2315 train_time:78210ms step_avg:60.44ms
step:1295/2315 train_time:78270ms step_avg:60.44ms
step:1296/2315 train_time:78331ms step_avg:60.44ms
step:1297/2315 train_time:78392ms step_avg:60.44ms
step:1298/2315 train_time:78453ms step_avg:60.44ms
step:1299/2315 train_time:78514ms step_avg:60.44ms
step:1300/2315 train_time:78576ms step_avg:60.44ms
step:1301/2315 train_time:78637ms step_avg:60.44ms
step:1302/2315 train_time:78700ms step_avg:60.45ms
step:1303/2315 train_time:78761ms step_avg:60.45ms
step:1304/2315 train_time:78822ms step_avg:60.45ms
step:1305/2315 train_time:78883ms step_avg:60.45ms
step:1306/2315 train_time:78943ms step_avg:60.45ms
step:1307/2315 train_time:79004ms step_avg:60.45ms
step:1308/2315 train_time:79065ms step_avg:60.45ms
step:1309/2315 train_time:79125ms step_avg:60.45ms
step:1310/2315 train_time:79186ms step_avg:60.45ms
step:1311/2315 train_time:79246ms step_avg:60.45ms
step:1312/2315 train_time:79308ms step_avg:60.45ms
step:1313/2315 train_time:79368ms step_avg:60.45ms
step:1314/2315 train_time:79429ms step_avg:60.45ms
step:1315/2315 train_time:79489ms step_avg:60.45ms
step:1316/2315 train_time:79550ms step_avg:60.45ms
step:1317/2315 train_time:79612ms step_avg:60.45ms
step:1318/2315 train_time:79673ms step_avg:60.45ms
step:1319/2315 train_time:79734ms step_avg:60.45ms
step:1320/2315 train_time:79796ms step_avg:60.45ms
step:1321/2315 train_time:79857ms step_avg:60.45ms
step:1322/2315 train_time:79918ms step_avg:60.45ms
step:1323/2315 train_time:79979ms step_avg:60.45ms
step:1324/2315 train_time:80041ms step_avg:60.45ms
step:1325/2315 train_time:80101ms step_avg:60.45ms
step:1326/2315 train_time:80162ms step_avg:60.45ms
step:1327/2315 train_time:80222ms step_avg:60.45ms
step:1328/2315 train_time:80284ms step_avg:60.45ms
step:1329/2315 train_time:80345ms step_avg:60.45ms
step:1330/2315 train_time:80406ms step_avg:60.46ms
step:1331/2315 train_time:80466ms step_avg:60.46ms
step:1332/2315 train_time:80527ms step_avg:60.46ms
step:1333/2315 train_time:80588ms step_avg:60.46ms
step:1334/2315 train_time:80649ms step_avg:60.46ms
step:1335/2315 train_time:80710ms step_avg:60.46ms
step:1336/2315 train_time:80771ms step_avg:60.46ms
step:1337/2315 train_time:80832ms step_avg:60.46ms
step:1338/2315 train_time:80894ms step_avg:60.46ms
step:1339/2315 train_time:80954ms step_avg:60.46ms
step:1340/2315 train_time:81015ms step_avg:60.46ms
step:1341/2315 train_time:81076ms step_avg:60.46ms
step:1342/2315 train_time:81138ms step_avg:60.46ms
step:1343/2315 train_time:81198ms step_avg:60.46ms
step:1344/2315 train_time:81260ms step_avg:60.46ms
step:1345/2315 train_time:81320ms step_avg:60.46ms
step:1346/2315 train_time:81382ms step_avg:60.46ms
step:1347/2315 train_time:81443ms step_avg:60.46ms
step:1348/2315 train_time:81504ms step_avg:60.46ms
step:1349/2315 train_time:81565ms step_avg:60.46ms
step:1350/2315 train_time:81626ms step_avg:60.46ms
step:1351/2315 train_time:81688ms step_avg:60.46ms
step:1352/2315 train_time:81748ms step_avg:60.46ms
step:1353/2315 train_time:81808ms step_avg:60.46ms
step:1354/2315 train_time:81869ms step_avg:60.46ms
step:1355/2315 train_time:81930ms step_avg:60.46ms
step:1356/2315 train_time:81991ms step_avg:60.47ms
step:1357/2315 train_time:82051ms step_avg:60.47ms
step:1358/2315 train_time:82112ms step_avg:60.47ms
step:1359/2315 train_time:82172ms step_avg:60.47ms
step:1360/2315 train_time:82234ms step_avg:60.47ms
step:1361/2315 train_time:82295ms step_avg:60.47ms
step:1362/2315 train_time:82357ms step_avg:60.47ms
step:1363/2315 train_time:82419ms step_avg:60.47ms
step:1364/2315 train_time:82481ms step_avg:60.47ms
step:1365/2315 train_time:82541ms step_avg:60.47ms
step:1366/2315 train_time:82602ms step_avg:60.47ms
step:1367/2315 train_time:82663ms step_avg:60.47ms
step:1368/2315 train_time:82724ms step_avg:60.47ms
step:1369/2315 train_time:82784ms step_avg:60.47ms
step:1370/2315 train_time:82846ms step_avg:60.47ms
step:1371/2315 train_time:82906ms step_avg:60.47ms
step:1372/2315 train_time:82967ms step_avg:60.47ms
step:1373/2315 train_time:83028ms step_avg:60.47ms
step:1374/2315 train_time:83089ms step_avg:60.47ms
step:1375/2315 train_time:83149ms step_avg:60.47ms
step:1376/2315 train_time:83209ms step_avg:60.47ms
step:1377/2315 train_time:83270ms step_avg:60.47ms
step:1378/2315 train_time:83332ms step_avg:60.47ms
step:1379/2315 train_time:83393ms step_avg:60.47ms
step:1380/2315 train_time:83455ms step_avg:60.47ms
step:1381/2315 train_time:83515ms step_avg:60.47ms
step:1382/2315 train_time:83577ms step_avg:60.48ms
step:1383/2315 train_time:83638ms step_avg:60.48ms
step:1384/2315 train_time:83699ms step_avg:60.48ms
step:1385/2315 train_time:83760ms step_avg:60.48ms
step:1386/2315 train_time:83821ms step_avg:60.48ms
step:1387/2315 train_time:83882ms step_avg:60.48ms
step:1388/2315 train_time:83943ms step_avg:60.48ms
step:1389/2315 train_time:84003ms step_avg:60.48ms
step:1390/2315 train_time:84065ms step_avg:60.48ms
step:1391/2315 train_time:84125ms step_avg:60.48ms
step:1392/2315 train_time:84186ms step_avg:60.48ms
step:1393/2315 train_time:84247ms step_avg:60.48ms
step:1394/2315 train_time:84308ms step_avg:60.48ms
step:1395/2315 train_time:84369ms step_avg:60.48ms
step:1396/2315 train_time:84430ms step_avg:60.48ms
step:1397/2315 train_time:84490ms step_avg:60.48ms
step:1398/2315 train_time:84551ms step_avg:60.48ms
step:1399/2315 train_time:84612ms step_avg:60.48ms
step:1400/2315 train_time:84674ms step_avg:60.48ms
step:1401/2315 train_time:84734ms step_avg:60.48ms
step:1402/2315 train_time:84796ms step_avg:60.48ms
step:1403/2315 train_time:84857ms step_avg:60.48ms
step:1404/2315 train_time:84918ms step_avg:60.48ms
step:1405/2315 train_time:84979ms step_avg:60.48ms
step:1406/2315 train_time:85041ms step_avg:60.48ms
step:1407/2315 train_time:85102ms step_avg:60.48ms
step:1408/2315 train_time:85163ms step_avg:60.49ms
step:1409/2315 train_time:85223ms step_avg:60.48ms
step:1410/2315 train_time:85284ms step_avg:60.49ms
step:1411/2315 train_time:85344ms step_avg:60.49ms
step:1412/2315 train_time:85406ms step_avg:60.49ms
step:1413/2315 train_time:85466ms step_avg:60.49ms
step:1414/2315 train_time:85527ms step_avg:60.49ms
step:1415/2315 train_time:85588ms step_avg:60.49ms
step:1416/2315 train_time:85648ms step_avg:60.49ms
step:1417/2315 train_time:85708ms step_avg:60.49ms
step:1418/2315 train_time:85769ms step_avg:60.49ms
step:1419/2315 train_time:85830ms step_avg:60.49ms
step:1420/2315 train_time:85891ms step_avg:60.49ms
step:1421/2315 train_time:85952ms step_avg:60.49ms
step:1422/2315 train_time:86014ms step_avg:60.49ms
step:1423/2315 train_time:86074ms step_avg:60.49ms
step:1424/2315 train_time:86136ms step_avg:60.49ms
step:1425/2315 train_time:86197ms step_avg:60.49ms
step:1426/2315 train_time:86259ms step_avg:60.49ms
step:1427/2315 train_time:86319ms step_avg:60.49ms
step:1428/2315 train_time:86381ms step_avg:60.49ms
step:1429/2315 train_time:86442ms step_avg:60.49ms
step:1430/2315 train_time:86503ms step_avg:60.49ms
step:1431/2315 train_time:86564ms step_avg:60.49ms
step:1432/2315 train_time:86625ms step_avg:60.49ms
step:1433/2315 train_time:86686ms step_avg:60.49ms
step:1434/2315 train_time:86746ms step_avg:60.49ms
step:1435/2315 train_time:86807ms step_avg:60.49ms
step:1436/2315 train_time:86868ms step_avg:60.49ms
step:1437/2315 train_time:86928ms step_avg:60.49ms
step:1438/2315 train_time:86989ms step_avg:60.49ms
step:1439/2315 train_time:87050ms step_avg:60.49ms
step:1440/2315 train_time:87112ms step_avg:60.49ms
step:1441/2315 train_time:87172ms step_avg:60.49ms
step:1442/2315 train_time:87234ms step_avg:60.49ms
step:1443/2315 train_time:87295ms step_avg:60.50ms
step:1444/2315 train_time:87357ms step_avg:60.50ms
step:1445/2315 train_time:87418ms step_avg:60.50ms
step:1446/2315 train_time:87480ms step_avg:60.50ms
step:1447/2315 train_time:87540ms step_avg:60.50ms
step:1448/2315 train_time:87601ms step_avg:60.50ms
step:1449/2315 train_time:87661ms step_avg:60.50ms
step:1450/2315 train_time:87723ms step_avg:60.50ms
step:1451/2315 train_time:87783ms step_avg:60.50ms
step:1452/2315 train_time:87844ms step_avg:60.50ms
step:1453/2315 train_time:87905ms step_avg:60.50ms
step:1454/2315 train_time:87967ms step_avg:60.50ms
step:1455/2315 train_time:88027ms step_avg:60.50ms
step:1456/2315 train_time:88088ms step_avg:60.50ms
step:1457/2315 train_time:88148ms step_avg:60.50ms
step:1458/2315 train_time:88209ms step_avg:60.50ms
step:1459/2315 train_time:88269ms step_avg:60.50ms
step:1460/2315 train_time:88330ms step_avg:60.50ms
step:1461/2315 train_time:88390ms step_avg:60.50ms
step:1462/2315 train_time:88453ms step_avg:60.50ms
step:1463/2315 train_time:88514ms step_avg:60.50ms
step:1464/2315 train_time:88575ms step_avg:60.50ms
step:1465/2315 train_time:88636ms step_avg:60.50ms
step:1466/2315 train_time:88698ms step_avg:60.50ms
step:1467/2315 train_time:88758ms step_avg:60.50ms
step:1468/2315 train_time:88819ms step_avg:60.50ms
step:1469/2315 train_time:88881ms step_avg:60.50ms
step:1470/2315 train_time:88942ms step_avg:60.50ms
step:1471/2315 train_time:89003ms step_avg:60.51ms
step:1472/2315 train_time:89064ms step_avg:60.51ms
step:1473/2315 train_time:89125ms step_avg:60.51ms
step:1474/2315 train_time:89186ms step_avg:60.51ms
step:1475/2315 train_time:89247ms step_avg:60.51ms
step:1476/2315 train_time:89308ms step_avg:60.51ms
step:1477/2315 train_time:89368ms step_avg:60.51ms
step:1478/2315 train_time:89429ms step_avg:60.51ms
step:1479/2315 train_time:89489ms step_avg:60.51ms
step:1480/2315 train_time:89550ms step_avg:60.51ms
step:1481/2315 train_time:89611ms step_avg:60.51ms
step:1482/2315 train_time:89672ms step_avg:60.51ms
step:1483/2315 train_time:89733ms step_avg:60.51ms
step:1484/2315 train_time:89794ms step_avg:60.51ms
step:1485/2315 train_time:89855ms step_avg:60.51ms
step:1486/2315 train_time:89918ms step_avg:60.51ms
step:1487/2315 train_time:89979ms step_avg:60.51ms
step:1488/2315 train_time:90040ms step_avg:60.51ms
step:1489/2315 train_time:90102ms step_avg:60.51ms
step:1490/2315 train_time:90163ms step_avg:60.51ms
step:1491/2315 train_time:90223ms step_avg:60.51ms
step:1492/2315 train_time:90284ms step_avg:60.51ms
step:1493/2315 train_time:90345ms step_avg:60.51ms
step:1494/2315 train_time:90406ms step_avg:60.51ms
step:1495/2315 train_time:90466ms step_avg:60.51ms
step:1496/2315 train_time:90528ms step_avg:60.51ms
step:1497/2315 train_time:90589ms step_avg:60.51ms
step:1498/2315 train_time:90649ms step_avg:60.51ms
step:1499/2315 train_time:90710ms step_avg:60.51ms
step:1500/2315 train_time:90773ms step_avg:60.52ms
step:1500/2315 val_loss:3.4487 train_time:90833ms step_avg:60.56ms
step:1501/2315 train_time:90853ms step_avg:60.53ms
step:1502/2315 train_time:90894ms step_avg:60.52ms
step:1503/2315 train_time:90957ms step_avg:60.52ms
step:1504/2315 train_time:91019ms step_avg:60.52ms
step:1505/2315 train_time:91080ms step_avg:60.52ms
step:1506/2315 train_time:91141ms step_avg:60.52ms
step:1507/2315 train_time:91200ms step_avg:60.52ms
step:1508/2315 train_time:91261ms step_avg:60.52ms
step:1509/2315 train_time:91321ms step_avg:60.52ms
step:1510/2315 train_time:91381ms step_avg:60.52ms
step:1511/2315 train_time:91441ms step_avg:60.52ms
step:1512/2315 train_time:91501ms step_avg:60.52ms
step:1513/2315 train_time:91561ms step_avg:60.52ms
step:1514/2315 train_time:91621ms step_avg:60.52ms
step:1515/2315 train_time:91681ms step_avg:60.52ms
step:1516/2315 train_time:91742ms step_avg:60.52ms
step:1517/2315 train_time:91805ms step_avg:60.52ms
step:1518/2315 train_time:91867ms step_avg:60.52ms
step:1519/2315 train_time:91929ms step_avg:60.52ms
step:1520/2315 train_time:91991ms step_avg:60.52ms
step:1521/2315 train_time:92053ms step_avg:60.52ms
step:1522/2315 train_time:92115ms step_avg:60.52ms
step:1523/2315 train_time:92176ms step_avg:60.52ms
step:1524/2315 train_time:92237ms step_avg:60.52ms
step:1525/2315 train_time:92298ms step_avg:60.52ms
step:1526/2315 train_time:92359ms step_avg:60.52ms
step:1527/2315 train_time:92419ms step_avg:60.52ms
step:1528/2315 train_time:92480ms step_avg:60.52ms
step:1529/2315 train_time:92540ms step_avg:60.52ms
step:1530/2315 train_time:92601ms step_avg:60.52ms
step:1531/2315 train_time:92661ms step_avg:60.52ms
step:1532/2315 train_time:92723ms step_avg:60.52ms
step:1533/2315 train_time:92785ms step_avg:60.53ms
step:1534/2315 train_time:92847ms step_avg:60.53ms
step:1535/2315 train_time:92909ms step_avg:60.53ms
step:1536/2315 train_time:92972ms step_avg:60.53ms
step:1537/2315 train_time:93034ms step_avg:60.53ms
step:1538/2315 train_time:93095ms step_avg:60.53ms
step:1539/2315 train_time:93156ms step_avg:60.53ms
step:1540/2315 train_time:93218ms step_avg:60.53ms
step:1541/2315 train_time:93279ms step_avg:60.53ms
step:1542/2315 train_time:93340ms step_avg:60.53ms
step:1543/2315 train_time:93400ms step_avg:60.53ms
step:1544/2315 train_time:93461ms step_avg:60.53ms
step:1545/2315 train_time:93522ms step_avg:60.53ms
step:1546/2315 train_time:93583ms step_avg:60.53ms
step:1547/2315 train_time:93643ms step_avg:60.53ms
step:1548/2315 train_time:93707ms step_avg:60.53ms
step:1549/2315 train_time:93765ms step_avg:60.53ms
step:1550/2315 train_time:93827ms step_avg:60.53ms
step:1551/2315 train_time:93889ms step_avg:60.53ms
step:1552/2315 train_time:93952ms step_avg:60.54ms
step:1553/2315 train_time:94013ms step_avg:60.54ms
step:1554/2315 train_time:94076ms step_avg:60.54ms
step:1555/2315 train_time:94137ms step_avg:60.54ms
step:1556/2315 train_time:94199ms step_avg:60.54ms
step:1557/2315 train_time:94259ms step_avg:60.54ms
step:1558/2315 train_time:94320ms step_avg:60.54ms
step:1559/2315 train_time:94381ms step_avg:60.54ms
step:1560/2315 train_time:94442ms step_avg:60.54ms
step:1561/2315 train_time:94503ms step_avg:60.54ms
step:1562/2315 train_time:94564ms step_avg:60.54ms
step:1563/2315 train_time:94624ms step_avg:60.54ms
step:1564/2315 train_time:94686ms step_avg:60.54ms
step:1565/2315 train_time:94746ms step_avg:60.54ms
step:1566/2315 train_time:94808ms step_avg:60.54ms
step:1567/2315 train_time:94869ms step_avg:60.54ms
step:1568/2315 train_time:94932ms step_avg:60.54ms
step:1569/2315 train_time:94993ms step_avg:60.54ms
step:1570/2315 train_time:95056ms step_avg:60.55ms
step:1571/2315 train_time:95117ms step_avg:60.55ms
step:1572/2315 train_time:95178ms step_avg:60.55ms
step:1573/2315 train_time:95239ms step_avg:60.55ms
step:1574/2315 train_time:95301ms step_avg:60.55ms
step:1575/2315 train_time:95361ms step_avg:60.55ms
step:1576/2315 train_time:95423ms step_avg:60.55ms
step:1577/2315 train_time:95483ms step_avg:60.55ms
step:1578/2315 train_time:95545ms step_avg:60.55ms
step:1579/2315 train_time:95606ms step_avg:60.55ms
step:1580/2315 train_time:95667ms step_avg:60.55ms
step:1581/2315 train_time:95728ms step_avg:60.55ms
step:1582/2315 train_time:95789ms step_avg:60.55ms
step:1583/2315 train_time:95851ms step_avg:60.55ms
step:1584/2315 train_time:95913ms step_avg:60.55ms
step:1585/2315 train_time:95974ms step_avg:60.55ms
step:1586/2315 train_time:96036ms step_avg:60.55ms
step:1587/2315 train_time:96098ms step_avg:60.55ms
step:1588/2315 train_time:96159ms step_avg:60.55ms
step:1589/2315 train_time:96219ms step_avg:60.55ms
step:1590/2315 train_time:96281ms step_avg:60.55ms
step:1591/2315 train_time:96341ms step_avg:60.55ms
step:1592/2315 train_time:96403ms step_avg:60.55ms
step:1593/2315 train_time:96464ms step_avg:60.55ms
step:1594/2315 train_time:96526ms step_avg:60.56ms
step:1595/2315 train_time:96586ms step_avg:60.56ms
step:1596/2315 train_time:96648ms step_avg:60.56ms
step:1597/2315 train_time:96710ms step_avg:60.56ms
step:1598/2315 train_time:96770ms step_avg:60.56ms
step:1599/2315 train_time:96831ms step_avg:60.56ms
step:1600/2315 train_time:96893ms step_avg:60.56ms
step:1601/2315 train_time:96954ms step_avg:60.56ms
step:1602/2315 train_time:97016ms step_avg:60.56ms
step:1603/2315 train_time:97077ms step_avg:60.56ms
step:1604/2315 train_time:97139ms step_avg:60.56ms
step:1605/2315 train_time:97200ms step_avg:60.56ms
step:1606/2315 train_time:97261ms step_avg:60.56ms
step:1607/2315 train_time:97322ms step_avg:60.56ms
step:1608/2315 train_time:97383ms step_avg:60.56ms
step:1609/2315 train_time:97444ms step_avg:60.56ms
step:1610/2315 train_time:97506ms step_avg:60.56ms
step:1611/2315 train_time:97566ms step_avg:60.56ms
step:1612/2315 train_time:97627ms step_avg:60.56ms
step:1613/2315 train_time:97688ms step_avg:60.56ms
step:1614/2315 train_time:97750ms step_avg:60.56ms
step:1615/2315 train_time:97811ms step_avg:60.56ms
step:1616/2315 train_time:97873ms step_avg:60.57ms
step:1617/2315 train_time:97934ms step_avg:60.57ms
step:1618/2315 train_time:97996ms step_avg:60.57ms
step:1619/2315 train_time:98057ms step_avg:60.57ms
step:1620/2315 train_time:98119ms step_avg:60.57ms
step:1621/2315 train_time:98180ms step_avg:60.57ms
step:1622/2315 train_time:98241ms step_avg:60.57ms
step:1623/2315 train_time:98302ms step_avg:60.57ms
step:1624/2315 train_time:98363ms step_avg:60.57ms
step:1625/2315 train_time:98424ms step_avg:60.57ms
step:1626/2315 train_time:98485ms step_avg:60.57ms
step:1627/2315 train_time:98546ms step_avg:60.57ms
step:1628/2315 train_time:98608ms step_avg:60.57ms
step:1629/2315 train_time:98669ms step_avg:60.57ms
step:1630/2315 train_time:98731ms step_avg:60.57ms
step:1631/2315 train_time:98792ms step_avg:60.57ms
step:1632/2315 train_time:98854ms step_avg:60.57ms
step:1633/2315 train_time:98915ms step_avg:60.57ms
step:1634/2315 train_time:98976ms step_avg:60.57ms
step:1635/2315 train_time:99037ms step_avg:60.57ms
step:1636/2315 train_time:99099ms step_avg:60.57ms
step:1637/2315 train_time:99160ms step_avg:60.57ms
step:1638/2315 train_time:99221ms step_avg:60.57ms
step:1639/2315 train_time:99282ms step_avg:60.57ms
step:1640/2315 train_time:99343ms step_avg:60.58ms
step:1641/2315 train_time:99404ms step_avg:60.58ms
step:1642/2315 train_time:99465ms step_avg:60.58ms
step:1643/2315 train_time:99526ms step_avg:60.58ms
step:1644/2315 train_time:99588ms step_avg:60.58ms
step:1645/2315 train_time:99649ms step_avg:60.58ms
step:1646/2315 train_time:99711ms step_avg:60.58ms
step:1647/2315 train_time:99773ms step_avg:60.58ms
step:1648/2315 train_time:99835ms step_avg:60.58ms
step:1649/2315 train_time:99895ms step_avg:60.58ms
step:1650/2315 train_time:99957ms step_avg:60.58ms
step:1651/2315 train_time:100018ms step_avg:60.58ms
step:1652/2315 train_time:100080ms step_avg:60.58ms
step:1653/2315 train_time:100141ms step_avg:60.58ms
step:1654/2315 train_time:100202ms step_avg:60.58ms
step:1655/2315 train_time:100263ms step_avg:60.58ms
step:1656/2315 train_time:100324ms step_avg:60.58ms
step:1657/2315 train_time:100385ms step_avg:60.58ms
step:1658/2315 train_time:100446ms step_avg:60.58ms
step:1659/2315 train_time:100507ms step_avg:60.58ms
step:1660/2315 train_time:100569ms step_avg:60.58ms
step:1661/2315 train_time:100630ms step_avg:60.58ms
step:1662/2315 train_time:100692ms step_avg:60.58ms
step:1663/2315 train_time:100753ms step_avg:60.59ms
step:1664/2315 train_time:100814ms step_avg:60.59ms
step:1665/2315 train_time:100875ms step_avg:60.59ms
step:1666/2315 train_time:100936ms step_avg:60.59ms
step:1667/2315 train_time:100997ms step_avg:60.59ms
step:1668/2315 train_time:101058ms step_avg:60.59ms
step:1669/2315 train_time:101119ms step_avg:60.59ms
step:1670/2315 train_time:101181ms step_avg:60.59ms
step:1671/2315 train_time:101241ms step_avg:60.59ms
step:1672/2315 train_time:101303ms step_avg:60.59ms
step:1673/2315 train_time:101363ms step_avg:60.59ms
step:1674/2315 train_time:101425ms step_avg:60.59ms
step:1675/2315 train_time:101486ms step_avg:60.59ms
step:1676/2315 train_time:101547ms step_avg:60.59ms
step:1677/2315 train_time:101609ms step_avg:60.59ms
step:1678/2315 train_time:101671ms step_avg:60.59ms
step:1679/2315 train_time:101733ms step_avg:60.59ms
step:1680/2315 train_time:101795ms step_avg:60.59ms
step:1681/2315 train_time:101855ms step_avg:60.59ms
step:1682/2315 train_time:101917ms step_avg:60.59ms
step:1683/2315 train_time:101978ms step_avg:60.59ms
step:1684/2315 train_time:102040ms step_avg:60.59ms
step:1685/2315 train_time:102101ms step_avg:60.59ms
step:1686/2315 train_time:102162ms step_avg:60.59ms
step:1687/2315 train_time:102223ms step_avg:60.59ms
step:1688/2315 train_time:102284ms step_avg:60.59ms
step:1689/2315 train_time:102345ms step_avg:60.60ms
step:1690/2315 train_time:102407ms step_avg:60.60ms
step:1691/2315 train_time:102468ms step_avg:60.60ms
step:1692/2315 train_time:102530ms step_avg:60.60ms
step:1693/2315 train_time:102590ms step_avg:60.60ms
step:1694/2315 train_time:102652ms step_avg:60.60ms
step:1695/2315 train_time:102713ms step_avg:60.60ms
step:1696/2315 train_time:102775ms step_avg:60.60ms
step:1697/2315 train_time:102836ms step_avg:60.60ms
step:1698/2315 train_time:102897ms step_avg:60.60ms
step:1699/2315 train_time:102958ms step_avg:60.60ms
step:1700/2315 train_time:103020ms step_avg:60.60ms
step:1701/2315 train_time:103080ms step_avg:60.60ms
step:1702/2315 train_time:103141ms step_avg:60.60ms
step:1703/2315 train_time:103202ms step_avg:60.60ms
step:1704/2315 train_time:103263ms step_avg:60.60ms
step:1705/2315 train_time:103324ms step_avg:60.60ms
step:1706/2315 train_time:103385ms step_avg:60.60ms
step:1707/2315 train_time:103447ms step_avg:60.60ms
step:1708/2315 train_time:103509ms step_avg:60.60ms
step:1709/2315 train_time:103570ms step_avg:60.60ms
step:1710/2315 train_time:103632ms step_avg:60.60ms
step:1711/2315 train_time:103693ms step_avg:60.60ms
step:1712/2315 train_time:103754ms step_avg:60.60ms
step:1713/2315 train_time:103816ms step_avg:60.60ms
step:1714/2315 train_time:103877ms step_avg:60.60ms
step:1715/2315 train_time:103937ms step_avg:60.60ms
step:1716/2315 train_time:103999ms step_avg:60.61ms
step:1717/2315 train_time:104059ms step_avg:60.61ms
step:1718/2315 train_time:104121ms step_avg:60.61ms
step:1719/2315 train_time:104181ms step_avg:60.61ms
step:1720/2315 train_time:104242ms step_avg:60.61ms
step:1721/2315 train_time:104303ms step_avg:60.61ms
step:1722/2315 train_time:104365ms step_avg:60.61ms
step:1723/2315 train_time:104426ms step_avg:60.61ms
step:1724/2315 train_time:104488ms step_avg:60.61ms
step:1725/2315 train_time:104549ms step_avg:60.61ms
step:1726/2315 train_time:104612ms step_avg:60.61ms
step:1727/2315 train_time:104673ms step_avg:60.61ms
step:1728/2315 train_time:104734ms step_avg:60.61ms
step:1729/2315 train_time:104795ms step_avg:60.61ms
step:1730/2315 train_time:104857ms step_avg:60.61ms
step:1731/2315 train_time:104918ms step_avg:60.61ms
step:1732/2315 train_time:104980ms step_avg:60.61ms
step:1733/2315 train_time:105040ms step_avg:60.61ms
step:1734/2315 train_time:105102ms step_avg:60.61ms
step:1735/2315 train_time:105162ms step_avg:60.61ms
step:1736/2315 train_time:105224ms step_avg:60.61ms
step:1737/2315 train_time:105284ms step_avg:60.61ms
step:1738/2315 train_time:105346ms step_avg:60.61ms
step:1739/2315 train_time:105407ms step_avg:60.61ms
step:1740/2315 train_time:105469ms step_avg:60.61ms
step:1741/2315 train_time:105530ms step_avg:60.61ms
step:1742/2315 train_time:105592ms step_avg:60.62ms
step:1743/2315 train_time:105653ms step_avg:60.62ms
step:1744/2315 train_time:105715ms step_avg:60.62ms
step:1745/2315 train_time:105776ms step_avg:60.62ms
step:1746/2315 train_time:105838ms step_avg:60.62ms
step:1747/2315 train_time:105899ms step_avg:60.62ms
step:1748/2315 train_time:105960ms step_avg:60.62ms
step:1749/2315 train_time:106021ms step_avg:60.62ms
step:1750/2315 train_time:106083ms step_avg:60.62ms
step:1750/2315 val_loss:3.3795 train_time:106145ms step_avg:60.65ms
step:1751/2315 train_time:106163ms step_avg:60.63ms
step:1752/2315 train_time:106204ms step_avg:60.62ms
step:1753/2315 train_time:106268ms step_avg:60.62ms
step:1754/2315 train_time:106335ms step_avg:60.62ms
step:1755/2315 train_time:106396ms step_avg:60.62ms
step:1756/2315 train_time:106458ms step_avg:60.63ms
step:1757/2315 train_time:106518ms step_avg:60.62ms
step:1758/2315 train_time:106579ms step_avg:60.63ms
step:1759/2315 train_time:106639ms step_avg:60.62ms
step:1760/2315 train_time:106699ms step_avg:60.62ms
step:1761/2315 train_time:106759ms step_avg:60.62ms
step:1762/2315 train_time:106820ms step_avg:60.62ms
step:1763/2315 train_time:106880ms step_avg:60.62ms
step:1764/2315 train_time:106940ms step_avg:60.62ms
step:1765/2315 train_time:107000ms step_avg:60.62ms
step:1766/2315 train_time:107065ms step_avg:60.63ms
step:1767/2315 train_time:107130ms step_avg:60.63ms
step:1768/2315 train_time:107192ms step_avg:60.63ms
step:1769/2315 train_time:107254ms step_avg:60.63ms
step:1770/2315 train_time:107316ms step_avg:60.63ms
step:1771/2315 train_time:107378ms step_avg:60.63ms
step:1772/2315 train_time:107440ms step_avg:60.63ms
step:1773/2315 train_time:107501ms step_avg:60.63ms
step:1774/2315 train_time:107562ms step_avg:60.63ms
step:1775/2315 train_time:107622ms step_avg:60.63ms
step:1776/2315 train_time:107682ms step_avg:60.63ms
step:1777/2315 train_time:107742ms step_avg:60.63ms
step:1778/2315 train_time:107803ms step_avg:60.63ms
step:1779/2315 train_time:107863ms step_avg:60.63ms
step:1780/2315 train_time:107923ms step_avg:60.63ms
step:1781/2315 train_time:107984ms step_avg:60.63ms
step:1782/2315 train_time:108046ms step_avg:60.63ms
step:1783/2315 train_time:108107ms step_avg:60.63ms
step:1784/2315 train_time:108169ms step_avg:60.63ms
step:1785/2315 train_time:108231ms step_avg:60.63ms
step:1786/2315 train_time:108294ms step_avg:60.63ms
step:1787/2315 train_time:108355ms step_avg:60.64ms
step:1788/2315 train_time:108417ms step_avg:60.64ms
step:1789/2315 train_time:108480ms step_avg:60.64ms
step:1790/2315 train_time:108539ms step_avg:60.64ms
step:1791/2315 train_time:108599ms step_avg:60.64ms
step:1792/2315 train_time:108660ms step_avg:60.64ms
step:1793/2315 train_time:108721ms step_avg:60.64ms
step:1794/2315 train_time:108782ms step_avg:60.64ms
step:1795/2315 train_time:108842ms step_avg:60.64ms
step:1796/2315 train_time:108903ms step_avg:60.64ms
step:1797/2315 train_time:108963ms step_avg:60.64ms
step:1798/2315 train_time:109025ms step_avg:60.64ms
step:1799/2315 train_time:109087ms step_avg:60.64ms
step:1800/2315 train_time:109148ms step_avg:60.64ms
step:1801/2315 train_time:109210ms step_avg:60.64ms
step:1802/2315 train_time:109272ms step_avg:60.64ms
step:1803/2315 train_time:109333ms step_avg:60.64ms
step:1804/2315 train_time:109395ms step_avg:60.64ms
step:1805/2315 train_time:109456ms step_avg:60.64ms
step:1806/2315 train_time:109518ms step_avg:60.64ms
step:1807/2315 train_time:109579ms step_avg:60.64ms
step:1808/2315 train_time:109640ms step_avg:60.64ms
step:1809/2315 train_time:109700ms step_avg:60.64ms
step:1810/2315 train_time:109760ms step_avg:60.64ms
step:1811/2315 train_time:109821ms step_avg:60.64ms
step:1812/2315 train_time:109882ms step_avg:60.64ms
step:1813/2315 train_time:109943ms step_avg:60.64ms
step:1814/2315 train_time:110004ms step_avg:60.64ms
step:1815/2315 train_time:110065ms step_avg:60.64ms
step:1816/2315 train_time:110127ms step_avg:60.64ms
step:1817/2315 train_time:110189ms step_avg:60.64ms
step:1818/2315 train_time:110249ms step_avg:60.64ms
step:1819/2315 train_time:110310ms step_avg:60.64ms
step:1820/2315 train_time:110372ms step_avg:60.64ms
step:1821/2315 train_time:110433ms step_avg:60.64ms
step:1822/2315 train_time:110495ms step_avg:60.65ms
step:1823/2315 train_time:110557ms step_avg:60.65ms
step:1824/2315 train_time:110618ms step_avg:60.65ms
step:1825/2315 train_time:110679ms step_avg:60.65ms
step:1826/2315 train_time:110740ms step_avg:60.65ms
step:1827/2315 train_time:110800ms step_avg:60.65ms
step:1828/2315 train_time:110862ms step_avg:60.65ms
step:1829/2315 train_time:110923ms step_avg:60.65ms
step:1830/2315 train_time:110984ms step_avg:60.65ms
step:1831/2315 train_time:111045ms step_avg:60.65ms
step:1832/2315 train_time:111107ms step_avg:60.65ms
step:1833/2315 train_time:111167ms step_avg:60.65ms
step:1834/2315 train_time:111228ms step_avg:60.65ms
step:1835/2315 train_time:111290ms step_avg:60.65ms
step:1836/2315 train_time:111352ms step_avg:60.65ms
step:1837/2315 train_time:111413ms step_avg:60.65ms
step:1838/2315 train_time:111475ms step_avg:60.65ms
step:1839/2315 train_time:111537ms step_avg:60.65ms
step:1840/2315 train_time:111599ms step_avg:60.65ms
step:1841/2315 train_time:111660ms step_avg:60.65ms
step:1842/2315 train_time:111721ms step_avg:60.65ms
step:1843/2315 train_time:111781ms step_avg:60.65ms
step:1844/2315 train_time:111843ms step_avg:60.65ms
step:1845/2315 train_time:111904ms step_avg:60.65ms
step:1846/2315 train_time:111965ms step_avg:60.65ms
step:1847/2315 train_time:112026ms step_avg:60.65ms
step:1848/2315 train_time:112087ms step_avg:60.65ms
step:1849/2315 train_time:112148ms step_avg:60.65ms
step:1850/2315 train_time:112209ms step_avg:60.65ms
step:1851/2315 train_time:112270ms step_avg:60.65ms
step:1852/2315 train_time:112331ms step_avg:60.65ms
step:1853/2315 train_time:112393ms step_avg:60.65ms
step:1854/2315 train_time:112455ms step_avg:60.66ms
step:1855/2315 train_time:112516ms step_avg:60.66ms
step:1856/2315 train_time:112578ms step_avg:60.66ms
step:1857/2315 train_time:112639ms step_avg:60.66ms
step:1858/2315 train_time:112700ms step_avg:60.66ms
step:1859/2315 train_time:112761ms step_avg:60.66ms
step:1860/2315 train_time:112822ms step_avg:60.66ms
step:1861/2315 train_time:112883ms step_avg:60.66ms
step:1862/2315 train_time:112944ms step_avg:60.66ms
step:1863/2315 train_time:113005ms step_avg:60.66ms
step:1864/2315 train_time:113066ms step_avg:60.66ms
step:1865/2315 train_time:113127ms step_avg:60.66ms
step:1866/2315 train_time:113188ms step_avg:60.66ms
step:1867/2315 train_time:113249ms step_avg:60.66ms
step:1868/2315 train_time:113311ms step_avg:60.66ms
step:1869/2315 train_time:113372ms step_avg:60.66ms
step:1870/2315 train_time:113434ms step_avg:60.66ms
step:1871/2315 train_time:113495ms step_avg:60.66ms
step:1872/2315 train_time:113557ms step_avg:60.66ms
step:1873/2315 train_time:113618ms step_avg:60.66ms
step:1874/2315 train_time:113680ms step_avg:60.66ms
step:1875/2315 train_time:113741ms step_avg:60.66ms
step:1876/2315 train_time:113803ms step_avg:60.66ms
step:1877/2315 train_time:113863ms step_avg:60.66ms
step:1878/2315 train_time:113925ms step_avg:60.66ms
step:1879/2315 train_time:113985ms step_avg:60.66ms
step:1880/2315 train_time:114047ms step_avg:60.66ms
step:1881/2315 train_time:114108ms step_avg:60.66ms
step:1882/2315 train_time:114169ms step_avg:60.66ms
step:1883/2315 train_time:114230ms step_avg:60.66ms
step:1884/2315 train_time:114291ms step_avg:60.66ms
step:1885/2315 train_time:114351ms step_avg:60.66ms
step:1886/2315 train_time:114414ms step_avg:60.66ms
step:1887/2315 train_time:114475ms step_avg:60.66ms
step:1888/2315 train_time:114537ms step_avg:60.67ms
step:1889/2315 train_time:114598ms step_avg:60.67ms
step:1890/2315 train_time:114660ms step_avg:60.67ms
step:1891/2315 train_time:114721ms step_avg:60.67ms
step:1892/2315 train_time:114782ms step_avg:60.67ms
step:1893/2315 train_time:114843ms step_avg:60.67ms
step:1894/2315 train_time:114905ms step_avg:60.67ms
step:1895/2315 train_time:114966ms step_avg:60.67ms
step:1896/2315 train_time:115027ms step_avg:60.67ms
step:1897/2315 train_time:115088ms step_avg:60.67ms
step:1898/2315 train_time:115150ms step_avg:60.67ms
step:1899/2315 train_time:115210ms step_avg:60.67ms
step:1900/2315 train_time:115272ms step_avg:60.67ms
step:1901/2315 train_time:115333ms step_avg:60.67ms
step:1902/2315 train_time:115394ms step_avg:60.67ms
step:1903/2315 train_time:115455ms step_avg:60.67ms
step:1904/2315 train_time:115517ms step_avg:60.67ms
step:1905/2315 train_time:115578ms step_avg:60.67ms
step:1906/2315 train_time:115641ms step_avg:60.67ms
step:1907/2315 train_time:115701ms step_avg:60.67ms
step:1908/2315 train_time:115763ms step_avg:60.67ms
step:1909/2315 train_time:115824ms step_avg:60.67ms
step:1910/2315 train_time:115886ms step_avg:60.67ms
step:1911/2315 train_time:115946ms step_avg:60.67ms
step:1912/2315 train_time:116007ms step_avg:60.67ms
step:1913/2315 train_time:116068ms step_avg:60.67ms
step:1914/2315 train_time:116130ms step_avg:60.67ms
step:1915/2315 train_time:116191ms step_avg:60.67ms
step:1916/2315 train_time:116252ms step_avg:60.67ms
step:1917/2315 train_time:116313ms step_avg:60.67ms
step:1918/2315 train_time:116375ms step_avg:60.68ms
step:1919/2315 train_time:116436ms step_avg:60.68ms
step:1920/2315 train_time:116498ms step_avg:60.68ms
step:1921/2315 train_time:116559ms step_avg:60.68ms
step:1922/2315 train_time:116621ms step_avg:60.68ms
step:1923/2315 train_time:116682ms step_avg:60.68ms
step:1924/2315 train_time:116744ms step_avg:60.68ms
step:1925/2315 train_time:116805ms step_avg:60.68ms
step:1926/2315 train_time:116865ms step_avg:60.68ms
step:1927/2315 train_time:116926ms step_avg:60.68ms
step:1928/2315 train_time:116987ms step_avg:60.68ms
step:1929/2315 train_time:117048ms step_avg:60.68ms
step:1930/2315 train_time:117109ms step_avg:60.68ms
step:1931/2315 train_time:117171ms step_avg:60.68ms
step:1932/2315 train_time:117232ms step_avg:60.68ms
step:1933/2315 train_time:117293ms step_avg:60.68ms
step:1934/2315 train_time:117355ms step_avg:60.68ms
step:1935/2315 train_time:117417ms step_avg:60.68ms
step:1936/2315 train_time:117478ms step_avg:60.68ms
step:1937/2315 train_time:117540ms step_avg:60.68ms
step:1938/2315 train_time:117601ms step_avg:60.68ms
step:1939/2315 train_time:117662ms step_avg:60.68ms
step:1940/2315 train_time:117723ms step_avg:60.68ms
step:1941/2315 train_time:117784ms step_avg:60.68ms
step:1942/2315 train_time:117845ms step_avg:60.68ms
step:1943/2315 train_time:117906ms step_avg:60.68ms
step:1944/2315 train_time:117968ms step_avg:60.68ms
step:1945/2315 train_time:118028ms step_avg:60.68ms
step:1946/2315 train_time:118090ms step_avg:60.68ms
step:1947/2315 train_time:118151ms step_avg:60.68ms
step:1948/2315 train_time:118213ms step_avg:60.68ms
step:1949/2315 train_time:118274ms step_avg:60.68ms
step:1950/2315 train_time:118337ms step_avg:60.69ms
step:1951/2315 train_time:118397ms step_avg:60.69ms
step:1952/2315 train_time:118458ms step_avg:60.69ms
step:1953/2315 train_time:118519ms step_avg:60.69ms
step:1954/2315 train_time:118581ms step_avg:60.69ms
step:1955/2315 train_time:118642ms step_avg:60.69ms
step:1956/2315 train_time:118703ms step_avg:60.69ms
step:1957/2315 train_time:118765ms step_avg:60.69ms
step:1958/2315 train_time:118825ms step_avg:60.69ms
step:1959/2315 train_time:118886ms step_avg:60.69ms
step:1960/2315 train_time:118947ms step_avg:60.69ms
step:1961/2315 train_time:119008ms step_avg:60.69ms
step:1962/2315 train_time:119069ms step_avg:60.69ms
step:1963/2315 train_time:119130ms step_avg:60.69ms
step:1964/2315 train_time:119192ms step_avg:60.69ms
step:1965/2315 train_time:119253ms step_avg:60.69ms
step:1966/2315 train_time:119314ms step_avg:60.69ms
step:1967/2315 train_time:119376ms step_avg:60.69ms
step:1968/2315 train_time:119438ms step_avg:60.69ms
step:1969/2315 train_time:119499ms step_avg:60.69ms
step:1970/2315 train_time:119561ms step_avg:60.69ms
step:1971/2315 train_time:119622ms step_avg:60.69ms
step:1972/2315 train_time:119685ms step_avg:60.69ms
step:1973/2315 train_time:119745ms step_avg:60.69ms
step:1974/2315 train_time:119806ms step_avg:60.69ms
step:1975/2315 train_time:119866ms step_avg:60.69ms
step:1976/2315 train_time:119928ms step_avg:60.69ms
step:1977/2315 train_time:119988ms step_avg:60.69ms
step:1978/2315 train_time:120049ms step_avg:60.69ms
step:1979/2315 train_time:120110ms step_avg:60.69ms
step:1980/2315 train_time:120171ms step_avg:60.69ms
step:1981/2315 train_time:120232ms step_avg:60.69ms
step:1982/2315 train_time:120294ms step_avg:60.69ms
step:1983/2315 train_time:120356ms step_avg:60.69ms
step:1984/2315 train_time:120418ms step_avg:60.69ms
step:1985/2315 train_time:120481ms step_avg:60.70ms
step:1986/2315 train_time:120541ms step_avg:60.70ms
step:1987/2315 train_time:120601ms step_avg:60.70ms
step:1988/2315 train_time:120663ms step_avg:60.70ms
step:1989/2315 train_time:120724ms step_avg:60.70ms
step:1990/2315 train_time:120786ms step_avg:60.70ms
step:1991/2315 train_time:120846ms step_avg:60.70ms
step:1992/2315 train_time:120908ms step_avg:60.70ms
step:1993/2315 train_time:120969ms step_avg:60.70ms
step:1994/2315 train_time:121030ms step_avg:60.70ms
step:1995/2315 train_time:121091ms step_avg:60.70ms
step:1996/2315 train_time:121153ms step_avg:60.70ms
step:1997/2315 train_time:121213ms step_avg:60.70ms
step:1998/2315 train_time:121275ms step_avg:60.70ms
step:1999/2315 train_time:121337ms step_avg:60.70ms
step:2000/2315 train_time:121398ms step_avg:60.70ms
step:2000/2315 val_loss:3.3296 train_time:121461ms step_avg:60.73ms
step:2001/2315 train_time:121480ms step_avg:60.71ms
step:2002/2315 train_time:121525ms step_avg:60.70ms
step:2003/2315 train_time:121588ms step_avg:60.70ms
step:2004/2315 train_time:121650ms step_avg:60.70ms
step:2005/2315 train_time:121711ms step_avg:60.70ms
step:2006/2315 train_time:121772ms step_avg:60.70ms
step:2007/2315 train_time:121832ms step_avg:60.70ms
step:2008/2315 train_time:121893ms step_avg:60.70ms
step:2009/2315 train_time:121953ms step_avg:60.70ms
step:2010/2315 train_time:122013ms step_avg:60.70ms
step:2011/2315 train_time:122073ms step_avg:60.70ms
step:2012/2315 train_time:122134ms step_avg:60.70ms
step:2013/2315 train_time:122195ms step_avg:60.70ms
step:2014/2315 train_time:122256ms step_avg:60.70ms
step:2015/2315 train_time:122317ms step_avg:60.70ms
step:2016/2315 train_time:122379ms step_avg:60.70ms
step:2017/2315 train_time:122441ms step_avg:60.70ms
step:2018/2315 train_time:122505ms step_avg:60.71ms
step:2019/2315 train_time:122568ms step_avg:60.71ms
step:2020/2315 train_time:122630ms step_avg:60.71ms
step:2021/2315 train_time:122691ms step_avg:60.71ms
step:2022/2315 train_time:122752ms step_avg:60.71ms
step:2023/2315 train_time:122813ms step_avg:60.71ms
step:2024/2315 train_time:122874ms step_avg:60.71ms
step:2025/2315 train_time:122934ms step_avg:60.71ms
step:2026/2315 train_time:122995ms step_avg:60.71ms
step:2027/2315 train_time:123055ms step_avg:60.71ms
step:2028/2315 train_time:123117ms step_avg:60.71ms
step:2029/2315 train_time:123177ms step_avg:60.71ms
step:2030/2315 train_time:123238ms step_avg:60.71ms
step:2031/2315 train_time:123299ms step_avg:60.71ms
step:2032/2315 train_time:123361ms step_avg:60.71ms
step:2033/2315 train_time:123423ms step_avg:60.71ms
step:2034/2315 train_time:123486ms step_avg:60.71ms
step:2035/2315 train_time:123547ms step_avg:60.71ms
step:2036/2315 train_time:123610ms step_avg:60.71ms
step:2037/2315 train_time:123671ms step_avg:60.71ms
step:2038/2315 train_time:123733ms step_avg:60.71ms
step:2039/2315 train_time:123794ms step_avg:60.71ms
step:2040/2315 train_time:123855ms step_avg:60.71ms
step:2041/2315 train_time:123916ms step_avg:60.71ms
step:2042/2315 train_time:123978ms step_avg:60.71ms
step:2043/2315 train_time:124038ms step_avg:60.71ms
step:2044/2315 train_time:124100ms step_avg:60.71ms
step:2045/2315 train_time:124160ms step_avg:60.71ms
step:2046/2315 train_time:124222ms step_avg:60.71ms
step:2047/2315 train_time:124283ms step_avg:60.71ms
step:2048/2315 train_time:124345ms step_avg:60.72ms
step:2049/2315 train_time:124408ms step_avg:60.72ms
step:2050/2315 train_time:124469ms step_avg:60.72ms
step:2051/2315 train_time:124530ms step_avg:60.72ms
step:2052/2315 train_time:124591ms step_avg:60.72ms
step:2053/2315 train_time:124652ms step_avg:60.72ms
step:2054/2315 train_time:124714ms step_avg:60.72ms
step:2055/2315 train_time:124775ms step_avg:60.72ms
step:2056/2315 train_time:124837ms step_avg:60.72ms
step:2057/2315 train_time:124899ms step_avg:60.72ms
step:2058/2315 train_time:124959ms step_avg:60.72ms
step:2059/2315 train_time:125020ms step_avg:60.72ms
step:2060/2315 train_time:125081ms step_avg:60.72ms
step:2061/2315 train_time:125141ms step_avg:60.72ms
step:2062/2315 train_time:125203ms step_avg:60.72ms
step:2063/2315 train_time:125264ms step_avg:60.72ms
step:2064/2315 train_time:125325ms step_avg:60.72ms
step:2065/2315 train_time:125386ms step_avg:60.72ms
step:2066/2315 train_time:125448ms step_avg:60.72ms
step:2067/2315 train_time:125509ms step_avg:60.72ms
step:2068/2315 train_time:125572ms step_avg:60.72ms
step:2069/2315 train_time:125633ms step_avg:60.72ms
step:2070/2315 train_time:125695ms step_avg:60.72ms
step:2071/2315 train_time:125756ms step_avg:60.72ms
step:2072/2315 train_time:125818ms step_avg:60.72ms
step:2073/2315 train_time:125878ms step_avg:60.72ms
step:2074/2315 train_time:125939ms step_avg:60.72ms
step:2075/2315 train_time:126000ms step_avg:60.72ms
step:2076/2315 train_time:126062ms step_avg:60.72ms
step:2077/2315 train_time:126122ms step_avg:60.72ms
step:2078/2315 train_time:126183ms step_avg:60.72ms
step:2079/2315 train_time:126244ms step_avg:60.72ms
step:2080/2315 train_time:126306ms step_avg:60.72ms
step:2081/2315 train_time:126367ms step_avg:60.72ms
step:2082/2315 train_time:126428ms step_avg:60.72ms
step:2083/2315 train_time:126490ms step_avg:60.72ms
step:2084/2315 train_time:126551ms step_avg:60.72ms
step:2085/2315 train_time:126612ms step_avg:60.73ms
step:2086/2315 train_time:126674ms step_avg:60.73ms
step:2087/2315 train_time:126735ms step_avg:60.73ms
step:2088/2315 train_time:126796ms step_avg:60.73ms
step:2089/2315 train_time:126857ms step_avg:60.73ms
step:2090/2315 train_time:126918ms step_avg:60.73ms
step:2091/2315 train_time:126979ms step_avg:60.73ms
step:2092/2315 train_time:127041ms step_avg:60.73ms
step:2093/2315 train_time:127101ms step_avg:60.73ms
step:2094/2315 train_time:127163ms step_avg:60.73ms
step:2095/2315 train_time:127224ms step_avg:60.73ms
step:2096/2315 train_time:127286ms step_avg:60.73ms
step:2097/2315 train_time:127347ms step_avg:60.73ms
step:2098/2315 train_time:127409ms step_avg:60.73ms
step:2099/2315 train_time:127470ms step_avg:60.73ms
step:2100/2315 train_time:127532ms step_avg:60.73ms
step:2101/2315 train_time:127592ms step_avg:60.73ms
step:2102/2315 train_time:127654ms step_avg:60.73ms
step:2103/2315 train_time:127715ms step_avg:60.73ms
step:2104/2315 train_time:127777ms step_avg:60.73ms
step:2105/2315 train_time:127837ms step_avg:60.73ms
step:2106/2315 train_time:127899ms step_avg:60.73ms
step:2107/2315 train_time:127960ms step_avg:60.73ms
step:2108/2315 train_time:128022ms step_avg:60.73ms
step:2109/2315 train_time:128082ms step_avg:60.73ms
step:2110/2315 train_time:128144ms step_avg:60.73ms
step:2111/2315 train_time:128206ms step_avg:60.73ms
step:2112/2315 train_time:128267ms step_avg:60.73ms
step:2113/2315 train_time:128328ms step_avg:60.73ms
step:2114/2315 train_time:128390ms step_avg:60.73ms
step:2115/2315 train_time:128451ms step_avg:60.73ms
step:2116/2315 train_time:128512ms step_avg:60.73ms
step:2117/2315 train_time:128573ms step_avg:60.73ms
step:2118/2315 train_time:128634ms step_avg:60.73ms
step:2119/2315 train_time:128697ms step_avg:60.73ms
step:2120/2315 train_time:128756ms step_avg:60.73ms
step:2121/2315 train_time:128817ms step_avg:60.73ms
step:2122/2315 train_time:128878ms step_avg:60.73ms
step:2123/2315 train_time:128939ms step_avg:60.73ms
step:2124/2315 train_time:129001ms step_avg:60.74ms
step:2125/2315 train_time:129062ms step_avg:60.74ms
step:2126/2315 train_time:129124ms step_avg:60.74ms
step:2127/2315 train_time:129185ms step_avg:60.74ms
step:2128/2315 train_time:129247ms step_avg:60.74ms
step:2129/2315 train_time:129308ms step_avg:60.74ms
step:2130/2315 train_time:129370ms step_avg:60.74ms
step:2131/2315 train_time:129431ms step_avg:60.74ms
step:2132/2315 train_time:129493ms step_avg:60.74ms
step:2133/2315 train_time:129553ms step_avg:60.74ms
step:2134/2315 train_time:129615ms step_avg:60.74ms
step:2135/2315 train_time:129676ms step_avg:60.74ms
step:2136/2315 train_time:129737ms step_avg:60.74ms
step:2137/2315 train_time:129801ms step_avg:60.74ms
step:2138/2315 train_time:129861ms step_avg:60.74ms
step:2139/2315 train_time:129921ms step_avg:60.74ms
step:2140/2315 train_time:129983ms step_avg:60.74ms
step:2141/2315 train_time:130044ms step_avg:60.74ms
step:2142/2315 train_time:130106ms step_avg:60.74ms
step:2143/2315 train_time:130167ms step_avg:60.74ms
step:2144/2315 train_time:130228ms step_avg:60.74ms
step:2145/2315 train_time:130289ms step_avg:60.74ms
step:2146/2315 train_time:130350ms step_avg:60.74ms
step:2147/2315 train_time:130411ms step_avg:60.74ms
step:2148/2315 train_time:130473ms step_avg:60.74ms
step:2149/2315 train_time:130534ms step_avg:60.74ms
step:2150/2315 train_time:130596ms step_avg:60.74ms
step:2151/2315 train_time:130657ms step_avg:60.74ms
step:2152/2315 train_time:130719ms step_avg:60.74ms
step:2153/2315 train_time:130780ms step_avg:60.74ms
step:2154/2315 train_time:130842ms step_avg:60.74ms
step:2155/2315 train_time:130903ms step_avg:60.74ms
step:2156/2315 train_time:130964ms step_avg:60.74ms
step:2157/2315 train_time:131025ms step_avg:60.74ms
step:2158/2315 train_time:131087ms step_avg:60.74ms
step:2159/2315 train_time:131148ms step_avg:60.74ms
step:2160/2315 train_time:131210ms step_avg:60.75ms
step:2161/2315 train_time:131270ms step_avg:60.75ms
step:2162/2315 train_time:131332ms step_avg:60.75ms
step:2163/2315 train_time:131393ms step_avg:60.75ms
step:2164/2315 train_time:131454ms step_avg:60.75ms
step:2165/2315 train_time:131515ms step_avg:60.75ms
step:2166/2315 train_time:131577ms step_avg:60.75ms
step:2167/2315 train_time:131638ms step_avg:60.75ms
step:2168/2315 train_time:131700ms step_avg:60.75ms
step:2169/2315 train_time:131761ms step_avg:60.75ms
step:2170/2315 train_time:131822ms step_avg:60.75ms
step:2171/2315 train_time:131883ms step_avg:60.75ms
step:2172/2315 train_time:131945ms step_avg:60.75ms
step:2173/2315 train_time:132008ms step_avg:60.75ms
step:2174/2315 train_time:132067ms step_avg:60.75ms
step:2175/2315 train_time:132128ms step_avg:60.75ms
step:2176/2315 train_time:132190ms step_avg:60.75ms
step:2177/2315 train_time:132251ms step_avg:60.75ms
step:2178/2315 train_time:132313ms step_avg:60.75ms
step:2179/2315 train_time:132374ms step_avg:60.75ms
step:2180/2315 train_time:132435ms step_avg:60.75ms
step:2181/2315 train_time:132496ms step_avg:60.75ms
step:2182/2315 train_time:132557ms step_avg:60.75ms
step:2183/2315 train_time:132618ms step_avg:60.75ms
step:2184/2315 train_time:132680ms step_avg:60.75ms
step:2185/2315 train_time:132741ms step_avg:60.75ms
step:2186/2315 train_time:132803ms step_avg:60.75ms
step:2187/2315 train_time:132864ms step_avg:60.75ms
step:2188/2315 train_time:132926ms step_avg:60.75ms
step:2189/2315 train_time:132987ms step_avg:60.75ms
step:2190/2315 train_time:133048ms step_avg:60.75ms
step:2191/2315 train_time:133109ms step_avg:60.75ms
step:2192/2315 train_time:133171ms step_avg:60.75ms
step:2193/2315 train_time:133232ms step_avg:60.75ms
step:2194/2315 train_time:133294ms step_avg:60.75ms
step:2195/2315 train_time:133354ms step_avg:60.75ms
step:2196/2315 train_time:133416ms step_avg:60.75ms
step:2197/2315 train_time:133478ms step_avg:60.75ms
step:2198/2315 train_time:133539ms step_avg:60.75ms
step:2199/2315 train_time:133601ms step_avg:60.76ms
step:2200/2315 train_time:133663ms step_avg:60.76ms
step:2201/2315 train_time:133723ms step_avg:60.76ms
step:2202/2315 train_time:133784ms step_avg:60.76ms
step:2203/2315 train_time:133845ms step_avg:60.76ms
step:2204/2315 train_time:133907ms step_avg:60.76ms
step:2205/2315 train_time:133968ms step_avg:60.76ms
step:2206/2315 train_time:134029ms step_avg:60.76ms
step:2207/2315 train_time:134090ms step_avg:60.76ms
step:2208/2315 train_time:134152ms step_avg:60.76ms
step:2209/2315 train_time:134212ms step_avg:60.76ms
step:2210/2315 train_time:134275ms step_avg:60.76ms
step:2211/2315 train_time:134336ms step_avg:60.76ms
step:2212/2315 train_time:134398ms step_avg:60.76ms
step:2213/2315 train_time:134459ms step_avg:60.76ms
step:2214/2315 train_time:134521ms step_avg:60.76ms
step:2215/2315 train_time:134582ms step_avg:60.76ms
step:2216/2315 train_time:134644ms step_avg:60.76ms
step:2217/2315 train_time:134706ms step_avg:60.76ms
step:2218/2315 train_time:134767ms step_avg:60.76ms
step:2219/2315 train_time:134827ms step_avg:60.76ms
step:2220/2315 train_time:134889ms step_avg:60.76ms
step:2221/2315 train_time:134949ms step_avg:60.76ms
step:2222/2315 train_time:135011ms step_avg:60.76ms
step:2223/2315 train_time:135072ms step_avg:60.76ms
step:2224/2315 train_time:135134ms step_avg:60.76ms
step:2225/2315 train_time:135195ms step_avg:60.76ms
step:2226/2315 train_time:135256ms step_avg:60.76ms
step:2227/2315 train_time:135317ms step_avg:60.76ms
step:2228/2315 train_time:135379ms step_avg:60.76ms
step:2229/2315 train_time:135440ms step_avg:60.76ms
step:2230/2315 train_time:135505ms step_avg:60.76ms
step:2231/2315 train_time:135564ms step_avg:60.76ms
step:2232/2315 train_time:135625ms step_avg:60.76ms
step:2233/2315 train_time:135686ms step_avg:60.76ms
step:2234/2315 train_time:135748ms step_avg:60.76ms
step:2235/2315 train_time:135809ms step_avg:60.76ms
step:2236/2315 train_time:135871ms step_avg:60.77ms
step:2237/2315 train_time:135931ms step_avg:60.76ms
step:2238/2315 train_time:135993ms step_avg:60.77ms
step:2239/2315 train_time:136054ms step_avg:60.77ms
step:2240/2315 train_time:136115ms step_avg:60.77ms
step:2241/2315 train_time:136176ms step_avg:60.77ms
step:2242/2315 train_time:136238ms step_avg:60.77ms
step:2243/2315 train_time:136299ms step_avg:60.77ms
step:2244/2315 train_time:136361ms step_avg:60.77ms
step:2245/2315 train_time:136422ms step_avg:60.77ms
step:2246/2315 train_time:136484ms step_avg:60.77ms
step:2247/2315 train_time:136545ms step_avg:60.77ms
step:2248/2315 train_time:136607ms step_avg:60.77ms
step:2249/2315 train_time:136669ms step_avg:60.77ms
step:2250/2315 train_time:136730ms step_avg:60.77ms
step:2250/2315 val_loss:3.2898 train_time:136793ms step_avg:60.80ms
step:2251/2315 train_time:136811ms step_avg:60.78ms
step:2252/2315 train_time:136855ms step_avg:60.77ms
step:2253/2315 train_time:136919ms step_avg:60.77ms
step:2254/2315 train_time:136981ms step_avg:60.77ms
step:2255/2315 train_time:137042ms step_avg:60.77ms
step:2256/2315 train_time:137103ms step_avg:60.77ms
step:2257/2315 train_time:137163ms step_avg:60.77ms
step:2258/2315 train_time:137225ms step_avg:60.77ms
step:2259/2315 train_time:137284ms step_avg:60.77ms
step:2260/2315 train_time:137346ms step_avg:60.77ms
step:2261/2315 train_time:137407ms step_avg:60.77ms
step:2262/2315 train_time:137467ms step_avg:60.77ms
step:2263/2315 train_time:137527ms step_avg:60.77ms
step:2264/2315 train_time:137588ms step_avg:60.77ms
step:2265/2315 train_time:137649ms step_avg:60.77ms
step:2266/2315 train_time:137711ms step_avg:60.77ms
step:2267/2315 train_time:137774ms step_avg:60.77ms
step:2268/2315 train_time:137838ms step_avg:60.77ms
step:2269/2315 train_time:137900ms step_avg:60.78ms
step:2270/2315 train_time:137962ms step_avg:60.78ms
step:2271/2315 train_time:138023ms step_avg:60.78ms
step:2272/2315 train_time:138085ms step_avg:60.78ms
step:2273/2315 train_time:138145ms step_avg:60.78ms
step:2274/2315 train_time:138206ms step_avg:60.78ms
step:2275/2315 train_time:138266ms step_avg:60.78ms
step:2276/2315 train_time:138331ms step_avg:60.78ms
step:2277/2315 train_time:138389ms step_avg:60.78ms
step:2278/2315 train_time:138451ms step_avg:60.78ms
step:2279/2315 train_time:138511ms step_avg:60.78ms
step:2280/2315 train_time:138572ms step_avg:60.78ms
step:2281/2315 train_time:138633ms step_avg:60.78ms
step:2282/2315 train_time:138695ms step_avg:60.78ms
step:2283/2315 train_time:138757ms step_avg:60.78ms
step:2284/2315 train_time:138819ms step_avg:60.78ms
step:2285/2315 train_time:138881ms step_avg:60.78ms
step:2286/2315 train_time:138943ms step_avg:60.78ms
step:2287/2315 train_time:139004ms step_avg:60.78ms
step:2288/2315 train_time:139066ms step_avg:60.78ms
step:2289/2315 train_time:139127ms step_avg:60.78ms
step:2290/2315 train_time:139189ms step_avg:60.78ms
step:2291/2315 train_time:139250ms step_avg:60.78ms
step:2292/2315 train_time:139311ms step_avg:60.78ms
step:2293/2315 train_time:139372ms step_avg:60.78ms
step:2294/2315 train_time:139434ms step_avg:60.78ms
step:2295/2315 train_time:139494ms step_avg:60.78ms
step:2296/2315 train_time:139556ms step_avg:60.78ms
step:2297/2315 train_time:139617ms step_avg:60.78ms
step:2298/2315 train_time:139678ms step_avg:60.78ms
step:2299/2315 train_time:139740ms step_avg:60.78ms
step:2300/2315 train_time:139802ms step_avg:60.78ms
step:2301/2315 train_time:139863ms step_avg:60.78ms
step:2302/2315 train_time:139925ms step_avg:60.78ms
step:2303/2315 train_time:139986ms step_avg:60.78ms
step:2304/2315 train_time:140049ms step_avg:60.78ms
step:2305/2315 train_time:140109ms step_avg:60.79ms
step:2306/2315 train_time:140171ms step_avg:60.79ms
step:2307/2315 train_time:140232ms step_avg:60.79ms
step:2308/2315 train_time:140294ms step_avg:60.79ms
step:2309/2315 train_time:140355ms step_avg:60.79ms
step:2310/2315 train_time:140416ms step_avg:60.79ms
step:2311/2315 train_time:140477ms step_avg:60.79ms
step:2312/2315 train_time:140539ms step_avg:60.79ms
step:2313/2315 train_time:140600ms step_avg:60.79ms
step:2314/2315 train_time:140661ms step_avg:60.79ms
step:2315/2315 train_time:140722ms step_avg:60.79ms
step:2315/2315 val_loss:3.2774 train_time:140784ms step_avg:60.81ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

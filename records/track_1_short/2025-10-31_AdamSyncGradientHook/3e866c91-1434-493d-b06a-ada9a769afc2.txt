import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)


            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))



            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal
        self.group_to_param_group = {}

        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                self.group_to_param_group[param] = group

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        world_size = dist.get_world_size()
        grad = param.grad
        rank_size = grad.shape[0] // world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(), grad_slice)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']

            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
        # enable sync in the next training step for the adam optimizer
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = next(train_loader)

        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()


====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251031+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 31 16:26:47 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   34C    P0            123W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   31C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   30C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   33C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2315 train_time:77ms step_avg:76.54ms
step:2/2315 train_time:186ms step_avg:92.85ms
step:3/2315 train_time:205ms step_avg:68.45ms
step:4/2315 train_time:244ms step_avg:60.98ms
step:5/2315 train_time:301ms step_avg:60.27ms
step:6/2315 train_time:361ms step_avg:60.23ms
step:7/2315 train_time:420ms step_avg:60.03ms
step:8/2315 train_time:480ms step_avg:60.01ms
step:9/2315 train_time:539ms step_avg:59.93ms
step:10/2315 train_time:599ms step_avg:59.95ms
step:11/2315 train_time:658ms step_avg:59.86ms
step:12/2315 train_time:718ms step_avg:59.85ms
step:13/2315 train_time:777ms step_avg:59.79ms
step:14/2315 train_time:837ms step_avg:59.80ms
step:15/2315 train_time:897ms step_avg:59.78ms
step:16/2315 train_time:956ms step_avg:59.77ms
step:17/2315 train_time:1016ms step_avg:59.77ms
step:18/2315 train_time:1078ms step_avg:59.91ms
step:19/2315 train_time:1143ms step_avg:60.18ms
step:20/2315 train_time:1206ms step_avg:60.31ms
step:21/2315 train_time:1267ms step_avg:60.31ms
step:22/2315 train_time:1327ms step_avg:60.31ms
step:23/2315 train_time:1387ms step_avg:60.29ms
step:24/2315 train_time:1447ms step_avg:60.30ms
step:25/2315 train_time:1507ms step_avg:60.30ms
step:26/2315 train_time:1568ms step_avg:60.30ms
step:27/2315 train_time:1628ms step_avg:60.29ms
step:28/2315 train_time:1688ms step_avg:60.30ms
step:29/2315 train_time:1748ms step_avg:60.28ms
step:30/2315 train_time:1809ms step_avg:60.29ms
step:31/2315 train_time:1869ms step_avg:60.29ms
step:32/2315 train_time:1929ms step_avg:60.29ms
step:33/2315 train_time:1989ms step_avg:60.28ms
step:34/2315 train_time:2050ms step_avg:60.30ms
step:35/2315 train_time:2112ms step_avg:60.33ms
step:36/2315 train_time:2174ms step_avg:60.38ms
step:37/2315 train_time:2234ms step_avg:60.38ms
step:38/2315 train_time:2295ms step_avg:60.41ms
step:39/2315 train_time:2356ms step_avg:60.40ms
step:40/2315 train_time:2416ms step_avg:60.40ms
step:41/2315 train_time:2477ms step_avg:60.41ms
step:42/2315 train_time:2537ms step_avg:60.41ms
step:43/2315 train_time:2598ms step_avg:60.41ms
step:44/2315 train_time:2658ms step_avg:60.41ms
step:45/2315 train_time:2718ms step_avg:60.39ms
step:46/2315 train_time:2778ms step_avg:60.39ms
step:47/2315 train_time:2838ms step_avg:60.39ms
step:48/2315 train_time:2899ms step_avg:60.39ms
step:49/2315 train_time:2959ms step_avg:60.39ms
step:50/2315 train_time:3020ms step_avg:60.40ms
step:51/2315 train_time:3080ms step_avg:60.38ms
step:52/2315 train_time:3140ms step_avg:60.39ms
step:53/2315 train_time:3200ms step_avg:60.38ms
step:54/2315 train_time:3261ms step_avg:60.38ms
step:55/2315 train_time:3320ms step_avg:60.37ms
step:56/2315 train_time:3381ms step_avg:60.37ms
step:57/2315 train_time:3441ms step_avg:60.36ms
step:58/2315 train_time:3501ms step_avg:60.36ms
step:59/2315 train_time:3561ms step_avg:60.35ms
step:60/2315 train_time:3621ms step_avg:60.35ms
step:61/2315 train_time:3680ms step_avg:60.33ms
step:62/2315 train_time:3740ms step_avg:60.33ms
step:63/2315 train_time:3800ms step_avg:60.32ms
step:64/2315 train_time:3860ms step_avg:60.32ms
step:65/2315 train_time:3920ms step_avg:60.30ms
step:66/2315 train_time:3981ms step_avg:60.31ms
step:67/2315 train_time:4041ms step_avg:60.31ms
step:68/2315 train_time:4101ms step_avg:60.31ms
step:69/2315 train_time:4161ms step_avg:60.30ms
step:70/2315 train_time:4221ms step_avg:60.30ms
step:71/2315 train_time:4281ms step_avg:60.29ms
step:72/2315 train_time:4341ms step_avg:60.29ms
step:73/2315 train_time:4400ms step_avg:60.28ms
step:74/2315 train_time:4461ms step_avg:60.28ms
step:75/2315 train_time:4520ms step_avg:60.27ms
step:76/2315 train_time:4581ms step_avg:60.27ms
step:77/2315 train_time:4640ms step_avg:60.26ms
step:78/2315 train_time:4700ms step_avg:60.26ms
step:79/2315 train_time:4760ms step_avg:60.25ms
step:80/2315 train_time:4820ms step_avg:60.25ms
step:81/2315 train_time:4879ms step_avg:60.24ms
step:82/2315 train_time:4939ms step_avg:60.24ms
step:83/2315 train_time:4999ms step_avg:60.23ms
step:84/2315 train_time:5061ms step_avg:60.25ms
step:85/2315 train_time:5120ms step_avg:60.23ms
step:86/2315 train_time:5180ms step_avg:60.23ms
step:87/2315 train_time:5240ms step_avg:60.23ms
step:88/2315 train_time:5300ms step_avg:60.23ms
step:89/2315 train_time:5360ms step_avg:60.22ms
step:90/2315 train_time:5420ms step_avg:60.22ms
step:91/2315 train_time:5480ms step_avg:60.22ms
step:92/2315 train_time:5540ms step_avg:60.22ms
step:93/2315 train_time:5599ms step_avg:60.21ms
step:94/2315 train_time:5660ms step_avg:60.21ms
step:95/2315 train_time:5719ms step_avg:60.20ms
step:96/2315 train_time:5780ms step_avg:60.20ms
step:97/2315 train_time:5839ms step_avg:60.19ms
step:98/2315 train_time:5899ms step_avg:60.20ms
step:99/2315 train_time:5959ms step_avg:60.19ms
step:100/2315 train_time:6019ms step_avg:60.19ms
step:101/2315 train_time:6079ms step_avg:60.18ms
step:102/2315 train_time:6139ms step_avg:60.19ms
step:103/2315 train_time:6199ms step_avg:60.18ms
step:104/2315 train_time:6259ms step_avg:60.18ms
step:105/2315 train_time:6318ms step_avg:60.17ms
step:106/2315 train_time:6378ms step_avg:60.17ms
step:107/2315 train_time:6438ms step_avg:60.17ms
step:108/2315 train_time:6499ms step_avg:60.17ms
step:109/2315 train_time:6558ms step_avg:60.17ms
step:110/2315 train_time:6618ms step_avg:60.17ms
step:111/2315 train_time:6678ms step_avg:60.17ms
step:112/2315 train_time:6739ms step_avg:60.17ms
step:113/2315 train_time:6799ms step_avg:60.17ms
step:114/2315 train_time:6860ms step_avg:60.18ms
step:115/2315 train_time:6919ms step_avg:60.17ms
step:116/2315 train_time:6979ms step_avg:60.17ms
step:117/2315 train_time:7039ms step_avg:60.16ms
step:118/2315 train_time:7098ms step_avg:60.16ms
step:119/2315 train_time:7158ms step_avg:60.15ms
step:120/2315 train_time:7219ms step_avg:60.16ms
step:121/2315 train_time:7278ms step_avg:60.15ms
step:122/2315 train_time:7338ms step_avg:60.15ms
step:123/2315 train_time:7398ms step_avg:60.15ms
step:124/2315 train_time:7458ms step_avg:60.15ms
step:125/2315 train_time:7518ms step_avg:60.15ms
step:126/2315 train_time:7578ms step_avg:60.14ms
step:127/2315 train_time:7638ms step_avg:60.14ms
step:128/2315 train_time:7699ms step_avg:60.15ms
step:129/2315 train_time:7758ms step_avg:60.14ms
step:130/2315 train_time:7818ms step_avg:60.14ms
step:131/2315 train_time:7878ms step_avg:60.14ms
step:132/2315 train_time:7939ms step_avg:60.14ms
step:133/2315 train_time:7998ms step_avg:60.14ms
step:134/2315 train_time:8058ms step_avg:60.13ms
step:135/2315 train_time:8117ms step_avg:60.13ms
step:136/2315 train_time:8178ms step_avg:60.13ms
step:137/2315 train_time:8237ms step_avg:60.12ms
step:138/2315 train_time:8297ms step_avg:60.12ms
step:139/2315 train_time:8357ms step_avg:60.12ms
step:140/2315 train_time:8417ms step_avg:60.12ms
step:141/2315 train_time:8476ms step_avg:60.12ms
step:142/2315 train_time:8538ms step_avg:60.12ms
step:143/2315 train_time:8597ms step_avg:60.12ms
step:144/2315 train_time:8658ms step_avg:60.12ms
step:145/2315 train_time:8717ms step_avg:60.12ms
step:146/2315 train_time:8777ms step_avg:60.12ms
step:147/2315 train_time:8837ms step_avg:60.12ms
step:148/2315 train_time:8898ms step_avg:60.12ms
step:149/2315 train_time:8958ms step_avg:60.12ms
step:150/2315 train_time:9018ms step_avg:60.12ms
step:151/2315 train_time:9077ms step_avg:60.11ms
step:152/2315 train_time:9137ms step_avg:60.11ms
step:153/2315 train_time:9197ms step_avg:60.11ms
step:154/2315 train_time:9257ms step_avg:60.11ms
step:155/2315 train_time:9316ms step_avg:60.11ms
step:156/2315 train_time:9377ms step_avg:60.11ms
step:157/2315 train_time:9436ms step_avg:60.10ms
step:158/2315 train_time:9497ms step_avg:60.11ms
step:159/2315 train_time:9557ms step_avg:60.11ms
step:160/2315 train_time:9617ms step_avg:60.11ms
step:161/2315 train_time:9677ms step_avg:60.11ms
step:162/2315 train_time:9737ms step_avg:60.10ms
step:163/2315 train_time:9797ms step_avg:60.10ms
step:164/2315 train_time:9858ms step_avg:60.11ms
step:165/2315 train_time:9917ms step_avg:60.10ms
step:166/2315 train_time:9978ms step_avg:60.11ms
step:167/2315 train_time:10037ms step_avg:60.10ms
step:168/2315 train_time:10098ms step_avg:60.10ms
step:169/2315 train_time:10157ms step_avg:60.10ms
step:170/2315 train_time:10217ms step_avg:60.10ms
step:171/2315 train_time:10277ms step_avg:60.10ms
step:172/2315 train_time:10337ms step_avg:60.10ms
step:173/2315 train_time:10396ms step_avg:60.09ms
step:174/2315 train_time:10457ms step_avg:60.10ms
step:175/2315 train_time:10516ms step_avg:60.09ms
step:176/2315 train_time:10577ms step_avg:60.09ms
step:177/2315 train_time:10636ms step_avg:60.09ms
step:178/2315 train_time:10697ms step_avg:60.09ms
step:179/2315 train_time:10756ms step_avg:60.09ms
step:180/2315 train_time:10816ms step_avg:60.09ms
step:181/2315 train_time:10876ms step_avg:60.09ms
step:182/2315 train_time:10936ms step_avg:60.09ms
step:183/2315 train_time:10996ms step_avg:60.09ms
step:184/2315 train_time:11056ms step_avg:60.09ms
step:185/2315 train_time:11116ms step_avg:60.09ms
step:186/2315 train_time:11177ms step_avg:60.09ms
step:187/2315 train_time:11236ms step_avg:60.09ms
step:188/2315 train_time:11297ms step_avg:60.09ms
step:189/2315 train_time:11356ms step_avg:60.09ms
step:190/2315 train_time:11417ms step_avg:60.09ms
step:191/2315 train_time:11476ms step_avg:60.08ms
step:192/2315 train_time:11536ms step_avg:60.08ms
step:193/2315 train_time:11595ms step_avg:60.08ms
step:194/2315 train_time:11655ms step_avg:60.08ms
step:195/2315 train_time:11715ms step_avg:60.08ms
step:196/2315 train_time:11775ms step_avg:60.08ms
step:197/2315 train_time:11835ms step_avg:60.07ms
step:198/2315 train_time:11895ms step_avg:60.08ms
step:199/2315 train_time:11955ms step_avg:60.08ms
step:200/2315 train_time:12015ms step_avg:60.08ms
step:201/2315 train_time:12075ms step_avg:60.08ms
step:202/2315 train_time:12136ms step_avg:60.08ms
step:203/2315 train_time:12195ms step_avg:60.08ms
step:204/2315 train_time:12255ms step_avg:60.07ms
step:205/2315 train_time:12315ms step_avg:60.07ms
step:206/2315 train_time:12375ms step_avg:60.08ms
step:207/2315 train_time:12435ms step_avg:60.07ms
step:208/2315 train_time:12495ms step_avg:60.07ms
step:209/2315 train_time:12555ms step_avg:60.07ms
step:210/2315 train_time:12615ms step_avg:60.07ms
step:211/2315 train_time:12676ms step_avg:60.07ms
step:212/2315 train_time:12736ms step_avg:60.08ms
step:213/2315 train_time:12796ms step_avg:60.07ms
step:214/2315 train_time:12857ms step_avg:60.08ms
step:215/2315 train_time:12917ms step_avg:60.08ms
step:216/2315 train_time:12977ms step_avg:60.08ms
step:217/2315 train_time:13036ms step_avg:60.07ms
step:218/2315 train_time:13097ms step_avg:60.08ms
step:219/2315 train_time:13156ms step_avg:60.07ms
step:220/2315 train_time:13216ms step_avg:60.07ms
step:221/2315 train_time:13276ms step_avg:60.07ms
step:222/2315 train_time:13336ms step_avg:60.07ms
step:223/2315 train_time:13396ms step_avg:60.07ms
step:224/2315 train_time:13456ms step_avg:60.07ms
step:225/2315 train_time:13516ms step_avg:60.07ms
step:226/2315 train_time:13576ms step_avg:60.07ms
step:227/2315 train_time:13635ms step_avg:60.07ms
step:228/2315 train_time:13696ms step_avg:60.07ms
step:229/2315 train_time:13755ms step_avg:60.07ms
step:230/2315 train_time:13816ms step_avg:60.07ms
step:231/2315 train_time:13876ms step_avg:60.07ms
step:232/2315 train_time:13937ms step_avg:60.07ms
step:233/2315 train_time:13996ms step_avg:60.07ms
step:234/2315 train_time:14057ms step_avg:60.07ms
step:235/2315 train_time:14116ms step_avg:60.07ms
step:236/2315 train_time:14176ms step_avg:60.07ms
step:237/2315 train_time:14236ms step_avg:60.07ms
step:238/2315 train_time:14296ms step_avg:60.07ms
step:239/2315 train_time:14356ms step_avg:60.07ms
step:240/2315 train_time:14416ms step_avg:60.07ms
step:241/2315 train_time:14476ms step_avg:60.07ms
step:242/2315 train_time:14536ms step_avg:60.07ms
step:243/2315 train_time:14596ms step_avg:60.07ms
step:244/2315 train_time:14656ms step_avg:60.06ms
step:245/2315 train_time:14715ms step_avg:60.06ms
step:246/2315 train_time:14776ms step_avg:60.06ms
step:247/2315 train_time:14835ms step_avg:60.06ms
step:248/2315 train_time:14896ms step_avg:60.07ms
step:249/2315 train_time:14956ms step_avg:60.06ms
step:250/2315 train_time:15017ms step_avg:60.07ms
step:250/2315 val_loss:4.0827 train_time:15078ms step_avg:60.31ms
step:251/2315 train_time:15099ms step_avg:60.15ms
step:252/2315 train_time:15139ms step_avg:60.08ms
step:253/2315 train_time:15205ms step_avg:60.10ms
step:254/2315 train_time:15268ms step_avg:60.11ms
step:255/2315 train_time:15328ms step_avg:60.11ms
step:256/2315 train_time:15389ms step_avg:60.11ms
step:257/2315 train_time:15448ms step_avg:60.11ms
step:258/2315 train_time:15509ms step_avg:60.11ms
step:259/2315 train_time:15567ms step_avg:60.11ms
step:260/2315 train_time:15627ms step_avg:60.10ms
step:261/2315 train_time:15687ms step_avg:60.10ms
step:262/2315 train_time:15746ms step_avg:60.10ms
step:263/2315 train_time:15806ms step_avg:60.10ms
step:264/2315 train_time:15866ms step_avg:60.10ms
step:265/2315 train_time:15924ms step_avg:60.09ms
step:266/2315 train_time:15985ms step_avg:60.09ms
step:267/2315 train_time:16044ms step_avg:60.09ms
step:268/2315 train_time:16106ms step_avg:60.10ms
step:269/2315 train_time:16168ms step_avg:60.10ms
step:270/2315 train_time:16229ms step_avg:60.11ms
step:271/2315 train_time:16291ms step_avg:60.11ms
step:272/2315 train_time:16351ms step_avg:60.11ms
step:273/2315 train_time:16411ms step_avg:60.11ms
step:274/2315 train_time:16471ms step_avg:60.11ms
step:275/2315 train_time:16531ms step_avg:60.11ms
step:276/2315 train_time:16591ms step_avg:60.11ms
step:277/2315 train_time:16649ms step_avg:60.11ms
step:278/2315 train_time:16709ms step_avg:60.10ms
step:279/2315 train_time:16767ms step_avg:60.10ms
step:280/2315 train_time:16827ms step_avg:60.10ms
step:281/2315 train_time:16885ms step_avg:60.09ms
step:282/2315 train_time:16945ms step_avg:60.09ms
step:283/2315 train_time:17005ms step_avg:60.09ms
step:284/2315 train_time:17065ms step_avg:60.09ms
step:285/2315 train_time:17125ms step_avg:60.09ms
step:286/2315 train_time:17187ms step_avg:60.09ms
step:287/2315 train_time:17248ms step_avg:60.10ms
step:288/2315 train_time:17309ms step_avg:60.10ms
step:289/2315 train_time:17369ms step_avg:60.10ms
step:290/2315 train_time:17430ms step_avg:60.10ms
step:291/2315 train_time:17489ms step_avg:60.10ms
step:292/2315 train_time:17550ms step_avg:60.10ms
step:293/2315 train_time:17609ms step_avg:60.10ms
step:294/2315 train_time:17669ms step_avg:60.10ms
step:295/2315 train_time:17728ms step_avg:60.09ms
step:296/2315 train_time:17787ms step_avg:60.09ms
step:297/2315 train_time:17846ms step_avg:60.09ms
step:298/2315 train_time:17906ms step_avg:60.09ms
step:299/2315 train_time:17966ms step_avg:60.09ms
step:300/2315 train_time:18025ms step_avg:60.08ms
step:301/2315 train_time:18085ms step_avg:60.08ms
step:302/2315 train_time:18147ms step_avg:60.09ms
step:303/2315 train_time:18208ms step_avg:60.09ms
step:304/2315 train_time:18269ms step_avg:60.09ms
step:305/2315 train_time:18329ms step_avg:60.10ms
step:306/2315 train_time:18390ms step_avg:60.10ms
step:307/2315 train_time:18450ms step_avg:60.10ms
step:308/2315 train_time:18511ms step_avg:60.10ms
step:309/2315 train_time:18570ms step_avg:60.10ms
step:310/2315 train_time:18631ms step_avg:60.10ms
step:311/2315 train_time:18690ms step_avg:60.10ms
step:312/2315 train_time:18750ms step_avg:60.10ms
step:313/2315 train_time:18809ms step_avg:60.09ms
step:314/2315 train_time:18869ms step_avg:60.09ms
step:315/2315 train_time:18928ms step_avg:60.09ms
step:316/2315 train_time:18988ms step_avg:60.09ms
step:317/2315 train_time:19048ms step_avg:60.09ms
step:318/2315 train_time:19108ms step_avg:60.09ms
step:319/2315 train_time:19168ms step_avg:60.09ms
step:320/2315 train_time:19229ms step_avg:60.09ms
step:321/2315 train_time:19288ms step_avg:60.09ms
step:322/2315 train_time:19348ms step_avg:60.09ms
step:323/2315 train_time:19409ms step_avg:60.09ms
step:324/2315 train_time:19469ms step_avg:60.09ms
step:325/2315 train_time:19529ms step_avg:60.09ms
step:326/2315 train_time:19589ms step_avg:60.09ms
step:327/2315 train_time:19648ms step_avg:60.09ms
step:328/2315 train_time:19708ms step_avg:60.09ms
step:329/2315 train_time:19768ms step_avg:60.08ms
step:330/2315 train_time:19828ms step_avg:60.08ms
step:331/2315 train_time:19887ms step_avg:60.08ms
step:332/2315 train_time:19947ms step_avg:60.08ms
step:333/2315 train_time:20007ms step_avg:60.08ms
step:334/2315 train_time:20067ms step_avg:60.08ms
step:335/2315 train_time:20127ms step_avg:60.08ms
step:336/2315 train_time:20187ms step_avg:60.08ms
step:337/2315 train_time:20247ms step_avg:60.08ms
step:338/2315 train_time:20308ms step_avg:60.08ms
step:339/2315 train_time:20368ms step_avg:60.08ms
step:340/2315 train_time:20429ms step_avg:60.09ms
step:341/2315 train_time:20489ms step_avg:60.08ms
step:342/2315 train_time:20549ms step_avg:60.09ms
step:343/2315 train_time:20609ms step_avg:60.08ms
step:344/2315 train_time:20669ms step_avg:60.08ms
step:345/2315 train_time:20728ms step_avg:60.08ms
step:346/2315 train_time:20788ms step_avg:60.08ms
step:347/2315 train_time:20848ms step_avg:60.08ms
step:348/2315 train_time:20908ms step_avg:60.08ms
step:349/2315 train_time:20967ms step_avg:60.08ms
step:350/2315 train_time:21027ms step_avg:60.08ms
step:351/2315 train_time:21087ms step_avg:60.08ms
step:352/2315 train_time:21147ms step_avg:60.08ms
step:353/2315 train_time:21208ms step_avg:60.08ms
step:354/2315 train_time:21268ms step_avg:60.08ms
step:355/2315 train_time:21328ms step_avg:60.08ms
step:356/2315 train_time:21388ms step_avg:60.08ms
step:357/2315 train_time:21449ms step_avg:60.08ms
step:358/2315 train_time:21509ms step_avg:60.08ms
step:359/2315 train_time:21568ms step_avg:60.08ms
step:360/2315 train_time:21629ms step_avg:60.08ms
step:361/2315 train_time:21688ms step_avg:60.08ms
step:362/2315 train_time:21748ms step_avg:60.08ms
step:363/2315 train_time:21808ms step_avg:60.08ms
step:364/2315 train_time:21868ms step_avg:60.08ms
step:365/2315 train_time:21927ms step_avg:60.07ms
step:366/2315 train_time:21987ms step_avg:60.07ms
step:367/2315 train_time:22047ms step_avg:60.07ms
step:368/2315 train_time:22108ms step_avg:60.08ms
step:369/2315 train_time:22168ms step_avg:60.07ms
step:370/2315 train_time:22228ms step_avg:60.07ms
step:371/2315 train_time:22287ms step_avg:60.07ms
step:372/2315 train_time:22348ms step_avg:60.07ms
step:373/2315 train_time:22408ms step_avg:60.07ms
step:374/2315 train_time:22468ms step_avg:60.07ms
step:375/2315 train_time:22528ms step_avg:60.07ms
step:376/2315 train_time:22588ms step_avg:60.07ms
step:377/2315 train_time:22648ms step_avg:60.07ms
step:378/2315 train_time:22708ms step_avg:60.07ms
step:379/2315 train_time:22767ms step_avg:60.07ms
step:380/2315 train_time:22827ms step_avg:60.07ms
step:381/2315 train_time:22887ms step_avg:60.07ms
step:382/2315 train_time:22948ms step_avg:60.07ms
step:383/2315 train_time:23007ms step_avg:60.07ms
step:384/2315 train_time:23067ms step_avg:60.07ms
step:385/2315 train_time:23127ms step_avg:60.07ms
step:386/2315 train_time:23187ms step_avg:60.07ms
step:387/2315 train_time:23247ms step_avg:60.07ms
step:388/2315 train_time:23308ms step_avg:60.07ms
step:389/2315 train_time:23368ms step_avg:60.07ms
step:390/2315 train_time:23428ms step_avg:60.07ms
step:391/2315 train_time:23488ms step_avg:60.07ms
step:392/2315 train_time:23548ms step_avg:60.07ms
step:393/2315 train_time:23609ms step_avg:60.07ms
step:394/2315 train_time:23669ms step_avg:60.07ms
step:395/2315 train_time:23728ms step_avg:60.07ms
step:396/2315 train_time:23789ms step_avg:60.07ms
step:397/2315 train_time:23848ms step_avg:60.07ms
step:398/2315 train_time:23908ms step_avg:60.07ms
step:399/2315 train_time:23967ms step_avg:60.07ms
step:400/2315 train_time:24027ms step_avg:60.07ms
step:401/2315 train_time:24087ms step_avg:60.07ms
step:402/2315 train_time:24147ms step_avg:60.07ms
step:403/2315 train_time:24207ms step_avg:60.07ms
step:404/2315 train_time:24268ms step_avg:60.07ms
step:405/2315 train_time:24327ms step_avg:60.07ms
step:406/2315 train_time:24387ms step_avg:60.07ms
step:407/2315 train_time:24447ms step_avg:60.07ms
step:408/2315 train_time:24508ms step_avg:60.07ms
step:409/2315 train_time:24568ms step_avg:60.07ms
step:410/2315 train_time:24628ms step_avg:60.07ms
step:411/2315 train_time:24688ms step_avg:60.07ms
step:412/2315 train_time:24748ms step_avg:60.07ms
step:413/2315 train_time:24808ms step_avg:60.07ms
step:414/2315 train_time:24868ms step_avg:60.07ms
step:415/2315 train_time:24928ms step_avg:60.07ms
step:416/2315 train_time:24987ms step_avg:60.07ms
step:417/2315 train_time:25047ms step_avg:60.07ms
step:418/2315 train_time:25108ms step_avg:60.07ms
step:419/2315 train_time:25167ms step_avg:60.06ms
step:420/2315 train_time:25227ms step_avg:60.06ms
step:421/2315 train_time:25286ms step_avg:60.06ms
step:422/2315 train_time:25348ms step_avg:60.07ms
step:423/2315 train_time:25408ms step_avg:60.07ms
step:424/2315 train_time:25468ms step_avg:60.07ms
step:425/2315 train_time:25528ms step_avg:60.07ms
step:426/2315 train_time:25589ms step_avg:60.07ms
step:427/2315 train_time:25648ms step_avg:60.07ms
step:428/2315 train_time:25708ms step_avg:60.07ms
step:429/2315 train_time:25768ms step_avg:60.07ms
step:430/2315 train_time:25828ms step_avg:60.07ms
step:431/2315 train_time:25887ms step_avg:60.06ms
step:432/2315 train_time:25947ms step_avg:60.06ms
step:433/2315 train_time:26007ms step_avg:60.06ms
step:434/2315 train_time:26067ms step_avg:60.06ms
step:435/2315 train_time:26126ms step_avg:60.06ms
step:436/2315 train_time:26187ms step_avg:60.06ms
step:437/2315 train_time:26246ms step_avg:60.06ms
step:438/2315 train_time:26307ms step_avg:60.06ms
step:439/2315 train_time:26367ms step_avg:60.06ms
step:440/2315 train_time:26427ms step_avg:60.06ms
step:441/2315 train_time:26487ms step_avg:60.06ms
step:442/2315 train_time:26548ms step_avg:60.06ms
step:443/2315 train_time:26608ms step_avg:60.06ms
step:444/2315 train_time:26668ms step_avg:60.06ms
step:445/2315 train_time:26728ms step_avg:60.06ms
step:446/2315 train_time:26788ms step_avg:60.06ms
step:447/2315 train_time:26848ms step_avg:60.06ms
step:448/2315 train_time:26908ms step_avg:60.06ms
step:449/2315 train_time:26967ms step_avg:60.06ms
step:450/2315 train_time:27028ms step_avg:60.06ms
step:451/2315 train_time:27087ms step_avg:60.06ms
step:452/2315 train_time:27147ms step_avg:60.06ms
step:453/2315 train_time:27207ms step_avg:60.06ms
step:454/2315 train_time:27267ms step_avg:60.06ms
step:455/2315 train_time:27328ms step_avg:60.06ms
step:456/2315 train_time:27388ms step_avg:60.06ms
step:457/2315 train_time:27448ms step_avg:60.06ms
step:458/2315 train_time:27508ms step_avg:60.06ms
step:459/2315 train_time:27568ms step_avg:60.06ms
step:460/2315 train_time:27628ms step_avg:60.06ms
step:461/2315 train_time:27688ms step_avg:60.06ms
step:462/2315 train_time:27748ms step_avg:60.06ms
step:463/2315 train_time:27808ms step_avg:60.06ms
step:464/2315 train_time:27868ms step_avg:60.06ms
step:465/2315 train_time:27928ms step_avg:60.06ms
step:466/2315 train_time:27988ms step_avg:60.06ms
step:467/2315 train_time:28048ms step_avg:60.06ms
step:468/2315 train_time:28108ms step_avg:60.06ms
step:469/2315 train_time:28168ms step_avg:60.06ms
step:470/2315 train_time:28228ms step_avg:60.06ms
step:471/2315 train_time:28287ms step_avg:60.06ms
step:472/2315 train_time:28347ms step_avg:60.06ms
step:473/2315 train_time:28407ms step_avg:60.06ms
step:474/2315 train_time:28467ms step_avg:60.06ms
step:475/2315 train_time:28527ms step_avg:60.06ms
step:476/2315 train_time:28588ms step_avg:60.06ms
step:477/2315 train_time:28648ms step_avg:60.06ms
step:478/2315 train_time:28708ms step_avg:60.06ms
step:479/2315 train_time:28768ms step_avg:60.06ms
step:480/2315 train_time:28828ms step_avg:60.06ms
step:481/2315 train_time:28887ms step_avg:60.06ms
step:482/2315 train_time:28947ms step_avg:60.06ms
step:483/2315 train_time:29007ms step_avg:60.06ms
step:484/2315 train_time:29067ms step_avg:60.06ms
step:485/2315 train_time:29126ms step_avg:60.05ms
step:486/2315 train_time:29186ms step_avg:60.05ms
step:487/2315 train_time:29246ms step_avg:60.05ms
step:488/2315 train_time:29307ms step_avg:60.06ms
step:489/2315 train_time:29367ms step_avg:60.06ms
step:490/2315 train_time:29427ms step_avg:60.06ms
step:491/2315 train_time:29487ms step_avg:60.05ms
step:492/2315 train_time:29547ms step_avg:60.06ms
step:493/2315 train_time:29608ms step_avg:60.06ms
step:494/2315 train_time:29668ms step_avg:60.06ms
step:495/2315 train_time:29728ms step_avg:60.06ms
step:496/2315 train_time:29787ms step_avg:60.06ms
step:497/2315 train_time:29847ms step_avg:60.05ms
step:498/2315 train_time:29908ms step_avg:60.06ms
step:499/2315 train_time:29968ms step_avg:60.06ms
step:500/2315 train_time:30028ms step_avg:60.06ms
step:500/2315 val_loss:3.8132 train_time:30090ms step_avg:60.18ms
step:501/2315 train_time:30108ms step_avg:60.10ms
step:502/2315 train_time:30152ms step_avg:60.06ms
step:503/2315 train_time:30213ms step_avg:60.07ms
step:504/2315 train_time:30275ms step_avg:60.07ms
step:505/2315 train_time:30334ms step_avg:60.07ms
step:506/2315 train_time:30395ms step_avg:60.07ms
step:507/2315 train_time:30454ms step_avg:60.07ms
step:508/2315 train_time:30514ms step_avg:60.07ms
step:509/2315 train_time:30573ms step_avg:60.06ms
step:510/2315 train_time:30632ms step_avg:60.06ms
step:511/2315 train_time:30692ms step_avg:60.06ms
step:512/2315 train_time:30752ms step_avg:60.06ms
step:513/2315 train_time:30811ms step_avg:60.06ms
step:514/2315 train_time:30871ms step_avg:60.06ms
step:515/2315 train_time:30930ms step_avg:60.06ms
step:516/2315 train_time:30990ms step_avg:60.06ms
step:517/2315 train_time:31051ms step_avg:60.06ms
step:518/2315 train_time:31113ms step_avg:60.06ms
step:519/2315 train_time:31173ms step_avg:60.06ms
step:520/2315 train_time:31234ms step_avg:60.07ms
step:521/2315 train_time:31295ms step_avg:60.07ms
step:522/2315 train_time:31356ms step_avg:60.07ms
step:523/2315 train_time:31415ms step_avg:60.07ms
step:524/2315 train_time:31475ms step_avg:60.07ms
step:525/2315 train_time:31534ms step_avg:60.06ms
step:526/2315 train_time:31593ms step_avg:60.06ms
step:527/2315 train_time:31653ms step_avg:60.06ms
step:528/2315 train_time:31712ms step_avg:60.06ms
step:529/2315 train_time:31772ms step_avg:60.06ms
step:530/2315 train_time:31832ms step_avg:60.06ms
step:531/2315 train_time:31891ms step_avg:60.06ms
step:532/2315 train_time:31951ms step_avg:60.06ms
step:533/2315 train_time:32011ms step_avg:60.06ms
step:534/2315 train_time:32072ms step_avg:60.06ms
step:535/2315 train_time:32133ms step_avg:60.06ms
step:536/2315 train_time:32194ms step_avg:60.06ms
step:537/2315 train_time:32255ms step_avg:60.06ms
step:538/2315 train_time:32316ms step_avg:60.07ms
step:539/2315 train_time:32375ms step_avg:60.07ms
step:540/2315 train_time:32436ms step_avg:60.07ms
step:541/2315 train_time:32495ms step_avg:60.06ms
step:542/2315 train_time:32555ms step_avg:60.06ms
step:543/2315 train_time:32614ms step_avg:60.06ms
step:544/2315 train_time:32674ms step_avg:60.06ms
step:545/2315 train_time:32733ms step_avg:60.06ms
step:546/2315 train_time:32793ms step_avg:60.06ms
step:547/2315 train_time:32853ms step_avg:60.06ms
step:548/2315 train_time:32913ms step_avg:60.06ms
step:549/2315 train_time:32972ms step_avg:60.06ms
step:550/2315 train_time:33032ms step_avg:60.06ms
step:551/2315 train_time:33092ms step_avg:60.06ms
step:552/2315 train_time:33154ms step_avg:60.06ms
step:553/2315 train_time:33213ms step_avg:60.06ms
step:554/2315 train_time:33274ms step_avg:60.06ms
step:555/2315 train_time:33334ms step_avg:60.06ms
step:556/2315 train_time:33394ms step_avg:60.06ms
step:557/2315 train_time:33454ms step_avg:60.06ms
step:558/2315 train_time:33514ms step_avg:60.06ms
step:559/2315 train_time:33574ms step_avg:60.06ms
step:560/2315 train_time:33634ms step_avg:60.06ms
step:561/2315 train_time:33693ms step_avg:60.06ms
step:562/2315 train_time:33753ms step_avg:60.06ms
step:563/2315 train_time:33812ms step_avg:60.06ms
step:564/2315 train_time:33872ms step_avg:60.06ms
step:565/2315 train_time:33931ms step_avg:60.06ms
step:566/2315 train_time:33992ms step_avg:60.06ms
step:567/2315 train_time:34052ms step_avg:60.06ms
step:568/2315 train_time:34113ms step_avg:60.06ms
step:569/2315 train_time:34173ms step_avg:60.06ms
step:570/2315 train_time:34234ms step_avg:60.06ms
step:571/2315 train_time:34294ms step_avg:60.06ms
step:572/2315 train_time:34355ms step_avg:60.06ms
step:573/2315 train_time:34414ms step_avg:60.06ms
step:574/2315 train_time:34474ms step_avg:60.06ms
step:575/2315 train_time:34533ms step_avg:60.06ms
step:576/2315 train_time:34593ms step_avg:60.06ms
step:577/2315 train_time:34653ms step_avg:60.06ms
step:578/2315 train_time:34713ms step_avg:60.06ms
step:579/2315 train_time:34772ms step_avg:60.06ms
step:580/2315 train_time:34832ms step_avg:60.06ms
step:581/2315 train_time:34892ms step_avg:60.05ms
step:582/2315 train_time:34953ms step_avg:60.06ms
step:583/2315 train_time:35012ms step_avg:60.06ms
step:584/2315 train_time:35073ms step_avg:60.06ms
step:585/2315 train_time:35132ms step_avg:60.06ms
step:586/2315 train_time:35194ms step_avg:60.06ms
step:587/2315 train_time:35254ms step_avg:60.06ms
step:588/2315 train_time:35315ms step_avg:60.06ms
step:589/2315 train_time:35374ms step_avg:60.06ms
step:590/2315 train_time:35434ms step_avg:60.06ms
step:591/2315 train_time:35494ms step_avg:60.06ms
step:592/2315 train_time:35554ms step_avg:60.06ms
step:593/2315 train_time:35614ms step_avg:60.06ms
step:594/2315 train_time:35675ms step_avg:60.06ms
step:595/2315 train_time:35734ms step_avg:60.06ms
step:596/2315 train_time:35794ms step_avg:60.06ms
step:597/2315 train_time:35853ms step_avg:60.06ms
step:598/2315 train_time:35913ms step_avg:60.06ms
step:599/2315 train_time:35973ms step_avg:60.06ms
step:600/2315 train_time:36033ms step_avg:60.06ms
step:601/2315 train_time:36093ms step_avg:60.06ms
step:602/2315 train_time:36154ms step_avg:60.06ms
step:603/2315 train_time:36214ms step_avg:60.06ms
step:604/2315 train_time:36275ms step_avg:60.06ms
step:605/2315 train_time:36335ms step_avg:60.06ms
step:606/2315 train_time:36395ms step_avg:60.06ms
step:607/2315 train_time:36456ms step_avg:60.06ms
step:608/2315 train_time:36516ms step_avg:60.06ms
step:609/2315 train_time:36576ms step_avg:60.06ms
step:610/2315 train_time:36637ms step_avg:60.06ms
step:611/2315 train_time:36697ms step_avg:60.06ms
step:612/2315 train_time:36756ms step_avg:60.06ms
step:613/2315 train_time:36816ms step_avg:60.06ms
step:614/2315 train_time:36876ms step_avg:60.06ms
step:615/2315 train_time:36935ms step_avg:60.06ms
step:616/2315 train_time:36995ms step_avg:60.06ms
step:617/2315 train_time:37055ms step_avg:60.06ms
step:618/2315 train_time:37116ms step_avg:60.06ms
step:619/2315 train_time:37175ms step_avg:60.06ms
step:620/2315 train_time:37236ms step_avg:60.06ms
step:621/2315 train_time:37295ms step_avg:60.06ms
step:622/2315 train_time:37355ms step_avg:60.06ms
step:623/2315 train_time:37415ms step_avg:60.06ms
step:624/2315 train_time:37477ms step_avg:60.06ms
step:625/2315 train_time:37537ms step_avg:60.06ms
step:626/2315 train_time:37597ms step_avg:60.06ms
step:627/2315 train_time:37657ms step_avg:60.06ms
step:628/2315 train_time:37717ms step_avg:60.06ms
step:629/2315 train_time:37776ms step_avg:60.06ms
step:630/2315 train_time:37837ms step_avg:60.06ms
step:631/2315 train_time:37896ms step_avg:60.06ms
step:632/2315 train_time:37956ms step_avg:60.06ms
step:633/2315 train_time:38016ms step_avg:60.06ms
step:634/2315 train_time:38077ms step_avg:60.06ms
step:635/2315 train_time:38136ms step_avg:60.06ms
step:636/2315 train_time:38196ms step_avg:60.06ms
step:637/2315 train_time:38255ms step_avg:60.06ms
step:638/2315 train_time:38316ms step_avg:60.06ms
step:639/2315 train_time:38375ms step_avg:60.05ms
step:640/2315 train_time:38436ms step_avg:60.06ms
step:641/2315 train_time:38496ms step_avg:60.06ms
step:642/2315 train_time:38556ms step_avg:60.06ms
step:643/2315 train_time:38616ms step_avg:60.06ms
step:644/2315 train_time:38676ms step_avg:60.06ms
step:645/2315 train_time:38735ms step_avg:60.05ms
step:646/2315 train_time:38795ms step_avg:60.05ms
step:647/2315 train_time:38855ms step_avg:60.05ms
step:648/2315 train_time:38915ms step_avg:60.05ms
step:649/2315 train_time:38975ms step_avg:60.05ms
step:650/2315 train_time:39035ms step_avg:60.05ms
step:651/2315 train_time:39095ms step_avg:60.05ms
step:652/2315 train_time:39155ms step_avg:60.05ms
step:653/2315 train_time:39216ms step_avg:60.05ms
step:654/2315 train_time:39276ms step_avg:60.05ms
step:655/2315 train_time:39335ms step_avg:60.05ms
step:656/2315 train_time:39395ms step_avg:60.05ms
step:657/2315 train_time:39455ms step_avg:60.05ms
step:658/2315 train_time:39515ms step_avg:60.05ms
step:659/2315 train_time:39575ms step_avg:60.05ms
step:660/2315 train_time:39635ms step_avg:60.05ms
step:661/2315 train_time:39694ms step_avg:60.05ms
step:662/2315 train_time:39754ms step_avg:60.05ms
step:663/2315 train_time:39814ms step_avg:60.05ms
step:664/2315 train_time:39875ms step_avg:60.05ms
step:665/2315 train_time:39934ms step_avg:60.05ms
step:666/2315 train_time:39994ms step_avg:60.05ms
step:667/2315 train_time:40054ms step_avg:60.05ms
step:668/2315 train_time:40114ms step_avg:60.05ms
step:669/2315 train_time:40173ms step_avg:60.05ms
step:670/2315 train_time:40233ms step_avg:60.05ms
step:671/2315 train_time:40293ms step_avg:60.05ms
step:672/2315 train_time:40353ms step_avg:60.05ms
step:673/2315 train_time:40413ms step_avg:60.05ms
step:674/2315 train_time:40475ms step_avg:60.05ms
step:675/2315 train_time:40534ms step_avg:60.05ms
step:676/2315 train_time:40594ms step_avg:60.05ms
step:677/2315 train_time:40654ms step_avg:60.05ms
step:678/2315 train_time:40714ms step_avg:60.05ms
step:679/2315 train_time:40774ms step_avg:60.05ms
step:680/2315 train_time:40834ms step_avg:60.05ms
step:681/2315 train_time:40893ms step_avg:60.05ms
step:682/2315 train_time:40954ms step_avg:60.05ms
step:683/2315 train_time:41013ms step_avg:60.05ms
step:684/2315 train_time:41073ms step_avg:60.05ms
step:685/2315 train_time:41133ms step_avg:60.05ms
step:686/2315 train_time:41193ms step_avg:60.05ms
step:687/2315 train_time:41253ms step_avg:60.05ms
step:688/2315 train_time:41313ms step_avg:60.05ms
step:689/2315 train_time:41373ms step_avg:60.05ms
step:690/2315 train_time:41433ms step_avg:60.05ms
step:691/2315 train_time:41493ms step_avg:60.05ms
step:692/2315 train_time:41554ms step_avg:60.05ms
step:693/2315 train_time:41613ms step_avg:60.05ms
step:694/2315 train_time:41673ms step_avg:60.05ms
step:695/2315 train_time:41733ms step_avg:60.05ms
step:696/2315 train_time:41793ms step_avg:60.05ms
step:697/2315 train_time:41854ms step_avg:60.05ms
step:698/2315 train_time:41914ms step_avg:60.05ms
step:699/2315 train_time:41975ms step_avg:60.05ms
step:700/2315 train_time:42035ms step_avg:60.05ms
step:701/2315 train_time:42094ms step_avg:60.05ms
step:702/2315 train_time:42154ms step_avg:60.05ms
step:703/2315 train_time:42214ms step_avg:60.05ms
step:704/2315 train_time:42274ms step_avg:60.05ms
step:705/2315 train_time:42334ms step_avg:60.05ms
step:706/2315 train_time:42394ms step_avg:60.05ms
step:707/2315 train_time:42454ms step_avg:60.05ms
step:708/2315 train_time:42514ms step_avg:60.05ms
step:709/2315 train_time:42573ms step_avg:60.05ms
step:710/2315 train_time:42634ms step_avg:60.05ms
step:711/2315 train_time:42694ms step_avg:60.05ms
step:712/2315 train_time:42754ms step_avg:60.05ms
step:713/2315 train_time:42814ms step_avg:60.05ms
step:714/2315 train_time:42874ms step_avg:60.05ms
step:715/2315 train_time:42934ms step_avg:60.05ms
step:716/2315 train_time:42994ms step_avg:60.05ms
step:717/2315 train_time:43054ms step_avg:60.05ms
step:718/2315 train_time:43114ms step_avg:60.05ms
step:719/2315 train_time:43174ms step_avg:60.05ms
step:720/2315 train_time:43234ms step_avg:60.05ms
step:721/2315 train_time:43293ms step_avg:60.05ms
step:722/2315 train_time:43354ms step_avg:60.05ms
step:723/2315 train_time:43414ms step_avg:60.05ms
step:724/2315 train_time:43474ms step_avg:60.05ms
step:725/2315 train_time:43533ms step_avg:60.05ms
step:726/2315 train_time:43593ms step_avg:60.05ms
step:727/2315 train_time:43653ms step_avg:60.05ms
step:728/2315 train_time:43714ms step_avg:60.05ms
step:729/2315 train_time:43773ms step_avg:60.05ms
step:730/2315 train_time:43834ms step_avg:60.05ms
step:731/2315 train_time:43894ms step_avg:60.05ms
step:732/2315 train_time:43954ms step_avg:60.05ms
step:733/2315 train_time:44013ms step_avg:60.05ms
step:734/2315 train_time:44074ms step_avg:60.05ms
step:735/2315 train_time:44133ms step_avg:60.05ms
step:736/2315 train_time:44194ms step_avg:60.05ms
step:737/2315 train_time:44254ms step_avg:60.05ms
step:738/2315 train_time:44314ms step_avg:60.05ms
step:739/2315 train_time:44374ms step_avg:60.05ms
step:740/2315 train_time:44433ms step_avg:60.05ms
step:741/2315 train_time:44493ms step_avg:60.04ms
step:742/2315 train_time:44553ms step_avg:60.04ms
step:743/2315 train_time:44613ms step_avg:60.04ms
step:744/2315 train_time:44674ms step_avg:60.05ms
step:745/2315 train_time:44734ms step_avg:60.05ms
step:746/2315 train_time:44794ms step_avg:60.05ms
step:747/2315 train_time:44854ms step_avg:60.05ms
step:748/2315 train_time:44914ms step_avg:60.05ms
step:749/2315 train_time:44974ms step_avg:60.05ms
step:750/2315 train_time:45034ms step_avg:60.04ms
step:750/2315 val_loss:3.6820 train_time:45095ms step_avg:60.13ms
step:751/2315 train_time:45114ms step_avg:60.07ms
step:752/2315 train_time:45156ms step_avg:60.05ms
step:753/2315 train_time:45217ms step_avg:60.05ms
step:754/2315 train_time:45284ms step_avg:60.06ms
step:755/2315 train_time:45344ms step_avg:60.06ms
step:756/2315 train_time:45404ms step_avg:60.06ms
step:757/2315 train_time:45463ms step_avg:60.06ms
step:758/2315 train_time:45523ms step_avg:60.06ms
step:759/2315 train_time:45582ms step_avg:60.05ms
step:760/2315 train_time:45641ms step_avg:60.05ms
step:761/2315 train_time:45701ms step_avg:60.05ms
step:762/2315 train_time:45762ms step_avg:60.05ms
step:763/2315 train_time:45821ms step_avg:60.05ms
step:764/2315 train_time:45882ms step_avg:60.06ms
step:765/2315 train_time:45942ms step_avg:60.05ms
step:766/2315 train_time:46003ms step_avg:60.06ms
step:767/2315 train_time:46065ms step_avg:60.06ms
step:768/2315 train_time:46128ms step_avg:60.06ms
step:769/2315 train_time:46190ms step_avg:60.06ms
step:770/2315 train_time:46252ms step_avg:60.07ms
step:771/2315 train_time:46313ms step_avg:60.07ms
step:772/2315 train_time:46375ms step_avg:60.07ms
step:773/2315 train_time:46436ms step_avg:60.07ms
step:774/2315 train_time:46496ms step_avg:60.07ms
step:775/2315 train_time:46557ms step_avg:60.07ms
step:776/2315 train_time:46618ms step_avg:60.07ms
step:777/2315 train_time:46677ms step_avg:60.07ms
step:778/2315 train_time:46738ms step_avg:60.07ms
step:779/2315 train_time:46798ms step_avg:60.07ms
step:780/2315 train_time:46859ms step_avg:60.08ms
step:781/2315 train_time:46919ms step_avg:60.08ms
step:782/2315 train_time:46980ms step_avg:60.08ms
step:783/2315 train_time:47041ms step_avg:60.08ms
step:784/2315 train_time:47103ms step_avg:60.08ms
step:785/2315 train_time:47165ms step_avg:60.08ms
step:786/2315 train_time:47227ms step_avg:60.08ms
step:787/2315 train_time:47287ms step_avg:60.09ms
step:788/2315 train_time:47348ms step_avg:60.09ms
step:789/2315 train_time:47409ms step_avg:60.09ms
step:790/2315 train_time:47470ms step_avg:60.09ms
step:791/2315 train_time:47530ms step_avg:60.09ms
step:792/2315 train_time:47590ms step_avg:60.09ms
step:793/2315 train_time:47650ms step_avg:60.09ms
step:794/2315 train_time:47710ms step_avg:60.09ms
step:795/2315 train_time:47771ms step_avg:60.09ms
step:796/2315 train_time:47831ms step_avg:60.09ms
step:797/2315 train_time:47892ms step_avg:60.09ms
step:798/2315 train_time:47952ms step_avg:60.09ms
step:799/2315 train_time:48012ms step_avg:60.09ms
step:800/2315 train_time:48073ms step_avg:60.09ms
step:801/2315 train_time:48134ms step_avg:60.09ms
step:802/2315 train_time:48196ms step_avg:60.09ms
step:803/2315 train_time:48257ms step_avg:60.10ms
step:804/2315 train_time:48319ms step_avg:60.10ms
step:805/2315 train_time:48380ms step_avg:60.10ms
step:806/2315 train_time:48441ms step_avg:60.10ms
step:807/2315 train_time:48501ms step_avg:60.10ms
step:808/2315 train_time:48562ms step_avg:60.10ms
step:809/2315 train_time:48623ms step_avg:60.10ms
step:810/2315 train_time:48684ms step_avg:60.10ms
step:811/2315 train_time:48744ms step_avg:60.10ms
step:812/2315 train_time:48805ms step_avg:60.10ms
step:813/2315 train_time:48865ms step_avg:60.10ms
step:814/2315 train_time:48927ms step_avg:60.11ms
step:815/2315 train_time:48987ms step_avg:60.11ms
step:816/2315 train_time:49048ms step_avg:60.11ms
step:817/2315 train_time:49108ms step_avg:60.11ms
step:818/2315 train_time:49169ms step_avg:60.11ms
step:819/2315 train_time:49230ms step_avg:60.11ms
step:820/2315 train_time:49291ms step_avg:60.11ms
step:821/2315 train_time:49351ms step_avg:60.11ms
step:822/2315 train_time:49413ms step_avg:60.11ms
step:823/2315 train_time:49473ms step_avg:60.11ms
step:824/2315 train_time:49534ms step_avg:60.11ms
step:825/2315 train_time:49594ms step_avg:60.11ms
step:826/2315 train_time:49655ms step_avg:60.11ms
step:827/2315 train_time:49715ms step_avg:60.11ms
step:828/2315 train_time:49776ms step_avg:60.12ms
step:829/2315 train_time:49836ms step_avg:60.12ms
step:830/2315 train_time:49897ms step_avg:60.12ms
step:831/2315 train_time:49958ms step_avg:60.12ms
step:832/2315 train_time:50019ms step_avg:60.12ms
step:833/2315 train_time:50080ms step_avg:60.12ms
step:834/2315 train_time:50141ms step_avg:60.12ms
step:835/2315 train_time:50203ms step_avg:60.12ms
step:836/2315 train_time:50264ms step_avg:60.12ms
step:837/2315 train_time:50325ms step_avg:60.12ms
step:838/2315 train_time:50386ms step_avg:60.13ms
step:839/2315 train_time:50446ms step_avg:60.13ms
step:840/2315 train_time:50507ms step_avg:60.13ms
step:841/2315 train_time:50567ms step_avg:60.13ms
step:842/2315 train_time:50629ms step_avg:60.13ms
step:843/2315 train_time:50689ms step_avg:60.13ms
step:844/2315 train_time:50750ms step_avg:60.13ms
step:845/2315 train_time:50810ms step_avg:60.13ms
step:846/2315 train_time:50871ms step_avg:60.13ms
step:847/2315 train_time:50932ms step_avg:60.13ms
step:848/2315 train_time:50993ms step_avg:60.13ms
step:849/2315 train_time:51054ms step_avg:60.13ms
step:850/2315 train_time:51115ms step_avg:60.14ms
step:851/2315 train_time:51176ms step_avg:60.14ms
step:852/2315 train_time:51236ms step_avg:60.14ms
step:853/2315 train_time:51297ms step_avg:60.14ms
step:854/2315 train_time:51359ms step_avg:60.14ms
step:855/2315 train_time:51420ms step_avg:60.14ms
step:856/2315 train_time:51482ms step_avg:60.14ms
step:857/2315 train_time:51542ms step_avg:60.14ms
step:858/2315 train_time:51604ms step_avg:60.14ms
step:859/2315 train_time:51665ms step_avg:60.15ms
step:860/2315 train_time:51726ms step_avg:60.15ms
step:861/2315 train_time:51786ms step_avg:60.15ms
step:862/2315 train_time:51847ms step_avg:60.15ms
step:863/2315 train_time:51907ms step_avg:60.15ms
step:864/2315 train_time:51969ms step_avg:60.15ms
step:865/2315 train_time:52030ms step_avg:60.15ms
step:866/2315 train_time:52091ms step_avg:60.15ms
step:867/2315 train_time:52152ms step_avg:60.15ms
step:868/2315 train_time:52213ms step_avg:60.15ms
step:869/2315 train_time:52273ms step_avg:60.15ms
step:870/2315 train_time:52334ms step_avg:60.15ms
step:871/2315 train_time:52394ms step_avg:60.15ms
step:872/2315 train_time:52456ms step_avg:60.16ms
step:873/2315 train_time:52517ms step_avg:60.16ms
step:874/2315 train_time:52578ms step_avg:60.16ms
step:875/2315 train_time:52639ms step_avg:60.16ms
step:876/2315 train_time:52700ms step_avg:60.16ms
step:877/2315 train_time:52761ms step_avg:60.16ms
step:878/2315 train_time:52822ms step_avg:60.16ms
step:879/2315 train_time:52883ms step_avg:60.16ms
step:880/2315 train_time:52944ms step_avg:60.16ms
step:881/2315 train_time:53005ms step_avg:60.16ms
step:882/2315 train_time:53066ms step_avg:60.17ms
step:883/2315 train_time:53126ms step_avg:60.17ms
step:884/2315 train_time:53188ms step_avg:60.17ms
step:885/2315 train_time:53248ms step_avg:60.17ms
step:886/2315 train_time:53309ms step_avg:60.17ms
step:887/2315 train_time:53370ms step_avg:60.17ms
step:888/2315 train_time:53431ms step_avg:60.17ms
step:889/2315 train_time:53492ms step_avg:60.17ms
step:890/2315 train_time:53552ms step_avg:60.17ms
step:891/2315 train_time:53612ms step_avg:60.17ms
step:892/2315 train_time:53674ms step_avg:60.17ms
step:893/2315 train_time:53735ms step_avg:60.17ms
step:894/2315 train_time:53796ms step_avg:60.17ms
step:895/2315 train_time:53856ms step_avg:60.17ms
step:896/2315 train_time:53917ms step_avg:60.18ms
step:897/2315 train_time:53978ms step_avg:60.18ms
step:898/2315 train_time:54039ms step_avg:60.18ms
step:899/2315 train_time:54100ms step_avg:60.18ms
step:900/2315 train_time:54162ms step_avg:60.18ms
step:901/2315 train_time:54222ms step_avg:60.18ms
step:902/2315 train_time:54283ms step_avg:60.18ms
step:903/2315 train_time:54344ms step_avg:60.18ms
step:904/2315 train_time:54405ms step_avg:60.18ms
step:905/2315 train_time:54466ms step_avg:60.18ms
step:906/2315 train_time:54527ms step_avg:60.18ms
step:907/2315 train_time:54587ms step_avg:60.18ms
step:908/2315 train_time:54649ms step_avg:60.19ms
step:909/2315 train_time:54709ms step_avg:60.19ms
step:910/2315 train_time:54770ms step_avg:60.19ms
step:911/2315 train_time:54830ms step_avg:60.19ms
step:912/2315 train_time:54891ms step_avg:60.19ms
step:913/2315 train_time:54951ms step_avg:60.19ms
step:914/2315 train_time:55012ms step_avg:60.19ms
step:915/2315 train_time:55072ms step_avg:60.19ms
step:916/2315 train_time:55133ms step_avg:60.19ms
step:917/2315 train_time:55194ms step_avg:60.19ms
step:918/2315 train_time:55256ms step_avg:60.19ms
step:919/2315 train_time:55316ms step_avg:60.19ms
step:920/2315 train_time:55377ms step_avg:60.19ms
step:921/2315 train_time:55439ms step_avg:60.19ms
step:922/2315 train_time:55501ms step_avg:60.20ms
step:923/2315 train_time:55562ms step_avg:60.20ms
step:924/2315 train_time:55624ms step_avg:60.20ms
step:925/2315 train_time:55684ms step_avg:60.20ms
step:926/2315 train_time:55745ms step_avg:60.20ms
step:927/2315 train_time:55805ms step_avg:60.20ms
step:928/2315 train_time:55866ms step_avg:60.20ms
step:929/2315 train_time:55927ms step_avg:60.20ms
step:930/2315 train_time:55988ms step_avg:60.20ms
step:931/2315 train_time:56048ms step_avg:60.20ms
step:932/2315 train_time:56109ms step_avg:60.20ms
step:933/2315 train_time:56170ms step_avg:60.20ms
step:934/2315 train_time:56231ms step_avg:60.20ms
step:935/2315 train_time:56291ms step_avg:60.20ms
step:936/2315 train_time:56352ms step_avg:60.20ms
step:937/2315 train_time:56412ms step_avg:60.21ms
step:938/2315 train_time:56473ms step_avg:60.21ms
step:939/2315 train_time:56534ms step_avg:60.21ms
step:940/2315 train_time:56596ms step_avg:60.21ms
step:941/2315 train_time:56656ms step_avg:60.21ms
step:942/2315 train_time:56718ms step_avg:60.21ms
step:943/2315 train_time:56779ms step_avg:60.21ms
step:944/2315 train_time:56840ms step_avg:60.21ms
step:945/2315 train_time:56900ms step_avg:60.21ms
step:946/2315 train_time:56962ms step_avg:60.21ms
step:947/2315 train_time:57023ms step_avg:60.21ms
step:948/2315 train_time:57084ms step_avg:60.22ms
step:949/2315 train_time:57145ms step_avg:60.22ms
step:950/2315 train_time:57206ms step_avg:60.22ms
step:951/2315 train_time:57267ms step_avg:60.22ms
step:952/2315 train_time:57329ms step_avg:60.22ms
step:953/2315 train_time:57389ms step_avg:60.22ms
step:954/2315 train_time:57450ms step_avg:60.22ms
step:955/2315 train_time:57510ms step_avg:60.22ms
step:956/2315 train_time:57572ms step_avg:60.22ms
step:957/2315 train_time:57632ms step_avg:60.22ms
step:958/2315 train_time:57693ms step_avg:60.22ms
step:959/2315 train_time:57754ms step_avg:60.22ms
step:960/2315 train_time:57815ms step_avg:60.22ms
step:961/2315 train_time:57875ms step_avg:60.22ms
step:962/2315 train_time:57937ms step_avg:60.23ms
step:963/2315 train_time:57998ms step_avg:60.23ms
step:964/2315 train_time:58059ms step_avg:60.23ms
step:965/2315 train_time:58119ms step_avg:60.23ms
step:966/2315 train_time:58181ms step_avg:60.23ms
step:967/2315 train_time:58242ms step_avg:60.23ms
step:968/2315 train_time:58303ms step_avg:60.23ms
step:969/2315 train_time:58364ms step_avg:60.23ms
step:970/2315 train_time:58425ms step_avg:60.23ms
step:971/2315 train_time:58486ms step_avg:60.23ms
step:972/2315 train_time:58547ms step_avg:60.23ms
step:973/2315 train_time:58608ms step_avg:60.23ms
step:974/2315 train_time:58669ms step_avg:60.23ms
step:975/2315 train_time:58729ms step_avg:60.23ms
step:976/2315 train_time:58790ms step_avg:60.24ms
step:977/2315 train_time:58851ms step_avg:60.24ms
step:978/2315 train_time:58912ms step_avg:60.24ms
step:979/2315 train_time:58972ms step_avg:60.24ms
step:980/2315 train_time:59033ms step_avg:60.24ms
step:981/2315 train_time:59093ms step_avg:60.24ms
step:982/2315 train_time:59155ms step_avg:60.24ms
step:983/2315 train_time:59215ms step_avg:60.24ms
step:984/2315 train_time:59276ms step_avg:60.24ms
step:985/2315 train_time:59336ms step_avg:60.24ms
step:986/2315 train_time:59398ms step_avg:60.24ms
step:987/2315 train_time:59459ms step_avg:60.24ms
step:988/2315 train_time:59521ms step_avg:60.24ms
step:989/2315 train_time:59582ms step_avg:60.24ms
step:990/2315 train_time:59643ms step_avg:60.25ms
step:991/2315 train_time:59704ms step_avg:60.25ms
step:992/2315 train_time:59765ms step_avg:60.25ms
step:993/2315 train_time:59826ms step_avg:60.25ms
step:994/2315 train_time:59888ms step_avg:60.25ms
step:995/2315 train_time:59948ms step_avg:60.25ms
step:996/2315 train_time:60009ms step_avg:60.25ms
step:997/2315 train_time:60069ms step_avg:60.25ms
step:998/2315 train_time:60130ms step_avg:60.25ms
step:999/2315 train_time:60190ms step_avg:60.25ms
step:1000/2315 train_time:60251ms step_avg:60.25ms
step:1000/2315 val_loss:3.5696 train_time:60313ms step_avg:60.31ms
step:1001/2315 train_time:60331ms step_avg:60.27ms
step:1002/2315 train_time:60375ms step_avg:60.25ms
step:1003/2315 train_time:60443ms step_avg:60.26ms
step:1004/2315 train_time:60507ms step_avg:60.27ms
step:1005/2315 train_time:60568ms step_avg:60.27ms
step:1006/2315 train_time:60630ms step_avg:60.27ms
step:1007/2315 train_time:60690ms step_avg:60.27ms
step:1008/2315 train_time:60750ms step_avg:60.27ms
step:1009/2315 train_time:60809ms step_avg:60.27ms
step:1010/2315 train_time:60869ms step_avg:60.27ms
step:1011/2315 train_time:60929ms step_avg:60.27ms
step:1012/2315 train_time:60989ms step_avg:60.27ms
step:1013/2315 train_time:61049ms step_avg:60.27ms
step:1014/2315 train_time:61109ms step_avg:60.27ms
step:1015/2315 train_time:61168ms step_avg:60.26ms
step:1016/2315 train_time:61233ms step_avg:60.27ms
step:1017/2315 train_time:61295ms step_avg:60.27ms
step:1018/2315 train_time:61358ms step_avg:60.27ms
step:1019/2315 train_time:61420ms step_avg:60.28ms
step:1020/2315 train_time:61483ms step_avg:60.28ms
step:1021/2315 train_time:61544ms step_avg:60.28ms
step:1022/2315 train_time:61605ms step_avg:60.28ms
step:1023/2315 train_time:61666ms step_avg:60.28ms
step:1024/2315 train_time:61726ms step_avg:60.28ms
step:1025/2315 train_time:61786ms step_avg:60.28ms
step:1026/2315 train_time:61847ms step_avg:60.28ms
step:1027/2315 train_time:61906ms step_avg:60.28ms
step:1028/2315 train_time:61967ms step_avg:60.28ms
step:1029/2315 train_time:62026ms step_avg:60.28ms
step:1030/2315 train_time:62087ms step_avg:60.28ms
step:1031/2315 train_time:62147ms step_avg:60.28ms
step:1032/2315 train_time:62209ms step_avg:60.28ms
step:1033/2315 train_time:62272ms step_avg:60.28ms
step:1034/2315 train_time:62334ms step_avg:60.28ms
step:1035/2315 train_time:62395ms step_avg:60.29ms
step:1036/2315 train_time:62457ms step_avg:60.29ms
step:1037/2315 train_time:62518ms step_avg:60.29ms
step:1038/2315 train_time:62579ms step_avg:60.29ms
step:1039/2315 train_time:62639ms step_avg:60.29ms
step:1040/2315 train_time:62699ms step_avg:60.29ms
step:1041/2315 train_time:62759ms step_avg:60.29ms
step:1042/2315 train_time:62819ms step_avg:60.29ms
step:1043/2315 train_time:62880ms step_avg:60.29ms
step:1044/2315 train_time:62940ms step_avg:60.29ms
step:1045/2315 train_time:63000ms step_avg:60.29ms
step:1046/2315 train_time:63061ms step_avg:60.29ms
step:1047/2315 train_time:63121ms step_avg:60.29ms
step:1048/2315 train_time:63182ms step_avg:60.29ms
step:1049/2315 train_time:63243ms step_avg:60.29ms
step:1050/2315 train_time:63305ms step_avg:60.29ms
step:1051/2315 train_time:63366ms step_avg:60.29ms
step:1052/2315 train_time:63428ms step_avg:60.29ms
step:1053/2315 train_time:63489ms step_avg:60.29ms
step:1054/2315 train_time:63551ms step_avg:60.30ms
step:1055/2315 train_time:63612ms step_avg:60.30ms
step:1056/2315 train_time:63673ms step_avg:60.30ms
step:1057/2315 train_time:63734ms step_avg:60.30ms
step:1058/2315 train_time:63795ms step_avg:60.30ms
step:1059/2315 train_time:63855ms step_avg:60.30ms
step:1060/2315 train_time:63916ms step_avg:60.30ms
step:1061/2315 train_time:63976ms step_avg:60.30ms
step:1062/2315 train_time:64037ms step_avg:60.30ms
step:1063/2315 train_time:64097ms step_avg:60.30ms
step:1064/2315 train_time:64159ms step_avg:60.30ms
step:1065/2315 train_time:64219ms step_avg:60.30ms
step:1066/2315 train_time:64280ms step_avg:60.30ms
step:1067/2315 train_time:64341ms step_avg:60.30ms
step:1068/2315 train_time:64403ms step_avg:60.30ms
step:1069/2315 train_time:64463ms step_avg:60.30ms
step:1070/2315 train_time:64524ms step_avg:60.30ms
step:1071/2315 train_time:64585ms step_avg:60.30ms
step:1072/2315 train_time:64646ms step_avg:60.30ms
step:1073/2315 train_time:64707ms step_avg:60.30ms
step:1074/2315 train_time:64768ms step_avg:60.31ms
step:1075/2315 train_time:64829ms step_avg:60.31ms
step:1076/2315 train_time:64890ms step_avg:60.31ms
step:1077/2315 train_time:64951ms step_avg:60.31ms
step:1078/2315 train_time:65012ms step_avg:60.31ms
step:1079/2315 train_time:65072ms step_avg:60.31ms
step:1080/2315 train_time:65133ms step_avg:60.31ms
step:1081/2315 train_time:65194ms step_avg:60.31ms
step:1082/2315 train_time:65255ms step_avg:60.31ms
step:1083/2315 train_time:65316ms step_avg:60.31ms
step:1084/2315 train_time:65377ms step_avg:60.31ms
step:1085/2315 train_time:65437ms step_avg:60.31ms
step:1086/2315 train_time:65499ms step_avg:60.31ms
step:1087/2315 train_time:65560ms step_avg:60.31ms
step:1088/2315 train_time:65620ms step_avg:60.31ms
step:1089/2315 train_time:65680ms step_avg:60.31ms
step:1090/2315 train_time:65741ms step_avg:60.31ms
step:1091/2315 train_time:65802ms step_avg:60.31ms
step:1092/2315 train_time:65864ms step_avg:60.31ms
step:1093/2315 train_time:65924ms step_avg:60.31ms
step:1094/2315 train_time:65986ms step_avg:60.32ms
step:1095/2315 train_time:66047ms step_avg:60.32ms
step:1096/2315 train_time:66108ms step_avg:60.32ms
step:1097/2315 train_time:66169ms step_avg:60.32ms
step:1098/2315 train_time:66232ms step_avg:60.32ms
step:1099/2315 train_time:66292ms step_avg:60.32ms
step:1100/2315 train_time:66354ms step_avg:60.32ms
step:1101/2315 train_time:66414ms step_avg:60.32ms
step:1102/2315 train_time:66475ms step_avg:60.32ms
step:1103/2315 train_time:66536ms step_avg:60.32ms
step:1104/2315 train_time:66597ms step_avg:60.32ms
step:1105/2315 train_time:66657ms step_avg:60.32ms
step:1106/2315 train_time:66718ms step_avg:60.32ms
step:1107/2315 train_time:66778ms step_avg:60.32ms
step:1108/2315 train_time:66839ms step_avg:60.32ms
step:1109/2315 train_time:66900ms step_avg:60.32ms
step:1110/2315 train_time:66960ms step_avg:60.32ms
step:1111/2315 train_time:67021ms step_avg:60.32ms
step:1112/2315 train_time:67081ms step_avg:60.32ms
step:1113/2315 train_time:67142ms step_avg:60.33ms
step:1114/2315 train_time:67203ms step_avg:60.33ms
step:1115/2315 train_time:67264ms step_avg:60.33ms
step:1116/2315 train_time:67325ms step_avg:60.33ms
step:1117/2315 train_time:67386ms step_avg:60.33ms
step:1118/2315 train_time:67449ms step_avg:60.33ms
step:1119/2315 train_time:67509ms step_avg:60.33ms
step:1120/2315 train_time:67571ms step_avg:60.33ms
step:1121/2315 train_time:67632ms step_avg:60.33ms
step:1122/2315 train_time:67693ms step_avg:60.33ms
step:1123/2315 train_time:67753ms step_avg:60.33ms
step:1124/2315 train_time:67815ms step_avg:60.33ms
step:1125/2315 train_time:67875ms step_avg:60.33ms
step:1126/2315 train_time:67936ms step_avg:60.33ms
step:1127/2315 train_time:67996ms step_avg:60.33ms
step:1128/2315 train_time:68057ms step_avg:60.33ms
step:1129/2315 train_time:68118ms step_avg:60.33ms
step:1130/2315 train_time:68179ms step_avg:60.34ms
step:1131/2315 train_time:68239ms step_avg:60.34ms
step:1132/2315 train_time:68300ms step_avg:60.34ms
step:1133/2315 train_time:68361ms step_avg:60.34ms
step:1134/2315 train_time:68422ms step_avg:60.34ms
step:1135/2315 train_time:68482ms step_avg:60.34ms
step:1136/2315 train_time:68543ms step_avg:60.34ms
step:1137/2315 train_time:68604ms step_avg:60.34ms
step:1138/2315 train_time:68665ms step_avg:60.34ms
step:1139/2315 train_time:68726ms step_avg:60.34ms
step:1140/2315 train_time:68787ms step_avg:60.34ms
step:1141/2315 train_time:68848ms step_avg:60.34ms
step:1142/2315 train_time:68909ms step_avg:60.34ms
step:1143/2315 train_time:68970ms step_avg:60.34ms
step:1144/2315 train_time:69032ms step_avg:60.34ms
step:1145/2315 train_time:69093ms step_avg:60.34ms
step:1146/2315 train_time:69154ms step_avg:60.34ms
step:1147/2315 train_time:69214ms step_avg:60.34ms
step:1148/2315 train_time:69276ms step_avg:60.34ms
step:1149/2315 train_time:69336ms step_avg:60.35ms
step:1150/2315 train_time:69397ms step_avg:60.35ms
step:1151/2315 train_time:69457ms step_avg:60.35ms
step:1152/2315 train_time:69519ms step_avg:60.35ms
step:1153/2315 train_time:69579ms step_avg:60.35ms
step:1154/2315 train_time:69640ms step_avg:60.35ms
step:1155/2315 train_time:69700ms step_avg:60.35ms
step:1156/2315 train_time:69761ms step_avg:60.35ms
step:1157/2315 train_time:69822ms step_avg:60.35ms
step:1158/2315 train_time:69884ms step_avg:60.35ms
step:1159/2315 train_time:69945ms step_avg:60.35ms
step:1160/2315 train_time:70006ms step_avg:60.35ms
step:1161/2315 train_time:70067ms step_avg:60.35ms
step:1162/2315 train_time:70128ms step_avg:60.35ms
step:1163/2315 train_time:70189ms step_avg:60.35ms
step:1164/2315 train_time:70251ms step_avg:60.35ms
step:1165/2315 train_time:70311ms step_avg:60.35ms
step:1166/2315 train_time:70372ms step_avg:60.35ms
step:1167/2315 train_time:70433ms step_avg:60.35ms
step:1168/2315 train_time:70494ms step_avg:60.35ms
step:1169/2315 train_time:70555ms step_avg:60.36ms
step:1170/2315 train_time:70616ms step_avg:60.36ms
step:1171/2315 train_time:70676ms step_avg:60.36ms
step:1172/2315 train_time:70737ms step_avg:60.36ms
step:1173/2315 train_time:70798ms step_avg:60.36ms
step:1174/2315 train_time:70859ms step_avg:60.36ms
step:1175/2315 train_time:70920ms step_avg:60.36ms
step:1176/2315 train_time:70981ms step_avg:60.36ms
step:1177/2315 train_time:71041ms step_avg:60.36ms
step:1178/2315 train_time:71102ms step_avg:60.36ms
step:1179/2315 train_time:71162ms step_avg:60.36ms
step:1180/2315 train_time:71224ms step_avg:60.36ms
step:1181/2315 train_time:71285ms step_avg:60.36ms
step:1182/2315 train_time:71347ms step_avg:60.36ms
step:1183/2315 train_time:71408ms step_avg:60.36ms
step:1184/2315 train_time:71470ms step_avg:60.36ms
step:1185/2315 train_time:71531ms step_avg:60.36ms
step:1186/2315 train_time:71592ms step_avg:60.36ms
step:1187/2315 train_time:71653ms step_avg:60.36ms
step:1188/2315 train_time:71714ms step_avg:60.37ms
step:1189/2315 train_time:71775ms step_avg:60.37ms
step:1190/2315 train_time:71836ms step_avg:60.37ms
step:1191/2315 train_time:71897ms step_avg:60.37ms
step:1192/2315 train_time:71958ms step_avg:60.37ms
step:1193/2315 train_time:72019ms step_avg:60.37ms
step:1194/2315 train_time:72080ms step_avg:60.37ms
step:1195/2315 train_time:72140ms step_avg:60.37ms
step:1196/2315 train_time:72201ms step_avg:60.37ms
step:1197/2315 train_time:72261ms step_avg:60.37ms
step:1198/2315 train_time:72323ms step_avg:60.37ms
step:1199/2315 train_time:72383ms step_avg:60.37ms
step:1200/2315 train_time:72445ms step_avg:60.37ms
step:1201/2315 train_time:72506ms step_avg:60.37ms
step:1202/2315 train_time:72567ms step_avg:60.37ms
step:1203/2315 train_time:72629ms step_avg:60.37ms
step:1204/2315 train_time:72690ms step_avg:60.37ms
step:1205/2315 train_time:72751ms step_avg:60.37ms
step:1206/2315 train_time:72812ms step_avg:60.37ms
step:1207/2315 train_time:72873ms step_avg:60.38ms
step:1208/2315 train_time:72934ms step_avg:60.38ms
step:1209/2315 train_time:72995ms step_avg:60.38ms
step:1210/2315 train_time:73056ms step_avg:60.38ms
step:1211/2315 train_time:73117ms step_avg:60.38ms
step:1212/2315 train_time:73177ms step_avg:60.38ms
step:1213/2315 train_time:73238ms step_avg:60.38ms
step:1214/2315 train_time:73299ms step_avg:60.38ms
step:1215/2315 train_time:73359ms step_avg:60.38ms
step:1216/2315 train_time:73420ms step_avg:60.38ms
step:1217/2315 train_time:73481ms step_avg:60.38ms
step:1218/2315 train_time:73542ms step_avg:60.38ms
step:1219/2315 train_time:73603ms step_avg:60.38ms
step:1220/2315 train_time:73664ms step_avg:60.38ms
step:1221/2315 train_time:73725ms step_avg:60.38ms
step:1222/2315 train_time:73787ms step_avg:60.38ms
step:1223/2315 train_time:73848ms step_avg:60.38ms
step:1224/2315 train_time:73910ms step_avg:60.38ms
step:1225/2315 train_time:73971ms step_avg:60.38ms
step:1226/2315 train_time:74032ms step_avg:60.39ms
step:1227/2315 train_time:74093ms step_avg:60.39ms
step:1228/2315 train_time:74154ms step_avg:60.39ms
step:1229/2315 train_time:74215ms step_avg:60.39ms
step:1230/2315 train_time:74276ms step_avg:60.39ms
step:1231/2315 train_time:74336ms step_avg:60.39ms
step:1232/2315 train_time:74397ms step_avg:60.39ms
step:1233/2315 train_time:74458ms step_avg:60.39ms
step:1234/2315 train_time:74519ms step_avg:60.39ms
step:1235/2315 train_time:74580ms step_avg:60.39ms
step:1236/2315 train_time:74641ms step_avg:60.39ms
step:1237/2315 train_time:74701ms step_avg:60.39ms
step:1238/2315 train_time:74762ms step_avg:60.39ms
step:1239/2315 train_time:74823ms step_avg:60.39ms
step:1240/2315 train_time:74885ms step_avg:60.39ms
step:1241/2315 train_time:74946ms step_avg:60.39ms
step:1242/2315 train_time:75008ms step_avg:60.39ms
step:1243/2315 train_time:75068ms step_avg:60.39ms
step:1244/2315 train_time:75130ms step_avg:60.39ms
step:1245/2315 train_time:75191ms step_avg:60.39ms
step:1246/2315 train_time:75253ms step_avg:60.40ms
step:1247/2315 train_time:75313ms step_avg:60.40ms
step:1248/2315 train_time:75374ms step_avg:60.40ms
step:1249/2315 train_time:75435ms step_avg:60.40ms
step:1250/2315 train_time:75496ms step_avg:60.40ms
step:1250/2315 val_loss:3.5138 train_time:75559ms step_avg:60.45ms
step:1251/2315 train_time:75578ms step_avg:60.41ms
step:1252/2315 train_time:75621ms step_avg:60.40ms
step:1253/2315 train_time:75686ms step_avg:60.40ms
step:1254/2315 train_time:75748ms step_avg:60.41ms
step:1255/2315 train_time:75809ms step_avg:60.41ms
step:1256/2315 train_time:75869ms step_avg:60.41ms
step:1257/2315 train_time:75929ms step_avg:60.40ms
step:1258/2315 train_time:75989ms step_avg:60.40ms
step:1259/2315 train_time:76049ms step_avg:60.40ms
step:1260/2315 train_time:76109ms step_avg:60.40ms
step:1261/2315 train_time:76169ms step_avg:60.40ms
step:1262/2315 train_time:76229ms step_avg:60.40ms
step:1263/2315 train_time:76289ms step_avg:60.40ms
step:1264/2315 train_time:76349ms step_avg:60.40ms
step:1265/2315 train_time:76409ms step_avg:60.40ms
step:1266/2315 train_time:76470ms step_avg:60.40ms
step:1267/2315 train_time:76532ms step_avg:60.40ms
step:1268/2315 train_time:76595ms step_avg:60.41ms
step:1269/2315 train_time:76656ms step_avg:60.41ms
step:1270/2315 train_time:76719ms step_avg:60.41ms
step:1271/2315 train_time:76780ms step_avg:60.41ms
step:1272/2315 train_time:76841ms step_avg:60.41ms
step:1273/2315 train_time:76902ms step_avg:60.41ms
step:1274/2315 train_time:76962ms step_avg:60.41ms
step:1275/2315 train_time:77023ms step_avg:60.41ms
step:1276/2315 train_time:77084ms step_avg:60.41ms
step:1277/2315 train_time:77144ms step_avg:60.41ms
step:1278/2315 train_time:77205ms step_avg:60.41ms
step:1279/2315 train_time:77265ms step_avg:60.41ms
step:1280/2315 train_time:77326ms step_avg:60.41ms
step:1281/2315 train_time:77386ms step_avg:60.41ms
step:1282/2315 train_time:77447ms step_avg:60.41ms
step:1283/2315 train_time:77508ms step_avg:60.41ms
step:1284/2315 train_time:77570ms step_avg:60.41ms
step:1285/2315 train_time:77631ms step_avg:60.41ms
step:1286/2315 train_time:77693ms step_avg:60.41ms
step:1287/2315 train_time:77754ms step_avg:60.42ms
step:1288/2315 train_time:77815ms step_avg:60.42ms
step:1289/2315 train_time:77875ms step_avg:60.42ms
step:1290/2315 train_time:77936ms step_avg:60.42ms
step:1291/2315 train_time:77997ms step_avg:60.42ms
step:1292/2315 train_time:78057ms step_avg:60.42ms
step:1293/2315 train_time:78117ms step_avg:60.42ms
step:1294/2315 train_time:78178ms step_avg:60.42ms
step:1295/2315 train_time:78239ms step_avg:60.42ms
step:1296/2315 train_time:78300ms step_avg:60.42ms
step:1297/2315 train_time:78360ms step_avg:60.42ms
step:1298/2315 train_time:78422ms step_avg:60.42ms
step:1299/2315 train_time:78483ms step_avg:60.42ms
step:1300/2315 train_time:78546ms step_avg:60.42ms
step:1301/2315 train_time:78607ms step_avg:60.42ms
step:1302/2315 train_time:78669ms step_avg:60.42ms
step:1303/2315 train_time:78729ms step_avg:60.42ms
step:1304/2315 train_time:78791ms step_avg:60.42ms
step:1305/2315 train_time:78851ms step_avg:60.42ms
step:1306/2315 train_time:78912ms step_avg:60.42ms
step:1307/2315 train_time:78972ms step_avg:60.42ms
step:1308/2315 train_time:79033ms step_avg:60.42ms
step:1309/2315 train_time:79093ms step_avg:60.42ms
step:1310/2315 train_time:79154ms step_avg:60.42ms
step:1311/2315 train_time:79214ms step_avg:60.42ms
step:1312/2315 train_time:79275ms step_avg:60.42ms
step:1313/2315 train_time:79335ms step_avg:60.42ms
step:1314/2315 train_time:79396ms step_avg:60.42ms
step:1315/2315 train_time:79457ms step_avg:60.42ms
step:1316/2315 train_time:79519ms step_avg:60.42ms
step:1317/2315 train_time:79581ms step_avg:60.43ms
step:1318/2315 train_time:79642ms step_avg:60.43ms
step:1319/2315 train_time:79703ms step_avg:60.43ms
step:1320/2315 train_time:79765ms step_avg:60.43ms
step:1321/2315 train_time:79826ms step_avg:60.43ms
step:1322/2315 train_time:79886ms step_avg:60.43ms
step:1323/2315 train_time:79947ms step_avg:60.43ms
step:1324/2315 train_time:80008ms step_avg:60.43ms
step:1325/2315 train_time:80069ms step_avg:60.43ms
step:1326/2315 train_time:80130ms step_avg:60.43ms
step:1327/2315 train_time:80191ms step_avg:60.43ms
step:1328/2315 train_time:80253ms step_avg:60.43ms
step:1329/2315 train_time:80313ms step_avg:60.43ms
step:1330/2315 train_time:80373ms step_avg:60.43ms
step:1331/2315 train_time:80433ms step_avg:60.43ms
step:1332/2315 train_time:80494ms step_avg:60.43ms
step:1333/2315 train_time:80554ms step_avg:60.43ms
step:1334/2315 train_time:80617ms step_avg:60.43ms
step:1335/2315 train_time:80678ms step_avg:60.43ms
step:1336/2315 train_time:80739ms step_avg:60.43ms
step:1337/2315 train_time:80800ms step_avg:60.43ms
step:1338/2315 train_time:80863ms step_avg:60.44ms
step:1339/2315 train_time:80923ms step_avg:60.44ms
step:1340/2315 train_time:80985ms step_avg:60.44ms
step:1341/2315 train_time:81046ms step_avg:60.44ms
step:1342/2315 train_time:81107ms step_avg:60.44ms
step:1343/2315 train_time:81168ms step_avg:60.44ms
step:1344/2315 train_time:81228ms step_avg:60.44ms
step:1345/2315 train_time:81289ms step_avg:60.44ms
step:1346/2315 train_time:81351ms step_avg:60.44ms
step:1347/2315 train_time:81411ms step_avg:60.44ms
step:1348/2315 train_time:81472ms step_avg:60.44ms
step:1349/2315 train_time:81533ms step_avg:60.44ms
step:1350/2315 train_time:81594ms step_avg:60.44ms
step:1351/2315 train_time:81654ms step_avg:60.44ms
step:1352/2315 train_time:81715ms step_avg:60.44ms
step:1353/2315 train_time:81776ms step_avg:60.44ms
step:1354/2315 train_time:81837ms step_avg:60.44ms
step:1355/2315 train_time:81898ms step_avg:60.44ms
step:1356/2315 train_time:81960ms step_avg:60.44ms
step:1357/2315 train_time:82022ms step_avg:60.44ms
step:1358/2315 train_time:82082ms step_avg:60.44ms
step:1359/2315 train_time:82144ms step_avg:60.44ms
step:1360/2315 train_time:82205ms step_avg:60.44ms
step:1361/2315 train_time:82266ms step_avg:60.45ms
step:1362/2315 train_time:82327ms step_avg:60.45ms
step:1363/2315 train_time:82388ms step_avg:60.45ms
step:1364/2315 train_time:82450ms step_avg:60.45ms
step:1365/2315 train_time:82510ms step_avg:60.45ms
step:1366/2315 train_time:82571ms step_avg:60.45ms
step:1367/2315 train_time:82632ms step_avg:60.45ms
step:1368/2315 train_time:82693ms step_avg:60.45ms
step:1369/2315 train_time:82753ms step_avg:60.45ms
step:1370/2315 train_time:82814ms step_avg:60.45ms
step:1371/2315 train_time:82875ms step_avg:60.45ms
step:1372/2315 train_time:82936ms step_avg:60.45ms
step:1373/2315 train_time:82997ms step_avg:60.45ms
step:1374/2315 train_time:83059ms step_avg:60.45ms
step:1375/2315 train_time:83120ms step_avg:60.45ms
step:1376/2315 train_time:83182ms step_avg:60.45ms
step:1377/2315 train_time:83242ms step_avg:60.45ms
step:1378/2315 train_time:83304ms step_avg:60.45ms
step:1379/2315 train_time:83365ms step_avg:60.45ms
step:1380/2315 train_time:83426ms step_avg:60.45ms
step:1381/2315 train_time:83486ms step_avg:60.45ms
step:1382/2315 train_time:83547ms step_avg:60.45ms
step:1383/2315 train_time:83608ms step_avg:60.45ms
step:1384/2315 train_time:83669ms step_avg:60.45ms
step:1385/2315 train_time:83730ms step_avg:60.46ms
step:1386/2315 train_time:83792ms step_avg:60.46ms
step:1387/2315 train_time:83853ms step_avg:60.46ms
step:1388/2315 train_time:83914ms step_avg:60.46ms
step:1389/2315 train_time:83974ms step_avg:60.46ms
step:1390/2315 train_time:84034ms step_avg:60.46ms
step:1391/2315 train_time:84094ms step_avg:60.46ms
step:1392/2315 train_time:84156ms step_avg:60.46ms
step:1393/2315 train_time:84217ms step_avg:60.46ms
step:1394/2315 train_time:84279ms step_avg:60.46ms
step:1395/2315 train_time:84340ms step_avg:60.46ms
step:1396/2315 train_time:84402ms step_avg:60.46ms
step:1397/2315 train_time:84463ms step_avg:60.46ms
step:1398/2315 train_time:84524ms step_avg:60.46ms
step:1399/2315 train_time:84584ms step_avg:60.46ms
step:1400/2315 train_time:84645ms step_avg:60.46ms
step:1401/2315 train_time:84706ms step_avg:60.46ms
step:1402/2315 train_time:84767ms step_avg:60.46ms
step:1403/2315 train_time:84828ms step_avg:60.46ms
step:1404/2315 train_time:84889ms step_avg:60.46ms
step:1405/2315 train_time:84949ms step_avg:60.46ms
step:1406/2315 train_time:85010ms step_avg:60.46ms
step:1407/2315 train_time:85071ms step_avg:60.46ms
step:1408/2315 train_time:85132ms step_avg:60.46ms
step:1409/2315 train_time:85192ms step_avg:60.46ms
step:1410/2315 train_time:85254ms step_avg:60.46ms
step:1411/2315 train_time:85314ms step_avg:60.46ms
step:1412/2315 train_time:85375ms step_avg:60.46ms
step:1413/2315 train_time:85435ms step_avg:60.46ms
step:1414/2315 train_time:85497ms step_avg:60.46ms
step:1415/2315 train_time:85557ms step_avg:60.46ms
step:1416/2315 train_time:85619ms step_avg:60.47ms
step:1417/2315 train_time:85679ms step_avg:60.47ms
step:1418/2315 train_time:85741ms step_avg:60.47ms
step:1419/2315 train_time:85802ms step_avg:60.47ms
step:1420/2315 train_time:85864ms step_avg:60.47ms
step:1421/2315 train_time:85925ms step_avg:60.47ms
step:1422/2315 train_time:85986ms step_avg:60.47ms
step:1423/2315 train_time:86047ms step_avg:60.47ms
step:1424/2315 train_time:86108ms step_avg:60.47ms
step:1425/2315 train_time:86170ms step_avg:60.47ms
step:1426/2315 train_time:86231ms step_avg:60.47ms
step:1427/2315 train_time:86292ms step_avg:60.47ms
step:1428/2315 train_time:86353ms step_avg:60.47ms
step:1429/2315 train_time:86413ms step_avg:60.47ms
step:1430/2315 train_time:86474ms step_avg:60.47ms
step:1431/2315 train_time:86534ms step_avg:60.47ms
step:1432/2315 train_time:86595ms step_avg:60.47ms
step:1433/2315 train_time:86655ms step_avg:60.47ms
step:1434/2315 train_time:86717ms step_avg:60.47ms
step:1435/2315 train_time:86777ms step_avg:60.47ms
step:1436/2315 train_time:86839ms step_avg:60.47ms
step:1437/2315 train_time:86900ms step_avg:60.47ms
step:1438/2315 train_time:86962ms step_avg:60.47ms
step:1439/2315 train_time:87023ms step_avg:60.47ms
step:1440/2315 train_time:87085ms step_avg:60.48ms
step:1441/2315 train_time:87146ms step_avg:60.48ms
step:1442/2315 train_time:87207ms step_avg:60.48ms
step:1443/2315 train_time:87268ms step_avg:60.48ms
step:1444/2315 train_time:87329ms step_avg:60.48ms
step:1445/2315 train_time:87390ms step_avg:60.48ms
step:1446/2315 train_time:87451ms step_avg:60.48ms
step:1447/2315 train_time:87511ms step_avg:60.48ms
step:1448/2315 train_time:87572ms step_avg:60.48ms
step:1449/2315 train_time:87632ms step_avg:60.48ms
step:1450/2315 train_time:87693ms step_avg:60.48ms
step:1451/2315 train_time:87753ms step_avg:60.48ms
step:1452/2315 train_time:87814ms step_avg:60.48ms
step:1453/2315 train_time:87874ms step_avg:60.48ms
step:1454/2315 train_time:87936ms step_avg:60.48ms
step:1455/2315 train_time:87996ms step_avg:60.48ms
step:1456/2315 train_time:88058ms step_avg:60.48ms
step:1457/2315 train_time:88120ms step_avg:60.48ms
step:1458/2315 train_time:88182ms step_avg:60.48ms
step:1459/2315 train_time:88243ms step_avg:60.48ms
step:1460/2315 train_time:88304ms step_avg:60.48ms
step:1461/2315 train_time:88365ms step_avg:60.48ms
step:1462/2315 train_time:88426ms step_avg:60.48ms
step:1463/2315 train_time:88486ms step_avg:60.48ms
step:1464/2315 train_time:88547ms step_avg:60.48ms
step:1465/2315 train_time:88608ms step_avg:60.48ms
step:1466/2315 train_time:88669ms step_avg:60.48ms
step:1467/2315 train_time:88730ms step_avg:60.48ms
step:1468/2315 train_time:88791ms step_avg:60.48ms
step:1469/2315 train_time:88851ms step_avg:60.48ms
step:1470/2315 train_time:88913ms step_avg:60.48ms
step:1471/2315 train_time:88973ms step_avg:60.48ms
step:1472/2315 train_time:89033ms step_avg:60.48ms
step:1473/2315 train_time:89094ms step_avg:60.48ms
step:1474/2315 train_time:89155ms step_avg:60.49ms
step:1475/2315 train_time:89215ms step_avg:60.48ms
step:1476/2315 train_time:89276ms step_avg:60.49ms
step:1477/2315 train_time:89337ms step_avg:60.49ms
step:1478/2315 train_time:89398ms step_avg:60.49ms
step:1479/2315 train_time:89459ms step_avg:60.49ms
step:1480/2315 train_time:89521ms step_avg:60.49ms
step:1481/2315 train_time:89582ms step_avg:60.49ms
step:1482/2315 train_time:89644ms step_avg:60.49ms
step:1483/2315 train_time:89705ms step_avg:60.49ms
step:1484/2315 train_time:89767ms step_avg:60.49ms
step:1485/2315 train_time:89827ms step_avg:60.49ms
step:1486/2315 train_time:89889ms step_avg:60.49ms
step:1487/2315 train_time:89949ms step_avg:60.49ms
step:1488/2315 train_time:90010ms step_avg:60.49ms
step:1489/2315 train_time:90070ms step_avg:60.49ms
step:1490/2315 train_time:90132ms step_avg:60.49ms
step:1491/2315 train_time:90192ms step_avg:60.49ms
step:1492/2315 train_time:90253ms step_avg:60.49ms
step:1493/2315 train_time:90314ms step_avg:60.49ms
step:1494/2315 train_time:90374ms step_avg:60.49ms
step:1495/2315 train_time:90434ms step_avg:60.49ms
step:1496/2315 train_time:90496ms step_avg:60.49ms
step:1497/2315 train_time:90556ms step_avg:60.49ms
step:1498/2315 train_time:90619ms step_avg:60.49ms
step:1499/2315 train_time:90679ms step_avg:60.49ms
step:1500/2315 train_time:90741ms step_avg:60.49ms
step:1500/2315 val_loss:3.4485 train_time:90804ms step_avg:60.54ms
step:1501/2315 train_time:90822ms step_avg:60.51ms
step:1502/2315 train_time:90866ms step_avg:60.50ms
step:1503/2315 train_time:90929ms step_avg:60.50ms
step:1504/2315 train_time:90991ms step_avg:60.50ms
step:1505/2315 train_time:91051ms step_avg:60.50ms
step:1506/2315 train_time:91112ms step_avg:60.50ms
step:1507/2315 train_time:91171ms step_avg:60.50ms
step:1508/2315 train_time:91231ms step_avg:60.50ms
step:1509/2315 train_time:91291ms step_avg:60.50ms
step:1510/2315 train_time:91351ms step_avg:60.50ms
step:1511/2315 train_time:91411ms step_avg:60.50ms
step:1512/2315 train_time:91471ms step_avg:60.50ms
step:1513/2315 train_time:91531ms step_avg:60.50ms
step:1514/2315 train_time:91591ms step_avg:60.50ms
step:1515/2315 train_time:91650ms step_avg:60.50ms
step:1516/2315 train_time:91712ms step_avg:60.50ms
step:1517/2315 train_time:91774ms step_avg:60.50ms
step:1518/2315 train_time:91836ms step_avg:60.50ms
step:1519/2315 train_time:91899ms step_avg:60.50ms
step:1520/2315 train_time:91961ms step_avg:60.50ms
step:1521/2315 train_time:92024ms step_avg:60.50ms
step:1522/2315 train_time:92085ms step_avg:60.50ms
step:1523/2315 train_time:92146ms step_avg:60.50ms
step:1524/2315 train_time:92208ms step_avg:60.50ms
step:1525/2315 train_time:92268ms step_avg:60.50ms
step:1526/2315 train_time:92329ms step_avg:60.50ms
step:1527/2315 train_time:92390ms step_avg:60.50ms
step:1528/2315 train_time:92451ms step_avg:60.50ms
step:1529/2315 train_time:92511ms step_avg:60.50ms
step:1530/2315 train_time:92572ms step_avg:60.50ms
step:1531/2315 train_time:92632ms step_avg:60.50ms
step:1532/2315 train_time:92694ms step_avg:60.50ms
step:1533/2315 train_time:92755ms step_avg:60.51ms
step:1534/2315 train_time:92817ms step_avg:60.51ms
step:1535/2315 train_time:92879ms step_avg:60.51ms
step:1536/2315 train_time:92941ms step_avg:60.51ms
step:1537/2315 train_time:93003ms step_avg:60.51ms
step:1538/2315 train_time:93066ms step_avg:60.51ms
step:1539/2315 train_time:93126ms step_avg:60.51ms
step:1540/2315 train_time:93188ms step_avg:60.51ms
step:1541/2315 train_time:93248ms step_avg:60.51ms
step:1542/2315 train_time:93309ms step_avg:60.51ms
step:1543/2315 train_time:93370ms step_avg:60.51ms
step:1544/2315 train_time:93431ms step_avg:60.51ms
step:1545/2315 train_time:93491ms step_avg:60.51ms
step:1546/2315 train_time:93552ms step_avg:60.51ms
step:1547/2315 train_time:93613ms step_avg:60.51ms
step:1548/2315 train_time:93674ms step_avg:60.51ms
step:1549/2315 train_time:93735ms step_avg:60.51ms
step:1550/2315 train_time:93797ms step_avg:60.51ms
step:1551/2315 train_time:93858ms step_avg:60.51ms
step:1552/2315 train_time:93920ms step_avg:60.52ms
step:1553/2315 train_time:93982ms step_avg:60.52ms
step:1554/2315 train_time:94044ms step_avg:60.52ms
step:1555/2315 train_time:94105ms step_avg:60.52ms
step:1556/2315 train_time:94167ms step_avg:60.52ms
step:1557/2315 train_time:94228ms step_avg:60.52ms
step:1558/2315 train_time:94289ms step_avg:60.52ms
step:1559/2315 train_time:94350ms step_avg:60.52ms
step:1560/2315 train_time:94411ms step_avg:60.52ms
step:1561/2315 train_time:94471ms step_avg:60.52ms
step:1562/2315 train_time:94533ms step_avg:60.52ms
step:1563/2315 train_time:94593ms step_avg:60.52ms
step:1564/2315 train_time:94655ms step_avg:60.52ms
step:1565/2315 train_time:94715ms step_avg:60.52ms
step:1566/2315 train_time:94777ms step_avg:60.52ms
step:1567/2315 train_time:94838ms step_avg:60.52ms
step:1568/2315 train_time:94900ms step_avg:60.52ms
step:1569/2315 train_time:94961ms step_avg:60.52ms
step:1570/2315 train_time:95024ms step_avg:60.52ms
step:1571/2315 train_time:95086ms step_avg:60.53ms
step:1572/2315 train_time:95147ms step_avg:60.53ms
step:1573/2315 train_time:95209ms step_avg:60.53ms
step:1574/2315 train_time:95270ms step_avg:60.53ms
step:1575/2315 train_time:95331ms step_avg:60.53ms
step:1576/2315 train_time:95392ms step_avg:60.53ms
step:1577/2315 train_time:95453ms step_avg:60.53ms
step:1578/2315 train_time:95514ms step_avg:60.53ms
step:1579/2315 train_time:95574ms step_avg:60.53ms
step:1580/2315 train_time:95635ms step_avg:60.53ms
step:1581/2315 train_time:95696ms step_avg:60.53ms
step:1582/2315 train_time:95758ms step_avg:60.53ms
step:1583/2315 train_time:95819ms step_avg:60.53ms
step:1584/2315 train_time:95881ms step_avg:60.53ms
step:1585/2315 train_time:95942ms step_avg:60.53ms
step:1586/2315 train_time:96005ms step_avg:60.53ms
step:1587/2315 train_time:96066ms step_avg:60.53ms
step:1588/2315 train_time:96128ms step_avg:60.53ms
step:1589/2315 train_time:96189ms step_avg:60.53ms
step:1590/2315 train_time:96251ms step_avg:60.54ms
step:1591/2315 train_time:96312ms step_avg:60.54ms
step:1592/2315 train_time:96373ms step_avg:60.54ms
step:1593/2315 train_time:96434ms step_avg:60.54ms
step:1594/2315 train_time:96495ms step_avg:60.54ms
step:1595/2315 train_time:96556ms step_avg:60.54ms
step:1596/2315 train_time:96617ms step_avg:60.54ms
step:1597/2315 train_time:96678ms step_avg:60.54ms
step:1598/2315 train_time:96740ms step_avg:60.54ms
step:1599/2315 train_time:96800ms step_avg:60.54ms
step:1600/2315 train_time:96863ms step_avg:60.54ms
step:1601/2315 train_time:96923ms step_avg:60.54ms
step:1602/2315 train_time:96985ms step_avg:60.54ms
step:1603/2315 train_time:97046ms step_avg:60.54ms
step:1604/2315 train_time:97108ms step_avg:60.54ms
step:1605/2315 train_time:97170ms step_avg:60.54ms
step:1606/2315 train_time:97231ms step_avg:60.54ms
step:1607/2315 train_time:97292ms step_avg:60.54ms
step:1608/2315 train_time:97353ms step_avg:60.54ms
step:1609/2315 train_time:97414ms step_avg:60.54ms
step:1610/2315 train_time:97475ms step_avg:60.54ms
step:1611/2315 train_time:97536ms step_avg:60.54ms
step:1612/2315 train_time:97597ms step_avg:60.54ms
step:1613/2315 train_time:97659ms step_avg:60.54ms
step:1614/2315 train_time:97721ms step_avg:60.55ms
step:1615/2315 train_time:97782ms step_avg:60.55ms
step:1616/2315 train_time:97843ms step_avg:60.55ms
step:1617/2315 train_time:97904ms step_avg:60.55ms
step:1618/2315 train_time:97966ms step_avg:60.55ms
step:1619/2315 train_time:98027ms step_avg:60.55ms
step:1620/2315 train_time:98088ms step_avg:60.55ms
step:1621/2315 train_time:98149ms step_avg:60.55ms
step:1622/2315 train_time:98211ms step_avg:60.55ms
step:1623/2315 train_time:98271ms step_avg:60.55ms
step:1624/2315 train_time:98333ms step_avg:60.55ms
step:1625/2315 train_time:98393ms step_avg:60.55ms
step:1626/2315 train_time:98455ms step_avg:60.55ms
step:1627/2315 train_time:98515ms step_avg:60.55ms
step:1628/2315 train_time:98577ms step_avg:60.55ms
step:1629/2315 train_time:98637ms step_avg:60.55ms
step:1630/2315 train_time:98698ms step_avg:60.55ms
step:1631/2315 train_time:98760ms step_avg:60.55ms
step:1632/2315 train_time:98822ms step_avg:60.55ms
step:1633/2315 train_time:98883ms step_avg:60.55ms
step:1634/2315 train_time:98945ms step_avg:60.55ms
step:1635/2315 train_time:99005ms step_avg:60.55ms
step:1636/2315 train_time:99068ms step_avg:60.55ms
step:1637/2315 train_time:99128ms step_avg:60.55ms
step:1638/2315 train_time:99189ms step_avg:60.56ms
step:1639/2315 train_time:99250ms step_avg:60.56ms
step:1640/2315 train_time:99312ms step_avg:60.56ms
step:1641/2315 train_time:99372ms step_avg:60.56ms
step:1642/2315 train_time:99434ms step_avg:60.56ms
step:1643/2315 train_time:99495ms step_avg:60.56ms
step:1644/2315 train_time:99556ms step_avg:60.56ms
step:1645/2315 train_time:99616ms step_avg:60.56ms
step:1646/2315 train_time:99678ms step_avg:60.56ms
step:1647/2315 train_time:99739ms step_avg:60.56ms
step:1648/2315 train_time:99802ms step_avg:60.56ms
step:1649/2315 train_time:99863ms step_avg:60.56ms
step:1650/2315 train_time:99925ms step_avg:60.56ms
step:1651/2315 train_time:99986ms step_avg:60.56ms
step:1652/2315 train_time:100048ms step_avg:60.56ms
step:1653/2315 train_time:100110ms step_avg:60.56ms
step:1654/2315 train_time:100171ms step_avg:60.56ms
step:1655/2315 train_time:100231ms step_avg:60.56ms
step:1656/2315 train_time:100293ms step_avg:60.56ms
step:1657/2315 train_time:100353ms step_avg:60.56ms
step:1658/2315 train_time:100415ms step_avg:60.56ms
step:1659/2315 train_time:100475ms step_avg:60.56ms
step:1660/2315 train_time:100537ms step_avg:60.56ms
step:1661/2315 train_time:100597ms step_avg:60.56ms
step:1662/2315 train_time:100658ms step_avg:60.56ms
step:1663/2315 train_time:100719ms step_avg:60.56ms
step:1664/2315 train_time:100781ms step_avg:60.57ms
step:1665/2315 train_time:100842ms step_avg:60.57ms
step:1666/2315 train_time:100904ms step_avg:60.57ms
step:1667/2315 train_time:100966ms step_avg:60.57ms
step:1668/2315 train_time:101028ms step_avg:60.57ms
step:1669/2315 train_time:101089ms step_avg:60.57ms
step:1670/2315 train_time:101150ms step_avg:60.57ms
step:1671/2315 train_time:101212ms step_avg:60.57ms
step:1672/2315 train_time:101274ms step_avg:60.57ms
step:1673/2315 train_time:101335ms step_avg:60.57ms
step:1674/2315 train_time:101396ms step_avg:60.57ms
step:1675/2315 train_time:101456ms step_avg:60.57ms
step:1676/2315 train_time:101518ms step_avg:60.57ms
step:1677/2315 train_time:101578ms step_avg:60.57ms
step:1678/2315 train_time:101640ms step_avg:60.57ms
step:1679/2315 train_time:101701ms step_avg:60.57ms
step:1680/2315 train_time:101763ms step_avg:60.57ms
step:1681/2315 train_time:101824ms step_avg:60.57ms
step:1682/2315 train_time:101886ms step_avg:60.57ms
step:1683/2315 train_time:101947ms step_avg:60.57ms
step:1684/2315 train_time:102009ms step_avg:60.58ms
step:1685/2315 train_time:102070ms step_avg:60.58ms
step:1686/2315 train_time:102131ms step_avg:60.58ms
step:1687/2315 train_time:102192ms step_avg:60.58ms
step:1688/2315 train_time:102253ms step_avg:60.58ms
step:1689/2315 train_time:102315ms step_avg:60.58ms
step:1690/2315 train_time:102376ms step_avg:60.58ms
step:1691/2315 train_time:102437ms step_avg:60.58ms
step:1692/2315 train_time:102498ms step_avg:60.58ms
step:1693/2315 train_time:102559ms step_avg:60.58ms
step:1694/2315 train_time:102622ms step_avg:60.58ms
step:1695/2315 train_time:102683ms step_avg:60.58ms
step:1696/2315 train_time:102745ms step_avg:60.58ms
step:1697/2315 train_time:102806ms step_avg:60.58ms
step:1698/2315 train_time:102867ms step_avg:60.58ms
step:1699/2315 train_time:102929ms step_avg:60.58ms
step:1700/2315 train_time:102991ms step_avg:60.58ms
step:1701/2315 train_time:103051ms step_avg:60.58ms
step:1702/2315 train_time:103113ms step_avg:60.58ms
step:1703/2315 train_time:103173ms step_avg:60.58ms
step:1704/2315 train_time:103234ms step_avg:60.58ms
step:1705/2315 train_time:103295ms step_avg:60.58ms
step:1706/2315 train_time:103357ms step_avg:60.58ms
step:1707/2315 train_time:103418ms step_avg:60.58ms
step:1708/2315 train_time:103479ms step_avg:60.59ms
step:1709/2315 train_time:103540ms step_avg:60.59ms
step:1710/2315 train_time:103602ms step_avg:60.59ms
step:1711/2315 train_time:103663ms step_avg:60.59ms
step:1712/2315 train_time:103726ms step_avg:60.59ms
step:1713/2315 train_time:103787ms step_avg:60.59ms
step:1714/2315 train_time:103848ms step_avg:60.59ms
step:1715/2315 train_time:103909ms step_avg:60.59ms
step:1716/2315 train_time:103971ms step_avg:60.59ms
step:1717/2315 train_time:104032ms step_avg:60.59ms
step:1718/2315 train_time:104093ms step_avg:60.59ms
step:1719/2315 train_time:104153ms step_avg:60.59ms
step:1720/2315 train_time:104214ms step_avg:60.59ms
step:1721/2315 train_time:104275ms step_avg:60.59ms
step:1722/2315 train_time:104336ms step_avg:60.59ms
step:1723/2315 train_time:104396ms step_avg:60.59ms
step:1724/2315 train_time:104458ms step_avg:60.59ms
step:1725/2315 train_time:104519ms step_avg:60.59ms
step:1726/2315 train_time:104580ms step_avg:60.59ms
step:1727/2315 train_time:104642ms step_avg:60.59ms
step:1728/2315 train_time:104705ms step_avg:60.59ms
step:1729/2315 train_time:104766ms step_avg:60.59ms
step:1730/2315 train_time:104827ms step_avg:60.59ms
step:1731/2315 train_time:104888ms step_avg:60.59ms
step:1732/2315 train_time:104950ms step_avg:60.59ms
step:1733/2315 train_time:105012ms step_avg:60.60ms
step:1734/2315 train_time:105073ms step_avg:60.60ms
step:1735/2315 train_time:105134ms step_avg:60.60ms
step:1736/2315 train_time:105195ms step_avg:60.60ms
step:1737/2315 train_time:105255ms step_avg:60.60ms
step:1738/2315 train_time:105317ms step_avg:60.60ms
step:1739/2315 train_time:105378ms step_avg:60.60ms
step:1740/2315 train_time:105440ms step_avg:60.60ms
step:1741/2315 train_time:105501ms step_avg:60.60ms
step:1742/2315 train_time:105563ms step_avg:60.60ms
step:1743/2315 train_time:105625ms step_avg:60.60ms
step:1744/2315 train_time:105686ms step_avg:60.60ms
step:1745/2315 train_time:105747ms step_avg:60.60ms
step:1746/2315 train_time:105809ms step_avg:60.60ms
step:1747/2315 train_time:105870ms step_avg:60.60ms
step:1748/2315 train_time:105932ms step_avg:60.60ms
step:1749/2315 train_time:105993ms step_avg:60.60ms
step:1750/2315 train_time:106054ms step_avg:60.60ms
step:1750/2315 val_loss:3.3803 train_time:106116ms step_avg:60.64ms
step:1751/2315 train_time:106135ms step_avg:60.61ms
step:1752/2315 train_time:106179ms step_avg:60.60ms
step:1753/2315 train_time:106246ms step_avg:60.61ms
step:1754/2315 train_time:106309ms step_avg:60.61ms
step:1755/2315 train_time:106369ms step_avg:60.61ms
step:1756/2315 train_time:106431ms step_avg:60.61ms
step:1757/2315 train_time:106490ms step_avg:60.61ms
step:1758/2315 train_time:106551ms step_avg:60.61ms
step:1759/2315 train_time:106611ms step_avg:60.61ms
step:1760/2315 train_time:106672ms step_avg:60.61ms
step:1761/2315 train_time:106732ms step_avg:60.61ms
step:1762/2315 train_time:106793ms step_avg:60.61ms
step:1763/2315 train_time:106852ms step_avg:60.61ms
step:1764/2315 train_time:106913ms step_avg:60.61ms
step:1765/2315 train_time:106972ms step_avg:60.61ms
step:1766/2315 train_time:107035ms step_avg:60.61ms
step:1767/2315 train_time:107098ms step_avg:60.61ms
step:1768/2315 train_time:107161ms step_avg:60.61ms
step:1769/2315 train_time:107224ms step_avg:60.61ms
step:1770/2315 train_time:107286ms step_avg:60.61ms
step:1771/2315 train_time:107347ms step_avg:60.61ms
step:1772/2315 train_time:107409ms step_avg:60.61ms
step:1773/2315 train_time:107469ms step_avg:60.61ms
step:1774/2315 train_time:107530ms step_avg:60.61ms
step:1775/2315 train_time:107591ms step_avg:60.61ms
step:1776/2315 train_time:107651ms step_avg:60.61ms
step:1777/2315 train_time:107711ms step_avg:60.61ms
step:1778/2315 train_time:107772ms step_avg:60.61ms
step:1779/2315 train_time:107832ms step_avg:60.61ms
step:1780/2315 train_time:107892ms step_avg:60.61ms
step:1781/2315 train_time:107953ms step_avg:60.61ms
step:1782/2315 train_time:108014ms step_avg:60.61ms
step:1783/2315 train_time:108076ms step_avg:60.61ms
step:1784/2315 train_time:108139ms step_avg:60.62ms
step:1785/2315 train_time:108201ms step_avg:60.62ms
step:1786/2315 train_time:108263ms step_avg:60.62ms
step:1787/2315 train_time:108325ms step_avg:60.62ms
step:1788/2315 train_time:108387ms step_avg:60.62ms
step:1789/2315 train_time:108447ms step_avg:60.62ms
step:1790/2315 train_time:108509ms step_avg:60.62ms
step:1791/2315 train_time:108569ms step_avg:60.62ms
step:1792/2315 train_time:108630ms step_avg:60.62ms
step:1793/2315 train_time:108691ms step_avg:60.62ms
step:1794/2315 train_time:108751ms step_avg:60.62ms
step:1795/2315 train_time:108811ms step_avg:60.62ms
step:1796/2315 train_time:108872ms step_avg:60.62ms
step:1797/2315 train_time:108932ms step_avg:60.62ms
step:1798/2315 train_time:108994ms step_avg:60.62ms
step:1799/2315 train_time:109054ms step_avg:60.62ms
step:1800/2315 train_time:109116ms step_avg:60.62ms
step:1801/2315 train_time:109178ms step_avg:60.62ms
step:1802/2315 train_time:109241ms step_avg:60.62ms
step:1803/2315 train_time:109303ms step_avg:60.62ms
step:1804/2315 train_time:109366ms step_avg:60.62ms
step:1805/2315 train_time:109426ms step_avg:60.62ms
step:1806/2315 train_time:109487ms step_avg:60.62ms
step:1807/2315 train_time:109547ms step_avg:60.62ms
step:1808/2315 train_time:109609ms step_avg:60.62ms
step:1809/2315 train_time:109669ms step_avg:60.62ms
step:1810/2315 train_time:109730ms step_avg:60.62ms
step:1811/2315 train_time:109791ms step_avg:60.62ms
step:1812/2315 train_time:109852ms step_avg:60.62ms
step:1813/2315 train_time:109912ms step_avg:60.62ms
step:1814/2315 train_time:109973ms step_avg:60.62ms
step:1815/2315 train_time:110033ms step_avg:60.62ms
step:1816/2315 train_time:110095ms step_avg:60.63ms
step:1817/2315 train_time:110157ms step_avg:60.63ms
step:1818/2315 train_time:110218ms step_avg:60.63ms
step:1819/2315 train_time:110280ms step_avg:60.63ms
step:1820/2315 train_time:110342ms step_avg:60.63ms
step:1821/2315 train_time:110403ms step_avg:60.63ms
step:1822/2315 train_time:110465ms step_avg:60.63ms
step:1823/2315 train_time:110526ms step_avg:60.63ms
step:1824/2315 train_time:110587ms step_avg:60.63ms
step:1825/2315 train_time:110648ms step_avg:60.63ms
step:1826/2315 train_time:110709ms step_avg:60.63ms
step:1827/2315 train_time:110770ms step_avg:60.63ms
step:1828/2315 train_time:110831ms step_avg:60.63ms
step:1829/2315 train_time:110892ms step_avg:60.63ms
step:1830/2315 train_time:110953ms step_avg:60.63ms
step:1831/2315 train_time:111014ms step_avg:60.63ms
step:1832/2315 train_time:111075ms step_avg:60.63ms
step:1833/2315 train_time:111136ms step_avg:60.63ms
step:1834/2315 train_time:111198ms step_avg:60.63ms
step:1835/2315 train_time:111259ms step_avg:60.63ms
step:1836/2315 train_time:111321ms step_avg:60.63ms
step:1837/2315 train_time:111382ms step_avg:60.63ms
step:1838/2315 train_time:111444ms step_avg:60.63ms
step:1839/2315 train_time:111505ms step_avg:60.63ms
step:1840/2315 train_time:111567ms step_avg:60.63ms
step:1841/2315 train_time:111628ms step_avg:60.63ms
step:1842/2315 train_time:111689ms step_avg:60.63ms
step:1843/2315 train_time:111750ms step_avg:60.63ms
step:1844/2315 train_time:111811ms step_avg:60.64ms
step:1845/2315 train_time:111872ms step_avg:60.64ms
step:1846/2315 train_time:111933ms step_avg:60.64ms
step:1847/2315 train_time:111993ms step_avg:60.64ms
step:1848/2315 train_time:112054ms step_avg:60.64ms
step:1849/2315 train_time:112115ms step_avg:60.64ms
step:1850/2315 train_time:112177ms step_avg:60.64ms
step:1851/2315 train_time:112238ms step_avg:60.64ms
step:1852/2315 train_time:112300ms step_avg:60.64ms
step:1853/2315 train_time:112361ms step_avg:60.64ms
step:1854/2315 train_time:112423ms step_avg:60.64ms
step:1855/2315 train_time:112485ms step_avg:60.64ms
step:1856/2315 train_time:112547ms step_avg:60.64ms
step:1857/2315 train_time:112607ms step_avg:60.64ms
step:1858/2315 train_time:112669ms step_avg:60.64ms
step:1859/2315 train_time:112730ms step_avg:60.64ms
step:1860/2315 train_time:112791ms step_avg:60.64ms
step:1861/2315 train_time:112851ms step_avg:60.64ms
step:1862/2315 train_time:112913ms step_avg:60.64ms
step:1863/2315 train_time:112973ms step_avg:60.64ms
step:1864/2315 train_time:113035ms step_avg:60.64ms
step:1865/2315 train_time:113096ms step_avg:60.64ms
step:1866/2315 train_time:113157ms step_avg:60.64ms
step:1867/2315 train_time:113219ms step_avg:60.64ms
step:1868/2315 train_time:113281ms step_avg:60.64ms
step:1869/2315 train_time:113342ms step_avg:60.64ms
step:1870/2315 train_time:113404ms step_avg:60.64ms
step:1871/2315 train_time:113465ms step_avg:60.64ms
step:1872/2315 train_time:113527ms step_avg:60.64ms
step:1873/2315 train_time:113588ms step_avg:60.64ms
step:1874/2315 train_time:113649ms step_avg:60.65ms
step:1875/2315 train_time:113710ms step_avg:60.65ms
step:1876/2315 train_time:113771ms step_avg:60.65ms
step:1877/2315 train_time:113832ms step_avg:60.65ms
step:1878/2315 train_time:113893ms step_avg:60.65ms
step:1879/2315 train_time:113953ms step_avg:60.65ms
step:1880/2315 train_time:114014ms step_avg:60.65ms
step:1881/2315 train_time:114075ms step_avg:60.65ms
step:1882/2315 train_time:114138ms step_avg:60.65ms
step:1883/2315 train_time:114198ms step_avg:60.65ms
step:1884/2315 train_time:114260ms step_avg:60.65ms
step:1885/2315 train_time:114321ms step_avg:60.65ms
step:1886/2315 train_time:114383ms step_avg:60.65ms
step:1887/2315 train_time:114444ms step_avg:60.65ms
step:1888/2315 train_time:114507ms step_avg:60.65ms
step:1889/2315 train_time:114568ms step_avg:60.65ms
step:1890/2315 train_time:114630ms step_avg:60.65ms
step:1891/2315 train_time:114691ms step_avg:60.65ms
step:1892/2315 train_time:114753ms step_avg:60.65ms
step:1893/2315 train_time:114814ms step_avg:60.65ms
step:1894/2315 train_time:114875ms step_avg:60.65ms
step:1895/2315 train_time:114935ms step_avg:60.65ms
step:1896/2315 train_time:114997ms step_avg:60.65ms
step:1897/2315 train_time:115058ms step_avg:60.65ms
step:1898/2315 train_time:115120ms step_avg:60.65ms
step:1899/2315 train_time:115181ms step_avg:60.65ms
step:1900/2315 train_time:115243ms step_avg:60.65ms
step:1901/2315 train_time:115304ms step_avg:60.65ms
step:1902/2315 train_time:115365ms step_avg:60.65ms
step:1903/2315 train_time:115426ms step_avg:60.65ms
step:1904/2315 train_time:115488ms step_avg:60.66ms
step:1905/2315 train_time:115548ms step_avg:60.66ms
step:1906/2315 train_time:115610ms step_avg:60.66ms
step:1907/2315 train_time:115671ms step_avg:60.66ms
step:1908/2315 train_time:115732ms step_avg:60.66ms
step:1909/2315 train_time:115793ms step_avg:60.66ms
step:1910/2315 train_time:115854ms step_avg:60.66ms
step:1911/2315 train_time:115914ms step_avg:60.66ms
step:1912/2315 train_time:115976ms step_avg:60.66ms
step:1913/2315 train_time:116036ms step_avg:60.66ms
step:1914/2315 train_time:116097ms step_avg:60.66ms
step:1915/2315 train_time:116158ms step_avg:60.66ms
step:1916/2315 train_time:116220ms step_avg:60.66ms
step:1917/2315 train_time:116282ms step_avg:60.66ms
step:1918/2315 train_time:116344ms step_avg:60.66ms
step:1919/2315 train_time:116405ms step_avg:60.66ms
step:1920/2315 train_time:116467ms step_avg:60.66ms
step:1921/2315 train_time:116528ms step_avg:60.66ms
step:1922/2315 train_time:116590ms step_avg:60.66ms
step:1923/2315 train_time:116651ms step_avg:60.66ms
step:1924/2315 train_time:116712ms step_avg:60.66ms
step:1925/2315 train_time:116772ms step_avg:60.66ms
step:1926/2315 train_time:116834ms step_avg:60.66ms
step:1927/2315 train_time:116895ms step_avg:60.66ms
step:1928/2315 train_time:116956ms step_avg:60.66ms
step:1929/2315 train_time:117017ms step_avg:60.66ms
step:1930/2315 train_time:117078ms step_avg:60.66ms
step:1931/2315 train_time:117139ms step_avg:60.66ms
step:1932/2315 train_time:117201ms step_avg:60.66ms
step:1933/2315 train_time:117263ms step_avg:60.66ms
step:1934/2315 train_time:117324ms step_avg:60.66ms
step:1935/2315 train_time:117387ms step_avg:60.66ms
step:1936/2315 train_time:117448ms step_avg:60.67ms
step:1937/2315 train_time:117508ms step_avg:60.67ms
step:1938/2315 train_time:117570ms step_avg:60.67ms
step:1939/2315 train_time:117631ms step_avg:60.67ms
step:1940/2315 train_time:117693ms step_avg:60.67ms
step:1941/2315 train_time:117753ms step_avg:60.67ms
step:1942/2315 train_time:117814ms step_avg:60.67ms
step:1943/2315 train_time:117875ms step_avg:60.67ms
step:1944/2315 train_time:117936ms step_avg:60.67ms
step:1945/2315 train_time:117996ms step_avg:60.67ms
step:1946/2315 train_time:118058ms step_avg:60.67ms
step:1947/2315 train_time:118118ms step_avg:60.67ms
step:1948/2315 train_time:118180ms step_avg:60.67ms
step:1949/2315 train_time:118242ms step_avg:60.67ms
step:1950/2315 train_time:118305ms step_avg:60.67ms
step:1951/2315 train_time:118366ms step_avg:60.67ms
step:1952/2315 train_time:118427ms step_avg:60.67ms
step:1953/2315 train_time:118487ms step_avg:60.67ms
step:1954/2315 train_time:118549ms step_avg:60.67ms
step:1955/2315 train_time:118610ms step_avg:60.67ms
step:1956/2315 train_time:118671ms step_avg:60.67ms
step:1957/2315 train_time:118732ms step_avg:60.67ms
step:1958/2315 train_time:118793ms step_avg:60.67ms
step:1959/2315 train_time:118854ms step_avg:60.67ms
step:1960/2315 train_time:118915ms step_avg:60.67ms
step:1961/2315 train_time:118976ms step_avg:60.67ms
step:1962/2315 train_time:119037ms step_avg:60.67ms
step:1963/2315 train_time:119097ms step_avg:60.67ms
step:1964/2315 train_time:119159ms step_avg:60.67ms
step:1965/2315 train_time:119221ms step_avg:60.67ms
step:1966/2315 train_time:119284ms step_avg:60.67ms
step:1967/2315 train_time:119346ms step_avg:60.67ms
step:1968/2315 train_time:119407ms step_avg:60.67ms
step:1969/2315 train_time:119467ms step_avg:60.67ms
step:1970/2315 train_time:119530ms step_avg:60.67ms
step:1971/2315 train_time:119591ms step_avg:60.68ms
step:1972/2315 train_time:119652ms step_avg:60.68ms
step:1973/2315 train_time:119712ms step_avg:60.68ms
step:1974/2315 train_time:119774ms step_avg:60.68ms
step:1975/2315 train_time:119834ms step_avg:60.68ms
step:1976/2315 train_time:119895ms step_avg:60.68ms
step:1977/2315 train_time:119955ms step_avg:60.68ms
step:1978/2315 train_time:120017ms step_avg:60.68ms
step:1979/2315 train_time:120077ms step_avg:60.68ms
step:1980/2315 train_time:120138ms step_avg:60.68ms
step:1981/2315 train_time:120200ms step_avg:60.68ms
step:1982/2315 train_time:120262ms step_avg:60.68ms
step:1983/2315 train_time:120324ms step_avg:60.68ms
step:1984/2315 train_time:120386ms step_avg:60.68ms
step:1985/2315 train_time:120447ms step_avg:60.68ms
step:1986/2315 train_time:120508ms step_avg:60.68ms
step:1987/2315 train_time:120569ms step_avg:60.68ms
step:1988/2315 train_time:120631ms step_avg:60.68ms
step:1989/2315 train_time:120692ms step_avg:60.68ms
step:1990/2315 train_time:120753ms step_avg:60.68ms
step:1991/2315 train_time:120814ms step_avg:60.68ms
step:1992/2315 train_time:120875ms step_avg:60.68ms
step:1993/2315 train_time:120936ms step_avg:60.68ms
step:1994/2315 train_time:120997ms step_avg:60.68ms
step:1995/2315 train_time:121058ms step_avg:60.68ms
step:1996/2315 train_time:121119ms step_avg:60.68ms
step:1997/2315 train_time:121181ms step_avg:60.68ms
step:1998/2315 train_time:121243ms step_avg:60.68ms
step:1999/2315 train_time:121304ms step_avg:60.68ms
step:2000/2315 train_time:121366ms step_avg:60.68ms
step:2000/2315 val_loss:3.3302 train_time:121428ms step_avg:60.71ms
step:2001/2315 train_time:121446ms step_avg:60.69ms
step:2002/2315 train_time:121490ms step_avg:60.68ms
step:2003/2315 train_time:121553ms step_avg:60.69ms
step:2004/2315 train_time:121617ms step_avg:60.69ms
step:2005/2315 train_time:121678ms step_avg:60.69ms
step:2006/2315 train_time:121739ms step_avg:60.69ms
step:2007/2315 train_time:121799ms step_avg:60.69ms
step:2008/2315 train_time:121860ms step_avg:60.69ms
step:2009/2315 train_time:121920ms step_avg:60.69ms
step:2010/2315 train_time:121980ms step_avg:60.69ms
step:2011/2315 train_time:122041ms step_avg:60.69ms
step:2012/2315 train_time:122102ms step_avg:60.69ms
step:2013/2315 train_time:122163ms step_avg:60.69ms
step:2014/2315 train_time:122225ms step_avg:60.69ms
step:2015/2315 train_time:122285ms step_avg:60.69ms
step:2016/2315 train_time:122348ms step_avg:60.69ms
step:2017/2315 train_time:122410ms step_avg:60.69ms
step:2018/2315 train_time:122473ms step_avg:60.69ms
step:2019/2315 train_time:122535ms step_avg:60.69ms
step:2020/2315 train_time:122597ms step_avg:60.69ms
step:2021/2315 train_time:122658ms step_avg:60.69ms
step:2022/2315 train_time:122719ms step_avg:60.69ms
step:2023/2315 train_time:122780ms step_avg:60.69ms
step:2024/2315 train_time:122841ms step_avg:60.69ms
step:2025/2315 train_time:122901ms step_avg:60.69ms
step:2026/2315 train_time:122963ms step_avg:60.69ms
step:2027/2315 train_time:123023ms step_avg:60.69ms
step:2028/2315 train_time:123084ms step_avg:60.69ms
step:2029/2315 train_time:123145ms step_avg:60.69ms
step:2030/2315 train_time:123206ms step_avg:60.69ms
step:2031/2315 train_time:123267ms step_avg:60.69ms
step:2032/2315 train_time:123329ms step_avg:60.69ms
step:2033/2315 train_time:123392ms step_avg:60.69ms
step:2034/2315 train_time:123454ms step_avg:60.70ms
step:2035/2315 train_time:123515ms step_avg:60.70ms
step:2036/2315 train_time:123577ms step_avg:60.70ms
step:2037/2315 train_time:123638ms step_avg:60.70ms
step:2038/2315 train_time:123700ms step_avg:60.70ms
step:2039/2315 train_time:123761ms step_avg:60.70ms
step:2040/2315 train_time:123822ms step_avg:60.70ms
step:2041/2315 train_time:123883ms step_avg:60.70ms
step:2042/2315 train_time:123944ms step_avg:60.70ms
step:2043/2315 train_time:124004ms step_avg:60.70ms
step:2044/2315 train_time:124066ms step_avg:60.70ms
step:2045/2315 train_time:124127ms step_avg:60.70ms
step:2046/2315 train_time:124188ms step_avg:60.70ms
step:2047/2315 train_time:124249ms step_avg:60.70ms
step:2048/2315 train_time:124312ms step_avg:60.70ms
step:2049/2315 train_time:124374ms step_avg:60.70ms
step:2050/2315 train_time:124436ms step_avg:60.70ms
step:2051/2315 train_time:124497ms step_avg:60.70ms
step:2052/2315 train_time:124559ms step_avg:60.70ms
step:2053/2315 train_time:124620ms step_avg:60.70ms
step:2054/2315 train_time:124682ms step_avg:60.70ms
step:2055/2315 train_time:124743ms step_avg:60.70ms
step:2056/2315 train_time:124805ms step_avg:60.70ms
step:2057/2315 train_time:124865ms step_avg:60.70ms
step:2058/2315 train_time:124927ms step_avg:60.70ms
step:2059/2315 train_time:124988ms step_avg:60.70ms
step:2060/2315 train_time:125050ms step_avg:60.70ms
step:2061/2315 train_time:125111ms step_avg:60.70ms
step:2062/2315 train_time:125172ms step_avg:60.70ms
step:2063/2315 train_time:125233ms step_avg:60.70ms
step:2064/2315 train_time:125294ms step_avg:60.70ms
step:2065/2315 train_time:125355ms step_avg:60.70ms
step:2066/2315 train_time:125417ms step_avg:60.71ms
step:2067/2315 train_time:125478ms step_avg:60.71ms
step:2068/2315 train_time:125540ms step_avg:60.71ms
step:2069/2315 train_time:125600ms step_avg:60.71ms
step:2070/2315 train_time:125663ms step_avg:60.71ms
step:2071/2315 train_time:125724ms step_avg:60.71ms
step:2072/2315 train_time:125785ms step_avg:60.71ms
step:2073/2315 train_time:125846ms step_avg:60.71ms
step:2074/2315 train_time:125907ms step_avg:60.71ms
step:2075/2315 train_time:125968ms step_avg:60.71ms
step:2076/2315 train_time:126029ms step_avg:60.71ms
step:2077/2315 train_time:126090ms step_avg:60.71ms
step:2078/2315 train_time:126152ms step_avg:60.71ms
step:2079/2315 train_time:126213ms step_avg:60.71ms
step:2080/2315 train_time:126274ms step_avg:60.71ms
step:2081/2315 train_time:126335ms step_avg:60.71ms
step:2082/2315 train_time:126396ms step_avg:60.71ms
step:2083/2315 train_time:126457ms step_avg:60.71ms
step:2084/2315 train_time:126519ms step_avg:60.71ms
step:2085/2315 train_time:126580ms step_avg:60.71ms
step:2086/2315 train_time:126642ms step_avg:60.71ms
step:2087/2315 train_time:126702ms step_avg:60.71ms
step:2088/2315 train_time:126764ms step_avg:60.71ms
step:2089/2315 train_time:126825ms step_avg:60.71ms
step:2090/2315 train_time:126886ms step_avg:60.71ms
step:2091/2315 train_time:126948ms step_avg:60.71ms
step:2092/2315 train_time:127009ms step_avg:60.71ms
step:2093/2315 train_time:127070ms step_avg:60.71ms
step:2094/2315 train_time:127132ms step_avg:60.71ms
step:2095/2315 train_time:127193ms step_avg:60.71ms
step:2096/2315 train_time:127254ms step_avg:60.71ms
step:2097/2315 train_time:127315ms step_avg:60.71ms
step:2098/2315 train_time:127376ms step_avg:60.71ms
step:2099/2315 train_time:127437ms step_avg:60.71ms
step:2100/2315 train_time:127499ms step_avg:60.71ms
step:2101/2315 train_time:127560ms step_avg:60.71ms
step:2102/2315 train_time:127621ms step_avg:60.71ms
step:2103/2315 train_time:127682ms step_avg:60.71ms
step:2104/2315 train_time:127743ms step_avg:60.71ms
step:2105/2315 train_time:127804ms step_avg:60.71ms
step:2106/2315 train_time:127865ms step_avg:60.71ms
step:2107/2315 train_time:127927ms step_avg:60.72ms
step:2108/2315 train_time:127989ms step_avg:60.72ms
step:2109/2315 train_time:128051ms step_avg:60.72ms
step:2110/2315 train_time:128113ms step_avg:60.72ms
step:2111/2315 train_time:128173ms step_avg:60.72ms
step:2112/2315 train_time:128234ms step_avg:60.72ms
step:2113/2315 train_time:128295ms step_avg:60.72ms
step:2114/2315 train_time:128357ms step_avg:60.72ms
step:2115/2315 train_time:128418ms step_avg:60.72ms
step:2116/2315 train_time:128479ms step_avg:60.72ms
step:2117/2315 train_time:128540ms step_avg:60.72ms
step:2118/2315 train_time:128601ms step_avg:60.72ms
step:2119/2315 train_time:128662ms step_avg:60.72ms
step:2120/2315 train_time:128724ms step_avg:60.72ms
step:2121/2315 train_time:128785ms step_avg:60.72ms
step:2122/2315 train_time:128846ms step_avg:60.72ms
step:2123/2315 train_time:128907ms step_avg:60.72ms
step:2124/2315 train_time:128969ms step_avg:60.72ms
step:2125/2315 train_time:129031ms step_avg:60.72ms
step:2126/2315 train_time:129092ms step_avg:60.72ms
step:2127/2315 train_time:129153ms step_avg:60.72ms
step:2128/2315 train_time:129214ms step_avg:60.72ms
step:2129/2315 train_time:129275ms step_avg:60.72ms
step:2130/2315 train_time:129337ms step_avg:60.72ms
step:2131/2315 train_time:129398ms step_avg:60.72ms
step:2132/2315 train_time:129460ms step_avg:60.72ms
step:2133/2315 train_time:129521ms step_avg:60.72ms
step:2134/2315 train_time:129582ms step_avg:60.72ms
step:2135/2315 train_time:129643ms step_avg:60.72ms
step:2136/2315 train_time:129704ms step_avg:60.72ms
step:2137/2315 train_time:129765ms step_avg:60.72ms
step:2138/2315 train_time:129827ms step_avg:60.72ms
step:2139/2315 train_time:129888ms step_avg:60.72ms
step:2140/2315 train_time:129949ms step_avg:60.72ms
step:2141/2315 train_time:130010ms step_avg:60.72ms
step:2142/2315 train_time:130072ms step_avg:60.72ms
step:2143/2315 train_time:130133ms step_avg:60.72ms
step:2144/2315 train_time:130194ms step_avg:60.72ms
step:2145/2315 train_time:130255ms step_avg:60.72ms
step:2146/2315 train_time:130317ms step_avg:60.73ms
step:2147/2315 train_time:130378ms step_avg:60.73ms
step:2148/2315 train_time:130440ms step_avg:60.73ms
step:2149/2315 train_time:130501ms step_avg:60.73ms
step:2150/2315 train_time:130563ms step_avg:60.73ms
step:2151/2315 train_time:130624ms step_avg:60.73ms
step:2152/2315 train_time:130685ms step_avg:60.73ms
step:2153/2315 train_time:130746ms step_avg:60.73ms
step:2154/2315 train_time:130807ms step_avg:60.73ms
step:2155/2315 train_time:130869ms step_avg:60.73ms
step:2156/2315 train_time:130931ms step_avg:60.73ms
step:2157/2315 train_time:130992ms step_avg:60.73ms
step:2158/2315 train_time:131054ms step_avg:60.73ms
step:2159/2315 train_time:131115ms step_avg:60.73ms
step:2160/2315 train_time:131176ms step_avg:60.73ms
step:2161/2315 train_time:131237ms step_avg:60.73ms
step:2162/2315 train_time:131298ms step_avg:60.73ms
step:2163/2315 train_time:131359ms step_avg:60.73ms
step:2164/2315 train_time:131421ms step_avg:60.73ms
step:2165/2315 train_time:131481ms step_avg:60.73ms
step:2166/2315 train_time:131543ms step_avg:60.73ms
step:2167/2315 train_time:131603ms step_avg:60.73ms
step:2168/2315 train_time:131664ms step_avg:60.73ms
step:2169/2315 train_time:131725ms step_avg:60.73ms
step:2170/2315 train_time:131786ms step_avg:60.73ms
step:2171/2315 train_time:131847ms step_avg:60.73ms
step:2172/2315 train_time:131909ms step_avg:60.73ms
step:2173/2315 train_time:131971ms step_avg:60.73ms
step:2174/2315 train_time:132033ms step_avg:60.73ms
step:2175/2315 train_time:132094ms step_avg:60.73ms
step:2176/2315 train_time:132155ms step_avg:60.73ms
step:2177/2315 train_time:132216ms step_avg:60.73ms
step:2178/2315 train_time:132278ms step_avg:60.73ms
step:2179/2315 train_time:132340ms step_avg:60.73ms
step:2180/2315 train_time:132401ms step_avg:60.73ms
step:2181/2315 train_time:132462ms step_avg:60.73ms
step:2182/2315 train_time:132523ms step_avg:60.73ms
step:2183/2315 train_time:132585ms step_avg:60.74ms
step:2184/2315 train_time:132647ms step_avg:60.74ms
step:2185/2315 train_time:132707ms step_avg:60.74ms
step:2186/2315 train_time:132769ms step_avg:60.74ms
step:2187/2315 train_time:132830ms step_avg:60.74ms
step:2188/2315 train_time:132892ms step_avg:60.74ms
step:2189/2315 train_time:132953ms step_avg:60.74ms
step:2190/2315 train_time:133015ms step_avg:60.74ms
step:2191/2315 train_time:133076ms step_avg:60.74ms
step:2192/2315 train_time:133138ms step_avg:60.74ms
step:2193/2315 train_time:133199ms step_avg:60.74ms
step:2194/2315 train_time:133261ms step_avg:60.74ms
step:2195/2315 train_time:133321ms step_avg:60.74ms
step:2196/2315 train_time:133383ms step_avg:60.74ms
step:2197/2315 train_time:133444ms step_avg:60.74ms
step:2198/2315 train_time:133505ms step_avg:60.74ms
step:2199/2315 train_time:133566ms step_avg:60.74ms
step:2200/2315 train_time:133628ms step_avg:60.74ms
step:2201/2315 train_time:133689ms step_avg:60.74ms
step:2202/2315 train_time:133751ms step_avg:60.74ms
step:2203/2315 train_time:133812ms step_avg:60.74ms
step:2204/2315 train_time:133873ms step_avg:60.74ms
step:2205/2315 train_time:133934ms step_avg:60.74ms
step:2206/2315 train_time:133995ms step_avg:60.74ms
step:2207/2315 train_time:134056ms step_avg:60.74ms
step:2208/2315 train_time:134118ms step_avg:60.74ms
step:2209/2315 train_time:134179ms step_avg:60.74ms
step:2210/2315 train_time:134240ms step_avg:60.74ms
step:2211/2315 train_time:134301ms step_avg:60.74ms
step:2212/2315 train_time:134363ms step_avg:60.74ms
step:2213/2315 train_time:134425ms step_avg:60.74ms
step:2214/2315 train_time:134486ms step_avg:60.74ms
step:2215/2315 train_time:134547ms step_avg:60.74ms
step:2216/2315 train_time:134609ms step_avg:60.74ms
step:2217/2315 train_time:134670ms step_avg:60.74ms
step:2218/2315 train_time:134732ms step_avg:60.74ms
step:2219/2315 train_time:134793ms step_avg:60.75ms
step:2220/2315 train_time:134855ms step_avg:60.75ms
step:2221/2315 train_time:134915ms step_avg:60.75ms
step:2222/2315 train_time:134977ms step_avg:60.75ms
step:2223/2315 train_time:135038ms step_avg:60.75ms
step:2224/2315 train_time:135099ms step_avg:60.75ms
step:2225/2315 train_time:135160ms step_avg:60.75ms
step:2226/2315 train_time:135222ms step_avg:60.75ms
step:2227/2315 train_time:135282ms step_avg:60.75ms
step:2228/2315 train_time:135344ms step_avg:60.75ms
step:2229/2315 train_time:135405ms step_avg:60.75ms
step:2230/2315 train_time:135467ms step_avg:60.75ms
step:2231/2315 train_time:135528ms step_avg:60.75ms
step:2232/2315 train_time:135590ms step_avg:60.75ms
step:2233/2315 train_time:135651ms step_avg:60.75ms
step:2234/2315 train_time:135713ms step_avg:60.75ms
step:2235/2315 train_time:135774ms step_avg:60.75ms
step:2236/2315 train_time:135835ms step_avg:60.75ms
step:2237/2315 train_time:135896ms step_avg:60.75ms
step:2238/2315 train_time:135957ms step_avg:60.75ms
step:2239/2315 train_time:136018ms step_avg:60.75ms
step:2240/2315 train_time:136079ms step_avg:60.75ms
step:2241/2315 train_time:136140ms step_avg:60.75ms
step:2242/2315 train_time:136201ms step_avg:60.75ms
step:2243/2315 train_time:136262ms step_avg:60.75ms
step:2244/2315 train_time:136324ms step_avg:60.75ms
step:2245/2315 train_time:136385ms step_avg:60.75ms
step:2246/2315 train_time:136447ms step_avg:60.75ms
step:2247/2315 train_time:136509ms step_avg:60.75ms
step:2248/2315 train_time:136570ms step_avg:60.75ms
step:2249/2315 train_time:136632ms step_avg:60.75ms
step:2250/2315 train_time:136694ms step_avg:60.75ms
step:2250/2315 val_loss:3.2907 train_time:136756ms step_avg:60.78ms
step:2251/2315 train_time:136774ms step_avg:60.76ms
step:2252/2315 train_time:136820ms step_avg:60.75ms
step:2253/2315 train_time:136884ms step_avg:60.76ms
step:2254/2315 train_time:136947ms step_avg:60.76ms
step:2255/2315 train_time:137007ms step_avg:60.76ms
step:2256/2315 train_time:137069ms step_avg:60.76ms
step:2257/2315 train_time:137129ms step_avg:60.76ms
step:2258/2315 train_time:137190ms step_avg:60.76ms
step:2259/2315 train_time:137249ms step_avg:60.76ms
step:2260/2315 train_time:137310ms step_avg:60.76ms
step:2261/2315 train_time:137370ms step_avg:60.76ms
step:2262/2315 train_time:137431ms step_avg:60.76ms
step:2263/2315 train_time:137491ms step_avg:60.76ms
step:2264/2315 train_time:137552ms step_avg:60.76ms
step:2265/2315 train_time:137613ms step_avg:60.76ms
step:2266/2315 train_time:137675ms step_avg:60.76ms
step:2267/2315 train_time:137737ms step_avg:60.76ms
step:2268/2315 train_time:137801ms step_avg:60.76ms
step:2269/2315 train_time:137864ms step_avg:60.76ms
step:2270/2315 train_time:137927ms step_avg:60.76ms
step:2271/2315 train_time:137988ms step_avg:60.76ms
step:2272/2315 train_time:138049ms step_avg:60.76ms
step:2273/2315 train_time:138110ms step_avg:60.76ms
step:2274/2315 train_time:138171ms step_avg:60.76ms
step:2275/2315 train_time:138230ms step_avg:60.76ms
step:2276/2315 train_time:138291ms step_avg:60.76ms
step:2277/2315 train_time:138351ms step_avg:60.76ms
step:2278/2315 train_time:138412ms step_avg:60.76ms
step:2279/2315 train_time:138472ms step_avg:60.76ms
step:2280/2315 train_time:138533ms step_avg:60.76ms
step:2281/2315 train_time:138594ms step_avg:60.76ms
step:2282/2315 train_time:138657ms step_avg:60.76ms
step:2283/2315 train_time:138718ms step_avg:60.76ms
step:2284/2315 train_time:138781ms step_avg:60.76ms
step:2285/2315 train_time:138844ms step_avg:60.76ms
step:2286/2315 train_time:138906ms step_avg:60.76ms
step:2287/2315 train_time:138967ms step_avg:60.76ms
step:2288/2315 train_time:139029ms step_avg:60.76ms
step:2289/2315 train_time:139089ms step_avg:60.76ms
step:2290/2315 train_time:139151ms step_avg:60.76ms
step:2291/2315 train_time:139211ms step_avg:60.76ms
step:2292/2315 train_time:139272ms step_avg:60.76ms
step:2293/2315 train_time:139333ms step_avg:60.76ms
step:2294/2315 train_time:139394ms step_avg:60.76ms
step:2295/2315 train_time:139455ms step_avg:60.76ms
step:2296/2315 train_time:139515ms step_avg:60.76ms
step:2297/2315 train_time:139576ms step_avg:60.76ms
step:2298/2315 train_time:139637ms step_avg:60.76ms
step:2299/2315 train_time:139699ms step_avg:60.77ms
step:2300/2315 train_time:139761ms step_avg:60.77ms
step:2301/2315 train_time:139823ms step_avg:60.77ms
step:2302/2315 train_time:139885ms step_avg:60.77ms
step:2303/2315 train_time:139947ms step_avg:60.77ms
step:2304/2315 train_time:140008ms step_avg:60.77ms
step:2305/2315 train_time:140069ms step_avg:60.77ms
step:2306/2315 train_time:140130ms step_avg:60.77ms
step:2307/2315 train_time:140191ms step_avg:60.77ms
step:2308/2315 train_time:140253ms step_avg:60.77ms
step:2309/2315 train_time:140313ms step_avg:60.77ms
step:2310/2315 train_time:140374ms step_avg:60.77ms
step:2311/2315 train_time:140435ms step_avg:60.77ms
step:2312/2315 train_time:140496ms step_avg:60.77ms
step:2313/2315 train_time:140557ms step_avg:60.77ms
step:2314/2315 train_time:140619ms step_avg:60.77ms
step:2315/2315 train_time:140680ms step_avg:60.77ms
step:2315/2315 val_loss:3.2777 train_time:140743ms step_avg:60.80ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)


            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))



            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal
        self.group_to_param_group = {}

        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                self.group_to_param_group[param] = group

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        world_size = dist.get_world_size()
        grad = param.grad
        rank_size = grad.shape[0] // world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(), grad_slice)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']

            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
        # enable sync in the next training step for the adam optimizer
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = next(train_loader)

        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()


====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251031+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 31 15:47:17 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   35C    P0            126W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   30C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   33C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   34C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   33C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2315 train_time:70ms step_avg:70.31ms
step:2/2315 train_time:188ms step_avg:93.95ms
step:3/2315 train_time:206ms step_avg:68.79ms
step:4/2315 train_time:249ms step_avg:62.16ms
step:5/2315 train_time:306ms step_avg:61.25ms
step:6/2315 train_time:366ms step_avg:61.00ms
step:7/2315 train_time:425ms step_avg:60.71ms
step:8/2315 train_time:485ms step_avg:60.61ms
step:9/2315 train_time:544ms step_avg:60.39ms
step:10/2315 train_time:603ms step_avg:60.34ms
step:11/2315 train_time:663ms step_avg:60.23ms
step:12/2315 train_time:723ms step_avg:60.21ms
step:13/2315 train_time:782ms step_avg:60.12ms
step:14/2315 train_time:842ms step_avg:60.12ms
step:15/2315 train_time:901ms step_avg:60.10ms
step:16/2315 train_time:961ms step_avg:60.09ms
step:17/2315 train_time:1021ms step_avg:60.04ms
step:18/2315 train_time:1084ms step_avg:60.21ms
step:19/2315 train_time:1149ms step_avg:60.48ms
step:20/2315 train_time:1212ms step_avg:60.59ms
step:21/2315 train_time:1273ms step_avg:60.60ms
step:22/2315 train_time:1333ms step_avg:60.61ms
step:23/2315 train_time:1393ms step_avg:60.57ms
step:24/2315 train_time:1454ms step_avg:60.56ms
step:25/2315 train_time:1514ms step_avg:60.55ms
step:26/2315 train_time:1574ms step_avg:60.54ms
step:27/2315 train_time:1633ms step_avg:60.49ms
step:28/2315 train_time:1693ms step_avg:60.48ms
step:29/2315 train_time:1753ms step_avg:60.45ms
step:30/2315 train_time:1813ms step_avg:60.44ms
step:31/2315 train_time:1873ms step_avg:60.42ms
step:32/2315 train_time:1933ms step_avg:60.42ms
step:33/2315 train_time:1994ms step_avg:60.41ms
step:34/2315 train_time:2055ms step_avg:60.43ms
step:35/2315 train_time:2116ms step_avg:60.45ms
step:36/2315 train_time:2178ms step_avg:60.50ms
step:37/2315 train_time:2239ms step_avg:60.51ms
step:38/2315 train_time:2300ms step_avg:60.54ms
step:39/2315 train_time:2361ms step_avg:60.53ms
step:40/2315 train_time:2422ms step_avg:60.55ms
step:41/2315 train_time:2482ms step_avg:60.53ms
step:42/2315 train_time:2543ms step_avg:60.54ms
step:43/2315 train_time:2602ms step_avg:60.52ms
step:44/2315 train_time:2663ms step_avg:60.52ms
step:45/2315 train_time:2724ms step_avg:60.53ms
step:46/2315 train_time:2784ms step_avg:60.53ms
step:47/2315 train_time:2843ms step_avg:60.50ms
step:48/2315 train_time:2904ms step_avg:60.50ms
step:49/2315 train_time:2964ms step_avg:60.48ms
step:50/2315 train_time:3025ms step_avg:60.49ms
step:51/2315 train_time:3086ms step_avg:60.50ms
step:52/2315 train_time:3146ms step_avg:60.50ms
step:53/2315 train_time:3207ms step_avg:60.51ms
step:54/2315 train_time:3268ms step_avg:60.51ms
step:55/2315 train_time:3327ms step_avg:60.49ms
step:56/2315 train_time:3387ms step_avg:60.48ms
step:57/2315 train_time:3446ms step_avg:60.46ms
step:58/2315 train_time:3507ms step_avg:60.46ms
step:59/2315 train_time:3566ms step_avg:60.44ms
step:60/2315 train_time:3627ms step_avg:60.44ms
step:61/2315 train_time:3687ms step_avg:60.44ms
step:62/2315 train_time:3746ms step_avg:60.42ms
step:63/2315 train_time:3806ms step_avg:60.40ms
step:64/2315 train_time:3865ms step_avg:60.40ms
step:65/2315 train_time:3925ms step_avg:60.38ms
step:66/2315 train_time:3985ms step_avg:60.38ms
step:67/2315 train_time:4045ms step_avg:60.37ms
step:68/2315 train_time:4105ms step_avg:60.37ms
step:69/2315 train_time:4166ms step_avg:60.38ms
step:70/2315 train_time:4226ms step_avg:60.38ms
step:71/2315 train_time:4286ms step_avg:60.36ms
step:72/2315 train_time:4346ms step_avg:60.36ms
step:73/2315 train_time:4406ms step_avg:60.35ms
step:74/2315 train_time:4466ms step_avg:60.35ms
step:75/2315 train_time:4526ms step_avg:60.35ms
step:76/2315 train_time:4586ms step_avg:60.34ms
step:77/2315 train_time:4645ms step_avg:60.33ms
step:78/2315 train_time:4706ms step_avg:60.33ms
step:79/2315 train_time:4765ms step_avg:60.32ms
step:80/2315 train_time:4825ms step_avg:60.32ms
step:81/2315 train_time:4885ms step_avg:60.30ms
step:82/2315 train_time:4946ms step_avg:60.31ms
step:83/2315 train_time:5005ms step_avg:60.30ms
step:84/2315 train_time:5065ms step_avg:60.30ms
step:85/2315 train_time:5125ms step_avg:60.30ms
step:86/2315 train_time:5186ms step_avg:60.30ms
step:87/2315 train_time:5245ms step_avg:60.29ms
step:88/2315 train_time:5305ms step_avg:60.29ms
step:89/2315 train_time:5366ms step_avg:60.29ms
step:90/2315 train_time:5426ms step_avg:60.29ms
step:91/2315 train_time:5486ms step_avg:60.28ms
step:92/2315 train_time:5546ms step_avg:60.28ms
step:93/2315 train_time:5605ms step_avg:60.27ms
step:94/2315 train_time:5665ms step_avg:60.27ms
step:95/2315 train_time:5725ms step_avg:60.26ms
step:96/2315 train_time:5785ms step_avg:60.26ms
step:97/2315 train_time:5844ms step_avg:60.25ms
step:98/2315 train_time:5904ms step_avg:60.24ms
step:99/2315 train_time:5965ms step_avg:60.25ms
step:100/2315 train_time:6025ms step_avg:60.25ms
step:101/2315 train_time:6085ms step_avg:60.24ms
step:102/2315 train_time:6145ms step_avg:60.24ms
step:103/2315 train_time:6205ms step_avg:60.24ms
step:104/2315 train_time:6265ms step_avg:60.24ms
step:105/2315 train_time:6325ms step_avg:60.24ms
step:106/2315 train_time:6385ms step_avg:60.24ms
step:107/2315 train_time:6445ms step_avg:60.23ms
step:108/2315 train_time:6505ms step_avg:60.23ms
step:109/2315 train_time:6565ms step_avg:60.23ms
step:110/2315 train_time:6624ms step_avg:60.22ms
step:111/2315 train_time:6684ms step_avg:60.22ms
step:112/2315 train_time:6745ms step_avg:60.22ms
step:113/2315 train_time:6804ms step_avg:60.22ms
step:114/2315 train_time:6864ms step_avg:60.21ms
step:115/2315 train_time:6925ms step_avg:60.22ms
step:116/2315 train_time:6985ms step_avg:60.22ms
step:117/2315 train_time:7044ms step_avg:60.21ms
step:118/2315 train_time:7104ms step_avg:60.21ms
step:119/2315 train_time:7165ms step_avg:60.21ms
step:120/2315 train_time:7225ms step_avg:60.21ms
step:121/2315 train_time:7285ms step_avg:60.21ms
step:122/2315 train_time:7346ms step_avg:60.21ms
step:123/2315 train_time:7405ms step_avg:60.21ms
step:124/2315 train_time:7465ms step_avg:60.20ms
step:125/2315 train_time:7525ms step_avg:60.20ms
step:126/2315 train_time:7585ms step_avg:60.20ms
step:127/2315 train_time:7645ms step_avg:60.20ms
step:128/2315 train_time:7705ms step_avg:60.19ms
step:129/2315 train_time:7764ms step_avg:60.19ms
step:130/2315 train_time:7824ms step_avg:60.18ms
step:131/2315 train_time:7884ms step_avg:60.18ms
step:132/2315 train_time:7944ms step_avg:60.18ms
step:133/2315 train_time:8004ms step_avg:60.18ms
step:134/2315 train_time:8064ms step_avg:60.18ms
step:135/2315 train_time:8123ms step_avg:60.17ms
step:136/2315 train_time:8184ms step_avg:60.18ms
step:137/2315 train_time:8244ms step_avg:60.17ms
step:138/2315 train_time:8304ms step_avg:60.18ms
step:139/2315 train_time:8364ms step_avg:60.18ms
step:140/2315 train_time:8424ms step_avg:60.17ms
step:141/2315 train_time:8485ms step_avg:60.17ms
step:142/2315 train_time:8545ms step_avg:60.18ms
step:143/2315 train_time:8605ms step_avg:60.17ms
step:144/2315 train_time:8665ms step_avg:60.17ms
step:145/2315 train_time:8724ms step_avg:60.17ms
step:146/2315 train_time:8784ms step_avg:60.17ms
step:147/2315 train_time:8844ms step_avg:60.17ms
step:148/2315 train_time:8904ms step_avg:60.16ms
step:149/2315 train_time:8964ms step_avg:60.16ms
step:150/2315 train_time:9024ms step_avg:60.16ms
step:151/2315 train_time:9084ms step_avg:60.16ms
step:152/2315 train_time:9145ms step_avg:60.16ms
step:153/2315 train_time:9204ms step_avg:60.16ms
step:154/2315 train_time:9264ms step_avg:60.16ms
step:155/2315 train_time:9325ms step_avg:60.16ms
step:156/2315 train_time:9385ms step_avg:60.16ms
step:157/2315 train_time:9445ms step_avg:60.16ms
step:158/2315 train_time:9505ms step_avg:60.16ms
step:159/2315 train_time:9564ms step_avg:60.15ms
step:160/2315 train_time:9624ms step_avg:60.15ms
step:161/2315 train_time:9684ms step_avg:60.15ms
step:162/2315 train_time:9744ms step_avg:60.15ms
step:163/2315 train_time:9804ms step_avg:60.15ms
step:164/2315 train_time:9864ms step_avg:60.14ms
step:165/2315 train_time:9923ms step_avg:60.14ms
step:166/2315 train_time:9983ms step_avg:60.14ms
step:167/2315 train_time:10042ms step_avg:60.13ms
step:168/2315 train_time:10103ms step_avg:60.14ms
step:169/2315 train_time:10163ms step_avg:60.13ms
step:170/2315 train_time:10223ms step_avg:60.14ms
step:171/2315 train_time:10284ms step_avg:60.14ms
step:172/2315 train_time:10344ms step_avg:60.14ms
step:173/2315 train_time:10404ms step_avg:60.14ms
step:174/2315 train_time:10464ms step_avg:60.14ms
step:175/2315 train_time:10523ms step_avg:60.13ms
step:176/2315 train_time:10584ms step_avg:60.14ms
step:177/2315 train_time:10644ms step_avg:60.14ms
step:178/2315 train_time:10704ms step_avg:60.14ms
step:179/2315 train_time:10764ms step_avg:60.13ms
step:180/2315 train_time:10824ms step_avg:60.13ms
step:181/2315 train_time:10884ms step_avg:60.13ms
step:182/2315 train_time:10944ms step_avg:60.13ms
step:183/2315 train_time:11003ms step_avg:60.13ms
step:184/2315 train_time:11064ms step_avg:60.13ms
step:185/2315 train_time:11124ms step_avg:60.13ms
step:186/2315 train_time:11185ms step_avg:60.13ms
step:187/2315 train_time:11244ms step_avg:60.13ms
step:188/2315 train_time:11304ms step_avg:60.13ms
step:189/2315 train_time:11364ms step_avg:60.13ms
step:190/2315 train_time:11424ms step_avg:60.13ms
step:191/2315 train_time:11484ms step_avg:60.12ms
step:192/2315 train_time:11544ms step_avg:60.13ms
step:193/2315 train_time:11604ms step_avg:60.12ms
step:194/2315 train_time:11664ms step_avg:60.12ms
step:195/2315 train_time:11724ms step_avg:60.12ms
step:196/2315 train_time:11784ms step_avg:60.12ms
step:197/2315 train_time:11843ms step_avg:60.12ms
step:198/2315 train_time:11903ms step_avg:60.12ms
step:199/2315 train_time:11963ms step_avg:60.11ms
step:200/2315 train_time:12023ms step_avg:60.11ms
step:201/2315 train_time:12083ms step_avg:60.11ms
step:202/2315 train_time:12144ms step_avg:60.12ms
step:203/2315 train_time:12204ms step_avg:60.12ms
step:204/2315 train_time:12264ms step_avg:60.12ms
step:205/2315 train_time:12324ms step_avg:60.12ms
step:206/2315 train_time:12385ms step_avg:60.12ms
step:207/2315 train_time:12445ms step_avg:60.12ms
step:208/2315 train_time:12505ms step_avg:60.12ms
step:209/2315 train_time:12565ms step_avg:60.12ms
step:210/2315 train_time:12625ms step_avg:60.12ms
step:211/2315 train_time:12685ms step_avg:60.12ms
step:212/2315 train_time:12745ms step_avg:60.12ms
step:213/2315 train_time:12804ms step_avg:60.11ms
step:214/2315 train_time:12864ms step_avg:60.11ms
step:215/2315 train_time:12924ms step_avg:60.11ms
step:216/2315 train_time:12984ms step_avg:60.11ms
step:217/2315 train_time:13044ms step_avg:60.11ms
step:218/2315 train_time:13105ms step_avg:60.11ms
step:219/2315 train_time:13164ms step_avg:60.11ms
step:220/2315 train_time:13225ms step_avg:60.11ms
step:221/2315 train_time:13285ms step_avg:60.11ms
step:222/2315 train_time:13345ms step_avg:60.11ms
step:223/2315 train_time:13404ms step_avg:60.11ms
step:224/2315 train_time:13464ms step_avg:60.11ms
step:225/2315 train_time:13524ms step_avg:60.11ms
step:226/2315 train_time:13585ms step_avg:60.11ms
step:227/2315 train_time:13644ms step_avg:60.10ms
step:228/2315 train_time:13704ms step_avg:60.10ms
step:229/2315 train_time:13763ms step_avg:60.10ms
step:230/2315 train_time:13824ms step_avg:60.10ms
step:231/2315 train_time:13884ms step_avg:60.10ms
step:232/2315 train_time:13944ms step_avg:60.10ms
step:233/2315 train_time:14004ms step_avg:60.10ms
step:234/2315 train_time:14064ms step_avg:60.10ms
step:235/2315 train_time:14124ms step_avg:60.10ms
step:236/2315 train_time:14184ms step_avg:60.10ms
step:237/2315 train_time:14245ms step_avg:60.10ms
step:238/2315 train_time:14305ms step_avg:60.10ms
step:239/2315 train_time:14365ms step_avg:60.10ms
step:240/2315 train_time:14425ms step_avg:60.10ms
step:241/2315 train_time:14485ms step_avg:60.10ms
step:242/2315 train_time:14545ms step_avg:60.10ms
step:243/2315 train_time:14604ms step_avg:60.10ms
step:244/2315 train_time:14664ms step_avg:60.10ms
step:245/2315 train_time:14723ms step_avg:60.10ms
step:246/2315 train_time:14784ms step_avg:60.10ms
step:247/2315 train_time:14843ms step_avg:60.09ms
step:248/2315 train_time:14904ms step_avg:60.10ms
step:249/2315 train_time:14964ms step_avg:60.10ms
step:250/2315 train_time:15024ms step_avg:60.10ms
step:250/2315 val_loss:4.0787 train_time:15086ms step_avg:60.34ms
step:251/2315 train_time:15105ms step_avg:60.18ms
step:252/2315 train_time:15150ms step_avg:60.12ms
step:253/2315 train_time:15216ms step_avg:60.14ms
step:254/2315 train_time:15281ms step_avg:60.16ms
step:255/2315 train_time:15341ms step_avg:60.16ms
step:256/2315 train_time:15401ms step_avg:60.16ms
step:257/2315 train_time:15461ms step_avg:60.16ms
step:258/2315 train_time:15521ms step_avg:60.16ms
step:259/2315 train_time:15580ms step_avg:60.15ms
step:260/2315 train_time:15639ms step_avg:60.15ms
step:261/2315 train_time:15698ms step_avg:60.15ms
step:262/2315 train_time:15757ms step_avg:60.14ms
step:263/2315 train_time:15816ms step_avg:60.14ms
step:264/2315 train_time:15875ms step_avg:60.13ms
step:265/2315 train_time:15934ms step_avg:60.13ms
step:266/2315 train_time:15996ms step_avg:60.13ms
step:267/2315 train_time:16055ms step_avg:60.13ms
step:268/2315 train_time:16117ms step_avg:60.14ms
step:269/2315 train_time:16179ms step_avg:60.15ms
step:270/2315 train_time:16242ms step_avg:60.15ms
step:271/2315 train_time:16303ms step_avg:60.16ms
step:272/2315 train_time:16364ms step_avg:60.16ms
step:273/2315 train_time:16424ms step_avg:60.16ms
step:274/2315 train_time:16484ms step_avg:60.16ms
step:275/2315 train_time:16543ms step_avg:60.16ms
step:276/2315 train_time:16603ms step_avg:60.15ms
step:277/2315 train_time:16661ms step_avg:60.15ms
step:278/2315 train_time:16721ms step_avg:60.15ms
step:279/2315 train_time:16780ms step_avg:60.14ms
step:280/2315 train_time:16839ms step_avg:60.14ms
step:281/2315 train_time:16898ms step_avg:60.13ms
step:282/2315 train_time:16958ms step_avg:60.13ms
step:283/2315 train_time:17018ms step_avg:60.13ms
step:284/2315 train_time:17078ms step_avg:60.14ms
step:285/2315 train_time:17139ms step_avg:60.14ms
step:286/2315 train_time:17201ms step_avg:60.14ms
step:287/2315 train_time:17262ms step_avg:60.15ms
step:288/2315 train_time:17323ms step_avg:60.15ms
step:289/2315 train_time:17382ms step_avg:60.15ms
step:290/2315 train_time:17442ms step_avg:60.15ms
step:291/2315 train_time:17502ms step_avg:60.14ms
step:292/2315 train_time:17562ms step_avg:60.14ms
step:293/2315 train_time:17622ms step_avg:60.14ms
step:294/2315 train_time:17681ms step_avg:60.14ms
step:295/2315 train_time:17740ms step_avg:60.13ms
step:296/2315 train_time:17799ms step_avg:60.13ms
step:297/2315 train_time:17858ms step_avg:60.13ms
step:298/2315 train_time:17918ms step_avg:60.13ms
step:299/2315 train_time:17978ms step_avg:60.13ms
step:300/2315 train_time:18039ms step_avg:60.13ms
step:301/2315 train_time:18099ms step_avg:60.13ms
step:302/2315 train_time:18160ms step_avg:60.13ms
step:303/2315 train_time:18221ms step_avg:60.14ms
step:304/2315 train_time:18282ms step_avg:60.14ms
step:305/2315 train_time:18343ms step_avg:60.14ms
step:306/2315 train_time:18404ms step_avg:60.14ms
step:307/2315 train_time:18463ms step_avg:60.14ms
step:308/2315 train_time:18522ms step_avg:60.14ms
step:309/2315 train_time:18582ms step_avg:60.14ms
step:310/2315 train_time:18642ms step_avg:60.14ms
step:311/2315 train_time:18701ms step_avg:60.13ms
step:312/2315 train_time:18761ms step_avg:60.13ms
step:313/2315 train_time:18820ms step_avg:60.13ms
step:314/2315 train_time:18880ms step_avg:60.13ms
step:315/2315 train_time:18939ms step_avg:60.12ms
step:316/2315 train_time:19000ms step_avg:60.13ms
step:317/2315 train_time:19059ms step_avg:60.12ms
step:318/2315 train_time:19120ms step_avg:60.13ms
step:319/2315 train_time:19180ms step_avg:60.13ms
step:320/2315 train_time:19241ms step_avg:60.13ms
step:321/2315 train_time:19301ms step_avg:60.13ms
step:322/2315 train_time:19362ms step_avg:60.13ms
step:323/2315 train_time:19422ms step_avg:60.13ms
step:324/2315 train_time:19482ms step_avg:60.13ms
step:325/2315 train_time:19542ms step_avg:60.13ms
step:326/2315 train_time:19602ms step_avg:60.13ms
step:327/2315 train_time:19661ms step_avg:60.13ms
step:328/2315 train_time:19721ms step_avg:60.12ms
step:329/2315 train_time:19780ms step_avg:60.12ms
step:330/2315 train_time:19840ms step_avg:60.12ms
step:331/2315 train_time:19899ms step_avg:60.12ms
step:332/2315 train_time:19960ms step_avg:60.12ms
step:333/2315 train_time:20020ms step_avg:60.12ms
step:334/2315 train_time:20080ms step_avg:60.12ms
step:335/2315 train_time:20140ms step_avg:60.12ms
step:336/2315 train_time:20200ms step_avg:60.12ms
step:337/2315 train_time:20261ms step_avg:60.12ms
step:338/2315 train_time:20322ms step_avg:60.12ms
step:339/2315 train_time:20382ms step_avg:60.12ms
step:340/2315 train_time:20442ms step_avg:60.12ms
step:341/2315 train_time:20502ms step_avg:60.12ms
step:342/2315 train_time:20562ms step_avg:60.12ms
step:343/2315 train_time:20622ms step_avg:60.12ms
step:344/2315 train_time:20682ms step_avg:60.12ms
step:345/2315 train_time:20741ms step_avg:60.12ms
step:346/2315 train_time:20801ms step_avg:60.12ms
step:347/2315 train_time:20860ms step_avg:60.12ms
step:348/2315 train_time:20920ms step_avg:60.12ms
step:349/2315 train_time:20980ms step_avg:60.11ms
step:350/2315 train_time:21040ms step_avg:60.12ms
step:351/2315 train_time:21101ms step_avg:60.12ms
step:352/2315 train_time:21161ms step_avg:60.12ms
step:353/2315 train_time:21221ms step_avg:60.12ms
step:354/2315 train_time:21281ms step_avg:60.12ms
step:355/2315 train_time:21341ms step_avg:60.12ms
step:356/2315 train_time:21401ms step_avg:60.12ms
step:357/2315 train_time:21461ms step_avg:60.12ms
step:358/2315 train_time:21522ms step_avg:60.12ms
step:359/2315 train_time:21582ms step_avg:60.12ms
step:360/2315 train_time:21642ms step_avg:60.12ms
step:361/2315 train_time:21700ms step_avg:60.11ms
step:362/2315 train_time:21761ms step_avg:60.11ms
step:363/2315 train_time:21821ms step_avg:60.11ms
step:364/2315 train_time:21881ms step_avg:60.11ms
step:365/2315 train_time:21940ms step_avg:60.11ms
step:366/2315 train_time:22000ms step_avg:60.11ms
step:367/2315 train_time:22060ms step_avg:60.11ms
step:368/2315 train_time:22121ms step_avg:60.11ms
step:369/2315 train_time:22180ms step_avg:60.11ms
step:370/2315 train_time:22241ms step_avg:60.11ms
step:371/2315 train_time:22300ms step_avg:60.11ms
step:372/2315 train_time:22361ms step_avg:60.11ms
step:373/2315 train_time:22421ms step_avg:60.11ms
step:374/2315 train_time:22481ms step_avg:60.11ms
step:375/2315 train_time:22541ms step_avg:60.11ms
step:376/2315 train_time:22601ms step_avg:60.11ms
step:377/2315 train_time:22661ms step_avg:60.11ms
step:378/2315 train_time:22721ms step_avg:60.11ms
step:379/2315 train_time:22780ms step_avg:60.11ms
step:380/2315 train_time:22841ms step_avg:60.11ms
step:381/2315 train_time:22900ms step_avg:60.11ms
step:382/2315 train_time:22961ms step_avg:60.11ms
step:383/2315 train_time:23021ms step_avg:60.11ms
step:384/2315 train_time:23081ms step_avg:60.11ms
step:385/2315 train_time:23140ms step_avg:60.10ms
step:386/2315 train_time:23201ms step_avg:60.11ms
step:387/2315 train_time:23261ms step_avg:60.10ms
step:388/2315 train_time:23321ms step_avg:60.11ms
step:389/2315 train_time:23381ms step_avg:60.11ms
step:390/2315 train_time:23442ms step_avg:60.11ms
step:391/2315 train_time:23502ms step_avg:60.11ms
step:392/2315 train_time:23562ms step_avg:60.11ms
step:393/2315 train_time:23621ms step_avg:60.11ms
step:394/2315 train_time:23681ms step_avg:60.10ms
step:395/2315 train_time:23740ms step_avg:60.10ms
step:396/2315 train_time:23800ms step_avg:60.10ms
step:397/2315 train_time:23860ms step_avg:60.10ms
step:398/2315 train_time:23920ms step_avg:60.10ms
step:399/2315 train_time:23980ms step_avg:60.10ms
step:400/2315 train_time:24040ms step_avg:60.10ms
step:401/2315 train_time:24100ms step_avg:60.10ms
step:402/2315 train_time:24161ms step_avg:60.10ms
step:403/2315 train_time:24221ms step_avg:60.10ms
step:404/2315 train_time:24281ms step_avg:60.10ms
step:405/2315 train_time:24341ms step_avg:60.10ms
step:406/2315 train_time:24401ms step_avg:60.10ms
step:407/2315 train_time:24462ms step_avg:60.10ms
step:408/2315 train_time:24522ms step_avg:60.10ms
step:409/2315 train_time:24581ms step_avg:60.10ms
step:410/2315 train_time:24641ms step_avg:60.10ms
step:411/2315 train_time:24701ms step_avg:60.10ms
step:412/2315 train_time:24761ms step_avg:60.10ms
step:413/2315 train_time:24821ms step_avg:60.10ms
step:414/2315 train_time:24881ms step_avg:60.10ms
step:415/2315 train_time:24941ms step_avg:60.10ms
step:416/2315 train_time:25001ms step_avg:60.10ms
step:417/2315 train_time:25061ms step_avg:60.10ms
step:418/2315 train_time:25121ms step_avg:60.10ms
step:419/2315 train_time:25181ms step_avg:60.10ms
step:420/2315 train_time:25242ms step_avg:60.10ms
step:421/2315 train_time:25302ms step_avg:60.10ms
step:422/2315 train_time:25361ms step_avg:60.10ms
step:423/2315 train_time:25422ms step_avg:60.10ms
step:424/2315 train_time:25482ms step_avg:60.10ms
step:425/2315 train_time:25541ms step_avg:60.10ms
step:426/2315 train_time:25601ms step_avg:60.10ms
step:427/2315 train_time:25661ms step_avg:60.10ms
step:428/2315 train_time:25721ms step_avg:60.10ms
step:429/2315 train_time:25780ms step_avg:60.09ms
step:430/2315 train_time:25841ms step_avg:60.09ms
step:431/2315 train_time:25901ms step_avg:60.09ms
step:432/2315 train_time:25961ms step_avg:60.09ms
step:433/2315 train_time:26020ms step_avg:60.09ms
step:434/2315 train_time:26080ms step_avg:60.09ms
step:435/2315 train_time:26140ms step_avg:60.09ms
step:436/2315 train_time:26200ms step_avg:60.09ms
step:437/2315 train_time:26261ms step_avg:60.09ms
step:438/2315 train_time:26322ms step_avg:60.10ms
step:439/2315 train_time:26381ms step_avg:60.09ms
step:440/2315 train_time:26441ms step_avg:60.09ms
step:441/2315 train_time:26502ms step_avg:60.09ms
step:442/2315 train_time:26561ms step_avg:60.09ms
step:443/2315 train_time:26622ms step_avg:60.09ms
step:444/2315 train_time:26681ms step_avg:60.09ms
step:445/2315 train_time:26741ms step_avg:60.09ms
step:446/2315 train_time:26801ms step_avg:60.09ms
step:447/2315 train_time:26861ms step_avg:60.09ms
step:448/2315 train_time:26921ms step_avg:60.09ms
step:449/2315 train_time:26981ms step_avg:60.09ms
step:450/2315 train_time:27041ms step_avg:60.09ms
step:451/2315 train_time:27100ms step_avg:60.09ms
step:452/2315 train_time:27160ms step_avg:60.09ms
step:453/2315 train_time:27220ms step_avg:60.09ms
step:454/2315 train_time:27281ms step_avg:60.09ms
step:455/2315 train_time:27341ms step_avg:60.09ms
step:456/2315 train_time:27401ms step_avg:60.09ms
step:457/2315 train_time:27461ms step_avg:60.09ms
step:458/2315 train_time:27521ms step_avg:60.09ms
step:459/2315 train_time:27582ms step_avg:60.09ms
step:460/2315 train_time:27642ms step_avg:60.09ms
step:461/2315 train_time:27701ms step_avg:60.09ms
step:462/2315 train_time:27761ms step_avg:60.09ms
step:463/2315 train_time:27820ms step_avg:60.09ms
step:464/2315 train_time:27880ms step_avg:60.09ms
step:465/2315 train_time:27940ms step_avg:60.09ms
step:466/2315 train_time:28000ms step_avg:60.09ms
step:467/2315 train_time:28060ms step_avg:60.09ms
step:468/2315 train_time:28120ms step_avg:60.09ms
step:469/2315 train_time:28180ms step_avg:60.08ms
step:470/2315 train_time:28240ms step_avg:60.08ms
step:471/2315 train_time:28300ms step_avg:60.08ms
step:472/2315 train_time:28360ms step_avg:60.08ms
step:473/2315 train_time:28420ms step_avg:60.08ms
step:474/2315 train_time:28480ms step_avg:60.08ms
step:475/2315 train_time:28540ms step_avg:60.08ms
step:476/2315 train_time:28600ms step_avg:60.08ms
step:477/2315 train_time:28660ms step_avg:60.08ms
step:478/2315 train_time:28721ms step_avg:60.08ms
step:479/2315 train_time:28780ms step_avg:60.08ms
step:480/2315 train_time:28840ms step_avg:60.08ms
step:481/2315 train_time:28900ms step_avg:60.08ms
step:482/2315 train_time:28960ms step_avg:60.08ms
step:483/2315 train_time:29020ms step_avg:60.08ms
step:484/2315 train_time:29080ms step_avg:60.08ms
step:485/2315 train_time:29140ms step_avg:60.08ms
step:486/2315 train_time:29200ms step_avg:60.08ms
step:487/2315 train_time:29260ms step_avg:60.08ms
step:488/2315 train_time:29321ms step_avg:60.08ms
step:489/2315 train_time:29380ms step_avg:60.08ms
step:490/2315 train_time:29441ms step_avg:60.08ms
step:491/2315 train_time:29501ms step_avg:60.08ms
step:492/2315 train_time:29561ms step_avg:60.08ms
step:493/2315 train_time:29621ms step_avg:60.08ms
step:494/2315 train_time:29681ms step_avg:60.08ms
step:495/2315 train_time:29740ms step_avg:60.08ms
step:496/2315 train_time:29801ms step_avg:60.08ms
step:497/2315 train_time:29860ms step_avg:60.08ms
step:498/2315 train_time:29920ms step_avg:60.08ms
step:499/2315 train_time:29981ms step_avg:60.08ms
step:500/2315 train_time:30040ms step_avg:60.08ms
step:500/2315 val_loss:3.8285 train_time:30102ms step_avg:60.20ms
step:501/2315 train_time:30120ms step_avg:60.12ms
step:502/2315 train_time:30163ms step_avg:60.09ms
step:503/2315 train_time:30225ms step_avg:60.09ms
step:504/2315 train_time:30288ms step_avg:60.10ms
step:505/2315 train_time:30348ms step_avg:60.09ms
step:506/2315 train_time:30408ms step_avg:60.09ms
step:507/2315 train_time:30467ms step_avg:60.09ms
step:508/2315 train_time:30526ms step_avg:60.09ms
step:509/2315 train_time:30585ms step_avg:60.09ms
step:510/2315 train_time:30645ms step_avg:60.09ms
step:511/2315 train_time:30703ms step_avg:60.08ms
step:512/2315 train_time:30762ms step_avg:60.08ms
step:513/2315 train_time:30821ms step_avg:60.08ms
step:514/2315 train_time:30880ms step_avg:60.08ms
step:515/2315 train_time:30939ms step_avg:60.08ms
step:516/2315 train_time:31000ms step_avg:60.08ms
step:517/2315 train_time:31061ms step_avg:60.08ms
step:518/2315 train_time:31123ms step_avg:60.08ms
step:519/2315 train_time:31184ms step_avg:60.08ms
step:520/2315 train_time:31246ms step_avg:60.09ms
step:521/2315 train_time:31307ms step_avg:60.09ms
step:522/2315 train_time:31368ms step_avg:60.09ms
step:523/2315 train_time:31427ms step_avg:60.09ms
step:524/2315 train_time:31487ms step_avg:60.09ms
step:525/2315 train_time:31546ms step_avg:60.09ms
step:526/2315 train_time:31605ms step_avg:60.09ms
step:527/2315 train_time:31664ms step_avg:60.08ms
step:528/2315 train_time:31723ms step_avg:60.08ms
step:529/2315 train_time:31782ms step_avg:60.08ms
step:530/2315 train_time:31841ms step_avg:60.08ms
step:531/2315 train_time:31900ms step_avg:60.07ms
step:532/2315 train_time:31959ms step_avg:60.07ms
step:533/2315 train_time:32019ms step_avg:60.07ms
step:534/2315 train_time:32080ms step_avg:60.08ms
step:535/2315 train_time:32141ms step_avg:60.08ms
step:536/2315 train_time:32203ms step_avg:60.08ms
step:537/2315 train_time:32264ms step_avg:60.08ms
step:538/2315 train_time:32324ms step_avg:60.08ms
step:539/2315 train_time:32385ms step_avg:60.08ms
step:540/2315 train_time:32446ms step_avg:60.08ms
step:541/2315 train_time:32505ms step_avg:60.08ms
step:542/2315 train_time:32565ms step_avg:60.08ms
step:543/2315 train_time:32623ms step_avg:60.08ms
step:544/2315 train_time:32683ms step_avg:60.08ms
step:545/2315 train_time:32741ms step_avg:60.08ms
step:546/2315 train_time:32801ms step_avg:60.08ms
step:547/2315 train_time:32860ms step_avg:60.07ms
step:548/2315 train_time:32920ms step_avg:60.07ms
step:549/2315 train_time:32979ms step_avg:60.07ms
step:550/2315 train_time:33039ms step_avg:60.07ms
step:551/2315 train_time:33099ms step_avg:60.07ms
step:552/2315 train_time:33161ms step_avg:60.07ms
step:553/2315 train_time:33222ms step_avg:60.08ms
step:554/2315 train_time:33283ms step_avg:60.08ms
step:555/2315 train_time:33343ms step_avg:60.08ms
step:556/2315 train_time:33405ms step_avg:60.08ms
step:557/2315 train_time:33464ms step_avg:60.08ms
step:558/2315 train_time:33524ms step_avg:60.08ms
step:559/2315 train_time:33583ms step_avg:60.08ms
step:560/2315 train_time:33644ms step_avg:60.08ms
step:561/2315 train_time:33703ms step_avg:60.08ms
step:562/2315 train_time:33762ms step_avg:60.07ms
step:563/2315 train_time:33821ms step_avg:60.07ms
step:564/2315 train_time:33880ms step_avg:60.07ms
step:565/2315 train_time:33939ms step_avg:60.07ms
step:566/2315 train_time:34000ms step_avg:60.07ms
step:567/2315 train_time:34060ms step_avg:60.07ms
step:568/2315 train_time:34121ms step_avg:60.07ms
step:569/2315 train_time:34181ms step_avg:60.07ms
step:570/2315 train_time:34242ms step_avg:60.07ms
step:571/2315 train_time:34303ms step_avg:60.07ms
step:572/2315 train_time:34363ms step_avg:60.08ms
step:573/2315 train_time:34423ms step_avg:60.08ms
step:574/2315 train_time:34484ms step_avg:60.08ms
step:575/2315 train_time:34543ms step_avg:60.07ms
step:576/2315 train_time:34603ms step_avg:60.07ms
step:577/2315 train_time:34662ms step_avg:60.07ms
step:578/2315 train_time:34722ms step_avg:60.07ms
step:579/2315 train_time:34781ms step_avg:60.07ms
step:580/2315 train_time:34841ms step_avg:60.07ms
step:581/2315 train_time:34900ms step_avg:60.07ms
step:582/2315 train_time:34960ms step_avg:60.07ms
step:583/2315 train_time:35020ms step_avg:60.07ms
step:584/2315 train_time:35080ms step_avg:60.07ms
step:585/2315 train_time:35140ms step_avg:60.07ms
step:586/2315 train_time:35201ms step_avg:60.07ms
step:587/2315 train_time:35262ms step_avg:60.07ms
step:588/2315 train_time:35323ms step_avg:60.07ms
step:589/2315 train_time:35383ms step_avg:60.07ms
step:590/2315 train_time:35443ms step_avg:60.07ms
step:591/2315 train_time:35503ms step_avg:60.07ms
step:592/2315 train_time:35564ms step_avg:60.07ms
step:593/2315 train_time:35623ms step_avg:60.07ms
step:594/2315 train_time:35683ms step_avg:60.07ms
step:595/2315 train_time:35743ms step_avg:60.07ms
step:596/2315 train_time:35802ms step_avg:60.07ms
step:597/2315 train_time:35861ms step_avg:60.07ms
step:598/2315 train_time:35921ms step_avg:60.07ms
step:599/2315 train_time:35981ms step_avg:60.07ms
step:600/2315 train_time:36041ms step_avg:60.07ms
step:601/2315 train_time:36102ms step_avg:60.07ms
step:602/2315 train_time:36162ms step_avg:60.07ms
step:603/2315 train_time:36223ms step_avg:60.07ms
step:604/2315 train_time:36283ms step_avg:60.07ms
step:605/2315 train_time:36343ms step_avg:60.07ms
step:606/2315 train_time:36403ms step_avg:60.07ms
step:607/2315 train_time:36464ms step_avg:60.07ms
step:608/2315 train_time:36523ms step_avg:60.07ms
step:609/2315 train_time:36583ms step_avg:60.07ms
step:610/2315 train_time:36644ms step_avg:60.07ms
step:611/2315 train_time:36703ms step_avg:60.07ms
step:612/2315 train_time:36763ms step_avg:60.07ms
step:613/2315 train_time:36823ms step_avg:60.07ms
step:614/2315 train_time:36882ms step_avg:60.07ms
step:615/2315 train_time:36941ms step_avg:60.07ms
step:616/2315 train_time:37002ms step_avg:60.07ms
step:617/2315 train_time:37062ms step_avg:60.07ms
step:618/2315 train_time:37122ms step_avg:60.07ms
step:619/2315 train_time:37183ms step_avg:60.07ms
step:620/2315 train_time:37243ms step_avg:60.07ms
step:621/2315 train_time:37303ms step_avg:60.07ms
step:622/2315 train_time:37363ms step_avg:60.07ms
step:623/2315 train_time:37423ms step_avg:60.07ms
step:624/2315 train_time:37484ms step_avg:60.07ms
step:625/2315 train_time:37543ms step_avg:60.07ms
step:626/2315 train_time:37603ms step_avg:60.07ms
step:627/2315 train_time:37663ms step_avg:60.07ms
step:628/2315 train_time:37723ms step_avg:60.07ms
step:629/2315 train_time:37783ms step_avg:60.07ms
step:630/2315 train_time:37843ms step_avg:60.07ms
step:631/2315 train_time:37903ms step_avg:60.07ms
step:632/2315 train_time:37963ms step_avg:60.07ms
step:633/2315 train_time:38023ms step_avg:60.07ms
step:634/2315 train_time:38083ms step_avg:60.07ms
step:635/2315 train_time:38143ms step_avg:60.07ms
step:636/2315 train_time:38203ms step_avg:60.07ms
step:637/2315 train_time:38262ms step_avg:60.07ms
step:638/2315 train_time:38323ms step_avg:60.07ms
step:639/2315 train_time:38383ms step_avg:60.07ms
step:640/2315 train_time:38444ms step_avg:60.07ms
step:641/2315 train_time:38503ms step_avg:60.07ms
step:642/2315 train_time:38563ms step_avg:60.07ms
step:643/2315 train_time:38623ms step_avg:60.07ms
step:644/2315 train_time:38683ms step_avg:60.07ms
step:645/2315 train_time:38742ms step_avg:60.07ms
step:646/2315 train_time:38802ms step_avg:60.07ms
step:647/2315 train_time:38862ms step_avg:60.07ms
step:648/2315 train_time:38922ms step_avg:60.06ms
step:649/2315 train_time:38982ms step_avg:60.06ms
step:650/2315 train_time:39042ms step_avg:60.06ms
step:651/2315 train_time:39102ms step_avg:60.06ms
step:652/2315 train_time:39163ms step_avg:60.07ms
step:653/2315 train_time:39222ms step_avg:60.06ms
step:654/2315 train_time:39282ms step_avg:60.06ms
step:655/2315 train_time:39342ms step_avg:60.06ms
step:656/2315 train_time:39402ms step_avg:60.06ms
step:657/2315 train_time:39462ms step_avg:60.06ms
step:658/2315 train_time:39523ms step_avg:60.07ms
step:659/2315 train_time:39583ms step_avg:60.07ms
step:660/2315 train_time:39644ms step_avg:60.07ms
step:661/2315 train_time:39703ms step_avg:60.07ms
step:662/2315 train_time:39763ms step_avg:60.07ms
step:663/2315 train_time:39823ms step_avg:60.06ms
step:664/2315 train_time:39883ms step_avg:60.06ms
step:665/2315 train_time:39943ms step_avg:60.07ms
step:666/2315 train_time:40004ms step_avg:60.07ms
step:667/2315 train_time:40064ms step_avg:60.07ms
step:668/2315 train_time:40124ms step_avg:60.07ms
step:669/2315 train_time:40183ms step_avg:60.06ms
step:670/2315 train_time:40243ms step_avg:60.06ms
step:671/2315 train_time:40303ms step_avg:60.06ms
step:672/2315 train_time:40364ms step_avg:60.07ms
step:673/2315 train_time:40423ms step_avg:60.06ms
step:674/2315 train_time:40483ms step_avg:60.06ms
step:675/2315 train_time:40543ms step_avg:60.06ms
step:676/2315 train_time:40603ms step_avg:60.06ms
step:677/2315 train_time:40663ms step_avg:60.06ms
step:678/2315 train_time:40723ms step_avg:60.06ms
step:679/2315 train_time:40782ms step_avg:60.06ms
step:680/2315 train_time:40842ms step_avg:60.06ms
step:681/2315 train_time:40902ms step_avg:60.06ms
step:682/2315 train_time:40963ms step_avg:60.06ms
step:683/2315 train_time:41022ms step_avg:60.06ms
step:684/2315 train_time:41082ms step_avg:60.06ms
step:685/2315 train_time:41141ms step_avg:60.06ms
step:686/2315 train_time:41201ms step_avg:60.06ms
step:687/2315 train_time:41261ms step_avg:60.06ms
step:688/2315 train_time:41322ms step_avg:60.06ms
step:689/2315 train_time:41381ms step_avg:60.06ms
step:690/2315 train_time:41442ms step_avg:60.06ms
step:691/2315 train_time:41501ms step_avg:60.06ms
step:692/2315 train_time:41562ms step_avg:60.06ms
step:693/2315 train_time:41622ms step_avg:60.06ms
step:694/2315 train_time:41683ms step_avg:60.06ms
step:695/2315 train_time:41742ms step_avg:60.06ms
step:696/2315 train_time:41802ms step_avg:60.06ms
step:697/2315 train_time:41862ms step_avg:60.06ms
step:698/2315 train_time:41923ms step_avg:60.06ms
step:699/2315 train_time:41982ms step_avg:60.06ms
step:700/2315 train_time:42042ms step_avg:60.06ms
step:701/2315 train_time:42102ms step_avg:60.06ms
step:702/2315 train_time:42163ms step_avg:60.06ms
step:703/2315 train_time:42222ms step_avg:60.06ms
step:704/2315 train_time:42282ms step_avg:60.06ms
step:705/2315 train_time:42342ms step_avg:60.06ms
step:706/2315 train_time:42403ms step_avg:60.06ms
step:707/2315 train_time:42462ms step_avg:60.06ms
step:708/2315 train_time:42523ms step_avg:60.06ms
step:709/2315 train_time:42582ms step_avg:60.06ms
step:710/2315 train_time:42643ms step_avg:60.06ms
step:711/2315 train_time:42702ms step_avg:60.06ms
step:712/2315 train_time:42762ms step_avg:60.06ms
step:713/2315 train_time:42822ms step_avg:60.06ms
step:714/2315 train_time:42882ms step_avg:60.06ms
step:715/2315 train_time:42942ms step_avg:60.06ms
step:716/2315 train_time:43003ms step_avg:60.06ms
step:717/2315 train_time:43063ms step_avg:60.06ms
step:718/2315 train_time:43123ms step_avg:60.06ms
step:719/2315 train_time:43183ms step_avg:60.06ms
step:720/2315 train_time:43243ms step_avg:60.06ms
step:721/2315 train_time:43302ms step_avg:60.06ms
step:722/2315 train_time:43363ms step_avg:60.06ms
step:723/2315 train_time:43422ms step_avg:60.06ms
step:724/2315 train_time:43483ms step_avg:60.06ms
step:725/2315 train_time:43542ms step_avg:60.06ms
step:726/2315 train_time:43602ms step_avg:60.06ms
step:727/2315 train_time:43662ms step_avg:60.06ms
step:728/2315 train_time:43723ms step_avg:60.06ms
step:729/2315 train_time:43782ms step_avg:60.06ms
step:730/2315 train_time:43842ms step_avg:60.06ms
step:731/2315 train_time:43902ms step_avg:60.06ms
step:732/2315 train_time:43963ms step_avg:60.06ms
step:733/2315 train_time:44023ms step_avg:60.06ms
step:734/2315 train_time:44083ms step_avg:60.06ms
step:735/2315 train_time:44142ms step_avg:60.06ms
step:736/2315 train_time:44203ms step_avg:60.06ms
step:737/2315 train_time:44262ms step_avg:60.06ms
step:738/2315 train_time:44322ms step_avg:60.06ms
step:739/2315 train_time:44382ms step_avg:60.06ms
step:740/2315 train_time:44442ms step_avg:60.06ms
step:741/2315 train_time:44502ms step_avg:60.06ms
step:742/2315 train_time:44562ms step_avg:60.06ms
step:743/2315 train_time:44622ms step_avg:60.06ms
step:744/2315 train_time:44683ms step_avg:60.06ms
step:745/2315 train_time:44742ms step_avg:60.06ms
step:746/2315 train_time:44802ms step_avg:60.06ms
step:747/2315 train_time:44862ms step_avg:60.06ms
step:748/2315 train_time:44923ms step_avg:60.06ms
step:749/2315 train_time:44983ms step_avg:60.06ms
step:750/2315 train_time:45043ms step_avg:60.06ms
step:750/2315 val_loss:3.6847 train_time:45105ms step_avg:60.14ms
step:751/2315 train_time:45123ms step_avg:60.08ms
step:752/2315 train_time:45167ms step_avg:60.06ms
step:753/2315 train_time:45230ms step_avg:60.07ms
step:754/2315 train_time:45292ms step_avg:60.07ms
step:755/2315 train_time:45352ms step_avg:60.07ms
step:756/2315 train_time:45412ms step_avg:60.07ms
step:757/2315 train_time:45472ms step_avg:60.07ms
step:758/2315 train_time:45531ms step_avg:60.07ms
step:759/2315 train_time:45590ms step_avg:60.07ms
step:760/2315 train_time:45650ms step_avg:60.07ms
step:761/2315 train_time:45710ms step_avg:60.07ms
step:762/2315 train_time:45770ms step_avg:60.07ms
step:763/2315 train_time:45830ms step_avg:60.07ms
step:764/2315 train_time:45891ms step_avg:60.07ms
step:765/2315 train_time:45951ms step_avg:60.07ms
step:766/2315 train_time:46013ms step_avg:60.07ms
step:767/2315 train_time:46076ms step_avg:60.07ms
step:768/2315 train_time:46139ms step_avg:60.08ms
step:769/2315 train_time:46200ms step_avg:60.08ms
step:770/2315 train_time:46262ms step_avg:60.08ms
step:771/2315 train_time:46323ms step_avg:60.08ms
step:772/2315 train_time:46383ms step_avg:60.08ms
step:773/2315 train_time:46444ms step_avg:60.08ms
step:774/2315 train_time:46504ms step_avg:60.08ms
step:775/2315 train_time:46564ms step_avg:60.08ms
step:776/2315 train_time:46624ms step_avg:60.08ms
step:777/2315 train_time:46684ms step_avg:60.08ms
step:778/2315 train_time:46744ms step_avg:60.08ms
step:779/2315 train_time:46804ms step_avg:60.08ms
step:780/2315 train_time:46864ms step_avg:60.08ms
step:781/2315 train_time:46924ms step_avg:60.08ms
step:782/2315 train_time:46985ms step_avg:60.08ms
step:783/2315 train_time:47046ms step_avg:60.08ms
step:784/2315 train_time:47108ms step_avg:60.09ms
step:785/2315 train_time:47170ms step_avg:60.09ms
step:786/2315 train_time:47232ms step_avg:60.09ms
step:787/2315 train_time:47293ms step_avg:60.09ms
step:788/2315 train_time:47355ms step_avg:60.10ms
step:789/2315 train_time:47416ms step_avg:60.10ms
step:790/2315 train_time:47477ms step_avg:60.10ms
step:791/2315 train_time:47537ms step_avg:60.10ms
step:792/2315 train_time:47598ms step_avg:60.10ms
step:793/2315 train_time:47658ms step_avg:60.10ms
step:794/2315 train_time:47719ms step_avg:60.10ms
step:795/2315 train_time:47779ms step_avg:60.10ms
step:796/2315 train_time:47839ms step_avg:60.10ms
step:797/2315 train_time:47899ms step_avg:60.10ms
step:798/2315 train_time:47959ms step_avg:60.10ms
step:799/2315 train_time:48020ms step_avg:60.10ms
step:800/2315 train_time:48082ms step_avg:60.10ms
step:801/2315 train_time:48142ms step_avg:60.10ms
step:802/2315 train_time:48203ms step_avg:60.10ms
step:803/2315 train_time:48264ms step_avg:60.10ms
step:804/2315 train_time:48325ms step_avg:60.11ms
step:805/2315 train_time:48386ms step_avg:60.11ms
step:806/2315 train_time:48447ms step_avg:60.11ms
step:807/2315 train_time:48508ms step_avg:60.11ms
step:808/2315 train_time:48569ms step_avg:60.11ms
step:809/2315 train_time:48629ms step_avg:60.11ms
step:810/2315 train_time:48691ms step_avg:60.11ms
step:811/2315 train_time:48751ms step_avg:60.11ms
step:812/2315 train_time:48812ms step_avg:60.11ms
step:813/2315 train_time:48873ms step_avg:60.11ms
step:814/2315 train_time:48934ms step_avg:60.12ms
step:815/2315 train_time:48995ms step_avg:60.12ms
step:816/2315 train_time:49056ms step_avg:60.12ms
step:817/2315 train_time:49117ms step_avg:60.12ms
step:818/2315 train_time:49179ms step_avg:60.12ms
step:819/2315 train_time:49240ms step_avg:60.12ms
step:820/2315 train_time:49301ms step_avg:60.12ms
step:821/2315 train_time:49361ms step_avg:60.12ms
step:822/2315 train_time:49421ms step_avg:60.12ms
step:823/2315 train_time:49482ms step_avg:60.12ms
step:824/2315 train_time:49543ms step_avg:60.12ms
step:825/2315 train_time:49603ms step_avg:60.12ms
step:826/2315 train_time:49664ms step_avg:60.13ms
step:827/2315 train_time:49724ms step_avg:60.13ms
step:828/2315 train_time:49785ms step_avg:60.13ms
step:829/2315 train_time:49846ms step_avg:60.13ms
step:830/2315 train_time:49907ms step_avg:60.13ms
step:831/2315 train_time:49967ms step_avg:60.13ms
step:832/2315 train_time:50028ms step_avg:60.13ms
step:833/2315 train_time:50088ms step_avg:60.13ms
step:834/2315 train_time:50150ms step_avg:60.13ms
step:835/2315 train_time:50210ms step_avg:60.13ms
step:836/2315 train_time:50272ms step_avg:60.13ms
step:837/2315 train_time:50333ms step_avg:60.13ms
step:838/2315 train_time:50394ms step_avg:60.14ms
step:839/2315 train_time:50455ms step_avg:60.14ms
step:840/2315 train_time:50517ms step_avg:60.14ms
step:841/2315 train_time:50578ms step_avg:60.14ms
step:842/2315 train_time:50639ms step_avg:60.14ms
step:843/2315 train_time:50699ms step_avg:60.14ms
step:844/2315 train_time:50759ms step_avg:60.14ms
step:845/2315 train_time:50820ms step_avg:60.14ms
step:846/2315 train_time:50882ms step_avg:60.14ms
step:847/2315 train_time:50942ms step_avg:60.14ms
step:848/2315 train_time:51002ms step_avg:60.14ms
step:849/2315 train_time:51062ms step_avg:60.14ms
step:850/2315 train_time:51123ms step_avg:60.15ms
step:851/2315 train_time:51184ms step_avg:60.15ms
step:852/2315 train_time:51245ms step_avg:60.15ms
step:853/2315 train_time:51306ms step_avg:60.15ms
step:854/2315 train_time:51367ms step_avg:60.15ms
step:855/2315 train_time:51427ms step_avg:60.15ms
step:856/2315 train_time:51488ms step_avg:60.15ms
step:857/2315 train_time:51549ms step_avg:60.15ms
step:858/2315 train_time:51610ms step_avg:60.15ms
step:859/2315 train_time:51671ms step_avg:60.15ms
step:860/2315 train_time:51733ms step_avg:60.15ms
step:861/2315 train_time:51794ms step_avg:60.16ms
step:862/2315 train_time:51855ms step_avg:60.16ms
step:863/2315 train_time:51916ms step_avg:60.16ms
step:864/2315 train_time:51977ms step_avg:60.16ms
step:865/2315 train_time:52038ms step_avg:60.16ms
step:866/2315 train_time:52099ms step_avg:60.16ms
step:867/2315 train_time:52159ms step_avg:60.16ms
step:868/2315 train_time:52220ms step_avg:60.16ms
step:869/2315 train_time:52280ms step_avg:60.16ms
step:870/2315 train_time:52341ms step_avg:60.16ms
step:871/2315 train_time:52401ms step_avg:60.16ms
step:872/2315 train_time:52462ms step_avg:60.16ms
step:873/2315 train_time:52522ms step_avg:60.16ms
step:874/2315 train_time:52583ms step_avg:60.16ms
step:875/2315 train_time:52643ms step_avg:60.16ms
step:876/2315 train_time:52704ms step_avg:60.16ms
step:877/2315 train_time:52765ms step_avg:60.17ms
step:878/2315 train_time:52826ms step_avg:60.17ms
step:879/2315 train_time:52887ms step_avg:60.17ms
step:880/2315 train_time:52948ms step_avg:60.17ms
step:881/2315 train_time:53008ms step_avg:60.17ms
step:882/2315 train_time:53069ms step_avg:60.17ms
step:883/2315 train_time:53130ms step_avg:60.17ms
step:884/2315 train_time:53191ms step_avg:60.17ms
step:885/2315 train_time:53252ms step_avg:60.17ms
step:886/2315 train_time:53314ms step_avg:60.17ms
step:887/2315 train_time:53375ms step_avg:60.18ms
step:888/2315 train_time:53437ms step_avg:60.18ms
step:889/2315 train_time:53497ms step_avg:60.18ms
step:890/2315 train_time:53558ms step_avg:60.18ms
step:891/2315 train_time:53619ms step_avg:60.18ms
step:892/2315 train_time:53680ms step_avg:60.18ms
step:893/2315 train_time:53740ms step_avg:60.18ms
step:894/2315 train_time:53801ms step_avg:60.18ms
step:895/2315 train_time:53861ms step_avg:60.18ms
step:896/2315 train_time:53922ms step_avg:60.18ms
step:897/2315 train_time:53982ms step_avg:60.18ms
step:898/2315 train_time:54043ms step_avg:60.18ms
step:899/2315 train_time:54103ms step_avg:60.18ms
step:900/2315 train_time:54164ms step_avg:60.18ms
step:901/2315 train_time:54224ms step_avg:60.18ms
step:902/2315 train_time:54284ms step_avg:60.18ms
step:903/2315 train_time:54345ms step_avg:60.18ms
step:904/2315 train_time:54406ms step_avg:60.18ms
step:905/2315 train_time:54467ms step_avg:60.18ms
step:906/2315 train_time:54528ms step_avg:60.18ms
step:907/2315 train_time:54588ms step_avg:60.19ms
step:908/2315 train_time:54650ms step_avg:60.19ms
step:909/2315 train_time:54710ms step_avg:60.19ms
step:910/2315 train_time:54771ms step_avg:60.19ms
step:911/2315 train_time:54832ms step_avg:60.19ms
step:912/2315 train_time:54894ms step_avg:60.19ms
step:913/2315 train_time:54954ms step_avg:60.19ms
step:914/2315 train_time:55016ms step_avg:60.19ms
step:915/2315 train_time:55077ms step_avg:60.19ms
step:916/2315 train_time:55138ms step_avg:60.19ms
step:917/2315 train_time:55199ms step_avg:60.20ms
step:918/2315 train_time:55259ms step_avg:60.20ms
step:919/2315 train_time:55320ms step_avg:60.20ms
step:920/2315 train_time:55380ms step_avg:60.20ms
step:921/2315 train_time:55441ms step_avg:60.20ms
step:922/2315 train_time:55502ms step_avg:60.20ms
step:923/2315 train_time:55562ms step_avg:60.20ms
step:924/2315 train_time:55623ms step_avg:60.20ms
step:925/2315 train_time:55683ms step_avg:60.20ms
step:926/2315 train_time:55743ms step_avg:60.20ms
step:927/2315 train_time:55803ms step_avg:60.20ms
step:928/2315 train_time:55864ms step_avg:60.20ms
step:929/2315 train_time:55924ms step_avg:60.20ms
step:930/2315 train_time:55986ms step_avg:60.20ms
step:931/2315 train_time:56047ms step_avg:60.20ms
step:932/2315 train_time:56108ms step_avg:60.20ms
step:933/2315 train_time:56169ms step_avg:60.20ms
step:934/2315 train_time:56230ms step_avg:60.20ms
step:935/2315 train_time:56290ms step_avg:60.20ms
step:936/2315 train_time:56351ms step_avg:60.20ms
step:937/2315 train_time:56412ms step_avg:60.21ms
step:938/2315 train_time:56474ms step_avg:60.21ms
step:939/2315 train_time:56535ms step_avg:60.21ms
step:940/2315 train_time:56596ms step_avg:60.21ms
step:941/2315 train_time:56657ms step_avg:60.21ms
step:942/2315 train_time:56718ms step_avg:60.21ms
step:943/2315 train_time:56778ms step_avg:60.21ms
step:944/2315 train_time:56839ms step_avg:60.21ms
step:945/2315 train_time:56900ms step_avg:60.21ms
step:946/2315 train_time:56961ms step_avg:60.21ms
step:947/2315 train_time:57021ms step_avg:60.21ms
step:948/2315 train_time:57082ms step_avg:60.21ms
step:949/2315 train_time:57142ms step_avg:60.21ms
step:950/2315 train_time:57203ms step_avg:60.21ms
step:951/2315 train_time:57263ms step_avg:60.21ms
step:952/2315 train_time:57324ms step_avg:60.21ms
step:953/2315 train_time:57385ms step_avg:60.21ms
step:954/2315 train_time:57446ms step_avg:60.22ms
step:955/2315 train_time:57507ms step_avg:60.22ms
step:956/2315 train_time:57569ms step_avg:60.22ms
step:957/2315 train_time:57630ms step_avg:60.22ms
step:958/2315 train_time:57691ms step_avg:60.22ms
step:959/2315 train_time:57752ms step_avg:60.22ms
step:960/2315 train_time:57813ms step_avg:60.22ms
step:961/2315 train_time:57874ms step_avg:60.22ms
step:962/2315 train_time:57936ms step_avg:60.22ms
step:963/2315 train_time:57997ms step_avg:60.22ms
step:964/2315 train_time:58058ms step_avg:60.23ms
step:965/2315 train_time:58119ms step_avg:60.23ms
step:966/2315 train_time:58180ms step_avg:60.23ms
step:967/2315 train_time:58239ms step_avg:60.23ms
step:968/2315 train_time:58300ms step_avg:60.23ms
step:969/2315 train_time:58360ms step_avg:60.23ms
step:970/2315 train_time:58422ms step_avg:60.23ms
step:971/2315 train_time:58484ms step_avg:60.23ms
step:972/2315 train_time:58544ms step_avg:60.23ms
step:973/2315 train_time:58604ms step_avg:60.23ms
step:974/2315 train_time:58666ms step_avg:60.23ms
step:975/2315 train_time:58726ms step_avg:60.23ms
step:976/2315 train_time:58787ms step_avg:60.23ms
step:977/2315 train_time:58848ms step_avg:60.23ms
step:978/2315 train_time:58909ms step_avg:60.23ms
step:979/2315 train_time:58971ms step_avg:60.24ms
step:980/2315 train_time:59032ms step_avg:60.24ms
step:981/2315 train_time:59092ms step_avg:60.24ms
step:982/2315 train_time:59154ms step_avg:60.24ms
step:983/2315 train_time:59215ms step_avg:60.24ms
step:984/2315 train_time:59276ms step_avg:60.24ms
step:985/2315 train_time:59336ms step_avg:60.24ms
step:986/2315 train_time:59398ms step_avg:60.24ms
step:987/2315 train_time:59459ms step_avg:60.24ms
step:988/2315 train_time:59520ms step_avg:60.24ms
step:989/2315 train_time:59580ms step_avg:60.24ms
step:990/2315 train_time:59641ms step_avg:60.24ms
step:991/2315 train_time:59702ms step_avg:60.24ms
step:992/2315 train_time:59762ms step_avg:60.24ms
step:993/2315 train_time:59823ms step_avg:60.24ms
step:994/2315 train_time:59884ms step_avg:60.25ms
step:995/2315 train_time:59944ms step_avg:60.25ms
step:996/2315 train_time:60004ms step_avg:60.25ms
step:997/2315 train_time:60065ms step_avg:60.25ms
step:998/2315 train_time:60126ms step_avg:60.25ms
step:999/2315 train_time:60186ms step_avg:60.25ms
step:1000/2315 train_time:60247ms step_avg:60.25ms
step:1000/2315 val_loss:3.5723 train_time:60310ms step_avg:60.31ms
step:1001/2315 train_time:60330ms step_avg:60.27ms
step:1002/2315 train_time:60373ms step_avg:60.25ms
step:1003/2315 train_time:60438ms step_avg:60.26ms
step:1004/2315 train_time:60507ms step_avg:60.27ms
step:1005/2315 train_time:60569ms step_avg:60.27ms
step:1006/2315 train_time:60631ms step_avg:60.27ms
step:1007/2315 train_time:60691ms step_avg:60.27ms
step:1008/2315 train_time:60751ms step_avg:60.27ms
step:1009/2315 train_time:60811ms step_avg:60.27ms
step:1010/2315 train_time:60872ms step_avg:60.27ms
step:1011/2315 train_time:60932ms step_avg:60.27ms
step:1012/2315 train_time:60992ms step_avg:60.27ms
step:1013/2315 train_time:61052ms step_avg:60.27ms
step:1014/2315 train_time:61113ms step_avg:60.27ms
step:1015/2315 train_time:61172ms step_avg:60.27ms
step:1016/2315 train_time:61235ms step_avg:60.27ms
step:1017/2315 train_time:61297ms step_avg:60.27ms
step:1018/2315 train_time:61360ms step_avg:60.27ms
step:1019/2315 train_time:61422ms step_avg:60.28ms
step:1020/2315 train_time:61484ms step_avg:60.28ms
step:1021/2315 train_time:61545ms step_avg:60.28ms
step:1022/2315 train_time:61607ms step_avg:60.28ms
step:1023/2315 train_time:61667ms step_avg:60.28ms
step:1024/2315 train_time:61728ms step_avg:60.28ms
step:1025/2315 train_time:61788ms step_avg:60.28ms
step:1026/2315 train_time:61849ms step_avg:60.28ms
step:1027/2315 train_time:61909ms step_avg:60.28ms
step:1028/2315 train_time:61970ms step_avg:60.28ms
step:1029/2315 train_time:62030ms step_avg:60.28ms
step:1030/2315 train_time:62091ms step_avg:60.28ms
step:1031/2315 train_time:62152ms step_avg:60.28ms
step:1032/2315 train_time:62213ms step_avg:60.28ms
step:1033/2315 train_time:62275ms step_avg:60.29ms
step:1034/2315 train_time:62337ms step_avg:60.29ms
step:1035/2315 train_time:62398ms step_avg:60.29ms
step:1036/2315 train_time:62460ms step_avg:60.29ms
step:1037/2315 train_time:62521ms step_avg:60.29ms
step:1038/2315 train_time:62582ms step_avg:60.29ms
step:1039/2315 train_time:62642ms step_avg:60.29ms
step:1040/2315 train_time:62703ms step_avg:60.29ms
step:1041/2315 train_time:62763ms step_avg:60.29ms
step:1042/2315 train_time:62824ms step_avg:60.29ms
step:1043/2315 train_time:62884ms step_avg:60.29ms
step:1044/2315 train_time:62946ms step_avg:60.29ms
step:1045/2315 train_time:63006ms step_avg:60.29ms
step:1046/2315 train_time:63067ms step_avg:60.29ms
step:1047/2315 train_time:63128ms step_avg:60.29ms
step:1048/2315 train_time:63189ms step_avg:60.30ms
step:1049/2315 train_time:63251ms step_avg:60.30ms
step:1050/2315 train_time:63313ms step_avg:60.30ms
step:1051/2315 train_time:63374ms step_avg:60.30ms
step:1052/2315 train_time:63437ms step_avg:60.30ms
step:1053/2315 train_time:63497ms step_avg:60.30ms
step:1054/2315 train_time:63558ms step_avg:60.30ms
step:1055/2315 train_time:63619ms step_avg:60.30ms
step:1056/2315 train_time:63680ms step_avg:60.30ms
step:1057/2315 train_time:63740ms step_avg:60.30ms
step:1058/2315 train_time:63800ms step_avg:60.30ms
step:1059/2315 train_time:63860ms step_avg:60.30ms
step:1060/2315 train_time:63921ms step_avg:60.30ms
step:1061/2315 train_time:63980ms step_avg:60.30ms
step:1062/2315 train_time:64041ms step_avg:60.30ms
step:1063/2315 train_time:64102ms step_avg:60.30ms
step:1064/2315 train_time:64163ms step_avg:60.30ms
step:1065/2315 train_time:64223ms step_avg:60.30ms
step:1066/2315 train_time:64285ms step_avg:60.31ms
step:1067/2315 train_time:64347ms step_avg:60.31ms
step:1068/2315 train_time:64408ms step_avg:60.31ms
step:1069/2315 train_time:64469ms step_avg:60.31ms
step:1070/2315 train_time:64531ms step_avg:60.31ms
step:1071/2315 train_time:64592ms step_avg:60.31ms
step:1072/2315 train_time:64654ms step_avg:60.31ms
step:1073/2315 train_time:64715ms step_avg:60.31ms
step:1074/2315 train_time:64776ms step_avg:60.31ms
step:1075/2315 train_time:64837ms step_avg:60.31ms
step:1076/2315 train_time:64897ms step_avg:60.31ms
step:1077/2315 train_time:64958ms step_avg:60.31ms
step:1078/2315 train_time:65019ms step_avg:60.31ms
step:1079/2315 train_time:65078ms step_avg:60.31ms
step:1080/2315 train_time:65139ms step_avg:60.31ms
step:1081/2315 train_time:65199ms step_avg:60.31ms
step:1082/2315 train_time:65260ms step_avg:60.31ms
step:1083/2315 train_time:65321ms step_avg:60.31ms
step:1084/2315 train_time:65382ms step_avg:60.32ms
step:1085/2315 train_time:65443ms step_avg:60.32ms
step:1086/2315 train_time:65505ms step_avg:60.32ms
step:1087/2315 train_time:65565ms step_avg:60.32ms
step:1088/2315 train_time:65626ms step_avg:60.32ms
step:1089/2315 train_time:65688ms step_avg:60.32ms
step:1090/2315 train_time:65749ms step_avg:60.32ms
step:1091/2315 train_time:65810ms step_avg:60.32ms
step:1092/2315 train_time:65872ms step_avg:60.32ms
step:1093/2315 train_time:65932ms step_avg:60.32ms
step:1094/2315 train_time:65993ms step_avg:60.32ms
step:1095/2315 train_time:66054ms step_avg:60.32ms
step:1096/2315 train_time:66115ms step_avg:60.32ms
step:1097/2315 train_time:66176ms step_avg:60.32ms
step:1098/2315 train_time:66237ms step_avg:60.33ms
step:1099/2315 train_time:66297ms step_avg:60.33ms
step:1100/2315 train_time:66359ms step_avg:60.33ms
step:1101/2315 train_time:66420ms step_avg:60.33ms
step:1102/2315 train_time:66481ms step_avg:60.33ms
step:1103/2315 train_time:66541ms step_avg:60.33ms
step:1104/2315 train_time:66601ms step_avg:60.33ms
step:1105/2315 train_time:66662ms step_avg:60.33ms
step:1106/2315 train_time:66722ms step_avg:60.33ms
step:1107/2315 train_time:66783ms step_avg:60.33ms
step:1108/2315 train_time:66844ms step_avg:60.33ms
step:1109/2315 train_time:66905ms step_avg:60.33ms
step:1110/2315 train_time:66966ms step_avg:60.33ms
step:1111/2315 train_time:67027ms step_avg:60.33ms
step:1112/2315 train_time:67089ms step_avg:60.33ms
step:1113/2315 train_time:67150ms step_avg:60.33ms
step:1114/2315 train_time:67211ms step_avg:60.33ms
step:1115/2315 train_time:67272ms step_avg:60.33ms
step:1116/2315 train_time:67334ms step_avg:60.34ms
step:1117/2315 train_time:67396ms step_avg:60.34ms
step:1118/2315 train_time:67457ms step_avg:60.34ms
step:1119/2315 train_time:67518ms step_avg:60.34ms
step:1120/2315 train_time:67579ms step_avg:60.34ms
step:1121/2315 train_time:67639ms step_avg:60.34ms
step:1122/2315 train_time:67699ms step_avg:60.34ms
step:1123/2315 train_time:67760ms step_avg:60.34ms
step:1124/2315 train_time:67821ms step_avg:60.34ms
step:1125/2315 train_time:67880ms step_avg:60.34ms
step:1126/2315 train_time:67941ms step_avg:60.34ms
step:1127/2315 train_time:68001ms step_avg:60.34ms
step:1128/2315 train_time:68063ms step_avg:60.34ms
step:1129/2315 train_time:68124ms step_avg:60.34ms
step:1130/2315 train_time:68185ms step_avg:60.34ms
step:1131/2315 train_time:68247ms step_avg:60.34ms
step:1132/2315 train_time:68308ms step_avg:60.34ms
step:1133/2315 train_time:68369ms step_avg:60.34ms
step:1134/2315 train_time:68431ms step_avg:60.35ms
step:1135/2315 train_time:68492ms step_avg:60.35ms
step:1136/2315 train_time:68553ms step_avg:60.35ms
step:1137/2315 train_time:68614ms step_avg:60.35ms
step:1138/2315 train_time:68675ms step_avg:60.35ms
step:1139/2315 train_time:68736ms step_avg:60.35ms
step:1140/2315 train_time:68797ms step_avg:60.35ms
step:1141/2315 train_time:68857ms step_avg:60.35ms
step:1142/2315 train_time:68918ms step_avg:60.35ms
step:1143/2315 train_time:68979ms step_avg:60.35ms
step:1144/2315 train_time:69039ms step_avg:60.35ms
step:1145/2315 train_time:69100ms step_avg:60.35ms
step:1146/2315 train_time:69160ms step_avg:60.35ms
step:1147/2315 train_time:69221ms step_avg:60.35ms
step:1148/2315 train_time:69281ms step_avg:60.35ms
step:1149/2315 train_time:69341ms step_avg:60.35ms
step:1150/2315 train_time:69403ms step_avg:60.35ms
step:1151/2315 train_time:69464ms step_avg:60.35ms
step:1152/2315 train_time:69526ms step_avg:60.35ms
step:1153/2315 train_time:69586ms step_avg:60.35ms
step:1154/2315 train_time:69647ms step_avg:60.35ms
step:1155/2315 train_time:69708ms step_avg:60.35ms
step:1156/2315 train_time:69769ms step_avg:60.35ms
step:1157/2315 train_time:69830ms step_avg:60.35ms
step:1158/2315 train_time:69892ms step_avg:60.36ms
step:1159/2315 train_time:69953ms step_avg:60.36ms
step:1160/2315 train_time:70014ms step_avg:60.36ms
step:1161/2315 train_time:70075ms step_avg:60.36ms
step:1162/2315 train_time:70136ms step_avg:60.36ms
step:1163/2315 train_time:70198ms step_avg:60.36ms
step:1164/2315 train_time:70259ms step_avg:60.36ms
step:1165/2315 train_time:70319ms step_avg:60.36ms
step:1166/2315 train_time:70380ms step_avg:60.36ms
step:1167/2315 train_time:70441ms step_avg:60.36ms
step:1168/2315 train_time:70501ms step_avg:60.36ms
step:1169/2315 train_time:70561ms step_avg:60.36ms
step:1170/2315 train_time:70621ms step_avg:60.36ms
step:1171/2315 train_time:70682ms step_avg:60.36ms
step:1172/2315 train_time:70743ms step_avg:60.36ms
step:1173/2315 train_time:70803ms step_avg:60.36ms
step:1174/2315 train_time:70865ms step_avg:60.36ms
step:1175/2315 train_time:70927ms step_avg:60.36ms
step:1176/2315 train_time:70989ms step_avg:60.36ms
step:1177/2315 train_time:71050ms step_avg:60.37ms
step:1178/2315 train_time:71111ms step_avg:60.37ms
step:1179/2315 train_time:71172ms step_avg:60.37ms
step:1180/2315 train_time:71234ms step_avg:60.37ms
step:1181/2315 train_time:71295ms step_avg:60.37ms
step:1182/2315 train_time:71356ms step_avg:60.37ms
step:1183/2315 train_time:71416ms step_avg:60.37ms
step:1184/2315 train_time:71477ms step_avg:60.37ms
step:1185/2315 train_time:71538ms step_avg:60.37ms
step:1186/2315 train_time:71598ms step_avg:60.37ms
step:1187/2315 train_time:71659ms step_avg:60.37ms
step:1188/2315 train_time:71720ms step_avg:60.37ms
step:1189/2315 train_time:71780ms step_avg:60.37ms
step:1190/2315 train_time:71841ms step_avg:60.37ms
step:1191/2315 train_time:71901ms step_avg:60.37ms
step:1192/2315 train_time:71962ms step_avg:60.37ms
step:1193/2315 train_time:72022ms step_avg:60.37ms
step:1194/2315 train_time:72084ms step_avg:60.37ms
step:1195/2315 train_time:72145ms step_avg:60.37ms
step:1196/2315 train_time:72206ms step_avg:60.37ms
step:1197/2315 train_time:72267ms step_avg:60.37ms
step:1198/2315 train_time:72329ms step_avg:60.37ms
step:1199/2315 train_time:72389ms step_avg:60.37ms
step:1200/2315 train_time:72450ms step_avg:60.38ms
step:1201/2315 train_time:72511ms step_avg:60.38ms
step:1202/2315 train_time:72573ms step_avg:60.38ms
step:1203/2315 train_time:72633ms step_avg:60.38ms
step:1204/2315 train_time:72695ms step_avg:60.38ms
step:1205/2315 train_time:72756ms step_avg:60.38ms
step:1206/2315 train_time:72817ms step_avg:60.38ms
step:1207/2315 train_time:72877ms step_avg:60.38ms
step:1208/2315 train_time:72938ms step_avg:60.38ms
step:1209/2315 train_time:72998ms step_avg:60.38ms
step:1210/2315 train_time:73059ms step_avg:60.38ms
step:1211/2315 train_time:73119ms step_avg:60.38ms
step:1212/2315 train_time:73181ms step_avg:60.38ms
step:1213/2315 train_time:73241ms step_avg:60.38ms
step:1214/2315 train_time:73301ms step_avg:60.38ms
step:1215/2315 train_time:73361ms step_avg:60.38ms
step:1216/2315 train_time:73422ms step_avg:60.38ms
step:1217/2315 train_time:73482ms step_avg:60.38ms
step:1218/2315 train_time:73543ms step_avg:60.38ms
step:1219/2315 train_time:73605ms step_avg:60.38ms
step:1220/2315 train_time:73666ms step_avg:60.38ms
step:1221/2315 train_time:73727ms step_avg:60.38ms
step:1222/2315 train_time:73789ms step_avg:60.38ms
step:1223/2315 train_time:73850ms step_avg:60.38ms
step:1224/2315 train_time:73911ms step_avg:60.38ms
step:1225/2315 train_time:73972ms step_avg:60.39ms
step:1226/2315 train_time:74034ms step_avg:60.39ms
step:1227/2315 train_time:74095ms step_avg:60.39ms
step:1228/2315 train_time:74156ms step_avg:60.39ms
step:1229/2315 train_time:74216ms step_avg:60.39ms
step:1230/2315 train_time:74276ms step_avg:60.39ms
step:1231/2315 train_time:74337ms step_avg:60.39ms
step:1232/2315 train_time:74398ms step_avg:60.39ms
step:1233/2315 train_time:74458ms step_avg:60.39ms
step:1234/2315 train_time:74519ms step_avg:60.39ms
step:1235/2315 train_time:74579ms step_avg:60.39ms
step:1236/2315 train_time:74641ms step_avg:60.39ms
step:1237/2315 train_time:74700ms step_avg:60.39ms
step:1238/2315 train_time:74762ms step_avg:60.39ms
step:1239/2315 train_time:74823ms step_avg:60.39ms
step:1240/2315 train_time:74884ms step_avg:60.39ms
step:1241/2315 train_time:74944ms step_avg:60.39ms
step:1242/2315 train_time:75005ms step_avg:60.39ms
step:1243/2315 train_time:75066ms step_avg:60.39ms
step:1244/2315 train_time:75128ms step_avg:60.39ms
step:1245/2315 train_time:75188ms step_avg:60.39ms
step:1246/2315 train_time:75250ms step_avg:60.39ms
step:1247/2315 train_time:75311ms step_avg:60.39ms
step:1248/2315 train_time:75372ms step_avg:60.39ms
step:1249/2315 train_time:75434ms step_avg:60.40ms
step:1250/2315 train_time:75495ms step_avg:60.40ms
step:1250/2315 val_loss:3.5140 train_time:75558ms step_avg:60.45ms
step:1251/2315 train_time:75576ms step_avg:60.41ms
step:1252/2315 train_time:75621ms step_avg:60.40ms
step:1253/2315 train_time:75685ms step_avg:60.40ms
step:1254/2315 train_time:75747ms step_avg:60.40ms
step:1255/2315 train_time:75807ms step_avg:60.40ms
step:1256/2315 train_time:75868ms step_avg:60.40ms
step:1257/2315 train_time:75927ms step_avg:60.40ms
step:1258/2315 train_time:75988ms step_avg:60.40ms
step:1259/2315 train_time:76048ms step_avg:60.40ms
step:1260/2315 train_time:76108ms step_avg:60.40ms
step:1261/2315 train_time:76168ms step_avg:60.40ms
step:1262/2315 train_time:76228ms step_avg:60.40ms
step:1263/2315 train_time:76287ms step_avg:60.40ms
step:1264/2315 train_time:76348ms step_avg:60.40ms
step:1265/2315 train_time:76408ms step_avg:60.40ms
step:1266/2315 train_time:76470ms step_avg:60.40ms
step:1267/2315 train_time:76532ms step_avg:60.40ms
step:1268/2315 train_time:76595ms step_avg:60.41ms
step:1269/2315 train_time:76657ms step_avg:60.41ms
step:1270/2315 train_time:76719ms step_avg:60.41ms
step:1271/2315 train_time:76781ms step_avg:60.41ms
step:1272/2315 train_time:76842ms step_avg:60.41ms
step:1273/2315 train_time:76902ms step_avg:60.41ms
step:1274/2315 train_time:76963ms step_avg:60.41ms
step:1275/2315 train_time:77022ms step_avg:60.41ms
step:1276/2315 train_time:77083ms step_avg:60.41ms
step:1277/2315 train_time:77143ms step_avg:60.41ms
step:1278/2315 train_time:77204ms step_avg:60.41ms
step:1279/2315 train_time:77263ms step_avg:60.41ms
step:1280/2315 train_time:77323ms step_avg:60.41ms
step:1281/2315 train_time:77384ms step_avg:60.41ms
step:1282/2315 train_time:77445ms step_avg:60.41ms
step:1283/2315 train_time:77505ms step_avg:60.41ms
step:1284/2315 train_time:77567ms step_avg:60.41ms
step:1285/2315 train_time:77628ms step_avg:60.41ms
step:1286/2315 train_time:77690ms step_avg:60.41ms
step:1287/2315 train_time:77751ms step_avg:60.41ms
step:1288/2315 train_time:77812ms step_avg:60.41ms
step:1289/2315 train_time:77873ms step_avg:60.41ms
step:1290/2315 train_time:77935ms step_avg:60.41ms
step:1291/2315 train_time:77995ms step_avg:60.41ms
step:1292/2315 train_time:78057ms step_avg:60.42ms
step:1293/2315 train_time:78118ms step_avg:60.42ms
step:1294/2315 train_time:78179ms step_avg:60.42ms
step:1295/2315 train_time:78239ms step_avg:60.42ms
step:1296/2315 train_time:78300ms step_avg:60.42ms
step:1297/2315 train_time:78360ms step_avg:60.42ms
step:1298/2315 train_time:78421ms step_avg:60.42ms
step:1299/2315 train_time:78482ms step_avg:60.42ms
step:1300/2315 train_time:78543ms step_avg:60.42ms
step:1301/2315 train_time:78604ms step_avg:60.42ms
step:1302/2315 train_time:78665ms step_avg:60.42ms
step:1303/2315 train_time:78725ms step_avg:60.42ms
step:1304/2315 train_time:78786ms step_avg:60.42ms
step:1305/2315 train_time:78846ms step_avg:60.42ms
step:1306/2315 train_time:78908ms step_avg:60.42ms
step:1307/2315 train_time:78968ms step_avg:60.42ms
step:1308/2315 train_time:79029ms step_avg:60.42ms
step:1309/2315 train_time:79089ms step_avg:60.42ms
step:1310/2315 train_time:79150ms step_avg:60.42ms
step:1311/2315 train_time:79211ms step_avg:60.42ms
step:1312/2315 train_time:79272ms step_avg:60.42ms
step:1313/2315 train_time:79332ms step_avg:60.42ms
step:1314/2315 train_time:79393ms step_avg:60.42ms
step:1315/2315 train_time:79454ms step_avg:60.42ms
step:1316/2315 train_time:79516ms step_avg:60.42ms
step:1317/2315 train_time:79576ms step_avg:60.42ms
step:1318/2315 train_time:79638ms step_avg:60.42ms
step:1319/2315 train_time:79699ms step_avg:60.42ms
step:1320/2315 train_time:79761ms step_avg:60.42ms
step:1321/2315 train_time:79821ms step_avg:60.42ms
step:1322/2315 train_time:79882ms step_avg:60.43ms
step:1323/2315 train_time:79943ms step_avg:60.43ms
step:1324/2315 train_time:80004ms step_avg:60.43ms
step:1325/2315 train_time:80064ms step_avg:60.43ms
step:1326/2315 train_time:80124ms step_avg:60.43ms
step:1327/2315 train_time:80185ms step_avg:60.43ms
step:1328/2315 train_time:80246ms step_avg:60.43ms
step:1329/2315 train_time:80306ms step_avg:60.43ms
step:1330/2315 train_time:80367ms step_avg:60.43ms
step:1331/2315 train_time:80428ms step_avg:60.43ms
step:1332/2315 train_time:80489ms step_avg:60.43ms
step:1333/2315 train_time:80550ms step_avg:60.43ms
step:1334/2315 train_time:80612ms step_avg:60.43ms
step:1335/2315 train_time:80672ms step_avg:60.43ms
step:1336/2315 train_time:80734ms step_avg:60.43ms
step:1337/2315 train_time:80795ms step_avg:60.43ms
step:1338/2315 train_time:80858ms step_avg:60.43ms
step:1339/2315 train_time:80919ms step_avg:60.43ms
step:1340/2315 train_time:80980ms step_avg:60.43ms
step:1341/2315 train_time:81041ms step_avg:60.43ms
step:1342/2315 train_time:81102ms step_avg:60.43ms
step:1343/2315 train_time:81162ms step_avg:60.43ms
step:1344/2315 train_time:81222ms step_avg:60.43ms
step:1345/2315 train_time:81282ms step_avg:60.43ms
step:1346/2315 train_time:81344ms step_avg:60.43ms
step:1347/2315 train_time:81404ms step_avg:60.43ms
step:1348/2315 train_time:81465ms step_avg:60.43ms
step:1349/2315 train_time:81525ms step_avg:60.43ms
step:1350/2315 train_time:81586ms step_avg:60.43ms
step:1351/2315 train_time:81646ms step_avg:60.43ms
step:1352/2315 train_time:81708ms step_avg:60.43ms
step:1353/2315 train_time:81769ms step_avg:60.44ms
step:1354/2315 train_time:81830ms step_avg:60.44ms
step:1355/2315 train_time:81891ms step_avg:60.44ms
step:1356/2315 train_time:81952ms step_avg:60.44ms
step:1357/2315 train_time:82013ms step_avg:60.44ms
step:1358/2315 train_time:82075ms step_avg:60.44ms
step:1359/2315 train_time:82136ms step_avg:60.44ms
step:1360/2315 train_time:82198ms step_avg:60.44ms
step:1361/2315 train_time:82259ms step_avg:60.44ms
step:1362/2315 train_time:82320ms step_avg:60.44ms
step:1363/2315 train_time:82381ms step_avg:60.44ms
step:1364/2315 train_time:82441ms step_avg:60.44ms
step:1365/2315 train_time:82501ms step_avg:60.44ms
step:1366/2315 train_time:82562ms step_avg:60.44ms
step:1367/2315 train_time:82622ms step_avg:60.44ms
step:1368/2315 train_time:82684ms step_avg:60.44ms
step:1369/2315 train_time:82745ms step_avg:60.44ms
step:1370/2315 train_time:82805ms step_avg:60.44ms
step:1371/2315 train_time:82866ms step_avg:60.44ms
step:1372/2315 train_time:82926ms step_avg:60.44ms
step:1373/2315 train_time:82987ms step_avg:60.44ms
step:1374/2315 train_time:83048ms step_avg:60.44ms
step:1375/2315 train_time:83109ms step_avg:60.44ms
step:1376/2315 train_time:83170ms step_avg:60.44ms
step:1377/2315 train_time:83231ms step_avg:60.44ms
step:1378/2315 train_time:83293ms step_avg:60.44ms
step:1379/2315 train_time:83353ms step_avg:60.44ms
step:1380/2315 train_time:83415ms step_avg:60.45ms
step:1381/2315 train_time:83476ms step_avg:60.45ms
step:1382/2315 train_time:83537ms step_avg:60.45ms
step:1383/2315 train_time:83598ms step_avg:60.45ms
step:1384/2315 train_time:83660ms step_avg:60.45ms
step:1385/2315 train_time:83720ms step_avg:60.45ms
step:1386/2315 train_time:83781ms step_avg:60.45ms
step:1387/2315 train_time:83841ms step_avg:60.45ms
step:1388/2315 train_time:83903ms step_avg:60.45ms
step:1389/2315 train_time:83964ms step_avg:60.45ms
step:1390/2315 train_time:84025ms step_avg:60.45ms
step:1391/2315 train_time:84085ms step_avg:60.45ms
step:1392/2315 train_time:84146ms step_avg:60.45ms
step:1393/2315 train_time:84206ms step_avg:60.45ms
step:1394/2315 train_time:84267ms step_avg:60.45ms
step:1395/2315 train_time:84328ms step_avg:60.45ms
step:1396/2315 train_time:84388ms step_avg:60.45ms
step:1397/2315 train_time:84449ms step_avg:60.45ms
step:1398/2315 train_time:84510ms step_avg:60.45ms
step:1399/2315 train_time:84571ms step_avg:60.45ms
step:1400/2315 train_time:84632ms step_avg:60.45ms
step:1401/2315 train_time:84693ms step_avg:60.45ms
step:1402/2315 train_time:84755ms step_avg:60.45ms
step:1403/2315 train_time:84816ms step_avg:60.45ms
step:1404/2315 train_time:84878ms step_avg:60.45ms
step:1405/2315 train_time:84939ms step_avg:60.45ms
step:1406/2315 train_time:85000ms step_avg:60.46ms
step:1407/2315 train_time:85061ms step_avg:60.46ms
step:1408/2315 train_time:85121ms step_avg:60.46ms
step:1409/2315 train_time:85182ms step_avg:60.46ms
step:1410/2315 train_time:85243ms step_avg:60.46ms
step:1411/2315 train_time:85304ms step_avg:60.46ms
step:1412/2315 train_time:85365ms step_avg:60.46ms
step:1413/2315 train_time:85425ms step_avg:60.46ms
step:1414/2315 train_time:85486ms step_avg:60.46ms
step:1415/2315 train_time:85547ms step_avg:60.46ms
step:1416/2315 train_time:85608ms step_avg:60.46ms
step:1417/2315 train_time:85668ms step_avg:60.46ms
step:1418/2315 train_time:85729ms step_avg:60.46ms
step:1419/2315 train_time:85790ms step_avg:60.46ms
step:1420/2315 train_time:85852ms step_avg:60.46ms
step:1421/2315 train_time:85913ms step_avg:60.46ms
step:1422/2315 train_time:85974ms step_avg:60.46ms
step:1423/2315 train_time:86035ms step_avg:60.46ms
step:1424/2315 train_time:86097ms step_avg:60.46ms
step:1425/2315 train_time:86158ms step_avg:60.46ms
step:1426/2315 train_time:86219ms step_avg:60.46ms
step:1427/2315 train_time:86280ms step_avg:60.46ms
step:1428/2315 train_time:86341ms step_avg:60.46ms
step:1429/2315 train_time:86402ms step_avg:60.46ms
step:1430/2315 train_time:86462ms step_avg:60.46ms
step:1431/2315 train_time:86523ms step_avg:60.46ms
step:1432/2315 train_time:86584ms step_avg:60.46ms
step:1433/2315 train_time:86644ms step_avg:60.46ms
step:1434/2315 train_time:86705ms step_avg:60.46ms
step:1435/2315 train_time:86766ms step_avg:60.46ms
step:1436/2315 train_time:86826ms step_avg:60.46ms
step:1437/2315 train_time:86886ms step_avg:60.46ms
step:1438/2315 train_time:86947ms step_avg:60.46ms
step:1439/2315 train_time:87009ms step_avg:60.46ms
step:1440/2315 train_time:87071ms step_avg:60.47ms
step:1441/2315 train_time:87132ms step_avg:60.47ms
step:1442/2315 train_time:87194ms step_avg:60.47ms
step:1443/2315 train_time:87255ms step_avg:60.47ms
step:1444/2315 train_time:87316ms step_avg:60.47ms
step:1445/2315 train_time:87377ms step_avg:60.47ms
step:1446/2315 train_time:87438ms step_avg:60.47ms
step:1447/2315 train_time:87500ms step_avg:60.47ms
step:1448/2315 train_time:87561ms step_avg:60.47ms
step:1449/2315 train_time:87622ms step_avg:60.47ms
step:1450/2315 train_time:87682ms step_avg:60.47ms
step:1451/2315 train_time:87743ms step_avg:60.47ms
step:1452/2315 train_time:87804ms step_avg:60.47ms
step:1453/2315 train_time:87864ms step_avg:60.47ms
step:1454/2315 train_time:87925ms step_avg:60.47ms
step:1455/2315 train_time:87986ms step_avg:60.47ms
step:1456/2315 train_time:88046ms step_avg:60.47ms
step:1457/2315 train_time:88106ms step_avg:60.47ms
step:1458/2315 train_time:88167ms step_avg:60.47ms
step:1459/2315 train_time:88228ms step_avg:60.47ms
step:1460/2315 train_time:88290ms step_avg:60.47ms
step:1461/2315 train_time:88350ms step_avg:60.47ms
step:1462/2315 train_time:88411ms step_avg:60.47ms
step:1463/2315 train_time:88472ms step_avg:60.47ms
step:1464/2315 train_time:88533ms step_avg:60.47ms
step:1465/2315 train_time:88594ms step_avg:60.47ms
step:1466/2315 train_time:88656ms step_avg:60.48ms
step:1467/2315 train_time:88717ms step_avg:60.48ms
step:1468/2315 train_time:88779ms step_avg:60.48ms
step:1469/2315 train_time:88840ms step_avg:60.48ms
step:1470/2315 train_time:88901ms step_avg:60.48ms
step:1471/2315 train_time:88961ms step_avg:60.48ms
step:1472/2315 train_time:89022ms step_avg:60.48ms
step:1473/2315 train_time:89082ms step_avg:60.48ms
step:1474/2315 train_time:89144ms step_avg:60.48ms
step:1475/2315 train_time:89205ms step_avg:60.48ms
step:1476/2315 train_time:89265ms step_avg:60.48ms
step:1477/2315 train_time:89325ms step_avg:60.48ms
step:1478/2315 train_time:89386ms step_avg:60.48ms
step:1479/2315 train_time:89446ms step_avg:60.48ms
step:1480/2315 train_time:89506ms step_avg:60.48ms
step:1481/2315 train_time:89568ms step_avg:60.48ms
step:1482/2315 train_time:89629ms step_avg:60.48ms
step:1483/2315 train_time:89690ms step_avg:60.48ms
step:1484/2315 train_time:89751ms step_avg:60.48ms
step:1485/2315 train_time:89812ms step_avg:60.48ms
step:1486/2315 train_time:89873ms step_avg:60.48ms
step:1487/2315 train_time:89934ms step_avg:60.48ms
step:1488/2315 train_time:89995ms step_avg:60.48ms
step:1489/2315 train_time:90057ms step_avg:60.48ms
step:1490/2315 train_time:90118ms step_avg:60.48ms
step:1491/2315 train_time:90180ms step_avg:60.48ms
step:1492/2315 train_time:90241ms step_avg:60.48ms
step:1493/2315 train_time:90301ms step_avg:60.48ms
step:1494/2315 train_time:90362ms step_avg:60.48ms
step:1495/2315 train_time:90423ms step_avg:60.48ms
step:1496/2315 train_time:90484ms step_avg:60.48ms
step:1497/2315 train_time:90544ms step_avg:60.48ms
step:1498/2315 train_time:90605ms step_avg:60.48ms
step:1499/2315 train_time:90665ms step_avg:60.48ms
step:1500/2315 train_time:90726ms step_avg:60.48ms
step:1500/2315 val_loss:3.4501 train_time:90787ms step_avg:60.52ms
step:1501/2315 train_time:90805ms step_avg:60.50ms
step:1502/2315 train_time:90851ms step_avg:60.49ms
step:1503/2315 train_time:90915ms step_avg:60.49ms
step:1504/2315 train_time:90977ms step_avg:60.49ms
step:1505/2315 train_time:91037ms step_avg:60.49ms
step:1506/2315 train_time:91099ms step_avg:60.49ms
step:1507/2315 train_time:91159ms step_avg:60.49ms
step:1508/2315 train_time:91219ms step_avg:60.49ms
step:1509/2315 train_time:91279ms step_avg:60.49ms
step:1510/2315 train_time:91339ms step_avg:60.49ms
step:1511/2315 train_time:91399ms step_avg:60.49ms
step:1512/2315 train_time:91460ms step_avg:60.49ms
step:1513/2315 train_time:91520ms step_avg:60.49ms
step:1514/2315 train_time:91581ms step_avg:60.49ms
step:1515/2315 train_time:91641ms step_avg:60.49ms
step:1516/2315 train_time:91702ms step_avg:60.49ms
step:1517/2315 train_time:91764ms step_avg:60.49ms
step:1518/2315 train_time:91827ms step_avg:60.49ms
step:1519/2315 train_time:91890ms step_avg:60.49ms
step:1520/2315 train_time:91952ms step_avg:60.49ms
step:1521/2315 train_time:92014ms step_avg:60.50ms
step:1522/2315 train_time:92075ms step_avg:60.50ms
step:1523/2315 train_time:92135ms step_avg:60.50ms
step:1524/2315 train_time:92197ms step_avg:60.50ms
step:1525/2315 train_time:92257ms step_avg:60.50ms
step:1526/2315 train_time:92318ms step_avg:60.50ms
step:1527/2315 train_time:92379ms step_avg:60.50ms
step:1528/2315 train_time:92439ms step_avg:60.50ms
step:1529/2315 train_time:92500ms step_avg:60.50ms
step:1530/2315 train_time:92561ms step_avg:60.50ms
step:1531/2315 train_time:92621ms step_avg:60.50ms
step:1532/2315 train_time:92683ms step_avg:60.50ms
step:1533/2315 train_time:92745ms step_avg:60.50ms
step:1534/2315 train_time:92808ms step_avg:60.50ms
step:1535/2315 train_time:92870ms step_avg:60.50ms
step:1536/2315 train_time:92932ms step_avg:60.50ms
step:1537/2315 train_time:92994ms step_avg:60.50ms
step:1538/2315 train_time:93056ms step_avg:60.50ms
step:1539/2315 train_time:93117ms step_avg:60.50ms
step:1540/2315 train_time:93178ms step_avg:60.51ms
step:1541/2315 train_time:93239ms step_avg:60.51ms
step:1542/2315 train_time:93301ms step_avg:60.51ms
step:1543/2315 train_time:93361ms step_avg:60.51ms
step:1544/2315 train_time:93422ms step_avg:60.51ms
step:1545/2315 train_time:93483ms step_avg:60.51ms
step:1546/2315 train_time:93544ms step_avg:60.51ms
step:1547/2315 train_time:93605ms step_avg:60.51ms
step:1548/2315 train_time:93666ms step_avg:60.51ms
step:1549/2315 train_time:93728ms step_avg:60.51ms
step:1550/2315 train_time:93790ms step_avg:60.51ms
step:1551/2315 train_time:93852ms step_avg:60.51ms
step:1552/2315 train_time:93913ms step_avg:60.51ms
step:1553/2315 train_time:93974ms step_avg:60.51ms
step:1554/2315 train_time:94036ms step_avg:60.51ms
step:1555/2315 train_time:94098ms step_avg:60.51ms
step:1556/2315 train_time:94160ms step_avg:60.51ms
step:1557/2315 train_time:94221ms step_avg:60.51ms
step:1558/2315 train_time:94283ms step_avg:60.52ms
step:1559/2315 train_time:94343ms step_avg:60.52ms
step:1560/2315 train_time:94405ms step_avg:60.52ms
step:1561/2315 train_time:94466ms step_avg:60.52ms
step:1562/2315 train_time:94527ms step_avg:60.52ms
step:1563/2315 train_time:94588ms step_avg:60.52ms
step:1564/2315 train_time:94649ms step_avg:60.52ms
step:1565/2315 train_time:94710ms step_avg:60.52ms
step:1566/2315 train_time:94772ms step_avg:60.52ms
step:1567/2315 train_time:94832ms step_avg:60.52ms
step:1568/2315 train_time:94894ms step_avg:60.52ms
step:1569/2315 train_time:94955ms step_avg:60.52ms
step:1570/2315 train_time:95017ms step_avg:60.52ms
step:1571/2315 train_time:95078ms step_avg:60.52ms
step:1572/2315 train_time:95139ms step_avg:60.52ms
step:1573/2315 train_time:95200ms step_avg:60.52ms
step:1574/2315 train_time:95261ms step_avg:60.52ms
step:1575/2315 train_time:95322ms step_avg:60.52ms
step:1576/2315 train_time:95384ms step_avg:60.52ms
step:1577/2315 train_time:95444ms step_avg:60.52ms
step:1578/2315 train_time:95506ms step_avg:60.52ms
step:1579/2315 train_time:95567ms step_avg:60.52ms
step:1580/2315 train_time:95629ms step_avg:60.52ms
step:1581/2315 train_time:95690ms step_avg:60.53ms
step:1582/2315 train_time:95752ms step_avg:60.53ms
step:1583/2315 train_time:95813ms step_avg:60.53ms
step:1584/2315 train_time:95875ms step_avg:60.53ms
step:1585/2315 train_time:95936ms step_avg:60.53ms
step:1586/2315 train_time:95997ms step_avg:60.53ms
step:1587/2315 train_time:96057ms step_avg:60.53ms
step:1588/2315 train_time:96119ms step_avg:60.53ms
step:1589/2315 train_time:96179ms step_avg:60.53ms
step:1590/2315 train_time:96241ms step_avg:60.53ms
step:1591/2315 train_time:96302ms step_avg:60.53ms
step:1592/2315 train_time:96363ms step_avg:60.53ms
step:1593/2315 train_time:96424ms step_avg:60.53ms
step:1594/2315 train_time:96486ms step_avg:60.53ms
step:1595/2315 train_time:96547ms step_avg:60.53ms
step:1596/2315 train_time:96609ms step_avg:60.53ms
step:1597/2315 train_time:96670ms step_avg:60.53ms
step:1598/2315 train_time:96731ms step_avg:60.53ms
step:1599/2315 train_time:96793ms step_avg:60.53ms
step:1600/2315 train_time:96854ms step_avg:60.53ms
step:1601/2315 train_time:96915ms step_avg:60.53ms
step:1602/2315 train_time:96976ms step_avg:60.53ms
step:1603/2315 train_time:97037ms step_avg:60.53ms
step:1604/2315 train_time:97098ms step_avg:60.53ms
step:1605/2315 train_time:97158ms step_avg:60.53ms
step:1606/2315 train_time:97219ms step_avg:60.54ms
step:1607/2315 train_time:97280ms step_avg:60.54ms
step:1608/2315 train_time:97342ms step_avg:60.54ms
step:1609/2315 train_time:97403ms step_avg:60.54ms
step:1610/2315 train_time:97465ms step_avg:60.54ms
step:1611/2315 train_time:97526ms step_avg:60.54ms
step:1612/2315 train_time:97588ms step_avg:60.54ms
step:1613/2315 train_time:97650ms step_avg:60.54ms
step:1614/2315 train_time:97711ms step_avg:60.54ms
step:1615/2315 train_time:97772ms step_avg:60.54ms
step:1616/2315 train_time:97833ms step_avg:60.54ms
step:1617/2315 train_time:97894ms step_avg:60.54ms
step:1618/2315 train_time:97955ms step_avg:60.54ms
step:1619/2315 train_time:98016ms step_avg:60.54ms
step:1620/2315 train_time:98077ms step_avg:60.54ms
step:1621/2315 train_time:98138ms step_avg:60.54ms
step:1622/2315 train_time:98200ms step_avg:60.54ms
step:1623/2315 train_time:98261ms step_avg:60.54ms
step:1624/2315 train_time:98322ms step_avg:60.54ms
step:1625/2315 train_time:98385ms step_avg:60.54ms
step:1626/2315 train_time:98446ms step_avg:60.54ms
step:1627/2315 train_time:98507ms step_avg:60.55ms
step:1628/2315 train_time:98569ms step_avg:60.55ms
step:1629/2315 train_time:98631ms step_avg:60.55ms
step:1630/2315 train_time:98693ms step_avg:60.55ms
step:1631/2315 train_time:98753ms step_avg:60.55ms
step:1632/2315 train_time:98815ms step_avg:60.55ms
step:1633/2315 train_time:98876ms step_avg:60.55ms
step:1634/2315 train_time:98938ms step_avg:60.55ms
step:1635/2315 train_time:98998ms step_avg:60.55ms
step:1636/2315 train_time:99060ms step_avg:60.55ms
step:1637/2315 train_time:99120ms step_avg:60.55ms
step:1638/2315 train_time:99182ms step_avg:60.55ms
step:1639/2315 train_time:99243ms step_avg:60.55ms
step:1640/2315 train_time:99305ms step_avg:60.55ms
step:1641/2315 train_time:99366ms step_avg:60.55ms
step:1642/2315 train_time:99427ms step_avg:60.55ms
step:1643/2315 train_time:99488ms step_avg:60.55ms
step:1644/2315 train_time:99550ms step_avg:60.55ms
step:1645/2315 train_time:99612ms step_avg:60.55ms
step:1646/2315 train_time:99673ms step_avg:60.55ms
step:1647/2315 train_time:99735ms step_avg:60.56ms
step:1648/2315 train_time:99796ms step_avg:60.56ms
step:1649/2315 train_time:99857ms step_avg:60.56ms
step:1650/2315 train_time:99919ms step_avg:60.56ms
step:1651/2315 train_time:99980ms step_avg:60.56ms
step:1652/2315 train_time:100041ms step_avg:60.56ms
step:1653/2315 train_time:100102ms step_avg:60.56ms
step:1654/2315 train_time:100164ms step_avg:60.56ms
step:1655/2315 train_time:100225ms step_avg:60.56ms
step:1656/2315 train_time:100287ms step_avg:60.56ms
step:1657/2315 train_time:100348ms step_avg:60.56ms
step:1658/2315 train_time:100410ms step_avg:60.56ms
step:1659/2315 train_time:100470ms step_avg:60.56ms
step:1660/2315 train_time:100532ms step_avg:60.56ms
step:1661/2315 train_time:100592ms step_avg:60.56ms
step:1662/2315 train_time:100653ms step_avg:60.56ms
step:1663/2315 train_time:100714ms step_avg:60.56ms
step:1664/2315 train_time:100775ms step_avg:60.56ms
step:1665/2315 train_time:100837ms step_avg:60.56ms
step:1666/2315 train_time:100898ms step_avg:60.56ms
step:1667/2315 train_time:100959ms step_avg:60.56ms
step:1668/2315 train_time:101020ms step_avg:60.56ms
step:1669/2315 train_time:101081ms step_avg:60.56ms
step:1670/2315 train_time:101143ms step_avg:60.56ms
step:1671/2315 train_time:101204ms step_avg:60.57ms
step:1672/2315 train_time:101266ms step_avg:60.57ms
step:1673/2315 train_time:101328ms step_avg:60.57ms
step:1674/2315 train_time:101390ms step_avg:60.57ms
step:1675/2315 train_time:101451ms step_avg:60.57ms
step:1676/2315 train_time:101512ms step_avg:60.57ms
step:1677/2315 train_time:101573ms step_avg:60.57ms
step:1678/2315 train_time:101634ms step_avg:60.57ms
step:1679/2315 train_time:101695ms step_avg:60.57ms
step:1680/2315 train_time:101756ms step_avg:60.57ms
step:1681/2315 train_time:101817ms step_avg:60.57ms
step:1682/2315 train_time:101878ms step_avg:60.57ms
step:1683/2315 train_time:101939ms step_avg:60.57ms
step:1684/2315 train_time:102000ms step_avg:60.57ms
step:1685/2315 train_time:102061ms step_avg:60.57ms
step:1686/2315 train_time:102122ms step_avg:60.57ms
step:1687/2315 train_time:102184ms step_avg:60.57ms
step:1688/2315 train_time:102246ms step_avg:60.57ms
step:1689/2315 train_time:102307ms step_avg:60.57ms
step:1690/2315 train_time:102369ms step_avg:60.57ms
step:1691/2315 train_time:102430ms step_avg:60.57ms
step:1692/2315 train_time:102492ms step_avg:60.57ms
step:1693/2315 train_time:102552ms step_avg:60.57ms
step:1694/2315 train_time:102615ms step_avg:60.58ms
step:1695/2315 train_time:102675ms step_avg:60.58ms
step:1696/2315 train_time:102737ms step_avg:60.58ms
step:1697/2315 train_time:102797ms step_avg:60.58ms
step:1698/2315 train_time:102858ms step_avg:60.58ms
step:1699/2315 train_time:102920ms step_avg:60.58ms
step:1700/2315 train_time:102981ms step_avg:60.58ms
step:1701/2315 train_time:103042ms step_avg:60.58ms
step:1702/2315 train_time:103104ms step_avg:60.58ms
step:1703/2315 train_time:103165ms step_avg:60.58ms
step:1704/2315 train_time:103227ms step_avg:60.58ms
step:1705/2315 train_time:103289ms step_avg:60.58ms
step:1706/2315 train_time:103350ms step_avg:60.58ms
step:1707/2315 train_time:103411ms step_avg:60.58ms
step:1708/2315 train_time:103472ms step_avg:60.58ms
step:1709/2315 train_time:103533ms step_avg:60.58ms
step:1710/2315 train_time:103595ms step_avg:60.58ms
step:1711/2315 train_time:103656ms step_avg:60.58ms
step:1712/2315 train_time:103717ms step_avg:60.58ms
step:1713/2315 train_time:103778ms step_avg:60.58ms
step:1714/2315 train_time:103839ms step_avg:60.58ms
step:1715/2315 train_time:103900ms step_avg:60.58ms
step:1716/2315 train_time:103961ms step_avg:60.58ms
step:1717/2315 train_time:104022ms step_avg:60.58ms
step:1718/2315 train_time:104084ms step_avg:60.58ms
step:1719/2315 train_time:104145ms step_avg:60.58ms
step:1720/2315 train_time:104207ms step_avg:60.59ms
step:1721/2315 train_time:104268ms step_avg:60.59ms
step:1722/2315 train_time:104330ms step_avg:60.59ms
step:1723/2315 train_time:104392ms step_avg:60.59ms
step:1724/2315 train_time:104453ms step_avg:60.59ms
step:1725/2315 train_time:104514ms step_avg:60.59ms
step:1726/2315 train_time:104576ms step_avg:60.59ms
step:1727/2315 train_time:104637ms step_avg:60.59ms
step:1728/2315 train_time:104699ms step_avg:60.59ms
step:1729/2315 train_time:104760ms step_avg:60.59ms
step:1730/2315 train_time:104821ms step_avg:60.59ms
step:1731/2315 train_time:104882ms step_avg:60.59ms
step:1732/2315 train_time:104944ms step_avg:60.59ms
step:1733/2315 train_time:105005ms step_avg:60.59ms
step:1734/2315 train_time:105067ms step_avg:60.59ms
step:1735/2315 train_time:105128ms step_avg:60.59ms
step:1736/2315 train_time:105189ms step_avg:60.59ms
step:1737/2315 train_time:105250ms step_avg:60.59ms
step:1738/2315 train_time:105312ms step_avg:60.59ms
step:1739/2315 train_time:105372ms step_avg:60.59ms
step:1740/2315 train_time:105434ms step_avg:60.59ms
step:1741/2315 train_time:105495ms step_avg:60.59ms
step:1742/2315 train_time:105557ms step_avg:60.60ms
step:1743/2315 train_time:105617ms step_avg:60.60ms
step:1744/2315 train_time:105678ms step_avg:60.60ms
step:1745/2315 train_time:105739ms step_avg:60.60ms
step:1746/2315 train_time:105801ms step_avg:60.60ms
step:1747/2315 train_time:105862ms step_avg:60.60ms
step:1748/2315 train_time:105924ms step_avg:60.60ms
step:1749/2315 train_time:105985ms step_avg:60.60ms
step:1750/2315 train_time:106047ms step_avg:60.60ms
step:1750/2315 val_loss:3.3801 train_time:106110ms step_avg:60.63ms
step:1751/2315 train_time:106128ms step_avg:60.61ms
step:1752/2315 train_time:106175ms step_avg:60.60ms
step:1753/2315 train_time:106243ms step_avg:60.61ms
step:1754/2315 train_time:106307ms step_avg:60.61ms
step:1755/2315 train_time:106368ms step_avg:60.61ms
step:1756/2315 train_time:106428ms step_avg:60.61ms
step:1757/2315 train_time:106487ms step_avg:60.61ms
step:1758/2315 train_time:106548ms step_avg:60.61ms
step:1759/2315 train_time:106607ms step_avg:60.61ms
step:1760/2315 train_time:106667ms step_avg:60.61ms
step:1761/2315 train_time:106727ms step_avg:60.61ms
step:1762/2315 train_time:106787ms step_avg:60.61ms
step:1763/2315 train_time:106847ms step_avg:60.61ms
step:1764/2315 train_time:106907ms step_avg:60.60ms
step:1765/2315 train_time:106967ms step_avg:60.60ms
step:1766/2315 train_time:107032ms step_avg:60.61ms
step:1767/2315 train_time:107094ms step_avg:60.61ms
step:1768/2315 train_time:107157ms step_avg:60.61ms
step:1769/2315 train_time:107222ms step_avg:60.61ms
step:1770/2315 train_time:107284ms step_avg:60.61ms
step:1771/2315 train_time:107346ms step_avg:60.61ms
step:1772/2315 train_time:107407ms step_avg:60.61ms
step:1773/2315 train_time:107468ms step_avg:60.61ms
step:1774/2315 train_time:107529ms step_avg:60.61ms
step:1775/2315 train_time:107589ms step_avg:60.61ms
step:1776/2315 train_time:107650ms step_avg:60.61ms
step:1777/2315 train_time:107710ms step_avg:60.61ms
step:1778/2315 train_time:107771ms step_avg:60.61ms
step:1779/2315 train_time:107831ms step_avg:60.61ms
step:1780/2315 train_time:107891ms step_avg:60.61ms
step:1781/2315 train_time:107952ms step_avg:60.61ms
step:1782/2315 train_time:108014ms step_avg:60.61ms
step:1783/2315 train_time:108075ms step_avg:60.61ms
step:1784/2315 train_time:108137ms step_avg:60.62ms
step:1785/2315 train_time:108199ms step_avg:60.62ms
step:1786/2315 train_time:108262ms step_avg:60.62ms
step:1787/2315 train_time:108324ms step_avg:60.62ms
step:1788/2315 train_time:108386ms step_avg:60.62ms
step:1789/2315 train_time:108447ms step_avg:60.62ms
step:1790/2315 train_time:108508ms step_avg:60.62ms
step:1791/2315 train_time:108569ms step_avg:60.62ms
step:1792/2315 train_time:108629ms step_avg:60.62ms
step:1793/2315 train_time:108689ms step_avg:60.62ms
step:1794/2315 train_time:108750ms step_avg:60.62ms
step:1795/2315 train_time:108810ms step_avg:60.62ms
step:1796/2315 train_time:108871ms step_avg:60.62ms
step:1797/2315 train_time:108931ms step_avg:60.62ms
step:1798/2315 train_time:108993ms step_avg:60.62ms
step:1799/2315 train_time:109054ms step_avg:60.62ms
step:1800/2315 train_time:109116ms step_avg:60.62ms
step:1801/2315 train_time:109178ms step_avg:60.62ms
step:1802/2315 train_time:109240ms step_avg:60.62ms
step:1803/2315 train_time:109301ms step_avg:60.62ms
step:1804/2315 train_time:109364ms step_avg:60.62ms
step:1805/2315 train_time:109425ms step_avg:60.62ms
step:1806/2315 train_time:109487ms step_avg:60.62ms
step:1807/2315 train_time:109547ms step_avg:60.62ms
step:1808/2315 train_time:109608ms step_avg:60.62ms
step:1809/2315 train_time:109669ms step_avg:60.62ms
step:1810/2315 train_time:109730ms step_avg:60.62ms
step:1811/2315 train_time:109790ms step_avg:60.62ms
step:1812/2315 train_time:109851ms step_avg:60.62ms
step:1813/2315 train_time:109911ms step_avg:60.62ms
step:1814/2315 train_time:109972ms step_avg:60.62ms
step:1815/2315 train_time:110032ms step_avg:60.62ms
step:1816/2315 train_time:110094ms step_avg:60.62ms
step:1817/2315 train_time:110155ms step_avg:60.62ms
step:1818/2315 train_time:110218ms step_avg:60.63ms
step:1819/2315 train_time:110279ms step_avg:60.63ms
step:1820/2315 train_time:110341ms step_avg:60.63ms
step:1821/2315 train_time:110402ms step_avg:60.63ms
step:1822/2315 train_time:110464ms step_avg:60.63ms
step:1823/2315 train_time:110526ms step_avg:60.63ms
step:1824/2315 train_time:110587ms step_avg:60.63ms
step:1825/2315 train_time:110647ms step_avg:60.63ms
step:1826/2315 train_time:110708ms step_avg:60.63ms
step:1827/2315 train_time:110769ms step_avg:60.63ms
step:1828/2315 train_time:110830ms step_avg:60.63ms
step:1829/2315 train_time:110890ms step_avg:60.63ms
step:1830/2315 train_time:110951ms step_avg:60.63ms
step:1831/2315 train_time:111012ms step_avg:60.63ms
step:1832/2315 train_time:111073ms step_avg:60.63ms
step:1833/2315 train_time:111133ms step_avg:60.63ms
step:1834/2315 train_time:111195ms step_avg:60.63ms
step:1835/2315 train_time:111257ms step_avg:60.63ms
step:1836/2315 train_time:111320ms step_avg:60.63ms
step:1837/2315 train_time:111381ms step_avg:60.63ms
step:1838/2315 train_time:111443ms step_avg:60.63ms
step:1839/2315 train_time:111504ms step_avg:60.63ms
step:1840/2315 train_time:111566ms step_avg:60.63ms
step:1841/2315 train_time:111627ms step_avg:60.63ms
step:1842/2315 train_time:111688ms step_avg:60.63ms
step:1843/2315 train_time:111749ms step_avg:60.63ms
step:1844/2315 train_time:111810ms step_avg:60.63ms
step:1845/2315 train_time:111871ms step_avg:60.63ms
step:1846/2315 train_time:111932ms step_avg:60.63ms
step:1847/2315 train_time:111992ms step_avg:60.63ms
step:1848/2315 train_time:112054ms step_avg:60.64ms
step:1849/2315 train_time:112114ms step_avg:60.64ms
step:1850/2315 train_time:112176ms step_avg:60.64ms
step:1851/2315 train_time:112238ms step_avg:60.64ms
step:1852/2315 train_time:112299ms step_avg:60.64ms
step:1853/2315 train_time:112361ms step_avg:60.64ms
step:1854/2315 train_time:112423ms step_avg:60.64ms
step:1855/2315 train_time:112485ms step_avg:60.64ms
step:1856/2315 train_time:112546ms step_avg:60.64ms
step:1857/2315 train_time:112607ms step_avg:60.64ms
step:1858/2315 train_time:112668ms step_avg:60.64ms
step:1859/2315 train_time:112729ms step_avg:60.64ms
step:1860/2315 train_time:112790ms step_avg:60.64ms
step:1861/2315 train_time:112851ms step_avg:60.64ms
step:1862/2315 train_time:112912ms step_avg:60.64ms
step:1863/2315 train_time:112973ms step_avg:60.64ms
step:1864/2315 train_time:113034ms step_avg:60.64ms
step:1865/2315 train_time:113094ms step_avg:60.64ms
step:1866/2315 train_time:113156ms step_avg:60.64ms
step:1867/2315 train_time:113217ms step_avg:60.64ms
step:1868/2315 train_time:113279ms step_avg:60.64ms
step:1869/2315 train_time:113340ms step_avg:60.64ms
step:1870/2315 train_time:113402ms step_avg:60.64ms
step:1871/2315 train_time:113463ms step_avg:60.64ms
step:1872/2315 train_time:113526ms step_avg:60.64ms
step:1873/2315 train_time:113587ms step_avg:60.64ms
step:1874/2315 train_time:113648ms step_avg:60.64ms
step:1875/2315 train_time:113708ms step_avg:60.64ms
step:1876/2315 train_time:113770ms step_avg:60.64ms
step:1877/2315 train_time:113831ms step_avg:60.64ms
step:1878/2315 train_time:113892ms step_avg:60.65ms
step:1879/2315 train_time:113953ms step_avg:60.65ms
step:1880/2315 train_time:114014ms step_avg:60.65ms
step:1881/2315 train_time:114074ms step_avg:60.65ms
step:1882/2315 train_time:114136ms step_avg:60.65ms
step:1883/2315 train_time:114197ms step_avg:60.65ms
step:1884/2315 train_time:114259ms step_avg:60.65ms
step:1885/2315 train_time:114320ms step_avg:60.65ms
step:1886/2315 train_time:114383ms step_avg:60.65ms
step:1887/2315 train_time:114444ms step_avg:60.65ms
step:1888/2315 train_time:114506ms step_avg:60.65ms
step:1889/2315 train_time:114567ms step_avg:60.65ms
step:1890/2315 train_time:114628ms step_avg:60.65ms
step:1891/2315 train_time:114688ms step_avg:60.65ms
step:1892/2315 train_time:114750ms step_avg:60.65ms
step:1893/2315 train_time:114810ms step_avg:60.65ms
step:1894/2315 train_time:114873ms step_avg:60.65ms
step:1895/2315 train_time:114933ms step_avg:60.65ms
step:1896/2315 train_time:114994ms step_avg:60.65ms
step:1897/2315 train_time:115055ms step_avg:60.65ms
step:1898/2315 train_time:115116ms step_avg:60.65ms
step:1899/2315 train_time:115177ms step_avg:60.65ms
step:1900/2315 train_time:115239ms step_avg:60.65ms
step:1901/2315 train_time:115301ms step_avg:60.65ms
step:1902/2315 train_time:115364ms step_avg:60.65ms
step:1903/2315 train_time:115425ms step_avg:60.65ms
step:1904/2315 train_time:115486ms step_avg:60.65ms
step:1905/2315 train_time:115547ms step_avg:60.65ms
step:1906/2315 train_time:115609ms step_avg:60.66ms
step:1907/2315 train_time:115671ms step_avg:60.66ms
step:1908/2315 train_time:115732ms step_avg:60.66ms
step:1909/2315 train_time:115793ms step_avg:60.66ms
step:1910/2315 train_time:115854ms step_avg:60.66ms
step:1911/2315 train_time:115915ms step_avg:60.66ms
step:1912/2315 train_time:115977ms step_avg:60.66ms
step:1913/2315 train_time:116038ms step_avg:60.66ms
step:1914/2315 train_time:116099ms step_avg:60.66ms
step:1915/2315 train_time:116160ms step_avg:60.66ms
step:1916/2315 train_time:116222ms step_avg:60.66ms
step:1917/2315 train_time:116283ms step_avg:60.66ms
step:1918/2315 train_time:116345ms step_avg:60.66ms
step:1919/2315 train_time:116406ms step_avg:60.66ms
step:1920/2315 train_time:116469ms step_avg:60.66ms
step:1921/2315 train_time:116530ms step_avg:60.66ms
step:1922/2315 train_time:116591ms step_avg:60.66ms
step:1923/2315 train_time:116652ms step_avg:60.66ms
step:1924/2315 train_time:116713ms step_avg:60.66ms
step:1925/2315 train_time:116774ms step_avg:60.66ms
step:1926/2315 train_time:116835ms step_avg:60.66ms
step:1927/2315 train_time:116895ms step_avg:60.66ms
step:1928/2315 train_time:116957ms step_avg:60.66ms
step:1929/2315 train_time:117018ms step_avg:60.66ms
step:1930/2315 train_time:117080ms step_avg:60.66ms
step:1931/2315 train_time:117141ms step_avg:60.66ms
step:1932/2315 train_time:117202ms step_avg:60.66ms
step:1933/2315 train_time:117263ms step_avg:60.66ms
step:1934/2315 train_time:117325ms step_avg:60.66ms
step:1935/2315 train_time:117386ms step_avg:60.66ms
step:1936/2315 train_time:117448ms step_avg:60.67ms
step:1937/2315 train_time:117508ms step_avg:60.67ms
step:1938/2315 train_time:117570ms step_avg:60.67ms
step:1939/2315 train_time:117630ms step_avg:60.67ms
step:1940/2315 train_time:117692ms step_avg:60.67ms
step:1941/2315 train_time:117752ms step_avg:60.67ms
step:1942/2315 train_time:117814ms step_avg:60.67ms
step:1943/2315 train_time:117874ms step_avg:60.67ms
step:1944/2315 train_time:117935ms step_avg:60.67ms
step:1945/2315 train_time:117996ms step_avg:60.67ms
step:1946/2315 train_time:118058ms step_avg:60.67ms
step:1947/2315 train_time:118119ms step_avg:60.67ms
step:1948/2315 train_time:118181ms step_avg:60.67ms
step:1949/2315 train_time:118242ms step_avg:60.67ms
step:1950/2315 train_time:118304ms step_avg:60.67ms
step:1951/2315 train_time:118366ms step_avg:60.67ms
step:1952/2315 train_time:118428ms step_avg:60.67ms
step:1953/2315 train_time:118489ms step_avg:60.67ms
step:1954/2315 train_time:118550ms step_avg:60.67ms
step:1955/2315 train_time:118610ms step_avg:60.67ms
step:1956/2315 train_time:118672ms step_avg:60.67ms
step:1957/2315 train_time:118733ms step_avg:60.67ms
step:1958/2315 train_time:118794ms step_avg:60.67ms
step:1959/2315 train_time:118855ms step_avg:60.67ms
step:1960/2315 train_time:118916ms step_avg:60.67ms
step:1961/2315 train_time:118977ms step_avg:60.67ms
step:1962/2315 train_time:119039ms step_avg:60.67ms
step:1963/2315 train_time:119100ms step_avg:60.67ms
step:1964/2315 train_time:119162ms step_avg:60.67ms
step:1965/2315 train_time:119223ms step_avg:60.67ms
step:1966/2315 train_time:119285ms step_avg:60.67ms
step:1967/2315 train_time:119346ms step_avg:60.67ms
step:1968/2315 train_time:119408ms step_avg:60.67ms
step:1969/2315 train_time:119468ms step_avg:60.67ms
step:1970/2315 train_time:119530ms step_avg:60.67ms
step:1971/2315 train_time:119591ms step_avg:60.68ms
step:1972/2315 train_time:119653ms step_avg:60.68ms
step:1973/2315 train_time:119713ms step_avg:60.68ms
step:1974/2315 train_time:119775ms step_avg:60.68ms
step:1975/2315 train_time:119835ms step_avg:60.68ms
step:1976/2315 train_time:119897ms step_avg:60.68ms
step:1977/2315 train_time:119958ms step_avg:60.68ms
step:1978/2315 train_time:120019ms step_avg:60.68ms
step:1979/2315 train_time:120079ms step_avg:60.68ms
step:1980/2315 train_time:120141ms step_avg:60.68ms
step:1981/2315 train_time:120203ms step_avg:60.68ms
step:1982/2315 train_time:120265ms step_avg:60.68ms
step:1983/2315 train_time:120326ms step_avg:60.68ms
step:1984/2315 train_time:120388ms step_avg:60.68ms
step:1985/2315 train_time:120448ms step_avg:60.68ms
step:1986/2315 train_time:120510ms step_avg:60.68ms
step:1987/2315 train_time:120571ms step_avg:60.68ms
step:1988/2315 train_time:120632ms step_avg:60.68ms
step:1989/2315 train_time:120693ms step_avg:60.68ms
step:1990/2315 train_time:120754ms step_avg:60.68ms
step:1991/2315 train_time:120815ms step_avg:60.68ms
step:1992/2315 train_time:120876ms step_avg:60.68ms
step:1993/2315 train_time:120937ms step_avg:60.68ms
step:1994/2315 train_time:120999ms step_avg:60.68ms
step:1995/2315 train_time:121061ms step_avg:60.68ms
step:1996/2315 train_time:121122ms step_avg:60.68ms
step:1997/2315 train_time:121183ms step_avg:60.68ms
step:1998/2315 train_time:121245ms step_avg:60.68ms
step:1999/2315 train_time:121306ms step_avg:60.68ms
step:2000/2315 train_time:121367ms step_avg:60.68ms
step:2000/2315 val_loss:3.3308 train_time:121430ms step_avg:60.71ms
step:2001/2315 train_time:121448ms step_avg:60.69ms
step:2002/2315 train_time:121494ms step_avg:60.69ms
step:2003/2315 train_time:121557ms step_avg:60.69ms
step:2004/2315 train_time:121620ms step_avg:60.69ms
step:2005/2315 train_time:121681ms step_avg:60.69ms
step:2006/2315 train_time:121742ms step_avg:60.69ms
step:2007/2315 train_time:121803ms step_avg:60.69ms
step:2008/2315 train_time:121865ms step_avg:60.69ms
step:2009/2315 train_time:121925ms step_avg:60.69ms
step:2010/2315 train_time:121986ms step_avg:60.69ms
step:2011/2315 train_time:122046ms step_avg:60.69ms
step:2012/2315 train_time:122107ms step_avg:60.69ms
step:2013/2315 train_time:122167ms step_avg:60.69ms
step:2014/2315 train_time:122228ms step_avg:60.69ms
step:2015/2315 train_time:122288ms step_avg:60.69ms
step:2016/2315 train_time:122349ms step_avg:60.69ms
step:2017/2315 train_time:122412ms step_avg:60.69ms
step:2018/2315 train_time:122475ms step_avg:60.69ms
step:2019/2315 train_time:122538ms step_avg:60.69ms
step:2020/2315 train_time:122601ms step_avg:60.69ms
step:2021/2315 train_time:122662ms step_avg:60.69ms
step:2022/2315 train_time:122724ms step_avg:60.69ms
step:2023/2315 train_time:122784ms step_avg:60.69ms
step:2024/2315 train_time:122846ms step_avg:60.69ms
step:2025/2315 train_time:122906ms step_avg:60.69ms
step:2026/2315 train_time:122967ms step_avg:60.69ms
step:2027/2315 train_time:123027ms step_avg:60.69ms
step:2028/2315 train_time:123088ms step_avg:60.69ms
step:2029/2315 train_time:123148ms step_avg:60.69ms
step:2030/2315 train_time:123208ms step_avg:60.69ms
step:2031/2315 train_time:123269ms step_avg:60.69ms
step:2032/2315 train_time:123330ms step_avg:60.69ms
step:2033/2315 train_time:123391ms step_avg:60.69ms
step:2034/2315 train_time:123453ms step_avg:60.69ms
step:2035/2315 train_time:123514ms step_avg:60.69ms
step:2036/2315 train_time:123576ms step_avg:60.70ms
step:2037/2315 train_time:123638ms step_avg:60.70ms
step:2038/2315 train_time:123700ms step_avg:60.70ms
step:2039/2315 train_time:123762ms step_avg:60.70ms
step:2040/2315 train_time:123824ms step_avg:60.70ms
step:2041/2315 train_time:123885ms step_avg:60.70ms
step:2042/2315 train_time:123946ms step_avg:60.70ms
step:2043/2315 train_time:124007ms step_avg:60.70ms
step:2044/2315 train_time:124067ms step_avg:60.70ms
step:2045/2315 train_time:124128ms step_avg:60.70ms
step:2046/2315 train_time:124189ms step_avg:60.70ms
step:2047/2315 train_time:124250ms step_avg:60.70ms
step:2048/2315 train_time:124311ms step_avg:60.70ms
step:2049/2315 train_time:124372ms step_avg:60.70ms
step:2050/2315 train_time:124433ms step_avg:60.70ms
step:2051/2315 train_time:124494ms step_avg:60.70ms
step:2052/2315 train_time:124555ms step_avg:60.70ms
step:2053/2315 train_time:124616ms step_avg:60.70ms
step:2054/2315 train_time:124679ms step_avg:60.70ms
step:2055/2315 train_time:124741ms step_avg:60.70ms
step:2056/2315 train_time:124803ms step_avg:60.70ms
step:2057/2315 train_time:124864ms step_avg:60.70ms
step:2058/2315 train_time:124925ms step_avg:60.70ms
step:2059/2315 train_time:124986ms step_avg:60.70ms
step:2060/2315 train_time:125048ms step_avg:60.70ms
step:2061/2315 train_time:125108ms step_avg:60.70ms
step:2062/2315 train_time:125169ms step_avg:60.70ms
step:2063/2315 train_time:125229ms step_avg:60.70ms
step:2064/2315 train_time:125291ms step_avg:60.70ms
step:2065/2315 train_time:125352ms step_avg:60.70ms
step:2066/2315 train_time:125414ms step_avg:60.70ms
step:2067/2315 train_time:125475ms step_avg:60.70ms
step:2068/2315 train_time:125537ms step_avg:60.70ms
step:2069/2315 train_time:125598ms step_avg:60.70ms
step:2070/2315 train_time:125660ms step_avg:60.71ms
step:2071/2315 train_time:125721ms step_avg:60.71ms
step:2072/2315 train_time:125783ms step_avg:60.71ms
step:2073/2315 train_time:125844ms step_avg:60.71ms
step:2074/2315 train_time:125905ms step_avg:60.71ms
step:2075/2315 train_time:125966ms step_avg:60.71ms
step:2076/2315 train_time:126028ms step_avg:60.71ms
step:2077/2315 train_time:126088ms step_avg:60.71ms
step:2078/2315 train_time:126149ms step_avg:60.71ms
step:2079/2315 train_time:126209ms step_avg:60.71ms
step:2080/2315 train_time:126270ms step_avg:60.71ms
step:2081/2315 train_time:126331ms step_avg:60.71ms
step:2082/2315 train_time:126393ms step_avg:60.71ms
step:2083/2315 train_time:126454ms step_avg:60.71ms
step:2084/2315 train_time:126515ms step_avg:60.71ms
step:2085/2315 train_time:126576ms step_avg:60.71ms
step:2086/2315 train_time:126638ms step_avg:60.71ms
step:2087/2315 train_time:126699ms step_avg:60.71ms
step:2088/2315 train_time:126762ms step_avg:60.71ms
step:2089/2315 train_time:126823ms step_avg:60.71ms
step:2090/2315 train_time:126885ms step_avg:60.71ms
step:2091/2315 train_time:126946ms step_avg:60.71ms
step:2092/2315 train_time:127008ms step_avg:60.71ms
step:2093/2315 train_time:127068ms step_avg:60.71ms
step:2094/2315 train_time:127130ms step_avg:60.71ms
step:2095/2315 train_time:127191ms step_avg:60.71ms
step:2096/2315 train_time:127252ms step_avg:60.71ms
step:2097/2315 train_time:127313ms step_avg:60.71ms
step:2098/2315 train_time:127374ms step_avg:60.71ms
step:2099/2315 train_time:127434ms step_avg:60.71ms
step:2100/2315 train_time:127496ms step_avg:60.71ms
step:2101/2315 train_time:127557ms step_avg:60.71ms
step:2102/2315 train_time:127619ms step_avg:60.71ms
step:2103/2315 train_time:127680ms step_avg:60.71ms
step:2104/2315 train_time:127743ms step_avg:60.71ms
step:2105/2315 train_time:127804ms step_avg:60.71ms
step:2106/2315 train_time:127866ms step_avg:60.72ms
step:2107/2315 train_time:127928ms step_avg:60.72ms
step:2108/2315 train_time:127989ms step_avg:60.72ms
step:2109/2315 train_time:128050ms step_avg:60.72ms
step:2110/2315 train_time:128111ms step_avg:60.72ms
step:2111/2315 train_time:128172ms step_avg:60.72ms
step:2112/2315 train_time:128233ms step_avg:60.72ms
step:2113/2315 train_time:128294ms step_avg:60.72ms
step:2114/2315 train_time:128356ms step_avg:60.72ms
step:2115/2315 train_time:128416ms step_avg:60.72ms
step:2116/2315 train_time:128478ms step_avg:60.72ms
step:2117/2315 train_time:128538ms step_avg:60.72ms
step:2118/2315 train_time:128600ms step_avg:60.72ms
step:2119/2315 train_time:128661ms step_avg:60.72ms
step:2120/2315 train_time:128723ms step_avg:60.72ms
step:2121/2315 train_time:128784ms step_avg:60.72ms
step:2122/2315 train_time:128846ms step_avg:60.72ms
step:2123/2315 train_time:128907ms step_avg:60.72ms
step:2124/2315 train_time:128969ms step_avg:60.72ms
step:2125/2315 train_time:129030ms step_avg:60.72ms
step:2126/2315 train_time:129092ms step_avg:60.72ms
step:2127/2315 train_time:129152ms step_avg:60.72ms
step:2128/2315 train_time:129214ms step_avg:60.72ms
step:2129/2315 train_time:129275ms step_avg:60.72ms
step:2130/2315 train_time:129336ms step_avg:60.72ms
step:2131/2315 train_time:129397ms step_avg:60.72ms
step:2132/2315 train_time:129459ms step_avg:60.72ms
step:2133/2315 train_time:129520ms step_avg:60.72ms
step:2134/2315 train_time:129582ms step_avg:60.72ms
step:2135/2315 train_time:129643ms step_avg:60.72ms
step:2136/2315 train_time:129705ms step_avg:60.72ms
step:2137/2315 train_time:129766ms step_avg:60.72ms
step:2138/2315 train_time:129827ms step_avg:60.72ms
step:2139/2315 train_time:129888ms step_avg:60.72ms
step:2140/2315 train_time:129950ms step_avg:60.72ms
step:2141/2315 train_time:130010ms step_avg:60.72ms
step:2142/2315 train_time:130071ms step_avg:60.72ms
step:2143/2315 train_time:130132ms step_avg:60.72ms
step:2144/2315 train_time:130195ms step_avg:60.73ms
step:2145/2315 train_time:130255ms step_avg:60.72ms
step:2146/2315 train_time:130317ms step_avg:60.73ms
step:2147/2315 train_time:130377ms step_avg:60.73ms
step:2148/2315 train_time:130439ms step_avg:60.73ms
step:2149/2315 train_time:130500ms step_avg:60.73ms
step:2150/2315 train_time:130562ms step_avg:60.73ms
step:2151/2315 train_time:130623ms step_avg:60.73ms
step:2152/2315 train_time:130685ms step_avg:60.73ms
step:2153/2315 train_time:130746ms step_avg:60.73ms
step:2154/2315 train_time:130808ms step_avg:60.73ms
step:2155/2315 train_time:130868ms step_avg:60.73ms
step:2156/2315 train_time:130929ms step_avg:60.73ms
step:2157/2315 train_time:130990ms step_avg:60.73ms
step:2158/2315 train_time:131052ms step_avg:60.73ms
step:2159/2315 train_time:131113ms step_avg:60.73ms
step:2160/2315 train_time:131174ms step_avg:60.73ms
step:2161/2315 train_time:131235ms step_avg:60.73ms
step:2162/2315 train_time:131296ms step_avg:60.73ms
step:2163/2315 train_time:131358ms step_avg:60.73ms
step:2164/2315 train_time:131419ms step_avg:60.73ms
step:2165/2315 train_time:131480ms step_avg:60.73ms
step:2166/2315 train_time:131542ms step_avg:60.73ms
step:2167/2315 train_time:131603ms step_avg:60.73ms
step:2168/2315 train_time:131665ms step_avg:60.73ms
step:2169/2315 train_time:131726ms step_avg:60.73ms
step:2170/2315 train_time:131787ms step_avg:60.73ms
step:2171/2315 train_time:131848ms step_avg:60.73ms
step:2172/2315 train_time:131910ms step_avg:60.73ms
step:2173/2315 train_time:131972ms step_avg:60.73ms
step:2174/2315 train_time:132033ms step_avg:60.73ms
step:2175/2315 train_time:132095ms step_avg:60.73ms
step:2176/2315 train_time:132157ms step_avg:60.73ms
step:2177/2315 train_time:132218ms step_avg:60.73ms
step:2178/2315 train_time:132280ms step_avg:60.73ms
step:2179/2315 train_time:132341ms step_avg:60.73ms
step:2180/2315 train_time:132403ms step_avg:60.74ms
step:2181/2315 train_time:132464ms step_avg:60.74ms
step:2182/2315 train_time:132526ms step_avg:60.74ms
step:2183/2315 train_time:132587ms step_avg:60.74ms
step:2184/2315 train_time:132648ms step_avg:60.74ms
step:2185/2315 train_time:132709ms step_avg:60.74ms
step:2186/2315 train_time:132770ms step_avg:60.74ms
step:2187/2315 train_time:132831ms step_avg:60.74ms
step:2188/2315 train_time:132893ms step_avg:60.74ms
step:2189/2315 train_time:132954ms step_avg:60.74ms
step:2190/2315 train_time:133015ms step_avg:60.74ms
step:2191/2315 train_time:133076ms step_avg:60.74ms
step:2192/2315 train_time:133138ms step_avg:60.74ms
step:2193/2315 train_time:133199ms step_avg:60.74ms
step:2194/2315 train_time:133261ms step_avg:60.74ms
step:2195/2315 train_time:133322ms step_avg:60.74ms
step:2196/2315 train_time:133384ms step_avg:60.74ms
step:2197/2315 train_time:133445ms step_avg:60.74ms
step:2198/2315 train_time:133507ms step_avg:60.74ms
step:2199/2315 train_time:133568ms step_avg:60.74ms
step:2200/2315 train_time:133630ms step_avg:60.74ms
step:2201/2315 train_time:133691ms step_avg:60.74ms
step:2202/2315 train_time:133752ms step_avg:60.74ms
step:2203/2315 train_time:133813ms step_avg:60.74ms
step:2204/2315 train_time:133875ms step_avg:60.74ms
step:2205/2315 train_time:133936ms step_avg:60.74ms
step:2206/2315 train_time:133997ms step_avg:60.74ms
step:2207/2315 train_time:134058ms step_avg:60.74ms
step:2208/2315 train_time:134120ms step_avg:60.74ms
step:2209/2315 train_time:134181ms step_avg:60.74ms
step:2210/2315 train_time:134243ms step_avg:60.74ms
step:2211/2315 train_time:134304ms step_avg:60.74ms
step:2212/2315 train_time:134365ms step_avg:60.74ms
step:2213/2315 train_time:134426ms step_avg:60.74ms
step:2214/2315 train_time:134488ms step_avg:60.74ms
step:2215/2315 train_time:134549ms step_avg:60.74ms
step:2216/2315 train_time:134610ms step_avg:60.74ms
step:2217/2315 train_time:134671ms step_avg:60.74ms
step:2218/2315 train_time:134732ms step_avg:60.74ms
step:2219/2315 train_time:134793ms step_avg:60.74ms
step:2220/2315 train_time:134854ms step_avg:60.75ms
step:2221/2315 train_time:134914ms step_avg:60.74ms
step:2222/2315 train_time:134976ms step_avg:60.75ms
step:2223/2315 train_time:135038ms step_avg:60.75ms
step:2224/2315 train_time:135100ms step_avg:60.75ms
step:2225/2315 train_time:135161ms step_avg:60.75ms
step:2226/2315 train_time:135223ms step_avg:60.75ms
step:2227/2315 train_time:135285ms step_avg:60.75ms
step:2228/2315 train_time:135346ms step_avg:60.75ms
step:2229/2315 train_time:135407ms step_avg:60.75ms
step:2230/2315 train_time:135468ms step_avg:60.75ms
step:2231/2315 train_time:135529ms step_avg:60.75ms
step:2232/2315 train_time:135591ms step_avg:60.75ms
step:2233/2315 train_time:135651ms step_avg:60.75ms
step:2234/2315 train_time:135712ms step_avg:60.75ms
step:2235/2315 train_time:135773ms step_avg:60.75ms
step:2236/2315 train_time:135834ms step_avg:60.75ms
step:2237/2315 train_time:135894ms step_avg:60.75ms
step:2238/2315 train_time:135956ms step_avg:60.75ms
step:2239/2315 train_time:136017ms step_avg:60.75ms
step:2240/2315 train_time:136079ms step_avg:60.75ms
step:2241/2315 train_time:136140ms step_avg:60.75ms
step:2242/2315 train_time:136202ms step_avg:60.75ms
step:2243/2315 train_time:136263ms step_avg:60.75ms
step:2244/2315 train_time:136325ms step_avg:60.75ms
step:2245/2315 train_time:136386ms step_avg:60.75ms
step:2246/2315 train_time:136448ms step_avg:60.75ms
step:2247/2315 train_time:136509ms step_avg:60.75ms
step:2248/2315 train_time:136570ms step_avg:60.75ms
step:2249/2315 train_time:136631ms step_avg:60.75ms
step:2250/2315 train_time:136693ms step_avg:60.75ms
step:2250/2315 val_loss:3.2909 train_time:136755ms step_avg:60.78ms
step:2251/2315 train_time:136775ms step_avg:60.76ms
step:2252/2315 train_time:136819ms step_avg:60.75ms
step:2253/2315 train_time:136884ms step_avg:60.76ms
step:2254/2315 train_time:136946ms step_avg:60.76ms
step:2255/2315 train_time:137007ms step_avg:60.76ms
step:2256/2315 train_time:137069ms step_avg:60.76ms
step:2257/2315 train_time:137129ms step_avg:60.76ms
step:2258/2315 train_time:137190ms step_avg:60.76ms
step:2259/2315 train_time:137251ms step_avg:60.76ms
step:2260/2315 train_time:137312ms step_avg:60.76ms
step:2261/2315 train_time:137373ms step_avg:60.76ms
step:2262/2315 train_time:137433ms step_avg:60.76ms
step:2263/2315 train_time:137493ms step_avg:60.76ms
step:2264/2315 train_time:137554ms step_avg:60.76ms
step:2265/2315 train_time:137614ms step_avg:60.76ms
step:2266/2315 train_time:137675ms step_avg:60.76ms
step:2267/2315 train_time:137738ms step_avg:60.76ms
step:2268/2315 train_time:137801ms step_avg:60.76ms
step:2269/2315 train_time:137866ms step_avg:60.76ms
step:2270/2315 train_time:137928ms step_avg:60.76ms
step:2271/2315 train_time:137990ms step_avg:60.76ms
step:2272/2315 train_time:138051ms step_avg:60.76ms
step:2273/2315 train_time:138111ms step_avg:60.76ms
step:2274/2315 train_time:138173ms step_avg:60.76ms
step:2275/2315 train_time:138233ms step_avg:60.76ms
step:2276/2315 train_time:138294ms step_avg:60.76ms
step:2277/2315 train_time:138354ms step_avg:60.76ms
step:2278/2315 train_time:138415ms step_avg:60.76ms
step:2279/2315 train_time:138476ms step_avg:60.76ms
step:2280/2315 train_time:138537ms step_avg:60.76ms
step:2281/2315 train_time:138597ms step_avg:60.76ms
step:2282/2315 train_time:138658ms step_avg:60.76ms
step:2283/2315 train_time:138720ms step_avg:60.76ms
step:2284/2315 train_time:138783ms step_avg:60.76ms
step:2285/2315 train_time:138845ms step_avg:60.76ms
step:2286/2315 train_time:138908ms step_avg:60.76ms
step:2287/2315 train_time:138969ms step_avg:60.76ms
step:2288/2315 train_time:139031ms step_avg:60.77ms
step:2289/2315 train_time:139092ms step_avg:60.77ms
step:2290/2315 train_time:139153ms step_avg:60.77ms
step:2291/2315 train_time:139214ms step_avg:60.77ms
step:2292/2315 train_time:139275ms step_avg:60.77ms
step:2293/2315 train_time:139336ms step_avg:60.77ms
step:2294/2315 train_time:139397ms step_avg:60.77ms
step:2295/2315 train_time:139457ms step_avg:60.77ms
step:2296/2315 train_time:139518ms step_avg:60.77ms
step:2297/2315 train_time:139579ms step_avg:60.77ms
step:2298/2315 train_time:139640ms step_avg:60.77ms
step:2299/2315 train_time:139702ms step_avg:60.77ms
step:2300/2315 train_time:139764ms step_avg:60.77ms
step:2301/2315 train_time:139826ms step_avg:60.77ms
step:2302/2315 train_time:139889ms step_avg:60.77ms
step:2303/2315 train_time:139950ms step_avg:60.77ms
step:2304/2315 train_time:140012ms step_avg:60.77ms
step:2305/2315 train_time:140073ms step_avg:60.77ms
step:2306/2315 train_time:140135ms step_avg:60.77ms
step:2307/2315 train_time:140196ms step_avg:60.77ms
step:2308/2315 train_time:140257ms step_avg:60.77ms
step:2309/2315 train_time:140317ms step_avg:60.77ms
step:2310/2315 train_time:140378ms step_avg:60.77ms
step:2311/2315 train_time:140439ms step_avg:60.77ms
step:2312/2315 train_time:140500ms step_avg:60.77ms
step:2313/2315 train_time:140561ms step_avg:60.77ms
step:2314/2315 train_time:140623ms step_avg:60.77ms
step:2315/2315 train_time:140683ms step_avg:60.77ms
step:2315/2315 val_loss:3.2781 train_time:140747ms step_avg:60.80ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

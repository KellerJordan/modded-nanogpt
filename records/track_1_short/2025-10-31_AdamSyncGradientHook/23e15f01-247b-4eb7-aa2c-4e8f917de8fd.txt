import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2205  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()


====================================================================================================
Running Python 3.12.7 (main, Nov 15 2025, 15:35:49) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251115+cu126 compiled for CUDA 12.6
Running Triton version 3.5.1
Sat Nov 15 15:54:09 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   33C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   38C    P0            124W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   33C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   35C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   37C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   34C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           49859      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A           49860      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           49861      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           49862      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           49863      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           49864      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           49865      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           49866      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A           49860      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A           49861      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A           49862      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A           49863      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A           49864      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A           49865      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A           49866      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2245 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2245 train_time:117ms step_avg:116.75ms
step:2/2245 train_time:159ms step_avg:79.52ms
step:3/2245 train_time:179ms step_avg:59.54ms
step:4/2245 train_time:232ms step_avg:58.04ms
step:5/2245 train_time:292ms step_avg:58.32ms
step:6/2245 train_time:351ms step_avg:58.42ms
step:7/2245 train_time:412ms step_avg:58.81ms
step:8/2245 train_time:471ms step_avg:58.88ms
step:9/2245 train_time:532ms step_avg:59.11ms
step:10/2245 train_time:591ms step_avg:59.11ms
step:11/2245 train_time:652ms step_avg:59.28ms
step:12/2245 train_time:711ms step_avg:59.26ms
step:13/2245 train_time:772ms step_avg:59.38ms
step:14/2245 train_time:831ms step_avg:59.39ms
step:15/2245 train_time:893ms step_avg:59.53ms
step:16/2245 train_time:954ms step_avg:59.61ms
step:17/2245 train_time:1020ms step_avg:60.00ms
step:18/2245 train_time:1084ms step_avg:60.21ms
step:19/2245 train_time:1148ms step_avg:60.42ms
step:20/2245 train_time:1210ms step_avg:60.50ms
step:21/2245 train_time:1272ms step_avg:60.56ms
step:22/2245 train_time:1332ms step_avg:60.54ms
step:23/2245 train_time:1393ms step_avg:60.58ms
step:24/2245 train_time:1453ms step_avg:60.55ms
step:25/2245 train_time:1514ms step_avg:60.58ms
step:26/2245 train_time:1574ms step_avg:60.55ms
step:27/2245 train_time:1636ms step_avg:60.59ms
step:28/2245 train_time:1695ms step_avg:60.54ms
step:29/2245 train_time:1756ms step_avg:60.57ms
step:30/2245 train_time:1816ms step_avg:60.53ms
step:31/2245 train_time:1878ms step_avg:60.58ms
step:32/2245 train_time:1939ms step_avg:60.59ms
step:33/2245 train_time:2002ms step_avg:60.68ms
step:34/2245 train_time:2063ms step_avg:60.67ms
step:35/2245 train_time:2126ms step_avg:60.74ms
step:36/2245 train_time:2187ms step_avg:60.75ms
step:37/2245 train_time:2249ms step_avg:60.79ms
step:38/2245 train_time:2308ms step_avg:60.75ms
step:39/2245 train_time:2370ms step_avg:60.78ms
step:40/2245 train_time:2431ms step_avg:60.78ms
step:41/2245 train_time:2493ms step_avg:60.80ms
step:42/2245 train_time:2552ms step_avg:60.77ms
step:43/2245 train_time:2615ms step_avg:60.81ms
step:44/2245 train_time:2674ms step_avg:60.78ms
step:45/2245 train_time:2736ms step_avg:60.81ms
step:46/2245 train_time:2795ms step_avg:60.77ms
step:47/2245 train_time:2857ms step_avg:60.79ms
step:48/2245 train_time:2917ms step_avg:60.78ms
step:49/2245 train_time:2980ms step_avg:60.82ms
step:50/2245 train_time:3041ms step_avg:60.82ms
step:51/2245 train_time:3103ms step_avg:60.85ms
step:52/2245 train_time:3164ms step_avg:60.85ms
step:53/2245 train_time:3226ms step_avg:60.86ms
step:54/2245 train_time:3285ms step_avg:60.84ms
step:55/2245 train_time:3347ms step_avg:60.86ms
step:56/2245 train_time:3407ms step_avg:60.83ms
step:57/2245 train_time:3469ms step_avg:60.86ms
step:58/2245 train_time:3529ms step_avg:60.85ms
step:59/2245 train_time:3591ms step_avg:60.87ms
step:60/2245 train_time:3651ms step_avg:60.86ms
step:61/2245 train_time:3713ms step_avg:60.87ms
step:62/2245 train_time:3773ms step_avg:60.85ms
step:63/2245 train_time:3835ms step_avg:60.88ms
step:64/2245 train_time:3895ms step_avg:60.86ms
step:65/2245 train_time:3957ms step_avg:60.88ms
step:66/2245 train_time:4018ms step_avg:60.87ms
step:67/2245 train_time:4081ms step_avg:60.91ms
step:68/2245 train_time:4141ms step_avg:60.90ms
step:69/2245 train_time:4203ms step_avg:60.92ms
step:70/2245 train_time:4263ms step_avg:60.91ms
step:71/2245 train_time:4325ms step_avg:60.91ms
step:72/2245 train_time:4385ms step_avg:60.90ms
step:73/2245 train_time:4446ms step_avg:60.90ms
step:74/2245 train_time:4505ms step_avg:60.88ms
step:75/2245 train_time:4567ms step_avg:60.89ms
step:76/2245 train_time:4627ms step_avg:60.89ms
step:77/2245 train_time:4689ms step_avg:60.89ms
step:78/2245 train_time:4749ms step_avg:60.88ms
step:79/2245 train_time:4811ms step_avg:60.90ms
step:80/2245 train_time:4871ms step_avg:60.89ms
step:81/2245 train_time:4935ms step_avg:60.93ms
step:82/2245 train_time:4993ms step_avg:60.90ms
step:83/2245 train_time:5056ms step_avg:60.91ms
step:84/2245 train_time:5116ms step_avg:60.90ms
step:85/2245 train_time:5179ms step_avg:60.93ms
step:86/2245 train_time:5239ms step_avg:60.92ms
step:87/2245 train_time:5301ms step_avg:60.93ms
step:88/2245 train_time:5360ms step_avg:60.91ms
step:89/2245 train_time:5422ms step_avg:60.92ms
step:90/2245 train_time:5482ms step_avg:60.91ms
step:91/2245 train_time:5544ms step_avg:60.92ms
step:92/2245 train_time:5603ms step_avg:60.90ms
step:93/2245 train_time:5665ms step_avg:60.92ms
step:94/2245 train_time:5724ms step_avg:60.90ms
step:95/2245 train_time:5785ms step_avg:60.90ms
step:96/2245 train_time:5844ms step_avg:60.88ms
step:97/2245 train_time:5907ms step_avg:60.90ms
step:98/2245 train_time:5967ms step_avg:60.89ms
step:99/2245 train_time:6030ms step_avg:60.91ms
step:100/2245 train_time:6090ms step_avg:60.90ms
step:101/2245 train_time:6153ms step_avg:60.92ms
step:102/2245 train_time:6212ms step_avg:60.90ms
step:103/2245 train_time:6274ms step_avg:60.92ms
step:104/2245 train_time:6334ms step_avg:60.90ms
step:105/2245 train_time:6396ms step_avg:60.92ms
step:106/2245 train_time:6457ms step_avg:60.91ms
step:107/2245 train_time:6519ms step_avg:60.92ms
step:108/2245 train_time:6579ms step_avg:60.92ms
step:109/2245 train_time:6641ms step_avg:60.93ms
step:110/2245 train_time:6701ms step_avg:60.91ms
step:111/2245 train_time:6762ms step_avg:60.92ms
step:112/2245 train_time:6822ms step_avg:60.91ms
step:113/2245 train_time:6883ms step_avg:60.91ms
step:114/2245 train_time:6943ms step_avg:60.90ms
step:115/2245 train_time:7005ms step_avg:60.91ms
step:116/2245 train_time:7064ms step_avg:60.90ms
step:117/2245 train_time:7126ms step_avg:60.90ms
step:118/2245 train_time:7185ms step_avg:60.89ms
step:119/2245 train_time:7247ms step_avg:60.90ms
step:120/2245 train_time:7306ms step_avg:60.89ms
step:121/2245 train_time:7368ms step_avg:60.89ms
step:122/2245 train_time:7428ms step_avg:60.88ms
step:123/2245 train_time:7490ms step_avg:60.89ms
step:124/2245 train_time:7550ms step_avg:60.89ms
step:125/2245 train_time:7612ms step_avg:60.89ms
step:126/2245 train_time:7671ms step_avg:60.88ms
step:127/2245 train_time:7733ms step_avg:60.89ms
step:128/2245 train_time:7793ms step_avg:60.88ms
step:129/2245 train_time:7855ms step_avg:60.89ms
step:130/2245 train_time:7915ms step_avg:60.88ms
step:131/2245 train_time:7977ms step_avg:60.89ms
step:132/2245 train_time:8037ms step_avg:60.88ms
step:133/2245 train_time:8099ms step_avg:60.89ms
step:134/2245 train_time:8158ms step_avg:60.88ms
step:135/2245 train_time:8220ms step_avg:60.89ms
step:136/2245 train_time:8280ms step_avg:60.88ms
step:137/2245 train_time:8341ms step_avg:60.88ms
step:138/2245 train_time:8400ms step_avg:60.87ms
step:139/2245 train_time:8462ms step_avg:60.88ms
step:140/2245 train_time:8521ms step_avg:60.87ms
step:141/2245 train_time:8583ms step_avg:60.87ms
step:142/2245 train_time:8643ms step_avg:60.87ms
step:143/2245 train_time:8705ms step_avg:60.87ms
step:144/2245 train_time:8763ms step_avg:60.86ms
step:145/2245 train_time:8825ms step_avg:60.86ms
step:146/2245 train_time:8885ms step_avg:60.85ms
step:147/2245 train_time:8947ms step_avg:60.87ms
step:148/2245 train_time:9006ms step_avg:60.85ms
step:149/2245 train_time:9067ms step_avg:60.85ms
step:150/2245 train_time:9127ms step_avg:60.84ms
step:151/2245 train_time:9188ms step_avg:60.85ms
step:152/2245 train_time:9248ms step_avg:60.84ms
step:153/2245 train_time:9309ms step_avg:60.85ms
step:154/2245 train_time:9369ms step_avg:60.84ms
step:155/2245 train_time:9431ms step_avg:60.85ms
step:156/2245 train_time:9491ms step_avg:60.84ms
step:157/2245 train_time:9552ms step_avg:60.84ms
step:158/2245 train_time:9612ms step_avg:60.83ms
step:159/2245 train_time:9673ms step_avg:60.84ms
step:160/2245 train_time:9733ms step_avg:60.83ms
step:161/2245 train_time:9795ms step_avg:60.84ms
step:162/2245 train_time:9855ms step_avg:60.83ms
step:163/2245 train_time:9917ms step_avg:60.84ms
step:164/2245 train_time:9977ms step_avg:60.83ms
step:165/2245 train_time:10039ms step_avg:60.84ms
step:166/2245 train_time:10099ms step_avg:60.84ms
step:167/2245 train_time:10160ms step_avg:60.84ms
step:168/2245 train_time:10220ms step_avg:60.83ms
step:169/2245 train_time:10282ms step_avg:60.84ms
step:170/2245 train_time:10341ms step_avg:60.83ms
step:171/2245 train_time:10402ms step_avg:60.83ms
step:172/2245 train_time:10462ms step_avg:60.83ms
step:173/2245 train_time:10523ms step_avg:60.83ms
step:174/2245 train_time:10583ms step_avg:60.82ms
step:175/2245 train_time:10644ms step_avg:60.83ms
step:176/2245 train_time:10703ms step_avg:60.81ms
step:177/2245 train_time:10765ms step_avg:60.82ms
step:178/2245 train_time:10824ms step_avg:60.81ms
step:179/2245 train_time:10886ms step_avg:60.82ms
step:180/2245 train_time:10946ms step_avg:60.81ms
step:181/2245 train_time:11008ms step_avg:60.82ms
step:182/2245 train_time:11068ms step_avg:60.81ms
step:183/2245 train_time:11130ms step_avg:60.82ms
step:184/2245 train_time:11190ms step_avg:60.82ms
step:185/2245 train_time:11251ms step_avg:60.82ms
step:186/2245 train_time:11310ms step_avg:60.81ms
step:187/2245 train_time:11373ms step_avg:60.82ms
step:188/2245 train_time:11434ms step_avg:60.82ms
step:189/2245 train_time:11495ms step_avg:60.82ms
step:190/2245 train_time:11555ms step_avg:60.81ms
step:191/2245 train_time:11617ms step_avg:60.82ms
step:192/2245 train_time:11676ms step_avg:60.81ms
step:193/2245 train_time:11739ms step_avg:60.82ms
step:194/2245 train_time:11799ms step_avg:60.82ms
step:195/2245 train_time:11861ms step_avg:60.82ms
step:196/2245 train_time:11920ms step_avg:60.82ms
step:197/2245 train_time:11982ms step_avg:60.82ms
step:198/2245 train_time:12042ms step_avg:60.82ms
step:199/2245 train_time:12103ms step_avg:60.82ms
step:200/2245 train_time:12162ms step_avg:60.81ms
step:201/2245 train_time:12224ms step_avg:60.81ms
step:202/2245 train_time:12283ms step_avg:60.81ms
step:203/2245 train_time:12344ms step_avg:60.81ms
step:204/2245 train_time:12403ms step_avg:60.80ms
step:205/2245 train_time:12465ms step_avg:60.81ms
step:206/2245 train_time:12525ms step_avg:60.80ms
step:207/2245 train_time:12586ms step_avg:60.80ms
step:208/2245 train_time:12646ms step_avg:60.80ms
step:209/2245 train_time:12708ms step_avg:60.80ms
step:210/2245 train_time:12768ms step_avg:60.80ms
step:211/2245 train_time:12830ms step_avg:60.81ms
step:212/2245 train_time:12890ms step_avg:60.80ms
step:213/2245 train_time:12952ms step_avg:60.81ms
step:214/2245 train_time:13011ms step_avg:60.80ms
step:215/2245 train_time:13072ms step_avg:60.80ms
step:216/2245 train_time:13132ms step_avg:60.79ms
step:217/2245 train_time:13193ms step_avg:60.80ms
step:218/2245 train_time:13252ms step_avg:60.79ms
step:219/2245 train_time:13314ms step_avg:60.79ms
step:220/2245 train_time:13373ms step_avg:60.79ms
step:221/2245 train_time:13435ms step_avg:60.79ms
step:222/2245 train_time:13495ms step_avg:60.79ms
step:223/2245 train_time:13557ms step_avg:60.79ms
step:224/2245 train_time:13616ms step_avg:60.79ms
step:225/2245 train_time:13678ms step_avg:60.79ms
step:226/2245 train_time:13739ms step_avg:60.79ms
step:227/2245 train_time:13801ms step_avg:60.80ms
step:228/2245 train_time:13860ms step_avg:60.79ms
step:229/2245 train_time:13921ms step_avg:60.79ms
step:230/2245 train_time:13981ms step_avg:60.79ms
step:231/2245 train_time:14043ms step_avg:60.79ms
step:232/2245 train_time:14103ms step_avg:60.79ms
step:233/2245 train_time:14164ms step_avg:60.79ms
step:234/2245 train_time:14223ms step_avg:60.78ms
step:235/2245 train_time:14284ms step_avg:60.78ms
step:236/2245 train_time:14343ms step_avg:60.78ms
step:237/2245 train_time:14406ms step_avg:60.78ms
step:238/2245 train_time:14464ms step_avg:60.77ms
step:239/2245 train_time:14525ms step_avg:60.78ms
step:240/2245 train_time:14585ms step_avg:60.77ms
step:241/2245 train_time:14646ms step_avg:60.77ms
step:242/2245 train_time:14706ms step_avg:60.77ms
step:243/2245 train_time:14768ms step_avg:60.78ms
step:244/2245 train_time:14828ms step_avg:60.77ms
step:245/2245 train_time:14890ms step_avg:60.77ms
step:246/2245 train_time:14949ms step_avg:60.77ms
step:247/2245 train_time:15011ms step_avg:60.77ms
step:248/2245 train_time:15071ms step_avg:60.77ms
step:249/2245 train_time:15133ms step_avg:60.77ms
step:250/2245 train_time:15193ms step_avg:60.77ms
step:250/2245 val_loss:4.0937 train_time:15256ms step_avg:61.02ms
step:251/2245 train_time:15276ms step_avg:60.86ms
step:252/2245 train_time:15315ms step_avg:60.77ms
step:253/2245 train_time:15382ms step_avg:60.80ms
step:254/2245 train_time:15445ms step_avg:60.81ms
step:255/2245 train_time:15507ms step_avg:60.81ms
step:256/2245 train_time:15567ms step_avg:60.81ms
step:257/2245 train_time:15628ms step_avg:60.81ms
step:258/2245 train_time:15686ms step_avg:60.80ms
step:259/2245 train_time:15747ms step_avg:60.80ms
step:260/2245 train_time:15806ms step_avg:60.79ms
step:261/2245 train_time:15866ms step_avg:60.79ms
step:262/2245 train_time:15926ms step_avg:60.78ms
step:263/2245 train_time:15986ms step_avg:60.78ms
step:264/2245 train_time:16045ms step_avg:60.77ms
step:265/2245 train_time:16106ms step_avg:60.78ms
step:266/2245 train_time:16165ms step_avg:60.77ms
step:267/2245 train_time:16227ms step_avg:60.78ms
step:268/2245 train_time:16287ms step_avg:60.77ms
step:269/2245 train_time:16351ms step_avg:60.78ms
step:270/2245 train_time:16413ms step_avg:60.79ms
step:271/2245 train_time:16475ms step_avg:60.79ms
step:272/2245 train_time:16535ms step_avg:60.79ms
step:273/2245 train_time:16596ms step_avg:60.79ms
step:274/2245 train_time:16656ms step_avg:60.79ms
step:275/2245 train_time:16717ms step_avg:60.79ms
step:276/2245 train_time:16776ms step_avg:60.78ms
step:277/2245 train_time:16837ms step_avg:60.78ms
step:278/2245 train_time:16896ms step_avg:60.78ms
step:279/2245 train_time:16957ms step_avg:60.78ms
step:280/2245 train_time:17016ms step_avg:60.77ms
step:281/2245 train_time:17078ms step_avg:60.77ms
step:282/2245 train_time:17137ms step_avg:60.77ms
step:283/2245 train_time:17198ms step_avg:60.77ms
step:284/2245 train_time:17257ms step_avg:60.76ms
step:285/2245 train_time:17319ms step_avg:60.77ms
step:286/2245 train_time:17378ms step_avg:60.76ms
step:287/2245 train_time:17440ms step_avg:60.77ms
step:288/2245 train_time:17500ms step_avg:60.76ms
step:289/2245 train_time:17562ms step_avg:60.77ms
step:290/2245 train_time:17622ms step_avg:60.76ms
step:291/2245 train_time:17683ms step_avg:60.77ms
step:292/2245 train_time:17743ms step_avg:60.76ms
step:293/2245 train_time:17804ms step_avg:60.76ms
step:294/2245 train_time:17864ms step_avg:60.76ms
step:295/2245 train_time:17925ms step_avg:60.76ms
step:296/2245 train_time:17984ms step_avg:60.76ms
step:297/2245 train_time:18046ms step_avg:60.76ms
step:298/2245 train_time:18105ms step_avg:60.76ms
step:299/2245 train_time:18167ms step_avg:60.76ms
step:300/2245 train_time:18226ms step_avg:60.75ms
step:301/2245 train_time:18288ms step_avg:60.76ms
step:302/2245 train_time:18347ms step_avg:60.75ms
step:303/2245 train_time:18409ms step_avg:60.76ms
step:304/2245 train_time:18469ms step_avg:60.75ms
step:305/2245 train_time:18531ms step_avg:60.76ms
step:306/2245 train_time:18591ms step_avg:60.75ms
step:307/2245 train_time:18652ms step_avg:60.76ms
step:308/2245 train_time:18713ms step_avg:60.76ms
step:309/2245 train_time:18775ms step_avg:60.76ms
step:310/2245 train_time:18834ms step_avg:60.76ms
step:311/2245 train_time:18896ms step_avg:60.76ms
step:312/2245 train_time:18956ms step_avg:60.76ms
step:313/2245 train_time:19017ms step_avg:60.76ms
step:314/2245 train_time:19076ms step_avg:60.75ms
step:315/2245 train_time:19139ms step_avg:60.76ms
step:316/2245 train_time:19197ms step_avg:60.75ms
step:317/2245 train_time:19259ms step_avg:60.76ms
step:318/2245 train_time:19319ms step_avg:60.75ms
step:319/2245 train_time:19380ms step_avg:60.75ms
step:320/2245 train_time:19439ms step_avg:60.75ms
step:321/2245 train_time:19500ms step_avg:60.75ms
step:322/2245 train_time:19559ms step_avg:60.74ms
step:323/2245 train_time:19621ms step_avg:60.75ms
step:324/2245 train_time:19681ms step_avg:60.75ms
step:325/2245 train_time:19743ms step_avg:60.75ms
step:326/2245 train_time:19803ms step_avg:60.75ms
step:327/2245 train_time:19864ms step_avg:60.75ms
step:328/2245 train_time:19924ms step_avg:60.74ms
step:329/2245 train_time:19985ms step_avg:60.74ms
step:330/2245 train_time:20044ms step_avg:60.74ms
step:331/2245 train_time:20106ms step_avg:60.74ms
step:332/2245 train_time:20166ms step_avg:60.74ms
step:333/2245 train_time:20227ms step_avg:60.74ms
step:334/2245 train_time:20287ms step_avg:60.74ms
step:335/2245 train_time:20348ms step_avg:60.74ms
step:336/2245 train_time:20408ms step_avg:60.74ms
step:337/2245 train_time:20470ms step_avg:60.74ms
step:338/2245 train_time:20530ms step_avg:60.74ms
step:339/2245 train_time:20591ms step_avg:60.74ms
step:340/2245 train_time:20651ms step_avg:60.74ms
step:341/2245 train_time:20713ms step_avg:60.74ms
step:342/2245 train_time:20772ms step_avg:60.74ms
step:343/2245 train_time:20834ms step_avg:60.74ms
step:344/2245 train_time:20894ms step_avg:60.74ms
step:345/2245 train_time:20956ms step_avg:60.74ms
step:346/2245 train_time:21015ms step_avg:60.74ms
step:347/2245 train_time:21077ms step_avg:60.74ms
step:348/2245 train_time:21137ms step_avg:60.74ms
step:349/2245 train_time:21198ms step_avg:60.74ms
step:350/2245 train_time:21257ms step_avg:60.74ms
step:351/2245 train_time:21319ms step_avg:60.74ms
step:352/2245 train_time:21379ms step_avg:60.74ms
step:353/2245 train_time:21441ms step_avg:60.74ms
step:354/2245 train_time:21499ms step_avg:60.73ms
step:355/2245 train_time:21561ms step_avg:60.73ms
step:356/2245 train_time:21620ms step_avg:60.73ms
step:357/2245 train_time:21681ms step_avg:60.73ms
step:358/2245 train_time:21741ms step_avg:60.73ms
step:359/2245 train_time:21803ms step_avg:60.73ms
step:360/2245 train_time:21862ms step_avg:60.73ms
step:361/2245 train_time:21924ms step_avg:60.73ms
step:362/2245 train_time:21984ms step_avg:60.73ms
step:363/2245 train_time:22046ms step_avg:60.73ms
step:364/2245 train_time:22105ms step_avg:60.73ms
step:365/2245 train_time:22167ms step_avg:60.73ms
step:366/2245 train_time:22226ms step_avg:60.73ms
step:367/2245 train_time:22288ms step_avg:60.73ms
step:368/2245 train_time:22347ms step_avg:60.73ms
step:369/2245 train_time:22408ms step_avg:60.73ms
step:370/2245 train_time:22468ms step_avg:60.72ms
step:371/2245 train_time:22531ms step_avg:60.73ms
step:372/2245 train_time:22591ms step_avg:60.73ms
step:373/2245 train_time:22653ms step_avg:60.73ms
step:374/2245 train_time:22712ms step_avg:60.73ms
step:375/2245 train_time:22774ms step_avg:60.73ms
step:376/2245 train_time:22834ms step_avg:60.73ms
step:377/2245 train_time:22896ms step_avg:60.73ms
step:378/2245 train_time:22956ms step_avg:60.73ms
step:379/2245 train_time:23017ms step_avg:60.73ms
step:380/2245 train_time:23077ms step_avg:60.73ms
step:381/2245 train_time:23138ms step_avg:60.73ms
step:382/2245 train_time:23197ms step_avg:60.73ms
step:383/2245 train_time:23258ms step_avg:60.73ms
step:384/2245 train_time:23318ms step_avg:60.72ms
step:385/2245 train_time:23380ms step_avg:60.73ms
step:386/2245 train_time:23439ms step_avg:60.72ms
step:387/2245 train_time:23500ms step_avg:60.72ms
step:388/2245 train_time:23559ms step_avg:60.72ms
step:389/2245 train_time:23621ms step_avg:60.72ms
step:390/2245 train_time:23680ms step_avg:60.72ms
step:391/2245 train_time:23742ms step_avg:60.72ms
step:392/2245 train_time:23801ms step_avg:60.72ms
step:393/2245 train_time:23863ms step_avg:60.72ms
step:394/2245 train_time:23922ms step_avg:60.72ms
step:395/2245 train_time:23984ms step_avg:60.72ms
step:396/2245 train_time:24044ms step_avg:60.72ms
step:397/2245 train_time:24105ms step_avg:60.72ms
step:398/2245 train_time:24165ms step_avg:60.72ms
step:399/2245 train_time:24227ms step_avg:60.72ms
step:400/2245 train_time:24287ms step_avg:60.72ms
step:401/2245 train_time:24349ms step_avg:60.72ms
step:402/2245 train_time:24408ms step_avg:60.72ms
step:403/2245 train_time:24470ms step_avg:60.72ms
step:404/2245 train_time:24530ms step_avg:60.72ms
step:405/2245 train_time:24591ms step_avg:60.72ms
step:406/2245 train_time:24651ms step_avg:60.72ms
step:407/2245 train_time:24713ms step_avg:60.72ms
step:408/2245 train_time:24772ms step_avg:60.72ms
step:409/2245 train_time:24835ms step_avg:60.72ms
step:410/2245 train_time:24894ms step_avg:60.72ms
step:411/2245 train_time:24957ms step_avg:60.72ms
step:412/2245 train_time:25017ms step_avg:60.72ms
step:413/2245 train_time:25078ms step_avg:60.72ms
step:414/2245 train_time:25137ms step_avg:60.72ms
step:415/2245 train_time:25199ms step_avg:60.72ms
step:416/2245 train_time:25258ms step_avg:60.72ms
step:417/2245 train_time:25319ms step_avg:60.72ms
step:418/2245 train_time:25378ms step_avg:60.71ms
step:419/2245 train_time:25439ms step_avg:60.71ms
step:420/2245 train_time:25498ms step_avg:60.71ms
step:421/2245 train_time:25560ms step_avg:60.71ms
step:422/2245 train_time:25619ms step_avg:60.71ms
step:423/2245 train_time:25681ms step_avg:60.71ms
step:424/2245 train_time:25739ms step_avg:60.71ms
step:425/2245 train_time:25801ms step_avg:60.71ms
step:426/2245 train_time:25860ms step_avg:60.70ms
step:427/2245 train_time:25922ms step_avg:60.71ms
step:428/2245 train_time:25982ms step_avg:60.70ms
step:429/2245 train_time:26043ms step_avg:60.71ms
step:430/2245 train_time:26102ms step_avg:60.70ms
step:431/2245 train_time:26163ms step_avg:60.70ms
step:432/2245 train_time:26222ms step_avg:60.70ms
step:433/2245 train_time:26283ms step_avg:60.70ms
step:434/2245 train_time:26342ms step_avg:60.70ms
step:435/2245 train_time:26403ms step_avg:60.70ms
step:436/2245 train_time:26462ms step_avg:60.69ms
step:437/2245 train_time:26524ms step_avg:60.70ms
step:438/2245 train_time:26583ms step_avg:60.69ms
step:439/2245 train_time:26645ms step_avg:60.70ms
step:440/2245 train_time:26705ms step_avg:60.69ms
step:441/2245 train_time:26766ms step_avg:60.69ms
step:442/2245 train_time:26826ms step_avg:60.69ms
step:443/2245 train_time:26888ms step_avg:60.69ms
step:444/2245 train_time:26947ms step_avg:60.69ms
step:445/2245 train_time:27009ms step_avg:60.69ms
step:446/2245 train_time:27068ms step_avg:60.69ms
step:447/2245 train_time:27130ms step_avg:60.69ms
step:448/2245 train_time:27190ms step_avg:60.69ms
step:449/2245 train_time:27251ms step_avg:60.69ms
step:450/2245 train_time:27311ms step_avg:60.69ms
step:451/2245 train_time:27372ms step_avg:60.69ms
step:452/2245 train_time:27432ms step_avg:60.69ms
step:453/2245 train_time:27494ms step_avg:60.69ms
step:454/2245 train_time:27554ms step_avg:60.69ms
step:455/2245 train_time:27616ms step_avg:60.70ms
step:456/2245 train_time:27675ms step_avg:60.69ms
step:457/2245 train_time:27737ms step_avg:60.69ms
step:458/2245 train_time:27796ms step_avg:60.69ms
step:459/2245 train_time:27858ms step_avg:60.69ms
step:460/2245 train_time:27917ms step_avg:60.69ms
step:461/2245 train_time:27978ms step_avg:60.69ms
step:462/2245 train_time:28038ms step_avg:60.69ms
step:463/2245 train_time:28099ms step_avg:60.69ms
step:464/2245 train_time:28157ms step_avg:60.68ms
step:465/2245 train_time:28219ms step_avg:60.69ms
step:466/2245 train_time:28278ms step_avg:60.68ms
step:467/2245 train_time:28339ms step_avg:60.68ms
step:468/2245 train_time:28398ms step_avg:60.68ms
step:469/2245 train_time:28458ms step_avg:60.68ms
step:470/2245 train_time:28517ms step_avg:60.68ms
step:471/2245 train_time:28579ms step_avg:60.68ms
step:472/2245 train_time:28638ms step_avg:60.67ms
step:473/2245 train_time:28700ms step_avg:60.68ms
step:474/2245 train_time:28759ms step_avg:60.67ms
step:475/2245 train_time:28821ms step_avg:60.67ms
step:476/2245 train_time:28880ms step_avg:60.67ms
step:477/2245 train_time:28941ms step_avg:60.67ms
step:478/2245 train_time:28999ms step_avg:60.67ms
step:479/2245 train_time:29061ms step_avg:60.67ms
step:480/2245 train_time:29120ms step_avg:60.67ms
step:481/2245 train_time:29181ms step_avg:60.67ms
step:482/2245 train_time:29240ms step_avg:60.66ms
step:483/2245 train_time:29301ms step_avg:60.67ms
step:484/2245 train_time:29360ms step_avg:60.66ms
step:485/2245 train_time:29421ms step_avg:60.66ms
step:486/2245 train_time:29481ms step_avg:60.66ms
step:487/2245 train_time:29542ms step_avg:60.66ms
step:488/2245 train_time:29602ms step_avg:60.66ms
step:489/2245 train_time:29663ms step_avg:60.66ms
step:490/2245 train_time:29723ms step_avg:60.66ms
step:491/2245 train_time:29784ms step_avg:60.66ms
step:492/2245 train_time:29844ms step_avg:60.66ms
step:493/2245 train_time:29906ms step_avg:60.66ms
step:494/2245 train_time:29965ms step_avg:60.66ms
step:495/2245 train_time:30027ms step_avg:60.66ms
step:496/2245 train_time:30086ms step_avg:60.66ms
step:497/2245 train_time:30147ms step_avg:60.66ms
step:498/2245 train_time:30207ms step_avg:60.66ms
step:499/2245 train_time:30269ms step_avg:60.66ms
step:500/2245 train_time:30328ms step_avg:60.66ms
step:500/2245 val_loss:3.8233 train_time:30391ms step_avg:60.78ms
step:501/2245 train_time:30411ms step_avg:60.70ms
step:502/2245 train_time:30453ms step_avg:60.66ms
step:503/2245 train_time:30518ms step_avg:60.67ms
step:504/2245 train_time:30579ms step_avg:60.67ms
step:505/2245 train_time:30641ms step_avg:60.67ms
step:506/2245 train_time:30700ms step_avg:60.67ms
step:507/2245 train_time:30762ms step_avg:60.67ms
step:508/2245 train_time:30820ms step_avg:60.67ms
step:509/2245 train_time:30881ms step_avg:60.67ms
step:510/2245 train_time:30939ms step_avg:60.67ms
step:511/2245 train_time:31000ms step_avg:60.67ms
step:512/2245 train_time:31059ms step_avg:60.66ms
step:513/2245 train_time:31119ms step_avg:60.66ms
step:514/2245 train_time:31178ms step_avg:60.66ms
step:515/2245 train_time:31239ms step_avg:60.66ms
step:516/2245 train_time:31298ms step_avg:60.66ms
step:517/2245 train_time:31361ms step_avg:60.66ms
step:518/2245 train_time:31421ms step_avg:60.66ms
step:519/2245 train_time:31483ms step_avg:60.66ms
step:520/2245 train_time:31543ms step_avg:60.66ms
step:521/2245 train_time:31606ms step_avg:60.66ms
step:522/2245 train_time:31666ms step_avg:60.66ms
step:523/2245 train_time:31727ms step_avg:60.66ms
step:524/2245 train_time:31786ms step_avg:60.66ms
step:525/2245 train_time:31847ms step_avg:60.66ms
step:526/2245 train_time:31907ms step_avg:60.66ms
step:527/2245 train_time:31968ms step_avg:60.66ms
step:528/2245 train_time:32027ms step_avg:60.66ms
step:529/2245 train_time:32088ms step_avg:60.66ms
step:530/2245 train_time:32147ms step_avg:60.66ms
step:531/2245 train_time:32208ms step_avg:60.66ms
step:532/2245 train_time:32268ms step_avg:60.65ms
step:533/2245 train_time:32330ms step_avg:60.66ms
step:534/2245 train_time:32390ms step_avg:60.65ms
step:535/2245 train_time:32452ms step_avg:60.66ms
step:536/2245 train_time:32511ms step_avg:60.66ms
step:537/2245 train_time:32574ms step_avg:60.66ms
step:538/2245 train_time:32634ms step_avg:60.66ms
step:539/2245 train_time:32695ms step_avg:60.66ms
step:540/2245 train_time:32754ms step_avg:60.66ms
step:541/2245 train_time:32816ms step_avg:60.66ms
step:542/2245 train_time:32876ms step_avg:60.66ms
step:543/2245 train_time:32936ms step_avg:60.66ms
step:544/2245 train_time:32995ms step_avg:60.65ms
step:545/2245 train_time:33056ms step_avg:60.65ms
step:546/2245 train_time:33115ms step_avg:60.65ms
step:547/2245 train_time:33176ms step_avg:60.65ms
step:548/2245 train_time:33235ms step_avg:60.65ms
step:549/2245 train_time:33296ms step_avg:60.65ms
step:550/2245 train_time:33355ms step_avg:60.65ms
step:551/2245 train_time:33417ms step_avg:60.65ms
step:552/2245 train_time:33476ms step_avg:60.65ms
step:553/2245 train_time:33539ms step_avg:60.65ms
step:554/2245 train_time:33598ms step_avg:60.65ms
step:555/2245 train_time:33660ms step_avg:60.65ms
step:556/2245 train_time:33719ms step_avg:60.65ms
step:557/2245 train_time:33780ms step_avg:60.65ms
step:558/2245 train_time:33839ms step_avg:60.64ms
step:559/2245 train_time:33900ms step_avg:60.64ms
step:560/2245 train_time:33959ms step_avg:60.64ms
step:561/2245 train_time:34021ms step_avg:60.64ms
step:562/2245 train_time:34079ms step_avg:60.64ms
step:563/2245 train_time:34140ms step_avg:60.64ms
step:564/2245 train_time:34199ms step_avg:60.64ms
step:565/2245 train_time:34261ms step_avg:60.64ms
step:566/2245 train_time:34320ms step_avg:60.64ms
step:567/2245 train_time:34381ms step_avg:60.64ms
step:568/2245 train_time:34440ms step_avg:60.63ms
step:569/2245 train_time:34502ms step_avg:60.64ms
step:570/2245 train_time:34562ms step_avg:60.64ms
step:571/2245 train_time:34624ms step_avg:60.64ms
step:572/2245 train_time:34684ms step_avg:60.64ms
step:573/2245 train_time:34745ms step_avg:60.64ms
step:574/2245 train_time:34804ms step_avg:60.63ms
step:575/2245 train_time:34866ms step_avg:60.64ms
step:576/2245 train_time:34925ms step_avg:60.63ms
step:577/2245 train_time:34987ms step_avg:60.64ms
step:578/2245 train_time:35047ms step_avg:60.63ms
step:579/2245 train_time:35109ms step_avg:60.64ms
step:580/2245 train_time:35169ms step_avg:60.64ms
step:581/2245 train_time:35231ms step_avg:60.64ms
step:582/2245 train_time:35291ms step_avg:60.64ms
step:583/2245 train_time:35353ms step_avg:60.64ms
step:584/2245 train_time:35412ms step_avg:60.64ms
step:585/2245 train_time:35474ms step_avg:60.64ms
step:586/2245 train_time:35533ms step_avg:60.64ms
step:587/2245 train_time:35595ms step_avg:60.64ms
step:588/2245 train_time:35655ms step_avg:60.64ms
step:589/2245 train_time:35716ms step_avg:60.64ms
step:590/2245 train_time:35776ms step_avg:60.64ms
step:591/2245 train_time:35837ms step_avg:60.64ms
step:592/2245 train_time:35896ms step_avg:60.64ms
step:593/2245 train_time:35958ms step_avg:60.64ms
step:594/2245 train_time:36017ms step_avg:60.64ms
step:595/2245 train_time:36079ms step_avg:60.64ms
step:596/2245 train_time:36138ms step_avg:60.63ms
step:597/2245 train_time:36199ms step_avg:60.64ms
step:598/2245 train_time:36259ms step_avg:60.63ms
step:599/2245 train_time:36321ms step_avg:60.64ms
step:600/2245 train_time:36380ms step_avg:60.63ms
step:601/2245 train_time:36441ms step_avg:60.63ms
step:602/2245 train_time:36501ms step_avg:60.63ms
step:603/2245 train_time:36563ms step_avg:60.64ms
step:604/2245 train_time:36623ms step_avg:60.63ms
step:605/2245 train_time:36685ms step_avg:60.64ms
step:606/2245 train_time:36744ms step_avg:60.63ms
step:607/2245 train_time:36805ms step_avg:60.63ms
step:608/2245 train_time:36865ms step_avg:60.63ms
step:609/2245 train_time:36928ms step_avg:60.64ms
step:610/2245 train_time:36987ms step_avg:60.64ms
step:611/2245 train_time:37049ms step_avg:60.64ms
step:612/2245 train_time:37109ms step_avg:60.64ms
step:613/2245 train_time:37170ms step_avg:60.64ms
step:614/2245 train_time:37230ms step_avg:60.63ms
step:615/2245 train_time:37292ms step_avg:60.64ms
step:616/2245 train_time:37352ms step_avg:60.64ms
step:617/2245 train_time:37414ms step_avg:60.64ms
step:618/2245 train_time:37474ms step_avg:60.64ms
step:619/2245 train_time:37536ms step_avg:60.64ms
step:620/2245 train_time:37595ms step_avg:60.64ms
step:621/2245 train_time:37657ms step_avg:60.64ms
step:622/2245 train_time:37716ms step_avg:60.64ms
step:623/2245 train_time:37777ms step_avg:60.64ms
step:624/2245 train_time:37836ms step_avg:60.64ms
step:625/2245 train_time:37897ms step_avg:60.64ms
step:626/2245 train_time:37957ms step_avg:60.63ms
step:627/2245 train_time:38018ms step_avg:60.64ms
step:628/2245 train_time:38077ms step_avg:60.63ms
step:629/2245 train_time:38139ms step_avg:60.63ms
step:630/2245 train_time:38198ms step_avg:60.63ms
step:631/2245 train_time:38259ms step_avg:60.63ms
step:632/2245 train_time:38319ms step_avg:60.63ms
step:633/2245 train_time:38381ms step_avg:60.63ms
step:634/2245 train_time:38441ms step_avg:60.63ms
step:635/2245 train_time:38502ms step_avg:60.63ms
step:636/2245 train_time:38561ms step_avg:60.63ms
step:637/2245 train_time:38622ms step_avg:60.63ms
step:638/2245 train_time:38682ms step_avg:60.63ms
step:639/2245 train_time:38742ms step_avg:60.63ms
step:640/2245 train_time:38802ms step_avg:60.63ms
step:641/2245 train_time:38863ms step_avg:60.63ms
step:642/2245 train_time:38923ms step_avg:60.63ms
step:643/2245 train_time:38985ms step_avg:60.63ms
step:644/2245 train_time:39045ms step_avg:60.63ms
step:645/2245 train_time:39106ms step_avg:60.63ms
step:646/2245 train_time:39166ms step_avg:60.63ms
step:647/2245 train_time:39228ms step_avg:60.63ms
step:648/2245 train_time:39288ms step_avg:60.63ms
step:649/2245 train_time:39350ms step_avg:60.63ms
step:650/2245 train_time:39409ms step_avg:60.63ms
step:651/2245 train_time:39471ms step_avg:60.63ms
step:652/2245 train_time:39530ms step_avg:60.63ms
step:653/2245 train_time:39592ms step_avg:60.63ms
step:654/2245 train_time:39652ms step_avg:60.63ms
step:655/2245 train_time:39714ms step_avg:60.63ms
step:656/2245 train_time:39773ms step_avg:60.63ms
step:657/2245 train_time:39835ms step_avg:60.63ms
step:658/2245 train_time:39894ms step_avg:60.63ms
step:659/2245 train_time:39956ms step_avg:60.63ms
step:660/2245 train_time:40015ms step_avg:60.63ms
step:661/2245 train_time:40077ms step_avg:60.63ms
step:662/2245 train_time:40136ms step_avg:60.63ms
step:663/2245 train_time:40197ms step_avg:60.63ms
step:664/2245 train_time:40257ms step_avg:60.63ms
step:665/2245 train_time:40318ms step_avg:60.63ms
step:666/2245 train_time:40378ms step_avg:60.63ms
step:667/2245 train_time:40439ms step_avg:60.63ms
step:668/2245 train_time:40499ms step_avg:60.63ms
step:669/2245 train_time:40560ms step_avg:60.63ms
step:670/2245 train_time:40620ms step_avg:60.63ms
step:671/2245 train_time:40682ms step_avg:60.63ms
step:672/2245 train_time:40741ms step_avg:60.63ms
step:673/2245 train_time:40803ms step_avg:60.63ms
step:674/2245 train_time:40861ms step_avg:60.63ms
step:675/2245 train_time:40923ms step_avg:60.63ms
step:676/2245 train_time:40982ms step_avg:60.62ms
step:677/2245 train_time:41044ms step_avg:60.63ms
step:678/2245 train_time:41104ms step_avg:60.63ms
step:679/2245 train_time:41166ms step_avg:60.63ms
step:680/2245 train_time:41226ms step_avg:60.63ms
step:681/2245 train_time:41288ms step_avg:60.63ms
step:682/2245 train_time:41347ms step_avg:60.63ms
step:683/2245 train_time:41409ms step_avg:60.63ms
step:684/2245 train_time:41469ms step_avg:60.63ms
step:685/2245 train_time:41530ms step_avg:60.63ms
step:686/2245 train_time:41590ms step_avg:60.63ms
step:687/2245 train_time:41652ms step_avg:60.63ms
step:688/2245 train_time:41712ms step_avg:60.63ms
step:689/2245 train_time:41773ms step_avg:60.63ms
step:690/2245 train_time:41833ms step_avg:60.63ms
step:691/2245 train_time:41894ms step_avg:60.63ms
step:692/2245 train_time:41953ms step_avg:60.63ms
step:693/2245 train_time:42015ms step_avg:60.63ms
step:694/2245 train_time:42075ms step_avg:60.63ms
step:695/2245 train_time:42137ms step_avg:60.63ms
step:696/2245 train_time:42196ms step_avg:60.63ms
step:697/2245 train_time:42257ms step_avg:60.63ms
step:698/2245 train_time:42317ms step_avg:60.63ms
step:699/2245 train_time:42378ms step_avg:60.63ms
step:700/2245 train_time:42437ms step_avg:60.62ms
step:701/2245 train_time:42499ms step_avg:60.63ms
step:702/2245 train_time:42559ms step_avg:60.62ms
step:703/2245 train_time:42621ms step_avg:60.63ms
step:704/2245 train_time:42680ms step_avg:60.62ms
step:705/2245 train_time:42742ms step_avg:60.63ms
step:706/2245 train_time:42802ms step_avg:60.63ms
step:707/2245 train_time:42864ms step_avg:60.63ms
step:708/2245 train_time:42923ms step_avg:60.63ms
step:709/2245 train_time:42985ms step_avg:60.63ms
step:710/2245 train_time:43044ms step_avg:60.63ms
step:711/2245 train_time:43106ms step_avg:60.63ms
step:712/2245 train_time:43165ms step_avg:60.63ms
step:713/2245 train_time:43227ms step_avg:60.63ms
step:714/2245 train_time:43286ms step_avg:60.62ms
step:715/2245 train_time:43348ms step_avg:60.63ms
step:716/2245 train_time:43408ms step_avg:60.63ms
step:717/2245 train_time:43470ms step_avg:60.63ms
step:718/2245 train_time:43530ms step_avg:60.63ms
step:719/2245 train_time:43592ms step_avg:60.63ms
step:720/2245 train_time:43652ms step_avg:60.63ms
step:721/2245 train_time:43715ms step_avg:60.63ms
step:722/2245 train_time:43774ms step_avg:60.63ms
step:723/2245 train_time:43836ms step_avg:60.63ms
step:724/2245 train_time:43895ms step_avg:60.63ms
step:725/2245 train_time:43956ms step_avg:60.63ms
step:726/2245 train_time:44015ms step_avg:60.63ms
step:727/2245 train_time:44076ms step_avg:60.63ms
step:728/2245 train_time:44135ms step_avg:60.63ms
step:729/2245 train_time:44197ms step_avg:60.63ms
step:730/2245 train_time:44256ms step_avg:60.62ms
step:731/2245 train_time:44317ms step_avg:60.63ms
step:732/2245 train_time:44376ms step_avg:60.62ms
step:733/2245 train_time:44437ms step_avg:60.62ms
step:734/2245 train_time:44496ms step_avg:60.62ms
step:735/2245 train_time:44558ms step_avg:60.62ms
step:736/2245 train_time:44617ms step_avg:60.62ms
step:737/2245 train_time:44680ms step_avg:60.62ms
step:738/2245 train_time:44740ms step_avg:60.62ms
step:739/2245 train_time:44803ms step_avg:60.63ms
step:740/2245 train_time:44863ms step_avg:60.63ms
step:741/2245 train_time:44925ms step_avg:60.63ms
step:742/2245 train_time:44985ms step_avg:60.63ms
step:743/2245 train_time:45047ms step_avg:60.63ms
step:744/2245 train_time:45108ms step_avg:60.63ms
step:745/2245 train_time:45169ms step_avg:60.63ms
step:746/2245 train_time:45229ms step_avg:60.63ms
step:747/2245 train_time:45292ms step_avg:60.63ms
step:748/2245 train_time:45352ms step_avg:60.63ms
step:749/2245 train_time:45415ms step_avg:60.63ms
step:750/2245 train_time:45474ms step_avg:60.63ms
step:750/2245 val_loss:3.6709 train_time:45537ms step_avg:60.72ms
step:751/2245 train_time:45558ms step_avg:60.66ms
step:752/2245 train_time:45598ms step_avg:60.64ms
step:753/2245 train_time:45665ms step_avg:60.64ms
step:754/2245 train_time:45727ms step_avg:60.65ms
step:755/2245 train_time:45789ms step_avg:60.65ms
step:756/2245 train_time:45849ms step_avg:60.65ms
step:757/2245 train_time:45910ms step_avg:60.65ms
step:758/2245 train_time:45970ms step_avg:60.65ms
step:759/2245 train_time:46031ms step_avg:60.65ms
step:760/2245 train_time:46090ms step_avg:60.65ms
step:761/2245 train_time:46152ms step_avg:60.65ms
step:762/2245 train_time:46211ms step_avg:60.64ms
step:763/2245 train_time:46273ms step_avg:60.65ms
step:764/2245 train_time:46332ms step_avg:60.64ms
step:765/2245 train_time:46393ms step_avg:60.65ms
step:766/2245 train_time:46455ms step_avg:60.65ms
step:767/2245 train_time:46522ms step_avg:60.65ms
step:768/2245 train_time:46584ms step_avg:60.66ms
step:769/2245 train_time:46647ms step_avg:60.66ms
step:770/2245 train_time:46708ms step_avg:60.66ms
step:771/2245 train_time:46770ms step_avg:60.66ms
step:772/2245 train_time:46831ms step_avg:60.66ms
step:773/2245 train_time:46893ms step_avg:60.66ms
step:774/2245 train_time:46952ms step_avg:60.66ms
step:775/2245 train_time:47014ms step_avg:60.66ms
step:776/2245 train_time:47074ms step_avg:60.66ms
step:777/2245 train_time:47136ms step_avg:60.66ms
step:778/2245 train_time:47195ms step_avg:60.66ms
step:779/2245 train_time:47256ms step_avg:60.66ms
step:780/2245 train_time:47316ms step_avg:60.66ms
step:781/2245 train_time:47377ms step_avg:60.66ms
step:782/2245 train_time:47439ms step_avg:60.66ms
step:783/2245 train_time:47503ms step_avg:60.67ms
step:784/2245 train_time:47564ms step_avg:60.67ms
step:785/2245 train_time:47627ms step_avg:60.67ms
step:786/2245 train_time:47688ms step_avg:60.67ms
step:787/2245 train_time:47751ms step_avg:60.67ms
step:788/2245 train_time:47811ms step_avg:60.67ms
step:789/2245 train_time:47873ms step_avg:60.68ms
step:790/2245 train_time:47932ms step_avg:60.67ms
step:791/2245 train_time:47994ms step_avg:60.68ms
step:792/2245 train_time:48054ms step_avg:60.67ms
step:793/2245 train_time:48116ms step_avg:60.68ms
step:794/2245 train_time:48175ms step_avg:60.67ms
step:795/2245 train_time:48237ms step_avg:60.67ms
step:796/2245 train_time:48296ms step_avg:60.67ms
step:797/2245 train_time:48359ms step_avg:60.68ms
step:798/2245 train_time:48419ms step_avg:60.68ms
step:799/2245 train_time:48483ms step_avg:60.68ms
step:800/2245 train_time:48543ms step_avg:60.68ms
step:801/2245 train_time:48606ms step_avg:60.68ms
step:802/2245 train_time:48666ms step_avg:60.68ms
step:803/2245 train_time:48728ms step_avg:60.68ms
step:804/2245 train_time:48788ms step_avg:60.68ms
step:805/2245 train_time:48850ms step_avg:60.68ms
step:806/2245 train_time:48909ms step_avg:60.68ms
step:807/2245 train_time:48971ms step_avg:60.68ms
step:808/2245 train_time:49031ms step_avg:60.68ms
step:809/2245 train_time:49093ms step_avg:60.68ms
step:810/2245 train_time:49152ms step_avg:60.68ms
step:811/2245 train_time:49214ms step_avg:60.68ms
step:812/2245 train_time:49274ms step_avg:60.68ms
step:813/2245 train_time:49336ms step_avg:60.68ms
step:814/2245 train_time:49396ms step_avg:60.68ms
step:815/2245 train_time:49460ms step_avg:60.69ms
step:816/2245 train_time:49521ms step_avg:60.69ms
step:817/2245 train_time:49584ms step_avg:60.69ms
step:818/2245 train_time:49644ms step_avg:60.69ms
step:819/2245 train_time:49708ms step_avg:60.69ms
step:820/2245 train_time:49767ms step_avg:60.69ms
step:821/2245 train_time:49830ms step_avg:60.69ms
step:822/2245 train_time:49889ms step_avg:60.69ms
step:823/2245 train_time:49952ms step_avg:60.69ms
step:824/2245 train_time:50011ms step_avg:60.69ms
step:825/2245 train_time:50073ms step_avg:60.69ms
step:826/2245 train_time:50132ms step_avg:60.69ms
step:827/2245 train_time:50194ms step_avg:60.69ms
step:828/2245 train_time:50255ms step_avg:60.69ms
step:829/2245 train_time:50317ms step_avg:60.70ms
step:830/2245 train_time:50377ms step_avg:60.70ms
step:831/2245 train_time:50440ms step_avg:60.70ms
step:832/2245 train_time:50501ms step_avg:60.70ms
step:833/2245 train_time:50564ms step_avg:60.70ms
step:834/2245 train_time:50624ms step_avg:60.70ms
step:835/2245 train_time:50687ms step_avg:60.70ms
step:836/2245 train_time:50746ms step_avg:60.70ms
step:837/2245 train_time:50808ms step_avg:60.70ms
step:838/2245 train_time:50868ms step_avg:60.70ms
step:839/2245 train_time:50930ms step_avg:60.70ms
step:840/2245 train_time:50990ms step_avg:60.70ms
step:841/2245 train_time:51052ms step_avg:60.70ms
step:842/2245 train_time:51111ms step_avg:60.70ms
step:843/2245 train_time:51173ms step_avg:60.70ms
step:844/2245 train_time:51233ms step_avg:60.70ms
step:845/2245 train_time:51295ms step_avg:60.70ms
step:846/2245 train_time:51356ms step_avg:60.70ms
step:847/2245 train_time:51419ms step_avg:60.71ms
step:848/2245 train_time:51479ms step_avg:60.71ms
step:849/2245 train_time:51542ms step_avg:60.71ms
step:850/2245 train_time:51603ms step_avg:60.71ms
step:851/2245 train_time:51665ms step_avg:60.71ms
step:852/2245 train_time:51725ms step_avg:60.71ms
step:853/2245 train_time:51788ms step_avg:60.71ms
step:854/2245 train_time:51848ms step_avg:60.71ms
step:855/2245 train_time:51910ms step_avg:60.71ms
step:856/2245 train_time:51969ms step_avg:60.71ms
step:857/2245 train_time:52031ms step_avg:60.71ms
step:858/2245 train_time:52091ms step_avg:60.71ms
step:859/2245 train_time:52153ms step_avg:60.71ms
step:860/2245 train_time:52213ms step_avg:60.71ms
step:861/2245 train_time:52275ms step_avg:60.71ms
step:862/2245 train_time:52335ms step_avg:60.71ms
step:863/2245 train_time:52397ms step_avg:60.72ms
step:864/2245 train_time:52458ms step_avg:60.72ms
step:865/2245 train_time:52521ms step_avg:60.72ms
step:866/2245 train_time:52581ms step_avg:60.72ms
step:867/2245 train_time:52644ms step_avg:60.72ms
step:868/2245 train_time:52705ms step_avg:60.72ms
step:869/2245 train_time:52767ms step_avg:60.72ms
step:870/2245 train_time:52827ms step_avg:60.72ms
step:871/2245 train_time:52889ms step_avg:60.72ms
step:872/2245 train_time:52949ms step_avg:60.72ms
step:873/2245 train_time:53011ms step_avg:60.72ms
step:874/2245 train_time:53071ms step_avg:60.72ms
step:875/2245 train_time:53133ms step_avg:60.72ms
step:876/2245 train_time:53193ms step_avg:60.72ms
step:877/2245 train_time:53255ms step_avg:60.72ms
step:878/2245 train_time:53315ms step_avg:60.72ms
step:879/2245 train_time:53378ms step_avg:60.73ms
step:880/2245 train_time:53438ms step_avg:60.72ms
step:881/2245 train_time:53501ms step_avg:60.73ms
step:882/2245 train_time:53561ms step_avg:60.73ms
step:883/2245 train_time:53623ms step_avg:60.73ms
step:884/2245 train_time:53683ms step_avg:60.73ms
step:885/2245 train_time:53746ms step_avg:60.73ms
step:886/2245 train_time:53806ms step_avg:60.73ms
step:887/2245 train_time:53868ms step_avg:60.73ms
step:888/2245 train_time:53928ms step_avg:60.73ms
step:889/2245 train_time:53990ms step_avg:60.73ms
step:890/2245 train_time:54050ms step_avg:60.73ms
step:891/2245 train_time:54112ms step_avg:60.73ms
step:892/2245 train_time:54172ms step_avg:60.73ms
step:893/2245 train_time:54234ms step_avg:60.73ms
step:894/2245 train_time:54294ms step_avg:60.73ms
step:895/2245 train_time:54356ms step_avg:60.73ms
step:896/2245 train_time:54417ms step_avg:60.73ms
step:897/2245 train_time:54479ms step_avg:60.73ms
step:898/2245 train_time:54540ms step_avg:60.73ms
step:899/2245 train_time:54603ms step_avg:60.74ms
step:900/2245 train_time:54663ms step_avg:60.74ms
step:901/2245 train_time:54725ms step_avg:60.74ms
step:902/2245 train_time:54785ms step_avg:60.74ms
step:903/2245 train_time:54847ms step_avg:60.74ms
step:904/2245 train_time:54907ms step_avg:60.74ms
step:905/2245 train_time:54968ms step_avg:60.74ms
step:906/2245 train_time:55028ms step_avg:60.74ms
step:907/2245 train_time:55091ms step_avg:60.74ms
step:908/2245 train_time:55151ms step_avg:60.74ms
step:909/2245 train_time:55213ms step_avg:60.74ms
step:910/2245 train_time:55274ms step_avg:60.74ms
step:911/2245 train_time:55336ms step_avg:60.74ms
step:912/2245 train_time:55396ms step_avg:60.74ms
step:913/2245 train_time:55460ms step_avg:60.74ms
step:914/2245 train_time:55520ms step_avg:60.74ms
step:915/2245 train_time:55582ms step_avg:60.75ms
step:916/2245 train_time:55642ms step_avg:60.74ms
step:917/2245 train_time:55705ms step_avg:60.75ms
step:918/2245 train_time:55765ms step_avg:60.75ms
step:919/2245 train_time:55827ms step_avg:60.75ms
step:920/2245 train_time:55887ms step_avg:60.75ms
step:921/2245 train_time:55949ms step_avg:60.75ms
step:922/2245 train_time:56009ms step_avg:60.75ms
step:923/2245 train_time:56071ms step_avg:60.75ms
step:924/2245 train_time:56131ms step_avg:60.75ms
step:925/2245 train_time:56193ms step_avg:60.75ms
step:926/2245 train_time:56254ms step_avg:60.75ms
step:927/2245 train_time:56316ms step_avg:60.75ms
step:928/2245 train_time:56376ms step_avg:60.75ms
step:929/2245 train_time:56438ms step_avg:60.75ms
step:930/2245 train_time:56499ms step_avg:60.75ms
step:931/2245 train_time:56562ms step_avg:60.75ms
step:932/2245 train_time:56622ms step_avg:60.75ms
step:933/2245 train_time:56684ms step_avg:60.75ms
step:934/2245 train_time:56744ms step_avg:60.75ms
step:935/2245 train_time:56807ms step_avg:60.76ms
step:936/2245 train_time:56867ms step_avg:60.76ms
step:937/2245 train_time:56929ms step_avg:60.76ms
step:938/2245 train_time:56989ms step_avg:60.76ms
step:939/2245 train_time:57050ms step_avg:60.76ms
step:940/2245 train_time:57110ms step_avg:60.76ms
step:941/2245 train_time:57173ms step_avg:60.76ms
step:942/2245 train_time:57232ms step_avg:60.76ms
step:943/2245 train_time:57295ms step_avg:60.76ms
step:944/2245 train_time:57354ms step_avg:60.76ms
step:945/2245 train_time:57417ms step_avg:60.76ms
step:946/2245 train_time:57477ms step_avg:60.76ms
step:947/2245 train_time:57541ms step_avg:60.76ms
step:948/2245 train_time:57602ms step_avg:60.76ms
step:949/2245 train_time:57664ms step_avg:60.76ms
step:950/2245 train_time:57723ms step_avg:60.76ms
step:951/2245 train_time:57785ms step_avg:60.76ms
step:952/2245 train_time:57845ms step_avg:60.76ms
step:953/2245 train_time:57908ms step_avg:60.76ms
step:954/2245 train_time:57967ms step_avg:60.76ms
step:955/2245 train_time:58029ms step_avg:60.76ms
step:956/2245 train_time:58089ms step_avg:60.76ms
step:957/2245 train_time:58152ms step_avg:60.76ms
step:958/2245 train_time:58212ms step_avg:60.76ms
step:959/2245 train_time:58275ms step_avg:60.77ms
step:960/2245 train_time:58335ms step_avg:60.77ms
step:961/2245 train_time:58396ms step_avg:60.77ms
step:962/2245 train_time:58457ms step_avg:60.77ms
step:963/2245 train_time:58519ms step_avg:60.77ms
step:964/2245 train_time:58579ms step_avg:60.77ms
step:965/2245 train_time:58642ms step_avg:60.77ms
step:966/2245 train_time:58702ms step_avg:60.77ms
step:967/2245 train_time:58765ms step_avg:60.77ms
step:968/2245 train_time:58824ms step_avg:60.77ms
step:969/2245 train_time:58887ms step_avg:60.77ms
step:970/2245 train_time:58947ms step_avg:60.77ms
step:971/2245 train_time:59010ms step_avg:60.77ms
step:972/2245 train_time:59069ms step_avg:60.77ms
step:973/2245 train_time:59131ms step_avg:60.77ms
step:974/2245 train_time:59191ms step_avg:60.77ms
step:975/2245 train_time:59254ms step_avg:60.77ms
step:976/2245 train_time:59314ms step_avg:60.77ms
step:977/2245 train_time:59376ms step_avg:60.77ms
step:978/2245 train_time:59436ms step_avg:60.77ms
step:979/2245 train_time:59499ms step_avg:60.77ms
step:980/2245 train_time:59559ms step_avg:60.77ms
step:981/2245 train_time:59622ms step_avg:60.78ms
step:982/2245 train_time:59682ms step_avg:60.78ms
step:983/2245 train_time:59744ms step_avg:60.78ms
step:984/2245 train_time:59805ms step_avg:60.78ms
step:985/2245 train_time:59867ms step_avg:60.78ms
step:986/2245 train_time:59927ms step_avg:60.78ms
step:987/2245 train_time:59990ms step_avg:60.78ms
step:988/2245 train_time:60050ms step_avg:60.78ms
step:989/2245 train_time:60112ms step_avg:60.78ms
step:990/2245 train_time:60172ms step_avg:60.78ms
step:991/2245 train_time:60234ms step_avg:60.78ms
step:992/2245 train_time:60294ms step_avg:60.78ms
step:993/2245 train_time:60357ms step_avg:60.78ms
step:994/2245 train_time:60417ms step_avg:60.78ms
step:995/2245 train_time:60479ms step_avg:60.78ms
step:996/2245 train_time:60540ms step_avg:60.78ms
step:997/2245 train_time:60602ms step_avg:60.78ms
step:998/2245 train_time:60662ms step_avg:60.78ms
step:999/2245 train_time:60724ms step_avg:60.79ms
step:1000/2245 train_time:60784ms step_avg:60.78ms
step:1000/2245 val_loss:3.5977 train_time:60847ms step_avg:60.85ms
step:1001/2245 train_time:60868ms step_avg:60.81ms
step:1002/2245 train_time:60912ms step_avg:60.79ms
step:1003/2245 train_time:60976ms step_avg:60.79ms
step:1004/2245 train_time:61041ms step_avg:60.80ms
step:1005/2245 train_time:61103ms step_avg:60.80ms
step:1006/2245 train_time:61164ms step_avg:60.80ms
step:1007/2245 train_time:61226ms step_avg:60.80ms
step:1008/2245 train_time:61285ms step_avg:60.80ms
step:1009/2245 train_time:61347ms step_avg:60.80ms
step:1010/2245 train_time:61407ms step_avg:60.80ms
step:1011/2245 train_time:61468ms step_avg:60.80ms
step:1012/2245 train_time:61527ms step_avg:60.80ms
step:1013/2245 train_time:61588ms step_avg:60.80ms
step:1014/2245 train_time:61647ms step_avg:60.80ms
step:1015/2245 train_time:61709ms step_avg:60.80ms
step:1016/2245 train_time:61769ms step_avg:60.80ms
step:1017/2245 train_time:61832ms step_avg:60.80ms
step:1018/2245 train_time:61894ms step_avg:60.80ms
step:1019/2245 train_time:61957ms step_avg:60.80ms
step:1020/2245 train_time:62018ms step_avg:60.80ms
step:1021/2245 train_time:62081ms step_avg:60.80ms
step:1022/2245 train_time:62141ms step_avg:60.80ms
step:1023/2245 train_time:62204ms step_avg:60.81ms
step:1024/2245 train_time:62263ms step_avg:60.80ms
step:1025/2245 train_time:62325ms step_avg:60.81ms
step:1026/2245 train_time:62385ms step_avg:60.80ms
step:1027/2245 train_time:62446ms step_avg:60.80ms
step:1028/2245 train_time:62506ms step_avg:60.80ms
step:1029/2245 train_time:62567ms step_avg:60.80ms
step:1030/2245 train_time:62626ms step_avg:60.80ms
step:1031/2245 train_time:62688ms step_avg:60.80ms
step:1032/2245 train_time:62748ms step_avg:60.80ms
step:1033/2245 train_time:62810ms step_avg:60.80ms
step:1034/2245 train_time:62871ms step_avg:60.80ms
step:1035/2245 train_time:62935ms step_avg:60.81ms
step:1036/2245 train_time:62995ms step_avg:60.81ms
step:1037/2245 train_time:63057ms step_avg:60.81ms
step:1038/2245 train_time:63117ms step_avg:60.81ms
step:1039/2245 train_time:63179ms step_avg:60.81ms
step:1040/2245 train_time:63239ms step_avg:60.81ms
step:1041/2245 train_time:63302ms step_avg:60.81ms
step:1042/2245 train_time:63362ms step_avg:60.81ms
step:1043/2245 train_time:63425ms step_avg:60.81ms
step:1044/2245 train_time:63485ms step_avg:60.81ms
step:1045/2245 train_time:63547ms step_avg:60.81ms
step:1046/2245 train_time:63606ms step_avg:60.81ms
step:1047/2245 train_time:63668ms step_avg:60.81ms
step:1048/2245 train_time:63728ms step_avg:60.81ms
step:1049/2245 train_time:63791ms step_avg:60.81ms
step:1050/2245 train_time:63851ms step_avg:60.81ms
step:1051/2245 train_time:63913ms step_avg:60.81ms
step:1052/2245 train_time:63973ms step_avg:60.81ms
step:1053/2245 train_time:64036ms step_avg:60.81ms
step:1054/2245 train_time:64096ms step_avg:60.81ms
step:1055/2245 train_time:64158ms step_avg:60.81ms
step:1056/2245 train_time:64219ms step_avg:60.81ms
step:1057/2245 train_time:64281ms step_avg:60.82ms
step:1058/2245 train_time:64342ms step_avg:60.81ms
step:1059/2245 train_time:64404ms step_avg:60.82ms
step:1060/2245 train_time:64463ms step_avg:60.81ms
step:1061/2245 train_time:64525ms step_avg:60.82ms
step:1062/2245 train_time:64585ms step_avg:60.81ms
step:1063/2245 train_time:64647ms step_avg:60.82ms
step:1064/2245 train_time:64707ms step_avg:60.82ms
step:1065/2245 train_time:64770ms step_avg:60.82ms
step:1066/2245 train_time:64830ms step_avg:60.82ms
step:1067/2245 train_time:64892ms step_avg:60.82ms
step:1068/2245 train_time:64953ms step_avg:60.82ms
step:1069/2245 train_time:65015ms step_avg:60.82ms
step:1070/2245 train_time:65075ms step_avg:60.82ms
step:1071/2245 train_time:65137ms step_avg:60.82ms
step:1072/2245 train_time:65197ms step_avg:60.82ms
step:1073/2245 train_time:65259ms step_avg:60.82ms
step:1074/2245 train_time:65319ms step_avg:60.82ms
step:1075/2245 train_time:65381ms step_avg:60.82ms
step:1076/2245 train_time:65441ms step_avg:60.82ms
step:1077/2245 train_time:65503ms step_avg:60.82ms
step:1078/2245 train_time:65563ms step_avg:60.82ms
step:1079/2245 train_time:65625ms step_avg:60.82ms
step:1080/2245 train_time:65685ms step_avg:60.82ms
step:1081/2245 train_time:65748ms step_avg:60.82ms
step:1082/2245 train_time:65808ms step_avg:60.82ms
step:1083/2245 train_time:65871ms step_avg:60.82ms
step:1084/2245 train_time:65932ms step_avg:60.82ms
step:1085/2245 train_time:65994ms step_avg:60.82ms
step:1086/2245 train_time:66054ms step_avg:60.82ms
step:1087/2245 train_time:66116ms step_avg:60.82ms
step:1088/2245 train_time:66175ms step_avg:60.82ms
step:1089/2245 train_time:66238ms step_avg:60.82ms
step:1090/2245 train_time:66298ms step_avg:60.82ms
step:1091/2245 train_time:66360ms step_avg:60.83ms
step:1092/2245 train_time:66420ms step_avg:60.82ms
step:1093/2245 train_time:66483ms step_avg:60.83ms
step:1094/2245 train_time:66543ms step_avg:60.83ms
step:1095/2245 train_time:66605ms step_avg:60.83ms
step:1096/2245 train_time:66665ms step_avg:60.83ms
step:1097/2245 train_time:66728ms step_avg:60.83ms
step:1098/2245 train_time:66788ms step_avg:60.83ms
step:1099/2245 train_time:66851ms step_avg:60.83ms
step:1100/2245 train_time:66910ms step_avg:60.83ms
step:1101/2245 train_time:66972ms step_avg:60.83ms
step:1102/2245 train_time:67033ms step_avg:60.83ms
step:1103/2245 train_time:67094ms step_avg:60.83ms
step:1104/2245 train_time:67154ms step_avg:60.83ms
step:1105/2245 train_time:67215ms step_avg:60.83ms
step:1106/2245 train_time:67275ms step_avg:60.83ms
step:1107/2245 train_time:67337ms step_avg:60.83ms
step:1108/2245 train_time:67397ms step_avg:60.83ms
step:1109/2245 train_time:67459ms step_avg:60.83ms
step:1110/2245 train_time:67520ms step_avg:60.83ms
step:1111/2245 train_time:67583ms step_avg:60.83ms
step:1112/2245 train_time:67644ms step_avg:60.83ms
step:1113/2245 train_time:67707ms step_avg:60.83ms
step:1114/2245 train_time:67768ms step_avg:60.83ms
step:1115/2245 train_time:67830ms step_avg:60.83ms
step:1116/2245 train_time:67890ms step_avg:60.83ms
step:1117/2245 train_time:67951ms step_avg:60.83ms
step:1118/2245 train_time:68012ms step_avg:60.83ms
step:1119/2245 train_time:68074ms step_avg:60.83ms
step:1120/2245 train_time:68134ms step_avg:60.83ms
step:1121/2245 train_time:68196ms step_avg:60.83ms
step:1122/2245 train_time:68256ms step_avg:60.83ms
step:1123/2245 train_time:68318ms step_avg:60.84ms
step:1124/2245 train_time:68379ms step_avg:60.84ms
step:1125/2245 train_time:68441ms step_avg:60.84ms
step:1126/2245 train_time:68501ms step_avg:60.84ms
step:1127/2245 train_time:68563ms step_avg:60.84ms
step:1128/2245 train_time:68623ms step_avg:60.84ms
step:1129/2245 train_time:68686ms step_avg:60.84ms
step:1130/2245 train_time:68746ms step_avg:60.84ms
step:1131/2245 train_time:68809ms step_avg:60.84ms
step:1132/2245 train_time:68870ms step_avg:60.84ms
step:1133/2245 train_time:68932ms step_avg:60.84ms
step:1134/2245 train_time:68991ms step_avg:60.84ms
step:1135/2245 train_time:69053ms step_avg:60.84ms
step:1136/2245 train_time:69113ms step_avg:60.84ms
step:1137/2245 train_time:69174ms step_avg:60.84ms
step:1138/2245 train_time:69234ms step_avg:60.84ms
step:1139/2245 train_time:69296ms step_avg:60.84ms
step:1140/2245 train_time:69356ms step_avg:60.84ms
step:1141/2245 train_time:69418ms step_avg:60.84ms
step:1142/2245 train_time:69478ms step_avg:60.84ms
step:1143/2245 train_time:69541ms step_avg:60.84ms
step:1144/2245 train_time:69602ms step_avg:60.84ms
step:1145/2245 train_time:69664ms step_avg:60.84ms
step:1146/2245 train_time:69725ms step_avg:60.84ms
step:1147/2245 train_time:69787ms step_avg:60.84ms
step:1148/2245 train_time:69847ms step_avg:60.84ms
step:1149/2245 train_time:69909ms step_avg:60.84ms
step:1150/2245 train_time:69970ms step_avg:60.84ms
step:1151/2245 train_time:70032ms step_avg:60.84ms
step:1152/2245 train_time:70092ms step_avg:60.84ms
step:1153/2245 train_time:70153ms step_avg:60.84ms
step:1154/2245 train_time:70213ms step_avg:60.84ms
step:1155/2245 train_time:70274ms step_avg:60.84ms
step:1156/2245 train_time:70334ms step_avg:60.84ms
step:1157/2245 train_time:70397ms step_avg:60.84ms
step:1158/2245 train_time:70456ms step_avg:60.84ms
step:1159/2245 train_time:70520ms step_avg:60.85ms
step:1160/2245 train_time:70581ms step_avg:60.85ms
step:1161/2245 train_time:70643ms step_avg:60.85ms
step:1162/2245 train_time:70703ms step_avg:60.85ms
step:1163/2245 train_time:70766ms step_avg:60.85ms
step:1164/2245 train_time:70826ms step_avg:60.85ms
step:1165/2245 train_time:70888ms step_avg:60.85ms
step:1166/2245 train_time:70949ms step_avg:60.85ms
step:1167/2245 train_time:71011ms step_avg:60.85ms
step:1168/2245 train_time:71071ms step_avg:60.85ms
step:1169/2245 train_time:71133ms step_avg:60.85ms
step:1170/2245 train_time:71193ms step_avg:60.85ms
step:1171/2245 train_time:71255ms step_avg:60.85ms
step:1172/2245 train_time:71314ms step_avg:60.85ms
step:1173/2245 train_time:71377ms step_avg:60.85ms
step:1174/2245 train_time:71437ms step_avg:60.85ms
step:1175/2245 train_time:71500ms step_avg:60.85ms
step:1176/2245 train_time:71560ms step_avg:60.85ms
step:1177/2245 train_time:71623ms step_avg:60.85ms
step:1178/2245 train_time:71683ms step_avg:60.85ms
step:1179/2245 train_time:71745ms step_avg:60.85ms
step:1180/2245 train_time:71805ms step_avg:60.85ms
step:1181/2245 train_time:71867ms step_avg:60.85ms
step:1182/2245 train_time:71928ms step_avg:60.85ms
step:1183/2245 train_time:71991ms step_avg:60.85ms
step:1184/2245 train_time:72051ms step_avg:60.85ms
step:1185/2245 train_time:72113ms step_avg:60.85ms
step:1186/2245 train_time:72173ms step_avg:60.85ms
step:1187/2245 train_time:72235ms step_avg:60.86ms
step:1188/2245 train_time:72295ms step_avg:60.85ms
step:1189/2245 train_time:72356ms step_avg:60.85ms
step:1190/2245 train_time:72416ms step_avg:60.85ms
step:1191/2245 train_time:72479ms step_avg:60.86ms
step:1192/2245 train_time:72539ms step_avg:60.85ms
step:1193/2245 train_time:72601ms step_avg:60.86ms
step:1194/2245 train_time:72661ms step_avg:60.85ms
step:1195/2245 train_time:72723ms step_avg:60.86ms
step:1196/2245 train_time:72783ms step_avg:60.86ms
step:1197/2245 train_time:72846ms step_avg:60.86ms
step:1198/2245 train_time:72906ms step_avg:60.86ms
step:1199/2245 train_time:72969ms step_avg:60.86ms
step:1200/2245 train_time:73030ms step_avg:60.86ms
step:1201/2245 train_time:73092ms step_avg:60.86ms
step:1202/2245 train_time:73151ms step_avg:60.86ms
step:1203/2245 train_time:73213ms step_avg:60.86ms
step:1204/2245 train_time:73272ms step_avg:60.86ms
step:1205/2245 train_time:73335ms step_avg:60.86ms
step:1206/2245 train_time:73394ms step_avg:60.86ms
step:1207/2245 train_time:73456ms step_avg:60.86ms
step:1208/2245 train_time:73516ms step_avg:60.86ms
step:1209/2245 train_time:73578ms step_avg:60.86ms
step:1210/2245 train_time:73639ms step_avg:60.86ms
step:1211/2245 train_time:73702ms step_avg:60.86ms
step:1212/2245 train_time:73762ms step_avg:60.86ms
step:1213/2245 train_time:73825ms step_avg:60.86ms
step:1214/2245 train_time:73885ms step_avg:60.86ms
step:1215/2245 train_time:73948ms step_avg:60.86ms
step:1216/2245 train_time:74008ms step_avg:60.86ms
step:1217/2245 train_time:74071ms step_avg:60.86ms
step:1218/2245 train_time:74131ms step_avg:60.86ms
step:1219/2245 train_time:74193ms step_avg:60.86ms
step:1220/2245 train_time:74253ms step_avg:60.86ms
step:1221/2245 train_time:74315ms step_avg:60.86ms
step:1222/2245 train_time:74375ms step_avg:60.86ms
step:1223/2245 train_time:74437ms step_avg:60.86ms
step:1224/2245 train_time:74497ms step_avg:60.86ms
step:1225/2245 train_time:74558ms step_avg:60.86ms
step:1226/2245 train_time:74619ms step_avg:60.86ms
step:1227/2245 train_time:74682ms step_avg:60.87ms
step:1228/2245 train_time:74742ms step_avg:60.86ms
step:1229/2245 train_time:74804ms step_avg:60.87ms
step:1230/2245 train_time:74865ms step_avg:60.87ms
step:1231/2245 train_time:74927ms step_avg:60.87ms
step:1232/2245 train_time:74988ms step_avg:60.87ms
step:1233/2245 train_time:75050ms step_avg:60.87ms
step:1234/2245 train_time:75110ms step_avg:60.87ms
step:1235/2245 train_time:75172ms step_avg:60.87ms
step:1236/2245 train_time:75232ms step_avg:60.87ms
step:1237/2245 train_time:75293ms step_avg:60.87ms
step:1238/2245 train_time:75354ms step_avg:60.87ms
step:1239/2245 train_time:75416ms step_avg:60.87ms
step:1240/2245 train_time:75476ms step_avg:60.87ms
step:1241/2245 train_time:75538ms step_avg:60.87ms
step:1242/2245 train_time:75598ms step_avg:60.87ms
step:1243/2245 train_time:75660ms step_avg:60.87ms
step:1244/2245 train_time:75721ms step_avg:60.87ms
step:1245/2245 train_time:75783ms step_avg:60.87ms
step:1246/2245 train_time:75844ms step_avg:60.87ms
step:1247/2245 train_time:75907ms step_avg:60.87ms
step:1248/2245 train_time:75967ms step_avg:60.87ms
step:1249/2245 train_time:76029ms step_avg:60.87ms
step:1250/2245 train_time:76090ms step_avg:60.87ms
step:1250/2245 val_loss:3.5222 train_time:76152ms step_avg:60.92ms
step:1251/2245 train_time:76173ms step_avg:60.89ms
step:1252/2245 train_time:76218ms step_avg:60.88ms
step:1253/2245 train_time:76284ms step_avg:60.88ms
step:1254/2245 train_time:76346ms step_avg:60.88ms
step:1255/2245 train_time:76409ms step_avg:60.88ms
step:1256/2245 train_time:76469ms step_avg:60.88ms
step:1257/2245 train_time:76531ms step_avg:60.88ms
step:1258/2245 train_time:76591ms step_avg:60.88ms
step:1259/2245 train_time:76652ms step_avg:60.88ms
step:1260/2245 train_time:76712ms step_avg:60.88ms
step:1261/2245 train_time:76774ms step_avg:60.88ms
step:1262/2245 train_time:76834ms step_avg:60.88ms
step:1263/2245 train_time:76896ms step_avg:60.88ms
step:1264/2245 train_time:76955ms step_avg:60.88ms
step:1265/2245 train_time:77016ms step_avg:60.88ms
step:1266/2245 train_time:77076ms step_avg:60.88ms
step:1267/2245 train_time:77139ms step_avg:60.88ms
step:1268/2245 train_time:77200ms step_avg:60.88ms
step:1269/2245 train_time:77264ms step_avg:60.89ms
step:1270/2245 train_time:77324ms step_avg:60.88ms
step:1271/2245 train_time:77387ms step_avg:60.89ms
step:1272/2245 train_time:77447ms step_avg:60.89ms
step:1273/2245 train_time:77509ms step_avg:60.89ms
step:1274/2245 train_time:77569ms step_avg:60.89ms
step:1275/2245 train_time:77631ms step_avg:60.89ms
step:1276/2245 train_time:77691ms step_avg:60.89ms
step:1277/2245 train_time:77754ms step_avg:60.89ms
step:1278/2245 train_time:77813ms step_avg:60.89ms
step:1279/2245 train_time:77875ms step_avg:60.89ms
step:1280/2245 train_time:77935ms step_avg:60.89ms
step:1281/2245 train_time:77997ms step_avg:60.89ms
step:1282/2245 train_time:78057ms step_avg:60.89ms
step:1283/2245 train_time:78119ms step_avg:60.89ms
step:1284/2245 train_time:78180ms step_avg:60.89ms
step:1285/2245 train_time:78242ms step_avg:60.89ms
step:1286/2245 train_time:78303ms step_avg:60.89ms
step:1287/2245 train_time:78366ms step_avg:60.89ms
step:1288/2245 train_time:78426ms step_avg:60.89ms
step:1289/2245 train_time:78489ms step_avg:60.89ms
step:1290/2245 train_time:78549ms step_avg:60.89ms
step:1291/2245 train_time:78611ms step_avg:60.89ms
step:1292/2245 train_time:78671ms step_avg:60.89ms
step:1293/2245 train_time:78733ms step_avg:60.89ms
step:1294/2245 train_time:78793ms step_avg:60.89ms
step:1295/2245 train_time:78856ms step_avg:60.89ms
step:1296/2245 train_time:78916ms step_avg:60.89ms
step:1297/2245 train_time:78978ms step_avg:60.89ms
step:1298/2245 train_time:79038ms step_avg:60.89ms
step:1299/2245 train_time:79100ms step_avg:60.89ms
step:1300/2245 train_time:79160ms step_avg:60.89ms
step:1301/2245 train_time:79222ms step_avg:60.89ms
step:1302/2245 train_time:79282ms step_avg:60.89ms
step:1303/2245 train_time:79344ms step_avg:60.89ms
step:1304/2245 train_time:79405ms step_avg:60.89ms
step:1305/2245 train_time:79467ms step_avg:60.89ms
step:1306/2245 train_time:79527ms step_avg:60.89ms
step:1307/2245 train_time:79589ms step_avg:60.89ms
step:1308/2245 train_time:79649ms step_avg:60.89ms
step:1309/2245 train_time:79712ms step_avg:60.90ms
step:1310/2245 train_time:79772ms step_avg:60.89ms
step:1311/2245 train_time:79834ms step_avg:60.90ms
step:1312/2245 train_time:79894ms step_avg:60.89ms
step:1313/2245 train_time:79956ms step_avg:60.90ms
step:1314/2245 train_time:80017ms step_avg:60.90ms
step:1315/2245 train_time:80079ms step_avg:60.90ms
step:1316/2245 train_time:80139ms step_avg:60.90ms
step:1317/2245 train_time:80201ms step_avg:60.90ms
step:1318/2245 train_time:80262ms step_avg:60.90ms
step:1319/2245 train_time:80323ms step_avg:60.90ms
step:1320/2245 train_time:80383ms step_avg:60.90ms
step:1321/2245 train_time:80445ms step_avg:60.90ms
step:1322/2245 train_time:80506ms step_avg:60.90ms
step:1323/2245 train_time:80568ms step_avg:60.90ms
step:1324/2245 train_time:80629ms step_avg:60.90ms
step:1325/2245 train_time:80690ms step_avg:60.90ms
step:1326/2245 train_time:80751ms step_avg:60.90ms
step:1327/2245 train_time:80814ms step_avg:60.90ms
step:1328/2245 train_time:80873ms step_avg:60.90ms
step:1329/2245 train_time:80936ms step_avg:60.90ms
step:1330/2245 train_time:80996ms step_avg:60.90ms
step:1331/2245 train_time:81059ms step_avg:60.90ms
step:1332/2245 train_time:81119ms step_avg:60.90ms
step:1333/2245 train_time:81180ms step_avg:60.90ms
step:1334/2245 train_time:81241ms step_avg:60.90ms
step:1335/2245 train_time:81303ms step_avg:60.90ms
step:1336/2245 train_time:81363ms step_avg:60.90ms
step:1337/2245 train_time:81425ms step_avg:60.90ms
step:1338/2245 train_time:81485ms step_avg:60.90ms
step:1339/2245 train_time:81548ms step_avg:60.90ms
step:1340/2245 train_time:81607ms step_avg:60.90ms
step:1341/2245 train_time:81669ms step_avg:60.90ms
step:1342/2245 train_time:81730ms step_avg:60.90ms
step:1343/2245 train_time:81792ms step_avg:60.90ms
step:1344/2245 train_time:81853ms step_avg:60.90ms
step:1345/2245 train_time:81915ms step_avg:60.90ms
step:1346/2245 train_time:81976ms step_avg:60.90ms
step:1347/2245 train_time:82039ms step_avg:60.90ms
step:1348/2245 train_time:82099ms step_avg:60.90ms
step:1349/2245 train_time:82161ms step_avg:60.91ms
step:1350/2245 train_time:82221ms step_avg:60.90ms
step:1351/2245 train_time:82283ms step_avg:60.91ms
step:1352/2245 train_time:82343ms step_avg:60.90ms
step:1353/2245 train_time:82406ms step_avg:60.91ms
step:1354/2245 train_time:82466ms step_avg:60.91ms
step:1355/2245 train_time:82529ms step_avg:60.91ms
step:1356/2245 train_time:82589ms step_avg:60.91ms
step:1357/2245 train_time:82651ms step_avg:60.91ms
step:1358/2245 train_time:82712ms step_avg:60.91ms
step:1359/2245 train_time:82774ms step_avg:60.91ms
step:1360/2245 train_time:82833ms step_avg:60.91ms
step:1361/2245 train_time:82896ms step_avg:60.91ms
step:1362/2245 train_time:82956ms step_avg:60.91ms
step:1363/2245 train_time:83019ms step_avg:60.91ms
step:1364/2245 train_time:83078ms step_avg:60.91ms
step:1365/2245 train_time:83141ms step_avg:60.91ms
step:1366/2245 train_time:83200ms step_avg:60.91ms
step:1367/2245 train_time:83263ms step_avg:60.91ms
step:1368/2245 train_time:83322ms step_avg:60.91ms
step:1369/2245 train_time:83384ms step_avg:60.91ms
step:1370/2245 train_time:83444ms step_avg:60.91ms
step:1371/2245 train_time:83506ms step_avg:60.91ms
step:1372/2245 train_time:83566ms step_avg:60.91ms
step:1373/2245 train_time:83629ms step_avg:60.91ms
step:1374/2245 train_time:83689ms step_avg:60.91ms
step:1375/2245 train_time:83751ms step_avg:60.91ms
step:1376/2245 train_time:83812ms step_avg:60.91ms
step:1377/2245 train_time:83874ms step_avg:60.91ms
step:1378/2245 train_time:83934ms step_avg:60.91ms
step:1379/2245 train_time:83997ms step_avg:60.91ms
step:1380/2245 train_time:84057ms step_avg:60.91ms
step:1381/2245 train_time:84119ms step_avg:60.91ms
step:1382/2245 train_time:84179ms step_avg:60.91ms
step:1383/2245 train_time:84241ms step_avg:60.91ms
step:1384/2245 train_time:84301ms step_avg:60.91ms
step:1385/2245 train_time:84363ms step_avg:60.91ms
step:1386/2245 train_time:84423ms step_avg:60.91ms
step:1387/2245 train_time:84485ms step_avg:60.91ms
step:1388/2245 train_time:84545ms step_avg:60.91ms
step:1389/2245 train_time:84608ms step_avg:60.91ms
step:1390/2245 train_time:84668ms step_avg:60.91ms
step:1391/2245 train_time:84730ms step_avg:60.91ms
step:1392/2245 train_time:84790ms step_avg:60.91ms
step:1393/2245 train_time:84853ms step_avg:60.91ms
step:1394/2245 train_time:84913ms step_avg:60.91ms
step:1395/2245 train_time:84976ms step_avg:60.91ms
step:1396/2245 train_time:85037ms step_avg:60.91ms
step:1397/2245 train_time:85099ms step_avg:60.92ms
step:1398/2245 train_time:85159ms step_avg:60.91ms
step:1399/2245 train_time:85221ms step_avg:60.92ms
step:1400/2245 train_time:85281ms step_avg:60.92ms
step:1401/2245 train_time:85343ms step_avg:60.92ms
step:1402/2245 train_time:85403ms step_avg:60.91ms
step:1403/2245 train_time:85465ms step_avg:60.92ms
step:1404/2245 train_time:85525ms step_avg:60.92ms
step:1405/2245 train_time:85587ms step_avg:60.92ms
step:1406/2245 train_time:85647ms step_avg:60.92ms
step:1407/2245 train_time:85709ms step_avg:60.92ms
step:1408/2245 train_time:85769ms step_avg:60.92ms
step:1409/2245 train_time:85832ms step_avg:60.92ms
step:1410/2245 train_time:85892ms step_avg:60.92ms
step:1411/2245 train_time:85955ms step_avg:60.92ms
step:1412/2245 train_time:86016ms step_avg:60.92ms
step:1413/2245 train_time:86078ms step_avg:60.92ms
step:1414/2245 train_time:86138ms step_avg:60.92ms
step:1415/2245 train_time:86200ms step_avg:60.92ms
step:1416/2245 train_time:86260ms step_avg:60.92ms
step:1417/2245 train_time:86322ms step_avg:60.92ms
step:1418/2245 train_time:86382ms step_avg:60.92ms
step:1419/2245 train_time:86444ms step_avg:60.92ms
step:1420/2245 train_time:86505ms step_avg:60.92ms
step:1421/2245 train_time:86567ms step_avg:60.92ms
step:1422/2245 train_time:86627ms step_avg:60.92ms
step:1423/2245 train_time:86689ms step_avg:60.92ms
step:1424/2245 train_time:86750ms step_avg:60.92ms
step:1425/2245 train_time:86814ms step_avg:60.92ms
step:1426/2245 train_time:86873ms step_avg:60.92ms
step:1427/2245 train_time:86936ms step_avg:60.92ms
step:1428/2245 train_time:86997ms step_avg:60.92ms
step:1429/2245 train_time:87059ms step_avg:60.92ms
step:1430/2245 train_time:87119ms step_avg:60.92ms
step:1431/2245 train_time:87181ms step_avg:60.92ms
step:1432/2245 train_time:87241ms step_avg:60.92ms
step:1433/2245 train_time:87303ms step_avg:60.92ms
step:1434/2245 train_time:87363ms step_avg:60.92ms
step:1435/2245 train_time:87425ms step_avg:60.92ms
step:1436/2245 train_time:87486ms step_avg:60.92ms
step:1437/2245 train_time:87548ms step_avg:60.92ms
step:1438/2245 train_time:87608ms step_avg:60.92ms
step:1439/2245 train_time:87670ms step_avg:60.92ms
step:1440/2245 train_time:87730ms step_avg:60.92ms
step:1441/2245 train_time:87792ms step_avg:60.92ms
step:1442/2245 train_time:87852ms step_avg:60.92ms
step:1443/2245 train_time:87916ms step_avg:60.93ms
step:1444/2245 train_time:87976ms step_avg:60.93ms
step:1445/2245 train_time:88039ms step_avg:60.93ms
step:1446/2245 train_time:88099ms step_avg:60.93ms
step:1447/2245 train_time:88162ms step_avg:60.93ms
step:1448/2245 train_time:88221ms step_avg:60.93ms
step:1449/2245 train_time:88284ms step_avg:60.93ms
step:1450/2245 train_time:88343ms step_avg:60.93ms
step:1451/2245 train_time:88405ms step_avg:60.93ms
step:1452/2245 train_time:88465ms step_avg:60.93ms
step:1453/2245 train_time:88526ms step_avg:60.93ms
step:1454/2245 train_time:88586ms step_avg:60.93ms
step:1455/2245 train_time:88649ms step_avg:60.93ms
step:1456/2245 train_time:88709ms step_avg:60.93ms
step:1457/2245 train_time:88771ms step_avg:60.93ms
step:1458/2245 train_time:88832ms step_avg:60.93ms
step:1459/2245 train_time:88896ms step_avg:60.93ms
step:1460/2245 train_time:88956ms step_avg:60.93ms
step:1461/2245 train_time:89019ms step_avg:60.93ms
step:1462/2245 train_time:89079ms step_avg:60.93ms
step:1463/2245 train_time:89141ms step_avg:60.93ms
step:1464/2245 train_time:89200ms step_avg:60.93ms
step:1465/2245 train_time:89263ms step_avg:60.93ms
step:1466/2245 train_time:89322ms step_avg:60.93ms
step:1467/2245 train_time:89384ms step_avg:60.93ms
step:1468/2245 train_time:89444ms step_avg:60.93ms
step:1469/2245 train_time:89507ms step_avg:60.93ms
step:1470/2245 train_time:89567ms step_avg:60.93ms
step:1471/2245 train_time:89630ms step_avg:60.93ms
step:1472/2245 train_time:89690ms step_avg:60.93ms
step:1473/2245 train_time:89753ms step_avg:60.93ms
step:1474/2245 train_time:89814ms step_avg:60.93ms
step:1475/2245 train_time:89877ms step_avg:60.93ms
step:1476/2245 train_time:89937ms step_avg:60.93ms
step:1477/2245 train_time:90001ms step_avg:60.93ms
step:1478/2245 train_time:90061ms step_avg:60.93ms
step:1479/2245 train_time:90123ms step_avg:60.94ms
step:1480/2245 train_time:90184ms step_avg:60.93ms
step:1481/2245 train_time:90246ms step_avg:60.94ms
step:1482/2245 train_time:90307ms step_avg:60.94ms
step:1483/2245 train_time:90369ms step_avg:60.94ms
step:1484/2245 train_time:90430ms step_avg:60.94ms
step:1485/2245 train_time:90493ms step_avg:60.94ms
step:1486/2245 train_time:90554ms step_avg:60.94ms
step:1487/2245 train_time:90617ms step_avg:60.94ms
step:1488/2245 train_time:90676ms step_avg:60.94ms
step:1489/2245 train_time:90739ms step_avg:60.94ms
step:1490/2245 train_time:90799ms step_avg:60.94ms
step:1491/2245 train_time:90863ms step_avg:60.94ms
step:1492/2245 train_time:90923ms step_avg:60.94ms
step:1493/2245 train_time:90987ms step_avg:60.94ms
step:1494/2245 train_time:91048ms step_avg:60.94ms
step:1495/2245 train_time:91111ms step_avg:60.94ms
step:1496/2245 train_time:91171ms step_avg:60.94ms
step:1497/2245 train_time:91234ms step_avg:60.94ms
step:1498/2245 train_time:91296ms step_avg:60.94ms
step:1499/2245 train_time:91359ms step_avg:60.95ms
step:1500/2245 train_time:91419ms step_avg:60.95ms
step:1500/2245 val_loss:3.4423 train_time:91482ms step_avg:60.99ms
step:1501/2245 train_time:91503ms step_avg:60.96ms
step:1502/2245 train_time:91543ms step_avg:60.95ms
step:1503/2245 train_time:91612ms step_avg:60.95ms
step:1504/2245 train_time:91676ms step_avg:60.95ms
step:1505/2245 train_time:91740ms step_avg:60.96ms
step:1506/2245 train_time:91802ms step_avg:60.96ms
step:1507/2245 train_time:91865ms step_avg:60.96ms
step:1508/2245 train_time:91924ms step_avg:60.96ms
step:1509/2245 train_time:91986ms step_avg:60.96ms
step:1510/2245 train_time:92045ms step_avg:60.96ms
step:1511/2245 train_time:92107ms step_avg:60.96ms
step:1512/2245 train_time:92166ms step_avg:60.96ms
step:1513/2245 train_time:92228ms step_avg:60.96ms
step:1514/2245 train_time:92287ms step_avg:60.96ms
step:1515/2245 train_time:92349ms step_avg:60.96ms
step:1516/2245 train_time:92415ms step_avg:60.96ms
step:1517/2245 train_time:92480ms step_avg:60.96ms
step:1518/2245 train_time:92542ms step_avg:60.96ms
step:1519/2245 train_time:92606ms step_avg:60.97ms
step:1520/2245 train_time:92667ms step_avg:60.97ms
step:1521/2245 train_time:92731ms step_avg:60.97ms
step:1522/2245 train_time:92791ms step_avg:60.97ms
step:1523/2245 train_time:92854ms step_avg:60.97ms
step:1524/2245 train_time:92915ms step_avg:60.97ms
step:1525/2245 train_time:92978ms step_avg:60.97ms
step:1526/2245 train_time:93038ms step_avg:60.97ms
step:1527/2245 train_time:93100ms step_avg:60.97ms
step:1528/2245 train_time:93160ms step_avg:60.97ms
step:1529/2245 train_time:93222ms step_avg:60.97ms
step:1530/2245 train_time:93282ms step_avg:60.97ms
step:1531/2245 train_time:93344ms step_avg:60.97ms
step:1532/2245 train_time:93406ms step_avg:60.97ms
step:1533/2245 train_time:93470ms step_avg:60.97ms
step:1534/2245 train_time:93530ms step_avg:60.97ms
step:1535/2245 train_time:93594ms step_avg:60.97ms
step:1536/2245 train_time:93655ms step_avg:60.97ms
step:1537/2245 train_time:93718ms step_avg:60.97ms
step:1538/2245 train_time:93779ms step_avg:60.97ms
step:1539/2245 train_time:93843ms step_avg:60.98ms
step:1540/2245 train_time:93904ms step_avg:60.98ms
step:1541/2245 train_time:93966ms step_avg:60.98ms
step:1542/2245 train_time:94027ms step_avg:60.98ms
step:1543/2245 train_time:94089ms step_avg:60.98ms
step:1544/2245 train_time:94149ms step_avg:60.98ms
step:1545/2245 train_time:94212ms step_avg:60.98ms
step:1546/2245 train_time:94272ms step_avg:60.98ms
step:1547/2245 train_time:94335ms step_avg:60.98ms
step:1548/2245 train_time:94396ms step_avg:60.98ms
step:1549/2245 train_time:94459ms step_avg:60.98ms
step:1550/2245 train_time:94520ms step_avg:60.98ms
step:1551/2245 train_time:94583ms step_avg:60.98ms
step:1552/2245 train_time:94643ms step_avg:60.98ms
step:1553/2245 train_time:94706ms step_avg:60.98ms
step:1554/2245 train_time:94767ms step_avg:60.98ms
step:1555/2245 train_time:94830ms step_avg:60.98ms
step:1556/2245 train_time:94890ms step_avg:60.98ms
step:1557/2245 train_time:94953ms step_avg:60.98ms
step:1558/2245 train_time:95014ms step_avg:60.98ms
step:1559/2245 train_time:95076ms step_avg:60.99ms
step:1560/2245 train_time:95137ms step_avg:60.99ms
step:1561/2245 train_time:95200ms step_avg:60.99ms
step:1562/2245 train_time:95261ms step_avg:60.99ms
step:1563/2245 train_time:95324ms step_avg:60.99ms
step:1564/2245 train_time:95384ms step_avg:60.99ms
step:1565/2245 train_time:95447ms step_avg:60.99ms
step:1566/2245 train_time:95506ms step_avg:60.99ms
step:1567/2245 train_time:95569ms step_avg:60.99ms
step:1568/2245 train_time:95630ms step_avg:60.99ms
step:1569/2245 train_time:95694ms step_avg:60.99ms
step:1570/2245 train_time:95754ms step_avg:60.99ms
step:1571/2245 train_time:95817ms step_avg:60.99ms
step:1572/2245 train_time:95878ms step_avg:60.99ms
step:1573/2245 train_time:95941ms step_avg:60.99ms
step:1574/2245 train_time:96002ms step_avg:60.99ms
step:1575/2245 train_time:96065ms step_avg:60.99ms
step:1576/2245 train_time:96125ms step_avg:60.99ms
step:1577/2245 train_time:96188ms step_avg:60.99ms
step:1578/2245 train_time:96248ms step_avg:60.99ms
step:1579/2245 train_time:96311ms step_avg:61.00ms
step:1580/2245 train_time:96371ms step_avg:60.99ms
step:1581/2245 train_time:96434ms step_avg:61.00ms
step:1582/2245 train_time:96495ms step_avg:61.00ms
step:1583/2245 train_time:96558ms step_avg:61.00ms
step:1584/2245 train_time:96620ms step_avg:61.00ms
step:1585/2245 train_time:96682ms step_avg:61.00ms
step:1586/2245 train_time:96743ms step_avg:61.00ms
step:1587/2245 train_time:96806ms step_avg:61.00ms
step:1588/2245 train_time:96867ms step_avg:61.00ms
step:1589/2245 train_time:96929ms step_avg:61.00ms
step:1590/2245 train_time:96990ms step_avg:61.00ms
step:1591/2245 train_time:97052ms step_avg:61.00ms
step:1592/2245 train_time:97114ms step_avg:61.00ms
step:1593/2245 train_time:97177ms step_avg:61.00ms
step:1594/2245 train_time:97237ms step_avg:61.00ms
step:1595/2245 train_time:97300ms step_avg:61.00ms
step:1596/2245 train_time:97361ms step_avg:61.00ms
step:1597/2245 train_time:97424ms step_avg:61.00ms
step:1598/2245 train_time:97484ms step_avg:61.00ms
step:1599/2245 train_time:97547ms step_avg:61.00ms
step:1600/2245 train_time:97608ms step_avg:61.00ms
step:1601/2245 train_time:97670ms step_avg:61.01ms
step:1602/2245 train_time:97730ms step_avg:61.01ms
step:1603/2245 train_time:97793ms step_avg:61.01ms
step:1604/2245 train_time:97855ms step_avg:61.01ms
step:1605/2245 train_time:97917ms step_avg:61.01ms
step:1606/2245 train_time:97978ms step_avg:61.01ms
step:1607/2245 train_time:98042ms step_avg:61.01ms
step:1608/2245 train_time:98102ms step_avg:61.01ms
step:1609/2245 train_time:98165ms step_avg:61.01ms
step:1610/2245 train_time:98225ms step_avg:61.01ms
step:1611/2245 train_time:98288ms step_avg:61.01ms
step:1612/2245 train_time:98348ms step_avg:61.01ms
step:1613/2245 train_time:98410ms step_avg:61.01ms
step:1614/2245 train_time:98470ms step_avg:61.01ms
step:1615/2245 train_time:98533ms step_avg:61.01ms
step:1616/2245 train_time:98594ms step_avg:61.01ms
step:1617/2245 train_time:98657ms step_avg:61.01ms
step:1618/2245 train_time:98717ms step_avg:61.01ms
step:1619/2245 train_time:98780ms step_avg:61.01ms
step:1620/2245 train_time:98841ms step_avg:61.01ms
step:1621/2245 train_time:98904ms step_avg:61.01ms
step:1622/2245 train_time:98965ms step_avg:61.01ms
step:1623/2245 train_time:99027ms step_avg:61.01ms
step:1624/2245 train_time:99087ms step_avg:61.01ms
step:1625/2245 train_time:99150ms step_avg:61.02ms
step:1626/2245 train_time:99211ms step_avg:61.02ms
step:1627/2245 train_time:99274ms step_avg:61.02ms
step:1628/2245 train_time:99335ms step_avg:61.02ms
step:1629/2245 train_time:99397ms step_avg:61.02ms
step:1630/2245 train_time:99458ms step_avg:61.02ms
step:1631/2245 train_time:99521ms step_avg:61.02ms
step:1632/2245 train_time:99582ms step_avg:61.02ms
step:1633/2245 train_time:99645ms step_avg:61.02ms
step:1634/2245 train_time:99705ms step_avg:61.02ms
step:1635/2245 train_time:99768ms step_avg:61.02ms
step:1636/2245 train_time:99829ms step_avg:61.02ms
step:1637/2245 train_time:99891ms step_avg:61.02ms
step:1638/2245 train_time:99952ms step_avg:61.02ms
step:1639/2245 train_time:100015ms step_avg:61.02ms
step:1640/2245 train_time:100075ms step_avg:61.02ms
step:1641/2245 train_time:100138ms step_avg:61.02ms
step:1642/2245 train_time:100199ms step_avg:61.02ms
step:1643/2245 train_time:100263ms step_avg:61.02ms
step:1644/2245 train_time:100324ms step_avg:61.02ms
step:1645/2245 train_time:100386ms step_avg:61.02ms
step:1646/2245 train_time:100446ms step_avg:61.02ms
step:1647/2245 train_time:100509ms step_avg:61.03ms
step:1648/2245 train_time:100569ms step_avg:61.02ms
step:1649/2245 train_time:100631ms step_avg:61.03ms
step:1650/2245 train_time:100692ms step_avg:61.03ms
step:1651/2245 train_time:100755ms step_avg:61.03ms
step:1652/2245 train_time:100816ms step_avg:61.03ms
step:1653/2245 train_time:100879ms step_avg:61.03ms
step:1654/2245 train_time:100939ms step_avg:61.03ms
step:1655/2245 train_time:101003ms step_avg:61.03ms
step:1656/2245 train_time:101064ms step_avg:61.03ms
step:1657/2245 train_time:101126ms step_avg:61.03ms
step:1658/2245 train_time:101186ms step_avg:61.03ms
step:1659/2245 train_time:101248ms step_avg:61.03ms
step:1660/2245 train_time:101309ms step_avg:61.03ms
step:1661/2245 train_time:101371ms step_avg:61.03ms
step:1662/2245 train_time:101431ms step_avg:61.03ms
step:1663/2245 train_time:101494ms step_avg:61.03ms
step:1664/2245 train_time:101555ms step_avg:61.03ms
step:1665/2245 train_time:101619ms step_avg:61.03ms
step:1666/2245 train_time:101679ms step_avg:61.03ms
step:1667/2245 train_time:101742ms step_avg:61.03ms
step:1668/2245 train_time:101803ms step_avg:61.03ms
step:1669/2245 train_time:101866ms step_avg:61.03ms
step:1670/2245 train_time:101926ms step_avg:61.03ms
step:1671/2245 train_time:101989ms step_avg:61.03ms
step:1672/2245 train_time:102049ms step_avg:61.03ms
step:1673/2245 train_time:102111ms step_avg:61.03ms
step:1674/2245 train_time:102171ms step_avg:61.03ms
step:1675/2245 train_time:102234ms step_avg:61.04ms
step:1676/2245 train_time:102294ms step_avg:61.03ms
step:1677/2245 train_time:102357ms step_avg:61.04ms
step:1678/2245 train_time:102419ms step_avg:61.04ms
step:1679/2245 train_time:102482ms step_avg:61.04ms
step:1680/2245 train_time:102543ms step_avg:61.04ms
step:1681/2245 train_time:102605ms step_avg:61.04ms
step:1682/2245 train_time:102666ms step_avg:61.04ms
step:1683/2245 train_time:102729ms step_avg:61.04ms
step:1684/2245 train_time:102789ms step_avg:61.04ms
step:1685/2245 train_time:102852ms step_avg:61.04ms
step:1686/2245 train_time:102913ms step_avg:61.04ms
step:1687/2245 train_time:102977ms step_avg:61.04ms
step:1688/2245 train_time:103037ms step_avg:61.04ms
step:1689/2245 train_time:103100ms step_avg:61.04ms
step:1690/2245 train_time:103162ms step_avg:61.04ms
step:1691/2245 train_time:103225ms step_avg:61.04ms
step:1692/2245 train_time:103285ms step_avg:61.04ms
step:1693/2245 train_time:103348ms step_avg:61.04ms
step:1694/2245 train_time:103409ms step_avg:61.04ms
step:1695/2245 train_time:103472ms step_avg:61.05ms
step:1696/2245 train_time:103532ms step_avg:61.04ms
step:1697/2245 train_time:103595ms step_avg:61.05ms
step:1698/2245 train_time:103656ms step_avg:61.05ms
step:1699/2245 train_time:103719ms step_avg:61.05ms
step:1700/2245 train_time:103780ms step_avg:61.05ms
step:1701/2245 train_time:103843ms step_avg:61.05ms
step:1702/2245 train_time:103904ms step_avg:61.05ms
step:1703/2245 train_time:103967ms step_avg:61.05ms
step:1704/2245 train_time:104028ms step_avg:61.05ms
step:1705/2245 train_time:104090ms step_avg:61.05ms
step:1706/2245 train_time:104151ms step_avg:61.05ms
step:1707/2245 train_time:104213ms step_avg:61.05ms
step:1708/2245 train_time:104275ms step_avg:61.05ms
step:1709/2245 train_time:104338ms step_avg:61.05ms
step:1710/2245 train_time:104399ms step_avg:61.05ms
step:1711/2245 train_time:104461ms step_avg:61.05ms
step:1712/2245 train_time:104523ms step_avg:61.05ms
step:1713/2245 train_time:104585ms step_avg:61.05ms
step:1714/2245 train_time:104646ms step_avg:61.05ms
step:1715/2245 train_time:104708ms step_avg:61.05ms
step:1716/2245 train_time:104768ms step_avg:61.05ms
step:1717/2245 train_time:104831ms step_avg:61.05ms
step:1718/2245 train_time:104891ms step_avg:61.05ms
step:1719/2245 train_time:104954ms step_avg:61.06ms
step:1720/2245 train_time:105015ms step_avg:61.06ms
step:1721/2245 train_time:105078ms step_avg:61.06ms
step:1722/2245 train_time:105139ms step_avg:61.06ms
step:1723/2245 train_time:105202ms step_avg:61.06ms
step:1724/2245 train_time:105263ms step_avg:61.06ms
step:1725/2245 train_time:105326ms step_avg:61.06ms
step:1726/2245 train_time:105386ms step_avg:61.06ms
step:1727/2245 train_time:105449ms step_avg:61.06ms
step:1728/2245 train_time:105510ms step_avg:61.06ms
step:1729/2245 train_time:105573ms step_avg:61.06ms
step:1730/2245 train_time:105634ms step_avg:61.06ms
step:1731/2245 train_time:105696ms step_avg:61.06ms
step:1732/2245 train_time:105758ms step_avg:61.06ms
step:1733/2245 train_time:105821ms step_avg:61.06ms
step:1734/2245 train_time:105882ms step_avg:61.06ms
step:1735/2245 train_time:105944ms step_avg:61.06ms
step:1736/2245 train_time:106005ms step_avg:61.06ms
step:1737/2245 train_time:106068ms step_avg:61.06ms
step:1738/2245 train_time:106128ms step_avg:61.06ms
step:1739/2245 train_time:106191ms step_avg:61.06ms
step:1740/2245 train_time:106251ms step_avg:61.06ms
step:1741/2245 train_time:106314ms step_avg:61.06ms
step:1742/2245 train_time:106375ms step_avg:61.06ms
step:1743/2245 train_time:106437ms step_avg:61.07ms
step:1744/2245 train_time:106498ms step_avg:61.07ms
step:1745/2245 train_time:106560ms step_avg:61.07ms
step:1746/2245 train_time:106621ms step_avg:61.07ms
step:1747/2245 train_time:106684ms step_avg:61.07ms
step:1748/2245 train_time:106745ms step_avg:61.07ms
step:1749/2245 train_time:106807ms step_avg:61.07ms
step:1750/2245 train_time:106868ms step_avg:61.07ms
step:1750/2245 val_loss:3.3774 train_time:106931ms step_avg:61.10ms
step:1751/2245 train_time:106951ms step_avg:61.08ms
step:1752/2245 train_time:106996ms step_avg:61.07ms
step:1753/2245 train_time:107063ms step_avg:61.07ms
step:1754/2245 train_time:107126ms step_avg:61.08ms
step:1755/2245 train_time:107188ms step_avg:61.08ms
step:1756/2245 train_time:107248ms step_avg:61.08ms
step:1757/2245 train_time:107309ms step_avg:61.08ms
step:1758/2245 train_time:107370ms step_avg:61.07ms
step:1759/2245 train_time:107432ms step_avg:61.08ms
step:1760/2245 train_time:107492ms step_avg:61.07ms
step:1761/2245 train_time:107554ms step_avg:61.08ms
step:1762/2245 train_time:107613ms step_avg:61.07ms
step:1763/2245 train_time:107675ms step_avg:61.08ms
step:1764/2245 train_time:107736ms step_avg:61.07ms
step:1765/2245 train_time:107798ms step_avg:61.08ms
step:1766/2245 train_time:107859ms step_avg:61.08ms
step:1767/2245 train_time:107922ms step_avg:61.08ms
step:1768/2245 train_time:107984ms step_avg:61.08ms
step:1769/2245 train_time:108048ms step_avg:61.08ms
step:1770/2245 train_time:108110ms step_avg:61.08ms
step:1771/2245 train_time:108173ms step_avg:61.08ms
step:1772/2245 train_time:108234ms step_avg:61.08ms
step:1773/2245 train_time:108297ms step_avg:61.08ms
step:1774/2245 train_time:108357ms step_avg:61.08ms
step:1775/2245 train_time:108420ms step_avg:61.08ms
step:1776/2245 train_time:108480ms step_avg:61.08ms
step:1777/2245 train_time:108542ms step_avg:61.08ms
step:1778/2245 train_time:108602ms step_avg:61.08ms
step:1779/2245 train_time:108665ms step_avg:61.08ms
step:1780/2245 train_time:108725ms step_avg:61.08ms
step:1781/2245 train_time:108787ms step_avg:61.08ms
step:1782/2245 train_time:108848ms step_avg:61.08ms
step:1783/2245 train_time:108911ms step_avg:61.08ms
step:1784/2245 train_time:108972ms step_avg:61.08ms
step:1785/2245 train_time:109036ms step_avg:61.08ms
step:1786/2245 train_time:109097ms step_avg:61.08ms
step:1787/2245 train_time:109160ms step_avg:61.09ms
step:1788/2245 train_time:109221ms step_avg:61.09ms
step:1789/2245 train_time:109284ms step_avg:61.09ms
step:1790/2245 train_time:109344ms step_avg:61.09ms
step:1791/2245 train_time:109407ms step_avg:61.09ms
step:1792/2245 train_time:109467ms step_avg:61.09ms
step:1793/2245 train_time:109529ms step_avg:61.09ms
step:1794/2245 train_time:109590ms step_avg:61.09ms
step:1795/2245 train_time:109651ms step_avg:61.09ms
step:1796/2245 train_time:109711ms step_avg:61.09ms
step:1797/2245 train_time:109774ms step_avg:61.09ms
step:1798/2245 train_time:109835ms step_avg:61.09ms
step:1799/2245 train_time:109898ms step_avg:61.09ms
step:1800/2245 train_time:109958ms step_avg:61.09ms
step:1801/2245 train_time:110022ms step_avg:61.09ms
step:1802/2245 train_time:110083ms step_avg:61.09ms
step:1803/2245 train_time:110147ms step_avg:61.09ms
step:1804/2245 train_time:110208ms step_avg:61.09ms
step:1805/2245 train_time:110270ms step_avg:61.09ms
step:1806/2245 train_time:110331ms step_avg:61.09ms
step:1807/2245 train_time:110393ms step_avg:61.09ms
step:1808/2245 train_time:110453ms step_avg:61.09ms
step:1809/2245 train_time:110516ms step_avg:61.09ms
step:1810/2245 train_time:110577ms step_avg:61.09ms
step:1811/2245 train_time:110640ms step_avg:61.09ms
step:1812/2245 train_time:110701ms step_avg:61.09ms
step:1813/2245 train_time:110764ms step_avg:61.09ms
step:1814/2245 train_time:110824ms step_avg:61.09ms
step:1815/2245 train_time:110887ms step_avg:61.10ms
step:1816/2245 train_time:110948ms step_avg:61.09ms
step:1817/2245 train_time:111011ms step_avg:61.10ms
step:1818/2245 train_time:111071ms step_avg:61.10ms
step:1819/2245 train_time:111135ms step_avg:61.10ms
step:1820/2245 train_time:111195ms step_avg:61.10ms
step:1821/2245 train_time:111258ms step_avg:61.10ms
step:1822/2245 train_time:111320ms step_avg:61.10ms
step:1823/2245 train_time:111384ms step_avg:61.10ms
step:1824/2245 train_time:111443ms step_avg:61.10ms
step:1825/2245 train_time:111505ms step_avg:61.10ms
step:1826/2245 train_time:111565ms step_avg:61.10ms
step:1827/2245 train_time:111628ms step_avg:61.10ms
step:1828/2245 train_time:111688ms step_avg:61.10ms
step:1829/2245 train_time:111751ms step_avg:61.10ms
step:1830/2245 train_time:111811ms step_avg:61.10ms
step:1831/2245 train_time:111875ms step_avg:61.10ms
step:1832/2245 train_time:111935ms step_avg:61.10ms
step:1833/2245 train_time:111998ms step_avg:61.10ms
step:1834/2245 train_time:112059ms step_avg:61.10ms
step:1835/2245 train_time:112122ms step_avg:61.10ms
step:1836/2245 train_time:112183ms step_avg:61.10ms
step:1837/2245 train_time:112246ms step_avg:61.10ms
step:1838/2245 train_time:112306ms step_avg:61.10ms
step:1839/2245 train_time:112369ms step_avg:61.10ms
step:1840/2245 train_time:112429ms step_avg:61.10ms
step:1841/2245 train_time:112492ms step_avg:61.10ms
step:1842/2245 train_time:112552ms step_avg:61.10ms
step:1843/2245 train_time:112615ms step_avg:61.10ms
step:1844/2245 train_time:112675ms step_avg:61.10ms
step:1845/2245 train_time:112739ms step_avg:61.11ms
step:1846/2245 train_time:112799ms step_avg:61.10ms
step:1847/2245 train_time:112861ms step_avg:61.11ms
step:1848/2245 train_time:112922ms step_avg:61.11ms
step:1849/2245 train_time:112985ms step_avg:61.11ms
step:1850/2245 train_time:113045ms step_avg:61.11ms
step:1851/2245 train_time:113108ms step_avg:61.11ms
step:1852/2245 train_time:113169ms step_avg:61.11ms
step:1853/2245 train_time:113232ms step_avg:61.11ms
step:1854/2245 train_time:113292ms step_avg:61.11ms
step:1855/2245 train_time:113355ms step_avg:61.11ms
step:1856/2245 train_time:113415ms step_avg:61.11ms
step:1857/2245 train_time:113478ms step_avg:61.11ms
step:1858/2245 train_time:113539ms step_avg:61.11ms
step:1859/2245 train_time:113602ms step_avg:61.11ms
step:1860/2245 train_time:113662ms step_avg:61.11ms
step:1861/2245 train_time:113725ms step_avg:61.11ms
step:1862/2245 train_time:113784ms step_avg:61.11ms
step:1863/2245 train_time:113847ms step_avg:61.11ms
step:1864/2245 train_time:113907ms step_avg:61.11ms
step:1865/2245 train_time:113970ms step_avg:61.11ms
step:1866/2245 train_time:114031ms step_avg:61.11ms
step:1867/2245 train_time:114094ms step_avg:61.11ms
step:1868/2245 train_time:114155ms step_avg:61.11ms
step:1869/2245 train_time:114218ms step_avg:61.11ms
step:1870/2245 train_time:114279ms step_avg:61.11ms
step:1871/2245 train_time:114342ms step_avg:61.11ms
step:1872/2245 train_time:114403ms step_avg:61.11ms
step:1873/2245 train_time:114465ms step_avg:61.11ms
step:1874/2245 train_time:114524ms step_avg:61.11ms
step:1875/2245 train_time:114587ms step_avg:61.11ms
step:1876/2245 train_time:114647ms step_avg:61.11ms
step:1877/2245 train_time:114710ms step_avg:61.11ms
step:1878/2245 train_time:114770ms step_avg:61.11ms
step:1879/2245 train_time:114834ms step_avg:61.11ms
step:1880/2245 train_time:114894ms step_avg:61.11ms
step:1881/2245 train_time:114957ms step_avg:61.11ms
step:1882/2245 train_time:115017ms step_avg:61.11ms
step:1883/2245 train_time:115080ms step_avg:61.12ms
step:1884/2245 train_time:115141ms step_avg:61.12ms
step:1885/2245 train_time:115204ms step_avg:61.12ms
step:1886/2245 train_time:115265ms step_avg:61.12ms
step:1887/2245 train_time:115326ms step_avg:61.12ms
step:1888/2245 train_time:115387ms step_avg:61.12ms
step:1889/2245 train_time:115450ms step_avg:61.12ms
step:1890/2245 train_time:115510ms step_avg:61.12ms
step:1891/2245 train_time:115574ms step_avg:61.12ms
step:1892/2245 train_time:115635ms step_avg:61.12ms
step:1893/2245 train_time:115698ms step_avg:61.12ms
step:1894/2245 train_time:115759ms step_avg:61.12ms
step:1895/2245 train_time:115822ms step_avg:61.12ms
step:1896/2245 train_time:115884ms step_avg:61.12ms
step:1897/2245 train_time:115947ms step_avg:61.12ms
step:1898/2245 train_time:116007ms step_avg:61.12ms
step:1899/2245 train_time:116071ms step_avg:61.12ms
step:1900/2245 train_time:116132ms step_avg:61.12ms
step:1901/2245 train_time:116195ms step_avg:61.12ms
step:1902/2245 train_time:116256ms step_avg:61.12ms
step:1903/2245 train_time:116318ms step_avg:61.12ms
step:1904/2245 train_time:116380ms step_avg:61.12ms
step:1905/2245 train_time:116442ms step_avg:61.12ms
step:1906/2245 train_time:116503ms step_avg:61.12ms
step:1907/2245 train_time:116565ms step_avg:61.12ms
step:1908/2245 train_time:116625ms step_avg:61.12ms
step:1909/2245 train_time:116687ms step_avg:61.12ms
step:1910/2245 train_time:116748ms step_avg:61.12ms
step:1911/2245 train_time:116811ms step_avg:61.13ms
step:1912/2245 train_time:116872ms step_avg:61.13ms
step:1913/2245 train_time:116935ms step_avg:61.13ms
step:1914/2245 train_time:116996ms step_avg:61.13ms
step:1915/2245 train_time:117058ms step_avg:61.13ms
step:1916/2245 train_time:117119ms step_avg:61.13ms
step:1917/2245 train_time:117182ms step_avg:61.13ms
step:1918/2245 train_time:117242ms step_avg:61.13ms
step:1919/2245 train_time:117305ms step_avg:61.13ms
step:1920/2245 train_time:117365ms step_avg:61.13ms
step:1921/2245 train_time:117427ms step_avg:61.13ms
step:1922/2245 train_time:117488ms step_avg:61.13ms
step:1923/2245 train_time:117551ms step_avg:61.13ms
step:1924/2245 train_time:117611ms step_avg:61.13ms
step:1925/2245 train_time:117674ms step_avg:61.13ms
step:1926/2245 train_time:117734ms step_avg:61.13ms
step:1927/2245 train_time:117797ms step_avg:61.13ms
step:1928/2245 train_time:117857ms step_avg:61.13ms
step:1929/2245 train_time:117920ms step_avg:61.13ms
step:1930/2245 train_time:117981ms step_avg:61.13ms
step:1931/2245 train_time:118044ms step_avg:61.13ms
step:1932/2245 train_time:118105ms step_avg:61.13ms
step:1933/2245 train_time:118167ms step_avg:61.13ms
step:1934/2245 train_time:118227ms step_avg:61.13ms
step:1935/2245 train_time:118290ms step_avg:61.13ms
step:1936/2245 train_time:118351ms step_avg:61.13ms
step:1937/2245 train_time:118414ms step_avg:61.13ms
step:1938/2245 train_time:118475ms step_avg:61.13ms
step:1939/2245 train_time:118538ms step_avg:61.13ms
step:1940/2245 train_time:118599ms step_avg:61.13ms
step:1941/2245 train_time:118661ms step_avg:61.13ms
step:1942/2245 train_time:118723ms step_avg:61.13ms
step:1943/2245 train_time:118785ms step_avg:61.14ms
step:1944/2245 train_time:118845ms step_avg:61.13ms
step:1945/2245 train_time:118908ms step_avg:61.14ms
step:1946/2245 train_time:118969ms step_avg:61.13ms
step:1947/2245 train_time:119032ms step_avg:61.14ms
step:1948/2245 train_time:119092ms step_avg:61.14ms
step:1949/2245 train_time:119155ms step_avg:61.14ms
step:1950/2245 train_time:119215ms step_avg:61.14ms
step:1951/2245 train_time:119278ms step_avg:61.14ms
step:1952/2245 train_time:119339ms step_avg:61.14ms
step:1953/2245 train_time:119402ms step_avg:61.14ms
step:1954/2245 train_time:119462ms step_avg:61.14ms
step:1955/2245 train_time:119525ms step_avg:61.14ms
step:1956/2245 train_time:119585ms step_avg:61.14ms
step:1957/2245 train_time:119648ms step_avg:61.14ms
step:1958/2245 train_time:119708ms step_avg:61.14ms
step:1959/2245 train_time:119771ms step_avg:61.14ms
step:1960/2245 train_time:119831ms step_avg:61.14ms
step:1961/2245 train_time:119894ms step_avg:61.14ms
step:1962/2245 train_time:119955ms step_avg:61.14ms
step:1963/2245 train_time:120017ms step_avg:61.14ms
step:1964/2245 train_time:120079ms step_avg:61.14ms
step:1965/2245 train_time:120142ms step_avg:61.14ms
step:1966/2245 train_time:120203ms step_avg:61.14ms
step:1967/2245 train_time:120266ms step_avg:61.14ms
step:1968/2245 train_time:120326ms step_avg:61.14ms
step:1969/2245 train_time:120389ms step_avg:61.14ms
step:1970/2245 train_time:120449ms step_avg:61.14ms
step:1971/2245 train_time:120513ms step_avg:61.14ms
step:1972/2245 train_time:120573ms step_avg:61.14ms
step:1973/2245 train_time:120636ms step_avg:61.14ms
step:1974/2245 train_time:120697ms step_avg:61.14ms
step:1975/2245 train_time:120759ms step_avg:61.14ms
step:1976/2245 train_time:120820ms step_avg:61.14ms
step:1977/2245 train_time:120883ms step_avg:61.14ms
step:1978/2245 train_time:120944ms step_avg:61.14ms
step:1979/2245 train_time:121006ms step_avg:61.15ms
step:1980/2245 train_time:121067ms step_avg:61.14ms
step:1981/2245 train_time:121130ms step_avg:61.15ms
step:1982/2245 train_time:121191ms step_avg:61.15ms
step:1983/2245 train_time:121254ms step_avg:61.15ms
step:1984/2245 train_time:121314ms step_avg:61.15ms
step:1985/2245 train_time:121378ms step_avg:61.15ms
step:1986/2245 train_time:121439ms step_avg:61.15ms
step:1987/2245 train_time:121502ms step_avg:61.15ms
step:1988/2245 train_time:121563ms step_avg:61.15ms
step:1989/2245 train_time:121625ms step_avg:61.15ms
step:1990/2245 train_time:121685ms step_avg:61.15ms
step:1991/2245 train_time:121748ms step_avg:61.15ms
step:1992/2245 train_time:121808ms step_avg:61.15ms
step:1993/2245 train_time:121871ms step_avg:61.15ms
step:1994/2245 train_time:121932ms step_avg:61.15ms
step:1995/2245 train_time:121995ms step_avg:61.15ms
step:1996/2245 train_time:122055ms step_avg:61.15ms
step:1997/2245 train_time:122118ms step_avg:61.15ms
step:1998/2245 train_time:122179ms step_avg:61.15ms
step:1999/2245 train_time:122242ms step_avg:61.15ms
step:2000/2245 train_time:122302ms step_avg:61.15ms
step:2000/2245 val_loss:3.3233 train_time:122366ms step_avg:61.18ms
step:2001/2245 train_time:122388ms step_avg:61.16ms
step:2002/2245 train_time:122429ms step_avg:61.15ms
step:2003/2245 train_time:122495ms step_avg:61.16ms
step:2004/2245 train_time:122558ms step_avg:61.16ms
step:2005/2245 train_time:122622ms step_avg:61.16ms
step:2006/2245 train_time:122682ms step_avg:61.16ms
step:2007/2245 train_time:122744ms step_avg:61.16ms
step:2008/2245 train_time:122804ms step_avg:61.16ms
step:2009/2245 train_time:122866ms step_avg:61.16ms
step:2010/2245 train_time:122926ms step_avg:61.16ms
step:2011/2245 train_time:122988ms step_avg:61.16ms
step:2012/2245 train_time:123048ms step_avg:61.16ms
step:2013/2245 train_time:123111ms step_avg:61.16ms
step:2014/2245 train_time:123171ms step_avg:61.16ms
step:2015/2245 train_time:123233ms step_avg:61.16ms
step:2016/2245 train_time:123294ms step_avg:61.16ms
step:2017/2245 train_time:123359ms step_avg:61.16ms
step:2018/2245 train_time:123420ms step_avg:61.16ms
step:2019/2245 train_time:123484ms step_avg:61.16ms
step:2020/2245 train_time:123546ms step_avg:61.16ms
step:2021/2245 train_time:123609ms step_avg:61.16ms
step:2022/2245 train_time:123671ms step_avg:61.16ms
step:2023/2245 train_time:123734ms step_avg:61.16ms
step:2024/2245 train_time:123795ms step_avg:61.16ms
step:2025/2245 train_time:123857ms step_avg:61.16ms
step:2026/2245 train_time:123918ms step_avg:61.16ms
step:2027/2245 train_time:123979ms step_avg:61.16ms
step:2028/2245 train_time:124040ms step_avg:61.16ms
step:2029/2245 train_time:124101ms step_avg:61.16ms
step:2030/2245 train_time:124162ms step_avg:61.16ms
step:2031/2245 train_time:124224ms step_avg:61.16ms
step:2032/2245 train_time:124284ms step_avg:61.16ms
step:2033/2245 train_time:124347ms step_avg:61.16ms
step:2034/2245 train_time:124409ms step_avg:61.16ms
step:2035/2245 train_time:124472ms step_avg:61.17ms
step:2036/2245 train_time:124533ms step_avg:61.17ms
step:2037/2245 train_time:124598ms step_avg:61.17ms
step:2038/2245 train_time:124659ms step_avg:61.17ms
step:2039/2245 train_time:124721ms step_avg:61.17ms
step:2040/2245 train_time:124781ms step_avg:61.17ms
step:2041/2245 train_time:124844ms step_avg:61.17ms
step:2042/2245 train_time:124904ms step_avg:61.17ms
step:2043/2245 train_time:124966ms step_avg:61.17ms
step:2044/2245 train_time:125026ms step_avg:61.17ms
step:2045/2245 train_time:125089ms step_avg:61.17ms
step:2046/2245 train_time:125149ms step_avg:61.17ms
step:2047/2245 train_time:125212ms step_avg:61.17ms
step:2048/2245 train_time:125272ms step_avg:61.17ms
step:2049/2245 train_time:125335ms step_avg:61.17ms
step:2050/2245 train_time:125397ms step_avg:61.17ms
step:2051/2245 train_time:125460ms step_avg:61.17ms
step:2052/2245 train_time:125521ms step_avg:61.17ms
step:2053/2245 train_time:125584ms step_avg:61.17ms
step:2054/2245 train_time:125645ms step_avg:61.17ms
step:2055/2245 train_time:125708ms step_avg:61.17ms
step:2056/2245 train_time:125769ms step_avg:61.17ms
step:2057/2245 train_time:125831ms step_avg:61.17ms
step:2058/2245 train_time:125892ms step_avg:61.17ms
step:2059/2245 train_time:125954ms step_avg:61.17ms
step:2060/2245 train_time:126015ms step_avg:61.17ms
step:2061/2245 train_time:126077ms step_avg:61.17ms
step:2062/2245 train_time:126137ms step_avg:61.17ms
step:2063/2245 train_time:126199ms step_avg:61.17ms
step:2064/2245 train_time:126260ms step_avg:61.17ms
step:2065/2245 train_time:126323ms step_avg:61.17ms
step:2066/2245 train_time:126384ms step_avg:61.17ms
step:2067/2245 train_time:126447ms step_avg:61.17ms
step:2068/2245 train_time:126508ms step_avg:61.17ms
step:2069/2245 train_time:126571ms step_avg:61.17ms
step:2070/2245 train_time:126631ms step_avg:61.17ms
step:2071/2245 train_time:126694ms step_avg:61.18ms
step:2072/2245 train_time:126755ms step_avg:61.18ms
step:2073/2245 train_time:126818ms step_avg:61.18ms
step:2074/2245 train_time:126879ms step_avg:61.18ms
step:2075/2245 train_time:126941ms step_avg:61.18ms
step:2076/2245 train_time:127001ms step_avg:61.18ms
step:2077/2245 train_time:127064ms step_avg:61.18ms
step:2078/2245 train_time:127124ms step_avg:61.18ms
step:2079/2245 train_time:127186ms step_avg:61.18ms
step:2080/2245 train_time:127246ms step_avg:61.18ms
step:2081/2245 train_time:127309ms step_avg:61.18ms
step:2082/2245 train_time:127370ms step_avg:61.18ms
step:2083/2245 train_time:127432ms step_avg:61.18ms
step:2084/2245 train_time:127493ms step_avg:61.18ms
step:2085/2245 train_time:127556ms step_avg:61.18ms
step:2086/2245 train_time:127617ms step_avg:61.18ms
step:2087/2245 train_time:127680ms step_avg:61.18ms
step:2088/2245 train_time:127741ms step_avg:61.18ms
step:2089/2245 train_time:127804ms step_avg:61.18ms
step:2090/2245 train_time:127864ms step_avg:61.18ms
step:2091/2245 train_time:127927ms step_avg:61.18ms
step:2092/2245 train_time:127988ms step_avg:61.18ms
step:2093/2245 train_time:128051ms step_avg:61.18ms
step:2094/2245 train_time:128111ms step_avg:61.18ms
step:2095/2245 train_time:128173ms step_avg:61.18ms
step:2096/2245 train_time:128234ms step_avg:61.18ms
step:2097/2245 train_time:128297ms step_avg:61.18ms
step:2098/2245 train_time:128358ms step_avg:61.18ms
step:2099/2245 train_time:128421ms step_avg:61.18ms
step:2100/2245 train_time:128482ms step_avg:61.18ms
step:2101/2245 train_time:128544ms step_avg:61.18ms
step:2102/2245 train_time:128605ms step_avg:61.18ms
step:2103/2245 train_time:128668ms step_avg:61.18ms
step:2104/2245 train_time:128729ms step_avg:61.18ms
step:2105/2245 train_time:128792ms step_avg:61.18ms
step:2106/2245 train_time:128854ms step_avg:61.18ms
step:2107/2245 train_time:128916ms step_avg:61.18ms
step:2108/2245 train_time:128977ms step_avg:61.18ms
step:2109/2245 train_time:129040ms step_avg:61.19ms
step:2110/2245 train_time:129101ms step_avg:61.19ms
step:2111/2245 train_time:129163ms step_avg:61.19ms
step:2112/2245 train_time:129224ms step_avg:61.19ms
step:2113/2245 train_time:129286ms step_avg:61.19ms
step:2114/2245 train_time:129348ms step_avg:61.19ms
step:2115/2245 train_time:129411ms step_avg:61.19ms
step:2116/2245 train_time:129471ms step_avg:61.19ms
step:2117/2245 train_time:129534ms step_avg:61.19ms
step:2118/2245 train_time:129595ms step_avg:61.19ms
step:2119/2245 train_time:129658ms step_avg:61.19ms
step:2120/2245 train_time:129719ms step_avg:61.19ms
step:2121/2245 train_time:129782ms step_avg:61.19ms
step:2122/2245 train_time:129842ms step_avg:61.19ms
step:2123/2245 train_time:129905ms step_avg:61.19ms
step:2124/2245 train_time:129966ms step_avg:61.19ms
step:2125/2245 train_time:130028ms step_avg:61.19ms
step:2126/2245 train_time:130089ms step_avg:61.19ms
step:2127/2245 train_time:130153ms step_avg:61.19ms
step:2128/2245 train_time:130214ms step_avg:61.19ms
step:2129/2245 train_time:130277ms step_avg:61.19ms
step:2130/2245 train_time:130338ms step_avg:61.19ms
step:2131/2245 train_time:130401ms step_avg:61.19ms
step:2132/2245 train_time:130461ms step_avg:61.19ms
step:2133/2245 train_time:130524ms step_avg:61.19ms
step:2134/2245 train_time:130584ms step_avg:61.19ms
step:2135/2245 train_time:130647ms step_avg:61.19ms
step:2136/2245 train_time:130707ms step_avg:61.19ms
step:2137/2245 train_time:130771ms step_avg:61.19ms
step:2138/2245 train_time:130831ms step_avg:61.19ms
step:2139/2245 train_time:130894ms step_avg:61.19ms
step:2140/2245 train_time:130956ms step_avg:61.19ms
step:2141/2245 train_time:131018ms step_avg:61.19ms
step:2142/2245 train_time:131079ms step_avg:61.19ms
step:2143/2245 train_time:131142ms step_avg:61.20ms
step:2144/2245 train_time:131202ms step_avg:61.19ms
step:2145/2245 train_time:131265ms step_avg:61.20ms
step:2146/2245 train_time:131326ms step_avg:61.20ms
step:2147/2245 train_time:131389ms step_avg:61.20ms
step:2148/2245 train_time:131450ms step_avg:61.20ms
step:2149/2245 train_time:131514ms step_avg:61.20ms
step:2150/2245 train_time:131574ms step_avg:61.20ms
step:2151/2245 train_time:131637ms step_avg:61.20ms
step:2152/2245 train_time:131698ms step_avg:61.20ms
step:2153/2245 train_time:131760ms step_avg:61.20ms
step:2154/2245 train_time:131821ms step_avg:61.20ms
step:2155/2245 train_time:131884ms step_avg:61.20ms
step:2156/2245 train_time:131943ms step_avg:61.20ms
step:2157/2245 train_time:132006ms step_avg:61.20ms
step:2158/2245 train_time:132067ms step_avg:61.20ms
step:2159/2245 train_time:132130ms step_avg:61.20ms
step:2160/2245 train_time:132191ms step_avg:61.20ms
step:2161/2245 train_time:132254ms step_avg:61.20ms
step:2162/2245 train_time:132315ms step_avg:61.20ms
step:2163/2245 train_time:132378ms step_avg:61.20ms
step:2164/2245 train_time:132438ms step_avg:61.20ms
step:2165/2245 train_time:132501ms step_avg:61.20ms
step:2166/2245 train_time:132562ms step_avg:61.20ms
step:2167/2245 train_time:132624ms step_avg:61.20ms
step:2168/2245 train_time:132685ms step_avg:61.20ms
step:2169/2245 train_time:132748ms step_avg:61.20ms
step:2170/2245 train_time:132809ms step_avg:61.20ms
step:2171/2245 train_time:132872ms step_avg:61.20ms
step:2172/2245 train_time:132933ms step_avg:61.20ms
step:2173/2245 train_time:132996ms step_avg:61.20ms
step:2174/2245 train_time:133057ms step_avg:61.20ms
step:2175/2245 train_time:133120ms step_avg:61.20ms
step:2176/2245 train_time:133179ms step_avg:61.20ms
step:2177/2245 train_time:133242ms step_avg:61.20ms
step:2178/2245 train_time:133303ms step_avg:61.20ms
step:2179/2245 train_time:133365ms step_avg:61.20ms
step:2180/2245 train_time:133426ms step_avg:61.20ms
step:2181/2245 train_time:133489ms step_avg:61.21ms
step:2182/2245 train_time:133549ms step_avg:61.21ms
step:2183/2245 train_time:133612ms step_avg:61.21ms
step:2184/2245 train_time:133674ms step_avg:61.21ms
step:2185/2245 train_time:133738ms step_avg:61.21ms
step:2186/2245 train_time:133799ms step_avg:61.21ms
step:2187/2245 train_time:133861ms step_avg:61.21ms
step:2188/2245 train_time:133921ms step_avg:61.21ms
step:2189/2245 train_time:133984ms step_avg:61.21ms
step:2190/2245 train_time:134045ms step_avg:61.21ms
step:2191/2245 train_time:134108ms step_avg:61.21ms
step:2192/2245 train_time:134169ms step_avg:61.21ms
step:2193/2245 train_time:134231ms step_avg:61.21ms
step:2194/2245 train_time:134292ms step_avg:61.21ms
step:2195/2245 train_time:134355ms step_avg:61.21ms
step:2196/2245 train_time:134416ms step_avg:61.21ms
step:2197/2245 train_time:134479ms step_avg:61.21ms
step:2198/2245 train_time:134540ms step_avg:61.21ms
step:2199/2245 train_time:134603ms step_avg:61.21ms
step:2200/2245 train_time:134663ms step_avg:61.21ms
step:2201/2245 train_time:134726ms step_avg:61.21ms
step:2202/2245 train_time:134786ms step_avg:61.21ms
step:2203/2245 train_time:134849ms step_avg:61.21ms
step:2204/2245 train_time:134910ms step_avg:61.21ms
step:2205/2245 train_time:134972ms step_avg:61.21ms
step:2206/2245 train_time:135033ms step_avg:61.21ms
step:2207/2245 train_time:135097ms step_avg:61.21ms
step:2208/2245 train_time:135158ms step_avg:61.21ms
step:2209/2245 train_time:135221ms step_avg:61.21ms
step:2210/2245 train_time:135281ms step_avg:61.21ms
step:2211/2245 train_time:135344ms step_avg:61.21ms
step:2212/2245 train_time:135404ms step_avg:61.21ms
step:2213/2245 train_time:135467ms step_avg:61.21ms
step:2214/2245 train_time:135528ms step_avg:61.21ms
step:2215/2245 train_time:135592ms step_avg:61.22ms
step:2216/2245 train_time:135653ms step_avg:61.22ms
step:2217/2245 train_time:135716ms step_avg:61.22ms
step:2218/2245 train_time:135777ms step_avg:61.22ms
step:2219/2245 train_time:135841ms step_avg:61.22ms
step:2220/2245 train_time:135902ms step_avg:61.22ms
step:2221/2245 train_time:135965ms step_avg:61.22ms
step:2222/2245 train_time:136025ms step_avg:61.22ms
step:2223/2245 train_time:136089ms step_avg:61.22ms
step:2224/2245 train_time:136149ms step_avg:61.22ms
step:2225/2245 train_time:136212ms step_avg:61.22ms
step:2226/2245 train_time:136273ms step_avg:61.22ms
step:2227/2245 train_time:136336ms step_avg:61.22ms
step:2228/2245 train_time:136398ms step_avg:61.22ms
step:2229/2245 train_time:136461ms step_avg:61.22ms
step:2230/2245 train_time:136521ms step_avg:61.22ms
step:2231/2245 train_time:136584ms step_avg:61.22ms
step:2232/2245 train_time:136645ms step_avg:61.22ms
step:2233/2245 train_time:136708ms step_avg:61.22ms
step:2234/2245 train_time:136769ms step_avg:61.22ms
step:2235/2245 train_time:136832ms step_avg:61.22ms
step:2236/2245 train_time:136894ms step_avg:61.22ms
step:2237/2245 train_time:136957ms step_avg:61.22ms
step:2238/2245 train_time:137018ms step_avg:61.22ms
step:2239/2245 train_time:137081ms step_avg:61.22ms
step:2240/2245 train_time:137142ms step_avg:61.22ms
step:2241/2245 train_time:137204ms step_avg:61.22ms
step:2242/2245 train_time:137265ms step_avg:61.22ms
step:2243/2245 train_time:137328ms step_avg:61.23ms
step:2244/2245 train_time:137390ms step_avg:61.23ms
step:2245/2245 train_time:137452ms step_avg:61.23ms
step:2245/2245 val_loss:3.2781 train_time:137514ms step_avg:61.25ms
peak memory allocated: 29244 MiB reserved: 44256 MiB

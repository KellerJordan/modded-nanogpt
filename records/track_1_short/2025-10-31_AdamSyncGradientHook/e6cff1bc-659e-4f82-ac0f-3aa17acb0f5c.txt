import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)


            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))



            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal
        self.group_to_param_group = {}

        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                self.group_to_param_group[param] = group

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        world_size = dist.get_world_size()
        grad = param.grad
        rank_size = grad.shape[0] // world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(), grad_slice)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']

            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
        # enable sync in the next training step for the adam optimizer
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = next(train_loader)

        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()


====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251031+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 31 16:16:15 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   37C    P0            127W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   31C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   35C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   35C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   34C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2315 train_time:82ms step_avg:81.67ms
step:2/2315 train_time:189ms step_avg:94.60ms
step:3/2315 train_time:208ms step_avg:69.26ms
step:4/2315 train_time:249ms step_avg:62.21ms
step:5/2315 train_time:307ms step_avg:61.31ms
step:6/2315 train_time:366ms step_avg:61.00ms
step:7/2315 train_time:425ms step_avg:60.68ms
step:8/2315 train_time:485ms step_avg:60.57ms
step:9/2315 train_time:543ms step_avg:60.37ms
step:10/2315 train_time:603ms step_avg:60.30ms
step:11/2315 train_time:662ms step_avg:60.19ms
step:12/2315 train_time:722ms step_avg:60.14ms
step:13/2315 train_time:781ms step_avg:60.05ms
step:14/2315 train_time:840ms step_avg:60.03ms
step:15/2315 train_time:899ms step_avg:59.95ms
step:16/2315 train_time:959ms step_avg:59.93ms
step:17/2315 train_time:1020ms step_avg:59.99ms
step:18/2315 train_time:1083ms step_avg:60.18ms
step:19/2315 train_time:1149ms step_avg:60.46ms
step:20/2315 train_time:1212ms step_avg:60.62ms
step:21/2315 train_time:1273ms step_avg:60.61ms
step:22/2315 train_time:1333ms step_avg:60.59ms
step:23/2315 train_time:1393ms step_avg:60.57ms
step:24/2315 train_time:1454ms step_avg:60.58ms
step:25/2315 train_time:1514ms step_avg:60.56ms
step:26/2315 train_time:1574ms step_avg:60.56ms
step:27/2315 train_time:1635ms step_avg:60.57ms
step:28/2315 train_time:1697ms step_avg:60.59ms
step:29/2315 train_time:1756ms step_avg:60.54ms
step:30/2315 train_time:1815ms step_avg:60.51ms
step:31/2315 train_time:1875ms step_avg:60.47ms
step:32/2315 train_time:1935ms step_avg:60.47ms
step:33/2315 train_time:1996ms step_avg:60.48ms
step:34/2315 train_time:2058ms step_avg:60.52ms
step:35/2315 train_time:2119ms step_avg:60.54ms
step:36/2315 train_time:2180ms step_avg:60.56ms
step:37/2315 train_time:2241ms step_avg:60.56ms
step:38/2315 train_time:2302ms step_avg:60.57ms
step:39/2315 train_time:2361ms step_avg:60.54ms
step:40/2315 train_time:2422ms step_avg:60.54ms
step:41/2315 train_time:2482ms step_avg:60.54ms
step:42/2315 train_time:2543ms step_avg:60.54ms
step:43/2315 train_time:2603ms step_avg:60.52ms
step:44/2315 train_time:2663ms step_avg:60.53ms
step:45/2315 train_time:2723ms step_avg:60.51ms
step:46/2315 train_time:2783ms step_avg:60.50ms
step:47/2315 train_time:2843ms step_avg:60.49ms
step:48/2315 train_time:2903ms step_avg:60.48ms
step:49/2315 train_time:2963ms step_avg:60.46ms
step:50/2315 train_time:3024ms step_avg:60.47ms
step:51/2315 train_time:3084ms step_avg:60.47ms
step:52/2315 train_time:3144ms step_avg:60.47ms
step:53/2315 train_time:3205ms step_avg:60.46ms
step:54/2315 train_time:3265ms step_avg:60.46ms
step:55/2315 train_time:3325ms step_avg:60.45ms
step:56/2315 train_time:3386ms step_avg:60.46ms
step:57/2315 train_time:3446ms step_avg:60.45ms
step:58/2315 train_time:3506ms step_avg:60.44ms
step:59/2315 train_time:3566ms step_avg:60.43ms
step:60/2315 train_time:3627ms step_avg:60.44ms
step:61/2315 train_time:3686ms step_avg:60.43ms
step:62/2315 train_time:3747ms step_avg:60.43ms
step:63/2315 train_time:3806ms step_avg:60.41ms
step:64/2315 train_time:3866ms step_avg:60.41ms
step:65/2315 train_time:3926ms step_avg:60.40ms
step:66/2315 train_time:3986ms step_avg:60.40ms
step:67/2315 train_time:4046ms step_avg:60.40ms
step:68/2315 train_time:4107ms step_avg:60.40ms
step:69/2315 train_time:4167ms step_avg:60.39ms
step:70/2315 train_time:4227ms step_avg:60.39ms
step:71/2315 train_time:4288ms step_avg:60.39ms
step:72/2315 train_time:4348ms step_avg:60.39ms
step:73/2315 train_time:4408ms step_avg:60.38ms
step:74/2315 train_time:4468ms step_avg:60.38ms
step:75/2315 train_time:4529ms step_avg:60.38ms
step:76/2315 train_time:4590ms step_avg:60.39ms
step:77/2315 train_time:4650ms step_avg:60.39ms
step:78/2315 train_time:4711ms step_avg:60.40ms
step:79/2315 train_time:4771ms step_avg:60.39ms
step:80/2315 train_time:4832ms step_avg:60.39ms
step:81/2315 train_time:4891ms step_avg:60.39ms
step:82/2315 train_time:4952ms step_avg:60.39ms
step:83/2315 train_time:5012ms step_avg:60.39ms
step:84/2315 train_time:5073ms step_avg:60.39ms
step:85/2315 train_time:5133ms step_avg:60.39ms
step:86/2315 train_time:5194ms step_avg:60.40ms
step:87/2315 train_time:5255ms step_avg:60.40ms
step:88/2315 train_time:5316ms step_avg:60.40ms
step:89/2315 train_time:5375ms step_avg:60.40ms
step:90/2315 train_time:5436ms step_avg:60.40ms
step:91/2315 train_time:5496ms step_avg:60.39ms
step:92/2315 train_time:5556ms step_avg:60.39ms
step:93/2315 train_time:5616ms step_avg:60.38ms
step:94/2315 train_time:5676ms step_avg:60.38ms
step:95/2315 train_time:5736ms step_avg:60.38ms
step:96/2315 train_time:5797ms step_avg:60.38ms
step:97/2315 train_time:5857ms step_avg:60.38ms
step:98/2315 train_time:5917ms step_avg:60.38ms
step:99/2315 train_time:5977ms step_avg:60.37ms
step:100/2315 train_time:6037ms step_avg:60.37ms
step:101/2315 train_time:6097ms step_avg:60.36ms
step:102/2315 train_time:6158ms step_avg:60.37ms
step:103/2315 train_time:6217ms step_avg:60.36ms
step:104/2315 train_time:6277ms step_avg:60.35ms
step:105/2315 train_time:6336ms step_avg:60.35ms
step:106/2315 train_time:6397ms step_avg:60.35ms
step:107/2315 train_time:6457ms step_avg:60.35ms
step:108/2315 train_time:6517ms step_avg:60.35ms
step:109/2315 train_time:6577ms step_avg:60.34ms
step:110/2315 train_time:6637ms step_avg:60.34ms
step:111/2315 train_time:6696ms step_avg:60.33ms
step:112/2315 train_time:6757ms step_avg:60.33ms
step:113/2315 train_time:6816ms step_avg:60.32ms
step:114/2315 train_time:6877ms step_avg:60.33ms
step:115/2315 train_time:6936ms step_avg:60.31ms
step:116/2315 train_time:6996ms step_avg:60.31ms
step:117/2315 train_time:7056ms step_avg:60.31ms
step:118/2315 train_time:7117ms step_avg:60.31ms
step:119/2315 train_time:7176ms step_avg:60.30ms
step:120/2315 train_time:7236ms step_avg:60.30ms
step:121/2315 train_time:7296ms step_avg:60.30ms
step:122/2315 train_time:7357ms step_avg:60.30ms
step:123/2315 train_time:7416ms step_avg:60.30ms
step:124/2315 train_time:7477ms step_avg:60.30ms
step:125/2315 train_time:7537ms step_avg:60.30ms
step:126/2315 train_time:7597ms step_avg:60.30ms
step:127/2315 train_time:7657ms step_avg:60.29ms
step:128/2315 train_time:7718ms step_avg:60.30ms
step:129/2315 train_time:7777ms step_avg:60.29ms
step:130/2315 train_time:7837ms step_avg:60.29ms
step:131/2315 train_time:7897ms step_avg:60.28ms
step:132/2315 train_time:7957ms step_avg:60.28ms
step:133/2315 train_time:8017ms step_avg:60.28ms
step:134/2315 train_time:8077ms step_avg:60.28ms
step:135/2315 train_time:8137ms step_avg:60.27ms
step:136/2315 train_time:8198ms step_avg:60.28ms
step:137/2315 train_time:8257ms step_avg:60.27ms
step:138/2315 train_time:8317ms step_avg:60.27ms
step:139/2315 train_time:8376ms step_avg:60.26ms
step:140/2315 train_time:8436ms step_avg:60.26ms
step:141/2315 train_time:8496ms step_avg:60.25ms
step:142/2315 train_time:8556ms step_avg:60.25ms
step:143/2315 train_time:8616ms step_avg:60.25ms
step:144/2315 train_time:8676ms step_avg:60.25ms
step:145/2315 train_time:8736ms step_avg:60.25ms
step:146/2315 train_time:8797ms step_avg:60.25ms
step:147/2315 train_time:8857ms step_avg:60.25ms
step:148/2315 train_time:8917ms step_avg:60.25ms
step:149/2315 train_time:8976ms step_avg:60.24ms
step:150/2315 train_time:9037ms step_avg:60.25ms
step:151/2315 train_time:9097ms step_avg:60.24ms
step:152/2315 train_time:9157ms step_avg:60.24ms
step:153/2315 train_time:9216ms step_avg:60.24ms
step:154/2315 train_time:9276ms step_avg:60.23ms
step:155/2315 train_time:9336ms step_avg:60.23ms
step:156/2315 train_time:9397ms step_avg:60.23ms
step:157/2315 train_time:9456ms step_avg:60.23ms
step:158/2315 train_time:9516ms step_avg:60.23ms
step:159/2315 train_time:9576ms step_avg:60.23ms
step:160/2315 train_time:9636ms step_avg:60.23ms
step:161/2315 train_time:9696ms step_avg:60.22ms
step:162/2315 train_time:9756ms step_avg:60.22ms
step:163/2315 train_time:9816ms step_avg:60.22ms
step:164/2315 train_time:9876ms step_avg:60.22ms
step:165/2315 train_time:9936ms step_avg:60.22ms
step:166/2315 train_time:9997ms step_avg:60.22ms
step:167/2315 train_time:10056ms step_avg:60.22ms
step:168/2315 train_time:10117ms step_avg:60.22ms
step:169/2315 train_time:10176ms step_avg:60.21ms
step:170/2315 train_time:10236ms step_avg:60.21ms
step:171/2315 train_time:10296ms step_avg:60.21ms
step:172/2315 train_time:10356ms step_avg:60.21ms
step:173/2315 train_time:10416ms step_avg:60.21ms
step:174/2315 train_time:10476ms step_avg:60.21ms
step:175/2315 train_time:10536ms step_avg:60.20ms
step:176/2315 train_time:10596ms step_avg:60.20ms
step:177/2315 train_time:10656ms step_avg:60.20ms
step:178/2315 train_time:10716ms step_avg:60.20ms
step:179/2315 train_time:10776ms step_avg:60.20ms
step:180/2315 train_time:10836ms step_avg:60.20ms
step:181/2315 train_time:10896ms step_avg:60.20ms
step:182/2315 train_time:10956ms step_avg:60.20ms
step:183/2315 train_time:11016ms step_avg:60.20ms
step:184/2315 train_time:11076ms step_avg:60.19ms
step:185/2315 train_time:11135ms step_avg:60.19ms
step:186/2315 train_time:11196ms step_avg:60.19ms
step:187/2315 train_time:11256ms step_avg:60.19ms
step:188/2315 train_time:11318ms step_avg:60.20ms
step:189/2315 train_time:11377ms step_avg:60.20ms
step:190/2315 train_time:11437ms step_avg:60.20ms
step:191/2315 train_time:11497ms step_avg:60.19ms
step:192/2315 train_time:11558ms step_avg:60.20ms
step:193/2315 train_time:11617ms step_avg:60.19ms
step:194/2315 train_time:11677ms step_avg:60.19ms
step:195/2315 train_time:11736ms step_avg:60.19ms
step:196/2315 train_time:11797ms step_avg:60.19ms
step:197/2315 train_time:11856ms step_avg:60.18ms
step:198/2315 train_time:11917ms step_avg:60.18ms
step:199/2315 train_time:11976ms step_avg:60.18ms
step:200/2315 train_time:12036ms step_avg:60.18ms
step:201/2315 train_time:12096ms step_avg:60.18ms
step:202/2315 train_time:12156ms step_avg:60.18ms
step:203/2315 train_time:12216ms step_avg:60.18ms
step:204/2315 train_time:12276ms step_avg:60.18ms
step:205/2315 train_time:12336ms step_avg:60.17ms
step:206/2315 train_time:12396ms step_avg:60.18ms
step:207/2315 train_time:12456ms step_avg:60.17ms
step:208/2315 train_time:12516ms step_avg:60.17ms
step:209/2315 train_time:12576ms step_avg:60.17ms
step:210/2315 train_time:12636ms step_avg:60.17ms
step:211/2315 train_time:12696ms step_avg:60.17ms
step:212/2315 train_time:12756ms step_avg:60.17ms
step:213/2315 train_time:12815ms step_avg:60.17ms
step:214/2315 train_time:12876ms step_avg:60.17ms
step:215/2315 train_time:12936ms step_avg:60.17ms
step:216/2315 train_time:12996ms step_avg:60.17ms
step:217/2315 train_time:13056ms step_avg:60.17ms
step:218/2315 train_time:13117ms step_avg:60.17ms
step:219/2315 train_time:13176ms step_avg:60.16ms
step:220/2315 train_time:13236ms step_avg:60.17ms
step:221/2315 train_time:13297ms step_avg:60.17ms
step:222/2315 train_time:13357ms step_avg:60.17ms
step:223/2315 train_time:13417ms step_avg:60.17ms
step:224/2315 train_time:13477ms step_avg:60.16ms
step:225/2315 train_time:13536ms step_avg:60.16ms
step:226/2315 train_time:13597ms step_avg:60.16ms
step:227/2315 train_time:13656ms step_avg:60.16ms
step:228/2315 train_time:13716ms step_avg:60.16ms
step:229/2315 train_time:13776ms step_avg:60.16ms
step:230/2315 train_time:13836ms step_avg:60.16ms
step:231/2315 train_time:13896ms step_avg:60.15ms
step:232/2315 train_time:13957ms step_avg:60.16ms
step:233/2315 train_time:14016ms step_avg:60.15ms
step:234/2315 train_time:14076ms step_avg:60.15ms
step:235/2315 train_time:14136ms step_avg:60.15ms
step:236/2315 train_time:14197ms step_avg:60.15ms
step:237/2315 train_time:14256ms step_avg:60.15ms
step:238/2315 train_time:14318ms step_avg:60.16ms
step:239/2315 train_time:14376ms step_avg:60.15ms
step:240/2315 train_time:14436ms step_avg:60.15ms
step:241/2315 train_time:14497ms step_avg:60.15ms
step:242/2315 train_time:14557ms step_avg:60.15ms
step:243/2315 train_time:14617ms step_avg:60.15ms
step:244/2315 train_time:14677ms step_avg:60.15ms
step:245/2315 train_time:14737ms step_avg:60.15ms
step:246/2315 train_time:14797ms step_avg:60.15ms
step:247/2315 train_time:14856ms step_avg:60.15ms
step:248/2315 train_time:14917ms step_avg:60.15ms
step:249/2315 train_time:14976ms step_avg:60.15ms
step:250/2315 train_time:15036ms step_avg:60.15ms
step:250/2315 val_loss:4.0779 train_time:15098ms step_avg:60.39ms
step:251/2315 train_time:15121ms step_avg:60.24ms
step:252/2315 train_time:15162ms step_avg:60.17ms
step:253/2315 train_time:15227ms step_avg:60.18ms
step:254/2315 train_time:15291ms step_avg:60.20ms
step:255/2315 train_time:15352ms step_avg:60.20ms
step:256/2315 train_time:15412ms step_avg:60.20ms
step:257/2315 train_time:15472ms step_avg:60.20ms
step:258/2315 train_time:15532ms step_avg:60.20ms
step:259/2315 train_time:15591ms step_avg:60.20ms
step:260/2315 train_time:15650ms step_avg:60.19ms
step:261/2315 train_time:15709ms step_avg:60.19ms
step:262/2315 train_time:15768ms step_avg:60.18ms
step:263/2315 train_time:15827ms step_avg:60.18ms
step:264/2315 train_time:15886ms step_avg:60.17ms
step:265/2315 train_time:15944ms step_avg:60.17ms
step:266/2315 train_time:16003ms step_avg:60.16ms
step:267/2315 train_time:16063ms step_avg:60.16ms
step:268/2315 train_time:16124ms step_avg:60.16ms
step:269/2315 train_time:16185ms step_avg:60.17ms
step:270/2315 train_time:16247ms step_avg:60.17ms
step:271/2315 train_time:16308ms step_avg:60.18ms
step:272/2315 train_time:16369ms step_avg:60.18ms
step:273/2315 train_time:16429ms step_avg:60.18ms
step:274/2315 train_time:16490ms step_avg:60.18ms
step:275/2315 train_time:16550ms step_avg:60.18ms
step:276/2315 train_time:16610ms step_avg:60.18ms
step:277/2315 train_time:16669ms step_avg:60.18ms
step:278/2315 train_time:16729ms step_avg:60.17ms
step:279/2315 train_time:16787ms step_avg:60.17ms
step:280/2315 train_time:16846ms step_avg:60.17ms
step:281/2315 train_time:16905ms step_avg:60.16ms
step:282/2315 train_time:16965ms step_avg:60.16ms
step:283/2315 train_time:17024ms step_avg:60.16ms
step:284/2315 train_time:17085ms step_avg:60.16ms
step:285/2315 train_time:17145ms step_avg:60.16ms
step:286/2315 train_time:17207ms step_avg:60.16ms
step:287/2315 train_time:17268ms step_avg:60.17ms
step:288/2315 train_time:17330ms step_avg:60.17ms
step:289/2315 train_time:17391ms step_avg:60.18ms
step:290/2315 train_time:17452ms step_avg:60.18ms
step:291/2315 train_time:17511ms step_avg:60.18ms
step:292/2315 train_time:17571ms step_avg:60.18ms
step:293/2315 train_time:17631ms step_avg:60.17ms
step:294/2315 train_time:17691ms step_avg:60.17ms
step:295/2315 train_time:17750ms step_avg:60.17ms
step:296/2315 train_time:17809ms step_avg:60.17ms
step:297/2315 train_time:17868ms step_avg:60.16ms
step:298/2315 train_time:17929ms step_avg:60.16ms
step:299/2315 train_time:17988ms step_avg:60.16ms
step:300/2315 train_time:18049ms step_avg:60.16ms
step:301/2315 train_time:18109ms step_avg:60.16ms
step:302/2315 train_time:18171ms step_avg:60.17ms
step:303/2315 train_time:18232ms step_avg:60.17ms
step:304/2315 train_time:18293ms step_avg:60.17ms
step:305/2315 train_time:18354ms step_avg:60.18ms
step:306/2315 train_time:18415ms step_avg:60.18ms
step:307/2315 train_time:18475ms step_avg:60.18ms
step:308/2315 train_time:18536ms step_avg:60.18ms
step:309/2315 train_time:18595ms step_avg:60.18ms
step:310/2315 train_time:18655ms step_avg:60.18ms
step:311/2315 train_time:18714ms step_avg:60.17ms
step:312/2315 train_time:18774ms step_avg:60.17ms
step:313/2315 train_time:18832ms step_avg:60.17ms
step:314/2315 train_time:18893ms step_avg:60.17ms
step:315/2315 train_time:18953ms step_avg:60.17ms
step:316/2315 train_time:19013ms step_avg:60.17ms
step:317/2315 train_time:19075ms step_avg:60.17ms
step:318/2315 train_time:19135ms step_avg:60.17ms
step:319/2315 train_time:19195ms step_avg:60.17ms
step:320/2315 train_time:19256ms step_avg:60.17ms
step:321/2315 train_time:19315ms step_avg:60.17ms
step:322/2315 train_time:19375ms step_avg:60.17ms
step:323/2315 train_time:19436ms step_avg:60.17ms
step:324/2315 train_time:19495ms step_avg:60.17ms
step:325/2315 train_time:19555ms step_avg:60.17ms
step:326/2315 train_time:19615ms step_avg:60.17ms
step:327/2315 train_time:19674ms step_avg:60.16ms
step:328/2315 train_time:19735ms step_avg:60.17ms
step:329/2315 train_time:19794ms step_avg:60.16ms
step:330/2315 train_time:19854ms step_avg:60.16ms
step:331/2315 train_time:19913ms step_avg:60.16ms
step:332/2315 train_time:19974ms step_avg:60.16ms
step:333/2315 train_time:20034ms step_avg:60.16ms
step:334/2315 train_time:20095ms step_avg:60.17ms
step:335/2315 train_time:20156ms step_avg:60.17ms
step:336/2315 train_time:20216ms step_avg:60.17ms
step:337/2315 train_time:20276ms step_avg:60.17ms
step:338/2315 train_time:20337ms step_avg:60.17ms
step:339/2315 train_time:20397ms step_avg:60.17ms
step:340/2315 train_time:20457ms step_avg:60.17ms
step:341/2315 train_time:20516ms step_avg:60.16ms
step:342/2315 train_time:20576ms step_avg:60.16ms
step:343/2315 train_time:20636ms step_avg:60.16ms
step:344/2315 train_time:20696ms step_avg:60.16ms
step:345/2315 train_time:20755ms step_avg:60.16ms
step:346/2315 train_time:20815ms step_avg:60.16ms
step:347/2315 train_time:20874ms step_avg:60.16ms
step:348/2315 train_time:20935ms step_avg:60.16ms
step:349/2315 train_time:20995ms step_avg:60.16ms
step:350/2315 train_time:21055ms step_avg:60.16ms
step:351/2315 train_time:21115ms step_avg:60.16ms
step:352/2315 train_time:21175ms step_avg:60.16ms
step:353/2315 train_time:21236ms step_avg:60.16ms
step:354/2315 train_time:21296ms step_avg:60.16ms
step:355/2315 train_time:21356ms step_avg:60.16ms
step:356/2315 train_time:21416ms step_avg:60.16ms
step:357/2315 train_time:21475ms step_avg:60.15ms
step:358/2315 train_time:21535ms step_avg:60.15ms
step:359/2315 train_time:21595ms step_avg:60.15ms
step:360/2315 train_time:21655ms step_avg:60.15ms
step:361/2315 train_time:21714ms step_avg:60.15ms
step:362/2315 train_time:21774ms step_avg:60.15ms
step:363/2315 train_time:21833ms step_avg:60.15ms
step:364/2315 train_time:21894ms step_avg:60.15ms
step:365/2315 train_time:21954ms step_avg:60.15ms
step:366/2315 train_time:22014ms step_avg:60.15ms
step:367/2315 train_time:22074ms step_avg:60.15ms
step:368/2315 train_time:22135ms step_avg:60.15ms
step:369/2315 train_time:22195ms step_avg:60.15ms
step:370/2315 train_time:22256ms step_avg:60.15ms
step:371/2315 train_time:22316ms step_avg:60.15ms
step:372/2315 train_time:22376ms step_avg:60.15ms
step:373/2315 train_time:22436ms step_avg:60.15ms
step:374/2315 train_time:22496ms step_avg:60.15ms
step:375/2315 train_time:22556ms step_avg:60.15ms
step:376/2315 train_time:22615ms step_avg:60.15ms
step:377/2315 train_time:22675ms step_avg:60.15ms
step:378/2315 train_time:22736ms step_avg:60.15ms
step:379/2315 train_time:22795ms step_avg:60.15ms
step:380/2315 train_time:22855ms step_avg:60.15ms
step:381/2315 train_time:22915ms step_avg:60.14ms
step:382/2315 train_time:22975ms step_avg:60.14ms
step:383/2315 train_time:23035ms step_avg:60.14ms
step:384/2315 train_time:23095ms step_avg:60.14ms
step:385/2315 train_time:23155ms step_avg:60.14ms
step:386/2315 train_time:23216ms step_avg:60.14ms
step:387/2315 train_time:23276ms step_avg:60.14ms
step:388/2315 train_time:23337ms step_avg:60.15ms
step:389/2315 train_time:23397ms step_avg:60.15ms
step:390/2315 train_time:23458ms step_avg:60.15ms
step:391/2315 train_time:23517ms step_avg:60.15ms
step:392/2315 train_time:23577ms step_avg:60.14ms
step:393/2315 train_time:23637ms step_avg:60.14ms
step:394/2315 train_time:23696ms step_avg:60.14ms
step:395/2315 train_time:23756ms step_avg:60.14ms
step:396/2315 train_time:23816ms step_avg:60.14ms
step:397/2315 train_time:23875ms step_avg:60.14ms
step:398/2315 train_time:23936ms step_avg:60.14ms
step:399/2315 train_time:23995ms step_avg:60.14ms
step:400/2315 train_time:24055ms step_avg:60.14ms
step:401/2315 train_time:24115ms step_avg:60.14ms
step:402/2315 train_time:24175ms step_avg:60.14ms
step:403/2315 train_time:24236ms step_avg:60.14ms
step:404/2315 train_time:24296ms step_avg:60.14ms
step:405/2315 train_time:24356ms step_avg:60.14ms
step:406/2315 train_time:24416ms step_avg:60.14ms
step:407/2315 train_time:24476ms step_avg:60.14ms
step:408/2315 train_time:24536ms step_avg:60.14ms
step:409/2315 train_time:24595ms step_avg:60.14ms
step:410/2315 train_time:24656ms step_avg:60.14ms
step:411/2315 train_time:24714ms step_avg:60.13ms
step:412/2315 train_time:24774ms step_avg:60.13ms
step:413/2315 train_time:24834ms step_avg:60.13ms
step:414/2315 train_time:24894ms step_avg:60.13ms
step:415/2315 train_time:24954ms step_avg:60.13ms
step:416/2315 train_time:25015ms step_avg:60.13ms
step:417/2315 train_time:25074ms step_avg:60.13ms
step:418/2315 train_time:25135ms step_avg:60.13ms
step:419/2315 train_time:25195ms step_avg:60.13ms
step:420/2315 train_time:25256ms step_avg:60.13ms
step:421/2315 train_time:25317ms step_avg:60.14ms
step:422/2315 train_time:25375ms step_avg:60.13ms
step:423/2315 train_time:25435ms step_avg:60.13ms
step:424/2315 train_time:25495ms step_avg:60.13ms
step:425/2315 train_time:25555ms step_avg:60.13ms
step:426/2315 train_time:25615ms step_avg:60.13ms
step:427/2315 train_time:25674ms step_avg:60.13ms
step:428/2315 train_time:25735ms step_avg:60.13ms
step:429/2315 train_time:25794ms step_avg:60.13ms
step:430/2315 train_time:25854ms step_avg:60.13ms
step:431/2315 train_time:25914ms step_avg:60.13ms
step:432/2315 train_time:25975ms step_avg:60.13ms
step:433/2315 train_time:26035ms step_avg:60.13ms
step:434/2315 train_time:26096ms step_avg:60.13ms
step:435/2315 train_time:26156ms step_avg:60.13ms
step:436/2315 train_time:26216ms step_avg:60.13ms
step:437/2315 train_time:26276ms step_avg:60.13ms
step:438/2315 train_time:26336ms step_avg:60.13ms
step:439/2315 train_time:26396ms step_avg:60.13ms
step:440/2315 train_time:26457ms step_avg:60.13ms
step:441/2315 train_time:26517ms step_avg:60.13ms
step:442/2315 train_time:26576ms step_avg:60.13ms
step:443/2315 train_time:26636ms step_avg:60.13ms
step:444/2315 train_time:26696ms step_avg:60.13ms
step:445/2315 train_time:26755ms step_avg:60.12ms
step:446/2315 train_time:26815ms step_avg:60.12ms
step:447/2315 train_time:26874ms step_avg:60.12ms
step:448/2315 train_time:26935ms step_avg:60.12ms
step:449/2315 train_time:26994ms step_avg:60.12ms
step:450/2315 train_time:27055ms step_avg:60.12ms
step:451/2315 train_time:27114ms step_avg:60.12ms
step:452/2315 train_time:27176ms step_avg:60.12ms
step:453/2315 train_time:27236ms step_avg:60.12ms
step:454/2315 train_time:27295ms step_avg:60.12ms
step:455/2315 train_time:27355ms step_avg:60.12ms
step:456/2315 train_time:27415ms step_avg:60.12ms
step:457/2315 train_time:27475ms step_avg:60.12ms
step:458/2315 train_time:27536ms step_avg:60.12ms
step:459/2315 train_time:27596ms step_avg:60.12ms
step:460/2315 train_time:27656ms step_avg:60.12ms
step:461/2315 train_time:27716ms step_avg:60.12ms
step:462/2315 train_time:27776ms step_avg:60.12ms
step:463/2315 train_time:27836ms step_avg:60.12ms
step:464/2315 train_time:27895ms step_avg:60.12ms
step:465/2315 train_time:27955ms step_avg:60.12ms
step:466/2315 train_time:28016ms step_avg:60.12ms
step:467/2315 train_time:28076ms step_avg:60.12ms
step:468/2315 train_time:28137ms step_avg:60.12ms
step:469/2315 train_time:28196ms step_avg:60.12ms
step:470/2315 train_time:28257ms step_avg:60.12ms
step:471/2315 train_time:28316ms step_avg:60.12ms
step:472/2315 train_time:28376ms step_avg:60.12ms
step:473/2315 train_time:28436ms step_avg:60.12ms
step:474/2315 train_time:28496ms step_avg:60.12ms
step:475/2315 train_time:28556ms step_avg:60.12ms
step:476/2315 train_time:28616ms step_avg:60.12ms
step:477/2315 train_time:28675ms step_avg:60.12ms
step:478/2315 train_time:28736ms step_avg:60.12ms
step:479/2315 train_time:28795ms step_avg:60.11ms
step:480/2315 train_time:28855ms step_avg:60.11ms
step:481/2315 train_time:28914ms step_avg:60.11ms
step:482/2315 train_time:28975ms step_avg:60.11ms
step:483/2315 train_time:29035ms step_avg:60.11ms
step:484/2315 train_time:29095ms step_avg:60.11ms
step:485/2315 train_time:29155ms step_avg:60.11ms
step:486/2315 train_time:29215ms step_avg:60.11ms
step:487/2315 train_time:29275ms step_avg:60.11ms
step:488/2315 train_time:29336ms step_avg:60.11ms
step:489/2315 train_time:29395ms step_avg:60.11ms
step:490/2315 train_time:29456ms step_avg:60.11ms
step:491/2315 train_time:29515ms step_avg:60.11ms
step:492/2315 train_time:29575ms step_avg:60.11ms
step:493/2315 train_time:29635ms step_avg:60.11ms
step:494/2315 train_time:29695ms step_avg:60.11ms
step:495/2315 train_time:29755ms step_avg:60.11ms
step:496/2315 train_time:29816ms step_avg:60.11ms
step:497/2315 train_time:29875ms step_avg:60.11ms
step:498/2315 train_time:29936ms step_avg:60.11ms
step:499/2315 train_time:29995ms step_avg:60.11ms
step:500/2315 train_time:30056ms step_avg:60.11ms
step:500/2315 val_loss:3.8120 train_time:30117ms step_avg:60.23ms
step:501/2315 train_time:30141ms step_avg:60.16ms
step:502/2315 train_time:30177ms step_avg:60.11ms
step:503/2315 train_time:30240ms step_avg:60.12ms
step:504/2315 train_time:30304ms step_avg:60.13ms
step:505/2315 train_time:30364ms step_avg:60.13ms
step:506/2315 train_time:30425ms step_avg:60.13ms
step:507/2315 train_time:30484ms step_avg:60.13ms
step:508/2315 train_time:30543ms step_avg:60.12ms
step:509/2315 train_time:30602ms step_avg:60.12ms
step:510/2315 train_time:30662ms step_avg:60.12ms
step:511/2315 train_time:30721ms step_avg:60.12ms
step:512/2315 train_time:30781ms step_avg:60.12ms
step:513/2315 train_time:30840ms step_avg:60.12ms
step:514/2315 train_time:30900ms step_avg:60.12ms
step:515/2315 train_time:30959ms step_avg:60.11ms
step:516/2315 train_time:31018ms step_avg:60.11ms
step:517/2315 train_time:31079ms step_avg:60.11ms
step:518/2315 train_time:31141ms step_avg:60.12ms
step:519/2315 train_time:31202ms step_avg:60.12ms
step:520/2315 train_time:31264ms step_avg:60.12ms
step:521/2315 train_time:31325ms step_avg:60.12ms
step:522/2315 train_time:31386ms step_avg:60.13ms
step:523/2315 train_time:31445ms step_avg:60.12ms
step:524/2315 train_time:31505ms step_avg:60.12ms
step:525/2315 train_time:31564ms step_avg:60.12ms
step:526/2315 train_time:31624ms step_avg:60.12ms
step:527/2315 train_time:31683ms step_avg:60.12ms
step:528/2315 train_time:31743ms step_avg:60.12ms
step:529/2315 train_time:31802ms step_avg:60.12ms
step:530/2315 train_time:31862ms step_avg:60.12ms
step:531/2315 train_time:31921ms step_avg:60.12ms
step:532/2315 train_time:31982ms step_avg:60.12ms
step:533/2315 train_time:32041ms step_avg:60.12ms
step:534/2315 train_time:32103ms step_avg:60.12ms
step:535/2315 train_time:32164ms step_avg:60.12ms
step:536/2315 train_time:32225ms step_avg:60.12ms
step:537/2315 train_time:32286ms step_avg:60.12ms
step:538/2315 train_time:32346ms step_avg:60.12ms
step:539/2315 train_time:32405ms step_avg:60.12ms
step:540/2315 train_time:32465ms step_avg:60.12ms
step:541/2315 train_time:32525ms step_avg:60.12ms
step:542/2315 train_time:32585ms step_avg:60.12ms
step:543/2315 train_time:32644ms step_avg:60.12ms
step:544/2315 train_time:32704ms step_avg:60.12ms
step:545/2315 train_time:32763ms step_avg:60.12ms
step:546/2315 train_time:32823ms step_avg:60.11ms
step:547/2315 train_time:32882ms step_avg:60.11ms
step:548/2315 train_time:32943ms step_avg:60.11ms
step:549/2315 train_time:33002ms step_avg:60.11ms
step:550/2315 train_time:33063ms step_avg:60.11ms
step:551/2315 train_time:33123ms step_avg:60.11ms
step:552/2315 train_time:33185ms step_avg:60.12ms
step:553/2315 train_time:33245ms step_avg:60.12ms
step:554/2315 train_time:33305ms step_avg:60.12ms
step:555/2315 train_time:33366ms step_avg:60.12ms
step:556/2315 train_time:33426ms step_avg:60.12ms
step:557/2315 train_time:33485ms step_avg:60.12ms
step:558/2315 train_time:33545ms step_avg:60.12ms
step:559/2315 train_time:33604ms step_avg:60.12ms
step:560/2315 train_time:33664ms step_avg:60.11ms
step:561/2315 train_time:33724ms step_avg:60.11ms
step:562/2315 train_time:33784ms step_avg:60.11ms
step:563/2315 train_time:33844ms step_avg:60.11ms
step:564/2315 train_time:33904ms step_avg:60.11ms
step:565/2315 train_time:33963ms step_avg:60.11ms
step:566/2315 train_time:34024ms step_avg:60.11ms
step:567/2315 train_time:34084ms step_avg:60.11ms
step:568/2315 train_time:34144ms step_avg:60.11ms
step:569/2315 train_time:34204ms step_avg:60.11ms
step:570/2315 train_time:34266ms step_avg:60.12ms
step:571/2315 train_time:34325ms step_avg:60.11ms
step:572/2315 train_time:34386ms step_avg:60.12ms
step:573/2315 train_time:34446ms step_avg:60.11ms
step:574/2315 train_time:34507ms step_avg:60.12ms
step:575/2315 train_time:34566ms step_avg:60.11ms
step:576/2315 train_time:34625ms step_avg:60.11ms
step:577/2315 train_time:34685ms step_avg:60.11ms
step:578/2315 train_time:34745ms step_avg:60.11ms
step:579/2315 train_time:34804ms step_avg:60.11ms
step:580/2315 train_time:34864ms step_avg:60.11ms
step:581/2315 train_time:34923ms step_avg:60.11ms
step:582/2315 train_time:34983ms step_avg:60.11ms
step:583/2315 train_time:35044ms step_avg:60.11ms
step:584/2315 train_time:35104ms step_avg:60.11ms
step:585/2315 train_time:35164ms step_avg:60.11ms
step:586/2315 train_time:35226ms step_avg:60.11ms
step:587/2315 train_time:35287ms step_avg:60.11ms
step:588/2315 train_time:35347ms step_avg:60.11ms
step:589/2315 train_time:35406ms step_avg:60.11ms
step:590/2315 train_time:35466ms step_avg:60.11ms
step:591/2315 train_time:35525ms step_avg:60.11ms
step:592/2315 train_time:35585ms step_avg:60.11ms
step:593/2315 train_time:35645ms step_avg:60.11ms
step:594/2315 train_time:35706ms step_avg:60.11ms
step:595/2315 train_time:35765ms step_avg:60.11ms
step:596/2315 train_time:35825ms step_avg:60.11ms
step:597/2315 train_time:35884ms step_avg:60.11ms
step:598/2315 train_time:35945ms step_avg:60.11ms
step:599/2315 train_time:36004ms step_avg:60.11ms
step:600/2315 train_time:36064ms step_avg:60.11ms
step:601/2315 train_time:36124ms step_avg:60.11ms
step:602/2315 train_time:36185ms step_avg:60.11ms
step:603/2315 train_time:36246ms step_avg:60.11ms
step:604/2315 train_time:36306ms step_avg:60.11ms
step:605/2315 train_time:36366ms step_avg:60.11ms
step:606/2315 train_time:36426ms step_avg:60.11ms
step:607/2315 train_time:36487ms step_avg:60.11ms
step:608/2315 train_time:36547ms step_avg:60.11ms
step:609/2315 train_time:36606ms step_avg:60.11ms
step:610/2315 train_time:36666ms step_avg:60.11ms
step:611/2315 train_time:36726ms step_avg:60.11ms
step:612/2315 train_time:36786ms step_avg:60.11ms
step:613/2315 train_time:36846ms step_avg:60.11ms
step:614/2315 train_time:36906ms step_avg:60.11ms
step:615/2315 train_time:36966ms step_avg:60.11ms
step:616/2315 train_time:37026ms step_avg:60.11ms
step:617/2315 train_time:37086ms step_avg:60.11ms
step:618/2315 train_time:37146ms step_avg:60.11ms
step:619/2315 train_time:37206ms step_avg:60.11ms
step:620/2315 train_time:37266ms step_avg:60.11ms
step:621/2315 train_time:37326ms step_avg:60.11ms
step:622/2315 train_time:37387ms step_avg:60.11ms
step:623/2315 train_time:37446ms step_avg:60.11ms
step:624/2315 train_time:37507ms step_avg:60.11ms
step:625/2315 train_time:37566ms step_avg:60.11ms
step:626/2315 train_time:37627ms step_avg:60.11ms
step:627/2315 train_time:37686ms step_avg:60.11ms
step:628/2315 train_time:37747ms step_avg:60.11ms
step:629/2315 train_time:37806ms step_avg:60.11ms
step:630/2315 train_time:37866ms step_avg:60.11ms
step:631/2315 train_time:37926ms step_avg:60.10ms
step:632/2315 train_time:37986ms step_avg:60.10ms
step:633/2315 train_time:38046ms step_avg:60.10ms
step:634/2315 train_time:38107ms step_avg:60.11ms
step:635/2315 train_time:38166ms step_avg:60.10ms
step:636/2315 train_time:38226ms step_avg:60.10ms
step:637/2315 train_time:38285ms step_avg:60.10ms
step:638/2315 train_time:38346ms step_avg:60.10ms
step:639/2315 train_time:38406ms step_avg:60.10ms
step:640/2315 train_time:38467ms step_avg:60.10ms
step:641/2315 train_time:38525ms step_avg:60.10ms
step:642/2315 train_time:38585ms step_avg:60.10ms
step:643/2315 train_time:38645ms step_avg:60.10ms
step:644/2315 train_time:38705ms step_avg:60.10ms
step:645/2315 train_time:38765ms step_avg:60.10ms
step:646/2315 train_time:38825ms step_avg:60.10ms
step:647/2315 train_time:38884ms step_avg:60.10ms
step:648/2315 train_time:38945ms step_avg:60.10ms
step:649/2315 train_time:39004ms step_avg:60.10ms
step:650/2315 train_time:39064ms step_avg:60.10ms
step:651/2315 train_time:39124ms step_avg:60.10ms
step:652/2315 train_time:39185ms step_avg:60.10ms
step:653/2315 train_time:39244ms step_avg:60.10ms
step:654/2315 train_time:39305ms step_avg:60.10ms
step:655/2315 train_time:39365ms step_avg:60.10ms
step:656/2315 train_time:39425ms step_avg:60.10ms
step:657/2315 train_time:39484ms step_avg:60.10ms
step:658/2315 train_time:39545ms step_avg:60.10ms
step:659/2315 train_time:39605ms step_avg:60.10ms
step:660/2315 train_time:39665ms step_avg:60.10ms
step:661/2315 train_time:39725ms step_avg:60.10ms
step:662/2315 train_time:39785ms step_avg:60.10ms
step:663/2315 train_time:39845ms step_avg:60.10ms
step:664/2315 train_time:39905ms step_avg:60.10ms
step:665/2315 train_time:39965ms step_avg:60.10ms
step:666/2315 train_time:40025ms step_avg:60.10ms
step:667/2315 train_time:40084ms step_avg:60.10ms
step:668/2315 train_time:40144ms step_avg:60.10ms
step:669/2315 train_time:40204ms step_avg:60.10ms
step:670/2315 train_time:40264ms step_avg:60.10ms
step:671/2315 train_time:40324ms step_avg:60.10ms
step:672/2315 train_time:40385ms step_avg:60.10ms
step:673/2315 train_time:40444ms step_avg:60.10ms
step:674/2315 train_time:40505ms step_avg:60.10ms
step:675/2315 train_time:40565ms step_avg:60.10ms
step:676/2315 train_time:40624ms step_avg:60.10ms
step:677/2315 train_time:40685ms step_avg:60.10ms
step:678/2315 train_time:40745ms step_avg:60.10ms
step:679/2315 train_time:40805ms step_avg:60.10ms
step:680/2315 train_time:40865ms step_avg:60.10ms
step:681/2315 train_time:40924ms step_avg:60.09ms
step:682/2315 train_time:40984ms step_avg:60.09ms
step:683/2315 train_time:41043ms step_avg:60.09ms
step:684/2315 train_time:41104ms step_avg:60.09ms
step:685/2315 train_time:41163ms step_avg:60.09ms
step:686/2315 train_time:41223ms step_avg:60.09ms
step:687/2315 train_time:41284ms step_avg:60.09ms
step:688/2315 train_time:41344ms step_avg:60.09ms
step:689/2315 train_time:41404ms step_avg:60.09ms
step:690/2315 train_time:41464ms step_avg:60.09ms
step:691/2315 train_time:41523ms step_avg:60.09ms
step:692/2315 train_time:41584ms step_avg:60.09ms
step:693/2315 train_time:41644ms step_avg:60.09ms
step:694/2315 train_time:41704ms step_avg:60.09ms
step:695/2315 train_time:41764ms step_avg:60.09ms
step:696/2315 train_time:41824ms step_avg:60.09ms
step:697/2315 train_time:41883ms step_avg:60.09ms
step:698/2315 train_time:41943ms step_avg:60.09ms
step:699/2315 train_time:42003ms step_avg:60.09ms
step:700/2315 train_time:42063ms step_avg:60.09ms
step:701/2315 train_time:42122ms step_avg:60.09ms
step:702/2315 train_time:42183ms step_avg:60.09ms
step:703/2315 train_time:42243ms step_avg:60.09ms
step:704/2315 train_time:42303ms step_avg:60.09ms
step:705/2315 train_time:42363ms step_avg:60.09ms
step:706/2315 train_time:42424ms step_avg:60.09ms
step:707/2315 train_time:42484ms step_avg:60.09ms
step:708/2315 train_time:42544ms step_avg:60.09ms
step:709/2315 train_time:42603ms step_avg:60.09ms
step:710/2315 train_time:42664ms step_avg:60.09ms
step:711/2315 train_time:42723ms step_avg:60.09ms
step:712/2315 train_time:42784ms step_avg:60.09ms
step:713/2315 train_time:42843ms step_avg:60.09ms
step:714/2315 train_time:42903ms step_avg:60.09ms
step:715/2315 train_time:42963ms step_avg:60.09ms
step:716/2315 train_time:43023ms step_avg:60.09ms
step:717/2315 train_time:43083ms step_avg:60.09ms
step:718/2315 train_time:43143ms step_avg:60.09ms
step:719/2315 train_time:43203ms step_avg:60.09ms
step:720/2315 train_time:43264ms step_avg:60.09ms
step:721/2315 train_time:43323ms step_avg:60.09ms
step:722/2315 train_time:43384ms step_avg:60.09ms
step:723/2315 train_time:43443ms step_avg:60.09ms
step:724/2315 train_time:43503ms step_avg:60.09ms
step:725/2315 train_time:43563ms step_avg:60.09ms
step:726/2315 train_time:43623ms step_avg:60.09ms
step:727/2315 train_time:43684ms step_avg:60.09ms
step:728/2315 train_time:43744ms step_avg:60.09ms
step:729/2315 train_time:43804ms step_avg:60.09ms
step:730/2315 train_time:43864ms step_avg:60.09ms
step:731/2315 train_time:43923ms step_avg:60.09ms
step:732/2315 train_time:43984ms step_avg:60.09ms
step:733/2315 train_time:44044ms step_avg:60.09ms
step:734/2315 train_time:44104ms step_avg:60.09ms
step:735/2315 train_time:44164ms step_avg:60.09ms
step:736/2315 train_time:44224ms step_avg:60.09ms
step:737/2315 train_time:44284ms step_avg:60.09ms
step:738/2315 train_time:44344ms step_avg:60.09ms
step:739/2315 train_time:44404ms step_avg:60.09ms
step:740/2315 train_time:44463ms step_avg:60.09ms
step:741/2315 train_time:44523ms step_avg:60.09ms
step:742/2315 train_time:44584ms step_avg:60.09ms
step:743/2315 train_time:44644ms step_avg:60.09ms
step:744/2315 train_time:44704ms step_avg:60.09ms
step:745/2315 train_time:44763ms step_avg:60.09ms
step:746/2315 train_time:44824ms step_avg:60.09ms
step:747/2315 train_time:44884ms step_avg:60.09ms
step:748/2315 train_time:44944ms step_avg:60.09ms
step:749/2315 train_time:45004ms step_avg:60.09ms
step:750/2315 train_time:45065ms step_avg:60.09ms
step:750/2315 val_loss:3.6827 train_time:45126ms step_avg:60.17ms
step:751/2315 train_time:45146ms step_avg:60.11ms
step:752/2315 train_time:45188ms step_avg:60.09ms
step:753/2315 train_time:45250ms step_avg:60.09ms
step:754/2315 train_time:45310ms step_avg:60.09ms
step:755/2315 train_time:45370ms step_avg:60.09ms
step:756/2315 train_time:45430ms step_avg:60.09ms
step:757/2315 train_time:45489ms step_avg:60.09ms
step:758/2315 train_time:45549ms step_avg:60.09ms
step:759/2315 train_time:45608ms step_avg:60.09ms
step:760/2315 train_time:45668ms step_avg:60.09ms
step:761/2315 train_time:45728ms step_avg:60.09ms
step:762/2315 train_time:45788ms step_avg:60.09ms
step:763/2315 train_time:45848ms step_avg:60.09ms
step:764/2315 train_time:45909ms step_avg:60.09ms
step:765/2315 train_time:45968ms step_avg:60.09ms
step:766/2315 train_time:46029ms step_avg:60.09ms
step:767/2315 train_time:46091ms step_avg:60.09ms
step:768/2315 train_time:46153ms step_avg:60.10ms
step:769/2315 train_time:46215ms step_avg:60.10ms
step:770/2315 train_time:46278ms step_avg:60.10ms
step:771/2315 train_time:46338ms step_avg:60.10ms
step:772/2315 train_time:46400ms step_avg:60.10ms
step:773/2315 train_time:46461ms step_avg:60.10ms
step:774/2315 train_time:46522ms step_avg:60.11ms
step:775/2315 train_time:46583ms step_avg:60.11ms
step:776/2315 train_time:46643ms step_avg:60.11ms
step:777/2315 train_time:46703ms step_avg:60.11ms
step:778/2315 train_time:46764ms step_avg:60.11ms
step:779/2315 train_time:46825ms step_avg:60.11ms
step:780/2315 train_time:46886ms step_avg:60.11ms
step:781/2315 train_time:46946ms step_avg:60.11ms
step:782/2315 train_time:47007ms step_avg:60.11ms
step:783/2315 train_time:47068ms step_avg:60.11ms
step:784/2315 train_time:47130ms step_avg:60.11ms
step:785/2315 train_time:47191ms step_avg:60.12ms
step:786/2315 train_time:47253ms step_avg:60.12ms
step:787/2315 train_time:47314ms step_avg:60.12ms
step:788/2315 train_time:47374ms step_avg:60.12ms
step:789/2315 train_time:47434ms step_avg:60.12ms
step:790/2315 train_time:47496ms step_avg:60.12ms
step:791/2315 train_time:47556ms step_avg:60.12ms
step:792/2315 train_time:47617ms step_avg:60.12ms
step:793/2315 train_time:47677ms step_avg:60.12ms
step:794/2315 train_time:47739ms step_avg:60.12ms
step:795/2315 train_time:47799ms step_avg:60.13ms
step:796/2315 train_time:47861ms step_avg:60.13ms
step:797/2315 train_time:47921ms step_avg:60.13ms
step:798/2315 train_time:47981ms step_avg:60.13ms
step:799/2315 train_time:48042ms step_avg:60.13ms
step:800/2315 train_time:48103ms step_avg:60.13ms
step:801/2315 train_time:48164ms step_avg:60.13ms
step:802/2315 train_time:48227ms step_avg:60.13ms
step:803/2315 train_time:48288ms step_avg:60.13ms
step:804/2315 train_time:48349ms step_avg:60.14ms
step:805/2315 train_time:48410ms step_avg:60.14ms
step:806/2315 train_time:48471ms step_avg:60.14ms
step:807/2315 train_time:48531ms step_avg:60.14ms
step:808/2315 train_time:48592ms step_avg:60.14ms
step:809/2315 train_time:48652ms step_avg:60.14ms
step:810/2315 train_time:48714ms step_avg:60.14ms
step:811/2315 train_time:48774ms step_avg:60.14ms
step:812/2315 train_time:48836ms step_avg:60.14ms
step:813/2315 train_time:48896ms step_avg:60.14ms
step:814/2315 train_time:48957ms step_avg:60.14ms
step:815/2315 train_time:49018ms step_avg:60.15ms
step:816/2315 train_time:49080ms step_avg:60.15ms
step:817/2315 train_time:49140ms step_avg:60.15ms
step:818/2315 train_time:49202ms step_avg:60.15ms
step:819/2315 train_time:49263ms step_avg:60.15ms
step:820/2315 train_time:49325ms step_avg:60.15ms
step:821/2315 train_time:49386ms step_avg:60.15ms
step:822/2315 train_time:49447ms step_avg:60.15ms
step:823/2315 train_time:49507ms step_avg:60.15ms
step:824/2315 train_time:49568ms step_avg:60.16ms
step:825/2315 train_time:49628ms step_avg:60.16ms
step:826/2315 train_time:49689ms step_avg:60.16ms
step:827/2315 train_time:49750ms step_avg:60.16ms
step:828/2315 train_time:49811ms step_avg:60.16ms
step:829/2315 train_time:49871ms step_avg:60.16ms
step:830/2315 train_time:49931ms step_avg:60.16ms
step:831/2315 train_time:49992ms step_avg:60.16ms
step:832/2315 train_time:50053ms step_avg:60.16ms
step:833/2315 train_time:50115ms step_avg:60.16ms
step:834/2315 train_time:50176ms step_avg:60.16ms
step:835/2315 train_time:50237ms step_avg:60.16ms
step:836/2315 train_time:50299ms step_avg:60.17ms
step:837/2315 train_time:50359ms step_avg:60.17ms
step:838/2315 train_time:50421ms step_avg:60.17ms
step:839/2315 train_time:50482ms step_avg:60.17ms
step:840/2315 train_time:50543ms step_avg:60.17ms
step:841/2315 train_time:50603ms step_avg:60.17ms
step:842/2315 train_time:50665ms step_avg:60.17ms
step:843/2315 train_time:50725ms step_avg:60.17ms
step:844/2315 train_time:50787ms step_avg:60.17ms
step:845/2315 train_time:50847ms step_avg:60.17ms
step:846/2315 train_time:50909ms step_avg:60.18ms
step:847/2315 train_time:50969ms step_avg:60.18ms
step:848/2315 train_time:51030ms step_avg:60.18ms
step:849/2315 train_time:51091ms step_avg:60.18ms
step:850/2315 train_time:51153ms step_avg:60.18ms
step:851/2315 train_time:51213ms step_avg:60.18ms
step:852/2315 train_time:51274ms step_avg:60.18ms
step:853/2315 train_time:51335ms step_avg:60.18ms
step:854/2315 train_time:51397ms step_avg:60.18ms
step:855/2315 train_time:51457ms step_avg:60.18ms
step:856/2315 train_time:51518ms step_avg:60.18ms
step:857/2315 train_time:51579ms step_avg:60.19ms
step:858/2315 train_time:51640ms step_avg:60.19ms
step:859/2315 train_time:51701ms step_avg:60.19ms
step:860/2315 train_time:51762ms step_avg:60.19ms
step:861/2315 train_time:51823ms step_avg:60.19ms
step:862/2315 train_time:51885ms step_avg:60.19ms
step:863/2315 train_time:51946ms step_avg:60.19ms
step:864/2315 train_time:52007ms step_avg:60.19ms
step:865/2315 train_time:52067ms step_avg:60.19ms
step:866/2315 train_time:52129ms step_avg:60.20ms
step:867/2315 train_time:52190ms step_avg:60.20ms
step:868/2315 train_time:52252ms step_avg:60.20ms
step:869/2315 train_time:52313ms step_avg:60.20ms
step:870/2315 train_time:52374ms step_avg:60.20ms
step:871/2315 train_time:52435ms step_avg:60.20ms
step:872/2315 train_time:52496ms step_avg:60.20ms
step:873/2315 train_time:52556ms step_avg:60.20ms
step:874/2315 train_time:52617ms step_avg:60.20ms
step:875/2315 train_time:52678ms step_avg:60.20ms
step:876/2315 train_time:52739ms step_avg:60.20ms
step:877/2315 train_time:52800ms step_avg:60.20ms
step:878/2315 train_time:52861ms step_avg:60.21ms
step:879/2315 train_time:52922ms step_avg:60.21ms
step:880/2315 train_time:52983ms step_avg:60.21ms
step:881/2315 train_time:53043ms step_avg:60.21ms
step:882/2315 train_time:53105ms step_avg:60.21ms
step:883/2315 train_time:53166ms step_avg:60.21ms
step:884/2315 train_time:53228ms step_avg:60.21ms
step:885/2315 train_time:53289ms step_avg:60.21ms
step:886/2315 train_time:53350ms step_avg:60.21ms
step:887/2315 train_time:53410ms step_avg:60.21ms
step:888/2315 train_time:53471ms step_avg:60.22ms
step:889/2315 train_time:53531ms step_avg:60.22ms
step:890/2315 train_time:53592ms step_avg:60.22ms
step:891/2315 train_time:53653ms step_avg:60.22ms
step:892/2315 train_time:53714ms step_avg:60.22ms
step:893/2315 train_time:53775ms step_avg:60.22ms
step:894/2315 train_time:53836ms step_avg:60.22ms
step:895/2315 train_time:53896ms step_avg:60.22ms
step:896/2315 train_time:53958ms step_avg:60.22ms
step:897/2315 train_time:54018ms step_avg:60.22ms
step:898/2315 train_time:54079ms step_avg:60.22ms
step:899/2315 train_time:54140ms step_avg:60.22ms
step:900/2315 train_time:54202ms step_avg:60.22ms
step:901/2315 train_time:54264ms step_avg:60.23ms
step:902/2315 train_time:54326ms step_avg:60.23ms
step:903/2315 train_time:54387ms step_avg:60.23ms
step:904/2315 train_time:54449ms step_avg:60.23ms
step:905/2315 train_time:54508ms step_avg:60.23ms
step:906/2315 train_time:54569ms step_avg:60.23ms
step:907/2315 train_time:54630ms step_avg:60.23ms
step:908/2315 train_time:54691ms step_avg:60.23ms
step:909/2315 train_time:54752ms step_avg:60.23ms
step:910/2315 train_time:54813ms step_avg:60.23ms
step:911/2315 train_time:54874ms step_avg:60.23ms
step:912/2315 train_time:54935ms step_avg:60.24ms
step:913/2315 train_time:54996ms step_avg:60.24ms
step:914/2315 train_time:55057ms step_avg:60.24ms
step:915/2315 train_time:55118ms step_avg:60.24ms
step:916/2315 train_time:55180ms step_avg:60.24ms
step:917/2315 train_time:55241ms step_avg:60.24ms
step:918/2315 train_time:55302ms step_avg:60.24ms
step:919/2315 train_time:55363ms step_avg:60.24ms
step:920/2315 train_time:55425ms step_avg:60.24ms
step:921/2315 train_time:55485ms step_avg:60.24ms
step:922/2315 train_time:55546ms step_avg:60.25ms
step:923/2315 train_time:55606ms step_avg:60.25ms
step:924/2315 train_time:55668ms step_avg:60.25ms
step:925/2315 train_time:55728ms step_avg:60.25ms
step:926/2315 train_time:55790ms step_avg:60.25ms
step:927/2315 train_time:55851ms step_avg:60.25ms
step:928/2315 train_time:55911ms step_avg:60.25ms
step:929/2315 train_time:55972ms step_avg:60.25ms
step:930/2315 train_time:56033ms step_avg:60.25ms
step:931/2315 train_time:56093ms step_avg:60.25ms
step:932/2315 train_time:56154ms step_avg:60.25ms
step:933/2315 train_time:56216ms step_avg:60.25ms
step:934/2315 train_time:56278ms step_avg:60.25ms
step:935/2315 train_time:56339ms step_avg:60.26ms
step:936/2315 train_time:56401ms step_avg:60.26ms
step:937/2315 train_time:56462ms step_avg:60.26ms
step:938/2315 train_time:56524ms step_avg:60.26ms
step:939/2315 train_time:56585ms step_avg:60.26ms
step:940/2315 train_time:56646ms step_avg:60.26ms
step:941/2315 train_time:56708ms step_avg:60.26ms
step:942/2315 train_time:56769ms step_avg:60.26ms
step:943/2315 train_time:56830ms step_avg:60.26ms
step:944/2315 train_time:56891ms step_avg:60.27ms
step:945/2315 train_time:56952ms step_avg:60.27ms
step:946/2315 train_time:57013ms step_avg:60.27ms
step:947/2315 train_time:57073ms step_avg:60.27ms
step:948/2315 train_time:57134ms step_avg:60.27ms
step:949/2315 train_time:57194ms step_avg:60.27ms
step:950/2315 train_time:57255ms step_avg:60.27ms
step:951/2315 train_time:57316ms step_avg:60.27ms
step:952/2315 train_time:57378ms step_avg:60.27ms
step:953/2315 train_time:57439ms step_avg:60.27ms
step:954/2315 train_time:57501ms step_avg:60.27ms
step:955/2315 train_time:57561ms step_avg:60.27ms
step:956/2315 train_time:57623ms step_avg:60.28ms
step:957/2315 train_time:57684ms step_avg:60.28ms
step:958/2315 train_time:57745ms step_avg:60.28ms
step:959/2315 train_time:57806ms step_avg:60.28ms
step:960/2315 train_time:57868ms step_avg:60.28ms
step:961/2315 train_time:57929ms step_avg:60.28ms
step:962/2315 train_time:57990ms step_avg:60.28ms
step:963/2315 train_time:58051ms step_avg:60.28ms
step:964/2315 train_time:58112ms step_avg:60.28ms
step:965/2315 train_time:58173ms step_avg:60.28ms
step:966/2315 train_time:58234ms step_avg:60.28ms
step:967/2315 train_time:58294ms step_avg:60.28ms
step:968/2315 train_time:58355ms step_avg:60.28ms
step:969/2315 train_time:58416ms step_avg:60.28ms
step:970/2315 train_time:58478ms step_avg:60.29ms
step:971/2315 train_time:58539ms step_avg:60.29ms
step:972/2315 train_time:58601ms step_avg:60.29ms
step:973/2315 train_time:58662ms step_avg:60.29ms
step:974/2315 train_time:58723ms step_avg:60.29ms
step:975/2315 train_time:58783ms step_avg:60.29ms
step:976/2315 train_time:58844ms step_avg:60.29ms
step:977/2315 train_time:58904ms step_avg:60.29ms
step:978/2315 train_time:58966ms step_avg:60.29ms
step:979/2315 train_time:59027ms step_avg:60.29ms
step:980/2315 train_time:59088ms step_avg:60.29ms
step:981/2315 train_time:59149ms step_avg:60.29ms
step:982/2315 train_time:59210ms step_avg:60.30ms
step:983/2315 train_time:59271ms step_avg:60.30ms
step:984/2315 train_time:59332ms step_avg:60.30ms
step:985/2315 train_time:59392ms step_avg:60.30ms
step:986/2315 train_time:59454ms step_avg:60.30ms
step:987/2315 train_time:59515ms step_avg:60.30ms
step:988/2315 train_time:59577ms step_avg:60.30ms
step:989/2315 train_time:59638ms step_avg:60.30ms
step:990/2315 train_time:59699ms step_avg:60.30ms
step:991/2315 train_time:59759ms step_avg:60.30ms
step:992/2315 train_time:59821ms step_avg:60.30ms
step:993/2315 train_time:59882ms step_avg:60.30ms
step:994/2315 train_time:59943ms step_avg:60.30ms
step:995/2315 train_time:60003ms step_avg:60.30ms
step:996/2315 train_time:60065ms step_avg:60.31ms
step:997/2315 train_time:60125ms step_avg:60.31ms
step:998/2315 train_time:60188ms step_avg:60.31ms
step:999/2315 train_time:60248ms step_avg:60.31ms
step:1000/2315 train_time:60309ms step_avg:60.31ms
step:1000/2315 val_loss:3.5736 train_time:60371ms step_avg:60.37ms
step:1001/2315 train_time:60398ms step_avg:60.34ms
step:1002/2315 train_time:60433ms step_avg:60.31ms
step:1003/2315 train_time:60500ms step_avg:60.32ms
step:1004/2315 train_time:60567ms step_avg:60.33ms
step:1005/2315 train_time:60627ms step_avg:60.33ms
step:1006/2315 train_time:60689ms step_avg:60.33ms
step:1007/2315 train_time:60748ms step_avg:60.33ms
step:1008/2315 train_time:60809ms step_avg:60.33ms
step:1009/2315 train_time:60868ms step_avg:60.33ms
step:1010/2315 train_time:60928ms step_avg:60.32ms
step:1011/2315 train_time:60988ms step_avg:60.32ms
step:1012/2315 train_time:61048ms step_avg:60.32ms
step:1013/2315 train_time:61107ms step_avg:60.32ms
step:1014/2315 train_time:61167ms step_avg:60.32ms
step:1015/2315 train_time:61226ms step_avg:60.32ms
step:1016/2315 train_time:61288ms step_avg:60.32ms
step:1017/2315 train_time:61350ms step_avg:60.32ms
step:1018/2315 train_time:61413ms step_avg:60.33ms
step:1019/2315 train_time:61475ms step_avg:60.33ms
step:1020/2315 train_time:61537ms step_avg:60.33ms
step:1021/2315 train_time:61597ms step_avg:60.33ms
step:1022/2315 train_time:61659ms step_avg:60.33ms
step:1023/2315 train_time:61720ms step_avg:60.33ms
step:1024/2315 train_time:61781ms step_avg:60.33ms
step:1025/2315 train_time:61841ms step_avg:60.33ms
step:1026/2315 train_time:61903ms step_avg:60.33ms
step:1027/2315 train_time:61963ms step_avg:60.33ms
step:1028/2315 train_time:62023ms step_avg:60.33ms
step:1029/2315 train_time:62082ms step_avg:60.33ms
step:1030/2315 train_time:62143ms step_avg:60.33ms
step:1031/2315 train_time:62203ms step_avg:60.33ms
step:1032/2315 train_time:62264ms step_avg:60.33ms
step:1033/2315 train_time:62325ms step_avg:60.33ms
step:1034/2315 train_time:62388ms step_avg:60.34ms
step:1035/2315 train_time:62449ms step_avg:60.34ms
step:1036/2315 train_time:62511ms step_avg:60.34ms
step:1037/2315 train_time:62572ms step_avg:60.34ms
step:1038/2315 train_time:62633ms step_avg:60.34ms
step:1039/2315 train_time:62694ms step_avg:60.34ms
step:1040/2315 train_time:62755ms step_avg:60.34ms
step:1041/2315 train_time:62816ms step_avg:60.34ms
step:1042/2315 train_time:62877ms step_avg:60.34ms
step:1043/2315 train_time:62938ms step_avg:60.34ms
step:1044/2315 train_time:62999ms step_avg:60.34ms
step:1045/2315 train_time:63059ms step_avg:60.34ms
step:1046/2315 train_time:63120ms step_avg:60.34ms
step:1047/2315 train_time:63180ms step_avg:60.34ms
step:1048/2315 train_time:63241ms step_avg:60.34ms
step:1049/2315 train_time:63302ms step_avg:60.35ms
step:1050/2315 train_time:63363ms step_avg:60.35ms
step:1051/2315 train_time:63425ms step_avg:60.35ms
step:1052/2315 train_time:63487ms step_avg:60.35ms
step:1053/2315 train_time:63549ms step_avg:60.35ms
step:1054/2315 train_time:63610ms step_avg:60.35ms
step:1055/2315 train_time:63671ms step_avg:60.35ms
step:1056/2315 train_time:63732ms step_avg:60.35ms
step:1057/2315 train_time:63792ms step_avg:60.35ms
step:1058/2315 train_time:63853ms step_avg:60.35ms
step:1059/2315 train_time:63914ms step_avg:60.35ms
step:1060/2315 train_time:63975ms step_avg:60.35ms
step:1061/2315 train_time:64035ms step_avg:60.35ms
step:1062/2315 train_time:64095ms step_avg:60.35ms
step:1063/2315 train_time:64156ms step_avg:60.35ms
step:1064/2315 train_time:64217ms step_avg:60.35ms
step:1065/2315 train_time:64278ms step_avg:60.35ms
step:1066/2315 train_time:64339ms step_avg:60.36ms
step:1067/2315 train_time:64400ms step_avg:60.36ms
step:1068/2315 train_time:64463ms step_avg:60.36ms
step:1069/2315 train_time:64523ms step_avg:60.36ms
step:1070/2315 train_time:64585ms step_avg:60.36ms
step:1071/2315 train_time:64647ms step_avg:60.36ms
step:1072/2315 train_time:64709ms step_avg:60.36ms
step:1073/2315 train_time:64769ms step_avg:60.36ms
step:1074/2315 train_time:64830ms step_avg:60.36ms
step:1075/2315 train_time:64891ms step_avg:60.36ms
step:1076/2315 train_time:64952ms step_avg:60.36ms
step:1077/2315 train_time:65012ms step_avg:60.36ms
step:1078/2315 train_time:65073ms step_avg:60.36ms
step:1079/2315 train_time:65133ms step_avg:60.36ms
step:1080/2315 train_time:65195ms step_avg:60.37ms
step:1081/2315 train_time:65254ms step_avg:60.36ms
step:1082/2315 train_time:65316ms step_avg:60.37ms
step:1083/2315 train_time:65377ms step_avg:60.37ms
step:1084/2315 train_time:65439ms step_avg:60.37ms
step:1085/2315 train_time:65501ms step_avg:60.37ms
step:1086/2315 train_time:65562ms step_avg:60.37ms
step:1087/2315 train_time:65622ms step_avg:60.37ms
step:1088/2315 train_time:65684ms step_avg:60.37ms
step:1089/2315 train_time:65745ms step_avg:60.37ms
step:1090/2315 train_time:65807ms step_avg:60.37ms
step:1091/2315 train_time:65868ms step_avg:60.37ms
step:1092/2315 train_time:65929ms step_avg:60.37ms
step:1093/2315 train_time:65990ms step_avg:60.37ms
step:1094/2315 train_time:66051ms step_avg:60.38ms
step:1095/2315 train_time:66111ms step_avg:60.38ms
step:1096/2315 train_time:66172ms step_avg:60.38ms
step:1097/2315 train_time:66232ms step_avg:60.38ms
step:1098/2315 train_time:66294ms step_avg:60.38ms
step:1099/2315 train_time:66354ms step_avg:60.38ms
step:1100/2315 train_time:66416ms step_avg:60.38ms
step:1101/2315 train_time:66477ms step_avg:60.38ms
step:1102/2315 train_time:66539ms step_avg:60.38ms
step:1103/2315 train_time:66600ms step_avg:60.38ms
step:1104/2315 train_time:66662ms step_avg:60.38ms
step:1105/2315 train_time:66723ms step_avg:60.38ms
step:1106/2315 train_time:66784ms step_avg:60.38ms
step:1107/2315 train_time:66845ms step_avg:60.38ms
step:1108/2315 train_time:66907ms step_avg:60.39ms
step:1109/2315 train_time:66968ms step_avg:60.39ms
step:1110/2315 train_time:67029ms step_avg:60.39ms
step:1111/2315 train_time:67090ms step_avg:60.39ms
step:1112/2315 train_time:67152ms step_avg:60.39ms
step:1113/2315 train_time:67211ms step_avg:60.39ms
step:1114/2315 train_time:67272ms step_avg:60.39ms
step:1115/2315 train_time:67333ms step_avg:60.39ms
step:1116/2315 train_time:67395ms step_avg:60.39ms
step:1117/2315 train_time:67455ms step_avg:60.39ms
step:1118/2315 train_time:67517ms step_avg:60.39ms
step:1119/2315 train_time:67578ms step_avg:60.39ms
step:1120/2315 train_time:67639ms step_avg:60.39ms
step:1121/2315 train_time:67700ms step_avg:60.39ms
step:1122/2315 train_time:67762ms step_avg:60.39ms
step:1123/2315 train_time:67822ms step_avg:60.39ms
step:1124/2315 train_time:67884ms step_avg:60.39ms
step:1125/2315 train_time:67944ms step_avg:60.39ms
step:1126/2315 train_time:68006ms step_avg:60.40ms
step:1127/2315 train_time:68067ms step_avg:60.40ms
step:1128/2315 train_time:68129ms step_avg:60.40ms
step:1129/2315 train_time:68189ms step_avg:60.40ms
step:1130/2315 train_time:68250ms step_avg:60.40ms
step:1131/2315 train_time:68310ms step_avg:60.40ms
step:1132/2315 train_time:68372ms step_avg:60.40ms
step:1133/2315 train_time:68432ms step_avg:60.40ms
step:1134/2315 train_time:68493ms step_avg:60.40ms
step:1135/2315 train_time:68554ms step_avg:60.40ms
step:1136/2315 train_time:68615ms step_avg:60.40ms
step:1137/2315 train_time:68676ms step_avg:60.40ms
step:1138/2315 train_time:68737ms step_avg:60.40ms
step:1139/2315 train_time:68798ms step_avg:60.40ms
step:1140/2315 train_time:68860ms step_avg:60.40ms
step:1141/2315 train_time:68921ms step_avg:60.40ms
step:1142/2315 train_time:68982ms step_avg:60.40ms
step:1143/2315 train_time:69043ms step_avg:60.41ms
step:1144/2315 train_time:69105ms step_avg:60.41ms
step:1145/2315 train_time:69165ms step_avg:60.41ms
step:1146/2315 train_time:69228ms step_avg:60.41ms
step:1147/2315 train_time:69289ms step_avg:60.41ms
step:1148/2315 train_time:69350ms step_avg:60.41ms
step:1149/2315 train_time:69410ms step_avg:60.41ms
step:1150/2315 train_time:69471ms step_avg:60.41ms
step:1151/2315 train_time:69532ms step_avg:60.41ms
step:1152/2315 train_time:69593ms step_avg:60.41ms
step:1153/2315 train_time:69653ms step_avg:60.41ms
step:1154/2315 train_time:69715ms step_avg:60.41ms
step:1155/2315 train_time:69775ms step_avg:60.41ms
step:1156/2315 train_time:69836ms step_avg:60.41ms
step:1157/2315 train_time:69897ms step_avg:60.41ms
step:1158/2315 train_time:69959ms step_avg:60.41ms
step:1159/2315 train_time:70020ms step_avg:60.41ms
step:1160/2315 train_time:70081ms step_avg:60.41ms
step:1161/2315 train_time:70142ms step_avg:60.42ms
step:1162/2315 train_time:70204ms step_avg:60.42ms
step:1163/2315 train_time:70265ms step_avg:60.42ms
step:1164/2315 train_time:70327ms step_avg:60.42ms
step:1165/2315 train_time:70388ms step_avg:60.42ms
step:1166/2315 train_time:70449ms step_avg:60.42ms
step:1167/2315 train_time:70509ms step_avg:60.42ms
step:1168/2315 train_time:70570ms step_avg:60.42ms
step:1169/2315 train_time:70631ms step_avg:60.42ms
step:1170/2315 train_time:70692ms step_avg:60.42ms
step:1171/2315 train_time:70753ms step_avg:60.42ms
step:1172/2315 train_time:70814ms step_avg:60.42ms
step:1173/2315 train_time:70874ms step_avg:60.42ms
step:1174/2315 train_time:70936ms step_avg:60.42ms
step:1175/2315 train_time:70996ms step_avg:60.42ms
step:1176/2315 train_time:71058ms step_avg:60.42ms
step:1177/2315 train_time:71119ms step_avg:60.42ms
step:1178/2315 train_time:71180ms step_avg:60.42ms
step:1179/2315 train_time:71242ms step_avg:60.43ms
step:1180/2315 train_time:71303ms step_avg:60.43ms
step:1181/2315 train_time:71363ms step_avg:60.43ms
step:1182/2315 train_time:71425ms step_avg:60.43ms
step:1183/2315 train_time:71486ms step_avg:60.43ms
step:1184/2315 train_time:71548ms step_avg:60.43ms
step:1185/2315 train_time:71609ms step_avg:60.43ms
step:1186/2315 train_time:71670ms step_avg:60.43ms
step:1187/2315 train_time:71731ms step_avg:60.43ms
step:1188/2315 train_time:71793ms step_avg:60.43ms
step:1189/2315 train_time:71853ms step_avg:60.43ms
step:1190/2315 train_time:71914ms step_avg:60.43ms
step:1191/2315 train_time:71975ms step_avg:60.43ms
step:1192/2315 train_time:72036ms step_avg:60.43ms
step:1193/2315 train_time:72097ms step_avg:60.43ms
step:1194/2315 train_time:72158ms step_avg:60.43ms
step:1195/2315 train_time:72219ms step_avg:60.43ms
step:1196/2315 train_time:72280ms step_avg:60.44ms
step:1197/2315 train_time:72341ms step_avg:60.44ms
step:1198/2315 train_time:72403ms step_avg:60.44ms
step:1199/2315 train_time:72464ms step_avg:60.44ms
step:1200/2315 train_time:72525ms step_avg:60.44ms
step:1201/2315 train_time:72586ms step_avg:60.44ms
step:1202/2315 train_time:72648ms step_avg:60.44ms
step:1203/2315 train_time:72709ms step_avg:60.44ms
step:1204/2315 train_time:72770ms step_avg:60.44ms
step:1205/2315 train_time:72830ms step_avg:60.44ms
step:1206/2315 train_time:72892ms step_avg:60.44ms
step:1207/2315 train_time:72953ms step_avg:60.44ms
step:1208/2315 train_time:73014ms step_avg:60.44ms
step:1209/2315 train_time:73074ms step_avg:60.44ms
step:1210/2315 train_time:73136ms step_avg:60.44ms
step:1211/2315 train_time:73197ms step_avg:60.44ms
step:1212/2315 train_time:73259ms step_avg:60.44ms
step:1213/2315 train_time:73320ms step_avg:60.45ms
step:1214/2315 train_time:73382ms step_avg:60.45ms
step:1215/2315 train_time:73442ms step_avg:60.45ms
step:1216/2315 train_time:73504ms step_avg:60.45ms
step:1217/2315 train_time:73565ms step_avg:60.45ms
step:1218/2315 train_time:73626ms step_avg:60.45ms
step:1219/2315 train_time:73686ms step_avg:60.45ms
step:1220/2315 train_time:73747ms step_avg:60.45ms
step:1221/2315 train_time:73808ms step_avg:60.45ms
step:1222/2315 train_time:73870ms step_avg:60.45ms
step:1223/2315 train_time:73931ms step_avg:60.45ms
step:1224/2315 train_time:73992ms step_avg:60.45ms
step:1225/2315 train_time:74053ms step_avg:60.45ms
step:1226/2315 train_time:74114ms step_avg:60.45ms
step:1227/2315 train_time:74174ms step_avg:60.45ms
step:1228/2315 train_time:74235ms step_avg:60.45ms
step:1229/2315 train_time:74295ms step_avg:60.45ms
step:1230/2315 train_time:74357ms step_avg:60.45ms
step:1231/2315 train_time:74418ms step_avg:60.45ms
step:1232/2315 train_time:74479ms step_avg:60.45ms
step:1233/2315 train_time:74540ms step_avg:60.45ms
step:1234/2315 train_time:74602ms step_avg:60.46ms
step:1235/2315 train_time:74662ms step_avg:60.46ms
step:1236/2315 train_time:74724ms step_avg:60.46ms
step:1237/2315 train_time:74785ms step_avg:60.46ms
step:1238/2315 train_time:74846ms step_avg:60.46ms
step:1239/2315 train_time:74907ms step_avg:60.46ms
step:1240/2315 train_time:74969ms step_avg:60.46ms
step:1241/2315 train_time:75029ms step_avg:60.46ms
step:1242/2315 train_time:75091ms step_avg:60.46ms
step:1243/2315 train_time:75151ms step_avg:60.46ms
step:1244/2315 train_time:75212ms step_avg:60.46ms
step:1245/2315 train_time:75272ms step_avg:60.46ms
step:1246/2315 train_time:75333ms step_avg:60.46ms
step:1247/2315 train_time:75394ms step_avg:60.46ms
step:1248/2315 train_time:75455ms step_avg:60.46ms
step:1249/2315 train_time:75516ms step_avg:60.46ms
step:1250/2315 train_time:75578ms step_avg:60.46ms
step:1250/2315 val_loss:3.5106 train_time:75641ms step_avg:60.51ms
step:1251/2315 train_time:75661ms step_avg:60.48ms
step:1252/2315 train_time:75704ms step_avg:60.47ms
step:1253/2315 train_time:75768ms step_avg:60.47ms
step:1254/2315 train_time:75831ms step_avg:60.47ms
step:1255/2315 train_time:75890ms step_avg:60.47ms
step:1256/2315 train_time:75952ms step_avg:60.47ms
step:1257/2315 train_time:76012ms step_avg:60.47ms
step:1258/2315 train_time:76073ms step_avg:60.47ms
step:1259/2315 train_time:76132ms step_avg:60.47ms
step:1260/2315 train_time:76193ms step_avg:60.47ms
step:1261/2315 train_time:76253ms step_avg:60.47ms
step:1262/2315 train_time:76313ms step_avg:60.47ms
step:1263/2315 train_time:76373ms step_avg:60.47ms
step:1264/2315 train_time:76434ms step_avg:60.47ms
step:1265/2315 train_time:76493ms step_avg:60.47ms
step:1266/2315 train_time:76555ms step_avg:60.47ms
step:1267/2315 train_time:76617ms step_avg:60.47ms
step:1268/2315 train_time:76679ms step_avg:60.47ms
step:1269/2315 train_time:76742ms step_avg:60.47ms
step:1270/2315 train_time:76804ms step_avg:60.48ms
step:1271/2315 train_time:76865ms step_avg:60.48ms
step:1272/2315 train_time:76927ms step_avg:60.48ms
step:1273/2315 train_time:76986ms step_avg:60.48ms
step:1274/2315 train_time:77047ms step_avg:60.48ms
step:1275/2315 train_time:77108ms step_avg:60.48ms
step:1276/2315 train_time:77168ms step_avg:60.48ms
step:1277/2315 train_time:77228ms step_avg:60.48ms
step:1278/2315 train_time:77289ms step_avg:60.48ms
step:1279/2315 train_time:77349ms step_avg:60.48ms
step:1280/2315 train_time:77410ms step_avg:60.48ms
step:1281/2315 train_time:77469ms step_avg:60.48ms
step:1282/2315 train_time:77531ms step_avg:60.48ms
step:1283/2315 train_time:77592ms step_avg:60.48ms
step:1284/2315 train_time:77655ms step_avg:60.48ms
step:1285/2315 train_time:77717ms step_avg:60.48ms
step:1286/2315 train_time:77778ms step_avg:60.48ms
step:1287/2315 train_time:77840ms step_avg:60.48ms
step:1288/2315 train_time:77902ms step_avg:60.48ms
step:1289/2315 train_time:77962ms step_avg:60.48ms
step:1290/2315 train_time:78023ms step_avg:60.48ms
step:1291/2315 train_time:78083ms step_avg:60.48ms
step:1292/2315 train_time:78144ms step_avg:60.48ms
step:1293/2315 train_time:78204ms step_avg:60.48ms
step:1294/2315 train_time:78266ms step_avg:60.48ms
step:1295/2315 train_time:78327ms step_avg:60.48ms
step:1296/2315 train_time:78387ms step_avg:60.48ms
step:1297/2315 train_time:78448ms step_avg:60.48ms
step:1298/2315 train_time:78509ms step_avg:60.48ms
step:1299/2315 train_time:78569ms step_avg:60.48ms
step:1300/2315 train_time:78630ms step_avg:60.48ms
step:1301/2315 train_time:78692ms step_avg:60.49ms
step:1302/2315 train_time:78754ms step_avg:60.49ms
step:1303/2315 train_time:78815ms step_avg:60.49ms
step:1304/2315 train_time:78876ms step_avg:60.49ms
step:1305/2315 train_time:78937ms step_avg:60.49ms
step:1306/2315 train_time:78998ms step_avg:60.49ms
step:1307/2315 train_time:79059ms step_avg:60.49ms
step:1308/2315 train_time:79121ms step_avg:60.49ms
step:1309/2315 train_time:79181ms step_avg:60.49ms
step:1310/2315 train_time:79243ms step_avg:60.49ms
step:1311/2315 train_time:79303ms step_avg:60.49ms
step:1312/2315 train_time:79364ms step_avg:60.49ms
step:1313/2315 train_time:79424ms step_avg:60.49ms
step:1314/2315 train_time:79486ms step_avg:60.49ms
step:1315/2315 train_time:79546ms step_avg:60.49ms
step:1316/2315 train_time:79607ms step_avg:60.49ms
step:1317/2315 train_time:79668ms step_avg:60.49ms
step:1318/2315 train_time:79729ms step_avg:60.49ms
step:1319/2315 train_time:79790ms step_avg:60.49ms
step:1320/2315 train_time:79851ms step_avg:60.49ms
step:1321/2315 train_time:79912ms step_avg:60.49ms
step:1322/2315 train_time:79974ms step_avg:60.49ms
step:1323/2315 train_time:80034ms step_avg:60.49ms
step:1324/2315 train_time:80096ms step_avg:60.50ms
step:1325/2315 train_time:80156ms step_avg:60.49ms
step:1326/2315 train_time:80217ms step_avg:60.50ms
step:1327/2315 train_time:80277ms step_avg:60.50ms
step:1328/2315 train_time:80338ms step_avg:60.50ms
step:1329/2315 train_time:80399ms step_avg:60.50ms
step:1330/2315 train_time:80461ms step_avg:60.50ms
step:1331/2315 train_time:80521ms step_avg:60.50ms
step:1332/2315 train_time:80582ms step_avg:60.50ms
step:1333/2315 train_time:80644ms step_avg:60.50ms
step:1334/2315 train_time:80705ms step_avg:60.50ms
step:1335/2315 train_time:80766ms step_avg:60.50ms
step:1336/2315 train_time:80828ms step_avg:60.50ms
step:1337/2315 train_time:80888ms step_avg:60.50ms
step:1338/2315 train_time:80949ms step_avg:60.50ms
step:1339/2315 train_time:81009ms step_avg:60.50ms
step:1340/2315 train_time:81071ms step_avg:60.50ms
step:1341/2315 train_time:81131ms step_avg:60.50ms
step:1342/2315 train_time:81192ms step_avg:60.50ms
step:1343/2315 train_time:81252ms step_avg:60.50ms
step:1344/2315 train_time:81314ms step_avg:60.50ms
step:1345/2315 train_time:81374ms step_avg:60.50ms
step:1346/2315 train_time:81435ms step_avg:60.50ms
step:1347/2315 train_time:81496ms step_avg:60.50ms
step:1348/2315 train_time:81557ms step_avg:60.50ms
step:1349/2315 train_time:81618ms step_avg:60.50ms
step:1350/2315 train_time:81680ms step_avg:60.50ms
step:1351/2315 train_time:81741ms step_avg:60.50ms
step:1352/2315 train_time:81803ms step_avg:60.51ms
step:1353/2315 train_time:81863ms step_avg:60.51ms
step:1354/2315 train_time:81925ms step_avg:60.51ms
step:1355/2315 train_time:81985ms step_avg:60.51ms
step:1356/2315 train_time:82046ms step_avg:60.51ms
step:1357/2315 train_time:82106ms step_avg:60.51ms
step:1358/2315 train_time:82168ms step_avg:60.51ms
step:1359/2315 train_time:82228ms step_avg:60.51ms
step:1360/2315 train_time:82289ms step_avg:60.51ms
step:1361/2315 train_time:82350ms step_avg:60.51ms
step:1362/2315 train_time:82411ms step_avg:60.51ms
step:1363/2315 train_time:82472ms step_avg:60.51ms
step:1364/2315 train_time:82533ms step_avg:60.51ms
step:1365/2315 train_time:82594ms step_avg:60.51ms
step:1366/2315 train_time:82655ms step_avg:60.51ms
step:1367/2315 train_time:82716ms step_avg:60.51ms
step:1368/2315 train_time:82777ms step_avg:60.51ms
step:1369/2315 train_time:82838ms step_avg:60.51ms
step:1370/2315 train_time:82900ms step_avg:60.51ms
step:1371/2315 train_time:82962ms step_avg:60.51ms
step:1372/2315 train_time:83024ms step_avg:60.51ms
step:1373/2315 train_time:83084ms step_avg:60.51ms
step:1374/2315 train_time:83146ms step_avg:60.51ms
step:1375/2315 train_time:83206ms step_avg:60.51ms
step:1376/2315 train_time:83267ms step_avg:60.51ms
step:1377/2315 train_time:83328ms step_avg:60.51ms
step:1378/2315 train_time:83389ms step_avg:60.51ms
step:1379/2315 train_time:83449ms step_avg:60.51ms
step:1380/2315 train_time:83510ms step_avg:60.51ms
step:1381/2315 train_time:83570ms step_avg:60.51ms
step:1382/2315 train_time:83631ms step_avg:60.51ms
step:1383/2315 train_time:83692ms step_avg:60.51ms
step:1384/2315 train_time:83754ms step_avg:60.52ms
step:1385/2315 train_time:83815ms step_avg:60.52ms
step:1386/2315 train_time:83877ms step_avg:60.52ms
step:1387/2315 train_time:83937ms step_avg:60.52ms
step:1388/2315 train_time:83999ms step_avg:60.52ms
step:1389/2315 train_time:84060ms step_avg:60.52ms
step:1390/2315 train_time:84122ms step_avg:60.52ms
step:1391/2315 train_time:84183ms step_avg:60.52ms
step:1392/2315 train_time:84244ms step_avg:60.52ms
step:1393/2315 train_time:84305ms step_avg:60.52ms
step:1394/2315 train_time:84366ms step_avg:60.52ms
step:1395/2315 train_time:84427ms step_avg:60.52ms
step:1396/2315 train_time:84488ms step_avg:60.52ms
step:1397/2315 train_time:84548ms step_avg:60.52ms
step:1398/2315 train_time:84608ms step_avg:60.52ms
step:1399/2315 train_time:84669ms step_avg:60.52ms
step:1400/2315 train_time:84730ms step_avg:60.52ms
step:1401/2315 train_time:84791ms step_avg:60.52ms
step:1402/2315 train_time:84852ms step_avg:60.52ms
step:1403/2315 train_time:84913ms step_avg:60.52ms
step:1404/2315 train_time:84974ms step_avg:60.52ms
step:1405/2315 train_time:85035ms step_avg:60.52ms
step:1406/2315 train_time:85098ms step_avg:60.52ms
step:1407/2315 train_time:85157ms step_avg:60.52ms
step:1408/2315 train_time:85218ms step_avg:60.52ms
step:1409/2315 train_time:85279ms step_avg:60.52ms
step:1410/2315 train_time:85341ms step_avg:60.53ms
step:1411/2315 train_time:85402ms step_avg:60.53ms
step:1412/2315 train_time:85463ms step_avg:60.53ms
step:1413/2315 train_time:85524ms step_avg:60.53ms
step:1414/2315 train_time:85586ms step_avg:60.53ms
step:1415/2315 train_time:85647ms step_avg:60.53ms
step:1416/2315 train_time:85708ms step_avg:60.53ms
step:1417/2315 train_time:85769ms step_avg:60.53ms
step:1418/2315 train_time:85830ms step_avg:60.53ms
step:1419/2315 train_time:85890ms step_avg:60.53ms
step:1420/2315 train_time:85952ms step_avg:60.53ms
step:1421/2315 train_time:86012ms step_avg:60.53ms
step:1422/2315 train_time:86074ms step_avg:60.53ms
step:1423/2315 train_time:86135ms step_avg:60.53ms
step:1424/2315 train_time:86195ms step_avg:60.53ms
step:1425/2315 train_time:86256ms step_avg:60.53ms
step:1426/2315 train_time:86318ms step_avg:60.53ms
step:1427/2315 train_time:86378ms step_avg:60.53ms
step:1428/2315 train_time:86439ms step_avg:60.53ms
step:1429/2315 train_time:86501ms step_avg:60.53ms
step:1430/2315 train_time:86563ms step_avg:60.53ms
step:1431/2315 train_time:86625ms step_avg:60.53ms
step:1432/2315 train_time:86685ms step_avg:60.53ms
step:1433/2315 train_time:86746ms step_avg:60.53ms
step:1434/2315 train_time:86807ms step_avg:60.53ms
step:1435/2315 train_time:86867ms step_avg:60.53ms
step:1436/2315 train_time:86929ms step_avg:60.54ms
step:1437/2315 train_time:86989ms step_avg:60.53ms
step:1438/2315 train_time:87050ms step_avg:60.54ms
step:1439/2315 train_time:87111ms step_avg:60.54ms
step:1440/2315 train_time:87172ms step_avg:60.54ms
step:1441/2315 train_time:87233ms step_avg:60.54ms
step:1442/2315 train_time:87295ms step_avg:60.54ms
step:1443/2315 train_time:87355ms step_avg:60.54ms
step:1444/2315 train_time:87417ms step_avg:60.54ms
step:1445/2315 train_time:87478ms step_avg:60.54ms
step:1446/2315 train_time:87540ms step_avg:60.54ms
step:1447/2315 train_time:87602ms step_avg:60.54ms
step:1448/2315 train_time:87664ms step_avg:60.54ms
step:1449/2315 train_time:87724ms step_avg:60.54ms
step:1450/2315 train_time:87786ms step_avg:60.54ms
step:1451/2315 train_time:87846ms step_avg:60.54ms
step:1452/2315 train_time:87908ms step_avg:60.54ms
step:1453/2315 train_time:87968ms step_avg:60.54ms
step:1454/2315 train_time:88029ms step_avg:60.54ms
step:1455/2315 train_time:88089ms step_avg:60.54ms
step:1456/2315 train_time:88150ms step_avg:60.54ms
step:1457/2315 train_time:88211ms step_avg:60.54ms
step:1458/2315 train_time:88273ms step_avg:60.54ms
step:1459/2315 train_time:88334ms step_avg:60.54ms
step:1460/2315 train_time:88395ms step_avg:60.54ms
step:1461/2315 train_time:88456ms step_avg:60.54ms
step:1462/2315 train_time:88518ms step_avg:60.55ms
step:1463/2315 train_time:88578ms step_avg:60.55ms
step:1464/2315 train_time:88639ms step_avg:60.55ms
step:1465/2315 train_time:88700ms step_avg:60.55ms
step:1466/2315 train_time:88762ms step_avg:60.55ms
step:1467/2315 train_time:88823ms step_avg:60.55ms
step:1468/2315 train_time:88885ms step_avg:60.55ms
step:1469/2315 train_time:88945ms step_avg:60.55ms
step:1470/2315 train_time:89006ms step_avg:60.55ms
step:1471/2315 train_time:89067ms step_avg:60.55ms
step:1472/2315 train_time:89128ms step_avg:60.55ms
step:1473/2315 train_time:89188ms step_avg:60.55ms
step:1474/2315 train_time:89249ms step_avg:60.55ms
step:1475/2315 train_time:89310ms step_avg:60.55ms
step:1476/2315 train_time:89371ms step_avg:60.55ms
step:1477/2315 train_time:89432ms step_avg:60.55ms
step:1478/2315 train_time:89493ms step_avg:60.55ms
step:1479/2315 train_time:89555ms step_avg:60.55ms
step:1480/2315 train_time:89616ms step_avg:60.55ms
step:1481/2315 train_time:89677ms step_avg:60.55ms
step:1482/2315 train_time:89739ms step_avg:60.55ms
step:1483/2315 train_time:89799ms step_avg:60.55ms
step:1484/2315 train_time:89860ms step_avg:60.55ms
step:1485/2315 train_time:89921ms step_avg:60.55ms
step:1486/2315 train_time:89981ms step_avg:60.55ms
step:1487/2315 train_time:90043ms step_avg:60.55ms
step:1488/2315 train_time:90104ms step_avg:60.55ms
step:1489/2315 train_time:90165ms step_avg:60.55ms
step:1490/2315 train_time:90227ms step_avg:60.56ms
step:1491/2315 train_time:90288ms step_avg:60.56ms
step:1492/2315 train_time:90349ms step_avg:60.56ms
step:1493/2315 train_time:90409ms step_avg:60.56ms
step:1494/2315 train_time:90470ms step_avg:60.56ms
step:1495/2315 train_time:90530ms step_avg:60.56ms
step:1496/2315 train_time:90592ms step_avg:60.56ms
step:1497/2315 train_time:90652ms step_avg:60.56ms
step:1498/2315 train_time:90714ms step_avg:60.56ms
step:1499/2315 train_time:90776ms step_avg:60.56ms
step:1500/2315 train_time:90837ms step_avg:60.56ms
step:1500/2315 val_loss:3.4470 train_time:90900ms step_avg:60.60ms
step:1501/2315 train_time:90924ms step_avg:60.58ms
step:1502/2315 train_time:90960ms step_avg:60.56ms
step:1503/2315 train_time:91022ms step_avg:60.56ms
step:1504/2315 train_time:91086ms step_avg:60.56ms
step:1505/2315 train_time:91147ms step_avg:60.56ms
step:1506/2315 train_time:91208ms step_avg:60.56ms
step:1507/2315 train_time:91268ms step_avg:60.56ms
step:1508/2315 train_time:91328ms step_avg:60.56ms
step:1509/2315 train_time:91388ms step_avg:60.56ms
step:1510/2315 train_time:91449ms step_avg:60.56ms
step:1511/2315 train_time:91510ms step_avg:60.56ms
step:1512/2315 train_time:91571ms step_avg:60.56ms
step:1513/2315 train_time:91630ms step_avg:60.56ms
step:1514/2315 train_time:91691ms step_avg:60.56ms
step:1515/2315 train_time:91751ms step_avg:60.56ms
step:1516/2315 train_time:91813ms step_avg:60.56ms
step:1517/2315 train_time:91875ms step_avg:60.56ms
step:1518/2315 train_time:91938ms step_avg:60.57ms
step:1519/2315 train_time:92000ms step_avg:60.57ms
step:1520/2315 train_time:92062ms step_avg:60.57ms
step:1521/2315 train_time:92124ms step_avg:60.57ms
step:1522/2315 train_time:92185ms step_avg:60.57ms
step:1523/2315 train_time:92246ms step_avg:60.57ms
step:1524/2315 train_time:92309ms step_avg:60.57ms
step:1525/2315 train_time:92370ms step_avg:60.57ms
step:1526/2315 train_time:92431ms step_avg:60.57ms
step:1527/2315 train_time:92492ms step_avg:60.57ms
step:1528/2315 train_time:92553ms step_avg:60.57ms
step:1529/2315 train_time:92613ms step_avg:60.57ms
step:1530/2315 train_time:92675ms step_avg:60.57ms
step:1531/2315 train_time:92736ms step_avg:60.57ms
step:1532/2315 train_time:92798ms step_avg:60.57ms
step:1533/2315 train_time:92859ms step_avg:60.57ms
step:1534/2315 train_time:92921ms step_avg:60.57ms
step:1535/2315 train_time:92983ms step_avg:60.57ms
step:1536/2315 train_time:93045ms step_avg:60.58ms
step:1537/2315 train_time:93106ms step_avg:60.58ms
step:1538/2315 train_time:93168ms step_avg:60.58ms
step:1539/2315 train_time:93228ms step_avg:60.58ms
step:1540/2315 train_time:93290ms step_avg:60.58ms
step:1541/2315 train_time:93351ms step_avg:60.58ms
step:1542/2315 train_time:93413ms step_avg:60.58ms
step:1543/2315 train_time:93474ms step_avg:60.58ms
step:1544/2315 train_time:93535ms step_avg:60.58ms
step:1545/2315 train_time:93596ms step_avg:60.58ms
step:1546/2315 train_time:93657ms step_avg:60.58ms
step:1547/2315 train_time:93717ms step_avg:60.58ms
step:1548/2315 train_time:93778ms step_avg:60.58ms
step:1549/2315 train_time:93839ms step_avg:60.58ms
step:1550/2315 train_time:93901ms step_avg:60.58ms
step:1551/2315 train_time:93962ms step_avg:60.58ms
step:1552/2315 train_time:94024ms step_avg:60.58ms
step:1553/2315 train_time:94085ms step_avg:60.58ms
step:1554/2315 train_time:94147ms step_avg:60.58ms
step:1555/2315 train_time:94208ms step_avg:60.58ms
step:1556/2315 train_time:94269ms step_avg:60.58ms
step:1557/2315 train_time:94331ms step_avg:60.59ms
step:1558/2315 train_time:94392ms step_avg:60.59ms
step:1559/2315 train_time:94453ms step_avg:60.59ms
step:1560/2315 train_time:94515ms step_avg:60.59ms
step:1561/2315 train_time:94575ms step_avg:60.59ms
step:1562/2315 train_time:94637ms step_avg:60.59ms
step:1563/2315 train_time:94698ms step_avg:60.59ms
step:1564/2315 train_time:94759ms step_avg:60.59ms
step:1565/2315 train_time:94820ms step_avg:60.59ms
step:1566/2315 train_time:94882ms step_avg:60.59ms
step:1567/2315 train_time:94943ms step_avg:60.59ms
step:1568/2315 train_time:95005ms step_avg:60.59ms
step:1569/2315 train_time:95066ms step_avg:60.59ms
step:1570/2315 train_time:95128ms step_avg:60.59ms
step:1571/2315 train_time:95189ms step_avg:60.59ms
step:1572/2315 train_time:95250ms step_avg:60.59ms
step:1573/2315 train_time:95311ms step_avg:60.59ms
step:1574/2315 train_time:95373ms step_avg:60.59ms
step:1575/2315 train_time:95435ms step_avg:60.59ms
step:1576/2315 train_time:95496ms step_avg:60.59ms
step:1577/2315 train_time:95557ms step_avg:60.59ms
step:1578/2315 train_time:95618ms step_avg:60.59ms
step:1579/2315 train_time:95679ms step_avg:60.59ms
step:1580/2315 train_time:95741ms step_avg:60.60ms
step:1581/2315 train_time:95801ms step_avg:60.60ms
step:1582/2315 train_time:95863ms step_avg:60.60ms
step:1583/2315 train_time:95923ms step_avg:60.60ms
step:1584/2315 train_time:95986ms step_avg:60.60ms
step:1585/2315 train_time:96046ms step_avg:60.60ms
step:1586/2315 train_time:96108ms step_avg:60.60ms
step:1587/2315 train_time:96169ms step_avg:60.60ms
step:1588/2315 train_time:96231ms step_avg:60.60ms
step:1589/2315 train_time:96292ms step_avg:60.60ms
step:1590/2315 train_time:96354ms step_avg:60.60ms
step:1591/2315 train_time:96416ms step_avg:60.60ms
step:1592/2315 train_time:96477ms step_avg:60.60ms
step:1593/2315 train_time:96538ms step_avg:60.60ms
step:1594/2315 train_time:96599ms step_avg:60.60ms
step:1595/2315 train_time:96660ms step_avg:60.60ms
step:1596/2315 train_time:96722ms step_avg:60.60ms
step:1597/2315 train_time:96783ms step_avg:60.60ms
step:1598/2315 train_time:96844ms step_avg:60.60ms
step:1599/2315 train_time:96905ms step_avg:60.60ms
step:1600/2315 train_time:96966ms step_avg:60.60ms
step:1601/2315 train_time:97027ms step_avg:60.60ms
step:1602/2315 train_time:97088ms step_avg:60.60ms
step:1603/2315 train_time:97150ms step_avg:60.60ms
step:1604/2315 train_time:97212ms step_avg:60.61ms
step:1605/2315 train_time:97272ms step_avg:60.61ms
step:1606/2315 train_time:97334ms step_avg:60.61ms
step:1607/2315 train_time:97394ms step_avg:60.61ms
step:1608/2315 train_time:97456ms step_avg:60.61ms
step:1609/2315 train_time:97517ms step_avg:60.61ms
step:1610/2315 train_time:97578ms step_avg:60.61ms
step:1611/2315 train_time:97639ms step_avg:60.61ms
step:1612/2315 train_time:97700ms step_avg:60.61ms
step:1613/2315 train_time:97761ms step_avg:60.61ms
step:1614/2315 train_time:97822ms step_avg:60.61ms
step:1615/2315 train_time:97883ms step_avg:60.61ms
step:1616/2315 train_time:97944ms step_avg:60.61ms
step:1617/2315 train_time:98005ms step_avg:60.61ms
step:1618/2315 train_time:98067ms step_avg:60.61ms
step:1619/2315 train_time:98128ms step_avg:60.61ms
step:1620/2315 train_time:98189ms step_avg:60.61ms
step:1621/2315 train_time:98250ms step_avg:60.61ms
step:1622/2315 train_time:98311ms step_avg:60.61ms
step:1623/2315 train_time:98372ms step_avg:60.61ms
step:1624/2315 train_time:98434ms step_avg:60.61ms
step:1625/2315 train_time:98495ms step_avg:60.61ms
step:1626/2315 train_time:98557ms step_avg:60.61ms
step:1627/2315 train_time:98617ms step_avg:60.61ms
step:1628/2315 train_time:98679ms step_avg:60.61ms
step:1629/2315 train_time:98740ms step_avg:60.61ms
step:1630/2315 train_time:98801ms step_avg:60.61ms
step:1631/2315 train_time:98862ms step_avg:60.61ms
step:1632/2315 train_time:98923ms step_avg:60.61ms
step:1633/2315 train_time:98984ms step_avg:60.61ms
step:1634/2315 train_time:99047ms step_avg:60.62ms
step:1635/2315 train_time:99106ms step_avg:60.62ms
step:1636/2315 train_time:99167ms step_avg:60.62ms
step:1637/2315 train_time:99229ms step_avg:60.62ms
step:1638/2315 train_time:99290ms step_avg:60.62ms
step:1639/2315 train_time:99351ms step_avg:60.62ms
step:1640/2315 train_time:99413ms step_avg:60.62ms
step:1641/2315 train_time:99474ms step_avg:60.62ms
step:1642/2315 train_time:99536ms step_avg:60.62ms
step:1643/2315 train_time:99597ms step_avg:60.62ms
step:1644/2315 train_time:99659ms step_avg:60.62ms
step:1645/2315 train_time:99720ms step_avg:60.62ms
step:1646/2315 train_time:99782ms step_avg:60.62ms
step:1647/2315 train_time:99843ms step_avg:60.62ms
step:1648/2315 train_time:99904ms step_avg:60.62ms
step:1649/2315 train_time:99965ms step_avg:60.62ms
step:1650/2315 train_time:100027ms step_avg:60.62ms
step:1651/2315 train_time:100088ms step_avg:60.62ms
step:1652/2315 train_time:100150ms step_avg:60.62ms
step:1653/2315 train_time:100210ms step_avg:60.62ms
step:1654/2315 train_time:100272ms step_avg:60.62ms
step:1655/2315 train_time:100334ms step_avg:60.62ms
step:1656/2315 train_time:100395ms step_avg:60.63ms
step:1657/2315 train_time:100457ms step_avg:60.63ms
step:1658/2315 train_time:100518ms step_avg:60.63ms
step:1659/2315 train_time:100579ms step_avg:60.63ms
step:1660/2315 train_time:100641ms step_avg:60.63ms
step:1661/2315 train_time:100702ms step_avg:60.63ms
step:1662/2315 train_time:100762ms step_avg:60.63ms
step:1663/2315 train_time:100823ms step_avg:60.63ms
step:1664/2315 train_time:100885ms step_avg:60.63ms
step:1665/2315 train_time:100945ms step_avg:60.63ms
step:1666/2315 train_time:101006ms step_avg:60.63ms
step:1667/2315 train_time:101068ms step_avg:60.63ms
step:1668/2315 train_time:101130ms step_avg:60.63ms
step:1669/2315 train_time:101190ms step_avg:60.63ms
step:1670/2315 train_time:101253ms step_avg:60.63ms
step:1671/2315 train_time:101314ms step_avg:60.63ms
step:1672/2315 train_time:101375ms step_avg:60.63ms
step:1673/2315 train_time:101437ms step_avg:60.63ms
step:1674/2315 train_time:101498ms step_avg:60.63ms
step:1675/2315 train_time:101559ms step_avg:60.63ms
step:1676/2315 train_time:101620ms step_avg:60.63ms
step:1677/2315 train_time:101681ms step_avg:60.63ms
step:1678/2315 train_time:101743ms step_avg:60.63ms
step:1679/2315 train_time:101804ms step_avg:60.63ms
step:1680/2315 train_time:101866ms step_avg:60.63ms
step:1681/2315 train_time:101927ms step_avg:60.63ms
step:1682/2315 train_time:101989ms step_avg:60.64ms
step:1683/2315 train_time:102050ms step_avg:60.64ms
step:1684/2315 train_time:102111ms step_avg:60.64ms
step:1685/2315 train_time:102172ms step_avg:60.64ms
step:1686/2315 train_time:102233ms step_avg:60.64ms
step:1687/2315 train_time:102295ms step_avg:60.64ms
step:1688/2315 train_time:102356ms step_avg:60.64ms
step:1689/2315 train_time:102418ms step_avg:60.64ms
step:1690/2315 train_time:102479ms step_avg:60.64ms
step:1691/2315 train_time:102540ms step_avg:60.64ms
step:1692/2315 train_time:102602ms step_avg:60.64ms
step:1693/2315 train_time:102662ms step_avg:60.64ms
step:1694/2315 train_time:102724ms step_avg:60.64ms
step:1695/2315 train_time:102785ms step_avg:60.64ms
step:1696/2315 train_time:102846ms step_avg:60.64ms
step:1697/2315 train_time:102907ms step_avg:60.64ms
step:1698/2315 train_time:102969ms step_avg:60.64ms
step:1699/2315 train_time:103030ms step_avg:60.64ms
step:1700/2315 train_time:103092ms step_avg:60.64ms
step:1701/2315 train_time:103152ms step_avg:60.64ms
step:1702/2315 train_time:103214ms step_avg:60.64ms
step:1703/2315 train_time:103275ms step_avg:60.64ms
step:1704/2315 train_time:103337ms step_avg:60.64ms
step:1705/2315 train_time:103398ms step_avg:60.64ms
step:1706/2315 train_time:103459ms step_avg:60.64ms
step:1707/2315 train_time:103520ms step_avg:60.64ms
step:1708/2315 train_time:103582ms step_avg:60.64ms
step:1709/2315 train_time:103642ms step_avg:60.64ms
step:1710/2315 train_time:103703ms step_avg:60.65ms
step:1711/2315 train_time:103764ms step_avg:60.65ms
step:1712/2315 train_time:103826ms step_avg:60.65ms
step:1713/2315 train_time:103887ms step_avg:60.65ms
step:1714/2315 train_time:103949ms step_avg:60.65ms
step:1715/2315 train_time:104010ms step_avg:60.65ms
step:1716/2315 train_time:104072ms step_avg:60.65ms
step:1717/2315 train_time:104133ms step_avg:60.65ms
step:1718/2315 train_time:104194ms step_avg:60.65ms
step:1719/2315 train_time:104255ms step_avg:60.65ms
step:1720/2315 train_time:104317ms step_avg:60.65ms
step:1721/2315 train_time:104378ms step_avg:60.65ms
step:1722/2315 train_time:104439ms step_avg:60.65ms
step:1723/2315 train_time:104501ms step_avg:60.65ms
step:1724/2315 train_time:104563ms step_avg:60.65ms
step:1725/2315 train_time:104623ms step_avg:60.65ms
step:1726/2315 train_time:104684ms step_avg:60.65ms
step:1727/2315 train_time:104745ms step_avg:60.65ms
step:1728/2315 train_time:104807ms step_avg:60.65ms
step:1729/2315 train_time:104868ms step_avg:60.65ms
step:1730/2315 train_time:104930ms step_avg:60.65ms
step:1731/2315 train_time:104990ms step_avg:60.65ms
step:1732/2315 train_time:105052ms step_avg:60.65ms
step:1733/2315 train_time:105113ms step_avg:60.65ms
step:1734/2315 train_time:105175ms step_avg:60.65ms
step:1735/2315 train_time:105236ms step_avg:60.65ms
step:1736/2315 train_time:105297ms step_avg:60.65ms
step:1737/2315 train_time:105357ms step_avg:60.65ms
step:1738/2315 train_time:105419ms step_avg:60.66ms
step:1739/2315 train_time:105480ms step_avg:60.66ms
step:1740/2315 train_time:105542ms step_avg:60.66ms
step:1741/2315 train_time:105603ms step_avg:60.66ms
step:1742/2315 train_time:105664ms step_avg:60.66ms
step:1743/2315 train_time:105725ms step_avg:60.66ms
step:1744/2315 train_time:105787ms step_avg:60.66ms
step:1745/2315 train_time:105848ms step_avg:60.66ms
step:1746/2315 train_time:105910ms step_avg:60.66ms
step:1747/2315 train_time:105971ms step_avg:60.66ms
step:1748/2315 train_time:106034ms step_avg:60.66ms
step:1749/2315 train_time:106095ms step_avg:60.66ms
step:1750/2315 train_time:106157ms step_avg:60.66ms
step:1750/2315 val_loss:3.3768 train_time:106219ms step_avg:60.70ms
step:1751/2315 train_time:106253ms step_avg:60.68ms
step:1752/2315 train_time:106284ms step_avg:60.66ms
step:1753/2315 train_time:106353ms step_avg:60.67ms
step:1754/2315 train_time:106419ms step_avg:60.67ms
step:1755/2315 train_time:106479ms step_avg:60.67ms
step:1756/2315 train_time:106539ms step_avg:60.67ms
step:1757/2315 train_time:106599ms step_avg:60.67ms
step:1758/2315 train_time:106660ms step_avg:60.67ms
step:1759/2315 train_time:106720ms step_avg:60.67ms
step:1760/2315 train_time:106781ms step_avg:60.67ms
step:1761/2315 train_time:106841ms step_avg:60.67ms
step:1762/2315 train_time:106902ms step_avg:60.67ms
step:1763/2315 train_time:106962ms step_avg:60.67ms
step:1764/2315 train_time:107022ms step_avg:60.67ms
step:1765/2315 train_time:107082ms step_avg:60.67ms
step:1766/2315 train_time:107147ms step_avg:60.67ms
step:1767/2315 train_time:107210ms step_avg:60.67ms
step:1768/2315 train_time:107273ms step_avg:60.67ms
step:1769/2315 train_time:107336ms step_avg:60.68ms
step:1770/2315 train_time:107399ms step_avg:60.68ms
step:1771/2315 train_time:107460ms step_avg:60.68ms
step:1772/2315 train_time:107521ms step_avg:60.68ms
step:1773/2315 train_time:107581ms step_avg:60.68ms
step:1774/2315 train_time:107642ms step_avg:60.68ms
step:1775/2315 train_time:107702ms step_avg:60.68ms
step:1776/2315 train_time:107764ms step_avg:60.68ms
step:1777/2315 train_time:107825ms step_avg:60.68ms
step:1778/2315 train_time:107886ms step_avg:60.68ms
step:1779/2315 train_time:107947ms step_avg:60.68ms
step:1780/2315 train_time:108008ms step_avg:60.68ms
step:1781/2315 train_time:108068ms step_avg:60.68ms
step:1782/2315 train_time:108130ms step_avg:60.68ms
step:1783/2315 train_time:108191ms step_avg:60.68ms
step:1784/2315 train_time:108254ms step_avg:60.68ms
step:1785/2315 train_time:108317ms step_avg:60.68ms
step:1786/2315 train_time:108379ms step_avg:60.68ms
step:1787/2315 train_time:108440ms step_avg:60.68ms
step:1788/2315 train_time:108502ms step_avg:60.68ms
step:1789/2315 train_time:108563ms step_avg:60.68ms
step:1790/2315 train_time:108624ms step_avg:60.68ms
step:1791/2315 train_time:108685ms step_avg:60.68ms
step:1792/2315 train_time:108746ms step_avg:60.68ms
step:1793/2315 train_time:108807ms step_avg:60.68ms
step:1794/2315 train_time:108868ms step_avg:60.68ms
step:1795/2315 train_time:108928ms step_avg:60.68ms
step:1796/2315 train_time:108988ms step_avg:60.68ms
step:1797/2315 train_time:109049ms step_avg:60.68ms
step:1798/2315 train_time:109111ms step_avg:60.68ms
step:1799/2315 train_time:109172ms step_avg:60.68ms
step:1800/2315 train_time:109235ms step_avg:60.69ms
step:1801/2315 train_time:109297ms step_avg:60.69ms
step:1802/2315 train_time:109359ms step_avg:60.69ms
step:1803/2315 train_time:109420ms step_avg:60.69ms
step:1804/2315 train_time:109482ms step_avg:60.69ms
step:1805/2315 train_time:109543ms step_avg:60.69ms
step:1806/2315 train_time:109604ms step_avg:60.69ms
step:1807/2315 train_time:109665ms step_avg:60.69ms
step:1808/2315 train_time:109727ms step_avg:60.69ms
step:1809/2315 train_time:109787ms step_avg:60.69ms
step:1810/2315 train_time:109848ms step_avg:60.69ms
step:1811/2315 train_time:109909ms step_avg:60.69ms
step:1812/2315 train_time:109970ms step_avg:60.69ms
step:1813/2315 train_time:110031ms step_avg:60.69ms
step:1814/2315 train_time:110092ms step_avg:60.69ms
step:1815/2315 train_time:110153ms step_avg:60.69ms
step:1816/2315 train_time:110216ms step_avg:60.69ms
step:1817/2315 train_time:110278ms step_avg:60.69ms
step:1818/2315 train_time:110340ms step_avg:60.69ms
step:1819/2315 train_time:110402ms step_avg:60.69ms
step:1820/2315 train_time:110462ms step_avg:60.69ms
step:1821/2315 train_time:110523ms step_avg:60.69ms
step:1822/2315 train_time:110584ms step_avg:60.69ms
step:1823/2315 train_time:110645ms step_avg:60.69ms
step:1824/2315 train_time:110707ms step_avg:60.69ms
step:1825/2315 train_time:110767ms step_avg:60.69ms
step:1826/2315 train_time:110829ms step_avg:60.69ms
step:1827/2315 train_time:110889ms step_avg:60.69ms
step:1828/2315 train_time:110951ms step_avg:60.70ms
step:1829/2315 train_time:111012ms step_avg:60.70ms
step:1830/2315 train_time:111073ms step_avg:60.70ms
step:1831/2315 train_time:111134ms step_avg:60.70ms
step:1832/2315 train_time:111196ms step_avg:60.70ms
step:1833/2315 train_time:111258ms step_avg:60.70ms
step:1834/2315 train_time:111320ms step_avg:60.70ms
step:1835/2315 train_time:111381ms step_avg:60.70ms
step:1836/2315 train_time:111443ms step_avg:60.70ms
step:1837/2315 train_time:111503ms step_avg:60.70ms
step:1838/2315 train_time:111565ms step_avg:60.70ms
step:1839/2315 train_time:111626ms step_avg:60.70ms
step:1840/2315 train_time:111687ms step_avg:60.70ms
step:1841/2315 train_time:111747ms step_avg:60.70ms
step:1842/2315 train_time:111808ms step_avg:60.70ms
step:1843/2315 train_time:111869ms step_avg:60.70ms
step:1844/2315 train_time:111930ms step_avg:60.70ms
step:1845/2315 train_time:111991ms step_avg:60.70ms
step:1846/2315 train_time:112053ms step_avg:60.70ms
step:1847/2315 train_time:112114ms step_avg:60.70ms
step:1848/2315 train_time:112176ms step_avg:60.70ms
step:1849/2315 train_time:112237ms step_avg:60.70ms
step:1850/2315 train_time:112298ms step_avg:60.70ms
step:1851/2315 train_time:112359ms step_avg:60.70ms
step:1852/2315 train_time:112421ms step_avg:60.70ms
step:1853/2315 train_time:112482ms step_avg:60.70ms
step:1854/2315 train_time:112544ms step_avg:60.70ms
step:1855/2315 train_time:112604ms step_avg:60.70ms
step:1856/2315 train_time:112666ms step_avg:60.70ms
step:1857/2315 train_time:112727ms step_avg:60.70ms
step:1858/2315 train_time:112788ms step_avg:60.70ms
step:1859/2315 train_time:112849ms step_avg:60.70ms
step:1860/2315 train_time:112911ms step_avg:60.70ms
step:1861/2315 train_time:112972ms step_avg:60.71ms
step:1862/2315 train_time:113033ms step_avg:60.71ms
step:1863/2315 train_time:113094ms step_avg:60.71ms
step:1864/2315 train_time:113156ms step_avg:60.71ms
step:1865/2315 train_time:113218ms step_avg:60.71ms
step:1866/2315 train_time:113279ms step_avg:60.71ms
step:1867/2315 train_time:113340ms step_avg:60.71ms
step:1868/2315 train_time:113402ms step_avg:60.71ms
step:1869/2315 train_time:113463ms step_avg:60.71ms
step:1870/2315 train_time:113524ms step_avg:60.71ms
step:1871/2315 train_time:113584ms step_avg:60.71ms
step:1872/2315 train_time:113646ms step_avg:60.71ms
step:1873/2315 train_time:113707ms step_avg:60.71ms
step:1874/2315 train_time:113770ms step_avg:60.71ms
step:1875/2315 train_time:113829ms step_avg:60.71ms
step:1876/2315 train_time:113891ms step_avg:60.71ms
step:1877/2315 train_time:113952ms step_avg:60.71ms
step:1878/2315 train_time:114013ms step_avg:60.71ms
step:1879/2315 train_time:114074ms step_avg:60.71ms
step:1880/2315 train_time:114136ms step_avg:60.71ms
step:1881/2315 train_time:114197ms step_avg:60.71ms
step:1882/2315 train_time:114259ms step_avg:60.71ms
step:1883/2315 train_time:114320ms step_avg:60.71ms
step:1884/2315 train_time:114382ms step_avg:60.71ms
step:1885/2315 train_time:114442ms step_avg:60.71ms
step:1886/2315 train_time:114504ms step_avg:60.71ms
step:1887/2315 train_time:114565ms step_avg:60.71ms
step:1888/2315 train_time:114626ms step_avg:60.71ms
step:1889/2315 train_time:114687ms step_avg:60.71ms
step:1890/2315 train_time:114749ms step_avg:60.71ms
step:1891/2315 train_time:114810ms step_avg:60.71ms
step:1892/2315 train_time:114872ms step_avg:60.71ms
step:1893/2315 train_time:114933ms step_avg:60.71ms
step:1894/2315 train_time:114994ms step_avg:60.71ms
step:1895/2315 train_time:115054ms step_avg:60.71ms
step:1896/2315 train_time:115117ms step_avg:60.72ms
step:1897/2315 train_time:115178ms step_avg:60.72ms
step:1898/2315 train_time:115239ms step_avg:60.72ms
step:1899/2315 train_time:115301ms step_avg:60.72ms
step:1900/2315 train_time:115362ms step_avg:60.72ms
step:1901/2315 train_time:115423ms step_avg:60.72ms
step:1902/2315 train_time:115485ms step_avg:60.72ms
step:1903/2315 train_time:115545ms step_avg:60.72ms
step:1904/2315 train_time:115606ms step_avg:60.72ms
step:1905/2315 train_time:115667ms step_avg:60.72ms
step:1906/2315 train_time:115729ms step_avg:60.72ms
step:1907/2315 train_time:115789ms step_avg:60.72ms
step:1908/2315 train_time:115851ms step_avg:60.72ms
step:1909/2315 train_time:115913ms step_avg:60.72ms
step:1910/2315 train_time:115975ms step_avg:60.72ms
step:1911/2315 train_time:116036ms step_avg:60.72ms
step:1912/2315 train_time:116097ms step_avg:60.72ms
step:1913/2315 train_time:116158ms step_avg:60.72ms
step:1914/2315 train_time:116220ms step_avg:60.72ms
step:1915/2315 train_time:116280ms step_avg:60.72ms
step:1916/2315 train_time:116342ms step_avg:60.72ms
step:1917/2315 train_time:116402ms step_avg:60.72ms
step:1918/2315 train_time:116464ms step_avg:60.72ms
step:1919/2315 train_time:116525ms step_avg:60.72ms
step:1920/2315 train_time:116586ms step_avg:60.72ms
step:1921/2315 train_time:116647ms step_avg:60.72ms
step:1922/2315 train_time:116709ms step_avg:60.72ms
step:1923/2315 train_time:116770ms step_avg:60.72ms
step:1924/2315 train_time:116831ms step_avg:60.72ms
step:1925/2315 train_time:116892ms step_avg:60.72ms
step:1926/2315 train_time:116955ms step_avg:60.72ms
step:1927/2315 train_time:117016ms step_avg:60.72ms
step:1928/2315 train_time:117078ms step_avg:60.73ms
step:1929/2315 train_time:117139ms step_avg:60.73ms
step:1930/2315 train_time:117200ms step_avg:60.73ms
step:1931/2315 train_time:117261ms step_avg:60.73ms
step:1932/2315 train_time:117323ms step_avg:60.73ms
step:1933/2315 train_time:117383ms step_avg:60.73ms
step:1934/2315 train_time:117445ms step_avg:60.73ms
step:1935/2315 train_time:117506ms step_avg:60.73ms
step:1936/2315 train_time:117568ms step_avg:60.73ms
step:1937/2315 train_time:117628ms step_avg:60.73ms
step:1938/2315 train_time:117689ms step_avg:60.73ms
step:1939/2315 train_time:117750ms step_avg:60.73ms
step:1940/2315 train_time:117812ms step_avg:60.73ms
step:1941/2315 train_time:117873ms step_avg:60.73ms
step:1942/2315 train_time:117935ms step_avg:60.73ms
step:1943/2315 train_time:117996ms step_avg:60.73ms
step:1944/2315 train_time:118058ms step_avg:60.73ms
step:1945/2315 train_time:118118ms step_avg:60.73ms
step:1946/2315 train_time:118179ms step_avg:60.73ms
step:1947/2315 train_time:118240ms step_avg:60.73ms
step:1948/2315 train_time:118301ms step_avg:60.73ms
step:1949/2315 train_time:118362ms step_avg:60.73ms
step:1950/2315 train_time:118424ms step_avg:60.73ms
step:1951/2315 train_time:118484ms step_avg:60.73ms
step:1952/2315 train_time:118546ms step_avg:60.73ms
step:1953/2315 train_time:118607ms step_avg:60.73ms
step:1954/2315 train_time:118669ms step_avg:60.73ms
step:1955/2315 train_time:118730ms step_avg:60.73ms
step:1956/2315 train_time:118792ms step_avg:60.73ms
step:1957/2315 train_time:118853ms step_avg:60.73ms
step:1958/2315 train_time:118915ms step_avg:60.73ms
step:1959/2315 train_time:118976ms step_avg:60.73ms
step:1960/2315 train_time:119038ms step_avg:60.73ms
step:1961/2315 train_time:119099ms step_avg:60.73ms
step:1962/2315 train_time:119160ms step_avg:60.73ms
step:1963/2315 train_time:119222ms step_avg:60.73ms
step:1964/2315 train_time:119283ms step_avg:60.73ms
step:1965/2315 train_time:119343ms step_avg:60.73ms
step:1966/2315 train_time:119405ms step_avg:60.74ms
step:1967/2315 train_time:119466ms step_avg:60.73ms
step:1968/2315 train_time:119528ms step_avg:60.74ms
step:1969/2315 train_time:119588ms step_avg:60.74ms
step:1970/2315 train_time:119649ms step_avg:60.74ms
step:1971/2315 train_time:119710ms step_avg:60.74ms
step:1972/2315 train_time:119772ms step_avg:60.74ms
step:1973/2315 train_time:119833ms step_avg:60.74ms
step:1974/2315 train_time:119894ms step_avg:60.74ms
step:1975/2315 train_time:119955ms step_avg:60.74ms
step:1976/2315 train_time:120017ms step_avg:60.74ms
step:1977/2315 train_time:120078ms step_avg:60.74ms
step:1978/2315 train_time:120139ms step_avg:60.74ms
step:1979/2315 train_time:120200ms step_avg:60.74ms
step:1980/2315 train_time:120261ms step_avg:60.74ms
step:1981/2315 train_time:120322ms step_avg:60.74ms
step:1982/2315 train_time:120383ms step_avg:60.74ms
step:1983/2315 train_time:120444ms step_avg:60.74ms
step:1984/2315 train_time:120505ms step_avg:60.74ms
step:1985/2315 train_time:120566ms step_avg:60.74ms
step:1986/2315 train_time:120627ms step_avg:60.74ms
step:1987/2315 train_time:120689ms step_avg:60.74ms
step:1988/2315 train_time:120751ms step_avg:60.74ms
step:1989/2315 train_time:120812ms step_avg:60.74ms
step:1990/2315 train_time:120874ms step_avg:60.74ms
step:1991/2315 train_time:120935ms step_avg:60.74ms
step:1992/2315 train_time:120996ms step_avg:60.74ms
step:1993/2315 train_time:121058ms step_avg:60.74ms
step:1994/2315 train_time:121119ms step_avg:60.74ms
step:1995/2315 train_time:121180ms step_avg:60.74ms
step:1996/2315 train_time:121241ms step_avg:60.74ms
step:1997/2315 train_time:121302ms step_avg:60.74ms
step:1998/2315 train_time:121363ms step_avg:60.74ms
step:1999/2315 train_time:121424ms step_avg:60.74ms
step:2000/2315 train_time:121485ms step_avg:60.74ms
step:2000/2315 val_loss:3.3276 train_time:121548ms step_avg:60.77ms
step:2001/2315 train_time:121568ms step_avg:60.75ms
step:2002/2315 train_time:121610ms step_avg:60.74ms
step:2003/2315 train_time:121674ms step_avg:60.75ms
step:2004/2315 train_time:121737ms step_avg:60.75ms
step:2005/2315 train_time:121798ms step_avg:60.75ms
step:2006/2315 train_time:121860ms step_avg:60.75ms
step:2007/2315 train_time:121921ms step_avg:60.75ms
step:2008/2315 train_time:121982ms step_avg:60.75ms
step:2009/2315 train_time:122042ms step_avg:60.75ms
step:2010/2315 train_time:122104ms step_avg:60.75ms
step:2011/2315 train_time:122164ms step_avg:60.75ms
step:2012/2315 train_time:122225ms step_avg:60.75ms
step:2013/2315 train_time:122286ms step_avg:60.75ms
step:2014/2315 train_time:122348ms step_avg:60.75ms
step:2015/2315 train_time:122408ms step_avg:60.75ms
step:2016/2315 train_time:122470ms step_avg:60.75ms
step:2017/2315 train_time:122532ms step_avg:60.75ms
step:2018/2315 train_time:122595ms step_avg:60.75ms
step:2019/2315 train_time:122657ms step_avg:60.75ms
step:2020/2315 train_time:122719ms step_avg:60.75ms
step:2021/2315 train_time:122781ms step_avg:60.75ms
step:2022/2315 train_time:122843ms step_avg:60.75ms
step:2023/2315 train_time:122904ms step_avg:60.75ms
step:2024/2315 train_time:122965ms step_avg:60.75ms
step:2025/2315 train_time:123025ms step_avg:60.75ms
step:2026/2315 train_time:123086ms step_avg:60.75ms
step:2027/2315 train_time:123146ms step_avg:60.75ms
step:2028/2315 train_time:123207ms step_avg:60.75ms
step:2029/2315 train_time:123267ms step_avg:60.75ms
step:2030/2315 train_time:123329ms step_avg:60.75ms
step:2031/2315 train_time:123389ms step_avg:60.75ms
step:2032/2315 train_time:123450ms step_avg:60.75ms
step:2033/2315 train_time:123512ms step_avg:60.75ms
step:2034/2315 train_time:123574ms step_avg:60.75ms
step:2035/2315 train_time:123635ms step_avg:60.75ms
step:2036/2315 train_time:123698ms step_avg:60.76ms
step:2037/2315 train_time:123759ms step_avg:60.76ms
step:2038/2315 train_time:123822ms step_avg:60.76ms
step:2039/2315 train_time:123882ms step_avg:60.76ms
step:2040/2315 train_time:123944ms step_avg:60.76ms
step:2041/2315 train_time:124004ms step_avg:60.76ms
step:2042/2315 train_time:124066ms step_avg:60.76ms
step:2043/2315 train_time:124126ms step_avg:60.76ms
step:2044/2315 train_time:124187ms step_avg:60.76ms
step:2045/2315 train_time:124248ms step_avg:60.76ms
step:2046/2315 train_time:124309ms step_avg:60.76ms
step:2047/2315 train_time:124370ms step_avg:60.76ms
step:2048/2315 train_time:124431ms step_avg:60.76ms
step:2049/2315 train_time:124493ms step_avg:60.76ms
step:2050/2315 train_time:124554ms step_avg:60.76ms
step:2051/2315 train_time:124615ms step_avg:60.76ms
step:2052/2315 train_time:124678ms step_avg:60.76ms
step:2053/2315 train_time:124739ms step_avg:60.76ms
step:2054/2315 train_time:124802ms step_avg:60.76ms
step:2055/2315 train_time:124862ms step_avg:60.76ms
step:2056/2315 train_time:124925ms step_avg:60.76ms
step:2057/2315 train_time:124985ms step_avg:60.76ms
step:2058/2315 train_time:125046ms step_avg:60.76ms
step:2059/2315 train_time:125107ms step_avg:60.76ms
step:2060/2315 train_time:125168ms step_avg:60.76ms
step:2061/2315 train_time:125229ms step_avg:60.76ms
step:2062/2315 train_time:125290ms step_avg:60.76ms
step:2063/2315 train_time:125351ms step_avg:60.76ms
step:2064/2315 train_time:125412ms step_avg:60.76ms
step:2065/2315 train_time:125473ms step_avg:60.76ms
step:2066/2315 train_time:125534ms step_avg:60.76ms
step:2067/2315 train_time:125596ms step_avg:60.76ms
step:2068/2315 train_time:125658ms step_avg:60.76ms
step:2069/2315 train_time:125719ms step_avg:60.76ms
step:2070/2315 train_time:125781ms step_avg:60.76ms
step:2071/2315 train_time:125842ms step_avg:60.76ms
step:2072/2315 train_time:125905ms step_avg:60.76ms
step:2073/2315 train_time:125965ms step_avg:60.76ms
step:2074/2315 train_time:126027ms step_avg:60.77ms
step:2075/2315 train_time:126087ms step_avg:60.76ms
step:2076/2315 train_time:126149ms step_avg:60.77ms
step:2077/2315 train_time:126209ms step_avg:60.76ms
step:2078/2315 train_time:126270ms step_avg:60.77ms
step:2079/2315 train_time:126330ms step_avg:60.77ms
step:2080/2315 train_time:126392ms step_avg:60.77ms
step:2081/2315 train_time:126453ms step_avg:60.77ms
step:2082/2315 train_time:126514ms step_avg:60.77ms
step:2083/2315 train_time:126576ms step_avg:60.77ms
step:2084/2315 train_time:126637ms step_avg:60.77ms
step:2085/2315 train_time:126698ms step_avg:60.77ms
step:2086/2315 train_time:126760ms step_avg:60.77ms
step:2087/2315 train_time:126821ms step_avg:60.77ms
step:2088/2315 train_time:126883ms step_avg:60.77ms
step:2089/2315 train_time:126945ms step_avg:60.77ms
step:2090/2315 train_time:127006ms step_avg:60.77ms
step:2091/2315 train_time:127066ms step_avg:60.77ms
step:2092/2315 train_time:127127ms step_avg:60.77ms
step:2093/2315 train_time:127188ms step_avg:60.77ms
step:2094/2315 train_time:127250ms step_avg:60.77ms
step:2095/2315 train_time:127311ms step_avg:60.77ms
step:2096/2315 train_time:127372ms step_avg:60.77ms
step:2097/2315 train_time:127433ms step_avg:60.77ms
step:2098/2315 train_time:127495ms step_avg:60.77ms
step:2099/2315 train_time:127555ms step_avg:60.77ms
step:2100/2315 train_time:127617ms step_avg:60.77ms
step:2101/2315 train_time:127678ms step_avg:60.77ms
step:2102/2315 train_time:127740ms step_avg:60.77ms
step:2103/2315 train_time:127801ms step_avg:60.77ms
step:2104/2315 train_time:127863ms step_avg:60.77ms
step:2105/2315 train_time:127925ms step_avg:60.77ms
step:2106/2315 train_time:127987ms step_avg:60.77ms
step:2107/2315 train_time:128048ms step_avg:60.77ms
step:2108/2315 train_time:128110ms step_avg:60.77ms
step:2109/2315 train_time:128171ms step_avg:60.77ms
step:2110/2315 train_time:128232ms step_avg:60.77ms
step:2111/2315 train_time:128294ms step_avg:60.77ms
step:2112/2315 train_time:128355ms step_avg:60.77ms
step:2113/2315 train_time:128416ms step_avg:60.77ms
step:2114/2315 train_time:128479ms step_avg:60.78ms
step:2115/2315 train_time:128540ms step_avg:60.78ms
step:2116/2315 train_time:128601ms step_avg:60.78ms
step:2117/2315 train_time:128661ms step_avg:60.78ms
step:2118/2315 train_time:128723ms step_avg:60.78ms
step:2119/2315 train_time:128784ms step_avg:60.78ms
step:2120/2315 train_time:128846ms step_avg:60.78ms
step:2121/2315 train_time:128907ms step_avg:60.78ms
step:2122/2315 train_time:128968ms step_avg:60.78ms
step:2123/2315 train_time:129029ms step_avg:60.78ms
step:2124/2315 train_time:129090ms step_avg:60.78ms
step:2125/2315 train_time:129150ms step_avg:60.78ms
step:2126/2315 train_time:129212ms step_avg:60.78ms
step:2127/2315 train_time:129272ms step_avg:60.78ms
step:2128/2315 train_time:129333ms step_avg:60.78ms
step:2129/2315 train_time:129395ms step_avg:60.78ms
step:2130/2315 train_time:129457ms step_avg:60.78ms
step:2131/2315 train_time:129517ms step_avg:60.78ms
step:2132/2315 train_time:129580ms step_avg:60.78ms
step:2133/2315 train_time:129641ms step_avg:60.78ms
step:2134/2315 train_time:129703ms step_avg:60.78ms
step:2135/2315 train_time:129764ms step_avg:60.78ms
step:2136/2315 train_time:129826ms step_avg:60.78ms
step:2137/2315 train_time:129886ms step_avg:60.78ms
step:2138/2315 train_time:129948ms step_avg:60.78ms
step:2139/2315 train_time:130009ms step_avg:60.78ms
step:2140/2315 train_time:130071ms step_avg:60.78ms
step:2141/2315 train_time:130132ms step_avg:60.78ms
step:2142/2315 train_time:130193ms step_avg:60.78ms
step:2143/2315 train_time:130253ms step_avg:60.78ms
step:2144/2315 train_time:130315ms step_avg:60.78ms
step:2145/2315 train_time:130376ms step_avg:60.78ms
step:2146/2315 train_time:130437ms step_avg:60.78ms
step:2147/2315 train_time:130498ms step_avg:60.78ms
step:2148/2315 train_time:130560ms step_avg:60.78ms
step:2149/2315 train_time:130621ms step_avg:60.78ms
step:2150/2315 train_time:130683ms step_avg:60.78ms
step:2151/2315 train_time:130744ms step_avg:60.78ms
step:2152/2315 train_time:130806ms step_avg:60.78ms
step:2153/2315 train_time:130867ms step_avg:60.78ms
step:2154/2315 train_time:130928ms step_avg:60.78ms
step:2155/2315 train_time:130989ms step_avg:60.78ms
step:2156/2315 train_time:131050ms step_avg:60.78ms
step:2157/2315 train_time:131111ms step_avg:60.78ms
step:2158/2315 train_time:131173ms step_avg:60.78ms
step:2159/2315 train_time:131233ms step_avg:60.78ms
step:2160/2315 train_time:131294ms step_avg:60.78ms
step:2161/2315 train_time:131355ms step_avg:60.78ms
step:2162/2315 train_time:131416ms step_avg:60.78ms
step:2163/2315 train_time:131477ms step_avg:60.78ms
step:2164/2315 train_time:131539ms step_avg:60.79ms
step:2165/2315 train_time:131600ms step_avg:60.79ms
step:2166/2315 train_time:131662ms step_avg:60.79ms
step:2167/2315 train_time:131723ms step_avg:60.79ms
step:2168/2315 train_time:131785ms step_avg:60.79ms
step:2169/2315 train_time:131846ms step_avg:60.79ms
step:2170/2315 train_time:131908ms step_avg:60.79ms
step:2171/2315 train_time:131969ms step_avg:60.79ms
step:2172/2315 train_time:132030ms step_avg:60.79ms
step:2173/2315 train_time:132091ms step_avg:60.79ms
step:2174/2315 train_time:132153ms step_avg:60.79ms
step:2175/2315 train_time:132213ms step_avg:60.79ms
step:2176/2315 train_time:132275ms step_avg:60.79ms
step:2177/2315 train_time:132336ms step_avg:60.79ms
step:2178/2315 train_time:132398ms step_avg:60.79ms
step:2179/2315 train_time:132459ms step_avg:60.79ms
step:2180/2315 train_time:132520ms step_avg:60.79ms
step:2181/2315 train_time:132581ms step_avg:60.79ms
step:2182/2315 train_time:132643ms step_avg:60.79ms
step:2183/2315 train_time:132704ms step_avg:60.79ms
step:2184/2315 train_time:132766ms step_avg:60.79ms
step:2185/2315 train_time:132826ms step_avg:60.79ms
step:2186/2315 train_time:132888ms step_avg:60.79ms
step:2187/2315 train_time:132949ms step_avg:60.79ms
step:2188/2315 train_time:133011ms step_avg:60.79ms
step:2189/2315 train_time:133072ms step_avg:60.79ms
step:2190/2315 train_time:133133ms step_avg:60.79ms
step:2191/2315 train_time:133194ms step_avg:60.79ms
step:2192/2315 train_time:133256ms step_avg:60.79ms
step:2193/2315 train_time:133317ms step_avg:60.79ms
step:2194/2315 train_time:133378ms step_avg:60.79ms
step:2195/2315 train_time:133440ms step_avg:60.79ms
step:2196/2315 train_time:133501ms step_avg:60.79ms
step:2197/2315 train_time:133563ms step_avg:60.79ms
step:2198/2315 train_time:133625ms step_avg:60.79ms
step:2199/2315 train_time:133686ms step_avg:60.79ms
step:2200/2315 train_time:133748ms step_avg:60.79ms
step:2201/2315 train_time:133809ms step_avg:60.79ms
step:2202/2315 train_time:133870ms step_avg:60.79ms
step:2203/2315 train_time:133931ms step_avg:60.79ms
step:2204/2315 train_time:133992ms step_avg:60.80ms
step:2205/2315 train_time:134053ms step_avg:60.80ms
step:2206/2315 train_time:134115ms step_avg:60.80ms
step:2207/2315 train_time:134176ms step_avg:60.80ms
step:2208/2315 train_time:134237ms step_avg:60.80ms
step:2209/2315 train_time:134298ms step_avg:60.80ms
step:2210/2315 train_time:134360ms step_avg:60.80ms
step:2211/2315 train_time:134421ms step_avg:60.80ms
step:2212/2315 train_time:134483ms step_avg:60.80ms
step:2213/2315 train_time:134544ms step_avg:60.80ms
step:2214/2315 train_time:134606ms step_avg:60.80ms
step:2215/2315 train_time:134667ms step_avg:60.80ms
step:2216/2315 train_time:134729ms step_avg:60.80ms
step:2217/2315 train_time:134790ms step_avg:60.80ms
step:2218/2315 train_time:134851ms step_avg:60.80ms
step:2219/2315 train_time:134912ms step_avg:60.80ms
step:2220/2315 train_time:134974ms step_avg:60.80ms
step:2221/2315 train_time:135035ms step_avg:60.80ms
step:2222/2315 train_time:135097ms step_avg:60.80ms
step:2223/2315 train_time:135157ms step_avg:60.80ms
step:2224/2315 train_time:135220ms step_avg:60.80ms
step:2225/2315 train_time:135280ms step_avg:60.80ms
step:2226/2315 train_time:135342ms step_avg:60.80ms
step:2227/2315 train_time:135403ms step_avg:60.80ms
step:2228/2315 train_time:135465ms step_avg:60.80ms
step:2229/2315 train_time:135526ms step_avg:60.80ms
step:2230/2315 train_time:135588ms step_avg:60.80ms
step:2231/2315 train_time:135649ms step_avg:60.80ms
step:2232/2315 train_time:135711ms step_avg:60.80ms
step:2233/2315 train_time:135771ms step_avg:60.80ms
step:2234/2315 train_time:135833ms step_avg:60.80ms
step:2235/2315 train_time:135894ms step_avg:60.80ms
step:2236/2315 train_time:135956ms step_avg:60.80ms
step:2237/2315 train_time:136017ms step_avg:60.80ms
step:2238/2315 train_time:136080ms step_avg:60.80ms
step:2239/2315 train_time:136141ms step_avg:60.80ms
step:2240/2315 train_time:136202ms step_avg:60.80ms
step:2241/2315 train_time:136262ms step_avg:60.80ms
step:2242/2315 train_time:136324ms step_avg:60.80ms
step:2243/2315 train_time:136385ms step_avg:60.80ms
step:2244/2315 train_time:136447ms step_avg:60.81ms
step:2245/2315 train_time:136508ms step_avg:60.81ms
step:2246/2315 train_time:136570ms step_avg:60.81ms
step:2247/2315 train_time:136630ms step_avg:60.81ms
step:2248/2315 train_time:136692ms step_avg:60.81ms
step:2249/2315 train_time:136753ms step_avg:60.81ms
step:2250/2315 train_time:136815ms step_avg:60.81ms
step:2250/2315 val_loss:3.2877 train_time:136878ms step_avg:60.83ms
step:2251/2315 train_time:136898ms step_avg:60.82ms
step:2252/2315 train_time:136939ms step_avg:60.81ms
step:2253/2315 train_time:137003ms step_avg:60.81ms
step:2254/2315 train_time:137065ms step_avg:60.81ms
step:2255/2315 train_time:137127ms step_avg:60.81ms
step:2256/2315 train_time:137188ms step_avg:60.81ms
step:2257/2315 train_time:137248ms step_avg:60.81ms
step:2258/2315 train_time:137310ms step_avg:60.81ms
step:2259/2315 train_time:137370ms step_avg:60.81ms
step:2260/2315 train_time:137431ms step_avg:60.81ms
step:2261/2315 train_time:137491ms step_avg:60.81ms
step:2262/2315 train_time:137552ms step_avg:60.81ms
step:2263/2315 train_time:137612ms step_avg:60.81ms
step:2264/2315 train_time:137674ms step_avg:60.81ms
step:2265/2315 train_time:137735ms step_avg:60.81ms
step:2266/2315 train_time:137798ms step_avg:60.81ms
step:2267/2315 train_time:137859ms step_avg:60.81ms
step:2268/2315 train_time:137922ms step_avg:60.81ms
step:2269/2315 train_time:137984ms step_avg:60.81ms
step:2270/2315 train_time:138047ms step_avg:60.81ms
step:2271/2315 train_time:138109ms step_avg:60.81ms
step:2272/2315 train_time:138171ms step_avg:60.81ms
step:2273/2315 train_time:138232ms step_avg:60.81ms
step:2274/2315 train_time:138293ms step_avg:60.81ms
step:2275/2315 train_time:138353ms step_avg:60.81ms
step:2276/2315 train_time:138414ms step_avg:60.81ms
step:2277/2315 train_time:138474ms step_avg:60.81ms
step:2278/2315 train_time:138535ms step_avg:60.81ms
step:2279/2315 train_time:138595ms step_avg:60.81ms
step:2280/2315 train_time:138656ms step_avg:60.81ms
step:2281/2315 train_time:138716ms step_avg:60.81ms
step:2282/2315 train_time:138778ms step_avg:60.81ms
step:2283/2315 train_time:138838ms step_avg:60.81ms
step:2284/2315 train_time:138901ms step_avg:60.81ms
step:2285/2315 train_time:138962ms step_avg:60.82ms
step:2286/2315 train_time:139025ms step_avg:60.82ms
step:2287/2315 train_time:139087ms step_avg:60.82ms
step:2288/2315 train_time:139149ms step_avg:60.82ms
step:2289/2315 train_time:139211ms step_avg:60.82ms
step:2290/2315 train_time:139273ms step_avg:60.82ms
step:2291/2315 train_time:139333ms step_avg:60.82ms
step:2292/2315 train_time:139395ms step_avg:60.82ms
step:2293/2315 train_time:139454ms step_avg:60.82ms
step:2294/2315 train_time:139516ms step_avg:60.82ms
step:2295/2315 train_time:139576ms step_avg:60.82ms
step:2296/2315 train_time:139637ms step_avg:60.82ms
step:2297/2315 train_time:139697ms step_avg:60.82ms
step:2298/2315 train_time:139759ms step_avg:60.82ms
step:2299/2315 train_time:139820ms step_avg:60.82ms
step:2300/2315 train_time:139883ms step_avg:60.82ms
step:2301/2315 train_time:139944ms step_avg:60.82ms
step:2302/2315 train_time:140007ms step_avg:60.82ms
step:2303/2315 train_time:140068ms step_avg:60.82ms
step:2304/2315 train_time:140129ms step_avg:60.82ms
step:2305/2315 train_time:140191ms step_avg:60.82ms
step:2306/2315 train_time:140252ms step_avg:60.82ms
step:2307/2315 train_time:140312ms step_avg:60.82ms
step:2308/2315 train_time:140374ms step_avg:60.82ms
step:2309/2315 train_time:140434ms step_avg:60.82ms
step:2310/2315 train_time:140495ms step_avg:60.82ms
step:2311/2315 train_time:140556ms step_avg:60.82ms
step:2312/2315 train_time:140616ms step_avg:60.82ms
step:2313/2315 train_time:140677ms step_avg:60.82ms
step:2314/2315 train_time:140739ms step_avg:60.82ms
step:2315/2315 train_time:140799ms step_avg:60.82ms
step:2315/2315 val_loss:3.2751 train_time:140862ms step_avg:60.85ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

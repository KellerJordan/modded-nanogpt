import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)


            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))



            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal
        self.group_to_param_group = {}

        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                self.group_to_param_group[param] = group

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        world_size = dist.get_world_size()
        grad = param.grad
        rank_size = grad.shape[0] // world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(), grad_slice)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']

            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
        # enable sync in the next training step for the adam optimizer
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = next(train_loader)

        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()


====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251031+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 31 16:05:24 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   37C    P0            125W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   31C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   34C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   35C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   34C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2315 train_time:77ms step_avg:77.11ms
step:2/2315 train_time:190ms step_avg:94.99ms
step:3/2315 train_time:209ms step_avg:69.51ms
step:4/2315 train_time:248ms step_avg:61.92ms
step:5/2315 train_time:305ms step_avg:61.02ms
step:6/2315 train_time:365ms step_avg:60.84ms
step:7/2315 train_time:423ms step_avg:60.49ms
step:8/2315 train_time:483ms step_avg:60.38ms
step:9/2315 train_time:542ms step_avg:60.23ms
step:10/2315 train_time:602ms step_avg:60.20ms
step:11/2315 train_time:661ms step_avg:60.08ms
step:12/2315 train_time:721ms step_avg:60.09ms
step:13/2315 train_time:781ms step_avg:60.05ms
step:14/2315 train_time:841ms step_avg:60.07ms
step:15/2315 train_time:900ms step_avg:60.02ms
step:16/2315 train_time:960ms step_avg:60.03ms
step:17/2315 train_time:1021ms step_avg:60.08ms
step:18/2315 train_time:1085ms step_avg:60.26ms
step:19/2315 train_time:1150ms step_avg:60.50ms
step:20/2315 train_time:1212ms step_avg:60.61ms
step:21/2315 train_time:1273ms step_avg:60.62ms
step:22/2315 train_time:1334ms step_avg:60.62ms
step:23/2315 train_time:1394ms step_avg:60.59ms
step:24/2315 train_time:1454ms step_avg:60.59ms
step:25/2315 train_time:1514ms step_avg:60.56ms
step:26/2315 train_time:1574ms step_avg:60.55ms
step:27/2315 train_time:1634ms step_avg:60.51ms
step:28/2315 train_time:1693ms step_avg:60.48ms
step:29/2315 train_time:1753ms step_avg:60.45ms
step:30/2315 train_time:1813ms step_avg:60.43ms
step:31/2315 train_time:1873ms step_avg:60.40ms
step:32/2315 train_time:1933ms step_avg:60.42ms
step:33/2315 train_time:1993ms step_avg:60.41ms
step:34/2315 train_time:2056ms step_avg:60.48ms
step:35/2315 train_time:2119ms step_avg:60.54ms
step:36/2315 train_time:2182ms step_avg:60.60ms
step:37/2315 train_time:2244ms step_avg:60.64ms
step:38/2315 train_time:2304ms step_avg:60.63ms
step:39/2315 train_time:2364ms step_avg:60.61ms
step:40/2315 train_time:2424ms step_avg:60.60ms
step:41/2315 train_time:2484ms step_avg:60.58ms
step:42/2315 train_time:2545ms step_avg:60.59ms
step:43/2315 train_time:2604ms step_avg:60.57ms
step:44/2315 train_time:2665ms step_avg:60.57ms
step:45/2315 train_time:2725ms step_avg:60.55ms
step:46/2315 train_time:2786ms step_avg:60.56ms
step:47/2315 train_time:2846ms step_avg:60.55ms
step:48/2315 train_time:2906ms step_avg:60.54ms
step:49/2315 train_time:2966ms step_avg:60.52ms
step:50/2315 train_time:3027ms step_avg:60.55ms
step:51/2315 train_time:3088ms step_avg:60.55ms
step:52/2315 train_time:3149ms step_avg:60.55ms
step:53/2315 train_time:3209ms step_avg:60.55ms
step:54/2315 train_time:3270ms step_avg:60.56ms
step:55/2315 train_time:3330ms step_avg:60.54ms
step:56/2315 train_time:3390ms step_avg:60.54ms
step:57/2315 train_time:3449ms step_avg:60.51ms
step:58/2315 train_time:3510ms step_avg:60.51ms
step:59/2315 train_time:3569ms step_avg:60.50ms
step:60/2315 train_time:3629ms step_avg:60.49ms
step:61/2315 train_time:3689ms step_avg:60.48ms
step:62/2315 train_time:3749ms step_avg:60.47ms
step:63/2315 train_time:3809ms step_avg:60.46ms
step:64/2315 train_time:3869ms step_avg:60.45ms
step:65/2315 train_time:3929ms step_avg:60.44ms
step:66/2315 train_time:3990ms step_avg:60.45ms
step:67/2315 train_time:4050ms step_avg:60.45ms
step:68/2315 train_time:4110ms step_avg:60.44ms
step:69/2315 train_time:4170ms step_avg:60.44ms
step:70/2315 train_time:4231ms step_avg:60.44ms
step:71/2315 train_time:4290ms step_avg:60.43ms
step:72/2315 train_time:4350ms step_avg:60.42ms
step:73/2315 train_time:4409ms step_avg:60.40ms
step:74/2315 train_time:4470ms step_avg:60.40ms
step:75/2315 train_time:4530ms step_avg:60.39ms
step:76/2315 train_time:4590ms step_avg:60.39ms
step:77/2315 train_time:4649ms step_avg:60.38ms
step:78/2315 train_time:4709ms step_avg:60.37ms
step:79/2315 train_time:4768ms step_avg:60.36ms
step:80/2315 train_time:4829ms step_avg:60.36ms
step:81/2315 train_time:4889ms step_avg:60.35ms
step:82/2315 train_time:4949ms step_avg:60.35ms
step:83/2315 train_time:5008ms step_avg:60.34ms
step:84/2315 train_time:5069ms step_avg:60.34ms
step:85/2315 train_time:5129ms step_avg:60.35ms
step:86/2315 train_time:5190ms step_avg:60.35ms
step:87/2315 train_time:5250ms step_avg:60.34ms
step:88/2315 train_time:5309ms step_avg:60.33ms
step:89/2315 train_time:5369ms step_avg:60.32ms
step:90/2315 train_time:5429ms step_avg:60.32ms
step:91/2315 train_time:5489ms step_avg:60.31ms
step:92/2315 train_time:5549ms step_avg:60.31ms
step:93/2315 train_time:5609ms step_avg:60.31ms
step:94/2315 train_time:5670ms step_avg:60.32ms
step:95/2315 train_time:5729ms step_avg:60.30ms
step:96/2315 train_time:5789ms step_avg:60.30ms
step:97/2315 train_time:5849ms step_avg:60.30ms
step:98/2315 train_time:5909ms step_avg:60.29ms
step:99/2315 train_time:5968ms step_avg:60.28ms
step:100/2315 train_time:6028ms step_avg:60.28ms
step:101/2315 train_time:6088ms step_avg:60.28ms
step:102/2315 train_time:6149ms step_avg:60.29ms
step:103/2315 train_time:6209ms step_avg:60.28ms
step:104/2315 train_time:6269ms step_avg:60.28ms
step:105/2315 train_time:6328ms step_avg:60.27ms
step:106/2315 train_time:6389ms step_avg:60.28ms
step:107/2315 train_time:6448ms step_avg:60.26ms
step:108/2315 train_time:6508ms step_avg:60.26ms
step:109/2315 train_time:6568ms step_avg:60.26ms
step:110/2315 train_time:6628ms step_avg:60.26ms
step:111/2315 train_time:6687ms step_avg:60.24ms
step:112/2315 train_time:6747ms step_avg:60.24ms
step:113/2315 train_time:6806ms step_avg:60.23ms
step:114/2315 train_time:6866ms step_avg:60.23ms
step:115/2315 train_time:6925ms step_avg:60.22ms
step:116/2315 train_time:6986ms step_avg:60.22ms
step:117/2315 train_time:7046ms step_avg:60.22ms
step:118/2315 train_time:7106ms step_avg:60.22ms
step:119/2315 train_time:7166ms step_avg:60.22ms
step:120/2315 train_time:7227ms step_avg:60.22ms
step:121/2315 train_time:7287ms step_avg:60.22ms
step:122/2315 train_time:7347ms step_avg:60.22ms
step:123/2315 train_time:7407ms step_avg:60.22ms
step:124/2315 train_time:7467ms step_avg:60.22ms
step:125/2315 train_time:7526ms step_avg:60.21ms
step:126/2315 train_time:7587ms step_avg:60.22ms
step:127/2315 train_time:7647ms step_avg:60.21ms
step:128/2315 train_time:7707ms step_avg:60.21ms
step:129/2315 train_time:7766ms step_avg:60.20ms
step:130/2315 train_time:7826ms step_avg:60.20ms
step:131/2315 train_time:7886ms step_avg:60.20ms
step:132/2315 train_time:7946ms step_avg:60.20ms
step:133/2315 train_time:8006ms step_avg:60.20ms
step:134/2315 train_time:8067ms step_avg:60.20ms
step:135/2315 train_time:8127ms step_avg:60.20ms
step:136/2315 train_time:8188ms step_avg:60.20ms
step:137/2315 train_time:8247ms step_avg:60.20ms
step:138/2315 train_time:8307ms step_avg:60.20ms
step:139/2315 train_time:8367ms step_avg:60.19ms
step:140/2315 train_time:8427ms step_avg:60.20ms
step:141/2315 train_time:8487ms step_avg:60.19ms
step:142/2315 train_time:8548ms step_avg:60.20ms
step:143/2315 train_time:8607ms step_avg:60.19ms
step:144/2315 train_time:8668ms step_avg:60.19ms
step:145/2315 train_time:8727ms step_avg:60.19ms
step:146/2315 train_time:8787ms step_avg:60.19ms
step:147/2315 train_time:8847ms step_avg:60.18ms
step:148/2315 train_time:8907ms step_avg:60.18ms
step:149/2315 train_time:8966ms step_avg:60.17ms
step:150/2315 train_time:9026ms step_avg:60.17ms
step:151/2315 train_time:9086ms step_avg:60.17ms
step:152/2315 train_time:9146ms step_avg:60.17ms
step:153/2315 train_time:9206ms step_avg:60.17ms
step:154/2315 train_time:9266ms step_avg:60.17ms
step:155/2315 train_time:9327ms step_avg:60.17ms
step:156/2315 train_time:9387ms step_avg:60.17ms
step:157/2315 train_time:9446ms step_avg:60.17ms
step:158/2315 train_time:9507ms step_avg:60.17ms
step:159/2315 train_time:9566ms step_avg:60.16ms
step:160/2315 train_time:9626ms step_avg:60.17ms
step:161/2315 train_time:9686ms step_avg:60.16ms
step:162/2315 train_time:9746ms step_avg:60.16ms
step:163/2315 train_time:9806ms step_avg:60.16ms
step:164/2315 train_time:9866ms step_avg:60.16ms
step:165/2315 train_time:9925ms step_avg:60.15ms
step:166/2315 train_time:9985ms step_avg:60.15ms
step:167/2315 train_time:10045ms step_avg:60.15ms
step:168/2315 train_time:10105ms step_avg:60.15ms
step:169/2315 train_time:10165ms step_avg:60.15ms
step:170/2315 train_time:10225ms step_avg:60.15ms
step:171/2315 train_time:10285ms step_avg:60.14ms
step:172/2315 train_time:10345ms step_avg:60.14ms
step:173/2315 train_time:10404ms step_avg:60.14ms
step:174/2315 train_time:10465ms step_avg:60.14ms
step:175/2315 train_time:10525ms step_avg:60.14ms
step:176/2315 train_time:10585ms step_avg:60.14ms
step:177/2315 train_time:10645ms step_avg:60.14ms
step:178/2315 train_time:10705ms step_avg:60.14ms
step:179/2315 train_time:10765ms step_avg:60.14ms
step:180/2315 train_time:10825ms step_avg:60.14ms
step:181/2315 train_time:10884ms step_avg:60.13ms
step:182/2315 train_time:10944ms step_avg:60.13ms
step:183/2315 train_time:11004ms step_avg:60.13ms
step:184/2315 train_time:11065ms step_avg:60.13ms
step:185/2315 train_time:11124ms step_avg:60.13ms
step:186/2315 train_time:11185ms step_avg:60.14ms
step:187/2315 train_time:11245ms step_avg:60.13ms
step:188/2315 train_time:11305ms step_avg:60.13ms
step:189/2315 train_time:11364ms step_avg:60.13ms
step:190/2315 train_time:11424ms step_avg:60.13ms
step:191/2315 train_time:11484ms step_avg:60.13ms
step:192/2315 train_time:11545ms step_avg:60.13ms
step:193/2315 train_time:11604ms step_avg:60.13ms
step:194/2315 train_time:11665ms step_avg:60.13ms
step:195/2315 train_time:11725ms step_avg:60.13ms
step:196/2315 train_time:11786ms step_avg:60.13ms
step:197/2315 train_time:11845ms step_avg:60.13ms
step:198/2315 train_time:11905ms step_avg:60.13ms
step:199/2315 train_time:11965ms step_avg:60.12ms
step:200/2315 train_time:12025ms step_avg:60.12ms
step:201/2315 train_time:12084ms step_avg:60.12ms
step:202/2315 train_time:12145ms step_avg:60.12ms
step:203/2315 train_time:12205ms step_avg:60.12ms
step:204/2315 train_time:12265ms step_avg:60.12ms
step:205/2315 train_time:12325ms step_avg:60.12ms
step:206/2315 train_time:12385ms step_avg:60.12ms
step:207/2315 train_time:12445ms step_avg:60.12ms
step:208/2315 train_time:12505ms step_avg:60.12ms
step:209/2315 train_time:12565ms step_avg:60.12ms
step:210/2315 train_time:12626ms step_avg:60.12ms
step:211/2315 train_time:12686ms step_avg:60.12ms
step:212/2315 train_time:12747ms step_avg:60.13ms
step:213/2315 train_time:12806ms step_avg:60.12ms
step:214/2315 train_time:12867ms step_avg:60.13ms
step:215/2315 train_time:12927ms step_avg:60.12ms
step:216/2315 train_time:12987ms step_avg:60.12ms
step:217/2315 train_time:13046ms step_avg:60.12ms
step:218/2315 train_time:13107ms step_avg:60.12ms
step:219/2315 train_time:13167ms step_avg:60.12ms
step:220/2315 train_time:13227ms step_avg:60.12ms
step:221/2315 train_time:13288ms step_avg:60.13ms
step:222/2315 train_time:13348ms step_avg:60.13ms
step:223/2315 train_time:13408ms step_avg:60.12ms
step:224/2315 train_time:13468ms step_avg:60.12ms
step:225/2315 train_time:13527ms step_avg:60.12ms
step:226/2315 train_time:13588ms step_avg:60.12ms
step:227/2315 train_time:13647ms step_avg:60.12ms
step:228/2315 train_time:13708ms step_avg:60.12ms
step:229/2315 train_time:13767ms step_avg:60.12ms
step:230/2315 train_time:13828ms step_avg:60.12ms
step:231/2315 train_time:13887ms step_avg:60.12ms
step:232/2315 train_time:13947ms step_avg:60.12ms
step:233/2315 train_time:14008ms step_avg:60.12ms
step:234/2315 train_time:14068ms step_avg:60.12ms
step:235/2315 train_time:14128ms step_avg:60.12ms
step:236/2315 train_time:14188ms step_avg:60.12ms
step:237/2315 train_time:14248ms step_avg:60.12ms
step:238/2315 train_time:14308ms step_avg:60.12ms
step:239/2315 train_time:14368ms step_avg:60.12ms
step:240/2315 train_time:14429ms step_avg:60.12ms
step:241/2315 train_time:14489ms step_avg:60.12ms
step:242/2315 train_time:14550ms step_avg:60.12ms
step:243/2315 train_time:14609ms step_avg:60.12ms
step:244/2315 train_time:14670ms step_avg:60.12ms
step:245/2315 train_time:14730ms step_avg:60.12ms
step:246/2315 train_time:14790ms step_avg:60.12ms
step:247/2315 train_time:14849ms step_avg:60.12ms
step:248/2315 train_time:14909ms step_avg:60.12ms
step:249/2315 train_time:14969ms step_avg:60.12ms
step:250/2315 train_time:15029ms step_avg:60.12ms
step:250/2315 val_loss:4.0648 train_time:15091ms step_avg:60.36ms
step:251/2315 train_time:15110ms step_avg:60.20ms
step:252/2315 train_time:15152ms step_avg:60.13ms
step:253/2315 train_time:15220ms step_avg:60.16ms
step:254/2315 train_time:15286ms step_avg:60.18ms
step:255/2315 train_time:15345ms step_avg:60.17ms
step:256/2315 train_time:15406ms step_avg:60.18ms
step:257/2315 train_time:15465ms step_avg:60.17ms
step:258/2315 train_time:15524ms step_avg:60.17ms
step:259/2315 train_time:15584ms step_avg:60.17ms
step:260/2315 train_time:15643ms step_avg:60.17ms
step:261/2315 train_time:15702ms step_avg:60.16ms
step:262/2315 train_time:15762ms step_avg:60.16ms
step:263/2315 train_time:15820ms step_avg:60.15ms
step:264/2315 train_time:15880ms step_avg:60.15ms
step:265/2315 train_time:15939ms step_avg:60.15ms
step:266/2315 train_time:15999ms step_avg:60.15ms
step:267/2315 train_time:16060ms step_avg:60.15ms
step:268/2315 train_time:16122ms step_avg:60.16ms
step:269/2315 train_time:16184ms step_avg:60.16ms
step:270/2315 train_time:16245ms step_avg:60.17ms
step:271/2315 train_time:16307ms step_avg:60.17ms
step:272/2315 train_time:16368ms step_avg:60.17ms
step:273/2315 train_time:16427ms step_avg:60.17ms
step:274/2315 train_time:16488ms step_avg:60.17ms
step:275/2315 train_time:16546ms step_avg:60.17ms
step:276/2315 train_time:16606ms step_avg:60.17ms
step:277/2315 train_time:16665ms step_avg:60.16ms
step:278/2315 train_time:16725ms step_avg:60.16ms
step:279/2315 train_time:16784ms step_avg:60.16ms
step:280/2315 train_time:16843ms step_avg:60.15ms
step:281/2315 train_time:16903ms step_avg:60.15ms
step:282/2315 train_time:16963ms step_avg:60.15ms
step:283/2315 train_time:17023ms step_avg:60.15ms
step:284/2315 train_time:17083ms step_avg:60.15ms
step:285/2315 train_time:17145ms step_avg:60.16ms
step:286/2315 train_time:17206ms step_avg:60.16ms
step:287/2315 train_time:17266ms step_avg:60.16ms
step:288/2315 train_time:17328ms step_avg:60.17ms
step:289/2315 train_time:17387ms step_avg:60.16ms
step:290/2315 train_time:17447ms step_avg:60.16ms
step:291/2315 train_time:17507ms step_avg:60.16ms
step:292/2315 train_time:17567ms step_avg:60.16ms
step:293/2315 train_time:17626ms step_avg:60.16ms
step:294/2315 train_time:17686ms step_avg:60.16ms
step:295/2315 train_time:17745ms step_avg:60.15ms
step:296/2315 train_time:17805ms step_avg:60.15ms
step:297/2315 train_time:17864ms step_avg:60.15ms
step:298/2315 train_time:17924ms step_avg:60.15ms
step:299/2315 train_time:17983ms step_avg:60.14ms
step:300/2315 train_time:18044ms step_avg:60.15ms
step:301/2315 train_time:18104ms step_avg:60.15ms
step:302/2315 train_time:18165ms step_avg:60.15ms
step:303/2315 train_time:18225ms step_avg:60.15ms
step:304/2315 train_time:18286ms step_avg:60.15ms
step:305/2315 train_time:18346ms step_avg:60.15ms
step:306/2315 train_time:18407ms step_avg:60.15ms
step:307/2315 train_time:18467ms step_avg:60.15ms
step:308/2315 train_time:18527ms step_avg:60.15ms
step:309/2315 train_time:18586ms step_avg:60.15ms
step:310/2315 train_time:18646ms step_avg:60.15ms
step:311/2315 train_time:18706ms step_avg:60.15ms
step:312/2315 train_time:18765ms step_avg:60.15ms
step:313/2315 train_time:18824ms step_avg:60.14ms
step:314/2315 train_time:18886ms step_avg:60.14ms
step:315/2315 train_time:18945ms step_avg:60.14ms
step:316/2315 train_time:19004ms step_avg:60.14ms
step:317/2315 train_time:19064ms step_avg:60.14ms
step:318/2315 train_time:19125ms step_avg:60.14ms
step:319/2315 train_time:19185ms step_avg:60.14ms
step:320/2315 train_time:19246ms step_avg:60.14ms
step:321/2315 train_time:19307ms step_avg:60.15ms
step:322/2315 train_time:19368ms step_avg:60.15ms
step:323/2315 train_time:19427ms step_avg:60.15ms
step:324/2315 train_time:19487ms step_avg:60.15ms
step:325/2315 train_time:19547ms step_avg:60.14ms
step:326/2315 train_time:19606ms step_avg:60.14ms
step:327/2315 train_time:19665ms step_avg:60.14ms
step:328/2315 train_time:19726ms step_avg:60.14ms
step:329/2315 train_time:19785ms step_avg:60.14ms
step:330/2315 train_time:19844ms step_avg:60.13ms
step:331/2315 train_time:19904ms step_avg:60.13ms
step:332/2315 train_time:19964ms step_avg:60.13ms
step:333/2315 train_time:20024ms step_avg:60.13ms
step:334/2315 train_time:20084ms step_avg:60.13ms
step:335/2315 train_time:20143ms step_avg:60.13ms
step:336/2315 train_time:20204ms step_avg:60.13ms
step:337/2315 train_time:20265ms step_avg:60.13ms
step:338/2315 train_time:20326ms step_avg:60.14ms
step:339/2315 train_time:20386ms step_avg:60.14ms
step:340/2315 train_time:20446ms step_avg:60.14ms
step:341/2315 train_time:20506ms step_avg:60.14ms
step:342/2315 train_time:20566ms step_avg:60.13ms
step:343/2315 train_time:20626ms step_avg:60.13ms
step:344/2315 train_time:20685ms step_avg:60.13ms
step:345/2315 train_time:20744ms step_avg:60.13ms
step:346/2315 train_time:20804ms step_avg:60.13ms
step:347/2315 train_time:20864ms step_avg:60.13ms
step:348/2315 train_time:20925ms step_avg:60.13ms
step:349/2315 train_time:20984ms step_avg:60.13ms
step:350/2315 train_time:21044ms step_avg:60.13ms
step:351/2315 train_time:21104ms step_avg:60.12ms
step:352/2315 train_time:21164ms step_avg:60.12ms
step:353/2315 train_time:21224ms step_avg:60.12ms
step:354/2315 train_time:21284ms step_avg:60.12ms
step:355/2315 train_time:21344ms step_avg:60.12ms
step:356/2315 train_time:21404ms step_avg:60.12ms
step:357/2315 train_time:21464ms step_avg:60.12ms
step:358/2315 train_time:21524ms step_avg:60.12ms
step:359/2315 train_time:21584ms step_avg:60.12ms
step:360/2315 train_time:21644ms step_avg:60.12ms
step:361/2315 train_time:21703ms step_avg:60.12ms
step:362/2315 train_time:21762ms step_avg:60.12ms
step:363/2315 train_time:21822ms step_avg:60.11ms
step:364/2315 train_time:21882ms step_avg:60.11ms
step:365/2315 train_time:21941ms step_avg:60.11ms
step:366/2315 train_time:22001ms step_avg:60.11ms
step:367/2315 train_time:22061ms step_avg:60.11ms
step:368/2315 train_time:22121ms step_avg:60.11ms
step:369/2315 train_time:22181ms step_avg:60.11ms
step:370/2315 train_time:22241ms step_avg:60.11ms
step:371/2315 train_time:22301ms step_avg:60.11ms
step:372/2315 train_time:22362ms step_avg:60.11ms
step:373/2315 train_time:22421ms step_avg:60.11ms
step:374/2315 train_time:22481ms step_avg:60.11ms
step:375/2315 train_time:22541ms step_avg:60.11ms
step:376/2315 train_time:22602ms step_avg:60.11ms
step:377/2315 train_time:22662ms step_avg:60.11ms
step:378/2315 train_time:22722ms step_avg:60.11ms
step:379/2315 train_time:22782ms step_avg:60.11ms
step:380/2315 train_time:22842ms step_avg:60.11ms
step:381/2315 train_time:22901ms step_avg:60.11ms
step:382/2315 train_time:22962ms step_avg:60.11ms
step:383/2315 train_time:23021ms step_avg:60.11ms
step:384/2315 train_time:23082ms step_avg:60.11ms
step:385/2315 train_time:23141ms step_avg:60.11ms
step:386/2315 train_time:23201ms step_avg:60.11ms
step:387/2315 train_time:23261ms step_avg:60.11ms
step:388/2315 train_time:23322ms step_avg:60.11ms
step:389/2315 train_time:23382ms step_avg:60.11ms
step:390/2315 train_time:23442ms step_avg:60.11ms
step:391/2315 train_time:23502ms step_avg:60.11ms
step:392/2315 train_time:23563ms step_avg:60.11ms
step:393/2315 train_time:23623ms step_avg:60.11ms
step:394/2315 train_time:23683ms step_avg:60.11ms
step:395/2315 train_time:23743ms step_avg:60.11ms
step:396/2315 train_time:23803ms step_avg:60.11ms
step:397/2315 train_time:23862ms step_avg:60.11ms
step:398/2315 train_time:23923ms step_avg:60.11ms
step:399/2315 train_time:23983ms step_avg:60.11ms
step:400/2315 train_time:24043ms step_avg:60.11ms
step:401/2315 train_time:24103ms step_avg:60.11ms
step:402/2315 train_time:24163ms step_avg:60.11ms
step:403/2315 train_time:24223ms step_avg:60.11ms
step:404/2315 train_time:24283ms step_avg:60.11ms
step:405/2315 train_time:24342ms step_avg:60.10ms
step:406/2315 train_time:24402ms step_avg:60.10ms
step:407/2315 train_time:24462ms step_avg:60.10ms
step:408/2315 train_time:24523ms step_avg:60.10ms
step:409/2315 train_time:24582ms step_avg:60.10ms
step:410/2315 train_time:24642ms step_avg:60.10ms
step:411/2315 train_time:24702ms step_avg:60.10ms
step:412/2315 train_time:24762ms step_avg:60.10ms
step:413/2315 train_time:24822ms step_avg:60.10ms
step:414/2315 train_time:24882ms step_avg:60.10ms
step:415/2315 train_time:24942ms step_avg:60.10ms
step:416/2315 train_time:25002ms step_avg:60.10ms
step:417/2315 train_time:25062ms step_avg:60.10ms
step:418/2315 train_time:25122ms step_avg:60.10ms
step:419/2315 train_time:25182ms step_avg:60.10ms
step:420/2315 train_time:25242ms step_avg:60.10ms
step:421/2315 train_time:25302ms step_avg:60.10ms
step:422/2315 train_time:25362ms step_avg:60.10ms
step:423/2315 train_time:25422ms step_avg:60.10ms
step:424/2315 train_time:25482ms step_avg:60.10ms
step:425/2315 train_time:25542ms step_avg:60.10ms
step:426/2315 train_time:25602ms step_avg:60.10ms
step:427/2315 train_time:25662ms step_avg:60.10ms
step:428/2315 train_time:25722ms step_avg:60.10ms
step:429/2315 train_time:25782ms step_avg:60.10ms
step:430/2315 train_time:25842ms step_avg:60.10ms
step:431/2315 train_time:25901ms step_avg:60.10ms
step:432/2315 train_time:25962ms step_avg:60.10ms
step:433/2315 train_time:26022ms step_avg:60.10ms
step:434/2315 train_time:26082ms step_avg:60.10ms
step:435/2315 train_time:26142ms step_avg:60.10ms
step:436/2315 train_time:26202ms step_avg:60.10ms
step:437/2315 train_time:26262ms step_avg:60.10ms
step:438/2315 train_time:26323ms step_avg:60.10ms
step:439/2315 train_time:26382ms step_avg:60.10ms
step:440/2315 train_time:26442ms step_avg:60.10ms
step:441/2315 train_time:26502ms step_avg:60.09ms
step:442/2315 train_time:26562ms step_avg:60.10ms
step:443/2315 train_time:26622ms step_avg:60.10ms
step:444/2315 train_time:26682ms step_avg:60.10ms
step:445/2315 train_time:26742ms step_avg:60.09ms
step:446/2315 train_time:26802ms step_avg:60.09ms
step:447/2315 train_time:26861ms step_avg:60.09ms
step:448/2315 train_time:26922ms step_avg:60.09ms
step:449/2315 train_time:26981ms step_avg:60.09ms
step:450/2315 train_time:27041ms step_avg:60.09ms
step:451/2315 train_time:27101ms step_avg:60.09ms
step:452/2315 train_time:27162ms step_avg:60.09ms
step:453/2315 train_time:27221ms step_avg:60.09ms
step:454/2315 train_time:27281ms step_avg:60.09ms
step:455/2315 train_time:27341ms step_avg:60.09ms
step:456/2315 train_time:27401ms step_avg:60.09ms
step:457/2315 train_time:27461ms step_avg:60.09ms
step:458/2315 train_time:27521ms step_avg:60.09ms
step:459/2315 train_time:27581ms step_avg:60.09ms
step:460/2315 train_time:27641ms step_avg:60.09ms
step:461/2315 train_time:27701ms step_avg:60.09ms
step:462/2315 train_time:27761ms step_avg:60.09ms
step:463/2315 train_time:27820ms step_avg:60.09ms
step:464/2315 train_time:27881ms step_avg:60.09ms
step:465/2315 train_time:27940ms step_avg:60.09ms
step:466/2315 train_time:28002ms step_avg:60.09ms
step:467/2315 train_time:28061ms step_avg:60.09ms
step:468/2315 train_time:28121ms step_avg:60.09ms
step:469/2315 train_time:28180ms step_avg:60.09ms
step:470/2315 train_time:28241ms step_avg:60.09ms
step:471/2315 train_time:28300ms step_avg:60.09ms
step:472/2315 train_time:28361ms step_avg:60.09ms
step:473/2315 train_time:28420ms step_avg:60.09ms
step:474/2315 train_time:28481ms step_avg:60.09ms
step:475/2315 train_time:28541ms step_avg:60.09ms
step:476/2315 train_time:28601ms step_avg:60.09ms
step:477/2315 train_time:28662ms step_avg:60.09ms
step:478/2315 train_time:28722ms step_avg:60.09ms
step:479/2315 train_time:28782ms step_avg:60.09ms
step:480/2315 train_time:28842ms step_avg:60.09ms
step:481/2315 train_time:28901ms step_avg:60.09ms
step:482/2315 train_time:28962ms step_avg:60.09ms
step:483/2315 train_time:29022ms step_avg:60.09ms
step:484/2315 train_time:29082ms step_avg:60.09ms
step:485/2315 train_time:29142ms step_avg:60.09ms
step:486/2315 train_time:29202ms step_avg:60.09ms
step:487/2315 train_time:29261ms step_avg:60.08ms
step:488/2315 train_time:29321ms step_avg:60.08ms
step:489/2315 train_time:29381ms step_avg:60.08ms
step:490/2315 train_time:29441ms step_avg:60.08ms
step:491/2315 train_time:29502ms step_avg:60.08ms
step:492/2315 train_time:29562ms step_avg:60.09ms
step:493/2315 train_time:29622ms step_avg:60.09ms
step:494/2315 train_time:29682ms step_avg:60.09ms
step:495/2315 train_time:29742ms step_avg:60.08ms
step:496/2315 train_time:29802ms step_avg:60.08ms
step:497/2315 train_time:29863ms step_avg:60.09ms
step:498/2315 train_time:29923ms step_avg:60.09ms
step:499/2315 train_time:29983ms step_avg:60.09ms
step:500/2315 train_time:30043ms step_avg:60.09ms
step:500/2315 val_loss:3.8065 train_time:30105ms step_avg:60.21ms
step:501/2315 train_time:30123ms step_avg:60.13ms
step:502/2315 train_time:30168ms step_avg:60.10ms
step:503/2315 train_time:30231ms step_avg:60.10ms
step:504/2315 train_time:30292ms step_avg:60.10ms
step:505/2315 train_time:30351ms step_avg:60.10ms
step:506/2315 train_time:30412ms step_avg:60.10ms
step:507/2315 train_time:30471ms step_avg:60.10ms
step:508/2315 train_time:30530ms step_avg:60.10ms
step:509/2315 train_time:30589ms step_avg:60.10ms
step:510/2315 train_time:30649ms step_avg:60.10ms
step:511/2315 train_time:30708ms step_avg:60.09ms
step:512/2315 train_time:30769ms step_avg:60.10ms
step:513/2315 train_time:30827ms step_avg:60.09ms
step:514/2315 train_time:30887ms step_avg:60.09ms
step:515/2315 train_time:30945ms step_avg:60.09ms
step:516/2315 train_time:31006ms step_avg:60.09ms
step:517/2315 train_time:31067ms step_avg:60.09ms
step:518/2315 train_time:31129ms step_avg:60.10ms
step:519/2315 train_time:31191ms step_avg:60.10ms
step:520/2315 train_time:31253ms step_avg:60.10ms
step:521/2315 train_time:31313ms step_avg:60.10ms
step:522/2315 train_time:31373ms step_avg:60.10ms
step:523/2315 train_time:31432ms step_avg:60.10ms
step:524/2315 train_time:31492ms step_avg:60.10ms
step:525/2315 train_time:31551ms step_avg:60.10ms
step:526/2315 train_time:31610ms step_avg:60.09ms
step:527/2315 train_time:31669ms step_avg:60.09ms
step:528/2315 train_time:31729ms step_avg:60.09ms
step:529/2315 train_time:31788ms step_avg:60.09ms
step:530/2315 train_time:31848ms step_avg:60.09ms
step:531/2315 train_time:31907ms step_avg:60.09ms
step:532/2315 train_time:31967ms step_avg:60.09ms
step:533/2315 train_time:32028ms step_avg:60.09ms
step:534/2315 train_time:32090ms step_avg:60.09ms
step:535/2315 train_time:32150ms step_avg:60.09ms
step:536/2315 train_time:32212ms step_avg:60.10ms
step:537/2315 train_time:32272ms step_avg:60.10ms
step:538/2315 train_time:32332ms step_avg:60.10ms
step:539/2315 train_time:32392ms step_avg:60.10ms
step:540/2315 train_time:32452ms step_avg:60.10ms
step:541/2315 train_time:32511ms step_avg:60.09ms
step:542/2315 train_time:32571ms step_avg:60.09ms
step:543/2315 train_time:32630ms step_avg:60.09ms
step:544/2315 train_time:32690ms step_avg:60.09ms
step:545/2315 train_time:32749ms step_avg:60.09ms
step:546/2315 train_time:32809ms step_avg:60.09ms
step:547/2315 train_time:32869ms step_avg:60.09ms
step:548/2315 train_time:32928ms step_avg:60.09ms
step:549/2315 train_time:32988ms step_avg:60.09ms
step:550/2315 train_time:33049ms step_avg:60.09ms
step:551/2315 train_time:33109ms step_avg:60.09ms
step:552/2315 train_time:33171ms step_avg:60.09ms
step:553/2315 train_time:33231ms step_avg:60.09ms
step:554/2315 train_time:33292ms step_avg:60.09ms
step:555/2315 train_time:33352ms step_avg:60.09ms
step:556/2315 train_time:33412ms step_avg:60.09ms
step:557/2315 train_time:33472ms step_avg:60.09ms
step:558/2315 train_time:33532ms step_avg:60.09ms
step:559/2315 train_time:33591ms step_avg:60.09ms
step:560/2315 train_time:33651ms step_avg:60.09ms
step:561/2315 train_time:33710ms step_avg:60.09ms
step:562/2315 train_time:33770ms step_avg:60.09ms
step:563/2315 train_time:33829ms step_avg:60.09ms
step:564/2315 train_time:33889ms step_avg:60.09ms
step:565/2315 train_time:33949ms step_avg:60.09ms
step:566/2315 train_time:34009ms step_avg:60.09ms
step:567/2315 train_time:34069ms step_avg:60.09ms
step:568/2315 train_time:34130ms step_avg:60.09ms
step:569/2315 train_time:34190ms step_avg:60.09ms
step:570/2315 train_time:34251ms step_avg:60.09ms
step:571/2315 train_time:34311ms step_avg:60.09ms
step:572/2315 train_time:34372ms step_avg:60.09ms
step:573/2315 train_time:34432ms step_avg:60.09ms
step:574/2315 train_time:34493ms step_avg:60.09ms
step:575/2315 train_time:34552ms step_avg:60.09ms
step:576/2315 train_time:34612ms step_avg:60.09ms
step:577/2315 train_time:34671ms step_avg:60.09ms
step:578/2315 train_time:34731ms step_avg:60.09ms
step:579/2315 train_time:34790ms step_avg:60.09ms
step:580/2315 train_time:34850ms step_avg:60.09ms
step:581/2315 train_time:34909ms step_avg:60.08ms
step:582/2315 train_time:34969ms step_avg:60.08ms
step:583/2315 train_time:35029ms step_avg:60.08ms
step:584/2315 train_time:35090ms step_avg:60.09ms
step:585/2315 train_time:35150ms step_avg:60.09ms
step:586/2315 train_time:35210ms step_avg:60.09ms
step:587/2315 train_time:35271ms step_avg:60.09ms
step:588/2315 train_time:35331ms step_avg:60.09ms
step:589/2315 train_time:35391ms step_avg:60.09ms
step:590/2315 train_time:35451ms step_avg:60.09ms
step:591/2315 train_time:35511ms step_avg:60.09ms
step:592/2315 train_time:35571ms step_avg:60.09ms
step:593/2315 train_time:35630ms step_avg:60.08ms
step:594/2315 train_time:35690ms step_avg:60.08ms
step:595/2315 train_time:35750ms step_avg:60.08ms
step:596/2315 train_time:35809ms step_avg:60.08ms
step:597/2315 train_time:35868ms step_avg:60.08ms
step:598/2315 train_time:35928ms step_avg:60.08ms
step:599/2315 train_time:35988ms step_avg:60.08ms
step:600/2315 train_time:36048ms step_avg:60.08ms
step:601/2315 train_time:36108ms step_avg:60.08ms
step:602/2315 train_time:36168ms step_avg:60.08ms
step:603/2315 train_time:36228ms step_avg:60.08ms
step:604/2315 train_time:36288ms step_avg:60.08ms
step:605/2315 train_time:36348ms step_avg:60.08ms
step:606/2315 train_time:36409ms step_avg:60.08ms
step:607/2315 train_time:36469ms step_avg:60.08ms
step:608/2315 train_time:36529ms step_avg:60.08ms
step:609/2315 train_time:36588ms step_avg:60.08ms
step:610/2315 train_time:36649ms step_avg:60.08ms
step:611/2315 train_time:36708ms step_avg:60.08ms
step:612/2315 train_time:36770ms step_avg:60.08ms
step:613/2315 train_time:36829ms step_avg:60.08ms
step:614/2315 train_time:36889ms step_avg:60.08ms
step:615/2315 train_time:36948ms step_avg:60.08ms
step:616/2315 train_time:37008ms step_avg:60.08ms
step:617/2315 train_time:37068ms step_avg:60.08ms
step:618/2315 train_time:37128ms step_avg:60.08ms
step:619/2315 train_time:37188ms step_avg:60.08ms
step:620/2315 train_time:37250ms step_avg:60.08ms
step:621/2315 train_time:37310ms step_avg:60.08ms
step:622/2315 train_time:37371ms step_avg:60.08ms
step:623/2315 train_time:37430ms step_avg:60.08ms
step:624/2315 train_time:37490ms step_avg:60.08ms
step:625/2315 train_time:37550ms step_avg:60.08ms
step:626/2315 train_time:37610ms step_avg:60.08ms
step:627/2315 train_time:37670ms step_avg:60.08ms
step:628/2315 train_time:37730ms step_avg:60.08ms
step:629/2315 train_time:37789ms step_avg:60.08ms
step:630/2315 train_time:37849ms step_avg:60.08ms
step:631/2315 train_time:37908ms step_avg:60.08ms
step:632/2315 train_time:37969ms step_avg:60.08ms
step:633/2315 train_time:38028ms step_avg:60.08ms
step:634/2315 train_time:38088ms step_avg:60.08ms
step:635/2315 train_time:38148ms step_avg:60.08ms
step:636/2315 train_time:38209ms step_avg:60.08ms
step:637/2315 train_time:38269ms step_avg:60.08ms
step:638/2315 train_time:38330ms step_avg:60.08ms
step:639/2315 train_time:38389ms step_avg:60.08ms
step:640/2315 train_time:38450ms step_avg:60.08ms
step:641/2315 train_time:38509ms step_avg:60.08ms
step:642/2315 train_time:38570ms step_avg:60.08ms
step:643/2315 train_time:38629ms step_avg:60.08ms
step:644/2315 train_time:38689ms step_avg:60.08ms
step:645/2315 train_time:38748ms step_avg:60.08ms
step:646/2315 train_time:38808ms step_avg:60.07ms
step:647/2315 train_time:38868ms step_avg:60.07ms
step:648/2315 train_time:38928ms step_avg:60.07ms
step:649/2315 train_time:38988ms step_avg:60.07ms
step:650/2315 train_time:39047ms step_avg:60.07ms
step:651/2315 train_time:39107ms step_avg:60.07ms
step:652/2315 train_time:39168ms step_avg:60.07ms
step:653/2315 train_time:39228ms step_avg:60.07ms
step:654/2315 train_time:39289ms step_avg:60.07ms
step:655/2315 train_time:39349ms step_avg:60.07ms
step:656/2315 train_time:39410ms step_avg:60.08ms
step:657/2315 train_time:39470ms step_avg:60.08ms
step:658/2315 train_time:39530ms step_avg:60.08ms
step:659/2315 train_time:39589ms step_avg:60.07ms
step:660/2315 train_time:39650ms step_avg:60.08ms
step:661/2315 train_time:39710ms step_avg:60.08ms
step:662/2315 train_time:39770ms step_avg:60.08ms
step:663/2315 train_time:39829ms step_avg:60.07ms
step:664/2315 train_time:39889ms step_avg:60.07ms
step:665/2315 train_time:39949ms step_avg:60.07ms
step:666/2315 train_time:40009ms step_avg:60.07ms
step:667/2315 train_time:40069ms step_avg:60.07ms
step:668/2315 train_time:40130ms step_avg:60.07ms
step:669/2315 train_time:40189ms step_avg:60.07ms
step:670/2315 train_time:40250ms step_avg:60.07ms
step:671/2315 train_time:40310ms step_avg:60.07ms
step:672/2315 train_time:40370ms step_avg:60.08ms
step:673/2315 train_time:40430ms step_avg:60.07ms
step:674/2315 train_time:40490ms step_avg:60.07ms
step:675/2315 train_time:40550ms step_avg:60.07ms
step:676/2315 train_time:40610ms step_avg:60.07ms
step:677/2315 train_time:40669ms step_avg:60.07ms
step:678/2315 train_time:40729ms step_avg:60.07ms
step:679/2315 train_time:40790ms step_avg:60.07ms
step:680/2315 train_time:40850ms step_avg:60.07ms
step:681/2315 train_time:40910ms step_avg:60.07ms
step:682/2315 train_time:40970ms step_avg:60.07ms
step:683/2315 train_time:41029ms step_avg:60.07ms
step:684/2315 train_time:41089ms step_avg:60.07ms
step:685/2315 train_time:41150ms step_avg:60.07ms
step:686/2315 train_time:41210ms step_avg:60.07ms
step:687/2315 train_time:41270ms step_avg:60.07ms
step:688/2315 train_time:41330ms step_avg:60.07ms
step:689/2315 train_time:41389ms step_avg:60.07ms
step:690/2315 train_time:41450ms step_avg:60.07ms
step:691/2315 train_time:41509ms step_avg:60.07ms
step:692/2315 train_time:41570ms step_avg:60.07ms
step:693/2315 train_time:41629ms step_avg:60.07ms
step:694/2315 train_time:41689ms step_avg:60.07ms
step:695/2315 train_time:41749ms step_avg:60.07ms
step:696/2315 train_time:41809ms step_avg:60.07ms
step:697/2315 train_time:41868ms step_avg:60.07ms
step:698/2315 train_time:41929ms step_avg:60.07ms
step:699/2315 train_time:41988ms step_avg:60.07ms
step:700/2315 train_time:42049ms step_avg:60.07ms
step:701/2315 train_time:42109ms step_avg:60.07ms
step:702/2315 train_time:42169ms step_avg:60.07ms
step:703/2315 train_time:42228ms step_avg:60.07ms
step:704/2315 train_time:42289ms step_avg:60.07ms
step:705/2315 train_time:42349ms step_avg:60.07ms
step:706/2315 train_time:42409ms step_avg:60.07ms
step:707/2315 train_time:42469ms step_avg:60.07ms
step:708/2315 train_time:42529ms step_avg:60.07ms
step:709/2315 train_time:42589ms step_avg:60.07ms
step:710/2315 train_time:42649ms step_avg:60.07ms
step:711/2315 train_time:42709ms step_avg:60.07ms
step:712/2315 train_time:42769ms step_avg:60.07ms
step:713/2315 train_time:42828ms step_avg:60.07ms
step:714/2315 train_time:42888ms step_avg:60.07ms
step:715/2315 train_time:42948ms step_avg:60.07ms
step:716/2315 train_time:43008ms step_avg:60.07ms
step:717/2315 train_time:43068ms step_avg:60.07ms
step:718/2315 train_time:43129ms step_avg:60.07ms
step:719/2315 train_time:43188ms step_avg:60.07ms
step:720/2315 train_time:43249ms step_avg:60.07ms
step:721/2315 train_time:43309ms step_avg:60.07ms
step:722/2315 train_time:43369ms step_avg:60.07ms
step:723/2315 train_time:43429ms step_avg:60.07ms
step:724/2315 train_time:43489ms step_avg:60.07ms
step:725/2315 train_time:43549ms step_avg:60.07ms
step:726/2315 train_time:43609ms step_avg:60.07ms
step:727/2315 train_time:43669ms step_avg:60.07ms
step:728/2315 train_time:43729ms step_avg:60.07ms
step:729/2315 train_time:43788ms step_avg:60.07ms
step:730/2315 train_time:43848ms step_avg:60.07ms
step:731/2315 train_time:43908ms step_avg:60.07ms
step:732/2315 train_time:43968ms step_avg:60.07ms
step:733/2315 train_time:44028ms step_avg:60.07ms
step:734/2315 train_time:44088ms step_avg:60.07ms
step:735/2315 train_time:44148ms step_avg:60.06ms
step:736/2315 train_time:44208ms step_avg:60.07ms
step:737/2315 train_time:44268ms step_avg:60.06ms
step:738/2315 train_time:44329ms step_avg:60.07ms
step:739/2315 train_time:44388ms step_avg:60.07ms
step:740/2315 train_time:44449ms step_avg:60.07ms
step:741/2315 train_time:44509ms step_avg:60.07ms
step:742/2315 train_time:44569ms step_avg:60.07ms
step:743/2315 train_time:44628ms step_avg:60.07ms
step:744/2315 train_time:44689ms step_avg:60.07ms
step:745/2315 train_time:44748ms step_avg:60.06ms
step:746/2315 train_time:44808ms step_avg:60.06ms
step:747/2315 train_time:44868ms step_avg:60.06ms
step:748/2315 train_time:44928ms step_avg:60.06ms
step:749/2315 train_time:44987ms step_avg:60.06ms
step:750/2315 train_time:45048ms step_avg:60.06ms
step:750/2315 val_loss:3.6810 train_time:45109ms step_avg:60.15ms
step:751/2315 train_time:45127ms step_avg:60.09ms
step:752/2315 train_time:45172ms step_avg:60.07ms
step:753/2315 train_time:45234ms step_avg:60.07ms
step:754/2315 train_time:45296ms step_avg:60.07ms
step:755/2315 train_time:45355ms step_avg:60.07ms
step:756/2315 train_time:45416ms step_avg:60.07ms
step:757/2315 train_time:45475ms step_avg:60.07ms
step:758/2315 train_time:45535ms step_avg:60.07ms
step:759/2315 train_time:45596ms step_avg:60.07ms
step:760/2315 train_time:45655ms step_avg:60.07ms
step:761/2315 train_time:45714ms step_avg:60.07ms
step:762/2315 train_time:45775ms step_avg:60.07ms
step:763/2315 train_time:45834ms step_avg:60.07ms
step:764/2315 train_time:45895ms step_avg:60.07ms
step:765/2315 train_time:45954ms step_avg:60.07ms
step:766/2315 train_time:46016ms step_avg:60.07ms
step:767/2315 train_time:46077ms step_avg:60.07ms
step:768/2315 train_time:46140ms step_avg:60.08ms
step:769/2315 train_time:46202ms step_avg:60.08ms
step:770/2315 train_time:46263ms step_avg:60.08ms
step:771/2315 train_time:46324ms step_avg:60.08ms
step:772/2315 train_time:46386ms step_avg:60.08ms
step:773/2315 train_time:46446ms step_avg:60.09ms
step:774/2315 train_time:46508ms step_avg:60.09ms
step:775/2315 train_time:46569ms step_avg:60.09ms
step:776/2315 train_time:46629ms step_avg:60.09ms
step:777/2315 train_time:46689ms step_avg:60.09ms
step:778/2315 train_time:46750ms step_avg:60.09ms
step:779/2315 train_time:46810ms step_avg:60.09ms
step:780/2315 train_time:46871ms step_avg:60.09ms
step:781/2315 train_time:46931ms step_avg:60.09ms
step:782/2315 train_time:46992ms step_avg:60.09ms
step:783/2315 train_time:47053ms step_avg:60.09ms
step:784/2315 train_time:47115ms step_avg:60.10ms
step:785/2315 train_time:47176ms step_avg:60.10ms
step:786/2315 train_time:47238ms step_avg:60.10ms
step:787/2315 train_time:47299ms step_avg:60.10ms
step:788/2315 train_time:47360ms step_avg:60.10ms
step:789/2315 train_time:47421ms step_avg:60.10ms
step:790/2315 train_time:47481ms step_avg:60.10ms
step:791/2315 train_time:47542ms step_avg:60.10ms
step:792/2315 train_time:47602ms step_avg:60.10ms
step:793/2315 train_time:47662ms step_avg:60.10ms
step:794/2315 train_time:47723ms step_avg:60.10ms
step:795/2315 train_time:47784ms step_avg:60.11ms
step:796/2315 train_time:47845ms step_avg:60.11ms
step:797/2315 train_time:47906ms step_avg:60.11ms
step:798/2315 train_time:47967ms step_avg:60.11ms
step:799/2315 train_time:48029ms step_avg:60.11ms
step:800/2315 train_time:48091ms step_avg:60.11ms
step:801/2315 train_time:48152ms step_avg:60.12ms
step:802/2315 train_time:48214ms step_avg:60.12ms
step:803/2315 train_time:48274ms step_avg:60.12ms
step:804/2315 train_time:48336ms step_avg:60.12ms
step:805/2315 train_time:48397ms step_avg:60.12ms
step:806/2315 train_time:48459ms step_avg:60.12ms
step:807/2315 train_time:48520ms step_avg:60.12ms
step:808/2315 train_time:48580ms step_avg:60.12ms
step:809/2315 train_time:48640ms step_avg:60.12ms
step:810/2315 train_time:48700ms step_avg:60.12ms
step:811/2315 train_time:48761ms step_avg:60.12ms
step:812/2315 train_time:48821ms step_avg:60.12ms
step:813/2315 train_time:48882ms step_avg:60.13ms
step:814/2315 train_time:48944ms step_avg:60.13ms
step:815/2315 train_time:49005ms step_avg:60.13ms
step:816/2315 train_time:49067ms step_avg:60.13ms
step:817/2315 train_time:49128ms step_avg:60.13ms
step:818/2315 train_time:49190ms step_avg:60.13ms
step:819/2315 train_time:49250ms step_avg:60.13ms
step:820/2315 train_time:49311ms step_avg:60.14ms
step:821/2315 train_time:49373ms step_avg:60.14ms
step:822/2315 train_time:49434ms step_avg:60.14ms
step:823/2315 train_time:49495ms step_avg:60.14ms
step:824/2315 train_time:49556ms step_avg:60.14ms
step:825/2315 train_time:49616ms step_avg:60.14ms
step:826/2315 train_time:49677ms step_avg:60.14ms
step:827/2315 train_time:49738ms step_avg:60.14ms
step:828/2315 train_time:49799ms step_avg:60.14ms
step:829/2315 train_time:49860ms step_avg:60.14ms
step:830/2315 train_time:49920ms step_avg:60.15ms
step:831/2315 train_time:49980ms step_avg:60.14ms
step:832/2315 train_time:50041ms step_avg:60.15ms
step:833/2315 train_time:50103ms step_avg:60.15ms
step:834/2315 train_time:50164ms step_avg:60.15ms
step:835/2315 train_time:50225ms step_avg:60.15ms
step:836/2315 train_time:50287ms step_avg:60.15ms
step:837/2315 train_time:50348ms step_avg:60.15ms
step:838/2315 train_time:50410ms step_avg:60.16ms
step:839/2315 train_time:50471ms step_avg:60.16ms
step:840/2315 train_time:50532ms step_avg:60.16ms
step:841/2315 train_time:50593ms step_avg:60.16ms
step:842/2315 train_time:50653ms step_avg:60.16ms
step:843/2315 train_time:50714ms step_avg:60.16ms
step:844/2315 train_time:50775ms step_avg:60.16ms
step:845/2315 train_time:50837ms step_avg:60.16ms
step:846/2315 train_time:50898ms step_avg:60.16ms
step:847/2315 train_time:50959ms step_avg:60.16ms
step:848/2315 train_time:51020ms step_avg:60.17ms
step:849/2315 train_time:51080ms step_avg:60.17ms
step:850/2315 train_time:51141ms step_avg:60.17ms
step:851/2315 train_time:51202ms step_avg:60.17ms
step:852/2315 train_time:51263ms step_avg:60.17ms
step:853/2315 train_time:51324ms step_avg:60.17ms
step:854/2315 train_time:51386ms step_avg:60.17ms
step:855/2315 train_time:51447ms step_avg:60.17ms
step:856/2315 train_time:51508ms step_avg:60.17ms
step:857/2315 train_time:51570ms step_avg:60.17ms
step:858/2315 train_time:51631ms step_avg:60.18ms
step:859/2315 train_time:51691ms step_avg:60.18ms
step:860/2315 train_time:51752ms step_avg:60.18ms
step:861/2315 train_time:51813ms step_avg:60.18ms
step:862/2315 train_time:51875ms step_avg:60.18ms
step:863/2315 train_time:51935ms step_avg:60.18ms
step:864/2315 train_time:51997ms step_avg:60.18ms
step:865/2315 train_time:52058ms step_avg:60.18ms
step:866/2315 train_time:52119ms step_avg:60.18ms
step:867/2315 train_time:52180ms step_avg:60.18ms
step:868/2315 train_time:52241ms step_avg:60.19ms
step:869/2315 train_time:52301ms step_avg:60.19ms
step:870/2315 train_time:52362ms step_avg:60.19ms
step:871/2315 train_time:52422ms step_avg:60.19ms
step:872/2315 train_time:52484ms step_avg:60.19ms
step:873/2315 train_time:52544ms step_avg:60.19ms
step:874/2315 train_time:52606ms step_avg:60.19ms
step:875/2315 train_time:52667ms step_avg:60.19ms
step:876/2315 train_time:52729ms step_avg:60.19ms
step:877/2315 train_time:52791ms step_avg:60.19ms
step:878/2315 train_time:52852ms step_avg:60.20ms
step:879/2315 train_time:52913ms step_avg:60.20ms
step:880/2315 train_time:52974ms step_avg:60.20ms
step:881/2315 train_time:53034ms step_avg:60.20ms
step:882/2315 train_time:53095ms step_avg:60.20ms
step:883/2315 train_time:53156ms step_avg:60.20ms
step:884/2315 train_time:53218ms step_avg:60.20ms
step:885/2315 train_time:53279ms step_avg:60.20ms
step:886/2315 train_time:53340ms step_avg:60.20ms
step:887/2315 train_time:53401ms step_avg:60.20ms
step:888/2315 train_time:53461ms step_avg:60.20ms
step:889/2315 train_time:53521ms step_avg:60.20ms
step:890/2315 train_time:53583ms step_avg:60.21ms
step:891/2315 train_time:53644ms step_avg:60.21ms
step:892/2315 train_time:53705ms step_avg:60.21ms
step:893/2315 train_time:53766ms step_avg:60.21ms
step:894/2315 train_time:53828ms step_avg:60.21ms
step:895/2315 train_time:53889ms step_avg:60.21ms
step:896/2315 train_time:53951ms step_avg:60.21ms
step:897/2315 train_time:54012ms step_avg:60.21ms
step:898/2315 train_time:54073ms step_avg:60.21ms
step:899/2315 train_time:54133ms step_avg:60.21ms
step:900/2315 train_time:54194ms step_avg:60.22ms
step:901/2315 train_time:54255ms step_avg:60.22ms
step:902/2315 train_time:54316ms step_avg:60.22ms
step:903/2315 train_time:54377ms step_avg:60.22ms
step:904/2315 train_time:54438ms step_avg:60.22ms
step:905/2315 train_time:54499ms step_avg:60.22ms
step:906/2315 train_time:54560ms step_avg:60.22ms
step:907/2315 train_time:54621ms step_avg:60.22ms
step:908/2315 train_time:54682ms step_avg:60.22ms
step:909/2315 train_time:54743ms step_avg:60.22ms
step:910/2315 train_time:54804ms step_avg:60.22ms
step:911/2315 train_time:54865ms step_avg:60.22ms
step:912/2315 train_time:54927ms step_avg:60.23ms
step:913/2315 train_time:54988ms step_avg:60.23ms
step:914/2315 train_time:55049ms step_avg:60.23ms
step:915/2315 train_time:55110ms step_avg:60.23ms
step:916/2315 train_time:55171ms step_avg:60.23ms
step:917/2315 train_time:55232ms step_avg:60.23ms
step:918/2315 train_time:55293ms step_avg:60.23ms
step:919/2315 train_time:55353ms step_avg:60.23ms
step:920/2315 train_time:55415ms step_avg:60.23ms
step:921/2315 train_time:55476ms step_avg:60.23ms
step:922/2315 train_time:55537ms step_avg:60.24ms
step:923/2315 train_time:55598ms step_avg:60.24ms
step:924/2315 train_time:55659ms step_avg:60.24ms
step:925/2315 train_time:55720ms step_avg:60.24ms
step:926/2315 train_time:55781ms step_avg:60.24ms
step:927/2315 train_time:55841ms step_avg:60.24ms
step:928/2315 train_time:55902ms step_avg:60.24ms
step:929/2315 train_time:55963ms step_avg:60.24ms
step:930/2315 train_time:56024ms step_avg:60.24ms
step:931/2315 train_time:56085ms step_avg:60.24ms
step:932/2315 train_time:56146ms step_avg:60.24ms
step:933/2315 train_time:56207ms step_avg:60.24ms
step:934/2315 train_time:56269ms step_avg:60.24ms
step:935/2315 train_time:56330ms step_avg:60.25ms
step:936/2315 train_time:56391ms step_avg:60.25ms
step:937/2315 train_time:56452ms step_avg:60.25ms
step:938/2315 train_time:56513ms step_avg:60.25ms
step:939/2315 train_time:56574ms step_avg:60.25ms
step:940/2315 train_time:56635ms step_avg:60.25ms
step:941/2315 train_time:56696ms step_avg:60.25ms
step:942/2315 train_time:56757ms step_avg:60.25ms
step:943/2315 train_time:56819ms step_avg:60.25ms
step:944/2315 train_time:56880ms step_avg:60.25ms
step:945/2315 train_time:56940ms step_avg:60.25ms
step:946/2315 train_time:57001ms step_avg:60.25ms
step:947/2315 train_time:57061ms step_avg:60.25ms
step:948/2315 train_time:57122ms step_avg:60.26ms
step:949/2315 train_time:57183ms step_avg:60.26ms
step:950/2315 train_time:57244ms step_avg:60.26ms
step:951/2315 train_time:57305ms step_avg:60.26ms
step:952/2315 train_time:57366ms step_avg:60.26ms
step:953/2315 train_time:57428ms step_avg:60.26ms
step:954/2315 train_time:57490ms step_avg:60.26ms
step:955/2315 train_time:57550ms step_avg:60.26ms
step:956/2315 train_time:57612ms step_avg:60.26ms
step:957/2315 train_time:57673ms step_avg:60.26ms
step:958/2315 train_time:57734ms step_avg:60.27ms
step:959/2315 train_time:57795ms step_avg:60.27ms
step:960/2315 train_time:57856ms step_avg:60.27ms
step:961/2315 train_time:57918ms step_avg:60.27ms
step:962/2315 train_time:57979ms step_avg:60.27ms
step:963/2315 train_time:58040ms step_avg:60.27ms
step:964/2315 train_time:58101ms step_avg:60.27ms
step:965/2315 train_time:58161ms step_avg:60.27ms
step:966/2315 train_time:58221ms step_avg:60.27ms
step:967/2315 train_time:58283ms step_avg:60.27ms
step:968/2315 train_time:58344ms step_avg:60.27ms
step:969/2315 train_time:58405ms step_avg:60.27ms
step:970/2315 train_time:58466ms step_avg:60.27ms
step:971/2315 train_time:58527ms step_avg:60.28ms
step:972/2315 train_time:58589ms step_avg:60.28ms
step:973/2315 train_time:58650ms step_avg:60.28ms
step:974/2315 train_time:58712ms step_avg:60.28ms
step:975/2315 train_time:58772ms step_avg:60.28ms
step:976/2315 train_time:58833ms step_avg:60.28ms
step:977/2315 train_time:58894ms step_avg:60.28ms
step:978/2315 train_time:58955ms step_avg:60.28ms
step:979/2315 train_time:59016ms step_avg:60.28ms
step:980/2315 train_time:59077ms step_avg:60.28ms
step:981/2315 train_time:59138ms step_avg:60.28ms
step:982/2315 train_time:59200ms step_avg:60.28ms
step:983/2315 train_time:59260ms step_avg:60.28ms
step:984/2315 train_time:59321ms step_avg:60.29ms
step:985/2315 train_time:59381ms step_avg:60.29ms
step:986/2315 train_time:59442ms step_avg:60.29ms
step:987/2315 train_time:59502ms step_avg:60.29ms
step:988/2315 train_time:59563ms step_avg:60.29ms
step:989/2315 train_time:59624ms step_avg:60.29ms
step:990/2315 train_time:59686ms step_avg:60.29ms
step:991/2315 train_time:59748ms step_avg:60.29ms
step:992/2315 train_time:59809ms step_avg:60.29ms
step:993/2315 train_time:59871ms step_avg:60.29ms
step:994/2315 train_time:59932ms step_avg:60.29ms
step:995/2315 train_time:59992ms step_avg:60.29ms
step:996/2315 train_time:60053ms step_avg:60.29ms
step:997/2315 train_time:60114ms step_avg:60.29ms
step:998/2315 train_time:60175ms step_avg:60.30ms
step:999/2315 train_time:60235ms step_avg:60.30ms
step:1000/2315 train_time:60297ms step_avg:60.30ms
step:1000/2315 val_loss:3.5688 train_time:60359ms step_avg:60.36ms
step:1001/2315 train_time:60377ms step_avg:60.32ms
step:1002/2315 train_time:60421ms step_avg:60.30ms
step:1003/2315 train_time:60487ms step_avg:60.31ms
step:1004/2315 train_time:60554ms step_avg:60.31ms
step:1005/2315 train_time:60615ms step_avg:60.31ms
step:1006/2315 train_time:60677ms step_avg:60.32ms
step:1007/2315 train_time:60737ms step_avg:60.32ms
step:1008/2315 train_time:60798ms step_avg:60.31ms
step:1009/2315 train_time:60857ms step_avg:60.31ms
step:1010/2315 train_time:60917ms step_avg:60.31ms
step:1011/2315 train_time:60976ms step_avg:60.31ms
step:1012/2315 train_time:61036ms step_avg:60.31ms
step:1013/2315 train_time:61096ms step_avg:60.31ms
step:1014/2315 train_time:61156ms step_avg:60.31ms
step:1015/2315 train_time:61215ms step_avg:60.31ms
step:1016/2315 train_time:61277ms step_avg:60.31ms
step:1017/2315 train_time:61339ms step_avg:60.31ms
step:1018/2315 train_time:61402ms step_avg:60.32ms
step:1019/2315 train_time:61464ms step_avg:60.32ms
step:1020/2315 train_time:61527ms step_avg:60.32ms
step:1021/2315 train_time:61588ms step_avg:60.32ms
step:1022/2315 train_time:61650ms step_avg:60.32ms
step:1023/2315 train_time:61711ms step_avg:60.32ms
step:1024/2315 train_time:61771ms step_avg:60.32ms
step:1025/2315 train_time:61832ms step_avg:60.32ms
step:1026/2315 train_time:61892ms step_avg:60.32ms
step:1027/2315 train_time:61952ms step_avg:60.32ms
step:1028/2315 train_time:62013ms step_avg:60.32ms
step:1029/2315 train_time:62073ms step_avg:60.32ms
step:1030/2315 train_time:62134ms step_avg:60.32ms
step:1031/2315 train_time:62194ms step_avg:60.32ms
step:1032/2315 train_time:62256ms step_avg:60.33ms
step:1033/2315 train_time:62317ms step_avg:60.33ms
step:1034/2315 train_time:62380ms step_avg:60.33ms
step:1035/2315 train_time:62442ms step_avg:60.33ms
step:1036/2315 train_time:62503ms step_avg:60.33ms
step:1037/2315 train_time:62564ms step_avg:60.33ms
step:1038/2315 train_time:62626ms step_avg:60.33ms
step:1039/2315 train_time:62687ms step_avg:60.33ms
step:1040/2315 train_time:62748ms step_avg:60.33ms
step:1041/2315 train_time:62809ms step_avg:60.33ms
step:1042/2315 train_time:62870ms step_avg:60.34ms
step:1043/2315 train_time:62931ms step_avg:60.34ms
step:1044/2315 train_time:62992ms step_avg:60.34ms
step:1045/2315 train_time:63052ms step_avg:60.34ms
step:1046/2315 train_time:63114ms step_avg:60.34ms
step:1047/2315 train_time:63173ms step_avg:60.34ms
step:1048/2315 train_time:63234ms step_avg:60.34ms
step:1049/2315 train_time:63296ms step_avg:60.34ms
step:1050/2315 train_time:63358ms step_avg:60.34ms
step:1051/2315 train_time:63419ms step_avg:60.34ms
step:1052/2315 train_time:63481ms step_avg:60.34ms
step:1053/2315 train_time:63542ms step_avg:60.34ms
step:1054/2315 train_time:63602ms step_avg:60.34ms
step:1055/2315 train_time:63663ms step_avg:60.34ms
step:1056/2315 train_time:63724ms step_avg:60.34ms
step:1057/2315 train_time:63785ms step_avg:60.35ms
step:1058/2315 train_time:63846ms step_avg:60.35ms
step:1059/2315 train_time:63907ms step_avg:60.35ms
step:1060/2315 train_time:63968ms step_avg:60.35ms
step:1061/2315 train_time:64029ms step_avg:60.35ms
step:1062/2315 train_time:64090ms step_avg:60.35ms
step:1063/2315 train_time:64150ms step_avg:60.35ms
step:1064/2315 train_time:64212ms step_avg:60.35ms
step:1065/2315 train_time:64273ms step_avg:60.35ms
step:1066/2315 train_time:64335ms step_avg:60.35ms
step:1067/2315 train_time:64395ms step_avg:60.35ms
step:1068/2315 train_time:64458ms step_avg:60.35ms
step:1069/2315 train_time:64519ms step_avg:60.35ms
step:1070/2315 train_time:64580ms step_avg:60.36ms
step:1071/2315 train_time:64641ms step_avg:60.36ms
step:1072/2315 train_time:64702ms step_avg:60.36ms
step:1073/2315 train_time:64762ms step_avg:60.36ms
step:1074/2315 train_time:64822ms step_avg:60.36ms
step:1075/2315 train_time:64883ms step_avg:60.36ms
step:1076/2315 train_time:64944ms step_avg:60.36ms
step:1077/2315 train_time:65005ms step_avg:60.36ms
step:1078/2315 train_time:65067ms step_avg:60.36ms
step:1079/2315 train_time:65128ms step_avg:60.36ms
step:1080/2315 train_time:65189ms step_avg:60.36ms
step:1081/2315 train_time:65250ms step_avg:60.36ms
step:1082/2315 train_time:65312ms step_avg:60.36ms
step:1083/2315 train_time:65372ms step_avg:60.36ms
step:1084/2315 train_time:65433ms step_avg:60.36ms
step:1085/2315 train_time:65494ms step_avg:60.36ms
step:1086/2315 train_time:65556ms step_avg:60.36ms
step:1087/2315 train_time:65617ms step_avg:60.37ms
step:1088/2315 train_time:65679ms step_avg:60.37ms
step:1089/2315 train_time:65740ms step_avg:60.37ms
step:1090/2315 train_time:65801ms step_avg:60.37ms
step:1091/2315 train_time:65861ms step_avg:60.37ms
step:1092/2315 train_time:65922ms step_avg:60.37ms
step:1093/2315 train_time:65983ms step_avg:60.37ms
step:1094/2315 train_time:66045ms step_avg:60.37ms
step:1095/2315 train_time:66105ms step_avg:60.37ms
step:1096/2315 train_time:66167ms step_avg:60.37ms
step:1097/2315 train_time:66228ms step_avg:60.37ms
step:1098/2315 train_time:66289ms step_avg:60.37ms
step:1099/2315 train_time:66350ms step_avg:60.37ms
step:1100/2315 train_time:66412ms step_avg:60.37ms
step:1101/2315 train_time:66473ms step_avg:60.38ms
step:1102/2315 train_time:66534ms step_avg:60.38ms
step:1103/2315 train_time:66595ms step_avg:60.38ms
step:1104/2315 train_time:66656ms step_avg:60.38ms
step:1105/2315 train_time:66717ms step_avg:60.38ms
step:1106/2315 train_time:66777ms step_avg:60.38ms
step:1107/2315 train_time:66838ms step_avg:60.38ms
step:1108/2315 train_time:66900ms step_avg:60.38ms
step:1109/2315 train_time:66960ms step_avg:60.38ms
step:1110/2315 train_time:67022ms step_avg:60.38ms
step:1111/2315 train_time:67082ms step_avg:60.38ms
step:1112/2315 train_time:67142ms step_avg:60.38ms
step:1113/2315 train_time:67203ms step_avg:60.38ms
step:1114/2315 train_time:67264ms step_avg:60.38ms
step:1115/2315 train_time:67326ms step_avg:60.38ms
step:1116/2315 train_time:67387ms step_avg:60.38ms
step:1117/2315 train_time:67448ms step_avg:60.38ms
step:1118/2315 train_time:67510ms step_avg:60.38ms
step:1119/2315 train_time:67571ms step_avg:60.39ms
step:1120/2315 train_time:67633ms step_avg:60.39ms
step:1121/2315 train_time:67693ms step_avg:60.39ms
step:1122/2315 train_time:67755ms step_avg:60.39ms
step:1123/2315 train_time:67815ms step_avg:60.39ms
step:1124/2315 train_time:67877ms step_avg:60.39ms
step:1125/2315 train_time:67938ms step_avg:60.39ms
step:1126/2315 train_time:67999ms step_avg:60.39ms
step:1127/2315 train_time:68060ms step_avg:60.39ms
step:1128/2315 train_time:68121ms step_avg:60.39ms
step:1129/2315 train_time:68182ms step_avg:60.39ms
step:1130/2315 train_time:68242ms step_avg:60.39ms
step:1131/2315 train_time:68303ms step_avg:60.39ms
step:1132/2315 train_time:68364ms step_avg:60.39ms
step:1133/2315 train_time:68425ms step_avg:60.39ms
step:1134/2315 train_time:68486ms step_avg:60.39ms
step:1135/2315 train_time:68548ms step_avg:60.39ms
step:1136/2315 train_time:68610ms step_avg:60.40ms
step:1137/2315 train_time:68671ms step_avg:60.40ms
step:1138/2315 train_time:68731ms step_avg:60.40ms
step:1139/2315 train_time:68792ms step_avg:60.40ms
step:1140/2315 train_time:68853ms step_avg:60.40ms
step:1141/2315 train_time:68914ms step_avg:60.40ms
step:1142/2315 train_time:68975ms step_avg:60.40ms
step:1143/2315 train_time:69036ms step_avg:60.40ms
step:1144/2315 train_time:69098ms step_avg:60.40ms
step:1145/2315 train_time:69159ms step_avg:60.40ms
step:1146/2315 train_time:69220ms step_avg:60.40ms
step:1147/2315 train_time:69280ms step_avg:60.40ms
step:1148/2315 train_time:69342ms step_avg:60.40ms
step:1149/2315 train_time:69403ms step_avg:60.40ms
step:1150/2315 train_time:69464ms step_avg:60.40ms
step:1151/2315 train_time:69525ms step_avg:60.40ms
step:1152/2315 train_time:69587ms step_avg:60.41ms
step:1153/2315 train_time:69648ms step_avg:60.41ms
step:1154/2315 train_time:69710ms step_avg:60.41ms
step:1155/2315 train_time:69770ms step_avg:60.41ms
step:1156/2315 train_time:69832ms step_avg:60.41ms
step:1157/2315 train_time:69892ms step_avg:60.41ms
step:1158/2315 train_time:69953ms step_avg:60.41ms
step:1159/2315 train_time:70013ms step_avg:60.41ms
step:1160/2315 train_time:70074ms step_avg:60.41ms
step:1161/2315 train_time:70135ms step_avg:60.41ms
step:1162/2315 train_time:70197ms step_avg:60.41ms
step:1163/2315 train_time:70258ms step_avg:60.41ms
step:1164/2315 train_time:70320ms step_avg:60.41ms
step:1165/2315 train_time:70380ms step_avg:60.41ms
step:1166/2315 train_time:70441ms step_avg:60.41ms
step:1167/2315 train_time:70501ms step_avg:60.41ms
step:1168/2315 train_time:70563ms step_avg:60.41ms
step:1169/2315 train_time:70624ms step_avg:60.41ms
step:1170/2315 train_time:70685ms step_avg:60.41ms
step:1171/2315 train_time:70747ms step_avg:60.42ms
step:1172/2315 train_time:70809ms step_avg:60.42ms
step:1173/2315 train_time:70870ms step_avg:60.42ms
step:1174/2315 train_time:70931ms step_avg:60.42ms
step:1175/2315 train_time:70992ms step_avg:60.42ms
step:1176/2315 train_time:71053ms step_avg:60.42ms
step:1177/2315 train_time:71114ms step_avg:60.42ms
step:1178/2315 train_time:71175ms step_avg:60.42ms
step:1179/2315 train_time:71236ms step_avg:60.42ms
step:1180/2315 train_time:71298ms step_avg:60.42ms
step:1181/2315 train_time:71359ms step_avg:60.42ms
step:1182/2315 train_time:71419ms step_avg:60.42ms
step:1183/2315 train_time:71480ms step_avg:60.42ms
step:1184/2315 train_time:71541ms step_avg:60.42ms
step:1185/2315 train_time:71601ms step_avg:60.42ms
step:1186/2315 train_time:71662ms step_avg:60.42ms
step:1187/2315 train_time:71723ms step_avg:60.42ms
step:1188/2315 train_time:71785ms step_avg:60.43ms
step:1189/2315 train_time:71847ms step_avg:60.43ms
step:1190/2315 train_time:71909ms step_avg:60.43ms
step:1191/2315 train_time:71970ms step_avg:60.43ms
step:1192/2315 train_time:72032ms step_avg:60.43ms
step:1193/2315 train_time:72092ms step_avg:60.43ms
step:1194/2315 train_time:72153ms step_avg:60.43ms
step:1195/2315 train_time:72213ms step_avg:60.43ms
step:1196/2315 train_time:72274ms step_avg:60.43ms
step:1197/2315 train_time:72335ms step_avg:60.43ms
step:1198/2315 train_time:72396ms step_avg:60.43ms
step:1199/2315 train_time:72456ms step_avg:60.43ms
step:1200/2315 train_time:72518ms step_avg:60.43ms
step:1201/2315 train_time:72579ms step_avg:60.43ms
step:1202/2315 train_time:72640ms step_avg:60.43ms
step:1203/2315 train_time:72700ms step_avg:60.43ms
step:1204/2315 train_time:72761ms step_avg:60.43ms
step:1205/2315 train_time:72821ms step_avg:60.43ms
step:1206/2315 train_time:72882ms step_avg:60.43ms
step:1207/2315 train_time:72944ms step_avg:60.43ms
step:1208/2315 train_time:73006ms step_avg:60.44ms
step:1209/2315 train_time:73067ms step_avg:60.44ms
step:1210/2315 train_time:73129ms step_avg:60.44ms
step:1211/2315 train_time:73190ms step_avg:60.44ms
step:1212/2315 train_time:73252ms step_avg:60.44ms
step:1213/2315 train_time:73313ms step_avg:60.44ms
step:1214/2315 train_time:73374ms step_avg:60.44ms
step:1215/2315 train_time:73434ms step_avg:60.44ms
step:1216/2315 train_time:73495ms step_avg:60.44ms
step:1217/2315 train_time:73557ms step_avg:60.44ms
step:1218/2315 train_time:73618ms step_avg:60.44ms
step:1219/2315 train_time:73679ms step_avg:60.44ms
step:1220/2315 train_time:73740ms step_avg:60.44ms
step:1221/2315 train_time:73800ms step_avg:60.44ms
step:1222/2315 train_time:73861ms step_avg:60.44ms
step:1223/2315 train_time:73921ms step_avg:60.44ms
step:1224/2315 train_time:73982ms step_avg:60.44ms
step:1225/2315 train_time:74042ms step_avg:60.44ms
step:1226/2315 train_time:74104ms step_avg:60.44ms
step:1227/2315 train_time:74165ms step_avg:60.44ms
step:1228/2315 train_time:74227ms step_avg:60.45ms
step:1229/2315 train_time:74289ms step_avg:60.45ms
step:1230/2315 train_time:74351ms step_avg:60.45ms
step:1231/2315 train_time:74411ms step_avg:60.45ms
step:1232/2315 train_time:74472ms step_avg:60.45ms
step:1233/2315 train_time:74533ms step_avg:60.45ms
step:1234/2315 train_time:74594ms step_avg:60.45ms
step:1235/2315 train_time:74654ms step_avg:60.45ms
step:1236/2315 train_time:74716ms step_avg:60.45ms
step:1237/2315 train_time:74778ms step_avg:60.45ms
step:1238/2315 train_time:74839ms step_avg:60.45ms
step:1239/2315 train_time:74900ms step_avg:60.45ms
step:1240/2315 train_time:74960ms step_avg:60.45ms
step:1241/2315 train_time:75021ms step_avg:60.45ms
step:1242/2315 train_time:75081ms step_avg:60.45ms
step:1243/2315 train_time:75141ms step_avg:60.45ms
step:1244/2315 train_time:75202ms step_avg:60.45ms
step:1245/2315 train_time:75264ms step_avg:60.45ms
step:1246/2315 train_time:75326ms step_avg:60.45ms
step:1247/2315 train_time:75386ms step_avg:60.45ms
step:1248/2315 train_time:75448ms step_avg:60.46ms
step:1249/2315 train_time:75510ms step_avg:60.46ms
step:1250/2315 train_time:75571ms step_avg:60.46ms
step:1250/2315 val_loss:3.5146 train_time:75634ms step_avg:60.51ms
step:1251/2315 train_time:75652ms step_avg:60.47ms
step:1252/2315 train_time:75695ms step_avg:60.46ms
step:1253/2315 train_time:75760ms step_avg:60.46ms
step:1254/2315 train_time:75824ms step_avg:60.47ms
step:1255/2315 train_time:75884ms step_avg:60.47ms
step:1256/2315 train_time:75945ms step_avg:60.47ms
step:1257/2315 train_time:76005ms step_avg:60.47ms
step:1258/2315 train_time:76066ms step_avg:60.47ms
step:1259/2315 train_time:76125ms step_avg:60.47ms
step:1260/2315 train_time:76186ms step_avg:60.46ms
step:1261/2315 train_time:76246ms step_avg:60.46ms
step:1262/2315 train_time:76306ms step_avg:60.46ms
step:1263/2315 train_time:76366ms step_avg:60.46ms
step:1264/2315 train_time:76428ms step_avg:60.46ms
step:1265/2315 train_time:76488ms step_avg:60.46ms
step:1266/2315 train_time:76549ms step_avg:60.47ms
step:1267/2315 train_time:76610ms step_avg:60.47ms
step:1268/2315 train_time:76673ms step_avg:60.47ms
step:1269/2315 train_time:76736ms step_avg:60.47ms
step:1270/2315 train_time:76799ms step_avg:60.47ms
step:1271/2315 train_time:76860ms step_avg:60.47ms
step:1272/2315 train_time:76921ms step_avg:60.47ms
step:1273/2315 train_time:76981ms step_avg:60.47ms
step:1274/2315 train_time:77042ms step_avg:60.47ms
step:1275/2315 train_time:77102ms step_avg:60.47ms
step:1276/2315 train_time:77163ms step_avg:60.47ms
step:1277/2315 train_time:77223ms step_avg:60.47ms
step:1278/2315 train_time:77284ms step_avg:60.47ms
step:1279/2315 train_time:77344ms step_avg:60.47ms
step:1280/2315 train_time:77405ms step_avg:60.47ms
step:1281/2315 train_time:77466ms step_avg:60.47ms
step:1282/2315 train_time:77528ms step_avg:60.47ms
step:1283/2315 train_time:77589ms step_avg:60.47ms
step:1284/2315 train_time:77650ms step_avg:60.48ms
step:1285/2315 train_time:77712ms step_avg:60.48ms
step:1286/2315 train_time:77774ms step_avg:60.48ms
step:1287/2315 train_time:77834ms step_avg:60.48ms
step:1288/2315 train_time:77895ms step_avg:60.48ms
step:1289/2315 train_time:77956ms step_avg:60.48ms
step:1290/2315 train_time:78017ms step_avg:60.48ms
step:1291/2315 train_time:78077ms step_avg:60.48ms
step:1292/2315 train_time:78137ms step_avg:60.48ms
step:1293/2315 train_time:78197ms step_avg:60.48ms
step:1294/2315 train_time:78258ms step_avg:60.48ms
step:1295/2315 train_time:78318ms step_avg:60.48ms
step:1296/2315 train_time:78378ms step_avg:60.48ms
step:1297/2315 train_time:78439ms step_avg:60.48ms
step:1298/2315 train_time:78501ms step_avg:60.48ms
step:1299/2315 train_time:78563ms step_avg:60.48ms
step:1300/2315 train_time:78625ms step_avg:60.48ms
step:1301/2315 train_time:78687ms step_avg:60.48ms
step:1302/2315 train_time:78749ms step_avg:60.48ms
step:1303/2315 train_time:78810ms step_avg:60.48ms
step:1304/2315 train_time:78871ms step_avg:60.48ms
step:1305/2315 train_time:78932ms step_avg:60.48ms
step:1306/2315 train_time:78993ms step_avg:60.48ms
step:1307/2315 train_time:79053ms step_avg:60.48ms
step:1308/2315 train_time:79114ms step_avg:60.48ms
step:1309/2315 train_time:79174ms step_avg:60.48ms
step:1310/2315 train_time:79235ms step_avg:60.49ms
step:1311/2315 train_time:79295ms step_avg:60.48ms
step:1312/2315 train_time:79356ms step_avg:60.48ms
step:1313/2315 train_time:79416ms step_avg:60.48ms
step:1314/2315 train_time:79477ms step_avg:60.48ms
step:1315/2315 train_time:79538ms step_avg:60.49ms
step:1316/2315 train_time:79599ms step_avg:60.49ms
step:1317/2315 train_time:79660ms step_avg:60.49ms
step:1318/2315 train_time:79721ms step_avg:60.49ms
step:1319/2315 train_time:79782ms step_avg:60.49ms
step:1320/2315 train_time:79845ms step_avg:60.49ms
step:1321/2315 train_time:79906ms step_avg:60.49ms
step:1322/2315 train_time:79967ms step_avg:60.49ms
step:1323/2315 train_time:80028ms step_avg:60.49ms
step:1324/2315 train_time:80089ms step_avg:60.49ms
step:1325/2315 train_time:80150ms step_avg:60.49ms
step:1326/2315 train_time:80211ms step_avg:60.49ms
step:1327/2315 train_time:80271ms step_avg:60.49ms
step:1328/2315 train_time:80332ms step_avg:60.49ms
step:1329/2315 train_time:80393ms step_avg:60.49ms
step:1330/2315 train_time:80453ms step_avg:60.49ms
step:1331/2315 train_time:80514ms step_avg:60.49ms
step:1332/2315 train_time:80574ms step_avg:60.49ms
step:1333/2315 train_time:80635ms step_avg:60.49ms
step:1334/2315 train_time:80696ms step_avg:60.49ms
step:1335/2315 train_time:80757ms step_avg:60.49ms
step:1336/2315 train_time:80818ms step_avg:60.49ms
step:1337/2315 train_time:80879ms step_avg:60.49ms
step:1338/2315 train_time:80941ms step_avg:60.49ms
step:1339/2315 train_time:81002ms step_avg:60.49ms
step:1340/2315 train_time:81064ms step_avg:60.50ms
step:1341/2315 train_time:81125ms step_avg:60.50ms
step:1342/2315 train_time:81186ms step_avg:60.50ms
step:1343/2315 train_time:81247ms step_avg:60.50ms
step:1344/2315 train_time:81307ms step_avg:60.50ms
step:1345/2315 train_time:81367ms step_avg:60.50ms
step:1346/2315 train_time:81428ms step_avg:60.50ms
step:1347/2315 train_time:81489ms step_avg:60.50ms
step:1348/2315 train_time:81549ms step_avg:60.50ms
step:1349/2315 train_time:81610ms step_avg:60.50ms
step:1350/2315 train_time:81672ms step_avg:60.50ms
step:1351/2315 train_time:81733ms step_avg:60.50ms
step:1352/2315 train_time:81795ms step_avg:60.50ms
step:1353/2315 train_time:81855ms step_avg:60.50ms
step:1354/2315 train_time:81916ms step_avg:60.50ms
step:1355/2315 train_time:81977ms step_avg:60.50ms
step:1356/2315 train_time:82038ms step_avg:60.50ms
step:1357/2315 train_time:82099ms step_avg:60.50ms
step:1358/2315 train_time:82160ms step_avg:60.50ms
step:1359/2315 train_time:82221ms step_avg:60.50ms
step:1360/2315 train_time:82282ms step_avg:60.50ms
step:1361/2315 train_time:82343ms step_avg:60.50ms
step:1362/2315 train_time:82404ms step_avg:60.50ms
step:1363/2315 train_time:82466ms step_avg:60.50ms
step:1364/2315 train_time:82526ms step_avg:60.50ms
step:1365/2315 train_time:82588ms step_avg:60.50ms
step:1366/2315 train_time:82649ms step_avg:60.50ms
step:1367/2315 train_time:82710ms step_avg:60.50ms
step:1368/2315 train_time:82771ms step_avg:60.50ms
step:1369/2315 train_time:82832ms step_avg:60.51ms
step:1370/2315 train_time:82893ms step_avg:60.51ms
step:1371/2315 train_time:82954ms step_avg:60.51ms
step:1372/2315 train_time:83016ms step_avg:60.51ms
step:1373/2315 train_time:83076ms step_avg:60.51ms
step:1374/2315 train_time:83137ms step_avg:60.51ms
step:1375/2315 train_time:83197ms step_avg:60.51ms
step:1376/2315 train_time:83259ms step_avg:60.51ms
step:1377/2315 train_time:83319ms step_avg:60.51ms
step:1378/2315 train_time:83380ms step_avg:60.51ms
step:1379/2315 train_time:83441ms step_avg:60.51ms
step:1380/2315 train_time:83502ms step_avg:60.51ms
step:1381/2315 train_time:83564ms step_avg:60.51ms
step:1382/2315 train_time:83625ms step_avg:60.51ms
step:1383/2315 train_time:83686ms step_avg:60.51ms
step:1384/2315 train_time:83748ms step_avg:60.51ms
step:1385/2315 train_time:83808ms step_avg:60.51ms
step:1386/2315 train_time:83869ms step_avg:60.51ms
step:1387/2315 train_time:83930ms step_avg:60.51ms
step:1388/2315 train_time:83991ms step_avg:60.51ms
step:1389/2315 train_time:84052ms step_avg:60.51ms
step:1390/2315 train_time:84114ms step_avg:60.51ms
step:1391/2315 train_time:84174ms step_avg:60.51ms
step:1392/2315 train_time:84235ms step_avg:60.51ms
step:1393/2315 train_time:84296ms step_avg:60.51ms
step:1394/2315 train_time:84357ms step_avg:60.51ms
step:1395/2315 train_time:84417ms step_avg:60.51ms
step:1396/2315 train_time:84478ms step_avg:60.51ms
step:1397/2315 train_time:84539ms step_avg:60.51ms
step:1398/2315 train_time:84601ms step_avg:60.52ms
step:1399/2315 train_time:84662ms step_avg:60.52ms
step:1400/2315 train_time:84724ms step_avg:60.52ms
step:1401/2315 train_time:84785ms step_avg:60.52ms
step:1402/2315 train_time:84847ms step_avg:60.52ms
step:1403/2315 train_time:84908ms step_avg:60.52ms
step:1404/2315 train_time:84971ms step_avg:60.52ms
step:1405/2315 train_time:85031ms step_avg:60.52ms
step:1406/2315 train_time:85092ms step_avg:60.52ms
step:1407/2315 train_time:85153ms step_avg:60.52ms
step:1408/2315 train_time:85214ms step_avg:60.52ms
step:1409/2315 train_time:85275ms step_avg:60.52ms
step:1410/2315 train_time:85335ms step_avg:60.52ms
step:1411/2315 train_time:85396ms step_avg:60.52ms
step:1412/2315 train_time:85457ms step_avg:60.52ms
step:1413/2315 train_time:85518ms step_avg:60.52ms
step:1414/2315 train_time:85578ms step_avg:60.52ms
step:1415/2315 train_time:85640ms step_avg:60.52ms
step:1416/2315 train_time:85700ms step_avg:60.52ms
step:1417/2315 train_time:85761ms step_avg:60.52ms
step:1418/2315 train_time:85823ms step_avg:60.52ms
step:1419/2315 train_time:85884ms step_avg:60.52ms
step:1420/2315 train_time:85947ms step_avg:60.53ms
step:1421/2315 train_time:86008ms step_avg:60.53ms
step:1422/2315 train_time:86069ms step_avg:60.53ms
step:1423/2315 train_time:86130ms step_avg:60.53ms
step:1424/2315 train_time:86190ms step_avg:60.53ms
step:1425/2315 train_time:86251ms step_avg:60.53ms
step:1426/2315 train_time:86312ms step_avg:60.53ms
step:1427/2315 train_time:86373ms step_avg:60.53ms
step:1428/2315 train_time:86435ms step_avg:60.53ms
step:1429/2315 train_time:86496ms step_avg:60.53ms
step:1430/2315 train_time:86557ms step_avg:60.53ms
step:1431/2315 train_time:86618ms step_avg:60.53ms
step:1432/2315 train_time:86679ms step_avg:60.53ms
step:1433/2315 train_time:86739ms step_avg:60.53ms
step:1434/2315 train_time:86800ms step_avg:60.53ms
step:1435/2315 train_time:86861ms step_avg:60.53ms
step:1436/2315 train_time:86922ms step_avg:60.53ms
step:1437/2315 train_time:86984ms step_avg:60.53ms
step:1438/2315 train_time:87046ms step_avg:60.53ms
step:1439/2315 train_time:87107ms step_avg:60.53ms
step:1440/2315 train_time:87168ms step_avg:60.53ms
step:1441/2315 train_time:87228ms step_avg:60.53ms
step:1442/2315 train_time:87289ms step_avg:60.53ms
step:1443/2315 train_time:87350ms step_avg:60.53ms
step:1444/2315 train_time:87412ms step_avg:60.53ms
step:1445/2315 train_time:87473ms step_avg:60.53ms
step:1446/2315 train_time:87534ms step_avg:60.54ms
step:1447/2315 train_time:87596ms step_avg:60.54ms
step:1448/2315 train_time:87657ms step_avg:60.54ms
step:1449/2315 train_time:87717ms step_avg:60.54ms
step:1450/2315 train_time:87778ms step_avg:60.54ms
step:1451/2315 train_time:87838ms step_avg:60.54ms
step:1452/2315 train_time:87899ms step_avg:60.54ms
step:1453/2315 train_time:87960ms step_avg:60.54ms
step:1454/2315 train_time:88021ms step_avg:60.54ms
step:1455/2315 train_time:88083ms step_avg:60.54ms
step:1456/2315 train_time:88144ms step_avg:60.54ms
step:1457/2315 train_time:88206ms step_avg:60.54ms
step:1458/2315 train_time:88267ms step_avg:60.54ms
step:1459/2315 train_time:88328ms step_avg:60.54ms
step:1460/2315 train_time:88389ms step_avg:60.54ms
step:1461/2315 train_time:88451ms step_avg:60.54ms
step:1462/2315 train_time:88512ms step_avg:60.54ms
step:1463/2315 train_time:88573ms step_avg:60.54ms
step:1464/2315 train_time:88633ms step_avg:60.54ms
step:1465/2315 train_time:88694ms step_avg:60.54ms
step:1466/2315 train_time:88756ms step_avg:60.54ms
step:1467/2315 train_time:88817ms step_avg:60.54ms
step:1468/2315 train_time:88878ms step_avg:60.54ms
step:1469/2315 train_time:88938ms step_avg:60.54ms
step:1470/2315 train_time:88999ms step_avg:60.54ms
step:1471/2315 train_time:89060ms step_avg:60.54ms
step:1472/2315 train_time:89121ms step_avg:60.54ms
step:1473/2315 train_time:89183ms step_avg:60.54ms
step:1474/2315 train_time:89244ms step_avg:60.55ms
step:1475/2315 train_time:89305ms step_avg:60.55ms
step:1476/2315 train_time:89367ms step_avg:60.55ms
step:1477/2315 train_time:89428ms step_avg:60.55ms
step:1478/2315 train_time:89489ms step_avg:60.55ms
step:1479/2315 train_time:89550ms step_avg:60.55ms
step:1480/2315 train_time:89611ms step_avg:60.55ms
step:1481/2315 train_time:89672ms step_avg:60.55ms
step:1482/2315 train_time:89734ms step_avg:60.55ms
step:1483/2315 train_time:89794ms step_avg:60.55ms
step:1484/2315 train_time:89855ms step_avg:60.55ms
step:1485/2315 train_time:89916ms step_avg:60.55ms
step:1486/2315 train_time:89977ms step_avg:60.55ms
step:1487/2315 train_time:90037ms step_avg:60.55ms
step:1488/2315 train_time:90098ms step_avg:60.55ms
step:1489/2315 train_time:90159ms step_avg:60.55ms
step:1490/2315 train_time:90221ms step_avg:60.55ms
step:1491/2315 train_time:90281ms step_avg:60.55ms
step:1492/2315 train_time:90344ms step_avg:60.55ms
step:1493/2315 train_time:90405ms step_avg:60.55ms
step:1494/2315 train_time:90467ms step_avg:60.55ms
step:1495/2315 train_time:90528ms step_avg:60.55ms
step:1496/2315 train_time:90589ms step_avg:60.55ms
step:1497/2315 train_time:90649ms step_avg:60.55ms
step:1498/2315 train_time:90710ms step_avg:60.55ms
step:1499/2315 train_time:90771ms step_avg:60.55ms
step:1500/2315 train_time:90833ms step_avg:60.56ms
step:1500/2315 val_loss:3.4488 train_time:90895ms step_avg:60.60ms
step:1501/2315 train_time:90913ms step_avg:60.57ms
step:1502/2315 train_time:90957ms step_avg:60.56ms
step:1503/2315 train_time:91022ms step_avg:60.56ms
step:1504/2315 train_time:91084ms step_avg:60.56ms
step:1505/2315 train_time:91144ms step_avg:60.56ms
step:1506/2315 train_time:91206ms step_avg:60.56ms
step:1507/2315 train_time:91266ms step_avg:60.56ms
step:1508/2315 train_time:91326ms step_avg:60.56ms
step:1509/2315 train_time:91385ms step_avg:60.56ms
step:1510/2315 train_time:91445ms step_avg:60.56ms
step:1511/2315 train_time:91505ms step_avg:60.56ms
step:1512/2315 train_time:91566ms step_avg:60.56ms
step:1513/2315 train_time:91626ms step_avg:60.56ms
step:1514/2315 train_time:91686ms step_avg:60.56ms
step:1515/2315 train_time:91746ms step_avg:60.56ms
step:1516/2315 train_time:91808ms step_avg:60.56ms
step:1517/2315 train_time:91869ms step_avg:60.56ms
step:1518/2315 train_time:91932ms step_avg:60.56ms
step:1519/2315 train_time:91995ms step_avg:60.56ms
step:1520/2315 train_time:92058ms step_avg:60.56ms
step:1521/2315 train_time:92119ms step_avg:60.56ms
step:1522/2315 train_time:92181ms step_avg:60.57ms
step:1523/2315 train_time:92242ms step_avg:60.57ms
step:1524/2315 train_time:92304ms step_avg:60.57ms
step:1525/2315 train_time:92364ms step_avg:60.57ms
step:1526/2315 train_time:92425ms step_avg:60.57ms
step:1527/2315 train_time:92485ms step_avg:60.57ms
step:1528/2315 train_time:92547ms step_avg:60.57ms
step:1529/2315 train_time:92607ms step_avg:60.57ms
step:1530/2315 train_time:92668ms step_avg:60.57ms
step:1531/2315 train_time:92728ms step_avg:60.57ms
step:1532/2315 train_time:92789ms step_avg:60.57ms
step:1533/2315 train_time:92850ms step_avg:60.57ms
step:1534/2315 train_time:92913ms step_avg:60.57ms
step:1535/2315 train_time:92976ms step_avg:60.57ms
step:1536/2315 train_time:93038ms step_avg:60.57ms
step:1537/2315 train_time:93100ms step_avg:60.57ms
step:1538/2315 train_time:93162ms step_avg:60.57ms
step:1539/2315 train_time:93223ms step_avg:60.57ms
step:1540/2315 train_time:93285ms step_avg:60.57ms
step:1541/2315 train_time:93346ms step_avg:60.57ms
step:1542/2315 train_time:93407ms step_avg:60.58ms
step:1543/2315 train_time:93468ms step_avg:60.58ms
step:1544/2315 train_time:93529ms step_avg:60.58ms
step:1545/2315 train_time:93589ms step_avg:60.58ms
step:1546/2315 train_time:93650ms step_avg:60.58ms
step:1547/2315 train_time:93711ms step_avg:60.58ms
step:1548/2315 train_time:93772ms step_avg:60.58ms
step:1549/2315 train_time:93833ms step_avg:60.58ms
step:1550/2315 train_time:93895ms step_avg:60.58ms
step:1551/2315 train_time:93957ms step_avg:60.58ms
step:1552/2315 train_time:94019ms step_avg:60.58ms
step:1553/2315 train_time:94081ms step_avg:60.58ms
step:1554/2315 train_time:94142ms step_avg:60.58ms
step:1555/2315 train_time:94204ms step_avg:60.58ms
step:1556/2315 train_time:94265ms step_avg:60.58ms
step:1557/2315 train_time:94326ms step_avg:60.58ms
step:1558/2315 train_time:94388ms step_avg:60.58ms
step:1559/2315 train_time:94449ms step_avg:60.58ms
step:1560/2315 train_time:94510ms step_avg:60.58ms
step:1561/2315 train_time:94570ms step_avg:60.58ms
step:1562/2315 train_time:94631ms step_avg:60.58ms
step:1563/2315 train_time:94692ms step_avg:60.58ms
step:1564/2315 train_time:94754ms step_avg:60.58ms
step:1565/2315 train_time:94815ms step_avg:60.58ms
step:1566/2315 train_time:94876ms step_avg:60.59ms
step:1567/2315 train_time:94938ms step_avg:60.59ms
step:1568/2315 train_time:95000ms step_avg:60.59ms
step:1569/2315 train_time:95062ms step_avg:60.59ms
step:1570/2315 train_time:95124ms step_avg:60.59ms
step:1571/2315 train_time:95185ms step_avg:60.59ms
step:1572/2315 train_time:95247ms step_avg:60.59ms
step:1573/2315 train_time:95308ms step_avg:60.59ms
step:1574/2315 train_time:95369ms step_avg:60.59ms
step:1575/2315 train_time:95429ms step_avg:60.59ms
step:1576/2315 train_time:95491ms step_avg:60.59ms
step:1577/2315 train_time:95552ms step_avg:60.59ms
step:1578/2315 train_time:95614ms step_avg:60.59ms
step:1579/2315 train_time:95675ms step_avg:60.59ms
step:1580/2315 train_time:95737ms step_avg:60.59ms
step:1581/2315 train_time:95798ms step_avg:60.59ms
step:1582/2315 train_time:95860ms step_avg:60.59ms
step:1583/2315 train_time:95921ms step_avg:60.59ms
step:1584/2315 train_time:95983ms step_avg:60.60ms
step:1585/2315 train_time:96044ms step_avg:60.60ms
step:1586/2315 train_time:96106ms step_avg:60.60ms
step:1587/2315 train_time:96168ms step_avg:60.60ms
step:1588/2315 train_time:96229ms step_avg:60.60ms
step:1589/2315 train_time:96291ms step_avg:60.60ms
step:1590/2315 train_time:96353ms step_avg:60.60ms
step:1591/2315 train_time:96413ms step_avg:60.60ms
step:1592/2315 train_time:96476ms step_avg:60.60ms
step:1593/2315 train_time:96537ms step_avg:60.60ms
step:1594/2315 train_time:96598ms step_avg:60.60ms
step:1595/2315 train_time:96659ms step_avg:60.60ms
step:1596/2315 train_time:96721ms step_avg:60.60ms
step:1597/2315 train_time:96781ms step_avg:60.60ms
step:1598/2315 train_time:96842ms step_avg:60.60ms
step:1599/2315 train_time:96904ms step_avg:60.60ms
step:1600/2315 train_time:96965ms step_avg:60.60ms
step:1601/2315 train_time:97027ms step_avg:60.60ms
step:1602/2315 train_time:97088ms step_avg:60.60ms
step:1603/2315 train_time:97149ms step_avg:60.60ms
step:1604/2315 train_time:97211ms step_avg:60.61ms
step:1605/2315 train_time:97273ms step_avg:60.61ms
step:1606/2315 train_time:97334ms step_avg:60.61ms
step:1607/2315 train_time:97395ms step_avg:60.61ms
step:1608/2315 train_time:97457ms step_avg:60.61ms
step:1609/2315 train_time:97519ms step_avg:60.61ms
step:1610/2315 train_time:97580ms step_avg:60.61ms
step:1611/2315 train_time:97641ms step_avg:60.61ms
step:1612/2315 train_time:97702ms step_avg:60.61ms
step:1613/2315 train_time:97763ms step_avg:60.61ms
step:1614/2315 train_time:97825ms step_avg:60.61ms
step:1615/2315 train_time:97886ms step_avg:60.61ms
step:1616/2315 train_time:97948ms step_avg:60.61ms
step:1617/2315 train_time:98009ms step_avg:60.61ms
step:1618/2315 train_time:98071ms step_avg:60.61ms
step:1619/2315 train_time:98132ms step_avg:60.61ms
step:1620/2315 train_time:98194ms step_avg:60.61ms
step:1621/2315 train_time:98255ms step_avg:60.61ms
step:1622/2315 train_time:98317ms step_avg:60.61ms
step:1623/2315 train_time:98378ms step_avg:60.61ms
step:1624/2315 train_time:98439ms step_avg:60.62ms
step:1625/2315 train_time:98500ms step_avg:60.62ms
step:1626/2315 train_time:98561ms step_avg:60.62ms
step:1627/2315 train_time:98623ms step_avg:60.62ms
step:1628/2315 train_time:98684ms step_avg:60.62ms
step:1629/2315 train_time:98745ms step_avg:60.62ms
step:1630/2315 train_time:98806ms step_avg:60.62ms
step:1631/2315 train_time:98867ms step_avg:60.62ms
step:1632/2315 train_time:98929ms step_avg:60.62ms
step:1633/2315 train_time:98990ms step_avg:60.62ms
step:1634/2315 train_time:99053ms step_avg:60.62ms
step:1635/2315 train_time:99113ms step_avg:60.62ms
step:1636/2315 train_time:99176ms step_avg:60.62ms
step:1637/2315 train_time:99237ms step_avg:60.62ms
step:1638/2315 train_time:99299ms step_avg:60.62ms
step:1639/2315 train_time:99359ms step_avg:60.62ms
step:1640/2315 train_time:99421ms step_avg:60.62ms
step:1641/2315 train_time:99482ms step_avg:60.62ms
step:1642/2315 train_time:99544ms step_avg:60.62ms
step:1643/2315 train_time:99604ms step_avg:60.62ms
step:1644/2315 train_time:99666ms step_avg:60.62ms
step:1645/2315 train_time:99726ms step_avg:60.62ms
step:1646/2315 train_time:99788ms step_avg:60.62ms
step:1647/2315 train_time:99849ms step_avg:60.62ms
step:1648/2315 train_time:99911ms step_avg:60.63ms
step:1649/2315 train_time:99972ms step_avg:60.63ms
step:1650/2315 train_time:100034ms step_avg:60.63ms
step:1651/2315 train_time:100095ms step_avg:60.63ms
step:1652/2315 train_time:100157ms step_avg:60.63ms
step:1653/2315 train_time:100219ms step_avg:60.63ms
step:1654/2315 train_time:100280ms step_avg:60.63ms
step:1655/2315 train_time:100341ms step_avg:60.63ms
step:1656/2315 train_time:100402ms step_avg:60.63ms
step:1657/2315 train_time:100463ms step_avg:60.63ms
step:1658/2315 train_time:100525ms step_avg:60.63ms
step:1659/2315 train_time:100586ms step_avg:60.63ms
step:1660/2315 train_time:100647ms step_avg:60.63ms
step:1661/2315 train_time:100708ms step_avg:60.63ms
step:1662/2315 train_time:100770ms step_avg:60.63ms
step:1663/2315 train_time:100832ms step_avg:60.63ms
step:1664/2315 train_time:100893ms step_avg:60.63ms
step:1665/2315 train_time:100955ms step_avg:60.63ms
step:1666/2315 train_time:101016ms step_avg:60.63ms
step:1667/2315 train_time:101078ms step_avg:60.63ms
step:1668/2315 train_time:101140ms step_avg:60.64ms
step:1669/2315 train_time:101201ms step_avg:60.64ms
step:1670/2315 train_time:101262ms step_avg:60.64ms
step:1671/2315 train_time:101323ms step_avg:60.64ms
step:1672/2315 train_time:101386ms step_avg:60.64ms
step:1673/2315 train_time:101447ms step_avg:60.64ms
step:1674/2315 train_time:101508ms step_avg:60.64ms
step:1675/2315 train_time:101569ms step_avg:60.64ms
step:1676/2315 train_time:101630ms step_avg:60.64ms
step:1677/2315 train_time:101691ms step_avg:60.64ms
step:1678/2315 train_time:101753ms step_avg:60.64ms
step:1679/2315 train_time:101815ms step_avg:60.64ms
step:1680/2315 train_time:101877ms step_avg:60.64ms
step:1681/2315 train_time:101938ms step_avg:60.64ms
step:1682/2315 train_time:102000ms step_avg:60.64ms
step:1683/2315 train_time:102061ms step_avg:60.64ms
step:1684/2315 train_time:102122ms step_avg:60.64ms
step:1685/2315 train_time:102184ms step_avg:60.64ms
step:1686/2315 train_time:102245ms step_avg:60.64ms
step:1687/2315 train_time:102306ms step_avg:60.64ms
step:1688/2315 train_time:102368ms step_avg:60.64ms
step:1689/2315 train_time:102429ms step_avg:60.64ms
step:1690/2315 train_time:102490ms step_avg:60.64ms
step:1691/2315 train_time:102551ms step_avg:60.64ms
step:1692/2315 train_time:102612ms step_avg:60.65ms
step:1693/2315 train_time:102674ms step_avg:60.65ms
step:1694/2315 train_time:102735ms step_avg:60.65ms
step:1695/2315 train_time:102796ms step_avg:60.65ms
step:1696/2315 train_time:102858ms step_avg:60.65ms
step:1697/2315 train_time:102919ms step_avg:60.65ms
step:1698/2315 train_time:102981ms step_avg:60.65ms
step:1699/2315 train_time:103042ms step_avg:60.65ms
step:1700/2315 train_time:103104ms step_avg:60.65ms
step:1701/2315 train_time:103165ms step_avg:60.65ms
step:1702/2315 train_time:103227ms step_avg:60.65ms
step:1703/2315 train_time:103288ms step_avg:60.65ms
step:1704/2315 train_time:103349ms step_avg:60.65ms
step:1705/2315 train_time:103411ms step_avg:60.65ms
step:1706/2315 train_time:103473ms step_avg:60.65ms
step:1707/2315 train_time:103534ms step_avg:60.65ms
step:1708/2315 train_time:103596ms step_avg:60.65ms
step:1709/2315 train_time:103657ms step_avg:60.65ms
step:1710/2315 train_time:103718ms step_avg:60.65ms
step:1711/2315 train_time:103780ms step_avg:60.65ms
step:1712/2315 train_time:103842ms step_avg:60.66ms
step:1713/2315 train_time:103902ms step_avg:60.66ms
step:1714/2315 train_time:103964ms step_avg:60.66ms
step:1715/2315 train_time:104024ms step_avg:60.66ms
step:1716/2315 train_time:104086ms step_avg:60.66ms
step:1717/2315 train_time:104147ms step_avg:60.66ms
step:1718/2315 train_time:104209ms step_avg:60.66ms
step:1719/2315 train_time:104270ms step_avg:60.66ms
step:1720/2315 train_time:104332ms step_avg:60.66ms
step:1721/2315 train_time:104392ms step_avg:60.66ms
step:1722/2315 train_time:104454ms step_avg:60.66ms
step:1723/2315 train_time:104515ms step_avg:60.66ms
step:1724/2315 train_time:104577ms step_avg:60.66ms
step:1725/2315 train_time:104638ms step_avg:60.66ms
step:1726/2315 train_time:104700ms step_avg:60.66ms
step:1727/2315 train_time:104760ms step_avg:60.66ms
step:1728/2315 train_time:104822ms step_avg:60.66ms
step:1729/2315 train_time:104882ms step_avg:60.66ms
step:1730/2315 train_time:104944ms step_avg:60.66ms
step:1731/2315 train_time:105005ms step_avg:60.66ms
step:1732/2315 train_time:105066ms step_avg:60.66ms
step:1733/2315 train_time:105127ms step_avg:60.66ms
step:1734/2315 train_time:105189ms step_avg:60.66ms
step:1735/2315 train_time:105250ms step_avg:60.66ms
step:1736/2315 train_time:105311ms step_avg:60.66ms
step:1737/2315 train_time:105372ms step_avg:60.66ms
step:1738/2315 train_time:105434ms step_avg:60.66ms
step:1739/2315 train_time:105496ms step_avg:60.66ms
step:1740/2315 train_time:105557ms step_avg:60.66ms
step:1741/2315 train_time:105618ms step_avg:60.67ms
step:1742/2315 train_time:105680ms step_avg:60.67ms
step:1743/2315 train_time:105741ms step_avg:60.67ms
step:1744/2315 train_time:105802ms step_avg:60.67ms
step:1745/2315 train_time:105863ms step_avg:60.67ms
step:1746/2315 train_time:105924ms step_avg:60.67ms
step:1747/2315 train_time:105986ms step_avg:60.67ms
step:1748/2315 train_time:106048ms step_avg:60.67ms
step:1749/2315 train_time:106109ms step_avg:60.67ms
step:1750/2315 train_time:106171ms step_avg:60.67ms
step:1750/2315 val_loss:3.3806 train_time:106234ms step_avg:60.71ms
step:1751/2315 train_time:106252ms step_avg:60.68ms
step:1752/2315 train_time:106296ms step_avg:60.67ms
step:1753/2315 train_time:106364ms step_avg:60.68ms
step:1754/2315 train_time:106430ms step_avg:60.68ms
step:1755/2315 train_time:106491ms step_avg:60.68ms
step:1756/2315 train_time:106552ms step_avg:60.68ms
step:1757/2315 train_time:106613ms step_avg:60.68ms
step:1758/2315 train_time:106674ms step_avg:60.68ms
step:1759/2315 train_time:106734ms step_avg:60.68ms
step:1760/2315 train_time:106795ms step_avg:60.68ms
step:1761/2315 train_time:106854ms step_avg:60.68ms
step:1762/2315 train_time:106915ms step_avg:60.68ms
step:1763/2315 train_time:106975ms step_avg:60.68ms
step:1764/2315 train_time:107036ms step_avg:60.68ms
step:1765/2315 train_time:107096ms step_avg:60.68ms
step:1766/2315 train_time:107160ms step_avg:60.68ms
step:1767/2315 train_time:107222ms step_avg:60.68ms
step:1768/2315 train_time:107286ms step_avg:60.68ms
step:1769/2315 train_time:107350ms step_avg:60.68ms
step:1770/2315 train_time:107413ms step_avg:60.69ms
step:1771/2315 train_time:107475ms step_avg:60.69ms
step:1772/2315 train_time:107537ms step_avg:60.69ms
step:1773/2315 train_time:107598ms step_avg:60.69ms
step:1774/2315 train_time:107659ms step_avg:60.69ms
step:1775/2315 train_time:107720ms step_avg:60.69ms
step:1776/2315 train_time:107782ms step_avg:60.69ms
step:1777/2315 train_time:107842ms step_avg:60.69ms
step:1778/2315 train_time:107903ms step_avg:60.69ms
step:1779/2315 train_time:107963ms step_avg:60.69ms
step:1780/2315 train_time:108025ms step_avg:60.69ms
step:1781/2315 train_time:108086ms step_avg:60.69ms
step:1782/2315 train_time:108147ms step_avg:60.69ms
step:1783/2315 train_time:108208ms step_avg:60.69ms
step:1784/2315 train_time:108270ms step_avg:60.69ms
step:1785/2315 train_time:108333ms step_avg:60.69ms
step:1786/2315 train_time:108395ms step_avg:60.69ms
step:1787/2315 train_time:108456ms step_avg:60.69ms
step:1788/2315 train_time:108518ms step_avg:60.69ms
step:1789/2315 train_time:108580ms step_avg:60.69ms
step:1790/2315 train_time:108641ms step_avg:60.69ms
step:1791/2315 train_time:108702ms step_avg:60.69ms
step:1792/2315 train_time:108763ms step_avg:60.69ms
step:1793/2315 train_time:108824ms step_avg:60.69ms
step:1794/2315 train_time:108885ms step_avg:60.69ms
step:1795/2315 train_time:108945ms step_avg:60.69ms
step:1796/2315 train_time:109006ms step_avg:60.69ms
step:1797/2315 train_time:109067ms step_avg:60.69ms
step:1798/2315 train_time:109128ms step_avg:60.69ms
step:1799/2315 train_time:109189ms step_avg:60.69ms
step:1800/2315 train_time:109252ms step_avg:60.70ms
step:1801/2315 train_time:109313ms step_avg:60.70ms
step:1802/2315 train_time:109375ms step_avg:60.70ms
step:1803/2315 train_time:109436ms step_avg:60.70ms
step:1804/2315 train_time:109498ms step_avg:60.70ms
step:1805/2315 train_time:109559ms step_avg:60.70ms
step:1806/2315 train_time:109621ms step_avg:60.70ms
step:1807/2315 train_time:109682ms step_avg:60.70ms
step:1808/2315 train_time:109744ms step_avg:60.70ms
step:1809/2315 train_time:109805ms step_avg:60.70ms
step:1810/2315 train_time:109866ms step_avg:60.70ms
step:1811/2315 train_time:109927ms step_avg:60.70ms
step:1812/2315 train_time:109988ms step_avg:60.70ms
step:1813/2315 train_time:110049ms step_avg:60.70ms
step:1814/2315 train_time:110110ms step_avg:60.70ms
step:1815/2315 train_time:110171ms step_avg:60.70ms
step:1816/2315 train_time:110233ms step_avg:60.70ms
step:1817/2315 train_time:110294ms step_avg:60.70ms
step:1818/2315 train_time:110356ms step_avg:60.70ms
step:1819/2315 train_time:110417ms step_avg:60.70ms
step:1820/2315 train_time:110478ms step_avg:60.70ms
step:1821/2315 train_time:110540ms step_avg:60.70ms
step:1822/2315 train_time:110601ms step_avg:60.70ms
step:1823/2315 train_time:110662ms step_avg:60.70ms
step:1824/2315 train_time:110723ms step_avg:60.70ms
step:1825/2315 train_time:110785ms step_avg:60.70ms
step:1826/2315 train_time:110846ms step_avg:60.70ms
step:1827/2315 train_time:110907ms step_avg:60.70ms
step:1828/2315 train_time:110968ms step_avg:60.70ms
step:1829/2315 train_time:111029ms step_avg:60.70ms
step:1830/2315 train_time:111090ms step_avg:60.70ms
step:1831/2315 train_time:111151ms step_avg:60.71ms
step:1832/2315 train_time:111213ms step_avg:60.71ms
step:1833/2315 train_time:111274ms step_avg:60.71ms
step:1834/2315 train_time:111335ms step_avg:60.71ms
step:1835/2315 train_time:111396ms step_avg:60.71ms
step:1836/2315 train_time:111457ms step_avg:60.71ms
step:1837/2315 train_time:111518ms step_avg:60.71ms
step:1838/2315 train_time:111580ms step_avg:60.71ms
step:1839/2315 train_time:111641ms step_avg:60.71ms
step:1840/2315 train_time:111702ms step_avg:60.71ms
step:1841/2315 train_time:111764ms step_avg:60.71ms
step:1842/2315 train_time:111825ms step_avg:60.71ms
step:1843/2315 train_time:111887ms step_avg:60.71ms
step:1844/2315 train_time:111948ms step_avg:60.71ms
step:1845/2315 train_time:112009ms step_avg:60.71ms
step:1846/2315 train_time:112070ms step_avg:60.71ms
step:1847/2315 train_time:112132ms step_avg:60.71ms
step:1848/2315 train_time:112193ms step_avg:60.71ms
step:1849/2315 train_time:112254ms step_avg:60.71ms
step:1850/2315 train_time:112316ms step_avg:60.71ms
step:1851/2315 train_time:112377ms step_avg:60.71ms
step:1852/2315 train_time:112439ms step_avg:60.71ms
step:1853/2315 train_time:112500ms step_avg:60.71ms
step:1854/2315 train_time:112561ms step_avg:60.71ms
step:1855/2315 train_time:112622ms step_avg:60.71ms
step:1856/2315 train_time:112684ms step_avg:60.71ms
step:1857/2315 train_time:112745ms step_avg:60.71ms
step:1858/2315 train_time:112807ms step_avg:60.71ms
step:1859/2315 train_time:112868ms step_avg:60.71ms
step:1860/2315 train_time:112928ms step_avg:60.71ms
step:1861/2315 train_time:112990ms step_avg:60.71ms
step:1862/2315 train_time:113051ms step_avg:60.71ms
step:1863/2315 train_time:113113ms step_avg:60.72ms
step:1864/2315 train_time:113174ms step_avg:60.72ms
step:1865/2315 train_time:113235ms step_avg:60.72ms
step:1866/2315 train_time:113297ms step_avg:60.72ms
step:1867/2315 train_time:113359ms step_avg:60.72ms
step:1868/2315 train_time:113420ms step_avg:60.72ms
step:1869/2315 train_time:113481ms step_avg:60.72ms
step:1870/2315 train_time:113543ms step_avg:60.72ms
step:1871/2315 train_time:113603ms step_avg:60.72ms
step:1872/2315 train_time:113665ms step_avg:60.72ms
step:1873/2315 train_time:113726ms step_avg:60.72ms
step:1874/2315 train_time:113787ms step_avg:60.72ms
step:1875/2315 train_time:113848ms step_avg:60.72ms
step:1876/2315 train_time:113909ms step_avg:60.72ms
step:1877/2315 train_time:113970ms step_avg:60.72ms
step:1878/2315 train_time:114031ms step_avg:60.72ms
step:1879/2315 train_time:114092ms step_avg:60.72ms
step:1880/2315 train_time:114153ms step_avg:60.72ms
step:1881/2315 train_time:114214ms step_avg:60.72ms
step:1882/2315 train_time:114276ms step_avg:60.72ms
step:1883/2315 train_time:114337ms step_avg:60.72ms
step:1884/2315 train_time:114399ms step_avg:60.72ms
step:1885/2315 train_time:114459ms step_avg:60.72ms
step:1886/2315 train_time:114521ms step_avg:60.72ms
step:1887/2315 train_time:114582ms step_avg:60.72ms
step:1888/2315 train_time:114645ms step_avg:60.72ms
step:1889/2315 train_time:114706ms step_avg:60.72ms
step:1890/2315 train_time:114768ms step_avg:60.72ms
step:1891/2315 train_time:114829ms step_avg:60.72ms
step:1892/2315 train_time:114890ms step_avg:60.72ms
step:1893/2315 train_time:114951ms step_avg:60.72ms
step:1894/2315 train_time:115012ms step_avg:60.72ms
step:1895/2315 train_time:115073ms step_avg:60.72ms
step:1896/2315 train_time:115135ms step_avg:60.72ms
step:1897/2315 train_time:115196ms step_avg:60.73ms
step:1898/2315 train_time:115257ms step_avg:60.73ms
step:1899/2315 train_time:115318ms step_avg:60.73ms
step:1900/2315 train_time:115380ms step_avg:60.73ms
step:1901/2315 train_time:115441ms step_avg:60.73ms
step:1902/2315 train_time:115503ms step_avg:60.73ms
step:1903/2315 train_time:115564ms step_avg:60.73ms
step:1904/2315 train_time:115626ms step_avg:60.73ms
step:1905/2315 train_time:115687ms step_avg:60.73ms
step:1906/2315 train_time:115748ms step_avg:60.73ms
step:1907/2315 train_time:115809ms step_avg:60.73ms
step:1908/2315 train_time:115870ms step_avg:60.73ms
step:1909/2315 train_time:115932ms step_avg:60.73ms
step:1910/2315 train_time:115993ms step_avg:60.73ms
step:1911/2315 train_time:116054ms step_avg:60.73ms
step:1912/2315 train_time:116116ms step_avg:60.73ms
step:1913/2315 train_time:116177ms step_avg:60.73ms
step:1914/2315 train_time:116238ms step_avg:60.73ms
step:1915/2315 train_time:116299ms step_avg:60.73ms
step:1916/2315 train_time:116361ms step_avg:60.73ms
step:1917/2315 train_time:116422ms step_avg:60.73ms
step:1918/2315 train_time:116484ms step_avg:60.73ms
step:1919/2315 train_time:116545ms step_avg:60.73ms
step:1920/2315 train_time:116607ms step_avg:60.73ms
step:1921/2315 train_time:116668ms step_avg:60.73ms
step:1922/2315 train_time:116729ms step_avg:60.73ms
step:1923/2315 train_time:116791ms step_avg:60.73ms
step:1924/2315 train_time:116852ms step_avg:60.73ms
step:1925/2315 train_time:116913ms step_avg:60.73ms
step:1926/2315 train_time:116974ms step_avg:60.73ms
step:1927/2315 train_time:117035ms step_avg:60.73ms
step:1928/2315 train_time:117098ms step_avg:60.74ms
step:1929/2315 train_time:117158ms step_avg:60.74ms
step:1930/2315 train_time:117220ms step_avg:60.74ms
step:1931/2315 train_time:117282ms step_avg:60.74ms
step:1932/2315 train_time:117343ms step_avg:60.74ms
step:1933/2315 train_time:117405ms step_avg:60.74ms
step:1934/2315 train_time:117466ms step_avg:60.74ms
step:1935/2315 train_time:117527ms step_avg:60.74ms
step:1936/2315 train_time:117588ms step_avg:60.74ms
step:1937/2315 train_time:117649ms step_avg:60.74ms
step:1938/2315 train_time:117710ms step_avg:60.74ms
step:1939/2315 train_time:117772ms step_avg:60.74ms
step:1940/2315 train_time:117833ms step_avg:60.74ms
step:1941/2315 train_time:117894ms step_avg:60.74ms
step:1942/2315 train_time:117955ms step_avg:60.74ms
step:1943/2315 train_time:118016ms step_avg:60.74ms
step:1944/2315 train_time:118077ms step_avg:60.74ms
step:1945/2315 train_time:118138ms step_avg:60.74ms
step:1946/2315 train_time:118200ms step_avg:60.74ms
step:1947/2315 train_time:118261ms step_avg:60.74ms
step:1948/2315 train_time:118322ms step_avg:60.74ms
step:1949/2315 train_time:118384ms step_avg:60.74ms
step:1950/2315 train_time:118445ms step_avg:60.74ms
step:1951/2315 train_time:118506ms step_avg:60.74ms
step:1952/2315 train_time:118568ms step_avg:60.74ms
step:1953/2315 train_time:118628ms step_avg:60.74ms
step:1954/2315 train_time:118689ms step_avg:60.74ms
step:1955/2315 train_time:118750ms step_avg:60.74ms
step:1956/2315 train_time:118811ms step_avg:60.74ms
step:1957/2315 train_time:118872ms step_avg:60.74ms
step:1958/2315 train_time:118934ms step_avg:60.74ms
step:1959/2315 train_time:118994ms step_avg:60.74ms
step:1960/2315 train_time:119056ms step_avg:60.74ms
step:1961/2315 train_time:119117ms step_avg:60.74ms
step:1962/2315 train_time:119179ms step_avg:60.74ms
step:1963/2315 train_time:119240ms step_avg:60.74ms
step:1964/2315 train_time:119301ms step_avg:60.74ms
step:1965/2315 train_time:119362ms step_avg:60.74ms
step:1966/2315 train_time:119425ms step_avg:60.75ms
step:1967/2315 train_time:119487ms step_avg:60.75ms
step:1968/2315 train_time:119548ms step_avg:60.75ms
step:1969/2315 train_time:119609ms step_avg:60.75ms
step:1970/2315 train_time:119670ms step_avg:60.75ms
step:1971/2315 train_time:119731ms step_avg:60.75ms
step:1972/2315 train_time:119792ms step_avg:60.75ms
step:1973/2315 train_time:119853ms step_avg:60.75ms
step:1974/2315 train_time:119915ms step_avg:60.75ms
step:1975/2315 train_time:119976ms step_avg:60.75ms
step:1976/2315 train_time:120037ms step_avg:60.75ms
step:1977/2315 train_time:120099ms step_avg:60.75ms
step:1978/2315 train_time:120161ms step_avg:60.75ms
step:1979/2315 train_time:120222ms step_avg:60.75ms
step:1980/2315 train_time:120284ms step_avg:60.75ms
step:1981/2315 train_time:120345ms step_avg:60.75ms
step:1982/2315 train_time:120407ms step_avg:60.75ms
step:1983/2315 train_time:120468ms step_avg:60.75ms
step:1984/2315 train_time:120529ms step_avg:60.75ms
step:1985/2315 train_time:120590ms step_avg:60.75ms
step:1986/2315 train_time:120652ms step_avg:60.75ms
step:1987/2315 train_time:120713ms step_avg:60.75ms
step:1988/2315 train_time:120774ms step_avg:60.75ms
step:1989/2315 train_time:120834ms step_avg:60.75ms
step:1990/2315 train_time:120895ms step_avg:60.75ms
step:1991/2315 train_time:120956ms step_avg:60.75ms
step:1992/2315 train_time:121017ms step_avg:60.75ms
step:1993/2315 train_time:121079ms step_avg:60.75ms
step:1994/2315 train_time:121140ms step_avg:60.75ms
step:1995/2315 train_time:121202ms step_avg:60.75ms
step:1996/2315 train_time:121263ms step_avg:60.75ms
step:1997/2315 train_time:121324ms step_avg:60.75ms
step:1998/2315 train_time:121386ms step_avg:60.75ms
step:1999/2315 train_time:121447ms step_avg:60.75ms
step:2000/2315 train_time:121508ms step_avg:60.75ms
step:2000/2315 val_loss:3.3300 train_time:121571ms step_avg:60.79ms
step:2001/2315 train_time:121589ms step_avg:60.76ms
step:2002/2315 train_time:121634ms step_avg:60.76ms
step:2003/2315 train_time:121698ms step_avg:60.76ms
step:2004/2315 train_time:121761ms step_avg:60.76ms
step:2005/2315 train_time:121822ms step_avg:60.76ms
step:2006/2315 train_time:121884ms step_avg:60.76ms
step:2007/2315 train_time:121944ms step_avg:60.76ms
step:2008/2315 train_time:122005ms step_avg:60.76ms
step:2009/2315 train_time:122066ms step_avg:60.76ms
step:2010/2315 train_time:122126ms step_avg:60.76ms
step:2011/2315 train_time:122187ms step_avg:60.76ms
step:2012/2315 train_time:122248ms step_avg:60.76ms
step:2013/2315 train_time:122308ms step_avg:60.76ms
step:2014/2315 train_time:122369ms step_avg:60.76ms
step:2015/2315 train_time:122430ms step_avg:60.76ms
step:2016/2315 train_time:122493ms step_avg:60.76ms
step:2017/2315 train_time:122556ms step_avg:60.76ms
step:2018/2315 train_time:122619ms step_avg:60.76ms
step:2019/2315 train_time:122681ms step_avg:60.76ms
step:2020/2315 train_time:122743ms step_avg:60.76ms
step:2021/2315 train_time:122805ms step_avg:60.76ms
step:2022/2315 train_time:122867ms step_avg:60.76ms
step:2023/2315 train_time:122927ms step_avg:60.76ms
step:2024/2315 train_time:122988ms step_avg:60.77ms
step:2025/2315 train_time:123048ms step_avg:60.76ms
step:2026/2315 train_time:123110ms step_avg:60.76ms
step:2027/2315 train_time:123170ms step_avg:60.76ms
step:2028/2315 train_time:123232ms step_avg:60.77ms
step:2029/2315 train_time:123292ms step_avg:60.76ms
step:2030/2315 train_time:123353ms step_avg:60.76ms
step:2031/2315 train_time:123414ms step_avg:60.76ms
step:2032/2315 train_time:123476ms step_avg:60.77ms
step:2033/2315 train_time:123538ms step_avg:60.77ms
step:2034/2315 train_time:123601ms step_avg:60.77ms
step:2035/2315 train_time:123663ms step_avg:60.77ms
step:2036/2315 train_time:123725ms step_avg:60.77ms
step:2037/2315 train_time:123787ms step_avg:60.77ms
step:2038/2315 train_time:123848ms step_avg:60.77ms
step:2039/2315 train_time:123909ms step_avg:60.77ms
step:2040/2315 train_time:123971ms step_avg:60.77ms
step:2041/2315 train_time:124032ms step_avg:60.77ms
step:2042/2315 train_time:124094ms step_avg:60.77ms
step:2043/2315 train_time:124154ms step_avg:60.77ms
step:2044/2315 train_time:124216ms step_avg:60.77ms
step:2045/2315 train_time:124276ms step_avg:60.77ms
step:2046/2315 train_time:124337ms step_avg:60.77ms
step:2047/2315 train_time:124398ms step_avg:60.77ms
step:2048/2315 train_time:124460ms step_avg:60.77ms
step:2049/2315 train_time:124521ms step_avg:60.77ms
step:2050/2315 train_time:124583ms step_avg:60.77ms
step:2051/2315 train_time:124645ms step_avg:60.77ms
step:2052/2315 train_time:124707ms step_avg:60.77ms
step:2053/2315 train_time:124768ms step_avg:60.77ms
step:2054/2315 train_time:124830ms step_avg:60.77ms
step:2055/2315 train_time:124891ms step_avg:60.77ms
step:2056/2315 train_time:124953ms step_avg:60.78ms
step:2057/2315 train_time:125014ms step_avg:60.77ms
step:2058/2315 train_time:125075ms step_avg:60.78ms
step:2059/2315 train_time:125136ms step_avg:60.78ms
step:2060/2315 train_time:125198ms step_avg:60.78ms
step:2061/2315 train_time:125259ms step_avg:60.78ms
step:2062/2315 train_time:125320ms step_avg:60.78ms
step:2063/2315 train_time:125381ms step_avg:60.78ms
step:2064/2315 train_time:125443ms step_avg:60.78ms
step:2065/2315 train_time:125505ms step_avg:60.78ms
step:2066/2315 train_time:125567ms step_avg:60.78ms
step:2067/2315 train_time:125628ms step_avg:60.78ms
step:2068/2315 train_time:125690ms step_avg:60.78ms
step:2069/2315 train_time:125751ms step_avg:60.78ms
step:2070/2315 train_time:125813ms step_avg:60.78ms
step:2071/2315 train_time:125874ms step_avg:60.78ms
step:2072/2315 train_time:125936ms step_avg:60.78ms
step:2073/2315 train_time:125996ms step_avg:60.78ms
step:2074/2315 train_time:126057ms step_avg:60.78ms
step:2075/2315 train_time:126118ms step_avg:60.78ms
step:2076/2315 train_time:126180ms step_avg:60.78ms
step:2077/2315 train_time:126241ms step_avg:60.78ms
step:2078/2315 train_time:126303ms step_avg:60.78ms
step:2079/2315 train_time:126365ms step_avg:60.78ms
step:2080/2315 train_time:126426ms step_avg:60.78ms
step:2081/2315 train_time:126487ms step_avg:60.78ms
step:2082/2315 train_time:126548ms step_avg:60.78ms
step:2083/2315 train_time:126610ms step_avg:60.78ms
step:2084/2315 train_time:126672ms step_avg:60.78ms
step:2085/2315 train_time:126733ms step_avg:60.78ms
step:2086/2315 train_time:126795ms step_avg:60.78ms
step:2087/2315 train_time:126856ms step_avg:60.78ms
step:2088/2315 train_time:126918ms step_avg:60.78ms
step:2089/2315 train_time:126979ms step_avg:60.78ms
step:2090/2315 train_time:127040ms step_avg:60.78ms
step:2091/2315 train_time:127101ms step_avg:60.78ms
step:2092/2315 train_time:127163ms step_avg:60.79ms
step:2093/2315 train_time:127224ms step_avg:60.79ms
step:2094/2315 train_time:127286ms step_avg:60.79ms
step:2095/2315 train_time:127347ms step_avg:60.79ms
step:2096/2315 train_time:127408ms step_avg:60.79ms
step:2097/2315 train_time:127469ms step_avg:60.79ms
step:2098/2315 train_time:127531ms step_avg:60.79ms
step:2099/2315 train_time:127592ms step_avg:60.79ms
step:2100/2315 train_time:127653ms step_avg:60.79ms
step:2101/2315 train_time:127714ms step_avg:60.79ms
step:2102/2315 train_time:127776ms step_avg:60.79ms
step:2103/2315 train_time:127837ms step_avg:60.79ms
step:2104/2315 train_time:127899ms step_avg:60.79ms
step:2105/2315 train_time:127960ms step_avg:60.79ms
step:2106/2315 train_time:128022ms step_avg:60.79ms
step:2107/2315 train_time:128083ms step_avg:60.79ms
step:2108/2315 train_time:128145ms step_avg:60.79ms
step:2109/2315 train_time:128206ms step_avg:60.79ms
step:2110/2315 train_time:128268ms step_avg:60.79ms
step:2111/2315 train_time:128329ms step_avg:60.79ms
step:2112/2315 train_time:128390ms step_avg:60.79ms
step:2113/2315 train_time:128452ms step_avg:60.79ms
step:2114/2315 train_time:128513ms step_avg:60.79ms
step:2115/2315 train_time:128574ms step_avg:60.79ms
step:2116/2315 train_time:128635ms step_avg:60.79ms
step:2117/2315 train_time:128696ms step_avg:60.79ms
step:2118/2315 train_time:128758ms step_avg:60.79ms
step:2119/2315 train_time:128819ms step_avg:60.79ms
step:2120/2315 train_time:128880ms step_avg:60.79ms
step:2121/2315 train_time:128941ms step_avg:60.79ms
step:2122/2315 train_time:129003ms step_avg:60.79ms
step:2123/2315 train_time:129065ms step_avg:60.79ms
step:2124/2315 train_time:129127ms step_avg:60.79ms
step:2125/2315 train_time:129187ms step_avg:60.79ms
step:2126/2315 train_time:129248ms step_avg:60.79ms
step:2127/2315 train_time:129309ms step_avg:60.79ms
step:2128/2315 train_time:129370ms step_avg:60.79ms
step:2129/2315 train_time:129431ms step_avg:60.79ms
step:2130/2315 train_time:129493ms step_avg:60.79ms
step:2131/2315 train_time:129554ms step_avg:60.79ms
step:2132/2315 train_time:129615ms step_avg:60.80ms
step:2133/2315 train_time:129677ms step_avg:60.80ms
step:2134/2315 train_time:129739ms step_avg:60.80ms
step:2135/2315 train_time:129800ms step_avg:60.80ms
step:2136/2315 train_time:129861ms step_avg:60.80ms
step:2137/2315 train_time:129922ms step_avg:60.80ms
step:2138/2315 train_time:129984ms step_avg:60.80ms
step:2139/2315 train_time:130045ms step_avg:60.80ms
step:2140/2315 train_time:130107ms step_avg:60.80ms
step:2141/2315 train_time:130168ms step_avg:60.80ms
step:2142/2315 train_time:130229ms step_avg:60.80ms
step:2143/2315 train_time:130290ms step_avg:60.80ms
step:2144/2315 train_time:130351ms step_avg:60.80ms
step:2145/2315 train_time:130412ms step_avg:60.80ms
step:2146/2315 train_time:130474ms step_avg:60.80ms
step:2147/2315 train_time:130534ms step_avg:60.80ms
step:2148/2315 train_time:130596ms step_avg:60.80ms
step:2149/2315 train_time:130657ms step_avg:60.80ms
step:2150/2315 train_time:130719ms step_avg:60.80ms
step:2151/2315 train_time:130780ms step_avg:60.80ms
step:2152/2315 train_time:130841ms step_avg:60.80ms
step:2153/2315 train_time:130902ms step_avg:60.80ms
step:2154/2315 train_time:130964ms step_avg:60.80ms
step:2155/2315 train_time:131026ms step_avg:60.80ms
step:2156/2315 train_time:131087ms step_avg:60.80ms
step:2157/2315 train_time:131148ms step_avg:60.80ms
step:2158/2315 train_time:131209ms step_avg:60.80ms
step:2159/2315 train_time:131270ms step_avg:60.80ms
step:2160/2315 train_time:131332ms step_avg:60.80ms
step:2161/2315 train_time:131393ms step_avg:60.80ms
step:2162/2315 train_time:131454ms step_avg:60.80ms
step:2163/2315 train_time:131515ms step_avg:60.80ms
step:2164/2315 train_time:131577ms step_avg:60.80ms
step:2165/2315 train_time:131638ms step_avg:60.80ms
step:2166/2315 train_time:131700ms step_avg:60.80ms
step:2167/2315 train_time:131760ms step_avg:60.80ms
step:2168/2315 train_time:131822ms step_avg:60.80ms
step:2169/2315 train_time:131883ms step_avg:60.80ms
step:2170/2315 train_time:131945ms step_avg:60.80ms
step:2171/2315 train_time:132007ms step_avg:60.80ms
step:2172/2315 train_time:132068ms step_avg:60.80ms
step:2173/2315 train_time:132129ms step_avg:60.80ms
step:2174/2315 train_time:132191ms step_avg:60.81ms
step:2175/2315 train_time:132252ms step_avg:60.81ms
step:2176/2315 train_time:132313ms step_avg:60.81ms
step:2177/2315 train_time:132375ms step_avg:60.81ms
step:2178/2315 train_time:132437ms step_avg:60.81ms
step:2179/2315 train_time:132498ms step_avg:60.81ms
step:2180/2315 train_time:132560ms step_avg:60.81ms
step:2181/2315 train_time:132621ms step_avg:60.81ms
step:2182/2315 train_time:132683ms step_avg:60.81ms
step:2183/2315 train_time:132744ms step_avg:60.81ms
step:2184/2315 train_time:132805ms step_avg:60.81ms
step:2185/2315 train_time:132867ms step_avg:60.81ms
step:2186/2315 train_time:132929ms step_avg:60.81ms
step:2187/2315 train_time:132990ms step_avg:60.81ms
step:2188/2315 train_time:133052ms step_avg:60.81ms
step:2189/2315 train_time:133112ms step_avg:60.81ms
step:2190/2315 train_time:133174ms step_avg:60.81ms
step:2191/2315 train_time:133234ms step_avg:60.81ms
step:2192/2315 train_time:133296ms step_avg:60.81ms
step:2193/2315 train_time:133357ms step_avg:60.81ms
step:2194/2315 train_time:133419ms step_avg:60.81ms
step:2195/2315 train_time:133480ms step_avg:60.81ms
step:2196/2315 train_time:133542ms step_avg:60.81ms
step:2197/2315 train_time:133604ms step_avg:60.81ms
step:2198/2315 train_time:133666ms step_avg:60.81ms
step:2199/2315 train_time:133727ms step_avg:60.81ms
step:2200/2315 train_time:133788ms step_avg:60.81ms
step:2201/2315 train_time:133849ms step_avg:60.81ms
step:2202/2315 train_time:133911ms step_avg:60.81ms
step:2203/2315 train_time:133971ms step_avg:60.81ms
step:2204/2315 train_time:134033ms step_avg:60.81ms
step:2205/2315 train_time:134094ms step_avg:60.81ms
step:2206/2315 train_time:134156ms step_avg:60.81ms
step:2207/2315 train_time:134217ms step_avg:60.81ms
step:2208/2315 train_time:134278ms step_avg:60.81ms
step:2209/2315 train_time:134339ms step_avg:60.81ms
step:2210/2315 train_time:134401ms step_avg:60.81ms
step:2211/2315 train_time:134463ms step_avg:60.82ms
step:2212/2315 train_time:134525ms step_avg:60.82ms
step:2213/2315 train_time:134585ms step_avg:60.82ms
step:2214/2315 train_time:134647ms step_avg:60.82ms
step:2215/2315 train_time:134709ms step_avg:60.82ms
step:2216/2315 train_time:134771ms step_avg:60.82ms
step:2217/2315 train_time:134832ms step_avg:60.82ms
step:2218/2315 train_time:134893ms step_avg:60.82ms
step:2219/2315 train_time:134954ms step_avg:60.82ms
step:2220/2315 train_time:135017ms step_avg:60.82ms
step:2221/2315 train_time:135077ms step_avg:60.82ms
step:2222/2315 train_time:135138ms step_avg:60.82ms
step:2223/2315 train_time:135199ms step_avg:60.82ms
step:2224/2315 train_time:135260ms step_avg:60.82ms
step:2225/2315 train_time:135321ms step_avg:60.82ms
step:2226/2315 train_time:135384ms step_avg:60.82ms
step:2227/2315 train_time:135445ms step_avg:60.82ms
step:2228/2315 train_time:135507ms step_avg:60.82ms
step:2229/2315 train_time:135568ms step_avg:60.82ms
step:2230/2315 train_time:135629ms step_avg:60.82ms
step:2231/2315 train_time:135690ms step_avg:60.82ms
step:2232/2315 train_time:135752ms step_avg:60.82ms
step:2233/2315 train_time:135812ms step_avg:60.82ms
step:2234/2315 train_time:135874ms step_avg:60.82ms
step:2235/2315 train_time:135935ms step_avg:60.82ms
step:2236/2315 train_time:135996ms step_avg:60.82ms
step:2237/2315 train_time:136058ms step_avg:60.82ms
step:2238/2315 train_time:136120ms step_avg:60.82ms
step:2239/2315 train_time:136181ms step_avg:60.82ms
step:2240/2315 train_time:136243ms step_avg:60.82ms
step:2241/2315 train_time:136304ms step_avg:60.82ms
step:2242/2315 train_time:136366ms step_avg:60.82ms
step:2243/2315 train_time:136427ms step_avg:60.82ms
step:2244/2315 train_time:136489ms step_avg:60.82ms
step:2245/2315 train_time:136550ms step_avg:60.82ms
step:2246/2315 train_time:136612ms step_avg:60.82ms
step:2247/2315 train_time:136672ms step_avg:60.82ms
step:2248/2315 train_time:136733ms step_avg:60.82ms
step:2249/2315 train_time:136794ms step_avg:60.82ms
step:2250/2315 train_time:136856ms step_avg:60.82ms
step:2250/2315 val_loss:3.2901 train_time:136918ms step_avg:60.85ms
step:2251/2315 train_time:136936ms step_avg:60.83ms
step:2252/2315 train_time:136982ms step_avg:60.83ms
step:2253/2315 train_time:137047ms step_avg:60.83ms
step:2254/2315 train_time:137111ms step_avg:60.83ms
step:2255/2315 train_time:137172ms step_avg:60.83ms
step:2256/2315 train_time:137234ms step_avg:60.83ms
step:2257/2315 train_time:137294ms step_avg:60.83ms
step:2258/2315 train_time:137355ms step_avg:60.83ms
step:2259/2315 train_time:137415ms step_avg:60.83ms
step:2260/2315 train_time:137476ms step_avg:60.83ms
step:2261/2315 train_time:137537ms step_avg:60.83ms
step:2262/2315 train_time:137598ms step_avg:60.83ms
step:2263/2315 train_time:137658ms step_avg:60.83ms
step:2264/2315 train_time:137718ms step_avg:60.83ms
step:2265/2315 train_time:137778ms step_avg:60.83ms
step:2266/2315 train_time:137840ms step_avg:60.83ms
step:2267/2315 train_time:137902ms step_avg:60.83ms
step:2268/2315 train_time:137966ms step_avg:60.83ms
step:2269/2315 train_time:138029ms step_avg:60.83ms
step:2270/2315 train_time:138092ms step_avg:60.83ms
step:2271/2315 train_time:138153ms step_avg:60.83ms
step:2272/2315 train_time:138215ms step_avg:60.83ms
step:2273/2315 train_time:138275ms step_avg:60.83ms
step:2274/2315 train_time:138337ms step_avg:60.83ms
step:2275/2315 train_time:138397ms step_avg:60.83ms
step:2276/2315 train_time:138457ms step_avg:60.83ms
step:2277/2315 train_time:138518ms step_avg:60.83ms
step:2278/2315 train_time:138579ms step_avg:60.83ms
step:2279/2315 train_time:138639ms step_avg:60.83ms
step:2280/2315 train_time:138700ms step_avg:60.83ms
step:2281/2315 train_time:138760ms step_avg:60.83ms
step:2282/2315 train_time:138821ms step_avg:60.83ms
step:2283/2315 train_time:138882ms step_avg:60.83ms
step:2284/2315 train_time:138945ms step_avg:60.83ms
step:2285/2315 train_time:139006ms step_avg:60.83ms
step:2286/2315 train_time:139069ms step_avg:60.83ms
step:2287/2315 train_time:139131ms step_avg:60.84ms
step:2288/2315 train_time:139193ms step_avg:60.84ms
step:2289/2315 train_time:139254ms step_avg:60.84ms
step:2290/2315 train_time:139316ms step_avg:60.84ms
step:2291/2315 train_time:139376ms step_avg:60.84ms
step:2292/2315 train_time:139437ms step_avg:60.84ms
step:2293/2315 train_time:139498ms step_avg:60.84ms
step:2294/2315 train_time:139559ms step_avg:60.84ms
step:2295/2315 train_time:139619ms step_avg:60.84ms
step:2296/2315 train_time:139680ms step_avg:60.84ms
step:2297/2315 train_time:139741ms step_avg:60.84ms
step:2298/2315 train_time:139802ms step_avg:60.84ms
step:2299/2315 train_time:139863ms step_avg:60.84ms
step:2300/2315 train_time:139925ms step_avg:60.84ms
step:2301/2315 train_time:139987ms step_avg:60.84ms
step:2302/2315 train_time:140050ms step_avg:60.84ms
step:2303/2315 train_time:140112ms step_avg:60.84ms
step:2304/2315 train_time:140174ms step_avg:60.84ms
step:2305/2315 train_time:140235ms step_avg:60.84ms
step:2306/2315 train_time:140297ms step_avg:60.84ms
step:2307/2315 train_time:140358ms step_avg:60.84ms
step:2308/2315 train_time:140420ms step_avg:60.84ms
step:2309/2315 train_time:140480ms step_avg:60.84ms
step:2310/2315 train_time:140541ms step_avg:60.84ms
step:2311/2315 train_time:140602ms step_avg:60.84ms
step:2312/2315 train_time:140664ms step_avg:60.84ms
step:2313/2315 train_time:140724ms step_avg:60.84ms
step:2314/2315 train_time:140786ms step_avg:60.84ms
step:2315/2315 train_time:140847ms step_avg:60.84ms
step:2315/2315 val_loss:3.2775 train_time:140909ms step_avg:60.87ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)


            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))



            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal
        self.group_to_param_group = {}

        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                self.group_to_param_group[param] = group

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        world_size = dist.get_world_size()
        grad = param.grad
        rank_size = grad.shape[0] // world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(), grad_slice)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']

            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
        # enable sync in the next training step for the adam optimizer
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = next(train_loader)

        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()


====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251031+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 31 16:20:33 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   39C    P0            127W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   32C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2315 train_time:79ms step_avg:79.24ms
step:2/2315 train_time:196ms step_avg:97.83ms
step:3/2315 train_time:215ms step_avg:71.62ms
step:4/2315 train_time:255ms step_avg:63.64ms
step:5/2315 train_time:312ms step_avg:62.43ms
step:6/2315 train_time:372ms step_avg:62.01ms
step:7/2315 train_time:431ms step_avg:61.50ms
step:8/2315 train_time:491ms step_avg:61.32ms
step:9/2315 train_time:550ms step_avg:61.07ms
step:10/2315 train_time:609ms step_avg:60.95ms
step:11/2315 train_time:668ms step_avg:60.76ms
step:12/2315 train_time:728ms step_avg:60.68ms
step:13/2315 train_time:788ms step_avg:60.61ms
step:14/2315 train_time:848ms step_avg:60.57ms
step:15/2315 train_time:907ms step_avg:60.46ms
step:16/2315 train_time:967ms step_avg:60.45ms
step:17/2315 train_time:1027ms step_avg:60.42ms
step:18/2315 train_time:1089ms step_avg:60.51ms
step:19/2315 train_time:1154ms step_avg:60.75ms
step:20/2315 train_time:1218ms step_avg:60.88ms
step:21/2315 train_time:1278ms step_avg:60.87ms
step:22/2315 train_time:1339ms step_avg:60.87ms
step:23/2315 train_time:1399ms step_avg:60.83ms
step:24/2315 train_time:1460ms step_avg:60.83ms
step:25/2315 train_time:1521ms step_avg:60.83ms
step:26/2315 train_time:1582ms step_avg:60.85ms
step:27/2315 train_time:1642ms step_avg:60.81ms
step:28/2315 train_time:1703ms step_avg:60.83ms
step:29/2315 train_time:1764ms step_avg:60.82ms
step:30/2315 train_time:1824ms step_avg:60.81ms
step:31/2315 train_time:1884ms step_avg:60.78ms
step:32/2315 train_time:1945ms step_avg:60.77ms
step:33/2315 train_time:2004ms step_avg:60.74ms
step:34/2315 train_time:2066ms step_avg:60.77ms
step:35/2315 train_time:2128ms step_avg:60.79ms
step:36/2315 train_time:2190ms step_avg:60.84ms
step:37/2315 train_time:2251ms step_avg:60.84ms
step:38/2315 train_time:2312ms step_avg:60.85ms
step:39/2315 train_time:2372ms step_avg:60.82ms
step:40/2315 train_time:2432ms step_avg:60.81ms
step:41/2315 train_time:2493ms step_avg:60.79ms
step:42/2315 train_time:2553ms step_avg:60.78ms
step:43/2315 train_time:2612ms step_avg:60.75ms
step:44/2315 train_time:2673ms step_avg:60.76ms
step:45/2315 train_time:2733ms step_avg:60.74ms
step:46/2315 train_time:2794ms step_avg:60.74ms
step:47/2315 train_time:2855ms step_avg:60.75ms
step:48/2315 train_time:2916ms step_avg:60.75ms
step:49/2315 train_time:2976ms step_avg:60.74ms
step:50/2315 train_time:3037ms step_avg:60.75ms
step:51/2315 train_time:3098ms step_avg:60.74ms
step:52/2315 train_time:3160ms step_avg:60.77ms
step:53/2315 train_time:3220ms step_avg:60.76ms
step:54/2315 train_time:3282ms step_avg:60.77ms
step:55/2315 train_time:3342ms step_avg:60.76ms
step:56/2315 train_time:3403ms step_avg:60.76ms
step:57/2315 train_time:3463ms step_avg:60.76ms
step:58/2315 train_time:3524ms step_avg:60.76ms
step:59/2315 train_time:3584ms step_avg:60.75ms
step:60/2315 train_time:3645ms step_avg:60.75ms
step:61/2315 train_time:3705ms step_avg:60.74ms
step:62/2315 train_time:3766ms step_avg:60.74ms
step:63/2315 train_time:3826ms step_avg:60.73ms
step:64/2315 train_time:3886ms step_avg:60.72ms
step:65/2315 train_time:3946ms step_avg:60.71ms
step:66/2315 train_time:4006ms step_avg:60.70ms
step:67/2315 train_time:4067ms step_avg:60.69ms
step:68/2315 train_time:4127ms step_avg:60.69ms
step:69/2315 train_time:4187ms step_avg:60.68ms
step:70/2315 train_time:4247ms step_avg:60.68ms
step:71/2315 train_time:4308ms step_avg:60.67ms
step:72/2315 train_time:4368ms step_avg:60.67ms
step:73/2315 train_time:4427ms step_avg:60.65ms
step:74/2315 train_time:4488ms step_avg:60.65ms
step:75/2315 train_time:4548ms step_avg:60.64ms
step:76/2315 train_time:4608ms step_avg:60.63ms
step:77/2315 train_time:4668ms step_avg:60.62ms
step:78/2315 train_time:4728ms step_avg:60.62ms
step:79/2315 train_time:4789ms step_avg:60.62ms
step:80/2315 train_time:4849ms step_avg:60.61ms
step:81/2315 train_time:4909ms step_avg:60.60ms
step:82/2315 train_time:4969ms step_avg:60.60ms
step:83/2315 train_time:5029ms step_avg:60.59ms
step:84/2315 train_time:5090ms step_avg:60.59ms
step:85/2315 train_time:5149ms step_avg:60.58ms
step:86/2315 train_time:5209ms step_avg:60.57ms
step:87/2315 train_time:5269ms step_avg:60.57ms
step:88/2315 train_time:5329ms step_avg:60.56ms
step:89/2315 train_time:5390ms step_avg:60.56ms
step:90/2315 train_time:5450ms step_avg:60.55ms
step:91/2315 train_time:5509ms step_avg:60.54ms
step:92/2315 train_time:5570ms step_avg:60.54ms
step:93/2315 train_time:5629ms step_avg:60.53ms
step:94/2315 train_time:5690ms step_avg:60.53ms
step:95/2315 train_time:5749ms step_avg:60.52ms
step:96/2315 train_time:5810ms step_avg:60.52ms
step:97/2315 train_time:5870ms step_avg:60.51ms
step:98/2315 train_time:5930ms step_avg:60.51ms
step:99/2315 train_time:5990ms step_avg:60.50ms
step:100/2315 train_time:6050ms step_avg:60.50ms
step:101/2315 train_time:6110ms step_avg:60.50ms
step:102/2315 train_time:6170ms step_avg:60.49ms
step:103/2315 train_time:6230ms step_avg:60.49ms
step:104/2315 train_time:6290ms step_avg:60.48ms
step:105/2315 train_time:6350ms step_avg:60.48ms
step:106/2315 train_time:6411ms step_avg:60.48ms
step:107/2315 train_time:6470ms step_avg:60.47ms
step:108/2315 train_time:6531ms step_avg:60.47ms
step:109/2315 train_time:6590ms step_avg:60.46ms
step:110/2315 train_time:6651ms step_avg:60.46ms
step:111/2315 train_time:6711ms step_avg:60.46ms
step:112/2315 train_time:6771ms step_avg:60.45ms
step:113/2315 train_time:6830ms step_avg:60.44ms
step:114/2315 train_time:6890ms step_avg:60.44ms
step:115/2315 train_time:6949ms step_avg:60.43ms
step:116/2315 train_time:7010ms step_avg:60.43ms
step:117/2315 train_time:7070ms step_avg:60.43ms
step:118/2315 train_time:7130ms step_avg:60.42ms
step:119/2315 train_time:7190ms step_avg:60.42ms
step:120/2315 train_time:7251ms step_avg:60.43ms
step:121/2315 train_time:7311ms step_avg:60.42ms
step:122/2315 train_time:7372ms step_avg:60.42ms
step:123/2315 train_time:7431ms step_avg:60.42ms
step:124/2315 train_time:7491ms step_avg:60.41ms
step:125/2315 train_time:7551ms step_avg:60.41ms
step:126/2315 train_time:7612ms step_avg:60.41ms
step:127/2315 train_time:7672ms step_avg:60.41ms
step:128/2315 train_time:7732ms step_avg:60.40ms
step:129/2315 train_time:7791ms step_avg:60.40ms
step:130/2315 train_time:7851ms step_avg:60.39ms
step:131/2315 train_time:7911ms step_avg:60.39ms
step:132/2315 train_time:7971ms step_avg:60.39ms
step:133/2315 train_time:8031ms step_avg:60.38ms
step:134/2315 train_time:8091ms step_avg:60.38ms
step:135/2315 train_time:8152ms step_avg:60.38ms
step:136/2315 train_time:8212ms step_avg:60.38ms
step:137/2315 train_time:8272ms step_avg:60.38ms
step:138/2315 train_time:8332ms step_avg:60.38ms
step:139/2315 train_time:8391ms step_avg:60.37ms
step:140/2315 train_time:8451ms step_avg:60.37ms
step:141/2315 train_time:8511ms step_avg:60.36ms
step:142/2315 train_time:8572ms step_avg:60.37ms
step:143/2315 train_time:8632ms step_avg:60.36ms
step:144/2315 train_time:8691ms step_avg:60.36ms
step:145/2315 train_time:8751ms step_avg:60.35ms
step:146/2315 train_time:8811ms step_avg:60.35ms
step:147/2315 train_time:8871ms step_avg:60.35ms
step:148/2315 train_time:8932ms step_avg:60.35ms
step:149/2315 train_time:8991ms step_avg:60.34ms
step:150/2315 train_time:9051ms step_avg:60.34ms
step:151/2315 train_time:9110ms step_avg:60.33ms
step:152/2315 train_time:9171ms step_avg:60.34ms
step:153/2315 train_time:9231ms step_avg:60.33ms
step:154/2315 train_time:9291ms step_avg:60.33ms
step:155/2315 train_time:9351ms step_avg:60.33ms
step:156/2315 train_time:9411ms step_avg:60.33ms
step:157/2315 train_time:9471ms step_avg:60.32ms
step:158/2315 train_time:9532ms step_avg:60.33ms
step:159/2315 train_time:9591ms step_avg:60.32ms
step:160/2315 train_time:9651ms step_avg:60.32ms
step:161/2315 train_time:9711ms step_avg:60.32ms
step:162/2315 train_time:9771ms step_avg:60.31ms
step:163/2315 train_time:9830ms step_avg:60.31ms
step:164/2315 train_time:9890ms step_avg:60.31ms
step:165/2315 train_time:9950ms step_avg:60.30ms
step:166/2315 train_time:10010ms step_avg:60.30ms
step:167/2315 train_time:10070ms step_avg:60.30ms
step:168/2315 train_time:10129ms step_avg:60.29ms
step:169/2315 train_time:10189ms step_avg:60.29ms
step:170/2315 train_time:10249ms step_avg:60.29ms
step:171/2315 train_time:10309ms step_avg:60.29ms
step:172/2315 train_time:10369ms step_avg:60.29ms
step:173/2315 train_time:10429ms step_avg:60.28ms
step:174/2315 train_time:10489ms step_avg:60.28ms
step:175/2315 train_time:10549ms step_avg:60.28ms
step:176/2315 train_time:10609ms step_avg:60.28ms
step:177/2315 train_time:10669ms step_avg:60.28ms
step:178/2315 train_time:10730ms step_avg:60.28ms
step:179/2315 train_time:10789ms step_avg:60.28ms
step:180/2315 train_time:10850ms step_avg:60.28ms
step:181/2315 train_time:10909ms step_avg:60.27ms
step:182/2315 train_time:10970ms step_avg:60.27ms
step:183/2315 train_time:11029ms step_avg:60.27ms
step:184/2315 train_time:11089ms step_avg:60.27ms
step:185/2315 train_time:11150ms step_avg:60.27ms
step:186/2315 train_time:11209ms step_avg:60.27ms
step:187/2315 train_time:11269ms step_avg:60.26ms
step:188/2315 train_time:11329ms step_avg:60.26ms
step:189/2315 train_time:11389ms step_avg:60.26ms
step:190/2315 train_time:11449ms step_avg:60.26ms
step:191/2315 train_time:11509ms step_avg:60.25ms
step:192/2315 train_time:11569ms step_avg:60.25ms
step:193/2315 train_time:11628ms step_avg:60.25ms
step:194/2315 train_time:11689ms step_avg:60.25ms
step:195/2315 train_time:11748ms step_avg:60.25ms
step:196/2315 train_time:11808ms step_avg:60.25ms
step:197/2315 train_time:11868ms step_avg:60.24ms
step:198/2315 train_time:11928ms step_avg:60.24ms
step:199/2315 train_time:11987ms step_avg:60.24ms
step:200/2315 train_time:12047ms step_avg:60.24ms
step:201/2315 train_time:12107ms step_avg:60.23ms
step:202/2315 train_time:12167ms step_avg:60.23ms
step:203/2315 train_time:12227ms step_avg:60.23ms
step:204/2315 train_time:12287ms step_avg:60.23ms
step:205/2315 train_time:12346ms step_avg:60.23ms
step:206/2315 train_time:12406ms step_avg:60.22ms
step:207/2315 train_time:12466ms step_avg:60.22ms
step:208/2315 train_time:12526ms step_avg:60.22ms
step:209/2315 train_time:12586ms step_avg:60.22ms
step:210/2315 train_time:12646ms step_avg:60.22ms
step:211/2315 train_time:12705ms step_avg:60.22ms
step:212/2315 train_time:12766ms step_avg:60.22ms
step:213/2315 train_time:12826ms step_avg:60.22ms
step:214/2315 train_time:12887ms step_avg:60.22ms
step:215/2315 train_time:12947ms step_avg:60.22ms
step:216/2315 train_time:13007ms step_avg:60.22ms
step:217/2315 train_time:13066ms step_avg:60.21ms
step:218/2315 train_time:13126ms step_avg:60.21ms
step:219/2315 train_time:13186ms step_avg:60.21ms
step:220/2315 train_time:13246ms step_avg:60.21ms
step:221/2315 train_time:13306ms step_avg:60.21ms
step:222/2315 train_time:13366ms step_avg:60.21ms
step:223/2315 train_time:13426ms step_avg:60.21ms
step:224/2315 train_time:13487ms step_avg:60.21ms
step:225/2315 train_time:13547ms step_avg:60.21ms
step:226/2315 train_time:13607ms step_avg:60.21ms
step:227/2315 train_time:13667ms step_avg:60.20ms
step:228/2315 train_time:13727ms step_avg:60.20ms
step:229/2315 train_time:13786ms step_avg:60.20ms
step:230/2315 train_time:13847ms step_avg:60.20ms
step:231/2315 train_time:13906ms step_avg:60.20ms
step:232/2315 train_time:13966ms step_avg:60.20ms
step:233/2315 train_time:14026ms step_avg:60.20ms
step:234/2315 train_time:14086ms step_avg:60.20ms
step:235/2315 train_time:14146ms step_avg:60.20ms
step:236/2315 train_time:14206ms step_avg:60.20ms
step:237/2315 train_time:14266ms step_avg:60.19ms
step:238/2315 train_time:14326ms step_avg:60.19ms
step:239/2315 train_time:14385ms step_avg:60.19ms
step:240/2315 train_time:14446ms step_avg:60.19ms
step:241/2315 train_time:14505ms step_avg:60.19ms
step:242/2315 train_time:14566ms step_avg:60.19ms
step:243/2315 train_time:14625ms step_avg:60.19ms
step:244/2315 train_time:14686ms step_avg:60.19ms
step:245/2315 train_time:14746ms step_avg:60.19ms
step:246/2315 train_time:14806ms step_avg:60.19ms
step:247/2315 train_time:14866ms step_avg:60.18ms
step:248/2315 train_time:14927ms step_avg:60.19ms
step:249/2315 train_time:14986ms step_avg:60.19ms
step:250/2315 train_time:15046ms step_avg:60.19ms
step:250/2315 val_loss:4.0804 train_time:15108ms step_avg:60.43ms
step:251/2315 train_time:15127ms step_avg:60.27ms
step:252/2315 train_time:15168ms step_avg:60.19ms
step:253/2315 train_time:15233ms step_avg:60.21ms
step:254/2315 train_time:15300ms step_avg:60.23ms
step:255/2315 train_time:15361ms step_avg:60.24ms
step:256/2315 train_time:15422ms step_avg:60.24ms
step:257/2315 train_time:15482ms step_avg:60.24ms
step:258/2315 train_time:15542ms step_avg:60.24ms
step:259/2315 train_time:15602ms step_avg:60.24ms
step:260/2315 train_time:15661ms step_avg:60.23ms
step:261/2315 train_time:15720ms step_avg:60.23ms
step:262/2315 train_time:15780ms step_avg:60.23ms
step:263/2315 train_time:15838ms step_avg:60.22ms
step:264/2315 train_time:15898ms step_avg:60.22ms
step:265/2315 train_time:15957ms step_avg:60.21ms
step:266/2315 train_time:16016ms step_avg:60.21ms
step:267/2315 train_time:16075ms step_avg:60.21ms
step:268/2315 train_time:16137ms step_avg:60.21ms
step:269/2315 train_time:16200ms step_avg:60.22ms
step:270/2315 train_time:16263ms step_avg:60.23ms
step:271/2315 train_time:16324ms step_avg:60.24ms
step:272/2315 train_time:16385ms step_avg:60.24ms
step:273/2315 train_time:16445ms step_avg:60.24ms
step:274/2315 train_time:16505ms step_avg:60.24ms
step:275/2315 train_time:16564ms step_avg:60.23ms
step:276/2315 train_time:16624ms step_avg:60.23ms
step:277/2315 train_time:16683ms step_avg:60.23ms
step:278/2315 train_time:16743ms step_avg:60.22ms
step:279/2315 train_time:16802ms step_avg:60.22ms
step:280/2315 train_time:16861ms step_avg:60.22ms
step:281/2315 train_time:16920ms step_avg:60.21ms
step:282/2315 train_time:16980ms step_avg:60.21ms
step:283/2315 train_time:17039ms step_avg:60.21ms
step:284/2315 train_time:17101ms step_avg:60.21ms
step:285/2315 train_time:17162ms step_avg:60.22ms
step:286/2315 train_time:17223ms step_avg:60.22ms
step:287/2315 train_time:17284ms step_avg:60.22ms
step:288/2315 train_time:17345ms step_avg:60.23ms
step:289/2315 train_time:17406ms step_avg:60.23ms
step:290/2315 train_time:17466ms step_avg:60.23ms
step:291/2315 train_time:17525ms step_avg:60.22ms
step:292/2315 train_time:17585ms step_avg:60.22ms
step:293/2315 train_time:17644ms step_avg:60.22ms
step:294/2315 train_time:17705ms step_avg:60.22ms
step:295/2315 train_time:17763ms step_avg:60.22ms
step:296/2315 train_time:17823ms step_avg:60.21ms
step:297/2315 train_time:17882ms step_avg:60.21ms
step:298/2315 train_time:17942ms step_avg:60.21ms
step:299/2315 train_time:18002ms step_avg:60.21ms
step:300/2315 train_time:18062ms step_avg:60.21ms
step:301/2315 train_time:18122ms step_avg:60.21ms
step:302/2315 train_time:18183ms step_avg:60.21ms
step:303/2315 train_time:18244ms step_avg:60.21ms
step:304/2315 train_time:18305ms step_avg:60.21ms
step:305/2315 train_time:18366ms step_avg:60.22ms
step:306/2315 train_time:18426ms step_avg:60.22ms
step:307/2315 train_time:18486ms step_avg:60.21ms
step:308/2315 train_time:18546ms step_avg:60.21ms
step:309/2315 train_time:18605ms step_avg:60.21ms
step:310/2315 train_time:18665ms step_avg:60.21ms
step:311/2315 train_time:18724ms step_avg:60.21ms
step:312/2315 train_time:18784ms step_avg:60.20ms
step:313/2315 train_time:18843ms step_avg:60.20ms
step:314/2315 train_time:18902ms step_avg:60.20ms
step:315/2315 train_time:18962ms step_avg:60.20ms
step:316/2315 train_time:19022ms step_avg:60.19ms
step:317/2315 train_time:19081ms step_avg:60.19ms
step:318/2315 train_time:19143ms step_avg:60.20ms
step:319/2315 train_time:19203ms step_avg:60.20ms
step:320/2315 train_time:19264ms step_avg:60.20ms
step:321/2315 train_time:19325ms step_avg:60.20ms
step:322/2315 train_time:19385ms step_avg:60.20ms
step:323/2315 train_time:19445ms step_avg:60.20ms
step:324/2315 train_time:19505ms step_avg:60.20ms
step:325/2315 train_time:19565ms step_avg:60.20ms
step:326/2315 train_time:19624ms step_avg:60.20ms
step:327/2315 train_time:19684ms step_avg:60.19ms
step:328/2315 train_time:19743ms step_avg:60.19ms
step:329/2315 train_time:19802ms step_avg:60.19ms
step:330/2315 train_time:19862ms step_avg:60.19ms
step:331/2315 train_time:19922ms step_avg:60.19ms
step:332/2315 train_time:19982ms step_avg:60.19ms
step:333/2315 train_time:20041ms step_avg:60.18ms
step:334/2315 train_time:20101ms step_avg:60.18ms
step:335/2315 train_time:20161ms step_avg:60.18ms
step:336/2315 train_time:20223ms step_avg:60.19ms
step:337/2315 train_time:20283ms step_avg:60.19ms
step:338/2315 train_time:20343ms step_avg:60.19ms
step:339/2315 train_time:20403ms step_avg:60.19ms
step:340/2315 train_time:20464ms step_avg:60.19ms
step:341/2315 train_time:20524ms step_avg:60.19ms
step:342/2315 train_time:20584ms step_avg:60.19ms
step:343/2315 train_time:20644ms step_avg:60.19ms
step:344/2315 train_time:20704ms step_avg:60.19ms
step:345/2315 train_time:20763ms step_avg:60.18ms
step:346/2315 train_time:20824ms step_avg:60.18ms
step:347/2315 train_time:20882ms step_avg:60.18ms
step:348/2315 train_time:20942ms step_avg:60.18ms
step:349/2315 train_time:21002ms step_avg:60.18ms
step:350/2315 train_time:21062ms step_avg:60.18ms
step:351/2315 train_time:21122ms step_avg:60.18ms
step:352/2315 train_time:21183ms step_avg:60.18ms
step:353/2315 train_time:21243ms step_avg:60.18ms
step:354/2315 train_time:21304ms step_avg:60.18ms
step:355/2315 train_time:21365ms step_avg:60.18ms
step:356/2315 train_time:21425ms step_avg:60.18ms
step:357/2315 train_time:21485ms step_avg:60.18ms
step:358/2315 train_time:21546ms step_avg:60.18ms
step:359/2315 train_time:21605ms step_avg:60.18ms
step:360/2315 train_time:21665ms step_avg:60.18ms
step:361/2315 train_time:21724ms step_avg:60.18ms
step:362/2315 train_time:21784ms step_avg:60.18ms
step:363/2315 train_time:21843ms step_avg:60.17ms
step:364/2315 train_time:21904ms step_avg:60.17ms
step:365/2315 train_time:21963ms step_avg:60.17ms
step:366/2315 train_time:22023ms step_avg:60.17ms
step:367/2315 train_time:22083ms step_avg:60.17ms
step:368/2315 train_time:22143ms step_avg:60.17ms
step:369/2315 train_time:22204ms step_avg:60.17ms
step:370/2315 train_time:22264ms step_avg:60.17ms
step:371/2315 train_time:22324ms step_avg:60.17ms
step:372/2315 train_time:22385ms step_avg:60.17ms
step:373/2315 train_time:22444ms step_avg:60.17ms
step:374/2315 train_time:22505ms step_avg:60.17ms
step:375/2315 train_time:22564ms step_avg:60.17ms
step:376/2315 train_time:22624ms step_avg:60.17ms
step:377/2315 train_time:22683ms step_avg:60.17ms
step:378/2315 train_time:22744ms step_avg:60.17ms
step:379/2315 train_time:22804ms step_avg:60.17ms
step:380/2315 train_time:22864ms step_avg:60.17ms
step:381/2315 train_time:22923ms step_avg:60.17ms
step:382/2315 train_time:22984ms step_avg:60.17ms
step:383/2315 train_time:23043ms step_avg:60.16ms
step:384/2315 train_time:23103ms step_avg:60.16ms
step:385/2315 train_time:23166ms step_avg:60.17ms
step:386/2315 train_time:23224ms step_avg:60.17ms
step:387/2315 train_time:23284ms step_avg:60.17ms
step:388/2315 train_time:23345ms step_avg:60.17ms
step:389/2315 train_time:23404ms step_avg:60.17ms
step:390/2315 train_time:23465ms step_avg:60.17ms
step:391/2315 train_time:23524ms step_avg:60.16ms
step:392/2315 train_time:23585ms step_avg:60.16ms
step:393/2315 train_time:23644ms step_avg:60.16ms
step:394/2315 train_time:23704ms step_avg:60.16ms
step:395/2315 train_time:23764ms step_avg:60.16ms
step:396/2315 train_time:23823ms step_avg:60.16ms
step:397/2315 train_time:23883ms step_avg:60.16ms
step:398/2315 train_time:23943ms step_avg:60.16ms
step:399/2315 train_time:24003ms step_avg:60.16ms
step:400/2315 train_time:24064ms step_avg:60.16ms
step:401/2315 train_time:24123ms step_avg:60.16ms
step:402/2315 train_time:24184ms step_avg:60.16ms
step:403/2315 train_time:24244ms step_avg:60.16ms
step:404/2315 train_time:24305ms step_avg:60.16ms
step:405/2315 train_time:24364ms step_avg:60.16ms
step:406/2315 train_time:24424ms step_avg:60.16ms
step:407/2315 train_time:24484ms step_avg:60.16ms
step:408/2315 train_time:24545ms step_avg:60.16ms
step:409/2315 train_time:24605ms step_avg:60.16ms
step:410/2315 train_time:24665ms step_avg:60.16ms
step:411/2315 train_time:24725ms step_avg:60.16ms
step:412/2315 train_time:24785ms step_avg:60.16ms
step:413/2315 train_time:24844ms step_avg:60.16ms
step:414/2315 train_time:24904ms step_avg:60.16ms
step:415/2315 train_time:24964ms step_avg:60.15ms
step:416/2315 train_time:25024ms step_avg:60.15ms
step:417/2315 train_time:25083ms step_avg:60.15ms
step:418/2315 train_time:25143ms step_avg:60.15ms
step:419/2315 train_time:25203ms step_avg:60.15ms
step:420/2315 train_time:25263ms step_avg:60.15ms
step:421/2315 train_time:25323ms step_avg:60.15ms
step:422/2315 train_time:25383ms step_avg:60.15ms
step:423/2315 train_time:25443ms step_avg:60.15ms
step:424/2315 train_time:25503ms step_avg:60.15ms
step:425/2315 train_time:25564ms step_avg:60.15ms
step:426/2315 train_time:25625ms step_avg:60.15ms
step:427/2315 train_time:25685ms step_avg:60.15ms
step:428/2315 train_time:25744ms step_avg:60.15ms
step:429/2315 train_time:25803ms step_avg:60.15ms
step:430/2315 train_time:25864ms step_avg:60.15ms
step:431/2315 train_time:25924ms step_avg:60.15ms
step:432/2315 train_time:25984ms step_avg:60.15ms
step:433/2315 train_time:26043ms step_avg:60.15ms
step:434/2315 train_time:26103ms step_avg:60.15ms
step:435/2315 train_time:26164ms step_avg:60.15ms
step:436/2315 train_time:26224ms step_avg:60.15ms
step:437/2315 train_time:26284ms step_avg:60.15ms
step:438/2315 train_time:26344ms step_avg:60.15ms
step:439/2315 train_time:26404ms step_avg:60.15ms
step:440/2315 train_time:26464ms step_avg:60.15ms
step:441/2315 train_time:26524ms step_avg:60.14ms
step:442/2315 train_time:26584ms step_avg:60.14ms
step:443/2315 train_time:26644ms step_avg:60.15ms
step:444/2315 train_time:26705ms step_avg:60.15ms
step:445/2315 train_time:26765ms step_avg:60.15ms
step:446/2315 train_time:26826ms step_avg:60.15ms
step:447/2315 train_time:26885ms step_avg:60.14ms
step:448/2315 train_time:26944ms step_avg:60.14ms
step:449/2315 train_time:27004ms step_avg:60.14ms
step:450/2315 train_time:27064ms step_avg:60.14ms
step:451/2315 train_time:27123ms step_avg:60.14ms
step:452/2315 train_time:27184ms step_avg:60.14ms
step:453/2315 train_time:27244ms step_avg:60.14ms
step:454/2315 train_time:27304ms step_avg:60.14ms
step:455/2315 train_time:27365ms step_avg:60.14ms
step:456/2315 train_time:27424ms step_avg:60.14ms
step:457/2315 train_time:27483ms step_avg:60.14ms
step:458/2315 train_time:27543ms step_avg:60.14ms
step:459/2315 train_time:27603ms step_avg:60.14ms
step:460/2315 train_time:27665ms step_avg:60.14ms
step:461/2315 train_time:27724ms step_avg:60.14ms
step:462/2315 train_time:27783ms step_avg:60.14ms
step:463/2315 train_time:27843ms step_avg:60.14ms
step:464/2315 train_time:27903ms step_avg:60.14ms
step:465/2315 train_time:27963ms step_avg:60.14ms
step:466/2315 train_time:28024ms step_avg:60.14ms
step:467/2315 train_time:28083ms step_avg:60.13ms
step:468/2315 train_time:28143ms step_avg:60.13ms
step:469/2315 train_time:28202ms step_avg:60.13ms
step:470/2315 train_time:28263ms step_avg:60.13ms
step:471/2315 train_time:28322ms step_avg:60.13ms
step:472/2315 train_time:28382ms step_avg:60.13ms
step:473/2315 train_time:28442ms step_avg:60.13ms
step:474/2315 train_time:28503ms step_avg:60.13ms
step:475/2315 train_time:28563ms step_avg:60.13ms
step:476/2315 train_time:28624ms step_avg:60.13ms
step:477/2315 train_time:28683ms step_avg:60.13ms
step:478/2315 train_time:28743ms step_avg:60.13ms
step:479/2315 train_time:28803ms step_avg:60.13ms
step:480/2315 train_time:28863ms step_avg:60.13ms
step:481/2315 train_time:28923ms step_avg:60.13ms
step:482/2315 train_time:28983ms step_avg:60.13ms
step:483/2315 train_time:29043ms step_avg:60.13ms
step:484/2315 train_time:29103ms step_avg:60.13ms
step:485/2315 train_time:29163ms step_avg:60.13ms
step:486/2315 train_time:29223ms step_avg:60.13ms
step:487/2315 train_time:29282ms step_avg:60.13ms
step:488/2315 train_time:29342ms step_avg:60.13ms
step:489/2315 train_time:29402ms step_avg:60.13ms
step:490/2315 train_time:29464ms step_avg:60.13ms
step:491/2315 train_time:29523ms step_avg:60.13ms
step:492/2315 train_time:29583ms step_avg:60.13ms
step:493/2315 train_time:29643ms step_avg:60.13ms
step:494/2315 train_time:29703ms step_avg:60.13ms
step:495/2315 train_time:29763ms step_avg:60.13ms
step:496/2315 train_time:29824ms step_avg:60.13ms
step:497/2315 train_time:29883ms step_avg:60.13ms
step:498/2315 train_time:29943ms step_avg:60.13ms
step:499/2315 train_time:30003ms step_avg:60.13ms
step:500/2315 train_time:30063ms step_avg:60.13ms
step:500/2315 val_loss:3.8053 train_time:30125ms step_avg:60.25ms
step:501/2315 train_time:30144ms step_avg:60.17ms
step:502/2315 train_time:30187ms step_avg:60.13ms
step:503/2315 train_time:30249ms step_avg:60.14ms
step:504/2315 train_time:30315ms step_avg:60.15ms
step:505/2315 train_time:30373ms step_avg:60.14ms
step:506/2315 train_time:30433ms step_avg:60.15ms
step:507/2315 train_time:30492ms step_avg:60.14ms
step:508/2315 train_time:30551ms step_avg:60.14ms
step:509/2315 train_time:30610ms step_avg:60.14ms
step:510/2315 train_time:30669ms step_avg:60.14ms
step:511/2315 train_time:30728ms step_avg:60.13ms
step:512/2315 train_time:30788ms step_avg:60.13ms
step:513/2315 train_time:30847ms step_avg:60.13ms
step:514/2315 train_time:30906ms step_avg:60.13ms
step:515/2315 train_time:30965ms step_avg:60.13ms
step:516/2315 train_time:31025ms step_avg:60.13ms
step:517/2315 train_time:31087ms step_avg:60.13ms
step:518/2315 train_time:31150ms step_avg:60.14ms
step:519/2315 train_time:31212ms step_avg:60.14ms
step:520/2315 train_time:31273ms step_avg:60.14ms
step:521/2315 train_time:31334ms step_avg:60.14ms
step:522/2315 train_time:31395ms step_avg:60.14ms
step:523/2315 train_time:31454ms step_avg:60.14ms
step:524/2315 train_time:31514ms step_avg:60.14ms
step:525/2315 train_time:31573ms step_avg:60.14ms
step:526/2315 train_time:31632ms step_avg:60.14ms
step:527/2315 train_time:31691ms step_avg:60.13ms
step:528/2315 train_time:31751ms step_avg:60.13ms
step:529/2315 train_time:31810ms step_avg:60.13ms
step:530/2315 train_time:31870ms step_avg:60.13ms
step:531/2315 train_time:31929ms step_avg:60.13ms
step:532/2315 train_time:31989ms step_avg:60.13ms
step:533/2315 train_time:32049ms step_avg:60.13ms
step:534/2315 train_time:32110ms step_avg:60.13ms
step:535/2315 train_time:32171ms step_avg:60.13ms
step:536/2315 train_time:32232ms step_avg:60.13ms
step:537/2315 train_time:32292ms step_avg:60.13ms
step:538/2315 train_time:32353ms step_avg:60.14ms
step:539/2315 train_time:32413ms step_avg:60.13ms
step:540/2315 train_time:32473ms step_avg:60.13ms
step:541/2315 train_time:32533ms step_avg:60.13ms
step:542/2315 train_time:32593ms step_avg:60.13ms
step:543/2315 train_time:32651ms step_avg:60.13ms
step:544/2315 train_time:32711ms step_avg:60.13ms
step:545/2315 train_time:32770ms step_avg:60.13ms
step:546/2315 train_time:32829ms step_avg:60.13ms
step:547/2315 train_time:32888ms step_avg:60.12ms
step:548/2315 train_time:32948ms step_avg:60.12ms
step:549/2315 train_time:33008ms step_avg:60.12ms
step:550/2315 train_time:33068ms step_avg:60.12ms
step:551/2315 train_time:33129ms step_avg:60.12ms
step:552/2315 train_time:33190ms step_avg:60.13ms
step:553/2315 train_time:33251ms step_avg:60.13ms
step:554/2315 train_time:33311ms step_avg:60.13ms
step:555/2315 train_time:33371ms step_avg:60.13ms
step:556/2315 train_time:33432ms step_avg:60.13ms
step:557/2315 train_time:33492ms step_avg:60.13ms
step:558/2315 train_time:33552ms step_avg:60.13ms
step:559/2315 train_time:33611ms step_avg:60.13ms
step:560/2315 train_time:33672ms step_avg:60.13ms
step:561/2315 train_time:33730ms step_avg:60.13ms
step:562/2315 train_time:33790ms step_avg:60.12ms
step:563/2315 train_time:33849ms step_avg:60.12ms
step:564/2315 train_time:33910ms step_avg:60.12ms
step:565/2315 train_time:33969ms step_avg:60.12ms
step:566/2315 train_time:34029ms step_avg:60.12ms
step:567/2315 train_time:34089ms step_avg:60.12ms
step:568/2315 train_time:34150ms step_avg:60.12ms
step:569/2315 train_time:34211ms step_avg:60.12ms
step:570/2315 train_time:34271ms step_avg:60.13ms
step:571/2315 train_time:34331ms step_avg:60.12ms
step:572/2315 train_time:34392ms step_avg:60.13ms
step:573/2315 train_time:34452ms step_avg:60.13ms
step:574/2315 train_time:34512ms step_avg:60.12ms
step:575/2315 train_time:34571ms step_avg:60.12ms
step:576/2315 train_time:34631ms step_avg:60.12ms
step:577/2315 train_time:34690ms step_avg:60.12ms
step:578/2315 train_time:34751ms step_avg:60.12ms
step:579/2315 train_time:34810ms step_avg:60.12ms
step:580/2315 train_time:34869ms step_avg:60.12ms
step:581/2315 train_time:34928ms step_avg:60.12ms
step:582/2315 train_time:34989ms step_avg:60.12ms
step:583/2315 train_time:35049ms step_avg:60.12ms
step:584/2315 train_time:35110ms step_avg:60.12ms
step:585/2315 train_time:35170ms step_avg:60.12ms
step:586/2315 train_time:35232ms step_avg:60.12ms
step:587/2315 train_time:35291ms step_avg:60.12ms
step:588/2315 train_time:35352ms step_avg:60.12ms
step:589/2315 train_time:35412ms step_avg:60.12ms
step:590/2315 train_time:35473ms step_avg:60.12ms
step:591/2315 train_time:35532ms step_avg:60.12ms
step:592/2315 train_time:35592ms step_avg:60.12ms
step:593/2315 train_time:35652ms step_avg:60.12ms
step:594/2315 train_time:35711ms step_avg:60.12ms
step:595/2315 train_time:35771ms step_avg:60.12ms
step:596/2315 train_time:35831ms step_avg:60.12ms
step:597/2315 train_time:35891ms step_avg:60.12ms
step:598/2315 train_time:35951ms step_avg:60.12ms
step:599/2315 train_time:36010ms step_avg:60.12ms
step:600/2315 train_time:36071ms step_avg:60.12ms
step:601/2315 train_time:36130ms step_avg:60.12ms
step:602/2315 train_time:36191ms step_avg:60.12ms
step:603/2315 train_time:36251ms step_avg:60.12ms
step:604/2315 train_time:36312ms step_avg:60.12ms
step:605/2315 train_time:36372ms step_avg:60.12ms
step:606/2315 train_time:36432ms step_avg:60.12ms
step:607/2315 train_time:36492ms step_avg:60.12ms
step:608/2315 train_time:36552ms step_avg:60.12ms
step:609/2315 train_time:36612ms step_avg:60.12ms
step:610/2315 train_time:36672ms step_avg:60.12ms
step:611/2315 train_time:36731ms step_avg:60.12ms
step:612/2315 train_time:36791ms step_avg:60.12ms
step:613/2315 train_time:36850ms step_avg:60.11ms
step:614/2315 train_time:36910ms step_avg:60.11ms
step:615/2315 train_time:36970ms step_avg:60.11ms
step:616/2315 train_time:37030ms step_avg:60.11ms
step:617/2315 train_time:37089ms step_avg:60.11ms
step:618/2315 train_time:37150ms step_avg:60.11ms
step:619/2315 train_time:37211ms step_avg:60.11ms
step:620/2315 train_time:37270ms step_avg:60.11ms
step:621/2315 train_time:37330ms step_avg:60.11ms
step:622/2315 train_time:37391ms step_avg:60.11ms
step:623/2315 train_time:37451ms step_avg:60.11ms
step:624/2315 train_time:37511ms step_avg:60.11ms
step:625/2315 train_time:37571ms step_avg:60.11ms
step:626/2315 train_time:37632ms step_avg:60.11ms
step:627/2315 train_time:37691ms step_avg:60.11ms
step:628/2315 train_time:37751ms step_avg:60.11ms
step:629/2315 train_time:37810ms step_avg:60.11ms
step:630/2315 train_time:37870ms step_avg:60.11ms
step:631/2315 train_time:37930ms step_avg:60.11ms
step:632/2315 train_time:37990ms step_avg:60.11ms
step:633/2315 train_time:38049ms step_avg:60.11ms
step:634/2315 train_time:38110ms step_avg:60.11ms
step:635/2315 train_time:38170ms step_avg:60.11ms
step:636/2315 train_time:38230ms step_avg:60.11ms
step:637/2315 train_time:38290ms step_avg:60.11ms
step:638/2315 train_time:38351ms step_avg:60.11ms
step:639/2315 train_time:38411ms step_avg:60.11ms
step:640/2315 train_time:38471ms step_avg:60.11ms
step:641/2315 train_time:38531ms step_avg:60.11ms
step:642/2315 train_time:38591ms step_avg:60.11ms
step:643/2315 train_time:38651ms step_avg:60.11ms
step:644/2315 train_time:38711ms step_avg:60.11ms
step:645/2315 train_time:38770ms step_avg:60.11ms
step:646/2315 train_time:38831ms step_avg:60.11ms
step:647/2315 train_time:38890ms step_avg:60.11ms
step:648/2315 train_time:38951ms step_avg:60.11ms
step:649/2315 train_time:39010ms step_avg:60.11ms
step:650/2315 train_time:39071ms step_avg:60.11ms
step:651/2315 train_time:39130ms step_avg:60.11ms
step:652/2315 train_time:39190ms step_avg:60.11ms
step:653/2315 train_time:39250ms step_avg:60.11ms
step:654/2315 train_time:39311ms step_avg:60.11ms
step:655/2315 train_time:39371ms step_avg:60.11ms
step:656/2315 train_time:39431ms step_avg:60.11ms
step:657/2315 train_time:39491ms step_avg:60.11ms
step:658/2315 train_time:39551ms step_avg:60.11ms
step:659/2315 train_time:39611ms step_avg:60.11ms
step:660/2315 train_time:39671ms step_avg:60.11ms
step:661/2315 train_time:39731ms step_avg:60.11ms
step:662/2315 train_time:39791ms step_avg:60.11ms
step:663/2315 train_time:39851ms step_avg:60.11ms
step:664/2315 train_time:39911ms step_avg:60.11ms
step:665/2315 train_time:39970ms step_avg:60.11ms
step:666/2315 train_time:40030ms step_avg:60.11ms
step:667/2315 train_time:40090ms step_avg:60.11ms
step:668/2315 train_time:40151ms step_avg:60.11ms
step:669/2315 train_time:40211ms step_avg:60.11ms
step:670/2315 train_time:40272ms step_avg:60.11ms
step:671/2315 train_time:40332ms step_avg:60.11ms
step:672/2315 train_time:40393ms step_avg:60.11ms
step:673/2315 train_time:40452ms step_avg:60.11ms
step:674/2315 train_time:40512ms step_avg:60.11ms
step:675/2315 train_time:40573ms step_avg:60.11ms
step:676/2315 train_time:40634ms step_avg:60.11ms
step:677/2315 train_time:40693ms step_avg:60.11ms
step:678/2315 train_time:40753ms step_avg:60.11ms
step:679/2315 train_time:40812ms step_avg:60.11ms
step:680/2315 train_time:40872ms step_avg:60.11ms
step:681/2315 train_time:40932ms step_avg:60.11ms
step:682/2315 train_time:40991ms step_avg:60.10ms
step:683/2315 train_time:41051ms step_avg:60.10ms
step:684/2315 train_time:41112ms step_avg:60.11ms
step:685/2315 train_time:41171ms step_avg:60.10ms
step:686/2315 train_time:41232ms step_avg:60.10ms
step:687/2315 train_time:41292ms step_avg:60.10ms
step:688/2315 train_time:41352ms step_avg:60.11ms
step:689/2315 train_time:41412ms step_avg:60.11ms
step:690/2315 train_time:41473ms step_avg:60.11ms
step:691/2315 train_time:41532ms step_avg:60.10ms
step:692/2315 train_time:41592ms step_avg:60.10ms
step:693/2315 train_time:41652ms step_avg:60.10ms
step:694/2315 train_time:41712ms step_avg:60.10ms
step:695/2315 train_time:41772ms step_avg:60.10ms
step:696/2315 train_time:41832ms step_avg:60.10ms
step:697/2315 train_time:41891ms step_avg:60.10ms
step:698/2315 train_time:41951ms step_avg:60.10ms
step:699/2315 train_time:42010ms step_avg:60.10ms
step:700/2315 train_time:42071ms step_avg:60.10ms
step:701/2315 train_time:42131ms step_avg:60.10ms
step:702/2315 train_time:42191ms step_avg:60.10ms
step:703/2315 train_time:42251ms step_avg:60.10ms
step:704/2315 train_time:42311ms step_avg:60.10ms
step:705/2315 train_time:42371ms step_avg:60.10ms
step:706/2315 train_time:42432ms step_avg:60.10ms
step:707/2315 train_time:42491ms step_avg:60.10ms
step:708/2315 train_time:42552ms step_avg:60.10ms
step:709/2315 train_time:42611ms step_avg:60.10ms
step:710/2315 train_time:42671ms step_avg:60.10ms
step:711/2315 train_time:42731ms step_avg:60.10ms
step:712/2315 train_time:42792ms step_avg:60.10ms
step:713/2315 train_time:42851ms step_avg:60.10ms
step:714/2315 train_time:42911ms step_avg:60.10ms
step:715/2315 train_time:42971ms step_avg:60.10ms
step:716/2315 train_time:43031ms step_avg:60.10ms
step:717/2315 train_time:43090ms step_avg:60.10ms
step:718/2315 train_time:43151ms step_avg:60.10ms
step:719/2315 train_time:43210ms step_avg:60.10ms
step:720/2315 train_time:43271ms step_avg:60.10ms
step:721/2315 train_time:43330ms step_avg:60.10ms
step:722/2315 train_time:43391ms step_avg:60.10ms
step:723/2315 train_time:43451ms step_avg:60.10ms
step:724/2315 train_time:43511ms step_avg:60.10ms
step:725/2315 train_time:43571ms step_avg:60.10ms
step:726/2315 train_time:43631ms step_avg:60.10ms
step:727/2315 train_time:43691ms step_avg:60.10ms
step:728/2315 train_time:43751ms step_avg:60.10ms
step:729/2315 train_time:43811ms step_avg:60.10ms
step:730/2315 train_time:43871ms step_avg:60.10ms
step:731/2315 train_time:43930ms step_avg:60.10ms
step:732/2315 train_time:43990ms step_avg:60.10ms
step:733/2315 train_time:44051ms step_avg:60.10ms
step:734/2315 train_time:44111ms step_avg:60.10ms
step:735/2315 train_time:44170ms step_avg:60.10ms
step:736/2315 train_time:44231ms step_avg:60.10ms
step:737/2315 train_time:44290ms step_avg:60.10ms
step:738/2315 train_time:44351ms step_avg:60.10ms
step:739/2315 train_time:44410ms step_avg:60.10ms
step:740/2315 train_time:44471ms step_avg:60.10ms
step:741/2315 train_time:44530ms step_avg:60.09ms
step:742/2315 train_time:44590ms step_avg:60.09ms
step:743/2315 train_time:44650ms step_avg:60.09ms
step:744/2315 train_time:44711ms step_avg:60.10ms
step:745/2315 train_time:44771ms step_avg:60.09ms
step:746/2315 train_time:44831ms step_avg:60.09ms
step:747/2315 train_time:44891ms step_avg:60.09ms
step:748/2315 train_time:44951ms step_avg:60.09ms
step:749/2315 train_time:45011ms step_avg:60.09ms
step:750/2315 train_time:45071ms step_avg:60.10ms
step:750/2315 val_loss:3.6778 train_time:45133ms step_avg:60.18ms
step:751/2315 train_time:45152ms step_avg:60.12ms
step:752/2315 train_time:45194ms step_avg:60.10ms
step:753/2315 train_time:45258ms step_avg:60.10ms
step:754/2315 train_time:45321ms step_avg:60.11ms
step:755/2315 train_time:45380ms step_avg:60.11ms
step:756/2315 train_time:45441ms step_avg:60.11ms
step:757/2315 train_time:45500ms step_avg:60.11ms
step:758/2315 train_time:45559ms step_avg:60.10ms
step:759/2315 train_time:45618ms step_avg:60.10ms
step:760/2315 train_time:45679ms step_avg:60.10ms
step:761/2315 train_time:45738ms step_avg:60.10ms
step:762/2315 train_time:45798ms step_avg:60.10ms
step:763/2315 train_time:45858ms step_avg:60.10ms
step:764/2315 train_time:45919ms step_avg:60.10ms
step:765/2315 train_time:45979ms step_avg:60.10ms
step:766/2315 train_time:46040ms step_avg:60.10ms
step:767/2315 train_time:46101ms step_avg:60.11ms
step:768/2315 train_time:46163ms step_avg:60.11ms
step:769/2315 train_time:46225ms step_avg:60.11ms
step:770/2315 train_time:46286ms step_avg:60.11ms
step:771/2315 train_time:46347ms step_avg:60.11ms
step:772/2315 train_time:46408ms step_avg:60.11ms
step:773/2315 train_time:46469ms step_avg:60.11ms
step:774/2315 train_time:46530ms step_avg:60.12ms
step:775/2315 train_time:46591ms step_avg:60.12ms
step:776/2315 train_time:46651ms step_avg:60.12ms
step:777/2315 train_time:46711ms step_avg:60.12ms
step:778/2315 train_time:46772ms step_avg:60.12ms
step:779/2315 train_time:46832ms step_avg:60.12ms
step:780/2315 train_time:46893ms step_avg:60.12ms
step:781/2315 train_time:46953ms step_avg:60.12ms
step:782/2315 train_time:47014ms step_avg:60.12ms
step:783/2315 train_time:47076ms step_avg:60.12ms
step:784/2315 train_time:47138ms step_avg:60.12ms
step:785/2315 train_time:47199ms step_avg:60.13ms
step:786/2315 train_time:47261ms step_avg:60.13ms
step:787/2315 train_time:47323ms step_avg:60.13ms
step:788/2315 train_time:47384ms step_avg:60.13ms
step:789/2315 train_time:47444ms step_avg:60.13ms
step:790/2315 train_time:47505ms step_avg:60.13ms
step:791/2315 train_time:47565ms step_avg:60.13ms
step:792/2315 train_time:47626ms step_avg:60.13ms
step:793/2315 train_time:47687ms step_avg:60.13ms
step:794/2315 train_time:47749ms step_avg:60.14ms
step:795/2315 train_time:47809ms step_avg:60.14ms
step:796/2315 train_time:47871ms step_avg:60.14ms
step:797/2315 train_time:47931ms step_avg:60.14ms
step:798/2315 train_time:47992ms step_avg:60.14ms
step:799/2315 train_time:48053ms step_avg:60.14ms
step:800/2315 train_time:48116ms step_avg:60.14ms
step:801/2315 train_time:48177ms step_avg:60.15ms
step:802/2315 train_time:48239ms step_avg:60.15ms
step:803/2315 train_time:48299ms step_avg:60.15ms
step:804/2315 train_time:48361ms step_avg:60.15ms
step:805/2315 train_time:48422ms step_avg:60.15ms
step:806/2315 train_time:48482ms step_avg:60.15ms
step:807/2315 train_time:48543ms step_avg:60.15ms
step:808/2315 train_time:48603ms step_avg:60.15ms
step:809/2315 train_time:48663ms step_avg:60.15ms
step:810/2315 train_time:48724ms step_avg:60.15ms
step:811/2315 train_time:48785ms step_avg:60.15ms
step:812/2315 train_time:48846ms step_avg:60.16ms
step:813/2315 train_time:48906ms step_avg:60.16ms
step:814/2315 train_time:48968ms step_avg:60.16ms
step:815/2315 train_time:49029ms step_avg:60.16ms
step:816/2315 train_time:49090ms step_avg:60.16ms
step:817/2315 train_time:49151ms step_avg:60.16ms
step:818/2315 train_time:49213ms step_avg:60.16ms
step:819/2315 train_time:49274ms step_avg:60.16ms
step:820/2315 train_time:49336ms step_avg:60.17ms
step:821/2315 train_time:49397ms step_avg:60.17ms
step:822/2315 train_time:49458ms step_avg:60.17ms
step:823/2315 train_time:49519ms step_avg:60.17ms
step:824/2315 train_time:49580ms step_avg:60.17ms
step:825/2315 train_time:49641ms step_avg:60.17ms
step:826/2315 train_time:49702ms step_avg:60.17ms
step:827/2315 train_time:49762ms step_avg:60.17ms
step:828/2315 train_time:49823ms step_avg:60.17ms
step:829/2315 train_time:49884ms step_avg:60.17ms
step:830/2315 train_time:49945ms step_avg:60.17ms
step:831/2315 train_time:50005ms step_avg:60.18ms
step:832/2315 train_time:50067ms step_avg:60.18ms
step:833/2315 train_time:50129ms step_avg:60.18ms
step:834/2315 train_time:50190ms step_avg:60.18ms
step:835/2315 train_time:50251ms step_avg:60.18ms
step:836/2315 train_time:50312ms step_avg:60.18ms
step:837/2315 train_time:50374ms step_avg:60.18ms
step:838/2315 train_time:50436ms step_avg:60.19ms
step:839/2315 train_time:50496ms step_avg:60.19ms
step:840/2315 train_time:50558ms step_avg:60.19ms
step:841/2315 train_time:50619ms step_avg:60.19ms
step:842/2315 train_time:50680ms step_avg:60.19ms
step:843/2315 train_time:50740ms step_avg:60.19ms
step:844/2315 train_time:50802ms step_avg:60.19ms
step:845/2315 train_time:50862ms step_avg:60.19ms
step:846/2315 train_time:50923ms step_avg:60.19ms
step:847/2315 train_time:50984ms step_avg:60.19ms
step:848/2315 train_time:51045ms step_avg:60.19ms
step:849/2315 train_time:51106ms step_avg:60.20ms
step:850/2315 train_time:51167ms step_avg:60.20ms
step:851/2315 train_time:51228ms step_avg:60.20ms
step:852/2315 train_time:51290ms step_avg:60.20ms
step:853/2315 train_time:51351ms step_avg:60.20ms
step:854/2315 train_time:51412ms step_avg:60.20ms
step:855/2315 train_time:51474ms step_avg:60.20ms
step:856/2315 train_time:51536ms step_avg:60.21ms
step:857/2315 train_time:51597ms step_avg:60.21ms
step:858/2315 train_time:51659ms step_avg:60.21ms
step:859/2315 train_time:51719ms step_avg:60.21ms
step:860/2315 train_time:51780ms step_avg:60.21ms
step:861/2315 train_time:51840ms step_avg:60.21ms
step:862/2315 train_time:51901ms step_avg:60.21ms
step:863/2315 train_time:51962ms step_avg:60.21ms
step:864/2315 train_time:52023ms step_avg:60.21ms
step:865/2315 train_time:52084ms step_avg:60.21ms
step:866/2315 train_time:52144ms step_avg:60.21ms
step:867/2315 train_time:52205ms step_avg:60.21ms
step:868/2315 train_time:52266ms step_avg:60.21ms
step:869/2315 train_time:52327ms step_avg:60.22ms
step:870/2315 train_time:52389ms step_avg:60.22ms
step:871/2315 train_time:52450ms step_avg:60.22ms
step:872/2315 train_time:52511ms step_avg:60.22ms
step:873/2315 train_time:52573ms step_avg:60.22ms
step:874/2315 train_time:52634ms step_avg:60.22ms
step:875/2315 train_time:52696ms step_avg:60.22ms
step:876/2315 train_time:52757ms step_avg:60.23ms
step:877/2315 train_time:52819ms step_avg:60.23ms
step:878/2315 train_time:52880ms step_avg:60.23ms
step:879/2315 train_time:52940ms step_avg:60.23ms
step:880/2315 train_time:53001ms step_avg:60.23ms
step:881/2315 train_time:53061ms step_avg:60.23ms
step:882/2315 train_time:53124ms step_avg:60.23ms
step:883/2315 train_time:53184ms step_avg:60.23ms
step:884/2315 train_time:53245ms step_avg:60.23ms
step:885/2315 train_time:53305ms step_avg:60.23ms
step:886/2315 train_time:53367ms step_avg:60.23ms
step:887/2315 train_time:53427ms step_avg:60.23ms
step:888/2315 train_time:53489ms step_avg:60.24ms
step:889/2315 train_time:53551ms step_avg:60.24ms
step:890/2315 train_time:53612ms step_avg:60.24ms
step:891/2315 train_time:53673ms step_avg:60.24ms
step:892/2315 train_time:53734ms step_avg:60.24ms
step:893/2315 train_time:53795ms step_avg:60.24ms
step:894/2315 train_time:53857ms step_avg:60.24ms
step:895/2315 train_time:53918ms step_avg:60.24ms
step:896/2315 train_time:53979ms step_avg:60.24ms
step:897/2315 train_time:54039ms step_avg:60.24ms
step:898/2315 train_time:54101ms step_avg:60.25ms
step:899/2315 train_time:54161ms step_avg:60.25ms
step:900/2315 train_time:54223ms step_avg:60.25ms
step:901/2315 train_time:54284ms step_avg:60.25ms
step:902/2315 train_time:54345ms step_avg:60.25ms
step:903/2315 train_time:54405ms step_avg:60.25ms
step:904/2315 train_time:54467ms step_avg:60.25ms
step:905/2315 train_time:54528ms step_avg:60.25ms
step:906/2315 train_time:54590ms step_avg:60.25ms
step:907/2315 train_time:54651ms step_avg:60.26ms
step:908/2315 train_time:54712ms step_avg:60.26ms
step:909/2315 train_time:54774ms step_avg:60.26ms
step:910/2315 train_time:54834ms step_avg:60.26ms
step:911/2315 train_time:54895ms step_avg:60.26ms
step:912/2315 train_time:54957ms step_avg:60.26ms
step:913/2315 train_time:55017ms step_avg:60.26ms
step:914/2315 train_time:55079ms step_avg:60.26ms
step:915/2315 train_time:55139ms step_avg:60.26ms
step:916/2315 train_time:55201ms step_avg:60.26ms
step:917/2315 train_time:55262ms step_avg:60.26ms
step:918/2315 train_time:55323ms step_avg:60.26ms
step:919/2315 train_time:55383ms step_avg:60.26ms
step:920/2315 train_time:55445ms step_avg:60.27ms
step:921/2315 train_time:55505ms step_avg:60.27ms
step:922/2315 train_time:55566ms step_avg:60.27ms
step:923/2315 train_time:55627ms step_avg:60.27ms
step:924/2315 train_time:55689ms step_avg:60.27ms
step:925/2315 train_time:55750ms step_avg:60.27ms
step:926/2315 train_time:55812ms step_avg:60.27ms
step:927/2315 train_time:55872ms step_avg:60.27ms
step:928/2315 train_time:55934ms step_avg:60.27ms
step:929/2315 train_time:55995ms step_avg:60.27ms
step:930/2315 train_time:56056ms step_avg:60.27ms
step:931/2315 train_time:56117ms step_avg:60.28ms
step:932/2315 train_time:56178ms step_avg:60.28ms
step:933/2315 train_time:56239ms step_avg:60.28ms
step:934/2315 train_time:56300ms step_avg:60.28ms
step:935/2315 train_time:56361ms step_avg:60.28ms
step:936/2315 train_time:56422ms step_avg:60.28ms
step:937/2315 train_time:56482ms step_avg:60.28ms
step:938/2315 train_time:56543ms step_avg:60.28ms
step:939/2315 train_time:56604ms step_avg:60.28ms
step:940/2315 train_time:56666ms step_avg:60.28ms
step:941/2315 train_time:56726ms step_avg:60.28ms
step:942/2315 train_time:56788ms step_avg:60.28ms
step:943/2315 train_time:56849ms step_avg:60.29ms
step:944/2315 train_time:56911ms step_avg:60.29ms
step:945/2315 train_time:56972ms step_avg:60.29ms
step:946/2315 train_time:57033ms step_avg:60.29ms
step:947/2315 train_time:57095ms step_avg:60.29ms
step:948/2315 train_time:57156ms step_avg:60.29ms
step:949/2315 train_time:57218ms step_avg:60.29ms
step:950/2315 train_time:57279ms step_avg:60.29ms
step:951/2315 train_time:57339ms step_avg:60.29ms
step:952/2315 train_time:57401ms step_avg:60.29ms
step:953/2315 train_time:57461ms step_avg:60.29ms
step:954/2315 train_time:57523ms step_avg:60.30ms
step:955/2315 train_time:57582ms step_avg:60.30ms
step:956/2315 train_time:57644ms step_avg:60.30ms
step:957/2315 train_time:57704ms step_avg:60.30ms
step:958/2315 train_time:57765ms step_avg:60.30ms
step:959/2315 train_time:57826ms step_avg:60.30ms
step:960/2315 train_time:57888ms step_avg:60.30ms
step:961/2315 train_time:57950ms step_avg:60.30ms
step:962/2315 train_time:58011ms step_avg:60.30ms
step:963/2315 train_time:58072ms step_avg:60.30ms
step:964/2315 train_time:58134ms step_avg:60.30ms
step:965/2315 train_time:58195ms step_avg:60.31ms
step:966/2315 train_time:58256ms step_avg:60.31ms
step:967/2315 train_time:58317ms step_avg:60.31ms
step:968/2315 train_time:58379ms step_avg:60.31ms
step:969/2315 train_time:58439ms step_avg:60.31ms
step:970/2315 train_time:58501ms step_avg:60.31ms
step:971/2315 train_time:58561ms step_avg:60.31ms
step:972/2315 train_time:58622ms step_avg:60.31ms
step:973/2315 train_time:58684ms step_avg:60.31ms
step:974/2315 train_time:58744ms step_avg:60.31ms
step:975/2315 train_time:58805ms step_avg:60.31ms
step:976/2315 train_time:58866ms step_avg:60.31ms
step:977/2315 train_time:58927ms step_avg:60.31ms
step:978/2315 train_time:58988ms step_avg:60.32ms
step:979/2315 train_time:59049ms step_avg:60.32ms
step:980/2315 train_time:59111ms step_avg:60.32ms
step:981/2315 train_time:59171ms step_avg:60.32ms
step:982/2315 train_time:59234ms step_avg:60.32ms
step:983/2315 train_time:59296ms step_avg:60.32ms
step:984/2315 train_time:59357ms step_avg:60.32ms
step:985/2315 train_time:59418ms step_avg:60.32ms
step:986/2315 train_time:59479ms step_avg:60.32ms
step:987/2315 train_time:59540ms step_avg:60.32ms
step:988/2315 train_time:59601ms step_avg:60.33ms
step:989/2315 train_time:59661ms step_avg:60.32ms
step:990/2315 train_time:59722ms step_avg:60.33ms
step:991/2315 train_time:59783ms step_avg:60.33ms
step:992/2315 train_time:59844ms step_avg:60.33ms
step:993/2315 train_time:59904ms step_avg:60.33ms
step:994/2315 train_time:59966ms step_avg:60.33ms
step:995/2315 train_time:60026ms step_avg:60.33ms
step:996/2315 train_time:60088ms step_avg:60.33ms
step:997/2315 train_time:60150ms step_avg:60.33ms
step:998/2315 train_time:60211ms step_avg:60.33ms
step:999/2315 train_time:60272ms step_avg:60.33ms
step:1000/2315 train_time:60333ms step_avg:60.33ms
step:1000/2315 val_loss:3.5670 train_time:60396ms step_avg:60.40ms
step:1001/2315 train_time:60415ms step_avg:60.35ms
step:1002/2315 train_time:60458ms step_avg:60.34ms
step:1003/2315 train_time:60526ms step_avg:60.35ms
step:1004/2315 train_time:60591ms step_avg:60.35ms
step:1005/2315 train_time:60652ms step_avg:60.35ms
step:1006/2315 train_time:60713ms step_avg:60.35ms
step:1007/2315 train_time:60773ms step_avg:60.35ms
step:1008/2315 train_time:60833ms step_avg:60.35ms
step:1009/2315 train_time:60893ms step_avg:60.35ms
step:1010/2315 train_time:60954ms step_avg:60.35ms
step:1011/2315 train_time:61013ms step_avg:60.35ms
step:1012/2315 train_time:61074ms step_avg:60.35ms
step:1013/2315 train_time:61133ms step_avg:60.35ms
step:1014/2315 train_time:61193ms step_avg:60.35ms
step:1015/2315 train_time:61253ms step_avg:60.35ms
step:1016/2315 train_time:61313ms step_avg:60.35ms
step:1017/2315 train_time:61374ms step_avg:60.35ms
step:1018/2315 train_time:61437ms step_avg:60.35ms
step:1019/2315 train_time:61500ms step_avg:60.35ms
step:1020/2315 train_time:61562ms step_avg:60.36ms
step:1021/2315 train_time:61623ms step_avg:60.36ms
step:1022/2315 train_time:61686ms step_avg:60.36ms
step:1023/2315 train_time:61746ms step_avg:60.36ms
step:1024/2315 train_time:61807ms step_avg:60.36ms
step:1025/2315 train_time:61867ms step_avg:60.36ms
step:1026/2315 train_time:61928ms step_avg:60.36ms
step:1027/2315 train_time:61988ms step_avg:60.36ms
step:1028/2315 train_time:62049ms step_avg:60.36ms
step:1029/2315 train_time:62109ms step_avg:60.36ms
step:1030/2315 train_time:62170ms step_avg:60.36ms
step:1031/2315 train_time:62230ms step_avg:60.36ms
step:1032/2315 train_time:62291ms step_avg:60.36ms
step:1033/2315 train_time:62351ms step_avg:60.36ms
step:1034/2315 train_time:62414ms step_avg:60.36ms
step:1035/2315 train_time:62475ms step_avg:60.36ms
step:1036/2315 train_time:62537ms step_avg:60.36ms
step:1037/2315 train_time:62598ms step_avg:60.36ms
step:1038/2315 train_time:62659ms step_avg:60.37ms
step:1039/2315 train_time:62720ms step_avg:60.37ms
step:1040/2315 train_time:62782ms step_avg:60.37ms
step:1041/2315 train_time:62842ms step_avg:60.37ms
step:1042/2315 train_time:62904ms step_avg:60.37ms
step:1043/2315 train_time:62964ms step_avg:60.37ms
step:1044/2315 train_time:63026ms step_avg:60.37ms
step:1045/2315 train_time:63086ms step_avg:60.37ms
step:1046/2315 train_time:63148ms step_avg:60.37ms
step:1047/2315 train_time:63207ms step_avg:60.37ms
step:1048/2315 train_time:63269ms step_avg:60.37ms
step:1049/2315 train_time:63330ms step_avg:60.37ms
step:1050/2315 train_time:63391ms step_avg:60.37ms
step:1051/2315 train_time:63452ms step_avg:60.37ms
step:1052/2315 train_time:63514ms step_avg:60.37ms
step:1053/2315 train_time:63575ms step_avg:60.37ms
step:1054/2315 train_time:63636ms step_avg:60.38ms
step:1055/2315 train_time:63696ms step_avg:60.38ms
step:1056/2315 train_time:63758ms step_avg:60.38ms
step:1057/2315 train_time:63818ms step_avg:60.38ms
step:1058/2315 train_time:63879ms step_avg:60.38ms
step:1059/2315 train_time:63940ms step_avg:60.38ms
step:1060/2315 train_time:64001ms step_avg:60.38ms
step:1061/2315 train_time:64061ms step_avg:60.38ms
step:1062/2315 train_time:64122ms step_avg:60.38ms
step:1063/2315 train_time:64182ms step_avg:60.38ms
step:1064/2315 train_time:64244ms step_avg:60.38ms
step:1065/2315 train_time:64305ms step_avg:60.38ms
step:1066/2315 train_time:64367ms step_avg:60.38ms
step:1067/2315 train_time:64429ms step_avg:60.38ms
step:1068/2315 train_time:64490ms step_avg:60.38ms
step:1069/2315 train_time:64551ms step_avg:60.38ms
step:1070/2315 train_time:64612ms step_avg:60.39ms
step:1071/2315 train_time:64672ms step_avg:60.38ms
step:1072/2315 train_time:64734ms step_avg:60.39ms
step:1073/2315 train_time:64794ms step_avg:60.39ms
step:1074/2315 train_time:64855ms step_avg:60.39ms
step:1075/2315 train_time:64916ms step_avg:60.39ms
step:1076/2315 train_time:64977ms step_avg:60.39ms
step:1077/2315 train_time:65037ms step_avg:60.39ms
step:1078/2315 train_time:65098ms step_avg:60.39ms
step:1079/2315 train_time:65158ms step_avg:60.39ms
step:1080/2315 train_time:65220ms step_avg:60.39ms
step:1081/2315 train_time:65280ms step_avg:60.39ms
step:1082/2315 train_time:65342ms step_avg:60.39ms
step:1083/2315 train_time:65403ms step_avg:60.39ms
step:1084/2315 train_time:65465ms step_avg:60.39ms
step:1085/2315 train_time:65525ms step_avg:60.39ms
step:1086/2315 train_time:65587ms step_avg:60.39ms
step:1087/2315 train_time:65648ms step_avg:60.39ms
step:1088/2315 train_time:65710ms step_avg:60.40ms
step:1089/2315 train_time:65770ms step_avg:60.40ms
step:1090/2315 train_time:65831ms step_avg:60.40ms
step:1091/2315 train_time:65892ms step_avg:60.40ms
step:1092/2315 train_time:65953ms step_avg:60.40ms
step:1093/2315 train_time:66014ms step_avg:60.40ms
step:1094/2315 train_time:66075ms step_avg:60.40ms
step:1095/2315 train_time:66136ms step_avg:60.40ms
step:1096/2315 train_time:66199ms step_avg:60.40ms
step:1097/2315 train_time:66257ms step_avg:60.40ms
step:1098/2315 train_time:66318ms step_avg:60.40ms
step:1099/2315 train_time:66379ms step_avg:60.40ms
step:1100/2315 train_time:66442ms step_avg:60.40ms
step:1101/2315 train_time:66503ms step_avg:60.40ms
step:1102/2315 train_time:66564ms step_avg:60.40ms
step:1103/2315 train_time:66626ms step_avg:60.40ms
step:1104/2315 train_time:66687ms step_avg:60.40ms
step:1105/2315 train_time:66748ms step_avg:60.41ms
step:1106/2315 train_time:66809ms step_avg:60.41ms
step:1107/2315 train_time:66870ms step_avg:60.41ms
step:1108/2315 train_time:66932ms step_avg:60.41ms
step:1109/2315 train_time:66992ms step_avg:60.41ms
step:1110/2315 train_time:67055ms step_avg:60.41ms
step:1111/2315 train_time:67114ms step_avg:60.41ms
step:1112/2315 train_time:67175ms step_avg:60.41ms
step:1113/2315 train_time:67235ms step_avg:60.41ms
step:1114/2315 train_time:67296ms step_avg:60.41ms
step:1115/2315 train_time:67358ms step_avg:60.41ms
step:1116/2315 train_time:67419ms step_avg:60.41ms
step:1117/2315 train_time:67480ms step_avg:60.41ms
step:1118/2315 train_time:67540ms step_avg:60.41ms
step:1119/2315 train_time:67601ms step_avg:60.41ms
step:1120/2315 train_time:67662ms step_avg:60.41ms
step:1121/2315 train_time:67723ms step_avg:60.41ms
step:1122/2315 train_time:67785ms step_avg:60.41ms
step:1123/2315 train_time:67846ms step_avg:60.41ms
step:1124/2315 train_time:67909ms step_avg:60.42ms
step:1125/2315 train_time:67968ms step_avg:60.42ms
step:1126/2315 train_time:68030ms step_avg:60.42ms
step:1127/2315 train_time:68091ms step_avg:60.42ms
step:1128/2315 train_time:68152ms step_avg:60.42ms
step:1129/2315 train_time:68213ms step_avg:60.42ms
step:1130/2315 train_time:68275ms step_avg:60.42ms
step:1131/2315 train_time:68335ms step_avg:60.42ms
step:1132/2315 train_time:68396ms step_avg:60.42ms
step:1133/2315 train_time:68457ms step_avg:60.42ms
step:1134/2315 train_time:68518ms step_avg:60.42ms
step:1135/2315 train_time:68578ms step_avg:60.42ms
step:1136/2315 train_time:68639ms step_avg:60.42ms
step:1137/2315 train_time:68699ms step_avg:60.42ms
step:1138/2315 train_time:68761ms step_avg:60.42ms
step:1139/2315 train_time:68822ms step_avg:60.42ms
step:1140/2315 train_time:68884ms step_avg:60.42ms
step:1141/2315 train_time:68945ms step_avg:60.42ms
step:1142/2315 train_time:69006ms step_avg:60.43ms
step:1143/2315 train_time:69067ms step_avg:60.43ms
step:1144/2315 train_time:69129ms step_avg:60.43ms
step:1145/2315 train_time:69190ms step_avg:60.43ms
step:1146/2315 train_time:69251ms step_avg:60.43ms
step:1147/2315 train_time:69312ms step_avg:60.43ms
step:1148/2315 train_time:69373ms step_avg:60.43ms
step:1149/2315 train_time:69434ms step_avg:60.43ms
step:1150/2315 train_time:69495ms step_avg:60.43ms
step:1151/2315 train_time:69556ms step_avg:60.43ms
step:1152/2315 train_time:69618ms step_avg:60.43ms
step:1153/2315 train_time:69678ms step_avg:60.43ms
step:1154/2315 train_time:69739ms step_avg:60.43ms
step:1155/2315 train_time:69799ms step_avg:60.43ms
step:1156/2315 train_time:69861ms step_avg:60.43ms
step:1157/2315 train_time:69922ms step_avg:60.43ms
step:1158/2315 train_time:69984ms step_avg:60.44ms
step:1159/2315 train_time:70045ms step_avg:60.44ms
step:1160/2315 train_time:70106ms step_avg:60.44ms
step:1161/2315 train_time:70168ms step_avg:60.44ms
step:1162/2315 train_time:70230ms step_avg:60.44ms
step:1163/2315 train_time:70291ms step_avg:60.44ms
step:1164/2315 train_time:70352ms step_avg:60.44ms
step:1165/2315 train_time:70413ms step_avg:60.44ms
step:1166/2315 train_time:70473ms step_avg:60.44ms
step:1167/2315 train_time:70534ms step_avg:60.44ms
step:1168/2315 train_time:70596ms step_avg:60.44ms
step:1169/2315 train_time:70656ms step_avg:60.44ms
step:1170/2315 train_time:70717ms step_avg:60.44ms
step:1171/2315 train_time:70777ms step_avg:60.44ms
step:1172/2315 train_time:70839ms step_avg:60.44ms
step:1173/2315 train_time:70899ms step_avg:60.44ms
step:1174/2315 train_time:70961ms step_avg:60.44ms
step:1175/2315 train_time:71022ms step_avg:60.44ms
step:1176/2315 train_time:71084ms step_avg:60.45ms
step:1177/2315 train_time:71145ms step_avg:60.45ms
step:1178/2315 train_time:71208ms step_avg:60.45ms
step:1179/2315 train_time:71268ms step_avg:60.45ms
step:1180/2315 train_time:71330ms step_avg:60.45ms
step:1181/2315 train_time:71391ms step_avg:60.45ms
step:1182/2315 train_time:71451ms step_avg:60.45ms
step:1183/2315 train_time:71512ms step_avg:60.45ms
step:1184/2315 train_time:71573ms step_avg:60.45ms
step:1185/2315 train_time:71633ms step_avg:60.45ms
step:1186/2315 train_time:71695ms step_avg:60.45ms
step:1187/2315 train_time:71756ms step_avg:60.45ms
step:1188/2315 train_time:71817ms step_avg:60.45ms
step:1189/2315 train_time:71876ms step_avg:60.45ms
step:1190/2315 train_time:71938ms step_avg:60.45ms
step:1191/2315 train_time:71999ms step_avg:60.45ms
step:1192/2315 train_time:72060ms step_avg:60.45ms
step:1193/2315 train_time:72121ms step_avg:60.45ms
step:1194/2315 train_time:72183ms step_avg:60.46ms
step:1195/2315 train_time:72244ms step_avg:60.46ms
step:1196/2315 train_time:72306ms step_avg:60.46ms
step:1197/2315 train_time:72367ms step_avg:60.46ms
step:1198/2315 train_time:72429ms step_avg:60.46ms
step:1199/2315 train_time:72490ms step_avg:60.46ms
step:1200/2315 train_time:72551ms step_avg:60.46ms
step:1201/2315 train_time:72611ms step_avg:60.46ms
step:1202/2315 train_time:72673ms step_avg:60.46ms
step:1203/2315 train_time:72733ms step_avg:60.46ms
step:1204/2315 train_time:72795ms step_avg:60.46ms
step:1205/2315 train_time:72855ms step_avg:60.46ms
step:1206/2315 train_time:72916ms step_avg:60.46ms
step:1207/2315 train_time:72977ms step_avg:60.46ms
step:1208/2315 train_time:73038ms step_avg:60.46ms
step:1209/2315 train_time:73099ms step_avg:60.46ms
step:1210/2315 train_time:73161ms step_avg:60.46ms
step:1211/2315 train_time:73222ms step_avg:60.46ms
step:1212/2315 train_time:73284ms step_avg:60.47ms
step:1213/2315 train_time:73345ms step_avg:60.47ms
step:1214/2315 train_time:73406ms step_avg:60.47ms
step:1215/2315 train_time:73467ms step_avg:60.47ms
step:1216/2315 train_time:73529ms step_avg:60.47ms
step:1217/2315 train_time:73589ms step_avg:60.47ms
step:1218/2315 train_time:73651ms step_avg:60.47ms
step:1219/2315 train_time:73711ms step_avg:60.47ms
step:1220/2315 train_time:73772ms step_avg:60.47ms
step:1221/2315 train_time:73832ms step_avg:60.47ms
step:1222/2315 train_time:73894ms step_avg:60.47ms
step:1223/2315 train_time:73954ms step_avg:60.47ms
step:1224/2315 train_time:74016ms step_avg:60.47ms
step:1225/2315 train_time:74077ms step_avg:60.47ms
step:1226/2315 train_time:74138ms step_avg:60.47ms
step:1227/2315 train_time:74201ms step_avg:60.47ms
step:1228/2315 train_time:74259ms step_avg:60.47ms
step:1229/2315 train_time:74320ms step_avg:60.47ms
step:1230/2315 train_time:74382ms step_avg:60.47ms
step:1231/2315 train_time:74443ms step_avg:60.47ms
step:1232/2315 train_time:74505ms step_avg:60.47ms
step:1233/2315 train_time:74565ms step_avg:60.47ms
step:1234/2315 train_time:74627ms step_avg:60.48ms
step:1235/2315 train_time:74687ms step_avg:60.48ms
step:1236/2315 train_time:74749ms step_avg:60.48ms
step:1237/2315 train_time:74810ms step_avg:60.48ms
step:1238/2315 train_time:74871ms step_avg:60.48ms
step:1239/2315 train_time:74932ms step_avg:60.48ms
step:1240/2315 train_time:74994ms step_avg:60.48ms
step:1241/2315 train_time:75055ms step_avg:60.48ms
step:1242/2315 train_time:75115ms step_avg:60.48ms
step:1243/2315 train_time:75176ms step_avg:60.48ms
step:1244/2315 train_time:75237ms step_avg:60.48ms
step:1245/2315 train_time:75297ms step_avg:60.48ms
step:1246/2315 train_time:75358ms step_avg:60.48ms
step:1247/2315 train_time:75420ms step_avg:60.48ms
step:1248/2315 train_time:75481ms step_avg:60.48ms
step:1249/2315 train_time:75542ms step_avg:60.48ms
step:1250/2315 train_time:75604ms step_avg:60.48ms
step:1250/2315 val_loss:3.5096 train_time:75667ms step_avg:60.53ms
step:1251/2315 train_time:75685ms step_avg:60.50ms
step:1252/2315 train_time:75728ms step_avg:60.49ms
step:1253/2315 train_time:75792ms step_avg:60.49ms
step:1254/2315 train_time:75857ms step_avg:60.49ms
step:1255/2315 train_time:75916ms step_avg:60.49ms
step:1256/2315 train_time:75978ms step_avg:60.49ms
step:1257/2315 train_time:76038ms step_avg:60.49ms
step:1258/2315 train_time:76098ms step_avg:60.49ms
step:1259/2315 train_time:76158ms step_avg:60.49ms
step:1260/2315 train_time:76218ms step_avg:60.49ms
step:1261/2315 train_time:76278ms step_avg:60.49ms
step:1262/2315 train_time:76338ms step_avg:60.49ms
step:1263/2315 train_time:76397ms step_avg:60.49ms
step:1264/2315 train_time:76459ms step_avg:60.49ms
step:1265/2315 train_time:76518ms step_avg:60.49ms
step:1266/2315 train_time:76580ms step_avg:60.49ms
step:1267/2315 train_time:76642ms step_avg:60.49ms
step:1268/2315 train_time:76705ms step_avg:60.49ms
step:1269/2315 train_time:76766ms step_avg:60.49ms
step:1270/2315 train_time:76831ms step_avg:60.50ms
step:1271/2315 train_time:76889ms step_avg:60.49ms
step:1272/2315 train_time:76951ms step_avg:60.50ms
step:1273/2315 train_time:77011ms step_avg:60.50ms
step:1274/2315 train_time:77072ms step_avg:60.50ms
step:1275/2315 train_time:77132ms step_avg:60.50ms
step:1276/2315 train_time:77193ms step_avg:60.50ms
step:1277/2315 train_time:77253ms step_avg:60.50ms
step:1278/2315 train_time:77313ms step_avg:60.50ms
step:1279/2315 train_time:77374ms step_avg:60.50ms
step:1280/2315 train_time:77434ms step_avg:60.50ms
step:1281/2315 train_time:77495ms step_avg:60.50ms
step:1282/2315 train_time:77558ms step_avg:60.50ms
step:1283/2315 train_time:77619ms step_avg:60.50ms
step:1284/2315 train_time:77681ms step_avg:60.50ms
step:1285/2315 train_time:77742ms step_avg:60.50ms
step:1286/2315 train_time:77803ms step_avg:60.50ms
step:1287/2315 train_time:77864ms step_avg:60.50ms
step:1288/2315 train_time:77925ms step_avg:60.50ms
step:1289/2315 train_time:77985ms step_avg:60.50ms
step:1290/2315 train_time:78046ms step_avg:60.50ms
step:1291/2315 train_time:78107ms step_avg:60.50ms
step:1292/2315 train_time:78168ms step_avg:60.50ms
step:1293/2315 train_time:78228ms step_avg:60.50ms
step:1294/2315 train_time:78289ms step_avg:60.50ms
step:1295/2315 train_time:78350ms step_avg:60.50ms
step:1296/2315 train_time:78410ms step_avg:60.50ms
step:1297/2315 train_time:78471ms step_avg:60.50ms
step:1298/2315 train_time:78533ms step_avg:60.50ms
step:1299/2315 train_time:78594ms step_avg:60.50ms
step:1300/2315 train_time:78657ms step_avg:60.51ms
step:1301/2315 train_time:78718ms step_avg:60.51ms
step:1302/2315 train_time:78779ms step_avg:60.51ms
step:1303/2315 train_time:78839ms step_avg:60.51ms
step:1304/2315 train_time:78901ms step_avg:60.51ms
step:1305/2315 train_time:78962ms step_avg:60.51ms
step:1306/2315 train_time:79023ms step_avg:60.51ms
step:1307/2315 train_time:79083ms step_avg:60.51ms
step:1308/2315 train_time:79144ms step_avg:60.51ms
step:1309/2315 train_time:79205ms step_avg:60.51ms
step:1310/2315 train_time:79266ms step_avg:60.51ms
step:1311/2315 train_time:79326ms step_avg:60.51ms
step:1312/2315 train_time:79387ms step_avg:60.51ms
step:1313/2315 train_time:79447ms step_avg:60.51ms
step:1314/2315 train_time:79509ms step_avg:60.51ms
step:1315/2315 train_time:79569ms step_avg:60.51ms
step:1316/2315 train_time:79631ms step_avg:60.51ms
step:1317/2315 train_time:79693ms step_avg:60.51ms
step:1318/2315 train_time:79755ms step_avg:60.51ms
step:1319/2315 train_time:79816ms step_avg:60.51ms
step:1320/2315 train_time:79878ms step_avg:60.51ms
step:1321/2315 train_time:79938ms step_avg:60.51ms
step:1322/2315 train_time:80000ms step_avg:60.51ms
step:1323/2315 train_time:80061ms step_avg:60.51ms
step:1324/2315 train_time:80121ms step_avg:60.51ms
step:1325/2315 train_time:80182ms step_avg:60.51ms
step:1326/2315 train_time:80243ms step_avg:60.52ms
step:1327/2315 train_time:80304ms step_avg:60.52ms
step:1328/2315 train_time:80364ms step_avg:60.52ms
step:1329/2315 train_time:80424ms step_avg:60.52ms
step:1330/2315 train_time:80485ms step_avg:60.52ms
step:1331/2315 train_time:80546ms step_avg:60.52ms
step:1332/2315 train_time:80607ms step_avg:60.52ms
step:1333/2315 train_time:80669ms step_avg:60.52ms
step:1334/2315 train_time:80731ms step_avg:60.52ms
step:1335/2315 train_time:80791ms step_avg:60.52ms
step:1336/2315 train_time:80853ms step_avg:60.52ms
step:1337/2315 train_time:80914ms step_avg:60.52ms
step:1338/2315 train_time:80977ms step_avg:60.52ms
step:1339/2315 train_time:81037ms step_avg:60.52ms
step:1340/2315 train_time:81098ms step_avg:60.52ms
step:1341/2315 train_time:81159ms step_avg:60.52ms
step:1342/2315 train_time:81220ms step_avg:60.52ms
step:1343/2315 train_time:81280ms step_avg:60.52ms
step:1344/2315 train_time:81342ms step_avg:60.52ms
step:1345/2315 train_time:81402ms step_avg:60.52ms
step:1346/2315 train_time:81463ms step_avg:60.52ms
step:1347/2315 train_time:81523ms step_avg:60.52ms
step:1348/2315 train_time:81584ms step_avg:60.52ms
step:1349/2315 train_time:81646ms step_avg:60.52ms
step:1350/2315 train_time:81707ms step_avg:60.52ms
step:1351/2315 train_time:81767ms step_avg:60.52ms
step:1352/2315 train_time:81829ms step_avg:60.52ms
step:1353/2315 train_time:81890ms step_avg:60.52ms
step:1354/2315 train_time:81951ms step_avg:60.53ms
step:1355/2315 train_time:82013ms step_avg:60.53ms
step:1356/2315 train_time:82074ms step_avg:60.53ms
step:1357/2315 train_time:82135ms step_avg:60.53ms
step:1358/2315 train_time:82197ms step_avg:60.53ms
step:1359/2315 train_time:82257ms step_avg:60.53ms
step:1360/2315 train_time:82319ms step_avg:60.53ms
step:1361/2315 train_time:82379ms step_avg:60.53ms
step:1362/2315 train_time:82440ms step_avg:60.53ms
step:1363/2315 train_time:82500ms step_avg:60.53ms
step:1364/2315 train_time:82561ms step_avg:60.53ms
step:1365/2315 train_time:82622ms step_avg:60.53ms
step:1366/2315 train_time:82683ms step_avg:60.53ms
step:1367/2315 train_time:82744ms step_avg:60.53ms
step:1368/2315 train_time:82805ms step_avg:60.53ms
step:1369/2315 train_time:82866ms step_avg:60.53ms
step:1370/2315 train_time:82928ms step_avg:60.53ms
step:1371/2315 train_time:82989ms step_avg:60.53ms
step:1372/2315 train_time:83050ms step_avg:60.53ms
step:1373/2315 train_time:83110ms step_avg:60.53ms
step:1374/2315 train_time:83172ms step_avg:60.53ms
step:1375/2315 train_time:83233ms step_avg:60.53ms
step:1376/2315 train_time:83295ms step_avg:60.53ms
step:1377/2315 train_time:83355ms step_avg:60.53ms
step:1378/2315 train_time:83417ms step_avg:60.53ms
step:1379/2315 train_time:83478ms step_avg:60.53ms
step:1380/2315 train_time:83539ms step_avg:60.54ms
step:1381/2315 train_time:83599ms step_avg:60.54ms
step:1382/2315 train_time:83661ms step_avg:60.54ms
step:1383/2315 train_time:83721ms step_avg:60.54ms
step:1384/2315 train_time:83783ms step_avg:60.54ms
step:1385/2315 train_time:83843ms step_avg:60.54ms
step:1386/2315 train_time:83904ms step_avg:60.54ms
step:1387/2315 train_time:83965ms step_avg:60.54ms
step:1388/2315 train_time:84026ms step_avg:60.54ms
step:1389/2315 train_time:84086ms step_avg:60.54ms
step:1390/2315 train_time:84148ms step_avg:60.54ms
step:1391/2315 train_time:84209ms step_avg:60.54ms
step:1392/2315 train_time:84270ms step_avg:60.54ms
step:1393/2315 train_time:84332ms step_avg:60.54ms
step:1394/2315 train_time:84394ms step_avg:60.54ms
step:1395/2315 train_time:84455ms step_avg:60.54ms
step:1396/2315 train_time:84516ms step_avg:60.54ms
step:1397/2315 train_time:84577ms step_avg:60.54ms
step:1398/2315 train_time:84638ms step_avg:60.54ms
step:1399/2315 train_time:84698ms step_avg:60.54ms
step:1400/2315 train_time:84760ms step_avg:60.54ms
step:1401/2315 train_time:84820ms step_avg:60.54ms
step:1402/2315 train_time:84883ms step_avg:60.54ms
step:1403/2315 train_time:84944ms step_avg:60.54ms
step:1404/2315 train_time:85004ms step_avg:60.54ms
step:1405/2315 train_time:85065ms step_avg:60.54ms
step:1406/2315 train_time:85126ms step_avg:60.54ms
step:1407/2315 train_time:85187ms step_avg:60.54ms
step:1408/2315 train_time:85248ms step_avg:60.55ms
step:1409/2315 train_time:85308ms step_avg:60.55ms
step:1410/2315 train_time:85370ms step_avg:60.55ms
step:1411/2315 train_time:85431ms step_avg:60.55ms
step:1412/2315 train_time:85493ms step_avg:60.55ms
step:1413/2315 train_time:85554ms step_avg:60.55ms
step:1414/2315 train_time:85616ms step_avg:60.55ms
step:1415/2315 train_time:85677ms step_avg:60.55ms
step:1416/2315 train_time:85738ms step_avg:60.55ms
step:1417/2315 train_time:85799ms step_avg:60.55ms
step:1418/2315 train_time:85860ms step_avg:60.55ms
step:1419/2315 train_time:85921ms step_avg:60.55ms
step:1420/2315 train_time:85982ms step_avg:60.55ms
step:1421/2315 train_time:86043ms step_avg:60.55ms
step:1422/2315 train_time:86104ms step_avg:60.55ms
step:1423/2315 train_time:86164ms step_avg:60.55ms
step:1424/2315 train_time:86225ms step_avg:60.55ms
step:1425/2315 train_time:86285ms step_avg:60.55ms
step:1426/2315 train_time:86347ms step_avg:60.55ms
step:1427/2315 train_time:86409ms step_avg:60.55ms
step:1428/2315 train_time:86470ms step_avg:60.55ms
step:1429/2315 train_time:86531ms step_avg:60.55ms
step:1430/2315 train_time:86592ms step_avg:60.55ms
step:1431/2315 train_time:86654ms step_avg:60.55ms
step:1432/2315 train_time:86716ms step_avg:60.56ms
step:1433/2315 train_time:86777ms step_avg:60.56ms
step:1434/2315 train_time:86838ms step_avg:60.56ms
step:1435/2315 train_time:86899ms step_avg:60.56ms
step:1436/2315 train_time:86960ms step_avg:60.56ms
step:1437/2315 train_time:87020ms step_avg:60.56ms
step:1438/2315 train_time:87081ms step_avg:60.56ms
step:1439/2315 train_time:87142ms step_avg:60.56ms
step:1440/2315 train_time:87204ms step_avg:60.56ms
step:1441/2315 train_time:87264ms step_avg:60.56ms
step:1442/2315 train_time:87325ms step_avg:60.56ms
step:1443/2315 train_time:87385ms step_avg:60.56ms
step:1444/2315 train_time:87446ms step_avg:60.56ms
step:1445/2315 train_time:87507ms step_avg:60.56ms
step:1446/2315 train_time:87568ms step_avg:60.56ms
step:1447/2315 train_time:87632ms step_avg:60.56ms
step:1448/2315 train_time:87691ms step_avg:60.56ms
step:1449/2315 train_time:87752ms step_avg:60.56ms
step:1450/2315 train_time:87814ms step_avg:60.56ms
step:1451/2315 train_time:87875ms step_avg:60.56ms
step:1452/2315 train_time:87936ms step_avg:60.56ms
step:1453/2315 train_time:87996ms step_avg:60.56ms
step:1454/2315 train_time:88058ms step_avg:60.56ms
step:1455/2315 train_time:88118ms step_avg:60.56ms
step:1456/2315 train_time:88180ms step_avg:60.56ms
step:1457/2315 train_time:88240ms step_avg:60.56ms
step:1458/2315 train_time:88301ms step_avg:60.56ms
step:1459/2315 train_time:88361ms step_avg:60.56ms
step:1460/2315 train_time:88423ms step_avg:60.56ms
step:1461/2315 train_time:88483ms step_avg:60.56ms
step:1462/2315 train_time:88545ms step_avg:60.56ms
step:1463/2315 train_time:88605ms step_avg:60.56ms
step:1464/2315 train_time:88667ms step_avg:60.56ms
step:1465/2315 train_time:88728ms step_avg:60.57ms
step:1466/2315 train_time:88789ms step_avg:60.57ms
step:1467/2315 train_time:88850ms step_avg:60.57ms
step:1468/2315 train_time:88911ms step_avg:60.57ms
step:1469/2315 train_time:88972ms step_avg:60.57ms
step:1470/2315 train_time:89034ms step_avg:60.57ms
step:1471/2315 train_time:89095ms step_avg:60.57ms
step:1472/2315 train_time:89157ms step_avg:60.57ms
step:1473/2315 train_time:89218ms step_avg:60.57ms
step:1474/2315 train_time:89279ms step_avg:60.57ms
step:1475/2315 train_time:89339ms step_avg:60.57ms
step:1476/2315 train_time:89400ms step_avg:60.57ms
step:1477/2315 train_time:89461ms step_avg:60.57ms
step:1478/2315 train_time:89522ms step_avg:60.57ms
step:1479/2315 train_time:89583ms step_avg:60.57ms
step:1480/2315 train_time:89645ms step_avg:60.57ms
step:1481/2315 train_time:89705ms step_avg:60.57ms
step:1482/2315 train_time:89766ms step_avg:60.57ms
step:1483/2315 train_time:89826ms step_avg:60.57ms
step:1484/2315 train_time:89887ms step_avg:60.57ms
step:1485/2315 train_time:89948ms step_avg:60.57ms
step:1486/2315 train_time:90009ms step_avg:60.57ms
step:1487/2315 train_time:90070ms step_avg:60.57ms
step:1488/2315 train_time:90132ms step_avg:60.57ms
step:1489/2315 train_time:90194ms step_avg:60.57ms
step:1490/2315 train_time:90256ms step_avg:60.57ms
step:1491/2315 train_time:90317ms step_avg:60.57ms
step:1492/2315 train_time:90378ms step_avg:60.58ms
step:1493/2315 train_time:90438ms step_avg:60.57ms
step:1494/2315 train_time:90499ms step_avg:60.58ms
step:1495/2315 train_time:90560ms step_avg:60.58ms
step:1496/2315 train_time:90622ms step_avg:60.58ms
step:1497/2315 train_time:90682ms step_avg:60.58ms
step:1498/2315 train_time:90744ms step_avg:60.58ms
step:1499/2315 train_time:90805ms step_avg:60.58ms
step:1500/2315 train_time:90865ms step_avg:60.58ms
step:1500/2315 val_loss:3.4456 train_time:90927ms step_avg:60.62ms
step:1501/2315 train_time:90946ms step_avg:60.59ms
step:1502/2315 train_time:90989ms step_avg:60.58ms
step:1503/2315 train_time:91052ms step_avg:60.58ms
step:1504/2315 train_time:91116ms step_avg:60.58ms
step:1505/2315 train_time:91177ms step_avg:60.58ms
step:1506/2315 train_time:91238ms step_avg:60.58ms
step:1507/2315 train_time:91297ms step_avg:60.58ms
step:1508/2315 train_time:91357ms step_avg:60.58ms
step:1509/2315 train_time:91417ms step_avg:60.58ms
step:1510/2315 train_time:91478ms step_avg:60.58ms
step:1511/2315 train_time:91537ms step_avg:60.58ms
step:1512/2315 train_time:91598ms step_avg:60.58ms
step:1513/2315 train_time:91658ms step_avg:60.58ms
step:1514/2315 train_time:91720ms step_avg:60.58ms
step:1515/2315 train_time:91780ms step_avg:60.58ms
step:1516/2315 train_time:91841ms step_avg:60.58ms
step:1517/2315 train_time:91904ms step_avg:60.58ms
step:1518/2315 train_time:91967ms step_avg:60.58ms
step:1519/2315 train_time:92030ms step_avg:60.59ms
step:1520/2315 train_time:92093ms step_avg:60.59ms
step:1521/2315 train_time:92154ms step_avg:60.59ms
step:1522/2315 train_time:92215ms step_avg:60.59ms
step:1523/2315 train_time:92276ms step_avg:60.59ms
step:1524/2315 train_time:92337ms step_avg:60.59ms
step:1525/2315 train_time:92398ms step_avg:60.59ms
step:1526/2315 train_time:92461ms step_avg:60.59ms
step:1527/2315 train_time:92520ms step_avg:60.59ms
step:1528/2315 train_time:92581ms step_avg:60.59ms
step:1529/2315 train_time:92642ms step_avg:60.59ms
step:1530/2315 train_time:92703ms step_avg:60.59ms
step:1531/2315 train_time:92764ms step_avg:60.59ms
step:1532/2315 train_time:92826ms step_avg:60.59ms
step:1533/2315 train_time:92888ms step_avg:60.59ms
step:1534/2315 train_time:92950ms step_avg:60.59ms
step:1535/2315 train_time:93012ms step_avg:60.59ms
step:1536/2315 train_time:93074ms step_avg:60.59ms
step:1537/2315 train_time:93134ms step_avg:60.59ms
step:1538/2315 train_time:93196ms step_avg:60.60ms
step:1539/2315 train_time:93257ms step_avg:60.60ms
step:1540/2315 train_time:93319ms step_avg:60.60ms
step:1541/2315 train_time:93380ms step_avg:60.60ms
step:1542/2315 train_time:93441ms step_avg:60.60ms
step:1543/2315 train_time:93502ms step_avg:60.60ms
step:1544/2315 train_time:93563ms step_avg:60.60ms
step:1545/2315 train_time:93624ms step_avg:60.60ms
step:1546/2315 train_time:93686ms step_avg:60.60ms
step:1547/2315 train_time:93747ms step_avg:60.60ms
step:1548/2315 train_time:93809ms step_avg:60.60ms
step:1549/2315 train_time:93870ms step_avg:60.60ms
step:1550/2315 train_time:93933ms step_avg:60.60ms
step:1551/2315 train_time:93994ms step_avg:60.60ms
step:1552/2315 train_time:94055ms step_avg:60.60ms
step:1553/2315 train_time:94117ms step_avg:60.60ms
step:1554/2315 train_time:94178ms step_avg:60.60ms
step:1555/2315 train_time:94240ms step_avg:60.60ms
step:1556/2315 train_time:94302ms step_avg:60.61ms
step:1557/2315 train_time:94362ms step_avg:60.61ms
step:1558/2315 train_time:94424ms step_avg:60.61ms
step:1559/2315 train_time:94484ms step_avg:60.61ms
step:1560/2315 train_time:94546ms step_avg:60.61ms
step:1561/2315 train_time:94607ms step_avg:60.61ms
step:1562/2315 train_time:94668ms step_avg:60.61ms
step:1563/2315 train_time:94729ms step_avg:60.61ms
step:1564/2315 train_time:94791ms step_avg:60.61ms
step:1565/2315 train_time:94851ms step_avg:60.61ms
step:1566/2315 train_time:94913ms step_avg:60.61ms
step:1567/2315 train_time:94973ms step_avg:60.61ms
step:1568/2315 train_time:95035ms step_avg:60.61ms
step:1569/2315 train_time:95096ms step_avg:60.61ms
step:1570/2315 train_time:95158ms step_avg:60.61ms
step:1571/2315 train_time:95219ms step_avg:60.61ms
step:1572/2315 train_time:95280ms step_avg:60.61ms
step:1573/2315 train_time:95342ms step_avg:60.61ms
step:1574/2315 train_time:95403ms step_avg:60.61ms
step:1575/2315 train_time:95463ms step_avg:60.61ms
step:1576/2315 train_time:95525ms step_avg:60.61ms
step:1577/2315 train_time:95586ms step_avg:60.61ms
step:1578/2315 train_time:95648ms step_avg:60.61ms
step:1579/2315 train_time:95709ms step_avg:60.61ms
step:1580/2315 train_time:95771ms step_avg:60.61ms
step:1581/2315 train_time:95832ms step_avg:60.61ms
step:1582/2315 train_time:95894ms step_avg:60.62ms
step:1583/2315 train_time:95955ms step_avg:60.62ms
step:1584/2315 train_time:96017ms step_avg:60.62ms
step:1585/2315 train_time:96077ms step_avg:60.62ms
step:1586/2315 train_time:96140ms step_avg:60.62ms
step:1587/2315 train_time:96200ms step_avg:60.62ms
step:1588/2315 train_time:96262ms step_avg:60.62ms
step:1589/2315 train_time:96323ms step_avg:60.62ms
step:1590/2315 train_time:96384ms step_avg:60.62ms
step:1591/2315 train_time:96446ms step_avg:60.62ms
step:1592/2315 train_time:96508ms step_avg:60.62ms
step:1593/2315 train_time:96569ms step_avg:60.62ms
step:1594/2315 train_time:96630ms step_avg:60.62ms
step:1595/2315 train_time:96691ms step_avg:60.62ms
step:1596/2315 train_time:96752ms step_avg:60.62ms
step:1597/2315 train_time:96813ms step_avg:60.62ms
step:1598/2315 train_time:96875ms step_avg:60.62ms
step:1599/2315 train_time:96936ms step_avg:60.62ms
step:1600/2315 train_time:96997ms step_avg:60.62ms
step:1601/2315 train_time:97061ms step_avg:60.62ms
step:1602/2315 train_time:97119ms step_avg:60.62ms
step:1603/2315 train_time:97180ms step_avg:60.62ms
step:1604/2315 train_time:97243ms step_avg:60.63ms
step:1605/2315 train_time:97304ms step_avg:60.63ms
step:1606/2315 train_time:97365ms step_avg:60.63ms
step:1607/2315 train_time:97426ms step_avg:60.63ms
step:1608/2315 train_time:97487ms step_avg:60.63ms
step:1609/2315 train_time:97548ms step_avg:60.63ms
step:1610/2315 train_time:97610ms step_avg:60.63ms
step:1611/2315 train_time:97671ms step_avg:60.63ms
step:1612/2315 train_time:97732ms step_avg:60.63ms
step:1613/2315 train_time:97793ms step_avg:60.63ms
step:1614/2315 train_time:97854ms step_avg:60.63ms
step:1615/2315 train_time:97915ms step_avg:60.63ms
step:1616/2315 train_time:97976ms step_avg:60.63ms
step:1617/2315 train_time:98037ms step_avg:60.63ms
step:1618/2315 train_time:98099ms step_avg:60.63ms
step:1619/2315 train_time:98160ms step_avg:60.63ms
step:1620/2315 train_time:98222ms step_avg:60.63ms
step:1621/2315 train_time:98283ms step_avg:60.63ms
step:1622/2315 train_time:98345ms step_avg:60.63ms
step:1623/2315 train_time:98406ms step_avg:60.63ms
step:1624/2315 train_time:98467ms step_avg:60.63ms
step:1625/2315 train_time:98528ms step_avg:60.63ms
step:1626/2315 train_time:98590ms step_avg:60.63ms
step:1627/2315 train_time:98650ms step_avg:60.63ms
step:1628/2315 train_time:98712ms step_avg:60.63ms
step:1629/2315 train_time:98773ms step_avg:60.63ms
step:1630/2315 train_time:98834ms step_avg:60.63ms
step:1631/2315 train_time:98895ms step_avg:60.63ms
step:1632/2315 train_time:98957ms step_avg:60.64ms
step:1633/2315 train_time:99017ms step_avg:60.63ms
step:1634/2315 train_time:99079ms step_avg:60.64ms
step:1635/2315 train_time:99140ms step_avg:60.64ms
step:1636/2315 train_time:99203ms step_avg:60.64ms
step:1637/2315 train_time:99263ms step_avg:60.64ms
step:1638/2315 train_time:99325ms step_avg:60.64ms
step:1639/2315 train_time:99386ms step_avg:60.64ms
step:1640/2315 train_time:99448ms step_avg:60.64ms
step:1641/2315 train_time:99510ms step_avg:60.64ms
step:1642/2315 train_time:99571ms step_avg:60.64ms
step:1643/2315 train_time:99632ms step_avg:60.64ms
step:1644/2315 train_time:99694ms step_avg:60.64ms
step:1645/2315 train_time:99754ms step_avg:60.64ms
step:1646/2315 train_time:99816ms step_avg:60.64ms
step:1647/2315 train_time:99877ms step_avg:60.64ms
step:1648/2315 train_time:99938ms step_avg:60.64ms
step:1649/2315 train_time:99998ms step_avg:60.64ms
step:1650/2315 train_time:100060ms step_avg:60.64ms
step:1651/2315 train_time:100122ms step_avg:60.64ms
step:1652/2315 train_time:100184ms step_avg:60.64ms
step:1653/2315 train_time:100245ms step_avg:60.64ms
step:1654/2315 train_time:100306ms step_avg:60.64ms
step:1655/2315 train_time:100367ms step_avg:60.64ms
step:1656/2315 train_time:100429ms step_avg:60.65ms
step:1657/2315 train_time:100490ms step_avg:60.65ms
step:1658/2315 train_time:100551ms step_avg:60.65ms
step:1659/2315 train_time:100612ms step_avg:60.65ms
step:1660/2315 train_time:100674ms step_avg:60.65ms
step:1661/2315 train_time:100735ms step_avg:60.65ms
step:1662/2315 train_time:100796ms step_avg:60.65ms
step:1663/2315 train_time:100857ms step_avg:60.65ms
step:1664/2315 train_time:100919ms step_avg:60.65ms
step:1665/2315 train_time:100980ms step_avg:60.65ms
step:1666/2315 train_time:101042ms step_avg:60.65ms
step:1667/2315 train_time:101102ms step_avg:60.65ms
step:1668/2315 train_time:101164ms step_avg:60.65ms
step:1669/2315 train_time:101226ms step_avg:60.65ms
step:1670/2315 train_time:101288ms step_avg:60.65ms
step:1671/2315 train_time:101349ms step_avg:60.65ms
step:1672/2315 train_time:101410ms step_avg:60.65ms
step:1673/2315 train_time:101470ms step_avg:60.65ms
step:1674/2315 train_time:101532ms step_avg:60.65ms
step:1675/2315 train_time:101594ms step_avg:60.65ms
step:1676/2315 train_time:101656ms step_avg:60.65ms
step:1677/2315 train_time:101716ms step_avg:60.65ms
step:1678/2315 train_time:101777ms step_avg:60.65ms
step:1679/2315 train_time:101838ms step_avg:60.65ms
step:1680/2315 train_time:101900ms step_avg:60.65ms
step:1681/2315 train_time:101963ms step_avg:60.66ms
step:1682/2315 train_time:102022ms step_avg:60.66ms
step:1683/2315 train_time:102083ms step_avg:60.66ms
step:1684/2315 train_time:102145ms step_avg:60.66ms
step:1685/2315 train_time:102206ms step_avg:60.66ms
step:1686/2315 train_time:102268ms step_avg:60.66ms
step:1687/2315 train_time:102329ms step_avg:60.66ms
step:1688/2315 train_time:102390ms step_avg:60.66ms
step:1689/2315 train_time:102451ms step_avg:60.66ms
step:1690/2315 train_time:102512ms step_avg:60.66ms
step:1691/2315 train_time:102574ms step_avg:60.66ms
step:1692/2315 train_time:102635ms step_avg:60.66ms
step:1693/2315 train_time:102696ms step_avg:60.66ms
step:1694/2315 train_time:102758ms step_avg:60.66ms
step:1695/2315 train_time:102819ms step_avg:60.66ms
step:1696/2315 train_time:102880ms step_avg:60.66ms
step:1697/2315 train_time:102941ms step_avg:60.66ms
step:1698/2315 train_time:103002ms step_avg:60.66ms
step:1699/2315 train_time:103064ms step_avg:60.66ms
step:1700/2315 train_time:103126ms step_avg:60.66ms
step:1701/2315 train_time:103187ms step_avg:60.66ms
step:1702/2315 train_time:103249ms step_avg:60.66ms
step:1703/2315 train_time:103309ms step_avg:60.66ms
step:1704/2315 train_time:103371ms step_avg:60.66ms
step:1705/2315 train_time:103432ms step_avg:60.66ms
step:1706/2315 train_time:103494ms step_avg:60.66ms
step:1707/2315 train_time:103556ms step_avg:60.67ms
step:1708/2315 train_time:103617ms step_avg:60.67ms
step:1709/2315 train_time:103678ms step_avg:60.67ms
step:1710/2315 train_time:103739ms step_avg:60.67ms
step:1711/2315 train_time:103801ms step_avg:60.67ms
step:1712/2315 train_time:103864ms step_avg:60.67ms
step:1713/2315 train_time:103924ms step_avg:60.67ms
step:1714/2315 train_time:103986ms step_avg:60.67ms
step:1715/2315 train_time:104047ms step_avg:60.67ms
step:1716/2315 train_time:104108ms step_avg:60.67ms
step:1717/2315 train_time:104170ms step_avg:60.67ms
step:1718/2315 train_time:104231ms step_avg:60.67ms
step:1719/2315 train_time:104292ms step_avg:60.67ms
step:1720/2315 train_time:104353ms step_avg:60.67ms
step:1721/2315 train_time:104414ms step_avg:60.67ms
step:1722/2315 train_time:104476ms step_avg:60.67ms
step:1723/2315 train_time:104538ms step_avg:60.67ms
step:1724/2315 train_time:104599ms step_avg:60.67ms
step:1725/2315 train_time:104660ms step_avg:60.67ms
step:1726/2315 train_time:104722ms step_avg:60.67ms
step:1727/2315 train_time:104782ms step_avg:60.67ms
step:1728/2315 train_time:104844ms step_avg:60.67ms
step:1729/2315 train_time:104905ms step_avg:60.67ms
step:1730/2315 train_time:104966ms step_avg:60.67ms
step:1731/2315 train_time:105028ms step_avg:60.67ms
step:1732/2315 train_time:105090ms step_avg:60.68ms
step:1733/2315 train_time:105151ms step_avg:60.68ms
step:1734/2315 train_time:105213ms step_avg:60.68ms
step:1735/2315 train_time:105273ms step_avg:60.68ms
step:1736/2315 train_time:105334ms step_avg:60.68ms
step:1737/2315 train_time:105395ms step_avg:60.68ms
step:1738/2315 train_time:105457ms step_avg:60.68ms
step:1739/2315 train_time:105517ms step_avg:60.68ms
step:1740/2315 train_time:105579ms step_avg:60.68ms
step:1741/2315 train_time:105640ms step_avg:60.68ms
step:1742/2315 train_time:105701ms step_avg:60.68ms
step:1743/2315 train_time:105762ms step_avg:60.68ms
step:1744/2315 train_time:105824ms step_avg:60.68ms
step:1745/2315 train_time:105886ms step_avg:60.68ms
step:1746/2315 train_time:105948ms step_avg:60.68ms
step:1747/2315 train_time:106009ms step_avg:60.68ms
step:1748/2315 train_time:106070ms step_avg:60.68ms
step:1749/2315 train_time:106132ms step_avg:60.68ms
step:1750/2315 train_time:106194ms step_avg:60.68ms
step:1750/2315 val_loss:3.3765 train_time:106256ms step_avg:60.72ms
step:1751/2315 train_time:106275ms step_avg:60.69ms
step:1752/2315 train_time:106321ms step_avg:60.69ms
step:1753/2315 train_time:106388ms step_avg:60.69ms
step:1754/2315 train_time:106452ms step_avg:60.69ms
step:1755/2315 train_time:106513ms step_avg:60.69ms
step:1756/2315 train_time:106575ms step_avg:60.69ms
step:1757/2315 train_time:106635ms step_avg:60.69ms
step:1758/2315 train_time:106697ms step_avg:60.69ms
step:1759/2315 train_time:106758ms step_avg:60.69ms
step:1760/2315 train_time:106819ms step_avg:60.69ms
step:1761/2315 train_time:106880ms step_avg:60.69ms
step:1762/2315 train_time:106941ms step_avg:60.69ms
step:1763/2315 train_time:107001ms step_avg:60.69ms
step:1764/2315 train_time:107062ms step_avg:60.69ms
step:1765/2315 train_time:107122ms step_avg:60.69ms
step:1766/2315 train_time:107186ms step_avg:60.69ms
step:1767/2315 train_time:107248ms step_avg:60.70ms
step:1768/2315 train_time:107311ms step_avg:60.70ms
step:1769/2315 train_time:107373ms step_avg:60.70ms
step:1770/2315 train_time:107436ms step_avg:60.70ms
step:1771/2315 train_time:107497ms step_avg:60.70ms
step:1772/2315 train_time:107559ms step_avg:60.70ms
step:1773/2315 train_time:107620ms step_avg:60.70ms
step:1774/2315 train_time:107681ms step_avg:60.70ms
step:1775/2315 train_time:107741ms step_avg:60.70ms
step:1776/2315 train_time:107802ms step_avg:60.70ms
step:1777/2315 train_time:107862ms step_avg:60.70ms
step:1778/2315 train_time:107923ms step_avg:60.70ms
step:1779/2315 train_time:107982ms step_avg:60.70ms
step:1780/2315 train_time:108043ms step_avg:60.70ms
step:1781/2315 train_time:108104ms step_avg:60.70ms
step:1782/2315 train_time:108166ms step_avg:60.70ms
step:1783/2315 train_time:108227ms step_avg:60.70ms
step:1784/2315 train_time:108289ms step_avg:60.70ms
step:1785/2315 train_time:108351ms step_avg:60.70ms
step:1786/2315 train_time:108413ms step_avg:60.70ms
step:1787/2315 train_time:108475ms step_avg:60.70ms
step:1788/2315 train_time:108537ms step_avg:60.70ms
step:1789/2315 train_time:108599ms step_avg:60.70ms
step:1790/2315 train_time:108661ms step_avg:60.70ms
step:1791/2315 train_time:108721ms step_avg:60.70ms
step:1792/2315 train_time:108783ms step_avg:60.70ms
step:1793/2315 train_time:108843ms step_avg:60.70ms
step:1794/2315 train_time:108904ms step_avg:60.70ms
step:1795/2315 train_time:108964ms step_avg:60.70ms
step:1796/2315 train_time:109025ms step_avg:60.70ms
step:1797/2315 train_time:109086ms step_avg:60.70ms
step:1798/2315 train_time:109147ms step_avg:60.70ms
step:1799/2315 train_time:109208ms step_avg:60.70ms
step:1800/2315 train_time:109270ms step_avg:60.71ms
step:1801/2315 train_time:109332ms step_avg:60.71ms
step:1802/2315 train_time:109394ms step_avg:60.71ms
step:1803/2315 train_time:109456ms step_avg:60.71ms
step:1804/2315 train_time:109518ms step_avg:60.71ms
step:1805/2315 train_time:109579ms step_avg:60.71ms
step:1806/2315 train_time:109641ms step_avg:60.71ms
step:1807/2315 train_time:109701ms step_avg:60.71ms
step:1808/2315 train_time:109762ms step_avg:60.71ms
step:1809/2315 train_time:109823ms step_avg:60.71ms
step:1810/2315 train_time:109884ms step_avg:60.71ms
step:1811/2315 train_time:109946ms step_avg:60.71ms
step:1812/2315 train_time:110006ms step_avg:60.71ms
step:1813/2315 train_time:110067ms step_avg:60.71ms
step:1814/2315 train_time:110128ms step_avg:60.71ms
step:1815/2315 train_time:110189ms step_avg:60.71ms
step:1816/2315 train_time:110250ms step_avg:60.71ms
step:1817/2315 train_time:110312ms step_avg:60.71ms
step:1818/2315 train_time:110374ms step_avg:60.71ms
step:1819/2315 train_time:110436ms step_avg:60.71ms
step:1820/2315 train_time:110498ms step_avg:60.71ms
step:1821/2315 train_time:110559ms step_avg:60.71ms
step:1822/2315 train_time:110621ms step_avg:60.71ms
step:1823/2315 train_time:110681ms step_avg:60.71ms
step:1824/2315 train_time:110742ms step_avg:60.71ms
step:1825/2315 train_time:110802ms step_avg:60.71ms
step:1826/2315 train_time:110864ms step_avg:60.71ms
step:1827/2315 train_time:110925ms step_avg:60.71ms
step:1828/2315 train_time:110986ms step_avg:60.71ms
step:1829/2315 train_time:111047ms step_avg:60.71ms
step:1830/2315 train_time:111108ms step_avg:60.71ms
step:1831/2315 train_time:111169ms step_avg:60.71ms
step:1832/2315 train_time:111230ms step_avg:60.72ms
step:1833/2315 train_time:111292ms step_avg:60.72ms
step:1834/2315 train_time:111353ms step_avg:60.72ms
step:1835/2315 train_time:111415ms step_avg:60.72ms
step:1836/2315 train_time:111477ms step_avg:60.72ms
step:1837/2315 train_time:111539ms step_avg:60.72ms
step:1838/2315 train_time:111601ms step_avg:60.72ms
step:1839/2315 train_time:111662ms step_avg:60.72ms
step:1840/2315 train_time:111723ms step_avg:60.72ms
step:1841/2315 train_time:111784ms step_avg:60.72ms
step:1842/2315 train_time:111845ms step_avg:60.72ms
step:1843/2315 train_time:111906ms step_avg:60.72ms
step:1844/2315 train_time:111968ms step_avg:60.72ms
step:1845/2315 train_time:112028ms step_avg:60.72ms
step:1846/2315 train_time:112089ms step_avg:60.72ms
step:1847/2315 train_time:112150ms step_avg:60.72ms
step:1848/2315 train_time:112213ms step_avg:60.72ms
step:1849/2315 train_time:112273ms step_avg:60.72ms
step:1850/2315 train_time:112335ms step_avg:60.72ms
step:1851/2315 train_time:112396ms step_avg:60.72ms
step:1852/2315 train_time:112458ms step_avg:60.72ms
step:1853/2315 train_time:112520ms step_avg:60.72ms
step:1854/2315 train_time:112581ms step_avg:60.72ms
step:1855/2315 train_time:112642ms step_avg:60.72ms
step:1856/2315 train_time:112704ms step_avg:60.72ms
step:1857/2315 train_time:112764ms step_avg:60.72ms
step:1858/2315 train_time:112826ms step_avg:60.72ms
step:1859/2315 train_time:112886ms step_avg:60.72ms
step:1860/2315 train_time:112947ms step_avg:60.72ms
step:1861/2315 train_time:113008ms step_avg:60.72ms
step:1862/2315 train_time:113069ms step_avg:60.72ms
step:1863/2315 train_time:113130ms step_avg:60.72ms
step:1864/2315 train_time:113192ms step_avg:60.73ms
step:1865/2315 train_time:113253ms step_avg:60.73ms
step:1866/2315 train_time:113315ms step_avg:60.73ms
step:1867/2315 train_time:113376ms step_avg:60.73ms
step:1868/2315 train_time:113438ms step_avg:60.73ms
step:1869/2315 train_time:113499ms step_avg:60.73ms
step:1870/2315 train_time:113561ms step_avg:60.73ms
step:1871/2315 train_time:113622ms step_avg:60.73ms
step:1872/2315 train_time:113684ms step_avg:60.73ms
step:1873/2315 train_time:113744ms step_avg:60.73ms
step:1874/2315 train_time:113805ms step_avg:60.73ms
step:1875/2315 train_time:113866ms step_avg:60.73ms
step:1876/2315 train_time:113928ms step_avg:60.73ms
step:1877/2315 train_time:113988ms step_avg:60.73ms
step:1878/2315 train_time:114050ms step_avg:60.73ms
step:1879/2315 train_time:114111ms step_avg:60.73ms
step:1880/2315 train_time:114172ms step_avg:60.73ms
step:1881/2315 train_time:114233ms step_avg:60.73ms
step:1882/2315 train_time:114295ms step_avg:60.73ms
step:1883/2315 train_time:114356ms step_avg:60.73ms
step:1884/2315 train_time:114418ms step_avg:60.73ms
step:1885/2315 train_time:114479ms step_avg:60.73ms
step:1886/2315 train_time:114541ms step_avg:60.73ms
step:1887/2315 train_time:114602ms step_avg:60.73ms
step:1888/2315 train_time:114664ms step_avg:60.73ms
step:1889/2315 train_time:114725ms step_avg:60.73ms
step:1890/2315 train_time:114786ms step_avg:60.73ms
step:1891/2315 train_time:114847ms step_avg:60.73ms
step:1892/2315 train_time:114908ms step_avg:60.73ms
step:1893/2315 train_time:114969ms step_avg:60.73ms
step:1894/2315 train_time:115030ms step_avg:60.73ms
step:1895/2315 train_time:115090ms step_avg:60.73ms
step:1896/2315 train_time:115151ms step_avg:60.73ms
step:1897/2315 train_time:115213ms step_avg:60.73ms
step:1898/2315 train_time:115275ms step_avg:60.74ms
step:1899/2315 train_time:115337ms step_avg:60.74ms
step:1900/2315 train_time:115398ms step_avg:60.74ms
step:1901/2315 train_time:115459ms step_avg:60.74ms
step:1902/2315 train_time:115521ms step_avg:60.74ms
step:1903/2315 train_time:115582ms step_avg:60.74ms
step:1904/2315 train_time:115644ms step_avg:60.74ms
step:1905/2315 train_time:115705ms step_avg:60.74ms
step:1906/2315 train_time:115767ms step_avg:60.74ms
step:1907/2315 train_time:115827ms step_avg:60.74ms
step:1908/2315 train_time:115888ms step_avg:60.74ms
step:1909/2315 train_time:115949ms step_avg:60.74ms
step:1910/2315 train_time:116011ms step_avg:60.74ms
step:1911/2315 train_time:116072ms step_avg:60.74ms
step:1912/2315 train_time:116133ms step_avg:60.74ms
step:1913/2315 train_time:116194ms step_avg:60.74ms
step:1914/2315 train_time:116255ms step_avg:60.74ms
step:1915/2315 train_time:116316ms step_avg:60.74ms
step:1916/2315 train_time:116378ms step_avg:60.74ms
step:1917/2315 train_time:116439ms step_avg:60.74ms
step:1918/2315 train_time:116501ms step_avg:60.74ms
step:1919/2315 train_time:116562ms step_avg:60.74ms
step:1920/2315 train_time:116625ms step_avg:60.74ms
step:1921/2315 train_time:116686ms step_avg:60.74ms
step:1922/2315 train_time:116748ms step_avg:60.74ms
step:1923/2315 train_time:116809ms step_avg:60.74ms
step:1924/2315 train_time:116871ms step_avg:60.74ms
step:1925/2315 train_time:116931ms step_avg:60.74ms
step:1926/2315 train_time:116993ms step_avg:60.74ms
step:1927/2315 train_time:117055ms step_avg:60.74ms
step:1928/2315 train_time:117118ms step_avg:60.75ms
step:1929/2315 train_time:117179ms step_avg:60.75ms
step:1930/2315 train_time:117240ms step_avg:60.75ms
step:1931/2315 train_time:117301ms step_avg:60.75ms
step:1932/2315 train_time:117363ms step_avg:60.75ms
step:1933/2315 train_time:117423ms step_avg:60.75ms
step:1934/2315 train_time:117485ms step_avg:60.75ms
step:1935/2315 train_time:117546ms step_avg:60.75ms
step:1936/2315 train_time:117608ms step_avg:60.75ms
step:1937/2315 train_time:117669ms step_avg:60.75ms
step:1938/2315 train_time:117731ms step_avg:60.75ms
step:1939/2315 train_time:117792ms step_avg:60.75ms
step:1940/2315 train_time:117854ms step_avg:60.75ms
step:1941/2315 train_time:117915ms step_avg:60.75ms
step:1942/2315 train_time:117976ms step_avg:60.75ms
step:1943/2315 train_time:118037ms step_avg:60.75ms
step:1944/2315 train_time:118099ms step_avg:60.75ms
step:1945/2315 train_time:118160ms step_avg:60.75ms
step:1946/2315 train_time:118221ms step_avg:60.75ms
step:1947/2315 train_time:118282ms step_avg:60.75ms
step:1948/2315 train_time:118344ms step_avg:60.75ms
step:1949/2315 train_time:118404ms step_avg:60.75ms
step:1950/2315 train_time:118466ms step_avg:60.75ms
step:1951/2315 train_time:118527ms step_avg:60.75ms
step:1952/2315 train_time:118588ms step_avg:60.75ms
step:1953/2315 train_time:118649ms step_avg:60.75ms
step:1954/2315 train_time:118711ms step_avg:60.75ms
step:1955/2315 train_time:118773ms step_avg:60.75ms
step:1956/2315 train_time:118834ms step_avg:60.75ms
step:1957/2315 train_time:118895ms step_avg:60.75ms
step:1958/2315 train_time:118957ms step_avg:60.75ms
step:1959/2315 train_time:119018ms step_avg:60.75ms
step:1960/2315 train_time:119080ms step_avg:60.75ms
step:1961/2315 train_time:119141ms step_avg:60.75ms
step:1962/2315 train_time:119202ms step_avg:60.76ms
step:1963/2315 train_time:119263ms step_avg:60.76ms
step:1964/2315 train_time:119324ms step_avg:60.76ms
step:1965/2315 train_time:119385ms step_avg:60.76ms
step:1966/2315 train_time:119447ms step_avg:60.76ms
step:1967/2315 train_time:119508ms step_avg:60.76ms
step:1968/2315 train_time:119569ms step_avg:60.76ms
step:1969/2315 train_time:119630ms step_avg:60.76ms
step:1970/2315 train_time:119692ms step_avg:60.76ms
step:1971/2315 train_time:119753ms step_avg:60.76ms
step:1972/2315 train_time:119815ms step_avg:60.76ms
step:1973/2315 train_time:119875ms step_avg:60.76ms
step:1974/2315 train_time:119937ms step_avg:60.76ms
step:1975/2315 train_time:119999ms step_avg:60.76ms
step:1976/2315 train_time:120061ms step_avg:60.76ms
step:1977/2315 train_time:120121ms step_avg:60.76ms
step:1978/2315 train_time:120183ms step_avg:60.76ms
step:1979/2315 train_time:120244ms step_avg:60.76ms
step:1980/2315 train_time:120305ms step_avg:60.76ms
step:1981/2315 train_time:120366ms step_avg:60.76ms
step:1982/2315 train_time:120428ms step_avg:60.76ms
step:1983/2315 train_time:120488ms step_avg:60.76ms
step:1984/2315 train_time:120549ms step_avg:60.76ms
step:1985/2315 train_time:120610ms step_avg:60.76ms
step:1986/2315 train_time:120672ms step_avg:60.76ms
step:1987/2315 train_time:120733ms step_avg:60.76ms
step:1988/2315 train_time:120795ms step_avg:60.76ms
step:1989/2315 train_time:120856ms step_avg:60.76ms
step:1990/2315 train_time:120919ms step_avg:60.76ms
step:1991/2315 train_time:120980ms step_avg:60.76ms
step:1992/2315 train_time:121043ms step_avg:60.76ms
step:1993/2315 train_time:121102ms step_avg:60.76ms
step:1994/2315 train_time:121164ms step_avg:60.76ms
step:1995/2315 train_time:121225ms step_avg:60.76ms
step:1996/2315 train_time:121287ms step_avg:60.76ms
step:1997/2315 train_time:121347ms step_avg:60.76ms
step:1998/2315 train_time:121409ms step_avg:60.77ms
step:1999/2315 train_time:121470ms step_avg:60.77ms
step:2000/2315 train_time:121532ms step_avg:60.77ms
step:2000/2315 val_loss:3.3263 train_time:121595ms step_avg:60.80ms
step:2001/2315 train_time:121614ms step_avg:60.78ms
step:2002/2315 train_time:121658ms step_avg:60.77ms
step:2003/2315 train_time:121723ms step_avg:60.77ms
step:2004/2315 train_time:121786ms step_avg:60.77ms
step:2005/2315 train_time:121848ms step_avg:60.77ms
step:2006/2315 train_time:121910ms step_avg:60.77ms
step:2007/2315 train_time:121970ms step_avg:60.77ms
step:2008/2315 train_time:122032ms step_avg:60.77ms
step:2009/2315 train_time:122093ms step_avg:60.77ms
step:2010/2315 train_time:122153ms step_avg:60.77ms
step:2011/2315 train_time:122213ms step_avg:60.77ms
step:2012/2315 train_time:122274ms step_avg:60.77ms
step:2013/2315 train_time:122334ms step_avg:60.77ms
step:2014/2315 train_time:122395ms step_avg:60.77ms
step:2015/2315 train_time:122456ms step_avg:60.77ms
step:2016/2315 train_time:122517ms step_avg:60.77ms
step:2017/2315 train_time:122579ms step_avg:60.77ms
step:2018/2315 train_time:122643ms step_avg:60.77ms
step:2019/2315 train_time:122706ms step_avg:60.78ms
step:2020/2315 train_time:122770ms step_avg:60.78ms
step:2021/2315 train_time:122830ms step_avg:60.78ms
step:2022/2315 train_time:122891ms step_avg:60.78ms
step:2023/2315 train_time:122952ms step_avg:60.78ms
step:2024/2315 train_time:123014ms step_avg:60.78ms
step:2025/2315 train_time:123074ms step_avg:60.78ms
step:2026/2315 train_time:123135ms step_avg:60.78ms
step:2027/2315 train_time:123196ms step_avg:60.78ms
step:2028/2315 train_time:123258ms step_avg:60.78ms
step:2029/2315 train_time:123318ms step_avg:60.78ms
step:2030/2315 train_time:123380ms step_avg:60.78ms
step:2031/2315 train_time:123441ms step_avg:60.78ms
step:2032/2315 train_time:123503ms step_avg:60.78ms
step:2033/2315 train_time:123564ms step_avg:60.78ms
step:2034/2315 train_time:123627ms step_avg:60.78ms
step:2035/2315 train_time:123689ms step_avg:60.78ms
step:2036/2315 train_time:123751ms step_avg:60.78ms
step:2037/2315 train_time:123812ms step_avg:60.78ms
step:2038/2315 train_time:123874ms step_avg:60.78ms
step:2039/2315 train_time:123935ms step_avg:60.78ms
step:2040/2315 train_time:123998ms step_avg:60.78ms
step:2041/2315 train_time:124058ms step_avg:60.78ms
step:2042/2315 train_time:124121ms step_avg:60.78ms
step:2043/2315 train_time:124181ms step_avg:60.78ms
step:2044/2315 train_time:124244ms step_avg:60.78ms
step:2045/2315 train_time:124305ms step_avg:60.78ms
step:2046/2315 train_time:124366ms step_avg:60.79ms
step:2047/2315 train_time:124427ms step_avg:60.79ms
step:2048/2315 train_time:124489ms step_avg:60.79ms
step:2049/2315 train_time:124550ms step_avg:60.79ms
step:2050/2315 train_time:124612ms step_avg:60.79ms
step:2051/2315 train_time:124673ms step_avg:60.79ms
step:2052/2315 train_time:124735ms step_avg:60.79ms
step:2053/2315 train_time:124797ms step_avg:60.79ms
step:2054/2315 train_time:124859ms step_avg:60.79ms
step:2055/2315 train_time:124920ms step_avg:60.79ms
step:2056/2315 train_time:124982ms step_avg:60.79ms
step:2057/2315 train_time:125044ms step_avg:60.79ms
step:2058/2315 train_time:125106ms step_avg:60.79ms
step:2059/2315 train_time:125167ms step_avg:60.79ms
step:2060/2315 train_time:125228ms step_avg:60.79ms
step:2061/2315 train_time:125289ms step_avg:60.79ms
step:2062/2315 train_time:125350ms step_avg:60.79ms
step:2063/2315 train_time:125411ms step_avg:60.79ms
step:2064/2315 train_time:125473ms step_avg:60.79ms
step:2065/2315 train_time:125533ms step_avg:60.79ms
step:2066/2315 train_time:125595ms step_avg:60.79ms
step:2067/2315 train_time:125656ms step_avg:60.79ms
step:2068/2315 train_time:125717ms step_avg:60.79ms
step:2069/2315 train_time:125779ms step_avg:60.79ms
step:2070/2315 train_time:125842ms step_avg:60.79ms
step:2071/2315 train_time:125902ms step_avg:60.79ms
step:2072/2315 train_time:125964ms step_avg:60.79ms
step:2073/2315 train_time:126025ms step_avg:60.79ms
step:2074/2315 train_time:126087ms step_avg:60.79ms
step:2075/2315 train_time:126148ms step_avg:60.79ms
step:2076/2315 train_time:126209ms step_avg:60.79ms
step:2077/2315 train_time:126270ms step_avg:60.79ms
step:2078/2315 train_time:126331ms step_avg:60.79ms
step:2079/2315 train_time:126393ms step_avg:60.79ms
step:2080/2315 train_time:126454ms step_avg:60.80ms
step:2081/2315 train_time:126515ms step_avg:60.80ms
step:2082/2315 train_time:126576ms step_avg:60.80ms
step:2083/2315 train_time:126637ms step_avg:60.80ms
step:2084/2315 train_time:126699ms step_avg:60.80ms
step:2085/2315 train_time:126760ms step_avg:60.80ms
step:2086/2315 train_time:126823ms step_avg:60.80ms
step:2087/2315 train_time:126885ms step_avg:60.80ms
step:2088/2315 train_time:126946ms step_avg:60.80ms
step:2089/2315 train_time:127007ms step_avg:60.80ms
step:2090/2315 train_time:127068ms step_avg:60.80ms
step:2091/2315 train_time:127130ms step_avg:60.80ms
step:2092/2315 train_time:127192ms step_avg:60.80ms
step:2093/2315 train_time:127252ms step_avg:60.80ms
step:2094/2315 train_time:127313ms step_avg:60.80ms
step:2095/2315 train_time:127374ms step_avg:60.80ms
step:2096/2315 train_time:127436ms step_avg:60.80ms
step:2097/2315 train_time:127497ms step_avg:60.80ms
step:2098/2315 train_time:127559ms step_avg:60.80ms
step:2099/2315 train_time:127620ms step_avg:60.80ms
step:2100/2315 train_time:127683ms step_avg:60.80ms
step:2101/2315 train_time:127744ms step_avg:60.80ms
step:2102/2315 train_time:127805ms step_avg:60.80ms
step:2103/2315 train_time:127866ms step_avg:60.80ms
step:2104/2315 train_time:127927ms step_avg:60.80ms
step:2105/2315 train_time:127988ms step_avg:60.80ms
step:2106/2315 train_time:128050ms step_avg:60.80ms
step:2107/2315 train_time:128111ms step_avg:60.80ms
step:2108/2315 train_time:128172ms step_avg:60.80ms
step:2109/2315 train_time:128233ms step_avg:60.80ms
step:2110/2315 train_time:128294ms step_avg:60.80ms
step:2111/2315 train_time:128354ms step_avg:60.80ms
step:2112/2315 train_time:128416ms step_avg:60.80ms
step:2113/2315 train_time:128477ms step_avg:60.80ms
step:2114/2315 train_time:128538ms step_avg:60.80ms
step:2115/2315 train_time:128600ms step_avg:60.80ms
step:2116/2315 train_time:128661ms step_avg:60.80ms
step:2117/2315 train_time:128723ms step_avg:60.80ms
step:2118/2315 train_time:128786ms step_avg:60.81ms
step:2119/2315 train_time:128846ms step_avg:60.81ms
step:2120/2315 train_time:128908ms step_avg:60.81ms
step:2121/2315 train_time:128969ms step_avg:60.81ms
step:2122/2315 train_time:129030ms step_avg:60.81ms
step:2123/2315 train_time:129090ms step_avg:60.81ms
step:2124/2315 train_time:129152ms step_avg:60.81ms
step:2125/2315 train_time:129213ms step_avg:60.81ms
step:2126/2315 train_time:129274ms step_avg:60.81ms
step:2127/2315 train_time:129335ms step_avg:60.81ms
step:2128/2315 train_time:129397ms step_avg:60.81ms
step:2129/2315 train_time:129457ms step_avg:60.81ms
step:2130/2315 train_time:129519ms step_avg:60.81ms
step:2131/2315 train_time:129580ms step_avg:60.81ms
step:2132/2315 train_time:129642ms step_avg:60.81ms
step:2133/2315 train_time:129703ms step_avg:60.81ms
step:2134/2315 train_time:129765ms step_avg:60.81ms
step:2135/2315 train_time:129826ms step_avg:60.81ms
step:2136/2315 train_time:129888ms step_avg:60.81ms
step:2137/2315 train_time:129949ms step_avg:60.81ms
step:2138/2315 train_time:130011ms step_avg:60.81ms
step:2139/2315 train_time:130072ms step_avg:60.81ms
step:2140/2315 train_time:130133ms step_avg:60.81ms
step:2141/2315 train_time:130193ms step_avg:60.81ms
step:2142/2315 train_time:130255ms step_avg:60.81ms
step:2143/2315 train_time:130316ms step_avg:60.81ms
step:2144/2315 train_time:130377ms step_avg:60.81ms
step:2145/2315 train_time:130438ms step_avg:60.81ms
step:2146/2315 train_time:130499ms step_avg:60.81ms
step:2147/2315 train_time:130561ms step_avg:60.81ms
step:2148/2315 train_time:130623ms step_avg:60.81ms
step:2149/2315 train_time:130684ms step_avg:60.81ms
step:2150/2315 train_time:130746ms step_avg:60.81ms
step:2151/2315 train_time:130808ms step_avg:60.81ms
step:2152/2315 train_time:130870ms step_avg:60.81ms
step:2153/2315 train_time:130931ms step_avg:60.81ms
step:2154/2315 train_time:130993ms step_avg:60.81ms
step:2155/2315 train_time:131054ms step_avg:60.81ms
step:2156/2315 train_time:131115ms step_avg:60.81ms
step:2157/2315 train_time:131176ms step_avg:60.81ms
step:2158/2315 train_time:131238ms step_avg:60.81ms
step:2159/2315 train_time:131299ms step_avg:60.81ms
step:2160/2315 train_time:131361ms step_avg:60.82ms
step:2161/2315 train_time:131421ms step_avg:60.82ms
step:2162/2315 train_time:131483ms step_avg:60.82ms
step:2163/2315 train_time:131544ms step_avg:60.82ms
step:2164/2315 train_time:131605ms step_avg:60.82ms
step:2165/2315 train_time:131666ms step_avg:60.82ms
step:2166/2315 train_time:131728ms step_avg:60.82ms
step:2167/2315 train_time:131789ms step_avg:60.82ms
step:2168/2315 train_time:131850ms step_avg:60.82ms
step:2169/2315 train_time:131911ms step_avg:60.82ms
step:2170/2315 train_time:131972ms step_avg:60.82ms
step:2171/2315 train_time:132033ms step_avg:60.82ms
step:2172/2315 train_time:132094ms step_avg:60.82ms
step:2173/2315 train_time:132155ms step_avg:60.82ms
step:2174/2315 train_time:132217ms step_avg:60.82ms
step:2175/2315 train_time:132277ms step_avg:60.82ms
step:2176/2315 train_time:132339ms step_avg:60.82ms
step:2177/2315 train_time:132400ms step_avg:60.82ms
step:2178/2315 train_time:132462ms step_avg:60.82ms
step:2179/2315 train_time:132523ms step_avg:60.82ms
step:2180/2315 train_time:132585ms step_avg:60.82ms
step:2181/2315 train_time:132647ms step_avg:60.82ms
step:2182/2315 train_time:132708ms step_avg:60.82ms
step:2183/2315 train_time:132769ms step_avg:60.82ms
step:2184/2315 train_time:132831ms step_avg:60.82ms
step:2185/2315 train_time:132892ms step_avg:60.82ms
step:2186/2315 train_time:132953ms step_avg:60.82ms
step:2187/2315 train_time:133014ms step_avg:60.82ms
step:2188/2315 train_time:133075ms step_avg:60.82ms
step:2189/2315 train_time:133137ms step_avg:60.82ms
step:2190/2315 train_time:133199ms step_avg:60.82ms
step:2191/2315 train_time:133260ms step_avg:60.82ms
step:2192/2315 train_time:133322ms step_avg:60.82ms
step:2193/2315 train_time:133383ms step_avg:60.82ms
step:2194/2315 train_time:133444ms step_avg:60.82ms
step:2195/2315 train_time:133505ms step_avg:60.82ms
step:2196/2315 train_time:133567ms step_avg:60.82ms
step:2197/2315 train_time:133628ms step_avg:60.82ms
step:2198/2315 train_time:133690ms step_avg:60.82ms
step:2199/2315 train_time:133750ms step_avg:60.82ms
step:2200/2315 train_time:133812ms step_avg:60.82ms
step:2201/2315 train_time:133873ms step_avg:60.82ms
step:2202/2315 train_time:133935ms step_avg:60.82ms
step:2203/2315 train_time:133996ms step_avg:60.82ms
step:2204/2315 train_time:134061ms step_avg:60.83ms
step:2205/2315 train_time:134119ms step_avg:60.82ms
step:2206/2315 train_time:134180ms step_avg:60.83ms
step:2207/2315 train_time:134241ms step_avg:60.83ms
step:2208/2315 train_time:134303ms step_avg:60.83ms
step:2209/2315 train_time:134364ms step_avg:60.83ms
step:2210/2315 train_time:134426ms step_avg:60.83ms
step:2211/2315 train_time:134487ms step_avg:60.83ms
step:2212/2315 train_time:134548ms step_avg:60.83ms
step:2213/2315 train_time:134609ms step_avg:60.83ms
step:2214/2315 train_time:134671ms step_avg:60.83ms
step:2215/2315 train_time:134732ms step_avg:60.83ms
step:2216/2315 train_time:134794ms step_avg:60.83ms
step:2217/2315 train_time:134854ms step_avg:60.83ms
step:2218/2315 train_time:134916ms step_avg:60.83ms
step:2219/2315 train_time:134978ms step_avg:60.83ms
step:2220/2315 train_time:135040ms step_avg:60.83ms
step:2221/2315 train_time:135100ms step_avg:60.83ms
step:2222/2315 train_time:135162ms step_avg:60.83ms
step:2223/2315 train_time:135222ms step_avg:60.83ms
step:2224/2315 train_time:135285ms step_avg:60.83ms
step:2225/2315 train_time:135345ms step_avg:60.83ms
step:2226/2315 train_time:135407ms step_avg:60.83ms
step:2227/2315 train_time:135468ms step_avg:60.83ms
step:2228/2315 train_time:135529ms step_avg:60.83ms
step:2229/2315 train_time:135590ms step_avg:60.83ms
step:2230/2315 train_time:135652ms step_avg:60.83ms
step:2231/2315 train_time:135713ms step_avg:60.83ms
step:2232/2315 train_time:135774ms step_avg:60.83ms
step:2233/2315 train_time:135835ms step_avg:60.83ms
step:2234/2315 train_time:135896ms step_avg:60.83ms
step:2235/2315 train_time:135957ms step_avg:60.83ms
step:2236/2315 train_time:136019ms step_avg:60.83ms
step:2237/2315 train_time:136080ms step_avg:60.83ms
step:2238/2315 train_time:136142ms step_avg:60.83ms
step:2239/2315 train_time:136203ms step_avg:60.83ms
step:2240/2315 train_time:136264ms step_avg:60.83ms
step:2241/2315 train_time:136325ms step_avg:60.83ms
step:2242/2315 train_time:136387ms step_avg:60.83ms
step:2243/2315 train_time:136448ms step_avg:60.83ms
step:2244/2315 train_time:136510ms step_avg:60.83ms
step:2245/2315 train_time:136572ms step_avg:60.83ms
step:2246/2315 train_time:136633ms step_avg:60.83ms
step:2247/2315 train_time:136694ms step_avg:60.83ms
step:2248/2315 train_time:136755ms step_avg:60.83ms
step:2249/2315 train_time:136816ms step_avg:60.83ms
step:2250/2315 train_time:136877ms step_avg:60.83ms
step:2250/2315 val_loss:3.2863 train_time:136940ms step_avg:60.86ms
step:2251/2315 train_time:136960ms step_avg:60.84ms
step:2252/2315 train_time:137003ms step_avg:60.84ms
step:2253/2315 train_time:137067ms step_avg:60.84ms
step:2254/2315 train_time:137130ms step_avg:60.84ms
step:2255/2315 train_time:137191ms step_avg:60.84ms
step:2256/2315 train_time:137252ms step_avg:60.84ms
step:2257/2315 train_time:137313ms step_avg:60.84ms
step:2258/2315 train_time:137374ms step_avg:60.84ms
step:2259/2315 train_time:137433ms step_avg:60.84ms
step:2260/2315 train_time:137494ms step_avg:60.84ms
step:2261/2315 train_time:137554ms step_avg:60.84ms
step:2262/2315 train_time:137615ms step_avg:60.84ms
step:2263/2315 train_time:137675ms step_avg:60.84ms
step:2264/2315 train_time:137736ms step_avg:60.84ms
step:2265/2315 train_time:137796ms step_avg:60.84ms
step:2266/2315 train_time:137859ms step_avg:60.84ms
step:2267/2315 train_time:137923ms step_avg:60.84ms
step:2268/2315 train_time:137986ms step_avg:60.84ms
step:2269/2315 train_time:138049ms step_avg:60.84ms
step:2270/2315 train_time:138112ms step_avg:60.84ms
step:2271/2315 train_time:138174ms step_avg:60.84ms
step:2272/2315 train_time:138235ms step_avg:60.84ms
step:2273/2315 train_time:138296ms step_avg:60.84ms
step:2274/2315 train_time:138357ms step_avg:60.84ms
step:2275/2315 train_time:138417ms step_avg:60.84ms
step:2276/2315 train_time:138478ms step_avg:60.84ms
step:2277/2315 train_time:138538ms step_avg:60.84ms
step:2278/2315 train_time:138599ms step_avg:60.84ms
step:2279/2315 train_time:138660ms step_avg:60.84ms
step:2280/2315 train_time:138722ms step_avg:60.84ms
step:2281/2315 train_time:138783ms step_avg:60.84ms
step:2282/2315 train_time:138845ms step_avg:60.84ms
step:2283/2315 train_time:138907ms step_avg:60.84ms
step:2284/2315 train_time:138969ms step_avg:60.84ms
step:2285/2315 train_time:139031ms step_avg:60.84ms
step:2286/2315 train_time:139092ms step_avg:60.85ms
step:2287/2315 train_time:139153ms step_avg:60.85ms
step:2288/2315 train_time:139215ms step_avg:60.85ms
step:2289/2315 train_time:139276ms step_avg:60.85ms
step:2290/2315 train_time:139337ms step_avg:60.85ms
step:2291/2315 train_time:139398ms step_avg:60.85ms
step:2292/2315 train_time:139459ms step_avg:60.85ms
step:2293/2315 train_time:139520ms step_avg:60.85ms
step:2294/2315 train_time:139581ms step_avg:60.85ms
step:2295/2315 train_time:139642ms step_avg:60.85ms
step:2296/2315 train_time:139704ms step_avg:60.85ms
step:2297/2315 train_time:139765ms step_avg:60.85ms
step:2298/2315 train_time:139827ms step_avg:60.85ms
step:2299/2315 train_time:139888ms step_avg:60.85ms
step:2300/2315 train_time:139950ms step_avg:60.85ms
step:2301/2315 train_time:140012ms step_avg:60.85ms
step:2302/2315 train_time:140075ms step_avg:60.85ms
step:2303/2315 train_time:140136ms step_avg:60.85ms
step:2304/2315 train_time:140197ms step_avg:60.85ms
step:2305/2315 train_time:140258ms step_avg:60.85ms
step:2306/2315 train_time:140320ms step_avg:60.85ms
step:2307/2315 train_time:140381ms step_avg:60.85ms
step:2308/2315 train_time:140443ms step_avg:60.85ms
step:2309/2315 train_time:140504ms step_avg:60.85ms
step:2310/2315 train_time:140565ms step_avg:60.85ms
step:2311/2315 train_time:140626ms step_avg:60.85ms
step:2312/2315 train_time:140688ms step_avg:60.85ms
step:2313/2315 train_time:140749ms step_avg:60.85ms
step:2314/2315 train_time:140810ms step_avg:60.85ms
step:2315/2315 train_time:140871ms step_avg:60.85ms
step:2315/2315 val_loss:3.2739 train_time:140934ms step_avg:60.88ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)


            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))



            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal
        self.group_to_param_group = {}

        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                self.group_to_param_group[param] = group

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        world_size = dist.get_world_size()
        grad = param.grad
        rank_size = grad.shape[0] // world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(), grad_slice)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']

            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
        # enable sync in the next training step for the adam optimizer
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = next(train_loader)

        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()


====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251031+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 31 15:56:30 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   37C    P0            125W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   31C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   34C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   35C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   34C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2315 train_time:68ms step_avg:67.92ms
step:2/2315 train_time:184ms step_avg:92.08ms
step:3/2315 train_time:203ms step_avg:67.81ms
step:4/2315 train_time:241ms step_avg:60.28ms
step:5/2315 train_time:299ms step_avg:59.76ms
step:6/2315 train_time:359ms step_avg:59.79ms
step:7/2315 train_time:418ms step_avg:59.76ms
step:8/2315 train_time:479ms step_avg:59.82ms
step:9/2315 train_time:537ms step_avg:59.70ms
step:10/2315 train_time:597ms step_avg:59.69ms
step:11/2315 train_time:656ms step_avg:59.62ms
step:12/2315 train_time:715ms step_avg:59.62ms
step:13/2315 train_time:775ms step_avg:59.58ms
step:14/2315 train_time:834ms step_avg:59.59ms
step:15/2315 train_time:893ms step_avg:59.53ms
step:16/2315 train_time:953ms step_avg:59.55ms
step:17/2315 train_time:1012ms step_avg:59.54ms
step:18/2315 train_time:1075ms step_avg:59.71ms
step:19/2315 train_time:1140ms step_avg:59.98ms
step:20/2315 train_time:1203ms step_avg:60.15ms
step:21/2315 train_time:1263ms step_avg:60.16ms
step:22/2315 train_time:1324ms step_avg:60.19ms
step:23/2315 train_time:1385ms step_avg:60.20ms
step:24/2315 train_time:1445ms step_avg:60.21ms
step:25/2315 train_time:1505ms step_avg:60.19ms
step:26/2315 train_time:1565ms step_avg:60.21ms
step:27/2315 train_time:1625ms step_avg:60.20ms
step:28/2315 train_time:1686ms step_avg:60.22ms
step:29/2315 train_time:1746ms step_avg:60.21ms
step:30/2315 train_time:1807ms step_avg:60.22ms
step:31/2315 train_time:1866ms step_avg:60.20ms
step:32/2315 train_time:1927ms step_avg:60.22ms
step:33/2315 train_time:1988ms step_avg:60.23ms
step:34/2315 train_time:2049ms step_avg:60.27ms
step:35/2315 train_time:2111ms step_avg:60.31ms
step:36/2315 train_time:2172ms step_avg:60.32ms
step:37/2315 train_time:2232ms step_avg:60.32ms
step:38/2315 train_time:2292ms step_avg:60.33ms
step:39/2315 train_time:2352ms step_avg:60.32ms
step:40/2315 train_time:2412ms step_avg:60.31ms
step:41/2315 train_time:2473ms step_avg:60.31ms
step:42/2315 train_time:2533ms step_avg:60.30ms
step:43/2315 train_time:2592ms step_avg:60.29ms
step:44/2315 train_time:2652ms step_avg:60.27ms
step:45/2315 train_time:2712ms step_avg:60.26ms
step:46/2315 train_time:2772ms step_avg:60.25ms
step:47/2315 train_time:2831ms step_avg:60.24ms
step:48/2315 train_time:2892ms step_avg:60.24ms
step:49/2315 train_time:2951ms step_avg:60.22ms
step:50/2315 train_time:3012ms step_avg:60.24ms
step:51/2315 train_time:3072ms step_avg:60.23ms
step:52/2315 train_time:3132ms step_avg:60.23ms
step:53/2315 train_time:3193ms step_avg:60.24ms
step:54/2315 train_time:3253ms step_avg:60.24ms
step:55/2315 train_time:3313ms step_avg:60.23ms
step:56/2315 train_time:3373ms step_avg:60.22ms
step:57/2315 train_time:3432ms step_avg:60.22ms
step:58/2315 train_time:3493ms step_avg:60.22ms
step:59/2315 train_time:3553ms step_avg:60.21ms
step:60/2315 train_time:3612ms step_avg:60.21ms
step:61/2315 train_time:3672ms step_avg:60.19ms
step:62/2315 train_time:3732ms step_avg:60.19ms
step:63/2315 train_time:3792ms step_avg:60.18ms
step:64/2315 train_time:3851ms step_avg:60.18ms
step:65/2315 train_time:3911ms step_avg:60.17ms
step:66/2315 train_time:3971ms step_avg:60.17ms
step:67/2315 train_time:4031ms step_avg:60.16ms
step:68/2315 train_time:4091ms step_avg:60.17ms
step:69/2315 train_time:4152ms step_avg:60.17ms
step:70/2315 train_time:4213ms step_avg:60.18ms
step:71/2315 train_time:4272ms step_avg:60.17ms
step:72/2315 train_time:4332ms step_avg:60.17ms
step:73/2315 train_time:4392ms step_avg:60.16ms
step:74/2315 train_time:4453ms step_avg:60.17ms
step:75/2315 train_time:4512ms step_avg:60.16ms
step:76/2315 train_time:4572ms step_avg:60.16ms
step:77/2315 train_time:4631ms step_avg:60.15ms
step:78/2315 train_time:4693ms step_avg:60.16ms
step:79/2315 train_time:4752ms step_avg:60.15ms
step:80/2315 train_time:4812ms step_avg:60.14ms
step:81/2315 train_time:4871ms step_avg:60.13ms
step:82/2315 train_time:4931ms step_avg:60.14ms
step:83/2315 train_time:4991ms step_avg:60.13ms
step:84/2315 train_time:5051ms step_avg:60.13ms
step:85/2315 train_time:5111ms step_avg:60.13ms
step:86/2315 train_time:5171ms step_avg:60.13ms
step:87/2315 train_time:5231ms step_avg:60.13ms
step:88/2315 train_time:5292ms step_avg:60.14ms
step:89/2315 train_time:5351ms step_avg:60.13ms
step:90/2315 train_time:5412ms step_avg:60.13ms
step:91/2315 train_time:5472ms step_avg:60.13ms
step:92/2315 train_time:5532ms step_avg:60.13ms
step:93/2315 train_time:5591ms step_avg:60.12ms
step:94/2315 train_time:5651ms step_avg:60.12ms
step:95/2315 train_time:5711ms step_avg:60.11ms
step:96/2315 train_time:5771ms step_avg:60.11ms
step:97/2315 train_time:5830ms step_avg:60.11ms
step:98/2315 train_time:5890ms step_avg:60.11ms
step:99/2315 train_time:5950ms step_avg:60.11ms
step:100/2315 train_time:6011ms step_avg:60.11ms
step:101/2315 train_time:6070ms step_avg:60.10ms
step:102/2315 train_time:6131ms step_avg:60.10ms
step:103/2315 train_time:6190ms step_avg:60.10ms
step:104/2315 train_time:6251ms step_avg:60.10ms
step:105/2315 train_time:6311ms step_avg:60.10ms
step:106/2315 train_time:6372ms step_avg:60.11ms
step:107/2315 train_time:6431ms step_avg:60.10ms
step:108/2315 train_time:6491ms step_avg:60.10ms
step:109/2315 train_time:6550ms step_avg:60.09ms
step:110/2315 train_time:6610ms step_avg:60.09ms
step:111/2315 train_time:6669ms step_avg:60.08ms
step:112/2315 train_time:6731ms step_avg:60.10ms
step:113/2315 train_time:6790ms step_avg:60.09ms
step:114/2315 train_time:6850ms step_avg:60.09ms
step:115/2315 train_time:6910ms step_avg:60.09ms
step:116/2315 train_time:6970ms step_avg:60.09ms
step:117/2315 train_time:7030ms step_avg:60.08ms
step:118/2315 train_time:7090ms step_avg:60.09ms
step:119/2315 train_time:7150ms step_avg:60.09ms
step:120/2315 train_time:7211ms step_avg:60.09ms
step:121/2315 train_time:7270ms step_avg:60.08ms
step:122/2315 train_time:7331ms step_avg:60.09ms
step:123/2315 train_time:7390ms step_avg:60.08ms
step:124/2315 train_time:7451ms step_avg:60.09ms
step:125/2315 train_time:7510ms step_avg:60.08ms
step:126/2315 train_time:7571ms step_avg:60.08ms
step:127/2315 train_time:7630ms step_avg:60.08ms
step:128/2315 train_time:7690ms step_avg:60.08ms
step:129/2315 train_time:7749ms step_avg:60.07ms
step:130/2315 train_time:7810ms step_avg:60.07ms
step:131/2315 train_time:7869ms step_avg:60.07ms
step:132/2315 train_time:7929ms step_avg:60.07ms
step:133/2315 train_time:7989ms step_avg:60.07ms
step:134/2315 train_time:8050ms step_avg:60.07ms
step:135/2315 train_time:8110ms step_avg:60.07ms
step:136/2315 train_time:8171ms step_avg:60.08ms
step:137/2315 train_time:8231ms step_avg:60.08ms
step:138/2315 train_time:8292ms step_avg:60.09ms
step:139/2315 train_time:8351ms step_avg:60.08ms
step:140/2315 train_time:8411ms step_avg:60.08ms
step:141/2315 train_time:8471ms step_avg:60.08ms
step:142/2315 train_time:8531ms step_avg:60.08ms
step:143/2315 train_time:8590ms step_avg:60.07ms
step:144/2315 train_time:8650ms step_avg:60.07ms
step:145/2315 train_time:8710ms step_avg:60.07ms
step:146/2315 train_time:8771ms step_avg:60.07ms
step:147/2315 train_time:8830ms step_avg:60.07ms
step:148/2315 train_time:8891ms step_avg:60.07ms
step:149/2315 train_time:8950ms step_avg:60.07ms
step:150/2315 train_time:9011ms step_avg:60.07ms
step:151/2315 train_time:9070ms step_avg:60.07ms
step:152/2315 train_time:9130ms step_avg:60.07ms
step:153/2315 train_time:9190ms step_avg:60.07ms
step:154/2315 train_time:9251ms step_avg:60.07ms
step:155/2315 train_time:9310ms step_avg:60.07ms
step:156/2315 train_time:9370ms step_avg:60.07ms
step:157/2315 train_time:9430ms step_avg:60.06ms
step:158/2315 train_time:9490ms step_avg:60.06ms
step:159/2315 train_time:9549ms step_avg:60.06ms
step:160/2315 train_time:9610ms step_avg:60.06ms
step:161/2315 train_time:9670ms step_avg:60.06ms
step:162/2315 train_time:9731ms step_avg:60.07ms
step:163/2315 train_time:9790ms step_avg:60.06ms
step:164/2315 train_time:9850ms step_avg:60.06ms
step:165/2315 train_time:9910ms step_avg:60.06ms
step:166/2315 train_time:9970ms step_avg:60.06ms
step:167/2315 train_time:10029ms step_avg:60.06ms
step:168/2315 train_time:10089ms step_avg:60.06ms
step:169/2315 train_time:10150ms step_avg:60.06ms
step:170/2315 train_time:10211ms step_avg:60.06ms
step:171/2315 train_time:10270ms step_avg:60.06ms
step:172/2315 train_time:10331ms step_avg:60.06ms
step:173/2315 train_time:10390ms step_avg:60.06ms
step:174/2315 train_time:10450ms step_avg:60.06ms
step:175/2315 train_time:10510ms step_avg:60.05ms
step:176/2315 train_time:10570ms step_avg:60.06ms
step:177/2315 train_time:10629ms step_avg:60.05ms
step:178/2315 train_time:10690ms step_avg:60.06ms
step:179/2315 train_time:10750ms step_avg:60.05ms
step:180/2315 train_time:10811ms step_avg:60.06ms
step:181/2315 train_time:10870ms step_avg:60.06ms
step:182/2315 train_time:10930ms step_avg:60.06ms
step:183/2315 train_time:10990ms step_avg:60.05ms
step:184/2315 train_time:11050ms step_avg:60.06ms
step:185/2315 train_time:11110ms step_avg:60.05ms
step:186/2315 train_time:11171ms step_avg:60.06ms
step:187/2315 train_time:11230ms step_avg:60.06ms
step:188/2315 train_time:11291ms step_avg:60.06ms
step:189/2315 train_time:11352ms step_avg:60.06ms
step:190/2315 train_time:11411ms step_avg:60.06ms
step:191/2315 train_time:11470ms step_avg:60.05ms
step:192/2315 train_time:11530ms step_avg:60.05ms
step:193/2315 train_time:11590ms step_avg:60.05ms
step:194/2315 train_time:11650ms step_avg:60.05ms
step:195/2315 train_time:11710ms step_avg:60.05ms
step:196/2315 train_time:11770ms step_avg:60.05ms
step:197/2315 train_time:11829ms step_avg:60.05ms
step:198/2315 train_time:11889ms step_avg:60.05ms
step:199/2315 train_time:11949ms step_avg:60.04ms
step:200/2315 train_time:12009ms step_avg:60.04ms
step:201/2315 train_time:12069ms step_avg:60.04ms
step:202/2315 train_time:12129ms step_avg:60.05ms
step:203/2315 train_time:12189ms step_avg:60.05ms
step:204/2315 train_time:12250ms step_avg:60.05ms
step:205/2315 train_time:12310ms step_avg:60.05ms
step:206/2315 train_time:12370ms step_avg:60.05ms
step:207/2315 train_time:12430ms step_avg:60.05ms
step:208/2315 train_time:12490ms step_avg:60.05ms
step:209/2315 train_time:12549ms step_avg:60.04ms
step:210/2315 train_time:12610ms step_avg:60.05ms
step:211/2315 train_time:12670ms step_avg:60.05ms
step:212/2315 train_time:12731ms step_avg:60.05ms
step:213/2315 train_time:12790ms step_avg:60.05ms
step:214/2315 train_time:12850ms step_avg:60.05ms
step:215/2315 train_time:12910ms step_avg:60.05ms
step:216/2315 train_time:12970ms step_avg:60.05ms
step:217/2315 train_time:13029ms step_avg:60.04ms
step:218/2315 train_time:13089ms step_avg:60.04ms
step:219/2315 train_time:13148ms step_avg:60.04ms
step:220/2315 train_time:13209ms step_avg:60.04ms
step:221/2315 train_time:13268ms step_avg:60.04ms
step:222/2315 train_time:13329ms step_avg:60.04ms
step:223/2315 train_time:13388ms step_avg:60.04ms
step:224/2315 train_time:13449ms step_avg:60.04ms
step:225/2315 train_time:13509ms step_avg:60.04ms
step:226/2315 train_time:13569ms step_avg:60.04ms
step:227/2315 train_time:13629ms step_avg:60.04ms
step:228/2315 train_time:13689ms step_avg:60.04ms
step:229/2315 train_time:13749ms step_avg:60.04ms
step:230/2315 train_time:13809ms step_avg:60.04ms
step:231/2315 train_time:13869ms step_avg:60.04ms
step:232/2315 train_time:13930ms step_avg:60.04ms
step:233/2315 train_time:13989ms step_avg:60.04ms
step:234/2315 train_time:14050ms step_avg:60.04ms
step:235/2315 train_time:14110ms step_avg:60.04ms
step:236/2315 train_time:14170ms step_avg:60.04ms
step:237/2315 train_time:14231ms step_avg:60.04ms
step:238/2315 train_time:14291ms step_avg:60.04ms
step:239/2315 train_time:14351ms step_avg:60.05ms
step:240/2315 train_time:14411ms step_avg:60.04ms
step:241/2315 train_time:14470ms step_avg:60.04ms
step:242/2315 train_time:14531ms step_avg:60.04ms
step:243/2315 train_time:14590ms step_avg:60.04ms
step:244/2315 train_time:14650ms step_avg:60.04ms
step:245/2315 train_time:14710ms step_avg:60.04ms
step:246/2315 train_time:14770ms step_avg:60.04ms
step:247/2315 train_time:14829ms step_avg:60.04ms
step:248/2315 train_time:14889ms step_avg:60.04ms
step:249/2315 train_time:14949ms step_avg:60.04ms
step:250/2315 train_time:15009ms step_avg:60.04ms
step:250/2315 val_loss:4.0737 train_time:15070ms step_avg:60.28ms
step:251/2315 train_time:15089ms step_avg:60.11ms
step:252/2315 train_time:15131ms step_avg:60.04ms
step:253/2315 train_time:15196ms step_avg:60.06ms
step:254/2315 train_time:15263ms step_avg:60.09ms
step:255/2315 train_time:15324ms step_avg:60.10ms
step:256/2315 train_time:15385ms step_avg:60.10ms
step:257/2315 train_time:15444ms step_avg:60.09ms
step:258/2315 train_time:15504ms step_avg:60.09ms
step:259/2315 train_time:15564ms step_avg:60.09ms
step:260/2315 train_time:15623ms step_avg:60.09ms
step:261/2315 train_time:15682ms step_avg:60.09ms
step:262/2315 train_time:15742ms step_avg:60.08ms
step:263/2315 train_time:15801ms step_avg:60.08ms
step:264/2315 train_time:15860ms step_avg:60.08ms
step:265/2315 train_time:15919ms step_avg:60.07ms
step:266/2315 train_time:15979ms step_avg:60.07ms
step:267/2315 train_time:16039ms step_avg:60.07ms
step:268/2315 train_time:16100ms step_avg:60.08ms
step:269/2315 train_time:16162ms step_avg:60.08ms
step:270/2315 train_time:16226ms step_avg:60.10ms
step:271/2315 train_time:16288ms step_avg:60.10ms
step:272/2315 train_time:16349ms step_avg:60.11ms
step:273/2315 train_time:16409ms step_avg:60.10ms
step:274/2315 train_time:16469ms step_avg:60.11ms
step:275/2315 train_time:16529ms step_avg:60.10ms
step:276/2315 train_time:16588ms step_avg:60.10ms
step:277/2315 train_time:16647ms step_avg:60.10ms
step:278/2315 train_time:16707ms step_avg:60.10ms
step:279/2315 train_time:16766ms step_avg:60.09ms
step:280/2315 train_time:16826ms step_avg:60.09ms
step:281/2315 train_time:16885ms step_avg:60.09ms
step:282/2315 train_time:16946ms step_avg:60.09ms
step:283/2315 train_time:17005ms step_avg:60.09ms
step:284/2315 train_time:17067ms step_avg:60.09ms
step:285/2315 train_time:17128ms step_avg:60.10ms
step:286/2315 train_time:17189ms step_avg:60.10ms
step:287/2315 train_time:17249ms step_avg:60.10ms
step:288/2315 train_time:17310ms step_avg:60.11ms
step:289/2315 train_time:17370ms step_avg:60.10ms
step:290/2315 train_time:17431ms step_avg:60.11ms
step:291/2315 train_time:17490ms step_avg:60.10ms
step:292/2315 train_time:17550ms step_avg:60.10ms
step:293/2315 train_time:17609ms step_avg:60.10ms
step:294/2315 train_time:17668ms step_avg:60.10ms
step:295/2315 train_time:17727ms step_avg:60.09ms
step:296/2315 train_time:17790ms step_avg:60.10ms
step:297/2315 train_time:17846ms step_avg:60.09ms
step:298/2315 train_time:17906ms step_avg:60.09ms
step:299/2315 train_time:17966ms step_avg:60.09ms
step:300/2315 train_time:18027ms step_avg:60.09ms
step:301/2315 train_time:18089ms step_avg:60.10ms
step:302/2315 train_time:18149ms step_avg:60.09ms
step:303/2315 train_time:18209ms step_avg:60.10ms
step:304/2315 train_time:18270ms step_avg:60.10ms
step:305/2315 train_time:18330ms step_avg:60.10ms
step:306/2315 train_time:18390ms step_avg:60.10ms
step:307/2315 train_time:18450ms step_avg:60.10ms
step:308/2315 train_time:18511ms step_avg:60.10ms
step:309/2315 train_time:18570ms step_avg:60.10ms
step:310/2315 train_time:18630ms step_avg:60.10ms
step:311/2315 train_time:18689ms step_avg:60.09ms
step:312/2315 train_time:18748ms step_avg:60.09ms
step:313/2315 train_time:18807ms step_avg:60.09ms
step:314/2315 train_time:18867ms step_avg:60.09ms
step:315/2315 train_time:18927ms step_avg:60.08ms
step:316/2315 train_time:18987ms step_avg:60.09ms
step:317/2315 train_time:19047ms step_avg:60.09ms
step:318/2315 train_time:19108ms step_avg:60.09ms
step:319/2315 train_time:19168ms step_avg:60.09ms
step:320/2315 train_time:19229ms step_avg:60.09ms
step:321/2315 train_time:19289ms step_avg:60.09ms
step:322/2315 train_time:19350ms step_avg:60.09ms
step:323/2315 train_time:19409ms step_avg:60.09ms
step:324/2315 train_time:19469ms step_avg:60.09ms
step:325/2315 train_time:19529ms step_avg:60.09ms
step:326/2315 train_time:19589ms step_avg:60.09ms
step:327/2315 train_time:19648ms step_avg:60.09ms
step:328/2315 train_time:19708ms step_avg:60.09ms
step:329/2315 train_time:19767ms step_avg:60.08ms
step:330/2315 train_time:19826ms step_avg:60.08ms
step:331/2315 train_time:19886ms step_avg:60.08ms
step:332/2315 train_time:19946ms step_avg:60.08ms
step:333/2315 train_time:20006ms step_avg:60.08ms
step:334/2315 train_time:20066ms step_avg:60.08ms
step:335/2315 train_time:20126ms step_avg:60.08ms
step:336/2315 train_time:20187ms step_avg:60.08ms
step:337/2315 train_time:20248ms step_avg:60.08ms
step:338/2315 train_time:20308ms step_avg:60.08ms
step:339/2315 train_time:20369ms step_avg:60.09ms
step:340/2315 train_time:20429ms step_avg:60.09ms
step:341/2315 train_time:20489ms step_avg:60.08ms
step:342/2315 train_time:20549ms step_avg:60.08ms
step:343/2315 train_time:20609ms step_avg:60.08ms
step:344/2315 train_time:20669ms step_avg:60.08ms
step:345/2315 train_time:20728ms step_avg:60.08ms
step:346/2315 train_time:20788ms step_avg:60.08ms
step:347/2315 train_time:20848ms step_avg:60.08ms
step:348/2315 train_time:20908ms step_avg:60.08ms
step:349/2315 train_time:20968ms step_avg:60.08ms
step:350/2315 train_time:21028ms step_avg:60.08ms
step:351/2315 train_time:21088ms step_avg:60.08ms
step:352/2315 train_time:21149ms step_avg:60.08ms
step:353/2315 train_time:21208ms step_avg:60.08ms
step:354/2315 train_time:21269ms step_avg:60.08ms
step:355/2315 train_time:21329ms step_avg:60.08ms
step:356/2315 train_time:21389ms step_avg:60.08ms
step:357/2315 train_time:21449ms step_avg:60.08ms
step:358/2315 train_time:21510ms step_avg:60.08ms
step:359/2315 train_time:21570ms step_avg:60.08ms
step:360/2315 train_time:21629ms step_avg:60.08ms
step:361/2315 train_time:21688ms step_avg:60.08ms
step:362/2315 train_time:21749ms step_avg:60.08ms
step:363/2315 train_time:21809ms step_avg:60.08ms
step:364/2315 train_time:21870ms step_avg:60.08ms
step:365/2315 train_time:21929ms step_avg:60.08ms
step:366/2315 train_time:21992ms step_avg:60.09ms
step:367/2315 train_time:22049ms step_avg:60.08ms
step:368/2315 train_time:22110ms step_avg:60.08ms
step:369/2315 train_time:22170ms step_avg:60.08ms
step:370/2315 train_time:22231ms step_avg:60.08ms
step:371/2315 train_time:22291ms step_avg:60.08ms
step:372/2315 train_time:22351ms step_avg:60.08ms
step:373/2315 train_time:22410ms step_avg:60.08ms
step:374/2315 train_time:22470ms step_avg:60.08ms
step:375/2315 train_time:22530ms step_avg:60.08ms
step:376/2315 train_time:22590ms step_avg:60.08ms
step:377/2315 train_time:22649ms step_avg:60.08ms
step:378/2315 train_time:22710ms step_avg:60.08ms
step:379/2315 train_time:22769ms step_avg:60.08ms
step:380/2315 train_time:22829ms step_avg:60.08ms
step:381/2315 train_time:22888ms step_avg:60.07ms
step:382/2315 train_time:22949ms step_avg:60.08ms
step:383/2315 train_time:23008ms step_avg:60.07ms
step:384/2315 train_time:23070ms step_avg:60.08ms
step:385/2315 train_time:23130ms step_avg:60.08ms
step:386/2315 train_time:23190ms step_avg:60.08ms
step:387/2315 train_time:23250ms step_avg:60.08ms
step:388/2315 train_time:23310ms step_avg:60.08ms
step:389/2315 train_time:23370ms step_avg:60.08ms
step:390/2315 train_time:23430ms step_avg:60.08ms
step:391/2315 train_time:23490ms step_avg:60.08ms
step:392/2315 train_time:23551ms step_avg:60.08ms
step:393/2315 train_time:23610ms step_avg:60.08ms
step:394/2315 train_time:23670ms step_avg:60.08ms
step:395/2315 train_time:23730ms step_avg:60.08ms
step:396/2315 train_time:23790ms step_avg:60.08ms
step:397/2315 train_time:23850ms step_avg:60.07ms
step:398/2315 train_time:23910ms step_avg:60.08ms
step:399/2315 train_time:23970ms step_avg:60.08ms
step:400/2315 train_time:24030ms step_avg:60.08ms
step:401/2315 train_time:24089ms step_avg:60.07ms
step:402/2315 train_time:24149ms step_avg:60.07ms
step:403/2315 train_time:24209ms step_avg:60.07ms
step:404/2315 train_time:24269ms step_avg:60.07ms
step:405/2315 train_time:24329ms step_avg:60.07ms
step:406/2315 train_time:24389ms step_avg:60.07ms
step:407/2315 train_time:24449ms step_avg:60.07ms
step:408/2315 train_time:24509ms step_avg:60.07ms
step:409/2315 train_time:24569ms step_avg:60.07ms
step:410/2315 train_time:24629ms step_avg:60.07ms
step:411/2315 train_time:24688ms step_avg:60.07ms
step:412/2315 train_time:24749ms step_avg:60.07ms
step:413/2315 train_time:24809ms step_avg:60.07ms
step:414/2315 train_time:24870ms step_avg:60.07ms
step:415/2315 train_time:24929ms step_avg:60.07ms
step:416/2315 train_time:24989ms step_avg:60.07ms
step:417/2315 train_time:25049ms step_avg:60.07ms
step:418/2315 train_time:25109ms step_avg:60.07ms
step:419/2315 train_time:25169ms step_avg:60.07ms
step:420/2315 train_time:25230ms step_avg:60.07ms
step:421/2315 train_time:25290ms step_avg:60.07ms
step:422/2315 train_time:25350ms step_avg:60.07ms
step:423/2315 train_time:25409ms step_avg:60.07ms
step:424/2315 train_time:25470ms step_avg:60.07ms
step:425/2315 train_time:25529ms step_avg:60.07ms
step:426/2315 train_time:25589ms step_avg:60.07ms
step:427/2315 train_time:25649ms step_avg:60.07ms
step:428/2315 train_time:25709ms step_avg:60.07ms
step:429/2315 train_time:25768ms step_avg:60.07ms
step:430/2315 train_time:25828ms step_avg:60.07ms
step:431/2315 train_time:25888ms step_avg:60.07ms
step:432/2315 train_time:25949ms step_avg:60.07ms
step:433/2315 train_time:26009ms step_avg:60.07ms
step:434/2315 train_time:26070ms step_avg:60.07ms
step:435/2315 train_time:26129ms step_avg:60.07ms
step:436/2315 train_time:26189ms step_avg:60.07ms
step:437/2315 train_time:26249ms step_avg:60.07ms
step:438/2315 train_time:26310ms step_avg:60.07ms
step:439/2315 train_time:26370ms step_avg:60.07ms
step:440/2315 train_time:26430ms step_avg:60.07ms
step:441/2315 train_time:26489ms step_avg:60.07ms
step:442/2315 train_time:26549ms step_avg:60.07ms
step:443/2315 train_time:26609ms step_avg:60.06ms
step:444/2315 train_time:26670ms step_avg:60.07ms
step:445/2315 train_time:26729ms step_avg:60.07ms
step:446/2315 train_time:26789ms step_avg:60.07ms
step:447/2315 train_time:26849ms step_avg:60.06ms
step:448/2315 train_time:26909ms step_avg:60.07ms
step:449/2315 train_time:26969ms step_avg:60.06ms
step:450/2315 train_time:27029ms step_avg:60.07ms
step:451/2315 train_time:27089ms step_avg:60.06ms
step:452/2315 train_time:27148ms step_avg:60.06ms
step:453/2315 train_time:27208ms step_avg:60.06ms
step:454/2315 train_time:27268ms step_avg:60.06ms
step:455/2315 train_time:27329ms step_avg:60.06ms
step:456/2315 train_time:27389ms step_avg:60.06ms
step:457/2315 train_time:27449ms step_avg:60.06ms
step:458/2315 train_time:27510ms step_avg:60.06ms
step:459/2315 train_time:27570ms step_avg:60.06ms
step:460/2315 train_time:27629ms step_avg:60.06ms
step:461/2315 train_time:27689ms step_avg:60.06ms
step:462/2315 train_time:27749ms step_avg:60.06ms
step:463/2315 train_time:27810ms step_avg:60.06ms
step:464/2315 train_time:27870ms step_avg:60.07ms
step:465/2315 train_time:27930ms step_avg:60.06ms
step:466/2315 train_time:27989ms step_avg:60.06ms
step:467/2315 train_time:28049ms step_avg:60.06ms
step:468/2315 train_time:28110ms step_avg:60.06ms
step:469/2315 train_time:28170ms step_avg:60.06ms
step:470/2315 train_time:28230ms step_avg:60.06ms
step:471/2315 train_time:28289ms step_avg:60.06ms
step:472/2315 train_time:28350ms step_avg:60.06ms
step:473/2315 train_time:28409ms step_avg:60.06ms
step:474/2315 train_time:28469ms step_avg:60.06ms
step:475/2315 train_time:28529ms step_avg:60.06ms
step:476/2315 train_time:28589ms step_avg:60.06ms
step:477/2315 train_time:28649ms step_avg:60.06ms
step:478/2315 train_time:28710ms step_avg:60.06ms
step:479/2315 train_time:28770ms step_avg:60.06ms
step:480/2315 train_time:28830ms step_avg:60.06ms
step:481/2315 train_time:28889ms step_avg:60.06ms
step:482/2315 train_time:28949ms step_avg:60.06ms
step:483/2315 train_time:29009ms step_avg:60.06ms
step:484/2315 train_time:29069ms step_avg:60.06ms
step:485/2315 train_time:29129ms step_avg:60.06ms
step:486/2315 train_time:29189ms step_avg:60.06ms
step:487/2315 train_time:29249ms step_avg:60.06ms
step:488/2315 train_time:29310ms step_avg:60.06ms
step:489/2315 train_time:29369ms step_avg:60.06ms
step:490/2315 train_time:29429ms step_avg:60.06ms
step:491/2315 train_time:29488ms step_avg:60.06ms
step:492/2315 train_time:29548ms step_avg:60.06ms
step:493/2315 train_time:29608ms step_avg:60.06ms
step:494/2315 train_time:29669ms step_avg:60.06ms
step:495/2315 train_time:29729ms step_avg:60.06ms
step:496/2315 train_time:29789ms step_avg:60.06ms
step:497/2315 train_time:29848ms step_avg:60.06ms
step:498/2315 train_time:29908ms step_avg:60.06ms
step:499/2315 train_time:29969ms step_avg:60.06ms
step:500/2315 train_time:30028ms step_avg:60.06ms
step:500/2315 val_loss:3.8122 train_time:30090ms step_avg:60.18ms
step:501/2315 train_time:30108ms step_avg:60.10ms
step:502/2315 train_time:30151ms step_avg:60.06ms
step:503/2315 train_time:30213ms step_avg:60.07ms
step:504/2315 train_time:30277ms step_avg:60.07ms
step:505/2315 train_time:30337ms step_avg:60.07ms
step:506/2315 train_time:30397ms step_avg:60.07ms
step:507/2315 train_time:30456ms step_avg:60.07ms
step:508/2315 train_time:30517ms step_avg:60.07ms
step:509/2315 train_time:30577ms step_avg:60.07ms
step:510/2315 train_time:30636ms step_avg:60.07ms
step:511/2315 train_time:30695ms step_avg:60.07ms
step:512/2315 train_time:30755ms step_avg:60.07ms
step:513/2315 train_time:30814ms step_avg:60.07ms
step:514/2315 train_time:30874ms step_avg:60.07ms
step:515/2315 train_time:30933ms step_avg:60.06ms
step:516/2315 train_time:30994ms step_avg:60.07ms
step:517/2315 train_time:31054ms step_avg:60.07ms
step:518/2315 train_time:31116ms step_avg:60.07ms
step:519/2315 train_time:31177ms step_avg:60.07ms
step:520/2315 train_time:31239ms step_avg:60.07ms
step:521/2315 train_time:31301ms step_avg:60.08ms
step:522/2315 train_time:31361ms step_avg:60.08ms
step:523/2315 train_time:31420ms step_avg:60.08ms
step:524/2315 train_time:31480ms step_avg:60.08ms
step:525/2315 train_time:31540ms step_avg:60.08ms
step:526/2315 train_time:31601ms step_avg:60.08ms
step:527/2315 train_time:31660ms step_avg:60.08ms
step:528/2315 train_time:31719ms step_avg:60.07ms
step:529/2315 train_time:31779ms step_avg:60.07ms
step:530/2315 train_time:31839ms step_avg:60.07ms
step:531/2315 train_time:31898ms step_avg:60.07ms
step:532/2315 train_time:31958ms step_avg:60.07ms
step:533/2315 train_time:32019ms step_avg:60.07ms
step:534/2315 train_time:32081ms step_avg:60.08ms
step:535/2315 train_time:32141ms step_avg:60.08ms
step:536/2315 train_time:32202ms step_avg:60.08ms
step:537/2315 train_time:32263ms step_avg:60.08ms
step:538/2315 train_time:32324ms step_avg:60.08ms
step:539/2315 train_time:32384ms step_avg:60.08ms
step:540/2315 train_time:32444ms step_avg:60.08ms
step:541/2315 train_time:32503ms step_avg:60.08ms
step:542/2315 train_time:32563ms step_avg:60.08ms
step:543/2315 train_time:32622ms step_avg:60.08ms
step:544/2315 train_time:32682ms step_avg:60.08ms
step:545/2315 train_time:32741ms step_avg:60.08ms
step:546/2315 train_time:32801ms step_avg:60.07ms
step:547/2315 train_time:32860ms step_avg:60.07ms
step:548/2315 train_time:32920ms step_avg:60.07ms
step:549/2315 train_time:32980ms step_avg:60.07ms
step:550/2315 train_time:33041ms step_avg:60.07ms
step:551/2315 train_time:33101ms step_avg:60.08ms
step:552/2315 train_time:33164ms step_avg:60.08ms
step:553/2315 train_time:33223ms step_avg:60.08ms
step:554/2315 train_time:33284ms step_avg:60.08ms
step:555/2315 train_time:33344ms step_avg:60.08ms
step:556/2315 train_time:33404ms step_avg:60.08ms
step:557/2315 train_time:33464ms step_avg:60.08ms
step:558/2315 train_time:33523ms step_avg:60.08ms
step:559/2315 train_time:33582ms step_avg:60.08ms
step:560/2315 train_time:33642ms step_avg:60.08ms
step:561/2315 train_time:33702ms step_avg:60.07ms
step:562/2315 train_time:33762ms step_avg:60.07ms
step:563/2315 train_time:33821ms step_avg:60.07ms
step:564/2315 train_time:33881ms step_avg:60.07ms
step:565/2315 train_time:33941ms step_avg:60.07ms
step:566/2315 train_time:34001ms step_avg:60.07ms
step:567/2315 train_time:34061ms step_avg:60.07ms
step:568/2315 train_time:34122ms step_avg:60.07ms
step:569/2315 train_time:34182ms step_avg:60.07ms
step:570/2315 train_time:34243ms step_avg:60.08ms
step:571/2315 train_time:34303ms step_avg:60.08ms
step:572/2315 train_time:34364ms step_avg:60.08ms
step:573/2315 train_time:34424ms step_avg:60.08ms
step:574/2315 train_time:34485ms step_avg:60.08ms
step:575/2315 train_time:34544ms step_avg:60.08ms
step:576/2315 train_time:34603ms step_avg:60.07ms
step:577/2315 train_time:34662ms step_avg:60.07ms
step:578/2315 train_time:34722ms step_avg:60.07ms
step:579/2315 train_time:34782ms step_avg:60.07ms
step:580/2315 train_time:34842ms step_avg:60.07ms
step:581/2315 train_time:34902ms step_avg:60.07ms
step:582/2315 train_time:34963ms step_avg:60.07ms
step:583/2315 train_time:35023ms step_avg:60.07ms
step:584/2315 train_time:35083ms step_avg:60.07ms
step:585/2315 train_time:35143ms step_avg:60.07ms
step:586/2315 train_time:35204ms step_avg:60.07ms
step:587/2315 train_time:35264ms step_avg:60.07ms
step:588/2315 train_time:35324ms step_avg:60.07ms
step:589/2315 train_time:35384ms step_avg:60.07ms
step:590/2315 train_time:35444ms step_avg:60.07ms
step:591/2315 train_time:35504ms step_avg:60.07ms
step:592/2315 train_time:35564ms step_avg:60.07ms
step:593/2315 train_time:35623ms step_avg:60.07ms
step:594/2315 train_time:35682ms step_avg:60.07ms
step:595/2315 train_time:35742ms step_avg:60.07ms
step:596/2315 train_time:35802ms step_avg:60.07ms
step:597/2315 train_time:35862ms step_avg:60.07ms
step:598/2315 train_time:35922ms step_avg:60.07ms
step:599/2315 train_time:35982ms step_avg:60.07ms
step:600/2315 train_time:36042ms step_avg:60.07ms
step:601/2315 train_time:36102ms step_avg:60.07ms
step:602/2315 train_time:36163ms step_avg:60.07ms
step:603/2315 train_time:36223ms step_avg:60.07ms
step:604/2315 train_time:36283ms step_avg:60.07ms
step:605/2315 train_time:36343ms step_avg:60.07ms
step:606/2315 train_time:36404ms step_avg:60.07ms
step:607/2315 train_time:36463ms step_avg:60.07ms
step:608/2315 train_time:36523ms step_avg:60.07ms
step:609/2315 train_time:36583ms step_avg:60.07ms
step:610/2315 train_time:36643ms step_avg:60.07ms
step:611/2315 train_time:36702ms step_avg:60.07ms
step:612/2315 train_time:36762ms step_avg:60.07ms
step:613/2315 train_time:36823ms step_avg:60.07ms
step:614/2315 train_time:36883ms step_avg:60.07ms
step:615/2315 train_time:36942ms step_avg:60.07ms
step:616/2315 train_time:37003ms step_avg:60.07ms
step:617/2315 train_time:37062ms step_avg:60.07ms
step:618/2315 train_time:37123ms step_avg:60.07ms
step:619/2315 train_time:37183ms step_avg:60.07ms
step:620/2315 train_time:37243ms step_avg:60.07ms
step:621/2315 train_time:37304ms step_avg:60.07ms
step:622/2315 train_time:37364ms step_avg:60.07ms
step:623/2315 train_time:37424ms step_avg:60.07ms
step:624/2315 train_time:37484ms step_avg:60.07ms
step:625/2315 train_time:37543ms step_avg:60.07ms
step:626/2315 train_time:37604ms step_avg:60.07ms
step:627/2315 train_time:37663ms step_avg:60.07ms
step:628/2315 train_time:37723ms step_avg:60.07ms
step:629/2315 train_time:37783ms step_avg:60.07ms
step:630/2315 train_time:37843ms step_avg:60.07ms
step:631/2315 train_time:37902ms step_avg:60.07ms
step:632/2315 train_time:37963ms step_avg:60.07ms
step:633/2315 train_time:38022ms step_avg:60.07ms
step:634/2315 train_time:38083ms step_avg:60.07ms
step:635/2315 train_time:38142ms step_avg:60.07ms
step:636/2315 train_time:38203ms step_avg:60.07ms
step:637/2315 train_time:38263ms step_avg:60.07ms
step:638/2315 train_time:38324ms step_avg:60.07ms
step:639/2315 train_time:38383ms step_avg:60.07ms
step:640/2315 train_time:38443ms step_avg:60.07ms
step:641/2315 train_time:38503ms step_avg:60.07ms
step:642/2315 train_time:38563ms step_avg:60.07ms
step:643/2315 train_time:38622ms step_avg:60.07ms
step:644/2315 train_time:38682ms step_avg:60.07ms
step:645/2315 train_time:38741ms step_avg:60.06ms
step:646/2315 train_time:38802ms step_avg:60.06ms
step:647/2315 train_time:38862ms step_avg:60.06ms
step:648/2315 train_time:38922ms step_avg:60.06ms
step:649/2315 train_time:38982ms step_avg:60.06ms
step:650/2315 train_time:39042ms step_avg:60.06ms
step:651/2315 train_time:39102ms step_avg:60.06ms
step:652/2315 train_time:39162ms step_avg:60.06ms
step:653/2315 train_time:39222ms step_avg:60.06ms
step:654/2315 train_time:39282ms step_avg:60.06ms
step:655/2315 train_time:39342ms step_avg:60.06ms
step:656/2315 train_time:39402ms step_avg:60.06ms
step:657/2315 train_time:39462ms step_avg:60.06ms
step:658/2315 train_time:39522ms step_avg:60.06ms
step:659/2315 train_time:39581ms step_avg:60.06ms
step:660/2315 train_time:39642ms step_avg:60.06ms
step:661/2315 train_time:39701ms step_avg:60.06ms
step:662/2315 train_time:39762ms step_avg:60.06ms
step:663/2315 train_time:39822ms step_avg:60.06ms
step:664/2315 train_time:39882ms step_avg:60.06ms
step:665/2315 train_time:39941ms step_avg:60.06ms
step:666/2315 train_time:40001ms step_avg:60.06ms
step:667/2315 train_time:40061ms step_avg:60.06ms
step:668/2315 train_time:40122ms step_avg:60.06ms
step:669/2315 train_time:40182ms step_avg:60.06ms
step:670/2315 train_time:40242ms step_avg:60.06ms
step:671/2315 train_time:40302ms step_avg:60.06ms
step:672/2315 train_time:40363ms step_avg:60.06ms
step:673/2315 train_time:40422ms step_avg:60.06ms
step:674/2315 train_time:40482ms step_avg:60.06ms
step:675/2315 train_time:40542ms step_avg:60.06ms
step:676/2315 train_time:40602ms step_avg:60.06ms
step:677/2315 train_time:40662ms step_avg:60.06ms
step:678/2315 train_time:40722ms step_avg:60.06ms
step:679/2315 train_time:40782ms step_avg:60.06ms
step:680/2315 train_time:40842ms step_avg:60.06ms
step:681/2315 train_time:40902ms step_avg:60.06ms
step:682/2315 train_time:40962ms step_avg:60.06ms
step:683/2315 train_time:41022ms step_avg:60.06ms
step:684/2315 train_time:41083ms step_avg:60.06ms
step:685/2315 train_time:41142ms step_avg:60.06ms
step:686/2315 train_time:41203ms step_avg:60.06ms
step:687/2315 train_time:41264ms step_avg:60.06ms
step:688/2315 train_time:41324ms step_avg:60.06ms
step:689/2315 train_time:41383ms step_avg:60.06ms
step:690/2315 train_time:41444ms step_avg:60.06ms
step:691/2315 train_time:41503ms step_avg:60.06ms
step:692/2315 train_time:41564ms step_avg:60.06ms
step:693/2315 train_time:41624ms step_avg:60.06ms
step:694/2315 train_time:41684ms step_avg:60.06ms
step:695/2315 train_time:41743ms step_avg:60.06ms
step:696/2315 train_time:41803ms step_avg:60.06ms
step:697/2315 train_time:41863ms step_avg:60.06ms
step:698/2315 train_time:41923ms step_avg:60.06ms
step:699/2315 train_time:41983ms step_avg:60.06ms
step:700/2315 train_time:42043ms step_avg:60.06ms
step:701/2315 train_time:42103ms step_avg:60.06ms
step:702/2315 train_time:42164ms step_avg:60.06ms
step:703/2315 train_time:42223ms step_avg:60.06ms
step:704/2315 train_time:42283ms step_avg:60.06ms
step:705/2315 train_time:42344ms step_avg:60.06ms
step:706/2315 train_time:42404ms step_avg:60.06ms
step:707/2315 train_time:42463ms step_avg:60.06ms
step:708/2315 train_time:42524ms step_avg:60.06ms
step:709/2315 train_time:42583ms step_avg:60.06ms
step:710/2315 train_time:42643ms step_avg:60.06ms
step:711/2315 train_time:42703ms step_avg:60.06ms
step:712/2315 train_time:42763ms step_avg:60.06ms
step:713/2315 train_time:42823ms step_avg:60.06ms
step:714/2315 train_time:42883ms step_avg:60.06ms
step:715/2315 train_time:42943ms step_avg:60.06ms
step:716/2315 train_time:43003ms step_avg:60.06ms
step:717/2315 train_time:43062ms step_avg:60.06ms
step:718/2315 train_time:43123ms step_avg:60.06ms
step:719/2315 train_time:43183ms step_avg:60.06ms
step:720/2315 train_time:43243ms step_avg:60.06ms
step:721/2315 train_time:43303ms step_avg:60.06ms
step:722/2315 train_time:43364ms step_avg:60.06ms
step:723/2315 train_time:43423ms step_avg:60.06ms
step:724/2315 train_time:43484ms step_avg:60.06ms
step:725/2315 train_time:43544ms step_avg:60.06ms
step:726/2315 train_time:43604ms step_avg:60.06ms
step:727/2315 train_time:43663ms step_avg:60.06ms
step:728/2315 train_time:43724ms step_avg:60.06ms
step:729/2315 train_time:43783ms step_avg:60.06ms
step:730/2315 train_time:43843ms step_avg:60.06ms
step:731/2315 train_time:43903ms step_avg:60.06ms
step:732/2315 train_time:43963ms step_avg:60.06ms
step:733/2315 train_time:44023ms step_avg:60.06ms
step:734/2315 train_time:44083ms step_avg:60.06ms
step:735/2315 train_time:44143ms step_avg:60.06ms
step:736/2315 train_time:44203ms step_avg:60.06ms
step:737/2315 train_time:44263ms step_avg:60.06ms
step:738/2315 train_time:44323ms step_avg:60.06ms
step:739/2315 train_time:44383ms step_avg:60.06ms
step:740/2315 train_time:44443ms step_avg:60.06ms
step:741/2315 train_time:44503ms step_avg:60.06ms
step:742/2315 train_time:44563ms step_avg:60.06ms
step:743/2315 train_time:44622ms step_avg:60.06ms
step:744/2315 train_time:44683ms step_avg:60.06ms
step:745/2315 train_time:44743ms step_avg:60.06ms
step:746/2315 train_time:44803ms step_avg:60.06ms
step:747/2315 train_time:44863ms step_avg:60.06ms
step:748/2315 train_time:44924ms step_avg:60.06ms
step:749/2315 train_time:44983ms step_avg:60.06ms
step:750/2315 train_time:45043ms step_avg:60.06ms
step:750/2315 val_loss:3.6825 train_time:45105ms step_avg:60.14ms
step:751/2315 train_time:45123ms step_avg:60.08ms
step:752/2315 train_time:45165ms step_avg:60.06ms
step:753/2315 train_time:45227ms step_avg:60.06ms
step:754/2315 train_time:45290ms step_avg:60.07ms
step:755/2315 train_time:45351ms step_avg:60.07ms
step:756/2315 train_time:45412ms step_avg:60.07ms
step:757/2315 train_time:45471ms step_avg:60.07ms
step:758/2315 train_time:45530ms step_avg:60.07ms
step:759/2315 train_time:45589ms step_avg:60.07ms
step:760/2315 train_time:45649ms step_avg:60.06ms
step:761/2315 train_time:45710ms step_avg:60.07ms
step:762/2315 train_time:45771ms step_avg:60.07ms
step:763/2315 train_time:45831ms step_avg:60.07ms
step:764/2315 train_time:45893ms step_avg:60.07ms
step:765/2315 train_time:45953ms step_avg:60.07ms
step:766/2315 train_time:46014ms step_avg:60.07ms
step:767/2315 train_time:46076ms step_avg:60.07ms
step:768/2315 train_time:46138ms step_avg:60.08ms
step:769/2315 train_time:46199ms step_avg:60.08ms
step:770/2315 train_time:46260ms step_avg:60.08ms
step:771/2315 train_time:46321ms step_avg:60.08ms
step:772/2315 train_time:46382ms step_avg:60.08ms
step:773/2315 train_time:46443ms step_avg:60.08ms
step:774/2315 train_time:46504ms step_avg:60.08ms
step:775/2315 train_time:46564ms step_avg:60.08ms
step:776/2315 train_time:46626ms step_avg:60.08ms
step:777/2315 train_time:46686ms step_avg:60.09ms
step:778/2315 train_time:46747ms step_avg:60.09ms
step:779/2315 train_time:46807ms step_avg:60.09ms
step:780/2315 train_time:46868ms step_avg:60.09ms
step:781/2315 train_time:46929ms step_avg:60.09ms
step:782/2315 train_time:46989ms step_avg:60.09ms
step:783/2315 train_time:47051ms step_avg:60.09ms
step:784/2315 train_time:47113ms step_avg:60.09ms
step:785/2315 train_time:47175ms step_avg:60.09ms
step:786/2315 train_time:47236ms step_avg:60.10ms
step:787/2315 train_time:47297ms step_avg:60.10ms
step:788/2315 train_time:47358ms step_avg:60.10ms
step:789/2315 train_time:47417ms step_avg:60.10ms
step:790/2315 train_time:47478ms step_avg:60.10ms
step:791/2315 train_time:47538ms step_avg:60.10ms
step:792/2315 train_time:47598ms step_avg:60.10ms
step:793/2315 train_time:47658ms step_avg:60.10ms
step:794/2315 train_time:47718ms step_avg:60.10ms
step:795/2315 train_time:47779ms step_avg:60.10ms
step:796/2315 train_time:47840ms step_avg:60.10ms
step:797/2315 train_time:47901ms step_avg:60.10ms
step:798/2315 train_time:47962ms step_avg:60.10ms
step:799/2315 train_time:48024ms step_avg:60.11ms
step:800/2315 train_time:48085ms step_avg:60.11ms
step:801/2315 train_time:48146ms step_avg:60.11ms
step:802/2315 train_time:48211ms step_avg:60.11ms
step:803/2315 train_time:48270ms step_avg:60.11ms
step:804/2315 train_time:48332ms step_avg:60.11ms
step:805/2315 train_time:48393ms step_avg:60.12ms
step:806/2315 train_time:48454ms step_avg:60.12ms
step:807/2315 train_time:48515ms step_avg:60.12ms
step:808/2315 train_time:48576ms step_avg:60.12ms
step:809/2315 train_time:48636ms step_avg:60.12ms
step:810/2315 train_time:48696ms step_avg:60.12ms
step:811/2315 train_time:48756ms step_avg:60.12ms
step:812/2315 train_time:48816ms step_avg:60.12ms
step:813/2315 train_time:48876ms step_avg:60.12ms
step:814/2315 train_time:48937ms step_avg:60.12ms
step:815/2315 train_time:48997ms step_avg:60.12ms
step:816/2315 train_time:49057ms step_avg:60.12ms
step:817/2315 train_time:49118ms step_avg:60.12ms
step:818/2315 train_time:49179ms step_avg:60.12ms
step:819/2315 train_time:49240ms step_avg:60.12ms
step:820/2315 train_time:49301ms step_avg:60.12ms
step:821/2315 train_time:49363ms step_avg:60.13ms
step:822/2315 train_time:49425ms step_avg:60.13ms
step:823/2315 train_time:49486ms step_avg:60.13ms
step:824/2315 train_time:49547ms step_avg:60.13ms
step:825/2315 train_time:49608ms step_avg:60.13ms
step:826/2315 train_time:49669ms step_avg:60.13ms
step:827/2315 train_time:49730ms step_avg:60.13ms
step:828/2315 train_time:49792ms step_avg:60.13ms
step:829/2315 train_time:49853ms step_avg:60.14ms
step:830/2315 train_time:49914ms step_avg:60.14ms
step:831/2315 train_time:49975ms step_avg:60.14ms
step:832/2315 train_time:50036ms step_avg:60.14ms
step:833/2315 train_time:50096ms step_avg:60.14ms
step:834/2315 train_time:50157ms step_avg:60.14ms
step:835/2315 train_time:50217ms step_avg:60.14ms
step:836/2315 train_time:50278ms step_avg:60.14ms
step:837/2315 train_time:50338ms step_avg:60.14ms
step:838/2315 train_time:50399ms step_avg:60.14ms
step:839/2315 train_time:50460ms step_avg:60.14ms
step:840/2315 train_time:50521ms step_avg:60.14ms
step:841/2315 train_time:50581ms step_avg:60.14ms
step:842/2315 train_time:50642ms step_avg:60.15ms
step:843/2315 train_time:50703ms step_avg:60.15ms
step:844/2315 train_time:50764ms step_avg:60.15ms
step:845/2315 train_time:50826ms step_avg:60.15ms
step:846/2315 train_time:50888ms step_avg:60.15ms
step:847/2315 train_time:50949ms step_avg:60.15ms
step:848/2315 train_time:51010ms step_avg:60.15ms
step:849/2315 train_time:51070ms step_avg:60.15ms
step:850/2315 train_time:51132ms step_avg:60.15ms
step:851/2315 train_time:51193ms step_avg:60.16ms
step:852/2315 train_time:51254ms step_avg:60.16ms
step:853/2315 train_time:51315ms step_avg:60.16ms
step:854/2315 train_time:51376ms step_avg:60.16ms
step:855/2315 train_time:51436ms step_avg:60.16ms
step:856/2315 train_time:51497ms step_avg:60.16ms
step:857/2315 train_time:51557ms step_avg:60.16ms
step:858/2315 train_time:51617ms step_avg:60.16ms
step:859/2315 train_time:51677ms step_avg:60.16ms
step:860/2315 train_time:51738ms step_avg:60.16ms
step:861/2315 train_time:51798ms step_avg:60.16ms
step:862/2315 train_time:51859ms step_avg:60.16ms
step:863/2315 train_time:51919ms step_avg:60.16ms
step:864/2315 train_time:51980ms step_avg:60.16ms
step:865/2315 train_time:52041ms step_avg:60.16ms
step:866/2315 train_time:52102ms step_avg:60.16ms
step:867/2315 train_time:52163ms step_avg:60.17ms
step:868/2315 train_time:52225ms step_avg:60.17ms
step:869/2315 train_time:52286ms step_avg:60.17ms
step:870/2315 train_time:52348ms step_avg:60.17ms
step:871/2315 train_time:52408ms step_avg:60.17ms
step:872/2315 train_time:52470ms step_avg:60.17ms
step:873/2315 train_time:52530ms step_avg:60.17ms
step:874/2315 train_time:52592ms step_avg:60.17ms
step:875/2315 train_time:52653ms step_avg:60.17ms
step:876/2315 train_time:52714ms step_avg:60.18ms
step:877/2315 train_time:52775ms step_avg:60.18ms
step:878/2315 train_time:52835ms step_avg:60.18ms
step:879/2315 train_time:52896ms step_avg:60.18ms
step:880/2315 train_time:52956ms step_avg:60.18ms
step:881/2315 train_time:53017ms step_avg:60.18ms
step:882/2315 train_time:53077ms step_avg:60.18ms
step:883/2315 train_time:53137ms step_avg:60.18ms
step:884/2315 train_time:53198ms step_avg:60.18ms
step:885/2315 train_time:53258ms step_avg:60.18ms
step:886/2315 train_time:53319ms step_avg:60.18ms
step:887/2315 train_time:53379ms step_avg:60.18ms
step:888/2315 train_time:53439ms step_avg:60.18ms
step:889/2315 train_time:53500ms step_avg:60.18ms
step:890/2315 train_time:53562ms step_avg:60.18ms
step:891/2315 train_time:53623ms step_avg:60.18ms
step:892/2315 train_time:53684ms step_avg:60.18ms
step:893/2315 train_time:53745ms step_avg:60.18ms
step:894/2315 train_time:53806ms step_avg:60.19ms
step:895/2315 train_time:53866ms step_avg:60.19ms
step:896/2315 train_time:53928ms step_avg:60.19ms
step:897/2315 train_time:53989ms step_avg:60.19ms
step:898/2315 train_time:54050ms step_avg:60.19ms
step:899/2315 train_time:54111ms step_avg:60.19ms
step:900/2315 train_time:54172ms step_avg:60.19ms
step:901/2315 train_time:54232ms step_avg:60.19ms
step:902/2315 train_time:54293ms step_avg:60.19ms
step:903/2315 train_time:54354ms step_avg:60.19ms
step:904/2315 train_time:54415ms step_avg:60.19ms
step:905/2315 train_time:54476ms step_avg:60.19ms
step:906/2315 train_time:54536ms step_avg:60.19ms
step:907/2315 train_time:54597ms step_avg:60.19ms
step:908/2315 train_time:54658ms step_avg:60.20ms
step:909/2315 train_time:54718ms step_avg:60.20ms
step:910/2315 train_time:54778ms step_avg:60.20ms
step:911/2315 train_time:54839ms step_avg:60.20ms
step:912/2315 train_time:54900ms step_avg:60.20ms
step:913/2315 train_time:54960ms step_avg:60.20ms
step:914/2315 train_time:55022ms step_avg:60.20ms
step:915/2315 train_time:55082ms step_avg:60.20ms
step:916/2315 train_time:55143ms step_avg:60.20ms
step:917/2315 train_time:55205ms step_avg:60.20ms
step:918/2315 train_time:55266ms step_avg:60.20ms
step:919/2315 train_time:55327ms step_avg:60.20ms
step:920/2315 train_time:55389ms step_avg:60.21ms
step:921/2315 train_time:55449ms step_avg:60.21ms
step:922/2315 train_time:55511ms step_avg:60.21ms
step:923/2315 train_time:55572ms step_avg:60.21ms
step:924/2315 train_time:55633ms step_avg:60.21ms
step:925/2315 train_time:55693ms step_avg:60.21ms
step:926/2315 train_time:55754ms step_avg:60.21ms
step:927/2315 train_time:55815ms step_avg:60.21ms
step:928/2315 train_time:55876ms step_avg:60.21ms
step:929/2315 train_time:55936ms step_avg:60.21ms
step:930/2315 train_time:55997ms step_avg:60.21ms
step:931/2315 train_time:56057ms step_avg:60.21ms
step:932/2315 train_time:56118ms step_avg:60.21ms
step:933/2315 train_time:56178ms step_avg:60.21ms
step:934/2315 train_time:56239ms step_avg:60.21ms
step:935/2315 train_time:56299ms step_avg:60.21ms
step:936/2315 train_time:56360ms step_avg:60.21ms
step:937/2315 train_time:56421ms step_avg:60.21ms
step:938/2315 train_time:56482ms step_avg:60.22ms
step:939/2315 train_time:56544ms step_avg:60.22ms
step:940/2315 train_time:56605ms step_avg:60.22ms
step:941/2315 train_time:56666ms step_avg:60.22ms
step:942/2315 train_time:56727ms step_avg:60.22ms
step:943/2315 train_time:56789ms step_avg:60.22ms
step:944/2315 train_time:56850ms step_avg:60.22ms
step:945/2315 train_time:56911ms step_avg:60.22ms
step:946/2315 train_time:56972ms step_avg:60.22ms
step:947/2315 train_time:57033ms step_avg:60.22ms
step:948/2315 train_time:57094ms step_avg:60.23ms
step:949/2315 train_time:57154ms step_avg:60.23ms
step:950/2315 train_time:57216ms step_avg:60.23ms
step:951/2315 train_time:57276ms step_avg:60.23ms
step:952/2315 train_time:57337ms step_avg:60.23ms
step:953/2315 train_time:57397ms step_avg:60.23ms
step:954/2315 train_time:57458ms step_avg:60.23ms
step:955/2315 train_time:57519ms step_avg:60.23ms
step:956/2315 train_time:57579ms step_avg:60.23ms
step:957/2315 train_time:57639ms step_avg:60.23ms
step:958/2315 train_time:57699ms step_avg:60.23ms
step:959/2315 train_time:57759ms step_avg:60.23ms
step:960/2315 train_time:57820ms step_avg:60.23ms
step:961/2315 train_time:57881ms step_avg:60.23ms
step:962/2315 train_time:57942ms step_avg:60.23ms
step:963/2315 train_time:58003ms step_avg:60.23ms
step:964/2315 train_time:58065ms step_avg:60.23ms
step:965/2315 train_time:58126ms step_avg:60.23ms
step:966/2315 train_time:58188ms step_avg:60.24ms
step:967/2315 train_time:58249ms step_avg:60.24ms
step:968/2315 train_time:58310ms step_avg:60.24ms
step:969/2315 train_time:58370ms step_avg:60.24ms
step:970/2315 train_time:58431ms step_avg:60.24ms
step:971/2315 train_time:58492ms step_avg:60.24ms
step:972/2315 train_time:58553ms step_avg:60.24ms
step:973/2315 train_time:58614ms step_avg:60.24ms
step:974/2315 train_time:58676ms step_avg:60.24ms
step:975/2315 train_time:58736ms step_avg:60.24ms
step:976/2315 train_time:58796ms step_avg:60.24ms
step:977/2315 train_time:58857ms step_avg:60.24ms
step:978/2315 train_time:58917ms step_avg:60.24ms
step:979/2315 train_time:58978ms step_avg:60.24ms
step:980/2315 train_time:59038ms step_avg:60.24ms
step:981/2315 train_time:59099ms step_avg:60.24ms
step:982/2315 train_time:59160ms step_avg:60.24ms
step:983/2315 train_time:59220ms step_avg:60.24ms
step:984/2315 train_time:59281ms step_avg:60.24ms
step:985/2315 train_time:59341ms step_avg:60.24ms
step:986/2315 train_time:59402ms step_avg:60.25ms
step:987/2315 train_time:59463ms step_avg:60.25ms
step:988/2315 train_time:59524ms step_avg:60.25ms
step:989/2315 train_time:59585ms step_avg:60.25ms
step:990/2315 train_time:59647ms step_avg:60.25ms
step:991/2315 train_time:59708ms step_avg:60.25ms
step:992/2315 train_time:59769ms step_avg:60.25ms
step:993/2315 train_time:59829ms step_avg:60.25ms
step:994/2315 train_time:59891ms step_avg:60.25ms
step:995/2315 train_time:59953ms step_avg:60.25ms
step:996/2315 train_time:60015ms step_avg:60.26ms
step:997/2315 train_time:60074ms step_avg:60.26ms
step:998/2315 train_time:60135ms step_avg:60.26ms
step:999/2315 train_time:60195ms step_avg:60.26ms
step:1000/2315 train_time:60256ms step_avg:60.26ms
step:1000/2315 val_loss:3.5726 train_time:60319ms step_avg:60.32ms
step:1001/2315 train_time:60337ms step_avg:60.28ms
step:1002/2315 train_time:60380ms step_avg:60.26ms
step:1003/2315 train_time:60445ms step_avg:60.26ms
step:1004/2315 train_time:60509ms step_avg:60.27ms
step:1005/2315 train_time:60571ms step_avg:60.27ms
step:1006/2315 train_time:60632ms step_avg:60.27ms
step:1007/2315 train_time:60692ms step_avg:60.27ms
step:1008/2315 train_time:60753ms step_avg:60.27ms
step:1009/2315 train_time:60813ms step_avg:60.27ms
step:1010/2315 train_time:60873ms step_avg:60.27ms
step:1011/2315 train_time:60932ms step_avg:60.27ms
step:1012/2315 train_time:60992ms step_avg:60.27ms
step:1013/2315 train_time:61052ms step_avg:60.27ms
step:1014/2315 train_time:61113ms step_avg:60.27ms
step:1015/2315 train_time:61173ms step_avg:60.27ms
step:1016/2315 train_time:61234ms step_avg:60.27ms
step:1017/2315 train_time:61297ms step_avg:60.27ms
step:1018/2315 train_time:61361ms step_avg:60.28ms
step:1019/2315 train_time:61422ms step_avg:60.28ms
step:1020/2315 train_time:61484ms step_avg:60.28ms
step:1021/2315 train_time:61545ms step_avg:60.28ms
step:1022/2315 train_time:61606ms step_avg:60.28ms
step:1023/2315 train_time:61666ms step_avg:60.28ms
step:1024/2315 train_time:61727ms step_avg:60.28ms
step:1025/2315 train_time:61787ms step_avg:60.28ms
step:1026/2315 train_time:61847ms step_avg:60.28ms
step:1027/2315 train_time:61908ms step_avg:60.28ms
step:1028/2315 train_time:61968ms step_avg:60.28ms
step:1029/2315 train_time:62028ms step_avg:60.28ms
step:1030/2315 train_time:62088ms step_avg:60.28ms
step:1031/2315 train_time:62148ms step_avg:60.28ms
step:1032/2315 train_time:62208ms step_avg:60.28ms
step:1033/2315 train_time:62269ms step_avg:60.28ms
step:1034/2315 train_time:62330ms step_avg:60.28ms
step:1035/2315 train_time:62390ms step_avg:60.28ms
step:1036/2315 train_time:62452ms step_avg:60.28ms
step:1037/2315 train_time:62513ms step_avg:60.28ms
step:1038/2315 train_time:62574ms step_avg:60.28ms
step:1039/2315 train_time:62636ms step_avg:60.28ms
step:1040/2315 train_time:62697ms step_avg:60.29ms
step:1041/2315 train_time:62758ms step_avg:60.29ms
step:1042/2315 train_time:62819ms step_avg:60.29ms
step:1043/2315 train_time:62880ms step_avg:60.29ms
step:1044/2315 train_time:62941ms step_avg:60.29ms
step:1045/2315 train_time:63001ms step_avg:60.29ms
step:1046/2315 train_time:63062ms step_avg:60.29ms
step:1047/2315 train_time:63122ms step_avg:60.29ms
step:1048/2315 train_time:63183ms step_avg:60.29ms
step:1049/2315 train_time:63244ms step_avg:60.29ms
step:1050/2315 train_time:63306ms step_avg:60.29ms
step:1051/2315 train_time:63367ms step_avg:60.29ms
step:1052/2315 train_time:63428ms step_avg:60.29ms
step:1053/2315 train_time:63488ms step_avg:60.29ms
step:1054/2315 train_time:63549ms step_avg:60.29ms
step:1055/2315 train_time:63610ms step_avg:60.29ms
step:1056/2315 train_time:63671ms step_avg:60.29ms
step:1057/2315 train_time:63731ms step_avg:60.29ms
step:1058/2315 train_time:63792ms step_avg:60.29ms
step:1059/2315 train_time:63853ms step_avg:60.30ms
step:1060/2315 train_time:63914ms step_avg:60.30ms
step:1061/2315 train_time:63974ms step_avg:60.30ms
step:1062/2315 train_time:64035ms step_avg:60.30ms
step:1063/2315 train_time:64096ms step_avg:60.30ms
step:1064/2315 train_time:64157ms step_avg:60.30ms
step:1065/2315 train_time:64218ms step_avg:60.30ms
step:1066/2315 train_time:64279ms step_avg:60.30ms
step:1067/2315 train_time:64340ms step_avg:60.30ms
step:1068/2315 train_time:64402ms step_avg:60.30ms
step:1069/2315 train_time:64463ms step_avg:60.30ms
step:1070/2315 train_time:64525ms step_avg:60.30ms
step:1071/2315 train_time:64586ms step_avg:60.30ms
step:1072/2315 train_time:64647ms step_avg:60.30ms
step:1073/2315 train_time:64707ms step_avg:60.30ms
step:1074/2315 train_time:64767ms step_avg:60.30ms
step:1075/2315 train_time:64827ms step_avg:60.30ms
step:1076/2315 train_time:64888ms step_avg:60.30ms
step:1077/2315 train_time:64948ms step_avg:60.30ms
step:1078/2315 train_time:65008ms step_avg:60.30ms
step:1079/2315 train_time:65068ms step_avg:60.30ms
step:1080/2315 train_time:65129ms step_avg:60.30ms
step:1081/2315 train_time:65189ms step_avg:60.30ms
step:1082/2315 train_time:65252ms step_avg:60.31ms
step:1083/2315 train_time:65311ms step_avg:60.31ms
step:1084/2315 train_time:65372ms step_avg:60.31ms
step:1085/2315 train_time:65434ms step_avg:60.31ms
step:1086/2315 train_time:65495ms step_avg:60.31ms
step:1087/2315 train_time:65556ms step_avg:60.31ms
step:1088/2315 train_time:65617ms step_avg:60.31ms
step:1089/2315 train_time:65678ms step_avg:60.31ms
step:1090/2315 train_time:65739ms step_avg:60.31ms
step:1091/2315 train_time:65801ms step_avg:60.31ms
step:1092/2315 train_time:65862ms step_avg:60.31ms
step:1093/2315 train_time:65923ms step_avg:60.31ms
step:1094/2315 train_time:65984ms step_avg:60.31ms
step:1095/2315 train_time:66045ms step_avg:60.31ms
step:1096/2315 train_time:66106ms step_avg:60.32ms
step:1097/2315 train_time:66166ms step_avg:60.32ms
step:1098/2315 train_time:66227ms step_avg:60.32ms
step:1099/2315 train_time:66287ms step_avg:60.32ms
step:1100/2315 train_time:66349ms step_avg:60.32ms
step:1101/2315 train_time:66409ms step_avg:60.32ms
step:1102/2315 train_time:66470ms step_avg:60.32ms
step:1103/2315 train_time:66530ms step_avg:60.32ms
step:1104/2315 train_time:66591ms step_avg:60.32ms
step:1105/2315 train_time:66651ms step_avg:60.32ms
step:1106/2315 train_time:66712ms step_avg:60.32ms
step:1107/2315 train_time:66773ms step_avg:60.32ms
step:1108/2315 train_time:66834ms step_avg:60.32ms
step:1109/2315 train_time:66895ms step_avg:60.32ms
step:1110/2315 train_time:66956ms step_avg:60.32ms
step:1111/2315 train_time:67017ms step_avg:60.32ms
step:1112/2315 train_time:67077ms step_avg:60.32ms
step:1113/2315 train_time:67138ms step_avg:60.32ms
step:1114/2315 train_time:67199ms step_avg:60.32ms
step:1115/2315 train_time:67260ms step_avg:60.32ms
step:1116/2315 train_time:67322ms step_avg:60.32ms
step:1117/2315 train_time:67383ms step_avg:60.32ms
step:1118/2315 train_time:67446ms step_avg:60.33ms
step:1119/2315 train_time:67505ms step_avg:60.33ms
step:1120/2315 train_time:67567ms step_avg:60.33ms
step:1121/2315 train_time:67627ms step_avg:60.33ms
step:1122/2315 train_time:67688ms step_avg:60.33ms
step:1123/2315 train_time:67748ms step_avg:60.33ms
step:1124/2315 train_time:67809ms step_avg:60.33ms
step:1125/2315 train_time:67868ms step_avg:60.33ms
step:1126/2315 train_time:67929ms step_avg:60.33ms
step:1127/2315 train_time:67990ms step_avg:60.33ms
step:1128/2315 train_time:68051ms step_avg:60.33ms
step:1129/2315 train_time:68112ms step_avg:60.33ms
step:1130/2315 train_time:68173ms step_avg:60.33ms
step:1131/2315 train_time:68234ms step_avg:60.33ms
step:1132/2315 train_time:68295ms step_avg:60.33ms
step:1133/2315 train_time:68356ms step_avg:60.33ms
step:1134/2315 train_time:68418ms step_avg:60.33ms
step:1135/2315 train_time:68479ms step_avg:60.33ms
step:1136/2315 train_time:68540ms step_avg:60.33ms
step:1137/2315 train_time:68601ms step_avg:60.33ms
step:1138/2315 train_time:68662ms step_avg:60.34ms
step:1139/2315 train_time:68723ms step_avg:60.34ms
step:1140/2315 train_time:68785ms step_avg:60.34ms
step:1141/2315 train_time:68846ms step_avg:60.34ms
step:1142/2315 train_time:68907ms step_avg:60.34ms
step:1143/2315 train_time:68967ms step_avg:60.34ms
step:1144/2315 train_time:69028ms step_avg:60.34ms
step:1145/2315 train_time:69089ms step_avg:60.34ms
step:1146/2315 train_time:69149ms step_avg:60.34ms
step:1147/2315 train_time:69209ms step_avg:60.34ms
step:1148/2315 train_time:69269ms step_avg:60.34ms
step:1149/2315 train_time:69330ms step_avg:60.34ms
step:1150/2315 train_time:69391ms step_avg:60.34ms
step:1151/2315 train_time:69452ms step_avg:60.34ms
step:1152/2315 train_time:69513ms step_avg:60.34ms
step:1153/2315 train_time:69573ms step_avg:60.34ms
step:1154/2315 train_time:69635ms step_avg:60.34ms
step:1155/2315 train_time:69696ms step_avg:60.34ms
step:1156/2315 train_time:69758ms step_avg:60.34ms
step:1157/2315 train_time:69818ms step_avg:60.34ms
step:1158/2315 train_time:69880ms step_avg:60.35ms
step:1159/2315 train_time:69940ms step_avg:60.35ms
step:1160/2315 train_time:70002ms step_avg:60.35ms
step:1161/2315 train_time:70062ms step_avg:60.35ms
step:1162/2315 train_time:70124ms step_avg:60.35ms
step:1163/2315 train_time:70184ms step_avg:60.35ms
step:1164/2315 train_time:70246ms step_avg:60.35ms
step:1165/2315 train_time:70307ms step_avg:60.35ms
step:1166/2315 train_time:70367ms step_avg:60.35ms
step:1167/2315 train_time:70427ms step_avg:60.35ms
step:1168/2315 train_time:70488ms step_avg:60.35ms
step:1169/2315 train_time:70548ms step_avg:60.35ms
step:1170/2315 train_time:70609ms step_avg:60.35ms
step:1171/2315 train_time:70669ms step_avg:60.35ms
step:1172/2315 train_time:70730ms step_avg:60.35ms
step:1173/2315 train_time:70790ms step_avg:60.35ms
step:1174/2315 train_time:70850ms step_avg:60.35ms
step:1175/2315 train_time:70911ms step_avg:60.35ms
step:1176/2315 train_time:70973ms step_avg:60.35ms
step:1177/2315 train_time:71034ms step_avg:60.35ms
step:1178/2315 train_time:71095ms step_avg:60.35ms
step:1179/2315 train_time:71157ms step_avg:60.35ms
step:1180/2315 train_time:71218ms step_avg:60.35ms
step:1181/2315 train_time:71279ms step_avg:60.35ms
step:1182/2315 train_time:71340ms step_avg:60.36ms
step:1183/2315 train_time:71400ms step_avg:60.36ms
step:1184/2315 train_time:71462ms step_avg:60.36ms
step:1185/2315 train_time:71523ms step_avg:60.36ms
step:1186/2315 train_time:71584ms step_avg:60.36ms
step:1187/2315 train_time:71647ms step_avg:60.36ms
step:1188/2315 train_time:71706ms step_avg:60.36ms
step:1189/2315 train_time:71767ms step_avg:60.36ms
step:1190/2315 train_time:71828ms step_avg:60.36ms
step:1191/2315 train_time:71888ms step_avg:60.36ms
step:1192/2315 train_time:71949ms step_avg:60.36ms
step:1193/2315 train_time:72009ms step_avg:60.36ms
step:1194/2315 train_time:72070ms step_avg:60.36ms
step:1195/2315 train_time:72130ms step_avg:60.36ms
step:1196/2315 train_time:72190ms step_avg:60.36ms
step:1197/2315 train_time:72251ms step_avg:60.36ms
step:1198/2315 train_time:72312ms step_avg:60.36ms
step:1199/2315 train_time:72372ms step_avg:60.36ms
step:1200/2315 train_time:72433ms step_avg:60.36ms
step:1201/2315 train_time:72495ms step_avg:60.36ms
step:1202/2315 train_time:72556ms step_avg:60.36ms
step:1203/2315 train_time:72618ms step_avg:60.36ms
step:1204/2315 train_time:72679ms step_avg:60.36ms
step:1205/2315 train_time:72740ms step_avg:60.37ms
step:1206/2315 train_time:72801ms step_avg:60.37ms
step:1207/2315 train_time:72862ms step_avg:60.37ms
step:1208/2315 train_time:72924ms step_avg:60.37ms
step:1209/2315 train_time:72984ms step_avg:60.37ms
step:1210/2315 train_time:73046ms step_avg:60.37ms
step:1211/2315 train_time:73106ms step_avg:60.37ms
step:1212/2315 train_time:73167ms step_avg:60.37ms
step:1213/2315 train_time:73227ms step_avg:60.37ms
step:1214/2315 train_time:73288ms step_avg:60.37ms
step:1215/2315 train_time:73349ms step_avg:60.37ms
step:1216/2315 train_time:73409ms step_avg:60.37ms
step:1217/2315 train_time:73470ms step_avg:60.37ms
step:1218/2315 train_time:73530ms step_avg:60.37ms
step:1219/2315 train_time:73590ms step_avg:60.37ms
step:1220/2315 train_time:73652ms step_avg:60.37ms
step:1221/2315 train_time:73712ms step_avg:60.37ms
step:1222/2315 train_time:73773ms step_avg:60.37ms
step:1223/2315 train_time:73834ms step_avg:60.37ms
step:1224/2315 train_time:73895ms step_avg:60.37ms
step:1225/2315 train_time:73956ms step_avg:60.37ms
step:1226/2315 train_time:74018ms step_avg:60.37ms
step:1227/2315 train_time:74079ms step_avg:60.37ms
step:1228/2315 train_time:74141ms step_avg:60.38ms
step:1229/2315 train_time:74201ms step_avg:60.38ms
step:1230/2315 train_time:74262ms step_avg:60.38ms
step:1231/2315 train_time:74323ms step_avg:60.38ms
step:1232/2315 train_time:74385ms step_avg:60.38ms
step:1233/2315 train_time:74445ms step_avg:60.38ms
step:1234/2315 train_time:74507ms step_avg:60.38ms
step:1235/2315 train_time:74567ms step_avg:60.38ms
step:1236/2315 train_time:74628ms step_avg:60.38ms
step:1237/2315 train_time:74688ms step_avg:60.38ms
step:1238/2315 train_time:74749ms step_avg:60.38ms
step:1239/2315 train_time:74809ms step_avg:60.38ms
step:1240/2315 train_time:74870ms step_avg:60.38ms
step:1241/2315 train_time:74930ms step_avg:60.38ms
step:1242/2315 train_time:74991ms step_avg:60.38ms
step:1243/2315 train_time:75051ms step_avg:60.38ms
step:1244/2315 train_time:75112ms step_avg:60.38ms
step:1245/2315 train_time:75173ms step_avg:60.38ms
step:1246/2315 train_time:75234ms step_avg:60.38ms
step:1247/2315 train_time:75296ms step_avg:60.38ms
step:1248/2315 train_time:75358ms step_avg:60.38ms
step:1249/2315 train_time:75419ms step_avg:60.38ms
step:1250/2315 train_time:75480ms step_avg:60.38ms
step:1250/2315 val_loss:3.5134 train_time:75542ms step_avg:60.43ms
step:1251/2315 train_time:75560ms step_avg:60.40ms
step:1252/2315 train_time:75608ms step_avg:60.39ms
step:1253/2315 train_time:75670ms step_avg:60.39ms
step:1254/2315 train_time:75731ms step_avg:60.39ms
step:1255/2315 train_time:75792ms step_avg:60.39ms
step:1256/2315 train_time:75852ms step_avg:60.39ms
step:1257/2315 train_time:75912ms step_avg:60.39ms
step:1258/2315 train_time:75972ms step_avg:60.39ms
step:1259/2315 train_time:76032ms step_avg:60.39ms
step:1260/2315 train_time:76093ms step_avg:60.39ms
step:1261/2315 train_time:76154ms step_avg:60.39ms
step:1262/2315 train_time:76214ms step_avg:60.39ms
step:1263/2315 train_time:76275ms step_avg:60.39ms
step:1264/2315 train_time:76336ms step_avg:60.39ms
step:1265/2315 train_time:76396ms step_avg:60.39ms
step:1266/2315 train_time:76457ms step_avg:60.39ms
step:1267/2315 train_time:76521ms step_avg:60.40ms
step:1268/2315 train_time:76583ms step_avg:60.40ms
step:1269/2315 train_time:76646ms step_avg:60.40ms
step:1270/2315 train_time:76707ms step_avg:60.40ms
step:1271/2315 train_time:76767ms step_avg:60.40ms
step:1272/2315 train_time:76828ms step_avg:60.40ms
step:1273/2315 train_time:76888ms step_avg:60.40ms
step:1274/2315 train_time:76948ms step_avg:60.40ms
step:1275/2315 train_time:77008ms step_avg:60.40ms
step:1276/2315 train_time:77069ms step_avg:60.40ms
step:1277/2315 train_time:77129ms step_avg:60.40ms
step:1278/2315 train_time:77189ms step_avg:60.40ms
step:1279/2315 train_time:77249ms step_avg:60.40ms
step:1280/2315 train_time:77309ms step_avg:60.40ms
step:1281/2315 train_time:77369ms step_avg:60.40ms
step:1282/2315 train_time:77431ms step_avg:60.40ms
step:1283/2315 train_time:77493ms step_avg:60.40ms
step:1284/2315 train_time:77555ms step_avg:60.40ms
step:1285/2315 train_time:77617ms step_avg:60.40ms
step:1286/2315 train_time:77679ms step_avg:60.40ms
step:1287/2315 train_time:77740ms step_avg:60.40ms
step:1288/2315 train_time:77801ms step_avg:60.40ms
step:1289/2315 train_time:77862ms step_avg:60.40ms
step:1290/2315 train_time:77923ms step_avg:60.41ms
step:1291/2315 train_time:77984ms step_avg:60.41ms
step:1292/2315 train_time:78045ms step_avg:60.41ms
step:1293/2315 train_time:78104ms step_avg:60.41ms
step:1294/2315 train_time:78165ms step_avg:60.41ms
step:1295/2315 train_time:78225ms step_avg:60.41ms
step:1296/2315 train_time:78287ms step_avg:60.41ms
step:1297/2315 train_time:78346ms step_avg:60.41ms
step:1298/2315 train_time:78407ms step_avg:60.41ms
step:1299/2315 train_time:78467ms step_avg:60.41ms
step:1300/2315 train_time:78529ms step_avg:60.41ms
step:1301/2315 train_time:78590ms step_avg:60.41ms
step:1302/2315 train_time:78650ms step_avg:60.41ms
step:1303/2315 train_time:78712ms step_avg:60.41ms
step:1304/2315 train_time:78773ms step_avg:60.41ms
step:1305/2315 train_time:78835ms step_avg:60.41ms
step:1306/2315 train_time:78896ms step_avg:60.41ms
step:1307/2315 train_time:78957ms step_avg:60.41ms
step:1308/2315 train_time:79019ms step_avg:60.41ms
step:1309/2315 train_time:79079ms step_avg:60.41ms
step:1310/2315 train_time:79140ms step_avg:60.41ms
step:1311/2315 train_time:79201ms step_avg:60.41ms
step:1312/2315 train_time:79262ms step_avg:60.41ms
step:1313/2315 train_time:79322ms step_avg:60.41ms
step:1314/2315 train_time:79383ms step_avg:60.41ms
step:1315/2315 train_time:79444ms step_avg:60.41ms
step:1316/2315 train_time:79505ms step_avg:60.41ms
step:1317/2315 train_time:79566ms step_avg:60.41ms
step:1318/2315 train_time:79627ms step_avg:60.41ms
step:1319/2315 train_time:79688ms step_avg:60.42ms
step:1320/2315 train_time:79748ms step_avg:60.42ms
step:1321/2315 train_time:79809ms step_avg:60.42ms
step:1322/2315 train_time:79869ms step_avg:60.42ms
step:1323/2315 train_time:79930ms step_avg:60.42ms
step:1324/2315 train_time:79991ms step_avg:60.42ms
step:1325/2315 train_time:80051ms step_avg:60.42ms
step:1326/2315 train_time:80113ms step_avg:60.42ms
step:1327/2315 train_time:80173ms step_avg:60.42ms
step:1328/2315 train_time:80235ms step_avg:60.42ms
step:1329/2315 train_time:80296ms step_avg:60.42ms
step:1330/2315 train_time:80357ms step_avg:60.42ms
step:1331/2315 train_time:80419ms step_avg:60.42ms
step:1332/2315 train_time:80480ms step_avg:60.42ms
step:1333/2315 train_time:80540ms step_avg:60.42ms
step:1334/2315 train_time:80602ms step_avg:60.42ms
step:1335/2315 train_time:80662ms step_avg:60.42ms
step:1336/2315 train_time:80724ms step_avg:60.42ms
step:1337/2315 train_time:80785ms step_avg:60.42ms
step:1338/2315 train_time:80846ms step_avg:60.42ms
step:1339/2315 train_time:80905ms step_avg:60.42ms
step:1340/2315 train_time:80966ms step_avg:60.42ms
step:1341/2315 train_time:81027ms step_avg:60.42ms
step:1342/2315 train_time:81088ms step_avg:60.42ms
step:1343/2315 train_time:81148ms step_avg:60.42ms
step:1344/2315 train_time:81208ms step_avg:60.42ms
step:1345/2315 train_time:81268ms step_avg:60.42ms
step:1346/2315 train_time:81329ms step_avg:60.42ms
step:1347/2315 train_time:81391ms step_avg:60.42ms
step:1348/2315 train_time:81452ms step_avg:60.42ms
step:1349/2315 train_time:81512ms step_avg:60.42ms
step:1350/2315 train_time:81573ms step_avg:60.42ms
step:1351/2315 train_time:81635ms step_avg:60.43ms
step:1352/2315 train_time:81696ms step_avg:60.43ms
step:1353/2315 train_time:81757ms step_avg:60.43ms
step:1354/2315 train_time:81819ms step_avg:60.43ms
step:1355/2315 train_time:81879ms step_avg:60.43ms
step:1356/2315 train_time:81941ms step_avg:60.43ms
step:1357/2315 train_time:82001ms step_avg:60.43ms
step:1358/2315 train_time:82063ms step_avg:60.43ms
step:1359/2315 train_time:82124ms step_avg:60.43ms
step:1360/2315 train_time:82185ms step_avg:60.43ms
step:1361/2315 train_time:82246ms step_avg:60.43ms
step:1362/2315 train_time:82306ms step_avg:60.43ms
step:1363/2315 train_time:82366ms step_avg:60.43ms
step:1364/2315 train_time:82427ms step_avg:60.43ms
step:1365/2315 train_time:82487ms step_avg:60.43ms
step:1366/2315 train_time:82547ms step_avg:60.43ms
step:1367/2315 train_time:82608ms step_avg:60.43ms
step:1368/2315 train_time:82669ms step_avg:60.43ms
step:1369/2315 train_time:82730ms step_avg:60.43ms
step:1370/2315 train_time:82791ms step_avg:60.43ms
step:1371/2315 train_time:82852ms step_avg:60.43ms
step:1372/2315 train_time:82913ms step_avg:60.43ms
step:1373/2315 train_time:82973ms step_avg:60.43ms
step:1374/2315 train_time:83035ms step_avg:60.43ms
step:1375/2315 train_time:83096ms step_avg:60.43ms
step:1376/2315 train_time:83158ms step_avg:60.43ms
step:1377/2315 train_time:83218ms step_avg:60.43ms
step:1378/2315 train_time:83280ms step_avg:60.44ms
step:1379/2315 train_time:83341ms step_avg:60.44ms
step:1380/2315 train_time:83402ms step_avg:60.44ms
step:1381/2315 train_time:83463ms step_avg:60.44ms
step:1382/2315 train_time:83524ms step_avg:60.44ms
step:1383/2315 train_time:83585ms step_avg:60.44ms
step:1384/2315 train_time:83646ms step_avg:60.44ms
step:1385/2315 train_time:83706ms step_avg:60.44ms
step:1386/2315 train_time:83767ms step_avg:60.44ms
step:1387/2315 train_time:83828ms step_avg:60.44ms
step:1388/2315 train_time:83888ms step_avg:60.44ms
step:1389/2315 train_time:83948ms step_avg:60.44ms
step:1390/2315 train_time:84009ms step_avg:60.44ms
step:1391/2315 train_time:84069ms step_avg:60.44ms
step:1392/2315 train_time:84131ms step_avg:60.44ms
step:1393/2315 train_time:84191ms step_avg:60.44ms
step:1394/2315 train_time:84253ms step_avg:60.44ms
step:1395/2315 train_time:84314ms step_avg:60.44ms
step:1396/2315 train_time:84375ms step_avg:60.44ms
step:1397/2315 train_time:84436ms step_avg:60.44ms
step:1398/2315 train_time:84497ms step_avg:60.44ms
step:1399/2315 train_time:84558ms step_avg:60.44ms
step:1400/2315 train_time:84620ms step_avg:60.44ms
step:1401/2315 train_time:84680ms step_avg:60.44ms
step:1402/2315 train_time:84741ms step_avg:60.44ms
step:1403/2315 train_time:84802ms step_avg:60.44ms
step:1404/2315 train_time:84863ms step_avg:60.44ms
step:1405/2315 train_time:84924ms step_avg:60.44ms
step:1406/2315 train_time:84985ms step_avg:60.44ms
step:1407/2315 train_time:85046ms step_avg:60.44ms
step:1408/2315 train_time:85106ms step_avg:60.44ms
step:1409/2315 train_time:85167ms step_avg:60.44ms
step:1410/2315 train_time:85228ms step_avg:60.45ms
step:1411/2315 train_time:85288ms step_avg:60.44ms
step:1412/2315 train_time:85348ms step_avg:60.44ms
step:1413/2315 train_time:85409ms step_avg:60.44ms
step:1414/2315 train_time:85470ms step_avg:60.45ms
step:1415/2315 train_time:85530ms step_avg:60.45ms
step:1416/2315 train_time:85592ms step_avg:60.45ms
step:1417/2315 train_time:85653ms step_avg:60.45ms
step:1418/2315 train_time:85714ms step_avg:60.45ms
step:1419/2315 train_time:85775ms step_avg:60.45ms
step:1420/2315 train_time:85836ms step_avg:60.45ms
step:1421/2315 train_time:85897ms step_avg:60.45ms
step:1422/2315 train_time:85959ms step_avg:60.45ms
step:1423/2315 train_time:86020ms step_avg:60.45ms
step:1424/2315 train_time:86081ms step_avg:60.45ms
step:1425/2315 train_time:86142ms step_avg:60.45ms
step:1426/2315 train_time:86203ms step_avg:60.45ms
step:1427/2315 train_time:86264ms step_avg:60.45ms
step:1428/2315 train_time:86325ms step_avg:60.45ms
step:1429/2315 train_time:86385ms step_avg:60.45ms
step:1430/2315 train_time:86446ms step_avg:60.45ms
step:1431/2315 train_time:86506ms step_avg:60.45ms
step:1432/2315 train_time:86566ms step_avg:60.45ms
step:1433/2315 train_time:86626ms step_avg:60.45ms
step:1434/2315 train_time:86689ms step_avg:60.45ms
step:1435/2315 train_time:86747ms step_avg:60.45ms
step:1436/2315 train_time:86808ms step_avg:60.45ms
step:1437/2315 train_time:86869ms step_avg:60.45ms
step:1438/2315 train_time:86930ms step_avg:60.45ms
step:1439/2315 train_time:86991ms step_avg:60.45ms
step:1440/2315 train_time:87052ms step_avg:60.45ms
step:1441/2315 train_time:87113ms step_avg:60.45ms
step:1442/2315 train_time:87174ms step_avg:60.45ms
step:1443/2315 train_time:87235ms step_avg:60.45ms
step:1444/2315 train_time:87297ms step_avg:60.45ms
step:1445/2315 train_time:87357ms step_avg:60.45ms
step:1446/2315 train_time:87419ms step_avg:60.46ms
step:1447/2315 train_time:87480ms step_avg:60.46ms
step:1448/2315 train_time:87541ms step_avg:60.46ms
step:1449/2315 train_time:87601ms step_avg:60.46ms
step:1450/2315 train_time:87663ms step_avg:60.46ms
step:1451/2315 train_time:87723ms step_avg:60.46ms
step:1452/2315 train_time:87784ms step_avg:60.46ms
step:1453/2315 train_time:87845ms step_avg:60.46ms
step:1454/2315 train_time:87905ms step_avg:60.46ms
step:1455/2315 train_time:87966ms step_avg:60.46ms
step:1456/2315 train_time:88027ms step_avg:60.46ms
step:1457/2315 train_time:88087ms step_avg:60.46ms
step:1458/2315 train_time:88148ms step_avg:60.46ms
step:1459/2315 train_time:88208ms step_avg:60.46ms
step:1460/2315 train_time:88269ms step_avg:60.46ms
step:1461/2315 train_time:88329ms step_avg:60.46ms
step:1462/2315 train_time:88390ms step_avg:60.46ms
step:1463/2315 train_time:88451ms step_avg:60.46ms
step:1464/2315 train_time:88512ms step_avg:60.46ms
step:1465/2315 train_time:88573ms step_avg:60.46ms
step:1466/2315 train_time:88634ms step_avg:60.46ms
step:1467/2315 train_time:88696ms step_avg:60.46ms
step:1468/2315 train_time:88757ms step_avg:60.46ms
step:1469/2315 train_time:88818ms step_avg:60.46ms
step:1470/2315 train_time:88882ms step_avg:60.46ms
step:1471/2315 train_time:88940ms step_avg:60.46ms
step:1472/2315 train_time:89001ms step_avg:60.46ms
step:1473/2315 train_time:89062ms step_avg:60.46ms
step:1474/2315 train_time:89123ms step_avg:60.46ms
step:1475/2315 train_time:89184ms step_avg:60.46ms
step:1476/2315 train_time:89246ms step_avg:60.46ms
step:1477/2315 train_time:89306ms step_avg:60.46ms
step:1478/2315 train_time:89366ms step_avg:60.46ms
step:1479/2315 train_time:89427ms step_avg:60.46ms
step:1480/2315 train_time:89489ms step_avg:60.47ms
step:1481/2315 train_time:89547ms step_avg:60.46ms
step:1482/2315 train_time:89608ms step_avg:60.46ms
step:1483/2315 train_time:89668ms step_avg:60.46ms
step:1484/2315 train_time:89729ms step_avg:60.46ms
step:1485/2315 train_time:89789ms step_avg:60.46ms
step:1486/2315 train_time:89850ms step_avg:60.46ms
step:1487/2315 train_time:89912ms step_avg:60.47ms
step:1488/2315 train_time:89973ms step_avg:60.47ms
step:1489/2315 train_time:90033ms step_avg:60.47ms
step:1490/2315 train_time:90096ms step_avg:60.47ms
step:1491/2315 train_time:90158ms step_avg:60.47ms
step:1492/2315 train_time:90220ms step_avg:60.47ms
step:1493/2315 train_time:90280ms step_avg:60.47ms
step:1494/2315 train_time:90341ms step_avg:60.47ms
step:1495/2315 train_time:90402ms step_avg:60.47ms
step:1496/2315 train_time:90464ms step_avg:60.47ms
step:1497/2315 train_time:90525ms step_avg:60.47ms
step:1498/2315 train_time:90586ms step_avg:60.47ms
step:1499/2315 train_time:90646ms step_avg:60.47ms
step:1500/2315 train_time:90706ms step_avg:60.47ms
step:1500/2315 val_loss:3.4496 train_time:90768ms step_avg:60.51ms
step:1501/2315 train_time:90786ms step_avg:60.48ms
step:1502/2315 train_time:90831ms step_avg:60.47ms
step:1503/2315 train_time:90893ms step_avg:60.47ms
step:1504/2315 train_time:90956ms step_avg:60.48ms
step:1505/2315 train_time:91016ms step_avg:60.48ms
step:1506/2315 train_time:91077ms step_avg:60.48ms
step:1507/2315 train_time:91137ms step_avg:60.48ms
step:1508/2315 train_time:91198ms step_avg:60.48ms
step:1509/2315 train_time:91258ms step_avg:60.48ms
step:1510/2315 train_time:91318ms step_avg:60.48ms
step:1511/2315 train_time:91378ms step_avg:60.48ms
step:1512/2315 train_time:91439ms step_avg:60.48ms
step:1513/2315 train_time:91498ms step_avg:60.47ms
step:1514/2315 train_time:91559ms step_avg:60.47ms
step:1515/2315 train_time:91619ms step_avg:60.47ms
step:1516/2315 train_time:91681ms step_avg:60.48ms
step:1517/2315 train_time:91743ms step_avg:60.48ms
step:1518/2315 train_time:91804ms step_avg:60.48ms
step:1519/2315 train_time:91866ms step_avg:60.48ms
step:1520/2315 train_time:91928ms step_avg:60.48ms
step:1521/2315 train_time:91990ms step_avg:60.48ms
step:1522/2315 train_time:92052ms step_avg:60.48ms
step:1523/2315 train_time:92113ms step_avg:60.48ms
step:1524/2315 train_time:92174ms step_avg:60.48ms
step:1525/2315 train_time:92235ms step_avg:60.48ms
step:1526/2315 train_time:92296ms step_avg:60.48ms
step:1527/2315 train_time:92357ms step_avg:60.48ms
step:1528/2315 train_time:92418ms step_avg:60.48ms
step:1529/2315 train_time:92478ms step_avg:60.48ms
step:1530/2315 train_time:92539ms step_avg:60.48ms
step:1531/2315 train_time:92600ms step_avg:60.48ms
step:1532/2315 train_time:92662ms step_avg:60.48ms
step:1533/2315 train_time:92723ms step_avg:60.48ms
step:1534/2315 train_time:92785ms step_avg:60.49ms
step:1535/2315 train_time:92846ms step_avg:60.49ms
step:1536/2315 train_time:92908ms step_avg:60.49ms
step:1537/2315 train_time:92969ms step_avg:60.49ms
step:1538/2315 train_time:93031ms step_avg:60.49ms
step:1539/2315 train_time:93092ms step_avg:60.49ms
step:1540/2315 train_time:93154ms step_avg:60.49ms
step:1541/2315 train_time:93214ms step_avg:60.49ms
step:1542/2315 train_time:93276ms step_avg:60.49ms
step:1543/2315 train_time:93337ms step_avg:60.49ms
step:1544/2315 train_time:93398ms step_avg:60.49ms
step:1545/2315 train_time:93459ms step_avg:60.49ms
step:1546/2315 train_time:93520ms step_avg:60.49ms
step:1547/2315 train_time:93581ms step_avg:60.49ms
step:1548/2315 train_time:93642ms step_avg:60.49ms
step:1549/2315 train_time:93703ms step_avg:60.49ms
step:1550/2315 train_time:93765ms step_avg:60.49ms
step:1551/2315 train_time:93825ms step_avg:60.49ms
step:1552/2315 train_time:93887ms step_avg:60.49ms
step:1553/2315 train_time:93948ms step_avg:60.49ms
step:1554/2315 train_time:94010ms step_avg:60.50ms
step:1555/2315 train_time:94071ms step_avg:60.50ms
step:1556/2315 train_time:94133ms step_avg:60.50ms
step:1557/2315 train_time:94194ms step_avg:60.50ms
step:1558/2315 train_time:94255ms step_avg:60.50ms
step:1559/2315 train_time:94316ms step_avg:60.50ms
step:1560/2315 train_time:94378ms step_avg:60.50ms
step:1561/2315 train_time:94439ms step_avg:60.50ms
step:1562/2315 train_time:94500ms step_avg:60.50ms
step:1563/2315 train_time:94561ms step_avg:60.50ms
step:1564/2315 train_time:94622ms step_avg:60.50ms
step:1565/2315 train_time:94682ms step_avg:60.50ms
step:1566/2315 train_time:94744ms step_avg:60.50ms
step:1567/2315 train_time:94805ms step_avg:60.50ms
step:1568/2315 train_time:94867ms step_avg:60.50ms
step:1569/2315 train_time:94927ms step_avg:60.50ms
step:1570/2315 train_time:94989ms step_avg:60.50ms
step:1571/2315 train_time:95051ms step_avg:60.50ms
step:1572/2315 train_time:95113ms step_avg:60.50ms
step:1573/2315 train_time:95174ms step_avg:60.50ms
step:1574/2315 train_time:95235ms step_avg:60.51ms
step:1575/2315 train_time:95297ms step_avg:60.51ms
step:1576/2315 train_time:95358ms step_avg:60.51ms
step:1577/2315 train_time:95419ms step_avg:60.51ms
step:1578/2315 train_time:95480ms step_avg:60.51ms
step:1579/2315 train_time:95541ms step_avg:60.51ms
step:1580/2315 train_time:95603ms step_avg:60.51ms
step:1581/2315 train_time:95664ms step_avg:60.51ms
step:1582/2315 train_time:95725ms step_avg:60.51ms
step:1583/2315 train_time:95786ms step_avg:60.51ms
step:1584/2315 train_time:95847ms step_avg:60.51ms
step:1585/2315 train_time:95908ms step_avg:60.51ms
step:1586/2315 train_time:95970ms step_avg:60.51ms
step:1587/2315 train_time:96031ms step_avg:60.51ms
step:1588/2315 train_time:96094ms step_avg:60.51ms
step:1589/2315 train_time:96155ms step_avg:60.51ms
step:1590/2315 train_time:96216ms step_avg:60.51ms
step:1591/2315 train_time:96277ms step_avg:60.51ms
step:1592/2315 train_time:96339ms step_avg:60.51ms
step:1593/2315 train_time:96400ms step_avg:60.51ms
step:1594/2315 train_time:96462ms step_avg:60.52ms
step:1595/2315 train_time:96524ms step_avg:60.52ms
step:1596/2315 train_time:96585ms step_avg:60.52ms
step:1597/2315 train_time:96646ms step_avg:60.52ms
step:1598/2315 train_time:96707ms step_avg:60.52ms
step:1599/2315 train_time:96768ms step_avg:60.52ms
step:1600/2315 train_time:96830ms step_avg:60.52ms
step:1601/2315 train_time:96890ms step_avg:60.52ms
step:1602/2315 train_time:96951ms step_avg:60.52ms
step:1603/2315 train_time:97012ms step_avg:60.52ms
step:1604/2315 train_time:97075ms step_avg:60.52ms
step:1605/2315 train_time:97136ms step_avg:60.52ms
step:1606/2315 train_time:97197ms step_avg:60.52ms
step:1607/2315 train_time:97258ms step_avg:60.52ms
step:1608/2315 train_time:97319ms step_avg:60.52ms
step:1609/2315 train_time:97380ms step_avg:60.52ms
step:1610/2315 train_time:97442ms step_avg:60.52ms
step:1611/2315 train_time:97502ms step_avg:60.52ms
step:1612/2315 train_time:97564ms step_avg:60.52ms
step:1613/2315 train_time:97624ms step_avg:60.52ms
step:1614/2315 train_time:97686ms step_avg:60.52ms
step:1615/2315 train_time:97747ms step_avg:60.52ms
step:1616/2315 train_time:97809ms step_avg:60.53ms
step:1617/2315 train_time:97870ms step_avg:60.53ms
step:1618/2315 train_time:97931ms step_avg:60.53ms
step:1619/2315 train_time:97993ms step_avg:60.53ms
step:1620/2315 train_time:98054ms step_avg:60.53ms
step:1621/2315 train_time:98115ms step_avg:60.53ms
step:1622/2315 train_time:98178ms step_avg:60.53ms
step:1623/2315 train_time:98238ms step_avg:60.53ms
step:1624/2315 train_time:98299ms step_avg:60.53ms
step:1625/2315 train_time:98360ms step_avg:60.53ms
step:1626/2315 train_time:98423ms step_avg:60.53ms
step:1627/2315 train_time:98483ms step_avg:60.53ms
step:1628/2315 train_time:98544ms step_avg:60.53ms
step:1629/2315 train_time:98604ms step_avg:60.53ms
step:1630/2315 train_time:98666ms step_avg:60.53ms
step:1631/2315 train_time:98727ms step_avg:60.53ms
step:1632/2315 train_time:98788ms step_avg:60.53ms
step:1633/2315 train_time:98848ms step_avg:60.53ms
step:1634/2315 train_time:98910ms step_avg:60.53ms
step:1635/2315 train_time:98971ms step_avg:60.53ms
step:1636/2315 train_time:99034ms step_avg:60.53ms
step:1637/2315 train_time:99096ms step_avg:60.53ms
step:1638/2315 train_time:99158ms step_avg:60.54ms
step:1639/2315 train_time:99218ms step_avg:60.54ms
step:1640/2315 train_time:99280ms step_avg:60.54ms
step:1641/2315 train_time:99341ms step_avg:60.54ms
step:1642/2315 train_time:99403ms step_avg:60.54ms
step:1643/2315 train_time:99463ms step_avg:60.54ms
step:1644/2315 train_time:99525ms step_avg:60.54ms
step:1645/2315 train_time:99586ms step_avg:60.54ms
step:1646/2315 train_time:99647ms step_avg:60.54ms
step:1647/2315 train_time:99707ms step_avg:60.54ms
step:1648/2315 train_time:99769ms step_avg:60.54ms
step:1649/2315 train_time:99829ms step_avg:60.54ms
step:1650/2315 train_time:99892ms step_avg:60.54ms
step:1651/2315 train_time:99953ms step_avg:60.54ms
step:1652/2315 train_time:100015ms step_avg:60.54ms
step:1653/2315 train_time:100077ms step_avg:60.54ms
step:1654/2315 train_time:100138ms step_avg:60.54ms
step:1655/2315 train_time:100199ms step_avg:60.54ms
step:1656/2315 train_time:100261ms step_avg:60.54ms
step:1657/2315 train_time:100322ms step_avg:60.54ms
step:1658/2315 train_time:100383ms step_avg:60.54ms
step:1659/2315 train_time:100444ms step_avg:60.54ms
step:1660/2315 train_time:100505ms step_avg:60.55ms
step:1661/2315 train_time:100566ms step_avg:60.55ms
step:1662/2315 train_time:100627ms step_avg:60.55ms
step:1663/2315 train_time:100688ms step_avg:60.55ms
step:1664/2315 train_time:100750ms step_avg:60.55ms
step:1665/2315 train_time:100810ms step_avg:60.55ms
step:1666/2315 train_time:100872ms step_avg:60.55ms
step:1667/2315 train_time:100933ms step_avg:60.55ms
step:1668/2315 train_time:100995ms step_avg:60.55ms
step:1669/2315 train_time:101056ms step_avg:60.55ms
step:1670/2315 train_time:101118ms step_avg:60.55ms
step:1671/2315 train_time:101179ms step_avg:60.55ms
step:1672/2315 train_time:101241ms step_avg:60.55ms
step:1673/2315 train_time:101302ms step_avg:60.55ms
step:1674/2315 train_time:101363ms step_avg:60.55ms
step:1675/2315 train_time:101423ms step_avg:60.55ms
step:1676/2315 train_time:101484ms step_avg:60.55ms
step:1677/2315 train_time:101545ms step_avg:60.55ms
step:1678/2315 train_time:101606ms step_avg:60.55ms
step:1679/2315 train_time:101667ms step_avg:60.55ms
step:1680/2315 train_time:101729ms step_avg:60.55ms
step:1681/2315 train_time:101790ms step_avg:60.55ms
step:1682/2315 train_time:101852ms step_avg:60.55ms
step:1683/2315 train_time:101914ms step_avg:60.55ms
step:1684/2315 train_time:101976ms step_avg:60.56ms
step:1685/2315 train_time:102037ms step_avg:60.56ms
step:1686/2315 train_time:102099ms step_avg:60.56ms
step:1687/2315 train_time:102160ms step_avg:60.56ms
step:1688/2315 train_time:102221ms step_avg:60.56ms
step:1689/2315 train_time:102283ms step_avg:60.56ms
step:1690/2315 train_time:102344ms step_avg:60.56ms
step:1691/2315 train_time:102404ms step_avg:60.56ms
step:1692/2315 train_time:102466ms step_avg:60.56ms
step:1693/2315 train_time:102527ms step_avg:60.56ms
step:1694/2315 train_time:102588ms step_avg:60.56ms
step:1695/2315 train_time:102649ms step_avg:60.56ms
step:1696/2315 train_time:102710ms step_avg:60.56ms
step:1697/2315 train_time:102771ms step_avg:60.56ms
step:1698/2315 train_time:102833ms step_avg:60.56ms
step:1699/2315 train_time:102894ms step_avg:60.56ms
step:1700/2315 train_time:102957ms step_avg:60.56ms
step:1701/2315 train_time:103018ms step_avg:60.56ms
step:1702/2315 train_time:103079ms step_avg:60.56ms
step:1703/2315 train_time:103141ms step_avg:60.56ms
step:1704/2315 train_time:103202ms step_avg:60.56ms
step:1705/2315 train_time:103263ms step_avg:60.56ms
step:1706/2315 train_time:103324ms step_avg:60.57ms
step:1707/2315 train_time:103385ms step_avg:60.57ms
step:1708/2315 train_time:103447ms step_avg:60.57ms
step:1709/2315 train_time:103507ms step_avg:60.57ms
step:1710/2315 train_time:103570ms step_avg:60.57ms
step:1711/2315 train_time:103631ms step_avg:60.57ms
step:1712/2315 train_time:103692ms step_avg:60.57ms
step:1713/2315 train_time:103754ms step_avg:60.57ms
step:1714/2315 train_time:103815ms step_avg:60.57ms
step:1715/2315 train_time:103876ms step_avg:60.57ms
step:1716/2315 train_time:103938ms step_avg:60.57ms
step:1717/2315 train_time:103999ms step_avg:60.57ms
step:1718/2315 train_time:104060ms step_avg:60.57ms
step:1719/2315 train_time:104121ms step_avg:60.57ms
step:1720/2315 train_time:104183ms step_avg:60.57ms
step:1721/2315 train_time:104244ms step_avg:60.57ms
step:1722/2315 train_time:104306ms step_avg:60.57ms
step:1723/2315 train_time:104367ms step_avg:60.57ms
step:1724/2315 train_time:104431ms step_avg:60.57ms
step:1725/2315 train_time:104490ms step_avg:60.57ms
step:1726/2315 train_time:104552ms step_avg:60.57ms
step:1727/2315 train_time:104613ms step_avg:60.57ms
step:1728/2315 train_time:104675ms step_avg:60.58ms
step:1729/2315 train_time:104735ms step_avg:60.58ms
step:1730/2315 train_time:104797ms step_avg:60.58ms
step:1731/2315 train_time:104857ms step_avg:60.58ms
step:1732/2315 train_time:104919ms step_avg:60.58ms
step:1733/2315 train_time:104980ms step_avg:60.58ms
step:1734/2315 train_time:105042ms step_avg:60.58ms
step:1735/2315 train_time:105102ms step_avg:60.58ms
step:1736/2315 train_time:105164ms step_avg:60.58ms
step:1737/2315 train_time:105225ms step_avg:60.58ms
step:1738/2315 train_time:105287ms step_avg:60.58ms
step:1739/2315 train_time:105348ms step_avg:60.58ms
step:1740/2315 train_time:105409ms step_avg:60.58ms
step:1741/2315 train_time:105470ms step_avg:60.58ms
step:1742/2315 train_time:105532ms step_avg:60.58ms
step:1743/2315 train_time:105593ms step_avg:60.58ms
step:1744/2315 train_time:105654ms step_avg:60.58ms
step:1745/2315 train_time:105715ms step_avg:60.58ms
step:1746/2315 train_time:105777ms step_avg:60.58ms
step:1747/2315 train_time:105838ms step_avg:60.58ms
step:1748/2315 train_time:105899ms step_avg:60.58ms
step:1749/2315 train_time:105960ms step_avg:60.58ms
step:1750/2315 train_time:106022ms step_avg:60.58ms
step:1750/2315 val_loss:3.3807 train_time:106085ms step_avg:60.62ms
step:1751/2315 train_time:106103ms step_avg:60.60ms
step:1752/2315 train_time:106147ms step_avg:60.59ms
step:1753/2315 train_time:106213ms step_avg:60.59ms
step:1754/2315 train_time:106283ms step_avg:60.59ms
step:1755/2315 train_time:106344ms step_avg:60.59ms
step:1756/2315 train_time:106406ms step_avg:60.60ms
step:1757/2315 train_time:106466ms step_avg:60.60ms
step:1758/2315 train_time:106527ms step_avg:60.60ms
step:1759/2315 train_time:106588ms step_avg:60.60ms
step:1760/2315 train_time:106648ms step_avg:60.60ms
step:1761/2315 train_time:106708ms step_avg:60.60ms
step:1762/2315 train_time:106769ms step_avg:60.60ms
step:1763/2315 train_time:106829ms step_avg:60.59ms
step:1764/2315 train_time:106889ms step_avg:60.59ms
step:1765/2315 train_time:106949ms step_avg:60.59ms
step:1766/2315 train_time:107010ms step_avg:60.59ms
step:1767/2315 train_time:107074ms step_avg:60.60ms
step:1768/2315 train_time:107136ms step_avg:60.60ms
step:1769/2315 train_time:107200ms step_avg:60.60ms
step:1770/2315 train_time:107263ms step_avg:60.60ms
step:1771/2315 train_time:107324ms step_avg:60.60ms
step:1772/2315 train_time:107386ms step_avg:60.60ms
step:1773/2315 train_time:107447ms step_avg:60.60ms
step:1774/2315 train_time:107509ms step_avg:60.60ms
step:1775/2315 train_time:107569ms step_avg:60.60ms
step:1776/2315 train_time:107630ms step_avg:60.60ms
step:1777/2315 train_time:107691ms step_avg:60.60ms
step:1778/2315 train_time:107752ms step_avg:60.60ms
step:1779/2315 train_time:107812ms step_avg:60.60ms
step:1780/2315 train_time:107872ms step_avg:60.60ms
step:1781/2315 train_time:107932ms step_avg:60.60ms
step:1782/2315 train_time:107993ms step_avg:60.60ms
step:1783/2315 train_time:108054ms step_avg:60.60ms
step:1784/2315 train_time:108116ms step_avg:60.60ms
step:1785/2315 train_time:108177ms step_avg:60.60ms
step:1786/2315 train_time:108239ms step_avg:60.60ms
step:1787/2315 train_time:108301ms step_avg:60.60ms
step:1788/2315 train_time:108363ms step_avg:60.61ms
step:1789/2315 train_time:108424ms step_avg:60.61ms
step:1790/2315 train_time:108486ms step_avg:60.61ms
step:1791/2315 train_time:108547ms step_avg:60.61ms
step:1792/2315 train_time:108608ms step_avg:60.61ms
step:1793/2315 train_time:108669ms step_avg:60.61ms
step:1794/2315 train_time:108730ms step_avg:60.61ms
step:1795/2315 train_time:108790ms step_avg:60.61ms
step:1796/2315 train_time:108851ms step_avg:60.61ms
step:1797/2315 train_time:108911ms step_avg:60.61ms
step:1798/2315 train_time:108972ms step_avg:60.61ms
step:1799/2315 train_time:109033ms step_avg:60.61ms
step:1800/2315 train_time:109095ms step_avg:60.61ms
step:1801/2315 train_time:109156ms step_avg:60.61ms
step:1802/2315 train_time:109218ms step_avg:60.61ms
step:1803/2315 train_time:109279ms step_avg:60.61ms
step:1804/2315 train_time:109342ms step_avg:60.61ms
step:1805/2315 train_time:109403ms step_avg:60.61ms
step:1806/2315 train_time:109465ms step_avg:60.61ms
step:1807/2315 train_time:109526ms step_avg:60.61ms
step:1808/2315 train_time:109588ms step_avg:60.61ms
step:1809/2315 train_time:109648ms step_avg:60.61ms
step:1810/2315 train_time:109709ms step_avg:60.61ms
step:1811/2315 train_time:109770ms step_avg:60.61ms
step:1812/2315 train_time:109831ms step_avg:60.61ms
step:1813/2315 train_time:109892ms step_avg:60.61ms
step:1814/2315 train_time:109953ms step_avg:60.61ms
step:1815/2315 train_time:110014ms step_avg:60.61ms
step:1816/2315 train_time:110075ms step_avg:60.61ms
step:1817/2315 train_time:110136ms step_avg:60.61ms
step:1818/2315 train_time:110198ms step_avg:60.61ms
step:1819/2315 train_time:110259ms step_avg:60.62ms
step:1820/2315 train_time:110321ms step_avg:60.62ms
step:1821/2315 train_time:110382ms step_avg:60.62ms
step:1822/2315 train_time:110444ms step_avg:60.62ms
step:1823/2315 train_time:110505ms step_avg:60.62ms
step:1824/2315 train_time:110567ms step_avg:60.62ms
step:1825/2315 train_time:110628ms step_avg:60.62ms
step:1826/2315 train_time:110689ms step_avg:60.62ms
step:1827/2315 train_time:110750ms step_avg:60.62ms
step:1828/2315 train_time:110812ms step_avg:60.62ms
step:1829/2315 train_time:110873ms step_avg:60.62ms
step:1830/2315 train_time:110936ms step_avg:60.62ms
step:1831/2315 train_time:110994ms step_avg:60.62ms
step:1832/2315 train_time:111055ms step_avg:60.62ms
step:1833/2315 train_time:111116ms step_avg:60.62ms
step:1834/2315 train_time:111177ms step_avg:60.62ms
step:1835/2315 train_time:111239ms step_avg:60.62ms
step:1836/2315 train_time:111300ms step_avg:60.62ms
step:1837/2315 train_time:111361ms step_avg:60.62ms
step:1838/2315 train_time:111423ms step_avg:60.62ms
step:1839/2315 train_time:111485ms step_avg:60.62ms
step:1840/2315 train_time:111546ms step_avg:60.62ms
step:1841/2315 train_time:111607ms step_avg:60.62ms
step:1842/2315 train_time:111669ms step_avg:60.62ms
step:1843/2315 train_time:111730ms step_avg:60.62ms
step:1844/2315 train_time:111791ms step_avg:60.62ms
step:1845/2315 train_time:111853ms step_avg:60.62ms
step:1846/2315 train_time:111913ms step_avg:60.62ms
step:1847/2315 train_time:111974ms step_avg:60.62ms
step:1848/2315 train_time:112035ms step_avg:60.62ms
step:1849/2315 train_time:112096ms step_avg:60.63ms
step:1850/2315 train_time:112157ms step_avg:60.63ms
step:1851/2315 train_time:112218ms step_avg:60.63ms
step:1852/2315 train_time:112280ms step_avg:60.63ms
step:1853/2315 train_time:112341ms step_avg:60.63ms
step:1854/2315 train_time:112403ms step_avg:60.63ms
step:1855/2315 train_time:112464ms step_avg:60.63ms
step:1856/2315 train_time:112526ms step_avg:60.63ms
step:1857/2315 train_time:112587ms step_avg:60.63ms
step:1858/2315 train_time:112649ms step_avg:60.63ms
step:1859/2315 train_time:112710ms step_avg:60.63ms
step:1860/2315 train_time:112772ms step_avg:60.63ms
step:1861/2315 train_time:112833ms step_avg:60.63ms
step:1862/2315 train_time:112894ms step_avg:60.63ms
step:1863/2315 train_time:112954ms step_avg:60.63ms
step:1864/2315 train_time:113016ms step_avg:60.63ms
step:1865/2315 train_time:113077ms step_avg:60.63ms
step:1866/2315 train_time:113138ms step_avg:60.63ms
step:1867/2315 train_time:113199ms step_avg:60.63ms
step:1868/2315 train_time:113260ms step_avg:60.63ms
step:1869/2315 train_time:113321ms step_avg:60.63ms
step:1870/2315 train_time:113383ms step_avg:60.63ms
step:1871/2315 train_time:113445ms step_avg:60.63ms
step:1872/2315 train_time:113506ms step_avg:60.63ms
step:1873/2315 train_time:113567ms step_avg:60.63ms
step:1874/2315 train_time:113628ms step_avg:60.63ms
step:1875/2315 train_time:113690ms step_avg:60.63ms
step:1876/2315 train_time:113752ms step_avg:60.64ms
step:1877/2315 train_time:113812ms step_avg:60.64ms
step:1878/2315 train_time:113873ms step_avg:60.64ms
step:1879/2315 train_time:113934ms step_avg:60.64ms
step:1880/2315 train_time:113996ms step_avg:60.64ms
step:1881/2315 train_time:114056ms step_avg:60.64ms
step:1882/2315 train_time:114118ms step_avg:60.64ms
step:1883/2315 train_time:114178ms step_avg:60.64ms
step:1884/2315 train_time:114240ms step_avg:60.64ms
step:1885/2315 train_time:114301ms step_avg:60.64ms
step:1886/2315 train_time:114363ms step_avg:60.64ms
step:1887/2315 train_time:114424ms step_avg:60.64ms
step:1888/2315 train_time:114486ms step_avg:60.64ms
step:1889/2315 train_time:114548ms step_avg:60.64ms
step:1890/2315 train_time:114610ms step_avg:60.64ms
step:1891/2315 train_time:114671ms step_avg:60.64ms
step:1892/2315 train_time:114732ms step_avg:60.64ms
step:1893/2315 train_time:114793ms step_avg:60.64ms
step:1894/2315 train_time:114854ms step_avg:60.64ms
step:1895/2315 train_time:114915ms step_avg:60.64ms
step:1896/2315 train_time:114976ms step_avg:60.64ms
step:1897/2315 train_time:115037ms step_avg:60.64ms
step:1898/2315 train_time:115099ms step_avg:60.64ms
step:1899/2315 train_time:115160ms step_avg:60.64ms
step:1900/2315 train_time:115221ms step_avg:60.64ms
step:1901/2315 train_time:115282ms step_avg:60.64ms
step:1902/2315 train_time:115344ms step_avg:60.64ms
step:1903/2315 train_time:115405ms step_avg:60.64ms
step:1904/2315 train_time:115467ms step_avg:60.64ms
step:1905/2315 train_time:115528ms step_avg:60.64ms
step:1906/2315 train_time:115590ms step_avg:60.65ms
step:1907/2315 train_time:115651ms step_avg:60.65ms
step:1908/2315 train_time:115713ms step_avg:60.65ms
step:1909/2315 train_time:115774ms step_avg:60.65ms
step:1910/2315 train_time:115835ms step_avg:60.65ms
step:1911/2315 train_time:115896ms step_avg:60.65ms
step:1912/2315 train_time:115957ms step_avg:60.65ms
step:1913/2315 train_time:116018ms step_avg:60.65ms
step:1914/2315 train_time:116080ms step_avg:60.65ms
step:1915/2315 train_time:116142ms step_avg:60.65ms
step:1916/2315 train_time:116202ms step_avg:60.65ms
step:1917/2315 train_time:116263ms step_avg:60.65ms
step:1918/2315 train_time:116324ms step_avg:60.65ms
step:1919/2315 train_time:116385ms step_avg:60.65ms
step:1920/2315 train_time:116447ms step_avg:60.65ms
step:1921/2315 train_time:116509ms step_avg:60.65ms
step:1922/2315 train_time:116570ms step_avg:60.65ms
step:1923/2315 train_time:116631ms step_avg:60.65ms
step:1924/2315 train_time:116693ms step_avg:60.65ms
step:1925/2315 train_time:116754ms step_avg:60.65ms
step:1926/2315 train_time:116815ms step_avg:60.65ms
step:1927/2315 train_time:116876ms step_avg:60.65ms
step:1928/2315 train_time:116938ms step_avg:60.65ms
step:1929/2315 train_time:116998ms step_avg:60.65ms
step:1930/2315 train_time:117060ms step_avg:60.65ms
step:1931/2315 train_time:117121ms step_avg:60.65ms
step:1932/2315 train_time:117182ms step_avg:60.65ms
step:1933/2315 train_time:117243ms step_avg:60.65ms
step:1934/2315 train_time:117305ms step_avg:60.65ms
step:1935/2315 train_time:117366ms step_avg:60.65ms
step:1936/2315 train_time:117428ms step_avg:60.65ms
step:1937/2315 train_time:117489ms step_avg:60.65ms
step:1938/2315 train_time:117550ms step_avg:60.66ms
step:1939/2315 train_time:117611ms step_avg:60.66ms
step:1940/2315 train_time:117673ms step_avg:60.66ms
step:1941/2315 train_time:117733ms step_avg:60.66ms
step:1942/2315 train_time:117794ms step_avg:60.66ms
step:1943/2315 train_time:117856ms step_avg:60.66ms
step:1944/2315 train_time:117917ms step_avg:60.66ms
step:1945/2315 train_time:117978ms step_avg:60.66ms
step:1946/2315 train_time:118039ms step_avg:60.66ms
step:1947/2315 train_time:118101ms step_avg:60.66ms
step:1948/2315 train_time:118163ms step_avg:60.66ms
step:1949/2315 train_time:118223ms step_avg:60.66ms
step:1950/2315 train_time:118285ms step_avg:60.66ms
step:1951/2315 train_time:118346ms step_avg:60.66ms
step:1952/2315 train_time:118408ms step_avg:60.66ms
step:1953/2315 train_time:118469ms step_avg:60.66ms
step:1954/2315 train_time:118530ms step_avg:60.66ms
step:1955/2315 train_time:118592ms step_avg:60.66ms
step:1956/2315 train_time:118654ms step_avg:60.66ms
step:1957/2315 train_time:118714ms step_avg:60.66ms
step:1958/2315 train_time:118776ms step_avg:60.66ms
step:1959/2315 train_time:118837ms step_avg:60.66ms
step:1960/2315 train_time:118899ms step_avg:60.66ms
step:1961/2315 train_time:118959ms step_avg:60.66ms
step:1962/2315 train_time:119021ms step_avg:60.66ms
step:1963/2315 train_time:119082ms step_avg:60.66ms
step:1964/2315 train_time:119144ms step_avg:60.66ms
step:1965/2315 train_time:119205ms step_avg:60.66ms
step:1966/2315 train_time:119267ms step_avg:60.66ms
step:1967/2315 train_time:119328ms step_avg:60.67ms
step:1968/2315 train_time:119389ms step_avg:60.67ms
step:1969/2315 train_time:119450ms step_avg:60.67ms
step:1970/2315 train_time:119511ms step_avg:60.67ms
step:1971/2315 train_time:119573ms step_avg:60.67ms
step:1972/2315 train_time:119634ms step_avg:60.67ms
step:1973/2315 train_time:119695ms step_avg:60.67ms
step:1974/2315 train_time:119756ms step_avg:60.67ms
step:1975/2315 train_time:119817ms step_avg:60.67ms
step:1976/2315 train_time:119879ms step_avg:60.67ms
step:1977/2315 train_time:119939ms step_avg:60.67ms
step:1978/2315 train_time:120001ms step_avg:60.67ms
step:1979/2315 train_time:120062ms step_avg:60.67ms
step:1980/2315 train_time:120123ms step_avg:60.67ms
step:1981/2315 train_time:120185ms step_avg:60.67ms
step:1982/2315 train_time:120246ms step_avg:60.67ms
step:1983/2315 train_time:120308ms step_avg:60.67ms
step:1984/2315 train_time:120369ms step_avg:60.67ms
step:1985/2315 train_time:120430ms step_avg:60.67ms
step:1986/2315 train_time:120492ms step_avg:60.67ms
step:1987/2315 train_time:120553ms step_avg:60.67ms
step:1988/2315 train_time:120614ms step_avg:60.67ms
step:1989/2315 train_time:120675ms step_avg:60.67ms
step:1990/2315 train_time:120736ms step_avg:60.67ms
step:1991/2315 train_time:120797ms step_avg:60.67ms
step:1992/2315 train_time:120858ms step_avg:60.67ms
step:1993/2315 train_time:120919ms step_avg:60.67ms
step:1994/2315 train_time:120981ms step_avg:60.67ms
step:1995/2315 train_time:121043ms step_avg:60.67ms
step:1996/2315 train_time:121104ms step_avg:60.67ms
step:1997/2315 train_time:121165ms step_avg:60.67ms
step:1998/2315 train_time:121226ms step_avg:60.67ms
step:1999/2315 train_time:121287ms step_avg:60.67ms
step:2000/2315 train_time:121349ms step_avg:60.67ms
step:2000/2315 val_loss:3.3310 train_time:121412ms step_avg:60.71ms
step:2001/2315 train_time:121430ms step_avg:60.68ms
step:2002/2315 train_time:121475ms step_avg:60.68ms
step:2003/2315 train_time:121537ms step_avg:60.68ms
step:2004/2315 train_time:121600ms step_avg:60.68ms
step:2005/2315 train_time:121660ms step_avg:60.68ms
step:2006/2315 train_time:121722ms step_avg:60.68ms
step:2007/2315 train_time:121782ms step_avg:60.68ms
step:2008/2315 train_time:121842ms step_avg:60.68ms
step:2009/2315 train_time:121902ms step_avg:60.68ms
step:2010/2315 train_time:121965ms step_avg:60.68ms
step:2011/2315 train_time:122024ms step_avg:60.68ms
step:2012/2315 train_time:122084ms step_avg:60.68ms
step:2013/2315 train_time:122145ms step_avg:60.68ms
step:2014/2315 train_time:122206ms step_avg:60.68ms
step:2015/2315 train_time:122267ms step_avg:60.68ms
step:2016/2315 train_time:122330ms step_avg:60.68ms
step:2017/2315 train_time:122393ms step_avg:60.68ms
step:2018/2315 train_time:122456ms step_avg:60.68ms
step:2019/2315 train_time:122518ms step_avg:60.68ms
step:2020/2315 train_time:122580ms step_avg:60.68ms
step:2021/2315 train_time:122641ms step_avg:60.68ms
step:2022/2315 train_time:122703ms step_avg:60.68ms
step:2023/2315 train_time:122764ms step_avg:60.68ms
step:2024/2315 train_time:122824ms step_avg:60.68ms
step:2025/2315 train_time:122885ms step_avg:60.68ms
step:2026/2315 train_time:122946ms step_avg:60.68ms
step:2027/2315 train_time:123006ms step_avg:60.68ms
step:2028/2315 train_time:123067ms step_avg:60.68ms
step:2029/2315 train_time:123128ms step_avg:60.68ms
step:2030/2315 train_time:123189ms step_avg:60.68ms
step:2031/2315 train_time:123250ms step_avg:60.68ms
step:2032/2315 train_time:123312ms step_avg:60.69ms
step:2033/2315 train_time:123375ms step_avg:60.69ms
step:2034/2315 train_time:123437ms step_avg:60.69ms
step:2035/2315 train_time:123499ms step_avg:60.69ms
step:2036/2315 train_time:123561ms step_avg:60.69ms
step:2037/2315 train_time:123623ms step_avg:60.69ms
step:2038/2315 train_time:123685ms step_avg:60.69ms
step:2039/2315 train_time:123746ms step_avg:60.69ms
step:2040/2315 train_time:123808ms step_avg:60.69ms
step:2041/2315 train_time:123868ms step_avg:60.69ms
step:2042/2315 train_time:123930ms step_avg:60.69ms
step:2043/2315 train_time:123991ms step_avg:60.69ms
step:2044/2315 train_time:124053ms step_avg:60.69ms
step:2045/2315 train_time:124113ms step_avg:60.69ms
step:2046/2315 train_time:124175ms step_avg:60.69ms
step:2047/2315 train_time:124235ms step_avg:60.69ms
step:2048/2315 train_time:124297ms step_avg:60.69ms
step:2049/2315 train_time:124358ms step_avg:60.69ms
step:2050/2315 train_time:124419ms step_avg:60.69ms
step:2051/2315 train_time:124482ms step_avg:60.69ms
step:2052/2315 train_time:124543ms step_avg:60.69ms
step:2053/2315 train_time:124605ms step_avg:60.69ms
step:2054/2315 train_time:124667ms step_avg:60.69ms
step:2055/2315 train_time:124728ms step_avg:60.69ms
step:2056/2315 train_time:124790ms step_avg:60.70ms
step:2057/2315 train_time:124851ms step_avg:60.70ms
step:2058/2315 train_time:124912ms step_avg:60.70ms
step:2059/2315 train_time:124973ms step_avg:60.70ms
step:2060/2315 train_time:125034ms step_avg:60.70ms
step:2061/2315 train_time:125095ms step_avg:60.70ms
step:2062/2315 train_time:125156ms step_avg:60.70ms
step:2063/2315 train_time:125217ms step_avg:60.70ms
step:2064/2315 train_time:125278ms step_avg:60.70ms
step:2065/2315 train_time:125339ms step_avg:60.70ms
step:2066/2315 train_time:125401ms step_avg:60.70ms
step:2067/2315 train_time:125461ms step_avg:60.70ms
step:2068/2315 train_time:125523ms step_avg:60.70ms
step:2069/2315 train_time:125584ms step_avg:60.70ms
step:2070/2315 train_time:125646ms step_avg:60.70ms
step:2071/2315 train_time:125707ms step_avg:60.70ms
step:2072/2315 train_time:125769ms step_avg:60.70ms
step:2073/2315 train_time:125830ms step_avg:60.70ms
step:2074/2315 train_time:125891ms step_avg:60.70ms
step:2075/2315 train_time:125953ms step_avg:60.70ms
step:2076/2315 train_time:126014ms step_avg:60.70ms
step:2077/2315 train_time:126075ms step_avg:60.70ms
step:2078/2315 train_time:126136ms step_avg:60.70ms
step:2079/2315 train_time:126197ms step_avg:60.70ms
step:2080/2315 train_time:126258ms step_avg:60.70ms
step:2081/2315 train_time:126320ms step_avg:60.70ms
step:2082/2315 train_time:126381ms step_avg:60.70ms
step:2083/2315 train_time:126441ms step_avg:60.70ms
step:2084/2315 train_time:126503ms step_avg:60.70ms
step:2085/2315 train_time:126565ms step_avg:60.70ms
step:2086/2315 train_time:126626ms step_avg:60.70ms
step:2087/2315 train_time:126687ms step_avg:60.70ms
step:2088/2315 train_time:126749ms step_avg:60.70ms
step:2089/2315 train_time:126810ms step_avg:60.70ms
step:2090/2315 train_time:126872ms step_avg:60.70ms
step:2091/2315 train_time:126933ms step_avg:60.70ms
step:2092/2315 train_time:126994ms step_avg:60.70ms
step:2093/2315 train_time:127055ms step_avg:60.70ms
step:2094/2315 train_time:127116ms step_avg:60.71ms
step:2095/2315 train_time:127178ms step_avg:60.71ms
step:2096/2315 train_time:127239ms step_avg:60.71ms
step:2097/2315 train_time:127300ms step_avg:60.71ms
step:2098/2315 train_time:127361ms step_avg:60.71ms
step:2099/2315 train_time:127422ms step_avg:60.71ms
step:2100/2315 train_time:127483ms step_avg:60.71ms
step:2101/2315 train_time:127545ms step_avg:60.71ms
step:2102/2315 train_time:127606ms step_avg:60.71ms
step:2103/2315 train_time:127667ms step_avg:60.71ms
step:2104/2315 train_time:127729ms step_avg:60.71ms
step:2105/2315 train_time:127790ms step_avg:60.71ms
step:2106/2315 train_time:127851ms step_avg:60.71ms
step:2107/2315 train_time:127913ms step_avg:60.71ms
step:2108/2315 train_time:127974ms step_avg:60.71ms
step:2109/2315 train_time:128035ms step_avg:60.71ms
step:2110/2315 train_time:128096ms step_avg:60.71ms
step:2111/2315 train_time:128157ms step_avg:60.71ms
step:2112/2315 train_time:128219ms step_avg:60.71ms
step:2113/2315 train_time:128279ms step_avg:60.71ms
step:2114/2315 train_time:128341ms step_avg:60.71ms
step:2115/2315 train_time:128401ms step_avg:60.71ms
step:2116/2315 train_time:128465ms step_avg:60.71ms
step:2117/2315 train_time:128523ms step_avg:60.71ms
step:2118/2315 train_time:128584ms step_avg:60.71ms
step:2119/2315 train_time:128645ms step_avg:60.71ms
step:2120/2315 train_time:128707ms step_avg:60.71ms
step:2121/2315 train_time:128768ms step_avg:60.71ms
step:2122/2315 train_time:128830ms step_avg:60.71ms
step:2123/2315 train_time:128892ms step_avg:60.71ms
step:2124/2315 train_time:128954ms step_avg:60.71ms
step:2125/2315 train_time:129014ms step_avg:60.71ms
step:2126/2315 train_time:129076ms step_avg:60.71ms
step:2127/2315 train_time:129137ms step_avg:60.71ms
step:2128/2315 train_time:129199ms step_avg:60.71ms
step:2129/2315 train_time:129260ms step_avg:60.71ms
step:2130/2315 train_time:129321ms step_avg:60.71ms
step:2131/2315 train_time:129382ms step_avg:60.71ms
step:2132/2315 train_time:129443ms step_avg:60.71ms
step:2133/2315 train_time:129504ms step_avg:60.71ms
step:2134/2315 train_time:129566ms step_avg:60.71ms
step:2135/2315 train_time:129627ms step_avg:60.72ms
step:2136/2315 train_time:129689ms step_avg:60.72ms
step:2137/2315 train_time:129750ms step_avg:60.72ms
step:2138/2315 train_time:129812ms step_avg:60.72ms
step:2139/2315 train_time:129873ms step_avg:60.72ms
step:2140/2315 train_time:129934ms step_avg:60.72ms
step:2141/2315 train_time:129995ms step_avg:60.72ms
step:2142/2315 train_time:130057ms step_avg:60.72ms
step:2143/2315 train_time:130118ms step_avg:60.72ms
step:2144/2315 train_time:130180ms step_avg:60.72ms
step:2145/2315 train_time:130241ms step_avg:60.72ms
step:2146/2315 train_time:130302ms step_avg:60.72ms
step:2147/2315 train_time:130363ms step_avg:60.72ms
step:2148/2315 train_time:130424ms step_avg:60.72ms
step:2149/2315 train_time:130485ms step_avg:60.72ms
step:2150/2315 train_time:130546ms step_avg:60.72ms
step:2151/2315 train_time:130607ms step_avg:60.72ms
step:2152/2315 train_time:130669ms step_avg:60.72ms
step:2153/2315 train_time:130730ms step_avg:60.72ms
step:2154/2315 train_time:130792ms step_avg:60.72ms
step:2155/2315 train_time:130854ms step_avg:60.72ms
step:2156/2315 train_time:130915ms step_avg:60.72ms
step:2157/2315 train_time:130977ms step_avg:60.72ms
step:2158/2315 train_time:131038ms step_avg:60.72ms
step:2159/2315 train_time:131100ms step_avg:60.72ms
step:2160/2315 train_time:131162ms step_avg:60.72ms
step:2161/2315 train_time:131223ms step_avg:60.72ms
step:2162/2315 train_time:131285ms step_avg:60.72ms
step:2163/2315 train_time:131345ms step_avg:60.72ms
step:2164/2315 train_time:131407ms step_avg:60.72ms
step:2165/2315 train_time:131468ms step_avg:60.72ms
step:2166/2315 train_time:131530ms step_avg:60.72ms
step:2167/2315 train_time:131591ms step_avg:60.73ms
step:2168/2315 train_time:131653ms step_avg:60.73ms
step:2169/2315 train_time:131714ms step_avg:60.73ms
step:2170/2315 train_time:131776ms step_avg:60.73ms
step:2171/2315 train_time:131837ms step_avg:60.73ms
step:2172/2315 train_time:131898ms step_avg:60.73ms
step:2173/2315 train_time:131959ms step_avg:60.73ms
step:2174/2315 train_time:132021ms step_avg:60.73ms
step:2175/2315 train_time:132082ms step_avg:60.73ms
step:2176/2315 train_time:132144ms step_avg:60.73ms
step:2177/2315 train_time:132206ms step_avg:60.73ms
step:2178/2315 train_time:132268ms step_avg:60.73ms
step:2179/2315 train_time:132329ms step_avg:60.73ms
step:2180/2315 train_time:132391ms step_avg:60.73ms
step:2181/2315 train_time:132452ms step_avg:60.73ms
step:2182/2315 train_time:132513ms step_avg:60.73ms
step:2183/2315 train_time:132574ms step_avg:60.73ms
step:2184/2315 train_time:132636ms step_avg:60.73ms
step:2185/2315 train_time:132697ms step_avg:60.73ms
step:2186/2315 train_time:132758ms step_avg:60.73ms
step:2187/2315 train_time:132820ms step_avg:60.73ms
step:2188/2315 train_time:132881ms step_avg:60.73ms
step:2189/2315 train_time:132943ms step_avg:60.73ms
step:2190/2315 train_time:133004ms step_avg:60.73ms
step:2191/2315 train_time:133066ms step_avg:60.73ms
step:2192/2315 train_time:133128ms step_avg:60.73ms
step:2193/2315 train_time:133189ms step_avg:60.73ms
step:2194/2315 train_time:133251ms step_avg:60.73ms
step:2195/2315 train_time:133312ms step_avg:60.73ms
step:2196/2315 train_time:133376ms step_avg:60.74ms
step:2197/2315 train_time:133435ms step_avg:60.74ms
step:2198/2315 train_time:133496ms step_avg:60.74ms
step:2199/2315 train_time:133558ms step_avg:60.74ms
step:2200/2315 train_time:133620ms step_avg:60.74ms
step:2201/2315 train_time:133681ms step_avg:60.74ms
step:2202/2315 train_time:133742ms step_avg:60.74ms
step:2203/2315 train_time:133803ms step_avg:60.74ms
step:2204/2315 train_time:133865ms step_avg:60.74ms
step:2205/2315 train_time:133926ms step_avg:60.74ms
step:2206/2315 train_time:133988ms step_avg:60.74ms
step:2207/2315 train_time:134049ms step_avg:60.74ms
step:2208/2315 train_time:134111ms step_avg:60.74ms
step:2209/2315 train_time:134172ms step_avg:60.74ms
step:2210/2315 train_time:134234ms step_avg:60.74ms
step:2211/2315 train_time:134295ms step_avg:60.74ms
step:2212/2315 train_time:134356ms step_avg:60.74ms
step:2213/2315 train_time:134416ms step_avg:60.74ms
step:2214/2315 train_time:134477ms step_avg:60.74ms
step:2215/2315 train_time:134538ms step_avg:60.74ms
step:2216/2315 train_time:134600ms step_avg:60.74ms
step:2217/2315 train_time:134660ms step_avg:60.74ms
step:2218/2315 train_time:134722ms step_avg:60.74ms
step:2219/2315 train_time:134783ms step_avg:60.74ms
step:2220/2315 train_time:134844ms step_avg:60.74ms
step:2221/2315 train_time:134905ms step_avg:60.74ms
step:2222/2315 train_time:134967ms step_avg:60.74ms
step:2223/2315 train_time:135028ms step_avg:60.74ms
step:2224/2315 train_time:135090ms step_avg:60.74ms
step:2225/2315 train_time:135151ms step_avg:60.74ms
step:2226/2315 train_time:135213ms step_avg:60.74ms
step:2227/2315 train_time:135274ms step_avg:60.74ms
step:2228/2315 train_time:135336ms step_avg:60.74ms
step:2229/2315 train_time:135397ms step_avg:60.74ms
step:2230/2315 train_time:135458ms step_avg:60.74ms
step:2231/2315 train_time:135519ms step_avg:60.74ms
step:2232/2315 train_time:135580ms step_avg:60.74ms
step:2233/2315 train_time:135640ms step_avg:60.74ms
step:2234/2315 train_time:135702ms step_avg:60.74ms
step:2235/2315 train_time:135762ms step_avg:60.74ms
step:2236/2315 train_time:135824ms step_avg:60.74ms
step:2237/2315 train_time:135884ms step_avg:60.74ms
step:2238/2315 train_time:135946ms step_avg:60.74ms
step:2239/2315 train_time:136007ms step_avg:60.74ms
step:2240/2315 train_time:136072ms step_avg:60.75ms
step:2241/2315 train_time:136130ms step_avg:60.75ms
step:2242/2315 train_time:136192ms step_avg:60.75ms
step:2243/2315 train_time:136253ms step_avg:60.75ms
step:2244/2315 train_time:136315ms step_avg:60.75ms
step:2245/2315 train_time:136377ms step_avg:60.75ms
step:2246/2315 train_time:136439ms step_avg:60.75ms
step:2247/2315 train_time:136500ms step_avg:60.75ms
step:2248/2315 train_time:136561ms step_avg:60.75ms
step:2249/2315 train_time:136622ms step_avg:60.75ms
step:2250/2315 train_time:136684ms step_avg:60.75ms
step:2250/2315 val_loss:3.2909 train_time:136746ms step_avg:60.78ms
step:2251/2315 train_time:136765ms step_avg:60.76ms
step:2252/2315 train_time:136810ms step_avg:60.75ms
step:2253/2315 train_time:136873ms step_avg:60.75ms
step:2254/2315 train_time:136937ms step_avg:60.75ms
step:2255/2315 train_time:136999ms step_avg:60.75ms
step:2256/2315 train_time:137060ms step_avg:60.75ms
step:2257/2315 train_time:137120ms step_avg:60.75ms
step:2258/2315 train_time:137182ms step_avg:60.75ms
step:2259/2315 train_time:137242ms step_avg:60.75ms
step:2260/2315 train_time:137303ms step_avg:60.75ms
step:2261/2315 train_time:137365ms step_avg:60.75ms
step:2262/2315 train_time:137426ms step_avg:60.75ms
step:2263/2315 train_time:137486ms step_avg:60.75ms
step:2264/2315 train_time:137547ms step_avg:60.75ms
step:2265/2315 train_time:137608ms step_avg:60.75ms
step:2266/2315 train_time:137669ms step_avg:60.75ms
step:2267/2315 train_time:137730ms step_avg:60.75ms
step:2268/2315 train_time:137793ms step_avg:60.76ms
step:2269/2315 train_time:137856ms step_avg:60.76ms
step:2270/2315 train_time:137919ms step_avg:60.76ms
step:2271/2315 train_time:137981ms step_avg:60.76ms
step:2272/2315 train_time:138042ms step_avg:60.76ms
step:2273/2315 train_time:138103ms step_avg:60.76ms
step:2274/2315 train_time:138164ms step_avg:60.76ms
step:2275/2315 train_time:138224ms step_avg:60.76ms
step:2276/2315 train_time:138285ms step_avg:60.76ms
step:2277/2315 train_time:138346ms step_avg:60.76ms
step:2278/2315 train_time:138407ms step_avg:60.76ms
step:2279/2315 train_time:138467ms step_avg:60.76ms
step:2280/2315 train_time:138528ms step_avg:60.76ms
step:2281/2315 train_time:138589ms step_avg:60.76ms
step:2282/2315 train_time:138650ms step_avg:60.76ms
step:2283/2315 train_time:138712ms step_avg:60.76ms
step:2284/2315 train_time:138774ms step_avg:60.76ms
step:2285/2315 train_time:138837ms step_avg:60.76ms
step:2286/2315 train_time:138899ms step_avg:60.76ms
step:2287/2315 train_time:138961ms step_avg:60.76ms
step:2288/2315 train_time:139023ms step_avg:60.76ms
step:2289/2315 train_time:139083ms step_avg:60.76ms
step:2290/2315 train_time:139144ms step_avg:60.76ms
step:2291/2315 train_time:139205ms step_avg:60.76ms
step:2292/2315 train_time:139266ms step_avg:60.76ms
step:2293/2315 train_time:139327ms step_avg:60.76ms
step:2294/2315 train_time:139388ms step_avg:60.76ms
step:2295/2315 train_time:139448ms step_avg:60.76ms
step:2296/2315 train_time:139509ms step_avg:60.76ms
step:2297/2315 train_time:139570ms step_avg:60.76ms
step:2298/2315 train_time:139631ms step_avg:60.76ms
step:2299/2315 train_time:139692ms step_avg:60.76ms
step:2300/2315 train_time:139754ms step_avg:60.76ms
step:2301/2315 train_time:139816ms step_avg:60.76ms
step:2302/2315 train_time:139878ms step_avg:60.76ms
step:2303/2315 train_time:139941ms step_avg:60.76ms
step:2304/2315 train_time:140002ms step_avg:60.76ms
step:2305/2315 train_time:140063ms step_avg:60.76ms
step:2306/2315 train_time:140124ms step_avg:60.76ms
step:2307/2315 train_time:140185ms step_avg:60.76ms
step:2308/2315 train_time:140246ms step_avg:60.77ms
step:2309/2315 train_time:140306ms step_avg:60.77ms
step:2310/2315 train_time:140367ms step_avg:60.77ms
step:2311/2315 train_time:140428ms step_avg:60.77ms
step:2312/2315 train_time:140489ms step_avg:60.77ms
step:2313/2315 train_time:140550ms step_avg:60.77ms
step:2314/2315 train_time:140611ms step_avg:60.77ms
step:2315/2315 train_time:140671ms step_avg:60.77ms
step:2315/2315 val_loss:3.2780 train_time:140734ms step_avg:60.79ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

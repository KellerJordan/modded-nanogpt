import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)


            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))



            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal
        self.group_to_param_group = {}

        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                self.group_to_param_group[param] = group

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        world_size = dist.get_world_size()
        grad = param.grad
        rank_size = grad.shape[0] // world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(), grad_slice)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']

            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
        # enable sync in the next training step for the adam optimizer
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = next(train_loader)

        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()


====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251031+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 31 15:51:48 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   37C    P0            125W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   31C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   35C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   36C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   35C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2315 train_time:79ms step_avg:78.74ms
step:2/2315 train_time:193ms step_avg:96.49ms
step:3/2315 train_time:211ms step_avg:70.38ms
step:4/2315 train_time:252ms step_avg:63.11ms
step:5/2315 train_time:310ms step_avg:62.03ms
step:6/2315 train_time:370ms step_avg:61.62ms
step:7/2315 train_time:429ms step_avg:61.23ms
step:8/2315 train_time:488ms step_avg:61.02ms
step:9/2315 train_time:548ms step_avg:60.89ms
step:10/2315 train_time:608ms step_avg:60.79ms
step:11/2315 train_time:667ms step_avg:60.66ms
step:12/2315 train_time:728ms step_avg:60.65ms
step:13/2315 train_time:787ms step_avg:60.52ms
step:14/2315 train_time:847ms step_avg:60.48ms
step:15/2315 train_time:906ms step_avg:60.39ms
step:16/2315 train_time:966ms step_avg:60.34ms
step:17/2315 train_time:1026ms step_avg:60.36ms
step:18/2315 train_time:1090ms step_avg:60.53ms
step:19/2315 train_time:1154ms step_avg:60.74ms
step:20/2315 train_time:1217ms step_avg:60.84ms
step:21/2315 train_time:1277ms step_avg:60.79ms
step:22/2315 train_time:1338ms step_avg:60.80ms
step:23/2315 train_time:1397ms step_avg:60.75ms
step:24/2315 train_time:1458ms step_avg:60.74ms
step:25/2315 train_time:1518ms step_avg:60.71ms
step:26/2315 train_time:1578ms step_avg:60.69ms
step:27/2315 train_time:1638ms step_avg:60.66ms
step:28/2315 train_time:1698ms step_avg:60.65ms
step:29/2315 train_time:1758ms step_avg:60.61ms
step:30/2315 train_time:1818ms step_avg:60.61ms
step:31/2315 train_time:1878ms step_avg:60.59ms
step:32/2315 train_time:1938ms step_avg:60.57ms
step:33/2315 train_time:1999ms step_avg:60.58ms
step:34/2315 train_time:2061ms step_avg:60.62ms
step:35/2315 train_time:2122ms step_avg:60.64ms
step:36/2315 train_time:2184ms step_avg:60.66ms
step:37/2315 train_time:2245ms step_avg:60.68ms
step:38/2315 train_time:2307ms step_avg:60.71ms
step:39/2315 train_time:2367ms step_avg:60.70ms
step:40/2315 train_time:2428ms step_avg:60.71ms
step:41/2315 train_time:2488ms step_avg:60.69ms
step:42/2315 train_time:2549ms step_avg:60.68ms
step:43/2315 train_time:2608ms step_avg:60.66ms
step:44/2315 train_time:2668ms step_avg:60.65ms
step:45/2315 train_time:2728ms step_avg:60.63ms
step:46/2315 train_time:2788ms step_avg:60.62ms
step:47/2315 train_time:2848ms step_avg:60.61ms
step:48/2315 train_time:2910ms step_avg:60.63ms
step:49/2315 train_time:2969ms step_avg:60.58ms
step:50/2315 train_time:3029ms step_avg:60.58ms
step:51/2315 train_time:3089ms step_avg:60.57ms
step:52/2315 train_time:3150ms step_avg:60.57ms
step:53/2315 train_time:3209ms step_avg:60.55ms
step:54/2315 train_time:3269ms step_avg:60.54ms
step:55/2315 train_time:3330ms step_avg:60.54ms
step:56/2315 train_time:3391ms step_avg:60.54ms
step:57/2315 train_time:3451ms step_avg:60.54ms
step:58/2315 train_time:3512ms step_avg:60.55ms
step:59/2315 train_time:3571ms step_avg:60.52ms
step:60/2315 train_time:3631ms step_avg:60.51ms
step:61/2315 train_time:3691ms step_avg:60.51ms
step:62/2315 train_time:3751ms step_avg:60.50ms
step:63/2315 train_time:3811ms step_avg:60.49ms
step:64/2315 train_time:3871ms step_avg:60.48ms
step:65/2315 train_time:3930ms step_avg:60.47ms
step:66/2315 train_time:3990ms step_avg:60.46ms
step:67/2315 train_time:4050ms step_avg:60.44ms
step:68/2315 train_time:4110ms step_avg:60.44ms
step:69/2315 train_time:4170ms step_avg:60.43ms
step:70/2315 train_time:4230ms step_avg:60.43ms
step:71/2315 train_time:4290ms step_avg:60.42ms
step:72/2315 train_time:4351ms step_avg:60.43ms
step:73/2315 train_time:4411ms step_avg:60.42ms
step:74/2315 train_time:4471ms step_avg:60.41ms
step:75/2315 train_time:4530ms step_avg:60.40ms
step:76/2315 train_time:4590ms step_avg:60.39ms
step:77/2315 train_time:4649ms step_avg:60.38ms
step:78/2315 train_time:4710ms step_avg:60.39ms
step:79/2315 train_time:4770ms step_avg:60.38ms
step:80/2315 train_time:4830ms step_avg:60.37ms
step:81/2315 train_time:4891ms step_avg:60.38ms
step:82/2315 train_time:4950ms step_avg:60.37ms
step:83/2315 train_time:5010ms step_avg:60.36ms
step:84/2315 train_time:5070ms step_avg:60.36ms
step:85/2315 train_time:5129ms step_avg:60.34ms
step:86/2315 train_time:5190ms step_avg:60.35ms
step:87/2315 train_time:5250ms step_avg:60.34ms
step:88/2315 train_time:5310ms step_avg:60.34ms
step:89/2315 train_time:5369ms step_avg:60.33ms
step:90/2315 train_time:5430ms step_avg:60.33ms
step:91/2315 train_time:5489ms step_avg:60.32ms
step:92/2315 train_time:5550ms step_avg:60.32ms
step:93/2315 train_time:5609ms step_avg:60.32ms
step:94/2315 train_time:5669ms step_avg:60.31ms
step:95/2315 train_time:5729ms step_avg:60.30ms
step:96/2315 train_time:5789ms step_avg:60.30ms
step:97/2315 train_time:5849ms step_avg:60.30ms
step:98/2315 train_time:5909ms step_avg:60.30ms
step:99/2315 train_time:5968ms step_avg:60.29ms
step:100/2315 train_time:6028ms step_avg:60.28ms
step:101/2315 train_time:6088ms step_avg:60.27ms
step:102/2315 train_time:6148ms step_avg:60.28ms
step:103/2315 train_time:6208ms step_avg:60.27ms
step:104/2315 train_time:6268ms step_avg:60.27ms
step:105/2315 train_time:6328ms step_avg:60.26ms
step:106/2315 train_time:6388ms step_avg:60.27ms
step:107/2315 train_time:6449ms step_avg:60.27ms
step:108/2315 train_time:6509ms step_avg:60.27ms
step:109/2315 train_time:6568ms step_avg:60.26ms
step:110/2315 train_time:6628ms step_avg:60.26ms
step:111/2315 train_time:6687ms step_avg:60.25ms
step:112/2315 train_time:6748ms step_avg:60.25ms
step:113/2315 train_time:6808ms step_avg:60.25ms
step:114/2315 train_time:6868ms step_avg:60.24ms
step:115/2315 train_time:6928ms step_avg:60.24ms
step:116/2315 train_time:6988ms step_avg:60.24ms
step:117/2315 train_time:7048ms step_avg:60.24ms
step:118/2315 train_time:7108ms step_avg:60.24ms
step:119/2315 train_time:7168ms step_avg:60.23ms
step:120/2315 train_time:7228ms step_avg:60.24ms
step:121/2315 train_time:7288ms step_avg:60.23ms
step:122/2315 train_time:7348ms step_avg:60.23ms
step:123/2315 train_time:7409ms step_avg:60.24ms
step:124/2315 train_time:7469ms step_avg:60.23ms
step:125/2315 train_time:7528ms step_avg:60.23ms
step:126/2315 train_time:7588ms step_avg:60.22ms
step:127/2315 train_time:7647ms step_avg:60.22ms
step:128/2315 train_time:7707ms step_avg:60.21ms
step:129/2315 train_time:7767ms step_avg:60.21ms
step:130/2315 train_time:7828ms step_avg:60.21ms
step:131/2315 train_time:7887ms step_avg:60.21ms
step:132/2315 train_time:7948ms step_avg:60.21ms
step:133/2315 train_time:8009ms step_avg:60.22ms
step:134/2315 train_time:8069ms step_avg:60.21ms
step:135/2315 train_time:8129ms step_avg:60.21ms
step:136/2315 train_time:8189ms step_avg:60.21ms
step:137/2315 train_time:8249ms step_avg:60.21ms
step:138/2315 train_time:8310ms step_avg:60.21ms
step:139/2315 train_time:8369ms step_avg:60.21ms
step:140/2315 train_time:8429ms step_avg:60.21ms
step:141/2315 train_time:8489ms step_avg:60.21ms
step:142/2315 train_time:8549ms step_avg:60.21ms
step:143/2315 train_time:8608ms step_avg:60.20ms
step:144/2315 train_time:8668ms step_avg:60.20ms
step:145/2315 train_time:8728ms step_avg:60.19ms
step:146/2315 train_time:8788ms step_avg:60.19ms
step:147/2315 train_time:8848ms step_avg:60.19ms
step:148/2315 train_time:8908ms step_avg:60.19ms
step:149/2315 train_time:8968ms step_avg:60.19ms
step:150/2315 train_time:9028ms step_avg:60.18ms
step:151/2315 train_time:9087ms step_avg:60.18ms
step:152/2315 train_time:9148ms step_avg:60.18ms
step:153/2315 train_time:9208ms step_avg:60.18ms
step:154/2315 train_time:9268ms step_avg:60.18ms
step:155/2315 train_time:9328ms step_avg:60.18ms
step:156/2315 train_time:9388ms step_avg:60.18ms
step:157/2315 train_time:9449ms step_avg:60.18ms
step:158/2315 train_time:9509ms step_avg:60.18ms
step:159/2315 train_time:9568ms step_avg:60.18ms
step:160/2315 train_time:9628ms step_avg:60.18ms
step:161/2315 train_time:9688ms step_avg:60.17ms
step:162/2315 train_time:9748ms step_avg:60.17ms
step:163/2315 train_time:9807ms step_avg:60.17ms
step:164/2315 train_time:9868ms step_avg:60.17ms
step:165/2315 train_time:9928ms step_avg:60.17ms
step:166/2315 train_time:9988ms step_avg:60.17ms
step:167/2315 train_time:10047ms step_avg:60.16ms
step:168/2315 train_time:10108ms step_avg:60.16ms
step:169/2315 train_time:10168ms step_avg:60.16ms
step:170/2315 train_time:10228ms step_avg:60.16ms
step:171/2315 train_time:10288ms step_avg:60.17ms
step:172/2315 train_time:10348ms step_avg:60.16ms
step:173/2315 train_time:10408ms step_avg:60.16ms
step:174/2315 train_time:10468ms step_avg:60.16ms
step:175/2315 train_time:10528ms step_avg:60.16ms
step:176/2315 train_time:10589ms step_avg:60.16ms
step:177/2315 train_time:10648ms step_avg:60.16ms
step:178/2315 train_time:10708ms step_avg:60.16ms
step:179/2315 train_time:10767ms step_avg:60.15ms
step:180/2315 train_time:10828ms step_avg:60.15ms
step:181/2315 train_time:10887ms step_avg:60.15ms
step:182/2315 train_time:10948ms step_avg:60.15ms
step:183/2315 train_time:11008ms step_avg:60.15ms
step:184/2315 train_time:11068ms step_avg:60.15ms
step:185/2315 train_time:11128ms step_avg:60.15ms
step:186/2315 train_time:11188ms step_avg:60.15ms
step:187/2315 train_time:11248ms step_avg:60.15ms
step:188/2315 train_time:11310ms step_avg:60.16ms
step:189/2315 train_time:11367ms step_avg:60.14ms
step:190/2315 train_time:11427ms step_avg:60.14ms
step:191/2315 train_time:11487ms step_avg:60.14ms
step:192/2315 train_time:11548ms step_avg:60.14ms
step:193/2315 train_time:11607ms step_avg:60.14ms
step:194/2315 train_time:11667ms step_avg:60.14ms
step:195/2315 train_time:11727ms step_avg:60.14ms
step:196/2315 train_time:11787ms step_avg:60.14ms
step:197/2315 train_time:11847ms step_avg:60.14ms
step:198/2315 train_time:11908ms step_avg:60.14ms
step:199/2315 train_time:11968ms step_avg:60.14ms
step:200/2315 train_time:12028ms step_avg:60.14ms
step:201/2315 train_time:12087ms step_avg:60.14ms
step:202/2315 train_time:12148ms step_avg:60.14ms
step:203/2315 train_time:12207ms step_avg:60.13ms
step:204/2315 train_time:12267ms step_avg:60.13ms
step:205/2315 train_time:12327ms step_avg:60.13ms
step:206/2315 train_time:12387ms step_avg:60.13ms
step:207/2315 train_time:12447ms step_avg:60.13ms
step:208/2315 train_time:12508ms step_avg:60.14ms
step:209/2315 train_time:12568ms step_avg:60.13ms
step:210/2315 train_time:12628ms step_avg:60.13ms
step:211/2315 train_time:12687ms step_avg:60.13ms
step:212/2315 train_time:12748ms step_avg:60.13ms
step:213/2315 train_time:12808ms step_avg:60.13ms
step:214/2315 train_time:12868ms step_avg:60.13ms
step:215/2315 train_time:12928ms step_avg:60.13ms
step:216/2315 train_time:12988ms step_avg:60.13ms
step:217/2315 train_time:13047ms step_avg:60.13ms
step:218/2315 train_time:13107ms step_avg:60.13ms
step:219/2315 train_time:13167ms step_avg:60.12ms
step:220/2315 train_time:13228ms step_avg:60.13ms
step:221/2315 train_time:13288ms step_avg:60.13ms
step:222/2315 train_time:13348ms step_avg:60.13ms
step:223/2315 train_time:13408ms step_avg:60.12ms
step:224/2315 train_time:13468ms step_avg:60.13ms
step:225/2315 train_time:13528ms step_avg:60.12ms
step:226/2315 train_time:13589ms step_avg:60.13ms
step:227/2315 train_time:13648ms step_avg:60.12ms
step:228/2315 train_time:13708ms step_avg:60.12ms
step:229/2315 train_time:13768ms step_avg:60.12ms
step:230/2315 train_time:13828ms step_avg:60.12ms
step:231/2315 train_time:13888ms step_avg:60.12ms
step:232/2315 train_time:13948ms step_avg:60.12ms
step:233/2315 train_time:14007ms step_avg:60.12ms
step:234/2315 train_time:14067ms step_avg:60.12ms
step:235/2315 train_time:14127ms step_avg:60.11ms
step:236/2315 train_time:14187ms step_avg:60.12ms
step:237/2315 train_time:14247ms step_avg:60.12ms
step:238/2315 train_time:14308ms step_avg:60.12ms
step:239/2315 train_time:14367ms step_avg:60.11ms
step:240/2315 train_time:14427ms step_avg:60.11ms
step:241/2315 train_time:14487ms step_avg:60.11ms
step:242/2315 train_time:14548ms step_avg:60.11ms
step:243/2315 train_time:14607ms step_avg:60.11ms
step:244/2315 train_time:14668ms step_avg:60.11ms
step:245/2315 train_time:14727ms step_avg:60.11ms
step:246/2315 train_time:14787ms step_avg:60.11ms
step:247/2315 train_time:14847ms step_avg:60.11ms
step:248/2315 train_time:14908ms step_avg:60.11ms
step:249/2315 train_time:14967ms step_avg:60.11ms
step:250/2315 train_time:15027ms step_avg:60.11ms
step:250/2315 val_loss:4.0686 train_time:15089ms step_avg:60.36ms
step:251/2315 train_time:15107ms step_avg:60.19ms
step:252/2315 train_time:15152ms step_avg:60.13ms
step:253/2315 train_time:15215ms step_avg:60.14ms
step:254/2315 train_time:15279ms step_avg:60.15ms
step:255/2315 train_time:15339ms step_avg:60.15ms
step:256/2315 train_time:15402ms step_avg:60.16ms
step:257/2315 train_time:15462ms step_avg:60.16ms
step:258/2315 train_time:15522ms step_avg:60.16ms
step:259/2315 train_time:15581ms step_avg:60.16ms
step:260/2315 train_time:15643ms step_avg:60.17ms
step:261/2315 train_time:15699ms step_avg:60.15ms
step:262/2315 train_time:15759ms step_avg:60.15ms
step:263/2315 train_time:15818ms step_avg:60.14ms
step:264/2315 train_time:15877ms step_avg:60.14ms
step:265/2315 train_time:15936ms step_avg:60.14ms
step:266/2315 train_time:15995ms step_avg:60.13ms
step:267/2315 train_time:16055ms step_avg:60.13ms
step:268/2315 train_time:16117ms step_avg:60.14ms
step:269/2315 train_time:16178ms step_avg:60.14ms
step:270/2315 train_time:16240ms step_avg:60.15ms
step:271/2315 train_time:16301ms step_avg:60.15ms
step:272/2315 train_time:16362ms step_avg:60.16ms
step:273/2315 train_time:16422ms step_avg:60.15ms
step:274/2315 train_time:16483ms step_avg:60.16ms
step:275/2315 train_time:16543ms step_avg:60.16ms
step:276/2315 train_time:16603ms step_avg:60.16ms
step:277/2315 train_time:16662ms step_avg:60.15ms
step:278/2315 train_time:16722ms step_avg:60.15ms
step:279/2315 train_time:16781ms step_avg:60.15ms
step:280/2315 train_time:16841ms step_avg:60.15ms
step:281/2315 train_time:16901ms step_avg:60.14ms
step:282/2315 train_time:16960ms step_avg:60.14ms
step:283/2315 train_time:17020ms step_avg:60.14ms
step:284/2315 train_time:17080ms step_avg:60.14ms
step:285/2315 train_time:17141ms step_avg:60.14ms
step:286/2315 train_time:17203ms step_avg:60.15ms
step:287/2315 train_time:17264ms step_avg:60.15ms
step:288/2315 train_time:17325ms step_avg:60.16ms
step:289/2315 train_time:17386ms step_avg:60.16ms
step:290/2315 train_time:17449ms step_avg:60.17ms
step:291/2315 train_time:17507ms step_avg:60.16ms
step:292/2315 train_time:17567ms step_avg:60.16ms
step:293/2315 train_time:17626ms step_avg:60.16ms
step:294/2315 train_time:17686ms step_avg:60.16ms
step:295/2315 train_time:17745ms step_avg:60.15ms
step:296/2315 train_time:17805ms step_avg:60.15ms
step:297/2315 train_time:17865ms step_avg:60.15ms
step:298/2315 train_time:17925ms step_avg:60.15ms
step:299/2315 train_time:17984ms step_avg:60.15ms
step:300/2315 train_time:18045ms step_avg:60.15ms
step:301/2315 train_time:18105ms step_avg:60.15ms
step:302/2315 train_time:18166ms step_avg:60.15ms
step:303/2315 train_time:18225ms step_avg:60.15ms
step:304/2315 train_time:18287ms step_avg:60.15ms
step:305/2315 train_time:18347ms step_avg:60.15ms
step:306/2315 train_time:18408ms step_avg:60.16ms
step:307/2315 train_time:18468ms step_avg:60.16ms
step:308/2315 train_time:18528ms step_avg:60.16ms
step:309/2315 train_time:18588ms step_avg:60.15ms
step:310/2315 train_time:18649ms step_avg:60.16ms
step:311/2315 train_time:18707ms step_avg:60.15ms
step:312/2315 train_time:18768ms step_avg:60.15ms
step:313/2315 train_time:18827ms step_avg:60.15ms
step:314/2315 train_time:18889ms step_avg:60.15ms
step:315/2315 train_time:18948ms step_avg:60.15ms
step:316/2315 train_time:19008ms step_avg:60.15ms
step:317/2315 train_time:19068ms step_avg:60.15ms
step:318/2315 train_time:19128ms step_avg:60.15ms
step:319/2315 train_time:19188ms step_avg:60.15ms
step:320/2315 train_time:19248ms step_avg:60.15ms
step:321/2315 train_time:19308ms step_avg:60.15ms
step:322/2315 train_time:19369ms step_avg:60.15ms
step:323/2315 train_time:19429ms step_avg:60.15ms
step:324/2315 train_time:19489ms step_avg:60.15ms
step:325/2315 train_time:19549ms step_avg:60.15ms
step:326/2315 train_time:19609ms step_avg:60.15ms
step:327/2315 train_time:19668ms step_avg:60.15ms
step:328/2315 train_time:19728ms step_avg:60.15ms
step:329/2315 train_time:19787ms step_avg:60.14ms
step:330/2315 train_time:19848ms step_avg:60.14ms
step:331/2315 train_time:19907ms step_avg:60.14ms
step:332/2315 train_time:19967ms step_avg:60.14ms
step:333/2315 train_time:20028ms step_avg:60.14ms
step:334/2315 train_time:20089ms step_avg:60.15ms
step:335/2315 train_time:20148ms step_avg:60.14ms
step:336/2315 train_time:20209ms step_avg:60.14ms
step:337/2315 train_time:20269ms step_avg:60.14ms
step:338/2315 train_time:20329ms step_avg:60.15ms
step:339/2315 train_time:20389ms step_avg:60.14ms
step:340/2315 train_time:20449ms step_avg:60.14ms
step:341/2315 train_time:20509ms step_avg:60.14ms
step:342/2315 train_time:20569ms step_avg:60.14ms
step:343/2315 train_time:20629ms step_avg:60.14ms
step:344/2315 train_time:20689ms step_avg:60.14ms
step:345/2315 train_time:20749ms step_avg:60.14ms
step:346/2315 train_time:20809ms step_avg:60.14ms
step:347/2315 train_time:20868ms step_avg:60.14ms
step:348/2315 train_time:20929ms step_avg:60.14ms
step:349/2315 train_time:20988ms step_avg:60.14ms
step:350/2315 train_time:21049ms step_avg:60.14ms
step:351/2315 train_time:21108ms step_avg:60.14ms
step:352/2315 train_time:21169ms step_avg:60.14ms
step:353/2315 train_time:21229ms step_avg:60.14ms
step:354/2315 train_time:21290ms step_avg:60.14ms
step:355/2315 train_time:21350ms step_avg:60.14ms
step:356/2315 train_time:21410ms step_avg:60.14ms
step:357/2315 train_time:21470ms step_avg:60.14ms
step:358/2315 train_time:21529ms step_avg:60.14ms
step:359/2315 train_time:21589ms step_avg:60.14ms
step:360/2315 train_time:21649ms step_avg:60.14ms
step:361/2315 train_time:21708ms step_avg:60.13ms
step:362/2315 train_time:21769ms step_avg:60.13ms
step:363/2315 train_time:21828ms step_avg:60.13ms
step:364/2315 train_time:21889ms step_avg:60.13ms
step:365/2315 train_time:21948ms step_avg:60.13ms
step:366/2315 train_time:22008ms step_avg:60.13ms
step:367/2315 train_time:22068ms step_avg:60.13ms
step:368/2315 train_time:22128ms step_avg:60.13ms
step:369/2315 train_time:22188ms step_avg:60.13ms
step:370/2315 train_time:22249ms step_avg:60.13ms
step:371/2315 train_time:22308ms step_avg:60.13ms
step:372/2315 train_time:22369ms step_avg:60.13ms
step:373/2315 train_time:22428ms step_avg:60.13ms
step:374/2315 train_time:22488ms step_avg:60.13ms
step:375/2315 train_time:22550ms step_avg:60.13ms
step:376/2315 train_time:22608ms step_avg:60.13ms
step:377/2315 train_time:22667ms step_avg:60.13ms
step:378/2315 train_time:22727ms step_avg:60.13ms
step:379/2315 train_time:22787ms step_avg:60.13ms
step:380/2315 train_time:22847ms step_avg:60.12ms
step:381/2315 train_time:22907ms step_avg:60.12ms
step:382/2315 train_time:22967ms step_avg:60.12ms
step:383/2315 train_time:23027ms step_avg:60.12ms
step:384/2315 train_time:23087ms step_avg:60.12ms
step:385/2315 train_time:23147ms step_avg:60.12ms
step:386/2315 train_time:23207ms step_avg:60.12ms
step:387/2315 train_time:23267ms step_avg:60.12ms
step:388/2315 train_time:23327ms step_avg:60.12ms
step:389/2315 train_time:23387ms step_avg:60.12ms
step:390/2315 train_time:23448ms step_avg:60.12ms
step:391/2315 train_time:23507ms step_avg:60.12ms
step:392/2315 train_time:23567ms step_avg:60.12ms
step:393/2315 train_time:23626ms step_avg:60.12ms
step:394/2315 train_time:23687ms step_avg:60.12ms
step:395/2315 train_time:23746ms step_avg:60.12ms
step:396/2315 train_time:23807ms step_avg:60.12ms
step:397/2315 train_time:23866ms step_avg:60.12ms
step:398/2315 train_time:23926ms step_avg:60.12ms
step:399/2315 train_time:23986ms step_avg:60.11ms
step:400/2315 train_time:24046ms step_avg:60.12ms
step:401/2315 train_time:24106ms step_avg:60.12ms
step:402/2315 train_time:24166ms step_avg:60.12ms
step:403/2315 train_time:24226ms step_avg:60.11ms
step:404/2315 train_time:24287ms step_avg:60.12ms
step:405/2315 train_time:24347ms step_avg:60.12ms
step:406/2315 train_time:24407ms step_avg:60.12ms
step:407/2315 train_time:24467ms step_avg:60.12ms
step:408/2315 train_time:24527ms step_avg:60.12ms
step:409/2315 train_time:24587ms step_avg:60.11ms
step:410/2315 train_time:24648ms step_avg:60.12ms
step:411/2315 train_time:24708ms step_avg:60.12ms
step:412/2315 train_time:24768ms step_avg:60.12ms
step:413/2315 train_time:24827ms step_avg:60.11ms
step:414/2315 train_time:24888ms step_avg:60.11ms
step:415/2315 train_time:24948ms step_avg:60.11ms
step:416/2315 train_time:25008ms step_avg:60.12ms
step:417/2315 train_time:25068ms step_avg:60.11ms
step:418/2315 train_time:25128ms step_avg:60.12ms
step:419/2315 train_time:25188ms step_avg:60.12ms
step:420/2315 train_time:25249ms step_avg:60.12ms
step:421/2315 train_time:25308ms step_avg:60.11ms
step:422/2315 train_time:25369ms step_avg:60.12ms
step:423/2315 train_time:25429ms step_avg:60.12ms
step:424/2315 train_time:25489ms step_avg:60.12ms
step:425/2315 train_time:25549ms step_avg:60.11ms
step:426/2315 train_time:25609ms step_avg:60.12ms
step:427/2315 train_time:25668ms step_avg:60.11ms
step:428/2315 train_time:25729ms step_avg:60.11ms
step:429/2315 train_time:25788ms step_avg:60.11ms
step:430/2315 train_time:25848ms step_avg:60.11ms
step:431/2315 train_time:25908ms step_avg:60.11ms
step:432/2315 train_time:25968ms step_avg:60.11ms
step:433/2315 train_time:26028ms step_avg:60.11ms
step:434/2315 train_time:26088ms step_avg:60.11ms
step:435/2315 train_time:26147ms step_avg:60.11ms
step:436/2315 train_time:26208ms step_avg:60.11ms
step:437/2315 train_time:26267ms step_avg:60.11ms
step:438/2315 train_time:26328ms step_avg:60.11ms
step:439/2315 train_time:26387ms step_avg:60.11ms
step:440/2315 train_time:26448ms step_avg:60.11ms
step:441/2315 train_time:26507ms step_avg:60.11ms
step:442/2315 train_time:26568ms step_avg:60.11ms
step:443/2315 train_time:26627ms step_avg:60.11ms
step:444/2315 train_time:26688ms step_avg:60.11ms
step:445/2315 train_time:26747ms step_avg:60.11ms
step:446/2315 train_time:26807ms step_avg:60.11ms
step:447/2315 train_time:26868ms step_avg:60.11ms
step:448/2315 train_time:26928ms step_avg:60.11ms
step:449/2315 train_time:26987ms step_avg:60.11ms
step:450/2315 train_time:27048ms step_avg:60.11ms
step:451/2315 train_time:27107ms step_avg:60.10ms
step:452/2315 train_time:27168ms step_avg:60.11ms
step:453/2315 train_time:27227ms step_avg:60.10ms
step:454/2315 train_time:27287ms step_avg:60.10ms
step:455/2315 train_time:27347ms step_avg:60.10ms
step:456/2315 train_time:27407ms step_avg:60.10ms
step:457/2315 train_time:27468ms step_avg:60.10ms
step:458/2315 train_time:27527ms step_avg:60.10ms
step:459/2315 train_time:27588ms step_avg:60.10ms
step:460/2315 train_time:27648ms step_avg:60.11ms
step:461/2315 train_time:27708ms step_avg:60.10ms
step:462/2315 train_time:27768ms step_avg:60.10ms
step:463/2315 train_time:27828ms step_avg:60.10ms
step:464/2315 train_time:27889ms step_avg:60.10ms
step:465/2315 train_time:27949ms step_avg:60.10ms
step:466/2315 train_time:28009ms step_avg:60.10ms
step:467/2315 train_time:28068ms step_avg:60.10ms
step:468/2315 train_time:28129ms step_avg:60.10ms
step:469/2315 train_time:28189ms step_avg:60.10ms
step:470/2315 train_time:28249ms step_avg:60.10ms
step:471/2315 train_time:28309ms step_avg:60.10ms
step:472/2315 train_time:28368ms step_avg:60.10ms
step:473/2315 train_time:28428ms step_avg:60.10ms
step:474/2315 train_time:28488ms step_avg:60.10ms
step:475/2315 train_time:28548ms step_avg:60.10ms
step:476/2315 train_time:28608ms step_avg:60.10ms
step:477/2315 train_time:28667ms step_avg:60.10ms
step:478/2315 train_time:28727ms step_avg:60.10ms
step:479/2315 train_time:28787ms step_avg:60.10ms
step:480/2315 train_time:28847ms step_avg:60.10ms
step:481/2315 train_time:28906ms step_avg:60.10ms
step:482/2315 train_time:28968ms step_avg:60.10ms
step:483/2315 train_time:29026ms step_avg:60.10ms
step:484/2315 train_time:29086ms step_avg:60.10ms
step:485/2315 train_time:29146ms step_avg:60.10ms
step:486/2315 train_time:29207ms step_avg:60.10ms
step:487/2315 train_time:29267ms step_avg:60.10ms
step:488/2315 train_time:29327ms step_avg:60.10ms
step:489/2315 train_time:29386ms step_avg:60.09ms
step:490/2315 train_time:29447ms step_avg:60.10ms
step:491/2315 train_time:29507ms step_avg:60.10ms
step:492/2315 train_time:29568ms step_avg:60.10ms
step:493/2315 train_time:29627ms step_avg:60.10ms
step:494/2315 train_time:29687ms step_avg:60.10ms
step:495/2315 train_time:29748ms step_avg:60.10ms
step:496/2315 train_time:29808ms step_avg:60.10ms
step:497/2315 train_time:29868ms step_avg:60.10ms
step:498/2315 train_time:29928ms step_avg:60.10ms
step:499/2315 train_time:29988ms step_avg:60.10ms
step:500/2315 train_time:30048ms step_avg:60.10ms
step:500/2315 val_loss:3.8121 train_time:30109ms step_avg:60.22ms
step:501/2315 train_time:30127ms step_avg:60.13ms
step:502/2315 train_time:30170ms step_avg:60.10ms
step:503/2315 train_time:30233ms step_avg:60.10ms
step:504/2315 train_time:30295ms step_avg:60.11ms
step:505/2315 train_time:30355ms step_avg:60.11ms
step:506/2315 train_time:30415ms step_avg:60.11ms
step:507/2315 train_time:30475ms step_avg:60.11ms
step:508/2315 train_time:30534ms step_avg:60.11ms
step:509/2315 train_time:30594ms step_avg:60.11ms
step:510/2315 train_time:30653ms step_avg:60.10ms
step:511/2315 train_time:30712ms step_avg:60.10ms
step:512/2315 train_time:30771ms step_avg:60.10ms
step:513/2315 train_time:30830ms step_avg:60.10ms
step:514/2315 train_time:30891ms step_avg:60.10ms
step:515/2315 train_time:30950ms step_avg:60.10ms
step:516/2315 train_time:31010ms step_avg:60.10ms
step:517/2315 train_time:31070ms step_avg:60.10ms
step:518/2315 train_time:31132ms step_avg:60.10ms
step:519/2315 train_time:31193ms step_avg:60.10ms
step:520/2315 train_time:31254ms step_avg:60.10ms
step:521/2315 train_time:31315ms step_avg:60.11ms
step:522/2315 train_time:31375ms step_avg:60.11ms
step:523/2315 train_time:31435ms step_avg:60.10ms
step:524/2315 train_time:31495ms step_avg:60.11ms
step:525/2315 train_time:31554ms step_avg:60.10ms
step:526/2315 train_time:31614ms step_avg:60.10ms
step:527/2315 train_time:31673ms step_avg:60.10ms
step:528/2315 train_time:31732ms step_avg:60.10ms
step:529/2315 train_time:31792ms step_avg:60.10ms
step:530/2315 train_time:31852ms step_avg:60.10ms
step:531/2315 train_time:31911ms step_avg:60.10ms
step:532/2315 train_time:31971ms step_avg:60.10ms
step:533/2315 train_time:32032ms step_avg:60.10ms
step:534/2315 train_time:32093ms step_avg:60.10ms
step:535/2315 train_time:32153ms step_avg:60.10ms
step:536/2315 train_time:32215ms step_avg:60.10ms
step:537/2315 train_time:32274ms step_avg:60.10ms
step:538/2315 train_time:32335ms step_avg:60.10ms
step:539/2315 train_time:32395ms step_avg:60.10ms
step:540/2315 train_time:32455ms step_avg:60.10ms
step:541/2315 train_time:32514ms step_avg:60.10ms
step:542/2315 train_time:32575ms step_avg:60.10ms
step:543/2315 train_time:32633ms step_avg:60.10ms
step:544/2315 train_time:32693ms step_avg:60.10ms
step:545/2315 train_time:32752ms step_avg:60.10ms
step:546/2315 train_time:32812ms step_avg:60.10ms
step:547/2315 train_time:32871ms step_avg:60.09ms
step:548/2315 train_time:32931ms step_avg:60.09ms
step:549/2315 train_time:32991ms step_avg:60.09ms
step:550/2315 train_time:33052ms step_avg:60.09ms
step:551/2315 train_time:33112ms step_avg:60.09ms
step:552/2315 train_time:33173ms step_avg:60.10ms
step:553/2315 train_time:33233ms step_avg:60.10ms
step:554/2315 train_time:33294ms step_avg:60.10ms
step:555/2315 train_time:33354ms step_avg:60.10ms
step:556/2315 train_time:33414ms step_avg:60.10ms
step:557/2315 train_time:33474ms step_avg:60.10ms
step:558/2315 train_time:33534ms step_avg:60.10ms
step:559/2315 train_time:33593ms step_avg:60.10ms
step:560/2315 train_time:33653ms step_avg:60.09ms
step:561/2315 train_time:33712ms step_avg:60.09ms
step:562/2315 train_time:33772ms step_avg:60.09ms
step:563/2315 train_time:33831ms step_avg:60.09ms
step:564/2315 train_time:33892ms step_avg:60.09ms
step:565/2315 train_time:33952ms step_avg:60.09ms
step:566/2315 train_time:34012ms step_avg:60.09ms
step:567/2315 train_time:34072ms step_avg:60.09ms
step:568/2315 train_time:34133ms step_avg:60.09ms
step:569/2315 train_time:34193ms step_avg:60.09ms
step:570/2315 train_time:34253ms step_avg:60.09ms
step:571/2315 train_time:34314ms step_avg:60.09ms
step:572/2315 train_time:34374ms step_avg:60.09ms
step:573/2315 train_time:34434ms step_avg:60.09ms
step:574/2315 train_time:34495ms step_avg:60.10ms
step:575/2315 train_time:34554ms step_avg:60.09ms
step:576/2315 train_time:34614ms step_avg:60.09ms
step:577/2315 train_time:34674ms step_avg:60.09ms
step:578/2315 train_time:34734ms step_avg:60.09ms
step:579/2315 train_time:34793ms step_avg:60.09ms
step:580/2315 train_time:34852ms step_avg:60.09ms
step:581/2315 train_time:34912ms step_avg:60.09ms
step:582/2315 train_time:34972ms step_avg:60.09ms
step:583/2315 train_time:35032ms step_avg:60.09ms
step:584/2315 train_time:35092ms step_avg:60.09ms
step:585/2315 train_time:35153ms step_avg:60.09ms
step:586/2315 train_time:35214ms step_avg:60.09ms
step:587/2315 train_time:35274ms step_avg:60.09ms
step:588/2315 train_time:35334ms step_avg:60.09ms
step:589/2315 train_time:35394ms step_avg:60.09ms
step:590/2315 train_time:35454ms step_avg:60.09ms
step:591/2315 train_time:35514ms step_avg:60.09ms
step:592/2315 train_time:35574ms step_avg:60.09ms
step:593/2315 train_time:35634ms step_avg:60.09ms
step:594/2315 train_time:35693ms step_avg:60.09ms
step:595/2315 train_time:35753ms step_avg:60.09ms
step:596/2315 train_time:35813ms step_avg:60.09ms
step:597/2315 train_time:35873ms step_avg:60.09ms
step:598/2315 train_time:35933ms step_avg:60.09ms
step:599/2315 train_time:35993ms step_avg:60.09ms
step:600/2315 train_time:36053ms step_avg:60.09ms
step:601/2315 train_time:36113ms step_avg:60.09ms
step:602/2315 train_time:36173ms step_avg:60.09ms
step:603/2315 train_time:36233ms step_avg:60.09ms
step:604/2315 train_time:36294ms step_avg:60.09ms
step:605/2315 train_time:36353ms step_avg:60.09ms
step:606/2315 train_time:36413ms step_avg:60.09ms
step:607/2315 train_time:36474ms step_avg:60.09ms
step:608/2315 train_time:36534ms step_avg:60.09ms
step:609/2315 train_time:36594ms step_avg:60.09ms
step:610/2315 train_time:36654ms step_avg:60.09ms
step:611/2315 train_time:36713ms step_avg:60.09ms
step:612/2315 train_time:36773ms step_avg:60.09ms
step:613/2315 train_time:36832ms step_avg:60.09ms
step:614/2315 train_time:36893ms step_avg:60.09ms
step:615/2315 train_time:36952ms step_avg:60.09ms
step:616/2315 train_time:37013ms step_avg:60.09ms
step:617/2315 train_time:37073ms step_avg:60.09ms
step:618/2315 train_time:37133ms step_avg:60.09ms
step:619/2315 train_time:37194ms step_avg:60.09ms
step:620/2315 train_time:37254ms step_avg:60.09ms
step:621/2315 train_time:37314ms step_avg:60.09ms
step:622/2315 train_time:37374ms step_avg:60.09ms
step:623/2315 train_time:37434ms step_avg:60.09ms
step:624/2315 train_time:37494ms step_avg:60.09ms
step:625/2315 train_time:37554ms step_avg:60.09ms
step:626/2315 train_time:37614ms step_avg:60.09ms
step:627/2315 train_time:37674ms step_avg:60.09ms
step:628/2315 train_time:37734ms step_avg:60.09ms
step:629/2315 train_time:37794ms step_avg:60.09ms
step:630/2315 train_time:37854ms step_avg:60.08ms
step:631/2315 train_time:37913ms step_avg:60.08ms
step:632/2315 train_time:37973ms step_avg:60.08ms
step:633/2315 train_time:38033ms step_avg:60.08ms
step:634/2315 train_time:38094ms step_avg:60.08ms
step:635/2315 train_time:38153ms step_avg:60.08ms
step:636/2315 train_time:38213ms step_avg:60.08ms
step:637/2315 train_time:38273ms step_avg:60.08ms
step:638/2315 train_time:38333ms step_avg:60.08ms
step:639/2315 train_time:38393ms step_avg:60.08ms
step:640/2315 train_time:38453ms step_avg:60.08ms
step:641/2315 train_time:38513ms step_avg:60.08ms
step:642/2315 train_time:38574ms step_avg:60.08ms
step:643/2315 train_time:38633ms step_avg:60.08ms
step:644/2315 train_time:38694ms step_avg:60.08ms
step:645/2315 train_time:38753ms step_avg:60.08ms
step:646/2315 train_time:38813ms step_avg:60.08ms
step:647/2315 train_time:38873ms step_avg:60.08ms
step:648/2315 train_time:38933ms step_avg:60.08ms
step:649/2315 train_time:38993ms step_avg:60.08ms
step:650/2315 train_time:39053ms step_avg:60.08ms
step:651/2315 train_time:39112ms step_avg:60.08ms
step:652/2315 train_time:39173ms step_avg:60.08ms
step:653/2315 train_time:39232ms step_avg:60.08ms
step:654/2315 train_time:39292ms step_avg:60.08ms
step:655/2315 train_time:39353ms step_avg:60.08ms
step:656/2315 train_time:39413ms step_avg:60.08ms
step:657/2315 train_time:39473ms step_avg:60.08ms
step:658/2315 train_time:39533ms step_avg:60.08ms
step:659/2315 train_time:39593ms step_avg:60.08ms
step:660/2315 train_time:39653ms step_avg:60.08ms
step:661/2315 train_time:39713ms step_avg:60.08ms
step:662/2315 train_time:39773ms step_avg:60.08ms
step:663/2315 train_time:39833ms step_avg:60.08ms
step:664/2315 train_time:39893ms step_avg:60.08ms
step:665/2315 train_time:39952ms step_avg:60.08ms
step:666/2315 train_time:40013ms step_avg:60.08ms
step:667/2315 train_time:40073ms step_avg:60.08ms
step:668/2315 train_time:40133ms step_avg:60.08ms
step:669/2315 train_time:40193ms step_avg:60.08ms
step:670/2315 train_time:40253ms step_avg:60.08ms
step:671/2315 train_time:40312ms step_avg:60.08ms
step:672/2315 train_time:40374ms step_avg:60.08ms
step:673/2315 train_time:40432ms step_avg:60.08ms
step:674/2315 train_time:40493ms step_avg:60.08ms
step:675/2315 train_time:40553ms step_avg:60.08ms
step:676/2315 train_time:40614ms step_avg:60.08ms
step:677/2315 train_time:40673ms step_avg:60.08ms
step:678/2315 train_time:40733ms step_avg:60.08ms
step:679/2315 train_time:40793ms step_avg:60.08ms
step:680/2315 train_time:40854ms step_avg:60.08ms
step:681/2315 train_time:40913ms step_avg:60.08ms
step:682/2315 train_time:40973ms step_avg:60.08ms
step:683/2315 train_time:41033ms step_avg:60.08ms
step:684/2315 train_time:41094ms step_avg:60.08ms
step:685/2315 train_time:41153ms step_avg:60.08ms
step:686/2315 train_time:41213ms step_avg:60.08ms
step:687/2315 train_time:41273ms step_avg:60.08ms
step:688/2315 train_time:41334ms step_avg:60.08ms
step:689/2315 train_time:41394ms step_avg:60.08ms
step:690/2315 train_time:41454ms step_avg:60.08ms
step:691/2315 train_time:41514ms step_avg:60.08ms
step:692/2315 train_time:41574ms step_avg:60.08ms
step:693/2315 train_time:41634ms step_avg:60.08ms
step:694/2315 train_time:41695ms step_avg:60.08ms
step:695/2315 train_time:41754ms step_avg:60.08ms
step:696/2315 train_time:41814ms step_avg:60.08ms
step:697/2315 train_time:41874ms step_avg:60.08ms
step:698/2315 train_time:41934ms step_avg:60.08ms
step:699/2315 train_time:41994ms step_avg:60.08ms
step:700/2315 train_time:42053ms step_avg:60.08ms
step:701/2315 train_time:42113ms step_avg:60.08ms
step:702/2315 train_time:42173ms step_avg:60.08ms
step:703/2315 train_time:42234ms step_avg:60.08ms
step:704/2315 train_time:42294ms step_avg:60.08ms
step:705/2315 train_time:42353ms step_avg:60.08ms
step:706/2315 train_time:42415ms step_avg:60.08ms
step:707/2315 train_time:42474ms step_avg:60.08ms
step:708/2315 train_time:42534ms step_avg:60.08ms
step:709/2315 train_time:42593ms step_avg:60.07ms
step:710/2315 train_time:42653ms step_avg:60.07ms
step:711/2315 train_time:42713ms step_avg:60.07ms
step:712/2315 train_time:42773ms step_avg:60.07ms
step:713/2315 train_time:42833ms step_avg:60.07ms
step:714/2315 train_time:42894ms step_avg:60.08ms
step:715/2315 train_time:42954ms step_avg:60.08ms
step:716/2315 train_time:43014ms step_avg:60.08ms
step:717/2315 train_time:43073ms step_avg:60.07ms
step:718/2315 train_time:43134ms step_avg:60.07ms
step:719/2315 train_time:43193ms step_avg:60.07ms
step:720/2315 train_time:43253ms step_avg:60.07ms
step:721/2315 train_time:43313ms step_avg:60.07ms
step:722/2315 train_time:43373ms step_avg:60.07ms
step:723/2315 train_time:43433ms step_avg:60.07ms
step:724/2315 train_time:43493ms step_avg:60.07ms
step:725/2315 train_time:43553ms step_avg:60.07ms
step:726/2315 train_time:43613ms step_avg:60.07ms
step:727/2315 train_time:43674ms step_avg:60.07ms
step:728/2315 train_time:43733ms step_avg:60.07ms
step:729/2315 train_time:43793ms step_avg:60.07ms
step:730/2315 train_time:43853ms step_avg:60.07ms
step:731/2315 train_time:43913ms step_avg:60.07ms
step:732/2315 train_time:43974ms step_avg:60.07ms
step:733/2315 train_time:44033ms step_avg:60.07ms
step:734/2315 train_time:44093ms step_avg:60.07ms
step:735/2315 train_time:44153ms step_avg:60.07ms
step:736/2315 train_time:44214ms step_avg:60.07ms
step:737/2315 train_time:44273ms step_avg:60.07ms
step:738/2315 train_time:44334ms step_avg:60.07ms
step:739/2315 train_time:44394ms step_avg:60.07ms
step:740/2315 train_time:44453ms step_avg:60.07ms
step:741/2315 train_time:44514ms step_avg:60.07ms
step:742/2315 train_time:44573ms step_avg:60.07ms
step:743/2315 train_time:44633ms step_avg:60.07ms
step:744/2315 train_time:44693ms step_avg:60.07ms
step:745/2315 train_time:44753ms step_avg:60.07ms
step:746/2315 train_time:44813ms step_avg:60.07ms
step:747/2315 train_time:44873ms step_avg:60.07ms
step:748/2315 train_time:44934ms step_avg:60.07ms
step:749/2315 train_time:44993ms step_avg:60.07ms
step:750/2315 train_time:45053ms step_avg:60.07ms
step:750/2315 val_loss:3.6833 train_time:45115ms step_avg:60.15ms
step:751/2315 train_time:45141ms step_avg:60.11ms
step:752/2315 train_time:45176ms step_avg:60.07ms
step:753/2315 train_time:45239ms step_avg:60.08ms
step:754/2315 train_time:45301ms step_avg:60.08ms
step:755/2315 train_time:45362ms step_avg:60.08ms
step:756/2315 train_time:45422ms step_avg:60.08ms
step:757/2315 train_time:45481ms step_avg:60.08ms
step:758/2315 train_time:45541ms step_avg:60.08ms
step:759/2315 train_time:45600ms step_avg:60.08ms
step:760/2315 train_time:45660ms step_avg:60.08ms
step:761/2315 train_time:45719ms step_avg:60.08ms
step:762/2315 train_time:45780ms step_avg:60.08ms
step:763/2315 train_time:45839ms step_avg:60.08ms
step:764/2315 train_time:45900ms step_avg:60.08ms
step:765/2315 train_time:45960ms step_avg:60.08ms
step:766/2315 train_time:46021ms step_avg:60.08ms
step:767/2315 train_time:46083ms step_avg:60.08ms
step:768/2315 train_time:46145ms step_avg:60.09ms
step:769/2315 train_time:46207ms step_avg:60.09ms
step:770/2315 train_time:46269ms step_avg:60.09ms
step:771/2315 train_time:46330ms step_avg:60.09ms
step:772/2315 train_time:46391ms step_avg:60.09ms
step:773/2315 train_time:46452ms step_avg:60.09ms
step:774/2315 train_time:46513ms step_avg:60.09ms
step:775/2315 train_time:46573ms step_avg:60.09ms
step:776/2315 train_time:46634ms step_avg:60.10ms
step:777/2315 train_time:46695ms step_avg:60.10ms
step:778/2315 train_time:46756ms step_avg:60.10ms
step:779/2315 train_time:46816ms step_avg:60.10ms
step:780/2315 train_time:46878ms step_avg:60.10ms
step:781/2315 train_time:46939ms step_avg:60.10ms
step:782/2315 train_time:47000ms step_avg:60.10ms
step:783/2315 train_time:47062ms step_avg:60.10ms
step:784/2315 train_time:47123ms step_avg:60.11ms
step:785/2315 train_time:47185ms step_avg:60.11ms
step:786/2315 train_time:47246ms step_avg:60.11ms
step:787/2315 train_time:47306ms step_avg:60.11ms
step:788/2315 train_time:47367ms step_avg:60.11ms
step:789/2315 train_time:47427ms step_avg:60.11ms
step:790/2315 train_time:47487ms step_avg:60.11ms
step:791/2315 train_time:47547ms step_avg:60.11ms
step:792/2315 train_time:47608ms step_avg:60.11ms
step:793/2315 train_time:47669ms step_avg:60.11ms
step:794/2315 train_time:47730ms step_avg:60.11ms
step:795/2315 train_time:47790ms step_avg:60.11ms
step:796/2315 train_time:47851ms step_avg:60.11ms
step:797/2315 train_time:47912ms step_avg:60.12ms
step:798/2315 train_time:47973ms step_avg:60.12ms
step:799/2315 train_time:48034ms step_avg:60.12ms
step:800/2315 train_time:48095ms step_avg:60.12ms
step:801/2315 train_time:48156ms step_avg:60.12ms
step:802/2315 train_time:48218ms step_avg:60.12ms
step:803/2315 train_time:48279ms step_avg:60.12ms
step:804/2315 train_time:48341ms step_avg:60.13ms
step:805/2315 train_time:48402ms step_avg:60.13ms
step:806/2315 train_time:48463ms step_avg:60.13ms
step:807/2315 train_time:48523ms step_avg:60.13ms
step:808/2315 train_time:48584ms step_avg:60.13ms
step:809/2315 train_time:48645ms step_avg:60.13ms
step:810/2315 train_time:48705ms step_avg:60.13ms
step:811/2315 train_time:48765ms step_avg:60.13ms
step:812/2315 train_time:48825ms step_avg:60.13ms
step:813/2315 train_time:48885ms step_avg:60.13ms
step:814/2315 train_time:48946ms step_avg:60.13ms
step:815/2315 train_time:49007ms step_avg:60.13ms
step:816/2315 train_time:49068ms step_avg:60.13ms
step:817/2315 train_time:49129ms step_avg:60.13ms
step:818/2315 train_time:49191ms step_avg:60.14ms
step:819/2315 train_time:49252ms step_avg:60.14ms
step:820/2315 train_time:49313ms step_avg:60.14ms
step:821/2315 train_time:49375ms step_avg:60.14ms
step:822/2315 train_time:49436ms step_avg:60.14ms
step:823/2315 train_time:49497ms step_avg:60.14ms
step:824/2315 train_time:49559ms step_avg:60.14ms
step:825/2315 train_time:49619ms step_avg:60.14ms
step:826/2315 train_time:49680ms step_avg:60.15ms
step:827/2315 train_time:49741ms step_avg:60.15ms
step:828/2315 train_time:49802ms step_avg:60.15ms
step:829/2315 train_time:49863ms step_avg:60.15ms
step:830/2315 train_time:49923ms step_avg:60.15ms
step:831/2315 train_time:49983ms step_avg:60.15ms
step:832/2315 train_time:50044ms step_avg:60.15ms
step:833/2315 train_time:50104ms step_avg:60.15ms
step:834/2315 train_time:50165ms step_avg:60.15ms
step:835/2315 train_time:50226ms step_avg:60.15ms
step:836/2315 train_time:50287ms step_avg:60.15ms
step:837/2315 train_time:50347ms step_avg:60.15ms
step:838/2315 train_time:50408ms step_avg:60.15ms
step:839/2315 train_time:50469ms step_avg:60.15ms
step:840/2315 train_time:50530ms step_avg:60.16ms
step:841/2315 train_time:50592ms step_avg:60.16ms
step:842/2315 train_time:50653ms step_avg:60.16ms
step:843/2315 train_time:50714ms step_avg:60.16ms
step:844/2315 train_time:50776ms step_avg:60.16ms
step:845/2315 train_time:50836ms step_avg:60.16ms
step:846/2315 train_time:50898ms step_avg:60.16ms
step:847/2315 train_time:50959ms step_avg:60.16ms
step:848/2315 train_time:51020ms step_avg:60.17ms
step:849/2315 train_time:51081ms step_avg:60.17ms
step:850/2315 train_time:51142ms step_avg:60.17ms
step:851/2315 train_time:51202ms step_avg:60.17ms
step:852/2315 train_time:51265ms step_avg:60.17ms
step:853/2315 train_time:51323ms step_avg:60.17ms
step:854/2315 train_time:51384ms step_avg:60.17ms
step:855/2315 train_time:51445ms step_avg:60.17ms
step:856/2315 train_time:51505ms step_avg:60.17ms
step:857/2315 train_time:51566ms step_avg:60.17ms
step:858/2315 train_time:51626ms step_avg:60.17ms
step:859/2315 train_time:51687ms step_avg:60.17ms
step:860/2315 train_time:51748ms step_avg:60.17ms
step:861/2315 train_time:51808ms step_avg:60.17ms
step:862/2315 train_time:51869ms step_avg:60.17ms
step:863/2315 train_time:51930ms step_avg:60.17ms
step:864/2315 train_time:51991ms step_avg:60.17ms
step:865/2315 train_time:52052ms step_avg:60.18ms
step:866/2315 train_time:52113ms step_avg:60.18ms
step:867/2315 train_time:52174ms step_avg:60.18ms
step:868/2315 train_time:52236ms step_avg:60.18ms
step:869/2315 train_time:52297ms step_avg:60.18ms
step:870/2315 train_time:52359ms step_avg:60.18ms
step:871/2315 train_time:52420ms step_avg:60.18ms
step:872/2315 train_time:52482ms step_avg:60.19ms
step:873/2315 train_time:52542ms step_avg:60.19ms
step:874/2315 train_time:52603ms step_avg:60.19ms
step:875/2315 train_time:52664ms step_avg:60.19ms
step:876/2315 train_time:52725ms step_avg:60.19ms
step:877/2315 train_time:52785ms step_avg:60.19ms
step:878/2315 train_time:52845ms step_avg:60.19ms
step:879/2315 train_time:52905ms step_avg:60.19ms
step:880/2315 train_time:52966ms step_avg:60.19ms
step:881/2315 train_time:53027ms step_avg:60.19ms
step:882/2315 train_time:53087ms step_avg:60.19ms
step:883/2315 train_time:53148ms step_avg:60.19ms
step:884/2315 train_time:53210ms step_avg:60.19ms
step:885/2315 train_time:53271ms step_avg:60.19ms
step:886/2315 train_time:53333ms step_avg:60.19ms
step:887/2315 train_time:53393ms step_avg:60.20ms
step:888/2315 train_time:53454ms step_avg:60.20ms
step:889/2315 train_time:53514ms step_avg:60.20ms
step:890/2315 train_time:53576ms step_avg:60.20ms
step:891/2315 train_time:53636ms step_avg:60.20ms
step:892/2315 train_time:53698ms step_avg:60.20ms
step:893/2315 train_time:53759ms step_avg:60.20ms
step:894/2315 train_time:53820ms step_avg:60.20ms
step:895/2315 train_time:53881ms step_avg:60.20ms
step:896/2315 train_time:53942ms step_avg:60.20ms
step:897/2315 train_time:54003ms step_avg:60.20ms
step:898/2315 train_time:54064ms step_avg:60.20ms
step:899/2315 train_time:54124ms step_avg:60.21ms
step:900/2315 train_time:54185ms step_avg:60.21ms
step:901/2315 train_time:54245ms step_avg:60.21ms
step:902/2315 train_time:54306ms step_avg:60.21ms
step:903/2315 train_time:54365ms step_avg:60.21ms
step:904/2315 train_time:54426ms step_avg:60.21ms
step:905/2315 train_time:54486ms step_avg:60.21ms
step:906/2315 train_time:54547ms step_avg:60.21ms
step:907/2315 train_time:54607ms step_avg:60.21ms
step:908/2315 train_time:54668ms step_avg:60.21ms
step:909/2315 train_time:54728ms step_avg:60.21ms
step:910/2315 train_time:54789ms step_avg:60.21ms
step:911/2315 train_time:54851ms step_avg:60.21ms
step:912/2315 train_time:54912ms step_avg:60.21ms
step:913/2315 train_time:54973ms step_avg:60.21ms
step:914/2315 train_time:55035ms step_avg:60.21ms
step:915/2315 train_time:55095ms step_avg:60.21ms
step:916/2315 train_time:55157ms step_avg:60.22ms
step:917/2315 train_time:55218ms step_avg:60.22ms
step:918/2315 train_time:55279ms step_avg:60.22ms
step:919/2315 train_time:55341ms step_avg:60.22ms
step:920/2315 train_time:55401ms step_avg:60.22ms
step:921/2315 train_time:55462ms step_avg:60.22ms
step:922/2315 train_time:55523ms step_avg:60.22ms
step:923/2315 train_time:55583ms step_avg:60.22ms
step:924/2315 train_time:55643ms step_avg:60.22ms
step:925/2315 train_time:55703ms step_avg:60.22ms
step:926/2315 train_time:55766ms step_avg:60.22ms
step:927/2315 train_time:55825ms step_avg:60.22ms
step:928/2315 train_time:55885ms step_avg:60.22ms
step:929/2315 train_time:55946ms step_avg:60.22ms
step:930/2315 train_time:56007ms step_avg:60.22ms
step:931/2315 train_time:56067ms step_avg:60.22ms
step:932/2315 train_time:56128ms step_avg:60.22ms
step:933/2315 train_time:56188ms step_avg:60.22ms
step:934/2315 train_time:56249ms step_avg:60.22ms
step:935/2315 train_time:56309ms step_avg:60.22ms
step:936/2315 train_time:56370ms step_avg:60.22ms
step:937/2315 train_time:56431ms step_avg:60.22ms
step:938/2315 train_time:56492ms step_avg:60.23ms
step:939/2315 train_time:56553ms step_avg:60.23ms
step:940/2315 train_time:56614ms step_avg:60.23ms
step:941/2315 train_time:56674ms step_avg:60.23ms
step:942/2315 train_time:56735ms step_avg:60.23ms
step:943/2315 train_time:56796ms step_avg:60.23ms
step:944/2315 train_time:56858ms step_avg:60.23ms
step:945/2315 train_time:56919ms step_avg:60.23ms
step:946/2315 train_time:56980ms step_avg:60.23ms
step:947/2315 train_time:57041ms step_avg:60.23ms
step:948/2315 train_time:57102ms step_avg:60.23ms
step:949/2315 train_time:57162ms step_avg:60.23ms
step:950/2315 train_time:57223ms step_avg:60.23ms
step:951/2315 train_time:57284ms step_avg:60.24ms
step:952/2315 train_time:57345ms step_avg:60.24ms
step:953/2315 train_time:57405ms step_avg:60.24ms
step:954/2315 train_time:57465ms step_avg:60.24ms
step:955/2315 train_time:57526ms step_avg:60.24ms
step:956/2315 train_time:57587ms step_avg:60.24ms
step:957/2315 train_time:57647ms step_avg:60.24ms
step:958/2315 train_time:57707ms step_avg:60.24ms
step:959/2315 train_time:57768ms step_avg:60.24ms
step:960/2315 train_time:57830ms step_avg:60.24ms
step:961/2315 train_time:57890ms step_avg:60.24ms
step:962/2315 train_time:57952ms step_avg:60.24ms
step:963/2315 train_time:58012ms step_avg:60.24ms
step:964/2315 train_time:58074ms step_avg:60.24ms
step:965/2315 train_time:58135ms step_avg:60.24ms
step:966/2315 train_time:58196ms step_avg:60.24ms
step:967/2315 train_time:58256ms step_avg:60.24ms
step:968/2315 train_time:58318ms step_avg:60.25ms
step:969/2315 train_time:58379ms step_avg:60.25ms
step:970/2315 train_time:58440ms step_avg:60.25ms
step:971/2315 train_time:58501ms step_avg:60.25ms
step:972/2315 train_time:58562ms step_avg:60.25ms
step:973/2315 train_time:58623ms step_avg:60.25ms
step:974/2315 train_time:58683ms step_avg:60.25ms
step:975/2315 train_time:58744ms step_avg:60.25ms
step:976/2315 train_time:58805ms step_avg:60.25ms
step:977/2315 train_time:58865ms step_avg:60.25ms
step:978/2315 train_time:58926ms step_avg:60.25ms
step:979/2315 train_time:58986ms step_avg:60.25ms
step:980/2315 train_time:59047ms step_avg:60.25ms
step:981/2315 train_time:59107ms step_avg:60.25ms
step:982/2315 train_time:59168ms step_avg:60.25ms
step:983/2315 train_time:59228ms step_avg:60.25ms
step:984/2315 train_time:59290ms step_avg:60.25ms
step:985/2315 train_time:59351ms step_avg:60.25ms
step:986/2315 train_time:59412ms step_avg:60.26ms
step:987/2315 train_time:59473ms step_avg:60.26ms
step:988/2315 train_time:59534ms step_avg:60.26ms
step:989/2315 train_time:59595ms step_avg:60.26ms
step:990/2315 train_time:59657ms step_avg:60.26ms
step:991/2315 train_time:59718ms step_avg:60.26ms
step:992/2315 train_time:59780ms step_avg:60.26ms
step:993/2315 train_time:59840ms step_avg:60.26ms
step:994/2315 train_time:59902ms step_avg:60.26ms
step:995/2315 train_time:59962ms step_avg:60.26ms
step:996/2315 train_time:60023ms step_avg:60.26ms
step:997/2315 train_time:60083ms step_avg:60.26ms
step:998/2315 train_time:60144ms step_avg:60.26ms
step:999/2315 train_time:60204ms step_avg:60.26ms
step:1000/2315 train_time:60265ms step_avg:60.26ms
step:1000/2315 val_loss:3.5733 train_time:60327ms step_avg:60.33ms
step:1001/2315 train_time:60352ms step_avg:60.29ms
step:1002/2315 train_time:60388ms step_avg:60.27ms
step:1003/2315 train_time:60451ms step_avg:60.27ms
step:1004/2315 train_time:60513ms step_avg:60.27ms
step:1005/2315 train_time:60573ms step_avg:60.27ms
step:1006/2315 train_time:60633ms step_avg:60.27ms
step:1007/2315 train_time:60692ms step_avg:60.27ms
step:1008/2315 train_time:60752ms step_avg:60.27ms
step:1009/2315 train_time:60811ms step_avg:60.27ms
step:1010/2315 train_time:60871ms step_avg:60.27ms
step:1011/2315 train_time:60931ms step_avg:60.27ms
step:1012/2315 train_time:60990ms step_avg:60.27ms
step:1013/2315 train_time:61049ms step_avg:60.27ms
step:1014/2315 train_time:61109ms step_avg:60.27ms
step:1015/2315 train_time:61168ms step_avg:60.26ms
step:1016/2315 train_time:61232ms step_avg:60.27ms
step:1017/2315 train_time:61297ms step_avg:60.27ms
step:1018/2315 train_time:61360ms step_avg:60.28ms
step:1019/2315 train_time:61422ms step_avg:60.28ms
step:1020/2315 train_time:61483ms step_avg:60.28ms
step:1021/2315 train_time:61544ms step_avg:60.28ms
step:1022/2315 train_time:61605ms step_avg:60.28ms
step:1023/2315 train_time:61665ms step_avg:60.28ms
step:1024/2315 train_time:61726ms step_avg:60.28ms
step:1025/2315 train_time:61786ms step_avg:60.28ms
step:1026/2315 train_time:61847ms step_avg:60.28ms
step:1027/2315 train_time:61906ms step_avg:60.28ms
step:1028/2315 train_time:61967ms step_avg:60.28ms
step:1029/2315 train_time:62027ms step_avg:60.28ms
step:1030/2315 train_time:62088ms step_avg:60.28ms
step:1031/2315 train_time:62148ms step_avg:60.28ms
step:1032/2315 train_time:62209ms step_avg:60.28ms
step:1033/2315 train_time:62270ms step_avg:60.28ms
step:1034/2315 train_time:62332ms step_avg:60.28ms
step:1035/2315 train_time:62392ms step_avg:60.28ms
step:1036/2315 train_time:62453ms step_avg:60.28ms
step:1037/2315 train_time:62514ms step_avg:60.28ms
step:1038/2315 train_time:62575ms step_avg:60.28ms
step:1039/2315 train_time:62635ms step_avg:60.28ms
step:1040/2315 train_time:62697ms step_avg:60.29ms
step:1041/2315 train_time:62758ms step_avg:60.29ms
step:1042/2315 train_time:62819ms step_avg:60.29ms
step:1043/2315 train_time:62879ms step_avg:60.29ms
step:1044/2315 train_time:62941ms step_avg:60.29ms
step:1045/2315 train_time:63001ms step_avg:60.29ms
step:1046/2315 train_time:63063ms step_avg:60.29ms
step:1047/2315 train_time:63124ms step_avg:60.29ms
step:1048/2315 train_time:63186ms step_avg:60.29ms
step:1049/2315 train_time:63247ms step_avg:60.29ms
step:1050/2315 train_time:63309ms step_avg:60.29ms
step:1051/2315 train_time:63370ms step_avg:60.29ms
step:1052/2315 train_time:63430ms step_avg:60.29ms
step:1053/2315 train_time:63490ms step_avg:60.29ms
step:1054/2315 train_time:63551ms step_avg:60.30ms
step:1055/2315 train_time:63611ms step_avg:60.30ms
step:1056/2315 train_time:63672ms step_avg:60.30ms
step:1057/2315 train_time:63732ms step_avg:60.30ms
step:1058/2315 train_time:63793ms step_avg:60.30ms
step:1059/2315 train_time:63853ms step_avg:60.30ms
step:1060/2315 train_time:63915ms step_avg:60.30ms
step:1061/2315 train_time:63975ms step_avg:60.30ms
step:1062/2315 train_time:64036ms step_avg:60.30ms
step:1063/2315 train_time:64096ms step_avg:60.30ms
step:1064/2315 train_time:64157ms step_avg:60.30ms
step:1065/2315 train_time:64219ms step_avg:60.30ms
step:1066/2315 train_time:64281ms step_avg:60.30ms
step:1067/2315 train_time:64342ms step_avg:60.30ms
step:1068/2315 train_time:64404ms step_avg:60.30ms
step:1069/2315 train_time:64465ms step_avg:60.30ms
step:1070/2315 train_time:64526ms step_avg:60.31ms
step:1071/2315 train_time:64587ms step_avg:60.31ms
step:1072/2315 train_time:64648ms step_avg:60.31ms
step:1073/2315 train_time:64708ms step_avg:60.31ms
step:1074/2315 train_time:64769ms step_avg:60.31ms
step:1075/2315 train_time:64829ms step_avg:60.31ms
step:1076/2315 train_time:64890ms step_avg:60.31ms
step:1077/2315 train_time:64950ms step_avg:60.31ms
step:1078/2315 train_time:65010ms step_avg:60.31ms
step:1079/2315 train_time:65070ms step_avg:60.31ms
step:1080/2315 train_time:65131ms step_avg:60.31ms
step:1081/2315 train_time:65192ms step_avg:60.31ms
step:1082/2315 train_time:65253ms step_avg:60.31ms
step:1083/2315 train_time:65313ms step_avg:60.31ms
step:1084/2315 train_time:65374ms step_avg:60.31ms
step:1085/2315 train_time:65435ms step_avg:60.31ms
step:1086/2315 train_time:65496ms step_avg:60.31ms
step:1087/2315 train_time:65558ms step_avg:60.31ms
step:1088/2315 train_time:65619ms step_avg:60.31ms
step:1089/2315 train_time:65680ms step_avg:60.31ms
step:1090/2315 train_time:65742ms step_avg:60.31ms
step:1091/2315 train_time:65803ms step_avg:60.31ms
step:1092/2315 train_time:65864ms step_avg:60.32ms
step:1093/2315 train_time:65924ms step_avg:60.32ms
step:1094/2315 train_time:65986ms step_avg:60.32ms
step:1095/2315 train_time:66046ms step_avg:60.32ms
step:1096/2315 train_time:66107ms step_avg:60.32ms
step:1097/2315 train_time:66168ms step_avg:60.32ms
step:1098/2315 train_time:66229ms step_avg:60.32ms
step:1099/2315 train_time:66289ms step_avg:60.32ms
step:1100/2315 train_time:66350ms step_avg:60.32ms
step:1101/2315 train_time:66410ms step_avg:60.32ms
step:1102/2315 train_time:66471ms step_avg:60.32ms
step:1103/2315 train_time:66531ms step_avg:60.32ms
step:1104/2315 train_time:66592ms step_avg:60.32ms
step:1105/2315 train_time:66652ms step_avg:60.32ms
step:1106/2315 train_time:66714ms step_avg:60.32ms
step:1107/2315 train_time:66774ms step_avg:60.32ms
step:1108/2315 train_time:66836ms step_avg:60.32ms
step:1109/2315 train_time:66896ms step_avg:60.32ms
step:1110/2315 train_time:66958ms step_avg:60.32ms
step:1111/2315 train_time:67019ms step_avg:60.32ms
step:1112/2315 train_time:67080ms step_avg:60.32ms
step:1113/2315 train_time:67140ms step_avg:60.32ms
step:1114/2315 train_time:67202ms step_avg:60.33ms
step:1115/2315 train_time:67263ms step_avg:60.33ms
step:1116/2315 train_time:67324ms step_avg:60.33ms
step:1117/2315 train_time:67385ms step_avg:60.33ms
step:1118/2315 train_time:67447ms step_avg:60.33ms
step:1119/2315 train_time:67507ms step_avg:60.33ms
step:1120/2315 train_time:67568ms step_avg:60.33ms
step:1121/2315 train_time:67628ms step_avg:60.33ms
step:1122/2315 train_time:67689ms step_avg:60.33ms
step:1123/2315 train_time:67750ms step_avg:60.33ms
step:1124/2315 train_time:67810ms step_avg:60.33ms
step:1125/2315 train_time:67871ms step_avg:60.33ms
step:1126/2315 train_time:67931ms step_avg:60.33ms
step:1127/2315 train_time:67991ms step_avg:60.33ms
step:1128/2315 train_time:68052ms step_avg:60.33ms
step:1129/2315 train_time:68113ms step_avg:60.33ms
step:1130/2315 train_time:68174ms step_avg:60.33ms
step:1131/2315 train_time:68234ms step_avg:60.33ms
step:1132/2315 train_time:68295ms step_avg:60.33ms
step:1133/2315 train_time:68355ms step_avg:60.33ms
step:1134/2315 train_time:68417ms step_avg:60.33ms
step:1135/2315 train_time:68477ms step_avg:60.33ms
step:1136/2315 train_time:68538ms step_avg:60.33ms
step:1137/2315 train_time:68599ms step_avg:60.33ms
step:1138/2315 train_time:68660ms step_avg:60.33ms
step:1139/2315 train_time:68721ms step_avg:60.33ms
step:1140/2315 train_time:68783ms step_avg:60.34ms
step:1141/2315 train_time:68844ms step_avg:60.34ms
step:1142/2315 train_time:68906ms step_avg:60.34ms
step:1143/2315 train_time:68966ms step_avg:60.34ms
step:1144/2315 train_time:69027ms step_avg:60.34ms
step:1145/2315 train_time:69088ms step_avg:60.34ms
step:1146/2315 train_time:69149ms step_avg:60.34ms
step:1147/2315 train_time:69210ms step_avg:60.34ms
step:1148/2315 train_time:69270ms step_avg:60.34ms
step:1149/2315 train_time:69330ms step_avg:60.34ms
step:1150/2315 train_time:69391ms step_avg:60.34ms
step:1151/2315 train_time:69451ms step_avg:60.34ms
step:1152/2315 train_time:69512ms step_avg:60.34ms
step:1153/2315 train_time:69572ms step_avg:60.34ms
step:1154/2315 train_time:69632ms step_avg:60.34ms
step:1155/2315 train_time:69693ms step_avg:60.34ms
step:1156/2315 train_time:69754ms step_avg:60.34ms
step:1157/2315 train_time:69814ms step_avg:60.34ms
step:1158/2315 train_time:69875ms step_avg:60.34ms
step:1159/2315 train_time:69936ms step_avg:60.34ms
step:1160/2315 train_time:69998ms step_avg:60.34ms
step:1161/2315 train_time:70059ms step_avg:60.34ms
step:1162/2315 train_time:70121ms step_avg:60.34ms
step:1163/2315 train_time:70181ms step_avg:60.34ms
step:1164/2315 train_time:70242ms step_avg:60.35ms
step:1165/2315 train_time:70303ms step_avg:60.35ms
step:1166/2315 train_time:70364ms step_avg:60.35ms
step:1167/2315 train_time:70425ms step_avg:60.35ms
step:1168/2315 train_time:70486ms step_avg:60.35ms
step:1169/2315 train_time:70547ms step_avg:60.35ms
step:1170/2315 train_time:70608ms step_avg:60.35ms
step:1171/2315 train_time:70669ms step_avg:60.35ms
step:1172/2315 train_time:70729ms step_avg:60.35ms
step:1173/2315 train_time:70790ms step_avg:60.35ms
step:1174/2315 train_time:70850ms step_avg:60.35ms
step:1175/2315 train_time:70911ms step_avg:60.35ms
step:1176/2315 train_time:70971ms step_avg:60.35ms
step:1177/2315 train_time:71032ms step_avg:60.35ms
step:1178/2315 train_time:71093ms step_avg:60.35ms
step:1179/2315 train_time:71153ms step_avg:60.35ms
step:1180/2315 train_time:71214ms step_avg:60.35ms
step:1181/2315 train_time:71275ms step_avg:60.35ms
step:1182/2315 train_time:71337ms step_avg:60.35ms
step:1183/2315 train_time:71397ms step_avg:60.35ms
step:1184/2315 train_time:71459ms step_avg:60.35ms
step:1185/2315 train_time:71520ms step_avg:60.35ms
step:1186/2315 train_time:71581ms step_avg:60.36ms
step:1187/2315 train_time:71642ms step_avg:60.36ms
step:1188/2315 train_time:71704ms step_avg:60.36ms
step:1189/2315 train_time:71764ms step_avg:60.36ms
step:1190/2315 train_time:71826ms step_avg:60.36ms
step:1191/2315 train_time:71886ms step_avg:60.36ms
step:1192/2315 train_time:71947ms step_avg:60.36ms
step:1193/2315 train_time:72008ms step_avg:60.36ms
step:1194/2315 train_time:72068ms step_avg:60.36ms
step:1195/2315 train_time:72128ms step_avg:60.36ms
step:1196/2315 train_time:72189ms step_avg:60.36ms
step:1197/2315 train_time:72249ms step_avg:60.36ms
step:1198/2315 train_time:72310ms step_avg:60.36ms
step:1199/2315 train_time:72370ms step_avg:60.36ms
step:1200/2315 train_time:72430ms step_avg:60.36ms
step:1201/2315 train_time:72491ms step_avg:60.36ms
step:1202/2315 train_time:72552ms step_avg:60.36ms
step:1203/2315 train_time:72613ms step_avg:60.36ms
step:1204/2315 train_time:72673ms step_avg:60.36ms
step:1205/2315 train_time:72734ms step_avg:60.36ms
step:1206/2315 train_time:72795ms step_avg:60.36ms
step:1207/2315 train_time:72857ms step_avg:60.36ms
step:1208/2315 train_time:72918ms step_avg:60.36ms
step:1209/2315 train_time:72979ms step_avg:60.36ms
step:1210/2315 train_time:73040ms step_avg:60.36ms
step:1211/2315 train_time:73102ms step_avg:60.36ms
step:1212/2315 train_time:73164ms step_avg:60.37ms
step:1213/2315 train_time:73225ms step_avg:60.37ms
step:1214/2315 train_time:73286ms step_avg:60.37ms
step:1215/2315 train_time:73347ms step_avg:60.37ms
step:1216/2315 train_time:73408ms step_avg:60.37ms
step:1217/2315 train_time:73468ms step_avg:60.37ms
step:1218/2315 train_time:73528ms step_avg:60.37ms
step:1219/2315 train_time:73589ms step_avg:60.37ms
step:1220/2315 train_time:73650ms step_avg:60.37ms
step:1221/2315 train_time:73710ms step_avg:60.37ms
step:1222/2315 train_time:73770ms step_avg:60.37ms
step:1223/2315 train_time:73830ms step_avg:60.37ms
step:1224/2315 train_time:73892ms step_avg:60.37ms
step:1225/2315 train_time:73951ms step_avg:60.37ms
step:1226/2315 train_time:74012ms step_avg:60.37ms
step:1227/2315 train_time:74073ms step_avg:60.37ms
step:1228/2315 train_time:74136ms step_avg:60.37ms
step:1229/2315 train_time:74196ms step_avg:60.37ms
step:1230/2315 train_time:74257ms step_avg:60.37ms
step:1231/2315 train_time:74318ms step_avg:60.37ms
step:1232/2315 train_time:74379ms step_avg:60.37ms
step:1233/2315 train_time:74440ms step_avg:60.37ms
step:1234/2315 train_time:74502ms step_avg:60.37ms
step:1235/2315 train_time:74563ms step_avg:60.37ms
step:1236/2315 train_time:74624ms step_avg:60.38ms
step:1237/2315 train_time:74685ms step_avg:60.38ms
step:1238/2315 train_time:74746ms step_avg:60.38ms
step:1239/2315 train_time:74807ms step_avg:60.38ms
step:1240/2315 train_time:74868ms step_avg:60.38ms
step:1241/2315 train_time:74928ms step_avg:60.38ms
step:1242/2315 train_time:74989ms step_avg:60.38ms
step:1243/2315 train_time:75050ms step_avg:60.38ms
step:1244/2315 train_time:75110ms step_avg:60.38ms
step:1245/2315 train_time:75171ms step_avg:60.38ms
step:1246/2315 train_time:75231ms step_avg:60.38ms
step:1247/2315 train_time:75292ms step_avg:60.38ms
step:1248/2315 train_time:75352ms step_avg:60.38ms
step:1249/2315 train_time:75413ms step_avg:60.38ms
step:1250/2315 train_time:75475ms step_avg:60.38ms
step:1250/2315 val_loss:3.5143 train_time:75538ms step_avg:60.43ms
step:1251/2315 train_time:75556ms step_avg:60.40ms
step:1252/2315 train_time:75602ms step_avg:60.39ms
step:1253/2315 train_time:75667ms step_avg:60.39ms
step:1254/2315 train_time:75729ms step_avg:60.39ms
step:1255/2315 train_time:75790ms step_avg:60.39ms
step:1256/2315 train_time:75851ms step_avg:60.39ms
step:1257/2315 train_time:75911ms step_avg:60.39ms
step:1258/2315 train_time:75973ms step_avg:60.39ms
step:1259/2315 train_time:76033ms step_avg:60.39ms
step:1260/2315 train_time:76094ms step_avg:60.39ms
step:1261/2315 train_time:76155ms step_avg:60.39ms
step:1262/2315 train_time:76215ms step_avg:60.39ms
step:1263/2315 train_time:76275ms step_avg:60.39ms
step:1264/2315 train_time:76337ms step_avg:60.39ms
step:1265/2315 train_time:76396ms step_avg:60.39ms
step:1266/2315 train_time:76457ms step_avg:60.39ms
step:1267/2315 train_time:76518ms step_avg:60.39ms
step:1268/2315 train_time:76581ms step_avg:60.40ms
step:1269/2315 train_time:76643ms step_avg:60.40ms
step:1270/2315 train_time:76705ms step_avg:60.40ms
step:1271/2315 train_time:76766ms step_avg:60.40ms
step:1272/2315 train_time:76828ms step_avg:60.40ms
step:1273/2315 train_time:76887ms step_avg:60.40ms
step:1274/2315 train_time:76948ms step_avg:60.40ms
step:1275/2315 train_time:77009ms step_avg:60.40ms
step:1276/2315 train_time:77070ms step_avg:60.40ms
step:1277/2315 train_time:77130ms step_avg:60.40ms
step:1278/2315 train_time:77191ms step_avg:60.40ms
step:1279/2315 train_time:77250ms step_avg:60.40ms
step:1280/2315 train_time:77311ms step_avg:60.40ms
step:1281/2315 train_time:77372ms step_avg:60.40ms
step:1282/2315 train_time:77433ms step_avg:60.40ms
step:1283/2315 train_time:77495ms step_avg:60.40ms
step:1284/2315 train_time:77557ms step_avg:60.40ms
step:1285/2315 train_time:77619ms step_avg:60.40ms
step:1286/2315 train_time:77681ms step_avg:60.41ms
step:1287/2315 train_time:77742ms step_avg:60.41ms
step:1288/2315 train_time:77803ms step_avg:60.41ms
step:1289/2315 train_time:77863ms step_avg:60.41ms
step:1290/2315 train_time:77924ms step_avg:60.41ms
step:1291/2315 train_time:77984ms step_avg:60.41ms
step:1292/2315 train_time:78044ms step_avg:60.41ms
step:1293/2315 train_time:78104ms step_avg:60.41ms
step:1294/2315 train_time:78164ms step_avg:60.41ms
step:1295/2315 train_time:78225ms step_avg:60.41ms
step:1296/2315 train_time:78285ms step_avg:60.41ms
step:1297/2315 train_time:78345ms step_avg:60.41ms
step:1298/2315 train_time:78406ms step_avg:60.41ms
step:1299/2315 train_time:78466ms step_avg:60.41ms
step:1300/2315 train_time:78528ms step_avg:60.41ms
step:1301/2315 train_time:78590ms step_avg:60.41ms
step:1302/2315 train_time:78652ms step_avg:60.41ms
step:1303/2315 train_time:78713ms step_avg:60.41ms
step:1304/2315 train_time:78775ms step_avg:60.41ms
step:1305/2315 train_time:78836ms step_avg:60.41ms
step:1306/2315 train_time:78898ms step_avg:60.41ms
step:1307/2315 train_time:78959ms step_avg:60.41ms
step:1308/2315 train_time:79020ms step_avg:60.41ms
step:1309/2315 train_time:79080ms step_avg:60.41ms
step:1310/2315 train_time:79141ms step_avg:60.41ms
step:1311/2315 train_time:79201ms step_avg:60.41ms
step:1312/2315 train_time:79262ms step_avg:60.41ms
step:1313/2315 train_time:79322ms step_avg:60.41ms
step:1314/2315 train_time:79383ms step_avg:60.41ms
step:1315/2315 train_time:79443ms step_avg:60.41ms
step:1316/2315 train_time:79504ms step_avg:60.41ms
step:1317/2315 train_time:79564ms step_avg:60.41ms
step:1318/2315 train_time:79625ms step_avg:60.41ms
step:1319/2315 train_time:79685ms step_avg:60.41ms
step:1320/2315 train_time:79746ms step_avg:60.41ms
step:1321/2315 train_time:79807ms step_avg:60.41ms
step:1322/2315 train_time:79868ms step_avg:60.41ms
step:1323/2315 train_time:79929ms step_avg:60.41ms
step:1324/2315 train_time:79990ms step_avg:60.42ms
step:1325/2315 train_time:80051ms step_avg:60.42ms
step:1326/2315 train_time:80112ms step_avg:60.42ms
step:1327/2315 train_time:80173ms step_avg:60.42ms
step:1328/2315 train_time:80234ms step_avg:60.42ms
step:1329/2315 train_time:80295ms step_avg:60.42ms
step:1330/2315 train_time:80356ms step_avg:60.42ms
step:1331/2315 train_time:80417ms step_avg:60.42ms
step:1332/2315 train_time:80479ms step_avg:60.42ms
step:1333/2315 train_time:80539ms step_avg:60.42ms
step:1334/2315 train_time:80601ms step_avg:60.42ms
step:1335/2315 train_time:80661ms step_avg:60.42ms
step:1336/2315 train_time:80722ms step_avg:60.42ms
step:1337/2315 train_time:80783ms step_avg:60.42ms
step:1338/2315 train_time:80843ms step_avg:60.42ms
step:1339/2315 train_time:80903ms step_avg:60.42ms
step:1340/2315 train_time:80964ms step_avg:60.42ms
step:1341/2315 train_time:81024ms step_avg:60.42ms
step:1342/2315 train_time:81085ms step_avg:60.42ms
step:1343/2315 train_time:81145ms step_avg:60.42ms
step:1344/2315 train_time:81206ms step_avg:60.42ms
step:1345/2315 train_time:81266ms step_avg:60.42ms
step:1346/2315 train_time:81327ms step_avg:60.42ms
step:1347/2315 train_time:81387ms step_avg:60.42ms
step:1348/2315 train_time:81449ms step_avg:60.42ms
step:1349/2315 train_time:81510ms step_avg:60.42ms
step:1350/2315 train_time:81572ms step_avg:60.42ms
step:1351/2315 train_time:81633ms step_avg:60.42ms
step:1352/2315 train_time:81694ms step_avg:60.42ms
step:1353/2315 train_time:81754ms step_avg:60.42ms
step:1354/2315 train_time:81816ms step_avg:60.43ms
step:1355/2315 train_time:81877ms step_avg:60.43ms
step:1356/2315 train_time:81938ms step_avg:60.43ms
step:1357/2315 train_time:82000ms step_avg:60.43ms
step:1358/2315 train_time:82061ms step_avg:60.43ms
step:1359/2315 train_time:82121ms step_avg:60.43ms
step:1360/2315 train_time:82182ms step_avg:60.43ms
step:1361/2315 train_time:82242ms step_avg:60.43ms
step:1362/2315 train_time:82303ms step_avg:60.43ms
step:1363/2315 train_time:82363ms step_avg:60.43ms
step:1364/2315 train_time:82424ms step_avg:60.43ms
step:1365/2315 train_time:82484ms step_avg:60.43ms
step:1366/2315 train_time:82545ms step_avg:60.43ms
step:1367/2315 train_time:82605ms step_avg:60.43ms
step:1368/2315 train_time:82665ms step_avg:60.43ms
step:1369/2315 train_time:82726ms step_avg:60.43ms
step:1370/2315 train_time:82786ms step_avg:60.43ms
step:1371/2315 train_time:82847ms step_avg:60.43ms
step:1372/2315 train_time:82908ms step_avg:60.43ms
step:1373/2315 train_time:82969ms step_avg:60.43ms
step:1374/2315 train_time:83029ms step_avg:60.43ms
step:1375/2315 train_time:83090ms step_avg:60.43ms
step:1376/2315 train_time:83152ms step_avg:60.43ms
step:1377/2315 train_time:83213ms step_avg:60.43ms
step:1378/2315 train_time:83274ms step_avg:60.43ms
step:1379/2315 train_time:83334ms step_avg:60.43ms
step:1380/2315 train_time:83396ms step_avg:60.43ms
step:1381/2315 train_time:83457ms step_avg:60.43ms
step:1382/2315 train_time:83518ms step_avg:60.43ms
step:1383/2315 train_time:83579ms step_avg:60.43ms
step:1384/2315 train_time:83641ms step_avg:60.43ms
step:1385/2315 train_time:83701ms step_avg:60.43ms
step:1386/2315 train_time:83761ms step_avg:60.43ms
step:1387/2315 train_time:83822ms step_avg:60.43ms
step:1388/2315 train_time:83883ms step_avg:60.43ms
step:1389/2315 train_time:83943ms step_avg:60.43ms
step:1390/2315 train_time:84004ms step_avg:60.43ms
step:1391/2315 train_time:84064ms step_avg:60.43ms
step:1392/2315 train_time:84125ms step_avg:60.43ms
step:1393/2315 train_time:84185ms step_avg:60.43ms
step:1394/2315 train_time:84246ms step_avg:60.43ms
step:1395/2315 train_time:84306ms step_avg:60.43ms
step:1396/2315 train_time:84367ms step_avg:60.43ms
step:1397/2315 train_time:84427ms step_avg:60.43ms
step:1398/2315 train_time:84489ms step_avg:60.44ms
step:1399/2315 train_time:84550ms step_avg:60.44ms
step:1400/2315 train_time:84611ms step_avg:60.44ms
step:1401/2315 train_time:84672ms step_avg:60.44ms
step:1402/2315 train_time:84734ms step_avg:60.44ms
step:1403/2315 train_time:84796ms step_avg:60.44ms
step:1404/2315 train_time:84858ms step_avg:60.44ms
step:1405/2315 train_time:84918ms step_avg:60.44ms
step:1406/2315 train_time:84980ms step_avg:60.44ms
step:1407/2315 train_time:85040ms step_avg:60.44ms
step:1408/2315 train_time:85101ms step_avg:60.44ms
step:1409/2315 train_time:85162ms step_avg:60.44ms
step:1410/2315 train_time:85223ms step_avg:60.44ms
step:1411/2315 train_time:85283ms step_avg:60.44ms
step:1412/2315 train_time:85344ms step_avg:60.44ms
step:1413/2315 train_time:85404ms step_avg:60.44ms
step:1414/2315 train_time:85464ms step_avg:60.44ms
step:1415/2315 train_time:85524ms step_avg:60.44ms
step:1416/2315 train_time:85584ms step_avg:60.44ms
step:1417/2315 train_time:85645ms step_avg:60.44ms
step:1418/2315 train_time:85707ms step_avg:60.44ms
step:1419/2315 train_time:85767ms step_avg:60.44ms
step:1420/2315 train_time:85828ms step_avg:60.44ms
step:1421/2315 train_time:85889ms step_avg:60.44ms
step:1422/2315 train_time:85950ms step_avg:60.44ms
step:1423/2315 train_time:86011ms step_avg:60.44ms
step:1424/2315 train_time:86072ms step_avg:60.44ms
step:1425/2315 train_time:86133ms step_avg:60.44ms
step:1426/2315 train_time:86195ms step_avg:60.45ms
step:1427/2315 train_time:86256ms step_avg:60.45ms
step:1428/2315 train_time:86317ms step_avg:60.45ms
step:1429/2315 train_time:86378ms step_avg:60.45ms
step:1430/2315 train_time:86439ms step_avg:60.45ms
step:1431/2315 train_time:86500ms step_avg:60.45ms
step:1432/2315 train_time:86561ms step_avg:60.45ms
step:1433/2315 train_time:86621ms step_avg:60.45ms
step:1434/2315 train_time:86683ms step_avg:60.45ms
step:1435/2315 train_time:86743ms step_avg:60.45ms
step:1436/2315 train_time:86804ms step_avg:60.45ms
step:1437/2315 train_time:86864ms step_avg:60.45ms
step:1438/2315 train_time:86925ms step_avg:60.45ms
step:1439/2315 train_time:86984ms step_avg:60.45ms
step:1440/2315 train_time:87045ms step_avg:60.45ms
step:1441/2315 train_time:87106ms step_avg:60.45ms
step:1442/2315 train_time:87167ms step_avg:60.45ms
step:1443/2315 train_time:87227ms step_avg:60.45ms
step:1444/2315 train_time:87289ms step_avg:60.45ms
step:1445/2315 train_time:87349ms step_avg:60.45ms
step:1446/2315 train_time:87410ms step_avg:60.45ms
step:1447/2315 train_time:87471ms step_avg:60.45ms
step:1448/2315 train_time:87532ms step_avg:60.45ms
step:1449/2315 train_time:87593ms step_avg:60.45ms
step:1450/2315 train_time:87655ms step_avg:60.45ms
step:1451/2315 train_time:87716ms step_avg:60.45ms
step:1452/2315 train_time:87778ms step_avg:60.45ms
step:1453/2315 train_time:87839ms step_avg:60.45ms
step:1454/2315 train_time:87900ms step_avg:60.45ms
step:1455/2315 train_time:87961ms step_avg:60.45ms
step:1456/2315 train_time:88021ms step_avg:60.45ms
step:1457/2315 train_time:88081ms step_avg:60.45ms
step:1458/2315 train_time:88142ms step_avg:60.45ms
step:1459/2315 train_time:88202ms step_avg:60.45ms
step:1460/2315 train_time:88263ms step_avg:60.45ms
step:1461/2315 train_time:88323ms step_avg:60.45ms
step:1462/2315 train_time:88384ms step_avg:60.45ms
step:1463/2315 train_time:88444ms step_avg:60.45ms
step:1464/2315 train_time:88504ms step_avg:60.45ms
step:1465/2315 train_time:88565ms step_avg:60.45ms
step:1466/2315 train_time:88626ms step_avg:60.45ms
step:1467/2315 train_time:88686ms step_avg:60.45ms
step:1468/2315 train_time:88747ms step_avg:60.45ms
step:1469/2315 train_time:88807ms step_avg:60.45ms
step:1470/2315 train_time:88869ms step_avg:60.45ms
step:1471/2315 train_time:88930ms step_avg:60.46ms
step:1472/2315 train_time:88991ms step_avg:60.46ms
step:1473/2315 train_time:89052ms step_avg:60.46ms
step:1474/2315 train_time:89113ms step_avg:60.46ms
step:1475/2315 train_time:89174ms step_avg:60.46ms
step:1476/2315 train_time:89236ms step_avg:60.46ms
step:1477/2315 train_time:89296ms step_avg:60.46ms
step:1478/2315 train_time:89358ms step_avg:60.46ms
step:1479/2315 train_time:89419ms step_avg:60.46ms
step:1480/2315 train_time:89480ms step_avg:60.46ms
step:1481/2315 train_time:89540ms step_avg:60.46ms
step:1482/2315 train_time:89601ms step_avg:60.46ms
step:1483/2315 train_time:89661ms step_avg:60.46ms
step:1484/2315 train_time:89722ms step_avg:60.46ms
step:1485/2315 train_time:89782ms step_avg:60.46ms
step:1486/2315 train_time:89843ms step_avg:60.46ms
step:1487/2315 train_time:89903ms step_avg:60.46ms
step:1488/2315 train_time:89963ms step_avg:60.46ms
step:1489/2315 train_time:90024ms step_avg:60.46ms
step:1490/2315 train_time:90084ms step_avg:60.46ms
step:1491/2315 train_time:90144ms step_avg:60.46ms
step:1492/2315 train_time:90205ms step_avg:60.46ms
step:1493/2315 train_time:90265ms step_avg:60.46ms
step:1494/2315 train_time:90327ms step_avg:60.46ms
step:1495/2315 train_time:90387ms step_avg:60.46ms
step:1496/2315 train_time:90448ms step_avg:60.46ms
step:1497/2315 train_time:90509ms step_avg:60.46ms
step:1498/2315 train_time:90570ms step_avg:60.46ms
step:1499/2315 train_time:90630ms step_avg:60.46ms
step:1500/2315 train_time:90692ms step_avg:60.46ms
step:1500/2315 val_loss:3.4499 train_time:90755ms step_avg:60.50ms
step:1501/2315 train_time:90773ms step_avg:60.47ms
step:1502/2315 train_time:90818ms step_avg:60.46ms
step:1503/2315 train_time:90883ms step_avg:60.47ms
step:1504/2315 train_time:90946ms step_avg:60.47ms
step:1505/2315 train_time:91007ms step_avg:60.47ms
step:1506/2315 train_time:91069ms step_avg:60.47ms
step:1507/2315 train_time:91128ms step_avg:60.47ms
step:1508/2315 train_time:91190ms step_avg:60.47ms
step:1509/2315 train_time:91250ms step_avg:60.47ms
step:1510/2315 train_time:91310ms step_avg:60.47ms
step:1511/2315 train_time:91370ms step_avg:60.47ms
step:1512/2315 train_time:91429ms step_avg:60.47ms
step:1513/2315 train_time:91489ms step_avg:60.47ms
step:1514/2315 train_time:91550ms step_avg:60.47ms
step:1515/2315 train_time:91609ms step_avg:60.47ms
step:1516/2315 train_time:91670ms step_avg:60.47ms
step:1517/2315 train_time:91732ms step_avg:60.47ms
step:1518/2315 train_time:91794ms step_avg:60.47ms
step:1519/2315 train_time:91856ms step_avg:60.47ms
step:1520/2315 train_time:91918ms step_avg:60.47ms
step:1521/2315 train_time:91980ms step_avg:60.47ms
step:1522/2315 train_time:92042ms step_avg:60.47ms
step:1523/2315 train_time:92103ms step_avg:60.47ms
step:1524/2315 train_time:92165ms step_avg:60.48ms
step:1525/2315 train_time:92226ms step_avg:60.48ms
step:1526/2315 train_time:92288ms step_avg:60.48ms
step:1527/2315 train_time:92348ms step_avg:60.48ms
step:1528/2315 train_time:92409ms step_avg:60.48ms
step:1529/2315 train_time:92469ms step_avg:60.48ms
step:1530/2315 train_time:92530ms step_avg:60.48ms
step:1531/2315 train_time:92591ms step_avg:60.48ms
step:1532/2315 train_time:92652ms step_avg:60.48ms
step:1533/2315 train_time:92713ms step_avg:60.48ms
step:1534/2315 train_time:92775ms step_avg:60.48ms
step:1535/2315 train_time:92837ms step_avg:60.48ms
step:1536/2315 train_time:92899ms step_avg:60.48ms
step:1537/2315 train_time:92960ms step_avg:60.48ms
step:1538/2315 train_time:93023ms step_avg:60.48ms
step:1539/2315 train_time:93084ms step_avg:60.48ms
step:1540/2315 train_time:93146ms step_avg:60.48ms
step:1541/2315 train_time:93208ms step_avg:60.49ms
step:1542/2315 train_time:93269ms step_avg:60.49ms
step:1543/2315 train_time:93330ms step_avg:60.49ms
step:1544/2315 train_time:93391ms step_avg:60.49ms
step:1545/2315 train_time:93451ms step_avg:60.49ms
step:1546/2315 train_time:93511ms step_avg:60.49ms
step:1547/2315 train_time:93572ms step_avg:60.49ms
step:1548/2315 train_time:93633ms step_avg:60.49ms
step:1549/2315 train_time:93694ms step_avg:60.49ms
step:1550/2315 train_time:93756ms step_avg:60.49ms
step:1551/2315 train_time:93816ms step_avg:60.49ms
step:1552/2315 train_time:93878ms step_avg:60.49ms
step:1553/2315 train_time:93939ms step_avg:60.49ms
step:1554/2315 train_time:94001ms step_avg:60.49ms
step:1555/2315 train_time:94062ms step_avg:60.49ms
step:1556/2315 train_time:94124ms step_avg:60.49ms
step:1557/2315 train_time:94186ms step_avg:60.49ms
step:1558/2315 train_time:94248ms step_avg:60.49ms
step:1559/2315 train_time:94309ms step_avg:60.49ms
step:1560/2315 train_time:94370ms step_avg:60.49ms
step:1561/2315 train_time:94431ms step_avg:60.49ms
step:1562/2315 train_time:94492ms step_avg:60.49ms
step:1563/2315 train_time:94552ms step_avg:60.49ms
step:1564/2315 train_time:94613ms step_avg:60.49ms
step:1565/2315 train_time:94674ms step_avg:60.49ms
step:1566/2315 train_time:94736ms step_avg:60.50ms
step:1567/2315 train_time:94797ms step_avg:60.50ms
step:1568/2315 train_time:94858ms step_avg:60.50ms
step:1569/2315 train_time:94919ms step_avg:60.50ms
step:1570/2315 train_time:94981ms step_avg:60.50ms
step:1571/2315 train_time:95042ms step_avg:60.50ms
step:1572/2315 train_time:95104ms step_avg:60.50ms
step:1573/2315 train_time:95165ms step_avg:60.50ms
step:1574/2315 train_time:95226ms step_avg:60.50ms
step:1575/2315 train_time:95288ms step_avg:60.50ms
step:1576/2315 train_time:95350ms step_avg:60.50ms
step:1577/2315 train_time:95411ms step_avg:60.50ms
step:1578/2315 train_time:95472ms step_avg:60.50ms
step:1579/2315 train_time:95533ms step_avg:60.50ms
step:1580/2315 train_time:95594ms step_avg:60.50ms
step:1581/2315 train_time:95655ms step_avg:60.50ms
step:1582/2315 train_time:95716ms step_avg:60.50ms
step:1583/2315 train_time:95776ms step_avg:60.50ms
step:1584/2315 train_time:95838ms step_avg:60.50ms
step:1585/2315 train_time:95898ms step_avg:60.50ms
step:1586/2315 train_time:95960ms step_avg:60.50ms
step:1587/2315 train_time:96021ms step_avg:60.51ms
step:1588/2315 train_time:96084ms step_avg:60.51ms
step:1589/2315 train_time:96144ms step_avg:60.51ms
step:1590/2315 train_time:96206ms step_avg:60.51ms
step:1591/2315 train_time:96268ms step_avg:60.51ms
step:1592/2315 train_time:96330ms step_avg:60.51ms
step:1593/2315 train_time:96391ms step_avg:60.51ms
step:1594/2315 train_time:96452ms step_avg:60.51ms
step:1595/2315 train_time:96514ms step_avg:60.51ms
step:1596/2315 train_time:96575ms step_avg:60.51ms
step:1597/2315 train_time:96636ms step_avg:60.51ms
step:1598/2315 train_time:96698ms step_avg:60.51ms
step:1599/2315 train_time:96759ms step_avg:60.51ms
step:1600/2315 train_time:96820ms step_avg:60.51ms
step:1601/2315 train_time:96881ms step_avg:60.51ms
step:1602/2315 train_time:96943ms step_avg:60.51ms
step:1603/2315 train_time:97004ms step_avg:60.51ms
step:1604/2315 train_time:97065ms step_avg:60.51ms
step:1605/2315 train_time:97126ms step_avg:60.51ms
step:1606/2315 train_time:97188ms step_avg:60.52ms
step:1607/2315 train_time:97249ms step_avg:60.52ms
step:1608/2315 train_time:97311ms step_avg:60.52ms
step:1609/2315 train_time:97373ms step_avg:60.52ms
step:1610/2315 train_time:97434ms step_avg:60.52ms
step:1611/2315 train_time:97495ms step_avg:60.52ms
step:1612/2315 train_time:97557ms step_avg:60.52ms
step:1613/2315 train_time:97619ms step_avg:60.52ms
step:1614/2315 train_time:97681ms step_avg:60.52ms
step:1615/2315 train_time:97742ms step_avg:60.52ms
step:1616/2315 train_time:97803ms step_avg:60.52ms
step:1617/2315 train_time:97865ms step_avg:60.52ms
step:1618/2315 train_time:97926ms step_avg:60.52ms
step:1619/2315 train_time:97987ms step_avg:60.52ms
step:1620/2315 train_time:98048ms step_avg:60.52ms
step:1621/2315 train_time:98109ms step_avg:60.52ms
step:1622/2315 train_time:98171ms step_avg:60.52ms
step:1623/2315 train_time:98232ms step_avg:60.52ms
step:1624/2315 train_time:98293ms step_avg:60.53ms
step:1625/2315 train_time:98354ms step_avg:60.53ms
step:1626/2315 train_time:98415ms step_avg:60.53ms
step:1627/2315 train_time:98475ms step_avg:60.53ms
step:1628/2315 train_time:98537ms step_avg:60.53ms
step:1629/2315 train_time:98597ms step_avg:60.53ms
step:1630/2315 train_time:98659ms step_avg:60.53ms
step:1631/2315 train_time:98719ms step_avg:60.53ms
step:1632/2315 train_time:98781ms step_avg:60.53ms
step:1633/2315 train_time:98842ms step_avg:60.53ms
step:1634/2315 train_time:98904ms step_avg:60.53ms
step:1635/2315 train_time:98965ms step_avg:60.53ms
step:1636/2315 train_time:99028ms step_avg:60.53ms
step:1637/2315 train_time:99090ms step_avg:60.53ms
step:1638/2315 train_time:99151ms step_avg:60.53ms
step:1639/2315 train_time:99211ms step_avg:60.53ms
step:1640/2315 train_time:99273ms step_avg:60.53ms
step:1641/2315 train_time:99333ms step_avg:60.53ms
step:1642/2315 train_time:99395ms step_avg:60.53ms
step:1643/2315 train_time:99455ms step_avg:60.53ms
step:1644/2315 train_time:99517ms step_avg:60.53ms
step:1645/2315 train_time:99578ms step_avg:60.53ms
step:1646/2315 train_time:99640ms step_avg:60.53ms
step:1647/2315 train_time:99700ms step_avg:60.53ms
step:1648/2315 train_time:99762ms step_avg:60.53ms
step:1649/2315 train_time:99823ms step_avg:60.54ms
step:1650/2315 train_time:99885ms step_avg:60.54ms
step:1651/2315 train_time:99946ms step_avg:60.54ms
step:1652/2315 train_time:100009ms step_avg:60.54ms
step:1653/2315 train_time:100069ms step_avg:60.54ms
step:1654/2315 train_time:100131ms step_avg:60.54ms
step:1655/2315 train_time:100192ms step_avg:60.54ms
step:1656/2315 train_time:100254ms step_avg:60.54ms
step:1657/2315 train_time:100315ms step_avg:60.54ms
step:1658/2315 train_time:100376ms step_avg:60.54ms
step:1659/2315 train_time:100437ms step_avg:60.54ms
step:1660/2315 train_time:100498ms step_avg:60.54ms
step:1661/2315 train_time:100559ms step_avg:60.54ms
step:1662/2315 train_time:100620ms step_avg:60.54ms
step:1663/2315 train_time:100681ms step_avg:60.54ms
step:1664/2315 train_time:100743ms step_avg:60.54ms
step:1665/2315 train_time:100804ms step_avg:60.54ms
step:1666/2315 train_time:100866ms step_avg:60.54ms
step:1667/2315 train_time:100926ms step_avg:60.54ms
step:1668/2315 train_time:100989ms step_avg:60.54ms
step:1669/2315 train_time:101050ms step_avg:60.55ms
step:1670/2315 train_time:101112ms step_avg:60.55ms
step:1671/2315 train_time:101173ms step_avg:60.55ms
step:1672/2315 train_time:101235ms step_avg:60.55ms
step:1673/2315 train_time:101296ms step_avg:60.55ms
step:1674/2315 train_time:101358ms step_avg:60.55ms
step:1675/2315 train_time:101418ms step_avg:60.55ms
step:1676/2315 train_time:101480ms step_avg:60.55ms
step:1677/2315 train_time:101541ms step_avg:60.55ms
step:1678/2315 train_time:101603ms step_avg:60.55ms
step:1679/2315 train_time:101664ms step_avg:60.55ms
step:1680/2315 train_time:101726ms step_avg:60.55ms
step:1681/2315 train_time:101787ms step_avg:60.55ms
step:1682/2315 train_time:101849ms step_avg:60.55ms
step:1683/2315 train_time:101910ms step_avg:60.55ms
step:1684/2315 train_time:101971ms step_avg:60.55ms
step:1685/2315 train_time:102032ms step_avg:60.55ms
step:1686/2315 train_time:102093ms step_avg:60.55ms
step:1687/2315 train_time:102154ms step_avg:60.55ms
step:1688/2315 train_time:102216ms step_avg:60.55ms
step:1689/2315 train_time:102277ms step_avg:60.56ms
step:1690/2315 train_time:102339ms step_avg:60.56ms
step:1691/2315 train_time:102401ms step_avg:60.56ms
step:1692/2315 train_time:102463ms step_avg:60.56ms
step:1693/2315 train_time:102523ms step_avg:60.56ms
step:1694/2315 train_time:102585ms step_avg:60.56ms
step:1695/2315 train_time:102647ms step_avg:60.56ms
step:1696/2315 train_time:102709ms step_avg:60.56ms
step:1697/2315 train_time:102770ms step_avg:60.56ms
step:1698/2315 train_time:102831ms step_avg:60.56ms
step:1699/2315 train_time:102892ms step_avg:60.56ms
step:1700/2315 train_time:102954ms step_avg:60.56ms
step:1701/2315 train_time:103015ms step_avg:60.56ms
step:1702/2315 train_time:103077ms step_avg:60.56ms
step:1703/2315 train_time:103138ms step_avg:60.56ms
step:1704/2315 train_time:103200ms step_avg:60.56ms
step:1705/2315 train_time:103261ms step_avg:60.56ms
step:1706/2315 train_time:103323ms step_avg:60.56ms
step:1707/2315 train_time:103384ms step_avg:60.56ms
step:1708/2315 train_time:103446ms step_avg:60.57ms
step:1709/2315 train_time:103507ms step_avg:60.57ms
step:1710/2315 train_time:103569ms step_avg:60.57ms
step:1711/2315 train_time:103630ms step_avg:60.57ms
step:1712/2315 train_time:103691ms step_avg:60.57ms
step:1713/2315 train_time:103752ms step_avg:60.57ms
step:1714/2315 train_time:103813ms step_avg:60.57ms
step:1715/2315 train_time:103874ms step_avg:60.57ms
step:1716/2315 train_time:103935ms step_avg:60.57ms
step:1717/2315 train_time:103996ms step_avg:60.57ms
step:1718/2315 train_time:104058ms step_avg:60.57ms
step:1719/2315 train_time:104119ms step_avg:60.57ms
step:1720/2315 train_time:104181ms step_avg:60.57ms
step:1721/2315 train_time:104242ms step_avg:60.57ms
step:1722/2315 train_time:104304ms step_avg:60.57ms
step:1723/2315 train_time:104366ms step_avg:60.57ms
step:1724/2315 train_time:104428ms step_avg:60.57ms
step:1725/2315 train_time:104489ms step_avg:60.57ms
step:1726/2315 train_time:104551ms step_avg:60.57ms
step:1727/2315 train_time:104612ms step_avg:60.57ms
step:1728/2315 train_time:104674ms step_avg:60.58ms
step:1729/2315 train_time:104734ms step_avg:60.58ms
step:1730/2315 train_time:104795ms step_avg:60.58ms
step:1731/2315 train_time:104856ms step_avg:60.58ms
step:1732/2315 train_time:104917ms step_avg:60.58ms
step:1733/2315 train_time:104978ms step_avg:60.58ms
step:1734/2315 train_time:105039ms step_avg:60.58ms
step:1735/2315 train_time:105101ms step_avg:60.58ms
step:1736/2315 train_time:105163ms step_avg:60.58ms
step:1737/2315 train_time:105223ms step_avg:60.58ms
step:1738/2315 train_time:105285ms step_avg:60.58ms
step:1739/2315 train_time:105347ms step_avg:60.58ms
step:1740/2315 train_time:105408ms step_avg:60.58ms
step:1741/2315 train_time:105469ms step_avg:60.58ms
step:1742/2315 train_time:105531ms step_avg:60.58ms
step:1743/2315 train_time:105592ms step_avg:60.58ms
step:1744/2315 train_time:105654ms step_avg:60.58ms
step:1745/2315 train_time:105714ms step_avg:60.58ms
step:1746/2315 train_time:105776ms step_avg:60.58ms
step:1747/2315 train_time:105836ms step_avg:60.58ms
step:1748/2315 train_time:105897ms step_avg:60.58ms
step:1749/2315 train_time:105958ms step_avg:60.58ms
step:1750/2315 train_time:106019ms step_avg:60.58ms
step:1750/2315 val_loss:3.3810 train_time:106082ms step_avg:60.62ms
step:1751/2315 train_time:106103ms step_avg:60.60ms
step:1752/2315 train_time:106145ms step_avg:60.59ms
step:1753/2315 train_time:106212ms step_avg:60.59ms
step:1754/2315 train_time:106277ms step_avg:60.59ms
step:1755/2315 train_time:106337ms step_avg:60.59ms
step:1756/2315 train_time:106400ms step_avg:60.59ms
step:1757/2315 train_time:106458ms step_avg:60.59ms
step:1758/2315 train_time:106519ms step_avg:60.59ms
step:1759/2315 train_time:106579ms step_avg:60.59ms
step:1760/2315 train_time:106639ms step_avg:60.59ms
step:1761/2315 train_time:106699ms step_avg:60.59ms
step:1762/2315 train_time:106759ms step_avg:60.59ms
step:1763/2315 train_time:106818ms step_avg:60.59ms
step:1764/2315 train_time:106879ms step_avg:60.59ms
step:1765/2315 train_time:106939ms step_avg:60.59ms
step:1766/2315 train_time:107003ms step_avg:60.59ms
step:1767/2315 train_time:107066ms step_avg:60.59ms
step:1768/2315 train_time:107129ms step_avg:60.59ms
step:1769/2315 train_time:107192ms step_avg:60.59ms
step:1770/2315 train_time:107254ms step_avg:60.60ms
step:1771/2315 train_time:107315ms step_avg:60.60ms
step:1772/2315 train_time:107377ms step_avg:60.60ms
step:1773/2315 train_time:107438ms step_avg:60.60ms
step:1774/2315 train_time:107498ms step_avg:60.60ms
step:1775/2315 train_time:107559ms step_avg:60.60ms
step:1776/2315 train_time:107620ms step_avg:60.60ms
step:1777/2315 train_time:107679ms step_avg:60.60ms
step:1778/2315 train_time:107740ms step_avg:60.60ms
step:1779/2315 train_time:107800ms step_avg:60.60ms
step:1780/2315 train_time:107862ms step_avg:60.60ms
step:1781/2315 train_time:107922ms step_avg:60.60ms
step:1782/2315 train_time:107983ms step_avg:60.60ms
step:1783/2315 train_time:108045ms step_avg:60.60ms
step:1784/2315 train_time:108108ms step_avg:60.60ms
step:1785/2315 train_time:108170ms step_avg:60.60ms
step:1786/2315 train_time:108233ms step_avg:60.60ms
step:1787/2315 train_time:108295ms step_avg:60.60ms
step:1788/2315 train_time:108356ms step_avg:60.60ms
step:1789/2315 train_time:108417ms step_avg:60.60ms
step:1790/2315 train_time:108479ms step_avg:60.60ms
step:1791/2315 train_time:108539ms step_avg:60.60ms
step:1792/2315 train_time:108599ms step_avg:60.60ms
step:1793/2315 train_time:108660ms step_avg:60.60ms
step:1794/2315 train_time:108720ms step_avg:60.60ms
step:1795/2315 train_time:108781ms step_avg:60.60ms
step:1796/2315 train_time:108842ms step_avg:60.60ms
step:1797/2315 train_time:108902ms step_avg:60.60ms
step:1798/2315 train_time:108963ms step_avg:60.60ms
step:1799/2315 train_time:109024ms step_avg:60.60ms
step:1800/2315 train_time:109086ms step_avg:60.60ms
step:1801/2315 train_time:109147ms step_avg:60.60ms
step:1802/2315 train_time:109210ms step_avg:60.61ms
step:1803/2315 train_time:109272ms step_avg:60.61ms
step:1804/2315 train_time:109334ms step_avg:60.61ms
step:1805/2315 train_time:109395ms step_avg:60.61ms
step:1806/2315 train_time:109457ms step_avg:60.61ms
step:1807/2315 train_time:109518ms step_avg:60.61ms
step:1808/2315 train_time:109579ms step_avg:60.61ms
step:1809/2315 train_time:109640ms step_avg:60.61ms
step:1810/2315 train_time:109700ms step_avg:60.61ms
step:1811/2315 train_time:109760ms step_avg:60.61ms
step:1812/2315 train_time:109821ms step_avg:60.61ms
step:1813/2315 train_time:109882ms step_avg:60.61ms
step:1814/2315 train_time:109943ms step_avg:60.61ms
step:1815/2315 train_time:110004ms step_avg:60.61ms
step:1816/2315 train_time:110065ms step_avg:60.61ms
step:1817/2315 train_time:110127ms step_avg:60.61ms
step:1818/2315 train_time:110189ms step_avg:60.61ms
step:1819/2315 train_time:110251ms step_avg:60.61ms
step:1820/2315 train_time:110312ms step_avg:60.61ms
step:1821/2315 train_time:110374ms step_avg:60.61ms
step:1822/2315 train_time:110436ms step_avg:60.61ms
step:1823/2315 train_time:110497ms step_avg:60.61ms
step:1824/2315 train_time:110557ms step_avg:60.61ms
step:1825/2315 train_time:110618ms step_avg:60.61ms
step:1826/2315 train_time:110679ms step_avg:60.61ms
step:1827/2315 train_time:110739ms step_avg:60.61ms
step:1828/2315 train_time:110801ms step_avg:60.61ms
step:1829/2315 train_time:110861ms step_avg:60.61ms
step:1830/2315 train_time:110922ms step_avg:60.61ms
step:1831/2315 train_time:110982ms step_avg:60.61ms
step:1832/2315 train_time:111043ms step_avg:60.61ms
step:1833/2315 train_time:111105ms step_avg:60.61ms
step:1834/2315 train_time:111167ms step_avg:60.61ms
step:1835/2315 train_time:111230ms step_avg:60.62ms
step:1836/2315 train_time:111292ms step_avg:60.62ms
step:1837/2315 train_time:111354ms step_avg:60.62ms
step:1838/2315 train_time:111415ms step_avg:60.62ms
step:1839/2315 train_time:111477ms step_avg:60.62ms
step:1840/2315 train_time:111538ms step_avg:60.62ms
step:1841/2315 train_time:111599ms step_avg:60.62ms
step:1842/2315 train_time:111660ms step_avg:60.62ms
step:1843/2315 train_time:111720ms step_avg:60.62ms
step:1844/2315 train_time:111781ms step_avg:60.62ms
step:1845/2315 train_time:111842ms step_avg:60.62ms
step:1846/2315 train_time:111903ms step_avg:60.62ms
step:1847/2315 train_time:111963ms step_avg:60.62ms
step:1848/2315 train_time:112024ms step_avg:60.62ms
step:1849/2315 train_time:112085ms step_avg:60.62ms
step:1850/2315 train_time:112146ms step_avg:60.62ms
step:1851/2315 train_time:112208ms step_avg:60.62ms
step:1852/2315 train_time:112270ms step_avg:60.62ms
step:1853/2315 train_time:112332ms step_avg:60.62ms
step:1854/2315 train_time:112394ms step_avg:60.62ms
step:1855/2315 train_time:112456ms step_avg:60.62ms
step:1856/2315 train_time:112517ms step_avg:60.62ms
step:1857/2315 train_time:112578ms step_avg:60.62ms
step:1858/2315 train_time:112641ms step_avg:60.62ms
step:1859/2315 train_time:112701ms step_avg:60.62ms
step:1860/2315 train_time:112763ms step_avg:60.63ms
step:1861/2315 train_time:112823ms step_avg:60.62ms
step:1862/2315 train_time:112884ms step_avg:60.63ms
step:1863/2315 train_time:112944ms step_avg:60.62ms
step:1864/2315 train_time:113005ms step_avg:60.63ms
step:1865/2315 train_time:113066ms step_avg:60.63ms
step:1866/2315 train_time:113128ms step_avg:60.63ms
step:1867/2315 train_time:113189ms step_avg:60.63ms
step:1868/2315 train_time:113252ms step_avg:60.63ms
step:1869/2315 train_time:113313ms step_avg:60.63ms
step:1870/2315 train_time:113374ms step_avg:60.63ms
step:1871/2315 train_time:113436ms step_avg:60.63ms
step:1872/2315 train_time:113497ms step_avg:60.63ms
step:1873/2315 train_time:113558ms step_avg:60.63ms
step:1874/2315 train_time:113619ms step_avg:60.63ms
step:1875/2315 train_time:113680ms step_avg:60.63ms
step:1876/2315 train_time:113741ms step_avg:60.63ms
step:1877/2315 train_time:113801ms step_avg:60.63ms
step:1878/2315 train_time:113862ms step_avg:60.63ms
step:1879/2315 train_time:113923ms step_avg:60.63ms
step:1880/2315 train_time:113984ms step_avg:60.63ms
step:1881/2315 train_time:114045ms step_avg:60.63ms
step:1882/2315 train_time:114106ms step_avg:60.63ms
step:1883/2315 train_time:114168ms step_avg:60.63ms
step:1884/2315 train_time:114230ms step_avg:60.63ms
step:1885/2315 train_time:114291ms step_avg:60.63ms
step:1886/2315 train_time:114353ms step_avg:60.63ms
step:1887/2315 train_time:114415ms step_avg:60.63ms
step:1888/2315 train_time:114477ms step_avg:60.63ms
step:1889/2315 train_time:114538ms step_avg:60.63ms
step:1890/2315 train_time:114599ms step_avg:60.63ms
step:1891/2315 train_time:114661ms step_avg:60.64ms
step:1892/2315 train_time:114721ms step_avg:60.63ms
step:1893/2315 train_time:114782ms step_avg:60.63ms
step:1894/2315 train_time:114843ms step_avg:60.64ms
step:1895/2315 train_time:114904ms step_avg:60.64ms
step:1896/2315 train_time:114965ms step_avg:60.64ms
step:1897/2315 train_time:115025ms step_avg:60.64ms
step:1898/2315 train_time:115087ms step_avg:60.64ms
step:1899/2315 train_time:115149ms step_avg:60.64ms
step:1900/2315 train_time:115210ms step_avg:60.64ms
step:1901/2315 train_time:115271ms step_avg:60.64ms
step:1902/2315 train_time:115333ms step_avg:60.64ms
step:1903/2315 train_time:115395ms step_avg:60.64ms
step:1904/2315 train_time:115457ms step_avg:60.64ms
step:1905/2315 train_time:115518ms step_avg:60.64ms
step:1906/2315 train_time:115579ms step_avg:60.64ms
step:1907/2315 train_time:115639ms step_avg:60.64ms
step:1908/2315 train_time:115701ms step_avg:60.64ms
step:1909/2315 train_time:115761ms step_avg:60.64ms
step:1910/2315 train_time:115822ms step_avg:60.64ms
step:1911/2315 train_time:115883ms step_avg:60.64ms
step:1912/2315 train_time:115944ms step_avg:60.64ms
step:1913/2315 train_time:116005ms step_avg:60.64ms
step:1914/2315 train_time:116066ms step_avg:60.64ms
step:1915/2315 train_time:116127ms step_avg:60.64ms
step:1916/2315 train_time:116189ms step_avg:60.64ms
step:1917/2315 train_time:116251ms step_avg:60.64ms
step:1918/2315 train_time:116313ms step_avg:60.64ms
step:1919/2315 train_time:116374ms step_avg:60.64ms
step:1920/2315 train_time:116436ms step_avg:60.64ms
step:1921/2315 train_time:116497ms step_avg:60.64ms
step:1922/2315 train_time:116559ms step_avg:60.64ms
step:1923/2315 train_time:116620ms step_avg:60.64ms
step:1924/2315 train_time:116682ms step_avg:60.65ms
step:1925/2315 train_time:116742ms step_avg:60.65ms
step:1926/2315 train_time:116803ms step_avg:60.65ms
step:1927/2315 train_time:116864ms step_avg:60.65ms
step:1928/2315 train_time:116925ms step_avg:60.65ms
step:1929/2315 train_time:116986ms step_avg:60.65ms
step:1930/2315 train_time:117048ms step_avg:60.65ms
step:1931/2315 train_time:117110ms step_avg:60.65ms
step:1932/2315 train_time:117172ms step_avg:60.65ms
step:1933/2315 train_time:117233ms step_avg:60.65ms
step:1934/2315 train_time:117294ms step_avg:60.65ms
step:1935/2315 train_time:117356ms step_avg:60.65ms
step:1936/2315 train_time:117417ms step_avg:60.65ms
step:1937/2315 train_time:117477ms step_avg:60.65ms
step:1938/2315 train_time:117540ms step_avg:60.65ms
step:1939/2315 train_time:117600ms step_avg:60.65ms
step:1940/2315 train_time:117662ms step_avg:60.65ms
step:1941/2315 train_time:117723ms step_avg:60.65ms
step:1942/2315 train_time:117784ms step_avg:60.65ms
step:1943/2315 train_time:117844ms step_avg:60.65ms
step:1944/2315 train_time:117905ms step_avg:60.65ms
step:1945/2315 train_time:117966ms step_avg:60.65ms
step:1946/2315 train_time:118028ms step_avg:60.65ms
step:1947/2315 train_time:118089ms step_avg:60.65ms
step:1948/2315 train_time:118150ms step_avg:60.65ms
step:1949/2315 train_time:118212ms step_avg:60.65ms
step:1950/2315 train_time:118274ms step_avg:60.65ms
step:1951/2315 train_time:118334ms step_avg:60.65ms
step:1952/2315 train_time:118396ms step_avg:60.65ms
step:1953/2315 train_time:118457ms step_avg:60.65ms
step:1954/2315 train_time:118518ms step_avg:60.65ms
step:1955/2315 train_time:118579ms step_avg:60.65ms
step:1956/2315 train_time:118641ms step_avg:60.65ms
step:1957/2315 train_time:118701ms step_avg:60.65ms
step:1958/2315 train_time:118763ms step_avg:60.66ms
step:1959/2315 train_time:118823ms step_avg:60.66ms
step:1960/2315 train_time:118885ms step_avg:60.66ms
step:1961/2315 train_time:118946ms step_avg:60.66ms
step:1962/2315 train_time:119007ms step_avg:60.66ms
step:1963/2315 train_time:119069ms step_avg:60.66ms
step:1964/2315 train_time:119132ms step_avg:60.66ms
step:1965/2315 train_time:119193ms step_avg:60.66ms
step:1966/2315 train_time:119255ms step_avg:60.66ms
step:1967/2315 train_time:119316ms step_avg:60.66ms
step:1968/2315 train_time:119377ms step_avg:60.66ms
step:1969/2315 train_time:119437ms step_avg:60.66ms
step:1970/2315 train_time:119499ms step_avg:60.66ms
step:1971/2315 train_time:119560ms step_avg:60.66ms
step:1972/2315 train_time:119621ms step_avg:60.66ms
step:1973/2315 train_time:119682ms step_avg:60.66ms
step:1974/2315 train_time:119743ms step_avg:60.66ms
step:1975/2315 train_time:119804ms step_avg:60.66ms
step:1976/2315 train_time:119865ms step_avg:60.66ms
step:1977/2315 train_time:119925ms step_avg:60.66ms
step:1978/2315 train_time:119986ms step_avg:60.66ms
step:1979/2315 train_time:120047ms step_avg:60.66ms
step:1980/2315 train_time:120109ms step_avg:60.66ms
step:1981/2315 train_time:120171ms step_avg:60.66ms
step:1982/2315 train_time:120233ms step_avg:60.66ms
step:1983/2315 train_time:120295ms step_avg:60.66ms
step:1984/2315 train_time:120358ms step_avg:60.66ms
step:1985/2315 train_time:120417ms step_avg:60.66ms
step:1986/2315 train_time:120478ms step_avg:60.66ms
step:1987/2315 train_time:120539ms step_avg:60.66ms
step:1988/2315 train_time:120600ms step_avg:60.66ms
step:1989/2315 train_time:120661ms step_avg:60.66ms
step:1990/2315 train_time:120722ms step_avg:60.66ms
step:1991/2315 train_time:120782ms step_avg:60.66ms
step:1992/2315 train_time:120844ms step_avg:60.66ms
step:1993/2315 train_time:120904ms step_avg:60.66ms
step:1994/2315 train_time:120966ms step_avg:60.66ms
step:1995/2315 train_time:121028ms step_avg:60.67ms
step:1996/2315 train_time:121090ms step_avg:60.67ms
step:1997/2315 train_time:121151ms step_avg:60.67ms
step:1998/2315 train_time:121213ms step_avg:60.67ms
step:1999/2315 train_time:121274ms step_avg:60.67ms
step:2000/2315 train_time:121336ms step_avg:60.67ms
step:2000/2315 val_loss:3.3316 train_time:121399ms step_avg:60.70ms
step:2001/2315 train_time:121418ms step_avg:60.68ms
step:2002/2315 train_time:121463ms step_avg:60.67ms
step:2003/2315 train_time:121528ms step_avg:60.67ms
step:2004/2315 train_time:121590ms step_avg:60.67ms
step:2005/2315 train_time:121652ms step_avg:60.67ms
step:2006/2315 train_time:121714ms step_avg:60.67ms
step:2007/2315 train_time:121775ms step_avg:60.68ms
step:2008/2315 train_time:121838ms step_avg:60.68ms
step:2009/2315 train_time:121898ms step_avg:60.68ms
step:2010/2315 train_time:121959ms step_avg:60.68ms
step:2011/2315 train_time:122019ms step_avg:60.68ms
step:2012/2315 train_time:122080ms step_avg:60.68ms
step:2013/2315 train_time:122140ms step_avg:60.68ms
step:2014/2315 train_time:122201ms step_avg:60.68ms
step:2015/2315 train_time:122260ms step_avg:60.68ms
step:2016/2315 train_time:122321ms step_avg:60.68ms
step:2017/2315 train_time:122383ms step_avg:60.68ms
step:2018/2315 train_time:122446ms step_avg:60.68ms
step:2019/2315 train_time:122508ms step_avg:60.68ms
step:2020/2315 train_time:122570ms step_avg:60.68ms
step:2021/2315 train_time:122632ms step_avg:60.68ms
step:2022/2315 train_time:122694ms step_avg:60.68ms
step:2023/2315 train_time:122756ms step_avg:60.68ms
step:2024/2315 train_time:122818ms step_avg:60.68ms
step:2025/2315 train_time:122878ms step_avg:60.68ms
step:2026/2315 train_time:122939ms step_avg:60.68ms
step:2027/2315 train_time:123000ms step_avg:60.68ms
step:2028/2315 train_time:123061ms step_avg:60.68ms
step:2029/2315 train_time:123121ms step_avg:60.68ms
step:2030/2315 train_time:123182ms step_avg:60.68ms
step:2031/2315 train_time:123242ms step_avg:60.68ms
step:2032/2315 train_time:123303ms step_avg:60.68ms
step:2033/2315 train_time:123364ms step_avg:60.68ms
step:2034/2315 train_time:123425ms step_avg:60.68ms
step:2035/2315 train_time:123487ms step_avg:60.68ms
step:2036/2315 train_time:123550ms step_avg:60.68ms
step:2037/2315 train_time:123612ms step_avg:60.68ms
step:2038/2315 train_time:123674ms step_avg:60.68ms
step:2039/2315 train_time:123735ms step_avg:60.68ms
step:2040/2315 train_time:123797ms step_avg:60.68ms
step:2041/2315 train_time:123858ms step_avg:60.68ms
step:2042/2315 train_time:123919ms step_avg:60.69ms
step:2043/2315 train_time:123980ms step_avg:60.69ms
step:2044/2315 train_time:124041ms step_avg:60.69ms
step:2045/2315 train_time:124101ms step_avg:60.69ms
step:2046/2315 train_time:124162ms step_avg:60.69ms
step:2047/2315 train_time:124222ms step_avg:60.69ms
step:2048/2315 train_time:124284ms step_avg:60.69ms
step:2049/2315 train_time:124345ms step_avg:60.69ms
step:2050/2315 train_time:124407ms step_avg:60.69ms
step:2051/2315 train_time:124469ms step_avg:60.69ms
step:2052/2315 train_time:124530ms step_avg:60.69ms
step:2053/2315 train_time:124592ms step_avg:60.69ms
step:2054/2315 train_time:124655ms step_avg:60.69ms
step:2055/2315 train_time:124716ms step_avg:60.69ms
step:2056/2315 train_time:124778ms step_avg:60.69ms
step:2057/2315 train_time:124839ms step_avg:60.69ms
step:2058/2315 train_time:124901ms step_avg:60.69ms
step:2059/2315 train_time:124962ms step_avg:60.69ms
step:2060/2315 train_time:125023ms step_avg:60.69ms
step:2061/2315 train_time:125083ms step_avg:60.69ms
step:2062/2315 train_time:125144ms step_avg:60.69ms
step:2063/2315 train_time:125205ms step_avg:60.69ms
step:2064/2315 train_time:125267ms step_avg:60.69ms
step:2065/2315 train_time:125328ms step_avg:60.69ms
step:2066/2315 train_time:125389ms step_avg:60.69ms
step:2067/2315 train_time:125451ms step_avg:60.69ms
step:2068/2315 train_time:125514ms step_avg:60.69ms
step:2069/2315 train_time:125575ms step_avg:60.69ms
step:2070/2315 train_time:125637ms step_avg:60.69ms
step:2071/2315 train_time:125698ms step_avg:60.69ms
step:2072/2315 train_time:125760ms step_avg:60.69ms
step:2073/2315 train_time:125820ms step_avg:60.69ms
step:2074/2315 train_time:125882ms step_avg:60.70ms
step:2075/2315 train_time:125942ms step_avg:60.70ms
step:2076/2315 train_time:126004ms step_avg:60.70ms
step:2077/2315 train_time:126064ms step_avg:60.70ms
step:2078/2315 train_time:126125ms step_avg:60.70ms
step:2079/2315 train_time:126187ms step_avg:60.70ms
step:2080/2315 train_time:126248ms step_avg:60.70ms
step:2081/2315 train_time:126309ms step_avg:60.70ms
step:2082/2315 train_time:126375ms step_avg:60.70ms
step:2083/2315 train_time:126433ms step_avg:60.70ms
step:2084/2315 train_time:126495ms step_avg:60.70ms
step:2085/2315 train_time:126557ms step_avg:60.70ms
step:2086/2315 train_time:126618ms step_avg:60.70ms
step:2087/2315 train_time:126679ms step_avg:60.70ms
step:2088/2315 train_time:126741ms step_avg:60.70ms
step:2089/2315 train_time:126802ms step_avg:60.70ms
step:2090/2315 train_time:126864ms step_avg:60.70ms
step:2091/2315 train_time:126925ms step_avg:60.70ms
step:2092/2315 train_time:126987ms step_avg:60.70ms
step:2093/2315 train_time:127048ms step_avg:60.70ms
step:2094/2315 train_time:127110ms step_avg:60.70ms
step:2095/2315 train_time:127171ms step_avg:60.70ms
step:2096/2315 train_time:127232ms step_avg:60.70ms
step:2097/2315 train_time:127294ms step_avg:60.70ms
step:2098/2315 train_time:127355ms step_avg:60.70ms
step:2099/2315 train_time:127416ms step_avg:60.70ms
step:2100/2315 train_time:127478ms step_avg:60.70ms
step:2101/2315 train_time:127539ms step_avg:60.70ms
step:2102/2315 train_time:127600ms step_avg:60.70ms
step:2103/2315 train_time:127661ms step_avg:60.70ms
step:2104/2315 train_time:127722ms step_avg:60.70ms
step:2105/2315 train_time:127783ms step_avg:60.70ms
step:2106/2315 train_time:127844ms step_avg:60.70ms
step:2107/2315 train_time:127905ms step_avg:60.70ms
step:2108/2315 train_time:127967ms step_avg:60.71ms
step:2109/2315 train_time:128027ms step_avg:60.71ms
step:2110/2315 train_time:128089ms step_avg:60.71ms
step:2111/2315 train_time:128150ms step_avg:60.71ms
step:2112/2315 train_time:128212ms step_avg:60.71ms
step:2113/2315 train_time:128274ms step_avg:60.71ms
step:2114/2315 train_time:128335ms step_avg:60.71ms
step:2115/2315 train_time:128397ms step_avg:60.71ms
step:2116/2315 train_time:128458ms step_avg:60.71ms
step:2117/2315 train_time:128518ms step_avg:60.71ms
step:2118/2315 train_time:128580ms step_avg:60.71ms
step:2119/2315 train_time:128641ms step_avg:60.71ms
step:2120/2315 train_time:128702ms step_avg:60.71ms
step:2121/2315 train_time:128763ms step_avg:60.71ms
step:2122/2315 train_time:128824ms step_avg:60.71ms
step:2123/2315 train_time:128885ms step_avg:60.71ms
step:2124/2315 train_time:128946ms step_avg:60.71ms
step:2125/2315 train_time:129007ms step_avg:60.71ms
step:2126/2315 train_time:129069ms step_avg:60.71ms
step:2127/2315 train_time:129130ms step_avg:60.71ms
step:2128/2315 train_time:129192ms step_avg:60.71ms
step:2129/2315 train_time:129254ms step_avg:60.71ms
step:2130/2315 train_time:129316ms step_avg:60.71ms
step:2131/2315 train_time:129377ms step_avg:60.71ms
step:2132/2315 train_time:129439ms step_avg:60.71ms
step:2133/2315 train_time:129500ms step_avg:60.71ms
step:2134/2315 train_time:129561ms step_avg:60.71ms
step:2135/2315 train_time:129622ms step_avg:60.71ms
step:2136/2315 train_time:129684ms step_avg:60.71ms
step:2137/2315 train_time:129744ms step_avg:60.71ms
step:2138/2315 train_time:129806ms step_avg:60.71ms
step:2139/2315 train_time:129867ms step_avg:60.71ms
step:2140/2315 train_time:129928ms step_avg:60.71ms
step:2141/2315 train_time:129989ms step_avg:60.71ms
step:2142/2315 train_time:130051ms step_avg:60.71ms
step:2143/2315 train_time:130112ms step_avg:60.71ms
step:2144/2315 train_time:130174ms step_avg:60.72ms
step:2145/2315 train_time:130235ms step_avg:60.72ms
step:2146/2315 train_time:130296ms step_avg:60.72ms
step:2147/2315 train_time:130358ms step_avg:60.72ms
step:2148/2315 train_time:130420ms step_avg:60.72ms
step:2149/2315 train_time:130481ms step_avg:60.72ms
step:2150/2315 train_time:130542ms step_avg:60.72ms
step:2151/2315 train_time:130604ms step_avg:60.72ms
step:2152/2315 train_time:130666ms step_avg:60.72ms
step:2153/2315 train_time:130727ms step_avg:60.72ms
step:2154/2315 train_time:130789ms step_avg:60.72ms
step:2155/2315 train_time:130851ms step_avg:60.72ms
step:2156/2315 train_time:130912ms step_avg:60.72ms
step:2157/2315 train_time:130976ms step_avg:60.72ms
step:2158/2315 train_time:131035ms step_avg:60.72ms
step:2159/2315 train_time:131096ms step_avg:60.72ms
step:2160/2315 train_time:131157ms step_avg:60.72ms
step:2161/2315 train_time:131218ms step_avg:60.72ms
step:2162/2315 train_time:131280ms step_avg:60.72ms
step:2163/2315 train_time:131341ms step_avg:60.72ms
step:2164/2315 train_time:131402ms step_avg:60.72ms
step:2165/2315 train_time:131463ms step_avg:60.72ms
step:2166/2315 train_time:131525ms step_avg:60.72ms
step:2167/2315 train_time:131586ms step_avg:60.72ms
step:2168/2315 train_time:131648ms step_avg:60.72ms
step:2169/2315 train_time:131709ms step_avg:60.72ms
step:2170/2315 train_time:131771ms step_avg:60.72ms
step:2171/2315 train_time:131833ms step_avg:60.72ms
step:2172/2315 train_time:131894ms step_avg:60.72ms
step:2173/2315 train_time:131956ms step_avg:60.73ms
step:2174/2315 train_time:132017ms step_avg:60.73ms
step:2175/2315 train_time:132078ms step_avg:60.73ms
step:2176/2315 train_time:132140ms step_avg:60.73ms
step:2177/2315 train_time:132200ms step_avg:60.73ms
step:2178/2315 train_time:132263ms step_avg:60.73ms
step:2179/2315 train_time:132323ms step_avg:60.73ms
step:2180/2315 train_time:132385ms step_avg:60.73ms
step:2181/2315 train_time:132446ms step_avg:60.73ms
step:2182/2315 train_time:132508ms step_avg:60.73ms
step:2183/2315 train_time:132570ms step_avg:60.73ms
step:2184/2315 train_time:132632ms step_avg:60.73ms
step:2185/2315 train_time:132693ms step_avg:60.73ms
step:2186/2315 train_time:132755ms step_avg:60.73ms
step:2187/2315 train_time:132816ms step_avg:60.73ms
step:2188/2315 train_time:132877ms step_avg:60.73ms
step:2189/2315 train_time:132938ms step_avg:60.73ms
step:2190/2315 train_time:133000ms step_avg:60.73ms
step:2191/2315 train_time:133060ms step_avg:60.73ms
step:2192/2315 train_time:133121ms step_avg:60.73ms
step:2193/2315 train_time:133182ms step_avg:60.73ms
step:2194/2315 train_time:133244ms step_avg:60.73ms
step:2195/2315 train_time:133304ms step_avg:60.73ms
step:2196/2315 train_time:133366ms step_avg:60.73ms
step:2197/2315 train_time:133427ms step_avg:60.73ms
step:2198/2315 train_time:133489ms step_avg:60.73ms
step:2199/2315 train_time:133551ms step_avg:60.73ms
step:2200/2315 train_time:133613ms step_avg:60.73ms
step:2201/2315 train_time:133675ms step_avg:60.73ms
step:2202/2315 train_time:133737ms step_avg:60.73ms
step:2203/2315 train_time:133798ms step_avg:60.73ms
step:2204/2315 train_time:133859ms step_avg:60.73ms
step:2205/2315 train_time:133919ms step_avg:60.73ms
step:2206/2315 train_time:133981ms step_avg:60.73ms
step:2207/2315 train_time:134042ms step_avg:60.73ms
step:2208/2315 train_time:134103ms step_avg:60.74ms
step:2209/2315 train_time:134164ms step_avg:60.74ms
step:2210/2315 train_time:134226ms step_avg:60.74ms
step:2211/2315 train_time:134287ms step_avg:60.74ms
step:2212/2315 train_time:134349ms step_avg:60.74ms
step:2213/2315 train_time:134410ms step_avg:60.74ms
step:2214/2315 train_time:134471ms step_avg:60.74ms
step:2215/2315 train_time:134533ms step_avg:60.74ms
step:2216/2315 train_time:134595ms step_avg:60.74ms
step:2217/2315 train_time:134656ms step_avg:60.74ms
step:2218/2315 train_time:134718ms step_avg:60.74ms
step:2219/2315 train_time:134779ms step_avg:60.74ms
step:2220/2315 train_time:134840ms step_avg:60.74ms
step:2221/2315 train_time:134900ms step_avg:60.74ms
step:2222/2315 train_time:134962ms step_avg:60.74ms
step:2223/2315 train_time:135022ms step_avg:60.74ms
step:2224/2315 train_time:135084ms step_avg:60.74ms
step:2225/2315 train_time:135145ms step_avg:60.74ms
step:2226/2315 train_time:135207ms step_avg:60.74ms
step:2227/2315 train_time:135268ms step_avg:60.74ms
step:2228/2315 train_time:135330ms step_avg:60.74ms
step:2229/2315 train_time:135391ms step_avg:60.74ms
step:2230/2315 train_time:135452ms step_avg:60.74ms
step:2231/2315 train_time:135513ms step_avg:60.74ms
step:2232/2315 train_time:135576ms step_avg:60.74ms
step:2233/2315 train_time:135637ms step_avg:60.74ms
step:2234/2315 train_time:135699ms step_avg:60.74ms
step:2235/2315 train_time:135760ms step_avg:60.74ms
step:2236/2315 train_time:135822ms step_avg:60.74ms
step:2237/2315 train_time:135884ms step_avg:60.74ms
step:2238/2315 train_time:135945ms step_avg:60.74ms
step:2239/2315 train_time:136005ms step_avg:60.74ms
step:2240/2315 train_time:136067ms step_avg:60.74ms
step:2241/2315 train_time:136127ms step_avg:60.74ms
step:2242/2315 train_time:136188ms step_avg:60.74ms
step:2243/2315 train_time:136250ms step_avg:60.74ms
step:2244/2315 train_time:136312ms step_avg:60.75ms
step:2245/2315 train_time:136374ms step_avg:60.75ms
step:2246/2315 train_time:136435ms step_avg:60.75ms
step:2247/2315 train_time:136496ms step_avg:60.75ms
step:2248/2315 train_time:136558ms step_avg:60.75ms
step:2249/2315 train_time:136619ms step_avg:60.75ms
step:2250/2315 train_time:136680ms step_avg:60.75ms
step:2250/2315 val_loss:3.2912 train_time:136743ms step_avg:60.77ms
step:2251/2315 train_time:136766ms step_avg:60.76ms
step:2252/2315 train_time:136809ms step_avg:60.75ms
step:2253/2315 train_time:136871ms step_avg:60.75ms
step:2254/2315 train_time:136934ms step_avg:60.75ms
step:2255/2315 train_time:136995ms step_avg:60.75ms
step:2256/2315 train_time:137056ms step_avg:60.75ms
step:2257/2315 train_time:137117ms step_avg:60.75ms
step:2258/2315 train_time:137178ms step_avg:60.75ms
step:2259/2315 train_time:137238ms step_avg:60.75ms
step:2260/2315 train_time:137299ms step_avg:60.75ms
step:2261/2315 train_time:137359ms step_avg:60.75ms
step:2262/2315 train_time:137420ms step_avg:60.75ms
step:2263/2315 train_time:137481ms step_avg:60.75ms
step:2264/2315 train_time:137542ms step_avg:60.75ms
step:2265/2315 train_time:137601ms step_avg:60.75ms
step:2266/2315 train_time:137663ms step_avg:60.75ms
step:2267/2315 train_time:137725ms step_avg:60.75ms
step:2268/2315 train_time:137788ms step_avg:60.75ms
step:2269/2315 train_time:137850ms step_avg:60.75ms
step:2270/2315 train_time:137913ms step_avg:60.75ms
step:2271/2315 train_time:137974ms step_avg:60.75ms
step:2272/2315 train_time:138036ms step_avg:60.76ms
step:2273/2315 train_time:138097ms step_avg:60.76ms
step:2274/2315 train_time:138158ms step_avg:60.76ms
step:2275/2315 train_time:138218ms step_avg:60.76ms
step:2276/2315 train_time:138279ms step_avg:60.76ms
step:2277/2315 train_time:138339ms step_avg:60.75ms
step:2278/2315 train_time:138401ms step_avg:60.76ms
step:2279/2315 train_time:138461ms step_avg:60.76ms
step:2280/2315 train_time:138522ms step_avg:60.76ms
step:2281/2315 train_time:138582ms step_avg:60.75ms
step:2282/2315 train_time:138643ms step_avg:60.76ms
step:2283/2315 train_time:138704ms step_avg:60.76ms
step:2284/2315 train_time:138766ms step_avg:60.76ms
step:2285/2315 train_time:138828ms step_avg:60.76ms
step:2286/2315 train_time:138890ms step_avg:60.76ms
step:2287/2315 train_time:138951ms step_avg:60.76ms
step:2288/2315 train_time:139014ms step_avg:60.76ms
step:2289/2315 train_time:139076ms step_avg:60.76ms
step:2290/2315 train_time:139137ms step_avg:60.76ms
step:2291/2315 train_time:139198ms step_avg:60.76ms
step:2292/2315 train_time:139259ms step_avg:60.76ms
step:2293/2315 train_time:139320ms step_avg:60.76ms
step:2294/2315 train_time:139381ms step_avg:60.76ms
step:2295/2315 train_time:139441ms step_avg:60.76ms
step:2296/2315 train_time:139502ms step_avg:60.76ms
step:2297/2315 train_time:139562ms step_avg:60.76ms
step:2298/2315 train_time:139624ms step_avg:60.76ms
step:2299/2315 train_time:139685ms step_avg:60.76ms
step:2300/2315 train_time:139747ms step_avg:60.76ms
step:2301/2315 train_time:139808ms step_avg:60.76ms
step:2302/2315 train_time:139871ms step_avg:60.76ms
step:2303/2315 train_time:139932ms step_avg:60.76ms
step:2304/2315 train_time:139995ms step_avg:60.76ms
step:2305/2315 train_time:140056ms step_avg:60.76ms
step:2306/2315 train_time:140117ms step_avg:60.76ms
step:2307/2315 train_time:140178ms step_avg:60.76ms
step:2308/2315 train_time:140239ms step_avg:60.76ms
step:2309/2315 train_time:140300ms step_avg:60.76ms
step:2310/2315 train_time:140361ms step_avg:60.76ms
step:2311/2315 train_time:140422ms step_avg:60.76ms
step:2312/2315 train_time:140484ms step_avg:60.76ms
step:2313/2315 train_time:140545ms step_avg:60.76ms
step:2314/2315 train_time:140606ms step_avg:60.76ms
step:2315/2315 train_time:140666ms step_avg:60.76ms
step:2315/2315 val_loss:3.2786 train_time:140728ms step_avg:60.79ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

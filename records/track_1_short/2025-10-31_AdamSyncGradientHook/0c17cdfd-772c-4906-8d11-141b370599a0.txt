import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2205  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()


====================================================================================================
Running Python 3.12.7 (main, Nov 15 2025, 15:35:49) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251115+cu126 compiled for CUDA 12.6
Running Triton version 3.5.1
Sat Nov 15 15:50:24 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   33C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   37C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   38C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   38C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   37C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           48189      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A           48190      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           48191      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           48192      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           48193      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           48194      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           48195      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           48196      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A           48190      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A           48191      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A           48192      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A           48193      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A           48194      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A           48195      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A           48196      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2245 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2245 train_time:120ms step_avg:119.55ms
step:2/2245 train_time:161ms step_avg:80.34ms
step:3/2245 train_time:180ms step_avg:60.04ms
step:4/2245 train_time:235ms step_avg:58.73ms
step:5/2245 train_time:294ms step_avg:58.87ms
step:6/2245 train_time:353ms step_avg:58.89ms
step:7/2245 train_time:415ms step_avg:59.22ms
step:8/2245 train_time:473ms step_avg:59.15ms
step:9/2245 train_time:534ms step_avg:59.34ms
step:10/2245 train_time:593ms step_avg:59.30ms
step:11/2245 train_time:654ms step_avg:59.46ms
step:12/2245 train_time:713ms step_avg:59.42ms
step:13/2245 train_time:774ms step_avg:59.54ms
step:14/2245 train_time:833ms step_avg:59.50ms
step:15/2245 train_time:894ms step_avg:59.58ms
step:16/2245 train_time:954ms step_avg:59.62ms
step:17/2245 train_time:1019ms step_avg:59.94ms
step:18/2245 train_time:1083ms step_avg:60.16ms
step:19/2245 train_time:1147ms step_avg:60.38ms
step:20/2245 train_time:1210ms step_avg:60.48ms
step:21/2245 train_time:1272ms step_avg:60.57ms
step:22/2245 train_time:1332ms step_avg:60.56ms
step:23/2245 train_time:1394ms step_avg:60.61ms
step:24/2245 train_time:1454ms step_avg:60.57ms
step:25/2245 train_time:1515ms step_avg:60.60ms
step:26/2245 train_time:1574ms step_avg:60.54ms
step:27/2245 train_time:1635ms step_avg:60.56ms
step:28/2245 train_time:1694ms step_avg:60.50ms
step:29/2245 train_time:1755ms step_avg:60.53ms
step:30/2245 train_time:1815ms step_avg:60.49ms
step:31/2245 train_time:1875ms step_avg:60.50ms
step:32/2245 train_time:1936ms step_avg:60.49ms
step:33/2245 train_time:2000ms step_avg:60.59ms
step:34/2245 train_time:2061ms step_avg:60.63ms
step:35/2245 train_time:2126ms step_avg:60.73ms
step:36/2245 train_time:2186ms step_avg:60.73ms
step:37/2245 train_time:2249ms step_avg:60.78ms
step:38/2245 train_time:2309ms step_avg:60.77ms
step:39/2245 train_time:2372ms step_avg:60.81ms
step:40/2245 train_time:2431ms step_avg:60.78ms
step:41/2245 train_time:2493ms step_avg:60.80ms
step:42/2245 train_time:2553ms step_avg:60.78ms
step:43/2245 train_time:2614ms step_avg:60.78ms
step:44/2245 train_time:2673ms step_avg:60.75ms
step:45/2245 train_time:2735ms step_avg:60.77ms
step:46/2245 train_time:2794ms step_avg:60.73ms
step:47/2245 train_time:2855ms step_avg:60.74ms
step:48/2245 train_time:2914ms step_avg:60.71ms
step:49/2245 train_time:2976ms step_avg:60.73ms
step:50/2245 train_time:3036ms step_avg:60.71ms
step:51/2245 train_time:3099ms step_avg:60.76ms
step:52/2245 train_time:3160ms step_avg:60.77ms
step:53/2245 train_time:3223ms step_avg:60.82ms
step:54/2245 train_time:3283ms step_avg:60.80ms
step:55/2245 train_time:3345ms step_avg:60.83ms
step:56/2245 train_time:3406ms step_avg:60.82ms
step:57/2245 train_time:3468ms step_avg:60.84ms
step:58/2245 train_time:3528ms step_avg:60.82ms
step:59/2245 train_time:3590ms step_avg:60.84ms
step:60/2245 train_time:3649ms step_avg:60.82ms
step:61/2245 train_time:3711ms step_avg:60.84ms
step:62/2245 train_time:3770ms step_avg:60.81ms
step:63/2245 train_time:3832ms step_avg:60.83ms
step:64/2245 train_time:3892ms step_avg:60.81ms
step:65/2245 train_time:3954ms step_avg:60.83ms
step:66/2245 train_time:4014ms step_avg:60.82ms
step:67/2245 train_time:4077ms step_avg:60.85ms
step:68/2245 train_time:4136ms step_avg:60.83ms
step:69/2245 train_time:4199ms step_avg:60.85ms
step:70/2245 train_time:4260ms step_avg:60.86ms
step:71/2245 train_time:4323ms step_avg:60.89ms
step:72/2245 train_time:4383ms step_avg:60.88ms
step:73/2245 train_time:4445ms step_avg:60.89ms
step:74/2245 train_time:4505ms step_avg:60.87ms
step:75/2245 train_time:4567ms step_avg:60.89ms
step:76/2245 train_time:4627ms step_avg:60.88ms
step:77/2245 train_time:4688ms step_avg:60.89ms
step:78/2245 train_time:4749ms step_avg:60.88ms
step:79/2245 train_time:4811ms step_avg:60.90ms
step:80/2245 train_time:4870ms step_avg:60.88ms
step:81/2245 train_time:4932ms step_avg:60.89ms
step:82/2245 train_time:4992ms step_avg:60.88ms
step:83/2245 train_time:5054ms step_avg:60.89ms
step:84/2245 train_time:5114ms step_avg:60.88ms
step:85/2245 train_time:5176ms step_avg:60.89ms
step:86/2245 train_time:5235ms step_avg:60.88ms
step:87/2245 train_time:5298ms step_avg:60.90ms
step:88/2245 train_time:5358ms step_avg:60.89ms
step:89/2245 train_time:5419ms step_avg:60.89ms
step:90/2245 train_time:5479ms step_avg:60.88ms
step:91/2245 train_time:5541ms step_avg:60.89ms
step:92/2245 train_time:5602ms step_avg:60.89ms
step:93/2245 train_time:5664ms step_avg:60.90ms
step:94/2245 train_time:5723ms step_avg:60.89ms
step:95/2245 train_time:5785ms step_avg:60.90ms
step:96/2245 train_time:5845ms step_avg:60.89ms
step:97/2245 train_time:5908ms step_avg:60.91ms
step:98/2245 train_time:5968ms step_avg:60.90ms
step:99/2245 train_time:6031ms step_avg:60.91ms
step:100/2245 train_time:6091ms step_avg:60.91ms
step:101/2245 train_time:6153ms step_avg:60.92ms
step:102/2245 train_time:6213ms step_avg:60.91ms
step:103/2245 train_time:6275ms step_avg:60.92ms
step:104/2245 train_time:6334ms step_avg:60.90ms
step:105/2245 train_time:6396ms step_avg:60.91ms
step:106/2245 train_time:6456ms step_avg:60.91ms
step:107/2245 train_time:6518ms step_avg:60.91ms
step:108/2245 train_time:6577ms step_avg:60.90ms
step:109/2245 train_time:6638ms step_avg:60.90ms
step:110/2245 train_time:6699ms step_avg:60.90ms
step:111/2245 train_time:6761ms step_avg:60.91ms
step:112/2245 train_time:6821ms step_avg:60.90ms
step:113/2245 train_time:6882ms step_avg:60.91ms
step:114/2245 train_time:6942ms step_avg:60.89ms
step:115/2245 train_time:7004ms step_avg:60.91ms
step:116/2245 train_time:7064ms step_avg:60.90ms
step:117/2245 train_time:7126ms step_avg:60.91ms
step:118/2245 train_time:7186ms step_avg:60.90ms
step:119/2245 train_time:7248ms step_avg:60.91ms
step:120/2245 train_time:7308ms step_avg:60.90ms
step:121/2245 train_time:7370ms step_avg:60.91ms
step:122/2245 train_time:7430ms step_avg:60.90ms
step:123/2245 train_time:7492ms step_avg:60.91ms
step:124/2245 train_time:7551ms step_avg:60.90ms
step:125/2245 train_time:7615ms step_avg:60.92ms
step:126/2245 train_time:7674ms step_avg:60.91ms
step:127/2245 train_time:7736ms step_avg:60.91ms
step:128/2245 train_time:7795ms step_avg:60.90ms
step:129/2245 train_time:7856ms step_avg:60.90ms
step:130/2245 train_time:7916ms step_avg:60.89ms
step:131/2245 train_time:7977ms step_avg:60.89ms
step:132/2245 train_time:8036ms step_avg:60.88ms
step:133/2245 train_time:8098ms step_avg:60.89ms
step:134/2245 train_time:8158ms step_avg:60.88ms
step:135/2245 train_time:8220ms step_avg:60.89ms
step:136/2245 train_time:8279ms step_avg:60.88ms
step:137/2245 train_time:8341ms step_avg:60.89ms
step:138/2245 train_time:8401ms step_avg:60.88ms
step:139/2245 train_time:8464ms step_avg:60.89ms
step:140/2245 train_time:8524ms step_avg:60.88ms
step:141/2245 train_time:8587ms step_avg:60.90ms
step:142/2245 train_time:8646ms step_avg:60.89ms
step:143/2245 train_time:8708ms step_avg:60.90ms
step:144/2245 train_time:8769ms step_avg:60.89ms
step:145/2245 train_time:8831ms step_avg:60.90ms
step:146/2245 train_time:8890ms step_avg:60.89ms
step:147/2245 train_time:8952ms step_avg:60.90ms
step:148/2245 train_time:9013ms step_avg:60.90ms
step:149/2245 train_time:9075ms step_avg:60.90ms
step:150/2245 train_time:9134ms step_avg:60.89ms
step:151/2245 train_time:9195ms step_avg:60.89ms
step:152/2245 train_time:9254ms step_avg:60.88ms
step:153/2245 train_time:9315ms step_avg:60.89ms
step:154/2245 train_time:9374ms step_avg:60.87ms
step:155/2245 train_time:9436ms step_avg:60.87ms
step:156/2245 train_time:9495ms step_avg:60.86ms
step:157/2245 train_time:9556ms step_avg:60.87ms
step:158/2245 train_time:9616ms step_avg:60.86ms
step:159/2245 train_time:9678ms step_avg:60.87ms
step:160/2245 train_time:9738ms step_avg:60.86ms
step:161/2245 train_time:9799ms step_avg:60.86ms
step:162/2245 train_time:9859ms step_avg:60.86ms
step:163/2245 train_time:9921ms step_avg:60.87ms
step:164/2245 train_time:9981ms step_avg:60.86ms
step:165/2245 train_time:10042ms step_avg:60.86ms
step:166/2245 train_time:10102ms step_avg:60.85ms
step:167/2245 train_time:10164ms step_avg:60.86ms
step:168/2245 train_time:10223ms step_avg:60.85ms
step:169/2245 train_time:10286ms step_avg:60.86ms
step:170/2245 train_time:10345ms step_avg:60.85ms
step:171/2245 train_time:10406ms step_avg:60.86ms
step:172/2245 train_time:10466ms step_avg:60.85ms
step:173/2245 train_time:10528ms step_avg:60.86ms
step:174/2245 train_time:10588ms step_avg:60.85ms
step:175/2245 train_time:10651ms step_avg:60.86ms
step:176/2245 train_time:10711ms step_avg:60.86ms
step:177/2245 train_time:10773ms step_avg:60.87ms
step:178/2245 train_time:10833ms step_avg:60.86ms
step:179/2245 train_time:10894ms step_avg:60.86ms
step:180/2245 train_time:10953ms step_avg:60.85ms
step:181/2245 train_time:11014ms step_avg:60.85ms
step:182/2245 train_time:11074ms step_avg:60.84ms
step:183/2245 train_time:11135ms step_avg:60.85ms
step:184/2245 train_time:11194ms step_avg:60.84ms
step:185/2245 train_time:11256ms step_avg:60.84ms
step:186/2245 train_time:11315ms step_avg:60.83ms
step:187/2245 train_time:11376ms step_avg:60.83ms
step:188/2245 train_time:11435ms step_avg:60.82ms
step:189/2245 train_time:11496ms step_avg:60.83ms
step:190/2245 train_time:11555ms step_avg:60.82ms
step:191/2245 train_time:11617ms step_avg:60.82ms
step:192/2245 train_time:11676ms step_avg:60.81ms
step:193/2245 train_time:11738ms step_avg:60.82ms
step:194/2245 train_time:11798ms step_avg:60.81ms
step:195/2245 train_time:11859ms step_avg:60.82ms
step:196/2245 train_time:11919ms step_avg:60.81ms
step:197/2245 train_time:11980ms step_avg:60.81ms
step:198/2245 train_time:12039ms step_avg:60.80ms
step:199/2245 train_time:12101ms step_avg:60.81ms
step:200/2245 train_time:12160ms step_avg:60.80ms
step:201/2245 train_time:12223ms step_avg:60.81ms
step:202/2245 train_time:12282ms step_avg:60.80ms
step:203/2245 train_time:12342ms step_avg:60.80ms
step:204/2245 train_time:12401ms step_avg:60.79ms
step:205/2245 train_time:12463ms step_avg:60.80ms
step:206/2245 train_time:12523ms step_avg:60.79ms
step:207/2245 train_time:12585ms step_avg:60.80ms
step:208/2245 train_time:12644ms step_avg:60.79ms
step:209/2245 train_time:12706ms step_avg:60.80ms
step:210/2245 train_time:12766ms step_avg:60.79ms
step:211/2245 train_time:12828ms step_avg:60.79ms
step:212/2245 train_time:12888ms step_avg:60.79ms
step:213/2245 train_time:12951ms step_avg:60.80ms
step:214/2245 train_time:13010ms step_avg:60.80ms
step:215/2245 train_time:13072ms step_avg:60.80ms
step:216/2245 train_time:13132ms step_avg:60.80ms
step:217/2245 train_time:13194ms step_avg:60.80ms
step:218/2245 train_time:13254ms step_avg:60.80ms
step:219/2245 train_time:13315ms step_avg:60.80ms
step:220/2245 train_time:13374ms step_avg:60.79ms
step:221/2245 train_time:13437ms step_avg:60.80ms
step:222/2245 train_time:13495ms step_avg:60.79ms
step:223/2245 train_time:13557ms step_avg:60.79ms
step:224/2245 train_time:13617ms step_avg:60.79ms
step:225/2245 train_time:13678ms step_avg:60.79ms
step:226/2245 train_time:13738ms step_avg:60.79ms
step:227/2245 train_time:13799ms step_avg:60.79ms
step:228/2245 train_time:13860ms step_avg:60.79ms
step:229/2245 train_time:13922ms step_avg:60.79ms
step:230/2245 train_time:13982ms step_avg:60.79ms
step:231/2245 train_time:14044ms step_avg:60.79ms
step:232/2245 train_time:14104ms step_avg:60.79ms
step:233/2245 train_time:14165ms step_avg:60.79ms
step:234/2245 train_time:14224ms step_avg:60.79ms
step:235/2245 train_time:14286ms step_avg:60.79ms
step:236/2245 train_time:14346ms step_avg:60.79ms
step:237/2245 train_time:14408ms step_avg:60.79ms
step:238/2245 train_time:14467ms step_avg:60.79ms
step:239/2245 train_time:14529ms step_avg:60.79ms
step:240/2245 train_time:14589ms step_avg:60.79ms
step:241/2245 train_time:14651ms step_avg:60.79ms
step:242/2245 train_time:14711ms step_avg:60.79ms
step:243/2245 train_time:14773ms step_avg:60.79ms
step:244/2245 train_time:14832ms step_avg:60.79ms
step:245/2245 train_time:14894ms step_avg:60.79ms
step:246/2245 train_time:14954ms step_avg:60.79ms
step:247/2245 train_time:15017ms step_avg:60.80ms
step:248/2245 train_time:15075ms step_avg:60.79ms
step:249/2245 train_time:15136ms step_avg:60.79ms
step:250/2245 train_time:15196ms step_avg:60.78ms
step:250/2245 val_loss:4.0687 train_time:15258ms step_avg:61.03ms
step:251/2245 train_time:15278ms step_avg:60.87ms
step:252/2245 train_time:15318ms step_avg:60.79ms
step:253/2245 train_time:15385ms step_avg:60.81ms
step:254/2245 train_time:15448ms step_avg:60.82ms
step:255/2245 train_time:15510ms step_avg:60.83ms
step:256/2245 train_time:15571ms step_avg:60.83ms
step:257/2245 train_time:15633ms step_avg:60.83ms
step:258/2245 train_time:15693ms step_avg:60.82ms
step:259/2245 train_time:15755ms step_avg:60.83ms
step:260/2245 train_time:15813ms step_avg:60.82ms
step:261/2245 train_time:15874ms step_avg:60.82ms
step:262/2245 train_time:15933ms step_avg:60.81ms
step:263/2245 train_time:15994ms step_avg:60.81ms
step:264/2245 train_time:16052ms step_avg:60.80ms
step:265/2245 train_time:16113ms step_avg:60.80ms
step:266/2245 train_time:16172ms step_avg:60.80ms
step:267/2245 train_time:16235ms step_avg:60.81ms
step:268/2245 train_time:16297ms step_avg:60.81ms
step:269/2245 train_time:16361ms step_avg:60.82ms
step:270/2245 train_time:16422ms step_avg:60.82ms
step:271/2245 train_time:16485ms step_avg:60.83ms
step:272/2245 train_time:16544ms step_avg:60.82ms
step:273/2245 train_time:16605ms step_avg:60.82ms
step:274/2245 train_time:16665ms step_avg:60.82ms
step:275/2245 train_time:16726ms step_avg:60.82ms
step:276/2245 train_time:16785ms step_avg:60.82ms
step:277/2245 train_time:16847ms step_avg:60.82ms
step:278/2245 train_time:16905ms step_avg:60.81ms
step:279/2245 train_time:16967ms step_avg:60.81ms
step:280/2245 train_time:17026ms step_avg:60.81ms
step:281/2245 train_time:17087ms step_avg:60.81ms
step:282/2245 train_time:17147ms step_avg:60.80ms
step:283/2245 train_time:17208ms step_avg:60.81ms
step:284/2245 train_time:17268ms step_avg:60.80ms
step:285/2245 train_time:17331ms step_avg:60.81ms
step:286/2245 train_time:17390ms step_avg:60.81ms
step:287/2245 train_time:17453ms step_avg:60.81ms
step:288/2245 train_time:17513ms step_avg:60.81ms
step:289/2245 train_time:17575ms step_avg:60.81ms
step:290/2245 train_time:17636ms step_avg:60.81ms
step:291/2245 train_time:17698ms step_avg:60.82ms
step:292/2245 train_time:17757ms step_avg:60.81ms
step:293/2245 train_time:17819ms step_avg:60.82ms
step:294/2245 train_time:17879ms step_avg:60.81ms
step:295/2245 train_time:17940ms step_avg:60.81ms
step:296/2245 train_time:17999ms step_avg:60.81ms
step:297/2245 train_time:18061ms step_avg:60.81ms
step:298/2245 train_time:18120ms step_avg:60.81ms
step:299/2245 train_time:18181ms step_avg:60.81ms
step:300/2245 train_time:18240ms step_avg:60.80ms
step:301/2245 train_time:18303ms step_avg:60.81ms
step:302/2245 train_time:18362ms step_avg:60.80ms
step:303/2245 train_time:18423ms step_avg:60.80ms
step:304/2245 train_time:18483ms step_avg:60.80ms
step:305/2245 train_time:18544ms step_avg:60.80ms
step:306/2245 train_time:18604ms step_avg:60.80ms
step:307/2245 train_time:18666ms step_avg:60.80ms
step:308/2245 train_time:18726ms step_avg:60.80ms
step:309/2245 train_time:18787ms step_avg:60.80ms
step:310/2245 train_time:18847ms step_avg:60.80ms
step:311/2245 train_time:18908ms step_avg:60.80ms
step:312/2245 train_time:18967ms step_avg:60.79ms
step:313/2245 train_time:19029ms step_avg:60.79ms
step:314/2245 train_time:19088ms step_avg:60.79ms
step:315/2245 train_time:19149ms step_avg:60.79ms
step:316/2245 train_time:19209ms step_avg:60.79ms
step:317/2245 train_time:19271ms step_avg:60.79ms
step:318/2245 train_time:19331ms step_avg:60.79ms
step:319/2245 train_time:19393ms step_avg:60.79ms
step:320/2245 train_time:19453ms step_avg:60.79ms
step:321/2245 train_time:19515ms step_avg:60.79ms
step:322/2245 train_time:19575ms step_avg:60.79ms
step:323/2245 train_time:19637ms step_avg:60.79ms
step:324/2245 train_time:19697ms step_avg:60.79ms
step:325/2245 train_time:19758ms step_avg:60.79ms
step:326/2245 train_time:19819ms step_avg:60.79ms
step:327/2245 train_time:19881ms step_avg:60.80ms
step:328/2245 train_time:19939ms step_avg:60.79ms
step:329/2245 train_time:20001ms step_avg:60.79ms
step:330/2245 train_time:20061ms step_avg:60.79ms
step:331/2245 train_time:20122ms step_avg:60.79ms
step:332/2245 train_time:20181ms step_avg:60.79ms
step:333/2245 train_time:20243ms step_avg:60.79ms
step:334/2245 train_time:20302ms step_avg:60.78ms
step:335/2245 train_time:20363ms step_avg:60.78ms
step:336/2245 train_time:20422ms step_avg:60.78ms
step:337/2245 train_time:20483ms step_avg:60.78ms
step:338/2245 train_time:20543ms step_avg:60.78ms
step:339/2245 train_time:20605ms step_avg:60.78ms
step:340/2245 train_time:20664ms step_avg:60.78ms
step:341/2245 train_time:20727ms step_avg:60.78ms
step:342/2245 train_time:20787ms step_avg:60.78ms
step:343/2245 train_time:20849ms step_avg:60.79ms
step:344/2245 train_time:20909ms step_avg:60.78ms
step:345/2245 train_time:20970ms step_avg:60.78ms
step:346/2245 train_time:21029ms step_avg:60.78ms
step:347/2245 train_time:21092ms step_avg:60.78ms
step:348/2245 train_time:21151ms step_avg:60.78ms
step:349/2245 train_time:21212ms step_avg:60.78ms
step:350/2245 train_time:21272ms step_avg:60.78ms
step:351/2245 train_time:21334ms step_avg:60.78ms
step:352/2245 train_time:21394ms step_avg:60.78ms
step:353/2245 train_time:21457ms step_avg:60.78ms
step:354/2245 train_time:21516ms step_avg:60.78ms
step:355/2245 train_time:21578ms step_avg:60.78ms
step:356/2245 train_time:21638ms step_avg:60.78ms
step:357/2245 train_time:21700ms step_avg:60.78ms
step:358/2245 train_time:21759ms step_avg:60.78ms
step:359/2245 train_time:21821ms step_avg:60.78ms
step:360/2245 train_time:21881ms step_avg:60.78ms
step:361/2245 train_time:21945ms step_avg:60.79ms
step:362/2245 train_time:22002ms step_avg:60.78ms
step:363/2245 train_time:22063ms step_avg:60.78ms
step:364/2245 train_time:22122ms step_avg:60.78ms
step:365/2245 train_time:22184ms step_avg:60.78ms
step:366/2245 train_time:22243ms step_avg:60.77ms
step:367/2245 train_time:22304ms step_avg:60.77ms
step:368/2245 train_time:22363ms step_avg:60.77ms
step:369/2245 train_time:22424ms step_avg:60.77ms
step:370/2245 train_time:22484ms step_avg:60.77ms
step:371/2245 train_time:22547ms step_avg:60.77ms
step:372/2245 train_time:22606ms step_avg:60.77ms
step:373/2245 train_time:22668ms step_avg:60.77ms
step:374/2245 train_time:22728ms step_avg:60.77ms
step:375/2245 train_time:22789ms step_avg:60.77ms
step:376/2245 train_time:22849ms step_avg:60.77ms
step:377/2245 train_time:22911ms step_avg:60.77ms
step:378/2245 train_time:22970ms step_avg:60.77ms
step:379/2245 train_time:23032ms step_avg:60.77ms
step:380/2245 train_time:23091ms step_avg:60.77ms
step:381/2245 train_time:23153ms step_avg:60.77ms
step:382/2245 train_time:23212ms step_avg:60.76ms
step:383/2245 train_time:23274ms step_avg:60.77ms
step:384/2245 train_time:23333ms step_avg:60.76ms
step:385/2245 train_time:23396ms step_avg:60.77ms
step:386/2245 train_time:23456ms step_avg:60.77ms
step:387/2245 train_time:23519ms step_avg:60.77ms
step:388/2245 train_time:23579ms step_avg:60.77ms
step:389/2245 train_time:23640ms step_avg:60.77ms
step:390/2245 train_time:23699ms step_avg:60.77ms
step:391/2245 train_time:23761ms step_avg:60.77ms
step:392/2245 train_time:23821ms step_avg:60.77ms
step:393/2245 train_time:23882ms step_avg:60.77ms
step:394/2245 train_time:23941ms step_avg:60.76ms
step:395/2245 train_time:24003ms step_avg:60.77ms
step:396/2245 train_time:24062ms step_avg:60.76ms
step:397/2245 train_time:24124ms step_avg:60.77ms
step:398/2245 train_time:24183ms step_avg:60.76ms
step:399/2245 train_time:24244ms step_avg:60.76ms
step:400/2245 train_time:24303ms step_avg:60.76ms
step:401/2245 train_time:24365ms step_avg:60.76ms
step:402/2245 train_time:24424ms step_avg:60.76ms
step:403/2245 train_time:24486ms step_avg:60.76ms
step:404/2245 train_time:24546ms step_avg:60.76ms
step:405/2245 train_time:24607ms step_avg:60.76ms
step:406/2245 train_time:24666ms step_avg:60.75ms
step:407/2245 train_time:24728ms step_avg:60.76ms
step:408/2245 train_time:24787ms step_avg:60.75ms
step:409/2245 train_time:24848ms step_avg:60.75ms
step:410/2245 train_time:24907ms step_avg:60.75ms
step:411/2245 train_time:24968ms step_avg:60.75ms
step:412/2245 train_time:25028ms step_avg:60.75ms
step:413/2245 train_time:25090ms step_avg:60.75ms
step:414/2245 train_time:25148ms step_avg:60.74ms
step:415/2245 train_time:25209ms step_avg:60.75ms
step:416/2245 train_time:25269ms step_avg:60.74ms
step:417/2245 train_time:25330ms step_avg:60.74ms
step:418/2245 train_time:25389ms step_avg:60.74ms
step:419/2245 train_time:25451ms step_avg:60.74ms
step:420/2245 train_time:25511ms step_avg:60.74ms
step:421/2245 train_time:25573ms step_avg:60.74ms
step:422/2245 train_time:25632ms step_avg:60.74ms
step:423/2245 train_time:25695ms step_avg:60.74ms
step:424/2245 train_time:25755ms step_avg:60.74ms
step:425/2245 train_time:25816ms step_avg:60.74ms
step:426/2245 train_time:25876ms step_avg:60.74ms
step:427/2245 train_time:25937ms step_avg:60.74ms
step:428/2245 train_time:25996ms step_avg:60.74ms
step:429/2245 train_time:26059ms step_avg:60.74ms
step:430/2245 train_time:26118ms step_avg:60.74ms
step:431/2245 train_time:26179ms step_avg:60.74ms
step:432/2245 train_time:26239ms step_avg:60.74ms
step:433/2245 train_time:26300ms step_avg:60.74ms
step:434/2245 train_time:26359ms step_avg:60.74ms
step:435/2245 train_time:26421ms step_avg:60.74ms
step:436/2245 train_time:26480ms step_avg:60.73ms
step:437/2245 train_time:26542ms step_avg:60.74ms
step:438/2245 train_time:26601ms step_avg:60.73ms
step:439/2245 train_time:26662ms step_avg:60.73ms
step:440/2245 train_time:26721ms step_avg:60.73ms
step:441/2245 train_time:26783ms step_avg:60.73ms
step:442/2245 train_time:26842ms step_avg:60.73ms
step:443/2245 train_time:26903ms step_avg:60.73ms
step:444/2245 train_time:26962ms step_avg:60.73ms
step:445/2245 train_time:27024ms step_avg:60.73ms
step:446/2245 train_time:27083ms step_avg:60.72ms
step:447/2245 train_time:27144ms step_avg:60.73ms
step:448/2245 train_time:27203ms step_avg:60.72ms
step:449/2245 train_time:27265ms step_avg:60.72ms
step:450/2245 train_time:27324ms step_avg:60.72ms
step:451/2245 train_time:27385ms step_avg:60.72ms
step:452/2245 train_time:27444ms step_avg:60.72ms
step:453/2245 train_time:27505ms step_avg:60.72ms
step:454/2245 train_time:27564ms step_avg:60.71ms
step:455/2245 train_time:27625ms step_avg:60.72ms
step:456/2245 train_time:27684ms step_avg:60.71ms
step:457/2245 train_time:27746ms step_avg:60.71ms
step:458/2245 train_time:27805ms step_avg:60.71ms
step:459/2245 train_time:27866ms step_avg:60.71ms
step:460/2245 train_time:27925ms step_avg:60.71ms
step:461/2245 train_time:27987ms step_avg:60.71ms
step:462/2245 train_time:28045ms step_avg:60.70ms
step:463/2245 train_time:28107ms step_avg:60.71ms
step:464/2245 train_time:28166ms step_avg:60.70ms
step:465/2245 train_time:28228ms step_avg:60.70ms
step:466/2245 train_time:28287ms step_avg:60.70ms
step:467/2245 train_time:28349ms step_avg:60.71ms
step:468/2245 train_time:28408ms step_avg:60.70ms
step:469/2245 train_time:28469ms step_avg:60.70ms
step:470/2245 train_time:28528ms step_avg:60.70ms
step:471/2245 train_time:28590ms step_avg:60.70ms
step:472/2245 train_time:28649ms step_avg:60.70ms
step:473/2245 train_time:28710ms step_avg:60.70ms
step:474/2245 train_time:28770ms step_avg:60.70ms
step:475/2245 train_time:28831ms step_avg:60.70ms
step:476/2245 train_time:28891ms step_avg:60.70ms
step:477/2245 train_time:28952ms step_avg:60.70ms
step:478/2245 train_time:29012ms step_avg:60.69ms
step:479/2245 train_time:29074ms step_avg:60.70ms
step:480/2245 train_time:29134ms step_avg:60.70ms
step:481/2245 train_time:29196ms step_avg:60.70ms
step:482/2245 train_time:29255ms step_avg:60.70ms
step:483/2245 train_time:29317ms step_avg:60.70ms
step:484/2245 train_time:29376ms step_avg:60.69ms
step:485/2245 train_time:29438ms step_avg:60.70ms
step:486/2245 train_time:29497ms step_avg:60.69ms
step:487/2245 train_time:29559ms step_avg:60.70ms
step:488/2245 train_time:29619ms step_avg:60.69ms
step:489/2245 train_time:29681ms step_avg:60.70ms
step:490/2245 train_time:29740ms step_avg:60.69ms
step:491/2245 train_time:29801ms step_avg:60.70ms
step:492/2245 train_time:29860ms step_avg:60.69ms
step:493/2245 train_time:29922ms step_avg:60.69ms
step:494/2245 train_time:29982ms step_avg:60.69ms
step:495/2245 train_time:30043ms step_avg:60.69ms
step:496/2245 train_time:30102ms step_avg:60.69ms
step:497/2245 train_time:30163ms step_avg:60.69ms
step:498/2245 train_time:30222ms step_avg:60.69ms
step:499/2245 train_time:30283ms step_avg:60.69ms
step:500/2245 train_time:30343ms step_avg:60.69ms
step:500/2245 val_loss:3.8135 train_time:30405ms step_avg:60.81ms
step:501/2245 train_time:30425ms step_avg:60.73ms
step:502/2245 train_time:30467ms step_avg:60.69ms
step:503/2245 train_time:30532ms step_avg:60.70ms
step:504/2245 train_time:30593ms step_avg:60.70ms
step:505/2245 train_time:30655ms step_avg:60.70ms
step:506/2245 train_time:30715ms step_avg:60.70ms
step:507/2245 train_time:30776ms step_avg:60.70ms
step:508/2245 train_time:30834ms step_avg:60.70ms
step:509/2245 train_time:30895ms step_avg:60.70ms
step:510/2245 train_time:30953ms step_avg:60.69ms
step:511/2245 train_time:31014ms step_avg:60.69ms
step:512/2245 train_time:31072ms step_avg:60.69ms
step:513/2245 train_time:31133ms step_avg:60.69ms
step:514/2245 train_time:31193ms step_avg:60.69ms
step:515/2245 train_time:31253ms step_avg:60.69ms
step:516/2245 train_time:31313ms step_avg:60.68ms
step:517/2245 train_time:31377ms step_avg:60.69ms
step:518/2245 train_time:31438ms step_avg:60.69ms
step:519/2245 train_time:31501ms step_avg:60.69ms
step:520/2245 train_time:31560ms step_avg:60.69ms
step:521/2245 train_time:31622ms step_avg:60.69ms
step:522/2245 train_time:31681ms step_avg:60.69ms
step:523/2245 train_time:31742ms step_avg:60.69ms
step:524/2245 train_time:31802ms step_avg:60.69ms
step:525/2245 train_time:31863ms step_avg:60.69ms
step:526/2245 train_time:31922ms step_avg:60.69ms
step:527/2245 train_time:31984ms step_avg:60.69ms
step:528/2245 train_time:32042ms step_avg:60.69ms
step:529/2245 train_time:32103ms step_avg:60.69ms
step:530/2245 train_time:32162ms step_avg:60.68ms
step:531/2245 train_time:32223ms step_avg:60.68ms
step:532/2245 train_time:32282ms step_avg:60.68ms
step:533/2245 train_time:32344ms step_avg:60.68ms
step:534/2245 train_time:32404ms step_avg:60.68ms
step:535/2245 train_time:32467ms step_avg:60.69ms
step:536/2245 train_time:32527ms step_avg:60.68ms
step:537/2245 train_time:32589ms step_avg:60.69ms
step:538/2245 train_time:32649ms step_avg:60.69ms
step:539/2245 train_time:32711ms step_avg:60.69ms
step:540/2245 train_time:32770ms step_avg:60.69ms
step:541/2245 train_time:32832ms step_avg:60.69ms
step:542/2245 train_time:32891ms step_avg:60.69ms
step:543/2245 train_time:32952ms step_avg:60.69ms
step:544/2245 train_time:33011ms step_avg:60.68ms
step:545/2245 train_time:33072ms step_avg:60.68ms
step:546/2245 train_time:33132ms step_avg:60.68ms
step:547/2245 train_time:33193ms step_avg:60.68ms
step:548/2245 train_time:33252ms step_avg:60.68ms
step:549/2245 train_time:33314ms step_avg:60.68ms
step:550/2245 train_time:33374ms step_avg:60.68ms
step:551/2245 train_time:33436ms step_avg:60.68ms
step:552/2245 train_time:33495ms step_avg:60.68ms
step:553/2245 train_time:33556ms step_avg:60.68ms
step:554/2245 train_time:33615ms step_avg:60.68ms
step:555/2245 train_time:33677ms step_avg:60.68ms
step:556/2245 train_time:33736ms step_avg:60.68ms
step:557/2245 train_time:33797ms step_avg:60.68ms
step:558/2245 train_time:33856ms step_avg:60.67ms
step:559/2245 train_time:33918ms step_avg:60.68ms
step:560/2245 train_time:33977ms step_avg:60.67ms
step:561/2245 train_time:34039ms step_avg:60.68ms
step:562/2245 train_time:34098ms step_avg:60.67ms
step:563/2245 train_time:34159ms step_avg:60.67ms
step:564/2245 train_time:34218ms step_avg:60.67ms
step:565/2245 train_time:34279ms step_avg:60.67ms
step:566/2245 train_time:34338ms step_avg:60.67ms
step:567/2245 train_time:34400ms step_avg:60.67ms
step:568/2245 train_time:34459ms step_avg:60.67ms
step:569/2245 train_time:34520ms step_avg:60.67ms
step:570/2245 train_time:34579ms step_avg:60.67ms
step:571/2245 train_time:34641ms step_avg:60.67ms
step:572/2245 train_time:34699ms step_avg:60.66ms
step:573/2245 train_time:34761ms step_avg:60.66ms
step:574/2245 train_time:34820ms step_avg:60.66ms
step:575/2245 train_time:34881ms step_avg:60.66ms
step:576/2245 train_time:34941ms step_avg:60.66ms
step:577/2245 train_time:35003ms step_avg:60.66ms
step:578/2245 train_time:35062ms step_avg:60.66ms
step:579/2245 train_time:35124ms step_avg:60.66ms
step:580/2245 train_time:35183ms step_avg:60.66ms
step:581/2245 train_time:35244ms step_avg:60.66ms
step:582/2245 train_time:35303ms step_avg:60.66ms
step:583/2245 train_time:35365ms step_avg:60.66ms
step:584/2245 train_time:35424ms step_avg:60.66ms
step:585/2245 train_time:35486ms step_avg:60.66ms
step:586/2245 train_time:35546ms step_avg:60.66ms
step:587/2245 train_time:35608ms step_avg:60.66ms
step:588/2245 train_time:35667ms step_avg:60.66ms
step:589/2245 train_time:35729ms step_avg:60.66ms
step:590/2245 train_time:35789ms step_avg:60.66ms
step:591/2245 train_time:35851ms step_avg:60.66ms
step:592/2245 train_time:35910ms step_avg:60.66ms
step:593/2245 train_time:35972ms step_avg:60.66ms
step:594/2245 train_time:36032ms step_avg:60.66ms
step:595/2245 train_time:36094ms step_avg:60.66ms
step:596/2245 train_time:36153ms step_avg:60.66ms
step:597/2245 train_time:36215ms step_avg:60.66ms
step:598/2245 train_time:36275ms step_avg:60.66ms
step:599/2245 train_time:36336ms step_avg:60.66ms
step:600/2245 train_time:36395ms step_avg:60.66ms
step:601/2245 train_time:36458ms step_avg:60.66ms
step:602/2245 train_time:36517ms step_avg:60.66ms
step:603/2245 train_time:36578ms step_avg:60.66ms
step:604/2245 train_time:36637ms step_avg:60.66ms
step:605/2245 train_time:36699ms step_avg:60.66ms
step:606/2245 train_time:36758ms step_avg:60.66ms
step:607/2245 train_time:36820ms step_avg:60.66ms
step:608/2245 train_time:36879ms step_avg:60.66ms
step:609/2245 train_time:36941ms step_avg:60.66ms
step:610/2245 train_time:37000ms step_avg:60.66ms
step:611/2245 train_time:37062ms step_avg:60.66ms
step:612/2245 train_time:37121ms step_avg:60.66ms
step:613/2245 train_time:37183ms step_avg:60.66ms
step:614/2245 train_time:37242ms step_avg:60.65ms
step:615/2245 train_time:37303ms step_avg:60.66ms
step:616/2245 train_time:37362ms step_avg:60.65ms
step:617/2245 train_time:37424ms step_avg:60.65ms
step:618/2245 train_time:37483ms step_avg:60.65ms
step:619/2245 train_time:37544ms step_avg:60.65ms
step:620/2245 train_time:37604ms step_avg:60.65ms
step:621/2245 train_time:37667ms step_avg:60.65ms
step:622/2245 train_time:37727ms step_avg:60.65ms
step:623/2245 train_time:37788ms step_avg:60.66ms
step:624/2245 train_time:37848ms step_avg:60.65ms
step:625/2245 train_time:37910ms step_avg:60.66ms
step:626/2245 train_time:37970ms step_avg:60.66ms
step:627/2245 train_time:38032ms step_avg:60.66ms
step:628/2245 train_time:38092ms step_avg:60.66ms
step:629/2245 train_time:38153ms step_avg:60.66ms
step:630/2245 train_time:38213ms step_avg:60.66ms
step:631/2245 train_time:38276ms step_avg:60.66ms
step:632/2245 train_time:38336ms step_avg:60.66ms
step:633/2245 train_time:38397ms step_avg:60.66ms
step:634/2245 train_time:38456ms step_avg:60.66ms
step:635/2245 train_time:38519ms step_avg:60.66ms
step:636/2245 train_time:38578ms step_avg:60.66ms
step:637/2245 train_time:38639ms step_avg:60.66ms
step:638/2245 train_time:38697ms step_avg:60.65ms
step:639/2245 train_time:38758ms step_avg:60.65ms
step:640/2245 train_time:38817ms step_avg:60.65ms
step:641/2245 train_time:38879ms step_avg:60.65ms
step:642/2245 train_time:38938ms step_avg:60.65ms
step:643/2245 train_time:39000ms step_avg:60.65ms
step:644/2245 train_time:39060ms step_avg:60.65ms
step:645/2245 train_time:39123ms step_avg:60.66ms
step:646/2245 train_time:39182ms step_avg:60.65ms
step:647/2245 train_time:39244ms step_avg:60.65ms
step:648/2245 train_time:39303ms step_avg:60.65ms
step:649/2245 train_time:39364ms step_avg:60.65ms
step:650/2245 train_time:39424ms step_avg:60.65ms
step:651/2245 train_time:39486ms step_avg:60.65ms
step:652/2245 train_time:39545ms step_avg:60.65ms
step:653/2245 train_time:39607ms step_avg:60.65ms
step:654/2245 train_time:39666ms step_avg:60.65ms
step:655/2245 train_time:39728ms step_avg:60.65ms
step:656/2245 train_time:39788ms step_avg:60.65ms
step:657/2245 train_time:39851ms step_avg:60.66ms
step:658/2245 train_time:39912ms step_avg:60.66ms
step:659/2245 train_time:39975ms step_avg:60.66ms
step:660/2245 train_time:40034ms step_avg:60.66ms
step:661/2245 train_time:40095ms step_avg:60.66ms
step:662/2245 train_time:40155ms step_avg:60.66ms
step:663/2245 train_time:40216ms step_avg:60.66ms
step:664/2245 train_time:40276ms step_avg:60.66ms
step:665/2245 train_time:40337ms step_avg:60.66ms
step:666/2245 train_time:40396ms step_avg:60.66ms
step:667/2245 train_time:40458ms step_avg:60.66ms
step:668/2245 train_time:40517ms step_avg:60.65ms
step:669/2245 train_time:40579ms step_avg:60.66ms
step:670/2245 train_time:40638ms step_avg:60.65ms
step:671/2245 train_time:40699ms step_avg:60.65ms
step:672/2245 train_time:40758ms step_avg:60.65ms
step:673/2245 train_time:40820ms step_avg:60.65ms
step:674/2245 train_time:40880ms step_avg:60.65ms
step:675/2245 train_time:40942ms step_avg:60.66ms
step:676/2245 train_time:41001ms step_avg:60.65ms
step:677/2245 train_time:41063ms step_avg:60.65ms
step:678/2245 train_time:41122ms step_avg:60.65ms
step:679/2245 train_time:41184ms step_avg:60.65ms
step:680/2245 train_time:41243ms step_avg:60.65ms
step:681/2245 train_time:41304ms step_avg:60.65ms
step:682/2245 train_time:41363ms step_avg:60.65ms
step:683/2245 train_time:41425ms step_avg:60.65ms
step:684/2245 train_time:41485ms step_avg:60.65ms
step:685/2245 train_time:41546ms step_avg:60.65ms
step:686/2245 train_time:41606ms step_avg:60.65ms
step:687/2245 train_time:41668ms step_avg:60.65ms
step:688/2245 train_time:41727ms step_avg:60.65ms
step:689/2245 train_time:41789ms step_avg:60.65ms
step:690/2245 train_time:41849ms step_avg:60.65ms
step:691/2245 train_time:41911ms step_avg:60.65ms
step:692/2245 train_time:41971ms step_avg:60.65ms
step:693/2245 train_time:42033ms step_avg:60.65ms
step:694/2245 train_time:42092ms step_avg:60.65ms
step:695/2245 train_time:42154ms step_avg:60.65ms
step:696/2245 train_time:42213ms step_avg:60.65ms
step:697/2245 train_time:42275ms step_avg:60.65ms
step:698/2245 train_time:42334ms step_avg:60.65ms
step:699/2245 train_time:42396ms step_avg:60.65ms
step:700/2245 train_time:42455ms step_avg:60.65ms
step:701/2245 train_time:42517ms step_avg:60.65ms
step:702/2245 train_time:42577ms step_avg:60.65ms
step:703/2245 train_time:42639ms step_avg:60.65ms
step:704/2245 train_time:42697ms step_avg:60.65ms
step:705/2245 train_time:42758ms step_avg:60.65ms
step:706/2245 train_time:42818ms step_avg:60.65ms
step:707/2245 train_time:42880ms step_avg:60.65ms
step:708/2245 train_time:42939ms step_avg:60.65ms
step:709/2245 train_time:43001ms step_avg:60.65ms
step:710/2245 train_time:43061ms step_avg:60.65ms
step:711/2245 train_time:43122ms step_avg:60.65ms
step:712/2245 train_time:43181ms step_avg:60.65ms
step:713/2245 train_time:43242ms step_avg:60.65ms
step:714/2245 train_time:43302ms step_avg:60.65ms
step:715/2245 train_time:43363ms step_avg:60.65ms
step:716/2245 train_time:43423ms step_avg:60.65ms
step:717/2245 train_time:43484ms step_avg:60.65ms
step:718/2245 train_time:43543ms step_avg:60.65ms
step:719/2245 train_time:43605ms step_avg:60.65ms
step:720/2245 train_time:43665ms step_avg:60.65ms
step:721/2245 train_time:43727ms step_avg:60.65ms
step:722/2245 train_time:43787ms step_avg:60.65ms
step:723/2245 train_time:43849ms step_avg:60.65ms
step:724/2245 train_time:43909ms step_avg:60.65ms
step:725/2245 train_time:43971ms step_avg:60.65ms
step:726/2245 train_time:44031ms step_avg:60.65ms
step:727/2245 train_time:44094ms step_avg:60.65ms
step:728/2245 train_time:44153ms step_avg:60.65ms
step:729/2245 train_time:44215ms step_avg:60.65ms
step:730/2245 train_time:44275ms step_avg:60.65ms
step:731/2245 train_time:44336ms step_avg:60.65ms
step:732/2245 train_time:44395ms step_avg:60.65ms
step:733/2245 train_time:44457ms step_avg:60.65ms
step:734/2245 train_time:44517ms step_avg:60.65ms
step:735/2245 train_time:44578ms step_avg:60.65ms
step:736/2245 train_time:44638ms step_avg:60.65ms
step:737/2245 train_time:44699ms step_avg:60.65ms
step:738/2245 train_time:44759ms step_avg:60.65ms
step:739/2245 train_time:44822ms step_avg:60.65ms
step:740/2245 train_time:44882ms step_avg:60.65ms
step:741/2245 train_time:44945ms step_avg:60.65ms
step:742/2245 train_time:45005ms step_avg:60.65ms
step:743/2245 train_time:45067ms step_avg:60.66ms
step:744/2245 train_time:45127ms step_avg:60.65ms
step:745/2245 train_time:45190ms step_avg:60.66ms
step:746/2245 train_time:45251ms step_avg:60.66ms
step:747/2245 train_time:45314ms step_avg:60.66ms
step:748/2245 train_time:45374ms step_avg:60.66ms
step:749/2245 train_time:45437ms step_avg:60.66ms
step:750/2245 train_time:45498ms step_avg:60.66ms
step:750/2245 val_loss:3.6658 train_time:45561ms step_avg:60.75ms
step:751/2245 train_time:45581ms step_avg:60.69ms
step:752/2245 train_time:45622ms step_avg:60.67ms
step:753/2245 train_time:45692ms step_avg:60.68ms
step:754/2245 train_time:45757ms step_avg:60.69ms
step:755/2245 train_time:45818ms step_avg:60.69ms
step:756/2245 train_time:45877ms step_avg:60.68ms
step:757/2245 train_time:45939ms step_avg:60.69ms
step:758/2245 train_time:45998ms step_avg:60.68ms
step:759/2245 train_time:46060ms step_avg:60.68ms
step:760/2245 train_time:46119ms step_avg:60.68ms
step:761/2245 train_time:46180ms step_avg:60.68ms
step:762/2245 train_time:46239ms step_avg:60.68ms
step:763/2245 train_time:46301ms step_avg:60.68ms
step:764/2245 train_time:46360ms step_avg:60.68ms
step:765/2245 train_time:46421ms step_avg:60.68ms
step:766/2245 train_time:46485ms step_avg:60.69ms
step:767/2245 train_time:46549ms step_avg:60.69ms
step:768/2245 train_time:46611ms step_avg:60.69ms
step:769/2245 train_time:46675ms step_avg:60.70ms
step:770/2245 train_time:46735ms step_avg:60.69ms
step:771/2245 train_time:46798ms step_avg:60.70ms
step:772/2245 train_time:46858ms step_avg:60.70ms
step:773/2245 train_time:46919ms step_avg:60.70ms
step:774/2245 train_time:46979ms step_avg:60.70ms
step:775/2245 train_time:47041ms step_avg:60.70ms
step:776/2245 train_time:47100ms step_avg:60.70ms
step:777/2245 train_time:47162ms step_avg:60.70ms
step:778/2245 train_time:47221ms step_avg:60.70ms
step:779/2245 train_time:47283ms step_avg:60.70ms
step:780/2245 train_time:47342ms step_avg:60.70ms
step:781/2245 train_time:47405ms step_avg:60.70ms
step:782/2245 train_time:47465ms step_avg:60.70ms
step:783/2245 train_time:47528ms step_avg:60.70ms
step:784/2245 train_time:47590ms step_avg:60.70ms
step:785/2245 train_time:47654ms step_avg:60.71ms
step:786/2245 train_time:47714ms step_avg:60.70ms
step:787/2245 train_time:47776ms step_avg:60.71ms
step:788/2245 train_time:47836ms step_avg:60.71ms
step:789/2245 train_time:47898ms step_avg:60.71ms
step:790/2245 train_time:47958ms step_avg:60.71ms
step:791/2245 train_time:48019ms step_avg:60.71ms
step:792/2245 train_time:48079ms step_avg:60.71ms
step:793/2245 train_time:48140ms step_avg:60.71ms
step:794/2245 train_time:48200ms step_avg:60.71ms
step:795/2245 train_time:48261ms step_avg:60.71ms
step:796/2245 train_time:48321ms step_avg:60.70ms
step:797/2245 train_time:48382ms step_avg:60.71ms
step:798/2245 train_time:48442ms step_avg:60.70ms
step:799/2245 train_time:48506ms step_avg:60.71ms
step:800/2245 train_time:48568ms step_avg:60.71ms
step:801/2245 train_time:48631ms step_avg:60.71ms
step:802/2245 train_time:48692ms step_avg:60.71ms
step:803/2245 train_time:48755ms step_avg:60.72ms
step:804/2245 train_time:48814ms step_avg:60.71ms
step:805/2245 train_time:48877ms step_avg:60.72ms
step:806/2245 train_time:48937ms step_avg:60.72ms
step:807/2245 train_time:48999ms step_avg:60.72ms
step:808/2245 train_time:49059ms step_avg:60.72ms
step:809/2245 train_time:49120ms step_avg:60.72ms
step:810/2245 train_time:49180ms step_avg:60.72ms
step:811/2245 train_time:49242ms step_avg:60.72ms
step:812/2245 train_time:49302ms step_avg:60.72ms
step:813/2245 train_time:49364ms step_avg:60.72ms
step:814/2245 train_time:49424ms step_avg:60.72ms
step:815/2245 train_time:49487ms step_avg:60.72ms
step:816/2245 train_time:49548ms step_avg:60.72ms
step:817/2245 train_time:49611ms step_avg:60.72ms
step:818/2245 train_time:49672ms step_avg:60.72ms
step:819/2245 train_time:49734ms step_avg:60.73ms
step:820/2245 train_time:49795ms step_avg:60.73ms
step:821/2245 train_time:49857ms step_avg:60.73ms
step:822/2245 train_time:49917ms step_avg:60.73ms
step:823/2245 train_time:49980ms step_avg:60.73ms
step:824/2245 train_time:50039ms step_avg:60.73ms
step:825/2245 train_time:50101ms step_avg:60.73ms
step:826/2245 train_time:50161ms step_avg:60.73ms
step:827/2245 train_time:50222ms step_avg:60.73ms
step:828/2245 train_time:50282ms step_avg:60.73ms
step:829/2245 train_time:50344ms step_avg:60.73ms
step:830/2245 train_time:50404ms step_avg:60.73ms
step:831/2245 train_time:50467ms step_avg:60.73ms
step:832/2245 train_time:50528ms step_avg:60.73ms
step:833/2245 train_time:50590ms step_avg:60.73ms
step:834/2245 train_time:50651ms step_avg:60.73ms
step:835/2245 train_time:50714ms step_avg:60.74ms
step:836/2245 train_time:50774ms step_avg:60.73ms
step:837/2245 train_time:50836ms step_avg:60.74ms
step:838/2245 train_time:50896ms step_avg:60.74ms
step:839/2245 train_time:50958ms step_avg:60.74ms
step:840/2245 train_time:51018ms step_avg:60.74ms
step:841/2245 train_time:51080ms step_avg:60.74ms
step:842/2245 train_time:51139ms step_avg:60.74ms
step:843/2245 train_time:51201ms step_avg:60.74ms
step:844/2245 train_time:51261ms step_avg:60.74ms
step:845/2245 train_time:51323ms step_avg:60.74ms
step:846/2245 train_time:51383ms step_avg:60.74ms
step:847/2245 train_time:51446ms step_avg:60.74ms
step:848/2245 train_time:51507ms step_avg:60.74ms
step:849/2245 train_time:51570ms step_avg:60.74ms
step:850/2245 train_time:51630ms step_avg:60.74ms
step:851/2245 train_time:51693ms step_avg:60.74ms
step:852/2245 train_time:51754ms step_avg:60.74ms
step:853/2245 train_time:51816ms step_avg:60.75ms
step:854/2245 train_time:51876ms step_avg:60.74ms
step:855/2245 train_time:51938ms step_avg:60.75ms
step:856/2245 train_time:51997ms step_avg:60.74ms
step:857/2245 train_time:52059ms step_avg:60.75ms
step:858/2245 train_time:52119ms step_avg:60.74ms
step:859/2245 train_time:52181ms step_avg:60.75ms
step:860/2245 train_time:52240ms step_avg:60.74ms
step:861/2245 train_time:52302ms step_avg:60.75ms
step:862/2245 train_time:52363ms step_avg:60.75ms
step:863/2245 train_time:52425ms step_avg:60.75ms
step:864/2245 train_time:52486ms step_avg:60.75ms
step:865/2245 train_time:52548ms step_avg:60.75ms
step:866/2245 train_time:52609ms step_avg:60.75ms
step:867/2245 train_time:52672ms step_avg:60.75ms
step:868/2245 train_time:52732ms step_avg:60.75ms
step:869/2245 train_time:52795ms step_avg:60.75ms
step:870/2245 train_time:52855ms step_avg:60.75ms
step:871/2245 train_time:52917ms step_avg:60.75ms
step:872/2245 train_time:52976ms step_avg:60.75ms
step:873/2245 train_time:53038ms step_avg:60.75ms
step:874/2245 train_time:53098ms step_avg:60.75ms
step:875/2245 train_time:53160ms step_avg:60.75ms
step:876/2245 train_time:53220ms step_avg:60.75ms
step:877/2245 train_time:53281ms step_avg:60.75ms
step:878/2245 train_time:53342ms step_avg:60.75ms
step:879/2245 train_time:53405ms step_avg:60.76ms
step:880/2245 train_time:53465ms step_avg:60.76ms
step:881/2245 train_time:53527ms step_avg:60.76ms
step:882/2245 train_time:53587ms step_avg:60.76ms
step:883/2245 train_time:53650ms step_avg:60.76ms
step:884/2245 train_time:53710ms step_avg:60.76ms
step:885/2245 train_time:53773ms step_avg:60.76ms
step:886/2245 train_time:53833ms step_avg:60.76ms
step:887/2245 train_time:53895ms step_avg:60.76ms
step:888/2245 train_time:53955ms step_avg:60.76ms
step:889/2245 train_time:54017ms step_avg:60.76ms
step:890/2245 train_time:54077ms step_avg:60.76ms
step:891/2245 train_time:54138ms step_avg:60.76ms
step:892/2245 train_time:54199ms step_avg:60.76ms
step:893/2245 train_time:54261ms step_avg:60.76ms
step:894/2245 train_time:54321ms step_avg:60.76ms
step:895/2245 train_time:54384ms step_avg:60.76ms
step:896/2245 train_time:54444ms step_avg:60.76ms
step:897/2245 train_time:54506ms step_avg:60.77ms
step:898/2245 train_time:54566ms step_avg:60.76ms
step:899/2245 train_time:54629ms step_avg:60.77ms
step:900/2245 train_time:54690ms step_avg:60.77ms
step:901/2245 train_time:54753ms step_avg:60.77ms
step:902/2245 train_time:54813ms step_avg:60.77ms
step:903/2245 train_time:54875ms step_avg:60.77ms
step:904/2245 train_time:54936ms step_avg:60.77ms
step:905/2245 train_time:54998ms step_avg:60.77ms
step:906/2245 train_time:55058ms step_avg:60.77ms
step:907/2245 train_time:55120ms step_avg:60.77ms
step:908/2245 train_time:55180ms step_avg:60.77ms
step:909/2245 train_time:55243ms step_avg:60.77ms
step:910/2245 train_time:55303ms step_avg:60.77ms
step:911/2245 train_time:55365ms step_avg:60.77ms
step:912/2245 train_time:55425ms step_avg:60.77ms
step:913/2245 train_time:55487ms step_avg:60.77ms
step:914/2245 train_time:55547ms step_avg:60.77ms
step:915/2245 train_time:55610ms step_avg:60.78ms
step:916/2245 train_time:55670ms step_avg:60.78ms
step:917/2245 train_time:55733ms step_avg:60.78ms
step:918/2245 train_time:55793ms step_avg:60.78ms
step:919/2245 train_time:55856ms step_avg:60.78ms
step:920/2245 train_time:55915ms step_avg:60.78ms
step:921/2245 train_time:55977ms step_avg:60.78ms
step:922/2245 train_time:56037ms step_avg:60.78ms
step:923/2245 train_time:56098ms step_avg:60.78ms
step:924/2245 train_time:56158ms step_avg:60.78ms
step:925/2245 train_time:56220ms step_avg:60.78ms
step:926/2245 train_time:56281ms step_avg:60.78ms
step:927/2245 train_time:56344ms step_avg:60.78ms
step:928/2245 train_time:56404ms step_avg:60.78ms
step:929/2245 train_time:56467ms step_avg:60.78ms
step:930/2245 train_time:56526ms step_avg:60.78ms
step:931/2245 train_time:56589ms step_avg:60.78ms
step:932/2245 train_time:56649ms step_avg:60.78ms
step:933/2245 train_time:56712ms step_avg:60.78ms
step:934/2245 train_time:56772ms step_avg:60.78ms
step:935/2245 train_time:56834ms step_avg:60.78ms
step:936/2245 train_time:56894ms step_avg:60.78ms
step:937/2245 train_time:56957ms step_avg:60.79ms
step:938/2245 train_time:57017ms step_avg:60.79ms
step:939/2245 train_time:57079ms step_avg:60.79ms
step:940/2245 train_time:57138ms step_avg:60.79ms
step:941/2245 train_time:57201ms step_avg:60.79ms
step:942/2245 train_time:57261ms step_avg:60.79ms
step:943/2245 train_time:57323ms step_avg:60.79ms
step:944/2245 train_time:57383ms step_avg:60.79ms
step:945/2245 train_time:57445ms step_avg:60.79ms
step:946/2245 train_time:57505ms step_avg:60.79ms
step:947/2245 train_time:57568ms step_avg:60.79ms
step:948/2245 train_time:57628ms step_avg:60.79ms
step:949/2245 train_time:57691ms step_avg:60.79ms
step:950/2245 train_time:57751ms step_avg:60.79ms
step:951/2245 train_time:57813ms step_avg:60.79ms
step:952/2245 train_time:57873ms step_avg:60.79ms
step:953/2245 train_time:57936ms step_avg:60.79ms
step:954/2245 train_time:57996ms step_avg:60.79ms
step:955/2245 train_time:58058ms step_avg:60.79ms
step:956/2245 train_time:58117ms step_avg:60.79ms
step:957/2245 train_time:58179ms step_avg:60.79ms
step:958/2245 train_time:58239ms step_avg:60.79ms
step:959/2245 train_time:58302ms step_avg:60.79ms
step:960/2245 train_time:58362ms step_avg:60.79ms
step:961/2245 train_time:58425ms step_avg:60.80ms
step:962/2245 train_time:58485ms step_avg:60.80ms
step:963/2245 train_time:58548ms step_avg:60.80ms
step:964/2245 train_time:58608ms step_avg:60.80ms
step:965/2245 train_time:58671ms step_avg:60.80ms
step:966/2245 train_time:58731ms step_avg:60.80ms
step:967/2245 train_time:58793ms step_avg:60.80ms
step:968/2245 train_time:58854ms step_avg:60.80ms
step:969/2245 train_time:58917ms step_avg:60.80ms
step:970/2245 train_time:58977ms step_avg:60.80ms
step:971/2245 train_time:59039ms step_avg:60.80ms
step:972/2245 train_time:59099ms step_avg:60.80ms
step:973/2245 train_time:59160ms step_avg:60.80ms
step:974/2245 train_time:59220ms step_avg:60.80ms
step:975/2245 train_time:59283ms step_avg:60.80ms
step:976/2245 train_time:59342ms step_avg:60.80ms
step:977/2245 train_time:59405ms step_avg:60.80ms
step:978/2245 train_time:59465ms step_avg:60.80ms
step:979/2245 train_time:59527ms step_avg:60.80ms
step:980/2245 train_time:59587ms step_avg:60.80ms
step:981/2245 train_time:59650ms step_avg:60.80ms
step:982/2245 train_time:59710ms step_avg:60.80ms
step:983/2245 train_time:59773ms step_avg:60.81ms
step:984/2245 train_time:59833ms step_avg:60.81ms
step:985/2245 train_time:59896ms step_avg:60.81ms
step:986/2245 train_time:59956ms step_avg:60.81ms
step:987/2245 train_time:60018ms step_avg:60.81ms
step:988/2245 train_time:60077ms step_avg:60.81ms
step:989/2245 train_time:60140ms step_avg:60.81ms
step:990/2245 train_time:60199ms step_avg:60.81ms
step:991/2245 train_time:60261ms step_avg:60.81ms
step:992/2245 train_time:60321ms step_avg:60.81ms
step:993/2245 train_time:60384ms step_avg:60.81ms
step:994/2245 train_time:60444ms step_avg:60.81ms
step:995/2245 train_time:60507ms step_avg:60.81ms
step:996/2245 train_time:60567ms step_avg:60.81ms
step:997/2245 train_time:60629ms step_avg:60.81ms
step:998/2245 train_time:60689ms step_avg:60.81ms
step:999/2245 train_time:60752ms step_avg:60.81ms
step:1000/2245 train_time:60812ms step_avg:60.81ms
step:1000/2245 val_loss:3.5866 train_time:60875ms step_avg:60.88ms
step:1001/2245 train_time:60896ms step_avg:60.83ms
step:1002/2245 train_time:60936ms step_avg:60.81ms
step:1003/2245 train_time:61003ms step_avg:60.82ms
step:1004/2245 train_time:61066ms step_avg:60.82ms
step:1005/2245 train_time:61129ms step_avg:60.82ms
step:1006/2245 train_time:61189ms step_avg:60.82ms
step:1007/2245 train_time:61251ms step_avg:60.83ms
step:1008/2245 train_time:61310ms step_avg:60.82ms
step:1009/2245 train_time:61372ms step_avg:60.83ms
step:1010/2245 train_time:61432ms step_avg:60.82ms
step:1011/2245 train_time:61494ms step_avg:60.82ms
step:1012/2245 train_time:61553ms step_avg:60.82ms
step:1013/2245 train_time:61615ms step_avg:60.82ms
step:1014/2245 train_time:61674ms step_avg:60.82ms
step:1015/2245 train_time:61736ms step_avg:60.82ms
step:1016/2245 train_time:61796ms step_avg:60.82ms
step:1017/2245 train_time:61860ms step_avg:60.83ms
step:1018/2245 train_time:61921ms step_avg:60.83ms
step:1019/2245 train_time:61985ms step_avg:60.83ms
step:1020/2245 train_time:62046ms step_avg:60.83ms
step:1021/2245 train_time:62108ms step_avg:60.83ms
step:1022/2245 train_time:62169ms step_avg:60.83ms
step:1023/2245 train_time:62230ms step_avg:60.83ms
step:1024/2245 train_time:62291ms step_avg:60.83ms
step:1025/2245 train_time:62353ms step_avg:60.83ms
step:1026/2245 train_time:62412ms step_avg:60.83ms
step:1027/2245 train_time:62474ms step_avg:60.83ms
step:1028/2245 train_time:62533ms step_avg:60.83ms
step:1029/2245 train_time:62595ms step_avg:60.83ms
step:1030/2245 train_time:62655ms step_avg:60.83ms
step:1031/2245 train_time:62716ms step_avg:60.83ms
step:1032/2245 train_time:62777ms step_avg:60.83ms
step:1033/2245 train_time:62841ms step_avg:60.83ms
step:1034/2245 train_time:62901ms step_avg:60.83ms
step:1035/2245 train_time:62964ms step_avg:60.83ms
step:1036/2245 train_time:63024ms step_avg:60.83ms
step:1037/2245 train_time:63087ms step_avg:60.84ms
step:1038/2245 train_time:63148ms step_avg:60.84ms
step:1039/2245 train_time:63209ms step_avg:60.84ms
step:1040/2245 train_time:63270ms step_avg:60.84ms
step:1041/2245 train_time:63333ms step_avg:60.84ms
step:1042/2245 train_time:63392ms step_avg:60.84ms
step:1043/2245 train_time:63455ms step_avg:60.84ms
step:1044/2245 train_time:63514ms step_avg:60.84ms
step:1045/2245 train_time:63576ms step_avg:60.84ms
step:1046/2245 train_time:63636ms step_avg:60.84ms
step:1047/2245 train_time:63697ms step_avg:60.84ms
step:1048/2245 train_time:63758ms step_avg:60.84ms
step:1049/2245 train_time:63821ms step_avg:60.84ms
step:1050/2245 train_time:63882ms step_avg:60.84ms
step:1051/2245 train_time:63944ms step_avg:60.84ms
step:1052/2245 train_time:64005ms step_avg:60.84ms
step:1053/2245 train_time:64068ms step_avg:60.84ms
step:1054/2245 train_time:64128ms step_avg:60.84ms
step:1055/2245 train_time:64190ms step_avg:60.84ms
step:1056/2245 train_time:64250ms step_avg:60.84ms
step:1057/2245 train_time:64312ms step_avg:60.84ms
step:1058/2245 train_time:64372ms step_avg:60.84ms
step:1059/2245 train_time:64434ms step_avg:60.84ms
step:1060/2245 train_time:64494ms step_avg:60.84ms
step:1061/2245 train_time:64556ms step_avg:60.84ms
step:1062/2245 train_time:64616ms step_avg:60.84ms
step:1063/2245 train_time:64678ms step_avg:60.84ms
step:1064/2245 train_time:64738ms step_avg:60.84ms
step:1065/2245 train_time:64801ms step_avg:60.85ms
step:1066/2245 train_time:64861ms step_avg:60.85ms
step:1067/2245 train_time:64924ms step_avg:60.85ms
step:1068/2245 train_time:64984ms step_avg:60.85ms
step:1069/2245 train_time:65046ms step_avg:60.85ms
step:1070/2245 train_time:65106ms step_avg:60.85ms
step:1071/2245 train_time:65169ms step_avg:60.85ms
step:1072/2245 train_time:65229ms step_avg:60.85ms
step:1073/2245 train_time:65291ms step_avg:60.85ms
step:1074/2245 train_time:65351ms step_avg:60.85ms
step:1075/2245 train_time:65413ms step_avg:60.85ms
step:1076/2245 train_time:65473ms step_avg:60.85ms
step:1077/2245 train_time:65535ms step_avg:60.85ms
step:1078/2245 train_time:65595ms step_avg:60.85ms
step:1079/2245 train_time:65658ms step_avg:60.85ms
step:1080/2245 train_time:65718ms step_avg:60.85ms
step:1081/2245 train_time:65780ms step_avg:60.85ms
step:1082/2245 train_time:65841ms step_avg:60.85ms
step:1083/2245 train_time:65903ms step_avg:60.85ms
step:1084/2245 train_time:65963ms step_avg:60.85ms
step:1085/2245 train_time:66025ms step_avg:60.85ms
step:1086/2245 train_time:66086ms step_avg:60.85ms
step:1087/2245 train_time:66148ms step_avg:60.85ms
step:1088/2245 train_time:66207ms step_avg:60.85ms
step:1089/2245 train_time:66270ms step_avg:60.85ms
step:1090/2245 train_time:66329ms step_avg:60.85ms
step:1091/2245 train_time:66392ms step_avg:60.85ms
step:1092/2245 train_time:66452ms step_avg:60.85ms
step:1093/2245 train_time:66514ms step_avg:60.85ms
step:1094/2245 train_time:66574ms step_avg:60.85ms
step:1095/2245 train_time:66637ms step_avg:60.86ms
step:1096/2245 train_time:66697ms step_avg:60.85ms
step:1097/2245 train_time:66758ms step_avg:60.86ms
step:1098/2245 train_time:66819ms step_avg:60.86ms
step:1099/2245 train_time:66882ms step_avg:60.86ms
step:1100/2245 train_time:66942ms step_avg:60.86ms
step:1101/2245 train_time:67005ms step_avg:60.86ms
step:1102/2245 train_time:67065ms step_avg:60.86ms
step:1103/2245 train_time:67126ms step_avg:60.86ms
step:1104/2245 train_time:67187ms step_avg:60.86ms
step:1105/2245 train_time:67249ms step_avg:60.86ms
step:1106/2245 train_time:67308ms step_avg:60.86ms
step:1107/2245 train_time:67371ms step_avg:60.86ms
step:1108/2245 train_time:67431ms step_avg:60.86ms
step:1109/2245 train_time:67494ms step_avg:60.86ms
step:1110/2245 train_time:67554ms step_avg:60.86ms
step:1111/2245 train_time:67616ms step_avg:60.86ms
step:1112/2245 train_time:67676ms step_avg:60.86ms
step:1113/2245 train_time:67738ms step_avg:60.86ms
step:1114/2245 train_time:67798ms step_avg:60.86ms
step:1115/2245 train_time:67861ms step_avg:60.86ms
step:1116/2245 train_time:67921ms step_avg:60.86ms
step:1117/2245 train_time:67983ms step_avg:60.86ms
step:1118/2245 train_time:68044ms step_avg:60.86ms
step:1119/2245 train_time:68106ms step_avg:60.86ms
step:1120/2245 train_time:68166ms step_avg:60.86ms
step:1121/2245 train_time:68228ms step_avg:60.86ms
step:1122/2245 train_time:68288ms step_avg:60.86ms
step:1123/2245 train_time:68350ms step_avg:60.86ms
step:1124/2245 train_time:68410ms step_avg:60.86ms
step:1125/2245 train_time:68472ms step_avg:60.86ms
step:1126/2245 train_time:68532ms step_avg:60.86ms
step:1127/2245 train_time:68594ms step_avg:60.86ms
step:1128/2245 train_time:68655ms step_avg:60.86ms
step:1129/2245 train_time:68718ms step_avg:60.87ms
step:1130/2245 train_time:68778ms step_avg:60.87ms
step:1131/2245 train_time:68840ms step_avg:60.87ms
step:1132/2245 train_time:68900ms step_avg:60.87ms
step:1133/2245 train_time:68963ms step_avg:60.87ms
step:1134/2245 train_time:69023ms step_avg:60.87ms
step:1135/2245 train_time:69086ms step_avg:60.87ms
step:1136/2245 train_time:69146ms step_avg:60.87ms
step:1137/2245 train_time:69207ms step_avg:60.87ms
step:1138/2245 train_time:69267ms step_avg:60.87ms
step:1139/2245 train_time:69329ms step_avg:60.87ms
step:1140/2245 train_time:69389ms step_avg:60.87ms
step:1141/2245 train_time:69451ms step_avg:60.87ms
step:1142/2245 train_time:69510ms step_avg:60.87ms
step:1143/2245 train_time:69573ms step_avg:60.87ms
step:1144/2245 train_time:69634ms step_avg:60.87ms
step:1145/2245 train_time:69697ms step_avg:60.87ms
step:1146/2245 train_time:69757ms step_avg:60.87ms
step:1147/2245 train_time:69819ms step_avg:60.87ms
step:1148/2245 train_time:69881ms step_avg:60.87ms
step:1149/2245 train_time:69943ms step_avg:60.87ms
step:1150/2245 train_time:70003ms step_avg:60.87ms
step:1151/2245 train_time:70065ms step_avg:60.87ms
step:1152/2245 train_time:70125ms step_avg:60.87ms
step:1153/2245 train_time:70187ms step_avg:60.87ms
step:1154/2245 train_time:70247ms step_avg:60.87ms
step:1155/2245 train_time:70309ms step_avg:60.87ms
step:1156/2245 train_time:70369ms step_avg:60.87ms
step:1157/2245 train_time:70431ms step_avg:60.87ms
step:1158/2245 train_time:70490ms step_avg:60.87ms
step:1159/2245 train_time:70553ms step_avg:60.87ms
step:1160/2245 train_time:70612ms step_avg:60.87ms
step:1161/2245 train_time:70675ms step_avg:60.87ms
step:1162/2245 train_time:70736ms step_avg:60.87ms
step:1163/2245 train_time:70798ms step_avg:60.88ms
step:1164/2245 train_time:70859ms step_avg:60.88ms
step:1165/2245 train_time:70921ms step_avg:60.88ms
step:1166/2245 train_time:70981ms step_avg:60.88ms
step:1167/2245 train_time:71044ms step_avg:60.88ms
step:1168/2245 train_time:71104ms step_avg:60.88ms
step:1169/2245 train_time:71165ms step_avg:60.88ms
step:1170/2245 train_time:71225ms step_avg:60.88ms
step:1171/2245 train_time:71287ms step_avg:60.88ms
step:1172/2245 train_time:71347ms step_avg:60.88ms
step:1173/2245 train_time:71409ms step_avg:60.88ms
step:1174/2245 train_time:71469ms step_avg:60.88ms
step:1175/2245 train_time:71531ms step_avg:60.88ms
step:1176/2245 train_time:71592ms step_avg:60.88ms
step:1177/2245 train_time:71654ms step_avg:60.88ms
step:1178/2245 train_time:71714ms step_avg:60.88ms
step:1179/2245 train_time:71776ms step_avg:60.88ms
step:1180/2245 train_time:71837ms step_avg:60.88ms
step:1181/2245 train_time:71899ms step_avg:60.88ms
step:1182/2245 train_time:71959ms step_avg:60.88ms
step:1183/2245 train_time:72022ms step_avg:60.88ms
step:1184/2245 train_time:72082ms step_avg:60.88ms
step:1185/2245 train_time:72144ms step_avg:60.88ms
step:1186/2245 train_time:72205ms step_avg:60.88ms
step:1187/2245 train_time:72266ms step_avg:60.88ms
step:1188/2245 train_time:72327ms step_avg:60.88ms
step:1189/2245 train_time:72388ms step_avg:60.88ms
step:1190/2245 train_time:72448ms step_avg:60.88ms
step:1191/2245 train_time:72510ms step_avg:60.88ms
step:1192/2245 train_time:72570ms step_avg:60.88ms
step:1193/2245 train_time:72633ms step_avg:60.88ms
step:1194/2245 train_time:72694ms step_avg:60.88ms
step:1195/2245 train_time:72757ms step_avg:60.88ms
step:1196/2245 train_time:72817ms step_avg:60.88ms
step:1197/2245 train_time:72879ms step_avg:60.88ms
step:1198/2245 train_time:72940ms step_avg:60.88ms
step:1199/2245 train_time:73002ms step_avg:60.89ms
step:1200/2245 train_time:73062ms step_avg:60.88ms
step:1201/2245 train_time:73124ms step_avg:60.89ms
step:1202/2245 train_time:73184ms step_avg:60.89ms
step:1203/2245 train_time:73247ms step_avg:60.89ms
step:1204/2245 train_time:73306ms step_avg:60.89ms
step:1205/2245 train_time:73369ms step_avg:60.89ms
step:1206/2245 train_time:73428ms step_avg:60.89ms
step:1207/2245 train_time:73490ms step_avg:60.89ms
step:1208/2245 train_time:73550ms step_avg:60.89ms
step:1209/2245 train_time:73613ms step_avg:60.89ms
step:1210/2245 train_time:73673ms step_avg:60.89ms
step:1211/2245 train_time:73735ms step_avg:60.89ms
step:1212/2245 train_time:73796ms step_avg:60.89ms
step:1213/2245 train_time:73858ms step_avg:60.89ms
step:1214/2245 train_time:73918ms step_avg:60.89ms
step:1215/2245 train_time:73981ms step_avg:60.89ms
step:1216/2245 train_time:74041ms step_avg:60.89ms
step:1217/2245 train_time:74104ms step_avg:60.89ms
step:1218/2245 train_time:74163ms step_avg:60.89ms
step:1219/2245 train_time:74226ms step_avg:60.89ms
step:1220/2245 train_time:74286ms step_avg:60.89ms
step:1221/2245 train_time:74348ms step_avg:60.89ms
step:1222/2245 train_time:74408ms step_avg:60.89ms
step:1223/2245 train_time:74470ms step_avg:60.89ms
step:1224/2245 train_time:74530ms step_avg:60.89ms
step:1225/2245 train_time:74592ms step_avg:60.89ms
step:1226/2245 train_time:74652ms step_avg:60.89ms
step:1227/2245 train_time:74715ms step_avg:60.89ms
step:1228/2245 train_time:74775ms step_avg:60.89ms
step:1229/2245 train_time:74838ms step_avg:60.89ms
step:1230/2245 train_time:74898ms step_avg:60.89ms
step:1231/2245 train_time:74961ms step_avg:60.89ms
step:1232/2245 train_time:75021ms step_avg:60.89ms
step:1233/2245 train_time:75084ms step_avg:60.90ms
step:1234/2245 train_time:75144ms step_avg:60.89ms
step:1235/2245 train_time:75206ms step_avg:60.90ms
step:1236/2245 train_time:75266ms step_avg:60.89ms
step:1237/2245 train_time:75328ms step_avg:60.90ms
step:1238/2245 train_time:75388ms step_avg:60.89ms
step:1239/2245 train_time:75450ms step_avg:60.90ms
step:1240/2245 train_time:75509ms step_avg:60.89ms
step:1241/2245 train_time:75572ms step_avg:60.90ms
step:1242/2245 train_time:75632ms step_avg:60.90ms
step:1243/2245 train_time:75695ms step_avg:60.90ms
step:1244/2245 train_time:75755ms step_avg:60.90ms
step:1245/2245 train_time:75817ms step_avg:60.90ms
step:1246/2245 train_time:75877ms step_avg:60.90ms
step:1247/2245 train_time:75940ms step_avg:60.90ms
step:1248/2245 train_time:76000ms step_avg:60.90ms
step:1249/2245 train_time:76062ms step_avg:60.90ms
step:1250/2245 train_time:76122ms step_avg:60.90ms
step:1250/2245 val_loss:3.5175 train_time:76185ms step_avg:60.95ms
step:1251/2245 train_time:76206ms step_avg:60.92ms
step:1252/2245 train_time:76247ms step_avg:60.90ms
step:1253/2245 train_time:76313ms step_avg:60.90ms
step:1254/2245 train_time:76375ms step_avg:60.90ms
step:1255/2245 train_time:76439ms step_avg:60.91ms
step:1256/2245 train_time:76499ms step_avg:60.91ms
step:1257/2245 train_time:76562ms step_avg:60.91ms
step:1258/2245 train_time:76622ms step_avg:60.91ms
step:1259/2245 train_time:76683ms step_avg:60.91ms
step:1260/2245 train_time:76743ms step_avg:60.91ms
step:1261/2245 train_time:76805ms step_avg:60.91ms
step:1262/2245 train_time:76864ms step_avg:60.91ms
step:1263/2245 train_time:76926ms step_avg:60.91ms
step:1264/2245 train_time:76985ms step_avg:60.91ms
step:1265/2245 train_time:77047ms step_avg:60.91ms
step:1266/2245 train_time:77107ms step_avg:60.91ms
step:1267/2245 train_time:77170ms step_avg:60.91ms
step:1268/2245 train_time:77232ms step_avg:60.91ms
step:1269/2245 train_time:77295ms step_avg:60.91ms
step:1270/2245 train_time:77356ms step_avg:60.91ms
step:1271/2245 train_time:77419ms step_avg:60.91ms
step:1272/2245 train_time:77479ms step_avg:60.91ms
step:1273/2245 train_time:77541ms step_avg:60.91ms
step:1274/2245 train_time:77602ms step_avg:60.91ms
step:1275/2245 train_time:77664ms step_avg:60.91ms
step:1276/2245 train_time:77724ms step_avg:60.91ms
step:1277/2245 train_time:77785ms step_avg:60.91ms
step:1278/2245 train_time:77845ms step_avg:60.91ms
step:1279/2245 train_time:77906ms step_avg:60.91ms
step:1280/2245 train_time:77966ms step_avg:60.91ms
step:1281/2245 train_time:78028ms step_avg:60.91ms
step:1282/2245 train_time:78087ms step_avg:60.91ms
step:1283/2245 train_time:78150ms step_avg:60.91ms
step:1284/2245 train_time:78211ms step_avg:60.91ms
step:1285/2245 train_time:78274ms step_avg:60.91ms
step:1286/2245 train_time:78335ms step_avg:60.91ms
step:1287/2245 train_time:78397ms step_avg:60.91ms
step:1288/2245 train_time:78457ms step_avg:60.91ms
step:1289/2245 train_time:78520ms step_avg:60.92ms
step:1290/2245 train_time:78580ms step_avg:60.91ms
step:1291/2245 train_time:78643ms step_avg:60.92ms
step:1292/2245 train_time:78703ms step_avg:60.92ms
step:1293/2245 train_time:78765ms step_avg:60.92ms
step:1294/2245 train_time:78825ms step_avg:60.92ms
step:1295/2245 train_time:78887ms step_avg:60.92ms
step:1296/2245 train_time:78947ms step_avg:60.92ms
step:1297/2245 train_time:79009ms step_avg:60.92ms
step:1298/2245 train_time:79069ms step_avg:60.92ms
step:1299/2245 train_time:79132ms step_avg:60.92ms
step:1300/2245 train_time:79192ms step_avg:60.92ms
step:1301/2245 train_time:79255ms step_avg:60.92ms
step:1302/2245 train_time:79315ms step_avg:60.92ms
step:1303/2245 train_time:79377ms step_avg:60.92ms
step:1304/2245 train_time:79438ms step_avg:60.92ms
step:1305/2245 train_time:79501ms step_avg:60.92ms
step:1306/2245 train_time:79560ms step_avg:60.92ms
step:1307/2245 train_time:79623ms step_avg:60.92ms
step:1308/2245 train_time:79682ms step_avg:60.92ms
step:1309/2245 train_time:79744ms step_avg:60.92ms
step:1310/2245 train_time:79805ms step_avg:60.92ms
step:1311/2245 train_time:79867ms step_avg:60.92ms
step:1312/2245 train_time:79927ms step_avg:60.92ms
step:1313/2245 train_time:79988ms step_avg:60.92ms
step:1314/2245 train_time:80048ms step_avg:60.92ms
step:1315/2245 train_time:80110ms step_avg:60.92ms
step:1316/2245 train_time:80170ms step_avg:60.92ms
step:1317/2245 train_time:80233ms step_avg:60.92ms
step:1318/2245 train_time:80292ms step_avg:60.92ms
step:1319/2245 train_time:80354ms step_avg:60.92ms
step:1320/2245 train_time:80415ms step_avg:60.92ms
step:1321/2245 train_time:80477ms step_avg:60.92ms
step:1322/2245 train_time:80537ms step_avg:60.92ms
step:1323/2245 train_time:80599ms step_avg:60.92ms
step:1324/2245 train_time:80660ms step_avg:60.92ms
step:1325/2245 train_time:80723ms step_avg:60.92ms
step:1326/2245 train_time:80783ms step_avg:60.92ms
step:1327/2245 train_time:80846ms step_avg:60.92ms
step:1328/2245 train_time:80907ms step_avg:60.92ms
step:1329/2245 train_time:80969ms step_avg:60.92ms
step:1330/2245 train_time:81028ms step_avg:60.92ms
step:1331/2245 train_time:81090ms step_avg:60.92ms
step:1332/2245 train_time:81150ms step_avg:60.92ms
step:1333/2245 train_time:81213ms step_avg:60.93ms
step:1334/2245 train_time:81273ms step_avg:60.92ms
step:1335/2245 train_time:81335ms step_avg:60.93ms
step:1336/2245 train_time:81395ms step_avg:60.92ms
step:1337/2245 train_time:81457ms step_avg:60.93ms
step:1338/2245 train_time:81517ms step_avg:60.92ms
step:1339/2245 train_time:81579ms step_avg:60.93ms
step:1340/2245 train_time:81639ms step_avg:60.92ms
step:1341/2245 train_time:81701ms step_avg:60.93ms
step:1342/2245 train_time:81762ms step_avg:60.93ms
step:1343/2245 train_time:81826ms step_avg:60.93ms
step:1344/2245 train_time:81886ms step_avg:60.93ms
step:1345/2245 train_time:81948ms step_avg:60.93ms
step:1346/2245 train_time:82008ms step_avg:60.93ms
step:1347/2245 train_time:82070ms step_avg:60.93ms
step:1348/2245 train_time:82130ms step_avg:60.93ms
step:1349/2245 train_time:82192ms step_avg:60.93ms
step:1350/2245 train_time:82252ms step_avg:60.93ms
step:1351/2245 train_time:82314ms step_avg:60.93ms
step:1352/2245 train_time:82374ms step_avg:60.93ms
step:1353/2245 train_time:82436ms step_avg:60.93ms
step:1354/2245 train_time:82496ms step_avg:60.93ms
step:1355/2245 train_time:82558ms step_avg:60.93ms
step:1356/2245 train_time:82619ms step_avg:60.93ms
step:1357/2245 train_time:82681ms step_avg:60.93ms
step:1358/2245 train_time:82741ms step_avg:60.93ms
step:1359/2245 train_time:82804ms step_avg:60.93ms
step:1360/2245 train_time:82865ms step_avg:60.93ms
step:1361/2245 train_time:82927ms step_avg:60.93ms
step:1362/2245 train_time:82987ms step_avg:60.93ms
step:1363/2245 train_time:83049ms step_avg:60.93ms
step:1364/2245 train_time:83110ms step_avg:60.93ms
step:1365/2245 train_time:83173ms step_avg:60.93ms
step:1366/2245 train_time:83233ms step_avg:60.93ms
step:1367/2245 train_time:83295ms step_avg:60.93ms
step:1368/2245 train_time:83355ms step_avg:60.93ms
step:1369/2245 train_time:83417ms step_avg:60.93ms
step:1370/2245 train_time:83476ms step_avg:60.93ms
step:1371/2245 train_time:83539ms step_avg:60.93ms
step:1372/2245 train_time:83599ms step_avg:60.93ms
step:1373/2245 train_time:83661ms step_avg:60.93ms
step:1374/2245 train_time:83722ms step_avg:60.93ms
step:1375/2245 train_time:83784ms step_avg:60.93ms
step:1376/2245 train_time:83845ms step_avg:60.93ms
step:1377/2245 train_time:83909ms step_avg:60.94ms
step:1378/2245 train_time:83968ms step_avg:60.93ms
step:1379/2245 train_time:84030ms step_avg:60.94ms
step:1380/2245 train_time:84090ms step_avg:60.93ms
step:1381/2245 train_time:84153ms step_avg:60.94ms
step:1382/2245 train_time:84213ms step_avg:60.94ms
step:1383/2245 train_time:84275ms step_avg:60.94ms
step:1384/2245 train_time:84335ms step_avg:60.94ms
step:1385/2245 train_time:84396ms step_avg:60.94ms
step:1386/2245 train_time:84457ms step_avg:60.94ms
step:1387/2245 train_time:84519ms step_avg:60.94ms
step:1388/2245 train_time:84578ms step_avg:60.94ms
step:1389/2245 train_time:84641ms step_avg:60.94ms
step:1390/2245 train_time:84701ms step_avg:60.94ms
step:1391/2245 train_time:84764ms step_avg:60.94ms
step:1392/2245 train_time:84825ms step_avg:60.94ms
step:1393/2245 train_time:84888ms step_avg:60.94ms
step:1394/2245 train_time:84947ms step_avg:60.94ms
step:1395/2245 train_time:85011ms step_avg:60.94ms
step:1396/2245 train_time:85071ms step_avg:60.94ms
step:1397/2245 train_time:85132ms step_avg:60.94ms
step:1398/2245 train_time:85192ms step_avg:60.94ms
step:1399/2245 train_time:85255ms step_avg:60.94ms
step:1400/2245 train_time:85314ms step_avg:60.94ms
step:1401/2245 train_time:85377ms step_avg:60.94ms
step:1402/2245 train_time:85437ms step_avg:60.94ms
step:1403/2245 train_time:85500ms step_avg:60.94ms
step:1404/2245 train_time:85560ms step_avg:60.94ms
step:1405/2245 train_time:85622ms step_avg:60.94ms
step:1406/2245 train_time:85682ms step_avg:60.94ms
step:1407/2245 train_time:85744ms step_avg:60.94ms
step:1408/2245 train_time:85805ms step_avg:60.94ms
step:1409/2245 train_time:85867ms step_avg:60.94ms
step:1410/2245 train_time:85928ms step_avg:60.94ms
step:1411/2245 train_time:85991ms step_avg:60.94ms
step:1412/2245 train_time:86052ms step_avg:60.94ms
step:1413/2245 train_time:86114ms step_avg:60.94ms
step:1414/2245 train_time:86174ms step_avg:60.94ms
step:1415/2245 train_time:86235ms step_avg:60.94ms
step:1416/2245 train_time:86295ms step_avg:60.94ms
step:1417/2245 train_time:86357ms step_avg:60.94ms
step:1418/2245 train_time:86417ms step_avg:60.94ms
step:1419/2245 train_time:86479ms step_avg:60.94ms
step:1420/2245 train_time:86539ms step_avg:60.94ms
step:1421/2245 train_time:86601ms step_avg:60.94ms
step:1422/2245 train_time:86662ms step_avg:60.94ms
step:1423/2245 train_time:86724ms step_avg:60.94ms
step:1424/2245 train_time:86784ms step_avg:60.94ms
step:1425/2245 train_time:86847ms step_avg:60.95ms
step:1426/2245 train_time:86907ms step_avg:60.94ms
step:1427/2245 train_time:86971ms step_avg:60.95ms
step:1428/2245 train_time:87031ms step_avg:60.95ms
step:1429/2245 train_time:87095ms step_avg:60.95ms
step:1430/2245 train_time:87155ms step_avg:60.95ms
step:1431/2245 train_time:87217ms step_avg:60.95ms
step:1432/2245 train_time:87276ms step_avg:60.95ms
step:1433/2245 train_time:87338ms step_avg:60.95ms
step:1434/2245 train_time:87398ms step_avg:60.95ms
step:1435/2245 train_time:87461ms step_avg:60.95ms
step:1436/2245 train_time:87521ms step_avg:60.95ms
step:1437/2245 train_time:87583ms step_avg:60.95ms
step:1438/2245 train_time:87644ms step_avg:60.95ms
step:1439/2245 train_time:87706ms step_avg:60.95ms
step:1440/2245 train_time:87766ms step_avg:60.95ms
step:1441/2245 train_time:87829ms step_avg:60.95ms
step:1442/2245 train_time:87889ms step_avg:60.95ms
step:1443/2245 train_time:87952ms step_avg:60.95ms
step:1444/2245 train_time:88012ms step_avg:60.95ms
step:1445/2245 train_time:88075ms step_avg:60.95ms
step:1446/2245 train_time:88135ms step_avg:60.95ms
step:1447/2245 train_time:88197ms step_avg:60.95ms
step:1448/2245 train_time:88257ms step_avg:60.95ms
step:1449/2245 train_time:88319ms step_avg:60.95ms
step:1450/2245 train_time:88379ms step_avg:60.95ms
step:1451/2245 train_time:88441ms step_avg:60.95ms
step:1452/2245 train_time:88501ms step_avg:60.95ms
step:1453/2245 train_time:88564ms step_avg:60.95ms
step:1454/2245 train_time:88624ms step_avg:60.95ms
step:1455/2245 train_time:88686ms step_avg:60.95ms
step:1456/2245 train_time:88746ms step_avg:60.95ms
step:1457/2245 train_time:88809ms step_avg:60.95ms
step:1458/2245 train_time:88869ms step_avg:60.95ms
step:1459/2245 train_time:88931ms step_avg:60.95ms
step:1460/2245 train_time:88992ms step_avg:60.95ms
step:1461/2245 train_time:89054ms step_avg:60.95ms
step:1462/2245 train_time:89114ms step_avg:60.95ms
step:1463/2245 train_time:89176ms step_avg:60.95ms
step:1464/2245 train_time:89236ms step_avg:60.95ms
step:1465/2245 train_time:89298ms step_avg:60.95ms
step:1466/2245 train_time:89358ms step_avg:60.95ms
step:1467/2245 train_time:89420ms step_avg:60.95ms
step:1468/2245 train_time:89480ms step_avg:60.95ms
step:1469/2245 train_time:89542ms step_avg:60.95ms
step:1470/2245 train_time:89602ms step_avg:60.95ms
step:1471/2245 train_time:89665ms step_avg:60.96ms
step:1472/2245 train_time:89726ms step_avg:60.96ms
step:1473/2245 train_time:89789ms step_avg:60.96ms
step:1474/2245 train_time:89849ms step_avg:60.96ms
step:1475/2245 train_time:89912ms step_avg:60.96ms
step:1476/2245 train_time:89972ms step_avg:60.96ms
step:1477/2245 train_time:90035ms step_avg:60.96ms
step:1478/2245 train_time:90096ms step_avg:60.96ms
step:1479/2245 train_time:90159ms step_avg:60.96ms
step:1480/2245 train_time:90219ms step_avg:60.96ms
step:1481/2245 train_time:90282ms step_avg:60.96ms
step:1482/2245 train_time:90343ms step_avg:60.96ms
step:1483/2245 train_time:90405ms step_avg:60.96ms
step:1484/2245 train_time:90465ms step_avg:60.96ms
step:1485/2245 train_time:90528ms step_avg:60.96ms
step:1486/2245 train_time:90587ms step_avg:60.96ms
step:1487/2245 train_time:90650ms step_avg:60.96ms
step:1488/2245 train_time:90710ms step_avg:60.96ms
step:1489/2245 train_time:90773ms step_avg:60.96ms
step:1490/2245 train_time:90834ms step_avg:60.96ms
step:1491/2245 train_time:90898ms step_avg:60.96ms
step:1492/2245 train_time:90958ms step_avg:60.96ms
step:1493/2245 train_time:91021ms step_avg:60.96ms
step:1494/2245 train_time:91081ms step_avg:60.96ms
step:1495/2245 train_time:91144ms step_avg:60.97ms
step:1496/2245 train_time:91205ms step_avg:60.97ms
step:1497/2245 train_time:91267ms step_avg:60.97ms
step:1498/2245 train_time:91328ms step_avg:60.97ms
step:1499/2245 train_time:91390ms step_avg:60.97ms
step:1500/2245 train_time:91450ms step_avg:60.97ms
step:1500/2245 val_loss:3.4383 train_time:91514ms step_avg:61.01ms
step:1501/2245 train_time:91537ms step_avg:60.98ms
step:1502/2245 train_time:91577ms step_avg:60.97ms
step:1503/2245 train_time:91645ms step_avg:60.97ms
step:1504/2245 train_time:91710ms step_avg:60.98ms
step:1505/2245 train_time:91772ms step_avg:60.98ms
step:1506/2245 train_time:91832ms step_avg:60.98ms
step:1507/2245 train_time:91894ms step_avg:60.98ms
step:1508/2245 train_time:91954ms step_avg:60.98ms
step:1509/2245 train_time:92016ms step_avg:60.98ms
step:1510/2245 train_time:92075ms step_avg:60.98ms
step:1511/2245 train_time:92137ms step_avg:60.98ms
step:1512/2245 train_time:92197ms step_avg:60.98ms
step:1513/2245 train_time:92258ms step_avg:60.98ms
step:1514/2245 train_time:92318ms step_avg:60.98ms
step:1515/2245 train_time:92380ms step_avg:60.98ms
step:1516/2245 train_time:92446ms step_avg:60.98ms
step:1517/2245 train_time:92510ms step_avg:60.98ms
step:1518/2245 train_time:92572ms step_avg:60.98ms
step:1519/2245 train_time:92637ms step_avg:60.99ms
step:1520/2245 train_time:92699ms step_avg:60.99ms
step:1521/2245 train_time:92763ms step_avg:60.99ms
step:1522/2245 train_time:92823ms step_avg:60.99ms
step:1523/2245 train_time:92886ms step_avg:60.99ms
step:1524/2245 train_time:92946ms step_avg:60.99ms
step:1525/2245 train_time:93008ms step_avg:60.99ms
step:1526/2245 train_time:93068ms step_avg:60.99ms
step:1527/2245 train_time:93130ms step_avg:60.99ms
step:1528/2245 train_time:93190ms step_avg:60.99ms
step:1529/2245 train_time:93252ms step_avg:60.99ms
step:1530/2245 train_time:93312ms step_avg:60.99ms
step:1531/2245 train_time:93374ms step_avg:60.99ms
step:1532/2245 train_time:93435ms step_avg:60.99ms
step:1533/2245 train_time:93499ms step_avg:60.99ms
step:1534/2245 train_time:93561ms step_avg:60.99ms
step:1535/2245 train_time:93624ms step_avg:60.99ms
step:1536/2245 train_time:93685ms step_avg:60.99ms
step:1537/2245 train_time:93748ms step_avg:60.99ms
step:1538/2245 train_time:93809ms step_avg:60.99ms
step:1539/2245 train_time:93871ms step_avg:61.00ms
step:1540/2245 train_time:93932ms step_avg:60.99ms
step:1541/2245 train_time:93994ms step_avg:61.00ms
step:1542/2245 train_time:94055ms step_avg:61.00ms
step:1543/2245 train_time:94118ms step_avg:61.00ms
step:1544/2245 train_time:94178ms step_avg:61.00ms
step:1545/2245 train_time:94241ms step_avg:61.00ms
step:1546/2245 train_time:94301ms step_avg:61.00ms
step:1547/2245 train_time:94364ms step_avg:61.00ms
step:1548/2245 train_time:94424ms step_avg:61.00ms
step:1549/2245 train_time:94487ms step_avg:61.00ms
step:1550/2245 train_time:94548ms step_avg:61.00ms
step:1551/2245 train_time:94611ms step_avg:61.00ms
step:1552/2245 train_time:94672ms step_avg:61.00ms
step:1553/2245 train_time:94736ms step_avg:61.00ms
step:1554/2245 train_time:94797ms step_avg:61.00ms
step:1555/2245 train_time:94860ms step_avg:61.00ms
step:1556/2245 train_time:94921ms step_avg:61.00ms
step:1557/2245 train_time:94984ms step_avg:61.00ms
step:1558/2245 train_time:95044ms step_avg:61.00ms
step:1559/2245 train_time:95106ms step_avg:61.00ms
step:1560/2245 train_time:95166ms step_avg:61.00ms
step:1561/2245 train_time:95228ms step_avg:61.00ms
step:1562/2245 train_time:95287ms step_avg:61.00ms
step:1563/2245 train_time:95350ms step_avg:61.00ms
step:1564/2245 train_time:95410ms step_avg:61.00ms
step:1565/2245 train_time:95473ms step_avg:61.01ms
step:1566/2245 train_time:95533ms step_avg:61.00ms
step:1567/2245 train_time:95597ms step_avg:61.01ms
step:1568/2245 train_time:95658ms step_avg:61.01ms
step:1569/2245 train_time:95721ms step_avg:61.01ms
step:1570/2245 train_time:95782ms step_avg:61.01ms
step:1571/2245 train_time:95845ms step_avg:61.01ms
step:1572/2245 train_time:95906ms step_avg:61.01ms
step:1573/2245 train_time:95968ms step_avg:61.01ms
step:1574/2245 train_time:96028ms step_avg:61.01ms
step:1575/2245 train_time:96091ms step_avg:61.01ms
step:1576/2245 train_time:96151ms step_avg:61.01ms
step:1577/2245 train_time:96213ms step_avg:61.01ms
step:1578/2245 train_time:96274ms step_avg:61.01ms
step:1579/2245 train_time:96336ms step_avg:61.01ms
step:1580/2245 train_time:96397ms step_avg:61.01ms
step:1581/2245 train_time:96460ms step_avg:61.01ms
step:1582/2245 train_time:96521ms step_avg:61.01ms
step:1583/2245 train_time:96585ms step_avg:61.01ms
step:1584/2245 train_time:96646ms step_avg:61.01ms
step:1585/2245 train_time:96709ms step_avg:61.01ms
step:1586/2245 train_time:96769ms step_avg:61.01ms
step:1587/2245 train_time:96833ms step_avg:61.02ms
step:1588/2245 train_time:96893ms step_avg:61.02ms
step:1589/2245 train_time:96956ms step_avg:61.02ms
step:1590/2245 train_time:97017ms step_avg:61.02ms
step:1591/2245 train_time:97080ms step_avg:61.02ms
step:1592/2245 train_time:97141ms step_avg:61.02ms
step:1593/2245 train_time:97204ms step_avg:61.02ms
step:1594/2245 train_time:97264ms step_avg:61.02ms
step:1595/2245 train_time:97327ms step_avg:61.02ms
step:1596/2245 train_time:97387ms step_avg:61.02ms
step:1597/2245 train_time:97448ms step_avg:61.02ms
step:1598/2245 train_time:97509ms step_avg:61.02ms
step:1599/2245 train_time:97572ms step_avg:61.02ms
step:1600/2245 train_time:97633ms step_avg:61.02ms
step:1601/2245 train_time:97695ms step_avg:61.02ms
step:1602/2245 train_time:97756ms step_avg:61.02ms
step:1603/2245 train_time:97819ms step_avg:61.02ms
step:1604/2245 train_time:97880ms step_avg:61.02ms
step:1605/2245 train_time:97943ms step_avg:61.02ms
step:1606/2245 train_time:98004ms step_avg:61.02ms
step:1607/2245 train_time:98067ms step_avg:61.02ms
step:1608/2245 train_time:98126ms step_avg:61.02ms
step:1609/2245 train_time:98189ms step_avg:61.02ms
step:1610/2245 train_time:98249ms step_avg:61.02ms
step:1611/2245 train_time:98312ms step_avg:61.03ms
step:1612/2245 train_time:98372ms step_avg:61.02ms
step:1613/2245 train_time:98435ms step_avg:61.03ms
step:1614/2245 train_time:98496ms step_avg:61.03ms
step:1615/2245 train_time:98559ms step_avg:61.03ms
step:1616/2245 train_time:98620ms step_avg:61.03ms
step:1617/2245 train_time:98683ms step_avg:61.03ms
step:1618/2245 train_time:98744ms step_avg:61.03ms
step:1619/2245 train_time:98807ms step_avg:61.03ms
step:1620/2245 train_time:98868ms step_avg:61.03ms
step:1621/2245 train_time:98930ms step_avg:61.03ms
step:1622/2245 train_time:98991ms step_avg:61.03ms
step:1623/2245 train_time:99053ms step_avg:61.03ms
step:1624/2245 train_time:99114ms step_avg:61.03ms
step:1625/2245 train_time:99176ms step_avg:61.03ms
step:1626/2245 train_time:99237ms step_avg:61.03ms
step:1627/2245 train_time:99300ms step_avg:61.03ms
step:1628/2245 train_time:99361ms step_avg:61.03ms
step:1629/2245 train_time:99424ms step_avg:61.03ms
step:1630/2245 train_time:99484ms step_avg:61.03ms
step:1631/2245 train_time:99547ms step_avg:61.03ms
step:1632/2245 train_time:99608ms step_avg:61.03ms
step:1633/2245 train_time:99670ms step_avg:61.04ms
step:1634/2245 train_time:99731ms step_avg:61.03ms
step:1635/2245 train_time:99794ms step_avg:61.04ms
step:1636/2245 train_time:99855ms step_avg:61.04ms
step:1637/2245 train_time:99918ms step_avg:61.04ms
step:1638/2245 train_time:99979ms step_avg:61.04ms
step:1639/2245 train_time:100042ms step_avg:61.04ms
step:1640/2245 train_time:100103ms step_avg:61.04ms
step:1641/2245 train_time:100165ms step_avg:61.04ms
step:1642/2245 train_time:100225ms step_avg:61.04ms
step:1643/2245 train_time:100288ms step_avg:61.04ms
step:1644/2245 train_time:100349ms step_avg:61.04ms
step:1645/2245 train_time:100411ms step_avg:61.04ms
step:1646/2245 train_time:100472ms step_avg:61.04ms
step:1647/2245 train_time:100534ms step_avg:61.04ms
step:1648/2245 train_time:100595ms step_avg:61.04ms
step:1649/2245 train_time:100657ms step_avg:61.04ms
step:1650/2245 train_time:100718ms step_avg:61.04ms
step:1651/2245 train_time:100782ms step_avg:61.04ms
step:1652/2245 train_time:100843ms step_avg:61.04ms
step:1653/2245 train_time:100906ms step_avg:61.04ms
step:1654/2245 train_time:100966ms step_avg:61.04ms
step:1655/2245 train_time:101029ms step_avg:61.04ms
step:1656/2245 train_time:101090ms step_avg:61.04ms
step:1657/2245 train_time:101153ms step_avg:61.05ms
step:1658/2245 train_time:101213ms step_avg:61.05ms
step:1659/2245 train_time:101275ms step_avg:61.05ms
step:1660/2245 train_time:101336ms step_avg:61.05ms
step:1661/2245 train_time:101399ms step_avg:61.05ms
step:1662/2245 train_time:101459ms step_avg:61.05ms
step:1663/2245 train_time:101522ms step_avg:61.05ms
step:1664/2245 train_time:101582ms step_avg:61.05ms
step:1665/2245 train_time:101645ms step_avg:61.05ms
step:1666/2245 train_time:101706ms step_avg:61.05ms
step:1667/2245 train_time:101769ms step_avg:61.05ms
step:1668/2245 train_time:101829ms step_avg:61.05ms
step:1669/2245 train_time:101891ms step_avg:61.05ms
step:1670/2245 train_time:101952ms step_avg:61.05ms
step:1671/2245 train_time:102015ms step_avg:61.05ms
step:1672/2245 train_time:102075ms step_avg:61.05ms
step:1673/2245 train_time:102138ms step_avg:61.05ms
step:1674/2245 train_time:102198ms step_avg:61.05ms
step:1675/2245 train_time:102261ms step_avg:61.05ms
step:1676/2245 train_time:102322ms step_avg:61.05ms
step:1677/2245 train_time:102384ms step_avg:61.05ms
step:1678/2245 train_time:102445ms step_avg:61.05ms
step:1679/2245 train_time:102507ms step_avg:61.05ms
step:1680/2245 train_time:102567ms step_avg:61.05ms
step:1681/2245 train_time:102629ms step_avg:61.05ms
step:1682/2245 train_time:102690ms step_avg:61.05ms
step:1683/2245 train_time:102753ms step_avg:61.05ms
step:1684/2245 train_time:102814ms step_avg:61.05ms
step:1685/2245 train_time:102876ms step_avg:61.05ms
step:1686/2245 train_time:102937ms step_avg:61.05ms
step:1687/2245 train_time:103001ms step_avg:61.06ms
step:1688/2245 train_time:103062ms step_avg:61.06ms
step:1689/2245 train_time:103124ms step_avg:61.06ms
step:1690/2245 train_time:103185ms step_avg:61.06ms
step:1691/2245 train_time:103247ms step_avg:61.06ms
step:1692/2245 train_time:103308ms step_avg:61.06ms
step:1693/2245 train_time:103371ms step_avg:61.06ms
step:1694/2245 train_time:103432ms step_avg:61.06ms
step:1695/2245 train_time:103495ms step_avg:61.06ms
step:1696/2245 train_time:103556ms step_avg:61.06ms
step:1697/2245 train_time:103619ms step_avg:61.06ms
step:1698/2245 train_time:103681ms step_avg:61.06ms
step:1699/2245 train_time:103743ms step_avg:61.06ms
step:1700/2245 train_time:103804ms step_avg:61.06ms
step:1701/2245 train_time:103867ms step_avg:61.06ms
step:1702/2245 train_time:103927ms step_avg:61.06ms
step:1703/2245 train_time:103990ms step_avg:61.06ms
step:1704/2245 train_time:104050ms step_avg:61.06ms
step:1705/2245 train_time:104113ms step_avg:61.06ms
step:1706/2245 train_time:104175ms step_avg:61.06ms
step:1707/2245 train_time:104238ms step_avg:61.06ms
step:1708/2245 train_time:104298ms step_avg:61.06ms
step:1709/2245 train_time:104362ms step_avg:61.07ms
step:1710/2245 train_time:104422ms step_avg:61.07ms
step:1711/2245 train_time:104485ms step_avg:61.07ms
step:1712/2245 train_time:104546ms step_avg:61.07ms
step:1713/2245 train_time:104609ms step_avg:61.07ms
step:1714/2245 train_time:104669ms step_avg:61.07ms
step:1715/2245 train_time:104732ms step_avg:61.07ms
step:1716/2245 train_time:104793ms step_avg:61.07ms
step:1717/2245 train_time:104855ms step_avg:61.07ms
step:1718/2245 train_time:104916ms step_avg:61.07ms
step:1719/2245 train_time:104980ms step_avg:61.07ms
step:1720/2245 train_time:105041ms step_avg:61.07ms
step:1721/2245 train_time:105104ms step_avg:61.07ms
step:1722/2245 train_time:105164ms step_avg:61.07ms
step:1723/2245 train_time:105227ms step_avg:61.07ms
step:1724/2245 train_time:105288ms step_avg:61.07ms
step:1725/2245 train_time:105350ms step_avg:61.07ms
step:1726/2245 train_time:105411ms step_avg:61.07ms
step:1727/2245 train_time:105474ms step_avg:61.07ms
step:1728/2245 train_time:105535ms step_avg:61.07ms
step:1729/2245 train_time:105598ms step_avg:61.07ms
step:1730/2245 train_time:105659ms step_avg:61.07ms
step:1731/2245 train_time:105722ms step_avg:61.08ms
step:1732/2245 train_time:105783ms step_avg:61.08ms
step:1733/2245 train_time:105846ms step_avg:61.08ms
step:1734/2245 train_time:105906ms step_avg:61.08ms
step:1735/2245 train_time:105968ms step_avg:61.08ms
step:1736/2245 train_time:106029ms step_avg:61.08ms
step:1737/2245 train_time:106091ms step_avg:61.08ms
step:1738/2245 train_time:106152ms step_avg:61.08ms
step:1739/2245 train_time:106215ms step_avg:61.08ms
step:1740/2245 train_time:106275ms step_avg:61.08ms
step:1741/2245 train_time:106339ms step_avg:61.08ms
step:1742/2245 train_time:106401ms step_avg:61.08ms
step:1743/2245 train_time:106464ms step_avg:61.08ms
step:1744/2245 train_time:106524ms step_avg:61.08ms
step:1745/2245 train_time:106587ms step_avg:61.08ms
step:1746/2245 train_time:106647ms step_avg:61.08ms
step:1747/2245 train_time:106710ms step_avg:61.08ms
step:1748/2245 train_time:106771ms step_avg:61.08ms
step:1749/2245 train_time:106834ms step_avg:61.08ms
step:1750/2245 train_time:106894ms step_avg:61.08ms
step:1750/2245 val_loss:3.3746 train_time:106958ms step_avg:61.12ms
step:1751/2245 train_time:106978ms step_avg:61.10ms
step:1752/2245 train_time:107021ms step_avg:61.08ms
step:1753/2245 train_time:107086ms step_avg:61.09ms
step:1754/2245 train_time:107148ms step_avg:61.09ms
step:1755/2245 train_time:107210ms step_avg:61.09ms
step:1756/2245 train_time:107271ms step_avg:61.09ms
step:1757/2245 train_time:107333ms step_avg:61.09ms
step:1758/2245 train_time:107394ms step_avg:61.09ms
step:1759/2245 train_time:107456ms step_avg:61.09ms
step:1760/2245 train_time:107516ms step_avg:61.09ms
step:1761/2245 train_time:107580ms step_avg:61.09ms
step:1762/2245 train_time:107640ms step_avg:61.09ms
step:1763/2245 train_time:107702ms step_avg:61.09ms
step:1764/2245 train_time:107762ms step_avg:61.09ms
step:1765/2245 train_time:107823ms step_avg:61.09ms
step:1766/2245 train_time:107884ms step_avg:61.09ms
step:1767/2245 train_time:107948ms step_avg:61.09ms
step:1768/2245 train_time:108010ms step_avg:61.09ms
step:1769/2245 train_time:108074ms step_avg:61.09ms
step:1770/2245 train_time:108135ms step_avg:61.09ms
step:1771/2245 train_time:108199ms step_avg:61.10ms
step:1772/2245 train_time:108260ms step_avg:61.09ms
step:1773/2245 train_time:108323ms step_avg:61.10ms
step:1774/2245 train_time:108383ms step_avg:61.10ms
step:1775/2245 train_time:108446ms step_avg:61.10ms
step:1776/2245 train_time:108506ms step_avg:61.10ms
step:1777/2245 train_time:108568ms step_avg:61.10ms
step:1778/2245 train_time:108628ms step_avg:61.10ms
step:1779/2245 train_time:108691ms step_avg:61.10ms
step:1780/2245 train_time:108750ms step_avg:61.10ms
step:1781/2245 train_time:108813ms step_avg:61.10ms
step:1782/2245 train_time:108874ms step_avg:61.10ms
step:1783/2245 train_time:108937ms step_avg:61.10ms
step:1784/2245 train_time:108999ms step_avg:61.10ms
step:1785/2245 train_time:109063ms step_avg:61.10ms
step:1786/2245 train_time:109125ms step_avg:61.10ms
step:1787/2245 train_time:109187ms step_avg:61.10ms
step:1788/2245 train_time:109248ms step_avg:61.10ms
step:1789/2245 train_time:109311ms step_avg:61.10ms
step:1790/2245 train_time:109372ms step_avg:61.10ms
step:1791/2245 train_time:109434ms step_avg:61.10ms
step:1792/2245 train_time:109494ms step_avg:61.10ms
step:1793/2245 train_time:109557ms step_avg:61.10ms
step:1794/2245 train_time:109618ms step_avg:61.10ms
step:1795/2245 train_time:109680ms step_avg:61.10ms
step:1796/2245 train_time:109741ms step_avg:61.10ms
step:1797/2245 train_time:109804ms step_avg:61.10ms
step:1798/2245 train_time:109864ms step_avg:61.10ms
step:1799/2245 train_time:109927ms step_avg:61.10ms
step:1800/2245 train_time:109987ms step_avg:61.10ms
step:1801/2245 train_time:110050ms step_avg:61.10ms
step:1802/2245 train_time:110111ms step_avg:61.10ms
step:1803/2245 train_time:110174ms step_avg:61.11ms
step:1804/2245 train_time:110236ms step_avg:61.11ms
step:1805/2245 train_time:110299ms step_avg:61.11ms
step:1806/2245 train_time:110360ms step_avg:61.11ms
step:1807/2245 train_time:110423ms step_avg:61.11ms
step:1808/2245 train_time:110483ms step_avg:61.11ms
step:1809/2245 train_time:110546ms step_avg:61.11ms
step:1810/2245 train_time:110605ms step_avg:61.11ms
step:1811/2245 train_time:110668ms step_avg:61.11ms
step:1812/2245 train_time:110729ms step_avg:61.11ms
step:1813/2245 train_time:110791ms step_avg:61.11ms
step:1814/2245 train_time:110851ms step_avg:61.11ms
step:1815/2245 train_time:110915ms step_avg:61.11ms
step:1816/2245 train_time:110976ms step_avg:61.11ms
step:1817/2245 train_time:111039ms step_avg:61.11ms
step:1818/2245 train_time:111099ms step_avg:61.11ms
step:1819/2245 train_time:111163ms step_avg:61.11ms
step:1820/2245 train_time:111224ms step_avg:61.11ms
step:1821/2245 train_time:111287ms step_avg:61.11ms
step:1822/2245 train_time:111347ms step_avg:61.11ms
step:1823/2245 train_time:111410ms step_avg:61.11ms
step:1824/2245 train_time:111471ms step_avg:61.11ms
step:1825/2245 train_time:111534ms step_avg:61.11ms
step:1826/2245 train_time:111595ms step_avg:61.11ms
step:1827/2245 train_time:111659ms step_avg:61.12ms
step:1828/2245 train_time:111719ms step_avg:61.12ms
step:1829/2245 train_time:111782ms step_avg:61.12ms
step:1830/2245 train_time:111843ms step_avg:61.12ms
step:1831/2245 train_time:111905ms step_avg:61.12ms
step:1832/2245 train_time:111966ms step_avg:61.12ms
step:1833/2245 train_time:112028ms step_avg:61.12ms
step:1834/2245 train_time:112089ms step_avg:61.12ms
step:1835/2245 train_time:112152ms step_avg:61.12ms
step:1836/2245 train_time:112213ms step_avg:61.12ms
step:1837/2245 train_time:112276ms step_avg:61.12ms
step:1838/2245 train_time:112337ms step_avg:61.12ms
step:1839/2245 train_time:112401ms step_avg:61.12ms
step:1840/2245 train_time:112461ms step_avg:61.12ms
step:1841/2245 train_time:112524ms step_avg:61.12ms
step:1842/2245 train_time:112585ms step_avg:61.12ms
step:1843/2245 train_time:112647ms step_avg:61.12ms
step:1844/2245 train_time:112708ms step_avg:61.12ms
step:1845/2245 train_time:112771ms step_avg:61.12ms
step:1846/2245 train_time:112831ms step_avg:61.12ms
step:1847/2245 train_time:112894ms step_avg:61.12ms
step:1848/2245 train_time:112955ms step_avg:61.12ms
step:1849/2245 train_time:113018ms step_avg:61.12ms
step:1850/2245 train_time:113079ms step_avg:61.12ms
step:1851/2245 train_time:113142ms step_avg:61.12ms
step:1852/2245 train_time:113203ms step_avg:61.12ms
step:1853/2245 train_time:113265ms step_avg:61.13ms
step:1854/2245 train_time:113326ms step_avg:61.12ms
step:1855/2245 train_time:113389ms step_avg:61.13ms
step:1856/2245 train_time:113449ms step_avg:61.13ms
step:1857/2245 train_time:113512ms step_avg:61.13ms
step:1858/2245 train_time:113573ms step_avg:61.13ms
step:1859/2245 train_time:113636ms step_avg:61.13ms
step:1860/2245 train_time:113697ms step_avg:61.13ms
step:1861/2245 train_time:113760ms step_avg:61.13ms
step:1862/2245 train_time:113821ms step_avg:61.13ms
step:1863/2245 train_time:113883ms step_avg:61.13ms
step:1864/2245 train_time:113943ms step_avg:61.13ms
step:1865/2245 train_time:114006ms step_avg:61.13ms
step:1866/2245 train_time:114066ms step_avg:61.13ms
step:1867/2245 train_time:114129ms step_avg:61.13ms
step:1868/2245 train_time:114190ms step_avg:61.13ms
step:1869/2245 train_time:114252ms step_avg:61.13ms
step:1870/2245 train_time:114313ms step_avg:61.13ms
step:1871/2245 train_time:114376ms step_avg:61.13ms
step:1872/2245 train_time:114437ms step_avg:61.13ms
step:1873/2245 train_time:114500ms step_avg:61.13ms
step:1874/2245 train_time:114561ms step_avg:61.13ms
step:1875/2245 train_time:114624ms step_avg:61.13ms
step:1876/2245 train_time:114684ms step_avg:61.13ms
step:1877/2245 train_time:114747ms step_avg:61.13ms
step:1878/2245 train_time:114807ms step_avg:61.13ms
step:1879/2245 train_time:114870ms step_avg:61.13ms
step:1880/2245 train_time:114930ms step_avg:61.13ms
step:1881/2245 train_time:114993ms step_avg:61.13ms
step:1882/2245 train_time:115054ms step_avg:61.13ms
step:1883/2245 train_time:115117ms step_avg:61.13ms
step:1884/2245 train_time:115179ms step_avg:61.14ms
step:1885/2245 train_time:115242ms step_avg:61.14ms
step:1886/2245 train_time:115302ms step_avg:61.14ms
step:1887/2245 train_time:115365ms step_avg:61.14ms
step:1888/2245 train_time:115425ms step_avg:61.14ms
step:1889/2245 train_time:115489ms step_avg:61.14ms
step:1890/2245 train_time:115549ms step_avg:61.14ms
step:1891/2245 train_time:115613ms step_avg:61.14ms
step:1892/2245 train_time:115673ms step_avg:61.14ms
step:1893/2245 train_time:115736ms step_avg:61.14ms
step:1894/2245 train_time:115797ms step_avg:61.14ms
step:1895/2245 train_time:115860ms step_avg:61.14ms
step:1896/2245 train_time:115921ms step_avg:61.14ms
step:1897/2245 train_time:115984ms step_avg:61.14ms
step:1898/2245 train_time:116044ms step_avg:61.14ms
step:1899/2245 train_time:116106ms step_avg:61.14ms
step:1900/2245 train_time:116166ms step_avg:61.14ms
step:1901/2245 train_time:116228ms step_avg:61.14ms
step:1902/2245 train_time:116289ms step_avg:61.14ms
step:1903/2245 train_time:116352ms step_avg:61.14ms
step:1904/2245 train_time:116413ms step_avg:61.14ms
step:1905/2245 train_time:116475ms step_avg:61.14ms
step:1906/2245 train_time:116536ms step_avg:61.14ms
step:1907/2245 train_time:116599ms step_avg:61.14ms
step:1908/2245 train_time:116660ms step_avg:61.14ms
step:1909/2245 train_time:116724ms step_avg:61.14ms
step:1910/2245 train_time:116784ms step_avg:61.14ms
step:1911/2245 train_time:116846ms step_avg:61.14ms
step:1912/2245 train_time:116906ms step_avg:61.14ms
step:1913/2245 train_time:116969ms step_avg:61.14ms
step:1914/2245 train_time:117029ms step_avg:61.14ms
step:1915/2245 train_time:117093ms step_avg:61.15ms
step:1916/2245 train_time:117153ms step_avg:61.14ms
step:1917/2245 train_time:117216ms step_avg:61.15ms
step:1918/2245 train_time:117277ms step_avg:61.15ms
step:1919/2245 train_time:117340ms step_avg:61.15ms
step:1920/2245 train_time:117400ms step_avg:61.15ms
step:1921/2245 train_time:117463ms step_avg:61.15ms
step:1922/2245 train_time:117524ms step_avg:61.15ms
step:1923/2245 train_time:117587ms step_avg:61.15ms
step:1924/2245 train_time:117647ms step_avg:61.15ms
step:1925/2245 train_time:117710ms step_avg:61.15ms
step:1926/2245 train_time:117771ms step_avg:61.15ms
step:1927/2245 train_time:117834ms step_avg:61.15ms
step:1928/2245 train_time:117894ms step_avg:61.15ms
step:1929/2245 train_time:117958ms step_avg:61.15ms
step:1930/2245 train_time:118019ms step_avg:61.15ms
step:1931/2245 train_time:118081ms step_avg:61.15ms
step:1932/2245 train_time:118142ms step_avg:61.15ms
step:1933/2245 train_time:118204ms step_avg:61.15ms
step:1934/2245 train_time:118264ms step_avg:61.15ms
step:1935/2245 train_time:118327ms step_avg:61.15ms
step:1936/2245 train_time:118387ms step_avg:61.15ms
step:1937/2245 train_time:118450ms step_avg:61.15ms
step:1938/2245 train_time:118511ms step_avg:61.15ms
step:1939/2245 train_time:118574ms step_avg:61.15ms
step:1940/2245 train_time:118634ms step_avg:61.15ms
step:1941/2245 train_time:118698ms step_avg:61.15ms
step:1942/2245 train_time:118758ms step_avg:61.15ms
step:1943/2245 train_time:118821ms step_avg:61.15ms
step:1944/2245 train_time:118882ms step_avg:61.15ms
step:1945/2245 train_time:118945ms step_avg:61.15ms
step:1946/2245 train_time:119005ms step_avg:61.15ms
step:1947/2245 train_time:119068ms step_avg:61.15ms
step:1948/2245 train_time:119129ms step_avg:61.15ms
step:1949/2245 train_time:119192ms step_avg:61.16ms
step:1950/2245 train_time:119253ms step_avg:61.16ms
step:1951/2245 train_time:119316ms step_avg:61.16ms
step:1952/2245 train_time:119377ms step_avg:61.16ms
step:1953/2245 train_time:119440ms step_avg:61.16ms
step:1954/2245 train_time:119501ms step_avg:61.16ms
step:1955/2245 train_time:119564ms step_avg:61.16ms
step:1956/2245 train_time:119625ms step_avg:61.16ms
step:1957/2245 train_time:119688ms step_avg:61.16ms
step:1958/2245 train_time:119748ms step_avg:61.16ms
step:1959/2245 train_time:119811ms step_avg:61.16ms
step:1960/2245 train_time:119872ms step_avg:61.16ms
step:1961/2245 train_time:119935ms step_avg:61.16ms
step:1962/2245 train_time:119995ms step_avg:61.16ms
step:1963/2245 train_time:120058ms step_avg:61.16ms
step:1964/2245 train_time:120120ms step_avg:61.16ms
step:1965/2245 train_time:120182ms step_avg:61.16ms
step:1966/2245 train_time:120242ms step_avg:61.16ms
step:1967/2245 train_time:120305ms step_avg:61.16ms
step:1968/2245 train_time:120365ms step_avg:61.16ms
step:1969/2245 train_time:120427ms step_avg:61.16ms
step:1970/2245 train_time:120488ms step_avg:61.16ms
step:1971/2245 train_time:120550ms step_avg:61.16ms
step:1972/2245 train_time:120611ms step_avg:61.16ms
step:1973/2245 train_time:120674ms step_avg:61.16ms
step:1974/2245 train_time:120734ms step_avg:61.16ms
step:1975/2245 train_time:120796ms step_avg:61.16ms
step:1976/2245 train_time:120857ms step_avg:61.16ms
step:1977/2245 train_time:120920ms step_avg:61.16ms
step:1978/2245 train_time:120981ms step_avg:61.16ms
step:1979/2245 train_time:121044ms step_avg:61.16ms
step:1980/2245 train_time:121104ms step_avg:61.16ms
step:1981/2245 train_time:121167ms step_avg:61.16ms
step:1982/2245 train_time:121227ms step_avg:61.16ms
step:1983/2245 train_time:121290ms step_avg:61.16ms
step:1984/2245 train_time:121350ms step_avg:61.16ms
step:1985/2245 train_time:121413ms step_avg:61.17ms
step:1986/2245 train_time:121474ms step_avg:61.17ms
step:1987/2245 train_time:121537ms step_avg:61.17ms
step:1988/2245 train_time:121598ms step_avg:61.17ms
step:1989/2245 train_time:121661ms step_avg:61.17ms
step:1990/2245 train_time:121722ms step_avg:61.17ms
step:1991/2245 train_time:121785ms step_avg:61.17ms
step:1992/2245 train_time:121845ms step_avg:61.17ms
step:1993/2245 train_time:121908ms step_avg:61.17ms
step:1994/2245 train_time:121969ms step_avg:61.17ms
step:1995/2245 train_time:122032ms step_avg:61.17ms
step:1996/2245 train_time:122093ms step_avg:61.17ms
step:1997/2245 train_time:122156ms step_avg:61.17ms
step:1998/2245 train_time:122217ms step_avg:61.17ms
step:1999/2245 train_time:122280ms step_avg:61.17ms
step:2000/2245 train_time:122341ms step_avg:61.17ms
step:2000/2245 val_loss:3.3204 train_time:122404ms step_avg:61.20ms
step:2001/2245 train_time:122424ms step_avg:61.18ms
step:2002/2245 train_time:122466ms step_avg:61.17ms
step:2003/2245 train_time:122533ms step_avg:61.17ms
step:2004/2245 train_time:122596ms step_avg:61.18ms
step:2005/2245 train_time:122659ms step_avg:61.18ms
step:2006/2245 train_time:122719ms step_avg:61.18ms
step:2007/2245 train_time:122781ms step_avg:61.18ms
step:2008/2245 train_time:122841ms step_avg:61.18ms
step:2009/2245 train_time:122903ms step_avg:61.18ms
step:2010/2245 train_time:122964ms step_avg:61.18ms
step:2011/2245 train_time:123025ms step_avg:61.18ms
step:2012/2245 train_time:123086ms step_avg:61.18ms
step:2013/2245 train_time:123148ms step_avg:61.18ms
step:2014/2245 train_time:123207ms step_avg:61.18ms
step:2015/2245 train_time:123269ms step_avg:61.18ms
step:2016/2245 train_time:123331ms step_avg:61.18ms
step:2017/2245 train_time:123395ms step_avg:61.18ms
step:2018/2245 train_time:123457ms step_avg:61.18ms
step:2019/2245 train_time:123521ms step_avg:61.18ms
step:2020/2245 train_time:123582ms step_avg:61.18ms
step:2021/2245 train_time:123645ms step_avg:61.18ms
step:2022/2245 train_time:123706ms step_avg:61.18ms
step:2023/2245 train_time:123769ms step_avg:61.18ms
step:2024/2245 train_time:123830ms step_avg:61.18ms
step:2025/2245 train_time:123892ms step_avg:61.18ms
step:2026/2245 train_time:123953ms step_avg:61.18ms
step:2027/2245 train_time:124016ms step_avg:61.18ms
step:2028/2245 train_time:124076ms step_avg:61.18ms
step:2029/2245 train_time:124139ms step_avg:61.18ms
step:2030/2245 train_time:124199ms step_avg:61.18ms
step:2031/2245 train_time:124261ms step_avg:61.18ms
step:2032/2245 train_time:124321ms step_avg:61.18ms
step:2033/2245 train_time:124384ms step_avg:61.18ms
step:2034/2245 train_time:124445ms step_avg:61.18ms
step:2035/2245 train_time:124508ms step_avg:61.18ms
step:2036/2245 train_time:124569ms step_avg:61.18ms
step:2037/2245 train_time:124633ms step_avg:61.18ms
step:2038/2245 train_time:124694ms step_avg:61.18ms
step:2039/2245 train_time:124757ms step_avg:61.19ms
step:2040/2245 train_time:124818ms step_avg:61.19ms
step:2041/2245 train_time:124880ms step_avg:61.19ms
step:2042/2245 train_time:124941ms step_avg:61.19ms
step:2043/2245 train_time:125003ms step_avg:61.19ms
step:2044/2245 train_time:125063ms step_avg:61.19ms
step:2045/2245 train_time:125125ms step_avg:61.19ms
step:2046/2245 train_time:125185ms step_avg:61.19ms
step:2047/2245 train_time:125248ms step_avg:61.19ms
step:2048/2245 train_time:125309ms step_avg:61.19ms
step:2049/2245 train_time:125372ms step_avg:61.19ms
step:2050/2245 train_time:125433ms step_avg:61.19ms
step:2051/2245 train_time:125496ms step_avg:61.19ms
step:2052/2245 train_time:125557ms step_avg:61.19ms
step:2053/2245 train_time:125620ms step_avg:61.19ms
step:2054/2245 train_time:125680ms step_avg:61.19ms
step:2055/2245 train_time:125743ms step_avg:61.19ms
step:2056/2245 train_time:125805ms step_avg:61.19ms
step:2057/2245 train_time:125867ms step_avg:61.19ms
step:2058/2245 train_time:125928ms step_avg:61.19ms
step:2059/2245 train_time:125990ms step_avg:61.19ms
step:2060/2245 train_time:126051ms step_avg:61.19ms
step:2061/2245 train_time:126114ms step_avg:61.19ms
step:2062/2245 train_time:126174ms step_avg:61.19ms
step:2063/2245 train_time:126237ms step_avg:61.19ms
step:2064/2245 train_time:126298ms step_avg:61.19ms
step:2065/2245 train_time:126360ms step_avg:61.19ms
step:2066/2245 train_time:126421ms step_avg:61.19ms
step:2067/2245 train_time:126483ms step_avg:61.19ms
step:2068/2245 train_time:126544ms step_avg:61.19ms
step:2069/2245 train_time:126607ms step_avg:61.19ms
step:2070/2245 train_time:126668ms step_avg:61.19ms
step:2071/2245 train_time:126731ms step_avg:61.19ms
step:2072/2245 train_time:126792ms step_avg:61.19ms
step:2073/2245 train_time:126855ms step_avg:61.19ms
step:2074/2245 train_time:126915ms step_avg:61.19ms
step:2075/2245 train_time:126978ms step_avg:61.19ms
step:2076/2245 train_time:127038ms step_avg:61.19ms
step:2077/2245 train_time:127100ms step_avg:61.19ms
step:2078/2245 train_time:127160ms step_avg:61.19ms
step:2079/2245 train_time:127223ms step_avg:61.19ms
step:2080/2245 train_time:127284ms step_avg:61.19ms
step:2081/2245 train_time:127346ms step_avg:61.19ms
step:2082/2245 train_time:127406ms step_avg:61.19ms
step:2083/2245 train_time:127469ms step_avg:61.19ms
step:2084/2245 train_time:127529ms step_avg:61.19ms
step:2085/2245 train_time:127592ms step_avg:61.20ms
step:2086/2245 train_time:127653ms step_avg:61.20ms
step:2087/2245 train_time:127716ms step_avg:61.20ms
step:2088/2245 train_time:127778ms step_avg:61.20ms
step:2089/2245 train_time:127841ms step_avg:61.20ms
step:2090/2245 train_time:127900ms step_avg:61.20ms
step:2091/2245 train_time:127963ms step_avg:61.20ms
step:2092/2245 train_time:128023ms step_avg:61.20ms
step:2093/2245 train_time:128086ms step_avg:61.20ms
step:2094/2245 train_time:128146ms step_avg:61.20ms
step:2095/2245 train_time:128209ms step_avg:61.20ms
step:2096/2245 train_time:128269ms step_avg:61.20ms
step:2097/2245 train_time:128333ms step_avg:61.20ms
step:2098/2245 train_time:128395ms step_avg:61.20ms
step:2099/2245 train_time:128458ms step_avg:61.20ms
step:2100/2245 train_time:128519ms step_avg:61.20ms
step:2101/2245 train_time:128581ms step_avg:61.20ms
step:2102/2245 train_time:128642ms step_avg:61.20ms
step:2103/2245 train_time:128705ms step_avg:61.20ms
step:2104/2245 train_time:128766ms step_avg:61.20ms
step:2105/2245 train_time:128829ms step_avg:61.20ms
step:2106/2245 train_time:128890ms step_avg:61.20ms
step:2107/2245 train_time:128953ms step_avg:61.20ms
step:2108/2245 train_time:129014ms step_avg:61.20ms
step:2109/2245 train_time:129077ms step_avg:61.20ms
step:2110/2245 train_time:129138ms step_avg:61.20ms
step:2111/2245 train_time:129200ms step_avg:61.20ms
step:2112/2245 train_time:129260ms step_avg:61.20ms
step:2113/2245 train_time:129323ms step_avg:61.20ms
step:2114/2245 train_time:129384ms step_avg:61.20ms
step:2115/2245 train_time:129447ms step_avg:61.20ms
step:2116/2245 train_time:129507ms step_avg:61.20ms
step:2117/2245 train_time:129570ms step_avg:61.20ms
step:2118/2245 train_time:129631ms step_avg:61.20ms
step:2119/2245 train_time:129694ms step_avg:61.21ms
step:2120/2245 train_time:129755ms step_avg:61.21ms
step:2121/2245 train_time:129817ms step_avg:61.21ms
step:2122/2245 train_time:129878ms step_avg:61.21ms
step:2123/2245 train_time:129941ms step_avg:61.21ms
step:2124/2245 train_time:130001ms step_avg:61.21ms
step:2125/2245 train_time:130064ms step_avg:61.21ms
step:2126/2245 train_time:130124ms step_avg:61.21ms
step:2127/2245 train_time:130187ms step_avg:61.21ms
step:2128/2245 train_time:130247ms step_avg:61.21ms
step:2129/2245 train_time:130310ms step_avg:61.21ms
step:2130/2245 train_time:130371ms step_avg:61.21ms
step:2131/2245 train_time:130435ms step_avg:61.21ms
step:2132/2245 train_time:130495ms step_avg:61.21ms
step:2133/2245 train_time:130558ms step_avg:61.21ms
step:2134/2245 train_time:130619ms step_avg:61.21ms
step:2135/2245 train_time:130682ms step_avg:61.21ms
step:2136/2245 train_time:130742ms step_avg:61.21ms
step:2137/2245 train_time:130805ms step_avg:61.21ms
step:2138/2245 train_time:130866ms step_avg:61.21ms
step:2139/2245 train_time:130928ms step_avg:61.21ms
step:2140/2245 train_time:130989ms step_avg:61.21ms
step:2141/2245 train_time:131052ms step_avg:61.21ms
step:2142/2245 train_time:131113ms step_avg:61.21ms
step:2143/2245 train_time:131176ms step_avg:61.21ms
step:2144/2245 train_time:131238ms step_avg:61.21ms
step:2145/2245 train_time:131300ms step_avg:61.21ms
step:2146/2245 train_time:131360ms step_avg:61.21ms
step:2147/2245 train_time:131424ms step_avg:61.21ms
step:2148/2245 train_time:131485ms step_avg:61.21ms
step:2149/2245 train_time:131549ms step_avg:61.21ms
step:2150/2245 train_time:131610ms step_avg:61.21ms
step:2151/2245 train_time:131672ms step_avg:61.21ms
step:2152/2245 train_time:131734ms step_avg:61.21ms
step:2153/2245 train_time:131798ms step_avg:61.22ms
step:2154/2245 train_time:131858ms step_avg:61.22ms
step:2155/2245 train_time:131920ms step_avg:61.22ms
step:2156/2245 train_time:131982ms step_avg:61.22ms
step:2157/2245 train_time:132044ms step_avg:61.22ms
step:2158/2245 train_time:132104ms step_avg:61.22ms
step:2159/2245 train_time:132167ms step_avg:61.22ms
step:2160/2245 train_time:132228ms step_avg:61.22ms
step:2161/2245 train_time:132291ms step_avg:61.22ms
step:2162/2245 train_time:132352ms step_avg:61.22ms
step:2163/2245 train_time:132416ms step_avg:61.22ms
step:2164/2245 train_time:132477ms step_avg:61.22ms
step:2165/2245 train_time:132540ms step_avg:61.22ms
step:2166/2245 train_time:132601ms step_avg:61.22ms
step:2167/2245 train_time:132663ms step_avg:61.22ms
step:2168/2245 train_time:132723ms step_avg:61.22ms
step:2169/2245 train_time:132786ms step_avg:61.22ms
step:2170/2245 train_time:132847ms step_avg:61.22ms
step:2171/2245 train_time:132910ms step_avg:61.22ms
step:2172/2245 train_time:132971ms step_avg:61.22ms
step:2173/2245 train_time:133034ms step_avg:61.22ms
step:2174/2245 train_time:133094ms step_avg:61.22ms
step:2175/2245 train_time:133157ms step_avg:61.22ms
step:2176/2245 train_time:133218ms step_avg:61.22ms
step:2177/2245 train_time:133281ms step_avg:61.22ms
step:2178/2245 train_time:133342ms step_avg:61.22ms
step:2179/2245 train_time:133404ms step_avg:61.22ms
step:2180/2245 train_time:133465ms step_avg:61.22ms
step:2181/2245 train_time:133528ms step_avg:61.22ms
step:2182/2245 train_time:133589ms step_avg:61.22ms
step:2183/2245 train_time:133652ms step_avg:61.22ms
step:2184/2245 train_time:133713ms step_avg:61.22ms
step:2185/2245 train_time:133776ms step_avg:61.22ms
step:2186/2245 train_time:133837ms step_avg:61.22ms
step:2187/2245 train_time:133900ms step_avg:61.23ms
step:2188/2245 train_time:133960ms step_avg:61.23ms
step:2189/2245 train_time:134023ms step_avg:61.23ms
step:2190/2245 train_time:134084ms step_avg:61.23ms
step:2191/2245 train_time:134147ms step_avg:61.23ms
step:2192/2245 train_time:134207ms step_avg:61.23ms
step:2193/2245 train_time:134270ms step_avg:61.23ms
step:2194/2245 train_time:134331ms step_avg:61.23ms
step:2195/2245 train_time:134394ms step_avg:61.23ms
step:2196/2245 train_time:134456ms step_avg:61.23ms
step:2197/2245 train_time:134519ms step_avg:61.23ms
step:2198/2245 train_time:134579ms step_avg:61.23ms
step:2199/2245 train_time:134641ms step_avg:61.23ms
step:2200/2245 train_time:134702ms step_avg:61.23ms
step:2201/2245 train_time:134765ms step_avg:61.23ms
step:2202/2245 train_time:134826ms step_avg:61.23ms
step:2203/2245 train_time:134889ms step_avg:61.23ms
step:2204/2245 train_time:134950ms step_avg:61.23ms
step:2205/2245 train_time:135013ms step_avg:61.23ms
step:2206/2245 train_time:135074ms step_avg:61.23ms
step:2207/2245 train_time:135139ms step_avg:61.23ms
step:2208/2245 train_time:135199ms step_avg:61.23ms
step:2209/2245 train_time:135262ms step_avg:61.23ms
step:2210/2245 train_time:135323ms step_avg:61.23ms
step:2211/2245 train_time:135386ms step_avg:61.23ms
step:2212/2245 train_time:135447ms step_avg:61.23ms
step:2213/2245 train_time:135511ms step_avg:61.23ms
step:2214/2245 train_time:135571ms step_avg:61.23ms
step:2215/2245 train_time:135634ms step_avg:61.23ms
step:2216/2245 train_time:135695ms step_avg:61.23ms
step:2217/2245 train_time:135758ms step_avg:61.23ms
step:2218/2245 train_time:135819ms step_avg:61.23ms
step:2219/2245 train_time:135882ms step_avg:61.24ms
step:2220/2245 train_time:135942ms step_avg:61.24ms
step:2221/2245 train_time:136005ms step_avg:61.24ms
step:2222/2245 train_time:136066ms step_avg:61.24ms
step:2223/2245 train_time:136130ms step_avg:61.24ms
step:2224/2245 train_time:136191ms step_avg:61.24ms
step:2225/2245 train_time:136254ms step_avg:61.24ms
step:2226/2245 train_time:136315ms step_avg:61.24ms
step:2227/2245 train_time:136378ms step_avg:61.24ms
step:2228/2245 train_time:136438ms step_avg:61.24ms
step:2229/2245 train_time:136501ms step_avg:61.24ms
step:2230/2245 train_time:136562ms step_avg:61.24ms
step:2231/2245 train_time:136625ms step_avg:61.24ms
step:2232/2245 train_time:136685ms step_avg:61.24ms
step:2233/2245 train_time:136748ms step_avg:61.24ms
step:2234/2245 train_time:136808ms step_avg:61.24ms
step:2235/2245 train_time:136871ms step_avg:61.24ms
step:2236/2245 train_time:136933ms step_avg:61.24ms
step:2237/2245 train_time:136996ms step_avg:61.24ms
step:2238/2245 train_time:137058ms step_avg:61.24ms
step:2239/2245 train_time:137120ms step_avg:61.24ms
step:2240/2245 train_time:137181ms step_avg:61.24ms
step:2241/2245 train_time:137243ms step_avg:61.24ms
step:2242/2245 train_time:137304ms step_avg:61.24ms
step:2243/2245 train_time:137368ms step_avg:61.24ms
step:2244/2245 train_time:137429ms step_avg:61.24ms
step:2245/2245 train_time:137492ms step_avg:61.24ms
step:2245/2245 val_loss:3.2754 train_time:137554ms step_avg:61.27ms
peak memory allocated: 29244 MiB reserved: 44256 MiB

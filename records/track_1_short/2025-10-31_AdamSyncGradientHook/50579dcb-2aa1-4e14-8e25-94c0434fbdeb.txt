import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)


            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))



            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal
        self.group_to_param_group = {}

        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                self.group_to_param_group[param] = group

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        world_size = dist.get_world_size()
        grad = param.grad
        rank_size = grad.shape[0] // world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(), grad_slice)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']

            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
        # enable sync in the next training step for the adam optimizer
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = next(train_loader)

        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()


====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251031+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 31 16:11:41 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   34C    P0            123W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   31C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   30C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   33C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2315 train_time:89ms step_avg:88.92ms
step:2/2315 train_time:197ms step_avg:98.35ms
step:3/2315 train_time:216ms step_avg:72.02ms
step:4/2315 train_time:255ms step_avg:63.72ms
step:5/2315 train_time:312ms step_avg:62.47ms
step:6/2315 train_time:372ms step_avg:62.02ms
step:7/2315 train_time:431ms step_avg:61.64ms
step:8/2315 train_time:491ms step_avg:61.38ms
step:9/2315 train_time:551ms step_avg:61.18ms
step:10/2315 train_time:610ms step_avg:61.02ms
step:11/2315 train_time:669ms step_avg:60.83ms
step:12/2315 train_time:729ms step_avg:60.72ms
step:13/2315 train_time:788ms step_avg:60.60ms
step:14/2315 train_time:848ms step_avg:60.54ms
step:15/2315 train_time:907ms step_avg:60.47ms
step:16/2315 train_time:967ms step_avg:60.44ms
step:17/2315 train_time:1028ms step_avg:60.47ms
step:18/2315 train_time:1091ms step_avg:60.63ms
step:19/2315 train_time:1156ms step_avg:60.83ms
step:20/2315 train_time:1219ms step_avg:60.94ms
step:21/2315 train_time:1279ms step_avg:60.90ms
step:22/2315 train_time:1340ms step_avg:60.90ms
step:23/2315 train_time:1400ms step_avg:60.88ms
step:24/2315 train_time:1461ms step_avg:60.86ms
step:25/2315 train_time:1521ms step_avg:60.85ms
step:26/2315 train_time:1582ms step_avg:60.86ms
step:27/2315 train_time:1642ms step_avg:60.81ms
step:28/2315 train_time:1702ms step_avg:60.78ms
step:29/2315 train_time:1761ms step_avg:60.72ms
step:30/2315 train_time:1821ms step_avg:60.69ms
step:31/2315 train_time:1880ms step_avg:60.64ms
step:32/2315 train_time:1941ms step_avg:60.64ms
step:33/2315 train_time:2000ms step_avg:60.61ms
step:34/2315 train_time:2061ms step_avg:60.60ms
step:35/2315 train_time:2122ms step_avg:60.62ms
step:36/2315 train_time:2184ms step_avg:60.68ms
step:37/2315 train_time:2244ms step_avg:60.65ms
step:38/2315 train_time:2305ms step_avg:60.67ms
step:39/2315 train_time:2366ms step_avg:60.66ms
step:40/2315 train_time:2427ms step_avg:60.68ms
step:41/2315 train_time:2487ms step_avg:60.66ms
step:42/2315 train_time:2548ms step_avg:60.66ms
step:43/2315 train_time:2607ms step_avg:60.63ms
step:44/2315 train_time:2668ms step_avg:60.63ms
step:45/2315 train_time:2728ms step_avg:60.62ms
step:46/2315 train_time:2788ms step_avg:60.60ms
step:47/2315 train_time:2847ms step_avg:60.58ms
step:48/2315 train_time:2907ms step_avg:60.57ms
step:49/2315 train_time:2967ms step_avg:60.56ms
step:50/2315 train_time:3028ms step_avg:60.56ms
step:51/2315 train_time:3088ms step_avg:60.55ms
step:52/2315 train_time:3149ms step_avg:60.56ms
step:53/2315 train_time:3209ms step_avg:60.55ms
step:54/2315 train_time:3270ms step_avg:60.55ms
step:55/2315 train_time:3330ms step_avg:60.55ms
step:56/2315 train_time:3391ms step_avg:60.56ms
step:57/2315 train_time:3452ms step_avg:60.56ms
step:58/2315 train_time:3512ms step_avg:60.55ms
step:59/2315 train_time:3572ms step_avg:60.54ms
step:60/2315 train_time:3632ms step_avg:60.54ms
step:61/2315 train_time:3691ms step_avg:60.51ms
step:62/2315 train_time:3751ms step_avg:60.50ms
step:63/2315 train_time:3811ms step_avg:60.49ms
step:64/2315 train_time:3871ms step_avg:60.49ms
step:65/2315 train_time:3931ms step_avg:60.47ms
step:66/2315 train_time:3991ms step_avg:60.47ms
step:67/2315 train_time:4051ms step_avg:60.46ms
step:68/2315 train_time:4111ms step_avg:60.45ms
step:69/2315 train_time:4171ms step_avg:60.45ms
step:70/2315 train_time:4232ms step_avg:60.46ms
step:71/2315 train_time:4292ms step_avg:60.45ms
step:72/2315 train_time:4353ms step_avg:60.45ms
step:73/2315 train_time:4413ms step_avg:60.45ms
step:74/2315 train_time:4474ms step_avg:60.45ms
step:75/2315 train_time:4533ms step_avg:60.44ms
step:76/2315 train_time:4593ms step_avg:60.44ms
step:77/2315 train_time:4653ms step_avg:60.43ms
step:78/2315 train_time:4713ms step_avg:60.43ms
step:79/2315 train_time:4773ms step_avg:60.42ms
step:80/2315 train_time:4833ms step_avg:60.42ms
step:81/2315 train_time:4892ms step_avg:60.40ms
step:82/2315 train_time:4953ms step_avg:60.40ms
step:83/2315 train_time:5012ms step_avg:60.39ms
step:84/2315 train_time:5073ms step_avg:60.40ms
step:85/2315 train_time:5133ms step_avg:60.39ms
step:86/2315 train_time:5194ms step_avg:60.39ms
step:87/2315 train_time:5254ms step_avg:60.39ms
step:88/2315 train_time:5314ms step_avg:60.39ms
step:89/2315 train_time:5375ms step_avg:60.39ms
step:90/2315 train_time:5435ms step_avg:60.39ms
step:91/2315 train_time:5494ms step_avg:60.38ms
step:92/2315 train_time:5555ms step_avg:60.39ms
step:93/2315 train_time:5615ms step_avg:60.38ms
step:94/2315 train_time:5676ms step_avg:60.38ms
step:95/2315 train_time:5735ms step_avg:60.37ms
step:96/2315 train_time:5795ms step_avg:60.37ms
step:97/2315 train_time:5855ms step_avg:60.36ms
step:98/2315 train_time:5915ms step_avg:60.36ms
step:99/2315 train_time:5974ms step_avg:60.34ms
step:100/2315 train_time:6034ms step_avg:60.34ms
step:101/2315 train_time:6094ms step_avg:60.33ms
step:102/2315 train_time:6154ms step_avg:60.33ms
step:103/2315 train_time:6214ms step_avg:60.33ms
step:104/2315 train_time:6275ms step_avg:60.34ms
step:105/2315 train_time:6335ms step_avg:60.33ms
step:106/2315 train_time:6395ms step_avg:60.33ms
step:107/2315 train_time:6454ms step_avg:60.32ms
step:108/2315 train_time:6514ms step_avg:60.32ms
step:109/2315 train_time:6574ms step_avg:60.31ms
step:110/2315 train_time:6634ms step_avg:60.31ms
step:111/2315 train_time:6694ms step_avg:60.31ms
step:112/2315 train_time:6754ms step_avg:60.30ms
step:113/2315 train_time:6814ms step_avg:60.30ms
step:114/2315 train_time:6874ms step_avg:60.30ms
step:115/2315 train_time:6934ms step_avg:60.29ms
step:116/2315 train_time:6994ms step_avg:60.29ms
step:117/2315 train_time:7053ms step_avg:60.28ms
step:118/2315 train_time:7113ms step_avg:60.28ms
step:119/2315 train_time:7173ms step_avg:60.27ms
step:120/2315 train_time:7233ms step_avg:60.27ms
step:121/2315 train_time:7293ms step_avg:60.27ms
step:122/2315 train_time:7353ms step_avg:60.27ms
step:123/2315 train_time:7412ms step_avg:60.26ms
step:124/2315 train_time:7473ms step_avg:60.26ms
step:125/2315 train_time:7532ms step_avg:60.26ms
step:126/2315 train_time:7592ms step_avg:60.26ms
step:127/2315 train_time:7652ms step_avg:60.25ms
step:128/2315 train_time:7713ms step_avg:60.26ms
step:129/2315 train_time:7773ms step_avg:60.25ms
step:130/2315 train_time:7833ms step_avg:60.25ms
step:131/2315 train_time:7892ms step_avg:60.24ms
step:132/2315 train_time:7952ms step_avg:60.24ms
step:133/2315 train_time:8011ms step_avg:60.24ms
step:134/2315 train_time:8071ms step_avg:60.23ms
step:135/2315 train_time:8131ms step_avg:60.23ms
step:136/2315 train_time:8191ms step_avg:60.23ms
step:137/2315 train_time:8251ms step_avg:60.22ms
step:138/2315 train_time:8311ms step_avg:60.22ms
step:139/2315 train_time:8370ms step_avg:60.22ms
step:140/2315 train_time:8430ms step_avg:60.21ms
step:141/2315 train_time:8490ms step_avg:60.21ms
step:142/2315 train_time:8550ms step_avg:60.21ms
step:143/2315 train_time:8610ms step_avg:60.21ms
step:144/2315 train_time:8670ms step_avg:60.21ms
step:145/2315 train_time:8730ms step_avg:60.21ms
step:146/2315 train_time:8791ms step_avg:60.21ms
step:147/2315 train_time:8850ms step_avg:60.21ms
step:148/2315 train_time:8911ms step_avg:60.21ms
step:149/2315 train_time:8970ms step_avg:60.20ms
step:150/2315 train_time:9030ms step_avg:60.20ms
step:151/2315 train_time:9090ms step_avg:60.20ms
step:152/2315 train_time:9150ms step_avg:60.20ms
step:153/2315 train_time:9210ms step_avg:60.20ms
step:154/2315 train_time:9271ms step_avg:60.20ms
step:155/2315 train_time:9330ms step_avg:60.20ms
step:156/2315 train_time:9390ms step_avg:60.20ms
step:157/2315 train_time:9450ms step_avg:60.19ms
step:158/2315 train_time:9510ms step_avg:60.19ms
step:159/2315 train_time:9570ms step_avg:60.19ms
step:160/2315 train_time:9631ms step_avg:60.19ms
step:161/2315 train_time:9691ms step_avg:60.19ms
step:162/2315 train_time:9751ms step_avg:60.19ms
step:163/2315 train_time:9811ms step_avg:60.19ms
step:164/2315 train_time:9871ms step_avg:60.19ms
step:165/2315 train_time:9930ms step_avg:60.18ms
step:166/2315 train_time:9990ms step_avg:60.18ms
step:167/2315 train_time:10049ms step_avg:60.18ms
step:168/2315 train_time:10110ms step_avg:60.18ms
step:169/2315 train_time:10170ms step_avg:60.18ms
step:170/2315 train_time:10230ms step_avg:60.18ms
step:171/2315 train_time:10289ms step_avg:60.17ms
step:172/2315 train_time:10349ms step_avg:60.17ms
step:173/2315 train_time:10409ms step_avg:60.17ms
step:174/2315 train_time:10469ms step_avg:60.17ms
step:175/2315 train_time:10529ms step_avg:60.16ms
step:176/2315 train_time:10589ms step_avg:60.17ms
step:177/2315 train_time:10649ms step_avg:60.16ms
step:178/2315 train_time:10710ms step_avg:60.17ms
step:179/2315 train_time:10769ms step_avg:60.16ms
step:180/2315 train_time:10830ms step_avg:60.16ms
step:181/2315 train_time:10889ms step_avg:60.16ms
step:182/2315 train_time:10949ms step_avg:60.16ms
step:183/2315 train_time:11009ms step_avg:60.16ms
step:184/2315 train_time:11069ms step_avg:60.16ms
step:185/2315 train_time:11129ms step_avg:60.16ms
step:186/2315 train_time:11189ms step_avg:60.16ms
step:187/2315 train_time:11249ms step_avg:60.15ms
step:188/2315 train_time:11309ms step_avg:60.15ms
step:189/2315 train_time:11368ms step_avg:60.15ms
step:190/2315 train_time:11428ms step_avg:60.15ms
step:191/2315 train_time:11488ms step_avg:60.15ms
step:192/2315 train_time:11549ms step_avg:60.15ms
step:193/2315 train_time:11608ms step_avg:60.15ms
step:194/2315 train_time:11669ms step_avg:60.15ms
step:195/2315 train_time:11729ms step_avg:60.15ms
step:196/2315 train_time:11788ms step_avg:60.14ms
step:197/2315 train_time:11848ms step_avg:60.14ms
step:198/2315 train_time:11908ms step_avg:60.14ms
step:199/2315 train_time:11967ms step_avg:60.14ms
step:200/2315 train_time:12028ms step_avg:60.14ms
step:201/2315 train_time:12087ms step_avg:60.14ms
step:202/2315 train_time:12147ms step_avg:60.14ms
step:203/2315 train_time:12207ms step_avg:60.13ms
step:204/2315 train_time:12267ms step_avg:60.13ms
step:205/2315 train_time:12327ms step_avg:60.13ms
step:206/2315 train_time:12388ms step_avg:60.13ms
step:207/2315 train_time:12447ms step_avg:60.13ms
step:208/2315 train_time:12507ms step_avg:60.13ms
step:209/2315 train_time:12567ms step_avg:60.13ms
step:210/2315 train_time:12627ms step_avg:60.13ms
step:211/2315 train_time:12686ms step_avg:60.13ms
step:212/2315 train_time:12748ms step_avg:60.13ms
step:213/2315 train_time:12808ms step_avg:60.13ms
step:214/2315 train_time:12868ms step_avg:60.13ms
step:215/2315 train_time:12927ms step_avg:60.13ms
step:216/2315 train_time:12988ms step_avg:60.13ms
step:217/2315 train_time:13048ms step_avg:60.13ms
step:218/2315 train_time:13108ms step_avg:60.13ms
step:219/2315 train_time:13167ms step_avg:60.12ms
step:220/2315 train_time:13228ms step_avg:60.13ms
step:221/2315 train_time:13287ms step_avg:60.12ms
step:222/2315 train_time:13348ms step_avg:60.13ms
step:223/2315 train_time:13407ms step_avg:60.12ms
step:224/2315 train_time:13467ms step_avg:60.12ms
step:225/2315 train_time:13527ms step_avg:60.12ms
step:226/2315 train_time:13587ms step_avg:60.12ms
step:227/2315 train_time:13647ms step_avg:60.12ms
step:228/2315 train_time:13707ms step_avg:60.12ms
step:229/2315 train_time:13767ms step_avg:60.12ms
step:230/2315 train_time:13827ms step_avg:60.12ms
step:231/2315 train_time:13887ms step_avg:60.12ms
step:232/2315 train_time:13947ms step_avg:60.12ms
step:233/2315 train_time:14006ms step_avg:60.11ms
step:234/2315 train_time:14066ms step_avg:60.11ms
step:235/2315 train_time:14126ms step_avg:60.11ms
step:236/2315 train_time:14186ms step_avg:60.11ms
step:237/2315 train_time:14245ms step_avg:60.11ms
step:238/2315 train_time:14305ms step_avg:60.11ms
step:239/2315 train_time:14365ms step_avg:60.11ms
step:240/2315 train_time:14427ms step_avg:60.11ms
step:241/2315 train_time:14487ms step_avg:60.11ms
step:242/2315 train_time:14548ms step_avg:60.12ms
step:243/2315 train_time:14607ms step_avg:60.11ms
step:244/2315 train_time:14667ms step_avg:60.11ms
step:245/2315 train_time:14728ms step_avg:60.11ms
step:246/2315 train_time:14788ms step_avg:60.11ms
step:247/2315 train_time:14847ms step_avg:60.11ms
step:248/2315 train_time:14907ms step_avg:60.11ms
step:249/2315 train_time:14967ms step_avg:60.11ms
step:250/2315 train_time:15027ms step_avg:60.11ms
step:250/2315 val_loss:4.0683 train_time:15088ms step_avg:60.35ms
step:251/2315 train_time:15109ms step_avg:60.20ms
step:252/2315 train_time:15151ms step_avg:60.12ms
step:253/2315 train_time:15217ms step_avg:60.15ms
step:254/2315 train_time:15281ms step_avg:60.16ms
step:255/2315 train_time:15342ms step_avg:60.16ms
step:256/2315 train_time:15402ms step_avg:60.16ms
step:257/2315 train_time:15461ms step_avg:60.16ms
step:258/2315 train_time:15521ms step_avg:60.16ms
step:259/2315 train_time:15580ms step_avg:60.15ms
step:260/2315 train_time:15639ms step_avg:60.15ms
step:261/2315 train_time:15698ms step_avg:60.14ms
step:262/2315 train_time:15757ms step_avg:60.14ms
step:263/2315 train_time:15815ms step_avg:60.13ms
step:264/2315 train_time:15875ms step_avg:60.13ms
step:265/2315 train_time:15934ms step_avg:60.13ms
step:266/2315 train_time:15994ms step_avg:60.13ms
step:267/2315 train_time:16055ms step_avg:60.13ms
step:268/2315 train_time:16117ms step_avg:60.14ms
step:269/2315 train_time:16180ms step_avg:60.15ms
step:270/2315 train_time:16242ms step_avg:60.15ms
step:271/2315 train_time:16302ms step_avg:60.16ms
step:272/2315 train_time:16362ms step_avg:60.15ms
step:273/2315 train_time:16422ms step_avg:60.15ms
step:274/2315 train_time:16482ms step_avg:60.15ms
step:275/2315 train_time:16541ms step_avg:60.15ms
step:276/2315 train_time:16601ms step_avg:60.15ms
step:277/2315 train_time:16660ms step_avg:60.14ms
step:278/2315 train_time:16720ms step_avg:60.14ms
step:279/2315 train_time:16779ms step_avg:60.14ms
step:280/2315 train_time:16839ms step_avg:60.14ms
step:281/2315 train_time:16898ms step_avg:60.14ms
step:282/2315 train_time:16958ms step_avg:60.14ms
step:283/2315 train_time:17018ms step_avg:60.13ms
step:284/2315 train_time:17079ms step_avg:60.14ms
step:285/2315 train_time:17140ms step_avg:60.14ms
step:286/2315 train_time:17200ms step_avg:60.14ms
step:287/2315 train_time:17261ms step_avg:60.14ms
step:288/2315 train_time:17321ms step_avg:60.14ms
step:289/2315 train_time:17381ms step_avg:60.14ms
step:290/2315 train_time:17441ms step_avg:60.14ms
step:291/2315 train_time:17501ms step_avg:60.14ms
step:292/2315 train_time:17561ms step_avg:60.14ms
step:293/2315 train_time:17620ms step_avg:60.14ms
step:294/2315 train_time:17680ms step_avg:60.14ms
step:295/2315 train_time:17740ms step_avg:60.13ms
step:296/2315 train_time:17800ms step_avg:60.13ms
step:297/2315 train_time:17859ms step_avg:60.13ms
step:298/2315 train_time:17919ms step_avg:60.13ms
step:299/2315 train_time:17978ms step_avg:60.13ms
step:300/2315 train_time:18039ms step_avg:60.13ms
step:301/2315 train_time:18099ms step_avg:60.13ms
step:302/2315 train_time:18160ms step_avg:60.13ms
step:303/2315 train_time:18221ms step_avg:60.13ms
step:304/2315 train_time:18282ms step_avg:60.14ms
step:305/2315 train_time:18341ms step_avg:60.14ms
step:306/2315 train_time:18402ms step_avg:60.14ms
step:307/2315 train_time:18462ms step_avg:60.14ms
step:308/2315 train_time:18522ms step_avg:60.14ms
step:309/2315 train_time:18581ms step_avg:60.13ms
step:310/2315 train_time:18641ms step_avg:60.13ms
step:311/2315 train_time:18700ms step_avg:60.13ms
step:312/2315 train_time:18760ms step_avg:60.13ms
step:313/2315 train_time:18820ms step_avg:60.13ms
step:314/2315 train_time:18881ms step_avg:60.13ms
step:315/2315 train_time:18941ms step_avg:60.13ms
step:316/2315 train_time:19001ms step_avg:60.13ms
step:317/2315 train_time:19060ms step_avg:60.13ms
step:318/2315 train_time:19121ms step_avg:60.13ms
step:319/2315 train_time:19181ms step_avg:60.13ms
step:320/2315 train_time:19242ms step_avg:60.13ms
step:321/2315 train_time:19302ms step_avg:60.13ms
step:322/2315 train_time:19362ms step_avg:60.13ms
step:323/2315 train_time:19421ms step_avg:60.13ms
step:324/2315 train_time:19481ms step_avg:60.13ms
step:325/2315 train_time:19540ms step_avg:60.12ms
step:326/2315 train_time:19599ms step_avg:60.12ms
step:327/2315 train_time:19659ms step_avg:60.12ms
step:328/2315 train_time:19719ms step_avg:60.12ms
step:329/2315 train_time:19778ms step_avg:60.12ms
step:330/2315 train_time:19838ms step_avg:60.12ms
step:331/2315 train_time:19897ms step_avg:60.11ms
step:332/2315 train_time:19957ms step_avg:60.11ms
step:333/2315 train_time:20016ms step_avg:60.11ms
step:334/2315 train_time:20076ms step_avg:60.11ms
step:335/2315 train_time:20136ms step_avg:60.11ms
step:336/2315 train_time:20197ms step_avg:60.11ms
step:337/2315 train_time:20257ms step_avg:60.11ms
step:338/2315 train_time:20318ms step_avg:60.11ms
step:339/2315 train_time:20378ms step_avg:60.11ms
step:340/2315 train_time:20438ms step_avg:60.11ms
step:341/2315 train_time:20498ms step_avg:60.11ms
step:342/2315 train_time:20558ms step_avg:60.11ms
step:343/2315 train_time:20617ms step_avg:60.11ms
step:344/2315 train_time:20677ms step_avg:60.11ms
step:345/2315 train_time:20737ms step_avg:60.11ms
step:346/2315 train_time:20797ms step_avg:60.11ms
step:347/2315 train_time:20856ms step_avg:60.10ms
step:348/2315 train_time:20916ms step_avg:60.10ms
step:349/2315 train_time:20975ms step_avg:60.10ms
step:350/2315 train_time:21035ms step_avg:60.10ms
step:351/2315 train_time:21095ms step_avg:60.10ms
step:352/2315 train_time:21155ms step_avg:60.10ms
step:353/2315 train_time:21216ms step_avg:60.10ms
step:354/2315 train_time:21276ms step_avg:60.10ms
step:355/2315 train_time:21337ms step_avg:60.10ms
step:356/2315 train_time:21397ms step_avg:60.11ms
step:357/2315 train_time:21458ms step_avg:60.11ms
step:358/2315 train_time:21518ms step_avg:60.11ms
step:359/2315 train_time:21578ms step_avg:60.11ms
step:360/2315 train_time:21638ms step_avg:60.11ms
step:361/2315 train_time:21698ms step_avg:60.10ms
step:362/2315 train_time:21758ms step_avg:60.10ms
step:363/2315 train_time:21818ms step_avg:60.10ms
step:364/2315 train_time:21878ms step_avg:60.10ms
step:365/2315 train_time:21937ms step_avg:60.10ms
step:366/2315 train_time:21997ms step_avg:60.10ms
step:367/2315 train_time:22057ms step_avg:60.10ms
step:368/2315 train_time:22117ms step_avg:60.10ms
step:369/2315 train_time:22177ms step_avg:60.10ms
step:370/2315 train_time:22237ms step_avg:60.10ms
step:371/2315 train_time:22296ms step_avg:60.10ms
step:372/2315 train_time:22357ms step_avg:60.10ms
step:373/2315 train_time:22416ms step_avg:60.10ms
step:374/2315 train_time:22477ms step_avg:60.10ms
step:375/2315 train_time:22536ms step_avg:60.10ms
step:376/2315 train_time:22597ms step_avg:60.10ms
step:377/2315 train_time:22657ms step_avg:60.10ms
step:378/2315 train_time:22717ms step_avg:60.10ms
step:379/2315 train_time:22776ms step_avg:60.10ms
step:380/2315 train_time:22836ms step_avg:60.10ms
step:381/2315 train_time:22895ms step_avg:60.09ms
step:382/2315 train_time:22955ms step_avg:60.09ms
step:383/2315 train_time:23015ms step_avg:60.09ms
step:384/2315 train_time:23075ms step_avg:60.09ms
step:385/2315 train_time:23134ms step_avg:60.09ms
step:386/2315 train_time:23194ms step_avg:60.09ms
step:387/2315 train_time:23254ms step_avg:60.09ms
step:388/2315 train_time:23315ms step_avg:60.09ms
step:389/2315 train_time:23375ms step_avg:60.09ms
step:390/2315 train_time:23435ms step_avg:60.09ms
step:391/2315 train_time:23494ms step_avg:60.09ms
step:392/2315 train_time:23554ms step_avg:60.09ms
step:393/2315 train_time:23614ms step_avg:60.09ms
step:394/2315 train_time:23674ms step_avg:60.09ms
step:395/2315 train_time:23734ms step_avg:60.09ms
step:396/2315 train_time:23794ms step_avg:60.09ms
step:397/2315 train_time:23853ms step_avg:60.08ms
step:398/2315 train_time:23914ms step_avg:60.08ms
step:399/2315 train_time:23973ms step_avg:60.08ms
step:400/2315 train_time:24033ms step_avg:60.08ms
step:401/2315 train_time:24092ms step_avg:60.08ms
step:402/2315 train_time:24153ms step_avg:60.08ms
step:403/2315 train_time:24213ms step_avg:60.08ms
step:404/2315 train_time:24273ms step_avg:60.08ms
step:405/2315 train_time:24332ms step_avg:60.08ms
step:406/2315 train_time:24392ms step_avg:60.08ms
step:407/2315 train_time:24452ms step_avg:60.08ms
step:408/2315 train_time:24512ms step_avg:60.08ms
step:409/2315 train_time:24572ms step_avg:60.08ms
step:410/2315 train_time:24632ms step_avg:60.08ms
step:411/2315 train_time:24692ms step_avg:60.08ms
step:412/2315 train_time:24753ms step_avg:60.08ms
step:413/2315 train_time:24812ms step_avg:60.08ms
step:414/2315 train_time:24872ms step_avg:60.08ms
step:415/2315 train_time:24932ms step_avg:60.08ms
step:416/2315 train_time:24992ms step_avg:60.08ms
step:417/2315 train_time:25051ms step_avg:60.07ms
step:418/2315 train_time:25111ms step_avg:60.07ms
step:419/2315 train_time:25171ms step_avg:60.07ms
step:420/2315 train_time:25231ms step_avg:60.07ms
step:421/2315 train_time:25291ms step_avg:60.07ms
step:422/2315 train_time:25351ms step_avg:60.07ms
step:423/2315 train_time:25411ms step_avg:60.07ms
step:424/2315 train_time:25471ms step_avg:60.07ms
step:425/2315 train_time:25531ms step_avg:60.07ms
step:426/2315 train_time:25591ms step_avg:60.07ms
step:427/2315 train_time:25650ms step_avg:60.07ms
step:428/2315 train_time:25710ms step_avg:60.07ms
step:429/2315 train_time:25770ms step_avg:60.07ms
step:430/2315 train_time:25831ms step_avg:60.07ms
step:431/2315 train_time:25890ms step_avg:60.07ms
step:432/2315 train_time:25950ms step_avg:60.07ms
step:433/2315 train_time:26010ms step_avg:60.07ms
step:434/2315 train_time:26070ms step_avg:60.07ms
step:435/2315 train_time:26130ms step_avg:60.07ms
step:436/2315 train_time:26190ms step_avg:60.07ms
step:437/2315 train_time:26249ms step_avg:60.07ms
step:438/2315 train_time:26309ms step_avg:60.07ms
step:439/2315 train_time:26369ms step_avg:60.07ms
step:440/2315 train_time:26429ms step_avg:60.07ms
step:441/2315 train_time:26488ms step_avg:60.06ms
step:442/2315 train_time:26549ms step_avg:60.07ms
step:443/2315 train_time:26609ms step_avg:60.07ms
step:444/2315 train_time:26670ms step_avg:60.07ms
step:445/2315 train_time:26729ms step_avg:60.06ms
step:446/2315 train_time:26788ms step_avg:60.06ms
step:447/2315 train_time:26849ms step_avg:60.06ms
step:448/2315 train_time:26909ms step_avg:60.07ms
step:449/2315 train_time:26968ms step_avg:60.06ms
step:450/2315 train_time:27028ms step_avg:60.06ms
step:451/2315 train_time:27088ms step_avg:60.06ms
step:452/2315 train_time:27149ms step_avg:60.06ms
step:453/2315 train_time:27209ms step_avg:60.06ms
step:454/2315 train_time:27269ms step_avg:60.06ms
step:455/2315 train_time:27328ms step_avg:60.06ms
step:456/2315 train_time:27388ms step_avg:60.06ms
step:457/2315 train_time:27447ms step_avg:60.06ms
step:458/2315 train_time:27507ms step_avg:60.06ms
step:459/2315 train_time:27567ms step_avg:60.06ms
step:460/2315 train_time:27627ms step_avg:60.06ms
step:461/2315 train_time:27687ms step_avg:60.06ms
step:462/2315 train_time:27747ms step_avg:60.06ms
step:463/2315 train_time:27807ms step_avg:60.06ms
step:464/2315 train_time:27867ms step_avg:60.06ms
step:465/2315 train_time:27927ms step_avg:60.06ms
step:466/2315 train_time:27988ms step_avg:60.06ms
step:467/2315 train_time:28047ms step_avg:60.06ms
step:468/2315 train_time:28107ms step_avg:60.06ms
step:469/2315 train_time:28166ms step_avg:60.06ms
step:470/2315 train_time:28226ms step_avg:60.06ms
step:471/2315 train_time:28286ms step_avg:60.05ms
step:472/2315 train_time:28346ms step_avg:60.05ms
step:473/2315 train_time:28406ms step_avg:60.05ms
step:474/2315 train_time:28466ms step_avg:60.05ms
step:475/2315 train_time:28525ms step_avg:60.05ms
step:476/2315 train_time:28585ms step_avg:60.05ms
step:477/2315 train_time:28645ms step_avg:60.05ms
step:478/2315 train_time:28705ms step_avg:60.05ms
step:479/2315 train_time:28764ms step_avg:60.05ms
step:480/2315 train_time:28825ms step_avg:60.05ms
step:481/2315 train_time:28884ms step_avg:60.05ms
step:482/2315 train_time:28944ms step_avg:60.05ms
step:483/2315 train_time:29004ms step_avg:60.05ms
step:484/2315 train_time:29064ms step_avg:60.05ms
step:485/2315 train_time:29124ms step_avg:60.05ms
step:486/2315 train_time:29184ms step_avg:60.05ms
step:487/2315 train_time:29244ms step_avg:60.05ms
step:488/2315 train_time:29305ms step_avg:60.05ms
step:489/2315 train_time:29365ms step_avg:60.05ms
step:490/2315 train_time:29425ms step_avg:60.05ms
step:491/2315 train_time:29484ms step_avg:60.05ms
step:492/2315 train_time:29544ms step_avg:60.05ms
step:493/2315 train_time:29604ms step_avg:60.05ms
step:494/2315 train_time:29665ms step_avg:60.05ms
step:495/2315 train_time:29724ms step_avg:60.05ms
step:496/2315 train_time:29784ms step_avg:60.05ms
step:497/2315 train_time:29844ms step_avg:60.05ms
step:498/2315 train_time:29904ms step_avg:60.05ms
step:499/2315 train_time:29964ms step_avg:60.05ms
step:500/2315 train_time:30023ms step_avg:60.05ms
step:500/2315 val_loss:3.8108 train_time:30084ms step_avg:60.17ms
step:501/2315 train_time:30106ms step_avg:60.09ms
step:502/2315 train_time:30148ms step_avg:60.06ms
step:503/2315 train_time:30210ms step_avg:60.06ms
step:504/2315 train_time:30271ms step_avg:60.06ms
step:505/2315 train_time:30331ms step_avg:60.06ms
step:506/2315 train_time:30392ms step_avg:60.06ms
step:507/2315 train_time:30453ms step_avg:60.06ms
step:508/2315 train_time:30512ms step_avg:60.06ms
step:509/2315 train_time:30572ms step_avg:60.06ms
step:510/2315 train_time:30632ms step_avg:60.06ms
step:511/2315 train_time:30691ms step_avg:60.06ms
step:512/2315 train_time:30751ms step_avg:60.06ms
step:513/2315 train_time:30810ms step_avg:60.06ms
step:514/2315 train_time:30870ms step_avg:60.06ms
step:515/2315 train_time:30928ms step_avg:60.05ms
step:516/2315 train_time:30989ms step_avg:60.06ms
step:517/2315 train_time:31050ms step_avg:60.06ms
step:518/2315 train_time:31112ms step_avg:60.06ms
step:519/2315 train_time:31173ms step_avg:60.06ms
step:520/2315 train_time:31235ms step_avg:60.07ms
step:521/2315 train_time:31295ms step_avg:60.07ms
step:522/2315 train_time:31356ms step_avg:60.07ms
step:523/2315 train_time:31414ms step_avg:60.07ms
step:524/2315 train_time:31474ms step_avg:60.06ms
step:525/2315 train_time:31533ms step_avg:60.06ms
step:526/2315 train_time:31593ms step_avg:60.06ms
step:527/2315 train_time:31652ms step_avg:60.06ms
step:528/2315 train_time:31713ms step_avg:60.06ms
step:529/2315 train_time:31771ms step_avg:60.06ms
step:530/2315 train_time:31830ms step_avg:60.06ms
step:531/2315 train_time:31889ms step_avg:60.05ms
step:532/2315 train_time:31950ms step_avg:60.06ms
step:533/2315 train_time:32010ms step_avg:60.06ms
step:534/2315 train_time:32071ms step_avg:60.06ms
step:535/2315 train_time:32132ms step_avg:60.06ms
step:536/2315 train_time:32193ms step_avg:60.06ms
step:537/2315 train_time:32254ms step_avg:60.06ms
step:538/2315 train_time:32314ms step_avg:60.06ms
step:539/2315 train_time:32374ms step_avg:60.06ms
step:540/2315 train_time:32434ms step_avg:60.06ms
step:541/2315 train_time:32493ms step_avg:60.06ms
step:542/2315 train_time:32553ms step_avg:60.06ms
step:543/2315 train_time:32612ms step_avg:60.06ms
step:544/2315 train_time:32672ms step_avg:60.06ms
step:545/2315 train_time:32730ms step_avg:60.06ms
step:546/2315 train_time:32790ms step_avg:60.05ms
step:547/2315 train_time:32849ms step_avg:60.05ms
step:548/2315 train_time:32909ms step_avg:60.05ms
step:549/2315 train_time:32968ms step_avg:60.05ms
step:550/2315 train_time:33028ms step_avg:60.05ms
step:551/2315 train_time:33088ms step_avg:60.05ms
step:552/2315 train_time:33149ms step_avg:60.05ms
step:553/2315 train_time:33209ms step_avg:60.05ms
step:554/2315 train_time:33270ms step_avg:60.05ms
step:555/2315 train_time:33329ms step_avg:60.05ms
step:556/2315 train_time:33390ms step_avg:60.05ms
step:557/2315 train_time:33450ms step_avg:60.05ms
step:558/2315 train_time:33510ms step_avg:60.05ms
step:559/2315 train_time:33569ms step_avg:60.05ms
step:560/2315 train_time:33630ms step_avg:60.05ms
step:561/2315 train_time:33689ms step_avg:60.05ms
step:562/2315 train_time:33748ms step_avg:60.05ms
step:563/2315 train_time:33807ms step_avg:60.05ms
step:564/2315 train_time:33867ms step_avg:60.05ms
step:565/2315 train_time:33927ms step_avg:60.05ms
step:566/2315 train_time:33987ms step_avg:60.05ms
step:567/2315 train_time:34047ms step_avg:60.05ms
step:568/2315 train_time:34107ms step_avg:60.05ms
step:569/2315 train_time:34167ms step_avg:60.05ms
step:570/2315 train_time:34228ms step_avg:60.05ms
step:571/2315 train_time:34288ms step_avg:60.05ms
step:572/2315 train_time:34348ms step_avg:60.05ms
step:573/2315 train_time:34408ms step_avg:60.05ms
step:574/2315 train_time:34468ms step_avg:60.05ms
step:575/2315 train_time:34527ms step_avg:60.05ms
step:576/2315 train_time:34587ms step_avg:60.05ms
step:577/2315 train_time:34647ms step_avg:60.05ms
step:578/2315 train_time:34707ms step_avg:60.05ms
step:579/2315 train_time:34767ms step_avg:60.05ms
step:580/2315 train_time:34826ms step_avg:60.05ms
step:581/2315 train_time:34886ms step_avg:60.04ms
step:582/2315 train_time:34945ms step_avg:60.04ms
step:583/2315 train_time:35005ms step_avg:60.04ms
step:584/2315 train_time:35065ms step_avg:60.04ms
step:585/2315 train_time:35126ms step_avg:60.04ms
step:586/2315 train_time:35186ms step_avg:60.04ms
step:587/2315 train_time:35245ms step_avg:60.04ms
step:588/2315 train_time:35306ms step_avg:60.04ms
step:589/2315 train_time:35366ms step_avg:60.04ms
step:590/2315 train_time:35427ms step_avg:60.05ms
step:591/2315 train_time:35487ms step_avg:60.05ms
step:592/2315 train_time:35547ms step_avg:60.05ms
step:593/2315 train_time:35607ms step_avg:60.05ms
step:594/2315 train_time:35667ms step_avg:60.05ms
step:595/2315 train_time:35726ms step_avg:60.04ms
step:596/2315 train_time:35787ms step_avg:60.04ms
step:597/2315 train_time:35845ms step_avg:60.04ms
step:598/2315 train_time:35906ms step_avg:60.04ms
step:599/2315 train_time:35965ms step_avg:60.04ms
step:600/2315 train_time:36025ms step_avg:60.04ms
step:601/2315 train_time:36085ms step_avg:60.04ms
step:602/2315 train_time:36145ms step_avg:60.04ms
step:603/2315 train_time:36206ms step_avg:60.04ms
step:604/2315 train_time:36267ms step_avg:60.04ms
step:605/2315 train_time:36326ms step_avg:60.04ms
step:606/2315 train_time:36386ms step_avg:60.04ms
step:607/2315 train_time:36446ms step_avg:60.04ms
step:608/2315 train_time:36506ms step_avg:60.04ms
step:609/2315 train_time:36566ms step_avg:60.04ms
step:610/2315 train_time:36627ms step_avg:60.04ms
step:611/2315 train_time:36686ms step_avg:60.04ms
step:612/2315 train_time:36746ms step_avg:60.04ms
step:613/2315 train_time:36806ms step_avg:60.04ms
step:614/2315 train_time:36866ms step_avg:60.04ms
step:615/2315 train_time:36926ms step_avg:60.04ms
step:616/2315 train_time:36987ms step_avg:60.04ms
step:617/2315 train_time:37046ms step_avg:60.04ms
step:618/2315 train_time:37107ms step_avg:60.04ms
step:619/2315 train_time:37166ms step_avg:60.04ms
step:620/2315 train_time:37226ms step_avg:60.04ms
step:621/2315 train_time:37286ms step_avg:60.04ms
step:622/2315 train_time:37346ms step_avg:60.04ms
step:623/2315 train_time:37406ms step_avg:60.04ms
step:624/2315 train_time:37467ms step_avg:60.04ms
step:625/2315 train_time:37527ms step_avg:60.04ms
step:626/2315 train_time:37587ms step_avg:60.04ms
step:627/2315 train_time:37647ms step_avg:60.04ms
step:628/2315 train_time:37707ms step_avg:60.04ms
step:629/2315 train_time:37767ms step_avg:60.04ms
step:630/2315 train_time:37828ms step_avg:60.04ms
step:631/2315 train_time:37886ms step_avg:60.04ms
step:632/2315 train_time:37946ms step_avg:60.04ms
step:633/2315 train_time:38006ms step_avg:60.04ms
step:634/2315 train_time:38067ms step_avg:60.04ms
step:635/2315 train_time:38126ms step_avg:60.04ms
step:636/2315 train_time:38187ms step_avg:60.04ms
step:637/2315 train_time:38247ms step_avg:60.04ms
step:638/2315 train_time:38307ms step_avg:60.04ms
step:639/2315 train_time:38367ms step_avg:60.04ms
step:640/2315 train_time:38427ms step_avg:60.04ms
step:641/2315 train_time:38487ms step_avg:60.04ms
step:642/2315 train_time:38547ms step_avg:60.04ms
step:643/2315 train_time:38607ms step_avg:60.04ms
step:644/2315 train_time:38667ms step_avg:60.04ms
step:645/2315 train_time:38727ms step_avg:60.04ms
step:646/2315 train_time:38787ms step_avg:60.04ms
step:647/2315 train_time:38847ms step_avg:60.04ms
step:648/2315 train_time:38907ms step_avg:60.04ms
step:649/2315 train_time:38966ms step_avg:60.04ms
step:650/2315 train_time:39027ms step_avg:60.04ms
step:651/2315 train_time:39086ms step_avg:60.04ms
step:652/2315 train_time:39147ms step_avg:60.04ms
step:653/2315 train_time:39207ms step_avg:60.04ms
step:654/2315 train_time:39267ms step_avg:60.04ms
step:655/2315 train_time:39327ms step_avg:60.04ms
step:656/2315 train_time:39387ms step_avg:60.04ms
step:657/2315 train_time:39447ms step_avg:60.04ms
step:658/2315 train_time:39507ms step_avg:60.04ms
step:659/2315 train_time:39567ms step_avg:60.04ms
step:660/2315 train_time:39628ms step_avg:60.04ms
step:661/2315 train_time:39687ms step_avg:60.04ms
step:662/2315 train_time:39747ms step_avg:60.04ms
step:663/2315 train_time:39808ms step_avg:60.04ms
step:664/2315 train_time:39867ms step_avg:60.04ms
step:665/2315 train_time:39927ms step_avg:60.04ms
step:666/2315 train_time:39987ms step_avg:60.04ms
step:667/2315 train_time:40047ms step_avg:60.04ms
step:668/2315 train_time:40107ms step_avg:60.04ms
step:669/2315 train_time:40167ms step_avg:60.04ms
step:670/2315 train_time:40228ms step_avg:60.04ms
step:671/2315 train_time:40288ms step_avg:60.04ms
step:672/2315 train_time:40348ms step_avg:60.04ms
step:673/2315 train_time:40408ms step_avg:60.04ms
step:674/2315 train_time:40468ms step_avg:60.04ms
step:675/2315 train_time:40527ms step_avg:60.04ms
step:676/2315 train_time:40587ms step_avg:60.04ms
step:677/2315 train_time:40647ms step_avg:60.04ms
step:678/2315 train_time:40707ms step_avg:60.04ms
step:679/2315 train_time:40767ms step_avg:60.04ms
step:680/2315 train_time:40827ms step_avg:60.04ms
step:681/2315 train_time:40887ms step_avg:60.04ms
step:682/2315 train_time:40947ms step_avg:60.04ms
step:683/2315 train_time:41007ms step_avg:60.04ms
step:684/2315 train_time:41067ms step_avg:60.04ms
step:685/2315 train_time:41128ms step_avg:60.04ms
step:686/2315 train_time:41188ms step_avg:60.04ms
step:687/2315 train_time:41247ms step_avg:60.04ms
step:688/2315 train_time:41308ms step_avg:60.04ms
step:689/2315 train_time:41367ms step_avg:60.04ms
step:690/2315 train_time:41427ms step_avg:60.04ms
step:691/2315 train_time:41486ms step_avg:60.04ms
step:692/2315 train_time:41546ms step_avg:60.04ms
step:693/2315 train_time:41607ms step_avg:60.04ms
step:694/2315 train_time:41667ms step_avg:60.04ms
step:695/2315 train_time:41727ms step_avg:60.04ms
step:696/2315 train_time:41787ms step_avg:60.04ms
step:697/2315 train_time:41847ms step_avg:60.04ms
step:698/2315 train_time:41907ms step_avg:60.04ms
step:699/2315 train_time:41966ms step_avg:60.04ms
step:700/2315 train_time:42026ms step_avg:60.04ms
step:701/2315 train_time:42086ms step_avg:60.04ms
step:702/2315 train_time:42146ms step_avg:60.04ms
step:703/2315 train_time:42206ms step_avg:60.04ms
step:704/2315 train_time:42266ms step_avg:60.04ms
step:705/2315 train_time:42326ms step_avg:60.04ms
step:706/2315 train_time:42386ms step_avg:60.04ms
step:707/2315 train_time:42445ms step_avg:60.03ms
step:708/2315 train_time:42505ms step_avg:60.04ms
step:709/2315 train_time:42565ms step_avg:60.04ms
step:710/2315 train_time:42626ms step_avg:60.04ms
step:711/2315 train_time:42686ms step_avg:60.04ms
step:712/2315 train_time:42746ms step_avg:60.04ms
step:713/2315 train_time:42807ms step_avg:60.04ms
step:714/2315 train_time:42867ms step_avg:60.04ms
step:715/2315 train_time:42927ms step_avg:60.04ms
step:716/2315 train_time:42987ms step_avg:60.04ms
step:717/2315 train_time:43046ms step_avg:60.04ms
step:718/2315 train_time:43107ms step_avg:60.04ms
step:719/2315 train_time:43167ms step_avg:60.04ms
step:720/2315 train_time:43227ms step_avg:60.04ms
step:721/2315 train_time:43287ms step_avg:60.04ms
step:722/2315 train_time:43347ms step_avg:60.04ms
step:723/2315 train_time:43406ms step_avg:60.04ms
step:724/2315 train_time:43467ms step_avg:60.04ms
step:725/2315 train_time:43527ms step_avg:60.04ms
step:726/2315 train_time:43587ms step_avg:60.04ms
step:727/2315 train_time:43647ms step_avg:60.04ms
step:728/2315 train_time:43707ms step_avg:60.04ms
step:729/2315 train_time:43767ms step_avg:60.04ms
step:730/2315 train_time:43828ms step_avg:60.04ms
step:731/2315 train_time:43888ms step_avg:60.04ms
step:732/2315 train_time:43947ms step_avg:60.04ms
step:733/2315 train_time:44007ms step_avg:60.04ms
step:734/2315 train_time:44067ms step_avg:60.04ms
step:735/2315 train_time:44127ms step_avg:60.04ms
step:736/2315 train_time:44187ms step_avg:60.04ms
step:737/2315 train_time:44246ms step_avg:60.04ms
step:738/2315 train_time:44307ms step_avg:60.04ms
step:739/2315 train_time:44367ms step_avg:60.04ms
step:740/2315 train_time:44427ms step_avg:60.04ms
step:741/2315 train_time:44486ms step_avg:60.04ms
step:742/2315 train_time:44546ms step_avg:60.04ms
step:743/2315 train_time:44606ms step_avg:60.03ms
step:744/2315 train_time:44667ms step_avg:60.04ms
step:745/2315 train_time:44727ms step_avg:60.04ms
step:746/2315 train_time:44787ms step_avg:60.04ms
step:747/2315 train_time:44847ms step_avg:60.04ms
step:748/2315 train_time:44907ms step_avg:60.04ms
step:749/2315 train_time:44967ms step_avg:60.04ms
step:750/2315 train_time:45027ms step_avg:60.04ms
step:750/2315 val_loss:3.6856 train_time:45088ms step_avg:60.12ms
step:751/2315 train_time:45111ms step_avg:60.07ms
step:752/2315 train_time:45151ms step_avg:60.04ms
step:753/2315 train_time:45214ms step_avg:60.04ms
step:754/2315 train_time:45276ms step_avg:60.05ms
step:755/2315 train_time:45335ms step_avg:60.05ms
step:756/2315 train_time:45395ms step_avg:60.05ms
step:757/2315 train_time:45454ms step_avg:60.05ms
step:758/2315 train_time:45514ms step_avg:60.04ms
step:759/2315 train_time:45573ms step_avg:60.04ms
step:760/2315 train_time:45633ms step_avg:60.04ms
step:761/2315 train_time:45693ms step_avg:60.04ms
step:762/2315 train_time:45754ms step_avg:60.04ms
step:763/2315 train_time:45813ms step_avg:60.04ms
step:764/2315 train_time:45875ms step_avg:60.05ms
step:765/2315 train_time:45935ms step_avg:60.05ms
step:766/2315 train_time:45996ms step_avg:60.05ms
step:767/2315 train_time:46058ms step_avg:60.05ms
step:768/2315 train_time:46121ms step_avg:60.05ms
step:769/2315 train_time:46183ms step_avg:60.06ms
step:770/2315 train_time:46245ms step_avg:60.06ms
step:771/2315 train_time:46306ms step_avg:60.06ms
step:772/2315 train_time:46367ms step_avg:60.06ms
step:773/2315 train_time:46427ms step_avg:60.06ms
step:774/2315 train_time:46488ms step_avg:60.06ms
step:775/2315 train_time:46548ms step_avg:60.06ms
step:776/2315 train_time:46609ms step_avg:60.06ms
step:777/2315 train_time:46670ms step_avg:60.06ms
step:778/2315 train_time:46730ms step_avg:60.06ms
step:779/2315 train_time:46790ms step_avg:60.06ms
step:780/2315 train_time:46851ms step_avg:60.07ms
step:781/2315 train_time:46911ms step_avg:60.07ms
step:782/2315 train_time:46973ms step_avg:60.07ms
step:783/2315 train_time:47034ms step_avg:60.07ms
step:784/2315 train_time:47095ms step_avg:60.07ms
step:785/2315 train_time:47156ms step_avg:60.07ms
step:786/2315 train_time:47217ms step_avg:60.07ms
step:787/2315 train_time:47278ms step_avg:60.07ms
step:788/2315 train_time:47339ms step_avg:60.07ms
step:789/2315 train_time:47399ms step_avg:60.08ms
step:790/2315 train_time:47460ms step_avg:60.08ms
step:791/2315 train_time:47521ms step_avg:60.08ms
step:792/2315 train_time:47582ms step_avg:60.08ms
step:793/2315 train_time:47642ms step_avg:60.08ms
step:794/2315 train_time:47703ms step_avg:60.08ms
step:795/2315 train_time:47763ms step_avg:60.08ms
step:796/2315 train_time:47824ms step_avg:60.08ms
step:797/2315 train_time:47885ms step_avg:60.08ms
step:798/2315 train_time:47945ms step_avg:60.08ms
step:799/2315 train_time:48006ms step_avg:60.08ms
step:800/2315 train_time:48068ms step_avg:60.08ms
step:801/2315 train_time:48128ms step_avg:60.09ms
step:802/2315 train_time:48191ms step_avg:60.09ms
step:803/2315 train_time:48251ms step_avg:60.09ms
step:804/2315 train_time:48313ms step_avg:60.09ms
step:805/2315 train_time:48374ms step_avg:60.09ms
step:806/2315 train_time:48435ms step_avg:60.09ms
step:807/2315 train_time:48495ms step_avg:60.09ms
step:808/2315 train_time:48556ms step_avg:60.09ms
step:809/2315 train_time:48616ms step_avg:60.09ms
step:810/2315 train_time:48677ms step_avg:60.10ms
step:811/2315 train_time:48738ms step_avg:60.10ms
step:812/2315 train_time:48799ms step_avg:60.10ms
step:813/2315 train_time:48859ms step_avg:60.10ms
step:814/2315 train_time:48920ms step_avg:60.10ms
step:815/2315 train_time:48982ms step_avg:60.10ms
step:816/2315 train_time:49043ms step_avg:60.10ms
step:817/2315 train_time:49104ms step_avg:60.10ms
step:818/2315 train_time:49164ms step_avg:60.10ms
step:819/2315 train_time:49225ms step_avg:60.10ms
step:820/2315 train_time:49286ms step_avg:60.11ms
step:821/2315 train_time:49348ms step_avg:60.11ms
step:822/2315 train_time:49410ms step_avg:60.11ms
step:823/2315 train_time:49471ms step_avg:60.11ms
step:824/2315 train_time:49533ms step_avg:60.11ms
step:825/2315 train_time:49594ms step_avg:60.11ms
step:826/2315 train_time:49654ms step_avg:60.11ms
step:827/2315 train_time:49715ms step_avg:60.11ms
step:828/2315 train_time:49776ms step_avg:60.12ms
step:829/2315 train_time:49837ms step_avg:60.12ms
step:830/2315 train_time:49898ms step_avg:60.12ms
step:831/2315 train_time:49958ms step_avg:60.12ms
step:832/2315 train_time:50020ms step_avg:60.12ms
step:833/2315 train_time:50081ms step_avg:60.12ms
step:834/2315 train_time:50143ms step_avg:60.12ms
step:835/2315 train_time:50204ms step_avg:60.12ms
step:836/2315 train_time:50265ms step_avg:60.13ms
step:837/2315 train_time:50325ms step_avg:60.13ms
step:838/2315 train_time:50387ms step_avg:60.13ms
step:839/2315 train_time:50447ms step_avg:60.13ms
step:840/2315 train_time:50509ms step_avg:60.13ms
step:841/2315 train_time:50570ms step_avg:60.13ms
step:842/2315 train_time:50632ms step_avg:60.13ms
step:843/2315 train_time:50692ms step_avg:60.13ms
step:844/2315 train_time:50753ms step_avg:60.13ms
step:845/2315 train_time:50814ms step_avg:60.13ms
step:846/2315 train_time:50875ms step_avg:60.14ms
step:847/2315 train_time:50936ms step_avg:60.14ms
step:848/2315 train_time:50997ms step_avg:60.14ms
step:849/2315 train_time:51057ms step_avg:60.14ms
step:850/2315 train_time:51118ms step_avg:60.14ms
step:851/2315 train_time:51179ms step_avg:60.14ms
step:852/2315 train_time:51241ms step_avg:60.14ms
step:853/2315 train_time:51302ms step_avg:60.14ms
step:854/2315 train_time:51364ms step_avg:60.14ms
step:855/2315 train_time:51424ms step_avg:60.15ms
step:856/2315 train_time:51485ms step_avg:60.15ms
step:857/2315 train_time:51546ms step_avg:60.15ms
step:858/2315 train_time:51607ms step_avg:60.15ms
step:859/2315 train_time:51668ms step_avg:60.15ms
step:860/2315 train_time:51730ms step_avg:60.15ms
step:861/2315 train_time:51791ms step_avg:60.15ms
step:862/2315 train_time:51853ms step_avg:60.15ms
step:863/2315 train_time:51914ms step_avg:60.16ms
step:864/2315 train_time:51975ms step_avg:60.16ms
step:865/2315 train_time:52035ms step_avg:60.16ms
step:866/2315 train_time:52096ms step_avg:60.16ms
step:867/2315 train_time:52156ms step_avg:60.16ms
step:868/2315 train_time:52217ms step_avg:60.16ms
step:869/2315 train_time:52278ms step_avg:60.16ms
step:870/2315 train_time:52340ms step_avg:60.16ms
step:871/2315 train_time:52400ms step_avg:60.16ms
step:872/2315 train_time:52462ms step_avg:60.16ms
step:873/2315 train_time:52523ms step_avg:60.16ms
step:874/2315 train_time:52584ms step_avg:60.17ms
step:875/2315 train_time:52645ms step_avg:60.17ms
step:876/2315 train_time:52706ms step_avg:60.17ms
step:877/2315 train_time:52766ms step_avg:60.17ms
step:878/2315 train_time:52827ms step_avg:60.17ms
step:879/2315 train_time:52887ms step_avg:60.17ms
step:880/2315 train_time:52950ms step_avg:60.17ms
step:881/2315 train_time:53011ms step_avg:60.17ms
step:882/2315 train_time:53072ms step_avg:60.17ms
step:883/2315 train_time:53133ms step_avg:60.17ms
step:884/2315 train_time:53194ms step_avg:60.17ms
step:885/2315 train_time:53254ms step_avg:60.17ms
step:886/2315 train_time:53315ms step_avg:60.18ms
step:887/2315 train_time:53377ms step_avg:60.18ms
step:888/2315 train_time:53438ms step_avg:60.18ms
step:889/2315 train_time:53499ms step_avg:60.18ms
step:890/2315 train_time:53560ms step_avg:60.18ms
step:891/2315 train_time:53621ms step_avg:60.18ms
step:892/2315 train_time:53682ms step_avg:60.18ms
step:893/2315 train_time:53743ms step_avg:60.18ms
step:894/2315 train_time:53805ms step_avg:60.18ms
step:895/2315 train_time:53864ms step_avg:60.18ms
step:896/2315 train_time:53925ms step_avg:60.18ms
step:897/2315 train_time:53986ms step_avg:60.19ms
step:898/2315 train_time:54048ms step_avg:60.19ms
step:899/2315 train_time:54109ms step_avg:60.19ms
step:900/2315 train_time:54170ms step_avg:60.19ms
step:901/2315 train_time:54231ms step_avg:60.19ms
step:902/2315 train_time:54292ms step_avg:60.19ms
step:903/2315 train_time:54354ms step_avg:60.19ms
step:904/2315 train_time:54415ms step_avg:60.19ms
step:905/2315 train_time:54476ms step_avg:60.19ms
step:906/2315 train_time:54537ms step_avg:60.20ms
step:907/2315 train_time:54597ms step_avg:60.20ms
step:908/2315 train_time:54658ms step_avg:60.20ms
step:909/2315 train_time:54718ms step_avg:60.20ms
step:910/2315 train_time:54779ms step_avg:60.20ms
step:911/2315 train_time:54840ms step_avg:60.20ms
step:912/2315 train_time:54902ms step_avg:60.20ms
step:913/2315 train_time:54963ms step_avg:60.20ms
step:914/2315 train_time:55024ms step_avg:60.20ms
step:915/2315 train_time:55085ms step_avg:60.20ms
step:916/2315 train_time:55145ms step_avg:60.20ms
step:917/2315 train_time:55205ms step_avg:60.20ms
step:918/2315 train_time:55266ms step_avg:60.20ms
step:919/2315 train_time:55326ms step_avg:60.20ms
step:920/2315 train_time:55388ms step_avg:60.20ms
step:921/2315 train_time:55449ms step_avg:60.21ms
step:922/2315 train_time:55511ms step_avg:60.21ms
step:923/2315 train_time:55571ms step_avg:60.21ms
step:924/2315 train_time:55633ms step_avg:60.21ms
step:925/2315 train_time:55693ms step_avg:60.21ms
step:926/2315 train_time:55754ms step_avg:60.21ms
step:927/2315 train_time:55816ms step_avg:60.21ms
step:928/2315 train_time:55877ms step_avg:60.21ms
step:929/2315 train_time:55938ms step_avg:60.21ms
step:930/2315 train_time:55999ms step_avg:60.21ms
step:931/2315 train_time:56059ms step_avg:60.21ms
step:932/2315 train_time:56120ms step_avg:60.21ms
step:933/2315 train_time:56181ms step_avg:60.22ms
step:934/2315 train_time:56243ms step_avg:60.22ms
step:935/2315 train_time:56304ms step_avg:60.22ms
step:936/2315 train_time:56365ms step_avg:60.22ms
step:937/2315 train_time:56425ms step_avg:60.22ms
step:938/2315 train_time:56485ms step_avg:60.22ms
step:939/2315 train_time:56546ms step_avg:60.22ms
step:940/2315 train_time:56607ms step_avg:60.22ms
step:941/2315 train_time:56668ms step_avg:60.22ms
step:942/2315 train_time:56729ms step_avg:60.22ms
step:943/2315 train_time:56791ms step_avg:60.22ms
step:944/2315 train_time:56852ms step_avg:60.22ms
step:945/2315 train_time:56914ms step_avg:60.23ms
step:946/2315 train_time:56975ms step_avg:60.23ms
step:947/2315 train_time:57035ms step_avg:60.23ms
step:948/2315 train_time:57096ms step_avg:60.23ms
step:949/2315 train_time:57156ms step_avg:60.23ms
step:950/2315 train_time:57217ms step_avg:60.23ms
step:951/2315 train_time:57278ms step_avg:60.23ms
step:952/2315 train_time:57339ms step_avg:60.23ms
step:953/2315 train_time:57400ms step_avg:60.23ms
step:954/2315 train_time:57462ms step_avg:60.23ms
step:955/2315 train_time:57522ms step_avg:60.23ms
step:956/2315 train_time:57584ms step_avg:60.23ms
step:957/2315 train_time:57644ms step_avg:60.23ms
step:958/2315 train_time:57705ms step_avg:60.23ms
step:959/2315 train_time:57765ms step_avg:60.23ms
step:960/2315 train_time:57827ms step_avg:60.24ms
step:961/2315 train_time:57888ms step_avg:60.24ms
step:962/2315 train_time:57951ms step_avg:60.24ms
step:963/2315 train_time:58012ms step_avg:60.24ms
step:964/2315 train_time:58074ms step_avg:60.24ms
step:965/2315 train_time:58134ms step_avg:60.24ms
step:966/2315 train_time:58195ms step_avg:60.24ms
step:967/2315 train_time:58255ms step_avg:60.24ms
step:968/2315 train_time:58316ms step_avg:60.24ms
step:969/2315 train_time:58376ms step_avg:60.24ms
step:970/2315 train_time:58437ms step_avg:60.24ms
step:971/2315 train_time:58498ms step_avg:60.24ms
step:972/2315 train_time:58559ms step_avg:60.25ms
step:973/2315 train_time:58619ms step_avg:60.25ms
step:974/2315 train_time:58680ms step_avg:60.25ms
step:975/2315 train_time:58741ms step_avg:60.25ms
step:976/2315 train_time:58802ms step_avg:60.25ms
step:977/2315 train_time:58863ms step_avg:60.25ms
step:978/2315 train_time:58925ms step_avg:60.25ms
step:979/2315 train_time:58985ms step_avg:60.25ms
step:980/2315 train_time:59046ms step_avg:60.25ms
step:981/2315 train_time:59107ms step_avg:60.25ms
step:982/2315 train_time:59169ms step_avg:60.25ms
step:983/2315 train_time:59229ms step_avg:60.25ms
step:984/2315 train_time:59291ms step_avg:60.26ms
step:985/2315 train_time:59352ms step_avg:60.26ms
step:986/2315 train_time:59413ms step_avg:60.26ms
step:987/2315 train_time:59474ms step_avg:60.26ms
step:988/2315 train_time:59535ms step_avg:60.26ms
step:989/2315 train_time:59596ms step_avg:60.26ms
step:990/2315 train_time:59656ms step_avg:60.26ms
step:991/2315 train_time:59717ms step_avg:60.26ms
step:992/2315 train_time:59778ms step_avg:60.26ms
step:993/2315 train_time:59839ms step_avg:60.26ms
step:994/2315 train_time:59901ms step_avg:60.26ms
step:995/2315 train_time:59961ms step_avg:60.26ms
step:996/2315 train_time:60022ms step_avg:60.26ms
step:997/2315 train_time:60083ms step_avg:60.26ms
step:998/2315 train_time:60144ms step_avg:60.26ms
step:999/2315 train_time:60205ms step_avg:60.27ms
step:1000/2315 train_time:60267ms step_avg:60.27ms
step:1000/2315 val_loss:3.5785 train_time:60328ms step_avg:60.33ms
step:1001/2315 train_time:60354ms step_avg:60.29ms
step:1002/2315 train_time:60390ms step_avg:60.27ms
step:1003/2315 train_time:60456ms step_avg:60.28ms
step:1004/2315 train_time:60521ms step_avg:60.28ms
step:1005/2315 train_time:60582ms step_avg:60.28ms
step:1006/2315 train_time:60643ms step_avg:60.28ms
step:1007/2315 train_time:60702ms step_avg:60.28ms
step:1008/2315 train_time:60763ms step_avg:60.28ms
step:1009/2315 train_time:60823ms step_avg:60.28ms
step:1010/2315 train_time:60884ms step_avg:60.28ms
step:1011/2315 train_time:60943ms step_avg:60.28ms
step:1012/2315 train_time:61003ms step_avg:60.28ms
step:1013/2315 train_time:61062ms step_avg:60.28ms
step:1014/2315 train_time:61123ms step_avg:60.28ms
step:1015/2315 train_time:61183ms step_avg:60.28ms
step:1016/2315 train_time:61245ms step_avg:60.28ms
step:1017/2315 train_time:61308ms step_avg:60.28ms
step:1018/2315 train_time:61370ms step_avg:60.29ms
step:1019/2315 train_time:61433ms step_avg:60.29ms
step:1020/2315 train_time:61495ms step_avg:60.29ms
step:1021/2315 train_time:61556ms step_avg:60.29ms
step:1022/2315 train_time:61618ms step_avg:60.29ms
step:1023/2315 train_time:61678ms step_avg:60.29ms
step:1024/2315 train_time:61739ms step_avg:60.29ms
step:1025/2315 train_time:61799ms step_avg:60.29ms
step:1026/2315 train_time:61860ms step_avg:60.29ms
step:1027/2315 train_time:61920ms step_avg:60.29ms
step:1028/2315 train_time:61980ms step_avg:60.29ms
step:1029/2315 train_time:62040ms step_avg:60.29ms
step:1030/2315 train_time:62100ms step_avg:60.29ms
step:1031/2315 train_time:62160ms step_avg:60.29ms
step:1032/2315 train_time:62221ms step_avg:60.29ms
step:1033/2315 train_time:62282ms step_avg:60.29ms
step:1034/2315 train_time:62345ms step_avg:60.30ms
step:1035/2315 train_time:62408ms step_avg:60.30ms
step:1036/2315 train_time:62469ms step_avg:60.30ms
step:1037/2315 train_time:62530ms step_avg:60.30ms
step:1038/2315 train_time:62591ms step_avg:60.30ms
step:1039/2315 train_time:62651ms step_avg:60.30ms
step:1040/2315 train_time:62712ms step_avg:60.30ms
step:1041/2315 train_time:62773ms step_avg:60.30ms
step:1042/2315 train_time:62834ms step_avg:60.30ms
step:1043/2315 train_time:62895ms step_avg:60.30ms
step:1044/2315 train_time:62956ms step_avg:60.30ms
step:1045/2315 train_time:63016ms step_avg:60.30ms
step:1046/2315 train_time:63077ms step_avg:60.30ms
step:1047/2315 train_time:63137ms step_avg:60.30ms
step:1048/2315 train_time:63198ms step_avg:60.30ms
step:1049/2315 train_time:63258ms step_avg:60.30ms
step:1050/2315 train_time:63319ms step_avg:60.30ms
step:1051/2315 train_time:63381ms step_avg:60.31ms
step:1052/2315 train_time:63444ms step_avg:60.31ms
step:1053/2315 train_time:63504ms step_avg:60.31ms
step:1054/2315 train_time:63566ms step_avg:60.31ms
step:1055/2315 train_time:63626ms step_avg:60.31ms
step:1056/2315 train_time:63688ms step_avg:60.31ms
step:1057/2315 train_time:63749ms step_avg:60.31ms
step:1058/2315 train_time:63810ms step_avg:60.31ms
step:1059/2315 train_time:63871ms step_avg:60.31ms
step:1060/2315 train_time:63932ms step_avg:60.31ms
step:1061/2315 train_time:63992ms step_avg:60.31ms
step:1062/2315 train_time:64054ms step_avg:60.31ms
step:1063/2315 train_time:64113ms step_avg:60.31ms
step:1064/2315 train_time:64174ms step_avg:60.31ms
step:1065/2315 train_time:64235ms step_avg:60.31ms
step:1066/2315 train_time:64297ms step_avg:60.32ms
step:1067/2315 train_time:64358ms step_avg:60.32ms
step:1068/2315 train_time:64419ms step_avg:60.32ms
step:1069/2315 train_time:64480ms step_avg:60.32ms
step:1070/2315 train_time:64541ms step_avg:60.32ms
step:1071/2315 train_time:64601ms step_avg:60.32ms
step:1072/2315 train_time:64662ms step_avg:60.32ms
step:1073/2315 train_time:64723ms step_avg:60.32ms
step:1074/2315 train_time:64784ms step_avg:60.32ms
step:1075/2315 train_time:64845ms step_avg:60.32ms
step:1076/2315 train_time:64907ms step_avg:60.32ms
step:1077/2315 train_time:64967ms step_avg:60.32ms
step:1078/2315 train_time:65028ms step_avg:60.32ms
step:1079/2315 train_time:65088ms step_avg:60.32ms
step:1080/2315 train_time:65149ms step_avg:60.32ms
step:1081/2315 train_time:65210ms step_avg:60.32ms
step:1082/2315 train_time:65271ms step_avg:60.32ms
step:1083/2315 train_time:65332ms step_avg:60.33ms
step:1084/2315 train_time:65395ms step_avg:60.33ms
step:1085/2315 train_time:65456ms step_avg:60.33ms
step:1086/2315 train_time:65518ms step_avg:60.33ms
step:1087/2315 train_time:65578ms step_avg:60.33ms
step:1088/2315 train_time:65638ms step_avg:60.33ms
step:1089/2315 train_time:65699ms step_avg:60.33ms
step:1090/2315 train_time:65760ms step_avg:60.33ms
step:1091/2315 train_time:65821ms step_avg:60.33ms
step:1092/2315 train_time:65883ms step_avg:60.33ms
step:1093/2315 train_time:65944ms step_avg:60.33ms
step:1094/2315 train_time:66006ms step_avg:60.33ms
step:1095/2315 train_time:66067ms step_avg:60.33ms
step:1096/2315 train_time:66127ms step_avg:60.34ms
step:1097/2315 train_time:66188ms step_avg:60.34ms
step:1098/2315 train_time:66249ms step_avg:60.34ms
step:1099/2315 train_time:66310ms step_avg:60.34ms
step:1100/2315 train_time:66371ms step_avg:60.34ms
step:1101/2315 train_time:66432ms step_avg:60.34ms
step:1102/2315 train_time:66493ms step_avg:60.34ms
step:1103/2315 train_time:66554ms step_avg:60.34ms
step:1104/2315 train_time:66616ms step_avg:60.34ms
step:1105/2315 train_time:66677ms step_avg:60.34ms
step:1106/2315 train_time:66738ms step_avg:60.34ms
step:1107/2315 train_time:66799ms step_avg:60.34ms
step:1108/2315 train_time:66859ms step_avg:60.34ms
step:1109/2315 train_time:66920ms step_avg:60.34ms
step:1110/2315 train_time:66981ms step_avg:60.34ms
step:1111/2315 train_time:67042ms step_avg:60.34ms
step:1112/2315 train_time:67103ms step_avg:60.34ms
step:1113/2315 train_time:67164ms step_avg:60.35ms
step:1114/2315 train_time:67226ms step_avg:60.35ms
step:1115/2315 train_time:67287ms step_avg:60.35ms
step:1116/2315 train_time:67348ms step_avg:60.35ms
step:1117/2315 train_time:67409ms step_avg:60.35ms
step:1118/2315 train_time:67470ms step_avg:60.35ms
step:1119/2315 train_time:67531ms step_avg:60.35ms
step:1120/2315 train_time:67592ms step_avg:60.35ms
step:1121/2315 train_time:67653ms step_avg:60.35ms
step:1122/2315 train_time:67714ms step_avg:60.35ms
step:1123/2315 train_time:67775ms step_avg:60.35ms
step:1124/2315 train_time:67836ms step_avg:60.35ms
step:1125/2315 train_time:67897ms step_avg:60.35ms
step:1126/2315 train_time:67958ms step_avg:60.35ms
step:1127/2315 train_time:68019ms step_avg:60.35ms
step:1128/2315 train_time:68080ms step_avg:60.35ms
step:1129/2315 train_time:68140ms step_avg:60.35ms
step:1130/2315 train_time:68202ms step_avg:60.36ms
step:1131/2315 train_time:68263ms step_avg:60.36ms
step:1132/2315 train_time:68324ms step_avg:60.36ms
step:1133/2315 train_time:68385ms step_avg:60.36ms
step:1134/2315 train_time:68446ms step_avg:60.36ms
step:1135/2315 train_time:68507ms step_avg:60.36ms
step:1136/2315 train_time:68568ms step_avg:60.36ms
step:1137/2315 train_time:68628ms step_avg:60.36ms
step:1138/2315 train_time:68690ms step_avg:60.36ms
step:1139/2315 train_time:68750ms step_avg:60.36ms
step:1140/2315 train_time:68812ms step_avg:60.36ms
step:1141/2315 train_time:68873ms step_avg:60.36ms
step:1142/2315 train_time:68934ms step_avg:60.36ms
step:1143/2315 train_time:68996ms step_avg:60.36ms
step:1144/2315 train_time:69057ms step_avg:60.36ms
step:1145/2315 train_time:69118ms step_avg:60.36ms
step:1146/2315 train_time:69179ms step_avg:60.37ms
step:1147/2315 train_time:69239ms step_avg:60.37ms
step:1148/2315 train_time:69301ms step_avg:60.37ms
step:1149/2315 train_time:69361ms step_avg:60.37ms
step:1150/2315 train_time:69423ms step_avg:60.37ms
step:1151/2315 train_time:69484ms step_avg:60.37ms
step:1152/2315 train_time:69545ms step_avg:60.37ms
step:1153/2315 train_time:69606ms step_avg:60.37ms
step:1154/2315 train_time:69667ms step_avg:60.37ms
step:1155/2315 train_time:69728ms step_avg:60.37ms
step:1156/2315 train_time:69789ms step_avg:60.37ms
step:1157/2315 train_time:69850ms step_avg:60.37ms
step:1158/2315 train_time:69911ms step_avg:60.37ms
step:1159/2315 train_time:69971ms step_avg:60.37ms
step:1160/2315 train_time:70033ms step_avg:60.37ms
step:1161/2315 train_time:70094ms step_avg:60.37ms
step:1162/2315 train_time:70155ms step_avg:60.37ms
step:1163/2315 train_time:70216ms step_avg:60.37ms
step:1164/2315 train_time:70277ms step_avg:60.38ms
step:1165/2315 train_time:70338ms step_avg:60.38ms
step:1166/2315 train_time:70399ms step_avg:60.38ms
step:1167/2315 train_time:70459ms step_avg:60.38ms
step:1168/2315 train_time:70520ms step_avg:60.38ms
step:1169/2315 train_time:70581ms step_avg:60.38ms
step:1170/2315 train_time:70643ms step_avg:60.38ms
step:1171/2315 train_time:70704ms step_avg:60.38ms
step:1172/2315 train_time:70765ms step_avg:60.38ms
step:1173/2315 train_time:70826ms step_avg:60.38ms
step:1174/2315 train_time:70887ms step_avg:60.38ms
step:1175/2315 train_time:70949ms step_avg:60.38ms
step:1176/2315 train_time:71010ms step_avg:60.38ms
step:1177/2315 train_time:71071ms step_avg:60.38ms
step:1178/2315 train_time:71132ms step_avg:60.38ms
step:1179/2315 train_time:71192ms step_avg:60.38ms
step:1180/2315 train_time:71254ms step_avg:60.38ms
step:1181/2315 train_time:71315ms step_avg:60.39ms
step:1182/2315 train_time:71376ms step_avg:60.39ms
step:1183/2315 train_time:71437ms step_avg:60.39ms
step:1184/2315 train_time:71498ms step_avg:60.39ms
step:1185/2315 train_time:71559ms step_avg:60.39ms
step:1186/2315 train_time:71620ms step_avg:60.39ms
step:1187/2315 train_time:71680ms step_avg:60.39ms
step:1188/2315 train_time:71741ms step_avg:60.39ms
step:1189/2315 train_time:71802ms step_avg:60.39ms
step:1190/2315 train_time:71864ms step_avg:60.39ms
step:1191/2315 train_time:71926ms step_avg:60.39ms
step:1192/2315 train_time:71988ms step_avg:60.39ms
step:1193/2315 train_time:72048ms step_avg:60.39ms
step:1194/2315 train_time:72109ms step_avg:60.39ms
step:1195/2315 train_time:72170ms step_avg:60.39ms
step:1196/2315 train_time:72230ms step_avg:60.39ms
step:1197/2315 train_time:72291ms step_avg:60.39ms
step:1198/2315 train_time:72352ms step_avg:60.39ms
step:1199/2315 train_time:72413ms step_avg:60.39ms
step:1200/2315 train_time:72475ms step_avg:60.40ms
step:1201/2315 train_time:72536ms step_avg:60.40ms
step:1202/2315 train_time:72597ms step_avg:60.40ms
step:1203/2315 train_time:72658ms step_avg:60.40ms
step:1204/2315 train_time:72719ms step_avg:60.40ms
step:1205/2315 train_time:72780ms step_avg:60.40ms
step:1206/2315 train_time:72842ms step_avg:60.40ms
step:1207/2315 train_time:72902ms step_avg:60.40ms
step:1208/2315 train_time:72964ms step_avg:60.40ms
step:1209/2315 train_time:73025ms step_avg:60.40ms
step:1210/2315 train_time:73086ms step_avg:60.40ms
step:1211/2315 train_time:73147ms step_avg:60.40ms
step:1212/2315 train_time:73209ms step_avg:60.40ms
step:1213/2315 train_time:73269ms step_avg:60.40ms
step:1214/2315 train_time:73330ms step_avg:60.40ms
step:1215/2315 train_time:73390ms step_avg:60.40ms
step:1216/2315 train_time:73451ms step_avg:60.40ms
step:1217/2315 train_time:73512ms step_avg:60.40ms
step:1218/2315 train_time:73573ms step_avg:60.40ms
step:1219/2315 train_time:73633ms step_avg:60.40ms
step:1220/2315 train_time:73695ms step_avg:60.41ms
step:1221/2315 train_time:73757ms step_avg:60.41ms
step:1222/2315 train_time:73819ms step_avg:60.41ms
step:1223/2315 train_time:73880ms step_avg:60.41ms
step:1224/2315 train_time:73941ms step_avg:60.41ms
step:1225/2315 train_time:74001ms step_avg:60.41ms
step:1226/2315 train_time:74063ms step_avg:60.41ms
step:1227/2315 train_time:74124ms step_avg:60.41ms
step:1228/2315 train_time:74187ms step_avg:60.41ms
step:1229/2315 train_time:74247ms step_avg:60.41ms
step:1230/2315 train_time:74308ms step_avg:60.41ms
step:1231/2315 train_time:74369ms step_avg:60.41ms
step:1232/2315 train_time:74430ms step_avg:60.41ms
step:1233/2315 train_time:74490ms step_avg:60.41ms
step:1234/2315 train_time:74551ms step_avg:60.41ms
step:1235/2315 train_time:74612ms step_avg:60.41ms
step:1236/2315 train_time:74673ms step_avg:60.42ms
step:1237/2315 train_time:74735ms step_avg:60.42ms
step:1238/2315 train_time:74797ms step_avg:60.42ms
step:1239/2315 train_time:74858ms step_avg:60.42ms
step:1240/2315 train_time:74919ms step_avg:60.42ms
step:1241/2315 train_time:74980ms step_avg:60.42ms
step:1242/2315 train_time:75041ms step_avg:60.42ms
step:1243/2315 train_time:75102ms step_avg:60.42ms
step:1244/2315 train_time:75163ms step_avg:60.42ms
step:1245/2315 train_time:75224ms step_avg:60.42ms
step:1246/2315 train_time:75286ms step_avg:60.42ms
step:1247/2315 train_time:75347ms step_avg:60.42ms
step:1248/2315 train_time:75408ms step_avg:60.42ms
step:1249/2315 train_time:75468ms step_avg:60.42ms
step:1250/2315 train_time:75529ms step_avg:60.42ms
step:1250/2315 val_loss:3.5140 train_time:75591ms step_avg:60.47ms
step:1251/2315 train_time:75611ms step_avg:60.44ms
step:1252/2315 train_time:75652ms step_avg:60.43ms
step:1253/2315 train_time:75716ms step_avg:60.43ms
step:1254/2315 train_time:75779ms step_avg:60.43ms
step:1255/2315 train_time:75840ms step_avg:60.43ms
step:1256/2315 train_time:75900ms step_avg:60.43ms
step:1257/2315 train_time:75960ms step_avg:60.43ms
step:1258/2315 train_time:76020ms step_avg:60.43ms
step:1259/2315 train_time:76080ms step_avg:60.43ms
step:1260/2315 train_time:76140ms step_avg:60.43ms
step:1261/2315 train_time:76200ms step_avg:60.43ms
step:1262/2315 train_time:76260ms step_avg:60.43ms
step:1263/2315 train_time:76320ms step_avg:60.43ms
step:1264/2315 train_time:76382ms step_avg:60.43ms
step:1265/2315 train_time:76441ms step_avg:60.43ms
step:1266/2315 train_time:76502ms step_avg:60.43ms
step:1267/2315 train_time:76564ms step_avg:60.43ms
step:1268/2315 train_time:76627ms step_avg:60.43ms
step:1269/2315 train_time:76689ms step_avg:60.43ms
step:1270/2315 train_time:76751ms step_avg:60.43ms
step:1271/2315 train_time:76813ms step_avg:60.43ms
step:1272/2315 train_time:76874ms step_avg:60.44ms
step:1273/2315 train_time:76935ms step_avg:60.44ms
step:1274/2315 train_time:76996ms step_avg:60.44ms
step:1275/2315 train_time:77057ms step_avg:60.44ms
step:1276/2315 train_time:77118ms step_avg:60.44ms
step:1277/2315 train_time:77178ms step_avg:60.44ms
step:1278/2315 train_time:77238ms step_avg:60.44ms
step:1279/2315 train_time:77298ms step_avg:60.44ms
step:1280/2315 train_time:77358ms step_avg:60.44ms
step:1281/2315 train_time:77418ms step_avg:60.44ms
step:1282/2315 train_time:77479ms step_avg:60.44ms
step:1283/2315 train_time:77539ms step_avg:60.44ms
step:1284/2315 train_time:77602ms step_avg:60.44ms
step:1285/2315 train_time:77665ms step_avg:60.44ms
step:1286/2315 train_time:77727ms step_avg:60.44ms
step:1287/2315 train_time:77788ms step_avg:60.44ms
step:1288/2315 train_time:77849ms step_avg:60.44ms
step:1289/2315 train_time:77910ms step_avg:60.44ms
step:1290/2315 train_time:77971ms step_avg:60.44ms
step:1291/2315 train_time:78032ms step_avg:60.44ms
step:1292/2315 train_time:78093ms step_avg:60.44ms
step:1293/2315 train_time:78153ms step_avg:60.44ms
step:1294/2315 train_time:78214ms step_avg:60.44ms
step:1295/2315 train_time:78275ms step_avg:60.44ms
step:1296/2315 train_time:78335ms step_avg:60.44ms
step:1297/2315 train_time:78395ms step_avg:60.44ms
step:1298/2315 train_time:78457ms step_avg:60.44ms
step:1299/2315 train_time:78518ms step_avg:60.44ms
step:1300/2315 train_time:78579ms step_avg:60.45ms
step:1301/2315 train_time:78641ms step_avg:60.45ms
step:1302/2315 train_time:78702ms step_avg:60.45ms
step:1303/2315 train_time:78763ms step_avg:60.45ms
step:1304/2315 train_time:78824ms step_avg:60.45ms
step:1305/2315 train_time:78885ms step_avg:60.45ms
step:1306/2315 train_time:78947ms step_avg:60.45ms
step:1307/2315 train_time:79007ms step_avg:60.45ms
step:1308/2315 train_time:79068ms step_avg:60.45ms
step:1309/2315 train_time:79129ms step_avg:60.45ms
step:1310/2315 train_time:79190ms step_avg:60.45ms
step:1311/2315 train_time:79250ms step_avg:60.45ms
step:1312/2315 train_time:79310ms step_avg:60.45ms
step:1313/2315 train_time:79370ms step_avg:60.45ms
step:1314/2315 train_time:79431ms step_avg:60.45ms
step:1315/2315 train_time:79492ms step_avg:60.45ms
step:1316/2315 train_time:79554ms step_avg:60.45ms
step:1317/2315 train_time:79615ms step_avg:60.45ms
step:1318/2315 train_time:79677ms step_avg:60.45ms
step:1319/2315 train_time:79739ms step_avg:60.45ms
step:1320/2315 train_time:79800ms step_avg:60.45ms
step:1321/2315 train_time:79860ms step_avg:60.45ms
step:1322/2315 train_time:79921ms step_avg:60.45ms
step:1323/2315 train_time:79982ms step_avg:60.45ms
step:1324/2315 train_time:80043ms step_avg:60.46ms
step:1325/2315 train_time:80104ms step_avg:60.46ms
step:1326/2315 train_time:80166ms step_avg:60.46ms
step:1327/2315 train_time:80226ms step_avg:60.46ms
step:1328/2315 train_time:80288ms step_avg:60.46ms
step:1329/2315 train_time:80349ms step_avg:60.46ms
step:1330/2315 train_time:80410ms step_avg:60.46ms
step:1331/2315 train_time:80470ms step_avg:60.46ms
step:1332/2315 train_time:80531ms step_avg:60.46ms
step:1333/2315 train_time:80592ms step_avg:60.46ms
step:1334/2315 train_time:80654ms step_avg:60.46ms
step:1335/2315 train_time:80716ms step_avg:60.46ms
step:1336/2315 train_time:80777ms step_avg:60.46ms
step:1337/2315 train_time:80838ms step_avg:60.46ms
step:1338/2315 train_time:80899ms step_avg:60.46ms
step:1339/2315 train_time:80959ms step_avg:60.46ms
step:1340/2315 train_time:81020ms step_avg:60.46ms
step:1341/2315 train_time:81080ms step_avg:60.46ms
step:1342/2315 train_time:81142ms step_avg:60.46ms
step:1343/2315 train_time:81203ms step_avg:60.46ms
step:1344/2315 train_time:81264ms step_avg:60.46ms
step:1345/2315 train_time:81325ms step_avg:60.46ms
step:1346/2315 train_time:81387ms step_avg:60.47ms
step:1347/2315 train_time:81448ms step_avg:60.47ms
step:1348/2315 train_time:81508ms step_avg:60.47ms
step:1349/2315 train_time:81569ms step_avg:60.47ms
step:1350/2315 train_time:81630ms step_avg:60.47ms
step:1351/2315 train_time:81691ms step_avg:60.47ms
step:1352/2315 train_time:81752ms step_avg:60.47ms
step:1353/2315 train_time:81814ms step_avg:60.47ms
step:1354/2315 train_time:81875ms step_avg:60.47ms
step:1355/2315 train_time:81936ms step_avg:60.47ms
step:1356/2315 train_time:81997ms step_avg:60.47ms
step:1357/2315 train_time:82058ms step_avg:60.47ms
step:1358/2315 train_time:82119ms step_avg:60.47ms
step:1359/2315 train_time:82180ms step_avg:60.47ms
step:1360/2315 train_time:82241ms step_avg:60.47ms
step:1361/2315 train_time:82301ms step_avg:60.47ms
step:1362/2315 train_time:82363ms step_avg:60.47ms
step:1363/2315 train_time:82424ms step_avg:60.47ms
step:1364/2315 train_time:82485ms step_avg:60.47ms
step:1365/2315 train_time:82546ms step_avg:60.47ms
step:1366/2315 train_time:82608ms step_avg:60.47ms
step:1367/2315 train_time:82669ms step_avg:60.47ms
step:1368/2315 train_time:82730ms step_avg:60.47ms
step:1369/2315 train_time:82790ms step_avg:60.47ms
step:1370/2315 train_time:82852ms step_avg:60.48ms
step:1371/2315 train_time:82913ms step_avg:60.48ms
step:1372/2315 train_time:82974ms step_avg:60.48ms
step:1373/2315 train_time:83035ms step_avg:60.48ms
step:1374/2315 train_time:83097ms step_avg:60.48ms
step:1375/2315 train_time:83157ms step_avg:60.48ms
step:1376/2315 train_time:83218ms step_avg:60.48ms
step:1377/2315 train_time:83279ms step_avg:60.48ms
step:1378/2315 train_time:83341ms step_avg:60.48ms
step:1379/2315 train_time:83402ms step_avg:60.48ms
step:1380/2315 train_time:83463ms step_avg:60.48ms
step:1381/2315 train_time:83523ms step_avg:60.48ms
step:1382/2315 train_time:83584ms step_avg:60.48ms
step:1383/2315 train_time:83645ms step_avg:60.48ms
step:1384/2315 train_time:83707ms step_avg:60.48ms
step:1385/2315 train_time:83768ms step_avg:60.48ms
step:1386/2315 train_time:83829ms step_avg:60.48ms
step:1387/2315 train_time:83890ms step_avg:60.48ms
step:1388/2315 train_time:83951ms step_avg:60.48ms
step:1389/2315 train_time:84012ms step_avg:60.48ms
step:1390/2315 train_time:84073ms step_avg:60.48ms
step:1391/2315 train_time:84134ms step_avg:60.48ms
step:1392/2315 train_time:84196ms step_avg:60.49ms
step:1393/2315 train_time:84257ms step_avg:60.49ms
step:1394/2315 train_time:84319ms step_avg:60.49ms
step:1395/2315 train_time:84380ms step_avg:60.49ms
step:1396/2315 train_time:84441ms step_avg:60.49ms
step:1397/2315 train_time:84501ms step_avg:60.49ms
step:1398/2315 train_time:84562ms step_avg:60.49ms
step:1399/2315 train_time:84622ms step_avg:60.49ms
step:1400/2315 train_time:84683ms step_avg:60.49ms
step:1401/2315 train_time:84744ms step_avg:60.49ms
step:1402/2315 train_time:84805ms step_avg:60.49ms
step:1403/2315 train_time:84866ms step_avg:60.49ms
step:1404/2315 train_time:84927ms step_avg:60.49ms
step:1405/2315 train_time:84988ms step_avg:60.49ms
step:1406/2315 train_time:85049ms step_avg:60.49ms
step:1407/2315 train_time:85109ms step_avg:60.49ms
step:1408/2315 train_time:85170ms step_avg:60.49ms
step:1409/2315 train_time:85232ms step_avg:60.49ms
step:1410/2315 train_time:85294ms step_avg:60.49ms
step:1411/2315 train_time:85355ms step_avg:60.49ms
step:1412/2315 train_time:85417ms step_avg:60.49ms
step:1413/2315 train_time:85478ms step_avg:60.49ms
step:1414/2315 train_time:85538ms step_avg:60.49ms
step:1415/2315 train_time:85599ms step_avg:60.49ms
step:1416/2315 train_time:85660ms step_avg:60.49ms
step:1417/2315 train_time:85722ms step_avg:60.50ms
step:1418/2315 train_time:85782ms step_avg:60.50ms
step:1419/2315 train_time:85843ms step_avg:60.50ms
step:1420/2315 train_time:85904ms step_avg:60.50ms
step:1421/2315 train_time:85966ms step_avg:60.50ms
step:1422/2315 train_time:86027ms step_avg:60.50ms
step:1423/2315 train_time:86088ms step_avg:60.50ms
step:1424/2315 train_time:86149ms step_avg:60.50ms
step:1425/2315 train_time:86209ms step_avg:60.50ms
step:1426/2315 train_time:86270ms step_avg:60.50ms
step:1427/2315 train_time:86331ms step_avg:60.50ms
step:1428/2315 train_time:86392ms step_avg:60.50ms
step:1429/2315 train_time:86453ms step_avg:60.50ms
step:1430/2315 train_time:86515ms step_avg:60.50ms
step:1431/2315 train_time:86576ms step_avg:60.50ms
step:1432/2315 train_time:86638ms step_avg:60.50ms
step:1433/2315 train_time:86698ms step_avg:60.50ms
step:1434/2315 train_time:86759ms step_avg:60.50ms
step:1435/2315 train_time:86820ms step_avg:60.50ms
step:1436/2315 train_time:86880ms step_avg:60.50ms
step:1437/2315 train_time:86941ms step_avg:60.50ms
step:1438/2315 train_time:87002ms step_avg:60.50ms
step:1439/2315 train_time:87064ms step_avg:60.50ms
step:1440/2315 train_time:87126ms step_avg:60.50ms
step:1441/2315 train_time:87187ms step_avg:60.50ms
step:1442/2315 train_time:87248ms step_avg:60.50ms
step:1443/2315 train_time:87308ms step_avg:60.50ms
step:1444/2315 train_time:87369ms step_avg:60.50ms
step:1445/2315 train_time:87429ms step_avg:60.50ms
step:1446/2315 train_time:87491ms step_avg:60.51ms
step:1447/2315 train_time:87552ms step_avg:60.51ms
step:1448/2315 train_time:87614ms step_avg:60.51ms
step:1449/2315 train_time:87675ms step_avg:60.51ms
step:1450/2315 train_time:87736ms step_avg:60.51ms
step:1451/2315 train_time:87798ms step_avg:60.51ms
step:1452/2315 train_time:87858ms step_avg:60.51ms
step:1453/2315 train_time:87919ms step_avg:60.51ms
step:1454/2315 train_time:87980ms step_avg:60.51ms
step:1455/2315 train_time:88041ms step_avg:60.51ms
step:1456/2315 train_time:88102ms step_avg:60.51ms
step:1457/2315 train_time:88163ms step_avg:60.51ms
step:1458/2315 train_time:88224ms step_avg:60.51ms
step:1459/2315 train_time:88285ms step_avg:60.51ms
step:1460/2315 train_time:88347ms step_avg:60.51ms
step:1461/2315 train_time:88407ms step_avg:60.51ms
step:1462/2315 train_time:88469ms step_avg:60.51ms
step:1463/2315 train_time:88529ms step_avg:60.51ms
step:1464/2315 train_time:88590ms step_avg:60.51ms
step:1465/2315 train_time:88651ms step_avg:60.51ms
step:1466/2315 train_time:88712ms step_avg:60.51ms
step:1467/2315 train_time:88773ms step_avg:60.51ms
step:1468/2315 train_time:88836ms step_avg:60.51ms
step:1469/2315 train_time:88897ms step_avg:60.52ms
step:1470/2315 train_time:88958ms step_avg:60.52ms
step:1471/2315 train_time:89018ms step_avg:60.52ms
step:1472/2315 train_time:89079ms step_avg:60.52ms
step:1473/2315 train_time:89140ms step_avg:60.52ms
step:1474/2315 train_time:89201ms step_avg:60.52ms
step:1475/2315 train_time:89262ms step_avg:60.52ms
step:1476/2315 train_time:89323ms step_avg:60.52ms
step:1477/2315 train_time:89383ms step_avg:60.52ms
step:1478/2315 train_time:89445ms step_avg:60.52ms
step:1479/2315 train_time:89505ms step_avg:60.52ms
step:1480/2315 train_time:89567ms step_avg:60.52ms
step:1481/2315 train_time:89627ms step_avg:60.52ms
step:1482/2315 train_time:89688ms step_avg:60.52ms
step:1483/2315 train_time:89749ms step_avg:60.52ms
step:1484/2315 train_time:89810ms step_avg:60.52ms
step:1485/2315 train_time:89870ms step_avg:60.52ms
step:1486/2315 train_time:89932ms step_avg:60.52ms
step:1487/2315 train_time:89993ms step_avg:60.52ms
step:1488/2315 train_time:90054ms step_avg:60.52ms
step:1489/2315 train_time:90116ms step_avg:60.52ms
step:1490/2315 train_time:90177ms step_avg:60.52ms
step:1491/2315 train_time:90239ms step_avg:60.52ms
step:1492/2315 train_time:90300ms step_avg:60.52ms
step:1493/2315 train_time:90360ms step_avg:60.52ms
step:1494/2315 train_time:90421ms step_avg:60.52ms
step:1495/2315 train_time:90482ms step_avg:60.52ms
step:1496/2315 train_time:90543ms step_avg:60.52ms
step:1497/2315 train_time:90603ms step_avg:60.52ms
step:1498/2315 train_time:90664ms step_avg:60.52ms
step:1499/2315 train_time:90725ms step_avg:60.52ms
step:1500/2315 train_time:90787ms step_avg:60.52ms
step:1500/2315 val_loss:3.4490 train_time:90850ms step_avg:60.57ms
step:1501/2315 train_time:90876ms step_avg:60.54ms
step:1502/2315 train_time:90913ms step_avg:60.53ms
step:1503/2315 train_time:90978ms step_avg:60.53ms
step:1504/2315 train_time:91042ms step_avg:60.53ms
step:1505/2315 train_time:91103ms step_avg:60.53ms
step:1506/2315 train_time:91165ms step_avg:60.53ms
step:1507/2315 train_time:91225ms step_avg:60.53ms
step:1508/2315 train_time:91285ms step_avg:60.53ms
step:1509/2315 train_time:91346ms step_avg:60.53ms
step:1510/2315 train_time:91406ms step_avg:60.53ms
step:1511/2315 train_time:91465ms step_avg:60.53ms
step:1512/2315 train_time:91526ms step_avg:60.53ms
step:1513/2315 train_time:91585ms step_avg:60.53ms
step:1514/2315 train_time:91646ms step_avg:60.53ms
step:1515/2315 train_time:91706ms step_avg:60.53ms
step:1516/2315 train_time:91767ms step_avg:60.53ms
step:1517/2315 train_time:91828ms step_avg:60.53ms
step:1518/2315 train_time:91892ms step_avg:60.53ms
step:1519/2315 train_time:91955ms step_avg:60.54ms
step:1520/2315 train_time:92017ms step_avg:60.54ms
step:1521/2315 train_time:92079ms step_avg:60.54ms
step:1522/2315 train_time:92141ms step_avg:60.54ms
step:1523/2315 train_time:92202ms step_avg:60.54ms
step:1524/2315 train_time:92263ms step_avg:60.54ms
step:1525/2315 train_time:92323ms step_avg:60.54ms
step:1526/2315 train_time:92384ms step_avg:60.54ms
step:1527/2315 train_time:92445ms step_avg:60.54ms
step:1528/2315 train_time:92505ms step_avg:60.54ms
step:1529/2315 train_time:92565ms step_avg:60.54ms
step:1530/2315 train_time:92626ms step_avg:60.54ms
step:1531/2315 train_time:92687ms step_avg:60.54ms
step:1532/2315 train_time:92749ms step_avg:60.54ms
step:1533/2315 train_time:92810ms step_avg:60.54ms
step:1534/2315 train_time:92873ms step_avg:60.54ms
step:1535/2315 train_time:92936ms step_avg:60.54ms
step:1536/2315 train_time:92998ms step_avg:60.55ms
step:1537/2315 train_time:93059ms step_avg:60.55ms
step:1538/2315 train_time:93121ms step_avg:60.55ms
step:1539/2315 train_time:93182ms step_avg:60.55ms
step:1540/2315 train_time:93244ms step_avg:60.55ms
step:1541/2315 train_time:93305ms step_avg:60.55ms
step:1542/2315 train_time:93366ms step_avg:60.55ms
step:1543/2315 train_time:93427ms step_avg:60.55ms
step:1544/2315 train_time:93488ms step_avg:60.55ms
step:1545/2315 train_time:93548ms step_avg:60.55ms
step:1546/2315 train_time:93609ms step_avg:60.55ms
step:1547/2315 train_time:93670ms step_avg:60.55ms
step:1548/2315 train_time:93732ms step_avg:60.55ms
step:1549/2315 train_time:93793ms step_avg:60.55ms
step:1550/2315 train_time:93856ms step_avg:60.55ms
step:1551/2315 train_time:93917ms step_avg:60.55ms
step:1552/2315 train_time:93979ms step_avg:60.55ms
step:1553/2315 train_time:94041ms step_avg:60.55ms
step:1554/2315 train_time:94102ms step_avg:60.55ms
step:1555/2315 train_time:94164ms step_avg:60.56ms
step:1556/2315 train_time:94225ms step_avg:60.56ms
step:1557/2315 train_time:94286ms step_avg:60.56ms
step:1558/2315 train_time:94347ms step_avg:60.56ms
step:1559/2315 train_time:94408ms step_avg:60.56ms
step:1560/2315 train_time:94469ms step_avg:60.56ms
step:1561/2315 train_time:94530ms step_avg:60.56ms
step:1562/2315 train_time:94591ms step_avg:60.56ms
step:1563/2315 train_time:94652ms step_avg:60.56ms
step:1564/2315 train_time:94714ms step_avg:60.56ms
step:1565/2315 train_time:94775ms step_avg:60.56ms
step:1566/2315 train_time:94837ms step_avg:60.56ms
step:1567/2315 train_time:94898ms step_avg:60.56ms
step:1568/2315 train_time:94959ms step_avg:60.56ms
step:1569/2315 train_time:95020ms step_avg:60.56ms
step:1570/2315 train_time:95082ms step_avg:60.56ms
step:1571/2315 train_time:95143ms step_avg:60.56ms
step:1572/2315 train_time:95205ms step_avg:60.56ms
step:1573/2315 train_time:95266ms step_avg:60.56ms
step:1574/2315 train_time:95328ms step_avg:60.56ms
step:1575/2315 train_time:95389ms step_avg:60.56ms
step:1576/2315 train_time:95450ms step_avg:60.56ms
step:1577/2315 train_time:95511ms step_avg:60.56ms
step:1578/2315 train_time:95572ms step_avg:60.57ms
step:1579/2315 train_time:95633ms step_avg:60.57ms
step:1580/2315 train_time:95695ms step_avg:60.57ms
step:1581/2315 train_time:95756ms step_avg:60.57ms
step:1582/2315 train_time:95817ms step_avg:60.57ms
step:1583/2315 train_time:95879ms step_avg:60.57ms
step:1584/2315 train_time:95940ms step_avg:60.57ms
step:1585/2315 train_time:96001ms step_avg:60.57ms
step:1586/2315 train_time:96063ms step_avg:60.57ms
step:1587/2315 train_time:96124ms step_avg:60.57ms
step:1588/2315 train_time:96186ms step_avg:60.57ms
step:1589/2315 train_time:96247ms step_avg:60.57ms
step:1590/2315 train_time:96308ms step_avg:60.57ms
step:1591/2315 train_time:96368ms step_avg:60.57ms
step:1592/2315 train_time:96430ms step_avg:60.57ms
step:1593/2315 train_time:96490ms step_avg:60.57ms
step:1594/2315 train_time:96552ms step_avg:60.57ms
step:1595/2315 train_time:96613ms step_avg:60.57ms
step:1596/2315 train_time:96675ms step_avg:60.57ms
step:1597/2315 train_time:96736ms step_avg:60.57ms
step:1598/2315 train_time:96797ms step_avg:60.57ms
step:1599/2315 train_time:96859ms step_avg:60.57ms
step:1600/2315 train_time:96921ms step_avg:60.58ms
step:1601/2315 train_time:96981ms step_avg:60.58ms
step:1602/2315 train_time:97043ms step_avg:60.58ms
step:1603/2315 train_time:97104ms step_avg:60.58ms
step:1604/2315 train_time:97166ms step_avg:60.58ms
step:1605/2315 train_time:97227ms step_avg:60.58ms
step:1606/2315 train_time:97288ms step_avg:60.58ms
step:1607/2315 train_time:97349ms step_avg:60.58ms
step:1608/2315 train_time:97410ms step_avg:60.58ms
step:1609/2315 train_time:97471ms step_avg:60.58ms
step:1610/2315 train_time:97533ms step_avg:60.58ms
step:1611/2315 train_time:97593ms step_avg:60.58ms
step:1612/2315 train_time:97655ms step_avg:60.58ms
step:1613/2315 train_time:97716ms step_avg:60.58ms
step:1614/2315 train_time:97777ms step_avg:60.58ms
step:1615/2315 train_time:97838ms step_avg:60.58ms
step:1616/2315 train_time:97900ms step_avg:60.58ms
step:1617/2315 train_time:97960ms step_avg:60.58ms
step:1618/2315 train_time:98021ms step_avg:60.58ms
step:1619/2315 train_time:98082ms step_avg:60.58ms
step:1620/2315 train_time:98144ms step_avg:60.58ms
step:1621/2315 train_time:98205ms step_avg:60.58ms
step:1622/2315 train_time:98267ms step_avg:60.58ms
step:1623/2315 train_time:98327ms step_avg:60.58ms
step:1624/2315 train_time:98388ms step_avg:60.58ms
step:1625/2315 train_time:98449ms step_avg:60.58ms
step:1626/2315 train_time:98510ms step_avg:60.58ms
step:1627/2315 train_time:98572ms step_avg:60.59ms
step:1628/2315 train_time:98633ms step_avg:60.59ms
step:1629/2315 train_time:98695ms step_avg:60.59ms
step:1630/2315 train_time:98757ms step_avg:60.59ms
step:1631/2315 train_time:98818ms step_avg:60.59ms
step:1632/2315 train_time:98879ms step_avg:60.59ms
step:1633/2315 train_time:98940ms step_avg:60.59ms
step:1634/2315 train_time:99001ms step_avg:60.59ms
step:1635/2315 train_time:99063ms step_avg:60.59ms
step:1636/2315 train_time:99124ms step_avg:60.59ms
step:1637/2315 train_time:99185ms step_avg:60.59ms
step:1638/2315 train_time:99247ms step_avg:60.59ms
step:1639/2315 train_time:99307ms step_avg:60.59ms
step:1640/2315 train_time:99368ms step_avg:60.59ms
step:1641/2315 train_time:99429ms step_avg:60.59ms
step:1642/2315 train_time:99491ms step_avg:60.59ms
step:1643/2315 train_time:99551ms step_avg:60.59ms
step:1644/2315 train_time:99613ms step_avg:60.59ms
step:1645/2315 train_time:99675ms step_avg:60.59ms
step:1646/2315 train_time:99736ms step_avg:60.59ms
step:1647/2315 train_time:99798ms step_avg:60.59ms
step:1648/2315 train_time:99859ms step_avg:60.59ms
step:1649/2315 train_time:99920ms step_avg:60.59ms
step:1650/2315 train_time:99982ms step_avg:60.59ms
step:1651/2315 train_time:100042ms step_avg:60.59ms
step:1652/2315 train_time:100104ms step_avg:60.60ms
step:1653/2315 train_time:100166ms step_avg:60.60ms
step:1654/2315 train_time:100227ms step_avg:60.60ms
step:1655/2315 train_time:100287ms step_avg:60.60ms
step:1656/2315 train_time:100349ms step_avg:60.60ms
step:1657/2315 train_time:100410ms step_avg:60.60ms
step:1658/2315 train_time:100472ms step_avg:60.60ms
step:1659/2315 train_time:100533ms step_avg:60.60ms
step:1660/2315 train_time:100594ms step_avg:60.60ms
step:1661/2315 train_time:100656ms step_avg:60.60ms
step:1662/2315 train_time:100717ms step_avg:60.60ms
step:1663/2315 train_time:100779ms step_avg:60.60ms
step:1664/2315 train_time:100840ms step_avg:60.60ms
step:1665/2315 train_time:100901ms step_avg:60.60ms
step:1666/2315 train_time:100963ms step_avg:60.60ms
step:1667/2315 train_time:101023ms step_avg:60.60ms
step:1668/2315 train_time:101085ms step_avg:60.60ms
step:1669/2315 train_time:101146ms step_avg:60.60ms
step:1670/2315 train_time:101208ms step_avg:60.60ms
step:1671/2315 train_time:101268ms step_avg:60.60ms
step:1672/2315 train_time:101330ms step_avg:60.60ms
step:1673/2315 train_time:101391ms step_avg:60.60ms
step:1674/2315 train_time:101453ms step_avg:60.61ms
step:1675/2315 train_time:101514ms step_avg:60.61ms
step:1676/2315 train_time:101576ms step_avg:60.61ms
step:1677/2315 train_time:101637ms step_avg:60.61ms
step:1678/2315 train_time:101698ms step_avg:60.61ms
step:1679/2315 train_time:101759ms step_avg:60.61ms
step:1680/2315 train_time:101820ms step_avg:60.61ms
step:1681/2315 train_time:101881ms step_avg:60.61ms
step:1682/2315 train_time:101942ms step_avg:60.61ms
step:1683/2315 train_time:102004ms step_avg:60.61ms
step:1684/2315 train_time:102065ms step_avg:60.61ms
step:1685/2315 train_time:102127ms step_avg:60.61ms
step:1686/2315 train_time:102188ms step_avg:60.61ms
step:1687/2315 train_time:102249ms step_avg:60.61ms
step:1688/2315 train_time:102310ms step_avg:60.61ms
step:1689/2315 train_time:102371ms step_avg:60.61ms
step:1690/2315 train_time:102433ms step_avg:60.61ms
step:1691/2315 train_time:102494ms step_avg:60.61ms
step:1692/2315 train_time:102556ms step_avg:60.61ms
step:1693/2315 train_time:102617ms step_avg:60.61ms
step:1694/2315 train_time:102678ms step_avg:60.61ms
step:1695/2315 train_time:102739ms step_avg:60.61ms
step:1696/2315 train_time:102801ms step_avg:60.61ms
step:1697/2315 train_time:102862ms step_avg:60.61ms
step:1698/2315 train_time:102923ms step_avg:60.61ms
step:1699/2315 train_time:102984ms step_avg:60.61ms
step:1700/2315 train_time:103045ms step_avg:60.61ms
step:1701/2315 train_time:103106ms step_avg:60.61ms
step:1702/2315 train_time:103168ms step_avg:60.62ms
step:1703/2315 train_time:103229ms step_avg:60.62ms
step:1704/2315 train_time:103290ms step_avg:60.62ms
step:1705/2315 train_time:103351ms step_avg:60.62ms
step:1706/2315 train_time:103413ms step_avg:60.62ms
step:1707/2315 train_time:103474ms step_avg:60.62ms
step:1708/2315 train_time:103536ms step_avg:60.62ms
step:1709/2315 train_time:103597ms step_avg:60.62ms
step:1710/2315 train_time:103658ms step_avg:60.62ms
step:1711/2315 train_time:103718ms step_avg:60.62ms
step:1712/2315 train_time:103780ms step_avg:60.62ms
step:1713/2315 train_time:103841ms step_avg:60.62ms
step:1714/2315 train_time:103903ms step_avg:60.62ms
step:1715/2315 train_time:103964ms step_avg:60.62ms
step:1716/2315 train_time:104025ms step_avg:60.62ms
step:1717/2315 train_time:104086ms step_avg:60.62ms
step:1718/2315 train_time:104147ms step_avg:60.62ms
step:1719/2315 train_time:104208ms step_avg:60.62ms
step:1720/2315 train_time:104270ms step_avg:60.62ms
step:1721/2315 train_time:104330ms step_avg:60.62ms
step:1722/2315 train_time:104393ms step_avg:60.62ms
step:1723/2315 train_time:104454ms step_avg:60.62ms
step:1724/2315 train_time:104516ms step_avg:60.62ms
step:1725/2315 train_time:104576ms step_avg:60.62ms
step:1726/2315 train_time:104637ms step_avg:60.62ms
step:1727/2315 train_time:104698ms step_avg:60.62ms
step:1728/2315 train_time:104761ms step_avg:60.63ms
step:1729/2315 train_time:104821ms step_avg:60.63ms
step:1730/2315 train_time:104883ms step_avg:60.63ms
step:1731/2315 train_time:104944ms step_avg:60.63ms
step:1732/2315 train_time:105005ms step_avg:60.63ms
step:1733/2315 train_time:105066ms step_avg:60.63ms
step:1734/2315 train_time:105128ms step_avg:60.63ms
step:1735/2315 train_time:105188ms step_avg:60.63ms
step:1736/2315 train_time:105250ms step_avg:60.63ms
step:1737/2315 train_time:105311ms step_avg:60.63ms
step:1738/2315 train_time:105373ms step_avg:60.63ms
step:1739/2315 train_time:105434ms step_avg:60.63ms
step:1740/2315 train_time:105496ms step_avg:60.63ms
step:1741/2315 train_time:105557ms step_avg:60.63ms
step:1742/2315 train_time:105618ms step_avg:60.63ms
step:1743/2315 train_time:105678ms step_avg:60.63ms
step:1744/2315 train_time:105740ms step_avg:60.63ms
step:1745/2315 train_time:105800ms step_avg:60.63ms
step:1746/2315 train_time:105863ms step_avg:60.63ms
step:1747/2315 train_time:105923ms step_avg:60.63ms
step:1748/2315 train_time:105985ms step_avg:60.63ms
step:1749/2315 train_time:106046ms step_avg:60.63ms
step:1750/2315 train_time:106108ms step_avg:60.63ms
step:1750/2315 val_loss:3.3805 train_time:106170ms step_avg:60.67ms
step:1751/2315 train_time:106194ms step_avg:60.65ms
step:1752/2315 train_time:106233ms step_avg:60.64ms
step:1753/2315 train_time:106299ms step_avg:60.64ms
step:1754/2315 train_time:106366ms step_avg:60.64ms
step:1755/2315 train_time:106427ms step_avg:60.64ms
step:1756/2315 train_time:106488ms step_avg:60.64ms
step:1757/2315 train_time:106548ms step_avg:60.64ms
step:1758/2315 train_time:106609ms step_avg:60.64ms
step:1759/2315 train_time:106669ms step_avg:60.64ms
step:1760/2315 train_time:106729ms step_avg:60.64ms
step:1761/2315 train_time:106790ms step_avg:60.64ms
step:1762/2315 train_time:106851ms step_avg:60.64ms
step:1763/2315 train_time:106912ms step_avg:60.64ms
step:1764/2315 train_time:106972ms step_avg:60.64ms
step:1765/2315 train_time:107032ms step_avg:60.64ms
step:1766/2315 train_time:107094ms step_avg:60.64ms
step:1767/2315 train_time:107155ms step_avg:60.64ms
step:1768/2315 train_time:107217ms step_avg:60.64ms
step:1769/2315 train_time:107280ms step_avg:60.64ms
step:1770/2315 train_time:107344ms step_avg:60.65ms
step:1771/2315 train_time:107404ms step_avg:60.65ms
step:1772/2315 train_time:107466ms step_avg:60.65ms
step:1773/2315 train_time:107526ms step_avg:60.65ms
step:1774/2315 train_time:107588ms step_avg:60.65ms
step:1775/2315 train_time:107648ms step_avg:60.65ms
step:1776/2315 train_time:107710ms step_avg:60.65ms
step:1777/2315 train_time:107770ms step_avg:60.65ms
step:1778/2315 train_time:107831ms step_avg:60.65ms
step:1779/2315 train_time:107891ms step_avg:60.65ms
step:1780/2315 train_time:107953ms step_avg:60.65ms
step:1781/2315 train_time:108013ms step_avg:60.65ms
step:1782/2315 train_time:108074ms step_avg:60.65ms
step:1783/2315 train_time:108136ms step_avg:60.65ms
step:1784/2315 train_time:108198ms step_avg:60.65ms
step:1785/2315 train_time:108260ms step_avg:60.65ms
step:1786/2315 train_time:108323ms step_avg:60.65ms
step:1787/2315 train_time:108385ms step_avg:60.65ms
step:1788/2315 train_time:108447ms step_avg:60.65ms
step:1789/2315 train_time:108507ms step_avg:60.65ms
step:1790/2315 train_time:108569ms step_avg:60.65ms
step:1791/2315 train_time:108629ms step_avg:60.65ms
step:1792/2315 train_time:108691ms step_avg:60.65ms
step:1793/2315 train_time:108751ms step_avg:60.65ms
step:1794/2315 train_time:108812ms step_avg:60.65ms
step:1795/2315 train_time:108872ms step_avg:60.65ms
step:1796/2315 train_time:108934ms step_avg:60.65ms
step:1797/2315 train_time:108994ms step_avg:60.65ms
step:1798/2315 train_time:109056ms step_avg:60.65ms
step:1799/2315 train_time:109116ms step_avg:60.65ms
step:1800/2315 train_time:109179ms step_avg:60.65ms
step:1801/2315 train_time:109241ms step_avg:60.66ms
step:1802/2315 train_time:109303ms step_avg:60.66ms
step:1803/2315 train_time:109364ms step_avg:60.66ms
step:1804/2315 train_time:109426ms step_avg:60.66ms
step:1805/2315 train_time:109487ms step_avg:60.66ms
step:1806/2315 train_time:109548ms step_avg:60.66ms
step:1807/2315 train_time:109609ms step_avg:60.66ms
step:1808/2315 train_time:109670ms step_avg:60.66ms
step:1809/2315 train_time:109731ms step_avg:60.66ms
step:1810/2315 train_time:109792ms step_avg:60.66ms
step:1811/2315 train_time:109853ms step_avg:60.66ms
step:1812/2315 train_time:109914ms step_avg:60.66ms
step:1813/2315 train_time:109974ms step_avg:60.66ms
step:1814/2315 train_time:110036ms step_avg:60.66ms
step:1815/2315 train_time:110097ms step_avg:60.66ms
step:1816/2315 train_time:110159ms step_avg:60.66ms
step:1817/2315 train_time:110220ms step_avg:60.66ms
step:1818/2315 train_time:110282ms step_avg:60.66ms
step:1819/2315 train_time:110344ms step_avg:60.66ms
step:1820/2315 train_time:110405ms step_avg:60.66ms
step:1821/2315 train_time:110467ms step_avg:60.66ms
step:1822/2315 train_time:110527ms step_avg:60.66ms
step:1823/2315 train_time:110588ms step_avg:60.66ms
step:1824/2315 train_time:110649ms step_avg:60.66ms
step:1825/2315 train_time:110710ms step_avg:60.66ms
step:1826/2315 train_time:110771ms step_avg:60.66ms
step:1827/2315 train_time:110832ms step_avg:60.66ms
step:1828/2315 train_time:110893ms step_avg:60.66ms
step:1829/2315 train_time:110953ms step_avg:60.66ms
step:1830/2315 train_time:111015ms step_avg:60.66ms
step:1831/2315 train_time:111077ms step_avg:60.66ms
step:1832/2315 train_time:111138ms step_avg:60.66ms
step:1833/2315 train_time:111199ms step_avg:60.66ms
step:1834/2315 train_time:111261ms step_avg:60.67ms
step:1835/2315 train_time:111322ms step_avg:60.67ms
step:1836/2315 train_time:111385ms step_avg:60.67ms
step:1837/2315 train_time:111446ms step_avg:60.67ms
step:1838/2315 train_time:111507ms step_avg:60.67ms
step:1839/2315 train_time:111568ms step_avg:60.67ms
step:1840/2315 train_time:111630ms step_avg:60.67ms
step:1841/2315 train_time:111690ms step_avg:60.67ms
step:1842/2315 train_time:111752ms step_avg:60.67ms
step:1843/2315 train_time:111812ms step_avg:60.67ms
step:1844/2315 train_time:111874ms step_avg:60.67ms
step:1845/2315 train_time:111934ms step_avg:60.67ms
step:1846/2315 train_time:111996ms step_avg:60.67ms
step:1847/2315 train_time:112056ms step_avg:60.67ms
step:1848/2315 train_time:112118ms step_avg:60.67ms
step:1849/2315 train_time:112179ms step_avg:60.67ms
step:1850/2315 train_time:112241ms step_avg:60.67ms
step:1851/2315 train_time:112302ms step_avg:60.67ms
step:1852/2315 train_time:112364ms step_avg:60.67ms
step:1853/2315 train_time:112425ms step_avg:60.67ms
step:1854/2315 train_time:112486ms step_avg:60.67ms
step:1855/2315 train_time:112547ms step_avg:60.67ms
step:1856/2315 train_time:112608ms step_avg:60.67ms
step:1857/2315 train_time:112669ms step_avg:60.67ms
step:1858/2315 train_time:112731ms step_avg:60.67ms
step:1859/2315 train_time:112792ms step_avg:60.67ms
step:1860/2315 train_time:112853ms step_avg:60.67ms
step:1861/2315 train_time:112914ms step_avg:60.67ms
step:1862/2315 train_time:112976ms step_avg:60.67ms
step:1863/2315 train_time:113037ms step_avg:60.67ms
step:1864/2315 train_time:113098ms step_avg:60.67ms
step:1865/2315 train_time:113159ms step_avg:60.68ms
step:1866/2315 train_time:113221ms step_avg:60.68ms
step:1867/2315 train_time:113282ms step_avg:60.68ms
step:1868/2315 train_time:113345ms step_avg:60.68ms
step:1869/2315 train_time:113405ms step_avg:60.68ms
step:1870/2315 train_time:113466ms step_avg:60.68ms
step:1871/2315 train_time:113527ms step_avg:60.68ms
step:1872/2315 train_time:113588ms step_avg:60.68ms
step:1873/2315 train_time:113649ms step_avg:60.68ms
step:1874/2315 train_time:113710ms step_avg:60.68ms
step:1875/2315 train_time:113771ms step_avg:60.68ms
step:1876/2315 train_time:113832ms step_avg:60.68ms
step:1877/2315 train_time:113893ms step_avg:60.68ms
step:1878/2315 train_time:113955ms step_avg:60.68ms
step:1879/2315 train_time:114015ms step_avg:60.68ms
step:1880/2315 train_time:114077ms step_avg:60.68ms
step:1881/2315 train_time:114138ms step_avg:60.68ms
step:1882/2315 train_time:114200ms step_avg:60.68ms
step:1883/2315 train_time:114260ms step_avg:60.68ms
step:1884/2315 train_time:114322ms step_avg:60.68ms
step:1885/2315 train_time:114383ms step_avg:60.68ms
step:1886/2315 train_time:114446ms step_avg:60.68ms
step:1887/2315 train_time:114507ms step_avg:60.68ms
step:1888/2315 train_time:114568ms step_avg:60.68ms
step:1889/2315 train_time:114629ms step_avg:60.68ms
step:1890/2315 train_time:114691ms step_avg:60.68ms
step:1891/2315 train_time:114752ms step_avg:60.68ms
step:1892/2315 train_time:114814ms step_avg:60.68ms
step:1893/2315 train_time:114874ms step_avg:60.68ms
step:1894/2315 train_time:114935ms step_avg:60.68ms
step:1895/2315 train_time:114996ms step_avg:60.68ms
step:1896/2315 train_time:115057ms step_avg:60.68ms
step:1897/2315 train_time:115118ms step_avg:60.68ms
step:1898/2315 train_time:115180ms step_avg:60.68ms
step:1899/2315 train_time:115240ms step_avg:60.68ms
step:1900/2315 train_time:115302ms step_avg:60.69ms
step:1901/2315 train_time:115362ms step_avg:60.69ms
step:1902/2315 train_time:115425ms step_avg:60.69ms
step:1903/2315 train_time:115486ms step_avg:60.69ms
step:1904/2315 train_time:115548ms step_avg:60.69ms
step:1905/2315 train_time:115608ms step_avg:60.69ms
step:1906/2315 train_time:115670ms step_avg:60.69ms
step:1907/2315 train_time:115731ms step_avg:60.69ms
step:1908/2315 train_time:115792ms step_avg:60.69ms
step:1909/2315 train_time:115853ms step_avg:60.69ms
step:1910/2315 train_time:115915ms step_avg:60.69ms
step:1911/2315 train_time:115976ms step_avg:60.69ms
step:1912/2315 train_time:116037ms step_avg:60.69ms
step:1913/2315 train_time:116098ms step_avg:60.69ms
step:1914/2315 train_time:116159ms step_avg:60.69ms
step:1915/2315 train_time:116220ms step_avg:60.69ms
step:1916/2315 train_time:116282ms step_avg:60.69ms
step:1917/2315 train_time:116343ms step_avg:60.69ms
step:1918/2315 train_time:116405ms step_avg:60.69ms
step:1919/2315 train_time:116466ms step_avg:60.69ms
step:1920/2315 train_time:116528ms step_avg:60.69ms
step:1921/2315 train_time:116588ms step_avg:60.69ms
step:1922/2315 train_time:116650ms step_avg:60.69ms
step:1923/2315 train_time:116711ms step_avg:60.69ms
step:1924/2315 train_time:116773ms step_avg:60.69ms
step:1925/2315 train_time:116834ms step_avg:60.69ms
step:1926/2315 train_time:116896ms step_avg:60.69ms
step:1927/2315 train_time:116957ms step_avg:60.69ms
step:1928/2315 train_time:117018ms step_avg:60.69ms
step:1929/2315 train_time:117079ms step_avg:60.69ms
step:1930/2315 train_time:117140ms step_avg:60.69ms
step:1931/2315 train_time:117202ms step_avg:60.69ms
step:1932/2315 train_time:117263ms step_avg:60.70ms
step:1933/2315 train_time:117324ms step_avg:60.70ms
step:1934/2315 train_time:117386ms step_avg:60.70ms
step:1935/2315 train_time:117448ms step_avg:60.70ms
step:1936/2315 train_time:117509ms step_avg:60.70ms
step:1937/2315 train_time:117570ms step_avg:60.70ms
step:1938/2315 train_time:117632ms step_avg:60.70ms
step:1939/2315 train_time:117692ms step_avg:60.70ms
step:1940/2315 train_time:117754ms step_avg:60.70ms
step:1941/2315 train_time:117815ms step_avg:60.70ms
step:1942/2315 train_time:117876ms step_avg:60.70ms
step:1943/2315 train_time:117937ms step_avg:60.70ms
step:1944/2315 train_time:117998ms step_avg:60.70ms
step:1945/2315 train_time:118058ms step_avg:60.70ms
step:1946/2315 train_time:118120ms step_avg:60.70ms
step:1947/2315 train_time:118181ms step_avg:60.70ms
step:1948/2315 train_time:118243ms step_avg:60.70ms
step:1949/2315 train_time:118305ms step_avg:60.70ms
step:1950/2315 train_time:118366ms step_avg:60.70ms
step:1951/2315 train_time:118426ms step_avg:60.70ms
step:1952/2315 train_time:118488ms step_avg:60.70ms
step:1953/2315 train_time:118549ms step_avg:60.70ms
step:1954/2315 train_time:118610ms step_avg:60.70ms
step:1955/2315 train_time:118670ms step_avg:60.70ms
step:1956/2315 train_time:118732ms step_avg:60.70ms
step:1957/2315 train_time:118794ms step_avg:60.70ms
step:1958/2315 train_time:118855ms step_avg:60.70ms
step:1959/2315 train_time:118916ms step_avg:60.70ms
step:1960/2315 train_time:118978ms step_avg:60.70ms
step:1961/2315 train_time:119038ms step_avg:60.70ms
step:1962/2315 train_time:119100ms step_avg:60.70ms
step:1963/2315 train_time:119161ms step_avg:60.70ms
step:1964/2315 train_time:119223ms step_avg:60.70ms
step:1965/2315 train_time:119283ms step_avg:60.70ms
step:1966/2315 train_time:119345ms step_avg:60.70ms
step:1967/2315 train_time:119406ms step_avg:60.70ms
step:1968/2315 train_time:119467ms step_avg:60.70ms
step:1969/2315 train_time:119529ms step_avg:60.71ms
step:1970/2315 train_time:119590ms step_avg:60.71ms
step:1971/2315 train_time:119651ms step_avg:60.71ms
step:1972/2315 train_time:119712ms step_avg:60.71ms
step:1973/2315 train_time:119773ms step_avg:60.71ms
step:1974/2315 train_time:119835ms step_avg:60.71ms
step:1975/2315 train_time:119896ms step_avg:60.71ms
step:1976/2315 train_time:119957ms step_avg:60.71ms
step:1977/2315 train_time:120018ms step_avg:60.71ms
step:1978/2315 train_time:120079ms step_avg:60.71ms
step:1979/2315 train_time:120140ms step_avg:60.71ms
step:1980/2315 train_time:120202ms step_avg:60.71ms
step:1981/2315 train_time:120263ms step_avg:60.71ms
step:1982/2315 train_time:120325ms step_avg:60.71ms
step:1983/2315 train_time:120386ms step_avg:60.71ms
step:1984/2315 train_time:120448ms step_avg:60.71ms
step:1985/2315 train_time:120509ms step_avg:60.71ms
step:1986/2315 train_time:120571ms step_avg:60.71ms
step:1987/2315 train_time:120631ms step_avg:60.71ms
step:1988/2315 train_time:120694ms step_avg:60.71ms
step:1989/2315 train_time:120754ms step_avg:60.71ms
step:1990/2315 train_time:120816ms step_avg:60.71ms
step:1991/2315 train_time:120877ms step_avg:60.71ms
step:1992/2315 train_time:120939ms step_avg:60.71ms
step:1993/2315 train_time:121000ms step_avg:60.71ms
step:1994/2315 train_time:121061ms step_avg:60.71ms
step:1995/2315 train_time:121122ms step_avg:60.71ms
step:1996/2315 train_time:121183ms step_avg:60.71ms
step:1997/2315 train_time:121244ms step_avg:60.71ms
step:1998/2315 train_time:121306ms step_avg:60.71ms
step:1999/2315 train_time:121366ms step_avg:60.71ms
step:2000/2315 train_time:121428ms step_avg:60.71ms
step:2000/2315 val_loss:3.3301 train_time:121491ms step_avg:60.75ms
step:2001/2315 train_time:121510ms step_avg:60.72ms
step:2002/2315 train_time:121553ms step_avg:60.72ms
step:2003/2315 train_time:121619ms step_avg:60.72ms
step:2004/2315 train_time:121684ms step_avg:60.72ms
step:2005/2315 train_time:121746ms step_avg:60.72ms
step:2006/2315 train_time:121808ms step_avg:60.72ms
step:2007/2315 train_time:121869ms step_avg:60.72ms
step:2008/2315 train_time:121931ms step_avg:60.72ms
step:2009/2315 train_time:121992ms step_avg:60.72ms
step:2010/2315 train_time:122052ms step_avg:60.72ms
step:2011/2315 train_time:122112ms step_avg:60.72ms
step:2012/2315 train_time:122174ms step_avg:60.72ms
step:2013/2315 train_time:122233ms step_avg:60.72ms
step:2014/2315 train_time:122294ms step_avg:60.72ms
step:2015/2315 train_time:122354ms step_avg:60.72ms
step:2016/2315 train_time:122415ms step_avg:60.72ms
step:2017/2315 train_time:122477ms step_avg:60.72ms
step:2018/2315 train_time:122540ms step_avg:60.72ms
step:2019/2315 train_time:122603ms step_avg:60.72ms
step:2020/2315 train_time:122665ms step_avg:60.73ms
step:2021/2315 train_time:122727ms step_avg:60.73ms
step:2022/2315 train_time:122789ms step_avg:60.73ms
step:2023/2315 train_time:122851ms step_avg:60.73ms
step:2024/2315 train_time:122913ms step_avg:60.73ms
step:2025/2315 train_time:122973ms step_avg:60.73ms
step:2026/2315 train_time:123034ms step_avg:60.73ms
step:2027/2315 train_time:123093ms step_avg:60.73ms
step:2028/2315 train_time:123154ms step_avg:60.73ms
step:2029/2315 train_time:123214ms step_avg:60.73ms
step:2030/2315 train_time:123275ms step_avg:60.73ms
step:2031/2315 train_time:123335ms step_avg:60.73ms
step:2032/2315 train_time:123396ms step_avg:60.73ms
step:2033/2315 train_time:123458ms step_avg:60.73ms
step:2034/2315 train_time:123520ms step_avg:60.73ms
step:2035/2315 train_time:123582ms step_avg:60.73ms
step:2036/2315 train_time:123644ms step_avg:60.73ms
step:2037/2315 train_time:123705ms step_avg:60.73ms
step:2038/2315 train_time:123768ms step_avg:60.73ms
step:2039/2315 train_time:123829ms step_avg:60.73ms
step:2040/2315 train_time:123892ms step_avg:60.73ms
step:2041/2315 train_time:123952ms step_avg:60.73ms
step:2042/2315 train_time:124014ms step_avg:60.73ms
step:2043/2315 train_time:124074ms step_avg:60.73ms
step:2044/2315 train_time:124135ms step_avg:60.73ms
step:2045/2315 train_time:124195ms step_avg:60.73ms
step:2046/2315 train_time:124256ms step_avg:60.73ms
step:2047/2315 train_time:124316ms step_avg:60.73ms
step:2048/2315 train_time:124377ms step_avg:60.73ms
step:2049/2315 train_time:124439ms step_avg:60.73ms
step:2050/2315 train_time:124501ms step_avg:60.73ms
step:2051/2315 train_time:124563ms step_avg:60.73ms
step:2052/2315 train_time:124625ms step_avg:60.73ms
step:2053/2315 train_time:124686ms step_avg:60.73ms
step:2054/2315 train_time:124748ms step_avg:60.73ms
step:2055/2315 train_time:124810ms step_avg:60.73ms
step:2056/2315 train_time:124872ms step_avg:60.74ms
step:2057/2315 train_time:124933ms step_avg:60.74ms
step:2058/2315 train_time:124994ms step_avg:60.74ms
step:2059/2315 train_time:125054ms step_avg:60.74ms
step:2060/2315 train_time:125116ms step_avg:60.74ms
step:2061/2315 train_time:125176ms step_avg:60.74ms
step:2062/2315 train_time:125237ms step_avg:60.74ms
step:2063/2315 train_time:125297ms step_avg:60.74ms
step:2064/2315 train_time:125359ms step_avg:60.74ms
step:2065/2315 train_time:125420ms step_avg:60.74ms
step:2066/2315 train_time:125482ms step_avg:60.74ms
step:2067/2315 train_time:125544ms step_avg:60.74ms
step:2068/2315 train_time:125606ms step_avg:60.74ms
step:2069/2315 train_time:125667ms step_avg:60.74ms
step:2070/2315 train_time:125729ms step_avg:60.74ms
step:2071/2315 train_time:125791ms step_avg:60.74ms
step:2072/2315 train_time:125852ms step_avg:60.74ms
step:2073/2315 train_time:125913ms step_avg:60.74ms
step:2074/2315 train_time:125974ms step_avg:60.74ms
step:2075/2315 train_time:126035ms step_avg:60.74ms
step:2076/2315 train_time:126096ms step_avg:60.74ms
step:2077/2315 train_time:126157ms step_avg:60.74ms
step:2078/2315 train_time:126218ms step_avg:60.74ms
step:2079/2315 train_time:126279ms step_avg:60.74ms
step:2080/2315 train_time:126340ms step_avg:60.74ms
step:2081/2315 train_time:126402ms step_avg:60.74ms
step:2082/2315 train_time:126463ms step_avg:60.74ms
step:2083/2315 train_time:126525ms step_avg:60.74ms
step:2084/2315 train_time:126587ms step_avg:60.74ms
step:2085/2315 train_time:126648ms step_avg:60.74ms
step:2086/2315 train_time:126710ms step_avg:60.74ms
step:2087/2315 train_time:126772ms step_avg:60.74ms
step:2088/2315 train_time:126833ms step_avg:60.74ms
step:2089/2315 train_time:126894ms step_avg:60.74ms
step:2090/2315 train_time:126956ms step_avg:60.74ms
step:2091/2315 train_time:127016ms step_avg:60.74ms
step:2092/2315 train_time:127079ms step_avg:60.74ms
step:2093/2315 train_time:127139ms step_avg:60.74ms
step:2094/2315 train_time:127201ms step_avg:60.75ms
step:2095/2315 train_time:127261ms step_avg:60.75ms
step:2096/2315 train_time:127323ms step_avg:60.75ms
step:2097/2315 train_time:127384ms step_avg:60.75ms
step:2098/2315 train_time:127445ms step_avg:60.75ms
step:2099/2315 train_time:127506ms step_avg:60.75ms
step:2100/2315 train_time:127569ms step_avg:60.75ms
step:2101/2315 train_time:127629ms step_avg:60.75ms
step:2102/2315 train_time:127691ms step_avg:60.75ms
step:2103/2315 train_time:127752ms step_avg:60.75ms
step:2104/2315 train_time:127814ms step_avg:60.75ms
step:2105/2315 train_time:127875ms step_avg:60.75ms
step:2106/2315 train_time:127936ms step_avg:60.75ms
step:2107/2315 train_time:127997ms step_avg:60.75ms
step:2108/2315 train_time:128059ms step_avg:60.75ms
step:2109/2315 train_time:128119ms step_avg:60.75ms
step:2110/2315 train_time:128180ms step_avg:60.75ms
step:2111/2315 train_time:128241ms step_avg:60.75ms
step:2112/2315 train_time:128303ms step_avg:60.75ms
step:2113/2315 train_time:128364ms step_avg:60.75ms
step:2114/2315 train_time:128425ms step_avg:60.75ms
step:2115/2315 train_time:128486ms step_avg:60.75ms
step:2116/2315 train_time:128548ms step_avg:60.75ms
step:2117/2315 train_time:128609ms step_avg:60.75ms
step:2118/2315 train_time:128671ms step_avg:60.75ms
step:2119/2315 train_time:128732ms step_avg:60.75ms
step:2120/2315 train_time:128793ms step_avg:60.75ms
step:2121/2315 train_time:128854ms step_avg:60.75ms
step:2122/2315 train_time:128916ms step_avg:60.75ms
step:2123/2315 train_time:128976ms step_avg:60.75ms
step:2124/2315 train_time:129037ms step_avg:60.75ms
step:2125/2315 train_time:129098ms step_avg:60.75ms
step:2126/2315 train_time:129159ms step_avg:60.75ms
step:2127/2315 train_time:129221ms step_avg:60.75ms
step:2128/2315 train_time:129282ms step_avg:60.75ms
step:2129/2315 train_time:129343ms step_avg:60.75ms
step:2130/2315 train_time:129405ms step_avg:60.75ms
step:2131/2315 train_time:129466ms step_avg:60.75ms
step:2132/2315 train_time:129527ms step_avg:60.75ms
step:2133/2315 train_time:129589ms step_avg:60.75ms
step:2134/2315 train_time:129651ms step_avg:60.75ms
step:2135/2315 train_time:129712ms step_avg:60.75ms
step:2136/2315 train_time:129774ms step_avg:60.76ms
step:2137/2315 train_time:129835ms step_avg:60.76ms
step:2138/2315 train_time:129896ms step_avg:60.76ms
step:2139/2315 train_time:129957ms step_avg:60.76ms
step:2140/2315 train_time:130018ms step_avg:60.76ms
step:2141/2315 train_time:130079ms step_avg:60.76ms
step:2142/2315 train_time:130141ms step_avg:60.76ms
step:2143/2315 train_time:130202ms step_avg:60.76ms
step:2144/2315 train_time:130263ms step_avg:60.76ms
step:2145/2315 train_time:130325ms step_avg:60.76ms
step:2146/2315 train_time:130386ms step_avg:60.76ms
step:2147/2315 train_time:130447ms step_avg:60.76ms
step:2148/2315 train_time:130509ms step_avg:60.76ms
step:2149/2315 train_time:130570ms step_avg:60.76ms
step:2150/2315 train_time:130631ms step_avg:60.76ms
step:2151/2315 train_time:130693ms step_avg:60.76ms
step:2152/2315 train_time:130755ms step_avg:60.76ms
step:2153/2315 train_time:130815ms step_avg:60.76ms
step:2154/2315 train_time:130877ms step_avg:60.76ms
step:2155/2315 train_time:130937ms step_avg:60.76ms
step:2156/2315 train_time:130998ms step_avg:60.76ms
step:2157/2315 train_time:131059ms step_avg:60.76ms
step:2158/2315 train_time:131121ms step_avg:60.76ms
step:2159/2315 train_time:131182ms step_avg:60.76ms
step:2160/2315 train_time:131243ms step_avg:60.76ms
step:2161/2315 train_time:131304ms step_avg:60.76ms
step:2162/2315 train_time:131366ms step_avg:60.76ms
step:2163/2315 train_time:131427ms step_avg:60.76ms
step:2164/2315 train_time:131488ms step_avg:60.76ms
step:2165/2315 train_time:131549ms step_avg:60.76ms
step:2166/2315 train_time:131611ms step_avg:60.76ms
step:2167/2315 train_time:131672ms step_avg:60.76ms
step:2168/2315 train_time:131734ms step_avg:60.76ms
step:2169/2315 train_time:131795ms step_avg:60.76ms
step:2170/2315 train_time:131857ms step_avg:60.76ms
step:2171/2315 train_time:131918ms step_avg:60.76ms
step:2172/2315 train_time:131979ms step_avg:60.76ms
step:2173/2315 train_time:132040ms step_avg:60.76ms
step:2174/2315 train_time:132102ms step_avg:60.76ms
step:2175/2315 train_time:132163ms step_avg:60.76ms
step:2176/2315 train_time:132224ms step_avg:60.76ms
step:2177/2315 train_time:132285ms step_avg:60.76ms
step:2178/2315 train_time:132347ms step_avg:60.77ms
step:2179/2315 train_time:132408ms step_avg:60.77ms
step:2180/2315 train_time:132470ms step_avg:60.77ms
step:2181/2315 train_time:132531ms step_avg:60.77ms
step:2182/2315 train_time:132593ms step_avg:60.77ms
step:2183/2315 train_time:132654ms step_avg:60.77ms
step:2184/2315 train_time:132715ms step_avg:60.77ms
step:2185/2315 train_time:132777ms step_avg:60.77ms
step:2186/2315 train_time:132838ms step_avg:60.77ms
step:2187/2315 train_time:132898ms step_avg:60.77ms
step:2188/2315 train_time:132960ms step_avg:60.77ms
step:2189/2315 train_time:133021ms step_avg:60.77ms
step:2190/2315 train_time:133083ms step_avg:60.77ms
step:2191/2315 train_time:133144ms step_avg:60.77ms
step:2192/2315 train_time:133205ms step_avg:60.77ms
step:2193/2315 train_time:133266ms step_avg:60.77ms
step:2194/2315 train_time:133328ms step_avg:60.77ms
step:2195/2315 train_time:133388ms step_avg:60.77ms
step:2196/2315 train_time:133450ms step_avg:60.77ms
step:2197/2315 train_time:133511ms step_avg:60.77ms
step:2198/2315 train_time:133573ms step_avg:60.77ms
step:2199/2315 train_time:133634ms step_avg:60.77ms
step:2200/2315 train_time:133696ms step_avg:60.77ms
step:2201/2315 train_time:133757ms step_avg:60.77ms
step:2202/2315 train_time:133819ms step_avg:60.77ms
step:2203/2315 train_time:133879ms step_avg:60.77ms
step:2204/2315 train_time:133940ms step_avg:60.77ms
step:2205/2315 train_time:134001ms step_avg:60.77ms
step:2206/2315 train_time:134062ms step_avg:60.77ms
step:2207/2315 train_time:134124ms step_avg:60.77ms
step:2208/2315 train_time:134186ms step_avg:60.77ms
step:2209/2315 train_time:134246ms step_avg:60.77ms
step:2210/2315 train_time:134307ms step_avg:60.77ms
step:2211/2315 train_time:134368ms step_avg:60.77ms
step:2212/2315 train_time:134431ms step_avg:60.77ms
step:2213/2315 train_time:134492ms step_avg:60.77ms
step:2214/2315 train_time:134553ms step_avg:60.77ms
step:2215/2315 train_time:134614ms step_avg:60.77ms
step:2216/2315 train_time:134675ms step_avg:60.77ms
step:2217/2315 train_time:134736ms step_avg:60.77ms
step:2218/2315 train_time:134798ms step_avg:60.77ms
step:2219/2315 train_time:134859ms step_avg:60.77ms
step:2220/2315 train_time:134921ms step_avg:60.78ms
step:2221/2315 train_time:134981ms step_avg:60.78ms
step:2222/2315 train_time:135043ms step_avg:60.78ms
step:2223/2315 train_time:135104ms step_avg:60.78ms
step:2224/2315 train_time:135167ms step_avg:60.78ms
step:2225/2315 train_time:135227ms step_avg:60.78ms
step:2226/2315 train_time:135289ms step_avg:60.78ms
step:2227/2315 train_time:135350ms step_avg:60.78ms
step:2228/2315 train_time:135412ms step_avg:60.78ms
step:2229/2315 train_time:135473ms step_avg:60.78ms
step:2230/2315 train_time:135534ms step_avg:60.78ms
step:2231/2315 train_time:135595ms step_avg:60.78ms
step:2232/2315 train_time:135657ms step_avg:60.78ms
step:2233/2315 train_time:135717ms step_avg:60.78ms
step:2234/2315 train_time:135779ms step_avg:60.78ms
step:2235/2315 train_time:135840ms step_avg:60.78ms
step:2236/2315 train_time:135901ms step_avg:60.78ms
step:2237/2315 train_time:135962ms step_avg:60.78ms
step:2238/2315 train_time:136023ms step_avg:60.78ms
step:2239/2315 train_time:136084ms step_avg:60.78ms
step:2240/2315 train_time:136146ms step_avg:60.78ms
step:2241/2315 train_time:136206ms step_avg:60.78ms
step:2242/2315 train_time:136269ms step_avg:60.78ms
step:2243/2315 train_time:136330ms step_avg:60.78ms
step:2244/2315 train_time:136392ms step_avg:60.78ms
step:2245/2315 train_time:136453ms step_avg:60.78ms
step:2246/2315 train_time:136515ms step_avg:60.78ms
step:2247/2315 train_time:136576ms step_avg:60.78ms
step:2248/2315 train_time:136638ms step_avg:60.78ms
step:2249/2315 train_time:136698ms step_avg:60.78ms
step:2250/2315 train_time:136760ms step_avg:60.78ms
step:2250/2315 val_loss:3.2903 train_time:136822ms step_avg:60.81ms
step:2251/2315 train_time:136845ms step_avg:60.79ms
step:2252/2315 train_time:136885ms step_avg:60.78ms
step:2253/2315 train_time:136950ms step_avg:60.79ms
step:2254/2315 train_time:137012ms step_avg:60.79ms
step:2255/2315 train_time:137073ms step_avg:60.79ms
step:2256/2315 train_time:137135ms step_avg:60.79ms
step:2257/2315 train_time:137196ms step_avg:60.79ms
step:2258/2315 train_time:137256ms step_avg:60.79ms
step:2259/2315 train_time:137317ms step_avg:60.79ms
step:2260/2315 train_time:137378ms step_avg:60.79ms
step:2261/2315 train_time:137438ms step_avg:60.79ms
step:2262/2315 train_time:137499ms step_avg:60.79ms
step:2263/2315 train_time:137559ms step_avg:60.79ms
step:2264/2315 train_time:137620ms step_avg:60.79ms
step:2265/2315 train_time:137680ms step_avg:60.79ms
step:2266/2315 train_time:137742ms step_avg:60.79ms
step:2267/2315 train_time:137805ms step_avg:60.79ms
step:2268/2315 train_time:137868ms step_avg:60.79ms
step:2269/2315 train_time:137930ms step_avg:60.79ms
step:2270/2315 train_time:137992ms step_avg:60.79ms
step:2271/2315 train_time:138054ms step_avg:60.79ms
step:2272/2315 train_time:138117ms step_avg:60.79ms
step:2273/2315 train_time:138177ms step_avg:60.79ms
step:2274/2315 train_time:138239ms step_avg:60.79ms
step:2275/2315 train_time:138299ms step_avg:60.79ms
step:2276/2315 train_time:138360ms step_avg:60.79ms
step:2277/2315 train_time:138420ms step_avg:60.79ms
step:2278/2315 train_time:138481ms step_avg:60.79ms
step:2279/2315 train_time:138541ms step_avg:60.79ms
step:2280/2315 train_time:138602ms step_avg:60.79ms
step:2281/2315 train_time:138662ms step_avg:60.79ms
step:2282/2315 train_time:138723ms step_avg:60.79ms
step:2283/2315 train_time:138785ms step_avg:60.79ms
step:2284/2315 train_time:138848ms step_avg:60.79ms
step:2285/2315 train_time:138910ms step_avg:60.79ms
step:2286/2315 train_time:138971ms step_avg:60.79ms
step:2287/2315 train_time:139032ms step_avg:60.79ms
step:2288/2315 train_time:139095ms step_avg:60.79ms
step:2289/2315 train_time:139156ms step_avg:60.79ms
step:2290/2315 train_time:139217ms step_avg:60.79ms
step:2291/2315 train_time:139278ms step_avg:60.79ms
step:2292/2315 train_time:139339ms step_avg:60.79ms
step:2293/2315 train_time:139399ms step_avg:60.79ms
step:2294/2315 train_time:139460ms step_avg:60.79ms
step:2295/2315 train_time:139520ms step_avg:60.79ms
step:2296/2315 train_time:139582ms step_avg:60.79ms
step:2297/2315 train_time:139642ms step_avg:60.79ms
step:2298/2315 train_time:139703ms step_avg:60.79ms
step:2299/2315 train_time:139765ms step_avg:60.79ms
step:2300/2315 train_time:139827ms step_avg:60.79ms
step:2301/2315 train_time:139889ms step_avg:60.79ms
step:2302/2315 train_time:139951ms step_avg:60.80ms
step:2303/2315 train_time:140013ms step_avg:60.80ms
step:2304/2315 train_time:140075ms step_avg:60.80ms
step:2305/2315 train_time:140137ms step_avg:60.80ms
step:2306/2315 train_time:140199ms step_avg:60.80ms
step:2307/2315 train_time:140259ms step_avg:60.80ms
step:2308/2315 train_time:140321ms step_avg:60.80ms
step:2309/2315 train_time:140381ms step_avg:60.80ms
step:2310/2315 train_time:140442ms step_avg:60.80ms
step:2311/2315 train_time:140503ms step_avg:60.80ms
step:2312/2315 train_time:140564ms step_avg:60.80ms
step:2313/2315 train_time:140624ms step_avg:60.80ms
step:2314/2315 train_time:140685ms step_avg:60.80ms
step:2315/2315 train_time:140747ms step_avg:60.80ms
step:2315/2315 val_loss:3.2775 train_time:140809ms step_avg:60.82ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2205  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()


====================================================================================================
Running Python 3.12.7 (main, Nov 15 2025, 15:35:49) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251115+cu126 compiled for CUDA 12.6
Running Triton version 3.5.1
Sat Nov 15 15:43:03 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   30C    P0            115W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   31C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   31C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   29C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   31C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   31C    P0            115W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   31C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A             818      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A             819      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A             820      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A             821      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A             822      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A             823      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A             824      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A             825      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A             819      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A             820      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A             821      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A             822      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A             823      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A             824      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A             825      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2245 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2245 train_time:123ms step_avg:122.50ms
step:2/2245 train_time:162ms step_avg:81.01ms
step:3/2245 train_time:182ms step_avg:60.65ms
step:4/2245 train_time:238ms step_avg:59.53ms
step:5/2245 train_time:298ms step_avg:59.51ms
step:6/2245 train_time:356ms step_avg:59.33ms
step:7/2245 train_time:417ms step_avg:59.51ms
step:8/2245 train_time:475ms step_avg:59.43ms
step:9/2245 train_time:536ms step_avg:59.58ms
step:10/2245 train_time:595ms step_avg:59.53ms
step:11/2245 train_time:657ms step_avg:59.71ms
step:12/2245 train_time:716ms step_avg:59.65ms
step:13/2245 train_time:777ms step_avg:59.74ms
step:14/2245 train_time:836ms step_avg:59.70ms
step:15/2245 train_time:897ms step_avg:59.79ms
step:16/2245 train_time:957ms step_avg:59.79ms
step:17/2245 train_time:1022ms step_avg:60.09ms
step:18/2245 train_time:1086ms step_avg:60.34ms
step:19/2245 train_time:1152ms step_avg:60.62ms
step:20/2245 train_time:1213ms step_avg:60.65ms
step:21/2245 train_time:1276ms step_avg:60.76ms
step:22/2245 train_time:1335ms step_avg:60.69ms
step:23/2245 train_time:1397ms step_avg:60.73ms
step:24/2245 train_time:1457ms step_avg:60.69ms
step:25/2245 train_time:1519ms step_avg:60.74ms
step:26/2245 train_time:1578ms step_avg:60.69ms
step:27/2245 train_time:1639ms step_avg:60.71ms
step:28/2245 train_time:1698ms step_avg:60.66ms
step:29/2245 train_time:1759ms step_avg:60.67ms
step:30/2245 train_time:1818ms step_avg:60.61ms
step:31/2245 train_time:1880ms step_avg:60.64ms
step:32/2245 train_time:1940ms step_avg:60.62ms
step:33/2245 train_time:2003ms step_avg:60.70ms
step:34/2245 train_time:2066ms step_avg:60.75ms
step:35/2245 train_time:2129ms step_avg:60.82ms
step:36/2245 train_time:2190ms step_avg:60.82ms
step:37/2245 train_time:2252ms step_avg:60.86ms
step:38/2245 train_time:2312ms step_avg:60.84ms
step:39/2245 train_time:2374ms step_avg:60.87ms
step:40/2245 train_time:2433ms step_avg:60.84ms
step:41/2245 train_time:2495ms step_avg:60.85ms
step:42/2245 train_time:2554ms step_avg:60.82ms
step:43/2245 train_time:2617ms step_avg:60.86ms
step:44/2245 train_time:2676ms step_avg:60.82ms
step:45/2245 train_time:2737ms step_avg:60.83ms
step:46/2245 train_time:2796ms step_avg:60.79ms
step:47/2245 train_time:2858ms step_avg:60.80ms
step:48/2245 train_time:2917ms step_avg:60.78ms
step:49/2245 train_time:2980ms step_avg:60.83ms
step:50/2245 train_time:3042ms step_avg:60.85ms
step:51/2245 train_time:3107ms step_avg:60.92ms
step:52/2245 train_time:3167ms step_avg:60.91ms
step:53/2245 train_time:3230ms step_avg:60.94ms
step:54/2245 train_time:3290ms step_avg:60.92ms
step:55/2245 train_time:3351ms step_avg:60.93ms
step:56/2245 train_time:3410ms step_avg:60.90ms
step:57/2245 train_time:3472ms step_avg:60.90ms
step:58/2245 train_time:3531ms step_avg:60.88ms
step:59/2245 train_time:3593ms step_avg:60.90ms
step:60/2245 train_time:3652ms step_avg:60.87ms
step:61/2245 train_time:3714ms step_avg:60.88ms
step:62/2245 train_time:3773ms step_avg:60.86ms
step:63/2245 train_time:3835ms step_avg:60.87ms
step:64/2245 train_time:3894ms step_avg:60.85ms
step:65/2245 train_time:3957ms step_avg:60.88ms
step:66/2245 train_time:4018ms step_avg:60.88ms
step:67/2245 train_time:4081ms step_avg:60.91ms
step:68/2245 train_time:4141ms step_avg:60.89ms
step:69/2245 train_time:4204ms step_avg:60.93ms
step:70/2245 train_time:4265ms step_avg:60.92ms
step:71/2245 train_time:4326ms step_avg:60.93ms
step:72/2245 train_time:4386ms step_avg:60.91ms
step:73/2245 train_time:4447ms step_avg:60.92ms
step:74/2245 train_time:4508ms step_avg:60.91ms
step:75/2245 train_time:4569ms step_avg:60.92ms
step:76/2245 train_time:4629ms step_avg:60.90ms
step:77/2245 train_time:4689ms step_avg:60.90ms
step:78/2245 train_time:4749ms step_avg:60.88ms
step:79/2245 train_time:4810ms step_avg:60.88ms
step:80/2245 train_time:4870ms step_avg:60.87ms
step:81/2245 train_time:4931ms step_avg:60.88ms
step:82/2245 train_time:4992ms step_avg:60.87ms
step:83/2245 train_time:5054ms step_avg:60.89ms
step:84/2245 train_time:5114ms step_avg:60.88ms
step:85/2245 train_time:5176ms step_avg:60.90ms
step:86/2245 train_time:5237ms step_avg:60.89ms
step:87/2245 train_time:5299ms step_avg:60.90ms
step:88/2245 train_time:5359ms step_avg:60.89ms
step:89/2245 train_time:5421ms step_avg:60.91ms
step:90/2245 train_time:5481ms step_avg:60.90ms
step:91/2245 train_time:5542ms step_avg:60.90ms
step:92/2245 train_time:5602ms step_avg:60.89ms
step:93/2245 train_time:5665ms step_avg:60.91ms
step:94/2245 train_time:5724ms step_avg:60.90ms
step:95/2245 train_time:5785ms step_avg:60.90ms
step:96/2245 train_time:5845ms step_avg:60.89ms
step:97/2245 train_time:5907ms step_avg:60.90ms
step:98/2245 train_time:5967ms step_avg:60.89ms
step:99/2245 train_time:6029ms step_avg:60.90ms
step:100/2245 train_time:6088ms step_avg:60.88ms
step:101/2245 train_time:6149ms step_avg:60.89ms
step:102/2245 train_time:6209ms step_avg:60.87ms
step:103/2245 train_time:6271ms step_avg:60.88ms
step:104/2245 train_time:6331ms step_avg:60.87ms
step:105/2245 train_time:6393ms step_avg:60.88ms
step:106/2245 train_time:6453ms step_avg:60.87ms
step:107/2245 train_time:6514ms step_avg:60.88ms
step:108/2245 train_time:6574ms step_avg:60.87ms
step:109/2245 train_time:6635ms step_avg:60.87ms
step:110/2245 train_time:6695ms step_avg:60.86ms
step:111/2245 train_time:6756ms step_avg:60.87ms
step:112/2245 train_time:6816ms step_avg:60.86ms
step:113/2245 train_time:6878ms step_avg:60.87ms
step:114/2245 train_time:6938ms step_avg:60.86ms
step:115/2245 train_time:6999ms step_avg:60.86ms
step:116/2245 train_time:7059ms step_avg:60.86ms
step:117/2245 train_time:7121ms step_avg:60.86ms
step:118/2245 train_time:7181ms step_avg:60.86ms
step:119/2245 train_time:7243ms step_avg:60.87ms
step:120/2245 train_time:7303ms step_avg:60.86ms
step:121/2245 train_time:7365ms step_avg:60.87ms
step:122/2245 train_time:7425ms step_avg:60.86ms
step:123/2245 train_time:7485ms step_avg:60.86ms
step:124/2245 train_time:7545ms step_avg:60.85ms
step:125/2245 train_time:7606ms step_avg:60.85ms
step:126/2245 train_time:7665ms step_avg:60.84ms
step:127/2245 train_time:7727ms step_avg:60.84ms
step:128/2245 train_time:7786ms step_avg:60.83ms
step:129/2245 train_time:7848ms step_avg:60.84ms
step:130/2245 train_time:7907ms step_avg:60.82ms
step:131/2245 train_time:7968ms step_avg:60.82ms
step:132/2245 train_time:8027ms step_avg:60.81ms
step:133/2245 train_time:8089ms step_avg:60.82ms
step:134/2245 train_time:8148ms step_avg:60.80ms
step:135/2245 train_time:8209ms step_avg:60.81ms
step:136/2245 train_time:8268ms step_avg:60.80ms
step:137/2245 train_time:8330ms step_avg:60.80ms
step:138/2245 train_time:8389ms step_avg:60.79ms
step:139/2245 train_time:8451ms step_avg:60.80ms
step:140/2245 train_time:8511ms step_avg:60.79ms
step:141/2245 train_time:8573ms step_avg:60.80ms
step:142/2245 train_time:8632ms step_avg:60.79ms
step:143/2245 train_time:8693ms step_avg:60.79ms
step:144/2245 train_time:8753ms step_avg:60.78ms
step:145/2245 train_time:8814ms step_avg:60.79ms
step:146/2245 train_time:8873ms step_avg:60.77ms
step:147/2245 train_time:8934ms step_avg:60.78ms
step:148/2245 train_time:8993ms step_avg:60.77ms
step:149/2245 train_time:9055ms step_avg:60.77ms
step:150/2245 train_time:9114ms step_avg:60.76ms
step:151/2245 train_time:9176ms step_avg:60.77ms
step:152/2245 train_time:9235ms step_avg:60.76ms
step:153/2245 train_time:9297ms step_avg:60.77ms
step:154/2245 train_time:9357ms step_avg:60.76ms
step:155/2245 train_time:9420ms step_avg:60.77ms
step:156/2245 train_time:9480ms step_avg:60.77ms
step:157/2245 train_time:9543ms step_avg:60.78ms
step:158/2245 train_time:9603ms step_avg:60.78ms
step:159/2245 train_time:9665ms step_avg:60.79ms
step:160/2245 train_time:9725ms step_avg:60.78ms
step:161/2245 train_time:9786ms step_avg:60.78ms
step:162/2245 train_time:9845ms step_avg:60.77ms
step:163/2245 train_time:9907ms step_avg:60.78ms
step:164/2245 train_time:9966ms step_avg:60.77ms
step:165/2245 train_time:10027ms step_avg:60.77ms
step:166/2245 train_time:10086ms step_avg:60.76ms
step:167/2245 train_time:10148ms step_avg:60.76ms
step:168/2245 train_time:10207ms step_avg:60.76ms
step:169/2245 train_time:10269ms step_avg:60.76ms
step:170/2245 train_time:10328ms step_avg:60.75ms
step:171/2245 train_time:10389ms step_avg:60.76ms
step:172/2245 train_time:10448ms step_avg:60.75ms
step:173/2245 train_time:10511ms step_avg:60.76ms
step:174/2245 train_time:10570ms step_avg:60.75ms
step:175/2245 train_time:10632ms step_avg:60.75ms
step:176/2245 train_time:10691ms step_avg:60.75ms
step:177/2245 train_time:10752ms step_avg:60.75ms
step:178/2245 train_time:10812ms step_avg:60.74ms
step:179/2245 train_time:10873ms step_avg:60.74ms
step:180/2245 train_time:10932ms step_avg:60.73ms
step:181/2245 train_time:10993ms step_avg:60.73ms
step:182/2245 train_time:11052ms step_avg:60.73ms
step:183/2245 train_time:11113ms step_avg:60.73ms
step:184/2245 train_time:11172ms step_avg:60.72ms
step:185/2245 train_time:11234ms step_avg:60.72ms
step:186/2245 train_time:11292ms step_avg:60.71ms
step:187/2245 train_time:11354ms step_avg:60.71ms
step:188/2245 train_time:11413ms step_avg:60.71ms
step:189/2245 train_time:11475ms step_avg:60.71ms
step:190/2245 train_time:11534ms step_avg:60.71ms
step:191/2245 train_time:11596ms step_avg:60.71ms
step:192/2245 train_time:11656ms step_avg:60.71ms
step:193/2245 train_time:11717ms step_avg:60.71ms
step:194/2245 train_time:11777ms step_avg:60.70ms
step:195/2245 train_time:11838ms step_avg:60.71ms
step:196/2245 train_time:11898ms step_avg:60.70ms
step:197/2245 train_time:11960ms step_avg:60.71ms
step:198/2245 train_time:12020ms step_avg:60.71ms
step:199/2245 train_time:12081ms step_avg:60.71ms
step:200/2245 train_time:12141ms step_avg:60.70ms
step:201/2245 train_time:12202ms step_avg:60.71ms
step:202/2245 train_time:12262ms step_avg:60.70ms
step:203/2245 train_time:12323ms step_avg:60.71ms
step:204/2245 train_time:12383ms step_avg:60.70ms
step:205/2245 train_time:12444ms step_avg:60.70ms
step:206/2245 train_time:12504ms step_avg:60.70ms
step:207/2245 train_time:12566ms step_avg:60.71ms
step:208/2245 train_time:12625ms step_avg:60.70ms
step:209/2245 train_time:12688ms step_avg:60.71ms
step:210/2245 train_time:12747ms step_avg:60.70ms
step:211/2245 train_time:12809ms step_avg:60.71ms
step:212/2245 train_time:12868ms step_avg:60.70ms
step:213/2245 train_time:12929ms step_avg:60.70ms
step:214/2245 train_time:12988ms step_avg:60.69ms
step:215/2245 train_time:13050ms step_avg:60.70ms
step:216/2245 train_time:13109ms step_avg:60.69ms
step:217/2245 train_time:13171ms step_avg:60.70ms
step:218/2245 train_time:13230ms step_avg:60.69ms
step:219/2245 train_time:13292ms step_avg:60.69ms
step:220/2245 train_time:13351ms step_avg:60.69ms
step:221/2245 train_time:13413ms step_avg:60.69ms
step:222/2245 train_time:13473ms step_avg:60.69ms
step:223/2245 train_time:13535ms step_avg:60.69ms
step:224/2245 train_time:13594ms step_avg:60.69ms
step:225/2245 train_time:13656ms step_avg:60.69ms
step:226/2245 train_time:13715ms step_avg:60.69ms
step:227/2245 train_time:13777ms step_avg:60.69ms
step:228/2245 train_time:13836ms step_avg:60.69ms
step:229/2245 train_time:13898ms step_avg:60.69ms
step:230/2245 train_time:13958ms step_avg:60.69ms
step:231/2245 train_time:14020ms step_avg:60.69ms
step:232/2245 train_time:14080ms step_avg:60.69ms
step:233/2245 train_time:14141ms step_avg:60.69ms
step:234/2245 train_time:14201ms step_avg:60.69ms
step:235/2245 train_time:14263ms step_avg:60.69ms
step:236/2245 train_time:14323ms step_avg:60.69ms
step:237/2245 train_time:14385ms step_avg:60.70ms
step:238/2245 train_time:14445ms step_avg:60.69ms
step:239/2245 train_time:14507ms step_avg:60.70ms
step:240/2245 train_time:14567ms step_avg:60.70ms
step:241/2245 train_time:14628ms step_avg:60.70ms
step:242/2245 train_time:14687ms step_avg:60.69ms
step:243/2245 train_time:14748ms step_avg:60.69ms
step:244/2245 train_time:14808ms step_avg:60.69ms
step:245/2245 train_time:14869ms step_avg:60.69ms
step:246/2245 train_time:14928ms step_avg:60.68ms
step:247/2245 train_time:14989ms step_avg:60.68ms
step:248/2245 train_time:15048ms step_avg:60.68ms
step:249/2245 train_time:15110ms step_avg:60.68ms
step:250/2245 train_time:15170ms step_avg:60.68ms
step:250/2245 val_loss:4.0873 train_time:15232ms step_avg:60.93ms
step:251/2245 train_time:15251ms step_avg:60.76ms
step:252/2245 train_time:15291ms step_avg:60.68ms
step:253/2245 train_time:15360ms step_avg:60.71ms
step:254/2245 train_time:15423ms step_avg:60.72ms
step:255/2245 train_time:15487ms step_avg:60.73ms
step:256/2245 train_time:15547ms step_avg:60.73ms
step:257/2245 train_time:15609ms step_avg:60.73ms
step:258/2245 train_time:15667ms step_avg:60.72ms
step:259/2245 train_time:15728ms step_avg:60.73ms
step:260/2245 train_time:15787ms step_avg:60.72ms
step:261/2245 train_time:15848ms step_avg:60.72ms
step:262/2245 train_time:15906ms step_avg:60.71ms
step:263/2245 train_time:15967ms step_avg:60.71ms
step:264/2245 train_time:16025ms step_avg:60.70ms
step:265/2245 train_time:16086ms step_avg:60.70ms
step:266/2245 train_time:16144ms step_avg:60.69ms
step:267/2245 train_time:16206ms step_avg:60.70ms
step:268/2245 train_time:16266ms step_avg:60.69ms
step:269/2245 train_time:16330ms step_avg:60.71ms
step:270/2245 train_time:16393ms step_avg:60.71ms
step:271/2245 train_time:16456ms step_avg:60.72ms
step:272/2245 train_time:16516ms step_avg:60.72ms
step:273/2245 train_time:16578ms step_avg:60.73ms
step:274/2245 train_time:16637ms step_avg:60.72ms
step:275/2245 train_time:16698ms step_avg:60.72ms
step:276/2245 train_time:16757ms step_avg:60.71ms
step:277/2245 train_time:16817ms step_avg:60.71ms
step:278/2245 train_time:16876ms step_avg:60.71ms
step:279/2245 train_time:16937ms step_avg:60.71ms
step:280/2245 train_time:16996ms step_avg:60.70ms
step:281/2245 train_time:17057ms step_avg:60.70ms
step:282/2245 train_time:17116ms step_avg:60.70ms
step:283/2245 train_time:17177ms step_avg:60.70ms
step:284/2245 train_time:17236ms step_avg:60.69ms
step:285/2245 train_time:17298ms step_avg:60.70ms
step:286/2245 train_time:17358ms step_avg:60.69ms
step:287/2245 train_time:17420ms step_avg:60.70ms
step:288/2245 train_time:17479ms step_avg:60.69ms
step:289/2245 train_time:17542ms step_avg:60.70ms
step:290/2245 train_time:17602ms step_avg:60.70ms
step:291/2245 train_time:17663ms step_avg:60.70ms
step:292/2245 train_time:17722ms step_avg:60.69ms
step:293/2245 train_time:17783ms step_avg:60.69ms
step:294/2245 train_time:17842ms step_avg:60.69ms
step:295/2245 train_time:17903ms step_avg:60.69ms
step:296/2245 train_time:17962ms step_avg:60.68ms
step:297/2245 train_time:18023ms step_avg:60.68ms
step:298/2245 train_time:18082ms step_avg:60.68ms
step:299/2245 train_time:18144ms step_avg:60.68ms
step:300/2245 train_time:18203ms step_avg:60.68ms
step:301/2245 train_time:18265ms step_avg:60.68ms
step:302/2245 train_time:18325ms step_avg:60.68ms
step:303/2245 train_time:18387ms step_avg:60.68ms
step:304/2245 train_time:18447ms step_avg:60.68ms
step:305/2245 train_time:18509ms step_avg:60.68ms
step:306/2245 train_time:18568ms step_avg:60.68ms
step:307/2245 train_time:18630ms step_avg:60.69ms
step:308/2245 train_time:18690ms step_avg:60.68ms
step:309/2245 train_time:18753ms step_avg:60.69ms
step:310/2245 train_time:18813ms step_avg:60.69ms
step:311/2245 train_time:18874ms step_avg:60.69ms
step:312/2245 train_time:18933ms step_avg:60.68ms
step:313/2245 train_time:18994ms step_avg:60.68ms
step:314/2245 train_time:19054ms step_avg:60.68ms
step:315/2245 train_time:19114ms step_avg:60.68ms
step:316/2245 train_time:19173ms step_avg:60.68ms
step:317/2245 train_time:19236ms step_avg:60.68ms
step:318/2245 train_time:19295ms step_avg:60.68ms
step:319/2245 train_time:19357ms step_avg:60.68ms
step:320/2245 train_time:19416ms step_avg:60.67ms
step:321/2245 train_time:19477ms step_avg:60.68ms
step:322/2245 train_time:19536ms step_avg:60.67ms
step:323/2245 train_time:19598ms step_avg:60.67ms
step:324/2245 train_time:19658ms step_avg:60.67ms
step:325/2245 train_time:19719ms step_avg:60.67ms
step:326/2245 train_time:19778ms step_avg:60.67ms
step:327/2245 train_time:19840ms step_avg:60.67ms
step:328/2245 train_time:19899ms step_avg:60.67ms
step:329/2245 train_time:19960ms step_avg:60.67ms
step:330/2245 train_time:20019ms step_avg:60.66ms
step:331/2245 train_time:20081ms step_avg:60.67ms
step:332/2245 train_time:20140ms step_avg:60.66ms
step:333/2245 train_time:20201ms step_avg:60.66ms
step:334/2245 train_time:20261ms step_avg:60.66ms
step:335/2245 train_time:20322ms step_avg:60.66ms
step:336/2245 train_time:20381ms step_avg:60.66ms
step:337/2245 train_time:20443ms step_avg:60.66ms
step:338/2245 train_time:20502ms step_avg:60.66ms
step:339/2245 train_time:20564ms step_avg:60.66ms
step:340/2245 train_time:20623ms step_avg:60.66ms
step:341/2245 train_time:20685ms step_avg:60.66ms
step:342/2245 train_time:20744ms step_avg:60.66ms
step:343/2245 train_time:20806ms step_avg:60.66ms
step:344/2245 train_time:20865ms step_avg:60.65ms
step:345/2245 train_time:20927ms step_avg:60.66ms
step:346/2245 train_time:20986ms step_avg:60.65ms
step:347/2245 train_time:21048ms step_avg:60.66ms
step:348/2245 train_time:21108ms step_avg:60.65ms
step:349/2245 train_time:21170ms step_avg:60.66ms
step:350/2245 train_time:21229ms step_avg:60.65ms
step:351/2245 train_time:21291ms step_avg:60.66ms
step:352/2245 train_time:21351ms step_avg:60.66ms
step:353/2245 train_time:21412ms step_avg:60.66ms
step:354/2245 train_time:21472ms step_avg:60.65ms
step:355/2245 train_time:21534ms step_avg:60.66ms
step:356/2245 train_time:21593ms step_avg:60.65ms
step:357/2245 train_time:21654ms step_avg:60.66ms
step:358/2245 train_time:21715ms step_avg:60.66ms
step:359/2245 train_time:21776ms step_avg:60.66ms
step:360/2245 train_time:21835ms step_avg:60.65ms
step:361/2245 train_time:21897ms step_avg:60.66ms
step:362/2245 train_time:21956ms step_avg:60.65ms
step:363/2245 train_time:22017ms step_avg:60.65ms
step:364/2245 train_time:22076ms step_avg:60.65ms
step:365/2245 train_time:22136ms step_avg:60.65ms
step:366/2245 train_time:22195ms step_avg:60.64ms
step:367/2245 train_time:22257ms step_avg:60.65ms
step:368/2245 train_time:22315ms step_avg:60.64ms
step:369/2245 train_time:22377ms step_avg:60.64ms
step:370/2245 train_time:22436ms step_avg:60.64ms
step:371/2245 train_time:22497ms step_avg:60.64ms
step:372/2245 train_time:22556ms step_avg:60.63ms
step:373/2245 train_time:22617ms step_avg:60.64ms
step:374/2245 train_time:22676ms step_avg:60.63ms
step:375/2245 train_time:22738ms step_avg:60.63ms
step:376/2245 train_time:22797ms step_avg:60.63ms
step:377/2245 train_time:22858ms step_avg:60.63ms
step:378/2245 train_time:22917ms step_avg:60.63ms
step:379/2245 train_time:22978ms step_avg:60.63ms
step:380/2245 train_time:23037ms step_avg:60.62ms
step:381/2245 train_time:23099ms step_avg:60.63ms
step:382/2245 train_time:23158ms step_avg:60.62ms
step:383/2245 train_time:23219ms step_avg:60.62ms
step:384/2245 train_time:23278ms step_avg:60.62ms
step:385/2245 train_time:23339ms step_avg:60.62ms
step:386/2245 train_time:23398ms step_avg:60.62ms
step:387/2245 train_time:23459ms step_avg:60.62ms
step:388/2245 train_time:23518ms step_avg:60.61ms
step:389/2245 train_time:23579ms step_avg:60.61ms
step:390/2245 train_time:23639ms step_avg:60.61ms
step:391/2245 train_time:23700ms step_avg:60.61ms
step:392/2245 train_time:23760ms step_avg:60.61ms
step:393/2245 train_time:23821ms step_avg:60.61ms
step:394/2245 train_time:23880ms step_avg:60.61ms
step:395/2245 train_time:23941ms step_avg:60.61ms
step:396/2245 train_time:24000ms step_avg:60.61ms
step:397/2245 train_time:24061ms step_avg:60.61ms
step:398/2245 train_time:24120ms step_avg:60.60ms
step:399/2245 train_time:24181ms step_avg:60.60ms
step:400/2245 train_time:24240ms step_avg:60.60ms
step:401/2245 train_time:24301ms step_avg:60.60ms
step:402/2245 train_time:24360ms step_avg:60.60ms
step:403/2245 train_time:24422ms step_avg:60.60ms
step:404/2245 train_time:24481ms step_avg:60.60ms
step:405/2245 train_time:24542ms step_avg:60.60ms
step:406/2245 train_time:24601ms step_avg:60.59ms
step:407/2245 train_time:24663ms step_avg:60.60ms
step:408/2245 train_time:24722ms step_avg:60.59ms
step:409/2245 train_time:24784ms step_avg:60.60ms
step:410/2245 train_time:24843ms step_avg:60.59ms
step:411/2245 train_time:24904ms step_avg:60.59ms
step:412/2245 train_time:24964ms step_avg:60.59ms
step:413/2245 train_time:25026ms step_avg:60.60ms
step:414/2245 train_time:25085ms step_avg:60.59ms
step:415/2245 train_time:25146ms step_avg:60.59ms
step:416/2245 train_time:25206ms step_avg:60.59ms
step:417/2245 train_time:25267ms step_avg:60.59ms
step:418/2245 train_time:25327ms step_avg:60.59ms
step:419/2245 train_time:25389ms step_avg:60.60ms
step:420/2245 train_time:25449ms step_avg:60.59ms
step:421/2245 train_time:25511ms step_avg:60.60ms
step:422/2245 train_time:25571ms step_avg:60.59ms
step:423/2245 train_time:25632ms step_avg:60.60ms
step:424/2245 train_time:25692ms step_avg:60.59ms
step:425/2245 train_time:25754ms step_avg:60.60ms
step:426/2245 train_time:25814ms step_avg:60.60ms
step:427/2245 train_time:25875ms step_avg:60.60ms
step:428/2245 train_time:25935ms step_avg:60.60ms
step:429/2245 train_time:25996ms step_avg:60.60ms
step:430/2245 train_time:26055ms step_avg:60.59ms
step:431/2245 train_time:26117ms step_avg:60.60ms
step:432/2245 train_time:26176ms step_avg:60.59ms
step:433/2245 train_time:26237ms step_avg:60.59ms
step:434/2245 train_time:26297ms step_avg:60.59ms
step:435/2245 train_time:26358ms step_avg:60.59ms
step:436/2245 train_time:26417ms step_avg:60.59ms
step:437/2245 train_time:26478ms step_avg:60.59ms
step:438/2245 train_time:26537ms step_avg:60.59ms
step:439/2245 train_time:26599ms step_avg:60.59ms
step:440/2245 train_time:26657ms step_avg:60.58ms
step:441/2245 train_time:26718ms step_avg:60.59ms
step:442/2245 train_time:26777ms step_avg:60.58ms
step:443/2245 train_time:26839ms step_avg:60.58ms
step:444/2245 train_time:26899ms step_avg:60.58ms
step:445/2245 train_time:26960ms step_avg:60.58ms
step:446/2245 train_time:27019ms step_avg:60.58ms
step:447/2245 train_time:27080ms step_avg:60.58ms
step:448/2245 train_time:27139ms step_avg:60.58ms
step:449/2245 train_time:27201ms step_avg:60.58ms
step:450/2245 train_time:27259ms step_avg:60.58ms
step:451/2245 train_time:27320ms step_avg:60.58ms
step:452/2245 train_time:27378ms step_avg:60.57ms
step:453/2245 train_time:27440ms step_avg:60.57ms
step:454/2245 train_time:27499ms step_avg:60.57ms
step:455/2245 train_time:27560ms step_avg:60.57ms
step:456/2245 train_time:27619ms step_avg:60.57ms
step:457/2245 train_time:27680ms step_avg:60.57ms
step:458/2245 train_time:27740ms step_avg:60.57ms
step:459/2245 train_time:27801ms step_avg:60.57ms
step:460/2245 train_time:27860ms step_avg:60.57ms
step:461/2245 train_time:27921ms step_avg:60.57ms
step:462/2245 train_time:27980ms step_avg:60.56ms
step:463/2245 train_time:28041ms step_avg:60.56ms
step:464/2245 train_time:28100ms step_avg:60.56ms
step:465/2245 train_time:28161ms step_avg:60.56ms
step:466/2245 train_time:28220ms step_avg:60.56ms
step:467/2245 train_time:28282ms step_avg:60.56ms
step:468/2245 train_time:28340ms step_avg:60.56ms
step:469/2245 train_time:28401ms step_avg:60.56ms
step:470/2245 train_time:28461ms step_avg:60.55ms
step:471/2245 train_time:28522ms step_avg:60.56ms
step:472/2245 train_time:28581ms step_avg:60.55ms
step:473/2245 train_time:28642ms step_avg:60.55ms
step:474/2245 train_time:28701ms step_avg:60.55ms
step:475/2245 train_time:28763ms step_avg:60.55ms
step:476/2245 train_time:28823ms step_avg:60.55ms
step:477/2245 train_time:28885ms step_avg:60.56ms
step:478/2245 train_time:28944ms step_avg:60.55ms
step:479/2245 train_time:29006ms step_avg:60.56ms
step:480/2245 train_time:29065ms step_avg:60.55ms
step:481/2245 train_time:29126ms step_avg:60.55ms
step:482/2245 train_time:29186ms step_avg:60.55ms
step:483/2245 train_time:29247ms step_avg:60.55ms
step:484/2245 train_time:29306ms step_avg:60.55ms
step:485/2245 train_time:29368ms step_avg:60.55ms
step:486/2245 train_time:29428ms step_avg:60.55ms
step:487/2245 train_time:29489ms step_avg:60.55ms
step:488/2245 train_time:29549ms step_avg:60.55ms
step:489/2245 train_time:29611ms step_avg:60.55ms
step:490/2245 train_time:29670ms step_avg:60.55ms
step:491/2245 train_time:29732ms step_avg:60.55ms
step:492/2245 train_time:29791ms step_avg:60.55ms
step:493/2245 train_time:29852ms step_avg:60.55ms
step:494/2245 train_time:29911ms step_avg:60.55ms
step:495/2245 train_time:29973ms step_avg:60.55ms
step:496/2245 train_time:30032ms step_avg:60.55ms
step:497/2245 train_time:30093ms step_avg:60.55ms
step:498/2245 train_time:30152ms step_avg:60.55ms
step:499/2245 train_time:30213ms step_avg:60.55ms
step:500/2245 train_time:30273ms step_avg:60.55ms
step:500/2245 val_loss:3.8238 train_time:30335ms step_avg:60.67ms
step:501/2245 train_time:30354ms step_avg:60.59ms
step:502/2245 train_time:30396ms step_avg:60.55ms
step:503/2245 train_time:30460ms step_avg:60.56ms
step:504/2245 train_time:30521ms step_avg:60.56ms
step:505/2245 train_time:30583ms step_avg:60.56ms
step:506/2245 train_time:30643ms step_avg:60.56ms
step:507/2245 train_time:30704ms step_avg:60.56ms
step:508/2245 train_time:30763ms step_avg:60.56ms
step:509/2245 train_time:30824ms step_avg:60.56ms
step:510/2245 train_time:30882ms step_avg:60.55ms
step:511/2245 train_time:30943ms step_avg:60.55ms
step:512/2245 train_time:31002ms step_avg:60.55ms
step:513/2245 train_time:31062ms step_avg:60.55ms
step:514/2245 train_time:31121ms step_avg:60.55ms
step:515/2245 train_time:31181ms step_avg:60.55ms
step:516/2245 train_time:31240ms step_avg:60.54ms
step:517/2245 train_time:31303ms step_avg:60.55ms
step:518/2245 train_time:31364ms step_avg:60.55ms
step:519/2245 train_time:31427ms step_avg:60.55ms
step:520/2245 train_time:31487ms step_avg:60.55ms
step:521/2245 train_time:31550ms step_avg:60.56ms
step:522/2245 train_time:31609ms step_avg:60.55ms
step:523/2245 train_time:31671ms step_avg:60.56ms
step:524/2245 train_time:31731ms step_avg:60.55ms
step:525/2245 train_time:31791ms step_avg:60.56ms
step:526/2245 train_time:31850ms step_avg:60.55ms
step:527/2245 train_time:31912ms step_avg:60.55ms
step:528/2245 train_time:31971ms step_avg:60.55ms
step:529/2245 train_time:32033ms step_avg:60.55ms
step:530/2245 train_time:32092ms step_avg:60.55ms
step:531/2245 train_time:32153ms step_avg:60.55ms
step:532/2245 train_time:32212ms step_avg:60.55ms
step:533/2245 train_time:32274ms step_avg:60.55ms
step:534/2245 train_time:32333ms step_avg:60.55ms
step:535/2245 train_time:32394ms step_avg:60.55ms
step:536/2245 train_time:32454ms step_avg:60.55ms
step:537/2245 train_time:32515ms step_avg:60.55ms
step:538/2245 train_time:32575ms step_avg:60.55ms
step:539/2245 train_time:32636ms step_avg:60.55ms
step:540/2245 train_time:32695ms step_avg:60.55ms
step:541/2245 train_time:32756ms step_avg:60.55ms
step:542/2245 train_time:32815ms step_avg:60.54ms
step:543/2245 train_time:32876ms step_avg:60.55ms
step:544/2245 train_time:32936ms step_avg:60.54ms
step:545/2245 train_time:32996ms step_avg:60.54ms
step:546/2245 train_time:33055ms step_avg:60.54ms
step:547/2245 train_time:33116ms step_avg:60.54ms
step:548/2245 train_time:33175ms step_avg:60.54ms
step:549/2245 train_time:33236ms step_avg:60.54ms
step:550/2245 train_time:33295ms step_avg:60.54ms
step:551/2245 train_time:33356ms step_avg:60.54ms
step:552/2245 train_time:33416ms step_avg:60.54ms
step:553/2245 train_time:33476ms step_avg:60.54ms
step:554/2245 train_time:33536ms step_avg:60.53ms
step:555/2245 train_time:33597ms step_avg:60.54ms
step:556/2245 train_time:33656ms step_avg:60.53ms
step:557/2245 train_time:33717ms step_avg:60.53ms
step:558/2245 train_time:33776ms step_avg:60.53ms
step:559/2245 train_time:33837ms step_avg:60.53ms
step:560/2245 train_time:33896ms step_avg:60.53ms
step:561/2245 train_time:33957ms step_avg:60.53ms
step:562/2245 train_time:34016ms step_avg:60.53ms
step:563/2245 train_time:34077ms step_avg:60.53ms
step:564/2245 train_time:34136ms step_avg:60.52ms
step:565/2245 train_time:34197ms step_avg:60.53ms
step:566/2245 train_time:34256ms step_avg:60.52ms
step:567/2245 train_time:34317ms step_avg:60.52ms
step:568/2245 train_time:34376ms step_avg:60.52ms
step:569/2245 train_time:34438ms step_avg:60.52ms
step:570/2245 train_time:34496ms step_avg:60.52ms
step:571/2245 train_time:34558ms step_avg:60.52ms
step:572/2245 train_time:34616ms step_avg:60.52ms
step:573/2245 train_time:34677ms step_avg:60.52ms
step:574/2245 train_time:34736ms step_avg:60.52ms
step:575/2245 train_time:34798ms step_avg:60.52ms
step:576/2245 train_time:34856ms step_avg:60.51ms
step:577/2245 train_time:34918ms step_avg:60.52ms
step:578/2245 train_time:34977ms step_avg:60.51ms
step:579/2245 train_time:35038ms step_avg:60.51ms
step:580/2245 train_time:35097ms step_avg:60.51ms
step:581/2245 train_time:35159ms step_avg:60.51ms
step:582/2245 train_time:35218ms step_avg:60.51ms
step:583/2245 train_time:35279ms step_avg:60.51ms
step:584/2245 train_time:35338ms step_avg:60.51ms
step:585/2245 train_time:35399ms step_avg:60.51ms
step:586/2245 train_time:35458ms step_avg:60.51ms
step:587/2245 train_time:35520ms step_avg:60.51ms
step:588/2245 train_time:35578ms step_avg:60.51ms
step:589/2245 train_time:35639ms step_avg:60.51ms
step:590/2245 train_time:35699ms step_avg:60.51ms
step:591/2245 train_time:35761ms step_avg:60.51ms
step:592/2245 train_time:35820ms step_avg:60.51ms
step:593/2245 train_time:35881ms step_avg:60.51ms
step:594/2245 train_time:35940ms step_avg:60.51ms
step:595/2245 train_time:36001ms step_avg:60.51ms
step:596/2245 train_time:36060ms step_avg:60.50ms
step:597/2245 train_time:36121ms step_avg:60.50ms
step:598/2245 train_time:36180ms step_avg:60.50ms
step:599/2245 train_time:36242ms step_avg:60.50ms
step:600/2245 train_time:36301ms step_avg:60.50ms
step:601/2245 train_time:36362ms step_avg:60.50ms
step:602/2245 train_time:36421ms step_avg:60.50ms
step:603/2245 train_time:36483ms step_avg:60.50ms
step:604/2245 train_time:36542ms step_avg:60.50ms
step:605/2245 train_time:36603ms step_avg:60.50ms
step:606/2245 train_time:36663ms step_avg:60.50ms
step:607/2245 train_time:36724ms step_avg:60.50ms
step:608/2245 train_time:36784ms step_avg:60.50ms
step:609/2245 train_time:36846ms step_avg:60.50ms
step:610/2245 train_time:36905ms step_avg:60.50ms
step:611/2245 train_time:36967ms step_avg:60.50ms
step:612/2245 train_time:37026ms step_avg:60.50ms
step:613/2245 train_time:37088ms step_avg:60.50ms
step:614/2245 train_time:37147ms step_avg:60.50ms
step:615/2245 train_time:37209ms step_avg:60.50ms
step:616/2245 train_time:37268ms step_avg:60.50ms
step:617/2245 train_time:37330ms step_avg:60.50ms
step:618/2245 train_time:37390ms step_avg:60.50ms
step:619/2245 train_time:37451ms step_avg:60.50ms
step:620/2245 train_time:38043ms step_avg:61.36ms
step:621/2245 train_time:38101ms step_avg:61.35ms
step:622/2245 train_time:38159ms step_avg:61.35ms
step:623/2245 train_time:38219ms step_avg:61.35ms
step:624/2245 train_time:38277ms step_avg:61.34ms
step:625/2245 train_time:38337ms step_avg:61.34ms
step:626/2245 train_time:38395ms step_avg:61.33ms
step:627/2245 train_time:38456ms step_avg:61.33ms
step:628/2245 train_time:38514ms step_avg:61.33ms
step:629/2245 train_time:38574ms step_avg:61.33ms
step:630/2245 train_time:38632ms step_avg:61.32ms
step:631/2245 train_time:38692ms step_avg:61.32ms
step:632/2245 train_time:38751ms step_avg:61.31ms
step:633/2245 train_time:38811ms step_avg:61.31ms
step:634/2245 train_time:38870ms step_avg:61.31ms
step:635/2245 train_time:38934ms step_avg:61.31ms
step:636/2245 train_time:38999ms step_avg:61.32ms
step:637/2245 train_time:39063ms step_avg:61.32ms
step:638/2245 train_time:39123ms step_avg:61.32ms
step:639/2245 train_time:39185ms step_avg:61.32ms
step:640/2245 train_time:39244ms step_avg:61.32ms
step:641/2245 train_time:39306ms step_avg:61.32ms
step:642/2245 train_time:39366ms step_avg:61.32ms
step:643/2245 train_time:39427ms step_avg:61.32ms
step:644/2245 train_time:39486ms step_avg:61.31ms
step:645/2245 train_time:39547ms step_avg:61.31ms
step:646/2245 train_time:39606ms step_avg:61.31ms
step:647/2245 train_time:39667ms step_avg:61.31ms
step:648/2245 train_time:39726ms step_avg:61.31ms
step:649/2245 train_time:39787ms step_avg:61.31ms
step:650/2245 train_time:39846ms step_avg:61.30ms
step:651/2245 train_time:39909ms step_avg:61.30ms
step:652/2245 train_time:39971ms step_avg:61.31ms
step:653/2245 train_time:40034ms step_avg:61.31ms
step:654/2245 train_time:40095ms step_avg:61.31ms
step:655/2245 train_time:40156ms step_avg:61.31ms
step:656/2245 train_time:40216ms step_avg:61.30ms
step:657/2245 train_time:40277ms step_avg:61.30ms
step:658/2245 train_time:40336ms step_avg:61.30ms
step:659/2245 train_time:40397ms step_avg:61.30ms
step:660/2245 train_time:40456ms step_avg:61.30ms
step:661/2245 train_time:40517ms step_avg:61.30ms
step:662/2245 train_time:40576ms step_avg:61.29ms
step:663/2245 train_time:40638ms step_avg:61.29ms
step:664/2245 train_time:40696ms step_avg:61.29ms
step:665/2245 train_time:40758ms step_avg:61.29ms
step:666/2245 train_time:40818ms step_avg:61.29ms
step:667/2245 train_time:40879ms step_avg:61.29ms
step:668/2245 train_time:40939ms step_avg:61.29ms
step:669/2245 train_time:41001ms step_avg:61.29ms
step:670/2245 train_time:41061ms step_avg:61.28ms
step:671/2245 train_time:41123ms step_avg:61.29ms
step:672/2245 train_time:41183ms step_avg:61.28ms
step:673/2245 train_time:41246ms step_avg:61.29ms
step:674/2245 train_time:41305ms step_avg:61.28ms
step:675/2245 train_time:41367ms step_avg:61.28ms
step:676/2245 train_time:41427ms step_avg:61.28ms
step:677/2245 train_time:41488ms step_avg:61.28ms
step:678/2245 train_time:41547ms step_avg:61.28ms
step:679/2245 train_time:41609ms step_avg:61.28ms
step:680/2245 train_time:41669ms step_avg:61.28ms
step:681/2245 train_time:41730ms step_avg:61.28ms
step:682/2245 train_time:41790ms step_avg:61.28ms
step:683/2245 train_time:41851ms step_avg:61.28ms
step:684/2245 train_time:41911ms step_avg:61.27ms
step:685/2245 train_time:41974ms step_avg:61.28ms
step:686/2245 train_time:42034ms step_avg:61.27ms
step:687/2245 train_time:42095ms step_avg:61.27ms
step:688/2245 train_time:42154ms step_avg:61.27ms
step:689/2245 train_time:42216ms step_avg:61.27ms
step:690/2245 train_time:42275ms step_avg:61.27ms
step:691/2245 train_time:42336ms step_avg:61.27ms
step:692/2245 train_time:42395ms step_avg:61.26ms
step:693/2245 train_time:42456ms step_avg:61.26ms
step:694/2245 train_time:42515ms step_avg:61.26ms
step:695/2245 train_time:42576ms step_avg:61.26ms
step:696/2245 train_time:42635ms step_avg:61.26ms
step:697/2245 train_time:42696ms step_avg:61.26ms
step:698/2245 train_time:42755ms step_avg:61.25ms
step:699/2245 train_time:42817ms step_avg:61.25ms
step:700/2245 train_time:42876ms step_avg:61.25ms
step:701/2245 train_time:42937ms step_avg:61.25ms
step:702/2245 train_time:42997ms step_avg:61.25ms
step:703/2245 train_time:43058ms step_avg:61.25ms
step:704/2245 train_time:43117ms step_avg:61.25ms
step:705/2245 train_time:43179ms step_avg:61.25ms
step:706/2245 train_time:43238ms step_avg:61.24ms
step:707/2245 train_time:43299ms step_avg:61.24ms
step:708/2245 train_time:43358ms step_avg:61.24ms
step:709/2245 train_time:43420ms step_avg:61.24ms
step:710/2245 train_time:43479ms step_avg:61.24ms
step:711/2245 train_time:43540ms step_avg:61.24ms
step:712/2245 train_time:43599ms step_avg:61.24ms
step:713/2245 train_time:43661ms step_avg:61.24ms
step:714/2245 train_time:43720ms step_avg:61.23ms
step:715/2245 train_time:43782ms step_avg:61.23ms
step:716/2245 train_time:43841ms step_avg:61.23ms
step:717/2245 train_time:43903ms step_avg:61.23ms
step:718/2245 train_time:43963ms step_avg:61.23ms
step:719/2245 train_time:44024ms step_avg:61.23ms
step:720/2245 train_time:44083ms step_avg:61.23ms
step:721/2245 train_time:44146ms step_avg:61.23ms
step:722/2245 train_time:44206ms step_avg:61.23ms
step:723/2245 train_time:44268ms step_avg:61.23ms
step:724/2245 train_time:44327ms step_avg:61.23ms
step:725/2245 train_time:44390ms step_avg:61.23ms
step:726/2245 train_time:44450ms step_avg:61.23ms
step:727/2245 train_time:44511ms step_avg:61.23ms
step:728/2245 train_time:44571ms step_avg:61.22ms
step:729/2245 train_time:44633ms step_avg:61.22ms
step:730/2245 train_time:44692ms step_avg:61.22ms
step:731/2245 train_time:44754ms step_avg:61.22ms
step:732/2245 train_time:44813ms step_avg:61.22ms
step:733/2245 train_time:44874ms step_avg:61.22ms
step:734/2245 train_time:44934ms step_avg:61.22ms
step:735/2245 train_time:44995ms step_avg:61.22ms
step:736/2245 train_time:45055ms step_avg:61.22ms
step:737/2245 train_time:45117ms step_avg:61.22ms
step:738/2245 train_time:45176ms step_avg:61.21ms
step:739/2245 train_time:45238ms step_avg:61.22ms
step:740/2245 train_time:45298ms step_avg:61.21ms
step:741/2245 train_time:45360ms step_avg:61.21ms
step:742/2245 train_time:45421ms step_avg:61.21ms
step:743/2245 train_time:45483ms step_avg:61.22ms
step:744/2245 train_time:45543ms step_avg:61.21ms
step:745/2245 train_time:45605ms step_avg:61.22ms
step:746/2245 train_time:45666ms step_avg:61.21ms
step:747/2245 train_time:45729ms step_avg:61.22ms
step:748/2245 train_time:45790ms step_avg:61.22ms
step:749/2245 train_time:45853ms step_avg:61.22ms
step:750/2245 train_time:45913ms step_avg:61.22ms
step:750/2245 val_loss:3.6765 train_time:45976ms step_avg:61.30ms
step:751/2245 train_time:45997ms step_avg:61.25ms
step:752/2245 train_time:46040ms step_avg:61.22ms
step:753/2245 train_time:46105ms step_avg:61.23ms
step:754/2245 train_time:46167ms step_avg:61.23ms
step:755/2245 train_time:46230ms step_avg:61.23ms
step:756/2245 train_time:46290ms step_avg:61.23ms
step:757/2245 train_time:46352ms step_avg:61.23ms
step:758/2245 train_time:46411ms step_avg:61.23ms
step:759/2245 train_time:46472ms step_avg:61.23ms
step:760/2245 train_time:46531ms step_avg:61.23ms
step:761/2245 train_time:46592ms step_avg:61.23ms
step:762/2245 train_time:46651ms step_avg:61.22ms
step:763/2245 train_time:46712ms step_avg:61.22ms
step:764/2245 train_time:46771ms step_avg:61.22ms
step:765/2245 train_time:46832ms step_avg:61.22ms
step:766/2245 train_time:46896ms step_avg:61.22ms
step:767/2245 train_time:46962ms step_avg:61.23ms
step:768/2245 train_time:47024ms step_avg:61.23ms
step:769/2245 train_time:47088ms step_avg:61.23ms
step:770/2245 train_time:47149ms step_avg:61.23ms
step:771/2245 train_time:47212ms step_avg:61.23ms
step:772/2245 train_time:47272ms step_avg:61.23ms
step:773/2245 train_time:47333ms step_avg:61.23ms
step:774/2245 train_time:47393ms step_avg:61.23ms
step:775/2245 train_time:47454ms step_avg:61.23ms
step:776/2245 train_time:47513ms step_avg:61.23ms
step:777/2245 train_time:47574ms step_avg:61.23ms
step:778/2245 train_time:47633ms step_avg:61.22ms
step:779/2245 train_time:47694ms step_avg:61.22ms
step:780/2245 train_time:47753ms step_avg:61.22ms
step:781/2245 train_time:47816ms step_avg:61.22ms
step:782/2245 train_time:47876ms step_avg:61.22ms
step:783/2245 train_time:47939ms step_avg:61.23ms
step:784/2245 train_time:48001ms step_avg:61.23ms
step:785/2245 train_time:48064ms step_avg:61.23ms
step:786/2245 train_time:48124ms step_avg:61.23ms
step:787/2245 train_time:48187ms step_avg:61.23ms
step:788/2245 train_time:48248ms step_avg:61.23ms
step:789/2245 train_time:48311ms step_avg:61.23ms
step:790/2245 train_time:48371ms step_avg:61.23ms
step:791/2245 train_time:48432ms step_avg:61.23ms
step:792/2245 train_time:48492ms step_avg:61.23ms
step:793/2245 train_time:48553ms step_avg:61.23ms
step:794/2245 train_time:48612ms step_avg:61.22ms
step:795/2245 train_time:48673ms step_avg:61.22ms
step:796/2245 train_time:48733ms step_avg:61.22ms
step:797/2245 train_time:48795ms step_avg:61.22ms
step:798/2245 train_time:48855ms step_avg:61.22ms
step:799/2245 train_time:48919ms step_avg:61.22ms
step:800/2245 train_time:48979ms step_avg:61.22ms
step:801/2245 train_time:49041ms step_avg:61.22ms
step:802/2245 train_time:49101ms step_avg:61.22ms
step:803/2245 train_time:49164ms step_avg:61.23ms
step:804/2245 train_time:49225ms step_avg:61.22ms
step:805/2245 train_time:49287ms step_avg:61.23ms
step:806/2245 train_time:49347ms step_avg:61.22ms
step:807/2245 train_time:49409ms step_avg:61.23ms
step:808/2245 train_time:49469ms step_avg:61.22ms
step:809/2245 train_time:49531ms step_avg:61.22ms
step:810/2245 train_time:49590ms step_avg:61.22ms
step:811/2245 train_time:49652ms step_avg:61.22ms
step:812/2245 train_time:49712ms step_avg:61.22ms
step:813/2245 train_time:49774ms step_avg:61.22ms
step:814/2245 train_time:49834ms step_avg:61.22ms
step:815/2245 train_time:49897ms step_avg:61.22ms
step:816/2245 train_time:49957ms step_avg:61.22ms
step:817/2245 train_time:50019ms step_avg:61.22ms
step:818/2245 train_time:50080ms step_avg:61.22ms
step:819/2245 train_time:50142ms step_avg:61.22ms
step:820/2245 train_time:50203ms step_avg:61.22ms
step:821/2245 train_time:50265ms step_avg:61.22ms
step:822/2245 train_time:50325ms step_avg:61.22ms
step:823/2245 train_time:50387ms step_avg:61.22ms
step:824/2245 train_time:50447ms step_avg:61.22ms
step:825/2245 train_time:50510ms step_avg:61.22ms
step:826/2245 train_time:50570ms step_avg:61.22ms
step:827/2245 train_time:50632ms step_avg:61.22ms
step:828/2245 train_time:50692ms step_avg:61.22ms
step:829/2245 train_time:50754ms step_avg:61.22ms
step:830/2245 train_time:50814ms step_avg:61.22ms
step:831/2245 train_time:50876ms step_avg:61.22ms
step:832/2245 train_time:50936ms step_avg:61.22ms
step:833/2245 train_time:50999ms step_avg:61.22ms
step:834/2245 train_time:51059ms step_avg:61.22ms
step:835/2245 train_time:51121ms step_avg:61.22ms
step:836/2245 train_time:51183ms step_avg:61.22ms
step:837/2245 train_time:51246ms step_avg:61.23ms
step:838/2245 train_time:51305ms step_avg:61.22ms
step:839/2245 train_time:51368ms step_avg:61.22ms
step:840/2245 train_time:51427ms step_avg:61.22ms
step:841/2245 train_time:51490ms step_avg:61.22ms
step:842/2245 train_time:51550ms step_avg:61.22ms
step:843/2245 train_time:51613ms step_avg:61.23ms
step:844/2245 train_time:51672ms step_avg:61.22ms
step:845/2245 train_time:51735ms step_avg:61.22ms
step:846/2245 train_time:51794ms step_avg:61.22ms
step:847/2245 train_time:51857ms step_avg:61.22ms
step:848/2245 train_time:51917ms step_avg:61.22ms
step:849/2245 train_time:51979ms step_avg:61.22ms
step:850/2245 train_time:52038ms step_avg:61.22ms
step:851/2245 train_time:52101ms step_avg:61.22ms
step:852/2245 train_time:52162ms step_avg:61.22ms
step:853/2245 train_time:52224ms step_avg:61.22ms
step:854/2245 train_time:52284ms step_avg:61.22ms
step:855/2245 train_time:52346ms step_avg:61.22ms
step:856/2245 train_time:52406ms step_avg:61.22ms
step:857/2245 train_time:52468ms step_avg:61.22ms
step:858/2245 train_time:52528ms step_avg:61.22ms
step:859/2245 train_time:52591ms step_avg:61.22ms
step:860/2245 train_time:52651ms step_avg:61.22ms
step:861/2245 train_time:52714ms step_avg:61.22ms
step:862/2245 train_time:52773ms step_avg:61.22ms
step:863/2245 train_time:52835ms step_avg:61.22ms
step:864/2245 train_time:52896ms step_avg:61.22ms
step:865/2245 train_time:52958ms step_avg:61.22ms
step:866/2245 train_time:53018ms step_avg:61.22ms
step:867/2245 train_time:53080ms step_avg:61.22ms
step:868/2245 train_time:53140ms step_avg:61.22ms
step:869/2245 train_time:53203ms step_avg:61.22ms
step:870/2245 train_time:53264ms step_avg:61.22ms
step:871/2245 train_time:53327ms step_avg:61.22ms
step:872/2245 train_time:53387ms step_avg:61.22ms
step:873/2245 train_time:53449ms step_avg:61.22ms
step:874/2245 train_time:53509ms step_avg:61.22ms
step:875/2245 train_time:53572ms step_avg:61.22ms
step:876/2245 train_time:53632ms step_avg:61.22ms
step:877/2245 train_time:53694ms step_avg:61.22ms
step:878/2245 train_time:53754ms step_avg:61.22ms
step:879/2245 train_time:53816ms step_avg:61.22ms
step:880/2245 train_time:53876ms step_avg:61.22ms
step:881/2245 train_time:53938ms step_avg:61.22ms
step:882/2245 train_time:53998ms step_avg:61.22ms
step:883/2245 train_time:54060ms step_avg:61.22ms
step:884/2245 train_time:54120ms step_avg:61.22ms
step:885/2245 train_time:54183ms step_avg:61.22ms
step:886/2245 train_time:54243ms step_avg:61.22ms
step:887/2245 train_time:54306ms step_avg:61.22ms
step:888/2245 train_time:54366ms step_avg:61.22ms
step:889/2245 train_time:54428ms step_avg:61.22ms
step:890/2245 train_time:54488ms step_avg:61.22ms
step:891/2245 train_time:54551ms step_avg:61.22ms
step:892/2245 train_time:54611ms step_avg:61.22ms
step:893/2245 train_time:54673ms step_avg:61.22ms
step:894/2245 train_time:54734ms step_avg:61.22ms
step:895/2245 train_time:54797ms step_avg:61.23ms
step:896/2245 train_time:54857ms step_avg:61.22ms
step:897/2245 train_time:54919ms step_avg:61.22ms
step:898/2245 train_time:54978ms step_avg:61.22ms
step:899/2245 train_time:55040ms step_avg:61.22ms
step:900/2245 train_time:55100ms step_avg:61.22ms
step:901/2245 train_time:55163ms step_avg:61.22ms
step:902/2245 train_time:55223ms step_avg:61.22ms
step:903/2245 train_time:55285ms step_avg:61.22ms
step:904/2245 train_time:55346ms step_avg:61.22ms
step:905/2245 train_time:55408ms step_avg:61.22ms
step:906/2245 train_time:55468ms step_avg:61.22ms
step:907/2245 train_time:55530ms step_avg:61.22ms
step:908/2245 train_time:55591ms step_avg:61.22ms
step:909/2245 train_time:55654ms step_avg:61.23ms
step:910/2245 train_time:55714ms step_avg:61.22ms
step:911/2245 train_time:55776ms step_avg:61.22ms
step:912/2245 train_time:55836ms step_avg:61.22ms
step:913/2245 train_time:55898ms step_avg:61.22ms
step:914/2245 train_time:55958ms step_avg:61.22ms
step:915/2245 train_time:56020ms step_avg:61.22ms
step:916/2245 train_time:56080ms step_avg:61.22ms
step:917/2245 train_time:56142ms step_avg:61.22ms
step:918/2245 train_time:56202ms step_avg:61.22ms
step:919/2245 train_time:56265ms step_avg:61.22ms
step:920/2245 train_time:56324ms step_avg:61.22ms
step:921/2245 train_time:56386ms step_avg:61.22ms
step:922/2245 train_time:56446ms step_avg:61.22ms
step:923/2245 train_time:56509ms step_avg:61.22ms
step:924/2245 train_time:56569ms step_avg:61.22ms
step:925/2245 train_time:56632ms step_avg:61.22ms
step:926/2245 train_time:56693ms step_avg:61.22ms
step:927/2245 train_time:56755ms step_avg:61.22ms
step:928/2245 train_time:56815ms step_avg:61.22ms
step:929/2245 train_time:56878ms step_avg:61.22ms
step:930/2245 train_time:56937ms step_avg:61.22ms
step:931/2245 train_time:57000ms step_avg:61.22ms
step:932/2245 train_time:57060ms step_avg:61.22ms
step:933/2245 train_time:57122ms step_avg:61.22ms
step:934/2245 train_time:57182ms step_avg:61.22ms
step:935/2245 train_time:57245ms step_avg:61.22ms
step:936/2245 train_time:57304ms step_avg:61.22ms
step:937/2245 train_time:57366ms step_avg:61.22ms
step:938/2245 train_time:57426ms step_avg:61.22ms
step:939/2245 train_time:57489ms step_avg:61.22ms
step:940/2245 train_time:57549ms step_avg:61.22ms
step:941/2245 train_time:57612ms step_avg:61.22ms
step:942/2245 train_time:57673ms step_avg:61.22ms
step:943/2245 train_time:57736ms step_avg:61.23ms
step:944/2245 train_time:57796ms step_avg:61.22ms
step:945/2245 train_time:57858ms step_avg:61.22ms
step:946/2245 train_time:57917ms step_avg:61.22ms
step:947/2245 train_time:57979ms step_avg:61.22ms
step:948/2245 train_time:58039ms step_avg:61.22ms
step:949/2245 train_time:58102ms step_avg:61.22ms
step:950/2245 train_time:58162ms step_avg:61.22ms
step:951/2245 train_time:58224ms step_avg:61.22ms
step:952/2245 train_time:58285ms step_avg:61.22ms
step:953/2245 train_time:58347ms step_avg:61.22ms
step:954/2245 train_time:58407ms step_avg:61.22ms
step:955/2245 train_time:58469ms step_avg:61.22ms
step:956/2245 train_time:58529ms step_avg:61.22ms
step:957/2245 train_time:58592ms step_avg:61.22ms
step:958/2245 train_time:58652ms step_avg:61.22ms
step:959/2245 train_time:58714ms step_avg:61.22ms
step:960/2245 train_time:58774ms step_avg:61.22ms
step:961/2245 train_time:58836ms step_avg:61.22ms
step:962/2245 train_time:58896ms step_avg:61.22ms
step:963/2245 train_time:58959ms step_avg:61.22ms
step:964/2245 train_time:59019ms step_avg:61.22ms
step:965/2245 train_time:59080ms step_avg:61.22ms
step:966/2245 train_time:59140ms step_avg:61.22ms
step:967/2245 train_time:59203ms step_avg:61.22ms
step:968/2245 train_time:59263ms step_avg:61.22ms
step:969/2245 train_time:59325ms step_avg:61.22ms
step:970/2245 train_time:59386ms step_avg:61.22ms
step:971/2245 train_time:59448ms step_avg:61.22ms
step:972/2245 train_time:59508ms step_avg:61.22ms
step:973/2245 train_time:59571ms step_avg:61.22ms
step:974/2245 train_time:59631ms step_avg:61.22ms
step:975/2245 train_time:59694ms step_avg:61.22ms
step:976/2245 train_time:59754ms step_avg:61.22ms
step:977/2245 train_time:59817ms step_avg:61.23ms
step:978/2245 train_time:59877ms step_avg:61.22ms
step:979/2245 train_time:59939ms step_avg:61.22ms
step:980/2245 train_time:59999ms step_avg:61.22ms
step:981/2245 train_time:60061ms step_avg:61.22ms
step:982/2245 train_time:60121ms step_avg:61.22ms
step:983/2245 train_time:60183ms step_avg:61.22ms
step:984/2245 train_time:60244ms step_avg:61.22ms
step:985/2245 train_time:60306ms step_avg:61.22ms
step:986/2245 train_time:60366ms step_avg:61.22ms
step:987/2245 train_time:60428ms step_avg:61.22ms
step:988/2245 train_time:60489ms step_avg:61.22ms
step:989/2245 train_time:60552ms step_avg:61.23ms
step:990/2245 train_time:60612ms step_avg:61.22ms
step:991/2245 train_time:60674ms step_avg:61.23ms
step:992/2245 train_time:60734ms step_avg:61.22ms
step:993/2245 train_time:60797ms step_avg:61.23ms
step:994/2245 train_time:60856ms step_avg:61.22ms
step:995/2245 train_time:60919ms step_avg:61.22ms
step:996/2245 train_time:60979ms step_avg:61.22ms
step:997/2245 train_time:61040ms step_avg:61.22ms
step:998/2245 train_time:61100ms step_avg:61.22ms
step:999/2245 train_time:61162ms step_avg:61.22ms
step:1000/2245 train_time:61221ms step_avg:61.22ms
step:1000/2245 val_loss:3.5937 train_time:61285ms step_avg:61.28ms
step:1001/2245 train_time:61304ms step_avg:61.24ms
step:1002/2245 train_time:61348ms step_avg:61.23ms
step:1003/2245 train_time:61413ms step_avg:61.23ms
step:1004/2245 train_time:61475ms step_avg:61.23ms
step:1005/2245 train_time:61538ms step_avg:61.23ms
step:1006/2245 train_time:61597ms step_avg:61.23ms
step:1007/2245 train_time:61659ms step_avg:61.23ms
step:1008/2245 train_time:61719ms step_avg:61.23ms
step:1009/2245 train_time:61780ms step_avg:61.23ms
step:1010/2245 train_time:61839ms step_avg:61.23ms
step:1011/2245 train_time:61901ms step_avg:61.23ms
step:1012/2245 train_time:61960ms step_avg:61.23ms
step:1013/2245 train_time:62022ms step_avg:61.23ms
step:1014/2245 train_time:62081ms step_avg:61.22ms
step:1015/2245 train_time:62142ms step_avg:61.22ms
step:1016/2245 train_time:62202ms step_avg:61.22ms
step:1017/2245 train_time:62265ms step_avg:61.22ms
step:1018/2245 train_time:62327ms step_avg:61.22ms
step:1019/2245 train_time:62391ms step_avg:61.23ms
step:1020/2245 train_time:62452ms step_avg:61.23ms
step:1021/2245 train_time:62515ms step_avg:61.23ms
step:1022/2245 train_time:62576ms step_avg:61.23ms
step:1023/2245 train_time:62638ms step_avg:61.23ms
step:1024/2245 train_time:62697ms step_avg:61.23ms
step:1025/2245 train_time:62759ms step_avg:61.23ms
step:1026/2245 train_time:62820ms step_avg:61.23ms
step:1027/2245 train_time:62880ms step_avg:61.23ms
step:1028/2245 train_time:62940ms step_avg:61.23ms
step:1029/2245 train_time:63001ms step_avg:61.23ms
step:1030/2245 train_time:63060ms step_avg:61.22ms
step:1031/2245 train_time:63121ms step_avg:61.22ms
step:1032/2245 train_time:63181ms step_avg:61.22ms
step:1033/2245 train_time:63244ms step_avg:61.22ms
step:1034/2245 train_time:63304ms step_avg:61.22ms
step:1035/2245 train_time:63368ms step_avg:61.22ms
step:1036/2245 train_time:63428ms step_avg:61.22ms
step:1037/2245 train_time:63491ms step_avg:61.23ms
step:1038/2245 train_time:63552ms step_avg:61.22ms
step:1039/2245 train_time:63615ms step_avg:61.23ms
step:1040/2245 train_time:63676ms step_avg:61.23ms
step:1041/2245 train_time:63738ms step_avg:61.23ms
step:1042/2245 train_time:63798ms step_avg:61.23ms
step:1043/2245 train_time:63859ms step_avg:61.23ms
step:1044/2245 train_time:63919ms step_avg:61.23ms
step:1045/2245 train_time:63981ms step_avg:61.23ms
step:1046/2245 train_time:64040ms step_avg:61.22ms
step:1047/2245 train_time:64102ms step_avg:61.22ms
step:1048/2245 train_time:64161ms step_avg:61.22ms
step:1049/2245 train_time:64223ms step_avg:61.22ms
step:1050/2245 train_time:64283ms step_avg:61.22ms
step:1051/2245 train_time:64346ms step_avg:61.22ms
step:1052/2245 train_time:64407ms step_avg:61.22ms
step:1053/2245 train_time:64469ms step_avg:61.22ms
step:1054/2245 train_time:64530ms step_avg:61.22ms
step:1055/2245 train_time:64593ms step_avg:61.23ms
step:1056/2245 train_time:64653ms step_avg:61.22ms
step:1057/2245 train_time:64716ms step_avg:61.23ms
step:1058/2245 train_time:64776ms step_avg:61.23ms
step:1059/2245 train_time:64838ms step_avg:61.23ms
step:1060/2245 train_time:64898ms step_avg:61.22ms
step:1061/2245 train_time:64960ms step_avg:61.23ms
step:1062/2245 train_time:65020ms step_avg:61.22ms
step:1063/2245 train_time:65081ms step_avg:61.22ms
step:1064/2245 train_time:65141ms step_avg:61.22ms
step:1065/2245 train_time:65203ms step_avg:61.22ms
step:1066/2245 train_time:65262ms step_avg:61.22ms
step:1067/2245 train_time:65325ms step_avg:61.22ms
step:1068/2245 train_time:65385ms step_avg:61.22ms
step:1069/2245 train_time:65448ms step_avg:61.22ms
step:1070/2245 train_time:65509ms step_avg:61.22ms
step:1071/2245 train_time:65571ms step_avg:61.22ms
step:1072/2245 train_time:65631ms step_avg:61.22ms
step:1073/2245 train_time:65694ms step_avg:61.22ms
step:1074/2245 train_time:65755ms step_avg:61.22ms
step:1075/2245 train_time:65817ms step_avg:61.23ms
step:1076/2245 train_time:65878ms step_avg:61.22ms
step:1077/2245 train_time:65940ms step_avg:61.23ms
step:1078/2245 train_time:66000ms step_avg:61.22ms
step:1079/2245 train_time:66061ms step_avg:61.22ms
step:1080/2245 train_time:66121ms step_avg:61.22ms
step:1081/2245 train_time:66182ms step_avg:61.22ms
step:1082/2245 train_time:66242ms step_avg:61.22ms
step:1083/2245 train_time:66304ms step_avg:61.22ms
step:1084/2245 train_time:66364ms step_avg:61.22ms
step:1085/2245 train_time:66427ms step_avg:61.22ms
step:1086/2245 train_time:66487ms step_avg:61.22ms
step:1087/2245 train_time:66550ms step_avg:61.22ms
step:1088/2245 train_time:66610ms step_avg:61.22ms
step:1089/2245 train_time:66673ms step_avg:61.22ms
step:1090/2245 train_time:66734ms step_avg:61.22ms
step:1091/2245 train_time:66797ms step_avg:61.23ms
step:1092/2245 train_time:66857ms step_avg:61.22ms
step:1093/2245 train_time:66919ms step_avg:61.23ms
step:1094/2245 train_time:66980ms step_avg:61.22ms
step:1095/2245 train_time:67042ms step_avg:61.23ms
step:1096/2245 train_time:67101ms step_avg:61.22ms
step:1097/2245 train_time:67163ms step_avg:61.22ms
step:1098/2245 train_time:67223ms step_avg:61.22ms
step:1099/2245 train_time:67285ms step_avg:61.22ms
step:1100/2245 train_time:67345ms step_avg:61.22ms
step:1101/2245 train_time:67408ms step_avg:61.22ms
step:1102/2245 train_time:67468ms step_avg:61.22ms
step:1103/2245 train_time:67530ms step_avg:61.22ms
step:1104/2245 train_time:67590ms step_avg:61.22ms
step:1105/2245 train_time:67652ms step_avg:61.22ms
step:1106/2245 train_time:67712ms step_avg:61.22ms
step:1107/2245 train_time:67775ms step_avg:61.22ms
step:1108/2245 train_time:67835ms step_avg:61.22ms
step:1109/2245 train_time:67898ms step_avg:61.22ms
step:1110/2245 train_time:67959ms step_avg:61.22ms
step:1111/2245 train_time:68020ms step_avg:61.22ms
step:1112/2245 train_time:68080ms step_avg:61.22ms
step:1113/2245 train_time:68142ms step_avg:61.22ms
step:1114/2245 train_time:68202ms step_avg:61.22ms
step:1115/2245 train_time:68264ms step_avg:61.22ms
step:1116/2245 train_time:68324ms step_avg:61.22ms
step:1117/2245 train_time:68386ms step_avg:61.22ms
step:1118/2245 train_time:68446ms step_avg:61.22ms
step:1119/2245 train_time:68509ms step_avg:61.22ms
step:1120/2245 train_time:68569ms step_avg:61.22ms
step:1121/2245 train_time:68632ms step_avg:61.22ms
step:1122/2245 train_time:68693ms step_avg:61.22ms
step:1123/2245 train_time:68756ms step_avg:61.23ms
step:1124/2245 train_time:68816ms step_avg:61.22ms
step:1125/2245 train_time:68879ms step_avg:61.23ms
step:1126/2245 train_time:68938ms step_avg:61.22ms
step:1127/2245 train_time:69001ms step_avg:61.23ms
step:1128/2245 train_time:69061ms step_avg:61.22ms
step:1129/2245 train_time:69123ms step_avg:61.22ms
step:1130/2245 train_time:69183ms step_avg:61.22ms
step:1131/2245 train_time:69245ms step_avg:61.22ms
step:1132/2245 train_time:69305ms step_avg:61.22ms
step:1133/2245 train_time:69367ms step_avg:61.22ms
step:1134/2245 train_time:69427ms step_avg:61.22ms
step:1135/2245 train_time:69489ms step_avg:61.22ms
step:1136/2245 train_time:69549ms step_avg:61.22ms
step:1137/2245 train_time:69612ms step_avg:61.22ms
step:1138/2245 train_time:69674ms step_avg:61.22ms
step:1139/2245 train_time:69737ms step_avg:61.23ms
step:1140/2245 train_time:69797ms step_avg:61.23ms
step:1141/2245 train_time:69859ms step_avg:61.23ms
step:1142/2245 train_time:69920ms step_avg:61.23ms
step:1143/2245 train_time:69982ms step_avg:61.23ms
step:1144/2245 train_time:70042ms step_avg:61.23ms
step:1145/2245 train_time:70103ms step_avg:61.23ms
step:1146/2245 train_time:70163ms step_avg:61.22ms
step:1147/2245 train_time:70225ms step_avg:61.22ms
step:1148/2245 train_time:70285ms step_avg:61.22ms
step:1149/2245 train_time:70347ms step_avg:61.22ms
step:1150/2245 train_time:70407ms step_avg:61.22ms
step:1151/2245 train_time:70469ms step_avg:61.22ms
step:1152/2245 train_time:70529ms step_avg:61.22ms
step:1153/2245 train_time:70592ms step_avg:61.22ms
step:1154/2245 train_time:70652ms step_avg:61.22ms
step:1155/2245 train_time:70715ms step_avg:61.23ms
step:1156/2245 train_time:70776ms step_avg:61.23ms
step:1157/2245 train_time:70838ms step_avg:61.23ms
step:1158/2245 train_time:70899ms step_avg:61.23ms
step:1159/2245 train_time:70961ms step_avg:61.23ms
step:1160/2245 train_time:71020ms step_avg:61.22ms
step:1161/2245 train_time:71082ms step_avg:61.22ms
step:1162/2245 train_time:71142ms step_avg:61.22ms
step:1163/2245 train_time:71204ms step_avg:61.22ms
step:1164/2245 train_time:71264ms step_avg:61.22ms
step:1165/2245 train_time:71325ms step_avg:61.22ms
step:1166/2245 train_time:71385ms step_avg:61.22ms
step:1167/2245 train_time:71448ms step_avg:61.22ms
step:1168/2245 train_time:71508ms step_avg:61.22ms
step:1169/2245 train_time:71571ms step_avg:61.22ms
step:1170/2245 train_time:71631ms step_avg:61.22ms
step:1171/2245 train_time:71693ms step_avg:61.22ms
step:1172/2245 train_time:71754ms step_avg:61.22ms
step:1173/2245 train_time:71816ms step_avg:61.22ms
step:1174/2245 train_time:71877ms step_avg:61.22ms
step:1175/2245 train_time:71940ms step_avg:61.23ms
step:1176/2245 train_time:72000ms step_avg:61.22ms
step:1177/2245 train_time:72061ms step_avg:61.22ms
step:1178/2245 train_time:72121ms step_avg:61.22ms
step:1179/2245 train_time:72182ms step_avg:61.22ms
step:1180/2245 train_time:72243ms step_avg:61.22ms
step:1181/2245 train_time:72304ms step_avg:61.22ms
step:1182/2245 train_time:72365ms step_avg:61.22ms
step:1183/2245 train_time:72427ms step_avg:61.22ms
step:1184/2245 train_time:72487ms step_avg:61.22ms
step:1185/2245 train_time:72550ms step_avg:61.22ms
step:1186/2245 train_time:72610ms step_avg:61.22ms
step:1187/2245 train_time:72672ms step_avg:61.22ms
step:1188/2245 train_time:72733ms step_avg:61.22ms
step:1189/2245 train_time:72796ms step_avg:61.22ms
step:1190/2245 train_time:72856ms step_avg:61.22ms
step:1191/2245 train_time:72919ms step_avg:61.22ms
step:1192/2245 train_time:72979ms step_avg:61.22ms
step:1193/2245 train_time:73040ms step_avg:61.22ms
step:1194/2245 train_time:73100ms step_avg:61.22ms
step:1195/2245 train_time:73163ms step_avg:61.22ms
step:1196/2245 train_time:73223ms step_avg:61.22ms
step:1197/2245 train_time:73285ms step_avg:61.22ms
step:1198/2245 train_time:73345ms step_avg:61.22ms
step:1199/2245 train_time:73407ms step_avg:61.22ms
step:1200/2245 train_time:73468ms step_avg:61.22ms
step:1201/2245 train_time:73530ms step_avg:61.22ms
step:1202/2245 train_time:73590ms step_avg:61.22ms
step:1203/2245 train_time:73653ms step_avg:61.22ms
step:1204/2245 train_time:73714ms step_avg:61.22ms
step:1205/2245 train_time:73777ms step_avg:61.23ms
step:1206/2245 train_time:73837ms step_avg:61.22ms
step:1207/2245 train_time:73901ms step_avg:61.23ms
step:1208/2245 train_time:73960ms step_avg:61.23ms
step:1209/2245 train_time:74022ms step_avg:61.23ms
step:1210/2245 train_time:74081ms step_avg:61.22ms
step:1211/2245 train_time:74143ms step_avg:61.22ms
step:1212/2245 train_time:74203ms step_avg:61.22ms
step:1213/2245 train_time:74265ms step_avg:61.22ms
step:1214/2245 train_time:74325ms step_avg:61.22ms
step:1215/2245 train_time:74388ms step_avg:61.22ms
step:1216/2245 train_time:74447ms step_avg:61.22ms
step:1217/2245 train_time:74509ms step_avg:61.22ms
step:1218/2245 train_time:74570ms step_avg:61.22ms
step:1219/2245 train_time:74632ms step_avg:61.22ms
step:1220/2245 train_time:74692ms step_avg:61.22ms
step:1221/2245 train_time:74755ms step_avg:61.22ms
step:1222/2245 train_time:74816ms step_avg:61.22ms
step:1223/2245 train_time:74879ms step_avg:61.23ms
step:1224/2245 train_time:74939ms step_avg:61.22ms
step:1225/2245 train_time:75001ms step_avg:61.23ms
step:1226/2245 train_time:75061ms step_avg:61.22ms
step:1227/2245 train_time:75124ms step_avg:61.23ms
step:1228/2245 train_time:75184ms step_avg:61.22ms
step:1229/2245 train_time:75246ms step_avg:61.23ms
step:1230/2245 train_time:75307ms step_avg:61.22ms
step:1231/2245 train_time:75369ms step_avg:61.23ms
step:1232/2245 train_time:75429ms step_avg:61.23ms
step:1233/2245 train_time:75491ms step_avg:61.23ms
step:1234/2245 train_time:75551ms step_avg:61.22ms
step:1235/2245 train_time:75613ms step_avg:61.23ms
step:1236/2245 train_time:75674ms step_avg:61.22ms
step:1237/2245 train_time:75736ms step_avg:61.23ms
step:1238/2245 train_time:75797ms step_avg:61.23ms
step:1239/2245 train_time:75859ms step_avg:61.23ms
step:1240/2245 train_time:75920ms step_avg:61.23ms
step:1241/2245 train_time:75982ms step_avg:61.23ms
step:1242/2245 train_time:76042ms step_avg:61.23ms
step:1243/2245 train_time:76104ms step_avg:61.23ms
step:1244/2245 train_time:76164ms step_avg:61.23ms
step:1245/2245 train_time:76226ms step_avg:61.23ms
step:1246/2245 train_time:76286ms step_avg:61.23ms
step:1247/2245 train_time:76348ms step_avg:61.23ms
step:1248/2245 train_time:76409ms step_avg:61.22ms
step:1249/2245 train_time:76471ms step_avg:61.23ms
step:1250/2245 train_time:76531ms step_avg:61.22ms
step:1250/2245 val_loss:3.5240 train_time:76593ms step_avg:61.27ms
step:1251/2245 train_time:76612ms step_avg:61.24ms
step:1252/2245 train_time:76657ms step_avg:61.23ms
step:1253/2245 train_time:76722ms step_avg:61.23ms
step:1254/2245 train_time:76784ms step_avg:61.23ms
step:1255/2245 train_time:76846ms step_avg:61.23ms
step:1256/2245 train_time:76907ms step_avg:61.23ms
step:1257/2245 train_time:76968ms step_avg:61.23ms
step:1258/2245 train_time:77028ms step_avg:61.23ms
step:1259/2245 train_time:77089ms step_avg:61.23ms
step:1260/2245 train_time:77148ms step_avg:61.23ms
step:1261/2245 train_time:77210ms step_avg:61.23ms
step:1262/2245 train_time:77269ms step_avg:61.23ms
step:1263/2245 train_time:77330ms step_avg:61.23ms
step:1264/2245 train_time:77388ms step_avg:61.22ms
step:1265/2245 train_time:77449ms step_avg:61.22ms
step:1266/2245 train_time:77510ms step_avg:61.22ms
step:1267/2245 train_time:77573ms step_avg:61.23ms
step:1268/2245 train_time:77635ms step_avg:61.23ms
step:1269/2245 train_time:77699ms step_avg:61.23ms
step:1270/2245 train_time:77761ms step_avg:61.23ms
step:1271/2245 train_time:77824ms step_avg:61.23ms
step:1272/2245 train_time:77884ms step_avg:61.23ms
step:1273/2245 train_time:77945ms step_avg:61.23ms
step:1274/2245 train_time:78005ms step_avg:61.23ms
step:1275/2245 train_time:78067ms step_avg:61.23ms
step:1276/2245 train_time:78127ms step_avg:61.23ms
step:1277/2245 train_time:78189ms step_avg:61.23ms
step:1278/2245 train_time:78248ms step_avg:61.23ms
step:1279/2245 train_time:78309ms step_avg:61.23ms
step:1280/2245 train_time:78368ms step_avg:61.23ms
step:1281/2245 train_time:78430ms step_avg:61.23ms
step:1282/2245 train_time:78490ms step_avg:61.22ms
step:1283/2245 train_time:78553ms step_avg:61.23ms
step:1284/2245 train_time:78614ms step_avg:61.23ms
step:1285/2245 train_time:78677ms step_avg:61.23ms
step:1286/2245 train_time:78738ms step_avg:61.23ms
step:1287/2245 train_time:78802ms step_avg:61.23ms
step:1288/2245 train_time:78862ms step_avg:61.23ms
step:1289/2245 train_time:78924ms step_avg:61.23ms
step:1290/2245 train_time:78984ms step_avg:61.23ms
step:1291/2245 train_time:79046ms step_avg:61.23ms
step:1292/2245 train_time:79106ms step_avg:61.23ms
step:1293/2245 train_time:79168ms step_avg:61.23ms
step:1294/2245 train_time:79227ms step_avg:61.23ms
step:1295/2245 train_time:79289ms step_avg:61.23ms
step:1296/2245 train_time:79348ms step_avg:61.23ms
step:1297/2245 train_time:79409ms step_avg:61.23ms
step:1298/2245 train_time:79469ms step_avg:61.22ms
step:1299/2245 train_time:79533ms step_avg:61.23ms
step:1300/2245 train_time:79594ms step_avg:61.23ms
step:1301/2245 train_time:79657ms step_avg:61.23ms
step:1302/2245 train_time:79717ms step_avg:61.23ms
step:1303/2245 train_time:79780ms step_avg:61.23ms
step:1304/2245 train_time:79841ms step_avg:61.23ms
step:1305/2245 train_time:79904ms step_avg:61.23ms
step:1306/2245 train_time:79963ms step_avg:61.23ms
step:1307/2245 train_time:80026ms step_avg:61.23ms
step:1308/2245 train_time:80085ms step_avg:61.23ms
step:1309/2245 train_time:80147ms step_avg:61.23ms
step:1310/2245 train_time:80207ms step_avg:61.23ms
step:1311/2245 train_time:80268ms step_avg:61.23ms
step:1312/2245 train_time:80327ms step_avg:61.23ms
step:1313/2245 train_time:80390ms step_avg:61.23ms
step:1314/2245 train_time:80449ms step_avg:61.22ms
step:1315/2245 train_time:80512ms step_avg:61.23ms
step:1316/2245 train_time:80573ms step_avg:61.23ms
step:1317/2245 train_time:80636ms step_avg:61.23ms
step:1318/2245 train_time:80697ms step_avg:61.23ms
step:1319/2245 train_time:80759ms step_avg:61.23ms
step:1320/2245 train_time:80819ms step_avg:61.23ms
step:1321/2245 train_time:80883ms step_avg:61.23ms
step:1322/2245 train_time:80943ms step_avg:61.23ms
step:1323/2245 train_time:81005ms step_avg:61.23ms
step:1324/2245 train_time:81064ms step_avg:61.23ms
step:1325/2245 train_time:81126ms step_avg:61.23ms
step:1326/2245 train_time:81186ms step_avg:61.23ms
step:1327/2245 train_time:81248ms step_avg:61.23ms
step:1328/2245 train_time:81308ms step_avg:61.23ms
step:1329/2245 train_time:81370ms step_avg:61.23ms
step:1330/2245 train_time:81429ms step_avg:61.23ms
step:1331/2245 train_time:81492ms step_avg:61.23ms
step:1332/2245 train_time:81552ms step_avg:61.23ms
step:1333/2245 train_time:81616ms step_avg:61.23ms
step:1334/2245 train_time:81676ms step_avg:61.23ms
step:1335/2245 train_time:81738ms step_avg:61.23ms
step:1336/2245 train_time:81798ms step_avg:61.23ms
step:1337/2245 train_time:81860ms step_avg:61.23ms
step:1338/2245 train_time:81921ms step_avg:61.23ms
step:1339/2245 train_time:81984ms step_avg:61.23ms
step:1340/2245 train_time:82044ms step_avg:61.23ms
step:1341/2245 train_time:82107ms step_avg:61.23ms
step:1342/2245 train_time:82167ms step_avg:61.23ms
step:1343/2245 train_time:82229ms step_avg:61.23ms
step:1344/2245 train_time:82289ms step_avg:61.23ms
step:1345/2245 train_time:82352ms step_avg:61.23ms
step:1346/2245 train_time:82412ms step_avg:61.23ms
step:1347/2245 train_time:82474ms step_avg:61.23ms
step:1348/2245 train_time:82534ms step_avg:61.23ms
step:1349/2245 train_time:82596ms step_avg:61.23ms
step:1350/2245 train_time:82656ms step_avg:61.23ms
step:1351/2245 train_time:82718ms step_avg:61.23ms
step:1352/2245 train_time:82779ms step_avg:61.23ms
step:1353/2245 train_time:82841ms step_avg:61.23ms
step:1354/2245 train_time:82901ms step_avg:61.23ms
step:1355/2245 train_time:82964ms step_avg:61.23ms
step:1356/2245 train_time:83024ms step_avg:61.23ms
step:1357/2245 train_time:83086ms step_avg:61.23ms
step:1358/2245 train_time:83146ms step_avg:61.23ms
step:1359/2245 train_time:83208ms step_avg:61.23ms
step:1360/2245 train_time:83268ms step_avg:61.23ms
step:1361/2245 train_time:83330ms step_avg:61.23ms
step:1362/2245 train_time:83390ms step_avg:61.23ms
step:1363/2245 train_time:83452ms step_avg:61.23ms
step:1364/2245 train_time:83512ms step_avg:61.23ms
step:1365/2245 train_time:83574ms step_avg:61.23ms
step:1366/2245 train_time:83634ms step_avg:61.23ms
step:1367/2245 train_time:83697ms step_avg:61.23ms
step:1368/2245 train_time:83756ms step_avg:61.23ms
step:1369/2245 train_time:83819ms step_avg:61.23ms
step:1370/2245 train_time:83879ms step_avg:61.23ms
step:1371/2245 train_time:83942ms step_avg:61.23ms
step:1372/2245 train_time:84003ms step_avg:61.23ms
step:1373/2245 train_time:84065ms step_avg:61.23ms
step:1374/2245 train_time:84125ms step_avg:61.23ms
step:1375/2245 train_time:84188ms step_avg:61.23ms
step:1376/2245 train_time:84248ms step_avg:61.23ms
step:1377/2245 train_time:84310ms step_avg:61.23ms
step:1378/2245 train_time:84369ms step_avg:61.23ms
step:1379/2245 train_time:84431ms step_avg:61.23ms
step:1380/2245 train_time:84491ms step_avg:61.23ms
step:1381/2245 train_time:84554ms step_avg:61.23ms
step:1382/2245 train_time:84614ms step_avg:61.23ms
step:1383/2245 train_time:84676ms step_avg:61.23ms
step:1384/2245 train_time:84736ms step_avg:61.23ms
step:1385/2245 train_time:84798ms step_avg:61.23ms
step:1386/2245 train_time:84859ms step_avg:61.23ms
step:1387/2245 train_time:84922ms step_avg:61.23ms
step:1388/2245 train_time:84982ms step_avg:61.23ms
step:1389/2245 train_time:85045ms step_avg:61.23ms
step:1390/2245 train_time:85105ms step_avg:61.23ms
step:1391/2245 train_time:85167ms step_avg:61.23ms
step:1392/2245 train_time:85227ms step_avg:61.23ms
step:1393/2245 train_time:85290ms step_avg:61.23ms
step:1394/2245 train_time:85350ms step_avg:61.23ms
step:1395/2245 train_time:85412ms step_avg:61.23ms
step:1396/2245 train_time:85472ms step_avg:61.23ms
step:1397/2245 train_time:85534ms step_avg:61.23ms
step:1398/2245 train_time:85594ms step_avg:61.23ms
step:1399/2245 train_time:85656ms step_avg:61.23ms
step:1400/2245 train_time:85716ms step_avg:61.23ms
step:1401/2245 train_time:85778ms step_avg:61.23ms
step:1402/2245 train_time:85838ms step_avg:61.23ms
step:1403/2245 train_time:85901ms step_avg:61.23ms
step:1404/2245 train_time:85962ms step_avg:61.23ms
step:1405/2245 train_time:86025ms step_avg:61.23ms
step:1406/2245 train_time:86085ms step_avg:61.23ms
step:1407/2245 train_time:86148ms step_avg:61.23ms
step:1408/2245 train_time:86208ms step_avg:61.23ms
step:1409/2245 train_time:86270ms step_avg:61.23ms
step:1410/2245 train_time:86329ms step_avg:61.23ms
step:1411/2245 train_time:86392ms step_avg:61.23ms
step:1412/2245 train_time:86452ms step_avg:61.23ms
step:1413/2245 train_time:86514ms step_avg:61.23ms
step:1414/2245 train_time:86574ms step_avg:61.23ms
step:1415/2245 train_time:86637ms step_avg:61.23ms
step:1416/2245 train_time:86697ms step_avg:61.23ms
step:1417/2245 train_time:86759ms step_avg:61.23ms
step:1418/2245 train_time:86819ms step_avg:61.23ms
step:1419/2245 train_time:86882ms step_avg:61.23ms
step:1420/2245 train_time:86942ms step_avg:61.23ms
step:1421/2245 train_time:87005ms step_avg:61.23ms
step:1422/2245 train_time:87064ms step_avg:61.23ms
step:1423/2245 train_time:87127ms step_avg:61.23ms
step:1424/2245 train_time:87187ms step_avg:61.23ms
step:1425/2245 train_time:87249ms step_avg:61.23ms
step:1426/2245 train_time:87309ms step_avg:61.23ms
step:1427/2245 train_time:87371ms step_avg:61.23ms
step:1428/2245 train_time:87430ms step_avg:61.23ms
step:1429/2245 train_time:87493ms step_avg:61.23ms
step:1430/2245 train_time:87552ms step_avg:61.23ms
step:1431/2245 train_time:87614ms step_avg:61.23ms
step:1432/2245 train_time:87674ms step_avg:61.22ms
step:1433/2245 train_time:87736ms step_avg:61.23ms
step:1434/2245 train_time:87797ms step_avg:61.23ms
step:1435/2245 train_time:87860ms step_avg:61.23ms
step:1436/2245 train_time:87920ms step_avg:61.23ms
step:1437/2245 train_time:87984ms step_avg:61.23ms
step:1438/2245 train_time:88044ms step_avg:61.23ms
step:1439/2245 train_time:88107ms step_avg:61.23ms
step:1440/2245 train_time:88166ms step_avg:61.23ms
step:1441/2245 train_time:88229ms step_avg:61.23ms
step:1442/2245 train_time:88288ms step_avg:61.23ms
step:1443/2245 train_time:88350ms step_avg:61.23ms
step:1444/2245 train_time:88410ms step_avg:61.23ms
step:1445/2245 train_time:88472ms step_avg:61.23ms
step:1446/2245 train_time:88532ms step_avg:61.23ms
step:1447/2245 train_time:88594ms step_avg:61.23ms
step:1448/2245 train_time:88655ms step_avg:61.23ms
step:1449/2245 train_time:88717ms step_avg:61.23ms
step:1450/2245 train_time:88777ms step_avg:61.23ms
step:1451/2245 train_time:88840ms step_avg:61.23ms
step:1452/2245 train_time:88900ms step_avg:61.23ms
step:1453/2245 train_time:88963ms step_avg:61.23ms
step:1454/2245 train_time:89023ms step_avg:61.23ms
step:1455/2245 train_time:89086ms step_avg:61.23ms
step:1456/2245 train_time:89145ms step_avg:61.23ms
step:1457/2245 train_time:89207ms step_avg:61.23ms
step:1458/2245 train_time:89267ms step_avg:61.23ms
step:1459/2245 train_time:89329ms step_avg:61.23ms
step:1460/2245 train_time:89389ms step_avg:61.23ms
step:1461/2245 train_time:89451ms step_avg:61.23ms
step:1462/2245 train_time:89510ms step_avg:61.22ms
step:1463/2245 train_time:89573ms step_avg:61.23ms
step:1464/2245 train_time:89634ms step_avg:61.23ms
step:1465/2245 train_time:89697ms step_avg:61.23ms
step:1466/2245 train_time:89756ms step_avg:61.23ms
step:1467/2245 train_time:89818ms step_avg:61.23ms
step:1468/2245 train_time:89879ms step_avg:61.23ms
step:1469/2245 train_time:89942ms step_avg:61.23ms
step:1470/2245 train_time:90002ms step_avg:61.23ms
step:1471/2245 train_time:90065ms step_avg:61.23ms
step:1472/2245 train_time:90125ms step_avg:61.23ms
step:1473/2245 train_time:90188ms step_avg:61.23ms
step:1474/2245 train_time:90248ms step_avg:61.23ms
step:1475/2245 train_time:90311ms step_avg:61.23ms
step:1476/2245 train_time:90372ms step_avg:61.23ms
step:1477/2245 train_time:90435ms step_avg:61.23ms
step:1478/2245 train_time:90495ms step_avg:61.23ms
step:1479/2245 train_time:90558ms step_avg:61.23ms
step:1480/2245 train_time:90619ms step_avg:61.23ms
step:1481/2245 train_time:90682ms step_avg:61.23ms
step:1482/2245 train_time:90742ms step_avg:61.23ms
step:1483/2245 train_time:90805ms step_avg:61.23ms
step:1484/2245 train_time:90865ms step_avg:61.23ms
step:1485/2245 train_time:90929ms step_avg:61.23ms
step:1486/2245 train_time:90990ms step_avg:61.23ms
step:1487/2245 train_time:91053ms step_avg:61.23ms
step:1488/2245 train_time:91114ms step_avg:61.23ms
step:1489/2245 train_time:91177ms step_avg:61.23ms
step:1490/2245 train_time:91238ms step_avg:61.23ms
step:1491/2245 train_time:91302ms step_avg:61.24ms
step:1492/2245 train_time:91363ms step_avg:61.23ms
step:1493/2245 train_time:91425ms step_avg:61.24ms
step:1494/2245 train_time:91485ms step_avg:61.23ms
step:1495/2245 train_time:91548ms step_avg:61.24ms
step:1496/2245 train_time:91608ms step_avg:61.24ms
step:1497/2245 train_time:91671ms step_avg:61.24ms
step:1498/2245 train_time:91732ms step_avg:61.24ms
step:1499/2245 train_time:91796ms step_avg:61.24ms
step:1500/2245 train_time:91857ms step_avg:61.24ms
step:1500/2245 val_loss:3.4418 train_time:91921ms step_avg:61.28ms
step:1501/2245 train_time:91939ms step_avg:61.25ms
step:1502/2245 train_time:91983ms step_avg:61.24ms
step:1503/2245 train_time:92054ms step_avg:61.25ms
step:1504/2245 train_time:92116ms step_avg:61.25ms
step:1505/2245 train_time:92178ms step_avg:61.25ms
step:1506/2245 train_time:92239ms step_avg:61.25ms
step:1507/2245 train_time:92301ms step_avg:61.25ms
step:1508/2245 train_time:92361ms step_avg:61.25ms
step:1509/2245 train_time:92422ms step_avg:61.25ms
step:1510/2245 train_time:92482ms step_avg:61.25ms
step:1511/2245 train_time:92545ms step_avg:61.25ms
step:1512/2245 train_time:92604ms step_avg:61.25ms
step:1513/2245 train_time:92666ms step_avg:61.25ms
step:1514/2245 train_time:92726ms step_avg:61.25ms
step:1515/2245 train_time:92788ms step_avg:61.25ms
step:1516/2245 train_time:92853ms step_avg:61.25ms
step:1517/2245 train_time:92918ms step_avg:61.25ms
step:1518/2245 train_time:92980ms step_avg:61.25ms
step:1519/2245 train_time:93044ms step_avg:61.25ms
step:1520/2245 train_time:93106ms step_avg:61.25ms
step:1521/2245 train_time:93169ms step_avg:61.25ms
step:1522/2245 train_time:93229ms step_avg:61.25ms
step:1523/2245 train_time:93291ms step_avg:61.25ms
step:1524/2245 train_time:93351ms step_avg:61.25ms
step:1525/2245 train_time:93412ms step_avg:61.25ms
step:1526/2245 train_time:93473ms step_avg:61.25ms
step:1527/2245 train_time:93536ms step_avg:61.25ms
step:1528/2245 train_time:93596ms step_avg:61.25ms
step:1529/2245 train_time:93658ms step_avg:61.25ms
step:1530/2245 train_time:93719ms step_avg:61.25ms
step:1531/2245 train_time:93782ms step_avg:61.26ms
step:1532/2245 train_time:93843ms step_avg:61.26ms
step:1533/2245 train_time:93906ms step_avg:61.26ms
step:1534/2245 train_time:93967ms step_avg:61.26ms
step:1535/2245 train_time:94031ms step_avg:61.26ms
step:1536/2245 train_time:94092ms step_avg:61.26ms
step:1537/2245 train_time:94155ms step_avg:61.26ms
step:1538/2245 train_time:94216ms step_avg:61.26ms
step:1539/2245 train_time:94278ms step_avg:61.26ms
step:1540/2245 train_time:94338ms step_avg:61.26ms
step:1541/2245 train_time:94401ms step_avg:61.26ms
step:1542/2245 train_time:94462ms step_avg:61.26ms
step:1543/2245 train_time:94525ms step_avg:61.26ms
step:1544/2245 train_time:94585ms step_avg:61.26ms
step:1545/2245 train_time:94647ms step_avg:61.26ms
step:1546/2245 train_time:94707ms step_avg:61.26ms
step:1547/2245 train_time:94769ms step_avg:61.26ms
step:1548/2245 train_time:94830ms step_avg:61.26ms
step:1549/2245 train_time:94893ms step_avg:61.26ms
step:1550/2245 train_time:94953ms step_avg:61.26ms
step:1551/2245 train_time:95016ms step_avg:61.26ms
step:1552/2245 train_time:95076ms step_avg:61.26ms
step:1553/2245 train_time:95140ms step_avg:61.26ms
step:1554/2245 train_time:95201ms step_avg:61.26ms
step:1555/2245 train_time:95264ms step_avg:61.26ms
step:1556/2245 train_time:95325ms step_avg:61.26ms
step:1557/2245 train_time:95388ms step_avg:61.26ms
step:1558/2245 train_time:95448ms step_avg:61.26ms
step:1559/2245 train_time:95510ms step_avg:61.26ms
step:1560/2245 train_time:95569ms step_avg:61.26ms
step:1561/2245 train_time:95631ms step_avg:61.26ms
step:1562/2245 train_time:95691ms step_avg:61.26ms
step:1563/2245 train_time:95754ms step_avg:61.26ms
step:1564/2245 train_time:95814ms step_avg:61.26ms
step:1565/2245 train_time:95877ms step_avg:61.26ms
step:1566/2245 train_time:95937ms step_avg:61.26ms
step:1567/2245 train_time:96000ms step_avg:61.26ms
step:1568/2245 train_time:96060ms step_avg:61.26ms
step:1569/2245 train_time:96124ms step_avg:61.26ms
step:1570/2245 train_time:96184ms step_avg:61.26ms
step:1571/2245 train_time:96247ms step_avg:61.27ms
step:1572/2245 train_time:96308ms step_avg:61.26ms
step:1573/2245 train_time:96370ms step_avg:61.27ms
step:1574/2245 train_time:96430ms step_avg:61.26ms
step:1575/2245 train_time:96493ms step_avg:61.27ms
step:1576/2245 train_time:96553ms step_avg:61.26ms
step:1577/2245 train_time:96615ms step_avg:61.27ms
step:1578/2245 train_time:96675ms step_avg:61.26ms
step:1579/2245 train_time:96738ms step_avg:61.27ms
step:1580/2245 train_time:96799ms step_avg:61.27ms
step:1581/2245 train_time:96862ms step_avg:61.27ms
step:1582/2245 train_time:96922ms step_avg:61.27ms
step:1583/2245 train_time:96985ms step_avg:61.27ms
step:1584/2245 train_time:97046ms step_avg:61.27ms
step:1585/2245 train_time:97108ms step_avg:61.27ms
step:1586/2245 train_time:97169ms step_avg:61.27ms
step:1587/2245 train_time:97232ms step_avg:61.27ms
step:1588/2245 train_time:97292ms step_avg:61.27ms
step:1589/2245 train_time:97355ms step_avg:61.27ms
step:1590/2245 train_time:97416ms step_avg:61.27ms
step:1591/2245 train_time:97478ms step_avg:61.27ms
step:1592/2245 train_time:97538ms step_avg:61.27ms
step:1593/2245 train_time:97600ms step_avg:61.27ms
step:1594/2245 train_time:97661ms step_avg:61.27ms
step:1595/2245 train_time:97724ms step_avg:61.27ms
step:1596/2245 train_time:97785ms step_avg:61.27ms
step:1597/2245 train_time:97848ms step_avg:61.27ms
step:1598/2245 train_time:97907ms step_avg:61.27ms
step:1599/2245 train_time:97970ms step_avg:61.27ms
step:1600/2245 train_time:98031ms step_avg:61.27ms
step:1601/2245 train_time:98093ms step_avg:61.27ms
step:1602/2245 train_time:98154ms step_avg:61.27ms
step:1603/2245 train_time:98217ms step_avg:61.27ms
step:1604/2245 train_time:98277ms step_avg:61.27ms
step:1605/2245 train_time:98340ms step_avg:61.27ms
step:1606/2245 train_time:98401ms step_avg:61.27ms
step:1607/2245 train_time:98464ms step_avg:61.27ms
step:1608/2245 train_time:98526ms step_avg:61.27ms
step:1609/2245 train_time:98588ms step_avg:61.27ms
step:1610/2245 train_time:98648ms step_avg:61.27ms
step:1611/2245 train_time:98710ms step_avg:61.27ms
step:1612/2245 train_time:98769ms step_avg:61.27ms
step:1613/2245 train_time:98832ms step_avg:61.27ms
step:1614/2245 train_time:98893ms step_avg:61.27ms
step:1615/2245 train_time:98955ms step_avg:61.27ms
step:1616/2245 train_time:99016ms step_avg:61.27ms
step:1617/2245 train_time:99079ms step_avg:61.27ms
step:1618/2245 train_time:99139ms step_avg:61.27ms
step:1619/2245 train_time:99202ms step_avg:61.27ms
step:1620/2245 train_time:99263ms step_avg:61.27ms
step:1621/2245 train_time:99327ms step_avg:61.27ms
step:1622/2245 train_time:99387ms step_avg:61.27ms
step:1623/2245 train_time:99449ms step_avg:61.27ms
step:1624/2245 train_time:99510ms step_avg:61.27ms
step:1625/2245 train_time:99572ms step_avg:61.28ms
step:1626/2245 train_time:99632ms step_avg:61.27ms
step:1627/2245 train_time:99695ms step_avg:61.28ms
step:1628/2245 train_time:99755ms step_avg:61.27ms
step:1629/2245 train_time:99817ms step_avg:61.28ms
step:1630/2245 train_time:99877ms step_avg:61.27ms
step:1631/2245 train_time:99940ms step_avg:61.28ms
step:1632/2245 train_time:100001ms step_avg:61.28ms
step:1633/2245 train_time:100064ms step_avg:61.28ms
step:1634/2245 train_time:100125ms step_avg:61.28ms
step:1635/2245 train_time:100187ms step_avg:61.28ms
step:1636/2245 train_time:100248ms step_avg:61.28ms
step:1637/2245 train_time:100310ms step_avg:61.28ms
step:1638/2245 train_time:100370ms step_avg:61.28ms
step:1639/2245 train_time:100433ms step_avg:61.28ms
step:1640/2245 train_time:100494ms step_avg:61.28ms
step:1641/2245 train_time:100556ms step_avg:61.28ms
step:1642/2245 train_time:100616ms step_avg:61.28ms
step:1643/2245 train_time:100679ms step_avg:61.28ms
step:1644/2245 train_time:100740ms step_avg:61.28ms
step:1645/2245 train_time:100802ms step_avg:61.28ms
step:1646/2245 train_time:100863ms step_avg:61.28ms
step:1647/2245 train_time:100926ms step_avg:61.28ms
step:1648/2245 train_time:100986ms step_avg:61.28ms
step:1649/2245 train_time:101048ms step_avg:61.28ms
step:1650/2245 train_time:101108ms step_avg:61.28ms
step:1651/2245 train_time:101170ms step_avg:61.28ms
step:1652/2245 train_time:101231ms step_avg:61.28ms
step:1653/2245 train_time:101293ms step_avg:61.28ms
step:1654/2245 train_time:101354ms step_avg:61.28ms
step:1655/2245 train_time:101416ms step_avg:61.28ms
step:1656/2245 train_time:101477ms step_avg:61.28ms
step:1657/2245 train_time:101540ms step_avg:61.28ms
step:1658/2245 train_time:101601ms step_avg:61.28ms
step:1659/2245 train_time:101663ms step_avg:61.28ms
step:1660/2245 train_time:101724ms step_avg:61.28ms
step:1661/2245 train_time:101787ms step_avg:61.28ms
step:1662/2245 train_time:101847ms step_avg:61.28ms
step:1663/2245 train_time:101910ms step_avg:61.28ms
step:1664/2245 train_time:101970ms step_avg:61.28ms
step:1665/2245 train_time:102032ms step_avg:61.28ms
step:1666/2245 train_time:102092ms step_avg:61.28ms
step:1667/2245 train_time:102155ms step_avg:61.28ms
step:1668/2245 train_time:102216ms step_avg:61.28ms
step:1669/2245 train_time:102279ms step_avg:61.28ms
step:1670/2245 train_time:102339ms step_avg:61.28ms
step:1671/2245 train_time:102402ms step_avg:61.28ms
step:1672/2245 train_time:102463ms step_avg:61.28ms
step:1673/2245 train_time:102525ms step_avg:61.28ms
step:1674/2245 train_time:102586ms step_avg:61.28ms
step:1675/2245 train_time:102648ms step_avg:61.28ms
step:1676/2245 train_time:102708ms step_avg:61.28ms
step:1677/2245 train_time:102771ms step_avg:61.28ms
step:1678/2245 train_time:102831ms step_avg:61.28ms
step:1679/2245 train_time:102893ms step_avg:61.28ms
step:1680/2245 train_time:102954ms step_avg:61.28ms
step:1681/2245 train_time:103016ms step_avg:61.28ms
step:1682/2245 train_time:103076ms step_avg:61.28ms
step:1683/2245 train_time:103139ms step_avg:61.28ms
step:1684/2245 train_time:103200ms step_avg:61.28ms
step:1685/2245 train_time:103263ms step_avg:61.28ms
step:1686/2245 train_time:103324ms step_avg:61.28ms
step:1687/2245 train_time:103387ms step_avg:61.28ms
step:1688/2245 train_time:103447ms step_avg:61.28ms
step:1689/2245 train_time:103509ms step_avg:61.28ms
step:1690/2245 train_time:103569ms step_avg:61.28ms
step:1691/2245 train_time:103632ms step_avg:61.28ms
step:1692/2245 train_time:103692ms step_avg:61.28ms
step:1693/2245 train_time:103755ms step_avg:61.28ms
step:1694/2245 train_time:103816ms step_avg:61.28ms
step:1695/2245 train_time:103879ms step_avg:61.29ms
step:1696/2245 train_time:103939ms step_avg:61.28ms
step:1697/2245 train_time:104002ms step_avg:61.29ms
step:1698/2245 train_time:104063ms step_avg:61.29ms
step:1699/2245 train_time:104126ms step_avg:61.29ms
step:1700/2245 train_time:104187ms step_avg:61.29ms
step:1701/2245 train_time:104250ms step_avg:61.29ms
step:1702/2245 train_time:104310ms step_avg:61.29ms
step:1703/2245 train_time:104374ms step_avg:61.29ms
step:1704/2245 train_time:104434ms step_avg:61.29ms
step:1705/2245 train_time:104496ms step_avg:61.29ms
step:1706/2245 train_time:104556ms step_avg:61.29ms
step:1707/2245 train_time:104619ms step_avg:61.29ms
step:1708/2245 train_time:104680ms step_avg:61.29ms
step:1709/2245 train_time:104743ms step_avg:61.29ms
step:1710/2245 train_time:104804ms step_avg:61.29ms
step:1711/2245 train_time:104867ms step_avg:61.29ms
step:1712/2245 train_time:104927ms step_avg:61.29ms
step:1713/2245 train_time:104990ms step_avg:61.29ms
step:1714/2245 train_time:105051ms step_avg:61.29ms
step:1715/2245 train_time:105114ms step_avg:61.29ms
step:1716/2245 train_time:105175ms step_avg:61.29ms
step:1717/2245 train_time:105237ms step_avg:61.29ms
step:1718/2245 train_time:105298ms step_avg:61.29ms
step:1719/2245 train_time:105361ms step_avg:61.29ms
step:1720/2245 train_time:105422ms step_avg:61.29ms
step:1721/2245 train_time:105485ms step_avg:61.29ms
step:1722/2245 train_time:105545ms step_avg:61.29ms
step:1723/2245 train_time:105607ms step_avg:61.29ms
step:1724/2245 train_time:105667ms step_avg:61.29ms
step:1725/2245 train_time:105730ms step_avg:61.29ms
step:1726/2245 train_time:105790ms step_avg:61.29ms
step:1727/2245 train_time:105853ms step_avg:61.29ms
step:1728/2245 train_time:105913ms step_avg:61.29ms
step:1729/2245 train_time:105976ms step_avg:61.29ms
step:1730/2245 train_time:106036ms step_avg:61.29ms
step:1731/2245 train_time:106099ms step_avg:61.29ms
step:1732/2245 train_time:106160ms step_avg:61.29ms
step:1733/2245 train_time:106223ms step_avg:61.29ms
step:1734/2245 train_time:106285ms step_avg:61.29ms
step:1735/2245 train_time:106347ms step_avg:61.30ms
step:1736/2245 train_time:106407ms step_avg:61.29ms
step:1737/2245 train_time:106470ms step_avg:61.30ms
step:1738/2245 train_time:106530ms step_avg:61.29ms
step:1739/2245 train_time:106592ms step_avg:61.30ms
step:1740/2245 train_time:106652ms step_avg:61.29ms
step:1741/2245 train_time:106715ms step_avg:61.30ms
step:1742/2245 train_time:106775ms step_avg:61.29ms
step:1743/2245 train_time:106838ms step_avg:61.30ms
step:1744/2245 train_time:106899ms step_avg:61.30ms
step:1745/2245 train_time:106961ms step_avg:61.30ms
step:1746/2245 train_time:107022ms step_avg:61.30ms
step:1747/2245 train_time:107085ms step_avg:61.30ms
step:1748/2245 train_time:107145ms step_avg:61.30ms
step:1749/2245 train_time:107208ms step_avg:61.30ms
step:1750/2245 train_time:107268ms step_avg:61.30ms
step:1750/2245 val_loss:3.3775 train_time:107332ms step_avg:61.33ms
step:1751/2245 train_time:107351ms step_avg:61.31ms
step:1752/2245 train_time:107393ms step_avg:61.30ms
step:1753/2245 train_time:107461ms step_avg:61.30ms
step:1754/2245 train_time:107523ms step_avg:61.30ms
step:1755/2245 train_time:107586ms step_avg:61.30ms
step:1756/2245 train_time:107646ms step_avg:61.30ms
step:1757/2245 train_time:107708ms step_avg:61.30ms
step:1758/2245 train_time:107768ms step_avg:61.30ms
step:1759/2245 train_time:107829ms step_avg:61.30ms
step:1760/2245 train_time:107889ms step_avg:61.30ms
step:1761/2245 train_time:107951ms step_avg:61.30ms
step:1762/2245 train_time:108011ms step_avg:61.30ms
step:1763/2245 train_time:108072ms step_avg:61.30ms
step:1764/2245 train_time:108132ms step_avg:61.30ms
step:1765/2245 train_time:108194ms step_avg:61.30ms
step:1766/2245 train_time:108255ms step_avg:61.30ms
step:1767/2245 train_time:108319ms step_avg:61.30ms
step:1768/2245 train_time:108380ms step_avg:61.30ms
step:1769/2245 train_time:108445ms step_avg:61.30ms
step:1770/2245 train_time:108506ms step_avg:61.30ms
step:1771/2245 train_time:108570ms step_avg:61.30ms
step:1772/2245 train_time:108630ms step_avg:61.30ms
step:1773/2245 train_time:108693ms step_avg:61.30ms
step:1774/2245 train_time:108753ms step_avg:61.30ms
step:1775/2245 train_time:108816ms step_avg:61.30ms
step:1776/2245 train_time:108875ms step_avg:61.30ms
step:1777/2245 train_time:108938ms step_avg:61.30ms
step:1778/2245 train_time:108998ms step_avg:61.30ms
step:1779/2245 train_time:109061ms step_avg:61.30ms
step:1780/2245 train_time:109121ms step_avg:61.30ms
step:1781/2245 train_time:109183ms step_avg:61.30ms
step:1782/2245 train_time:109244ms step_avg:61.30ms
step:1783/2245 train_time:109307ms step_avg:61.31ms
step:1784/2245 train_time:109368ms step_avg:61.30ms
step:1785/2245 train_time:109431ms step_avg:61.31ms
step:1786/2245 train_time:109492ms step_avg:61.31ms
step:1787/2245 train_time:109556ms step_avg:61.31ms
step:1788/2245 train_time:109616ms step_avg:61.31ms
step:1789/2245 train_time:109679ms step_avg:61.31ms
step:1790/2245 train_time:109741ms step_avg:61.31ms
step:1791/2245 train_time:109804ms step_avg:61.31ms
step:1792/2245 train_time:109864ms step_avg:61.31ms
step:1793/2245 train_time:109925ms step_avg:61.31ms
step:1794/2245 train_time:109985ms step_avg:61.31ms
step:1795/2245 train_time:110047ms step_avg:61.31ms
step:1796/2245 train_time:110107ms step_avg:61.31ms
step:1797/2245 train_time:110169ms step_avg:61.31ms
step:1798/2245 train_time:110229ms step_avg:61.31ms
step:1799/2245 train_time:110292ms step_avg:61.31ms
step:1800/2245 train_time:110352ms step_avg:61.31ms
step:1801/2245 train_time:110415ms step_avg:61.31ms
step:1802/2245 train_time:110476ms step_avg:61.31ms
step:1803/2245 train_time:110540ms step_avg:61.31ms
step:1804/2245 train_time:110602ms step_avg:61.31ms
step:1805/2245 train_time:110664ms step_avg:61.31ms
step:1806/2245 train_time:110725ms step_avg:61.31ms
step:1807/2245 train_time:110788ms step_avg:61.31ms
step:1808/2245 train_time:110848ms step_avg:61.31ms
step:1809/2245 train_time:110910ms step_avg:61.31ms
step:1810/2245 train_time:110970ms step_avg:61.31ms
step:1811/2245 train_time:111032ms step_avg:61.31ms
step:1812/2245 train_time:111092ms step_avg:61.31ms
step:1813/2245 train_time:111155ms step_avg:61.31ms
step:1814/2245 train_time:111215ms step_avg:61.31ms
step:1815/2245 train_time:111277ms step_avg:61.31ms
step:1816/2245 train_time:111338ms step_avg:61.31ms
step:1817/2245 train_time:111401ms step_avg:61.31ms
step:1818/2245 train_time:111462ms step_avg:61.31ms
step:1819/2245 train_time:111526ms step_avg:61.31ms
step:1820/2245 train_time:111586ms step_avg:61.31ms
step:1821/2245 train_time:111648ms step_avg:61.31ms
step:1822/2245 train_time:111709ms step_avg:61.31ms
step:1823/2245 train_time:111771ms step_avg:61.31ms
step:1824/2245 train_time:111831ms step_avg:61.31ms
step:1825/2245 train_time:111893ms step_avg:61.31ms
step:1826/2245 train_time:111955ms step_avg:61.31ms
step:1827/2245 train_time:112018ms step_avg:61.31ms
step:1828/2245 train_time:112078ms step_avg:61.31ms
step:1829/2245 train_time:112141ms step_avg:61.31ms
step:1830/2245 train_time:112202ms step_avg:61.31ms
step:1831/2245 train_time:112264ms step_avg:61.31ms
step:1832/2245 train_time:112324ms step_avg:61.31ms
step:1833/2245 train_time:112387ms step_avg:61.31ms
step:1834/2245 train_time:112447ms step_avg:61.31ms
step:1835/2245 train_time:112510ms step_avg:61.31ms
step:1836/2245 train_time:112571ms step_avg:61.31ms
step:1837/2245 train_time:112633ms step_avg:61.31ms
step:1838/2245 train_time:112694ms step_avg:61.31ms
step:1839/2245 train_time:112757ms step_avg:61.31ms
step:1840/2245 train_time:112818ms step_avg:61.31ms
step:1841/2245 train_time:112880ms step_avg:61.31ms
step:1842/2245 train_time:112941ms step_avg:61.31ms
step:1843/2245 train_time:113004ms step_avg:61.32ms
step:1844/2245 train_time:113065ms step_avg:61.31ms
step:1845/2245 train_time:113127ms step_avg:61.32ms
step:1846/2245 train_time:113187ms step_avg:61.31ms
step:1847/2245 train_time:113250ms step_avg:61.32ms
step:1848/2245 train_time:113310ms step_avg:61.32ms
step:1849/2245 train_time:113373ms step_avg:61.32ms
step:1850/2245 train_time:113434ms step_avg:61.32ms
step:1851/2245 train_time:113497ms step_avg:61.32ms
step:1852/2245 train_time:113558ms step_avg:61.32ms
step:1853/2245 train_time:113621ms step_avg:61.32ms
step:1854/2245 train_time:113683ms step_avg:61.32ms
step:1855/2245 train_time:113747ms step_avg:61.32ms
step:1856/2245 train_time:113807ms step_avg:61.32ms
step:1857/2245 train_time:113870ms step_avg:61.32ms
step:1858/2245 train_time:113930ms step_avg:61.32ms
step:1859/2245 train_time:113992ms step_avg:61.32ms
step:1860/2245 train_time:114053ms step_avg:61.32ms
step:1861/2245 train_time:114116ms step_avg:61.32ms
step:1862/2245 train_time:114176ms step_avg:61.32ms
step:1863/2245 train_time:114238ms step_avg:61.32ms
step:1864/2245 train_time:114300ms step_avg:61.32ms
step:1865/2245 train_time:114363ms step_avg:61.32ms
step:1866/2245 train_time:114424ms step_avg:61.32ms
step:1867/2245 train_time:114487ms step_avg:61.32ms
step:1868/2245 train_time:114547ms step_avg:61.32ms
step:1869/2245 train_time:114609ms step_avg:61.32ms
step:1870/2245 train_time:114670ms step_avg:61.32ms
step:1871/2245 train_time:114733ms step_avg:61.32ms
step:1872/2245 train_time:114793ms step_avg:61.32ms
step:1873/2245 train_time:114856ms step_avg:61.32ms
step:1874/2245 train_time:114916ms step_avg:61.32ms
step:1875/2245 train_time:114979ms step_avg:61.32ms
step:1876/2245 train_time:115039ms step_avg:61.32ms
step:1877/2245 train_time:115103ms step_avg:61.32ms
step:1878/2245 train_time:115163ms step_avg:61.32ms
step:1879/2245 train_time:115226ms step_avg:61.32ms
step:1880/2245 train_time:115286ms step_avg:61.32ms
step:1881/2245 train_time:115349ms step_avg:61.32ms
step:1882/2245 train_time:115410ms step_avg:61.32ms
step:1883/2245 train_time:115471ms step_avg:61.32ms
step:1884/2245 train_time:115532ms step_avg:61.32ms
step:1885/2245 train_time:115595ms step_avg:61.32ms
step:1886/2245 train_time:115656ms step_avg:61.32ms
step:1887/2245 train_time:115718ms step_avg:61.32ms
step:1888/2245 train_time:115779ms step_avg:61.32ms
step:1889/2245 train_time:115843ms step_avg:61.33ms
step:1890/2245 train_time:115904ms step_avg:61.32ms
step:1891/2245 train_time:115967ms step_avg:61.33ms
step:1892/2245 train_time:116026ms step_avg:61.32ms
step:1893/2245 train_time:116089ms step_avg:61.33ms
step:1894/2245 train_time:116150ms step_avg:61.33ms
step:1895/2245 train_time:116212ms step_avg:61.33ms
step:1896/2245 train_time:116273ms step_avg:61.33ms
step:1897/2245 train_time:116336ms step_avg:61.33ms
step:1898/2245 train_time:116396ms step_avg:61.33ms
step:1899/2245 train_time:116459ms step_avg:61.33ms
step:1900/2245 train_time:116520ms step_avg:61.33ms
step:1901/2245 train_time:116584ms step_avg:61.33ms
step:1902/2245 train_time:116644ms step_avg:61.33ms
step:1903/2245 train_time:116707ms step_avg:61.33ms
step:1904/2245 train_time:116767ms step_avg:61.33ms
step:1905/2245 train_time:116829ms step_avg:61.33ms
step:1906/2245 train_time:116890ms step_avg:61.33ms
step:1907/2245 train_time:116953ms step_avg:61.33ms
step:1908/2245 train_time:117013ms step_avg:61.33ms
step:1909/2245 train_time:117075ms step_avg:61.33ms
step:1910/2245 train_time:117136ms step_avg:61.33ms
step:1911/2245 train_time:117199ms step_avg:61.33ms
step:1912/2245 train_time:117260ms step_avg:61.33ms
step:1913/2245 train_time:117323ms step_avg:61.33ms
step:1914/2245 train_time:117384ms step_avg:61.33ms
step:1915/2245 train_time:117447ms step_avg:61.33ms
step:1916/2245 train_time:117507ms step_avg:61.33ms
step:1917/2245 train_time:117571ms step_avg:61.33ms
step:1918/2245 train_time:117631ms step_avg:61.33ms
step:1919/2245 train_time:117695ms step_avg:61.33ms
step:1920/2245 train_time:117755ms step_avg:61.33ms
step:1921/2245 train_time:117818ms step_avg:61.33ms
step:1922/2245 train_time:117880ms step_avg:61.33ms
step:1923/2245 train_time:117943ms step_avg:61.33ms
step:1924/2245 train_time:118004ms step_avg:61.33ms
step:1925/2245 train_time:118066ms step_avg:61.33ms
step:1926/2245 train_time:118127ms step_avg:61.33ms
step:1927/2245 train_time:118189ms step_avg:61.33ms
step:1928/2245 train_time:118249ms step_avg:61.33ms
step:1929/2245 train_time:118312ms step_avg:61.33ms
step:1930/2245 train_time:118373ms step_avg:61.33ms
step:1931/2245 train_time:118436ms step_avg:61.33ms
step:1932/2245 train_time:118496ms step_avg:61.33ms
step:1933/2245 train_time:118559ms step_avg:61.33ms
step:1934/2245 train_time:118620ms step_avg:61.33ms
step:1935/2245 train_time:118684ms step_avg:61.34ms
step:1936/2245 train_time:118745ms step_avg:61.34ms
step:1937/2245 train_time:118808ms step_avg:61.34ms
step:1938/2245 train_time:118868ms step_avg:61.34ms
step:1939/2245 train_time:118930ms step_avg:61.34ms
step:1940/2245 train_time:118991ms step_avg:61.34ms
step:1941/2245 train_time:119053ms step_avg:61.34ms
step:1942/2245 train_time:119114ms step_avg:61.34ms
step:1943/2245 train_time:119177ms step_avg:61.34ms
step:1944/2245 train_time:119238ms step_avg:61.34ms
step:1945/2245 train_time:119300ms step_avg:61.34ms
step:1946/2245 train_time:119361ms step_avg:61.34ms
step:1947/2245 train_time:119423ms step_avg:61.34ms
step:1948/2245 train_time:119483ms step_avg:61.34ms
step:1949/2245 train_time:119546ms step_avg:61.34ms
step:1950/2245 train_time:119606ms step_avg:61.34ms
step:1951/2245 train_time:119669ms step_avg:61.34ms
step:1952/2245 train_time:119729ms step_avg:61.34ms
step:1953/2245 train_time:119791ms step_avg:61.34ms
step:1954/2245 train_time:119852ms step_avg:61.34ms
step:1955/2245 train_time:119914ms step_avg:61.34ms
step:1956/2245 train_time:119975ms step_avg:61.34ms
step:1957/2245 train_time:120038ms step_avg:61.34ms
step:1958/2245 train_time:120098ms step_avg:61.34ms
step:1959/2245 train_time:120161ms step_avg:61.34ms
step:1960/2245 train_time:120222ms step_avg:61.34ms
step:1961/2245 train_time:120285ms step_avg:61.34ms
step:1962/2245 train_time:120345ms step_avg:61.34ms
step:1963/2245 train_time:120408ms step_avg:61.34ms
step:1964/2245 train_time:120468ms step_avg:61.34ms
step:1965/2245 train_time:120531ms step_avg:61.34ms
step:1966/2245 train_time:120592ms step_avg:61.34ms
step:1967/2245 train_time:120656ms step_avg:61.34ms
step:1968/2245 train_time:120716ms step_avg:61.34ms
step:1969/2245 train_time:120779ms step_avg:61.34ms
step:1970/2245 train_time:120840ms step_avg:61.34ms
step:1971/2245 train_time:120903ms step_avg:61.34ms
step:1972/2245 train_time:120963ms step_avg:61.34ms
step:1973/2245 train_time:121026ms step_avg:61.34ms
step:1974/2245 train_time:121086ms step_avg:61.34ms
step:1975/2245 train_time:121149ms step_avg:61.34ms
step:1976/2245 train_time:121209ms step_avg:61.34ms
step:1977/2245 train_time:121272ms step_avg:61.34ms
step:1978/2245 train_time:121332ms step_avg:61.34ms
step:1979/2245 train_time:121396ms step_avg:61.34ms
step:1980/2245 train_time:121456ms step_avg:61.34ms
step:1981/2245 train_time:121519ms step_avg:61.34ms
step:1982/2245 train_time:121580ms step_avg:61.34ms
step:1983/2245 train_time:121643ms step_avg:61.34ms
step:1984/2245 train_time:121705ms step_avg:61.34ms
step:1985/2245 train_time:121768ms step_avg:61.34ms
step:1986/2245 train_time:121828ms step_avg:61.34ms
step:1987/2245 train_time:121891ms step_avg:61.34ms
step:1988/2245 train_time:121952ms step_avg:61.34ms
step:1989/2245 train_time:122014ms step_avg:61.34ms
step:1990/2245 train_time:122075ms step_avg:61.34ms
step:1991/2245 train_time:122138ms step_avg:61.34ms
step:1992/2245 train_time:122199ms step_avg:61.34ms
step:1993/2245 train_time:122262ms step_avg:61.35ms
step:1994/2245 train_time:122323ms step_avg:61.35ms
step:1995/2245 train_time:122387ms step_avg:61.35ms
step:1996/2245 train_time:122447ms step_avg:61.35ms
step:1997/2245 train_time:122510ms step_avg:61.35ms
step:1998/2245 train_time:122571ms step_avg:61.35ms
step:1999/2245 train_time:122634ms step_avg:61.35ms
step:2000/2245 train_time:122694ms step_avg:61.35ms
step:2000/2245 val_loss:3.3227 train_time:122758ms step_avg:61.38ms
step:2001/2245 train_time:122777ms step_avg:61.36ms
step:2002/2245 train_time:122820ms step_avg:61.35ms
step:2003/2245 train_time:122888ms step_avg:61.35ms
step:2004/2245 train_time:122951ms step_avg:61.35ms
step:2005/2245 train_time:123013ms step_avg:61.35ms
step:2006/2245 train_time:123075ms step_avg:61.35ms
step:2007/2245 train_time:123137ms step_avg:61.35ms
step:2008/2245 train_time:123197ms step_avg:61.35ms
step:2009/2245 train_time:123260ms step_avg:61.35ms
step:2010/2245 train_time:123320ms step_avg:61.35ms
step:2011/2245 train_time:123383ms step_avg:61.35ms
step:2012/2245 train_time:123443ms step_avg:61.35ms
step:2013/2245 train_time:123506ms step_avg:61.35ms
step:2014/2245 train_time:123565ms step_avg:61.35ms
step:2015/2245 train_time:123627ms step_avg:61.35ms
step:2016/2245 train_time:123687ms step_avg:61.35ms
step:2017/2245 train_time:123750ms step_avg:61.35ms
step:2018/2245 train_time:123813ms step_avg:61.35ms
step:2019/2245 train_time:123877ms step_avg:61.36ms
step:2020/2245 train_time:123939ms step_avg:61.36ms
step:2021/2245 train_time:124004ms step_avg:61.36ms
step:2022/2245 train_time:124065ms step_avg:61.36ms
step:2023/2245 train_time:124127ms step_avg:61.36ms
step:2024/2245 train_time:124187ms step_avg:61.36ms
step:2025/2245 train_time:124250ms step_avg:61.36ms
step:2026/2245 train_time:124310ms step_avg:61.36ms
step:2027/2245 train_time:124373ms step_avg:61.36ms
step:2028/2245 train_time:124433ms step_avg:61.36ms
step:2029/2245 train_time:124495ms step_avg:61.36ms
step:2030/2245 train_time:124555ms step_avg:61.36ms
step:2031/2245 train_time:124617ms step_avg:61.36ms
step:2032/2245 train_time:124678ms step_avg:61.36ms
step:2033/2245 train_time:124741ms step_avg:61.36ms
step:2034/2245 train_time:124802ms step_avg:61.36ms
step:2035/2245 train_time:124866ms step_avg:61.36ms
step:2036/2245 train_time:124928ms step_avg:61.36ms
step:2037/2245 train_time:124991ms step_avg:61.36ms
step:2038/2245 train_time:125052ms step_avg:61.36ms
step:2039/2245 train_time:125115ms step_avg:61.36ms
step:2040/2245 train_time:125176ms step_avg:61.36ms
step:2041/2245 train_time:125239ms step_avg:61.36ms
step:2042/2245 train_time:125300ms step_avg:61.36ms
step:2043/2245 train_time:125363ms step_avg:61.36ms
step:2044/2245 train_time:125423ms step_avg:61.36ms
step:2045/2245 train_time:125485ms step_avg:61.36ms
step:2046/2245 train_time:125545ms step_avg:61.36ms
step:2047/2245 train_time:125607ms step_avg:61.36ms
step:2048/2245 train_time:125667ms step_avg:61.36ms
step:2049/2245 train_time:125729ms step_avg:61.36ms
step:2050/2245 train_time:125790ms step_avg:61.36ms
step:2051/2245 train_time:125854ms step_avg:61.36ms
step:2052/2245 train_time:125915ms step_avg:61.36ms
step:2053/2245 train_time:125978ms step_avg:61.36ms
step:2054/2245 train_time:126039ms step_avg:61.36ms
step:2055/2245 train_time:126102ms step_avg:61.36ms
step:2056/2245 train_time:126163ms step_avg:61.36ms
step:2057/2245 train_time:126225ms step_avg:61.36ms
step:2058/2245 train_time:126285ms step_avg:61.36ms
step:2059/2245 train_time:126348ms step_avg:61.36ms
step:2060/2245 train_time:126409ms step_avg:61.36ms
step:2061/2245 train_time:126471ms step_avg:61.36ms
step:2062/2245 train_time:126531ms step_avg:61.36ms
step:2063/2245 train_time:126594ms step_avg:61.36ms
step:2064/2245 train_time:126654ms step_avg:61.36ms
step:2065/2245 train_time:126717ms step_avg:61.36ms
step:2066/2245 train_time:126778ms step_avg:61.36ms
step:2067/2245 train_time:126841ms step_avg:61.36ms
step:2068/2245 train_time:126902ms step_avg:61.36ms
step:2069/2245 train_time:126965ms step_avg:61.37ms
step:2070/2245 train_time:127026ms step_avg:61.37ms
step:2071/2245 train_time:127089ms step_avg:61.37ms
step:2072/2245 train_time:127149ms step_avg:61.37ms
step:2073/2245 train_time:127212ms step_avg:61.37ms
step:2074/2245 train_time:127273ms step_avg:61.37ms
step:2075/2245 train_time:127336ms step_avg:61.37ms
step:2076/2245 train_time:127396ms step_avg:61.37ms
step:2077/2245 train_time:127459ms step_avg:61.37ms
step:2078/2245 train_time:127520ms step_avg:61.37ms
step:2079/2245 train_time:127582ms step_avg:61.37ms
step:2080/2245 train_time:127643ms step_avg:61.37ms
step:2081/2245 train_time:127705ms step_avg:61.37ms
step:2082/2245 train_time:127766ms step_avg:61.37ms
step:2083/2245 train_time:127828ms step_avg:61.37ms
step:2084/2245 train_time:127889ms step_avg:61.37ms
step:2085/2245 train_time:127952ms step_avg:61.37ms
step:2086/2245 train_time:128013ms step_avg:61.37ms
step:2087/2245 train_time:128075ms step_avg:61.37ms
step:2088/2245 train_time:128136ms step_avg:61.37ms
step:2089/2245 train_time:128199ms step_avg:61.37ms
step:2090/2245 train_time:128260ms step_avg:61.37ms
step:2091/2245 train_time:128323ms step_avg:61.37ms
step:2092/2245 train_time:128384ms step_avg:61.37ms
step:2093/2245 train_time:128447ms step_avg:61.37ms
step:2094/2245 train_time:128507ms step_avg:61.37ms
step:2095/2245 train_time:128569ms step_avg:61.37ms
step:2096/2245 train_time:128630ms step_avg:61.37ms
step:2097/2245 train_time:128693ms step_avg:61.37ms
step:2098/2245 train_time:128753ms step_avg:61.37ms
step:2099/2245 train_time:128816ms step_avg:61.37ms
step:2100/2245 train_time:128877ms step_avg:61.37ms
step:2101/2245 train_time:128940ms step_avg:61.37ms
step:2102/2245 train_time:129001ms step_avg:61.37ms
step:2103/2245 train_time:129065ms step_avg:61.37ms
step:2104/2245 train_time:129125ms step_avg:61.37ms
step:2105/2245 train_time:129188ms step_avg:61.37ms
step:2106/2245 train_time:129248ms step_avg:61.37ms
step:2107/2245 train_time:129310ms step_avg:61.37ms
step:2108/2245 train_time:129371ms step_avg:61.37ms
step:2109/2245 train_time:129434ms step_avg:61.37ms
step:2110/2245 train_time:129494ms step_avg:61.37ms
step:2111/2245 train_time:129557ms step_avg:61.37ms
step:2112/2245 train_time:129618ms step_avg:61.37ms
step:2113/2245 train_time:129681ms step_avg:61.37ms
step:2114/2245 train_time:129741ms step_avg:61.37ms
step:2115/2245 train_time:129804ms step_avg:61.37ms
step:2116/2245 train_time:129865ms step_avg:61.37ms
step:2117/2245 train_time:129928ms step_avg:61.37ms
step:2118/2245 train_time:129988ms step_avg:61.37ms
step:2119/2245 train_time:130051ms step_avg:61.37ms
step:2120/2245 train_time:130111ms step_avg:61.37ms
step:2121/2245 train_time:130174ms step_avg:61.37ms
step:2122/2245 train_time:130235ms step_avg:61.37ms
step:2123/2245 train_time:130298ms step_avg:61.37ms
step:2124/2245 train_time:130359ms step_avg:61.37ms
step:2125/2245 train_time:130422ms step_avg:61.38ms
step:2126/2245 train_time:130483ms step_avg:61.37ms
step:2127/2245 train_time:130545ms step_avg:61.38ms
step:2128/2245 train_time:130605ms step_avg:61.37ms
step:2129/2245 train_time:130668ms step_avg:61.38ms
step:2130/2245 train_time:130728ms step_avg:61.37ms
step:2131/2245 train_time:130791ms step_avg:61.38ms
step:2132/2245 train_time:130851ms step_avg:61.37ms
step:2133/2245 train_time:130915ms step_avg:61.38ms
step:2134/2245 train_time:130975ms step_avg:61.38ms
step:2135/2245 train_time:131038ms step_avg:61.38ms
step:2136/2245 train_time:131099ms step_avg:61.38ms
step:2137/2245 train_time:131162ms step_avg:61.38ms
step:2138/2245 train_time:131223ms step_avg:61.38ms
step:2139/2245 train_time:131285ms step_avg:61.38ms
step:2140/2245 train_time:131346ms step_avg:61.38ms
step:2141/2245 train_time:131408ms step_avg:61.38ms
step:2142/2245 train_time:131469ms step_avg:61.38ms
step:2143/2245 train_time:131532ms step_avg:61.38ms
step:2144/2245 train_time:131592ms step_avg:61.38ms
step:2145/2245 train_time:131655ms step_avg:61.38ms
step:2146/2245 train_time:131715ms step_avg:61.38ms
step:2147/2245 train_time:131778ms step_avg:61.38ms
step:2148/2245 train_time:131838ms step_avg:61.38ms
step:2149/2245 train_time:131902ms step_avg:61.38ms
step:2150/2245 train_time:131963ms step_avg:61.38ms
step:2151/2245 train_time:132026ms step_avg:61.38ms
step:2152/2245 train_time:132086ms step_avg:61.38ms
step:2153/2245 train_time:132149ms step_avg:61.38ms
step:2154/2245 train_time:132210ms step_avg:61.38ms
step:2155/2245 train_time:132272ms step_avg:61.38ms
step:2156/2245 train_time:132333ms step_avg:61.38ms
step:2157/2245 train_time:132395ms step_avg:61.38ms
step:2158/2245 train_time:132456ms step_avg:61.38ms
step:2159/2245 train_time:132520ms step_avg:61.38ms
step:2160/2245 train_time:132580ms step_avg:61.38ms
step:2161/2245 train_time:132643ms step_avg:61.38ms
step:2162/2245 train_time:132703ms step_avg:61.38ms
step:2163/2245 train_time:132766ms step_avg:61.38ms
step:2164/2245 train_time:132826ms step_avg:61.38ms
step:2165/2245 train_time:132889ms step_avg:61.38ms
step:2166/2245 train_time:132950ms step_avg:61.38ms
step:2167/2245 train_time:133014ms step_avg:61.38ms
step:2168/2245 train_time:133074ms step_avg:61.38ms
step:2169/2245 train_time:133137ms step_avg:61.38ms
step:2170/2245 train_time:133198ms step_avg:61.38ms
step:2171/2245 train_time:133261ms step_avg:61.38ms
step:2172/2245 train_time:133323ms step_avg:61.38ms
step:2173/2245 train_time:133385ms step_avg:61.38ms
step:2174/2245 train_time:133446ms step_avg:61.38ms
step:2175/2245 train_time:133508ms step_avg:61.38ms
step:2176/2245 train_time:133568ms step_avg:61.38ms
step:2177/2245 train_time:133630ms step_avg:61.38ms
step:2178/2245 train_time:133691ms step_avg:61.38ms
step:2179/2245 train_time:133753ms step_avg:61.38ms
step:2180/2245 train_time:133814ms step_avg:61.38ms
step:2181/2245 train_time:133877ms step_avg:61.38ms
step:2182/2245 train_time:133938ms step_avg:61.38ms
step:2183/2245 train_time:134001ms step_avg:61.38ms
step:2184/2245 train_time:134062ms step_avg:61.38ms
step:2185/2245 train_time:134125ms step_avg:61.38ms
step:2186/2245 train_time:134186ms step_avg:61.38ms
step:2187/2245 train_time:134248ms step_avg:61.38ms
step:2188/2245 train_time:134309ms step_avg:61.38ms
step:2189/2245 train_time:134371ms step_avg:61.38ms
step:2190/2245 train_time:134432ms step_avg:61.38ms
step:2191/2245 train_time:134495ms step_avg:61.39ms
step:2192/2245 train_time:134556ms step_avg:61.39ms
step:2193/2245 train_time:134619ms step_avg:61.39ms
step:2194/2245 train_time:134680ms step_avg:61.39ms
step:2195/2245 train_time:134742ms step_avg:61.39ms
step:2196/2245 train_time:134803ms step_avg:61.39ms
step:2197/2245 train_time:134865ms step_avg:61.39ms
step:2198/2245 train_time:134926ms step_avg:61.39ms
step:2199/2245 train_time:134988ms step_avg:61.39ms
step:2200/2245 train_time:135049ms step_avg:61.39ms
step:2201/2245 train_time:135112ms step_avg:61.39ms
step:2202/2245 train_time:135173ms step_avg:61.39ms
step:2203/2245 train_time:135237ms step_avg:61.39ms
step:2204/2245 train_time:135297ms step_avg:61.39ms
step:2205/2245 train_time:135360ms step_avg:61.39ms
step:2206/2245 train_time:135422ms step_avg:61.39ms
step:2207/2245 train_time:135485ms step_avg:61.39ms
step:2208/2245 train_time:135546ms step_avg:61.39ms
step:2209/2245 train_time:135608ms step_avg:61.39ms
step:2210/2245 train_time:135669ms step_avg:61.39ms
step:2211/2245 train_time:135732ms step_avg:61.39ms
step:2212/2245 train_time:135792ms step_avg:61.39ms
step:2213/2245 train_time:135855ms step_avg:61.39ms
step:2214/2245 train_time:135916ms step_avg:61.39ms
step:2215/2245 train_time:135979ms step_avg:61.39ms
step:2216/2245 train_time:136040ms step_avg:61.39ms
step:2217/2245 train_time:136102ms step_avg:61.39ms
step:2218/2245 train_time:136163ms step_avg:61.39ms
step:2219/2245 train_time:136226ms step_avg:61.39ms
step:2220/2245 train_time:136286ms step_avg:61.39ms
step:2221/2245 train_time:136350ms step_avg:61.39ms
step:2222/2245 train_time:136410ms step_avg:61.39ms
step:2223/2245 train_time:136473ms step_avg:61.39ms
step:2224/2245 train_time:136534ms step_avg:61.39ms
step:2225/2245 train_time:136597ms step_avg:61.39ms
step:2226/2245 train_time:136658ms step_avg:61.39ms
step:2227/2245 train_time:136721ms step_avg:61.39ms
step:2228/2245 train_time:136782ms step_avg:61.39ms
step:2229/2245 train_time:136845ms step_avg:61.39ms
step:2230/2245 train_time:136905ms step_avg:61.39ms
step:2231/2245 train_time:136968ms step_avg:61.39ms
step:2232/2245 train_time:137029ms step_avg:61.39ms
step:2233/2245 train_time:137092ms step_avg:61.39ms
step:2234/2245 train_time:137152ms step_avg:61.39ms
step:2235/2245 train_time:137216ms step_avg:61.39ms
step:2236/2245 train_time:137277ms step_avg:61.39ms
step:2237/2245 train_time:137341ms step_avg:61.40ms
step:2238/2245 train_time:137402ms step_avg:61.40ms
step:2239/2245 train_time:137465ms step_avg:61.40ms
step:2240/2245 train_time:137526ms step_avg:61.40ms
step:2241/2245 train_time:137588ms step_avg:61.40ms
step:2242/2245 train_time:137649ms step_avg:61.40ms
step:2243/2245 train_time:137712ms step_avg:61.40ms
step:2244/2245 train_time:137773ms step_avg:61.40ms
step:2245/2245 train_time:137836ms step_avg:61.40ms
step:2245/2245 val_loss:3.2779 train_time:137898ms step_avg:61.42ms
peak memory allocated: 29244 MiB reserved: 44256 MiB

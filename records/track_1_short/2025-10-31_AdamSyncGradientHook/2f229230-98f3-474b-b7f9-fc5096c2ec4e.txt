import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)


            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))



            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal
        self.group_to_param_group = {}

        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                self.group_to_param_group[param] = group

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        world_size = dist.get_world_size()
        grad = param.grad
        rank_size = grad.shape[0] // world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(), grad_slice)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']

            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
        # enable sync in the next training step for the adam optimizer
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = next(train_loader)

        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()


====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251031+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 31 16:35:12 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   38C    P0            126W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   32C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   31C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2315 train_time:77ms step_avg:77.07ms
step:2/2315 train_time:198ms step_avg:98.89ms
step:3/2315 train_time:219ms step_avg:72.97ms
step:4/2315 train_time:255ms step_avg:63.69ms
step:5/2315 train_time:313ms step_avg:62.53ms
step:6/2315 train_time:372ms step_avg:62.01ms
step:7/2315 train_time:431ms step_avg:61.51ms
step:8/2315 train_time:490ms step_avg:61.29ms
step:9/2315 train_time:549ms step_avg:61.04ms
step:10/2315 train_time:609ms step_avg:60.93ms
step:11/2315 train_time:668ms step_avg:60.74ms
step:12/2315 train_time:728ms step_avg:60.68ms
step:13/2315 train_time:788ms step_avg:60.58ms
step:14/2315 train_time:848ms step_avg:60.54ms
step:15/2315 train_time:907ms step_avg:60.44ms
step:16/2315 train_time:966ms step_avg:60.40ms
step:17/2315 train_time:1026ms step_avg:60.34ms
step:18/2315 train_time:1088ms step_avg:60.42ms
step:19/2315 train_time:1153ms step_avg:60.68ms
step:20/2315 train_time:1216ms step_avg:60.80ms
step:21/2315 train_time:1276ms step_avg:60.77ms
step:22/2315 train_time:1337ms step_avg:60.76ms
step:23/2315 train_time:1397ms step_avg:60.73ms
step:24/2315 train_time:1458ms step_avg:60.74ms
step:25/2315 train_time:1518ms step_avg:60.71ms
step:26/2315 train_time:1579ms step_avg:60.72ms
step:27/2315 train_time:1639ms step_avg:60.69ms
step:28/2315 train_time:1699ms step_avg:60.68ms
step:29/2315 train_time:1759ms step_avg:60.64ms
step:30/2315 train_time:1820ms step_avg:60.67ms
step:31/2315 train_time:1880ms step_avg:60.66ms
step:32/2315 train_time:1941ms step_avg:60.66ms
step:33/2315 train_time:2001ms step_avg:60.62ms
step:34/2315 train_time:2062ms step_avg:60.65ms
step:35/2315 train_time:2124ms step_avg:60.70ms
step:36/2315 train_time:2187ms step_avg:60.74ms
step:37/2315 train_time:2248ms step_avg:60.76ms
step:38/2315 train_time:2308ms step_avg:60.74ms
step:39/2315 train_time:2369ms step_avg:60.73ms
step:40/2315 train_time:2430ms step_avg:60.76ms
step:41/2315 train_time:2491ms step_avg:60.75ms
step:42/2315 train_time:2552ms step_avg:60.75ms
step:43/2315 train_time:2611ms step_avg:60.73ms
step:44/2315 train_time:2672ms step_avg:60.72ms
step:45/2315 train_time:2732ms step_avg:60.70ms
step:46/2315 train_time:2792ms step_avg:60.69ms
step:47/2315 train_time:2852ms step_avg:60.67ms
step:48/2315 train_time:2913ms step_avg:60.68ms
step:49/2315 train_time:2973ms step_avg:60.66ms
step:50/2315 train_time:3033ms step_avg:60.66ms
step:51/2315 train_time:3093ms step_avg:60.64ms
step:52/2315 train_time:3153ms step_avg:60.64ms
step:53/2315 train_time:3213ms step_avg:60.63ms
step:54/2315 train_time:3274ms step_avg:60.63ms
step:55/2315 train_time:3334ms step_avg:60.62ms
step:56/2315 train_time:3395ms step_avg:60.62ms
step:57/2315 train_time:3455ms step_avg:60.62ms
step:58/2315 train_time:3516ms step_avg:60.63ms
step:59/2315 train_time:3576ms step_avg:60.61ms
step:60/2315 train_time:3637ms step_avg:60.62ms
step:61/2315 train_time:3697ms step_avg:60.60ms
step:62/2315 train_time:3757ms step_avg:60.60ms
step:63/2315 train_time:3818ms step_avg:60.60ms
step:64/2315 train_time:3878ms step_avg:60.60ms
step:65/2315 train_time:3938ms step_avg:60.59ms
step:66/2315 train_time:3999ms step_avg:60.59ms
step:67/2315 train_time:4059ms step_avg:60.58ms
step:68/2315 train_time:4120ms step_avg:60.59ms
step:69/2315 train_time:4180ms step_avg:60.58ms
step:70/2315 train_time:4240ms step_avg:60.58ms
step:71/2315 train_time:4301ms step_avg:60.58ms
step:72/2315 train_time:4362ms step_avg:60.59ms
step:73/2315 train_time:4423ms step_avg:60.59ms
step:74/2315 train_time:4484ms step_avg:60.59ms
step:75/2315 train_time:4543ms step_avg:60.58ms
step:76/2315 train_time:4604ms step_avg:60.58ms
step:77/2315 train_time:4664ms step_avg:60.57ms
step:78/2315 train_time:4725ms step_avg:60.57ms
step:79/2315 train_time:4787ms step_avg:60.59ms
step:80/2315 train_time:4845ms step_avg:60.56ms
step:81/2315 train_time:4905ms step_avg:60.55ms
step:82/2315 train_time:4965ms step_avg:60.55ms
step:83/2315 train_time:5025ms step_avg:60.55ms
step:84/2315 train_time:5086ms step_avg:60.55ms
step:85/2315 train_time:5146ms step_avg:60.54ms
step:86/2315 train_time:5206ms step_avg:60.54ms
step:87/2315 train_time:5266ms step_avg:60.53ms
step:88/2315 train_time:5327ms step_avg:60.53ms
step:89/2315 train_time:5387ms step_avg:60.53ms
step:90/2315 train_time:5448ms step_avg:60.53ms
step:91/2315 train_time:5507ms step_avg:60.52ms
step:92/2315 train_time:5568ms step_avg:60.52ms
step:93/2315 train_time:5628ms step_avg:60.51ms
step:94/2315 train_time:5689ms step_avg:60.52ms
step:95/2315 train_time:5748ms step_avg:60.51ms
step:96/2315 train_time:5809ms step_avg:60.51ms
step:97/2315 train_time:5869ms step_avg:60.50ms
step:98/2315 train_time:5929ms step_avg:60.50ms
step:99/2315 train_time:5989ms step_avg:60.50ms
step:100/2315 train_time:6050ms step_avg:60.50ms
step:101/2315 train_time:6109ms step_avg:60.48ms
step:102/2315 train_time:6169ms step_avg:60.48ms
step:103/2315 train_time:6228ms step_avg:60.47ms
step:104/2315 train_time:6289ms step_avg:60.47ms
step:105/2315 train_time:6349ms step_avg:60.47ms
step:106/2315 train_time:6409ms step_avg:60.47ms
step:107/2315 train_time:6468ms step_avg:60.45ms
step:108/2315 train_time:6529ms step_avg:60.45ms
step:109/2315 train_time:6589ms step_avg:60.45ms
step:110/2315 train_time:6649ms step_avg:60.45ms
step:111/2315 train_time:6708ms step_avg:60.44ms
step:112/2315 train_time:6768ms step_avg:60.43ms
step:113/2315 train_time:6828ms step_avg:60.43ms
step:114/2315 train_time:6889ms step_avg:60.43ms
step:115/2315 train_time:6949ms step_avg:60.43ms
step:116/2315 train_time:7010ms step_avg:60.43ms
step:117/2315 train_time:7069ms step_avg:60.42ms
step:118/2315 train_time:7129ms step_avg:60.42ms
step:119/2315 train_time:7189ms step_avg:60.41ms
step:120/2315 train_time:7249ms step_avg:60.41ms
step:121/2315 train_time:7310ms step_avg:60.41ms
step:122/2315 train_time:7370ms step_avg:60.41ms
step:123/2315 train_time:7429ms step_avg:60.40ms
step:124/2315 train_time:7489ms step_avg:60.40ms
step:125/2315 train_time:7549ms step_avg:60.39ms
step:126/2315 train_time:7610ms step_avg:60.39ms
step:127/2315 train_time:7669ms step_avg:60.39ms
step:128/2315 train_time:7729ms step_avg:60.39ms
step:129/2315 train_time:7789ms step_avg:60.38ms
step:130/2315 train_time:7849ms step_avg:60.38ms
step:131/2315 train_time:7909ms step_avg:60.37ms
step:132/2315 train_time:7969ms step_avg:60.37ms
step:133/2315 train_time:8028ms step_avg:60.36ms
step:134/2315 train_time:8088ms step_avg:60.36ms
step:135/2315 train_time:8148ms step_avg:60.36ms
step:136/2315 train_time:8209ms step_avg:60.36ms
step:137/2315 train_time:8268ms step_avg:60.35ms
step:138/2315 train_time:8329ms step_avg:60.35ms
step:139/2315 train_time:8389ms step_avg:60.35ms
step:140/2315 train_time:8449ms step_avg:60.35ms
step:141/2315 train_time:8509ms step_avg:60.34ms
step:142/2315 train_time:8569ms step_avg:60.34ms
step:143/2315 train_time:8628ms step_avg:60.34ms
step:144/2315 train_time:8688ms step_avg:60.33ms
step:145/2315 train_time:8748ms step_avg:60.33ms
step:146/2315 train_time:8808ms step_avg:60.33ms
step:147/2315 train_time:8867ms step_avg:60.32ms
step:148/2315 train_time:8928ms step_avg:60.32ms
step:149/2315 train_time:8988ms step_avg:60.32ms
step:150/2315 train_time:9048ms step_avg:60.32ms
step:151/2315 train_time:9108ms step_avg:60.32ms
step:152/2315 train_time:9169ms step_avg:60.32ms
step:153/2315 train_time:9228ms step_avg:60.32ms
step:154/2315 train_time:9289ms step_avg:60.32ms
step:155/2315 train_time:9349ms step_avg:60.32ms
step:156/2315 train_time:9410ms step_avg:60.32ms
step:157/2315 train_time:9469ms step_avg:60.31ms
step:158/2315 train_time:9530ms step_avg:60.31ms
step:159/2315 train_time:9589ms step_avg:60.31ms
step:160/2315 train_time:9649ms step_avg:60.30ms
step:161/2315 train_time:9708ms step_avg:60.30ms
step:162/2315 train_time:9768ms step_avg:60.30ms
step:163/2315 train_time:9828ms step_avg:60.29ms
step:164/2315 train_time:9888ms step_avg:60.29ms
step:165/2315 train_time:9948ms step_avg:60.29ms
step:166/2315 train_time:10008ms step_avg:60.29ms
step:167/2315 train_time:10069ms step_avg:60.29ms
step:168/2315 train_time:10129ms step_avg:60.29ms
step:169/2315 train_time:10189ms step_avg:60.29ms
step:170/2315 train_time:10249ms step_avg:60.29ms
step:171/2315 train_time:10309ms step_avg:60.28ms
step:172/2315 train_time:10369ms step_avg:60.29ms
step:173/2315 train_time:10429ms step_avg:60.28ms
step:174/2315 train_time:10490ms step_avg:60.29ms
step:175/2315 train_time:10549ms step_avg:60.28ms
step:176/2315 train_time:10609ms step_avg:60.28ms
step:177/2315 train_time:10669ms step_avg:60.28ms
step:178/2315 train_time:10729ms step_avg:60.28ms
step:179/2315 train_time:10789ms step_avg:60.27ms
step:180/2315 train_time:10849ms step_avg:60.27ms
step:181/2315 train_time:10908ms step_avg:60.27ms
step:182/2315 train_time:10969ms step_avg:60.27ms
step:183/2315 train_time:11028ms step_avg:60.26ms
step:184/2315 train_time:11089ms step_avg:60.27ms
step:185/2315 train_time:11149ms step_avg:60.26ms
step:186/2315 train_time:11209ms step_avg:60.26ms
step:187/2315 train_time:11268ms step_avg:60.26ms
step:188/2315 train_time:11328ms step_avg:60.26ms
step:189/2315 train_time:11388ms step_avg:60.25ms
step:190/2315 train_time:11448ms step_avg:60.25ms
step:191/2315 train_time:11508ms step_avg:60.25ms
step:192/2315 train_time:11569ms step_avg:60.25ms
step:193/2315 train_time:11629ms step_avg:60.25ms
step:194/2315 train_time:11689ms step_avg:60.25ms
step:195/2315 train_time:11748ms step_avg:60.25ms
step:196/2315 train_time:11808ms step_avg:60.24ms
step:197/2315 train_time:11868ms step_avg:60.24ms
step:198/2315 train_time:11929ms step_avg:60.25ms
step:199/2315 train_time:11988ms step_avg:60.24ms
step:200/2315 train_time:12049ms step_avg:60.24ms
step:201/2315 train_time:12108ms step_avg:60.24ms
step:202/2315 train_time:12169ms step_avg:60.24ms
step:203/2315 train_time:12228ms step_avg:60.24ms
step:204/2315 train_time:12288ms step_avg:60.24ms
step:205/2315 train_time:12347ms step_avg:60.23ms
step:206/2315 train_time:12408ms step_avg:60.23ms
step:207/2315 train_time:12467ms step_avg:60.23ms
step:208/2315 train_time:12527ms step_avg:60.23ms
step:209/2315 train_time:12587ms step_avg:60.23ms
step:210/2315 train_time:12648ms step_avg:60.23ms
step:211/2315 train_time:12708ms step_avg:60.23ms
step:212/2315 train_time:12768ms step_avg:60.23ms
step:213/2315 train_time:12828ms step_avg:60.22ms
step:214/2315 train_time:12889ms step_avg:60.23ms
step:215/2315 train_time:12948ms step_avg:60.22ms
step:216/2315 train_time:13008ms step_avg:60.22ms
step:217/2315 train_time:13067ms step_avg:60.22ms
step:218/2315 train_time:13127ms step_avg:60.22ms
step:219/2315 train_time:13187ms step_avg:60.21ms
step:220/2315 train_time:13247ms step_avg:60.21ms
step:221/2315 train_time:13307ms step_avg:60.21ms
step:222/2315 train_time:13368ms step_avg:60.21ms
step:223/2315 train_time:13427ms step_avg:60.21ms
step:224/2315 train_time:13488ms step_avg:60.21ms
step:225/2315 train_time:13547ms step_avg:60.21ms
step:226/2315 train_time:13608ms step_avg:60.21ms
step:227/2315 train_time:13667ms step_avg:60.21ms
step:228/2315 train_time:13727ms step_avg:60.21ms
step:229/2315 train_time:13787ms step_avg:60.21ms
step:230/2315 train_time:13847ms step_avg:60.21ms
step:231/2315 train_time:13907ms step_avg:60.20ms
step:232/2315 train_time:13968ms step_avg:60.21ms
step:233/2315 train_time:14028ms step_avg:60.21ms
step:234/2315 train_time:14089ms step_avg:60.21ms
step:235/2315 train_time:14148ms step_avg:60.20ms
step:236/2315 train_time:14209ms step_avg:60.21ms
step:237/2315 train_time:14269ms step_avg:60.20ms
step:238/2315 train_time:14329ms step_avg:60.20ms
step:239/2315 train_time:14388ms step_avg:60.20ms
step:240/2315 train_time:14449ms step_avg:60.20ms
step:241/2315 train_time:14508ms step_avg:60.20ms
step:242/2315 train_time:14569ms step_avg:60.20ms
step:243/2315 train_time:14628ms step_avg:60.20ms
step:244/2315 train_time:14689ms step_avg:60.20ms
step:245/2315 train_time:14749ms step_avg:60.20ms
step:246/2315 train_time:14809ms step_avg:60.20ms
step:247/2315 train_time:14869ms step_avg:60.20ms
step:248/2315 train_time:14929ms step_avg:60.20ms
step:249/2315 train_time:14988ms step_avg:60.19ms
step:250/2315 train_time:15049ms step_avg:60.20ms
step:250/2315 val_loss:4.0747 train_time:15110ms step_avg:60.44ms
step:251/2315 train_time:15128ms step_avg:60.27ms
step:252/2315 train_time:15170ms step_avg:60.20ms
step:253/2315 train_time:15236ms step_avg:60.22ms
step:254/2315 train_time:15300ms step_avg:60.24ms
step:255/2315 train_time:15361ms step_avg:60.24ms
step:256/2315 train_time:15422ms step_avg:60.24ms
step:257/2315 train_time:15483ms step_avg:60.25ms
step:258/2315 train_time:15543ms step_avg:60.24ms
step:259/2315 train_time:15602ms step_avg:60.24ms
step:260/2315 train_time:15661ms step_avg:60.24ms
step:261/2315 train_time:15721ms step_avg:60.23ms
step:262/2315 train_time:15781ms step_avg:60.23ms
step:263/2315 train_time:15840ms step_avg:60.23ms
step:264/2315 train_time:15899ms step_avg:60.22ms
step:265/2315 train_time:15958ms step_avg:60.22ms
step:266/2315 train_time:16017ms step_avg:60.22ms
step:267/2315 train_time:16077ms step_avg:60.21ms
step:268/2315 train_time:16138ms step_avg:60.22ms
step:269/2315 train_time:16200ms step_avg:60.22ms
step:270/2315 train_time:16263ms step_avg:60.23ms
step:271/2315 train_time:16324ms step_avg:60.24ms
step:272/2315 train_time:16385ms step_avg:60.24ms
step:273/2315 train_time:16444ms step_avg:60.24ms
step:274/2315 train_time:16504ms step_avg:60.24ms
step:275/2315 train_time:16564ms step_avg:60.23ms
step:276/2315 train_time:16623ms step_avg:60.23ms
step:277/2315 train_time:16683ms step_avg:60.23ms
step:278/2315 train_time:16743ms step_avg:60.23ms
step:279/2315 train_time:16801ms step_avg:60.22ms
step:280/2315 train_time:16861ms step_avg:60.22ms
step:281/2315 train_time:16920ms step_avg:60.22ms
step:282/2315 train_time:16980ms step_avg:60.21ms
step:283/2315 train_time:17040ms step_avg:60.21ms
step:284/2315 train_time:17100ms step_avg:60.21ms
step:285/2315 train_time:17161ms step_avg:60.22ms
step:286/2315 train_time:17223ms step_avg:60.22ms
step:287/2315 train_time:17283ms step_avg:60.22ms
step:288/2315 train_time:17344ms step_avg:60.22ms
step:289/2315 train_time:17404ms step_avg:60.22ms
step:290/2315 train_time:17465ms step_avg:60.22ms
step:291/2315 train_time:17525ms step_avg:60.22ms
step:292/2315 train_time:17585ms step_avg:60.22ms
step:293/2315 train_time:17644ms step_avg:60.22ms
step:294/2315 train_time:17704ms step_avg:60.22ms
step:295/2315 train_time:17763ms step_avg:60.21ms
step:296/2315 train_time:17823ms step_avg:60.21ms
step:297/2315 train_time:17882ms step_avg:60.21ms
step:298/2315 train_time:17942ms step_avg:60.21ms
step:299/2315 train_time:18001ms step_avg:60.21ms
step:300/2315 train_time:18062ms step_avg:60.21ms
step:301/2315 train_time:18122ms step_avg:60.21ms
step:302/2315 train_time:18183ms step_avg:60.21ms
step:303/2315 train_time:18243ms step_avg:60.21ms
step:304/2315 train_time:18305ms step_avg:60.21ms
step:305/2315 train_time:18365ms step_avg:60.21ms
step:306/2315 train_time:18426ms step_avg:60.21ms
step:307/2315 train_time:18486ms step_avg:60.22ms
step:308/2315 train_time:18546ms step_avg:60.22ms
step:309/2315 train_time:18606ms step_avg:60.21ms
step:310/2315 train_time:18666ms step_avg:60.21ms
step:311/2315 train_time:18725ms step_avg:60.21ms
step:312/2315 train_time:18785ms step_avg:60.21ms
step:313/2315 train_time:18844ms step_avg:60.20ms
step:314/2315 train_time:18904ms step_avg:60.20ms
step:315/2315 train_time:18964ms step_avg:60.20ms
step:316/2315 train_time:19024ms step_avg:60.20ms
step:317/2315 train_time:19085ms step_avg:60.20ms
step:318/2315 train_time:19145ms step_avg:60.20ms
step:319/2315 train_time:19205ms step_avg:60.20ms
step:320/2315 train_time:19266ms step_avg:60.21ms
step:321/2315 train_time:19325ms step_avg:60.20ms
step:322/2315 train_time:19386ms step_avg:60.20ms
step:323/2315 train_time:19446ms step_avg:60.20ms
step:324/2315 train_time:19506ms step_avg:60.20ms
step:325/2315 train_time:19565ms step_avg:60.20ms
step:326/2315 train_time:19625ms step_avg:60.20ms
step:327/2315 train_time:19684ms step_avg:60.20ms
step:328/2315 train_time:19744ms step_avg:60.20ms
step:329/2315 train_time:19803ms step_avg:60.19ms
step:330/2315 train_time:19864ms step_avg:60.19ms
step:331/2315 train_time:19923ms step_avg:60.19ms
step:332/2315 train_time:19983ms step_avg:60.19ms
step:333/2315 train_time:20043ms step_avg:60.19ms
step:334/2315 train_time:20103ms step_avg:60.19ms
step:335/2315 train_time:20163ms step_avg:60.19ms
step:336/2315 train_time:20224ms step_avg:60.19ms
step:337/2315 train_time:20285ms step_avg:60.19ms
step:338/2315 train_time:20345ms step_avg:60.19ms
step:339/2315 train_time:20404ms step_avg:60.19ms
step:340/2315 train_time:20465ms step_avg:60.19ms
step:341/2315 train_time:20525ms step_avg:60.19ms
step:342/2315 train_time:20585ms step_avg:60.19ms
step:343/2315 train_time:20644ms step_avg:60.19ms
step:344/2315 train_time:20704ms step_avg:60.19ms
step:345/2315 train_time:20764ms step_avg:60.18ms
step:346/2315 train_time:20823ms step_avg:60.18ms
step:347/2315 train_time:20883ms step_avg:60.18ms
step:348/2315 train_time:20942ms step_avg:60.18ms
step:349/2315 train_time:21002ms step_avg:60.18ms
step:350/2315 train_time:21062ms step_avg:60.18ms
step:351/2315 train_time:21123ms step_avg:60.18ms
step:352/2315 train_time:21183ms step_avg:60.18ms
step:353/2315 train_time:21243ms step_avg:60.18ms
step:354/2315 train_time:21304ms step_avg:60.18ms
step:355/2315 train_time:21364ms step_avg:60.18ms
step:356/2315 train_time:21425ms step_avg:60.18ms
step:357/2315 train_time:21485ms step_avg:60.18ms
step:358/2315 train_time:21545ms step_avg:60.18ms
step:359/2315 train_time:21605ms step_avg:60.18ms
step:360/2315 train_time:21665ms step_avg:60.18ms
step:361/2315 train_time:21725ms step_avg:60.18ms
step:362/2315 train_time:21785ms step_avg:60.18ms
step:363/2315 train_time:21844ms step_avg:60.18ms
step:364/2315 train_time:21904ms step_avg:60.18ms
step:365/2315 train_time:21964ms step_avg:60.18ms
step:366/2315 train_time:22024ms step_avg:60.18ms
step:367/2315 train_time:22084ms step_avg:60.17ms
step:368/2315 train_time:22144ms step_avg:60.17ms
step:369/2315 train_time:22204ms step_avg:60.17ms
step:370/2315 train_time:22264ms step_avg:60.17ms
step:371/2315 train_time:22324ms step_avg:60.17ms
step:372/2315 train_time:22385ms step_avg:60.17ms
step:373/2315 train_time:22444ms step_avg:60.17ms
step:374/2315 train_time:22505ms step_avg:60.17ms
step:375/2315 train_time:22565ms step_avg:60.17ms
step:376/2315 train_time:22625ms step_avg:60.17ms
step:377/2315 train_time:22684ms step_avg:60.17ms
step:378/2315 train_time:22745ms step_avg:60.17ms
step:379/2315 train_time:22804ms step_avg:60.17ms
step:380/2315 train_time:22864ms step_avg:60.17ms
step:381/2315 train_time:22924ms step_avg:60.17ms
step:382/2315 train_time:22984ms step_avg:60.17ms
step:383/2315 train_time:23044ms step_avg:60.17ms
step:384/2315 train_time:23104ms step_avg:60.17ms
step:385/2315 train_time:23164ms step_avg:60.17ms
step:386/2315 train_time:23224ms step_avg:60.17ms
step:387/2315 train_time:23284ms step_avg:60.16ms
step:388/2315 train_time:23344ms step_avg:60.16ms
step:389/2315 train_time:23404ms step_avg:60.16ms
step:390/2315 train_time:23464ms step_avg:60.16ms
step:391/2315 train_time:23524ms step_avg:60.16ms
step:392/2315 train_time:23584ms step_avg:60.16ms
step:393/2315 train_time:23644ms step_avg:60.16ms
step:394/2315 train_time:23704ms step_avg:60.16ms
step:395/2315 train_time:23764ms step_avg:60.16ms
step:396/2315 train_time:23824ms step_avg:60.16ms
step:397/2315 train_time:23883ms step_avg:60.16ms
step:398/2315 train_time:23944ms step_avg:60.16ms
step:399/2315 train_time:24003ms step_avg:60.16ms
step:400/2315 train_time:24063ms step_avg:60.16ms
step:401/2315 train_time:24123ms step_avg:60.16ms
step:402/2315 train_time:24183ms step_avg:60.16ms
step:403/2315 train_time:24243ms step_avg:60.16ms
step:404/2315 train_time:24303ms step_avg:60.16ms
step:405/2315 train_time:24363ms step_avg:60.15ms
step:406/2315 train_time:24423ms step_avg:60.16ms
step:407/2315 train_time:24483ms step_avg:60.16ms
step:408/2315 train_time:24543ms step_avg:60.15ms
step:409/2315 train_time:24603ms step_avg:60.15ms
step:410/2315 train_time:24664ms step_avg:60.16ms
step:411/2315 train_time:24724ms step_avg:60.16ms
step:412/2315 train_time:24784ms step_avg:60.16ms
step:413/2315 train_time:24843ms step_avg:60.15ms
step:414/2315 train_time:24903ms step_avg:60.15ms
step:415/2315 train_time:24963ms step_avg:60.15ms
step:416/2315 train_time:25023ms step_avg:60.15ms
step:417/2315 train_time:25083ms step_avg:60.15ms
step:418/2315 train_time:25143ms step_avg:60.15ms
step:419/2315 train_time:25203ms step_avg:60.15ms
step:420/2315 train_time:25263ms step_avg:60.15ms
step:421/2315 train_time:25323ms step_avg:60.15ms
step:422/2315 train_time:25383ms step_avg:60.15ms
step:423/2315 train_time:25443ms step_avg:60.15ms
step:424/2315 train_time:25504ms step_avg:60.15ms
step:425/2315 train_time:25563ms step_avg:60.15ms
step:426/2315 train_time:25623ms step_avg:60.15ms
step:427/2315 train_time:25683ms step_avg:60.15ms
step:428/2315 train_time:25743ms step_avg:60.15ms
step:429/2315 train_time:25802ms step_avg:60.15ms
step:430/2315 train_time:25863ms step_avg:60.15ms
step:431/2315 train_time:25922ms step_avg:60.14ms
step:432/2315 train_time:25982ms step_avg:60.14ms
step:433/2315 train_time:26042ms step_avg:60.14ms
step:434/2315 train_time:26102ms step_avg:60.14ms
step:435/2315 train_time:26163ms step_avg:60.14ms
step:436/2315 train_time:26223ms step_avg:60.14ms
step:437/2315 train_time:26283ms step_avg:60.14ms
step:438/2315 train_time:26343ms step_avg:60.14ms
step:439/2315 train_time:26403ms step_avg:60.14ms
step:440/2315 train_time:26464ms step_avg:60.14ms
step:441/2315 train_time:26526ms step_avg:60.15ms
step:442/2315 train_time:26584ms step_avg:60.14ms
step:443/2315 train_time:26643ms step_avg:60.14ms
step:444/2315 train_time:26703ms step_avg:60.14ms
step:445/2315 train_time:26763ms step_avg:60.14ms
step:446/2315 train_time:26823ms step_avg:60.14ms
step:447/2315 train_time:26883ms step_avg:60.14ms
step:448/2315 train_time:26943ms step_avg:60.14ms
step:449/2315 train_time:27003ms step_avg:60.14ms
step:450/2315 train_time:27063ms step_avg:60.14ms
step:451/2315 train_time:27123ms step_avg:60.14ms
step:452/2315 train_time:27183ms step_avg:60.14ms
step:453/2315 train_time:27242ms step_avg:60.14ms
step:454/2315 train_time:27303ms step_avg:60.14ms
step:455/2315 train_time:27363ms step_avg:60.14ms
step:456/2315 train_time:27423ms step_avg:60.14ms
step:457/2315 train_time:27482ms step_avg:60.14ms
step:458/2315 train_time:27542ms step_avg:60.14ms
step:459/2315 train_time:27603ms step_avg:60.14ms
step:460/2315 train_time:27663ms step_avg:60.14ms
step:461/2315 train_time:27723ms step_avg:60.14ms
step:462/2315 train_time:27783ms step_avg:60.14ms
step:463/2315 train_time:27842ms step_avg:60.13ms
step:464/2315 train_time:27902ms step_avg:60.13ms
step:465/2315 train_time:27962ms step_avg:60.13ms
step:466/2315 train_time:28022ms step_avg:60.13ms
step:467/2315 train_time:28082ms step_avg:60.13ms
step:468/2315 train_time:28143ms step_avg:60.13ms
step:469/2315 train_time:28203ms step_avg:60.13ms
step:470/2315 train_time:28263ms step_avg:60.13ms
step:471/2315 train_time:28323ms step_avg:60.13ms
step:472/2315 train_time:28383ms step_avg:60.13ms
step:473/2315 train_time:28443ms step_avg:60.13ms
step:474/2315 train_time:28503ms step_avg:60.13ms
step:475/2315 train_time:28563ms step_avg:60.13ms
step:476/2315 train_time:28623ms step_avg:60.13ms
step:477/2315 train_time:28683ms step_avg:60.13ms
step:478/2315 train_time:28743ms step_avg:60.13ms
step:479/2315 train_time:28802ms step_avg:60.13ms
step:480/2315 train_time:28863ms step_avg:60.13ms
step:481/2315 train_time:28923ms step_avg:60.13ms
step:482/2315 train_time:28983ms step_avg:60.13ms
step:483/2315 train_time:29043ms step_avg:60.13ms
step:484/2315 train_time:29103ms step_avg:60.13ms
step:485/2315 train_time:29163ms step_avg:60.13ms
step:486/2315 train_time:29225ms step_avg:60.13ms
step:487/2315 train_time:29283ms step_avg:60.13ms
step:488/2315 train_time:29343ms step_avg:60.13ms
step:489/2315 train_time:29403ms step_avg:60.13ms
step:490/2315 train_time:29464ms step_avg:60.13ms
step:491/2315 train_time:29523ms step_avg:60.13ms
step:492/2315 train_time:29583ms step_avg:60.13ms
step:493/2315 train_time:29643ms step_avg:60.13ms
step:494/2315 train_time:29703ms step_avg:60.13ms
step:495/2315 train_time:29762ms step_avg:60.13ms
step:496/2315 train_time:29823ms step_avg:60.13ms
step:497/2315 train_time:29882ms step_avg:60.13ms
step:498/2315 train_time:29942ms step_avg:60.13ms
step:499/2315 train_time:30002ms step_avg:60.12ms
step:500/2315 train_time:30062ms step_avg:60.12ms
step:500/2315 val_loss:3.8089 train_time:30124ms step_avg:60.25ms
step:501/2315 train_time:30141ms step_avg:60.16ms
step:502/2315 train_time:30184ms step_avg:60.13ms
step:503/2315 train_time:30249ms step_avg:60.14ms
step:504/2315 train_time:30314ms step_avg:60.15ms
step:505/2315 train_time:30374ms step_avg:60.15ms
step:506/2315 train_time:30433ms step_avg:60.14ms
step:507/2315 train_time:30492ms step_avg:60.14ms
step:508/2315 train_time:30552ms step_avg:60.14ms
step:509/2315 train_time:30612ms step_avg:60.14ms
step:510/2315 train_time:30671ms step_avg:60.14ms
step:511/2315 train_time:30730ms step_avg:60.14ms
step:512/2315 train_time:30790ms step_avg:60.14ms
step:513/2315 train_time:30849ms step_avg:60.13ms
step:514/2315 train_time:30909ms step_avg:60.13ms
step:515/2315 train_time:30967ms step_avg:60.13ms
step:516/2315 train_time:31028ms step_avg:60.13ms
step:517/2315 train_time:31090ms step_avg:60.13ms
step:518/2315 train_time:31151ms step_avg:60.14ms
step:519/2315 train_time:31213ms step_avg:60.14ms
step:520/2315 train_time:31274ms step_avg:60.14ms
step:521/2315 train_time:31334ms step_avg:60.14ms
step:522/2315 train_time:31395ms step_avg:60.14ms
step:523/2315 train_time:31455ms step_avg:60.14ms
step:524/2315 train_time:31515ms step_avg:60.14ms
step:525/2315 train_time:31574ms step_avg:60.14ms
step:526/2315 train_time:31634ms step_avg:60.14ms
step:527/2315 train_time:31693ms step_avg:60.14ms
step:528/2315 train_time:31753ms step_avg:60.14ms
step:529/2315 train_time:31812ms step_avg:60.14ms
step:530/2315 train_time:31871ms step_avg:60.13ms
step:531/2315 train_time:31931ms step_avg:60.13ms
step:532/2315 train_time:31991ms step_avg:60.13ms
step:533/2315 train_time:32051ms step_avg:60.13ms
step:534/2315 train_time:32113ms step_avg:60.14ms
step:535/2315 train_time:32173ms step_avg:60.14ms
step:536/2315 train_time:32234ms step_avg:60.14ms
step:537/2315 train_time:32295ms step_avg:60.14ms
step:538/2315 train_time:32355ms step_avg:60.14ms
step:539/2315 train_time:32416ms step_avg:60.14ms
step:540/2315 train_time:32476ms step_avg:60.14ms
step:541/2315 train_time:32535ms step_avg:60.14ms
step:542/2315 train_time:32595ms step_avg:60.14ms
step:543/2315 train_time:32654ms step_avg:60.14ms
step:544/2315 train_time:32713ms step_avg:60.13ms
step:545/2315 train_time:32773ms step_avg:60.13ms
step:546/2315 train_time:32833ms step_avg:60.13ms
step:547/2315 train_time:32893ms step_avg:60.13ms
step:548/2315 train_time:32953ms step_avg:60.13ms
step:549/2315 train_time:33013ms step_avg:60.13ms
step:550/2315 train_time:33074ms step_avg:60.13ms
step:551/2315 train_time:33134ms step_avg:60.13ms
step:552/2315 train_time:33195ms step_avg:60.14ms
step:553/2315 train_time:33255ms step_avg:60.14ms
step:554/2315 train_time:33316ms step_avg:60.14ms
step:555/2315 train_time:33376ms step_avg:60.14ms
step:556/2315 train_time:33437ms step_avg:60.14ms
step:557/2315 train_time:33496ms step_avg:60.14ms
step:558/2315 train_time:33556ms step_avg:60.14ms
step:559/2315 train_time:33615ms step_avg:60.13ms
step:560/2315 train_time:33675ms step_avg:60.13ms
step:561/2315 train_time:33734ms step_avg:60.13ms
step:562/2315 train_time:33794ms step_avg:60.13ms
step:563/2315 train_time:33853ms step_avg:60.13ms
step:564/2315 train_time:33913ms step_avg:60.13ms
step:565/2315 train_time:33973ms step_avg:60.13ms
step:566/2315 train_time:34034ms step_avg:60.13ms
step:567/2315 train_time:34094ms step_avg:60.13ms
step:568/2315 train_time:34154ms step_avg:60.13ms
step:569/2315 train_time:34214ms step_avg:60.13ms
step:570/2315 train_time:34275ms step_avg:60.13ms
step:571/2315 train_time:34335ms step_avg:60.13ms
step:572/2315 train_time:34396ms step_avg:60.13ms
step:573/2315 train_time:34455ms step_avg:60.13ms
step:574/2315 train_time:34515ms step_avg:60.13ms
step:575/2315 train_time:34575ms step_avg:60.13ms
step:576/2315 train_time:34635ms step_avg:60.13ms
step:577/2315 train_time:34694ms step_avg:60.13ms
step:578/2315 train_time:34754ms step_avg:60.13ms
step:579/2315 train_time:34813ms step_avg:60.13ms
step:580/2315 train_time:34873ms step_avg:60.13ms
step:581/2315 train_time:34932ms step_avg:60.12ms
step:582/2315 train_time:34992ms step_avg:60.12ms
step:583/2315 train_time:35052ms step_avg:60.12ms
step:584/2315 train_time:35113ms step_avg:60.12ms
step:585/2315 train_time:35172ms step_avg:60.12ms
step:586/2315 train_time:35233ms step_avg:60.12ms
step:587/2315 train_time:35293ms step_avg:60.12ms
step:588/2315 train_time:35353ms step_avg:60.12ms
step:589/2315 train_time:35413ms step_avg:60.12ms
step:590/2315 train_time:35474ms step_avg:60.13ms
step:591/2315 train_time:35533ms step_avg:60.12ms
step:592/2315 train_time:35594ms step_avg:60.12ms
step:593/2315 train_time:35654ms step_avg:60.12ms
step:594/2315 train_time:35714ms step_avg:60.12ms
step:595/2315 train_time:35773ms step_avg:60.12ms
step:596/2315 train_time:35833ms step_avg:60.12ms
step:597/2315 train_time:35892ms step_avg:60.12ms
step:598/2315 train_time:35952ms step_avg:60.12ms
step:599/2315 train_time:36012ms step_avg:60.12ms
step:600/2315 train_time:36072ms step_avg:60.12ms
step:601/2315 train_time:36132ms step_avg:60.12ms
step:602/2315 train_time:36193ms step_avg:60.12ms
step:603/2315 train_time:36253ms step_avg:60.12ms
step:604/2315 train_time:36313ms step_avg:60.12ms
step:605/2315 train_time:36373ms step_avg:60.12ms
step:606/2315 train_time:36433ms step_avg:60.12ms
step:607/2315 train_time:36493ms step_avg:60.12ms
step:608/2315 train_time:36553ms step_avg:60.12ms
step:609/2315 train_time:36613ms step_avg:60.12ms
step:610/2315 train_time:36673ms step_avg:60.12ms
step:611/2315 train_time:36733ms step_avg:60.12ms
step:612/2315 train_time:36793ms step_avg:60.12ms
step:613/2315 train_time:36853ms step_avg:60.12ms
step:614/2315 train_time:36913ms step_avg:60.12ms
step:615/2315 train_time:36973ms step_avg:60.12ms
step:616/2315 train_time:37034ms step_avg:60.12ms
step:617/2315 train_time:37093ms step_avg:60.12ms
step:618/2315 train_time:37154ms step_avg:60.12ms
step:619/2315 train_time:37213ms step_avg:60.12ms
step:620/2315 train_time:37273ms step_avg:60.12ms
step:621/2315 train_time:37333ms step_avg:60.12ms
step:622/2315 train_time:37394ms step_avg:60.12ms
step:623/2315 train_time:37454ms step_avg:60.12ms
step:624/2315 train_time:37515ms step_avg:60.12ms
step:625/2315 train_time:37574ms step_avg:60.12ms
step:626/2315 train_time:37634ms step_avg:60.12ms
step:627/2315 train_time:37694ms step_avg:60.12ms
step:628/2315 train_time:37754ms step_avg:60.12ms
step:629/2315 train_time:37814ms step_avg:60.12ms
step:630/2315 train_time:37874ms step_avg:60.12ms
step:631/2315 train_time:37933ms step_avg:60.12ms
step:632/2315 train_time:37994ms step_avg:60.12ms
step:633/2315 train_time:38053ms step_avg:60.12ms
step:634/2315 train_time:38113ms step_avg:60.12ms
step:635/2315 train_time:38173ms step_avg:60.11ms
step:636/2315 train_time:38233ms step_avg:60.12ms
step:637/2315 train_time:38293ms step_avg:60.11ms
step:638/2315 train_time:38354ms step_avg:60.12ms
step:639/2315 train_time:38413ms step_avg:60.11ms
step:640/2315 train_time:38475ms step_avg:60.12ms
step:641/2315 train_time:38534ms step_avg:60.12ms
step:642/2315 train_time:38594ms step_avg:60.11ms
step:643/2315 train_time:38654ms step_avg:60.11ms
step:644/2315 train_time:38715ms step_avg:60.12ms
step:645/2315 train_time:38774ms step_avg:60.11ms
step:646/2315 train_time:38834ms step_avg:60.12ms
step:647/2315 train_time:38894ms step_avg:60.11ms
step:648/2315 train_time:38954ms step_avg:60.11ms
step:649/2315 train_time:39014ms step_avg:60.11ms
step:650/2315 train_time:39074ms step_avg:60.11ms
step:651/2315 train_time:39134ms step_avg:60.11ms
step:652/2315 train_time:39195ms step_avg:60.11ms
step:653/2315 train_time:39254ms step_avg:60.11ms
step:654/2315 train_time:39314ms step_avg:60.11ms
step:655/2315 train_time:39374ms step_avg:60.11ms
step:656/2315 train_time:39435ms step_avg:60.11ms
step:657/2315 train_time:39494ms step_avg:60.11ms
step:658/2315 train_time:39554ms step_avg:60.11ms
step:659/2315 train_time:39614ms step_avg:60.11ms
step:660/2315 train_time:39675ms step_avg:60.11ms
step:661/2315 train_time:39734ms step_avg:60.11ms
step:662/2315 train_time:39795ms step_avg:60.11ms
step:663/2315 train_time:39854ms step_avg:60.11ms
step:664/2315 train_time:39914ms step_avg:60.11ms
step:665/2315 train_time:39974ms step_avg:60.11ms
step:666/2315 train_time:40034ms step_avg:60.11ms
step:667/2315 train_time:40094ms step_avg:60.11ms
step:668/2315 train_time:40154ms step_avg:60.11ms
step:669/2315 train_time:40214ms step_avg:60.11ms
step:670/2315 train_time:40274ms step_avg:60.11ms
step:671/2315 train_time:40333ms step_avg:60.11ms
step:672/2315 train_time:40394ms step_avg:60.11ms
step:673/2315 train_time:40454ms step_avg:60.11ms
step:674/2315 train_time:40514ms step_avg:60.11ms
step:675/2315 train_time:40574ms step_avg:60.11ms
step:676/2315 train_time:40635ms step_avg:60.11ms
step:677/2315 train_time:40694ms step_avg:60.11ms
step:678/2315 train_time:40754ms step_avg:60.11ms
step:679/2315 train_time:40814ms step_avg:60.11ms
step:680/2315 train_time:40874ms step_avg:60.11ms
step:681/2315 train_time:40933ms step_avg:60.11ms
step:682/2315 train_time:40993ms step_avg:60.11ms
step:683/2315 train_time:41053ms step_avg:60.11ms
step:684/2315 train_time:41113ms step_avg:60.11ms
step:685/2315 train_time:41173ms step_avg:60.11ms
step:686/2315 train_time:41233ms step_avg:60.11ms
step:687/2315 train_time:41293ms step_avg:60.11ms
step:688/2315 train_time:41353ms step_avg:60.11ms
step:689/2315 train_time:41413ms step_avg:60.11ms
step:690/2315 train_time:41474ms step_avg:60.11ms
step:691/2315 train_time:41534ms step_avg:60.11ms
step:692/2315 train_time:41594ms step_avg:60.11ms
step:693/2315 train_time:41654ms step_avg:60.11ms
step:694/2315 train_time:41714ms step_avg:60.11ms
step:695/2315 train_time:41774ms step_avg:60.11ms
step:696/2315 train_time:41835ms step_avg:60.11ms
step:697/2315 train_time:41894ms step_avg:60.11ms
step:698/2315 train_time:41954ms step_avg:60.11ms
step:699/2315 train_time:42014ms step_avg:60.11ms
step:700/2315 train_time:42077ms step_avg:60.11ms
step:701/2315 train_time:42134ms step_avg:60.11ms
step:702/2315 train_time:42194ms step_avg:60.11ms
step:703/2315 train_time:42254ms step_avg:60.11ms
step:704/2315 train_time:42315ms step_avg:60.11ms
step:705/2315 train_time:42374ms step_avg:60.10ms
step:706/2315 train_time:42434ms step_avg:60.10ms
step:707/2315 train_time:42494ms step_avg:60.10ms
step:708/2315 train_time:42554ms step_avg:60.10ms
step:709/2315 train_time:42614ms step_avg:60.10ms
step:710/2315 train_time:42674ms step_avg:60.10ms
step:711/2315 train_time:42734ms step_avg:60.10ms
step:712/2315 train_time:42794ms step_avg:60.10ms
step:713/2315 train_time:42854ms step_avg:60.10ms
step:714/2315 train_time:42914ms step_avg:60.10ms
step:715/2315 train_time:42974ms step_avg:60.10ms
step:716/2315 train_time:43034ms step_avg:60.10ms
step:717/2315 train_time:43094ms step_avg:60.10ms
step:718/2315 train_time:43155ms step_avg:60.10ms
step:719/2315 train_time:43215ms step_avg:60.10ms
step:720/2315 train_time:43275ms step_avg:60.10ms
step:721/2315 train_time:43335ms step_avg:60.10ms
step:722/2315 train_time:43395ms step_avg:60.10ms
step:723/2315 train_time:43455ms step_avg:60.10ms
step:724/2315 train_time:43515ms step_avg:60.10ms
step:725/2315 train_time:43576ms step_avg:60.10ms
step:726/2315 train_time:43635ms step_avg:60.10ms
step:727/2315 train_time:43695ms step_avg:60.10ms
step:728/2315 train_time:43755ms step_avg:60.10ms
step:729/2315 train_time:43814ms step_avg:60.10ms
step:730/2315 train_time:43874ms step_avg:60.10ms
step:731/2315 train_time:43934ms step_avg:60.10ms
step:732/2315 train_time:43994ms step_avg:60.10ms
step:733/2315 train_time:44054ms step_avg:60.10ms
step:734/2315 train_time:44114ms step_avg:60.10ms
step:735/2315 train_time:44174ms step_avg:60.10ms
step:736/2315 train_time:44234ms step_avg:60.10ms
step:737/2315 train_time:44294ms step_avg:60.10ms
step:738/2315 train_time:44354ms step_avg:60.10ms
step:739/2315 train_time:44414ms step_avg:60.10ms
step:740/2315 train_time:44475ms step_avg:60.10ms
step:741/2315 train_time:44534ms step_avg:60.10ms
step:742/2315 train_time:44595ms step_avg:60.10ms
step:743/2315 train_time:44654ms step_avg:60.10ms
step:744/2315 train_time:44714ms step_avg:60.10ms
step:745/2315 train_time:44774ms step_avg:60.10ms
step:746/2315 train_time:44834ms step_avg:60.10ms
step:747/2315 train_time:44893ms step_avg:60.10ms
step:748/2315 train_time:44954ms step_avg:60.10ms
step:749/2315 train_time:45013ms step_avg:60.10ms
step:750/2315 train_time:45074ms step_avg:60.10ms
step:750/2315 val_loss:3.6839 train_time:45136ms step_avg:60.18ms
step:751/2315 train_time:45153ms step_avg:60.12ms
step:752/2315 train_time:45199ms step_avg:60.10ms
step:753/2315 train_time:45263ms step_avg:60.11ms
step:754/2315 train_time:45325ms step_avg:60.11ms
step:755/2315 train_time:45384ms step_avg:60.11ms
step:756/2315 train_time:45446ms step_avg:60.11ms
step:757/2315 train_time:45505ms step_avg:60.11ms
step:758/2315 train_time:45564ms step_avg:60.11ms
step:759/2315 train_time:45623ms step_avg:60.11ms
step:760/2315 train_time:45683ms step_avg:60.11ms
step:761/2315 train_time:45743ms step_avg:60.11ms
step:762/2315 train_time:45803ms step_avg:60.11ms
step:763/2315 train_time:45863ms step_avg:60.11ms
step:764/2315 train_time:45924ms step_avg:60.11ms
step:765/2315 train_time:45984ms step_avg:60.11ms
step:766/2315 train_time:46045ms step_avg:60.11ms
step:767/2315 train_time:46108ms step_avg:60.11ms
step:768/2315 train_time:46171ms step_avg:60.12ms
step:769/2315 train_time:46232ms step_avg:60.12ms
step:770/2315 train_time:46294ms step_avg:60.12ms
step:771/2315 train_time:46355ms step_avg:60.12ms
step:772/2315 train_time:46416ms step_avg:60.12ms
step:773/2315 train_time:46477ms step_avg:60.12ms
step:774/2315 train_time:46537ms step_avg:60.13ms
step:775/2315 train_time:46597ms step_avg:60.13ms
step:776/2315 train_time:46658ms step_avg:60.13ms
step:777/2315 train_time:46719ms step_avg:60.13ms
step:778/2315 train_time:46780ms step_avg:60.13ms
step:779/2315 train_time:46839ms step_avg:60.13ms
step:780/2315 train_time:46901ms step_avg:60.13ms
step:781/2315 train_time:46961ms step_avg:60.13ms
step:782/2315 train_time:47022ms step_avg:60.13ms
step:783/2315 train_time:47083ms step_avg:60.13ms
step:784/2315 train_time:47145ms step_avg:60.13ms
step:785/2315 train_time:47206ms step_avg:60.14ms
step:786/2315 train_time:47268ms step_avg:60.14ms
step:787/2315 train_time:47329ms step_avg:60.14ms
step:788/2315 train_time:47390ms step_avg:60.14ms
step:789/2315 train_time:47450ms step_avg:60.14ms
step:790/2315 train_time:47511ms step_avg:60.14ms
step:791/2315 train_time:47571ms step_avg:60.14ms
step:792/2315 train_time:47631ms step_avg:60.14ms
step:793/2315 train_time:47692ms step_avg:60.14ms
step:794/2315 train_time:47752ms step_avg:60.14ms
step:795/2315 train_time:47813ms step_avg:60.14ms
step:796/2315 train_time:47874ms step_avg:60.14ms
step:797/2315 train_time:47935ms step_avg:60.14ms
step:798/2315 train_time:47996ms step_avg:60.15ms
step:799/2315 train_time:48058ms step_avg:60.15ms
step:800/2315 train_time:48120ms step_avg:60.15ms
step:801/2315 train_time:48182ms step_avg:60.15ms
step:802/2315 train_time:48244ms step_avg:60.15ms
step:803/2315 train_time:48305ms step_avg:60.16ms
step:804/2315 train_time:48366ms step_avg:60.16ms
step:805/2315 train_time:48426ms step_avg:60.16ms
step:806/2315 train_time:48487ms step_avg:60.16ms
step:807/2315 train_time:48548ms step_avg:60.16ms
step:808/2315 train_time:48608ms step_avg:60.16ms
step:809/2315 train_time:48669ms step_avg:60.16ms
step:810/2315 train_time:48729ms step_avg:60.16ms
step:811/2315 train_time:48790ms step_avg:60.16ms
step:812/2315 train_time:48850ms step_avg:60.16ms
step:813/2315 train_time:48910ms step_avg:60.16ms
step:814/2315 train_time:48971ms step_avg:60.16ms
step:815/2315 train_time:49032ms step_avg:60.16ms
step:816/2315 train_time:49094ms step_avg:60.16ms
step:817/2315 train_time:49155ms step_avg:60.17ms
step:818/2315 train_time:49217ms step_avg:60.17ms
step:819/2315 train_time:49278ms step_avg:60.17ms
step:820/2315 train_time:49339ms step_avg:60.17ms
step:821/2315 train_time:49400ms step_avg:60.17ms
step:822/2315 train_time:49462ms step_avg:60.17ms
step:823/2315 train_time:49522ms step_avg:60.17ms
step:824/2315 train_time:49583ms step_avg:60.17ms
step:825/2315 train_time:49644ms step_avg:60.17ms
step:826/2315 train_time:49704ms step_avg:60.17ms
step:827/2315 train_time:49766ms step_avg:60.18ms
step:828/2315 train_time:49827ms step_avg:60.18ms
step:829/2315 train_time:49887ms step_avg:60.18ms
step:830/2315 train_time:49949ms step_avg:60.18ms
step:831/2315 train_time:50010ms step_avg:60.18ms
step:832/2315 train_time:50071ms step_avg:60.18ms
step:833/2315 train_time:50132ms step_avg:60.18ms
step:834/2315 train_time:50192ms step_avg:60.18ms
step:835/2315 train_time:50253ms step_avg:60.18ms
step:836/2315 train_time:50315ms step_avg:60.19ms
step:837/2315 train_time:50376ms step_avg:60.19ms
step:838/2315 train_time:50437ms step_avg:60.19ms
step:839/2315 train_time:50498ms step_avg:60.19ms
step:840/2315 train_time:50561ms step_avg:60.19ms
step:841/2315 train_time:50621ms step_avg:60.19ms
step:842/2315 train_time:50682ms step_avg:60.19ms
step:843/2315 train_time:50742ms step_avg:60.19ms
step:844/2315 train_time:50804ms step_avg:60.19ms
step:845/2315 train_time:50865ms step_avg:60.20ms
step:846/2315 train_time:50927ms step_avg:60.20ms
step:847/2315 train_time:50987ms step_avg:60.20ms
step:848/2315 train_time:51048ms step_avg:60.20ms
step:849/2315 train_time:51109ms step_avg:60.20ms
step:850/2315 train_time:51170ms step_avg:60.20ms
step:851/2315 train_time:51231ms step_avg:60.20ms
step:852/2315 train_time:51292ms step_avg:60.20ms
step:853/2315 train_time:51352ms step_avg:60.20ms
step:854/2315 train_time:51413ms step_avg:60.20ms
step:855/2315 train_time:51474ms step_avg:60.20ms
step:856/2315 train_time:51536ms step_avg:60.21ms
step:857/2315 train_time:51597ms step_avg:60.21ms
step:858/2315 train_time:51658ms step_avg:60.21ms
step:859/2315 train_time:51719ms step_avg:60.21ms
step:860/2315 train_time:51781ms step_avg:60.21ms
step:861/2315 train_time:51842ms step_avg:60.21ms
step:862/2315 train_time:51904ms step_avg:60.21ms
step:863/2315 train_time:51964ms step_avg:60.21ms
step:864/2315 train_time:52025ms step_avg:60.21ms
step:865/2315 train_time:52087ms step_avg:60.22ms
step:866/2315 train_time:52148ms step_avg:60.22ms
step:867/2315 train_time:52209ms step_avg:60.22ms
step:868/2315 train_time:52270ms step_avg:60.22ms
step:869/2315 train_time:52330ms step_avg:60.22ms
step:870/2315 train_time:52391ms step_avg:60.22ms
step:871/2315 train_time:52452ms step_avg:60.22ms
step:872/2315 train_time:52513ms step_avg:60.22ms
step:873/2315 train_time:52573ms step_avg:60.22ms
step:874/2315 train_time:52634ms step_avg:60.22ms
step:875/2315 train_time:52695ms step_avg:60.22ms
step:876/2315 train_time:52757ms step_avg:60.23ms
step:877/2315 train_time:52819ms step_avg:60.23ms
step:878/2315 train_time:52881ms step_avg:60.23ms
step:879/2315 train_time:52942ms step_avg:60.23ms
step:880/2315 train_time:53003ms step_avg:60.23ms
step:881/2315 train_time:53064ms step_avg:60.23ms
step:882/2315 train_time:53126ms step_avg:60.23ms
step:883/2315 train_time:53186ms step_avg:60.23ms
step:884/2315 train_time:53247ms step_avg:60.23ms
step:885/2315 train_time:53307ms step_avg:60.23ms
step:886/2315 train_time:53369ms step_avg:60.24ms
step:887/2315 train_time:53429ms step_avg:60.24ms
step:888/2315 train_time:53490ms step_avg:60.24ms
step:889/2315 train_time:53551ms step_avg:60.24ms
step:890/2315 train_time:53611ms step_avg:60.24ms
step:891/2315 train_time:53672ms step_avg:60.24ms
step:892/2315 train_time:53732ms step_avg:60.24ms
step:893/2315 train_time:53794ms step_avg:60.24ms
step:894/2315 train_time:53855ms step_avg:60.24ms
step:895/2315 train_time:53917ms step_avg:60.24ms
step:896/2315 train_time:53978ms step_avg:60.24ms
step:897/2315 train_time:54039ms step_avg:60.24ms
step:898/2315 train_time:54100ms step_avg:60.25ms
step:899/2315 train_time:54162ms step_avg:60.25ms
step:900/2315 train_time:54223ms step_avg:60.25ms
step:901/2315 train_time:54283ms step_avg:60.25ms
step:902/2315 train_time:54344ms step_avg:60.25ms
step:903/2315 train_time:54405ms step_avg:60.25ms
step:904/2315 train_time:54466ms step_avg:60.25ms
step:905/2315 train_time:54527ms step_avg:60.25ms
step:906/2315 train_time:54588ms step_avg:60.25ms
step:907/2315 train_time:54649ms step_avg:60.25ms
step:908/2315 train_time:54710ms step_avg:60.25ms
step:909/2315 train_time:54771ms step_avg:60.25ms
step:910/2315 train_time:54831ms step_avg:60.25ms
step:911/2315 train_time:54892ms step_avg:60.25ms
step:912/2315 train_time:54953ms step_avg:60.26ms
step:913/2315 train_time:55014ms step_avg:60.26ms
step:914/2315 train_time:55075ms step_avg:60.26ms
step:915/2315 train_time:55136ms step_avg:60.26ms
step:916/2315 train_time:55197ms step_avg:60.26ms
step:917/2315 train_time:55259ms step_avg:60.26ms
step:918/2315 train_time:55321ms step_avg:60.26ms
step:919/2315 train_time:55382ms step_avg:60.26ms
step:920/2315 train_time:55442ms step_avg:60.26ms
step:921/2315 train_time:55503ms step_avg:60.26ms
step:922/2315 train_time:55565ms step_avg:60.27ms
step:923/2315 train_time:55626ms step_avg:60.27ms
step:924/2315 train_time:55688ms step_avg:60.27ms
step:925/2315 train_time:55748ms step_avg:60.27ms
step:926/2315 train_time:55808ms step_avg:60.27ms
step:927/2315 train_time:55869ms step_avg:60.27ms
step:928/2315 train_time:55930ms step_avg:60.27ms
step:929/2315 train_time:55991ms step_avg:60.27ms
step:930/2315 train_time:56052ms step_avg:60.27ms
step:931/2315 train_time:56112ms step_avg:60.27ms
step:932/2315 train_time:56174ms step_avg:60.27ms
step:933/2315 train_time:56235ms step_avg:60.27ms
step:934/2315 train_time:56297ms step_avg:60.27ms
step:935/2315 train_time:56358ms step_avg:60.28ms
step:936/2315 train_time:56420ms step_avg:60.28ms
step:937/2315 train_time:56481ms step_avg:60.28ms
step:938/2315 train_time:56542ms step_avg:60.28ms
step:939/2315 train_time:56603ms step_avg:60.28ms
step:940/2315 train_time:56665ms step_avg:60.28ms
step:941/2315 train_time:56725ms step_avg:60.28ms
step:942/2315 train_time:56786ms step_avg:60.28ms
step:943/2315 train_time:56847ms step_avg:60.28ms
step:944/2315 train_time:56908ms step_avg:60.28ms
step:945/2315 train_time:56968ms step_avg:60.28ms
step:946/2315 train_time:57029ms step_avg:60.28ms
step:947/2315 train_time:57089ms step_avg:60.28ms
step:948/2315 train_time:57150ms step_avg:60.29ms
step:949/2315 train_time:57211ms step_avg:60.29ms
step:950/2315 train_time:57271ms step_avg:60.29ms
step:951/2315 train_time:57332ms step_avg:60.29ms
step:952/2315 train_time:57393ms step_avg:60.29ms
step:953/2315 train_time:57455ms step_avg:60.29ms
step:954/2315 train_time:57516ms step_avg:60.29ms
step:955/2315 train_time:57577ms step_avg:60.29ms
step:956/2315 train_time:57638ms step_avg:60.29ms
step:957/2315 train_time:57699ms step_avg:60.29ms
step:958/2315 train_time:57761ms step_avg:60.29ms
step:959/2315 train_time:57821ms step_avg:60.29ms
step:960/2315 train_time:57883ms step_avg:60.29ms
step:961/2315 train_time:57943ms step_avg:60.29ms
step:962/2315 train_time:58005ms step_avg:60.30ms
step:963/2315 train_time:58065ms step_avg:60.30ms
step:964/2315 train_time:58127ms step_avg:60.30ms
step:965/2315 train_time:58187ms step_avg:60.30ms
step:966/2315 train_time:58249ms step_avg:60.30ms
step:967/2315 train_time:58310ms step_avg:60.30ms
step:968/2315 train_time:58371ms step_avg:60.30ms
step:969/2315 train_time:58431ms step_avg:60.30ms
step:970/2315 train_time:58491ms step_avg:60.30ms
step:971/2315 train_time:58551ms step_avg:60.30ms
step:972/2315 train_time:58613ms step_avg:60.30ms
step:973/2315 train_time:58674ms step_avg:60.30ms
step:974/2315 train_time:58736ms step_avg:60.30ms
step:975/2315 train_time:58797ms step_avg:60.30ms
step:976/2315 train_time:58858ms step_avg:60.31ms
step:977/2315 train_time:58919ms step_avg:60.31ms
step:978/2315 train_time:58981ms step_avg:60.31ms
step:979/2315 train_time:59041ms step_avg:60.31ms
step:980/2315 train_time:59103ms step_avg:60.31ms
step:981/2315 train_time:59163ms step_avg:60.31ms
step:982/2315 train_time:59224ms step_avg:60.31ms
step:983/2315 train_time:59284ms step_avg:60.31ms
step:984/2315 train_time:59346ms step_avg:60.31ms
step:985/2315 train_time:59407ms step_avg:60.31ms
step:986/2315 train_time:59467ms step_avg:60.31ms
step:987/2315 train_time:59528ms step_avg:60.31ms
step:988/2315 train_time:59590ms step_avg:60.31ms
step:989/2315 train_time:59651ms step_avg:60.31ms
step:990/2315 train_time:59712ms step_avg:60.31ms
step:991/2315 train_time:59772ms step_avg:60.31ms
step:992/2315 train_time:59833ms step_avg:60.32ms
step:993/2315 train_time:59894ms step_avg:60.32ms
step:994/2315 train_time:59956ms step_avg:60.32ms
step:995/2315 train_time:60016ms step_avg:60.32ms
step:996/2315 train_time:60077ms step_avg:60.32ms
step:997/2315 train_time:60138ms step_avg:60.32ms
step:998/2315 train_time:60200ms step_avg:60.32ms
step:999/2315 train_time:60260ms step_avg:60.32ms
step:1000/2315 train_time:60322ms step_avg:60.32ms
step:1000/2315 val_loss:3.5727 train_time:60385ms step_avg:60.38ms
step:1001/2315 train_time:60402ms step_avg:60.34ms
step:1002/2315 train_time:60446ms step_avg:60.33ms
step:1003/2315 train_time:60515ms step_avg:60.33ms
step:1004/2315 train_time:60582ms step_avg:60.34ms
step:1005/2315 train_time:60642ms step_avg:60.34ms
step:1006/2315 train_time:60702ms step_avg:60.34ms
step:1007/2315 train_time:60761ms step_avg:60.34ms
step:1008/2315 train_time:60822ms step_avg:60.34ms
step:1009/2315 train_time:60881ms step_avg:60.34ms
step:1010/2315 train_time:60941ms step_avg:60.34ms
step:1011/2315 train_time:61001ms step_avg:60.34ms
step:1012/2315 train_time:61061ms step_avg:60.34ms
step:1013/2315 train_time:61121ms step_avg:60.34ms
step:1014/2315 train_time:61181ms step_avg:60.34ms
step:1015/2315 train_time:61241ms step_avg:60.34ms
step:1016/2315 train_time:61308ms step_avg:60.34ms
step:1017/2315 train_time:61371ms step_avg:60.35ms
step:1018/2315 train_time:61433ms step_avg:60.35ms
step:1019/2315 train_time:61496ms step_avg:60.35ms
step:1020/2315 train_time:61558ms step_avg:60.35ms
step:1021/2315 train_time:61619ms step_avg:60.35ms
step:1022/2315 train_time:61680ms step_avg:60.35ms
step:1023/2315 train_time:61741ms step_avg:60.35ms
step:1024/2315 train_time:61801ms step_avg:60.35ms
step:1025/2315 train_time:61861ms step_avg:60.35ms
step:1026/2315 train_time:61921ms step_avg:60.35ms
step:1027/2315 train_time:61981ms step_avg:60.35ms
step:1028/2315 train_time:62041ms step_avg:60.35ms
step:1029/2315 train_time:62100ms step_avg:60.35ms
step:1030/2315 train_time:62161ms step_avg:60.35ms
step:1031/2315 train_time:62223ms step_avg:60.35ms
step:1032/2315 train_time:62286ms step_avg:60.35ms
step:1033/2315 train_time:62348ms step_avg:60.36ms
step:1034/2315 train_time:62411ms step_avg:60.36ms
step:1035/2315 train_time:62472ms step_avg:60.36ms
step:1036/2315 train_time:62537ms step_avg:60.36ms
step:1037/2315 train_time:62594ms step_avg:60.36ms
step:1038/2315 train_time:62656ms step_avg:60.36ms
step:1039/2315 train_time:62716ms step_avg:60.36ms
step:1040/2315 train_time:62777ms step_avg:60.36ms
step:1041/2315 train_time:62837ms step_avg:60.36ms
step:1042/2315 train_time:62898ms step_avg:60.36ms
step:1043/2315 train_time:62957ms step_avg:60.36ms
step:1044/2315 train_time:63018ms step_avg:60.36ms
step:1045/2315 train_time:63078ms step_avg:60.36ms
step:1046/2315 train_time:63140ms step_avg:60.36ms
step:1047/2315 train_time:63201ms step_avg:60.36ms
step:1048/2315 train_time:63263ms step_avg:60.37ms
step:1049/2315 train_time:63325ms step_avg:60.37ms
step:1050/2315 train_time:63387ms step_avg:60.37ms
step:1051/2315 train_time:63448ms step_avg:60.37ms
step:1052/2315 train_time:63510ms step_avg:60.37ms
step:1053/2315 train_time:63571ms step_avg:60.37ms
step:1054/2315 train_time:63632ms step_avg:60.37ms
step:1055/2315 train_time:63692ms step_avg:60.37ms
step:1056/2315 train_time:63752ms step_avg:60.37ms
step:1057/2315 train_time:63813ms step_avg:60.37ms
step:1058/2315 train_time:63873ms step_avg:60.37ms
step:1059/2315 train_time:63933ms step_avg:60.37ms
step:1060/2315 train_time:63994ms step_avg:60.37ms
step:1061/2315 train_time:64054ms step_avg:60.37ms
step:1062/2315 train_time:64115ms step_avg:60.37ms
step:1063/2315 train_time:64176ms step_avg:60.37ms
step:1064/2315 train_time:64237ms step_avg:60.37ms
step:1065/2315 train_time:64298ms step_avg:60.37ms
step:1066/2315 train_time:64360ms step_avg:60.38ms
step:1067/2315 train_time:64422ms step_avg:60.38ms
step:1068/2315 train_time:64484ms step_avg:60.38ms
step:1069/2315 train_time:64544ms step_avg:60.38ms
step:1070/2315 train_time:64606ms step_avg:60.38ms
step:1071/2315 train_time:64667ms step_avg:60.38ms
step:1072/2315 train_time:64728ms step_avg:60.38ms
step:1073/2315 train_time:64788ms step_avg:60.38ms
step:1074/2315 train_time:64849ms step_avg:60.38ms
step:1075/2315 train_time:64910ms step_avg:60.38ms
step:1076/2315 train_time:64971ms step_avg:60.38ms
step:1077/2315 train_time:65031ms step_avg:60.38ms
step:1078/2315 train_time:65092ms step_avg:60.38ms
step:1079/2315 train_time:65152ms step_avg:60.38ms
step:1080/2315 train_time:65212ms step_avg:60.38ms
step:1081/2315 train_time:65273ms step_avg:60.38ms
step:1082/2315 train_time:65334ms step_avg:60.38ms
step:1083/2315 train_time:65395ms step_avg:60.38ms
step:1084/2315 train_time:65457ms step_avg:60.39ms
step:1085/2315 train_time:65518ms step_avg:60.39ms
step:1086/2315 train_time:65581ms step_avg:60.39ms
step:1087/2315 train_time:65642ms step_avg:60.39ms
step:1088/2315 train_time:65703ms step_avg:60.39ms
step:1089/2315 train_time:65764ms step_avg:60.39ms
step:1090/2315 train_time:65825ms step_avg:60.39ms
step:1091/2315 train_time:65886ms step_avg:60.39ms
step:1092/2315 train_time:65947ms step_avg:60.39ms
step:1093/2315 train_time:66007ms step_avg:60.39ms
step:1094/2315 train_time:66068ms step_avg:60.39ms
step:1095/2315 train_time:66128ms step_avg:60.39ms
step:1096/2315 train_time:66190ms step_avg:60.39ms
step:1097/2315 train_time:66250ms step_avg:60.39ms
step:1098/2315 train_time:66311ms step_avg:60.39ms
step:1099/2315 train_time:66372ms step_avg:60.39ms
step:1100/2315 train_time:66433ms step_avg:60.39ms
step:1101/2315 train_time:66493ms step_avg:60.39ms
step:1102/2315 train_time:66554ms step_avg:60.39ms
step:1103/2315 train_time:66615ms step_avg:60.39ms
step:1104/2315 train_time:66677ms step_avg:60.40ms
step:1105/2315 train_time:66737ms step_avg:60.40ms
step:1106/2315 train_time:66798ms step_avg:60.40ms
step:1107/2315 train_time:66859ms step_avg:60.40ms
step:1108/2315 train_time:66921ms step_avg:60.40ms
step:1109/2315 train_time:66981ms step_avg:60.40ms
step:1110/2315 train_time:67043ms step_avg:60.40ms
step:1111/2315 train_time:67104ms step_avg:60.40ms
step:1112/2315 train_time:67165ms step_avg:60.40ms
step:1113/2315 train_time:67226ms step_avg:60.40ms
step:1114/2315 train_time:67287ms step_avg:60.40ms
step:1115/2315 train_time:67348ms step_avg:60.40ms
step:1116/2315 train_time:67409ms step_avg:60.40ms
step:1117/2315 train_time:67469ms step_avg:60.40ms
step:1118/2315 train_time:67533ms step_avg:60.41ms
step:1119/2315 train_time:67591ms step_avg:60.40ms
step:1120/2315 train_time:67652ms step_avg:60.40ms
step:1121/2315 train_time:67713ms step_avg:60.40ms
step:1122/2315 train_time:67774ms step_avg:60.40ms
step:1123/2315 train_time:67834ms step_avg:60.40ms
step:1124/2315 train_time:67895ms step_avg:60.40ms
step:1125/2315 train_time:67956ms step_avg:60.40ms
step:1126/2315 train_time:68017ms step_avg:60.41ms
step:1127/2315 train_time:68078ms step_avg:60.41ms
step:1128/2315 train_time:68139ms step_avg:60.41ms
step:1129/2315 train_time:68200ms step_avg:60.41ms
step:1130/2315 train_time:68262ms step_avg:60.41ms
step:1131/2315 train_time:68323ms step_avg:60.41ms
step:1132/2315 train_time:68384ms step_avg:60.41ms
step:1133/2315 train_time:68445ms step_avg:60.41ms
step:1134/2315 train_time:68506ms step_avg:60.41ms
step:1135/2315 train_time:68567ms step_avg:60.41ms
step:1136/2315 train_time:68628ms step_avg:60.41ms
step:1137/2315 train_time:68688ms step_avg:60.41ms
step:1138/2315 train_time:68749ms step_avg:60.41ms
step:1139/2315 train_time:68810ms step_avg:60.41ms
step:1140/2315 train_time:68871ms step_avg:60.41ms
step:1141/2315 train_time:68935ms step_avg:60.42ms
step:1142/2315 train_time:68993ms step_avg:60.41ms
step:1143/2315 train_time:69053ms step_avg:60.41ms
step:1144/2315 train_time:69114ms step_avg:60.41ms
step:1145/2315 train_time:69174ms step_avg:60.41ms
step:1146/2315 train_time:69237ms step_avg:60.42ms
step:1147/2315 train_time:69296ms step_avg:60.42ms
step:1148/2315 train_time:69358ms step_avg:60.42ms
step:1149/2315 train_time:69420ms step_avg:60.42ms
step:1150/2315 train_time:69481ms step_avg:60.42ms
step:1151/2315 train_time:69542ms step_avg:60.42ms
step:1152/2315 train_time:69603ms step_avg:60.42ms
step:1153/2315 train_time:69664ms step_avg:60.42ms
step:1154/2315 train_time:69725ms step_avg:60.42ms
step:1155/2315 train_time:69787ms step_avg:60.42ms
step:1156/2315 train_time:69847ms step_avg:60.42ms
step:1157/2315 train_time:69907ms step_avg:60.42ms
step:1158/2315 train_time:69969ms step_avg:60.42ms
step:1159/2315 train_time:70029ms step_avg:60.42ms
step:1160/2315 train_time:70091ms step_avg:60.42ms
step:1161/2315 train_time:70151ms step_avg:60.42ms
step:1162/2315 train_time:70212ms step_avg:60.42ms
step:1163/2315 train_time:70272ms step_avg:60.42ms
step:1164/2315 train_time:70333ms step_avg:60.42ms
step:1165/2315 train_time:70393ms step_avg:60.42ms
step:1166/2315 train_time:70454ms step_avg:60.42ms
step:1167/2315 train_time:70515ms step_avg:60.42ms
step:1168/2315 train_time:70576ms step_avg:60.42ms
step:1169/2315 train_time:70637ms step_avg:60.43ms
step:1170/2315 train_time:70699ms step_avg:60.43ms
step:1171/2315 train_time:70760ms step_avg:60.43ms
step:1172/2315 train_time:70821ms step_avg:60.43ms
step:1173/2315 train_time:70883ms step_avg:60.43ms
step:1174/2315 train_time:70944ms step_avg:60.43ms
step:1175/2315 train_time:71004ms step_avg:60.43ms
step:1176/2315 train_time:71066ms step_avg:60.43ms
step:1177/2315 train_time:71126ms step_avg:60.43ms
step:1178/2315 train_time:71188ms step_avg:60.43ms
step:1179/2315 train_time:71249ms step_avg:60.43ms
step:1180/2315 train_time:71310ms step_avg:60.43ms
step:1181/2315 train_time:71370ms step_avg:60.43ms
step:1182/2315 train_time:71431ms step_avg:60.43ms
step:1183/2315 train_time:71492ms step_avg:60.43ms
step:1184/2315 train_time:71553ms step_avg:60.43ms
step:1185/2315 train_time:71613ms step_avg:60.43ms
step:1186/2315 train_time:71674ms step_avg:60.43ms
step:1187/2315 train_time:71735ms step_avg:60.43ms
step:1188/2315 train_time:71796ms step_avg:60.43ms
step:1189/2315 train_time:71858ms step_avg:60.44ms
step:1190/2315 train_time:71920ms step_avg:60.44ms
step:1191/2315 train_time:71981ms step_avg:60.44ms
step:1192/2315 train_time:72042ms step_avg:60.44ms
step:1193/2315 train_time:72103ms step_avg:60.44ms
step:1194/2315 train_time:72164ms step_avg:60.44ms
step:1195/2315 train_time:72225ms step_avg:60.44ms
step:1196/2315 train_time:72286ms step_avg:60.44ms
step:1197/2315 train_time:72347ms step_avg:60.44ms
step:1198/2315 train_time:72408ms step_avg:60.44ms
step:1199/2315 train_time:72468ms step_avg:60.44ms
step:1200/2315 train_time:72530ms step_avg:60.44ms
step:1201/2315 train_time:72590ms step_avg:60.44ms
step:1202/2315 train_time:72651ms step_avg:60.44ms
step:1203/2315 train_time:72711ms step_avg:60.44ms
step:1204/2315 train_time:72772ms step_avg:60.44ms
step:1205/2315 train_time:72833ms step_avg:60.44ms
step:1206/2315 train_time:72894ms step_avg:60.44ms
step:1207/2315 train_time:72954ms step_avg:60.44ms
step:1208/2315 train_time:73015ms step_avg:60.44ms
step:1209/2315 train_time:73076ms step_avg:60.44ms
step:1210/2315 train_time:73138ms step_avg:60.44ms
step:1211/2315 train_time:73199ms step_avg:60.45ms
step:1212/2315 train_time:73260ms step_avg:60.45ms
step:1213/2315 train_time:73321ms step_avg:60.45ms
step:1214/2315 train_time:73383ms step_avg:60.45ms
step:1215/2315 train_time:73444ms step_avg:60.45ms
step:1216/2315 train_time:73505ms step_avg:60.45ms
step:1217/2315 train_time:73566ms step_avg:60.45ms
step:1218/2315 train_time:73628ms step_avg:60.45ms
step:1219/2315 train_time:73688ms step_avg:60.45ms
step:1220/2315 train_time:73750ms step_avg:60.45ms
step:1221/2315 train_time:73810ms step_avg:60.45ms
step:1222/2315 train_time:73871ms step_avg:60.45ms
step:1223/2315 train_time:73932ms step_avg:60.45ms
step:1224/2315 train_time:73993ms step_avg:60.45ms
step:1225/2315 train_time:74053ms step_avg:60.45ms
step:1226/2315 train_time:74114ms step_avg:60.45ms
step:1227/2315 train_time:74175ms step_avg:60.45ms
step:1228/2315 train_time:74236ms step_avg:60.45ms
step:1229/2315 train_time:74297ms step_avg:60.45ms
step:1230/2315 train_time:74359ms step_avg:60.45ms
step:1231/2315 train_time:74420ms step_avg:60.46ms
step:1232/2315 train_time:74481ms step_avg:60.46ms
step:1233/2315 train_time:74543ms step_avg:60.46ms
step:1234/2315 train_time:74604ms step_avg:60.46ms
step:1235/2315 train_time:74665ms step_avg:60.46ms
step:1236/2315 train_time:74726ms step_avg:60.46ms
step:1237/2315 train_time:74787ms step_avg:60.46ms
step:1238/2315 train_time:74848ms step_avg:60.46ms
step:1239/2315 train_time:74908ms step_avg:60.46ms
step:1240/2315 train_time:74969ms step_avg:60.46ms
step:1241/2315 train_time:75029ms step_avg:60.46ms
step:1242/2315 train_time:75090ms step_avg:60.46ms
step:1243/2315 train_time:75151ms step_avg:60.46ms
step:1244/2315 train_time:75212ms step_avg:60.46ms
step:1245/2315 train_time:75273ms step_avg:60.46ms
step:1246/2315 train_time:75333ms step_avg:60.46ms
step:1247/2315 train_time:75393ms step_avg:60.46ms
step:1248/2315 train_time:75455ms step_avg:60.46ms
step:1249/2315 train_time:75516ms step_avg:60.46ms
step:1250/2315 train_time:75578ms step_avg:60.46ms
step:1250/2315 val_loss:3.5134 train_time:75641ms step_avg:60.51ms
step:1251/2315 train_time:75658ms step_avg:60.48ms
step:1252/2315 train_time:75703ms step_avg:60.47ms
step:1253/2315 train_time:75767ms step_avg:60.47ms
step:1254/2315 train_time:75834ms step_avg:60.47ms
step:1255/2315 train_time:75896ms step_avg:60.47ms
step:1256/2315 train_time:75957ms step_avg:60.48ms
step:1257/2315 train_time:76017ms step_avg:60.47ms
step:1258/2315 train_time:76077ms step_avg:60.47ms
step:1259/2315 train_time:76137ms step_avg:60.47ms
step:1260/2315 train_time:76198ms step_avg:60.47ms
step:1261/2315 train_time:76258ms step_avg:60.47ms
step:1262/2315 train_time:76318ms step_avg:60.47ms
step:1263/2315 train_time:76378ms step_avg:60.47ms
step:1264/2315 train_time:76439ms step_avg:60.47ms
step:1265/2315 train_time:76499ms step_avg:60.47ms
step:1266/2315 train_time:76561ms step_avg:60.47ms
step:1267/2315 train_time:76623ms step_avg:60.48ms
step:1268/2315 train_time:76685ms step_avg:60.48ms
step:1269/2315 train_time:76748ms step_avg:60.48ms
step:1270/2315 train_time:76811ms step_avg:60.48ms
step:1271/2315 train_time:76871ms step_avg:60.48ms
step:1272/2315 train_time:76932ms step_avg:60.48ms
step:1273/2315 train_time:76992ms step_avg:60.48ms
step:1274/2315 train_time:77053ms step_avg:60.48ms
step:1275/2315 train_time:77113ms step_avg:60.48ms
step:1276/2315 train_time:77173ms step_avg:60.48ms
step:1277/2315 train_time:77232ms step_avg:60.48ms
step:1278/2315 train_time:77293ms step_avg:60.48ms
step:1279/2315 train_time:77353ms step_avg:60.48ms
step:1280/2315 train_time:77413ms step_avg:60.48ms
step:1281/2315 train_time:77473ms step_avg:60.48ms
step:1282/2315 train_time:77534ms step_avg:60.48ms
step:1283/2315 train_time:77595ms step_avg:60.48ms
step:1284/2315 train_time:77658ms step_avg:60.48ms
step:1285/2315 train_time:77720ms step_avg:60.48ms
step:1286/2315 train_time:77782ms step_avg:60.48ms
step:1287/2315 train_time:77844ms step_avg:60.48ms
step:1288/2315 train_time:77906ms step_avg:60.49ms
step:1289/2315 train_time:77966ms step_avg:60.49ms
step:1290/2315 train_time:78027ms step_avg:60.49ms
step:1291/2315 train_time:78088ms step_avg:60.49ms
step:1292/2315 train_time:78149ms step_avg:60.49ms
step:1293/2315 train_time:78209ms step_avg:60.49ms
step:1294/2315 train_time:78270ms step_avg:60.49ms
step:1295/2315 train_time:78331ms step_avg:60.49ms
step:1296/2315 train_time:78392ms step_avg:60.49ms
step:1297/2315 train_time:78452ms step_avg:60.49ms
step:1298/2315 train_time:78512ms step_avg:60.49ms
step:1299/2315 train_time:78573ms step_avg:60.49ms
step:1300/2315 train_time:78634ms step_avg:60.49ms
step:1301/2315 train_time:78695ms step_avg:60.49ms
step:1302/2315 train_time:78757ms step_avg:60.49ms
step:1303/2315 train_time:78819ms step_avg:60.49ms
step:1304/2315 train_time:78881ms step_avg:60.49ms
step:1305/2315 train_time:78942ms step_avg:60.49ms
step:1306/2315 train_time:79003ms step_avg:60.49ms
step:1307/2315 train_time:79064ms step_avg:60.49ms
step:1308/2315 train_time:79125ms step_avg:60.49ms
step:1309/2315 train_time:79185ms step_avg:60.49ms
step:1310/2315 train_time:79247ms step_avg:60.49ms
step:1311/2315 train_time:79308ms step_avg:60.49ms
step:1312/2315 train_time:79369ms step_avg:60.49ms
step:1313/2315 train_time:79429ms step_avg:60.49ms
step:1314/2315 train_time:79490ms step_avg:60.49ms
step:1315/2315 train_time:79550ms step_avg:60.49ms
step:1316/2315 train_time:79611ms step_avg:60.50ms
step:1317/2315 train_time:79672ms step_avg:60.50ms
step:1318/2315 train_time:79734ms step_avg:60.50ms
step:1319/2315 train_time:79794ms step_avg:60.50ms
step:1320/2315 train_time:79856ms step_avg:60.50ms
step:1321/2315 train_time:79918ms step_avg:60.50ms
step:1322/2315 train_time:79979ms step_avg:60.50ms
step:1323/2315 train_time:80041ms step_avg:60.50ms
step:1324/2315 train_time:80103ms step_avg:60.50ms
step:1325/2315 train_time:80163ms step_avg:60.50ms
step:1326/2315 train_time:80224ms step_avg:60.50ms
step:1327/2315 train_time:80286ms step_avg:60.50ms
step:1328/2315 train_time:80347ms step_avg:60.50ms
step:1329/2315 train_time:80408ms step_avg:60.50ms
step:1330/2315 train_time:80468ms step_avg:60.50ms
step:1331/2315 train_time:80529ms step_avg:60.50ms
step:1332/2315 train_time:80590ms step_avg:60.50ms
step:1333/2315 train_time:80650ms step_avg:60.50ms
step:1334/2315 train_time:80712ms step_avg:60.50ms
step:1335/2315 train_time:80772ms step_avg:60.50ms
step:1336/2315 train_time:80834ms step_avg:60.50ms
step:1337/2315 train_time:80894ms step_avg:60.50ms
step:1338/2315 train_time:80955ms step_avg:60.50ms
step:1339/2315 train_time:81016ms step_avg:60.50ms
step:1340/2315 train_time:81077ms step_avg:60.51ms
step:1341/2315 train_time:81138ms step_avg:60.51ms
step:1342/2315 train_time:81200ms step_avg:60.51ms
step:1343/2315 train_time:81261ms step_avg:60.51ms
step:1344/2315 train_time:81322ms step_avg:60.51ms
step:1345/2315 train_time:81382ms step_avg:60.51ms
step:1346/2315 train_time:81443ms step_avg:60.51ms
step:1347/2315 train_time:81504ms step_avg:60.51ms
step:1348/2315 train_time:81565ms step_avg:60.51ms
step:1349/2315 train_time:81626ms step_avg:60.51ms
step:1350/2315 train_time:81687ms step_avg:60.51ms
step:1351/2315 train_time:81748ms step_avg:60.51ms
step:1352/2315 train_time:81809ms step_avg:60.51ms
step:1353/2315 train_time:81870ms step_avg:60.51ms
step:1354/2315 train_time:81932ms step_avg:60.51ms
step:1355/2315 train_time:81993ms step_avg:60.51ms
step:1356/2315 train_time:82054ms step_avg:60.51ms
step:1357/2315 train_time:82114ms step_avg:60.51ms
step:1358/2315 train_time:82175ms step_avg:60.51ms
step:1359/2315 train_time:82235ms step_avg:60.51ms
step:1360/2315 train_time:82297ms step_avg:60.51ms
step:1361/2315 train_time:82358ms step_avg:60.51ms
step:1362/2315 train_time:82420ms step_avg:60.51ms
step:1363/2315 train_time:82481ms step_avg:60.51ms
step:1364/2315 train_time:82543ms step_avg:60.52ms
step:1365/2315 train_time:82604ms step_avg:60.52ms
step:1366/2315 train_time:82666ms step_avg:60.52ms
step:1367/2315 train_time:82727ms step_avg:60.52ms
step:1368/2315 train_time:82788ms step_avg:60.52ms
step:1369/2315 train_time:82849ms step_avg:60.52ms
step:1370/2315 train_time:82910ms step_avg:60.52ms
step:1371/2315 train_time:82970ms step_avg:60.52ms
step:1372/2315 train_time:83032ms step_avg:60.52ms
step:1373/2315 train_time:83092ms step_avg:60.52ms
step:1374/2315 train_time:83153ms step_avg:60.52ms
step:1375/2315 train_time:83213ms step_avg:60.52ms
step:1376/2315 train_time:83274ms step_avg:60.52ms
step:1377/2315 train_time:83335ms step_avg:60.52ms
step:1378/2315 train_time:83395ms step_avg:60.52ms
step:1379/2315 train_time:83456ms step_avg:60.52ms
step:1380/2315 train_time:83518ms step_avg:60.52ms
step:1381/2315 train_time:83580ms step_avg:60.52ms
step:1382/2315 train_time:83641ms step_avg:60.52ms
step:1383/2315 train_time:83703ms step_avg:60.52ms
step:1384/2315 train_time:83765ms step_avg:60.52ms
step:1385/2315 train_time:83825ms step_avg:60.52ms
step:1386/2315 train_time:83887ms step_avg:60.52ms
step:1387/2315 train_time:83948ms step_avg:60.52ms
step:1388/2315 train_time:84008ms step_avg:60.52ms
step:1389/2315 train_time:84069ms step_avg:60.52ms
step:1390/2315 train_time:84130ms step_avg:60.53ms
step:1391/2315 train_time:84191ms step_avg:60.53ms
step:1392/2315 train_time:84252ms step_avg:60.53ms
step:1393/2315 train_time:84313ms step_avg:60.53ms
step:1394/2315 train_time:84373ms step_avg:60.53ms
step:1395/2315 train_time:84433ms step_avg:60.53ms
step:1396/2315 train_time:84495ms step_avg:60.53ms
step:1397/2315 train_time:84556ms step_avg:60.53ms
step:1398/2315 train_time:84617ms step_avg:60.53ms
step:1399/2315 train_time:84678ms step_avg:60.53ms
step:1400/2315 train_time:84740ms step_avg:60.53ms
step:1401/2315 train_time:84801ms step_avg:60.53ms
step:1402/2315 train_time:84862ms step_avg:60.53ms
step:1403/2315 train_time:84923ms step_avg:60.53ms
step:1404/2315 train_time:84984ms step_avg:60.53ms
step:1405/2315 train_time:85045ms step_avg:60.53ms
step:1406/2315 train_time:85106ms step_avg:60.53ms
step:1407/2315 train_time:85167ms step_avg:60.53ms
step:1408/2315 train_time:85229ms step_avg:60.53ms
step:1409/2315 train_time:85289ms step_avg:60.53ms
step:1410/2315 train_time:85350ms step_avg:60.53ms
step:1411/2315 train_time:85411ms step_avg:60.53ms
step:1412/2315 train_time:85472ms step_avg:60.53ms
step:1413/2315 train_time:85533ms step_avg:60.53ms
step:1414/2315 train_time:85594ms step_avg:60.53ms
step:1415/2315 train_time:85654ms step_avg:60.53ms
step:1416/2315 train_time:85715ms step_avg:60.53ms
step:1417/2315 train_time:85776ms step_avg:60.53ms
step:1418/2315 train_time:85837ms step_avg:60.53ms
step:1419/2315 train_time:85898ms step_avg:60.53ms
step:1420/2315 train_time:85960ms step_avg:60.54ms
step:1421/2315 train_time:86022ms step_avg:60.54ms
step:1422/2315 train_time:86083ms step_avg:60.54ms
step:1423/2315 train_time:86144ms step_avg:60.54ms
step:1424/2315 train_time:86205ms step_avg:60.54ms
step:1425/2315 train_time:86265ms step_avg:60.54ms
step:1426/2315 train_time:86327ms step_avg:60.54ms
step:1427/2315 train_time:86387ms step_avg:60.54ms
step:1428/2315 train_time:86449ms step_avg:60.54ms
step:1429/2315 train_time:86509ms step_avg:60.54ms
step:1430/2315 train_time:86570ms step_avg:60.54ms
step:1431/2315 train_time:86631ms step_avg:60.54ms
step:1432/2315 train_time:86692ms step_avg:60.54ms
step:1433/2315 train_time:86753ms step_avg:60.54ms
step:1434/2315 train_time:86814ms step_avg:60.54ms
step:1435/2315 train_time:86874ms step_avg:60.54ms
step:1436/2315 train_time:86935ms step_avg:60.54ms
step:1437/2315 train_time:86996ms step_avg:60.54ms
step:1438/2315 train_time:87058ms step_avg:60.54ms
step:1439/2315 train_time:87119ms step_avg:60.54ms
step:1440/2315 train_time:87181ms step_avg:60.54ms
step:1441/2315 train_time:87241ms step_avg:60.54ms
step:1442/2315 train_time:87303ms step_avg:60.54ms
step:1443/2315 train_time:87363ms step_avg:60.54ms
step:1444/2315 train_time:87424ms step_avg:60.54ms
step:1445/2315 train_time:87487ms step_avg:60.54ms
step:1446/2315 train_time:87547ms step_avg:60.54ms
step:1447/2315 train_time:87608ms step_avg:60.54ms
step:1448/2315 train_time:87669ms step_avg:60.54ms
step:1449/2315 train_time:87729ms step_avg:60.54ms
step:1450/2315 train_time:87791ms step_avg:60.55ms
step:1451/2315 train_time:87851ms step_avg:60.55ms
step:1452/2315 train_time:87912ms step_avg:60.55ms
step:1453/2315 train_time:87973ms step_avg:60.55ms
step:1454/2315 train_time:88034ms step_avg:60.55ms
step:1455/2315 train_time:88094ms step_avg:60.55ms
step:1456/2315 train_time:88155ms step_avg:60.55ms
step:1457/2315 train_time:88216ms step_avg:60.55ms
step:1458/2315 train_time:88277ms step_avg:60.55ms
step:1459/2315 train_time:88338ms step_avg:60.55ms
step:1460/2315 train_time:88400ms step_avg:60.55ms
step:1461/2315 train_time:88462ms step_avg:60.55ms
step:1462/2315 train_time:88523ms step_avg:60.55ms
step:1463/2315 train_time:88584ms step_avg:60.55ms
step:1464/2315 train_time:88646ms step_avg:60.55ms
step:1465/2315 train_time:88707ms step_avg:60.55ms
step:1466/2315 train_time:88768ms step_avg:60.55ms
step:1467/2315 train_time:88829ms step_avg:60.55ms
step:1468/2315 train_time:88890ms step_avg:60.55ms
step:1469/2315 train_time:88951ms step_avg:60.55ms
step:1470/2315 train_time:89011ms step_avg:60.55ms
step:1471/2315 train_time:89072ms step_avg:60.55ms
step:1472/2315 train_time:89133ms step_avg:60.55ms
step:1473/2315 train_time:89193ms step_avg:60.55ms
step:1474/2315 train_time:89254ms step_avg:60.55ms
step:1475/2315 train_time:89315ms step_avg:60.55ms
step:1476/2315 train_time:89376ms step_avg:60.55ms
step:1477/2315 train_time:89437ms step_avg:60.55ms
step:1478/2315 train_time:89498ms step_avg:60.55ms
step:1479/2315 train_time:89560ms step_avg:60.55ms
step:1480/2315 train_time:89622ms step_avg:60.56ms
step:1481/2315 train_time:89683ms step_avg:60.56ms
step:1482/2315 train_time:89745ms step_avg:60.56ms
step:1483/2315 train_time:89806ms step_avg:60.56ms
step:1484/2315 train_time:89867ms step_avg:60.56ms
step:1485/2315 train_time:89928ms step_avg:60.56ms
step:1486/2315 train_time:89988ms step_avg:60.56ms
step:1487/2315 train_time:90049ms step_avg:60.56ms
step:1488/2315 train_time:90110ms step_avg:60.56ms
step:1489/2315 train_time:90170ms step_avg:60.56ms
step:1490/2315 train_time:90232ms step_avg:60.56ms
step:1491/2315 train_time:90292ms step_avg:60.56ms
step:1492/2315 train_time:90353ms step_avg:60.56ms
step:1493/2315 train_time:90413ms step_avg:60.56ms
step:1494/2315 train_time:90474ms step_avg:60.56ms
step:1495/2315 train_time:90534ms step_avg:60.56ms
step:1496/2315 train_time:90596ms step_avg:60.56ms
step:1497/2315 train_time:90657ms step_avg:60.56ms
step:1498/2315 train_time:90719ms step_avg:60.56ms
step:1499/2315 train_time:90780ms step_avg:60.56ms
step:1500/2315 train_time:90842ms step_avg:60.56ms
step:1500/2315 val_loss:3.4492 train_time:90905ms step_avg:60.60ms
step:1501/2315 train_time:90923ms step_avg:60.57ms
step:1502/2315 train_time:90968ms step_avg:60.56ms
step:1503/2315 train_time:91030ms step_avg:60.57ms
step:1504/2315 train_time:91093ms step_avg:60.57ms
step:1505/2315 train_time:91153ms step_avg:60.57ms
step:1506/2315 train_time:91214ms step_avg:60.57ms
step:1507/2315 train_time:91274ms step_avg:60.57ms
step:1508/2315 train_time:91335ms step_avg:60.57ms
step:1509/2315 train_time:91395ms step_avg:60.57ms
step:1510/2315 train_time:91456ms step_avg:60.57ms
step:1511/2315 train_time:91516ms step_avg:60.57ms
step:1512/2315 train_time:91576ms step_avg:60.57ms
step:1513/2315 train_time:91636ms step_avg:60.57ms
step:1514/2315 train_time:91697ms step_avg:60.57ms
step:1515/2315 train_time:91757ms step_avg:60.57ms
step:1516/2315 train_time:91819ms step_avg:60.57ms
step:1517/2315 train_time:91882ms step_avg:60.57ms
step:1518/2315 train_time:91945ms step_avg:60.57ms
step:1519/2315 train_time:92007ms step_avg:60.57ms
step:1520/2315 train_time:92069ms step_avg:60.57ms
step:1521/2315 train_time:92130ms step_avg:60.57ms
step:1522/2315 train_time:92191ms step_avg:60.57ms
step:1523/2315 train_time:92252ms step_avg:60.57ms
step:1524/2315 train_time:92314ms step_avg:60.57ms
step:1525/2315 train_time:92375ms step_avg:60.57ms
step:1526/2315 train_time:92437ms step_avg:60.57ms
step:1527/2315 train_time:92497ms step_avg:60.57ms
step:1528/2315 train_time:92558ms step_avg:60.57ms
step:1529/2315 train_time:92619ms step_avg:60.57ms
step:1530/2315 train_time:92680ms step_avg:60.58ms
step:1531/2315 train_time:92741ms step_avg:60.58ms
step:1532/2315 train_time:92803ms step_avg:60.58ms
step:1533/2315 train_time:92865ms step_avg:60.58ms
step:1534/2315 train_time:92927ms step_avg:60.58ms
step:1535/2315 train_time:92988ms step_avg:60.58ms
step:1536/2315 train_time:93050ms step_avg:60.58ms
step:1537/2315 train_time:93111ms step_avg:60.58ms
step:1538/2315 train_time:93173ms step_avg:60.58ms
step:1539/2315 train_time:93233ms step_avg:60.58ms
step:1540/2315 train_time:93295ms step_avg:60.58ms
step:1541/2315 train_time:93356ms step_avg:60.58ms
step:1542/2315 train_time:93418ms step_avg:60.58ms
step:1543/2315 train_time:93478ms step_avg:60.58ms
step:1544/2315 train_time:93540ms step_avg:60.58ms
step:1545/2315 train_time:93600ms step_avg:60.58ms
step:1546/2315 train_time:93662ms step_avg:60.58ms
step:1547/2315 train_time:93723ms step_avg:60.58ms
step:1548/2315 train_time:93785ms step_avg:60.58ms
step:1549/2315 train_time:93846ms step_avg:60.58ms
step:1550/2315 train_time:93908ms step_avg:60.59ms
step:1551/2315 train_time:93969ms step_avg:60.59ms
step:1552/2315 train_time:94031ms step_avg:60.59ms
step:1553/2315 train_time:94093ms step_avg:60.59ms
step:1554/2315 train_time:94155ms step_avg:60.59ms
step:1555/2315 train_time:94216ms step_avg:60.59ms
step:1556/2315 train_time:94278ms step_avg:60.59ms
step:1557/2315 train_time:94340ms step_avg:60.59ms
step:1558/2315 train_time:94401ms step_avg:60.59ms
step:1559/2315 train_time:94461ms step_avg:60.59ms
step:1560/2315 train_time:94523ms step_avg:60.59ms
step:1561/2315 train_time:94582ms step_avg:60.59ms
step:1562/2315 train_time:94644ms step_avg:60.59ms
step:1563/2315 train_time:94705ms step_avg:60.59ms
step:1564/2315 train_time:94766ms step_avg:60.59ms
step:1565/2315 train_time:94827ms step_avg:60.59ms
step:1566/2315 train_time:94889ms step_avg:60.59ms
step:1567/2315 train_time:94950ms step_avg:60.59ms
step:1568/2315 train_time:95012ms step_avg:60.59ms
step:1569/2315 train_time:95073ms step_avg:60.59ms
step:1570/2315 train_time:95135ms step_avg:60.60ms
step:1571/2315 train_time:95196ms step_avg:60.60ms
step:1572/2315 train_time:95257ms step_avg:60.60ms
step:1573/2315 train_time:95318ms step_avg:60.60ms
step:1574/2315 train_time:95379ms step_avg:60.60ms
step:1575/2315 train_time:95440ms step_avg:60.60ms
step:1576/2315 train_time:95502ms step_avg:60.60ms
step:1577/2315 train_time:95562ms step_avg:60.60ms
step:1578/2315 train_time:95624ms step_avg:60.60ms
step:1579/2315 train_time:95685ms step_avg:60.60ms
step:1580/2315 train_time:95747ms step_avg:60.60ms
step:1581/2315 train_time:95807ms step_avg:60.60ms
step:1582/2315 train_time:95869ms step_avg:60.60ms
step:1583/2315 train_time:95929ms step_avg:60.60ms
step:1584/2315 train_time:95991ms step_avg:60.60ms
step:1585/2315 train_time:96051ms step_avg:60.60ms
step:1586/2315 train_time:96113ms step_avg:60.60ms
step:1587/2315 train_time:96174ms step_avg:60.60ms
step:1588/2315 train_time:96236ms step_avg:60.60ms
step:1589/2315 train_time:96297ms step_avg:60.60ms
step:1590/2315 train_time:96359ms step_avg:60.60ms
step:1591/2315 train_time:96420ms step_avg:60.60ms
step:1592/2315 train_time:96482ms step_avg:60.60ms
step:1593/2315 train_time:96543ms step_avg:60.60ms
step:1594/2315 train_time:96604ms step_avg:60.60ms
step:1595/2315 train_time:96665ms step_avg:60.61ms
step:1596/2315 train_time:96726ms step_avg:60.61ms
step:1597/2315 train_time:96787ms step_avg:60.61ms
step:1598/2315 train_time:96848ms step_avg:60.61ms
step:1599/2315 train_time:96909ms step_avg:60.61ms
step:1600/2315 train_time:96970ms step_avg:60.61ms
step:1601/2315 train_time:97031ms step_avg:60.61ms
step:1602/2315 train_time:97092ms step_avg:60.61ms
step:1603/2315 train_time:97153ms step_avg:60.61ms
step:1604/2315 train_time:97215ms step_avg:60.61ms
step:1605/2315 train_time:97277ms step_avg:60.61ms
step:1606/2315 train_time:97339ms step_avg:60.61ms
step:1607/2315 train_time:97400ms step_avg:60.61ms
step:1608/2315 train_time:97462ms step_avg:60.61ms
step:1609/2315 train_time:97523ms step_avg:60.61ms
step:1610/2315 train_time:97585ms step_avg:60.61ms
step:1611/2315 train_time:97645ms step_avg:60.61ms
step:1612/2315 train_time:97707ms step_avg:60.61ms
step:1613/2315 train_time:97767ms step_avg:60.61ms
step:1614/2315 train_time:97829ms step_avg:60.61ms
step:1615/2315 train_time:97890ms step_avg:60.61ms
step:1616/2315 train_time:97952ms step_avg:60.61ms
step:1617/2315 train_time:98012ms step_avg:60.61ms
step:1618/2315 train_time:98074ms step_avg:60.61ms
step:1619/2315 train_time:98135ms step_avg:60.61ms
step:1620/2315 train_time:98197ms step_avg:60.62ms
step:1621/2315 train_time:98258ms step_avg:60.62ms
step:1622/2315 train_time:98320ms step_avg:60.62ms
step:1623/2315 train_time:98380ms step_avg:60.62ms
step:1624/2315 train_time:98442ms step_avg:60.62ms
step:1625/2315 train_time:98503ms step_avg:60.62ms
step:1626/2315 train_time:98564ms step_avg:60.62ms
step:1627/2315 train_time:98625ms step_avg:60.62ms
step:1628/2315 train_time:98687ms step_avg:60.62ms
step:1629/2315 train_time:98748ms step_avg:60.62ms
step:1630/2315 train_time:98809ms step_avg:60.62ms
step:1631/2315 train_time:98869ms step_avg:60.62ms
step:1632/2315 train_time:98932ms step_avg:60.62ms
step:1633/2315 train_time:98990ms step_avg:60.62ms
step:1634/2315 train_time:99052ms step_avg:60.62ms
step:1635/2315 train_time:99112ms step_avg:60.62ms
step:1636/2315 train_time:99175ms step_avg:60.62ms
step:1637/2315 train_time:99237ms step_avg:60.62ms
step:1638/2315 train_time:99298ms step_avg:60.62ms
step:1639/2315 train_time:99359ms step_avg:60.62ms
step:1640/2315 train_time:99421ms step_avg:60.62ms
step:1641/2315 train_time:99482ms step_avg:60.62ms
step:1642/2315 train_time:99545ms step_avg:60.62ms
step:1643/2315 train_time:99605ms step_avg:60.62ms
step:1644/2315 train_time:99666ms step_avg:60.62ms
step:1645/2315 train_time:99726ms step_avg:60.62ms
step:1646/2315 train_time:99789ms step_avg:60.63ms
step:1647/2315 train_time:99850ms step_avg:60.63ms
step:1648/2315 train_time:99911ms step_avg:60.63ms
step:1649/2315 train_time:99972ms step_avg:60.63ms
step:1650/2315 train_time:100034ms step_avg:60.63ms
step:1651/2315 train_time:100095ms step_avg:60.63ms
step:1652/2315 train_time:100157ms step_avg:60.63ms
step:1653/2315 train_time:100218ms step_avg:60.63ms
step:1654/2315 train_time:100280ms step_avg:60.63ms
step:1655/2315 train_time:100340ms step_avg:60.63ms
step:1656/2315 train_time:100402ms step_avg:60.63ms
step:1657/2315 train_time:100462ms step_avg:60.63ms
step:1658/2315 train_time:100524ms step_avg:60.63ms
step:1659/2315 train_time:100585ms step_avg:60.63ms
step:1660/2315 train_time:100646ms step_avg:60.63ms
step:1661/2315 train_time:100707ms step_avg:60.63ms
step:1662/2315 train_time:100768ms step_avg:60.63ms
step:1663/2315 train_time:100829ms step_avg:60.63ms
step:1664/2315 train_time:100890ms step_avg:60.63ms
step:1665/2315 train_time:100951ms step_avg:60.63ms
step:1666/2315 train_time:101012ms step_avg:60.63ms
step:1667/2315 train_time:101073ms step_avg:60.63ms
step:1668/2315 train_time:101136ms step_avg:60.63ms
step:1669/2315 train_time:101197ms step_avg:60.63ms
step:1670/2315 train_time:101259ms step_avg:60.63ms
step:1671/2315 train_time:101320ms step_avg:60.63ms
step:1672/2315 train_time:101383ms step_avg:60.64ms
step:1673/2315 train_time:101444ms step_avg:60.64ms
step:1674/2315 train_time:101505ms step_avg:60.64ms
step:1675/2315 train_time:101566ms step_avg:60.64ms
step:1676/2315 train_time:101627ms step_avg:60.64ms
step:1677/2315 train_time:101687ms step_avg:60.64ms
step:1678/2315 train_time:101750ms step_avg:60.64ms
step:1679/2315 train_time:101811ms step_avg:60.64ms
step:1680/2315 train_time:101872ms step_avg:60.64ms
step:1681/2315 train_time:101933ms step_avg:60.64ms
step:1682/2315 train_time:101994ms step_avg:60.64ms
step:1683/2315 train_time:102055ms step_avg:60.64ms
step:1684/2315 train_time:102117ms step_avg:60.64ms
step:1685/2315 train_time:102178ms step_avg:60.64ms
step:1686/2315 train_time:102240ms step_avg:60.64ms
step:1687/2315 train_time:102301ms step_avg:60.64ms
step:1688/2315 train_time:102363ms step_avg:60.64ms
step:1689/2315 train_time:102423ms step_avg:60.64ms
step:1690/2315 train_time:102485ms step_avg:60.64ms
step:1691/2315 train_time:102546ms step_avg:60.64ms
step:1692/2315 train_time:102607ms step_avg:60.64ms
step:1693/2315 train_time:102668ms step_avg:60.64ms
step:1694/2315 train_time:102729ms step_avg:60.64ms
step:1695/2315 train_time:102790ms step_avg:60.64ms
step:1696/2315 train_time:102851ms step_avg:60.64ms
step:1697/2315 train_time:102912ms step_avg:60.64ms
step:1698/2315 train_time:102973ms step_avg:60.64ms
step:1699/2315 train_time:103034ms step_avg:60.64ms
step:1700/2315 train_time:103097ms step_avg:60.65ms
step:1701/2315 train_time:103158ms step_avg:60.65ms
step:1702/2315 train_time:103219ms step_avg:60.65ms
step:1703/2315 train_time:103281ms step_avg:60.65ms
step:1704/2315 train_time:103343ms step_avg:60.65ms
step:1705/2315 train_time:103404ms step_avg:60.65ms
step:1706/2315 train_time:103466ms step_avg:60.65ms
step:1707/2315 train_time:103526ms step_avg:60.65ms
step:1708/2315 train_time:103589ms step_avg:60.65ms
step:1709/2315 train_time:103649ms step_avg:60.65ms
step:1710/2315 train_time:103710ms step_avg:60.65ms
step:1711/2315 train_time:103772ms step_avg:60.65ms
step:1712/2315 train_time:103834ms step_avg:60.65ms
step:1713/2315 train_time:103894ms step_avg:60.65ms
step:1714/2315 train_time:103956ms step_avg:60.65ms
step:1715/2315 train_time:104017ms step_avg:60.65ms
step:1716/2315 train_time:104079ms step_avg:60.65ms
step:1717/2315 train_time:104140ms step_avg:60.65ms
step:1718/2315 train_time:104201ms step_avg:60.65ms
step:1719/2315 train_time:104262ms step_avg:60.65ms
step:1720/2315 train_time:104323ms step_avg:60.65ms
step:1721/2315 train_time:104384ms step_avg:60.65ms
step:1722/2315 train_time:104445ms step_avg:60.65ms
step:1723/2315 train_time:104506ms step_avg:60.65ms
step:1724/2315 train_time:104568ms step_avg:60.65ms
step:1725/2315 train_time:104628ms step_avg:60.65ms
step:1726/2315 train_time:104689ms step_avg:60.65ms
step:1727/2315 train_time:104750ms step_avg:60.65ms
step:1728/2315 train_time:104812ms step_avg:60.65ms
step:1729/2315 train_time:104873ms step_avg:60.66ms
step:1730/2315 train_time:104934ms step_avg:60.66ms
step:1731/2315 train_time:104995ms step_avg:60.66ms
step:1732/2315 train_time:105058ms step_avg:60.66ms
step:1733/2315 train_time:105119ms step_avg:60.66ms
step:1734/2315 train_time:105180ms step_avg:60.66ms
step:1735/2315 train_time:105241ms step_avg:60.66ms
step:1736/2315 train_time:105303ms step_avg:60.66ms
step:1737/2315 train_time:105364ms step_avg:60.66ms
step:1738/2315 train_time:105425ms step_avg:60.66ms
step:1739/2315 train_time:105486ms step_avg:60.66ms
step:1740/2315 train_time:105548ms step_avg:60.66ms
step:1741/2315 train_time:105608ms step_avg:60.66ms
step:1742/2315 train_time:105670ms step_avg:60.66ms
step:1743/2315 train_time:105731ms step_avg:60.66ms
step:1744/2315 train_time:105792ms step_avg:60.66ms
step:1745/2315 train_time:105853ms step_avg:60.66ms
step:1746/2315 train_time:105915ms step_avg:60.66ms
step:1747/2315 train_time:105975ms step_avg:60.66ms
step:1748/2315 train_time:106037ms step_avg:60.66ms
step:1749/2315 train_time:106099ms step_avg:60.66ms
step:1750/2315 train_time:106161ms step_avg:60.66ms
step:1750/2315 val_loss:3.3812 train_time:106224ms step_avg:60.70ms
step:1751/2315 train_time:106242ms step_avg:60.67ms
step:1752/2315 train_time:106286ms step_avg:60.67ms
step:1753/2315 train_time:106354ms step_avg:60.67ms
step:1754/2315 train_time:106420ms step_avg:60.67ms
step:1755/2315 train_time:106481ms step_avg:60.67ms
step:1756/2315 train_time:106542ms step_avg:60.67ms
step:1757/2315 train_time:106602ms step_avg:60.67ms
step:1758/2315 train_time:106662ms step_avg:60.67ms
step:1759/2315 train_time:106722ms step_avg:60.67ms
step:1760/2315 train_time:106782ms step_avg:60.67ms
step:1761/2315 train_time:106842ms step_avg:60.67ms
step:1762/2315 train_time:106903ms step_avg:60.67ms
step:1763/2315 train_time:106962ms step_avg:60.67ms
step:1764/2315 train_time:107023ms step_avg:60.67ms
step:1765/2315 train_time:107084ms step_avg:60.67ms
step:1766/2315 train_time:107151ms step_avg:60.67ms
step:1767/2315 train_time:107214ms step_avg:60.68ms
step:1768/2315 train_time:107277ms step_avg:60.68ms
step:1769/2315 train_time:107340ms step_avg:60.68ms
step:1770/2315 train_time:107402ms step_avg:60.68ms
step:1771/2315 train_time:107462ms step_avg:60.68ms
step:1772/2315 train_time:107523ms step_avg:60.68ms
step:1773/2315 train_time:107584ms step_avg:60.68ms
step:1774/2315 train_time:107645ms step_avg:60.68ms
step:1775/2315 train_time:107705ms step_avg:60.68ms
step:1776/2315 train_time:107766ms step_avg:60.68ms
step:1777/2315 train_time:107826ms step_avg:60.68ms
step:1778/2315 train_time:107886ms step_avg:60.68ms
step:1779/2315 train_time:107946ms step_avg:60.68ms
step:1780/2315 train_time:108006ms step_avg:60.68ms
step:1781/2315 train_time:108067ms step_avg:60.68ms
step:1782/2315 train_time:108130ms step_avg:60.68ms
step:1783/2315 train_time:108191ms step_avg:60.68ms
step:1784/2315 train_time:108253ms step_avg:60.68ms
step:1785/2315 train_time:108316ms step_avg:60.68ms
step:1786/2315 train_time:108379ms step_avg:60.68ms
step:1787/2315 train_time:108441ms step_avg:60.68ms
step:1788/2315 train_time:108502ms step_avg:60.68ms
step:1789/2315 train_time:108563ms step_avg:60.68ms
step:1790/2315 train_time:108623ms step_avg:60.68ms
step:1791/2315 train_time:108684ms step_avg:60.68ms
step:1792/2315 train_time:108745ms step_avg:60.68ms
step:1793/2315 train_time:108805ms step_avg:60.68ms
step:1794/2315 train_time:108866ms step_avg:60.68ms
step:1795/2315 train_time:108926ms step_avg:60.68ms
step:1796/2315 train_time:108987ms step_avg:60.68ms
step:1797/2315 train_time:109047ms step_avg:60.68ms
step:1798/2315 train_time:109109ms step_avg:60.68ms
step:1799/2315 train_time:109170ms step_avg:60.68ms
step:1800/2315 train_time:109232ms step_avg:60.68ms
step:1801/2315 train_time:109294ms step_avg:60.69ms
step:1802/2315 train_time:109356ms step_avg:60.69ms
step:1803/2315 train_time:109418ms step_avg:60.69ms
step:1804/2315 train_time:109482ms step_avg:60.69ms
step:1805/2315 train_time:109541ms step_avg:60.69ms
step:1806/2315 train_time:109602ms step_avg:60.69ms
step:1807/2315 train_time:109663ms step_avg:60.69ms
step:1808/2315 train_time:109723ms step_avg:60.69ms
step:1809/2315 train_time:109784ms step_avg:60.69ms
step:1810/2315 train_time:109845ms step_avg:60.69ms
step:1811/2315 train_time:109905ms step_avg:60.69ms
step:1812/2315 train_time:109966ms step_avg:60.69ms
step:1813/2315 train_time:110027ms step_avg:60.69ms
step:1814/2315 train_time:110088ms step_avg:60.69ms
step:1815/2315 train_time:110150ms step_avg:60.69ms
step:1816/2315 train_time:110211ms step_avg:60.69ms
step:1817/2315 train_time:110272ms step_avg:60.69ms
step:1818/2315 train_time:110334ms step_avg:60.69ms
step:1819/2315 train_time:110396ms step_avg:60.69ms
step:1820/2315 train_time:110458ms step_avg:60.69ms
step:1821/2315 train_time:110520ms step_avg:60.69ms
step:1822/2315 train_time:110581ms step_avg:60.69ms
step:1823/2315 train_time:110642ms step_avg:60.69ms
step:1824/2315 train_time:110704ms step_avg:60.69ms
step:1825/2315 train_time:110764ms step_avg:60.69ms
step:1826/2315 train_time:110826ms step_avg:60.69ms
step:1827/2315 train_time:110886ms step_avg:60.69ms
step:1828/2315 train_time:110947ms step_avg:60.69ms
step:1829/2315 train_time:111007ms step_avg:60.69ms
step:1830/2315 train_time:111068ms step_avg:60.69ms
step:1831/2315 train_time:111129ms step_avg:60.69ms
step:1832/2315 train_time:111191ms step_avg:60.69ms
step:1833/2315 train_time:111252ms step_avg:60.69ms
step:1834/2315 train_time:111314ms step_avg:60.69ms
step:1835/2315 train_time:111375ms step_avg:60.70ms
step:1836/2315 train_time:111438ms step_avg:60.70ms
step:1837/2315 train_time:111499ms step_avg:60.70ms
step:1838/2315 train_time:111560ms step_avg:60.70ms
step:1839/2315 train_time:111621ms step_avg:60.70ms
step:1840/2315 train_time:111683ms step_avg:60.70ms
step:1841/2315 train_time:111743ms step_avg:60.70ms
step:1842/2315 train_time:111804ms step_avg:60.70ms
step:1843/2315 train_time:111865ms step_avg:60.70ms
step:1844/2315 train_time:111926ms step_avg:60.70ms
step:1845/2315 train_time:111987ms step_avg:60.70ms
step:1846/2315 train_time:112049ms step_avg:60.70ms
step:1847/2315 train_time:112109ms step_avg:60.70ms
step:1848/2315 train_time:112170ms step_avg:60.70ms
step:1849/2315 train_time:112231ms step_avg:60.70ms
step:1850/2315 train_time:112293ms step_avg:60.70ms
step:1851/2315 train_time:112355ms step_avg:60.70ms
step:1852/2315 train_time:112417ms step_avg:60.70ms
step:1853/2315 train_time:112479ms step_avg:60.70ms
step:1854/2315 train_time:112541ms step_avg:60.70ms
step:1855/2315 train_time:112602ms step_avg:60.70ms
step:1856/2315 train_time:112663ms step_avg:60.70ms
step:1857/2315 train_time:112725ms step_avg:60.70ms
step:1858/2315 train_time:112787ms step_avg:60.70ms
step:1859/2315 train_time:112847ms step_avg:60.70ms
step:1860/2315 train_time:112908ms step_avg:60.70ms
step:1861/2315 train_time:112969ms step_avg:60.70ms
step:1862/2315 train_time:113031ms step_avg:60.70ms
step:1863/2315 train_time:113091ms step_avg:60.70ms
step:1864/2315 train_time:113152ms step_avg:60.70ms
step:1865/2315 train_time:113213ms step_avg:60.70ms
step:1866/2315 train_time:113275ms step_avg:60.70ms
step:1867/2315 train_time:113336ms step_avg:60.70ms
step:1868/2315 train_time:113398ms step_avg:60.71ms
step:1869/2315 train_time:113459ms step_avg:60.71ms
step:1870/2315 train_time:113520ms step_avg:60.71ms
step:1871/2315 train_time:113581ms step_avg:60.71ms
step:1872/2315 train_time:113643ms step_avg:60.71ms
step:1873/2315 train_time:113704ms step_avg:60.71ms
step:1874/2315 train_time:113765ms step_avg:60.71ms
step:1875/2315 train_time:113825ms step_avg:60.71ms
step:1876/2315 train_time:113887ms step_avg:60.71ms
step:1877/2315 train_time:113947ms step_avg:60.71ms
step:1878/2315 train_time:114009ms step_avg:60.71ms
step:1879/2315 train_time:114070ms step_avg:60.71ms
step:1880/2315 train_time:114131ms step_avg:60.71ms
step:1881/2315 train_time:114192ms step_avg:60.71ms
step:1882/2315 train_time:114254ms step_avg:60.71ms
step:1883/2315 train_time:114315ms step_avg:60.71ms
step:1884/2315 train_time:114377ms step_avg:60.71ms
step:1885/2315 train_time:114438ms step_avg:60.71ms
step:1886/2315 train_time:114499ms step_avg:60.71ms
step:1887/2315 train_time:114560ms step_avg:60.71ms
step:1888/2315 train_time:114622ms step_avg:60.71ms
step:1889/2315 train_time:114683ms step_avg:60.71ms
step:1890/2315 train_time:114744ms step_avg:60.71ms
step:1891/2315 train_time:114805ms step_avg:60.71ms
step:1892/2315 train_time:114867ms step_avg:60.71ms
step:1893/2315 train_time:114927ms step_avg:60.71ms
step:1894/2315 train_time:114989ms step_avg:60.71ms
step:1895/2315 train_time:115049ms step_avg:60.71ms
step:1896/2315 train_time:115111ms step_avg:60.71ms
step:1897/2315 train_time:115171ms step_avg:60.71ms
step:1898/2315 train_time:115233ms step_avg:60.71ms
step:1899/2315 train_time:115295ms step_avg:60.71ms
step:1900/2315 train_time:115356ms step_avg:60.71ms
step:1901/2315 train_time:115418ms step_avg:60.71ms
step:1902/2315 train_time:115480ms step_avg:60.72ms
step:1903/2315 train_time:115541ms step_avg:60.72ms
step:1904/2315 train_time:115603ms step_avg:60.72ms
step:1905/2315 train_time:115664ms step_avg:60.72ms
step:1906/2315 train_time:115725ms step_avg:60.72ms
step:1907/2315 train_time:115786ms step_avg:60.72ms
step:1908/2315 train_time:115847ms step_avg:60.72ms
step:1909/2315 train_time:115908ms step_avg:60.72ms
step:1910/2315 train_time:115970ms step_avg:60.72ms
step:1911/2315 train_time:116031ms step_avg:60.72ms
step:1912/2315 train_time:116092ms step_avg:60.72ms
step:1913/2315 train_time:116153ms step_avg:60.72ms
step:1914/2315 train_time:116215ms step_avg:60.72ms
step:1915/2315 train_time:116276ms step_avg:60.72ms
step:1916/2315 train_time:116337ms step_avg:60.72ms
step:1917/2315 train_time:116398ms step_avg:60.72ms
step:1918/2315 train_time:116460ms step_avg:60.72ms
step:1919/2315 train_time:116521ms step_avg:60.72ms
step:1920/2315 train_time:116583ms step_avg:60.72ms
step:1921/2315 train_time:116644ms step_avg:60.72ms
step:1922/2315 train_time:116705ms step_avg:60.72ms
step:1923/2315 train_time:116766ms step_avg:60.72ms
step:1924/2315 train_time:116828ms step_avg:60.72ms
step:1925/2315 train_time:116888ms step_avg:60.72ms
step:1926/2315 train_time:116950ms step_avg:60.72ms
step:1927/2315 train_time:117011ms step_avg:60.72ms
step:1928/2315 train_time:117072ms step_avg:60.72ms
step:1929/2315 train_time:117133ms step_avg:60.72ms
step:1930/2315 train_time:117195ms step_avg:60.72ms
step:1931/2315 train_time:117257ms step_avg:60.72ms
step:1932/2315 train_time:117319ms step_avg:60.72ms
step:1933/2315 train_time:117380ms step_avg:60.72ms
step:1934/2315 train_time:117441ms step_avg:60.72ms
step:1935/2315 train_time:117502ms step_avg:60.72ms
step:1936/2315 train_time:117564ms step_avg:60.73ms
step:1937/2315 train_time:117625ms step_avg:60.73ms
step:1938/2315 train_time:117686ms step_avg:60.73ms
step:1939/2315 train_time:117747ms step_avg:60.73ms
step:1940/2315 train_time:117808ms step_avg:60.73ms
step:1941/2315 train_time:117869ms step_avg:60.73ms
step:1942/2315 train_time:117931ms step_avg:60.73ms
step:1943/2315 train_time:117992ms step_avg:60.73ms
step:1944/2315 train_time:118053ms step_avg:60.73ms
step:1945/2315 train_time:118114ms step_avg:60.73ms
step:1946/2315 train_time:118176ms step_avg:60.73ms
step:1947/2315 train_time:118237ms step_avg:60.73ms
step:1948/2315 train_time:118299ms step_avg:60.73ms
step:1949/2315 train_time:118360ms step_avg:60.73ms
step:1950/2315 train_time:118421ms step_avg:60.73ms
step:1951/2315 train_time:118482ms step_avg:60.73ms
step:1952/2315 train_time:118543ms step_avg:60.73ms
step:1953/2315 train_time:118604ms step_avg:60.73ms
step:1954/2315 train_time:118666ms step_avg:60.73ms
step:1955/2315 train_time:118726ms step_avg:60.73ms
step:1956/2315 train_time:118788ms step_avg:60.73ms
step:1957/2315 train_time:118849ms step_avg:60.73ms
step:1958/2315 train_time:118910ms step_avg:60.73ms
step:1959/2315 train_time:118971ms step_avg:60.73ms
step:1960/2315 train_time:119033ms step_avg:60.73ms
step:1961/2315 train_time:119094ms step_avg:60.73ms
step:1962/2315 train_time:119155ms step_avg:60.73ms
step:1963/2315 train_time:119216ms step_avg:60.73ms
step:1964/2315 train_time:119282ms step_avg:60.73ms
step:1965/2315 train_time:119340ms step_avg:60.73ms
step:1966/2315 train_time:119401ms step_avg:60.73ms
step:1967/2315 train_time:119462ms step_avg:60.73ms
step:1968/2315 train_time:119523ms step_avg:60.73ms
step:1969/2315 train_time:119584ms step_avg:60.73ms
step:1970/2315 train_time:119646ms step_avg:60.73ms
step:1971/2315 train_time:119706ms step_avg:60.73ms
step:1972/2315 train_time:119768ms step_avg:60.73ms
step:1973/2315 train_time:119829ms step_avg:60.73ms
step:1974/2315 train_time:119890ms step_avg:60.73ms
step:1975/2315 train_time:119951ms step_avg:60.73ms
step:1976/2315 train_time:120012ms step_avg:60.73ms
step:1977/2315 train_time:120074ms step_avg:60.74ms
step:1978/2315 train_time:120135ms step_avg:60.74ms
step:1979/2315 train_time:120196ms step_avg:60.74ms
step:1980/2315 train_time:120258ms step_avg:60.74ms
step:1981/2315 train_time:120318ms step_avg:60.74ms
step:1982/2315 train_time:120380ms step_avg:60.74ms
step:1983/2315 train_time:120441ms step_avg:60.74ms
step:1984/2315 train_time:120503ms step_avg:60.74ms
step:1985/2315 train_time:120564ms step_avg:60.74ms
step:1986/2315 train_time:120625ms step_avg:60.74ms
step:1987/2315 train_time:120686ms step_avg:60.74ms
step:1988/2315 train_time:120747ms step_avg:60.74ms
step:1989/2315 train_time:120808ms step_avg:60.74ms
step:1990/2315 train_time:120870ms step_avg:60.74ms
step:1991/2315 train_time:120931ms step_avg:60.74ms
step:1992/2315 train_time:120992ms step_avg:60.74ms
step:1993/2315 train_time:121052ms step_avg:60.74ms
step:1994/2315 train_time:121114ms step_avg:60.74ms
step:1995/2315 train_time:121175ms step_avg:60.74ms
step:1996/2315 train_time:121237ms step_avg:60.74ms
step:1997/2315 train_time:121298ms step_avg:60.74ms
step:1998/2315 train_time:121360ms step_avg:60.74ms
step:1999/2315 train_time:121421ms step_avg:60.74ms
step:2000/2315 train_time:121482ms step_avg:60.74ms
step:2000/2315 val_loss:3.3302 train_time:121546ms step_avg:60.77ms
step:2001/2315 train_time:121565ms step_avg:60.75ms
step:2002/2315 train_time:121609ms step_avg:60.74ms
step:2003/2315 train_time:121673ms step_avg:60.75ms
step:2004/2315 train_time:121735ms step_avg:60.75ms
step:2005/2315 train_time:121796ms step_avg:60.75ms
step:2006/2315 train_time:121857ms step_avg:60.75ms
step:2007/2315 train_time:121917ms step_avg:60.75ms
step:2008/2315 train_time:121978ms step_avg:60.75ms
step:2009/2315 train_time:122038ms step_avg:60.75ms
step:2010/2315 train_time:122099ms step_avg:60.75ms
step:2011/2315 train_time:122159ms step_avg:60.75ms
step:2012/2315 train_time:122220ms step_avg:60.75ms
step:2013/2315 train_time:122281ms step_avg:60.75ms
step:2014/2315 train_time:122343ms step_avg:60.75ms
step:2015/2315 train_time:122403ms step_avg:60.75ms
step:2016/2315 train_time:122465ms step_avg:60.75ms
step:2017/2315 train_time:122529ms step_avg:60.75ms
step:2018/2315 train_time:122591ms step_avg:60.75ms
step:2019/2315 train_time:122653ms step_avg:60.75ms
step:2020/2315 train_time:122716ms step_avg:60.75ms
step:2021/2315 train_time:122777ms step_avg:60.75ms
step:2022/2315 train_time:122839ms step_avg:60.75ms
step:2023/2315 train_time:122899ms step_avg:60.75ms
step:2024/2315 train_time:122961ms step_avg:60.75ms
step:2025/2315 train_time:123021ms step_avg:60.75ms
step:2026/2315 train_time:123082ms step_avg:60.75ms
step:2027/2315 train_time:123143ms step_avg:60.75ms
step:2028/2315 train_time:123204ms step_avg:60.75ms
step:2029/2315 train_time:123264ms step_avg:60.75ms
step:2030/2315 train_time:123325ms step_avg:60.75ms
step:2031/2315 train_time:123385ms step_avg:60.75ms
step:2032/2315 train_time:123447ms step_avg:60.75ms
step:2033/2315 train_time:123510ms step_avg:60.75ms
step:2034/2315 train_time:123572ms step_avg:60.75ms
step:2035/2315 train_time:123634ms step_avg:60.75ms
step:2036/2315 train_time:123696ms step_avg:60.75ms
step:2037/2315 train_time:123758ms step_avg:60.75ms
step:2038/2315 train_time:123819ms step_avg:60.76ms
step:2039/2315 train_time:123880ms step_avg:60.76ms
step:2040/2315 train_time:123942ms step_avg:60.76ms
step:2041/2315 train_time:124003ms step_avg:60.76ms
step:2042/2315 train_time:124064ms step_avg:60.76ms
step:2043/2315 train_time:124125ms step_avg:60.76ms
step:2044/2315 train_time:124186ms step_avg:60.76ms
step:2045/2315 train_time:124246ms step_avg:60.76ms
step:2046/2315 train_time:124309ms step_avg:60.76ms
step:2047/2315 train_time:124368ms step_avg:60.76ms
step:2048/2315 train_time:124430ms step_avg:60.76ms
step:2049/2315 train_time:124490ms step_avg:60.76ms
step:2050/2315 train_time:124552ms step_avg:60.76ms
step:2051/2315 train_time:124614ms step_avg:60.76ms
step:2052/2315 train_time:124676ms step_avg:60.76ms
step:2053/2315 train_time:124737ms step_avg:60.76ms
step:2054/2315 train_time:124799ms step_avg:60.76ms
step:2055/2315 train_time:124860ms step_avg:60.76ms
step:2056/2315 train_time:124922ms step_avg:60.76ms
step:2057/2315 train_time:124983ms step_avg:60.76ms
step:2058/2315 train_time:125045ms step_avg:60.76ms
step:2059/2315 train_time:125105ms step_avg:60.76ms
step:2060/2315 train_time:125167ms step_avg:60.76ms
step:2061/2315 train_time:125227ms step_avg:60.76ms
step:2062/2315 train_time:125288ms step_avg:60.76ms
step:2063/2315 train_time:125349ms step_avg:60.76ms
step:2064/2315 train_time:125411ms step_avg:60.76ms
step:2065/2315 train_time:125472ms step_avg:60.76ms
step:2066/2315 train_time:125534ms step_avg:60.76ms
step:2067/2315 train_time:125595ms step_avg:60.76ms
step:2068/2315 train_time:125657ms step_avg:60.76ms
step:2069/2315 train_time:125719ms step_avg:60.76ms
step:2070/2315 train_time:125781ms step_avg:60.76ms
step:2071/2315 train_time:125842ms step_avg:60.76ms
step:2072/2315 train_time:125904ms step_avg:60.76ms
step:2073/2315 train_time:125964ms step_avg:60.76ms
step:2074/2315 train_time:126026ms step_avg:60.76ms
step:2075/2315 train_time:126087ms step_avg:60.76ms
step:2076/2315 train_time:126148ms step_avg:60.76ms
step:2077/2315 train_time:126209ms step_avg:60.76ms
step:2078/2315 train_time:126270ms step_avg:60.77ms
step:2079/2315 train_time:126330ms step_avg:60.76ms
step:2080/2315 train_time:126392ms step_avg:60.77ms
step:2081/2315 train_time:126453ms step_avg:60.77ms
step:2082/2315 train_time:126514ms step_avg:60.77ms
step:2083/2315 train_time:126576ms step_avg:60.77ms
step:2084/2315 train_time:126638ms step_avg:60.77ms
step:2085/2315 train_time:126699ms step_avg:60.77ms
step:2086/2315 train_time:126761ms step_avg:60.77ms
step:2087/2315 train_time:126823ms step_avg:60.77ms
step:2088/2315 train_time:126885ms step_avg:60.77ms
step:2089/2315 train_time:126946ms step_avg:60.77ms
step:2090/2315 train_time:127007ms step_avg:60.77ms
step:2091/2315 train_time:127068ms step_avg:60.77ms
step:2092/2315 train_time:127130ms step_avg:60.77ms
step:2093/2315 train_time:127191ms step_avg:60.77ms
step:2094/2315 train_time:127253ms step_avg:60.77ms
step:2095/2315 train_time:127313ms step_avg:60.77ms
step:2096/2315 train_time:127375ms step_avg:60.77ms
step:2097/2315 train_time:127436ms step_avg:60.77ms
step:2098/2315 train_time:127498ms step_avg:60.77ms
step:2099/2315 train_time:127559ms step_avg:60.77ms
step:2100/2315 train_time:127621ms step_avg:60.77ms
step:2101/2315 train_time:127682ms step_avg:60.77ms
step:2102/2315 train_time:127744ms step_avg:60.77ms
step:2103/2315 train_time:127805ms step_avg:60.77ms
step:2104/2315 train_time:127867ms step_avg:60.77ms
step:2105/2315 train_time:127927ms step_avg:60.77ms
step:2106/2315 train_time:127988ms step_avg:60.77ms
step:2107/2315 train_time:128049ms step_avg:60.77ms
step:2108/2315 train_time:128110ms step_avg:60.77ms
step:2109/2315 train_time:128171ms step_avg:60.77ms
step:2110/2315 train_time:128232ms step_avg:60.77ms
step:2111/2315 train_time:128293ms step_avg:60.77ms
step:2112/2315 train_time:128355ms step_avg:60.77ms
step:2113/2315 train_time:128416ms step_avg:60.77ms
step:2114/2315 train_time:128477ms step_avg:60.77ms
step:2115/2315 train_time:128538ms step_avg:60.77ms
step:2116/2315 train_time:128600ms step_avg:60.77ms
step:2117/2315 train_time:128660ms step_avg:60.77ms
step:2118/2315 train_time:128722ms step_avg:60.78ms
step:2119/2315 train_time:128783ms step_avg:60.78ms
step:2120/2315 train_time:128845ms step_avg:60.78ms
step:2121/2315 train_time:128905ms step_avg:60.78ms
step:2122/2315 train_time:128967ms step_avg:60.78ms
step:2123/2315 train_time:129028ms step_avg:60.78ms
step:2124/2315 train_time:129090ms step_avg:60.78ms
step:2125/2315 train_time:129151ms step_avg:60.78ms
step:2126/2315 train_time:129212ms step_avg:60.78ms
step:2127/2315 train_time:129273ms step_avg:60.78ms
step:2128/2315 train_time:129335ms step_avg:60.78ms
step:2129/2315 train_time:129395ms step_avg:60.78ms
step:2130/2315 train_time:129457ms step_avg:60.78ms
step:2131/2315 train_time:129518ms step_avg:60.78ms
step:2132/2315 train_time:129580ms step_avg:60.78ms
step:2133/2315 train_time:129642ms step_avg:60.78ms
step:2134/2315 train_time:129704ms step_avg:60.78ms
step:2135/2315 train_time:129764ms step_avg:60.78ms
step:2136/2315 train_time:129826ms step_avg:60.78ms
step:2137/2315 train_time:129887ms step_avg:60.78ms
step:2138/2315 train_time:129948ms step_avg:60.78ms
step:2139/2315 train_time:130009ms step_avg:60.78ms
step:2140/2315 train_time:130071ms step_avg:60.78ms
step:2141/2315 train_time:130131ms step_avg:60.78ms
step:2142/2315 train_time:130193ms step_avg:60.78ms
step:2143/2315 train_time:130254ms step_avg:60.78ms
step:2144/2315 train_time:130316ms step_avg:60.78ms
step:2145/2315 train_time:130376ms step_avg:60.78ms
step:2146/2315 train_time:130437ms step_avg:60.78ms
step:2147/2315 train_time:130499ms step_avg:60.78ms
step:2148/2315 train_time:130561ms step_avg:60.78ms
step:2149/2315 train_time:130622ms step_avg:60.78ms
step:2150/2315 train_time:130684ms step_avg:60.78ms
step:2151/2315 train_time:130745ms step_avg:60.78ms
step:2152/2315 train_time:130807ms step_avg:60.78ms
step:2153/2315 train_time:130868ms step_avg:60.78ms
step:2154/2315 train_time:130930ms step_avg:60.78ms
step:2155/2315 train_time:130991ms step_avg:60.78ms
step:2156/2315 train_time:131053ms step_avg:60.79ms
step:2157/2315 train_time:131113ms step_avg:60.79ms
step:2158/2315 train_time:131175ms step_avg:60.79ms
step:2159/2315 train_time:131236ms step_avg:60.79ms
step:2160/2315 train_time:131298ms step_avg:60.79ms
step:2161/2315 train_time:131359ms step_avg:60.79ms
step:2162/2315 train_time:131420ms step_avg:60.79ms
step:2163/2315 train_time:131480ms step_avg:60.79ms
step:2164/2315 train_time:131542ms step_avg:60.79ms
step:2165/2315 train_time:131604ms step_avg:60.79ms
step:2166/2315 train_time:131666ms step_avg:60.79ms
step:2167/2315 train_time:131727ms step_avg:60.79ms
step:2168/2315 train_time:131788ms step_avg:60.79ms
step:2169/2315 train_time:131849ms step_avg:60.79ms
step:2170/2315 train_time:131911ms step_avg:60.79ms
step:2171/2315 train_time:131971ms step_avg:60.79ms
step:2172/2315 train_time:132033ms step_avg:60.79ms
step:2173/2315 train_time:132094ms step_avg:60.79ms
step:2174/2315 train_time:132155ms step_avg:60.79ms
step:2175/2315 train_time:132217ms step_avg:60.79ms
step:2176/2315 train_time:132278ms step_avg:60.79ms
step:2177/2315 train_time:132339ms step_avg:60.79ms
step:2178/2315 train_time:132401ms step_avg:60.79ms
step:2179/2315 train_time:132462ms step_avg:60.79ms
step:2180/2315 train_time:132524ms step_avg:60.79ms
step:2181/2315 train_time:132585ms step_avg:60.79ms
step:2182/2315 train_time:132648ms step_avg:60.79ms
step:2183/2315 train_time:132709ms step_avg:60.79ms
step:2184/2315 train_time:132771ms step_avg:60.79ms
step:2185/2315 train_time:132831ms step_avg:60.79ms
step:2186/2315 train_time:132893ms step_avg:60.79ms
step:2187/2315 train_time:132954ms step_avg:60.79ms
step:2188/2315 train_time:133016ms step_avg:60.79ms
step:2189/2315 train_time:133077ms step_avg:60.79ms
step:2190/2315 train_time:133139ms step_avg:60.79ms
step:2191/2315 train_time:133200ms step_avg:60.79ms
step:2192/2315 train_time:133262ms step_avg:60.79ms
step:2193/2315 train_time:133323ms step_avg:60.79ms
step:2194/2315 train_time:133385ms step_avg:60.80ms
step:2195/2315 train_time:133446ms step_avg:60.80ms
step:2196/2315 train_time:133507ms step_avg:60.80ms
step:2197/2315 train_time:133568ms step_avg:60.80ms
step:2198/2315 train_time:133629ms step_avg:60.80ms
step:2199/2315 train_time:133691ms step_avg:60.80ms
step:2200/2315 train_time:133752ms step_avg:60.80ms
step:2201/2315 train_time:133813ms step_avg:60.80ms
step:2202/2315 train_time:133874ms step_avg:60.80ms
step:2203/2315 train_time:133935ms step_avg:60.80ms
step:2204/2315 train_time:133997ms step_avg:60.80ms
step:2205/2315 train_time:134058ms step_avg:60.80ms
step:2206/2315 train_time:134120ms step_avg:60.80ms
step:2207/2315 train_time:134180ms step_avg:60.80ms
step:2208/2315 train_time:134242ms step_avg:60.80ms
step:2209/2315 train_time:134304ms step_avg:60.80ms
step:2210/2315 train_time:134366ms step_avg:60.80ms
step:2211/2315 train_time:134426ms step_avg:60.80ms
step:2212/2315 train_time:134488ms step_avg:60.80ms
step:2213/2315 train_time:134549ms step_avg:60.80ms
step:2214/2315 train_time:134611ms step_avg:60.80ms
step:2215/2315 train_time:134672ms step_avg:60.80ms
step:2216/2315 train_time:134733ms step_avg:60.80ms
step:2217/2315 train_time:134794ms step_avg:60.80ms
step:2218/2315 train_time:134855ms step_avg:60.80ms
step:2219/2315 train_time:134916ms step_avg:60.80ms
step:2220/2315 train_time:134978ms step_avg:60.80ms
step:2221/2315 train_time:135038ms step_avg:60.80ms
step:2222/2315 train_time:135100ms step_avg:60.80ms
step:2223/2315 train_time:135161ms step_avg:60.80ms
step:2224/2315 train_time:135223ms step_avg:60.80ms
step:2225/2315 train_time:135284ms step_avg:60.80ms
step:2226/2315 train_time:135346ms step_avg:60.80ms
step:2227/2315 train_time:135406ms step_avg:60.80ms
step:2228/2315 train_time:135468ms step_avg:60.80ms
step:2229/2315 train_time:135529ms step_avg:60.80ms
step:2230/2315 train_time:135591ms step_avg:60.80ms
step:2231/2315 train_time:135652ms step_avg:60.80ms
step:2232/2315 train_time:135713ms step_avg:60.80ms
step:2233/2315 train_time:135774ms step_avg:60.80ms
step:2234/2315 train_time:135836ms step_avg:60.80ms
step:2235/2315 train_time:135896ms step_avg:60.80ms
step:2236/2315 train_time:135958ms step_avg:60.80ms
step:2237/2315 train_time:136019ms step_avg:60.80ms
step:2238/2315 train_time:136081ms step_avg:60.80ms
step:2239/2315 train_time:136143ms step_avg:60.81ms
step:2240/2315 train_time:136207ms step_avg:60.81ms
step:2241/2315 train_time:136265ms step_avg:60.81ms
step:2242/2315 train_time:136328ms step_avg:60.81ms
step:2243/2315 train_time:136389ms step_avg:60.81ms
step:2244/2315 train_time:136450ms step_avg:60.81ms
step:2245/2315 train_time:136511ms step_avg:60.81ms
step:2246/2315 train_time:136573ms step_avg:60.81ms
step:2247/2315 train_time:136634ms step_avg:60.81ms
step:2248/2315 train_time:136695ms step_avg:60.81ms
step:2249/2315 train_time:136756ms step_avg:60.81ms
step:2250/2315 train_time:136818ms step_avg:60.81ms
step:2250/2315 val_loss:3.2904 train_time:136881ms step_avg:60.84ms
step:2251/2315 train_time:136899ms step_avg:60.82ms
step:2252/2315 train_time:136944ms step_avg:60.81ms
step:2253/2315 train_time:137006ms step_avg:60.81ms
step:2254/2315 train_time:137069ms step_avg:60.81ms
step:2255/2315 train_time:137131ms step_avg:60.81ms
step:2256/2315 train_time:137192ms step_avg:60.81ms
step:2257/2315 train_time:137253ms step_avg:60.81ms
step:2258/2315 train_time:137315ms step_avg:60.81ms
step:2259/2315 train_time:137375ms step_avg:60.81ms
step:2260/2315 train_time:137436ms step_avg:60.81ms
step:2261/2315 train_time:137497ms step_avg:60.81ms
step:2262/2315 train_time:137557ms step_avg:60.81ms
step:2263/2315 train_time:137619ms step_avg:60.81ms
step:2264/2315 train_time:137681ms step_avg:60.81ms
step:2265/2315 train_time:137741ms step_avg:60.81ms
step:2266/2315 train_time:137803ms step_avg:60.81ms
step:2267/2315 train_time:137865ms step_avg:60.81ms
step:2268/2315 train_time:137928ms step_avg:60.81ms
step:2269/2315 train_time:137990ms step_avg:60.82ms
step:2270/2315 train_time:138053ms step_avg:60.82ms
step:2271/2315 train_time:138114ms step_avg:60.82ms
step:2272/2315 train_time:138176ms step_avg:60.82ms
step:2273/2315 train_time:138237ms step_avg:60.82ms
step:2274/2315 train_time:138298ms step_avg:60.82ms
step:2275/2315 train_time:138358ms step_avg:60.82ms
step:2276/2315 train_time:138419ms step_avg:60.82ms
step:2277/2315 train_time:138479ms step_avg:60.82ms
step:2278/2315 train_time:138540ms step_avg:60.82ms
step:2279/2315 train_time:138601ms step_avg:60.82ms
step:2280/2315 train_time:138662ms step_avg:60.82ms
step:2281/2315 train_time:138723ms step_avg:60.82ms
step:2282/2315 train_time:138785ms step_avg:60.82ms
step:2283/2315 train_time:138846ms step_avg:60.82ms
step:2284/2315 train_time:138908ms step_avg:60.82ms
step:2285/2315 train_time:138969ms step_avg:60.82ms
step:2286/2315 train_time:139031ms step_avg:60.82ms
step:2287/2315 train_time:139094ms step_avg:60.82ms
step:2288/2315 train_time:139156ms step_avg:60.82ms
step:2289/2315 train_time:139217ms step_avg:60.82ms
step:2290/2315 train_time:139278ms step_avg:60.82ms
step:2291/2315 train_time:139339ms step_avg:60.82ms
step:2292/2315 train_time:139400ms step_avg:60.82ms
step:2293/2315 train_time:139461ms step_avg:60.82ms
step:2294/2315 train_time:139522ms step_avg:60.82ms
step:2295/2315 train_time:139583ms step_avg:60.82ms
step:2296/2315 train_time:139644ms step_avg:60.82ms
step:2297/2315 train_time:139705ms step_avg:60.82ms
step:2298/2315 train_time:139766ms step_avg:60.82ms
step:2299/2315 train_time:139828ms step_avg:60.82ms
step:2300/2315 train_time:139891ms step_avg:60.82ms
step:2301/2315 train_time:139953ms step_avg:60.82ms
step:2302/2315 train_time:140015ms step_avg:60.82ms
step:2303/2315 train_time:140076ms step_avg:60.82ms
step:2304/2315 train_time:140138ms step_avg:60.82ms
step:2305/2315 train_time:140199ms step_avg:60.82ms
step:2306/2315 train_time:140261ms step_avg:60.82ms
step:2307/2315 train_time:140321ms step_avg:60.82ms
step:2308/2315 train_time:140382ms step_avg:60.82ms
step:2309/2315 train_time:140443ms step_avg:60.82ms
step:2310/2315 train_time:140504ms step_avg:60.82ms
step:2311/2315 train_time:140565ms step_avg:60.82ms
step:2312/2315 train_time:140626ms step_avg:60.82ms
step:2313/2315 train_time:140687ms step_avg:60.82ms
step:2314/2315 train_time:140748ms step_avg:60.82ms
step:2315/2315 train_time:140809ms step_avg:60.82ms
step:2315/2315 val_loss:3.2776 train_time:140872ms step_avg:60.85ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

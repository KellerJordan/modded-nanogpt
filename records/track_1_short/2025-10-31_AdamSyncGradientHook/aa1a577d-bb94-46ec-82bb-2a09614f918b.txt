import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)


            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))



            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal
        self.group_to_param_group = {}

        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                self.group_to_param_group[param] = group

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        world_size = dist.get_world_size()
        grad = param.grad
        rank_size = grad.shape[0] // world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(), grad_slice)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']

            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
        # enable sync in the next training step for the adam optimizer
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = next(train_loader)

        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()


====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251031+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 31 15:41:51 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   36C    P0            125W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   31C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   34C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   35C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   34C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2315 train_time:81ms step_avg:81.05ms
step:2/2315 train_time:191ms step_avg:95.35ms
step:3/2315 train_time:210ms step_avg:69.89ms
step:4/2315 train_time:248ms step_avg:61.98ms
step:5/2315 train_time:305ms step_avg:61.09ms
step:6/2315 train_time:365ms step_avg:60.83ms
step:7/2315 train_time:424ms step_avg:60.55ms
step:8/2315 train_time:484ms step_avg:60.46ms
step:9/2315 train_time:542ms step_avg:60.25ms
step:10/2315 train_time:602ms step_avg:60.21ms
step:11/2315 train_time:661ms step_avg:60.08ms
step:12/2315 train_time:721ms step_avg:60.05ms
step:13/2315 train_time:780ms step_avg:59.97ms
step:14/2315 train_time:840ms step_avg:60.00ms
step:15/2315 train_time:900ms step_avg:59.98ms
step:16/2315 train_time:960ms step_avg:59.99ms
step:17/2315 train_time:1020ms step_avg:60.02ms
step:18/2315 train_time:1084ms step_avg:60.23ms
step:19/2315 train_time:1148ms step_avg:60.44ms
step:20/2315 train_time:1211ms step_avg:60.55ms
step:21/2315 train_time:1272ms step_avg:60.56ms
step:22/2315 train_time:1332ms step_avg:60.56ms
step:23/2315 train_time:1392ms step_avg:60.53ms
step:24/2315 train_time:1452ms step_avg:60.52ms
step:25/2315 train_time:1512ms step_avg:60.49ms
step:26/2315 train_time:1572ms step_avg:60.47ms
step:27/2315 train_time:1632ms step_avg:60.43ms
step:28/2315 train_time:1691ms step_avg:60.41ms
step:29/2315 train_time:1751ms step_avg:60.37ms
step:30/2315 train_time:1811ms step_avg:60.38ms
step:31/2315 train_time:1872ms step_avg:60.37ms
step:32/2315 train_time:1932ms step_avg:60.37ms
step:33/2315 train_time:1992ms step_avg:60.36ms
step:34/2315 train_time:2054ms step_avg:60.42ms
step:35/2315 train_time:2115ms step_avg:60.43ms
step:36/2315 train_time:2177ms step_avg:60.46ms
step:37/2315 train_time:2237ms step_avg:60.46ms
step:38/2315 train_time:2298ms step_avg:60.47ms
step:39/2315 train_time:2358ms step_avg:60.47ms
step:40/2315 train_time:2419ms step_avg:60.48ms
step:41/2315 train_time:2480ms step_avg:60.48ms
step:42/2315 train_time:2540ms step_avg:60.48ms
step:43/2315 train_time:2600ms step_avg:60.46ms
step:44/2315 train_time:2660ms step_avg:60.46ms
step:45/2315 train_time:2720ms step_avg:60.45ms
step:46/2315 train_time:2781ms step_avg:60.46ms
step:47/2315 train_time:2841ms step_avg:60.45ms
step:48/2315 train_time:2902ms step_avg:60.45ms
step:49/2315 train_time:2962ms step_avg:60.44ms
step:50/2315 train_time:3022ms step_avg:60.45ms
step:51/2315 train_time:3083ms step_avg:60.44ms
step:52/2315 train_time:3143ms step_avg:60.44ms
step:53/2315 train_time:3204ms step_avg:60.44ms
step:54/2315 train_time:3265ms step_avg:60.46ms
step:55/2315 train_time:3324ms step_avg:60.44ms
step:56/2315 train_time:3385ms step_avg:60.45ms
step:57/2315 train_time:3445ms step_avg:60.43ms
step:58/2315 train_time:3505ms step_avg:60.43ms
step:59/2315 train_time:3565ms step_avg:60.42ms
step:60/2315 train_time:3625ms step_avg:60.41ms
step:61/2315 train_time:3685ms step_avg:60.40ms
step:62/2315 train_time:3744ms step_avg:60.39ms
step:63/2315 train_time:3805ms step_avg:60.39ms
step:64/2315 train_time:3865ms step_avg:60.39ms
step:65/2315 train_time:3925ms step_avg:60.38ms
step:66/2315 train_time:3985ms step_avg:60.38ms
step:67/2315 train_time:4046ms step_avg:60.38ms
step:68/2315 train_time:4106ms step_avg:60.38ms
step:69/2315 train_time:4166ms step_avg:60.38ms
step:70/2315 train_time:4227ms step_avg:60.39ms
step:71/2315 train_time:4287ms step_avg:60.38ms
step:72/2315 train_time:4347ms step_avg:60.38ms
step:73/2315 train_time:4407ms step_avg:60.37ms
step:74/2315 train_time:4467ms step_avg:60.37ms
step:75/2315 train_time:4526ms step_avg:60.35ms
step:76/2315 train_time:4586ms step_avg:60.34ms
step:77/2315 train_time:4646ms step_avg:60.33ms
step:78/2315 train_time:4705ms step_avg:60.33ms
step:79/2315 train_time:4766ms step_avg:60.33ms
step:80/2315 train_time:4825ms step_avg:60.32ms
step:81/2315 train_time:4885ms step_avg:60.31ms
step:82/2315 train_time:4945ms step_avg:60.31ms
step:83/2315 train_time:5005ms step_avg:60.30ms
step:84/2315 train_time:5065ms step_avg:60.30ms
step:85/2315 train_time:5125ms step_avg:60.29ms
step:86/2315 train_time:5185ms step_avg:60.29ms
step:87/2315 train_time:5245ms step_avg:60.29ms
step:88/2315 train_time:5305ms step_avg:60.29ms
step:89/2315 train_time:5366ms step_avg:60.29ms
step:90/2315 train_time:5426ms step_avg:60.29ms
step:91/2315 train_time:5485ms step_avg:60.27ms
step:92/2315 train_time:5545ms step_avg:60.27ms
step:93/2315 train_time:5604ms step_avg:60.26ms
step:94/2315 train_time:5665ms step_avg:60.26ms
step:95/2315 train_time:5724ms step_avg:60.26ms
step:96/2315 train_time:5784ms step_avg:60.25ms
step:97/2315 train_time:5844ms step_avg:60.24ms
step:98/2315 train_time:5904ms step_avg:60.25ms
step:99/2315 train_time:5965ms step_avg:60.25ms
step:100/2315 train_time:6024ms step_avg:60.24ms
step:101/2315 train_time:6084ms step_avg:60.23ms
step:102/2315 train_time:6144ms step_avg:60.23ms
step:103/2315 train_time:6204ms step_avg:60.23ms
step:104/2315 train_time:6265ms step_avg:60.24ms
step:105/2315 train_time:6325ms step_avg:60.23ms
step:106/2315 train_time:6385ms step_avg:60.24ms
step:107/2315 train_time:6446ms step_avg:60.24ms
step:108/2315 train_time:6506ms step_avg:60.24ms
step:109/2315 train_time:6565ms step_avg:60.23ms
step:110/2315 train_time:6624ms step_avg:60.22ms
step:111/2315 train_time:6684ms step_avg:60.22ms
step:112/2315 train_time:6744ms step_avg:60.21ms
step:113/2315 train_time:6804ms step_avg:60.21ms
step:114/2315 train_time:6864ms step_avg:60.21ms
step:115/2315 train_time:6923ms step_avg:60.20ms
step:116/2315 train_time:6983ms step_avg:60.20ms
step:117/2315 train_time:7043ms step_avg:60.20ms
step:118/2315 train_time:7104ms step_avg:60.20ms
step:119/2315 train_time:7165ms step_avg:60.21ms
step:120/2315 train_time:7225ms step_avg:60.21ms
step:121/2315 train_time:7284ms step_avg:60.20ms
step:122/2315 train_time:7345ms step_avg:60.20ms
step:123/2315 train_time:7404ms step_avg:60.20ms
step:124/2315 train_time:7465ms step_avg:60.20ms
step:125/2315 train_time:7524ms step_avg:60.19ms
step:126/2315 train_time:7584ms step_avg:60.19ms
step:127/2315 train_time:7644ms step_avg:60.19ms
step:128/2315 train_time:7704ms step_avg:60.19ms
step:129/2315 train_time:7764ms step_avg:60.18ms
step:130/2315 train_time:7823ms step_avg:60.18ms
step:131/2315 train_time:7883ms step_avg:60.17ms
step:132/2315 train_time:7943ms step_avg:60.17ms
step:133/2315 train_time:8003ms step_avg:60.17ms
step:134/2315 train_time:8064ms step_avg:60.18ms
step:135/2315 train_time:8123ms step_avg:60.17ms
step:136/2315 train_time:8184ms step_avg:60.18ms
step:137/2315 train_time:8244ms step_avg:60.17ms
step:138/2315 train_time:8304ms step_avg:60.17ms
step:139/2315 train_time:8364ms step_avg:60.17ms
step:140/2315 train_time:8423ms step_avg:60.17ms
step:141/2315 train_time:8483ms step_avg:60.16ms
step:142/2315 train_time:8544ms step_avg:60.17ms
step:143/2315 train_time:8604ms step_avg:60.17ms
step:144/2315 train_time:8664ms step_avg:60.17ms
step:145/2315 train_time:8723ms step_avg:60.16ms
step:146/2315 train_time:8783ms step_avg:60.16ms
step:147/2315 train_time:8843ms step_avg:60.16ms
step:148/2315 train_time:8903ms step_avg:60.16ms
step:149/2315 train_time:8963ms step_avg:60.16ms
step:150/2315 train_time:9023ms step_avg:60.16ms
step:151/2315 train_time:9083ms step_avg:60.16ms
step:152/2315 train_time:9144ms step_avg:60.16ms
step:153/2315 train_time:9203ms step_avg:60.15ms
step:154/2315 train_time:9264ms step_avg:60.16ms
step:155/2315 train_time:9324ms step_avg:60.15ms
step:156/2315 train_time:9384ms step_avg:60.15ms
step:157/2315 train_time:9443ms step_avg:60.15ms
step:158/2315 train_time:9503ms step_avg:60.15ms
step:159/2315 train_time:9563ms step_avg:60.15ms
step:160/2315 train_time:9623ms step_avg:60.14ms
step:161/2315 train_time:9683ms step_avg:60.14ms
step:162/2315 train_time:9743ms step_avg:60.14ms
step:163/2315 train_time:9803ms step_avg:60.14ms
step:164/2315 train_time:9864ms step_avg:60.14ms
step:165/2315 train_time:9923ms step_avg:60.14ms
step:166/2315 train_time:9983ms step_avg:60.14ms
step:167/2315 train_time:10043ms step_avg:60.14ms
step:168/2315 train_time:10104ms step_avg:60.14ms
step:169/2315 train_time:10164ms step_avg:60.14ms
step:170/2315 train_time:10224ms step_avg:60.14ms
step:171/2315 train_time:10284ms step_avg:60.14ms
step:172/2315 train_time:10344ms step_avg:60.14ms
step:173/2315 train_time:10403ms step_avg:60.14ms
step:174/2315 train_time:10464ms step_avg:60.14ms
step:175/2315 train_time:10524ms step_avg:60.14ms
step:176/2315 train_time:10585ms step_avg:60.14ms
step:177/2315 train_time:10644ms step_avg:60.14ms
step:178/2315 train_time:10704ms step_avg:60.14ms
step:179/2315 train_time:10764ms step_avg:60.13ms
step:180/2315 train_time:10824ms step_avg:60.13ms
step:181/2315 train_time:10884ms step_avg:60.13ms
step:182/2315 train_time:10943ms step_avg:60.13ms
step:183/2315 train_time:11003ms step_avg:60.13ms
step:184/2315 train_time:11063ms step_avg:60.13ms
step:185/2315 train_time:11123ms step_avg:60.12ms
step:186/2315 train_time:11183ms step_avg:60.13ms
step:187/2315 train_time:11243ms step_avg:60.12ms
step:188/2315 train_time:11303ms step_avg:60.12ms
step:189/2315 train_time:11362ms step_avg:60.12ms
step:190/2315 train_time:11424ms step_avg:60.12ms
step:191/2315 train_time:11483ms step_avg:60.12ms
step:192/2315 train_time:11543ms step_avg:60.12ms
step:193/2315 train_time:11603ms step_avg:60.12ms
step:194/2315 train_time:11664ms step_avg:60.12ms
step:195/2315 train_time:11724ms step_avg:60.12ms
step:196/2315 train_time:11783ms step_avg:60.12ms
step:197/2315 train_time:11843ms step_avg:60.12ms
step:198/2315 train_time:11903ms step_avg:60.12ms
step:199/2315 train_time:11964ms step_avg:60.12ms
step:200/2315 train_time:12024ms step_avg:60.12ms
step:201/2315 train_time:12083ms step_avg:60.11ms
step:202/2315 train_time:12143ms step_avg:60.11ms
step:203/2315 train_time:12203ms step_avg:60.11ms
step:204/2315 train_time:12264ms step_avg:60.12ms
step:205/2315 train_time:12323ms step_avg:60.11ms
step:206/2315 train_time:12383ms step_avg:60.11ms
step:207/2315 train_time:12443ms step_avg:60.11ms
step:208/2315 train_time:12504ms step_avg:60.11ms
step:209/2315 train_time:12564ms step_avg:60.11ms
step:210/2315 train_time:12624ms step_avg:60.11ms
step:211/2315 train_time:12684ms step_avg:60.11ms
step:212/2315 train_time:12744ms step_avg:60.11ms
step:213/2315 train_time:12804ms step_avg:60.11ms
step:214/2315 train_time:12864ms step_avg:60.11ms
step:215/2315 train_time:12923ms step_avg:60.11ms
step:216/2315 train_time:12984ms step_avg:60.11ms
step:217/2315 train_time:13043ms step_avg:60.11ms
step:218/2315 train_time:13104ms step_avg:60.11ms
step:219/2315 train_time:13164ms step_avg:60.11ms
step:220/2315 train_time:13224ms step_avg:60.11ms
step:221/2315 train_time:13283ms step_avg:60.11ms
step:222/2315 train_time:13343ms step_avg:60.10ms
step:223/2315 train_time:13403ms step_avg:60.10ms
step:224/2315 train_time:13464ms step_avg:60.11ms
step:225/2315 train_time:13524ms step_avg:60.10ms
step:226/2315 train_time:13584ms step_avg:60.11ms
step:227/2315 train_time:13643ms step_avg:60.10ms
step:228/2315 train_time:13704ms step_avg:60.10ms
step:229/2315 train_time:13763ms step_avg:60.10ms
step:230/2315 train_time:13823ms step_avg:60.10ms
step:231/2315 train_time:13883ms step_avg:60.10ms
step:232/2315 train_time:13943ms step_avg:60.10ms
step:233/2315 train_time:14002ms step_avg:60.10ms
step:234/2315 train_time:14064ms step_avg:60.10ms
step:235/2315 train_time:14123ms step_avg:60.10ms
step:236/2315 train_time:14183ms step_avg:60.10ms
step:237/2315 train_time:14243ms step_avg:60.10ms
step:238/2315 train_time:14303ms step_avg:60.10ms
step:239/2315 train_time:14363ms step_avg:60.10ms
step:240/2315 train_time:14423ms step_avg:60.10ms
step:241/2315 train_time:14483ms step_avg:60.09ms
step:242/2315 train_time:14543ms step_avg:60.10ms
step:243/2315 train_time:14603ms step_avg:60.10ms
step:244/2315 train_time:14664ms step_avg:60.10ms
step:245/2315 train_time:14723ms step_avg:60.10ms
step:246/2315 train_time:14784ms step_avg:60.10ms
step:247/2315 train_time:14843ms step_avg:60.09ms
step:248/2315 train_time:14903ms step_avg:60.09ms
step:249/2315 train_time:14963ms step_avg:60.09ms
step:250/2315 train_time:15024ms step_avg:60.09ms
step:250/2315 val_loss:4.0608 train_time:15085ms step_avg:60.34ms
step:251/2315 train_time:15103ms step_avg:60.17ms
step:252/2315 train_time:15144ms step_avg:60.10ms
step:253/2315 train_time:15207ms step_avg:60.11ms
step:254/2315 train_time:15270ms step_avg:60.12ms
step:255/2315 train_time:15330ms step_avg:60.12ms
step:256/2315 train_time:15391ms step_avg:60.12ms
step:257/2315 train_time:15450ms step_avg:60.12ms
step:258/2315 train_time:15510ms step_avg:60.12ms
step:259/2315 train_time:15568ms step_avg:60.11ms
step:260/2315 train_time:15628ms step_avg:60.11ms
step:261/2315 train_time:15686ms step_avg:60.10ms
step:262/2315 train_time:15746ms step_avg:60.10ms
step:263/2315 train_time:15804ms step_avg:60.09ms
step:264/2315 train_time:15864ms step_avg:60.09ms
step:265/2315 train_time:15924ms step_avg:60.09ms
step:266/2315 train_time:15984ms step_avg:60.09ms
step:267/2315 train_time:16046ms step_avg:60.10ms
step:268/2315 train_time:16108ms step_avg:60.10ms
step:269/2315 train_time:16169ms step_avg:60.11ms
step:270/2315 train_time:16230ms step_avg:60.11ms
step:271/2315 train_time:16291ms step_avg:60.11ms
step:272/2315 train_time:16352ms step_avg:60.12ms
step:273/2315 train_time:16411ms step_avg:60.11ms
step:274/2315 train_time:16471ms step_avg:60.11ms
step:275/2315 train_time:16531ms step_avg:60.11ms
step:276/2315 train_time:16591ms step_avg:60.11ms
step:277/2315 train_time:16650ms step_avg:60.11ms
step:278/2315 train_time:16710ms step_avg:60.11ms
step:279/2315 train_time:16770ms step_avg:60.11ms
step:280/2315 train_time:16830ms step_avg:60.11ms
step:281/2315 train_time:16889ms step_avg:60.10ms
step:282/2315 train_time:16950ms step_avg:60.11ms
step:283/2315 train_time:17010ms step_avg:60.11ms
step:284/2315 train_time:17071ms step_avg:60.11ms
step:285/2315 train_time:17131ms step_avg:60.11ms
step:286/2315 train_time:17192ms step_avg:60.11ms
step:287/2315 train_time:17253ms step_avg:60.11ms
step:288/2315 train_time:17313ms step_avg:60.12ms
step:289/2315 train_time:17373ms step_avg:60.12ms
step:290/2315 train_time:17434ms step_avg:60.12ms
step:291/2315 train_time:17494ms step_avg:60.12ms
step:292/2315 train_time:17554ms step_avg:60.12ms
step:293/2315 train_time:17614ms step_avg:60.11ms
step:294/2315 train_time:17673ms step_avg:60.11ms
step:295/2315 train_time:17733ms step_avg:60.11ms
step:296/2315 train_time:17794ms step_avg:60.11ms
step:297/2315 train_time:17854ms step_avg:60.11ms
step:298/2315 train_time:17914ms step_avg:60.12ms
step:299/2315 train_time:17975ms step_avg:60.12ms
step:300/2315 train_time:18036ms step_avg:60.12ms
step:301/2315 train_time:18096ms step_avg:60.12ms
step:302/2315 train_time:18156ms step_avg:60.12ms
step:303/2315 train_time:18217ms step_avg:60.12ms
step:304/2315 train_time:18277ms step_avg:60.12ms
step:305/2315 train_time:18337ms step_avg:60.12ms
step:306/2315 train_time:18397ms step_avg:60.12ms
step:307/2315 train_time:18457ms step_avg:60.12ms
step:308/2315 train_time:18517ms step_avg:60.12ms
step:309/2315 train_time:18577ms step_avg:60.12ms
step:310/2315 train_time:18636ms step_avg:60.12ms
step:311/2315 train_time:18696ms step_avg:60.12ms
step:312/2315 train_time:18757ms step_avg:60.12ms
step:313/2315 train_time:18816ms step_avg:60.12ms
step:314/2315 train_time:18877ms step_avg:60.12ms
step:315/2315 train_time:18936ms step_avg:60.12ms
step:316/2315 train_time:18997ms step_avg:60.12ms
step:317/2315 train_time:19058ms step_avg:60.12ms
step:318/2315 train_time:19118ms step_avg:60.12ms
step:319/2315 train_time:19178ms step_avg:60.12ms
step:320/2315 train_time:19239ms step_avg:60.12ms
step:321/2315 train_time:19299ms step_avg:60.12ms
step:322/2315 train_time:19360ms step_avg:60.12ms
step:323/2315 train_time:19419ms step_avg:60.12ms
step:324/2315 train_time:19479ms step_avg:60.12ms
step:325/2315 train_time:19538ms step_avg:60.12ms
step:326/2315 train_time:19599ms step_avg:60.12ms
step:327/2315 train_time:19658ms step_avg:60.12ms
step:328/2315 train_time:19718ms step_avg:60.12ms
step:329/2315 train_time:19778ms step_avg:60.11ms
step:330/2315 train_time:19838ms step_avg:60.11ms
step:331/2315 train_time:19898ms step_avg:60.11ms
step:332/2315 train_time:19959ms step_avg:60.12ms
step:333/2315 train_time:20019ms step_avg:60.12ms
step:334/2315 train_time:20079ms step_avg:60.12ms
step:335/2315 train_time:20139ms step_avg:60.12ms
step:336/2315 train_time:20199ms step_avg:60.12ms
step:337/2315 train_time:20259ms step_avg:60.12ms
step:338/2315 train_time:20319ms step_avg:60.12ms
step:339/2315 train_time:20379ms step_avg:60.12ms
step:340/2315 train_time:20440ms step_avg:60.12ms
step:341/2315 train_time:20500ms step_avg:60.12ms
step:342/2315 train_time:20560ms step_avg:60.12ms
step:343/2315 train_time:20620ms step_avg:60.12ms
step:344/2315 train_time:20679ms step_avg:60.11ms
step:345/2315 train_time:20738ms step_avg:60.11ms
step:346/2315 train_time:20799ms step_avg:60.11ms
step:347/2315 train_time:20859ms step_avg:60.11ms
step:348/2315 train_time:20919ms step_avg:60.11ms
step:349/2315 train_time:20979ms step_avg:60.11ms
step:350/2315 train_time:21039ms step_avg:60.11ms
step:351/2315 train_time:21098ms step_avg:60.11ms
step:352/2315 train_time:21158ms step_avg:60.11ms
step:353/2315 train_time:21219ms step_avg:60.11ms
step:354/2315 train_time:21279ms step_avg:60.11ms
step:355/2315 train_time:21339ms step_avg:60.11ms
step:356/2315 train_time:21399ms step_avg:60.11ms
step:357/2315 train_time:21458ms step_avg:60.11ms
step:358/2315 train_time:21518ms step_avg:60.11ms
step:359/2315 train_time:21578ms step_avg:60.10ms
step:360/2315 train_time:21638ms step_avg:60.11ms
step:361/2315 train_time:21698ms step_avg:60.11ms
step:362/2315 train_time:21759ms step_avg:60.11ms
step:363/2315 train_time:21818ms step_avg:60.11ms
step:364/2315 train_time:21879ms step_avg:60.11ms
step:365/2315 train_time:21938ms step_avg:60.10ms
step:366/2315 train_time:21998ms step_avg:60.10ms
step:367/2315 train_time:22058ms step_avg:60.10ms
step:368/2315 train_time:22118ms step_avg:60.10ms
step:369/2315 train_time:22178ms step_avg:60.10ms
step:370/2315 train_time:22237ms step_avg:60.10ms
step:371/2315 train_time:22297ms step_avg:60.10ms
step:372/2315 train_time:22358ms step_avg:60.10ms
step:373/2315 train_time:22417ms step_avg:60.10ms
step:374/2315 train_time:22478ms step_avg:60.10ms
step:375/2315 train_time:22537ms step_avg:60.10ms
step:376/2315 train_time:22597ms step_avg:60.10ms
step:377/2315 train_time:22657ms step_avg:60.10ms
step:378/2315 train_time:22717ms step_avg:60.10ms
step:379/2315 train_time:22777ms step_avg:60.10ms
step:380/2315 train_time:22837ms step_avg:60.10ms
step:381/2315 train_time:22897ms step_avg:60.10ms
step:382/2315 train_time:22957ms step_avg:60.10ms
step:383/2315 train_time:23017ms step_avg:60.10ms
step:384/2315 train_time:23077ms step_avg:60.10ms
step:385/2315 train_time:23137ms step_avg:60.10ms
step:386/2315 train_time:23197ms step_avg:60.10ms
step:387/2315 train_time:23256ms step_avg:60.09ms
step:388/2315 train_time:23316ms step_avg:60.09ms
step:389/2315 train_time:23376ms step_avg:60.09ms
step:390/2315 train_time:23436ms step_avg:60.09ms
step:391/2315 train_time:23496ms step_avg:60.09ms
step:392/2315 train_time:23557ms step_avg:60.09ms
step:393/2315 train_time:23616ms step_avg:60.09ms
step:394/2315 train_time:23676ms step_avg:60.09ms
step:395/2315 train_time:23736ms step_avg:60.09ms
step:396/2315 train_time:23797ms step_avg:60.09ms
step:397/2315 train_time:23857ms step_avg:60.09ms
step:398/2315 train_time:23917ms step_avg:60.09ms
step:399/2315 train_time:23977ms step_avg:60.09ms
step:400/2315 train_time:24037ms step_avg:60.09ms
step:401/2315 train_time:24097ms step_avg:60.09ms
step:402/2315 train_time:24157ms step_avg:60.09ms
step:403/2315 train_time:24217ms step_avg:60.09ms
step:404/2315 train_time:24277ms step_avg:60.09ms
step:405/2315 train_time:24337ms step_avg:60.09ms
step:406/2315 train_time:24397ms step_avg:60.09ms
step:407/2315 train_time:24457ms step_avg:60.09ms
step:408/2315 train_time:24517ms step_avg:60.09ms
step:409/2315 train_time:24577ms step_avg:60.09ms
step:410/2315 train_time:24638ms step_avg:60.09ms
step:411/2315 train_time:24698ms step_avg:60.09ms
step:412/2315 train_time:24758ms step_avg:60.09ms
step:413/2315 train_time:24818ms step_avg:60.09ms
step:414/2315 train_time:24878ms step_avg:60.09ms
step:415/2315 train_time:24937ms step_avg:60.09ms
step:416/2315 train_time:24998ms step_avg:60.09ms
step:417/2315 train_time:25058ms step_avg:60.09ms
step:418/2315 train_time:25118ms step_avg:60.09ms
step:419/2315 train_time:25177ms step_avg:60.09ms
step:420/2315 train_time:25237ms step_avg:60.09ms
step:421/2315 train_time:25297ms step_avg:60.09ms
step:422/2315 train_time:25357ms step_avg:60.09ms
step:423/2315 train_time:25416ms step_avg:60.09ms
step:424/2315 train_time:25476ms step_avg:60.09ms
step:425/2315 train_time:25536ms step_avg:60.08ms
step:426/2315 train_time:25596ms step_avg:60.08ms
step:427/2315 train_time:25655ms step_avg:60.08ms
step:428/2315 train_time:25716ms step_avg:60.08ms
step:429/2315 train_time:25776ms step_avg:60.08ms
step:430/2315 train_time:25837ms step_avg:60.09ms
step:431/2315 train_time:25897ms step_avg:60.08ms
step:432/2315 train_time:25957ms step_avg:60.09ms
step:433/2315 train_time:26017ms step_avg:60.09ms
step:434/2315 train_time:26077ms step_avg:60.09ms
step:435/2315 train_time:26137ms step_avg:60.09ms
step:436/2315 train_time:26198ms step_avg:60.09ms
step:437/2315 train_time:26258ms step_avg:60.09ms
step:438/2315 train_time:26318ms step_avg:60.09ms
step:439/2315 train_time:26378ms step_avg:60.09ms
step:440/2315 train_time:26438ms step_avg:60.09ms
step:441/2315 train_time:26498ms step_avg:60.09ms
step:442/2315 train_time:26558ms step_avg:60.09ms
step:443/2315 train_time:26618ms step_avg:60.08ms
step:444/2315 train_time:26677ms step_avg:60.08ms
step:445/2315 train_time:26737ms step_avg:60.08ms
step:446/2315 train_time:26797ms step_avg:60.08ms
step:447/2315 train_time:26857ms step_avg:60.08ms
step:448/2315 train_time:26917ms step_avg:60.08ms
step:449/2315 train_time:26977ms step_avg:60.08ms
step:450/2315 train_time:27037ms step_avg:60.08ms
step:451/2315 train_time:27097ms step_avg:60.08ms
step:452/2315 train_time:27157ms step_avg:60.08ms
step:453/2315 train_time:27217ms step_avg:60.08ms
step:454/2315 train_time:27277ms step_avg:60.08ms
step:455/2315 train_time:27336ms step_avg:60.08ms
step:456/2315 train_time:27397ms step_avg:60.08ms
step:457/2315 train_time:27457ms step_avg:60.08ms
step:458/2315 train_time:27517ms step_avg:60.08ms
step:459/2315 train_time:27577ms step_avg:60.08ms
step:460/2315 train_time:27637ms step_avg:60.08ms
step:461/2315 train_time:27696ms step_avg:60.08ms
step:462/2315 train_time:27756ms step_avg:60.08ms
step:463/2315 train_time:27816ms step_avg:60.08ms
step:464/2315 train_time:27876ms step_avg:60.08ms
step:465/2315 train_time:27936ms step_avg:60.08ms
step:466/2315 train_time:27997ms step_avg:60.08ms
step:467/2315 train_time:28056ms step_avg:60.08ms
step:468/2315 train_time:28117ms step_avg:60.08ms
step:469/2315 train_time:28177ms step_avg:60.08ms
step:470/2315 train_time:28237ms step_avg:60.08ms
step:471/2315 train_time:28297ms step_avg:60.08ms
step:472/2315 train_time:28357ms step_avg:60.08ms
step:473/2315 train_time:28416ms step_avg:60.08ms
step:474/2315 train_time:28477ms step_avg:60.08ms
step:475/2315 train_time:28536ms step_avg:60.08ms
step:476/2315 train_time:28597ms step_avg:60.08ms
step:477/2315 train_time:28657ms step_avg:60.08ms
step:478/2315 train_time:28717ms step_avg:60.08ms
step:479/2315 train_time:28777ms step_avg:60.08ms
step:480/2315 train_time:28837ms step_avg:60.08ms
step:481/2315 train_time:28897ms step_avg:60.08ms
step:482/2315 train_time:28957ms step_avg:60.08ms
step:483/2315 train_time:29017ms step_avg:60.08ms
step:484/2315 train_time:29077ms step_avg:60.08ms
step:485/2315 train_time:29137ms step_avg:60.08ms
step:486/2315 train_time:29198ms step_avg:60.08ms
step:487/2315 train_time:29258ms step_avg:60.08ms
step:488/2315 train_time:29318ms step_avg:60.08ms
step:489/2315 train_time:29378ms step_avg:60.08ms
step:490/2315 train_time:29439ms step_avg:60.08ms
step:491/2315 train_time:29498ms step_avg:60.08ms
step:492/2315 train_time:29559ms step_avg:60.08ms
step:493/2315 train_time:29618ms step_avg:60.08ms
step:494/2315 train_time:29679ms step_avg:60.08ms
step:495/2315 train_time:29738ms step_avg:60.08ms
step:496/2315 train_time:29798ms step_avg:60.08ms
step:497/2315 train_time:29858ms step_avg:60.08ms
step:498/2315 train_time:29918ms step_avg:60.08ms
step:499/2315 train_time:29977ms step_avg:60.07ms
step:500/2315 train_time:30037ms step_avg:60.07ms
step:500/2315 val_loss:3.8087 train_time:30099ms step_avg:60.20ms
step:501/2315 train_time:30118ms step_avg:60.12ms
step:502/2315 train_time:30160ms step_avg:60.08ms
step:503/2315 train_time:30223ms step_avg:60.09ms
step:504/2315 train_time:30288ms step_avg:60.09ms
step:505/2315 train_time:30347ms step_avg:60.09ms
step:506/2315 train_time:30408ms step_avg:60.09ms
step:507/2315 train_time:30467ms step_avg:60.09ms
step:508/2315 train_time:30527ms step_avg:60.09ms
step:509/2315 train_time:30585ms step_avg:60.09ms
step:510/2315 train_time:30645ms step_avg:60.09ms
step:511/2315 train_time:30703ms step_avg:60.08ms
step:512/2315 train_time:30763ms step_avg:60.08ms
step:513/2315 train_time:30822ms step_avg:60.08ms
step:514/2315 train_time:30882ms step_avg:60.08ms
step:515/2315 train_time:30941ms step_avg:60.08ms
step:516/2315 train_time:31002ms step_avg:60.08ms
step:517/2315 train_time:31062ms step_avg:60.08ms
step:518/2315 train_time:31124ms step_avg:60.09ms
step:519/2315 train_time:31186ms step_avg:60.09ms
step:520/2315 train_time:31248ms step_avg:60.09ms
step:521/2315 train_time:31310ms step_avg:60.10ms
step:522/2315 train_time:31370ms step_avg:60.10ms
step:523/2315 train_time:31429ms step_avg:60.09ms
step:524/2315 train_time:31489ms step_avg:60.09ms
step:525/2315 train_time:31548ms step_avg:60.09ms
step:526/2315 train_time:31608ms step_avg:60.09ms
step:527/2315 train_time:31667ms step_avg:60.09ms
step:528/2315 train_time:31727ms step_avg:60.09ms
step:529/2315 train_time:31787ms step_avg:60.09ms
step:530/2315 train_time:31846ms step_avg:60.09ms
step:531/2315 train_time:31906ms step_avg:60.09ms
step:532/2315 train_time:31967ms step_avg:60.09ms
step:533/2315 train_time:32027ms step_avg:60.09ms
step:534/2315 train_time:32087ms step_avg:60.09ms
step:535/2315 train_time:32148ms step_avg:60.09ms
step:536/2315 train_time:32210ms step_avg:60.09ms
step:537/2315 train_time:32271ms step_avg:60.10ms
step:538/2315 train_time:32332ms step_avg:60.10ms
step:539/2315 train_time:32391ms step_avg:60.09ms
step:540/2315 train_time:32451ms step_avg:60.09ms
step:541/2315 train_time:32510ms step_avg:60.09ms
step:542/2315 train_time:32569ms step_avg:60.09ms
step:543/2315 train_time:32629ms step_avg:60.09ms
step:544/2315 train_time:32689ms step_avg:60.09ms
step:545/2315 train_time:32748ms step_avg:60.09ms
step:546/2315 train_time:32808ms step_avg:60.09ms
step:547/2315 train_time:32867ms step_avg:60.09ms
step:548/2315 train_time:32927ms step_avg:60.09ms
step:549/2315 train_time:32987ms step_avg:60.08ms
step:550/2315 train_time:33047ms step_avg:60.09ms
step:551/2315 train_time:33107ms step_avg:60.09ms
step:552/2315 train_time:33169ms step_avg:60.09ms
step:553/2315 train_time:33229ms step_avg:60.09ms
step:554/2315 train_time:33289ms step_avg:60.09ms
step:555/2315 train_time:33350ms step_avg:60.09ms
step:556/2315 train_time:33410ms step_avg:60.09ms
step:557/2315 train_time:33469ms step_avg:60.09ms
step:558/2315 train_time:33529ms step_avg:60.09ms
step:559/2315 train_time:33588ms step_avg:60.09ms
step:560/2315 train_time:33648ms step_avg:60.09ms
step:561/2315 train_time:33708ms step_avg:60.08ms
step:562/2315 train_time:33767ms step_avg:60.08ms
step:563/2315 train_time:33826ms step_avg:60.08ms
step:564/2315 train_time:33886ms step_avg:60.08ms
step:565/2315 train_time:33946ms step_avg:60.08ms
step:566/2315 train_time:34007ms step_avg:60.08ms
step:567/2315 train_time:34067ms step_avg:60.08ms
step:568/2315 train_time:34129ms step_avg:60.09ms
step:569/2315 train_time:34188ms step_avg:60.09ms
step:570/2315 train_time:34249ms step_avg:60.09ms
step:571/2315 train_time:34310ms step_avg:60.09ms
step:572/2315 train_time:34370ms step_avg:60.09ms
step:573/2315 train_time:34430ms step_avg:60.09ms
step:574/2315 train_time:34490ms step_avg:60.09ms
step:575/2315 train_time:34549ms step_avg:60.09ms
step:576/2315 train_time:34610ms step_avg:60.09ms
step:577/2315 train_time:34669ms step_avg:60.08ms
step:578/2315 train_time:34729ms step_avg:60.08ms
step:579/2315 train_time:34788ms step_avg:60.08ms
step:580/2315 train_time:34848ms step_avg:60.08ms
step:581/2315 train_time:34907ms step_avg:60.08ms
step:582/2315 train_time:34967ms step_avg:60.08ms
step:583/2315 train_time:35027ms step_avg:60.08ms
step:584/2315 train_time:35088ms step_avg:60.08ms
step:585/2315 train_time:35148ms step_avg:60.08ms
step:586/2315 train_time:35210ms step_avg:60.08ms
step:587/2315 train_time:35270ms step_avg:60.09ms
step:588/2315 train_time:35331ms step_avg:60.09ms
step:589/2315 train_time:35391ms step_avg:60.09ms
step:590/2315 train_time:35450ms step_avg:60.09ms
step:591/2315 train_time:35510ms step_avg:60.08ms
step:592/2315 train_time:35570ms step_avg:60.08ms
step:593/2315 train_time:35629ms step_avg:60.08ms
step:594/2315 train_time:35689ms step_avg:60.08ms
step:595/2315 train_time:35749ms step_avg:60.08ms
step:596/2315 train_time:35809ms step_avg:60.08ms
step:597/2315 train_time:35868ms step_avg:60.08ms
step:598/2315 train_time:35928ms step_avg:60.08ms
step:599/2315 train_time:35987ms step_avg:60.08ms
step:600/2315 train_time:36048ms step_avg:60.08ms
step:601/2315 train_time:36109ms step_avg:60.08ms
step:602/2315 train_time:36169ms step_avg:60.08ms
step:603/2315 train_time:36228ms step_avg:60.08ms
step:604/2315 train_time:36289ms step_avg:60.08ms
step:605/2315 train_time:36349ms step_avg:60.08ms
step:606/2315 train_time:36409ms step_avg:60.08ms
step:607/2315 train_time:36469ms step_avg:60.08ms
step:608/2315 train_time:36529ms step_avg:60.08ms
step:609/2315 train_time:36588ms step_avg:60.08ms
step:610/2315 train_time:36650ms step_avg:60.08ms
step:611/2315 train_time:36710ms step_avg:60.08ms
step:612/2315 train_time:36769ms step_avg:60.08ms
step:613/2315 train_time:36829ms step_avg:60.08ms
step:614/2315 train_time:36889ms step_avg:60.08ms
step:615/2315 train_time:36949ms step_avg:60.08ms
step:616/2315 train_time:37010ms step_avg:60.08ms
step:617/2315 train_time:37070ms step_avg:60.08ms
step:618/2315 train_time:37131ms step_avg:60.08ms
step:619/2315 train_time:37190ms step_avg:60.08ms
step:620/2315 train_time:37251ms step_avg:60.08ms
step:621/2315 train_time:37310ms step_avg:60.08ms
step:622/2315 train_time:37370ms step_avg:60.08ms
step:623/2315 train_time:37430ms step_avg:60.08ms
step:624/2315 train_time:37490ms step_avg:60.08ms
step:625/2315 train_time:37550ms step_avg:60.08ms
step:626/2315 train_time:37610ms step_avg:60.08ms
step:627/2315 train_time:37669ms step_avg:60.08ms
step:628/2315 train_time:37729ms step_avg:60.08ms
step:629/2315 train_time:37788ms step_avg:60.08ms
step:630/2315 train_time:37849ms step_avg:60.08ms
step:631/2315 train_time:37908ms step_avg:60.08ms
step:632/2315 train_time:37969ms step_avg:60.08ms
step:633/2315 train_time:38028ms step_avg:60.08ms
step:634/2315 train_time:38089ms step_avg:60.08ms
step:635/2315 train_time:38149ms step_avg:60.08ms
step:636/2315 train_time:38210ms step_avg:60.08ms
step:637/2315 train_time:38270ms step_avg:60.08ms
step:638/2315 train_time:38330ms step_avg:60.08ms
step:639/2315 train_time:38390ms step_avg:60.08ms
step:640/2315 train_time:38450ms step_avg:60.08ms
step:641/2315 train_time:38510ms step_avg:60.08ms
step:642/2315 train_time:38570ms step_avg:60.08ms
step:643/2315 train_time:38629ms step_avg:60.08ms
step:644/2315 train_time:38690ms step_avg:60.08ms
step:645/2315 train_time:38749ms step_avg:60.08ms
step:646/2315 train_time:38809ms step_avg:60.08ms
step:647/2315 train_time:38868ms step_avg:60.07ms
step:648/2315 train_time:38928ms step_avg:60.07ms
step:649/2315 train_time:38987ms step_avg:60.07ms
step:650/2315 train_time:39048ms step_avg:60.07ms
step:651/2315 train_time:39109ms step_avg:60.08ms
step:652/2315 train_time:39170ms step_avg:60.08ms
step:653/2315 train_time:39229ms step_avg:60.07ms
step:654/2315 train_time:39289ms step_avg:60.07ms
step:655/2315 train_time:39349ms step_avg:60.07ms
step:656/2315 train_time:39409ms step_avg:60.07ms
step:657/2315 train_time:39469ms step_avg:60.07ms
step:658/2315 train_time:39529ms step_avg:60.07ms
step:659/2315 train_time:39589ms step_avg:60.07ms
step:660/2315 train_time:39649ms step_avg:60.07ms
step:661/2315 train_time:39708ms step_avg:60.07ms
step:662/2315 train_time:39769ms step_avg:60.07ms
step:663/2315 train_time:39828ms step_avg:60.07ms
step:664/2315 train_time:39889ms step_avg:60.07ms
step:665/2315 train_time:39948ms step_avg:60.07ms
step:666/2315 train_time:40008ms step_avg:60.07ms
step:667/2315 train_time:40068ms step_avg:60.07ms
step:668/2315 train_time:40129ms step_avg:60.07ms
step:669/2315 train_time:40189ms step_avg:60.07ms
step:670/2315 train_time:40250ms step_avg:60.07ms
step:671/2315 train_time:40309ms step_avg:60.07ms
step:672/2315 train_time:40370ms step_avg:60.07ms
step:673/2315 train_time:40429ms step_avg:60.07ms
step:674/2315 train_time:40490ms step_avg:60.07ms
step:675/2315 train_time:40549ms step_avg:60.07ms
step:676/2315 train_time:40609ms step_avg:60.07ms
step:677/2315 train_time:40669ms step_avg:60.07ms
step:678/2315 train_time:40729ms step_avg:60.07ms
step:679/2315 train_time:40789ms step_avg:60.07ms
step:680/2315 train_time:40850ms step_avg:60.07ms
step:681/2315 train_time:40909ms step_avg:60.07ms
step:682/2315 train_time:40969ms step_avg:60.07ms
step:683/2315 train_time:41029ms step_avg:60.07ms
step:684/2315 train_time:41089ms step_avg:60.07ms
step:685/2315 train_time:41149ms step_avg:60.07ms
step:686/2315 train_time:41210ms step_avg:60.07ms
step:687/2315 train_time:41270ms step_avg:60.07ms
step:688/2315 train_time:41330ms step_avg:60.07ms
step:689/2315 train_time:41389ms step_avg:60.07ms
step:690/2315 train_time:41449ms step_avg:60.07ms
step:691/2315 train_time:41509ms step_avg:60.07ms
step:692/2315 train_time:41569ms step_avg:60.07ms
step:693/2315 train_time:41629ms step_avg:60.07ms
step:694/2315 train_time:41689ms step_avg:60.07ms
step:695/2315 train_time:41749ms step_avg:60.07ms
step:696/2315 train_time:41809ms step_avg:60.07ms
step:697/2315 train_time:41869ms step_avg:60.07ms
step:698/2315 train_time:41929ms step_avg:60.07ms
step:699/2315 train_time:41989ms step_avg:60.07ms
step:700/2315 train_time:42049ms step_avg:60.07ms
step:701/2315 train_time:42109ms step_avg:60.07ms
step:702/2315 train_time:42169ms step_avg:60.07ms
step:703/2315 train_time:42228ms step_avg:60.07ms
step:704/2315 train_time:42288ms step_avg:60.07ms
step:705/2315 train_time:42348ms step_avg:60.07ms
step:706/2315 train_time:42410ms step_avg:60.07ms
step:707/2315 train_time:42470ms step_avg:60.07ms
step:708/2315 train_time:42530ms step_avg:60.07ms
step:709/2315 train_time:42589ms step_avg:60.07ms
step:710/2315 train_time:42650ms step_avg:60.07ms
step:711/2315 train_time:42710ms step_avg:60.07ms
step:712/2315 train_time:42769ms step_avg:60.07ms
step:713/2315 train_time:42829ms step_avg:60.07ms
step:714/2315 train_time:42889ms step_avg:60.07ms
step:715/2315 train_time:42948ms step_avg:60.07ms
step:716/2315 train_time:43009ms step_avg:60.07ms
step:717/2315 train_time:43069ms step_avg:60.07ms
step:718/2315 train_time:43129ms step_avg:60.07ms
step:719/2315 train_time:43188ms step_avg:60.07ms
step:720/2315 train_time:43249ms step_avg:60.07ms
step:721/2315 train_time:43309ms step_avg:60.07ms
step:722/2315 train_time:43369ms step_avg:60.07ms
step:723/2315 train_time:43429ms step_avg:60.07ms
step:724/2315 train_time:43489ms step_avg:60.07ms
step:725/2315 train_time:43548ms step_avg:60.07ms
step:726/2315 train_time:43609ms step_avg:60.07ms
step:727/2315 train_time:43669ms step_avg:60.07ms
step:728/2315 train_time:43729ms step_avg:60.07ms
step:729/2315 train_time:43788ms step_avg:60.07ms
step:730/2315 train_time:43849ms step_avg:60.07ms
step:731/2315 train_time:43909ms step_avg:60.07ms
step:732/2315 train_time:43969ms step_avg:60.07ms
step:733/2315 train_time:44028ms step_avg:60.07ms
step:734/2315 train_time:44088ms step_avg:60.07ms
step:735/2315 train_time:44148ms step_avg:60.06ms
step:736/2315 train_time:44208ms step_avg:60.07ms
step:737/2315 train_time:44268ms step_avg:60.06ms
step:738/2315 train_time:44328ms step_avg:60.06ms
step:739/2315 train_time:44388ms step_avg:60.06ms
step:740/2315 train_time:44448ms step_avg:60.06ms
step:741/2315 train_time:44508ms step_avg:60.06ms
step:742/2315 train_time:44569ms step_avg:60.07ms
step:743/2315 train_time:44628ms step_avg:60.07ms
step:744/2315 train_time:44689ms step_avg:60.07ms
step:745/2315 train_time:44748ms step_avg:60.06ms
step:746/2315 train_time:44808ms step_avg:60.06ms
step:747/2315 train_time:44868ms step_avg:60.06ms
step:748/2315 train_time:44929ms step_avg:60.06ms
step:749/2315 train_time:44988ms step_avg:60.06ms
step:750/2315 train_time:45049ms step_avg:60.06ms
step:750/2315 val_loss:3.6843 train_time:45110ms step_avg:60.15ms
step:751/2315 train_time:45128ms step_avg:60.09ms
step:752/2315 train_time:45172ms step_avg:60.07ms
step:753/2315 train_time:45234ms step_avg:60.07ms
step:754/2315 train_time:45297ms step_avg:60.08ms
step:755/2315 train_time:45357ms step_avg:60.08ms
step:756/2315 train_time:45417ms step_avg:60.08ms
step:757/2315 train_time:45476ms step_avg:60.07ms
step:758/2315 train_time:45537ms step_avg:60.07ms
step:759/2315 train_time:45596ms step_avg:60.07ms
step:760/2315 train_time:45656ms step_avg:60.07ms
step:761/2315 train_time:45716ms step_avg:60.07ms
step:762/2315 train_time:45777ms step_avg:60.07ms
step:763/2315 train_time:45837ms step_avg:60.07ms
step:764/2315 train_time:45898ms step_avg:60.08ms
step:765/2315 train_time:45959ms step_avg:60.08ms
step:766/2315 train_time:46019ms step_avg:60.08ms
step:767/2315 train_time:46081ms step_avg:60.08ms
step:768/2315 train_time:46144ms step_avg:60.08ms
step:769/2315 train_time:46206ms step_avg:60.09ms
step:770/2315 train_time:46268ms step_avg:60.09ms
step:771/2315 train_time:46328ms step_avg:60.09ms
step:772/2315 train_time:46389ms step_avg:60.09ms
step:773/2315 train_time:46450ms step_avg:60.09ms
step:774/2315 train_time:46511ms step_avg:60.09ms
step:775/2315 train_time:46571ms step_avg:60.09ms
step:776/2315 train_time:46632ms step_avg:60.09ms
step:777/2315 train_time:46692ms step_avg:60.09ms
step:778/2315 train_time:46753ms step_avg:60.09ms
step:779/2315 train_time:46813ms step_avg:60.09ms
step:780/2315 train_time:46875ms step_avg:60.10ms
step:781/2315 train_time:46935ms step_avg:60.10ms
step:782/2315 train_time:46997ms step_avg:60.10ms
step:783/2315 train_time:47059ms step_avg:60.10ms
step:784/2315 train_time:47121ms step_avg:60.10ms
step:785/2315 train_time:47182ms step_avg:60.10ms
step:786/2315 train_time:47243ms step_avg:60.11ms
step:787/2315 train_time:47303ms step_avg:60.11ms
step:788/2315 train_time:47365ms step_avg:60.11ms
step:789/2315 train_time:47424ms step_avg:60.11ms
step:790/2315 train_time:47485ms step_avg:60.11ms
step:791/2315 train_time:47545ms step_avg:60.11ms
step:792/2315 train_time:47606ms step_avg:60.11ms
step:793/2315 train_time:47666ms step_avg:60.11ms
step:794/2315 train_time:47727ms step_avg:60.11ms
step:795/2315 train_time:47787ms step_avg:60.11ms
step:796/2315 train_time:47847ms step_avg:60.11ms
step:797/2315 train_time:47908ms step_avg:60.11ms
step:798/2315 train_time:47969ms step_avg:60.11ms
step:799/2315 train_time:48030ms step_avg:60.11ms
step:800/2315 train_time:48092ms step_avg:60.11ms
step:801/2315 train_time:48153ms step_avg:60.12ms
step:802/2315 train_time:48215ms step_avg:60.12ms
step:803/2315 train_time:48277ms step_avg:60.12ms
step:804/2315 train_time:48339ms step_avg:60.12ms
step:805/2315 train_time:48400ms step_avg:60.12ms
step:806/2315 train_time:48461ms step_avg:60.13ms
step:807/2315 train_time:48521ms step_avg:60.12ms
step:808/2315 train_time:48581ms step_avg:60.12ms
step:809/2315 train_time:48641ms step_avg:60.12ms
step:810/2315 train_time:48701ms step_avg:60.13ms
step:811/2315 train_time:48761ms step_avg:60.13ms
step:812/2315 train_time:48822ms step_avg:60.13ms
step:813/2315 train_time:48882ms step_avg:60.13ms
step:814/2315 train_time:48943ms step_avg:60.13ms
step:815/2315 train_time:49004ms step_avg:60.13ms
step:816/2315 train_time:49064ms step_avg:60.13ms
step:817/2315 train_time:49125ms step_avg:60.13ms
step:818/2315 train_time:49185ms step_avg:60.13ms
step:819/2315 train_time:49246ms step_avg:60.13ms
step:820/2315 train_time:49307ms step_avg:60.13ms
step:821/2315 train_time:49368ms step_avg:60.13ms
step:822/2315 train_time:49429ms step_avg:60.13ms
step:823/2315 train_time:49490ms step_avg:60.13ms
step:824/2315 train_time:49551ms step_avg:60.13ms
step:825/2315 train_time:49611ms step_avg:60.13ms
step:826/2315 train_time:49672ms step_avg:60.14ms
step:827/2315 train_time:49733ms step_avg:60.14ms
step:828/2315 train_time:49794ms step_avg:60.14ms
step:829/2315 train_time:49855ms step_avg:60.14ms
step:830/2315 train_time:49917ms step_avg:60.14ms
step:831/2315 train_time:49977ms step_avg:60.14ms
step:832/2315 train_time:50038ms step_avg:60.14ms
step:833/2315 train_time:50099ms step_avg:60.14ms
step:834/2315 train_time:50160ms step_avg:60.14ms
step:835/2315 train_time:50220ms step_avg:60.14ms
step:836/2315 train_time:50281ms step_avg:60.14ms
step:837/2315 train_time:50341ms step_avg:60.14ms
step:838/2315 train_time:50403ms step_avg:60.15ms
step:839/2315 train_time:50463ms step_avg:60.15ms
step:840/2315 train_time:50524ms step_avg:60.15ms
step:841/2315 train_time:50584ms step_avg:60.15ms
step:842/2315 train_time:50645ms step_avg:60.15ms
step:843/2315 train_time:50705ms step_avg:60.15ms
step:844/2315 train_time:50766ms step_avg:60.15ms
step:845/2315 train_time:50826ms step_avg:60.15ms
step:846/2315 train_time:50888ms step_avg:60.15ms
step:847/2315 train_time:50948ms step_avg:60.15ms
step:848/2315 train_time:51009ms step_avg:60.15ms
step:849/2315 train_time:51070ms step_avg:60.15ms
step:850/2315 train_time:51131ms step_avg:60.15ms
step:851/2315 train_time:51192ms step_avg:60.15ms
step:852/2315 train_time:51253ms step_avg:60.16ms
step:853/2315 train_time:51315ms step_avg:60.16ms
step:854/2315 train_time:51376ms step_avg:60.16ms
step:855/2315 train_time:51437ms step_avg:60.16ms
step:856/2315 train_time:51499ms step_avg:60.16ms
step:857/2315 train_time:51559ms step_avg:60.16ms
step:858/2315 train_time:51620ms step_avg:60.16ms
step:859/2315 train_time:51681ms step_avg:60.16ms
step:860/2315 train_time:51742ms step_avg:60.16ms
step:861/2315 train_time:51802ms step_avg:60.16ms
step:862/2315 train_time:51863ms step_avg:60.17ms
step:863/2315 train_time:51923ms step_avg:60.17ms
step:864/2315 train_time:51984ms step_avg:60.17ms
step:865/2315 train_time:52044ms step_avg:60.17ms
step:866/2315 train_time:52104ms step_avg:60.17ms
step:867/2315 train_time:52165ms step_avg:60.17ms
step:868/2315 train_time:52226ms step_avg:60.17ms
step:869/2315 train_time:52286ms step_avg:60.17ms
step:870/2315 train_time:52348ms step_avg:60.17ms
step:871/2315 train_time:52408ms step_avg:60.17ms
step:872/2315 train_time:52469ms step_avg:60.17ms
step:873/2315 train_time:52530ms step_avg:60.17ms
step:874/2315 train_time:52591ms step_avg:60.17ms
step:875/2315 train_time:52652ms step_avg:60.17ms
step:876/2315 train_time:52713ms step_avg:60.17ms
step:877/2315 train_time:52774ms step_avg:60.18ms
step:878/2315 train_time:52835ms step_avg:60.18ms
step:879/2315 train_time:52896ms step_avg:60.18ms
step:880/2315 train_time:52958ms step_avg:60.18ms
step:881/2315 train_time:53018ms step_avg:60.18ms
step:882/2315 train_time:53080ms step_avg:60.18ms
step:883/2315 train_time:53140ms step_avg:60.18ms
step:884/2315 train_time:53201ms step_avg:60.18ms
step:885/2315 train_time:53261ms step_avg:60.18ms
step:886/2315 train_time:53322ms step_avg:60.18ms
step:887/2315 train_time:53382ms step_avg:60.18ms
step:888/2315 train_time:53443ms step_avg:60.18ms
step:889/2315 train_time:53504ms step_avg:60.18ms
step:890/2315 train_time:53565ms step_avg:60.19ms
step:891/2315 train_time:53625ms step_avg:60.18ms
step:892/2315 train_time:53685ms step_avg:60.19ms
step:893/2315 train_time:53746ms step_avg:60.19ms
step:894/2315 train_time:53807ms step_avg:60.19ms
step:895/2315 train_time:53868ms step_avg:60.19ms
step:896/2315 train_time:53929ms step_avg:60.19ms
step:897/2315 train_time:53990ms step_avg:60.19ms
step:898/2315 train_time:54051ms step_avg:60.19ms
step:899/2315 train_time:54112ms step_avg:60.19ms
step:900/2315 train_time:54173ms step_avg:60.19ms
step:901/2315 train_time:54234ms step_avg:60.19ms
step:902/2315 train_time:54296ms step_avg:60.19ms
step:903/2315 train_time:54356ms step_avg:60.20ms
step:904/2315 train_time:54418ms step_avg:60.20ms
step:905/2315 train_time:54479ms step_avg:60.20ms
step:906/2315 train_time:54539ms step_avg:60.20ms
step:907/2315 train_time:54600ms step_avg:60.20ms
step:908/2315 train_time:54661ms step_avg:60.20ms
step:909/2315 train_time:54721ms step_avg:60.20ms
step:910/2315 train_time:54782ms step_avg:60.20ms
step:911/2315 train_time:54842ms step_avg:60.20ms
step:912/2315 train_time:54903ms step_avg:60.20ms
step:913/2315 train_time:54963ms step_avg:60.20ms
step:914/2315 train_time:55023ms step_avg:60.20ms
step:915/2315 train_time:55084ms step_avg:60.20ms
step:916/2315 train_time:55144ms step_avg:60.20ms
step:917/2315 train_time:55205ms step_avg:60.20ms
step:918/2315 train_time:55267ms step_avg:60.20ms
step:919/2315 train_time:55328ms step_avg:60.20ms
step:920/2315 train_time:55390ms step_avg:60.21ms
step:921/2315 train_time:55451ms step_avg:60.21ms
step:922/2315 train_time:55512ms step_avg:60.21ms
step:923/2315 train_time:55573ms step_avg:60.21ms
step:924/2315 train_time:55634ms step_avg:60.21ms
step:925/2315 train_time:55695ms step_avg:60.21ms
step:926/2315 train_time:55757ms step_avg:60.21ms
step:927/2315 train_time:55819ms step_avg:60.21ms
step:928/2315 train_time:55880ms step_avg:60.22ms
step:929/2315 train_time:55940ms step_avg:60.21ms
step:930/2315 train_time:56000ms step_avg:60.22ms
step:931/2315 train_time:56060ms step_avg:60.22ms
step:932/2315 train_time:56121ms step_avg:60.22ms
step:933/2315 train_time:56181ms step_avg:60.22ms
step:934/2315 train_time:56242ms step_avg:60.22ms
step:935/2315 train_time:56302ms step_avg:60.22ms
step:936/2315 train_time:56363ms step_avg:60.22ms
step:937/2315 train_time:56423ms step_avg:60.22ms
step:938/2315 train_time:56484ms step_avg:60.22ms
step:939/2315 train_time:56544ms step_avg:60.22ms
step:940/2315 train_time:56605ms step_avg:60.22ms
step:941/2315 train_time:56666ms step_avg:60.22ms
step:942/2315 train_time:56727ms step_avg:60.22ms
step:943/2315 train_time:56787ms step_avg:60.22ms
step:944/2315 train_time:56849ms step_avg:60.22ms
step:945/2315 train_time:56910ms step_avg:60.22ms
step:946/2315 train_time:56971ms step_avg:60.22ms
step:947/2315 train_time:57032ms step_avg:60.22ms
step:948/2315 train_time:57092ms step_avg:60.22ms
step:949/2315 train_time:57153ms step_avg:60.22ms
step:950/2315 train_time:57215ms step_avg:60.23ms
step:951/2315 train_time:57275ms step_avg:60.23ms
step:952/2315 train_time:57337ms step_avg:60.23ms
step:953/2315 train_time:57398ms step_avg:60.23ms
step:954/2315 train_time:57460ms step_avg:60.23ms
step:955/2315 train_time:57520ms step_avg:60.23ms
step:956/2315 train_time:57581ms step_avg:60.23ms
step:957/2315 train_time:57641ms step_avg:60.23ms
step:958/2315 train_time:57702ms step_avg:60.23ms
step:959/2315 train_time:57762ms step_avg:60.23ms
step:960/2315 train_time:57822ms step_avg:60.23ms
step:961/2315 train_time:57883ms step_avg:60.23ms
step:962/2315 train_time:57944ms step_avg:60.23ms
step:963/2315 train_time:58004ms step_avg:60.23ms
step:964/2315 train_time:58064ms step_avg:60.23ms
step:965/2315 train_time:58125ms step_avg:60.23ms
step:966/2315 train_time:58186ms step_avg:60.23ms
step:967/2315 train_time:58247ms step_avg:60.23ms
step:968/2315 train_time:58308ms step_avg:60.24ms
step:969/2315 train_time:58369ms step_avg:60.24ms
step:970/2315 train_time:58430ms step_avg:60.24ms
step:971/2315 train_time:58491ms step_avg:60.24ms
step:972/2315 train_time:58553ms step_avg:60.24ms
step:973/2315 train_time:58613ms step_avg:60.24ms
step:974/2315 train_time:58674ms step_avg:60.24ms
step:975/2315 train_time:58735ms step_avg:60.24ms
step:976/2315 train_time:58797ms step_avg:60.24ms
step:977/2315 train_time:58858ms step_avg:60.24ms
step:978/2315 train_time:58919ms step_avg:60.24ms
step:979/2315 train_time:58979ms step_avg:60.24ms
step:980/2315 train_time:59040ms step_avg:60.24ms
step:981/2315 train_time:59101ms step_avg:60.25ms
step:982/2315 train_time:59162ms step_avg:60.25ms
step:983/2315 train_time:59222ms step_avg:60.25ms
step:984/2315 train_time:59283ms step_avg:60.25ms
step:985/2315 train_time:59342ms step_avg:60.25ms
step:986/2315 train_time:59403ms step_avg:60.25ms
step:987/2315 train_time:59463ms step_avg:60.25ms
step:988/2315 train_time:59524ms step_avg:60.25ms
step:989/2315 train_time:59584ms step_avg:60.25ms
step:990/2315 train_time:59646ms step_avg:60.25ms
step:991/2315 train_time:59707ms step_avg:60.25ms
step:992/2315 train_time:59768ms step_avg:60.25ms
step:993/2315 train_time:59828ms step_avg:60.25ms
step:994/2315 train_time:59889ms step_avg:60.25ms
step:995/2315 train_time:59949ms step_avg:60.25ms
step:996/2315 train_time:60010ms step_avg:60.25ms
step:997/2315 train_time:60071ms step_avg:60.25ms
step:998/2315 train_time:60133ms step_avg:60.25ms
step:999/2315 train_time:60193ms step_avg:60.25ms
step:1000/2315 train_time:60255ms step_avg:60.26ms
step:1000/2315 val_loss:3.5714 train_time:60318ms step_avg:60.32ms
step:1001/2315 train_time:60337ms step_avg:60.28ms
step:1002/2315 train_time:60380ms step_avg:60.26ms
step:1003/2315 train_time:60447ms step_avg:60.27ms
step:1004/2315 train_time:60510ms step_avg:60.27ms
step:1005/2315 train_time:60569ms step_avg:60.27ms
step:1006/2315 train_time:60630ms step_avg:60.27ms
step:1007/2315 train_time:60690ms step_avg:60.27ms
step:1008/2315 train_time:60750ms step_avg:60.27ms
step:1009/2315 train_time:60810ms step_avg:60.27ms
step:1010/2315 train_time:60870ms step_avg:60.27ms
step:1011/2315 train_time:60929ms step_avg:60.27ms
step:1012/2315 train_time:60989ms step_avg:60.27ms
step:1013/2315 train_time:61049ms step_avg:60.27ms
step:1014/2315 train_time:61109ms step_avg:60.27ms
step:1015/2315 train_time:61168ms step_avg:60.26ms
step:1016/2315 train_time:61231ms step_avg:60.27ms
step:1017/2315 train_time:61295ms step_avg:60.27ms
step:1018/2315 train_time:61359ms step_avg:60.27ms
step:1019/2315 train_time:61422ms step_avg:60.28ms
step:1020/2315 train_time:61483ms step_avg:60.28ms
step:1021/2315 train_time:61544ms step_avg:60.28ms
step:1022/2315 train_time:61605ms step_avg:60.28ms
step:1023/2315 train_time:61665ms step_avg:60.28ms
step:1024/2315 train_time:61726ms step_avg:60.28ms
step:1025/2315 train_time:61786ms step_avg:60.28ms
step:1026/2315 train_time:61846ms step_avg:60.28ms
step:1027/2315 train_time:61906ms step_avg:60.28ms
step:1028/2315 train_time:61967ms step_avg:60.28ms
step:1029/2315 train_time:62027ms step_avg:60.28ms
step:1030/2315 train_time:62087ms step_avg:60.28ms
step:1031/2315 train_time:62148ms step_avg:60.28ms
step:1032/2315 train_time:62210ms step_avg:60.28ms
step:1033/2315 train_time:62273ms step_avg:60.28ms
step:1034/2315 train_time:62335ms step_avg:60.29ms
step:1035/2315 train_time:62397ms step_avg:60.29ms
step:1036/2315 train_time:62458ms step_avg:60.29ms
step:1037/2315 train_time:62519ms step_avg:60.29ms
step:1038/2315 train_time:62580ms step_avg:60.29ms
step:1039/2315 train_time:62640ms step_avg:60.29ms
step:1040/2315 train_time:62701ms step_avg:60.29ms
step:1041/2315 train_time:62762ms step_avg:60.29ms
step:1042/2315 train_time:62823ms step_avg:60.29ms
step:1043/2315 train_time:62882ms step_avg:60.29ms
step:1044/2315 train_time:62944ms step_avg:60.29ms
step:1045/2315 train_time:63003ms step_avg:60.29ms
step:1046/2315 train_time:63063ms step_avg:60.29ms
step:1047/2315 train_time:63124ms step_avg:60.29ms
step:1048/2315 train_time:63186ms step_avg:60.29ms
step:1049/2315 train_time:63247ms step_avg:60.29ms
step:1050/2315 train_time:63309ms step_avg:60.29ms
step:1051/2315 train_time:63371ms step_avg:60.30ms
step:1052/2315 train_time:63433ms step_avg:60.30ms
step:1053/2315 train_time:63494ms step_avg:60.30ms
step:1054/2315 train_time:63556ms step_avg:60.30ms
step:1055/2315 train_time:63616ms step_avg:60.30ms
step:1056/2315 train_time:63677ms step_avg:60.30ms
step:1057/2315 train_time:63737ms step_avg:60.30ms
step:1058/2315 train_time:63798ms step_avg:60.30ms
step:1059/2315 train_time:63857ms step_avg:60.30ms
step:1060/2315 train_time:63917ms step_avg:60.30ms
step:1061/2315 train_time:63978ms step_avg:60.30ms
step:1062/2315 train_time:64039ms step_avg:60.30ms
step:1063/2315 train_time:64099ms step_avg:60.30ms
step:1064/2315 train_time:64159ms step_avg:60.30ms
step:1065/2315 train_time:64220ms step_avg:60.30ms
step:1066/2315 train_time:64281ms step_avg:60.30ms
step:1067/2315 train_time:64342ms step_avg:60.30ms
step:1068/2315 train_time:64403ms step_avg:60.30ms
step:1069/2315 train_time:64464ms step_avg:60.30ms
step:1070/2315 train_time:64526ms step_avg:60.30ms
step:1071/2315 train_time:64587ms step_avg:60.31ms
step:1072/2315 train_time:64648ms step_avg:60.31ms
step:1073/2315 train_time:64709ms step_avg:60.31ms
step:1074/2315 train_time:64771ms step_avg:60.31ms
step:1075/2315 train_time:64831ms step_avg:60.31ms
step:1076/2315 train_time:64893ms step_avg:60.31ms
step:1077/2315 train_time:64952ms step_avg:60.31ms
step:1078/2315 train_time:65013ms step_avg:60.31ms
step:1079/2315 train_time:65073ms step_avg:60.31ms
step:1080/2315 train_time:65134ms step_avg:60.31ms
step:1081/2315 train_time:65195ms step_avg:60.31ms
step:1082/2315 train_time:65256ms step_avg:60.31ms
step:1083/2315 train_time:65317ms step_avg:60.31ms
step:1084/2315 train_time:65377ms step_avg:60.31ms
step:1085/2315 train_time:65437ms step_avg:60.31ms
step:1086/2315 train_time:65498ms step_avg:60.31ms
step:1087/2315 train_time:65558ms step_avg:60.31ms
step:1088/2315 train_time:65619ms step_avg:60.31ms
step:1089/2315 train_time:65680ms step_avg:60.31ms
step:1090/2315 train_time:65740ms step_avg:60.31ms
step:1091/2315 train_time:65801ms step_avg:60.31ms
step:1092/2315 train_time:65863ms step_avg:60.31ms
step:1093/2315 train_time:65923ms step_avg:60.31ms
step:1094/2315 train_time:65984ms step_avg:60.31ms
step:1095/2315 train_time:66045ms step_avg:60.31ms
step:1096/2315 train_time:66106ms step_avg:60.32ms
step:1097/2315 train_time:66166ms step_avg:60.32ms
step:1098/2315 train_time:66228ms step_avg:60.32ms
step:1099/2315 train_time:66289ms step_avg:60.32ms
step:1100/2315 train_time:66351ms step_avg:60.32ms
step:1101/2315 train_time:66411ms step_avg:60.32ms
step:1102/2315 train_time:66473ms step_avg:60.32ms
step:1103/2315 train_time:66534ms step_avg:60.32ms
step:1104/2315 train_time:66596ms step_avg:60.32ms
step:1105/2315 train_time:66656ms step_avg:60.32ms
step:1106/2315 train_time:66716ms step_avg:60.32ms
step:1107/2315 train_time:66776ms step_avg:60.32ms
step:1108/2315 train_time:66837ms step_avg:60.32ms
step:1109/2315 train_time:66897ms step_avg:60.32ms
step:1110/2315 train_time:66958ms step_avg:60.32ms
step:1111/2315 train_time:67018ms step_avg:60.32ms
step:1112/2315 train_time:67078ms step_avg:60.32ms
step:1113/2315 train_time:67139ms step_avg:60.32ms
step:1114/2315 train_time:67200ms step_avg:60.32ms
step:1115/2315 train_time:67260ms step_avg:60.32ms
step:1116/2315 train_time:67321ms step_avg:60.32ms
step:1117/2315 train_time:67382ms step_avg:60.32ms
step:1118/2315 train_time:67444ms step_avg:60.33ms
step:1119/2315 train_time:67505ms step_avg:60.33ms
step:1120/2315 train_time:67566ms step_avg:60.33ms
step:1121/2315 train_time:67627ms step_avg:60.33ms
step:1122/2315 train_time:67689ms step_avg:60.33ms
step:1123/2315 train_time:67749ms step_avg:60.33ms
step:1124/2315 train_time:67811ms step_avg:60.33ms
step:1125/2315 train_time:67872ms step_avg:60.33ms
step:1126/2315 train_time:67933ms step_avg:60.33ms
step:1127/2315 train_time:67993ms step_avg:60.33ms
step:1128/2315 train_time:68054ms step_avg:60.33ms
step:1129/2315 train_time:68115ms step_avg:60.33ms
step:1130/2315 train_time:68175ms step_avg:60.33ms
step:1131/2315 train_time:68236ms step_avg:60.33ms
step:1132/2315 train_time:68297ms step_avg:60.33ms
step:1133/2315 train_time:68357ms step_avg:60.33ms
step:1134/2315 train_time:68418ms step_avg:60.33ms
step:1135/2315 train_time:68478ms step_avg:60.33ms
step:1136/2315 train_time:68539ms step_avg:60.33ms
step:1137/2315 train_time:68599ms step_avg:60.33ms
step:1138/2315 train_time:68660ms step_avg:60.33ms
step:1139/2315 train_time:68721ms step_avg:60.33ms
step:1140/2315 train_time:68783ms step_avg:60.34ms
step:1141/2315 train_time:68843ms step_avg:60.34ms
step:1142/2315 train_time:68904ms step_avg:60.34ms
step:1143/2315 train_time:68965ms step_avg:60.34ms
step:1144/2315 train_time:69026ms step_avg:60.34ms
step:1145/2315 train_time:69086ms step_avg:60.34ms
step:1146/2315 train_time:69148ms step_avg:60.34ms
step:1147/2315 train_time:69209ms step_avg:60.34ms
step:1148/2315 train_time:69271ms step_avg:60.34ms
step:1149/2315 train_time:69332ms step_avg:60.34ms
step:1150/2315 train_time:69393ms step_avg:60.34ms
step:1151/2315 train_time:69454ms step_avg:60.34ms
step:1152/2315 train_time:69515ms step_avg:60.34ms
step:1153/2315 train_time:69575ms step_avg:60.34ms
step:1154/2315 train_time:69636ms step_avg:60.34ms
step:1155/2315 train_time:69696ms step_avg:60.34ms
step:1156/2315 train_time:69757ms step_avg:60.34ms
step:1157/2315 train_time:69817ms step_avg:60.34ms
step:1158/2315 train_time:69878ms step_avg:60.34ms
step:1159/2315 train_time:69938ms step_avg:60.34ms
step:1160/2315 train_time:69999ms step_avg:60.34ms
step:1161/2315 train_time:70059ms step_avg:60.34ms
step:1162/2315 train_time:70120ms step_avg:60.34ms
step:1163/2315 train_time:70180ms step_avg:60.34ms
step:1164/2315 train_time:70241ms step_avg:60.34ms
step:1165/2315 train_time:70303ms step_avg:60.35ms
step:1166/2315 train_time:70364ms step_avg:60.35ms
step:1167/2315 train_time:70425ms step_avg:60.35ms
step:1168/2315 train_time:70486ms step_avg:60.35ms
step:1169/2315 train_time:70547ms step_avg:60.35ms
step:1170/2315 train_time:70608ms step_avg:60.35ms
step:1171/2315 train_time:70669ms step_avg:60.35ms
step:1172/2315 train_time:70730ms step_avg:60.35ms
step:1173/2315 train_time:70792ms step_avg:60.35ms
step:1174/2315 train_time:70854ms step_avg:60.35ms
step:1175/2315 train_time:70914ms step_avg:60.35ms
step:1176/2315 train_time:70974ms step_avg:60.35ms
step:1177/2315 train_time:71034ms step_avg:60.35ms
step:1178/2315 train_time:71095ms step_avg:60.35ms
step:1179/2315 train_time:71155ms step_avg:60.35ms
step:1180/2315 train_time:71216ms step_avg:60.35ms
step:1181/2315 train_time:71277ms step_avg:60.35ms
step:1182/2315 train_time:71338ms step_avg:60.35ms
step:1183/2315 train_time:71398ms step_avg:60.35ms
step:1184/2315 train_time:71459ms step_avg:60.35ms
step:1185/2315 train_time:71519ms step_avg:60.35ms
step:1186/2315 train_time:71580ms step_avg:60.35ms
step:1187/2315 train_time:71640ms step_avg:60.35ms
step:1188/2315 train_time:71702ms step_avg:60.35ms
step:1189/2315 train_time:71762ms step_avg:60.35ms
step:1190/2315 train_time:71823ms step_avg:60.36ms
step:1191/2315 train_time:71883ms step_avg:60.36ms
step:1192/2315 train_time:71944ms step_avg:60.36ms
step:1193/2315 train_time:72005ms step_avg:60.36ms
step:1194/2315 train_time:72066ms step_avg:60.36ms
step:1195/2315 train_time:72127ms step_avg:60.36ms
step:1196/2315 train_time:72188ms step_avg:60.36ms
step:1197/2315 train_time:72249ms step_avg:60.36ms
step:1198/2315 train_time:72311ms step_avg:60.36ms
step:1199/2315 train_time:72372ms step_avg:60.36ms
step:1200/2315 train_time:72434ms step_avg:60.36ms
step:1201/2315 train_time:72495ms step_avg:60.36ms
step:1202/2315 train_time:72555ms step_avg:60.36ms
step:1203/2315 train_time:72615ms step_avg:60.36ms
step:1204/2315 train_time:72676ms step_avg:60.36ms
step:1205/2315 train_time:72737ms step_avg:60.36ms
step:1206/2315 train_time:72797ms step_avg:60.36ms
step:1207/2315 train_time:72857ms step_avg:60.36ms
step:1208/2315 train_time:72918ms step_avg:60.36ms
step:1209/2315 train_time:72979ms step_avg:60.36ms
step:1210/2315 train_time:73039ms step_avg:60.36ms
step:1211/2315 train_time:73100ms step_avg:60.36ms
step:1212/2315 train_time:73160ms step_avg:60.36ms
step:1213/2315 train_time:73221ms step_avg:60.36ms
step:1214/2315 train_time:73282ms step_avg:60.36ms
step:1215/2315 train_time:73342ms step_avg:60.36ms
step:1216/2315 train_time:73404ms step_avg:60.37ms
step:1217/2315 train_time:73465ms step_avg:60.37ms
step:1218/2315 train_time:73526ms step_avg:60.37ms
step:1219/2315 train_time:73587ms step_avg:60.37ms
step:1220/2315 train_time:73648ms step_avg:60.37ms
step:1221/2315 train_time:73709ms step_avg:60.37ms
step:1222/2315 train_time:73770ms step_avg:60.37ms
step:1223/2315 train_time:73830ms step_avg:60.37ms
step:1224/2315 train_time:73892ms step_avg:60.37ms
step:1225/2315 train_time:73952ms step_avg:60.37ms
step:1226/2315 train_time:74014ms step_avg:60.37ms
step:1227/2315 train_time:74074ms step_avg:60.37ms
step:1228/2315 train_time:74135ms step_avg:60.37ms
step:1229/2315 train_time:74196ms step_avg:60.37ms
step:1230/2315 train_time:74256ms step_avg:60.37ms
step:1231/2315 train_time:74317ms step_avg:60.37ms
step:1232/2315 train_time:74378ms step_avg:60.37ms
step:1233/2315 train_time:74438ms step_avg:60.37ms
step:1234/2315 train_time:74500ms step_avg:60.37ms
step:1235/2315 train_time:74560ms step_avg:60.37ms
step:1236/2315 train_time:74620ms step_avg:60.37ms
step:1237/2315 train_time:74681ms step_avg:60.37ms
step:1238/2315 train_time:74743ms step_avg:60.37ms
step:1239/2315 train_time:74803ms step_avg:60.37ms
step:1240/2315 train_time:74864ms step_avg:60.37ms
step:1241/2315 train_time:74925ms step_avg:60.37ms
step:1242/2315 train_time:74986ms step_avg:60.38ms
step:1243/2315 train_time:75047ms step_avg:60.38ms
step:1244/2315 train_time:75108ms step_avg:60.38ms
step:1245/2315 train_time:75169ms step_avg:60.38ms
step:1246/2315 train_time:75230ms step_avg:60.38ms
step:1247/2315 train_time:75291ms step_avg:60.38ms
step:1248/2315 train_time:75353ms step_avg:60.38ms
step:1249/2315 train_time:75415ms step_avg:60.38ms
step:1250/2315 train_time:75476ms step_avg:60.38ms
step:1250/2315 val_loss:3.5179 train_time:75538ms step_avg:60.43ms
step:1251/2315 train_time:75556ms step_avg:60.40ms
step:1252/2315 train_time:75600ms step_avg:60.38ms
step:1253/2315 train_time:75665ms step_avg:60.39ms
step:1254/2315 train_time:75730ms step_avg:60.39ms
step:1255/2315 train_time:75790ms step_avg:60.39ms
step:1256/2315 train_time:75850ms step_avg:60.39ms
step:1257/2315 train_time:75910ms step_avg:60.39ms
step:1258/2315 train_time:75970ms step_avg:60.39ms
step:1259/2315 train_time:76030ms step_avg:60.39ms
step:1260/2315 train_time:76090ms step_avg:60.39ms
step:1261/2315 train_time:76150ms step_avg:60.39ms
step:1262/2315 train_time:76210ms step_avg:60.39ms
step:1263/2315 train_time:76269ms step_avg:60.39ms
step:1264/2315 train_time:76329ms step_avg:60.39ms
step:1265/2315 train_time:76388ms step_avg:60.39ms
step:1266/2315 train_time:76450ms step_avg:60.39ms
step:1267/2315 train_time:76512ms step_avg:60.39ms
step:1268/2315 train_time:76574ms step_avg:60.39ms
step:1269/2315 train_time:76637ms step_avg:60.39ms
step:1270/2315 train_time:76699ms step_avg:60.39ms
step:1271/2315 train_time:76760ms step_avg:60.39ms
step:1272/2315 train_time:76822ms step_avg:60.39ms
step:1273/2315 train_time:76882ms step_avg:60.39ms
step:1274/2315 train_time:76943ms step_avg:60.40ms
step:1275/2315 train_time:77004ms step_avg:60.40ms
step:1276/2315 train_time:77065ms step_avg:60.40ms
step:1277/2315 train_time:77125ms step_avg:60.40ms
step:1278/2315 train_time:77186ms step_avg:60.40ms
step:1279/2315 train_time:77246ms step_avg:60.40ms
step:1280/2315 train_time:77307ms step_avg:60.40ms
step:1281/2315 train_time:77367ms step_avg:60.40ms
step:1282/2315 train_time:77428ms step_avg:60.40ms
step:1283/2315 train_time:77489ms step_avg:60.40ms
step:1284/2315 train_time:77551ms step_avg:60.40ms
step:1285/2315 train_time:77612ms step_avg:60.40ms
step:1286/2315 train_time:77673ms step_avg:60.40ms
step:1287/2315 train_time:77734ms step_avg:60.40ms
step:1288/2315 train_time:77795ms step_avg:60.40ms
step:1289/2315 train_time:77855ms step_avg:60.40ms
step:1290/2315 train_time:77917ms step_avg:60.40ms
step:1291/2315 train_time:77977ms step_avg:60.40ms
step:1292/2315 train_time:78038ms step_avg:60.40ms
step:1293/2315 train_time:78099ms step_avg:60.40ms
step:1294/2315 train_time:78160ms step_avg:60.40ms
step:1295/2315 train_time:78220ms step_avg:60.40ms
step:1296/2315 train_time:78281ms step_avg:60.40ms
step:1297/2315 train_time:78342ms step_avg:60.40ms
step:1298/2315 train_time:78403ms step_avg:60.40ms
step:1299/2315 train_time:78464ms step_avg:60.40ms
step:1300/2315 train_time:78526ms step_avg:60.40ms
step:1301/2315 train_time:78588ms step_avg:60.41ms
step:1302/2315 train_time:78649ms step_avg:60.41ms
step:1303/2315 train_time:78710ms step_avg:60.41ms
step:1304/2315 train_time:78770ms step_avg:60.41ms
step:1305/2315 train_time:78831ms step_avg:60.41ms
step:1306/2315 train_time:78892ms step_avg:60.41ms
step:1307/2315 train_time:78952ms step_avg:60.41ms
step:1308/2315 train_time:79013ms step_avg:60.41ms
step:1309/2315 train_time:79073ms step_avg:60.41ms
step:1310/2315 train_time:79133ms step_avg:60.41ms
step:1311/2315 train_time:79193ms step_avg:60.41ms
step:1312/2315 train_time:79254ms step_avg:60.41ms
step:1313/2315 train_time:79314ms step_avg:60.41ms
step:1314/2315 train_time:79375ms step_avg:60.41ms
step:1315/2315 train_time:79436ms step_avg:60.41ms
step:1316/2315 train_time:79497ms step_avg:60.41ms
step:1317/2315 train_time:79558ms step_avg:60.41ms
step:1318/2315 train_time:79619ms step_avg:60.41ms
step:1319/2315 train_time:79680ms step_avg:60.41ms
step:1320/2315 train_time:79743ms step_avg:60.41ms
step:1321/2315 train_time:79804ms step_avg:60.41ms
step:1322/2315 train_time:79865ms step_avg:60.41ms
step:1323/2315 train_time:79927ms step_avg:60.41ms
step:1324/2315 train_time:79988ms step_avg:60.41ms
step:1325/2315 train_time:80048ms step_avg:60.41ms
step:1326/2315 train_time:80108ms step_avg:60.41ms
step:1327/2315 train_time:80168ms step_avg:60.41ms
step:1328/2315 train_time:80229ms step_avg:60.41ms
step:1329/2315 train_time:80289ms step_avg:60.41ms
step:1330/2315 train_time:80350ms step_avg:60.41ms
step:1331/2315 train_time:80410ms step_avg:60.41ms
step:1332/2315 train_time:80472ms step_avg:60.41ms
step:1333/2315 train_time:80532ms step_avg:60.41ms
step:1334/2315 train_time:80593ms step_avg:60.41ms
step:1335/2315 train_time:80653ms step_avg:60.41ms
step:1336/2315 train_time:80715ms step_avg:60.42ms
step:1337/2315 train_time:80776ms step_avg:60.42ms
step:1338/2315 train_time:80837ms step_avg:60.42ms
step:1339/2315 train_time:80898ms step_avg:60.42ms
step:1340/2315 train_time:80960ms step_avg:60.42ms
step:1341/2315 train_time:81021ms step_avg:60.42ms
step:1342/2315 train_time:81082ms step_avg:60.42ms
step:1343/2315 train_time:81142ms step_avg:60.42ms
step:1344/2315 train_time:81203ms step_avg:60.42ms
step:1345/2315 train_time:81263ms step_avg:60.42ms
step:1346/2315 train_time:81325ms step_avg:60.42ms
step:1347/2315 train_time:81385ms step_avg:60.42ms
step:1348/2315 train_time:81447ms step_avg:60.42ms
step:1349/2315 train_time:81507ms step_avg:60.42ms
step:1350/2315 train_time:81568ms step_avg:60.42ms
step:1351/2315 train_time:81628ms step_avg:60.42ms
step:1352/2315 train_time:81689ms step_avg:60.42ms
step:1353/2315 train_time:81749ms step_avg:60.42ms
step:1354/2315 train_time:81810ms step_avg:60.42ms
step:1355/2315 train_time:81871ms step_avg:60.42ms
step:1356/2315 train_time:81931ms step_avg:60.42ms
step:1357/2315 train_time:81991ms step_avg:60.42ms
step:1358/2315 train_time:82052ms step_avg:60.42ms
step:1359/2315 train_time:82112ms step_avg:60.42ms
step:1360/2315 train_time:82173ms step_avg:60.42ms
step:1361/2315 train_time:82233ms step_avg:60.42ms
step:1362/2315 train_time:82294ms step_avg:60.42ms
step:1363/2315 train_time:82354ms step_avg:60.42ms
step:1364/2315 train_time:82415ms step_avg:60.42ms
step:1365/2315 train_time:82476ms step_avg:60.42ms
step:1366/2315 train_time:82537ms step_avg:60.42ms
step:1367/2315 train_time:82598ms step_avg:60.42ms
step:1368/2315 train_time:82659ms step_avg:60.42ms
step:1369/2315 train_time:82720ms step_avg:60.42ms
step:1370/2315 train_time:82782ms step_avg:60.42ms
step:1371/2315 train_time:82843ms step_avg:60.43ms
step:1372/2315 train_time:82904ms step_avg:60.43ms
step:1373/2315 train_time:82965ms step_avg:60.43ms
step:1374/2315 train_time:83026ms step_avg:60.43ms
step:1375/2315 train_time:83086ms step_avg:60.43ms
step:1376/2315 train_time:83148ms step_avg:60.43ms
step:1377/2315 train_time:83208ms step_avg:60.43ms
step:1378/2315 train_time:83268ms step_avg:60.43ms
step:1379/2315 train_time:83329ms step_avg:60.43ms
step:1380/2315 train_time:83391ms step_avg:60.43ms
step:1381/2315 train_time:83451ms step_avg:60.43ms
step:1382/2315 train_time:83511ms step_avg:60.43ms
step:1383/2315 train_time:83572ms step_avg:60.43ms
step:1384/2315 train_time:83633ms step_avg:60.43ms
step:1385/2315 train_time:83693ms step_avg:60.43ms
step:1386/2315 train_time:83754ms step_avg:60.43ms
step:1387/2315 train_time:83814ms step_avg:60.43ms
step:1388/2315 train_time:83875ms step_avg:60.43ms
step:1389/2315 train_time:83936ms step_avg:60.43ms
step:1390/2315 train_time:83997ms step_avg:60.43ms
step:1391/2315 train_time:84058ms step_avg:60.43ms
step:1392/2315 train_time:84119ms step_avg:60.43ms
step:1393/2315 train_time:84180ms step_avg:60.43ms
step:1394/2315 train_time:84241ms step_avg:60.43ms
step:1395/2315 train_time:84302ms step_avg:60.43ms
step:1396/2315 train_time:84363ms step_avg:60.43ms
step:1397/2315 train_time:84425ms step_avg:60.43ms
step:1398/2315 train_time:84487ms step_avg:60.43ms
step:1399/2315 train_time:84547ms step_avg:60.43ms
step:1400/2315 train_time:84608ms step_avg:60.43ms
step:1401/2315 train_time:84668ms step_avg:60.43ms
step:1402/2315 train_time:84729ms step_avg:60.43ms
step:1403/2315 train_time:84789ms step_avg:60.43ms
step:1404/2315 train_time:84850ms step_avg:60.43ms
step:1405/2315 train_time:84910ms step_avg:60.43ms
step:1406/2315 train_time:84970ms step_avg:60.43ms
step:1407/2315 train_time:85031ms step_avg:60.43ms
step:1408/2315 train_time:85092ms step_avg:60.43ms
step:1409/2315 train_time:85152ms step_avg:60.43ms
step:1410/2315 train_time:85212ms step_avg:60.43ms
step:1411/2315 train_time:85273ms step_avg:60.43ms
step:1412/2315 train_time:85335ms step_avg:60.44ms
step:1413/2315 train_time:85395ms step_avg:60.44ms
step:1414/2315 train_time:85456ms step_avg:60.44ms
step:1415/2315 train_time:85517ms step_avg:60.44ms
step:1416/2315 train_time:85578ms step_avg:60.44ms
step:1417/2315 train_time:85638ms step_avg:60.44ms
step:1418/2315 train_time:85699ms step_avg:60.44ms
step:1419/2315 train_time:85760ms step_avg:60.44ms
step:1420/2315 train_time:85822ms step_avg:60.44ms
step:1421/2315 train_time:85883ms step_avg:60.44ms
step:1422/2315 train_time:85945ms step_avg:60.44ms
step:1423/2315 train_time:86005ms step_avg:60.44ms
step:1424/2315 train_time:86067ms step_avg:60.44ms
step:1425/2315 train_time:86128ms step_avg:60.44ms
step:1426/2315 train_time:86189ms step_avg:60.44ms
step:1427/2315 train_time:86249ms step_avg:60.44ms
step:1428/2315 train_time:86310ms step_avg:60.44ms
step:1429/2315 train_time:86370ms step_avg:60.44ms
step:1430/2315 train_time:86431ms step_avg:60.44ms
step:1431/2315 train_time:86491ms step_avg:60.44ms
step:1432/2315 train_time:86552ms step_avg:60.44ms
step:1433/2315 train_time:86612ms step_avg:60.44ms
step:1434/2315 train_time:86673ms step_avg:60.44ms
step:1435/2315 train_time:86733ms step_avg:60.44ms
step:1436/2315 train_time:86794ms step_avg:60.44ms
step:1437/2315 train_time:86855ms step_avg:60.44ms
step:1438/2315 train_time:86916ms step_avg:60.44ms
step:1439/2315 train_time:86977ms step_avg:60.44ms
step:1440/2315 train_time:87038ms step_avg:60.44ms
step:1441/2315 train_time:87098ms step_avg:60.44ms
step:1442/2315 train_time:87160ms step_avg:60.44ms
step:1443/2315 train_time:87220ms step_avg:60.44ms
step:1444/2315 train_time:87282ms step_avg:60.44ms
step:1445/2315 train_time:87343ms step_avg:60.44ms
step:1446/2315 train_time:87404ms step_avg:60.45ms
step:1447/2315 train_time:87465ms step_avg:60.45ms
step:1448/2315 train_time:87527ms step_avg:60.45ms
step:1449/2315 train_time:87587ms step_avg:60.45ms
step:1450/2315 train_time:87648ms step_avg:60.45ms
step:1451/2315 train_time:87708ms step_avg:60.45ms
step:1452/2315 train_time:87769ms step_avg:60.45ms
step:1453/2315 train_time:87829ms step_avg:60.45ms
step:1454/2315 train_time:87890ms step_avg:60.45ms
step:1455/2315 train_time:87951ms step_avg:60.45ms
step:1456/2315 train_time:88011ms step_avg:60.45ms
step:1457/2315 train_time:88072ms step_avg:60.45ms
step:1458/2315 train_time:88133ms step_avg:60.45ms
step:1459/2315 train_time:88193ms step_avg:60.45ms
step:1460/2315 train_time:88254ms step_avg:60.45ms
step:1461/2315 train_time:88315ms step_avg:60.45ms
step:1462/2315 train_time:88376ms step_avg:60.45ms
step:1463/2315 train_time:88437ms step_avg:60.45ms
step:1464/2315 train_time:88498ms step_avg:60.45ms
step:1465/2315 train_time:88560ms step_avg:60.45ms
step:1466/2315 train_time:88621ms step_avg:60.45ms
step:1467/2315 train_time:88681ms step_avg:60.45ms
step:1468/2315 train_time:88743ms step_avg:60.45ms
step:1469/2315 train_time:88803ms step_avg:60.45ms
step:1470/2315 train_time:88865ms step_avg:60.45ms
step:1471/2315 train_time:88925ms step_avg:60.45ms
step:1472/2315 train_time:88986ms step_avg:60.45ms
step:1473/2315 train_time:89047ms step_avg:60.45ms
step:1474/2315 train_time:89108ms step_avg:60.45ms
step:1475/2315 train_time:89168ms step_avg:60.45ms
step:1476/2315 train_time:89229ms step_avg:60.45ms
step:1477/2315 train_time:89289ms step_avg:60.45ms
step:1478/2315 train_time:89350ms step_avg:60.45ms
step:1479/2315 train_time:89411ms step_avg:60.45ms
step:1480/2315 train_time:89472ms step_avg:60.45ms
step:1481/2315 train_time:89532ms step_avg:60.45ms
step:1482/2315 train_time:89592ms step_avg:60.45ms
step:1483/2315 train_time:89652ms step_avg:60.45ms
step:1484/2315 train_time:89713ms step_avg:60.45ms
step:1485/2315 train_time:89774ms step_avg:60.45ms
step:1486/2315 train_time:89835ms step_avg:60.45ms
step:1487/2315 train_time:89896ms step_avg:60.45ms
step:1488/2315 train_time:89957ms step_avg:60.46ms
step:1489/2315 train_time:90018ms step_avg:60.46ms
step:1490/2315 train_time:90079ms step_avg:60.46ms
step:1491/2315 train_time:90140ms step_avg:60.46ms
step:1492/2315 train_time:90201ms step_avg:60.46ms
step:1493/2315 train_time:90262ms step_avg:60.46ms
step:1494/2315 train_time:90324ms step_avg:60.46ms
step:1495/2315 train_time:90385ms step_avg:60.46ms
step:1496/2315 train_time:90446ms step_avg:60.46ms
step:1497/2315 train_time:90506ms step_avg:60.46ms
step:1498/2315 train_time:90568ms step_avg:60.46ms
step:1499/2315 train_time:90628ms step_avg:60.46ms
step:1500/2315 train_time:90689ms step_avg:60.46ms
step:1500/2315 val_loss:3.4677 train_time:90751ms step_avg:60.50ms
step:1501/2315 train_time:90770ms step_avg:60.47ms
step:1502/2315 train_time:90814ms step_avg:60.46ms
step:1503/2315 train_time:90880ms step_avg:60.47ms
step:1504/2315 train_time:90945ms step_avg:60.47ms
step:1505/2315 train_time:91004ms step_avg:60.47ms
step:1506/2315 train_time:91064ms step_avg:60.47ms
step:1507/2315 train_time:91124ms step_avg:60.47ms
step:1508/2315 train_time:91184ms step_avg:60.47ms
step:1509/2315 train_time:91244ms step_avg:60.47ms
step:1510/2315 train_time:91303ms step_avg:60.47ms
step:1511/2315 train_time:91363ms step_avg:60.47ms
step:1512/2315 train_time:91424ms step_avg:60.47ms
step:1513/2315 train_time:91483ms step_avg:60.46ms
step:1514/2315 train_time:91545ms step_avg:60.47ms
step:1515/2315 train_time:91604ms step_avg:60.46ms
step:1516/2315 train_time:91665ms step_avg:60.46ms
step:1517/2315 train_time:91726ms step_avg:60.47ms
step:1518/2315 train_time:91787ms step_avg:60.47ms
step:1519/2315 train_time:91850ms step_avg:60.47ms
step:1520/2315 train_time:91912ms step_avg:60.47ms
step:1521/2315 train_time:91974ms step_avg:60.47ms
step:1522/2315 train_time:92036ms step_avg:60.47ms
step:1523/2315 train_time:92097ms step_avg:60.47ms
step:1524/2315 train_time:92158ms step_avg:60.47ms
step:1525/2315 train_time:92219ms step_avg:60.47ms
step:1526/2315 train_time:92281ms step_avg:60.47ms
step:1527/2315 train_time:92341ms step_avg:60.47ms
step:1528/2315 train_time:92402ms step_avg:60.47ms
step:1529/2315 train_time:92462ms step_avg:60.47ms
step:1530/2315 train_time:92523ms step_avg:60.47ms
step:1531/2315 train_time:92583ms step_avg:60.47ms
step:1532/2315 train_time:92645ms step_avg:60.47ms
step:1533/2315 train_time:92706ms step_avg:60.47ms
step:1534/2315 train_time:92768ms step_avg:60.47ms
step:1535/2315 train_time:92829ms step_avg:60.48ms
step:1536/2315 train_time:92891ms step_avg:60.48ms
step:1537/2315 train_time:92952ms step_avg:60.48ms
step:1538/2315 train_time:93014ms step_avg:60.48ms
step:1539/2315 train_time:93075ms step_avg:60.48ms
step:1540/2315 train_time:93137ms step_avg:60.48ms
step:1541/2315 train_time:93198ms step_avg:60.48ms
step:1542/2315 train_time:93260ms step_avg:60.48ms
step:1543/2315 train_time:93321ms step_avg:60.48ms
step:1544/2315 train_time:93382ms step_avg:60.48ms
step:1545/2315 train_time:93442ms step_avg:60.48ms
step:1546/2315 train_time:93503ms step_avg:60.48ms
step:1547/2315 train_time:93564ms step_avg:60.48ms
step:1548/2315 train_time:93625ms step_avg:60.48ms
step:1549/2315 train_time:93686ms step_avg:60.48ms
step:1550/2315 train_time:93748ms step_avg:60.48ms
step:1551/2315 train_time:93809ms step_avg:60.48ms
step:1552/2315 train_time:93871ms step_avg:60.48ms
step:1553/2315 train_time:93932ms step_avg:60.48ms
step:1554/2315 train_time:93994ms step_avg:60.49ms
step:1555/2315 train_time:94056ms step_avg:60.49ms
step:1556/2315 train_time:94117ms step_avg:60.49ms
step:1557/2315 train_time:94179ms step_avg:60.49ms
step:1558/2315 train_time:94240ms step_avg:60.49ms
step:1559/2315 train_time:94301ms step_avg:60.49ms
step:1560/2315 train_time:94362ms step_avg:60.49ms
step:1561/2315 train_time:94423ms step_avg:60.49ms
step:1562/2315 train_time:94484ms step_avg:60.49ms
step:1563/2315 train_time:94544ms step_avg:60.49ms
step:1564/2315 train_time:94606ms step_avg:60.49ms
step:1565/2315 train_time:94666ms step_avg:60.49ms
step:1566/2315 train_time:94728ms step_avg:60.49ms
step:1567/2315 train_time:94789ms step_avg:60.49ms
step:1568/2315 train_time:94851ms step_avg:60.49ms
step:1569/2315 train_time:94911ms step_avg:60.49ms
step:1570/2315 train_time:94973ms step_avg:60.49ms
step:1571/2315 train_time:95035ms step_avg:60.49ms
step:1572/2315 train_time:95097ms step_avg:60.49ms
step:1573/2315 train_time:95158ms step_avg:60.49ms
step:1574/2315 train_time:95220ms step_avg:60.50ms
step:1575/2315 train_time:95281ms step_avg:60.50ms
step:1576/2315 train_time:95342ms step_avg:60.50ms
step:1577/2315 train_time:95403ms step_avg:60.50ms
step:1578/2315 train_time:95464ms step_avg:60.50ms
step:1579/2315 train_time:95524ms step_avg:60.50ms
step:1580/2315 train_time:95586ms step_avg:60.50ms
step:1581/2315 train_time:95646ms step_avg:60.50ms
step:1582/2315 train_time:95708ms step_avg:60.50ms
step:1583/2315 train_time:95769ms step_avg:60.50ms
step:1584/2315 train_time:95830ms step_avg:60.50ms
step:1585/2315 train_time:95890ms step_avg:60.50ms
step:1586/2315 train_time:95952ms step_avg:60.50ms
step:1587/2315 train_time:96013ms step_avg:60.50ms
step:1588/2315 train_time:96075ms step_avg:60.50ms
step:1589/2315 train_time:96136ms step_avg:60.50ms
step:1590/2315 train_time:96198ms step_avg:60.50ms
step:1591/2315 train_time:96260ms step_avg:60.50ms
step:1592/2315 train_time:96322ms step_avg:60.50ms
step:1593/2315 train_time:96383ms step_avg:60.50ms
step:1594/2315 train_time:96444ms step_avg:60.50ms
step:1595/2315 train_time:96505ms step_avg:60.50ms
step:1596/2315 train_time:96567ms step_avg:60.51ms
step:1597/2315 train_time:96628ms step_avg:60.51ms
step:1598/2315 train_time:96689ms step_avg:60.51ms
step:1599/2315 train_time:96750ms step_avg:60.51ms
step:1600/2315 train_time:96811ms step_avg:60.51ms
step:1601/2315 train_time:96872ms step_avg:60.51ms
step:1602/2315 train_time:96933ms step_avg:60.51ms
step:1603/2315 train_time:96994ms step_avg:60.51ms
step:1604/2315 train_time:97056ms step_avg:60.51ms
step:1605/2315 train_time:97117ms step_avg:60.51ms
step:1606/2315 train_time:97179ms step_avg:60.51ms
step:1607/2315 train_time:97240ms step_avg:60.51ms
step:1608/2315 train_time:97302ms step_avg:60.51ms
step:1609/2315 train_time:97364ms step_avg:60.51ms
step:1610/2315 train_time:97425ms step_avg:60.51ms
step:1611/2315 train_time:97485ms step_avg:60.51ms
step:1612/2315 train_time:97547ms step_avg:60.51ms
step:1613/2315 train_time:97608ms step_avg:60.51ms
step:1614/2315 train_time:97669ms step_avg:60.51ms
step:1615/2315 train_time:97730ms step_avg:60.51ms
step:1616/2315 train_time:97791ms step_avg:60.51ms
step:1617/2315 train_time:97852ms step_avg:60.51ms
step:1618/2315 train_time:97913ms step_avg:60.51ms
step:1619/2315 train_time:97974ms step_avg:60.52ms
step:1620/2315 train_time:98035ms step_avg:60.52ms
step:1621/2315 train_time:98096ms step_avg:60.52ms
step:1622/2315 train_time:98159ms step_avg:60.52ms
step:1623/2315 train_time:98220ms step_avg:60.52ms
step:1624/2315 train_time:98282ms step_avg:60.52ms
step:1625/2315 train_time:98343ms step_avg:60.52ms
step:1626/2315 train_time:98404ms step_avg:60.52ms
step:1627/2315 train_time:98464ms step_avg:60.52ms
step:1628/2315 train_time:98525ms step_avg:60.52ms
step:1629/2315 train_time:98586ms step_avg:60.52ms
step:1630/2315 train_time:98648ms step_avg:60.52ms
step:1631/2315 train_time:98708ms step_avg:60.52ms
step:1632/2315 train_time:98771ms step_avg:60.52ms
step:1633/2315 train_time:98830ms step_avg:60.52ms
step:1634/2315 train_time:98892ms step_avg:60.52ms
step:1635/2315 train_time:98953ms step_avg:60.52ms
step:1636/2315 train_time:99014ms step_avg:60.52ms
step:1637/2315 train_time:99075ms step_avg:60.52ms
step:1638/2315 train_time:99136ms step_avg:60.52ms
step:1639/2315 train_time:99198ms step_avg:60.52ms
step:1640/2315 train_time:99260ms step_avg:60.52ms
step:1641/2315 train_time:99322ms step_avg:60.53ms
step:1642/2315 train_time:99383ms step_avg:60.53ms
step:1643/2315 train_time:99445ms step_avg:60.53ms
step:1644/2315 train_time:99506ms step_avg:60.53ms
step:1645/2315 train_time:99567ms step_avg:60.53ms
step:1646/2315 train_time:99628ms step_avg:60.53ms
step:1647/2315 train_time:99688ms step_avg:60.53ms
step:1648/2315 train_time:99749ms step_avg:60.53ms
step:1649/2315 train_time:99809ms step_avg:60.53ms
step:1650/2315 train_time:99871ms step_avg:60.53ms
step:1651/2315 train_time:99932ms step_avg:60.53ms
step:1652/2315 train_time:99993ms step_avg:60.53ms
step:1653/2315 train_time:100055ms step_avg:60.53ms
step:1654/2315 train_time:100116ms step_avg:60.53ms
step:1655/2315 train_time:100178ms step_avg:60.53ms
step:1656/2315 train_time:100239ms step_avg:60.53ms
step:1657/2315 train_time:100301ms step_avg:60.53ms
step:1658/2315 train_time:100363ms step_avg:60.53ms
step:1659/2315 train_time:100423ms step_avg:60.53ms
step:1660/2315 train_time:100485ms step_avg:60.53ms
step:1661/2315 train_time:100545ms step_avg:60.53ms
step:1662/2315 train_time:100607ms step_avg:60.53ms
step:1663/2315 train_time:100668ms step_avg:60.53ms
step:1664/2315 train_time:100729ms step_avg:60.53ms
step:1665/2315 train_time:100789ms step_avg:60.53ms
step:1666/2315 train_time:100850ms step_avg:60.53ms
step:1667/2315 train_time:100911ms step_avg:60.53ms
step:1668/2315 train_time:100972ms step_avg:60.53ms
step:1669/2315 train_time:101033ms step_avg:60.54ms
step:1670/2315 train_time:101095ms step_avg:60.54ms
step:1671/2315 train_time:101156ms step_avg:60.54ms
step:1672/2315 train_time:101218ms step_avg:60.54ms
step:1673/2315 train_time:101280ms step_avg:60.54ms
step:1674/2315 train_time:101342ms step_avg:60.54ms
step:1675/2315 train_time:101403ms step_avg:60.54ms
step:1676/2315 train_time:101464ms step_avg:60.54ms
step:1677/2315 train_time:101525ms step_avg:60.54ms
step:1678/2315 train_time:101586ms step_avg:60.54ms
step:1679/2315 train_time:101647ms step_avg:60.54ms
step:1680/2315 train_time:101709ms step_avg:60.54ms
step:1681/2315 train_time:101769ms step_avg:60.54ms
step:1682/2315 train_time:101830ms step_avg:60.54ms
step:1683/2315 train_time:101890ms step_avg:60.54ms
step:1684/2315 train_time:101952ms step_avg:60.54ms
step:1685/2315 train_time:102013ms step_avg:60.54ms
step:1686/2315 train_time:102075ms step_avg:60.54ms
step:1687/2315 train_time:102136ms step_avg:60.54ms
step:1688/2315 train_time:102198ms step_avg:60.54ms
step:1689/2315 train_time:102260ms step_avg:60.54ms
step:1690/2315 train_time:102321ms step_avg:60.55ms
step:1691/2315 train_time:102383ms step_avg:60.55ms
step:1692/2315 train_time:102445ms step_avg:60.55ms
step:1693/2315 train_time:102506ms step_avg:60.55ms
step:1694/2315 train_time:102567ms step_avg:60.55ms
step:1695/2315 train_time:102628ms step_avg:60.55ms
step:1696/2315 train_time:102690ms step_avg:60.55ms
step:1697/2315 train_time:102750ms step_avg:60.55ms
step:1698/2315 train_time:102811ms step_avg:60.55ms
step:1699/2315 train_time:102872ms step_avg:60.55ms
step:1700/2315 train_time:102933ms step_avg:60.55ms
step:1701/2315 train_time:102994ms step_avg:60.55ms
step:1702/2315 train_time:103055ms step_avg:60.55ms
step:1703/2315 train_time:103117ms step_avg:60.55ms
step:1704/2315 train_time:103179ms step_avg:60.55ms
step:1705/2315 train_time:103240ms step_avg:60.55ms
step:1706/2315 train_time:103302ms step_avg:60.55ms
step:1707/2315 train_time:103364ms step_avg:60.55ms
step:1708/2315 train_time:103425ms step_avg:60.55ms
step:1709/2315 train_time:103486ms step_avg:60.55ms
step:1710/2315 train_time:103547ms step_avg:60.55ms
step:1711/2315 train_time:103608ms step_avg:60.55ms
step:1712/2315 train_time:103670ms step_avg:60.55ms
step:1713/2315 train_time:103730ms step_avg:60.55ms
step:1714/2315 train_time:103791ms step_avg:60.56ms
step:1715/2315 train_time:103852ms step_avg:60.55ms
step:1716/2315 train_time:103913ms step_avg:60.56ms
step:1717/2315 train_time:103975ms step_avg:60.56ms
step:1718/2315 train_time:104036ms step_avg:60.56ms
step:1719/2315 train_time:104097ms step_avg:60.56ms
step:1720/2315 train_time:104159ms step_avg:60.56ms
step:1721/2315 train_time:104220ms step_avg:60.56ms
step:1722/2315 train_time:104282ms step_avg:60.56ms
step:1723/2315 train_time:104343ms step_avg:60.56ms
step:1724/2315 train_time:104405ms step_avg:60.56ms
step:1725/2315 train_time:104466ms step_avg:60.56ms
step:1726/2315 train_time:104527ms step_avg:60.56ms
step:1727/2315 train_time:104588ms step_avg:60.56ms
step:1728/2315 train_time:104649ms step_avg:60.56ms
step:1729/2315 train_time:104710ms step_avg:60.56ms
step:1730/2315 train_time:104771ms step_avg:60.56ms
step:1731/2315 train_time:104832ms step_avg:60.56ms
step:1732/2315 train_time:104893ms step_avg:60.56ms
step:1733/2315 train_time:104955ms step_avg:60.56ms
step:1734/2315 train_time:105017ms step_avg:60.56ms
step:1735/2315 train_time:105077ms step_avg:60.56ms
step:1736/2315 train_time:105139ms step_avg:60.56ms
step:1737/2315 train_time:105201ms step_avg:60.56ms
step:1738/2315 train_time:105262ms step_avg:60.57ms
step:1739/2315 train_time:105323ms step_avg:60.57ms
step:1740/2315 train_time:105385ms step_avg:60.57ms
step:1741/2315 train_time:105445ms step_avg:60.57ms
step:1742/2315 train_time:105507ms step_avg:60.57ms
step:1743/2315 train_time:105568ms step_avg:60.57ms
step:1744/2315 train_time:105629ms step_avg:60.57ms
step:1745/2315 train_time:105689ms step_avg:60.57ms
step:1746/2315 train_time:105750ms step_avg:60.57ms
step:1747/2315 train_time:105811ms step_avg:60.57ms
step:1748/2315 train_time:105873ms step_avg:60.57ms
step:1749/2315 train_time:105933ms step_avg:60.57ms
step:1750/2315 train_time:105996ms step_avg:60.57ms
step:1750/2315 val_loss:3.3813 train_time:106059ms step_avg:60.60ms
step:1751/2315 train_time:106078ms step_avg:60.58ms
step:1752/2315 train_time:106121ms step_avg:60.57ms
step:1753/2315 train_time:106188ms step_avg:60.57ms
step:1754/2315 train_time:106252ms step_avg:60.58ms
step:1755/2315 train_time:106313ms step_avg:60.58ms
step:1756/2315 train_time:106374ms step_avg:60.58ms
step:1757/2315 train_time:106434ms step_avg:60.58ms
step:1758/2315 train_time:106494ms step_avg:60.58ms
step:1759/2315 train_time:106554ms step_avg:60.58ms
step:1760/2315 train_time:106614ms step_avg:60.58ms
step:1761/2315 train_time:106674ms step_avg:60.58ms
step:1762/2315 train_time:106734ms step_avg:60.58ms
step:1763/2315 train_time:106794ms step_avg:60.58ms
step:1764/2315 train_time:106855ms step_avg:60.58ms
step:1765/2315 train_time:106914ms step_avg:60.57ms
step:1766/2315 train_time:106976ms step_avg:60.58ms
step:1767/2315 train_time:107040ms step_avg:60.58ms
step:1768/2315 train_time:107103ms step_avg:60.58ms
step:1769/2315 train_time:107166ms step_avg:60.58ms
step:1770/2315 train_time:107228ms step_avg:60.58ms
step:1771/2315 train_time:107290ms step_avg:60.58ms
step:1772/2315 train_time:107352ms step_avg:60.58ms
step:1773/2315 train_time:107413ms step_avg:60.58ms
step:1774/2315 train_time:107474ms step_avg:60.58ms
step:1775/2315 train_time:107534ms step_avg:60.58ms
step:1776/2315 train_time:107595ms step_avg:60.58ms
step:1777/2315 train_time:107654ms step_avg:60.58ms
step:1778/2315 train_time:107715ms step_avg:60.58ms
step:1779/2315 train_time:107775ms step_avg:60.58ms
step:1780/2315 train_time:107836ms step_avg:60.58ms
step:1781/2315 train_time:107896ms step_avg:60.58ms
step:1782/2315 train_time:107958ms step_avg:60.58ms
step:1783/2315 train_time:108019ms step_avg:60.58ms
step:1784/2315 train_time:108081ms step_avg:60.58ms
step:1785/2315 train_time:108142ms step_avg:60.58ms
step:1786/2315 train_time:108204ms step_avg:60.58ms
step:1787/2315 train_time:108267ms step_avg:60.59ms
step:1788/2315 train_time:108329ms step_avg:60.59ms
step:1789/2315 train_time:108390ms step_avg:60.59ms
step:1790/2315 train_time:108451ms step_avg:60.59ms
step:1791/2315 train_time:108512ms step_avg:60.59ms
step:1792/2315 train_time:108573ms step_avg:60.59ms
step:1793/2315 train_time:108633ms step_avg:60.59ms
step:1794/2315 train_time:108694ms step_avg:60.59ms
step:1795/2315 train_time:108754ms step_avg:60.59ms
step:1796/2315 train_time:108815ms step_avg:60.59ms
step:1797/2315 train_time:108876ms step_avg:60.59ms
step:1798/2315 train_time:108937ms step_avg:60.59ms
step:1799/2315 train_time:108998ms step_avg:60.59ms
step:1800/2315 train_time:109059ms step_avg:60.59ms
step:1801/2315 train_time:109120ms step_avg:60.59ms
step:1802/2315 train_time:109181ms step_avg:60.59ms
step:1803/2315 train_time:109243ms step_avg:60.59ms
step:1804/2315 train_time:109306ms step_avg:60.59ms
step:1805/2315 train_time:109367ms step_avg:60.59ms
step:1806/2315 train_time:109428ms step_avg:60.59ms
step:1807/2315 train_time:109490ms step_avg:60.59ms
step:1808/2315 train_time:109551ms step_avg:60.59ms
step:1809/2315 train_time:109612ms step_avg:60.59ms
step:1810/2315 train_time:109674ms step_avg:60.59ms
step:1811/2315 train_time:109734ms step_avg:60.59ms
step:1812/2315 train_time:109794ms step_avg:60.59ms
step:1813/2315 train_time:109854ms step_avg:60.59ms
step:1814/2315 train_time:109916ms step_avg:60.59ms
step:1815/2315 train_time:109978ms step_avg:60.59ms
step:1816/2315 train_time:110039ms step_avg:60.59ms
step:1817/2315 train_time:110100ms step_avg:60.59ms
step:1818/2315 train_time:110162ms step_avg:60.60ms
step:1819/2315 train_time:110223ms step_avg:60.60ms
step:1820/2315 train_time:110284ms step_avg:60.60ms
step:1821/2315 train_time:110346ms step_avg:60.60ms
step:1822/2315 train_time:110408ms step_avg:60.60ms
step:1823/2315 train_time:110469ms step_avg:60.60ms
step:1824/2315 train_time:110530ms step_avg:60.60ms
step:1825/2315 train_time:110592ms step_avg:60.60ms
step:1826/2315 train_time:110654ms step_avg:60.60ms
step:1827/2315 train_time:110715ms step_avg:60.60ms
step:1828/2315 train_time:110775ms step_avg:60.60ms
step:1829/2315 train_time:110836ms step_avg:60.60ms
step:1830/2315 train_time:110897ms step_avg:60.60ms
step:1831/2315 train_time:110958ms step_avg:60.60ms
step:1832/2315 train_time:111019ms step_avg:60.60ms
step:1833/2315 train_time:111080ms step_avg:60.60ms
step:1834/2315 train_time:111142ms step_avg:60.60ms
step:1835/2315 train_time:111203ms step_avg:60.60ms
step:1836/2315 train_time:111264ms step_avg:60.60ms
step:1837/2315 train_time:111325ms step_avg:60.60ms
step:1838/2315 train_time:111387ms step_avg:60.60ms
step:1839/2315 train_time:111448ms step_avg:60.60ms
step:1840/2315 train_time:111510ms step_avg:60.60ms
step:1841/2315 train_time:111571ms step_avg:60.60ms
step:1842/2315 train_time:111633ms step_avg:60.60ms
step:1843/2315 train_time:111693ms step_avg:60.60ms
step:1844/2315 train_time:111755ms step_avg:60.60ms
step:1845/2315 train_time:111815ms step_avg:60.60ms
step:1846/2315 train_time:111876ms step_avg:60.60ms
step:1847/2315 train_time:111937ms step_avg:60.60ms
step:1848/2315 train_time:111998ms step_avg:60.60ms
step:1849/2315 train_time:112058ms step_avg:60.60ms
step:1850/2315 train_time:112120ms step_avg:60.61ms
step:1851/2315 train_time:112180ms step_avg:60.61ms
step:1852/2315 train_time:112242ms step_avg:60.61ms
step:1853/2315 train_time:112303ms step_avg:60.61ms
step:1854/2315 train_time:112365ms step_avg:60.61ms
step:1855/2315 train_time:112426ms step_avg:60.61ms
step:1856/2315 train_time:112488ms step_avg:60.61ms
step:1857/2315 train_time:112550ms step_avg:60.61ms
step:1858/2315 train_time:112611ms step_avg:60.61ms
step:1859/2315 train_time:112673ms step_avg:60.61ms
step:1860/2315 train_time:112734ms step_avg:60.61ms
step:1861/2315 train_time:112795ms step_avg:60.61ms
step:1862/2315 train_time:112856ms step_avg:60.61ms
step:1863/2315 train_time:112916ms step_avg:60.61ms
step:1864/2315 train_time:112977ms step_avg:60.61ms
step:1865/2315 train_time:113038ms step_avg:60.61ms
step:1866/2315 train_time:113099ms step_avg:60.61ms
step:1867/2315 train_time:113160ms step_avg:60.61ms
step:1868/2315 train_time:113220ms step_avg:60.61ms
step:1869/2315 train_time:113282ms step_avg:60.61ms
step:1870/2315 train_time:113343ms step_avg:60.61ms
step:1871/2315 train_time:113404ms step_avg:60.61ms
step:1872/2315 train_time:113466ms step_avg:60.61ms
step:1873/2315 train_time:113527ms step_avg:60.61ms
step:1874/2315 train_time:113589ms step_avg:60.61ms
step:1875/2315 train_time:113650ms step_avg:60.61ms
step:1876/2315 train_time:113712ms step_avg:60.61ms
step:1877/2315 train_time:113774ms step_avg:60.61ms
step:1878/2315 train_time:113835ms step_avg:60.62ms
step:1879/2315 train_time:113896ms step_avg:60.62ms
step:1880/2315 train_time:113957ms step_avg:60.62ms
step:1881/2315 train_time:114018ms step_avg:60.62ms
step:1882/2315 train_time:114079ms step_avg:60.62ms
step:1883/2315 train_time:114140ms step_avg:60.62ms
step:1884/2315 train_time:114201ms step_avg:60.62ms
step:1885/2315 train_time:114262ms step_avg:60.62ms
step:1886/2315 train_time:114323ms step_avg:60.62ms
step:1887/2315 train_time:114384ms step_avg:60.62ms
step:1888/2315 train_time:114446ms step_avg:60.62ms
step:1889/2315 train_time:114507ms step_avg:60.62ms
step:1890/2315 train_time:114569ms step_avg:60.62ms
step:1891/2315 train_time:114630ms step_avg:60.62ms
step:1892/2315 train_time:114693ms step_avg:60.62ms
step:1893/2315 train_time:114754ms step_avg:60.62ms
step:1894/2315 train_time:114815ms step_avg:60.62ms
step:1895/2315 train_time:114876ms step_avg:60.62ms
step:1896/2315 train_time:114937ms step_avg:60.62ms
step:1897/2315 train_time:114998ms step_avg:60.62ms
step:1898/2315 train_time:115060ms step_avg:60.62ms
step:1899/2315 train_time:115121ms step_avg:60.62ms
step:1900/2315 train_time:115183ms step_avg:60.62ms
step:1901/2315 train_time:115243ms step_avg:60.62ms
step:1902/2315 train_time:115305ms step_avg:60.62ms
step:1903/2315 train_time:115365ms step_avg:60.62ms
step:1904/2315 train_time:115427ms step_avg:60.62ms
step:1905/2315 train_time:115488ms step_avg:60.62ms
step:1906/2315 train_time:115550ms step_avg:60.62ms
step:1907/2315 train_time:115611ms step_avg:60.62ms
step:1908/2315 train_time:115673ms step_avg:60.63ms
step:1909/2315 train_time:115734ms step_avg:60.63ms
step:1910/2315 train_time:115795ms step_avg:60.63ms
step:1911/2315 train_time:115855ms step_avg:60.63ms
step:1912/2315 train_time:115917ms step_avg:60.63ms
step:1913/2315 train_time:115978ms step_avg:60.63ms
step:1914/2315 train_time:116039ms step_avg:60.63ms
step:1915/2315 train_time:116099ms step_avg:60.63ms
step:1916/2315 train_time:116160ms step_avg:60.63ms
step:1917/2315 train_time:116221ms step_avg:60.63ms
step:1918/2315 train_time:116282ms step_avg:60.63ms
step:1919/2315 train_time:116343ms step_avg:60.63ms
step:1920/2315 train_time:116404ms step_avg:60.63ms
step:1921/2315 train_time:116465ms step_avg:60.63ms
step:1922/2315 train_time:116527ms step_avg:60.63ms
step:1923/2315 train_time:116588ms step_avg:60.63ms
step:1924/2315 train_time:116651ms step_avg:60.63ms
step:1925/2315 train_time:116713ms step_avg:60.63ms
step:1926/2315 train_time:116775ms step_avg:60.63ms
step:1927/2315 train_time:116836ms step_avg:60.63ms
step:1928/2315 train_time:116897ms step_avg:60.63ms
step:1929/2315 train_time:116958ms step_avg:60.63ms
step:1930/2315 train_time:117019ms step_avg:60.63ms
step:1931/2315 train_time:117080ms step_avg:60.63ms
step:1932/2315 train_time:117141ms step_avg:60.63ms
step:1933/2315 train_time:117202ms step_avg:60.63ms
step:1934/2315 train_time:117263ms step_avg:60.63ms
step:1935/2315 train_time:117324ms step_avg:60.63ms
step:1936/2315 train_time:117385ms step_avg:60.63ms
step:1937/2315 train_time:117447ms step_avg:60.63ms
step:1938/2315 train_time:117508ms step_avg:60.63ms
step:1939/2315 train_time:117569ms step_avg:60.63ms
step:1940/2315 train_time:117631ms step_avg:60.63ms
step:1941/2315 train_time:117692ms step_avg:60.63ms
step:1942/2315 train_time:117754ms step_avg:60.64ms
step:1943/2315 train_time:117815ms step_avg:60.64ms
step:1944/2315 train_time:117877ms step_avg:60.64ms
step:1945/2315 train_time:117938ms step_avg:60.64ms
step:1946/2315 train_time:117998ms step_avg:60.64ms
step:1947/2315 train_time:118059ms step_avg:60.64ms
step:1948/2315 train_time:118120ms step_avg:60.64ms
step:1949/2315 train_time:118181ms step_avg:60.64ms
step:1950/2315 train_time:118242ms step_avg:60.64ms
step:1951/2315 train_time:118302ms step_avg:60.64ms
step:1952/2315 train_time:118364ms step_avg:60.64ms
step:1953/2315 train_time:118425ms step_avg:60.64ms
step:1954/2315 train_time:118487ms step_avg:60.64ms
step:1955/2315 train_time:118548ms step_avg:60.64ms
step:1956/2315 train_time:118610ms step_avg:60.64ms
step:1957/2315 train_time:118671ms step_avg:60.64ms
step:1958/2315 train_time:118733ms step_avg:60.64ms
step:1959/2315 train_time:118794ms step_avg:60.64ms
step:1960/2315 train_time:118855ms step_avg:60.64ms
step:1961/2315 train_time:118917ms step_avg:60.64ms
step:1962/2315 train_time:118979ms step_avg:60.64ms
step:1963/2315 train_time:119039ms step_avg:60.64ms
step:1964/2315 train_time:119101ms step_avg:60.64ms
step:1965/2315 train_time:119162ms step_avg:60.64ms
step:1966/2315 train_time:119223ms step_avg:60.64ms
step:1967/2315 train_time:119283ms step_avg:60.64ms
step:1968/2315 train_time:119345ms step_avg:60.64ms
step:1969/2315 train_time:119406ms step_avg:60.64ms
step:1970/2315 train_time:119467ms step_avg:60.64ms
step:1971/2315 train_time:119528ms step_avg:60.64ms
step:1972/2315 train_time:119590ms step_avg:60.64ms
step:1973/2315 train_time:119651ms step_avg:60.64ms
step:1974/2315 train_time:119713ms step_avg:60.64ms
step:1975/2315 train_time:119774ms step_avg:60.64ms
step:1976/2315 train_time:119835ms step_avg:60.65ms
step:1977/2315 train_time:119896ms step_avg:60.65ms
step:1978/2315 train_time:119958ms step_avg:60.65ms
step:1979/2315 train_time:120018ms step_avg:60.65ms
step:1980/2315 train_time:120079ms step_avg:60.65ms
step:1981/2315 train_time:120140ms step_avg:60.65ms
step:1982/2315 train_time:120201ms step_avg:60.65ms
step:1983/2315 train_time:120262ms step_avg:60.65ms
step:1984/2315 train_time:120323ms step_avg:60.65ms
step:1985/2315 train_time:120384ms step_avg:60.65ms
step:1986/2315 train_time:120446ms step_avg:60.65ms
step:1987/2315 train_time:120506ms step_avg:60.65ms
step:1988/2315 train_time:120568ms step_avg:60.65ms
step:1989/2315 train_time:120629ms step_avg:60.65ms
step:1990/2315 train_time:120691ms step_avg:60.65ms
step:1991/2315 train_time:120752ms step_avg:60.65ms
step:1992/2315 train_time:120814ms step_avg:60.65ms
step:1993/2315 train_time:120876ms step_avg:60.65ms
step:1994/2315 train_time:120937ms step_avg:60.65ms
step:1995/2315 train_time:120999ms step_avg:60.65ms
step:1996/2315 train_time:121061ms step_avg:60.65ms
step:1997/2315 train_time:121121ms step_avg:60.65ms
step:1998/2315 train_time:121182ms step_avg:60.65ms
step:1999/2315 train_time:121243ms step_avg:60.65ms
step:2000/2315 train_time:121304ms step_avg:60.65ms
step:2000/2315 val_loss:3.3316 train_time:121367ms step_avg:60.68ms
step:2001/2315 train_time:121385ms step_avg:60.66ms
step:2002/2315 train_time:121430ms step_avg:60.65ms
step:2003/2315 train_time:121495ms step_avg:60.66ms
step:2004/2315 train_time:121558ms step_avg:60.66ms
step:2005/2315 train_time:121619ms step_avg:60.66ms
step:2006/2315 train_time:121680ms step_avg:60.66ms
step:2007/2315 train_time:121741ms step_avg:60.66ms
step:2008/2315 train_time:121803ms step_avg:60.66ms
step:2009/2315 train_time:121863ms step_avg:60.66ms
step:2010/2315 train_time:121923ms step_avg:60.66ms
step:2011/2315 train_time:121984ms step_avg:60.66ms
step:2012/2315 train_time:122045ms step_avg:60.66ms
step:2013/2315 train_time:122104ms step_avg:60.66ms
step:2014/2315 train_time:122165ms step_avg:60.66ms
step:2015/2315 train_time:122225ms step_avg:60.66ms
step:2016/2315 train_time:122286ms step_avg:60.66ms
step:2017/2315 train_time:122347ms step_avg:60.66ms
step:2018/2315 train_time:122411ms step_avg:60.66ms
step:2019/2315 train_time:122472ms step_avg:60.66ms
step:2020/2315 train_time:122534ms step_avg:60.66ms
step:2021/2315 train_time:122596ms step_avg:60.66ms
step:2022/2315 train_time:122658ms step_avg:60.66ms
step:2023/2315 train_time:122719ms step_avg:60.66ms
step:2024/2315 train_time:122781ms step_avg:60.66ms
step:2025/2315 train_time:122842ms step_avg:60.66ms
step:2026/2315 train_time:122903ms step_avg:60.66ms
step:2027/2315 train_time:122963ms step_avg:60.66ms
step:2028/2315 train_time:123023ms step_avg:60.66ms
step:2029/2315 train_time:123084ms step_avg:60.66ms
step:2030/2315 train_time:123144ms step_avg:60.66ms
step:2031/2315 train_time:123204ms step_avg:60.66ms
step:2032/2315 train_time:123265ms step_avg:60.66ms
step:2033/2315 train_time:123326ms step_avg:60.66ms
step:2034/2315 train_time:123388ms step_avg:60.66ms
step:2035/2315 train_time:123449ms step_avg:60.66ms
step:2036/2315 train_time:123511ms step_avg:60.66ms
step:2037/2315 train_time:123573ms step_avg:60.66ms
step:2038/2315 train_time:123635ms step_avg:60.66ms
step:2039/2315 train_time:123697ms step_avg:60.67ms
step:2040/2315 train_time:123759ms step_avg:60.67ms
step:2041/2315 train_time:123821ms step_avg:60.67ms
step:2042/2315 train_time:123882ms step_avg:60.67ms
step:2043/2315 train_time:123943ms step_avg:60.67ms
step:2044/2315 train_time:124004ms step_avg:60.67ms
step:2045/2315 train_time:124065ms step_avg:60.67ms
step:2046/2315 train_time:124126ms step_avg:60.67ms
step:2047/2315 train_time:124187ms step_avg:60.67ms
step:2048/2315 train_time:124248ms step_avg:60.67ms
step:2049/2315 train_time:124309ms step_avg:60.67ms
step:2050/2315 train_time:124370ms step_avg:60.67ms
step:2051/2315 train_time:124431ms step_avg:60.67ms
step:2052/2315 train_time:124494ms step_avg:60.67ms
step:2053/2315 train_time:124555ms step_avg:60.67ms
step:2054/2315 train_time:124616ms step_avg:60.67ms
step:2055/2315 train_time:124677ms step_avg:60.67ms
step:2056/2315 train_time:124740ms step_avg:60.67ms
step:2057/2315 train_time:124801ms step_avg:60.67ms
step:2058/2315 train_time:124862ms step_avg:60.67ms
step:2059/2315 train_time:124924ms step_avg:60.67ms
step:2060/2315 train_time:124985ms step_avg:60.67ms
step:2061/2315 train_time:125046ms step_avg:60.67ms
step:2062/2315 train_time:125107ms step_avg:60.67ms
step:2063/2315 train_time:125168ms step_avg:60.67ms
step:2064/2315 train_time:125229ms step_avg:60.67ms
step:2065/2315 train_time:125290ms step_avg:60.67ms
step:2066/2315 train_time:125351ms step_avg:60.67ms
step:2067/2315 train_time:125412ms step_avg:60.67ms
step:2068/2315 train_time:125474ms step_avg:60.67ms
step:2069/2315 train_time:125536ms step_avg:60.67ms
step:2070/2315 train_time:125597ms step_avg:60.67ms
step:2071/2315 train_time:125658ms step_avg:60.68ms
step:2072/2315 train_time:125721ms step_avg:60.68ms
step:2073/2315 train_time:125782ms step_avg:60.68ms
step:2074/2315 train_time:125843ms step_avg:60.68ms
step:2075/2315 train_time:125904ms step_avg:60.68ms
step:2076/2315 train_time:125966ms step_avg:60.68ms
step:2077/2315 train_time:126026ms step_avg:60.68ms
step:2078/2315 train_time:126088ms step_avg:60.68ms
step:2079/2315 train_time:126148ms step_avg:60.68ms
step:2080/2315 train_time:126209ms step_avg:60.68ms
step:2081/2315 train_time:126269ms step_avg:60.68ms
step:2082/2315 train_time:126331ms step_avg:60.68ms
step:2083/2315 train_time:126391ms step_avg:60.68ms
step:2084/2315 train_time:126453ms step_avg:60.68ms
step:2085/2315 train_time:126513ms step_avg:60.68ms
step:2086/2315 train_time:126575ms step_avg:60.68ms
step:2087/2315 train_time:126636ms step_avg:60.68ms
step:2088/2315 train_time:126698ms step_avg:60.68ms
step:2089/2315 train_time:126759ms step_avg:60.68ms
step:2090/2315 train_time:126822ms step_avg:60.68ms
step:2091/2315 train_time:126883ms step_avg:60.68ms
step:2092/2315 train_time:126945ms step_avg:60.68ms
step:2093/2315 train_time:127006ms step_avg:60.68ms
step:2094/2315 train_time:127067ms step_avg:60.68ms
step:2095/2315 train_time:127128ms step_avg:60.68ms
step:2096/2315 train_time:127190ms step_avg:60.68ms
step:2097/2315 train_time:127250ms step_avg:60.68ms
step:2098/2315 train_time:127311ms step_avg:60.68ms
step:2099/2315 train_time:127372ms step_avg:60.68ms
step:2100/2315 train_time:127433ms step_avg:60.68ms
step:2101/2315 train_time:127495ms step_avg:60.68ms
step:2102/2315 train_time:127556ms step_avg:60.68ms
step:2103/2315 train_time:127617ms step_avg:60.68ms
step:2104/2315 train_time:127680ms step_avg:60.68ms
step:2105/2315 train_time:127741ms step_avg:60.68ms
step:2106/2315 train_time:127803ms step_avg:60.69ms
step:2107/2315 train_time:127864ms step_avg:60.69ms
step:2108/2315 train_time:127925ms step_avg:60.69ms
step:2109/2315 train_time:127987ms step_avg:60.69ms
step:2110/2315 train_time:128048ms step_avg:60.69ms
step:2111/2315 train_time:128108ms step_avg:60.69ms
step:2112/2315 train_time:128170ms step_avg:60.69ms
step:2113/2315 train_time:128230ms step_avg:60.69ms
step:2114/2315 train_time:128291ms step_avg:60.69ms
step:2115/2315 train_time:128352ms step_avg:60.69ms
step:2116/2315 train_time:128413ms step_avg:60.69ms
step:2117/2315 train_time:128474ms step_avg:60.69ms
step:2118/2315 train_time:128536ms step_avg:60.69ms
step:2119/2315 train_time:128597ms step_avg:60.69ms
step:2120/2315 train_time:128659ms step_avg:60.69ms
step:2121/2315 train_time:128720ms step_avg:60.69ms
step:2122/2315 train_time:128782ms step_avg:60.69ms
step:2123/2315 train_time:128843ms step_avg:60.69ms
step:2124/2315 train_time:128905ms step_avg:60.69ms
step:2125/2315 train_time:128966ms step_avg:60.69ms
step:2126/2315 train_time:129027ms step_avg:60.69ms
step:2127/2315 train_time:129088ms step_avg:60.69ms
step:2128/2315 train_time:129150ms step_avg:60.69ms
step:2129/2315 train_time:129210ms step_avg:60.69ms
step:2130/2315 train_time:129272ms step_avg:60.69ms
step:2131/2315 train_time:129333ms step_avg:60.69ms
step:2132/2315 train_time:129394ms step_avg:60.69ms
step:2133/2315 train_time:129456ms step_avg:60.69ms
step:2134/2315 train_time:129518ms step_avg:60.69ms
step:2135/2315 train_time:129579ms step_avg:60.69ms
step:2136/2315 train_time:129641ms step_avg:60.69ms
step:2137/2315 train_time:129702ms step_avg:60.69ms
step:2138/2315 train_time:129764ms step_avg:60.69ms
step:2139/2315 train_time:129824ms step_avg:60.69ms
step:2140/2315 train_time:129886ms step_avg:60.69ms
step:2141/2315 train_time:129947ms step_avg:60.69ms
step:2142/2315 train_time:130009ms step_avg:60.70ms
step:2143/2315 train_time:130069ms step_avg:60.69ms
step:2144/2315 train_time:130130ms step_avg:60.70ms
step:2145/2315 train_time:130191ms step_avg:60.69ms
step:2146/2315 train_time:130252ms step_avg:60.70ms
step:2147/2315 train_time:130312ms step_avg:60.70ms
step:2148/2315 train_time:130375ms step_avg:60.70ms
step:2149/2315 train_time:130436ms step_avg:60.70ms
step:2150/2315 train_time:130498ms step_avg:60.70ms
step:2151/2315 train_time:130559ms step_avg:60.70ms
step:2152/2315 train_time:130621ms step_avg:60.70ms
step:2153/2315 train_time:130682ms step_avg:60.70ms
step:2154/2315 train_time:130744ms step_avg:60.70ms
step:2155/2315 train_time:130805ms step_avg:60.70ms
step:2156/2315 train_time:130867ms step_avg:60.70ms
step:2157/2315 train_time:130928ms step_avg:60.70ms
step:2158/2315 train_time:130990ms step_avg:60.70ms
step:2159/2315 train_time:131050ms step_avg:60.70ms
step:2160/2315 train_time:131112ms step_avg:60.70ms
step:2161/2315 train_time:131172ms step_avg:60.70ms
step:2162/2315 train_time:131233ms step_avg:60.70ms
step:2163/2315 train_time:131294ms step_avg:60.70ms
step:2164/2315 train_time:131356ms step_avg:60.70ms
step:2165/2315 train_time:131418ms step_avg:60.70ms
step:2166/2315 train_time:131479ms step_avg:60.70ms
step:2167/2315 train_time:131541ms step_avg:60.70ms
step:2168/2315 train_time:131602ms step_avg:60.70ms
step:2169/2315 train_time:131663ms step_avg:60.70ms
step:2170/2315 train_time:131725ms step_avg:60.70ms
step:2171/2315 train_time:131787ms step_avg:60.70ms
step:2172/2315 train_time:131848ms step_avg:60.70ms
step:2173/2315 train_time:131909ms step_avg:60.70ms
step:2174/2315 train_time:131971ms step_avg:60.70ms
step:2175/2315 train_time:132033ms step_avg:60.70ms
step:2176/2315 train_time:132094ms step_avg:60.71ms
step:2177/2315 train_time:132155ms step_avg:60.70ms
step:2178/2315 train_time:132216ms step_avg:60.71ms
step:2179/2315 train_time:132277ms step_avg:60.71ms
step:2180/2315 train_time:132338ms step_avg:60.71ms
step:2181/2315 train_time:132399ms step_avg:60.71ms
step:2182/2315 train_time:132461ms step_avg:60.71ms
step:2183/2315 train_time:132523ms step_avg:60.71ms
step:2184/2315 train_time:132585ms step_avg:60.71ms
step:2185/2315 train_time:132645ms step_avg:60.71ms
step:2186/2315 train_time:132707ms step_avg:60.71ms
step:2187/2315 train_time:132768ms step_avg:60.71ms
step:2188/2315 train_time:132830ms step_avg:60.71ms
step:2189/2315 train_time:132890ms step_avg:60.71ms
step:2190/2315 train_time:132952ms step_avg:60.71ms
step:2191/2315 train_time:133013ms step_avg:60.71ms
step:2192/2315 train_time:133074ms step_avg:60.71ms
step:2193/2315 train_time:133135ms step_avg:60.71ms
step:2194/2315 train_time:133197ms step_avg:60.71ms
step:2195/2315 train_time:133258ms step_avg:60.71ms
step:2196/2315 train_time:133320ms step_avg:60.71ms
step:2197/2315 train_time:133381ms step_avg:60.71ms
step:2198/2315 train_time:133443ms step_avg:60.71ms
step:2199/2315 train_time:133504ms step_avg:60.71ms
step:2200/2315 train_time:133565ms step_avg:60.71ms
step:2201/2315 train_time:133626ms step_avg:60.71ms
step:2202/2315 train_time:133688ms step_avg:60.71ms
step:2203/2315 train_time:133748ms step_avg:60.71ms
step:2204/2315 train_time:133810ms step_avg:60.71ms
step:2205/2315 train_time:133870ms step_avg:60.71ms
step:2206/2315 train_time:133932ms step_avg:60.71ms
step:2207/2315 train_time:133993ms step_avg:60.71ms
step:2208/2315 train_time:134055ms step_avg:60.71ms
step:2209/2315 train_time:134116ms step_avg:60.71ms
step:2210/2315 train_time:134178ms step_avg:60.71ms
step:2211/2315 train_time:134238ms step_avg:60.71ms
step:2212/2315 train_time:134300ms step_avg:60.71ms
step:2213/2315 train_time:134362ms step_avg:60.71ms
step:2214/2315 train_time:134423ms step_avg:60.72ms
step:2215/2315 train_time:134484ms step_avg:60.72ms
step:2216/2315 train_time:134546ms step_avg:60.72ms
step:2217/2315 train_time:134606ms step_avg:60.72ms
step:2218/2315 train_time:134668ms step_avg:60.72ms
step:2219/2315 train_time:134728ms step_avg:60.72ms
step:2220/2315 train_time:134790ms step_avg:60.72ms
step:2221/2315 train_time:134851ms step_avg:60.72ms
step:2222/2315 train_time:134913ms step_avg:60.72ms
step:2223/2315 train_time:134974ms step_avg:60.72ms
step:2224/2315 train_time:135037ms step_avg:60.72ms
step:2225/2315 train_time:135098ms step_avg:60.72ms
step:2226/2315 train_time:135160ms step_avg:60.72ms
step:2227/2315 train_time:135221ms step_avg:60.72ms
step:2228/2315 train_time:135283ms step_avg:60.72ms
step:2229/2315 train_time:135343ms step_avg:60.72ms
step:2230/2315 train_time:135405ms step_avg:60.72ms
step:2231/2315 train_time:135466ms step_avg:60.72ms
step:2232/2315 train_time:135527ms step_avg:60.72ms
step:2233/2315 train_time:135588ms step_avg:60.72ms
step:2234/2315 train_time:135649ms step_avg:60.72ms
step:2235/2315 train_time:135710ms step_avg:60.72ms
step:2236/2315 train_time:135772ms step_avg:60.72ms
step:2237/2315 train_time:135832ms step_avg:60.72ms
step:2238/2315 train_time:135894ms step_avg:60.72ms
step:2239/2315 train_time:135954ms step_avg:60.72ms
step:2240/2315 train_time:136016ms step_avg:60.72ms
step:2241/2315 train_time:136077ms step_avg:60.72ms
step:2242/2315 train_time:136139ms step_avg:60.72ms
step:2243/2315 train_time:136201ms step_avg:60.72ms
step:2244/2315 train_time:136262ms step_avg:60.72ms
step:2245/2315 train_time:136323ms step_avg:60.72ms
step:2246/2315 train_time:136385ms step_avg:60.72ms
step:2247/2315 train_time:136446ms step_avg:60.72ms
step:2248/2315 train_time:136507ms step_avg:60.72ms
step:2249/2315 train_time:136568ms step_avg:60.72ms
step:2250/2315 train_time:136629ms step_avg:60.72ms
step:2250/2315 val_loss:3.2921 train_time:136691ms step_avg:60.75ms
step:2251/2315 train_time:136709ms step_avg:60.73ms
step:2252/2315 train_time:136755ms step_avg:60.73ms
step:2253/2315 train_time:136817ms step_avg:60.73ms
step:2254/2315 train_time:136879ms step_avg:60.73ms
step:2255/2315 train_time:136940ms step_avg:60.73ms
step:2256/2315 train_time:137002ms step_avg:60.73ms
step:2257/2315 train_time:137063ms step_avg:60.73ms
step:2258/2315 train_time:137123ms step_avg:60.73ms
step:2259/2315 train_time:137183ms step_avg:60.73ms
step:2260/2315 train_time:137244ms step_avg:60.73ms
step:2261/2315 train_time:137305ms step_avg:60.73ms
step:2262/2315 train_time:137366ms step_avg:60.73ms
step:2263/2315 train_time:137428ms step_avg:60.73ms
step:2264/2315 train_time:137490ms step_avg:60.73ms
step:2265/2315 train_time:137550ms step_avg:60.73ms
step:2266/2315 train_time:137612ms step_avg:60.73ms
step:2267/2315 train_time:137676ms step_avg:60.73ms
step:2268/2315 train_time:137739ms step_avg:60.73ms
step:2269/2315 train_time:137801ms step_avg:60.73ms
step:2270/2315 train_time:137864ms step_avg:60.73ms
step:2271/2315 train_time:137926ms step_avg:60.73ms
step:2272/2315 train_time:137987ms step_avg:60.73ms
step:2273/2315 train_time:138048ms step_avg:60.73ms
step:2274/2315 train_time:138109ms step_avg:60.73ms
step:2275/2315 train_time:138168ms step_avg:60.73ms
step:2276/2315 train_time:138229ms step_avg:60.73ms
step:2277/2315 train_time:138289ms step_avg:60.73ms
step:2278/2315 train_time:138349ms step_avg:60.73ms
step:2279/2315 train_time:138410ms step_avg:60.73ms
step:2280/2315 train_time:138471ms step_avg:60.73ms
step:2281/2315 train_time:138531ms step_avg:60.73ms
step:2282/2315 train_time:138593ms step_avg:60.73ms
step:2283/2315 train_time:138654ms step_avg:60.73ms
step:2284/2315 train_time:138716ms step_avg:60.73ms
step:2285/2315 train_time:138777ms step_avg:60.73ms
step:2286/2315 train_time:138839ms step_avg:60.73ms
step:2287/2315 train_time:138901ms step_avg:60.74ms
step:2288/2315 train_time:138964ms step_avg:60.74ms
step:2289/2315 train_time:139025ms step_avg:60.74ms
step:2290/2315 train_time:139087ms step_avg:60.74ms
step:2291/2315 train_time:139147ms step_avg:60.74ms
step:2292/2315 train_time:139208ms step_avg:60.74ms
step:2293/2315 train_time:139269ms step_avg:60.74ms
step:2294/2315 train_time:139330ms step_avg:60.74ms
step:2295/2315 train_time:139390ms step_avg:60.74ms
step:2296/2315 train_time:139451ms step_avg:60.74ms
step:2297/2315 train_time:139512ms step_avg:60.74ms
step:2298/2315 train_time:139573ms step_avg:60.74ms
step:2299/2315 train_time:139634ms step_avg:60.74ms
step:2300/2315 train_time:139696ms step_avg:60.74ms
step:2301/2315 train_time:139758ms step_avg:60.74ms
step:2302/2315 train_time:139820ms step_avg:60.74ms
step:2303/2315 train_time:139881ms step_avg:60.74ms
step:2304/2315 train_time:139943ms step_avg:60.74ms
step:2305/2315 train_time:140004ms step_avg:60.74ms
step:2306/2315 train_time:140066ms step_avg:60.74ms
step:2307/2315 train_time:140127ms step_avg:60.74ms
step:2308/2315 train_time:140188ms step_avg:60.74ms
step:2309/2315 train_time:140249ms step_avg:60.74ms
step:2310/2315 train_time:140309ms step_avg:60.74ms
step:2311/2315 train_time:140370ms step_avg:60.74ms
step:2312/2315 train_time:140431ms step_avg:60.74ms
step:2313/2315 train_time:140492ms step_avg:60.74ms
step:2314/2315 train_time:140553ms step_avg:60.74ms
step:2315/2315 train_time:140615ms step_avg:60.74ms
step:2315/2315 val_loss:3.2793 train_time:140677ms step_avg:60.77ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

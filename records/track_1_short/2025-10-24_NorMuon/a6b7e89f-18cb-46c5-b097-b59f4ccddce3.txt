import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]
# polar_express_coeffs = [
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
# ]



@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    # state["second_momentum_buffer"] = torch.zeros_like(grad[...,0:1]) if param.size(-2) >= param.size(-1) else torch.zeros_like(grad[...,0:1,:])
                    # state["second_momentum_buffer"] = torch.zeros((*grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((*grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)
            
            ###################################
            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))
            ####################################


            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> (-1.5)  0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Oct 22 2025, 02:20:45) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251022+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 24 05:51:18 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000001:00:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000002:00:00.0 Off |                    0 |
| N/A   34C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000003:00:00.0 Off |                    0 |
| N/A   30C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000008:00:00.0 Off |                    0 |
| N/A   30C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000009:00:00.0 Off |                    0 |
| N/A   28C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   0000000A:00:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2315 train_time:86ms step_avg:86.24ms
step:2/2315 train_time:184ms step_avg:92.00ms
step:3/2315 train_time:205ms step_avg:68.34ms
step:4/2315 train_time:242ms step_avg:60.42ms
step:5/2315 train_time:300ms step_avg:60.07ms
step:6/2315 train_time:360ms step_avg:60.08ms
step:7/2315 train_time:420ms step_avg:59.96ms
step:8/2315 train_time:480ms step_avg:59.95ms
step:9/2315 train_time:539ms step_avg:59.91ms
step:10/2315 train_time:599ms step_avg:59.93ms
step:11/2315 train_time:660ms step_avg:59.99ms
step:12/2315 train_time:720ms step_avg:59.97ms
step:13/2315 train_time:780ms step_avg:59.98ms
step:14/2315 train_time:840ms step_avg:60.02ms
step:15/2315 train_time:901ms step_avg:60.06ms
step:16/2315 train_time:961ms step_avg:60.08ms
step:17/2315 train_time:1025ms step_avg:60.30ms
step:18/2315 train_time:1087ms step_avg:60.40ms
step:19/2315 train_time:1151ms step_avg:60.58ms
step:20/2315 train_time:1212ms step_avg:60.61ms
step:21/2315 train_time:1273ms step_avg:60.63ms
step:22/2315 train_time:1334ms step_avg:60.61ms
step:23/2315 train_time:1393ms step_avg:60.57ms
step:24/2315 train_time:1453ms step_avg:60.54ms
step:25/2315 train_time:1513ms step_avg:60.51ms
step:26/2315 train_time:1572ms step_avg:60.48ms
step:27/2315 train_time:1633ms step_avg:60.47ms
step:28/2315 train_time:1693ms step_avg:60.45ms
step:29/2315 train_time:1752ms step_avg:60.43ms
step:30/2315 train_time:1813ms step_avg:60.45ms
step:31/2315 train_time:1874ms step_avg:60.45ms
step:32/2315 train_time:1934ms step_avg:60.44ms
step:33/2315 train_time:1994ms step_avg:60.44ms
step:34/2315 train_time:2055ms step_avg:60.45ms
step:35/2315 train_time:2118ms step_avg:60.50ms
step:36/2315 train_time:2179ms step_avg:60.52ms
step:37/2315 train_time:2240ms step_avg:60.54ms
step:38/2315 train_time:2301ms step_avg:60.54ms
step:39/2315 train_time:2362ms step_avg:60.56ms
step:40/2315 train_time:2422ms step_avg:60.55ms
step:41/2315 train_time:2484ms step_avg:60.58ms
step:42/2315 train_time:2544ms step_avg:60.57ms
step:43/2315 train_time:2604ms step_avg:60.57ms
step:44/2315 train_time:2664ms step_avg:60.55ms
step:45/2315 train_time:2724ms step_avg:60.54ms
step:46/2315 train_time:2785ms step_avg:60.54ms
step:47/2315 train_time:2846ms step_avg:60.55ms
step:48/2315 train_time:2906ms step_avg:60.54ms
step:49/2315 train_time:2967ms step_avg:60.55ms
step:50/2315 train_time:3028ms step_avg:60.56ms
step:51/2315 train_time:3088ms step_avg:60.55ms
step:52/2315 train_time:3148ms step_avg:60.55ms
step:53/2315 train_time:3209ms step_avg:60.54ms
step:54/2315 train_time:3269ms step_avg:60.54ms
step:55/2315 train_time:3330ms step_avg:60.55ms
step:56/2315 train_time:3390ms step_avg:60.54ms
step:57/2315 train_time:3450ms step_avg:60.53ms
step:58/2315 train_time:3510ms step_avg:60.52ms
step:59/2315 train_time:3570ms step_avg:60.51ms
step:60/2315 train_time:3630ms step_avg:60.50ms
step:61/2315 train_time:3690ms step_avg:60.49ms
step:62/2315 train_time:3750ms step_avg:60.48ms
step:63/2315 train_time:3810ms step_avg:60.47ms
step:64/2315 train_time:3870ms step_avg:60.46ms
step:65/2315 train_time:3929ms step_avg:60.45ms
step:66/2315 train_time:3989ms step_avg:60.45ms
step:67/2315 train_time:4049ms step_avg:60.43ms
step:68/2315 train_time:4109ms step_avg:60.43ms
step:69/2315 train_time:4170ms step_avg:60.43ms
step:70/2315 train_time:4230ms step_avg:60.43ms
step:71/2315 train_time:4290ms step_avg:60.42ms
step:72/2315 train_time:4350ms step_avg:60.42ms
step:73/2315 train_time:4410ms step_avg:60.41ms
step:74/2315 train_time:4470ms step_avg:60.41ms
step:75/2315 train_time:4531ms step_avg:60.41ms
step:76/2315 train_time:4590ms step_avg:60.40ms
step:77/2315 train_time:4650ms step_avg:60.39ms
step:78/2315 train_time:4710ms step_avg:60.39ms
step:79/2315 train_time:4770ms step_avg:60.38ms
step:80/2315 train_time:4830ms step_avg:60.37ms
step:81/2315 train_time:4889ms step_avg:60.36ms
step:82/2315 train_time:4949ms step_avg:60.35ms
step:83/2315 train_time:5009ms step_avg:60.34ms
step:84/2315 train_time:5069ms step_avg:60.34ms
step:85/2315 train_time:5129ms step_avg:60.34ms
step:86/2315 train_time:5189ms step_avg:60.34ms
step:87/2315 train_time:5249ms step_avg:60.33ms
step:88/2315 train_time:5308ms step_avg:60.32ms
step:89/2315 train_time:5368ms step_avg:60.32ms
step:90/2315 train_time:5428ms step_avg:60.32ms
step:91/2315 train_time:5489ms step_avg:60.31ms
step:92/2315 train_time:5549ms step_avg:60.31ms
step:93/2315 train_time:5609ms step_avg:60.31ms
step:94/2315 train_time:5669ms step_avg:60.30ms
step:95/2315 train_time:5729ms step_avg:60.31ms
step:96/2315 train_time:5789ms step_avg:60.30ms
step:97/2315 train_time:5848ms step_avg:60.29ms
step:98/2315 train_time:5908ms step_avg:60.28ms
step:99/2315 train_time:5968ms step_avg:60.28ms
step:100/2315 train_time:6027ms step_avg:60.27ms
step:101/2315 train_time:6087ms step_avg:60.27ms
step:102/2315 train_time:6148ms step_avg:60.27ms
step:103/2315 train_time:6208ms step_avg:60.27ms
step:104/2315 train_time:6268ms step_avg:60.27ms
step:105/2315 train_time:6328ms step_avg:60.27ms
step:106/2315 train_time:6389ms step_avg:60.27ms
step:107/2315 train_time:6449ms step_avg:60.27ms
step:108/2315 train_time:6509ms step_avg:60.27ms
step:109/2315 train_time:6569ms step_avg:60.26ms
step:110/2315 train_time:6629ms step_avg:60.26ms
step:111/2315 train_time:6690ms step_avg:60.27ms
step:112/2315 train_time:6750ms step_avg:60.26ms
step:113/2315 train_time:6809ms step_avg:60.26ms
step:114/2315 train_time:6868ms step_avg:60.25ms
step:115/2315 train_time:6928ms step_avg:60.24ms
step:116/2315 train_time:6988ms step_avg:60.24ms
step:117/2315 train_time:7047ms step_avg:60.23ms
step:118/2315 train_time:7107ms step_avg:60.23ms
step:119/2315 train_time:7167ms step_avg:60.23ms
step:120/2315 train_time:7227ms step_avg:60.22ms
step:121/2315 train_time:7287ms step_avg:60.22ms
step:122/2315 train_time:7347ms step_avg:60.22ms
step:123/2315 train_time:7408ms step_avg:60.22ms
step:124/2315 train_time:7468ms step_avg:60.22ms
step:125/2315 train_time:7528ms step_avg:60.22ms
step:126/2315 train_time:7587ms step_avg:60.22ms
step:127/2315 train_time:7647ms step_avg:60.22ms
step:128/2315 train_time:7707ms step_avg:60.21ms
step:129/2315 train_time:7767ms step_avg:60.21ms
step:130/2315 train_time:7827ms step_avg:60.21ms
step:131/2315 train_time:7887ms step_avg:60.21ms
step:132/2315 train_time:7947ms step_avg:60.20ms
step:133/2315 train_time:8007ms step_avg:60.20ms
step:134/2315 train_time:8067ms step_avg:60.20ms
step:135/2315 train_time:8127ms step_avg:60.20ms
step:136/2315 train_time:8186ms step_avg:60.19ms
step:137/2315 train_time:8246ms step_avg:60.19ms
step:138/2315 train_time:8306ms step_avg:60.19ms
step:139/2315 train_time:8366ms step_avg:60.19ms
step:140/2315 train_time:8426ms step_avg:60.18ms
step:141/2315 train_time:8486ms step_avg:60.18ms
step:142/2315 train_time:8546ms step_avg:60.18ms
step:143/2315 train_time:8606ms step_avg:60.18ms
step:144/2315 train_time:8666ms step_avg:60.18ms
step:145/2315 train_time:8726ms step_avg:60.18ms
step:146/2315 train_time:8786ms step_avg:60.18ms
step:147/2315 train_time:8847ms step_avg:60.18ms
step:148/2315 train_time:8907ms step_avg:60.18ms
step:149/2315 train_time:8967ms step_avg:60.18ms
step:150/2315 train_time:9027ms step_avg:60.18ms
step:151/2315 train_time:9086ms step_avg:60.17ms
step:152/2315 train_time:9147ms step_avg:60.17ms
step:153/2315 train_time:9206ms step_avg:60.17ms
step:154/2315 train_time:9266ms step_avg:60.17ms
step:155/2315 train_time:9326ms step_avg:60.17ms
step:156/2315 train_time:9386ms step_avg:60.17ms
step:157/2315 train_time:9446ms step_avg:60.17ms
step:158/2315 train_time:9506ms step_avg:60.17ms
step:159/2315 train_time:9566ms step_avg:60.16ms
step:160/2315 train_time:9626ms step_avg:60.16ms
step:161/2315 train_time:9687ms step_avg:60.17ms
step:162/2315 train_time:9746ms step_avg:60.16ms
step:163/2315 train_time:9807ms step_avg:60.17ms
step:164/2315 train_time:9867ms step_avg:60.16ms
step:165/2315 train_time:9926ms step_avg:60.16ms
step:166/2315 train_time:9986ms step_avg:60.16ms
step:167/2315 train_time:10047ms step_avg:60.16ms
step:168/2315 train_time:10106ms step_avg:60.16ms
step:169/2315 train_time:10166ms step_avg:60.15ms
step:170/2315 train_time:10226ms step_avg:60.16ms
step:171/2315 train_time:10287ms step_avg:60.16ms
step:172/2315 train_time:10346ms step_avg:60.15ms
step:173/2315 train_time:10406ms step_avg:60.15ms
step:174/2315 train_time:10467ms step_avg:60.15ms
step:175/2315 train_time:10527ms step_avg:60.15ms
step:176/2315 train_time:10587ms step_avg:60.15ms
step:177/2315 train_time:10647ms step_avg:60.16ms
step:178/2315 train_time:10708ms step_avg:60.16ms
step:179/2315 train_time:10767ms step_avg:60.15ms
step:180/2315 train_time:10827ms step_avg:60.15ms
step:181/2315 train_time:10887ms step_avg:60.15ms
step:182/2315 train_time:10946ms step_avg:60.14ms
step:183/2315 train_time:11007ms step_avg:60.15ms
step:184/2315 train_time:11066ms step_avg:60.14ms
step:185/2315 train_time:11126ms step_avg:60.14ms
step:186/2315 train_time:11186ms step_avg:60.14ms
step:187/2315 train_time:11247ms step_avg:60.14ms
step:188/2315 train_time:11307ms step_avg:60.14ms
step:189/2315 train_time:11367ms step_avg:60.14ms
step:190/2315 train_time:11427ms step_avg:60.14ms
step:191/2315 train_time:11487ms step_avg:60.14ms
step:192/2315 train_time:11546ms step_avg:60.14ms
step:193/2315 train_time:11606ms step_avg:60.14ms
step:194/2315 train_time:11666ms step_avg:60.13ms
step:195/2315 train_time:11726ms step_avg:60.13ms
step:196/2315 train_time:11786ms step_avg:60.13ms
step:197/2315 train_time:11846ms step_avg:60.13ms
step:198/2315 train_time:11906ms step_avg:60.13ms
step:199/2315 train_time:11967ms step_avg:60.13ms
step:200/2315 train_time:12027ms step_avg:60.13ms
step:201/2315 train_time:12087ms step_avg:60.13ms
step:202/2315 train_time:12147ms step_avg:60.13ms
step:203/2315 train_time:12207ms step_avg:60.13ms
step:204/2315 train_time:12266ms step_avg:60.13ms
step:205/2315 train_time:12326ms step_avg:60.13ms
step:206/2315 train_time:12386ms step_avg:60.13ms
step:207/2315 train_time:12446ms step_avg:60.13ms
step:208/2315 train_time:12506ms step_avg:60.13ms
step:209/2315 train_time:12566ms step_avg:60.13ms
step:210/2315 train_time:12627ms step_avg:60.13ms
step:211/2315 train_time:12687ms step_avg:60.13ms
step:212/2315 train_time:12747ms step_avg:60.13ms
step:213/2315 train_time:12807ms step_avg:60.13ms
step:214/2315 train_time:12867ms step_avg:60.12ms
step:215/2315 train_time:12927ms step_avg:60.12ms
step:216/2315 train_time:12986ms step_avg:60.12ms
step:217/2315 train_time:13046ms step_avg:60.12ms
step:218/2315 train_time:13106ms step_avg:60.12ms
step:219/2315 train_time:13166ms step_avg:60.12ms
step:220/2315 train_time:13225ms step_avg:60.11ms
step:221/2315 train_time:13285ms step_avg:60.11ms
step:222/2315 train_time:13345ms step_avg:60.11ms
step:223/2315 train_time:13405ms step_avg:60.11ms
step:224/2315 train_time:13465ms step_avg:60.11ms
step:225/2315 train_time:13526ms step_avg:60.11ms
step:226/2315 train_time:13586ms step_avg:60.11ms
step:227/2315 train_time:13646ms step_avg:60.11ms
step:228/2315 train_time:13706ms step_avg:60.11ms
step:229/2315 train_time:13766ms step_avg:60.11ms
step:230/2315 train_time:13826ms step_avg:60.11ms
step:231/2315 train_time:13886ms step_avg:60.11ms
step:232/2315 train_time:13945ms step_avg:60.11ms
step:233/2315 train_time:14005ms step_avg:60.11ms
step:234/2315 train_time:14065ms step_avg:60.11ms
step:235/2315 train_time:14126ms step_avg:60.11ms
step:236/2315 train_time:14186ms step_avg:60.11ms
step:237/2315 train_time:14246ms step_avg:60.11ms
step:238/2315 train_time:14305ms step_avg:60.11ms
step:239/2315 train_time:14365ms step_avg:60.10ms
step:240/2315 train_time:14425ms step_avg:60.10ms
step:241/2315 train_time:14485ms step_avg:60.10ms
step:242/2315 train_time:14545ms step_avg:60.10ms
step:243/2315 train_time:14605ms step_avg:60.10ms
step:244/2315 train_time:14665ms step_avg:60.10ms
step:245/2315 train_time:14725ms step_avg:60.10ms
step:246/2315 train_time:14786ms step_avg:60.11ms
step:247/2315 train_time:14846ms step_avg:60.11ms
step:248/2315 train_time:14906ms step_avg:60.11ms
step:249/2315 train_time:14966ms step_avg:60.10ms
step:250/2315 train_time:15025ms step_avg:60.10ms
step:250/2315 val_loss:4.0793 train_time:15087ms step_avg:60.35ms
step:251/2315 train_time:15108ms step_avg:60.19ms
step:252/2315 train_time:15147ms step_avg:60.11ms
step:253/2315 train_time:15210ms step_avg:60.12ms
step:254/2315 train_time:15276ms step_avg:60.14ms
step:255/2315 train_time:15338ms step_avg:60.15ms
step:256/2315 train_time:15398ms step_avg:60.15ms
step:257/2315 train_time:15459ms step_avg:60.15ms
step:258/2315 train_time:15518ms step_avg:60.15ms
step:259/2315 train_time:15579ms step_avg:60.15ms
step:260/2315 train_time:15638ms step_avg:60.15ms
step:261/2315 train_time:15698ms step_avg:60.14ms
step:262/2315 train_time:15757ms step_avg:60.14ms
step:263/2315 train_time:15816ms step_avg:60.14ms
step:264/2315 train_time:15875ms step_avg:60.13ms
step:265/2315 train_time:15935ms step_avg:60.13ms
step:266/2315 train_time:15994ms step_avg:60.13ms
step:267/2315 train_time:16054ms step_avg:60.13ms
step:268/2315 train_time:16114ms step_avg:60.13ms
step:269/2315 train_time:16175ms step_avg:60.13ms
step:270/2315 train_time:16236ms step_avg:60.13ms
step:271/2315 train_time:16298ms step_avg:60.14ms
step:272/2315 train_time:16358ms step_avg:60.14ms
step:273/2315 train_time:16419ms step_avg:60.14ms
step:274/2315 train_time:16479ms step_avg:60.14ms
step:275/2315 train_time:16539ms step_avg:60.14ms
step:276/2315 train_time:16599ms step_avg:60.14ms
step:277/2315 train_time:16658ms step_avg:60.14ms
step:278/2315 train_time:16718ms step_avg:60.13ms
step:279/2315 train_time:16778ms step_avg:60.14ms
step:280/2315 train_time:16837ms step_avg:60.13ms
step:281/2315 train_time:16897ms step_avg:60.13ms
step:282/2315 train_time:16956ms step_avg:60.13ms
step:283/2315 train_time:17015ms step_avg:60.13ms
step:284/2315 train_time:17075ms step_avg:60.12ms
step:285/2315 train_time:17136ms step_avg:60.13ms
step:286/2315 train_time:17197ms step_avg:60.13ms
step:287/2315 train_time:17258ms step_avg:60.13ms
step:288/2315 train_time:17318ms step_avg:60.13ms
step:289/2315 train_time:17379ms step_avg:60.13ms
step:290/2315 train_time:17439ms step_avg:60.13ms
step:291/2315 train_time:17499ms step_avg:60.13ms
step:292/2315 train_time:17559ms step_avg:60.13ms
step:293/2315 train_time:17618ms step_avg:60.13ms
step:294/2315 train_time:17678ms step_avg:60.13ms
step:295/2315 train_time:17738ms step_avg:60.13ms
step:296/2315 train_time:17797ms step_avg:60.12ms
step:297/2315 train_time:17857ms step_avg:60.12ms
step:298/2315 train_time:17917ms step_avg:60.12ms
step:299/2315 train_time:17977ms step_avg:60.12ms
step:300/2315 train_time:18037ms step_avg:60.12ms
step:301/2315 train_time:18097ms step_avg:60.12ms
step:302/2315 train_time:18157ms step_avg:60.12ms
step:303/2315 train_time:18218ms step_avg:60.13ms
step:304/2315 train_time:18278ms step_avg:60.13ms
step:305/2315 train_time:18338ms step_avg:60.13ms
step:306/2315 train_time:18398ms step_avg:60.13ms
step:307/2315 train_time:18459ms step_avg:60.13ms
step:308/2315 train_time:18519ms step_avg:60.13ms
step:309/2315 train_time:18579ms step_avg:60.13ms
step:310/2315 train_time:18639ms step_avg:60.13ms
step:311/2315 train_time:18700ms step_avg:60.13ms
step:312/2315 train_time:18759ms step_avg:60.13ms
step:313/2315 train_time:18820ms step_avg:60.13ms
step:314/2315 train_time:18880ms step_avg:60.13ms
step:315/2315 train_time:18940ms step_avg:60.13ms
step:316/2315 train_time:19000ms step_avg:60.13ms
step:317/2315 train_time:19060ms step_avg:60.13ms
step:318/2315 train_time:19121ms step_avg:60.13ms
step:319/2315 train_time:19181ms step_avg:60.13ms
step:320/2315 train_time:19241ms step_avg:60.13ms
step:321/2315 train_time:19302ms step_avg:60.13ms
step:322/2315 train_time:19362ms step_avg:60.13ms
step:323/2315 train_time:19423ms step_avg:60.13ms
step:324/2315 train_time:19483ms step_avg:60.13ms
step:325/2315 train_time:19544ms step_avg:60.14ms
step:326/2315 train_time:19604ms step_avg:60.13ms
step:327/2315 train_time:19664ms step_avg:60.13ms
step:328/2315 train_time:19724ms step_avg:60.13ms
step:329/2315 train_time:19784ms step_avg:60.13ms
step:330/2315 train_time:19844ms step_avg:60.13ms
step:331/2315 train_time:19904ms step_avg:60.13ms
step:332/2315 train_time:19964ms step_avg:60.13ms
step:333/2315 train_time:20025ms step_avg:60.14ms
step:334/2315 train_time:20085ms step_avg:60.14ms
step:335/2315 train_time:20146ms step_avg:60.14ms
step:336/2315 train_time:20206ms step_avg:60.14ms
step:337/2315 train_time:20267ms step_avg:60.14ms
step:338/2315 train_time:20327ms step_avg:60.14ms
step:339/2315 train_time:20386ms step_avg:60.14ms
step:340/2315 train_time:20446ms step_avg:60.14ms
step:341/2315 train_time:20506ms step_avg:60.13ms
step:342/2315 train_time:20566ms step_avg:60.13ms
step:343/2315 train_time:20627ms step_avg:60.14ms
step:344/2315 train_time:20687ms step_avg:60.14ms
step:345/2315 train_time:20747ms step_avg:60.14ms
step:346/2315 train_time:20806ms step_avg:60.13ms
step:347/2315 train_time:20866ms step_avg:60.13ms
step:348/2315 train_time:20926ms step_avg:60.13ms
step:349/2315 train_time:20986ms step_avg:60.13ms
step:350/2315 train_time:21046ms step_avg:60.13ms
step:351/2315 train_time:21107ms step_avg:60.13ms
step:352/2315 train_time:21167ms step_avg:60.13ms
step:353/2315 train_time:21228ms step_avg:60.14ms
step:354/2315 train_time:21288ms step_avg:60.14ms
step:355/2315 train_time:21348ms step_avg:60.14ms
step:356/2315 train_time:21408ms step_avg:60.13ms
step:357/2315 train_time:21468ms step_avg:60.13ms
step:358/2315 train_time:21527ms step_avg:60.13ms
step:359/2315 train_time:21587ms step_avg:60.13ms
step:360/2315 train_time:21648ms step_avg:60.13ms
step:361/2315 train_time:21707ms step_avg:60.13ms
step:362/2315 train_time:21767ms step_avg:60.13ms
step:363/2315 train_time:21827ms step_avg:60.13ms
step:364/2315 train_time:21886ms step_avg:60.13ms
step:365/2315 train_time:21946ms step_avg:60.13ms
step:366/2315 train_time:22006ms step_avg:60.13ms
step:367/2315 train_time:22066ms step_avg:60.12ms
step:368/2315 train_time:22126ms step_avg:60.12ms
step:369/2315 train_time:22187ms step_avg:60.13ms
step:370/2315 train_time:22246ms step_avg:60.12ms
step:371/2315 train_time:22306ms step_avg:60.12ms
step:372/2315 train_time:22367ms step_avg:60.13ms
step:373/2315 train_time:22426ms step_avg:60.12ms
step:374/2315 train_time:22486ms step_avg:60.12ms
step:375/2315 train_time:22547ms step_avg:60.12ms
step:376/2315 train_time:22606ms step_avg:60.12ms
step:377/2315 train_time:22666ms step_avg:60.12ms
step:378/2315 train_time:22726ms step_avg:60.12ms
step:379/2315 train_time:22787ms step_avg:60.12ms
step:380/2315 train_time:22847ms step_avg:60.12ms
step:381/2315 train_time:22908ms step_avg:60.12ms
step:382/2315 train_time:22967ms step_avg:60.12ms
step:383/2315 train_time:23027ms step_avg:60.12ms
step:384/2315 train_time:23087ms step_avg:60.12ms
step:385/2315 train_time:23147ms step_avg:60.12ms
step:386/2315 train_time:23207ms step_avg:60.12ms
step:387/2315 train_time:23267ms step_avg:60.12ms
step:388/2315 train_time:23326ms step_avg:60.12ms
step:389/2315 train_time:23386ms step_avg:60.12ms
step:390/2315 train_time:23446ms step_avg:60.12ms
step:391/2315 train_time:23506ms step_avg:60.12ms
step:392/2315 train_time:23566ms step_avg:60.12ms
step:393/2315 train_time:23626ms step_avg:60.12ms
step:394/2315 train_time:23686ms step_avg:60.12ms
step:395/2315 train_time:23746ms step_avg:60.12ms
step:396/2315 train_time:23806ms step_avg:60.12ms
step:397/2315 train_time:23867ms step_avg:60.12ms
step:398/2315 train_time:23927ms step_avg:60.12ms
step:399/2315 train_time:23987ms step_avg:60.12ms
step:400/2315 train_time:24047ms step_avg:60.12ms
step:401/2315 train_time:24107ms step_avg:60.12ms
step:402/2315 train_time:24167ms step_avg:60.12ms
step:403/2315 train_time:24227ms step_avg:60.12ms
step:404/2315 train_time:24286ms step_avg:60.12ms
step:405/2315 train_time:24346ms step_avg:60.11ms
step:406/2315 train_time:24406ms step_avg:60.11ms
step:407/2315 train_time:24466ms step_avg:60.11ms
step:408/2315 train_time:24526ms step_avg:60.11ms
step:409/2315 train_time:24585ms step_avg:60.11ms
step:410/2315 train_time:24645ms step_avg:60.11ms
step:411/2315 train_time:24705ms step_avg:60.11ms
step:412/2315 train_time:24764ms step_avg:60.11ms
step:413/2315 train_time:24825ms step_avg:60.11ms
step:414/2315 train_time:24885ms step_avg:60.11ms
step:415/2315 train_time:24945ms step_avg:60.11ms
step:416/2315 train_time:25005ms step_avg:60.11ms
step:417/2315 train_time:25065ms step_avg:60.11ms
step:418/2315 train_time:25125ms step_avg:60.11ms
step:419/2315 train_time:25186ms step_avg:60.11ms
step:420/2315 train_time:25246ms step_avg:60.11ms
step:421/2315 train_time:25306ms step_avg:60.11ms
step:422/2315 train_time:25366ms step_avg:60.11ms
step:423/2315 train_time:25427ms step_avg:60.11ms
step:424/2315 train_time:25487ms step_avg:60.11ms
step:425/2315 train_time:25547ms step_avg:60.11ms
step:426/2315 train_time:25607ms step_avg:60.11ms
step:427/2315 train_time:25667ms step_avg:60.11ms
step:428/2315 train_time:25727ms step_avg:60.11ms
step:429/2315 train_time:25787ms step_avg:60.11ms
step:430/2315 train_time:25847ms step_avg:60.11ms
step:431/2315 train_time:25907ms step_avg:60.11ms
step:432/2315 train_time:25967ms step_avg:60.11ms
step:433/2315 train_time:26027ms step_avg:60.11ms
step:434/2315 train_time:26087ms step_avg:60.11ms
step:435/2315 train_time:26147ms step_avg:60.11ms
step:436/2315 train_time:26207ms step_avg:60.11ms
step:437/2315 train_time:26267ms step_avg:60.11ms
step:438/2315 train_time:26327ms step_avg:60.11ms
step:439/2315 train_time:26387ms step_avg:60.11ms
step:440/2315 train_time:26447ms step_avg:60.11ms
step:441/2315 train_time:26507ms step_avg:60.11ms
step:442/2315 train_time:26567ms step_avg:60.11ms
step:443/2315 train_time:26628ms step_avg:60.11ms
step:444/2315 train_time:26687ms step_avg:60.11ms
step:445/2315 train_time:26747ms step_avg:60.11ms
step:446/2315 train_time:26807ms step_avg:60.10ms
step:447/2315 train_time:26867ms step_avg:60.10ms
step:448/2315 train_time:26927ms step_avg:60.10ms
step:449/2315 train_time:26986ms step_avg:60.10ms
step:450/2315 train_time:27046ms step_avg:60.10ms
step:451/2315 train_time:27106ms step_avg:60.10ms
step:452/2315 train_time:27166ms step_avg:60.10ms
step:453/2315 train_time:27226ms step_avg:60.10ms
step:454/2315 train_time:27286ms step_avg:60.10ms
step:455/2315 train_time:27347ms step_avg:60.10ms
step:456/2315 train_time:27406ms step_avg:60.10ms
step:457/2315 train_time:27466ms step_avg:60.10ms
step:458/2315 train_time:27526ms step_avg:60.10ms
step:459/2315 train_time:27587ms step_avg:60.10ms
step:460/2315 train_time:27647ms step_avg:60.10ms
step:461/2315 train_time:27708ms step_avg:60.10ms
step:462/2315 train_time:27768ms step_avg:60.10ms
step:463/2315 train_time:27828ms step_avg:60.10ms
step:464/2315 train_time:27887ms step_avg:60.10ms
step:465/2315 train_time:27947ms step_avg:60.10ms
step:466/2315 train_time:28007ms step_avg:60.10ms
step:467/2315 train_time:28067ms step_avg:60.10ms
step:468/2315 train_time:28127ms step_avg:60.10ms
step:469/2315 train_time:28187ms step_avg:60.10ms
step:470/2315 train_time:28247ms step_avg:60.10ms
step:471/2315 train_time:28307ms step_avg:60.10ms
step:472/2315 train_time:28368ms step_avg:60.10ms
step:473/2315 train_time:28428ms step_avg:60.10ms
step:474/2315 train_time:28488ms step_avg:60.10ms
step:475/2315 train_time:28547ms step_avg:60.10ms
step:476/2315 train_time:28607ms step_avg:60.10ms
step:477/2315 train_time:28667ms step_avg:60.10ms
step:478/2315 train_time:28727ms step_avg:60.10ms
step:479/2315 train_time:28788ms step_avg:60.10ms
step:480/2315 train_time:28847ms step_avg:60.10ms
step:481/2315 train_time:28907ms step_avg:60.10ms
step:482/2315 train_time:28967ms step_avg:60.10ms
step:483/2315 train_time:29027ms step_avg:60.10ms
step:484/2315 train_time:29087ms step_avg:60.10ms
step:485/2315 train_time:29147ms step_avg:60.10ms
step:486/2315 train_time:29207ms step_avg:60.10ms
step:487/2315 train_time:29267ms step_avg:60.10ms
step:488/2315 train_time:29327ms step_avg:60.10ms
step:489/2315 train_time:29387ms step_avg:60.10ms
step:490/2315 train_time:29447ms step_avg:60.10ms
step:491/2315 train_time:29507ms step_avg:60.10ms
step:492/2315 train_time:29567ms step_avg:60.09ms
step:493/2315 train_time:29627ms step_avg:60.10ms
step:494/2315 train_time:29686ms step_avg:60.09ms
step:495/2315 train_time:29747ms step_avg:60.09ms
step:496/2315 train_time:29807ms step_avg:60.09ms
step:497/2315 train_time:29867ms step_avg:60.09ms
step:498/2315 train_time:29927ms step_avg:60.09ms
step:499/2315 train_time:29987ms step_avg:60.09ms
step:500/2315 train_time:30046ms step_avg:60.09ms
step:500/2315 val_loss:3.8357 train_time:30108ms step_avg:60.22ms
step:501/2315 train_time:30128ms step_avg:60.14ms
step:502/2315 train_time:30168ms step_avg:60.10ms
step:503/2315 train_time:30231ms step_avg:60.10ms
step:504/2315 train_time:30293ms step_avg:60.10ms
step:505/2315 train_time:30353ms step_avg:60.11ms
step:506/2315 train_time:30414ms step_avg:60.11ms
step:507/2315 train_time:30473ms step_avg:60.11ms
step:508/2315 train_time:30532ms step_avg:60.10ms
step:509/2315 train_time:30591ms step_avg:60.10ms
step:510/2315 train_time:30651ms step_avg:60.10ms
step:511/2315 train_time:30710ms step_avg:60.10ms
step:512/2315 train_time:30770ms step_avg:60.10ms
step:513/2315 train_time:30829ms step_avg:60.09ms
step:514/2315 train_time:30888ms step_avg:60.09ms
step:515/2315 train_time:30947ms step_avg:60.09ms
step:516/2315 train_time:31007ms step_avg:60.09ms
step:517/2315 train_time:31067ms step_avg:60.09ms
step:518/2315 train_time:31127ms step_avg:60.09ms
step:519/2315 train_time:31188ms step_avg:60.09ms
step:520/2315 train_time:31248ms step_avg:60.09ms
step:521/2315 train_time:31309ms step_avg:60.09ms
step:522/2315 train_time:31369ms step_avg:60.09ms
step:523/2315 train_time:31429ms step_avg:60.09ms
step:524/2315 train_time:31489ms step_avg:60.09ms
step:525/2315 train_time:31549ms step_avg:60.09ms
step:526/2315 train_time:31609ms step_avg:60.09ms
step:527/2315 train_time:31669ms step_avg:60.09ms
step:528/2315 train_time:31728ms step_avg:60.09ms
step:529/2315 train_time:31788ms step_avg:60.09ms
step:530/2315 train_time:31847ms step_avg:60.09ms
step:531/2315 train_time:31907ms step_avg:60.09ms
step:532/2315 train_time:31966ms step_avg:60.09ms
step:533/2315 train_time:32026ms step_avg:60.09ms
step:534/2315 train_time:32086ms step_avg:60.09ms
step:535/2315 train_time:32146ms step_avg:60.09ms
step:536/2315 train_time:32206ms step_avg:60.09ms
step:537/2315 train_time:32267ms step_avg:60.09ms
step:538/2315 train_time:32327ms step_avg:60.09ms
step:539/2315 train_time:32389ms step_avg:60.09ms
step:540/2315 train_time:32449ms step_avg:60.09ms
step:541/2315 train_time:32510ms step_avg:60.09ms
step:542/2315 train_time:32569ms step_avg:60.09ms
step:543/2315 train_time:32630ms step_avg:60.09ms
step:544/2315 train_time:32689ms step_avg:60.09ms
step:545/2315 train_time:32749ms step_avg:60.09ms
step:546/2315 train_time:32808ms step_avg:60.09ms
step:547/2315 train_time:32868ms step_avg:60.09ms
step:548/2315 train_time:32927ms step_avg:60.09ms
step:549/2315 train_time:32987ms step_avg:60.09ms
step:550/2315 train_time:33047ms step_avg:60.09ms
step:551/2315 train_time:33107ms step_avg:60.09ms
step:552/2315 train_time:33167ms step_avg:60.08ms
step:553/2315 train_time:33227ms step_avg:60.08ms
step:554/2315 train_time:33287ms step_avg:60.08ms
step:555/2315 train_time:33347ms step_avg:60.09ms
step:556/2315 train_time:33408ms step_avg:60.09ms
step:557/2315 train_time:33468ms step_avg:60.09ms
step:558/2315 train_time:33529ms step_avg:60.09ms
step:559/2315 train_time:33588ms step_avg:60.09ms
step:560/2315 train_time:33648ms step_avg:60.09ms
step:561/2315 train_time:33708ms step_avg:60.09ms
step:562/2315 train_time:33768ms step_avg:60.09ms
step:563/2315 train_time:33828ms step_avg:60.09ms
step:564/2315 train_time:33888ms step_avg:60.08ms
step:565/2315 train_time:33947ms step_avg:60.08ms
step:566/2315 train_time:34007ms step_avg:60.08ms
step:567/2315 train_time:34067ms step_avg:60.08ms
step:568/2315 train_time:34127ms step_avg:60.08ms
step:569/2315 train_time:34187ms step_avg:60.08ms
step:570/2315 train_time:34247ms step_avg:60.08ms
step:571/2315 train_time:34307ms step_avg:60.08ms
step:572/2315 train_time:34368ms step_avg:60.08ms
step:573/2315 train_time:34428ms step_avg:60.08ms
step:574/2315 train_time:34488ms step_avg:60.08ms
step:575/2315 train_time:34548ms step_avg:60.08ms
step:576/2315 train_time:34608ms step_avg:60.08ms
step:577/2315 train_time:34668ms step_avg:60.08ms
step:578/2315 train_time:34728ms step_avg:60.08ms
step:579/2315 train_time:34788ms step_avg:60.08ms
step:580/2315 train_time:34848ms step_avg:60.08ms
step:581/2315 train_time:34908ms step_avg:60.08ms
step:582/2315 train_time:34968ms step_avg:60.08ms
step:583/2315 train_time:35027ms step_avg:60.08ms
step:584/2315 train_time:35087ms step_avg:60.08ms
step:585/2315 train_time:35147ms step_avg:60.08ms
step:586/2315 train_time:35207ms step_avg:60.08ms
step:587/2315 train_time:35268ms step_avg:60.08ms
step:588/2315 train_time:35329ms step_avg:60.08ms
step:589/2315 train_time:35389ms step_avg:60.08ms
step:590/2315 train_time:35448ms step_avg:60.08ms
step:591/2315 train_time:35508ms step_avg:60.08ms
step:592/2315 train_time:35569ms step_avg:60.08ms
step:593/2315 train_time:35628ms step_avg:60.08ms
step:594/2315 train_time:35688ms step_avg:60.08ms
step:595/2315 train_time:35748ms step_avg:60.08ms
step:596/2315 train_time:35808ms step_avg:60.08ms
step:597/2315 train_time:35869ms step_avg:60.08ms
step:598/2315 train_time:35929ms step_avg:60.08ms
step:599/2315 train_time:35988ms step_avg:60.08ms
step:600/2315 train_time:36049ms step_avg:60.08ms
step:601/2315 train_time:36109ms step_avg:60.08ms
step:602/2315 train_time:36169ms step_avg:60.08ms
step:603/2315 train_time:36229ms step_avg:60.08ms
step:604/2315 train_time:36289ms step_avg:60.08ms
step:605/2315 train_time:36349ms step_avg:60.08ms
step:606/2315 train_time:36409ms step_avg:60.08ms
step:607/2315 train_time:36469ms step_avg:60.08ms
step:608/2315 train_time:36528ms step_avg:60.08ms
step:609/2315 train_time:36589ms step_avg:60.08ms
step:610/2315 train_time:36648ms step_avg:60.08ms
step:611/2315 train_time:36708ms step_avg:60.08ms
step:612/2315 train_time:36768ms step_avg:60.08ms
step:613/2315 train_time:36828ms step_avg:60.08ms
step:614/2315 train_time:36888ms step_avg:60.08ms
step:615/2315 train_time:36948ms step_avg:60.08ms
step:616/2315 train_time:37008ms step_avg:60.08ms
step:617/2315 train_time:37068ms step_avg:60.08ms
step:618/2315 train_time:37127ms step_avg:60.08ms
step:619/2315 train_time:37187ms step_avg:60.08ms
step:620/2315 train_time:37247ms step_avg:60.08ms
step:621/2315 train_time:37308ms step_avg:60.08ms
step:622/2315 train_time:37368ms step_avg:60.08ms
step:623/2315 train_time:37428ms step_avg:60.08ms
step:624/2315 train_time:37488ms step_avg:60.08ms
step:625/2315 train_time:37548ms step_avg:60.08ms
step:626/2315 train_time:37608ms step_avg:60.08ms
step:627/2315 train_time:37667ms step_avg:60.08ms
step:628/2315 train_time:37727ms step_avg:60.08ms
step:629/2315 train_time:37787ms step_avg:60.08ms
step:630/2315 train_time:37847ms step_avg:60.07ms
step:631/2315 train_time:37907ms step_avg:60.07ms
step:632/2315 train_time:37967ms step_avg:60.07ms
step:633/2315 train_time:38027ms step_avg:60.07ms
step:634/2315 train_time:38087ms step_avg:60.07ms
step:635/2315 train_time:38147ms step_avg:60.07ms
step:636/2315 train_time:38207ms step_avg:60.07ms
step:637/2315 train_time:38267ms step_avg:60.07ms
step:638/2315 train_time:38327ms step_avg:60.07ms
step:639/2315 train_time:38388ms step_avg:60.07ms
step:640/2315 train_time:38448ms step_avg:60.07ms
step:641/2315 train_time:38508ms step_avg:60.08ms
step:642/2315 train_time:38569ms step_avg:60.08ms
step:643/2315 train_time:38629ms step_avg:60.08ms
step:644/2315 train_time:38688ms step_avg:60.08ms
step:645/2315 train_time:38748ms step_avg:60.07ms
step:646/2315 train_time:38808ms step_avg:60.07ms
step:647/2315 train_time:38868ms step_avg:60.07ms
step:648/2315 train_time:38927ms step_avg:60.07ms
step:649/2315 train_time:38987ms step_avg:60.07ms
step:650/2315 train_time:39047ms step_avg:60.07ms
step:651/2315 train_time:39107ms step_avg:60.07ms
step:652/2315 train_time:39167ms step_avg:60.07ms
step:653/2315 train_time:39227ms step_avg:60.07ms
step:654/2315 train_time:39287ms step_avg:60.07ms
step:655/2315 train_time:39347ms step_avg:60.07ms
step:656/2315 train_time:39407ms step_avg:60.07ms
step:657/2315 train_time:39468ms step_avg:60.07ms
step:658/2315 train_time:39528ms step_avg:60.07ms
step:659/2315 train_time:39587ms step_avg:60.07ms
step:660/2315 train_time:39647ms step_avg:60.07ms
step:661/2315 train_time:39707ms step_avg:60.07ms
step:662/2315 train_time:39767ms step_avg:60.07ms
step:663/2315 train_time:39827ms step_avg:60.07ms
step:664/2315 train_time:39887ms step_avg:60.07ms
step:665/2315 train_time:39947ms step_avg:60.07ms
step:666/2315 train_time:40008ms step_avg:60.07ms
step:667/2315 train_time:40068ms step_avg:60.07ms
step:668/2315 train_time:40128ms step_avg:60.07ms
step:669/2315 train_time:40188ms step_avg:60.07ms
step:670/2315 train_time:40248ms step_avg:60.07ms
step:671/2315 train_time:40308ms step_avg:60.07ms
step:672/2315 train_time:40368ms step_avg:60.07ms
step:673/2315 train_time:40428ms step_avg:60.07ms
step:674/2315 train_time:40488ms step_avg:60.07ms
step:675/2315 train_time:40548ms step_avg:60.07ms
step:676/2315 train_time:40608ms step_avg:60.07ms
step:677/2315 train_time:40668ms step_avg:60.07ms
step:678/2315 train_time:40728ms step_avg:60.07ms
step:679/2315 train_time:40788ms step_avg:60.07ms
step:680/2315 train_time:40848ms step_avg:60.07ms
step:681/2315 train_time:40908ms step_avg:60.07ms
step:682/2315 train_time:40968ms step_avg:60.07ms
step:683/2315 train_time:41028ms step_avg:60.07ms
step:684/2315 train_time:41088ms step_avg:60.07ms
step:685/2315 train_time:41148ms step_avg:60.07ms
step:686/2315 train_time:41208ms step_avg:60.07ms
step:687/2315 train_time:41268ms step_avg:60.07ms
step:688/2315 train_time:41328ms step_avg:60.07ms
step:689/2315 train_time:41389ms step_avg:60.07ms
step:690/2315 train_time:41449ms step_avg:60.07ms
step:691/2315 train_time:41509ms step_avg:60.07ms
step:692/2315 train_time:41569ms step_avg:60.07ms
step:693/2315 train_time:41629ms step_avg:60.07ms
step:694/2315 train_time:41689ms step_avg:60.07ms
step:695/2315 train_time:41749ms step_avg:60.07ms
step:696/2315 train_time:41808ms step_avg:60.07ms
step:697/2315 train_time:41868ms step_avg:60.07ms
step:698/2315 train_time:41928ms step_avg:60.07ms
step:699/2315 train_time:41988ms step_avg:60.07ms
step:700/2315 train_time:42048ms step_avg:60.07ms
step:701/2315 train_time:42108ms step_avg:60.07ms
step:702/2315 train_time:42167ms step_avg:60.07ms
step:703/2315 train_time:42227ms step_avg:60.07ms
step:704/2315 train_time:42287ms step_avg:60.07ms
step:705/2315 train_time:42348ms step_avg:60.07ms
step:706/2315 train_time:42408ms step_avg:60.07ms
step:707/2315 train_time:42468ms step_avg:60.07ms
step:708/2315 train_time:42528ms step_avg:60.07ms
step:709/2315 train_time:42588ms step_avg:60.07ms
step:710/2315 train_time:42649ms step_avg:60.07ms
step:711/2315 train_time:42709ms step_avg:60.07ms
step:712/2315 train_time:42768ms step_avg:60.07ms
step:713/2315 train_time:42829ms step_avg:60.07ms
step:714/2315 train_time:42889ms step_avg:60.07ms
step:715/2315 train_time:42949ms step_avg:60.07ms
step:716/2315 train_time:43008ms step_avg:60.07ms
step:717/2315 train_time:43068ms step_avg:60.07ms
step:718/2315 train_time:43128ms step_avg:60.07ms
step:719/2315 train_time:43188ms step_avg:60.07ms
step:720/2315 train_time:43248ms step_avg:60.07ms
step:721/2315 train_time:43308ms step_avg:60.07ms
step:722/2315 train_time:43369ms step_avg:60.07ms
step:723/2315 train_time:43429ms step_avg:60.07ms
step:724/2315 train_time:43489ms step_avg:60.07ms
step:725/2315 train_time:43548ms step_avg:60.07ms
step:726/2315 train_time:43608ms step_avg:60.07ms
step:727/2315 train_time:43668ms step_avg:60.07ms
step:728/2315 train_time:43728ms step_avg:60.07ms
step:729/2315 train_time:43788ms step_avg:60.07ms
step:730/2315 train_time:43847ms step_avg:60.07ms
step:731/2315 train_time:43908ms step_avg:60.07ms
step:732/2315 train_time:43968ms step_avg:60.07ms
step:733/2315 train_time:44028ms step_avg:60.07ms
step:734/2315 train_time:44088ms step_avg:60.07ms
step:735/2315 train_time:44148ms step_avg:60.07ms
step:736/2315 train_time:44208ms step_avg:60.07ms
step:737/2315 train_time:44268ms step_avg:60.06ms
step:738/2315 train_time:44328ms step_avg:60.06ms
step:739/2315 train_time:44388ms step_avg:60.07ms
step:740/2315 train_time:44449ms step_avg:60.07ms
step:741/2315 train_time:44509ms step_avg:60.07ms
step:742/2315 train_time:44569ms step_avg:60.07ms
step:743/2315 train_time:44629ms step_avg:60.07ms
step:744/2315 train_time:44688ms step_avg:60.06ms
step:745/2315 train_time:44748ms step_avg:60.06ms
step:746/2315 train_time:44807ms step_avg:60.06ms
step:747/2315 train_time:44868ms step_avg:60.06ms
step:748/2315 train_time:44928ms step_avg:60.06ms
step:749/2315 train_time:44988ms step_avg:60.06ms
step:750/2315 train_time:45048ms step_avg:60.06ms
step:750/2315 val_loss:3.6839 train_time:45110ms step_avg:60.15ms
step:751/2315 train_time:45128ms step_avg:60.09ms
step:752/2315 train_time:45169ms step_avg:60.07ms
step:753/2315 train_time:45235ms step_avg:60.07ms
step:754/2315 train_time:45296ms step_avg:60.07ms
step:755/2315 train_time:45357ms step_avg:60.08ms
step:756/2315 train_time:45417ms step_avg:60.08ms
step:757/2315 train_time:45476ms step_avg:60.07ms
step:758/2315 train_time:45537ms step_avg:60.07ms
step:759/2315 train_time:45596ms step_avg:60.07ms
step:760/2315 train_time:45656ms step_avg:60.07ms
step:761/2315 train_time:45717ms step_avg:60.08ms
step:762/2315 train_time:45777ms step_avg:60.08ms
step:763/2315 train_time:45837ms step_avg:60.07ms
step:764/2315 train_time:45896ms step_avg:60.07ms
step:765/2315 train_time:45956ms step_avg:60.07ms
step:766/2315 train_time:46016ms step_avg:60.07ms
step:767/2315 train_time:46077ms step_avg:60.07ms
step:768/2315 train_time:46139ms step_avg:60.08ms
step:769/2315 train_time:46201ms step_avg:60.08ms
step:770/2315 train_time:46261ms step_avg:60.08ms
step:771/2315 train_time:46322ms step_avg:60.08ms
step:772/2315 train_time:46384ms step_avg:60.08ms
step:773/2315 train_time:46445ms step_avg:60.08ms
step:774/2315 train_time:46506ms step_avg:60.09ms
step:775/2315 train_time:46568ms step_avg:60.09ms
step:776/2315 train_time:46629ms step_avg:60.09ms
step:777/2315 train_time:46689ms step_avg:60.09ms
step:778/2315 train_time:46750ms step_avg:60.09ms
step:779/2315 train_time:46811ms step_avg:60.09ms
step:780/2315 train_time:46872ms step_avg:60.09ms
step:781/2315 train_time:46932ms step_avg:60.09ms
step:782/2315 train_time:46993ms step_avg:60.09ms
step:783/2315 train_time:47054ms step_avg:60.09ms
step:784/2315 train_time:47114ms step_avg:60.09ms
step:785/2315 train_time:47175ms step_avg:60.10ms
step:786/2315 train_time:47236ms step_avg:60.10ms
step:787/2315 train_time:47297ms step_avg:60.10ms
step:788/2315 train_time:47358ms step_avg:60.10ms
step:789/2315 train_time:47419ms step_avg:60.10ms
step:790/2315 train_time:47480ms step_avg:60.10ms
step:791/2315 train_time:47541ms step_avg:60.10ms
step:792/2315 train_time:47601ms step_avg:60.10ms
step:793/2315 train_time:47663ms step_avg:60.10ms
step:794/2315 train_time:47723ms step_avg:60.10ms
step:795/2315 train_time:47784ms step_avg:60.11ms
step:796/2315 train_time:47845ms step_avg:60.11ms
step:797/2315 train_time:47906ms step_avg:60.11ms
step:798/2315 train_time:47967ms step_avg:60.11ms
step:799/2315 train_time:48028ms step_avg:60.11ms
step:800/2315 train_time:48089ms step_avg:60.11ms
step:801/2315 train_time:48150ms step_avg:60.11ms
step:802/2315 train_time:48212ms step_avg:60.11ms
step:803/2315 train_time:48272ms step_avg:60.12ms
step:804/2315 train_time:48333ms step_avg:60.12ms
step:805/2315 train_time:48394ms step_avg:60.12ms
step:806/2315 train_time:48454ms step_avg:60.12ms
step:807/2315 train_time:48515ms step_avg:60.12ms
step:808/2315 train_time:48575ms step_avg:60.12ms
step:809/2315 train_time:48636ms step_avg:60.12ms
step:810/2315 train_time:48697ms step_avg:60.12ms
step:811/2315 train_time:48759ms step_avg:60.12ms
step:812/2315 train_time:48820ms step_avg:60.12ms
step:813/2315 train_time:48881ms step_avg:60.12ms
step:814/2315 train_time:48941ms step_avg:60.12ms
step:815/2315 train_time:49001ms step_avg:60.12ms
step:816/2315 train_time:49062ms step_avg:60.13ms
step:817/2315 train_time:49124ms step_avg:60.13ms
step:818/2315 train_time:49185ms step_avg:60.13ms
step:819/2315 train_time:49246ms step_avg:60.13ms
step:820/2315 train_time:49307ms step_avg:60.13ms
step:821/2315 train_time:49368ms step_avg:60.13ms
step:822/2315 train_time:49429ms step_avg:60.13ms
step:823/2315 train_time:49490ms step_avg:60.13ms
step:824/2315 train_time:49551ms step_avg:60.13ms
step:825/2315 train_time:49613ms step_avg:60.14ms
step:826/2315 train_time:49673ms step_avg:60.14ms
step:827/2315 train_time:49734ms step_avg:60.14ms
step:828/2315 train_time:49795ms step_avg:60.14ms
step:829/2315 train_time:49855ms step_avg:60.14ms
step:830/2315 train_time:49916ms step_avg:60.14ms
step:831/2315 train_time:49977ms step_avg:60.14ms
step:832/2315 train_time:50038ms step_avg:60.14ms
step:833/2315 train_time:50099ms step_avg:60.14ms
step:834/2315 train_time:50160ms step_avg:60.14ms
step:835/2315 train_time:50221ms step_avg:60.14ms
step:836/2315 train_time:50281ms step_avg:60.14ms
step:837/2315 train_time:50342ms step_avg:60.15ms
step:838/2315 train_time:50403ms step_avg:60.15ms
step:839/2315 train_time:50464ms step_avg:60.15ms
step:840/2315 train_time:50525ms step_avg:60.15ms
step:841/2315 train_time:50587ms step_avg:60.15ms
step:842/2315 train_time:50648ms step_avg:60.15ms
step:843/2315 train_time:50710ms step_avg:60.15ms
step:844/2315 train_time:50771ms step_avg:60.15ms
step:845/2315 train_time:50832ms step_avg:60.16ms
step:846/2315 train_time:50892ms step_avg:60.16ms
step:847/2315 train_time:50953ms step_avg:60.16ms
step:848/2315 train_time:51014ms step_avg:60.16ms
step:849/2315 train_time:51075ms step_avg:60.16ms
step:850/2315 train_time:51135ms step_avg:60.16ms
step:851/2315 train_time:51196ms step_avg:60.16ms
step:852/2315 train_time:51257ms step_avg:60.16ms
step:853/2315 train_time:51316ms step_avg:60.16ms
step:854/2315 train_time:51377ms step_avg:60.16ms
step:855/2315 train_time:51438ms step_avg:60.16ms
step:856/2315 train_time:51499ms step_avg:60.16ms
step:857/2315 train_time:51559ms step_avg:60.16ms
step:858/2315 train_time:51619ms step_avg:60.16ms
step:859/2315 train_time:51680ms step_avg:60.16ms
step:860/2315 train_time:51741ms step_avg:60.16ms
step:861/2315 train_time:51802ms step_avg:60.17ms
step:862/2315 train_time:51863ms step_avg:60.17ms
step:863/2315 train_time:51924ms step_avg:60.17ms
step:864/2315 train_time:51985ms step_avg:60.17ms
step:865/2315 train_time:52046ms step_avg:60.17ms
step:866/2315 train_time:52107ms step_avg:60.17ms
step:867/2315 train_time:52168ms step_avg:60.17ms
step:868/2315 train_time:52229ms step_avg:60.17ms
step:869/2315 train_time:52290ms step_avg:60.17ms
step:870/2315 train_time:52351ms step_avg:60.17ms
step:871/2315 train_time:52412ms step_avg:60.18ms
step:872/2315 train_time:52473ms step_avg:60.18ms
step:873/2315 train_time:52534ms step_avg:60.18ms
step:874/2315 train_time:52594ms step_avg:60.18ms
step:875/2315 train_time:52654ms step_avg:60.18ms
step:876/2315 train_time:52715ms step_avg:60.18ms
step:877/2315 train_time:52777ms step_avg:60.18ms
step:878/2315 train_time:52837ms step_avg:60.18ms
step:879/2315 train_time:52898ms step_avg:60.18ms
step:880/2315 train_time:52959ms step_avg:60.18ms
step:881/2315 train_time:53020ms step_avg:60.18ms
step:882/2315 train_time:53081ms step_avg:60.18ms
step:883/2315 train_time:53141ms step_avg:60.18ms
step:884/2315 train_time:53201ms step_avg:60.18ms
step:885/2315 train_time:53262ms step_avg:60.18ms
step:886/2315 train_time:53323ms step_avg:60.18ms
step:887/2315 train_time:53384ms step_avg:60.19ms
step:888/2315 train_time:53445ms step_avg:60.19ms
step:889/2315 train_time:53507ms step_avg:60.19ms
step:890/2315 train_time:53568ms step_avg:60.19ms
step:891/2315 train_time:53629ms step_avg:60.19ms
step:892/2315 train_time:53689ms step_avg:60.19ms
step:893/2315 train_time:53750ms step_avg:60.19ms
step:894/2315 train_time:53812ms step_avg:60.19ms
step:895/2315 train_time:53873ms step_avg:60.19ms
step:896/2315 train_time:53933ms step_avg:60.19ms
step:897/2315 train_time:53994ms step_avg:60.19ms
step:898/2315 train_time:54054ms step_avg:60.19ms
step:899/2315 train_time:54114ms step_avg:60.19ms
step:900/2315 train_time:54176ms step_avg:60.20ms
step:901/2315 train_time:54237ms step_avg:60.20ms
step:902/2315 train_time:54298ms step_avg:60.20ms
step:903/2315 train_time:54359ms step_avg:60.20ms
step:904/2315 train_time:54419ms step_avg:60.20ms
step:905/2315 train_time:54480ms step_avg:60.20ms
step:906/2315 train_time:54540ms step_avg:60.20ms
step:907/2315 train_time:54601ms step_avg:60.20ms
step:908/2315 train_time:54662ms step_avg:60.20ms
step:909/2315 train_time:54723ms step_avg:60.20ms
step:910/2315 train_time:54785ms step_avg:60.20ms
step:911/2315 train_time:54846ms step_avg:60.20ms
step:912/2315 train_time:54907ms step_avg:60.20ms
step:913/2315 train_time:54968ms step_avg:60.21ms
step:914/2315 train_time:55028ms step_avg:60.21ms
step:915/2315 train_time:55089ms step_avg:60.21ms
step:916/2315 train_time:55150ms step_avg:60.21ms
step:917/2315 train_time:55211ms step_avg:60.21ms
step:918/2315 train_time:55272ms step_avg:60.21ms
step:919/2315 train_time:55333ms step_avg:60.21ms
step:920/2315 train_time:55393ms step_avg:60.21ms
step:921/2315 train_time:55454ms step_avg:60.21ms
step:922/2315 train_time:55514ms step_avg:60.21ms
step:923/2315 train_time:55575ms step_avg:60.21ms
step:924/2315 train_time:55637ms step_avg:60.21ms
step:925/2315 train_time:55698ms step_avg:60.21ms
step:926/2315 train_time:55758ms step_avg:60.21ms
step:927/2315 train_time:55818ms step_avg:60.21ms
step:928/2315 train_time:55878ms step_avg:60.21ms
step:929/2315 train_time:55939ms step_avg:60.21ms
step:930/2315 train_time:56000ms step_avg:60.22ms
step:931/2315 train_time:56060ms step_avg:60.22ms
step:932/2315 train_time:56120ms step_avg:60.21ms
step:933/2315 train_time:56182ms step_avg:60.22ms
step:934/2315 train_time:56243ms step_avg:60.22ms
step:935/2315 train_time:56304ms step_avg:60.22ms
step:936/2315 train_time:56365ms step_avg:60.22ms
step:937/2315 train_time:56426ms step_avg:60.22ms
step:938/2315 train_time:56486ms step_avg:60.22ms
step:939/2315 train_time:56548ms step_avg:60.22ms
step:940/2315 train_time:56609ms step_avg:60.22ms
step:941/2315 train_time:56670ms step_avg:60.22ms
step:942/2315 train_time:56731ms step_avg:60.22ms
step:943/2315 train_time:56792ms step_avg:60.22ms
step:944/2315 train_time:56852ms step_avg:60.23ms
step:945/2315 train_time:56913ms step_avg:60.23ms
step:946/2315 train_time:56973ms step_avg:60.23ms
step:947/2315 train_time:57034ms step_avg:60.23ms
step:948/2315 train_time:57095ms step_avg:60.23ms
step:949/2315 train_time:57155ms step_avg:60.23ms
step:950/2315 train_time:57215ms step_avg:60.23ms
step:951/2315 train_time:57276ms step_avg:60.23ms
step:952/2315 train_time:57337ms step_avg:60.23ms
step:953/2315 train_time:57399ms step_avg:60.23ms
step:954/2315 train_time:57459ms step_avg:60.23ms
step:955/2315 train_time:57520ms step_avg:60.23ms
step:956/2315 train_time:57580ms step_avg:60.23ms
step:957/2315 train_time:57641ms step_avg:60.23ms
step:958/2315 train_time:57701ms step_avg:60.23ms
step:959/2315 train_time:57762ms step_avg:60.23ms
step:960/2315 train_time:57823ms step_avg:60.23ms
step:961/2315 train_time:57884ms step_avg:60.23ms
step:962/2315 train_time:57945ms step_avg:60.23ms
step:963/2315 train_time:58006ms step_avg:60.23ms
step:964/2315 train_time:58067ms step_avg:60.24ms
step:965/2315 train_time:58128ms step_avg:60.24ms
step:966/2315 train_time:58188ms step_avg:60.24ms
step:967/2315 train_time:58250ms step_avg:60.24ms
step:968/2315 train_time:58311ms step_avg:60.24ms
step:969/2315 train_time:58372ms step_avg:60.24ms
step:970/2315 train_time:58432ms step_avg:60.24ms
step:971/2315 train_time:58493ms step_avg:60.24ms
step:972/2315 train_time:58554ms step_avg:60.24ms
step:973/2315 train_time:58615ms step_avg:60.24ms
step:974/2315 train_time:58675ms step_avg:60.24ms
step:975/2315 train_time:58736ms step_avg:60.24ms
step:976/2315 train_time:58797ms step_avg:60.24ms
step:977/2315 train_time:58858ms step_avg:60.24ms
step:978/2315 train_time:58918ms step_avg:60.24ms
step:979/2315 train_time:58979ms step_avg:60.24ms
step:980/2315 train_time:59039ms step_avg:60.24ms
step:981/2315 train_time:59100ms step_avg:60.24ms
step:982/2315 train_time:59161ms step_avg:60.24ms
step:983/2315 train_time:59221ms step_avg:60.25ms
step:984/2315 train_time:59282ms step_avg:60.25ms
step:985/2315 train_time:59343ms step_avg:60.25ms
step:986/2315 train_time:59405ms step_avg:60.25ms
step:987/2315 train_time:59466ms step_avg:60.25ms
step:988/2315 train_time:59527ms step_avg:60.25ms
step:989/2315 train_time:59588ms step_avg:60.25ms
step:990/2315 train_time:59649ms step_avg:60.25ms
step:991/2315 train_time:59711ms step_avg:60.25ms
step:992/2315 train_time:59771ms step_avg:60.25ms
step:993/2315 train_time:59832ms step_avg:60.25ms
step:994/2315 train_time:59892ms step_avg:60.25ms
step:995/2315 train_time:59953ms step_avg:60.25ms
step:996/2315 train_time:60014ms step_avg:60.25ms
step:997/2315 train_time:60074ms step_avg:60.25ms
step:998/2315 train_time:60134ms step_avg:60.25ms
step:999/2315 train_time:60195ms step_avg:60.25ms
step:1000/2315 train_time:60255ms step_avg:60.26ms
step:1000/2315 val_loss:3.5740 train_time:60318ms step_avg:60.32ms
step:1001/2315 train_time:60338ms step_avg:60.28ms
step:1002/2315 train_time:60379ms step_avg:60.26ms
step:1003/2315 train_time:60447ms step_avg:60.27ms
step:1004/2315 train_time:60510ms step_avg:60.27ms
step:1005/2315 train_time:60571ms step_avg:60.27ms
step:1006/2315 train_time:60632ms step_avg:60.27ms
step:1007/2315 train_time:60692ms step_avg:60.27ms
step:1008/2315 train_time:60752ms step_avg:60.27ms
step:1009/2315 train_time:60812ms step_avg:60.27ms
step:1010/2315 train_time:60872ms step_avg:60.27ms
step:1011/2315 train_time:60932ms step_avg:60.27ms
step:1012/2315 train_time:60991ms step_avg:60.27ms
step:1013/2315 train_time:61051ms step_avg:60.27ms
step:1014/2315 train_time:61112ms step_avg:60.27ms
step:1015/2315 train_time:61171ms step_avg:60.27ms
step:1016/2315 train_time:61233ms step_avg:60.27ms
step:1017/2315 train_time:61296ms step_avg:60.27ms
step:1018/2315 train_time:61357ms step_avg:60.27ms
step:1019/2315 train_time:61419ms step_avg:60.27ms
step:1020/2315 train_time:61482ms step_avg:60.28ms
step:1021/2315 train_time:61544ms step_avg:60.28ms
step:1022/2315 train_time:61605ms step_avg:60.28ms
step:1023/2315 train_time:61665ms step_avg:60.28ms
step:1024/2315 train_time:61725ms step_avg:60.28ms
step:1025/2315 train_time:61786ms step_avg:60.28ms
step:1026/2315 train_time:61847ms step_avg:60.28ms
step:1027/2315 train_time:61908ms step_avg:60.28ms
step:1028/2315 train_time:61968ms step_avg:60.28ms
step:1029/2315 train_time:62028ms step_avg:60.28ms
step:1030/2315 train_time:62088ms step_avg:60.28ms
step:1031/2315 train_time:62148ms step_avg:60.28ms
step:1032/2315 train_time:62209ms step_avg:60.28ms
step:1033/2315 train_time:62271ms step_avg:60.28ms
step:1034/2315 train_time:62332ms step_avg:60.28ms
step:1035/2315 train_time:62394ms step_avg:60.28ms
step:1036/2315 train_time:62455ms step_avg:60.28ms
step:1037/2315 train_time:62516ms step_avg:60.29ms
step:1038/2315 train_time:62577ms step_avg:60.29ms
step:1039/2315 train_time:62638ms step_avg:60.29ms
step:1040/2315 train_time:62699ms step_avg:60.29ms
step:1041/2315 train_time:62761ms step_avg:60.29ms
step:1042/2315 train_time:62821ms step_avg:60.29ms
step:1043/2315 train_time:62882ms step_avg:60.29ms
step:1044/2315 train_time:62943ms step_avg:60.29ms
step:1045/2315 train_time:63004ms step_avg:60.29ms
step:1046/2315 train_time:63064ms step_avg:60.29ms
step:1047/2315 train_time:63125ms step_avg:60.29ms
step:1048/2315 train_time:63186ms step_avg:60.29ms
step:1049/2315 train_time:63247ms step_avg:60.29ms
step:1050/2315 train_time:63308ms step_avg:60.29ms
step:1051/2315 train_time:63369ms step_avg:60.29ms
step:1052/2315 train_time:63430ms step_avg:60.29ms
step:1053/2315 train_time:63491ms step_avg:60.30ms
step:1054/2315 train_time:63552ms step_avg:60.30ms
step:1055/2315 train_time:63613ms step_avg:60.30ms
step:1056/2315 train_time:63673ms step_avg:60.30ms
step:1057/2315 train_time:63734ms step_avg:60.30ms
step:1058/2315 train_time:63794ms step_avg:60.30ms
step:1059/2315 train_time:63854ms step_avg:60.30ms
step:1060/2315 train_time:63914ms step_avg:60.30ms
step:1061/2315 train_time:63976ms step_avg:60.30ms
step:1062/2315 train_time:64036ms step_avg:60.30ms
step:1063/2315 train_time:64098ms step_avg:60.30ms
step:1064/2315 train_time:64159ms step_avg:60.30ms
step:1065/2315 train_time:64221ms step_avg:60.30ms
step:1066/2315 train_time:64282ms step_avg:60.30ms
step:1067/2315 train_time:64343ms step_avg:60.30ms
step:1068/2315 train_time:64405ms step_avg:60.30ms
step:1069/2315 train_time:64466ms step_avg:60.30ms
step:1070/2315 train_time:64527ms step_avg:60.31ms
step:1071/2315 train_time:64587ms step_avg:60.31ms
step:1072/2315 train_time:64648ms step_avg:60.31ms
step:1073/2315 train_time:64708ms step_avg:60.31ms
step:1074/2315 train_time:64768ms step_avg:60.31ms
step:1075/2315 train_time:64829ms step_avg:60.31ms
step:1076/2315 train_time:64890ms step_avg:60.31ms
step:1077/2315 train_time:64951ms step_avg:60.31ms
step:1078/2315 train_time:65012ms step_avg:60.31ms
step:1079/2315 train_time:65072ms step_avg:60.31ms
step:1080/2315 train_time:65132ms step_avg:60.31ms
step:1081/2315 train_time:65193ms step_avg:60.31ms
step:1082/2315 train_time:65253ms step_avg:60.31ms
step:1083/2315 train_time:65314ms step_avg:60.31ms
step:1084/2315 train_time:65374ms step_avg:60.31ms
step:1085/2315 train_time:65435ms step_avg:60.31ms
step:1086/2315 train_time:65496ms step_avg:60.31ms
step:1087/2315 train_time:65557ms step_avg:60.31ms
step:1088/2315 train_time:65618ms step_avg:60.31ms
step:1089/2315 train_time:65680ms step_avg:60.31ms
step:1090/2315 train_time:65741ms step_avg:60.31ms
step:1091/2315 train_time:65803ms step_avg:60.31ms
step:1092/2315 train_time:65863ms step_avg:60.31ms
step:1093/2315 train_time:65924ms step_avg:60.32ms
step:1094/2315 train_time:65985ms step_avg:60.32ms
step:1095/2315 train_time:66046ms step_avg:60.32ms
step:1096/2315 train_time:66107ms step_avg:60.32ms
step:1097/2315 train_time:66167ms step_avg:60.32ms
step:1098/2315 train_time:66228ms step_avg:60.32ms
step:1099/2315 train_time:66289ms step_avg:60.32ms
step:1100/2315 train_time:66349ms step_avg:60.32ms
step:1101/2315 train_time:66410ms step_avg:60.32ms
step:1102/2315 train_time:66471ms step_avg:60.32ms
step:1103/2315 train_time:66531ms step_avg:60.32ms
step:1104/2315 train_time:66592ms step_avg:60.32ms
step:1105/2315 train_time:66653ms step_avg:60.32ms
step:1106/2315 train_time:66713ms step_avg:60.32ms
step:1107/2315 train_time:66774ms step_avg:60.32ms
step:1108/2315 train_time:66835ms step_avg:60.32ms
step:1109/2315 train_time:66896ms step_avg:60.32ms
step:1110/2315 train_time:66958ms step_avg:60.32ms
step:1111/2315 train_time:67019ms step_avg:60.32ms
step:1112/2315 train_time:67079ms step_avg:60.32ms
step:1113/2315 train_time:67140ms step_avg:60.32ms
step:1114/2315 train_time:67201ms step_avg:60.32ms
step:1115/2315 train_time:67263ms step_avg:60.33ms
step:1116/2315 train_time:67324ms step_avg:60.33ms
step:1117/2315 train_time:67385ms step_avg:60.33ms
step:1118/2315 train_time:67445ms step_avg:60.33ms
step:1119/2315 train_time:67507ms step_avg:60.33ms
step:1120/2315 train_time:67567ms step_avg:60.33ms
step:1121/2315 train_time:67628ms step_avg:60.33ms
step:1122/2315 train_time:67688ms step_avg:60.33ms
step:1123/2315 train_time:67749ms step_avg:60.33ms
step:1124/2315 train_time:67810ms step_avg:60.33ms
step:1125/2315 train_time:67871ms step_avg:60.33ms
step:1126/2315 train_time:67932ms step_avg:60.33ms
step:1127/2315 train_time:67992ms step_avg:60.33ms
step:1128/2315 train_time:68052ms step_avg:60.33ms
step:1129/2315 train_time:68114ms step_avg:60.33ms
step:1130/2315 train_time:68174ms step_avg:60.33ms
step:1131/2315 train_time:68235ms step_avg:60.33ms
step:1132/2315 train_time:68295ms step_avg:60.33ms
step:1133/2315 train_time:68356ms step_avg:60.33ms
step:1134/2315 train_time:68417ms step_avg:60.33ms
step:1135/2315 train_time:68478ms step_avg:60.33ms
step:1136/2315 train_time:68539ms step_avg:60.33ms
step:1137/2315 train_time:68600ms step_avg:60.33ms
step:1138/2315 train_time:68661ms step_avg:60.33ms
step:1139/2315 train_time:68722ms step_avg:60.34ms
step:1140/2315 train_time:68783ms step_avg:60.34ms
step:1141/2315 train_time:68844ms step_avg:60.34ms
step:1142/2315 train_time:68905ms step_avg:60.34ms
step:1143/2315 train_time:68967ms step_avg:60.34ms
step:1144/2315 train_time:69027ms step_avg:60.34ms
step:1145/2315 train_time:69088ms step_avg:60.34ms
step:1146/2315 train_time:69149ms step_avg:60.34ms
step:1147/2315 train_time:69210ms step_avg:60.34ms
step:1148/2315 train_time:69270ms step_avg:60.34ms
step:1149/2315 train_time:69330ms step_avg:60.34ms
step:1150/2315 train_time:69391ms step_avg:60.34ms
step:1151/2315 train_time:69452ms step_avg:60.34ms
step:1152/2315 train_time:69513ms step_avg:60.34ms
step:1153/2315 train_time:69574ms step_avg:60.34ms
step:1154/2315 train_time:69635ms step_avg:60.34ms
step:1155/2315 train_time:69695ms step_avg:60.34ms
step:1156/2315 train_time:69755ms step_avg:60.34ms
step:1157/2315 train_time:69817ms step_avg:60.34ms
step:1158/2315 train_time:69877ms step_avg:60.34ms
step:1159/2315 train_time:69938ms step_avg:60.34ms
step:1160/2315 train_time:69999ms step_avg:60.34ms
step:1161/2315 train_time:70061ms step_avg:60.34ms
step:1162/2315 train_time:70122ms step_avg:60.35ms
step:1163/2315 train_time:70183ms step_avg:60.35ms
step:1164/2315 train_time:70243ms step_avg:60.35ms
step:1165/2315 train_time:70304ms step_avg:60.35ms
step:1166/2315 train_time:70365ms step_avg:60.35ms
step:1167/2315 train_time:70426ms step_avg:60.35ms
step:1168/2315 train_time:70487ms step_avg:60.35ms
step:1169/2315 train_time:70548ms step_avg:60.35ms
step:1170/2315 train_time:70608ms step_avg:60.35ms
step:1171/2315 train_time:70669ms step_avg:60.35ms
step:1172/2315 train_time:70730ms step_avg:60.35ms
step:1173/2315 train_time:70790ms step_avg:60.35ms
step:1174/2315 train_time:70851ms step_avg:60.35ms
step:1175/2315 train_time:70912ms step_avg:60.35ms
step:1176/2315 train_time:70973ms step_avg:60.35ms
step:1177/2315 train_time:71034ms step_avg:60.35ms
step:1178/2315 train_time:71094ms step_avg:60.35ms
step:1179/2315 train_time:71155ms step_avg:60.35ms
step:1180/2315 train_time:71216ms step_avg:60.35ms
step:1181/2315 train_time:71277ms step_avg:60.35ms
step:1182/2315 train_time:71338ms step_avg:60.35ms
step:1183/2315 train_time:71399ms step_avg:60.35ms
step:1184/2315 train_time:71460ms step_avg:60.35ms
step:1185/2315 train_time:71521ms step_avg:60.36ms
step:1186/2315 train_time:71582ms step_avg:60.36ms
step:1187/2315 train_time:71644ms step_avg:60.36ms
step:1188/2315 train_time:71705ms step_avg:60.36ms
step:1189/2315 train_time:71766ms step_avg:60.36ms
step:1190/2315 train_time:71826ms step_avg:60.36ms
step:1191/2315 train_time:71888ms step_avg:60.36ms
step:1192/2315 train_time:71948ms step_avg:60.36ms
step:1193/2315 train_time:72009ms step_avg:60.36ms
step:1194/2315 train_time:72070ms step_avg:60.36ms
step:1195/2315 train_time:72130ms step_avg:60.36ms
step:1196/2315 train_time:72191ms step_avg:60.36ms
step:1197/2315 train_time:72251ms step_avg:60.36ms
step:1198/2315 train_time:72312ms step_avg:60.36ms
step:1199/2315 train_time:72373ms step_avg:60.36ms
step:1200/2315 train_time:72434ms step_avg:60.36ms
step:1201/2315 train_time:72495ms step_avg:60.36ms
step:1202/2315 train_time:72555ms step_avg:60.36ms
step:1203/2315 train_time:72617ms step_avg:60.36ms
step:1204/2315 train_time:72678ms step_avg:60.36ms
step:1205/2315 train_time:72739ms step_avg:60.36ms
step:1206/2315 train_time:72800ms step_avg:60.37ms
step:1207/2315 train_time:72862ms step_avg:60.37ms
step:1208/2315 train_time:72923ms step_avg:60.37ms
step:1209/2315 train_time:72984ms step_avg:60.37ms
step:1210/2315 train_time:73045ms step_avg:60.37ms
step:1211/2315 train_time:73106ms step_avg:60.37ms
step:1212/2315 train_time:73167ms step_avg:60.37ms
step:1213/2315 train_time:73227ms step_avg:60.37ms
step:1214/2315 train_time:73287ms step_avg:60.37ms
step:1215/2315 train_time:73348ms step_avg:60.37ms
step:1216/2315 train_time:73409ms step_avg:60.37ms
step:1217/2315 train_time:73470ms step_avg:60.37ms
step:1218/2315 train_time:73530ms step_avg:60.37ms
step:1219/2315 train_time:73592ms step_avg:60.37ms
step:1220/2315 train_time:73652ms step_avg:60.37ms
step:1221/2315 train_time:73714ms step_avg:60.37ms
step:1222/2315 train_time:73774ms step_avg:60.37ms
step:1223/2315 train_time:73834ms step_avg:60.37ms
step:1224/2315 train_time:73894ms step_avg:60.37ms
step:1225/2315 train_time:73956ms step_avg:60.37ms
step:1226/2315 train_time:74017ms step_avg:60.37ms
step:1227/2315 train_time:74078ms step_avg:60.37ms
step:1228/2315 train_time:74139ms step_avg:60.37ms
step:1229/2315 train_time:74201ms step_avg:60.37ms
step:1230/2315 train_time:74262ms step_avg:60.38ms
step:1231/2315 train_time:74323ms step_avg:60.38ms
step:1232/2315 train_time:74383ms step_avg:60.38ms
step:1233/2315 train_time:74444ms step_avg:60.38ms
step:1234/2315 train_time:74505ms step_avg:60.38ms
step:1235/2315 train_time:74566ms step_avg:60.38ms
step:1236/2315 train_time:74627ms step_avg:60.38ms
step:1237/2315 train_time:74688ms step_avg:60.38ms
step:1238/2315 train_time:74748ms step_avg:60.38ms
step:1239/2315 train_time:74809ms step_avg:60.38ms
step:1240/2315 train_time:74870ms step_avg:60.38ms
step:1241/2315 train_time:74930ms step_avg:60.38ms
step:1242/2315 train_time:74991ms step_avg:60.38ms
step:1243/2315 train_time:75052ms step_avg:60.38ms
step:1244/2315 train_time:75113ms step_avg:60.38ms
step:1245/2315 train_time:75173ms step_avg:60.38ms
step:1246/2315 train_time:75233ms step_avg:60.38ms
step:1247/2315 train_time:75294ms step_avg:60.38ms
step:1248/2315 train_time:75354ms step_avg:60.38ms
step:1249/2315 train_time:75415ms step_avg:60.38ms
step:1250/2315 train_time:75476ms step_avg:60.38ms
step:1250/2315 val_loss:3.5167 train_time:75539ms step_avg:60.43ms
step:1251/2315 train_time:75559ms step_avg:60.40ms
step:1252/2315 train_time:75601ms step_avg:60.38ms
step:1253/2315 train_time:75665ms step_avg:60.39ms
step:1254/2315 train_time:75727ms step_avg:60.39ms
step:1255/2315 train_time:75788ms step_avg:60.39ms
step:1256/2315 train_time:75848ms step_avg:60.39ms
step:1257/2315 train_time:75909ms step_avg:60.39ms
step:1258/2315 train_time:75969ms step_avg:60.39ms
step:1259/2315 train_time:76029ms step_avg:60.39ms
step:1260/2315 train_time:76090ms step_avg:60.39ms
step:1261/2315 train_time:76150ms step_avg:60.39ms
step:1262/2315 train_time:76211ms step_avg:60.39ms
step:1263/2315 train_time:76271ms step_avg:60.39ms
step:1264/2315 train_time:76331ms step_avg:60.39ms
step:1265/2315 train_time:76392ms step_avg:60.39ms
step:1266/2315 train_time:76452ms step_avg:60.39ms
step:1267/2315 train_time:76514ms step_avg:60.39ms
step:1268/2315 train_time:76576ms step_avg:60.39ms
step:1269/2315 train_time:76639ms step_avg:60.39ms
step:1270/2315 train_time:76700ms step_avg:60.39ms
step:1271/2315 train_time:76762ms step_avg:60.39ms
step:1272/2315 train_time:76823ms step_avg:60.40ms
step:1273/2315 train_time:76884ms step_avg:60.40ms
step:1274/2315 train_time:76944ms step_avg:60.40ms
step:1275/2315 train_time:77004ms step_avg:60.40ms
step:1276/2315 train_time:77065ms step_avg:60.40ms
step:1277/2315 train_time:77126ms step_avg:60.40ms
step:1278/2315 train_time:77186ms step_avg:60.40ms
step:1279/2315 train_time:77247ms step_avg:60.40ms
step:1280/2315 train_time:77307ms step_avg:60.40ms
step:1281/2315 train_time:77367ms step_avg:60.40ms
step:1282/2315 train_time:77427ms step_avg:60.40ms
step:1283/2315 train_time:77488ms step_avg:60.40ms
step:1284/2315 train_time:77548ms step_avg:60.40ms
step:1285/2315 train_time:77610ms step_avg:60.40ms
step:1286/2315 train_time:77672ms step_avg:60.40ms
step:1287/2315 train_time:77735ms step_avg:60.40ms
step:1288/2315 train_time:77796ms step_avg:60.40ms
step:1289/2315 train_time:77857ms step_avg:60.40ms
step:1290/2315 train_time:77918ms step_avg:60.40ms
step:1291/2315 train_time:77979ms step_avg:60.40ms
step:1292/2315 train_time:78039ms step_avg:60.40ms
step:1293/2315 train_time:78100ms step_avg:60.40ms
step:1294/2315 train_time:78160ms step_avg:60.40ms
step:1295/2315 train_time:78221ms step_avg:60.40ms
step:1296/2315 train_time:78281ms step_avg:60.40ms
step:1297/2315 train_time:78342ms step_avg:60.40ms
step:1298/2315 train_time:78403ms step_avg:60.40ms
step:1299/2315 train_time:78464ms step_avg:60.40ms
step:1300/2315 train_time:78524ms step_avg:60.40ms
step:1301/2315 train_time:78585ms step_avg:60.40ms
step:1302/2315 train_time:78646ms step_avg:60.40ms
step:1303/2315 train_time:78708ms step_avg:60.40ms
step:1304/2315 train_time:78768ms step_avg:60.40ms
step:1305/2315 train_time:78829ms step_avg:60.41ms
step:1306/2315 train_time:78890ms step_avg:60.41ms
step:1307/2315 train_time:78951ms step_avg:60.41ms
step:1308/2315 train_time:79013ms step_avg:60.41ms
step:1309/2315 train_time:79074ms step_avg:60.41ms
step:1310/2315 train_time:79135ms step_avg:60.41ms
step:1311/2315 train_time:79196ms step_avg:60.41ms
step:1312/2315 train_time:79256ms step_avg:60.41ms
step:1313/2315 train_time:79317ms step_avg:60.41ms
step:1314/2315 train_time:79378ms step_avg:60.41ms
step:1315/2315 train_time:79440ms step_avg:60.41ms
step:1316/2315 train_time:79500ms step_avg:60.41ms
step:1317/2315 train_time:79561ms step_avg:60.41ms
step:1318/2315 train_time:79621ms step_avg:60.41ms
step:1319/2315 train_time:79682ms step_avg:60.41ms
step:1320/2315 train_time:79743ms step_avg:60.41ms
step:1321/2315 train_time:79804ms step_avg:60.41ms
step:1322/2315 train_time:79865ms step_avg:60.41ms
step:1323/2315 train_time:79927ms step_avg:60.41ms
step:1324/2315 train_time:79987ms step_avg:60.41ms
step:1325/2315 train_time:80048ms step_avg:60.41ms
step:1326/2315 train_time:80109ms step_avg:60.41ms
step:1327/2315 train_time:80169ms step_avg:60.41ms
step:1328/2315 train_time:80230ms step_avg:60.41ms
step:1329/2315 train_time:80291ms step_avg:60.41ms
step:1330/2315 train_time:80353ms step_avg:60.42ms
step:1331/2315 train_time:80414ms step_avg:60.42ms
step:1332/2315 train_time:80475ms step_avg:60.42ms
step:1333/2315 train_time:80536ms step_avg:60.42ms
step:1334/2315 train_time:80597ms step_avg:60.42ms
step:1335/2315 train_time:80657ms step_avg:60.42ms
step:1336/2315 train_time:80718ms step_avg:60.42ms
step:1337/2315 train_time:80780ms step_avg:60.42ms
step:1338/2315 train_time:80840ms step_avg:60.42ms
step:1339/2315 train_time:80901ms step_avg:60.42ms
step:1340/2315 train_time:80962ms step_avg:60.42ms
step:1341/2315 train_time:81023ms step_avg:60.42ms
step:1342/2315 train_time:81084ms step_avg:60.42ms
step:1343/2315 train_time:81145ms step_avg:60.42ms
step:1344/2315 train_time:81205ms step_avg:60.42ms
step:1345/2315 train_time:81267ms step_avg:60.42ms
step:1346/2315 train_time:81328ms step_avg:60.42ms
step:1347/2315 train_time:81388ms step_avg:60.42ms
step:1348/2315 train_time:81448ms step_avg:60.42ms
step:1349/2315 train_time:81509ms step_avg:60.42ms
step:1350/2315 train_time:81570ms step_avg:60.42ms
step:1351/2315 train_time:81631ms step_avg:60.42ms
step:1352/2315 train_time:81692ms step_avg:60.42ms
step:1353/2315 train_time:81754ms step_avg:60.42ms
step:1354/2315 train_time:81815ms step_avg:60.42ms
step:1355/2315 train_time:81876ms step_avg:60.43ms
step:1356/2315 train_time:81937ms step_avg:60.43ms
step:1357/2315 train_time:81998ms step_avg:60.43ms
step:1358/2315 train_time:82058ms step_avg:60.43ms
step:1359/2315 train_time:82120ms step_avg:60.43ms
step:1360/2315 train_time:82180ms step_avg:60.43ms
step:1361/2315 train_time:82241ms step_avg:60.43ms
step:1362/2315 train_time:82302ms step_avg:60.43ms
step:1363/2315 train_time:82363ms step_avg:60.43ms
step:1364/2315 train_time:82423ms step_avg:60.43ms
step:1365/2315 train_time:82484ms step_avg:60.43ms
step:1366/2315 train_time:82545ms step_avg:60.43ms
step:1367/2315 train_time:82606ms step_avg:60.43ms
step:1368/2315 train_time:82667ms step_avg:60.43ms
step:1369/2315 train_time:82727ms step_avg:60.43ms
step:1370/2315 train_time:82787ms step_avg:60.43ms
step:1371/2315 train_time:82848ms step_avg:60.43ms
step:1372/2315 train_time:82909ms step_avg:60.43ms
step:1373/2315 train_time:82970ms step_avg:60.43ms
step:1374/2315 train_time:83031ms step_avg:60.43ms
step:1375/2315 train_time:83093ms step_avg:60.43ms
step:1376/2315 train_time:83154ms step_avg:60.43ms
step:1377/2315 train_time:83215ms step_avg:60.43ms
step:1378/2315 train_time:83276ms step_avg:60.43ms
step:1379/2315 train_time:83337ms step_avg:60.43ms
step:1380/2315 train_time:83398ms step_avg:60.43ms
step:1381/2315 train_time:83459ms step_avg:60.43ms
step:1382/2315 train_time:83520ms step_avg:60.43ms
step:1383/2315 train_time:83581ms step_avg:60.43ms
step:1384/2315 train_time:83642ms step_avg:60.43ms
step:1385/2315 train_time:83702ms step_avg:60.43ms
step:1386/2315 train_time:83763ms step_avg:60.44ms
step:1387/2315 train_time:83824ms step_avg:60.44ms
step:1388/2315 train_time:83884ms step_avg:60.44ms
step:1389/2315 train_time:83945ms step_avg:60.44ms
step:1390/2315 train_time:84007ms step_avg:60.44ms
step:1391/2315 train_time:84067ms step_avg:60.44ms
step:1392/2315 train_time:84127ms step_avg:60.44ms
step:1393/2315 train_time:84188ms step_avg:60.44ms
step:1394/2315 train_time:84249ms step_avg:60.44ms
step:1395/2315 train_time:84311ms step_avg:60.44ms
step:1396/2315 train_time:84372ms step_avg:60.44ms
step:1397/2315 train_time:84434ms step_avg:60.44ms
step:1398/2315 train_time:84494ms step_avg:60.44ms
step:1399/2315 train_time:84556ms step_avg:60.44ms
step:1400/2315 train_time:84617ms step_avg:60.44ms
step:1401/2315 train_time:84678ms step_avg:60.44ms
step:1402/2315 train_time:84739ms step_avg:60.44ms
step:1403/2315 train_time:84800ms step_avg:60.44ms
step:1404/2315 train_time:84860ms step_avg:60.44ms
step:1405/2315 train_time:84922ms step_avg:60.44ms
step:1406/2315 train_time:84982ms step_avg:60.44ms
step:1407/2315 train_time:85043ms step_avg:60.44ms
step:1408/2315 train_time:85103ms step_avg:60.44ms
step:1409/2315 train_time:85164ms step_avg:60.44ms
step:1410/2315 train_time:85225ms step_avg:60.44ms
step:1411/2315 train_time:85285ms step_avg:60.44ms
step:1412/2315 train_time:85346ms step_avg:60.44ms
step:1413/2315 train_time:85407ms step_avg:60.44ms
step:1414/2315 train_time:85467ms step_avg:60.44ms
step:1415/2315 train_time:85528ms step_avg:60.44ms
step:1416/2315 train_time:85588ms step_avg:60.44ms
step:1417/2315 train_time:85649ms step_avg:60.44ms
step:1418/2315 train_time:85710ms step_avg:60.44ms
step:1419/2315 train_time:85772ms step_avg:60.45ms
step:1420/2315 train_time:85833ms step_avg:60.45ms
step:1421/2315 train_time:85895ms step_avg:60.45ms
step:1422/2315 train_time:85956ms step_avg:60.45ms
step:1423/2315 train_time:86017ms step_avg:60.45ms
step:1424/2315 train_time:86077ms step_avg:60.45ms
step:1425/2315 train_time:86138ms step_avg:60.45ms
step:1426/2315 train_time:86199ms step_avg:60.45ms
step:1427/2315 train_time:86260ms step_avg:60.45ms
step:1428/2315 train_time:86320ms step_avg:60.45ms
step:1429/2315 train_time:86381ms step_avg:60.45ms
step:1430/2315 train_time:86443ms step_avg:60.45ms
step:1431/2315 train_time:86503ms step_avg:60.45ms
step:1432/2315 train_time:86564ms step_avg:60.45ms
step:1433/2315 train_time:86625ms step_avg:60.45ms
step:1434/2315 train_time:86686ms step_avg:60.45ms
step:1435/2315 train_time:86747ms step_avg:60.45ms
step:1436/2315 train_time:86807ms step_avg:60.45ms
step:1437/2315 train_time:86868ms step_avg:60.45ms
step:1438/2315 train_time:86928ms step_avg:60.45ms
step:1439/2315 train_time:86989ms step_avg:60.45ms
step:1440/2315 train_time:87050ms step_avg:60.45ms
step:1441/2315 train_time:87111ms step_avg:60.45ms
step:1442/2315 train_time:87172ms step_avg:60.45ms
step:1443/2315 train_time:87234ms step_avg:60.45ms
step:1444/2315 train_time:87295ms step_avg:60.45ms
step:1445/2315 train_time:87357ms step_avg:60.45ms
step:1446/2315 train_time:87417ms step_avg:60.45ms
step:1447/2315 train_time:87478ms step_avg:60.45ms
step:1448/2315 train_time:87539ms step_avg:60.46ms
step:1449/2315 train_time:87600ms step_avg:60.46ms
step:1450/2315 train_time:87661ms step_avg:60.46ms
step:1451/2315 train_time:87722ms step_avg:60.46ms
step:1452/2315 train_time:87782ms step_avg:60.46ms
step:1453/2315 train_time:87842ms step_avg:60.46ms
step:1454/2315 train_time:87903ms step_avg:60.46ms
step:1455/2315 train_time:87964ms step_avg:60.46ms
step:1456/2315 train_time:88026ms step_avg:60.46ms
step:1457/2315 train_time:88086ms step_avg:60.46ms
step:1458/2315 train_time:88146ms step_avg:60.46ms
step:1459/2315 train_time:88208ms step_avg:60.46ms
step:1460/2315 train_time:88267ms step_avg:60.46ms
step:1461/2315 train_time:88329ms step_avg:60.46ms
step:1462/2315 train_time:88390ms step_avg:60.46ms
step:1463/2315 train_time:88451ms step_avg:60.46ms
step:1464/2315 train_time:88512ms step_avg:60.46ms
step:1465/2315 train_time:88573ms step_avg:60.46ms
step:1466/2315 train_time:88634ms step_avg:60.46ms
step:1467/2315 train_time:88696ms step_avg:60.46ms
step:1468/2315 train_time:88756ms step_avg:60.46ms
step:1469/2315 train_time:88817ms step_avg:60.46ms
step:1470/2315 train_time:88878ms step_avg:60.46ms
step:1471/2315 train_time:88940ms step_avg:60.46ms
step:1472/2315 train_time:89000ms step_avg:60.46ms
step:1473/2315 train_time:89061ms step_avg:60.46ms
step:1474/2315 train_time:89121ms step_avg:60.46ms
step:1475/2315 train_time:89182ms step_avg:60.46ms
step:1476/2315 train_time:89242ms step_avg:60.46ms
step:1477/2315 train_time:89303ms step_avg:60.46ms
step:1478/2315 train_time:89364ms step_avg:60.46ms
step:1479/2315 train_time:89425ms step_avg:60.46ms
step:1480/2315 train_time:89486ms step_avg:60.46ms
step:1481/2315 train_time:89547ms step_avg:60.46ms
step:1482/2315 train_time:89608ms step_avg:60.46ms
step:1483/2315 train_time:89669ms step_avg:60.46ms
step:1484/2315 train_time:89729ms step_avg:60.46ms
step:1485/2315 train_time:89791ms step_avg:60.47ms
step:1486/2315 train_time:89852ms step_avg:60.47ms
step:1487/2315 train_time:89914ms step_avg:60.47ms
step:1488/2315 train_time:89975ms step_avg:60.47ms
step:1489/2315 train_time:90036ms step_avg:60.47ms
step:1490/2315 train_time:90097ms step_avg:60.47ms
step:1491/2315 train_time:90158ms step_avg:60.47ms
step:1492/2315 train_time:90218ms step_avg:60.47ms
step:1493/2315 train_time:90280ms step_avg:60.47ms
step:1494/2315 train_time:90340ms step_avg:60.47ms
step:1495/2315 train_time:90401ms step_avg:60.47ms
step:1496/2315 train_time:90461ms step_avg:60.47ms
step:1497/2315 train_time:90522ms step_avg:60.47ms
step:1498/2315 train_time:90583ms step_avg:60.47ms
step:1499/2315 train_time:90644ms step_avg:60.47ms
step:1500/2315 train_time:90706ms step_avg:60.47ms
step:1500/2315 val_loss:3.4516 train_time:90769ms step_avg:60.51ms
step:1501/2315 train_time:90788ms step_avg:60.49ms
step:1502/2315 train_time:90829ms step_avg:60.47ms
step:1503/2315 train_time:90896ms step_avg:60.48ms
step:1504/2315 train_time:90959ms step_avg:60.48ms
step:1505/2315 train_time:91020ms step_avg:60.48ms
step:1506/2315 train_time:91081ms step_avg:60.48ms
step:1507/2315 train_time:91141ms step_avg:60.48ms
step:1508/2315 train_time:91201ms step_avg:60.48ms
step:1509/2315 train_time:91261ms step_avg:60.48ms
step:1510/2315 train_time:91321ms step_avg:60.48ms
step:1511/2315 train_time:91381ms step_avg:60.48ms
step:1512/2315 train_time:91441ms step_avg:60.48ms
step:1513/2315 train_time:91501ms step_avg:60.48ms
step:1514/2315 train_time:91562ms step_avg:60.48ms
step:1515/2315 train_time:91622ms step_avg:60.48ms
step:1516/2315 train_time:91682ms step_avg:60.48ms
step:1517/2315 train_time:91743ms step_avg:60.48ms
step:1518/2315 train_time:91804ms step_avg:60.48ms
step:1519/2315 train_time:91866ms step_avg:60.48ms
step:1520/2315 train_time:91928ms step_avg:60.48ms
step:1521/2315 train_time:91989ms step_avg:60.48ms
step:1522/2315 train_time:92051ms step_avg:60.48ms
step:1523/2315 train_time:92112ms step_avg:60.48ms
step:1524/2315 train_time:92174ms step_avg:60.48ms
step:1525/2315 train_time:92235ms step_avg:60.48ms
step:1526/2315 train_time:92296ms step_avg:60.48ms
step:1527/2315 train_time:92357ms step_avg:60.48ms
step:1528/2315 train_time:92418ms step_avg:60.48ms
step:1529/2315 train_time:92479ms step_avg:60.48ms
step:1530/2315 train_time:92540ms step_avg:60.48ms
step:1531/2315 train_time:92601ms step_avg:60.48ms
step:1532/2315 train_time:92661ms step_avg:60.48ms
step:1533/2315 train_time:92722ms step_avg:60.48ms
step:1534/2315 train_time:92783ms step_avg:60.48ms
step:1535/2315 train_time:92845ms step_avg:60.49ms
step:1536/2315 train_time:92907ms step_avg:60.49ms
step:1537/2315 train_time:92968ms step_avg:60.49ms
step:1538/2315 train_time:93029ms step_avg:60.49ms
step:1539/2315 train_time:93091ms step_avg:60.49ms
step:1540/2315 train_time:93152ms step_avg:60.49ms
step:1541/2315 train_time:93215ms step_avg:60.49ms
step:1542/2315 train_time:93276ms step_avg:60.49ms
step:1543/2315 train_time:93337ms step_avg:60.49ms
step:1544/2315 train_time:93397ms step_avg:60.49ms
step:1545/2315 train_time:93459ms step_avg:60.49ms
step:1546/2315 train_time:93519ms step_avg:60.49ms
step:1547/2315 train_time:93580ms step_avg:60.49ms
step:1548/2315 train_time:93641ms step_avg:60.49ms
step:1549/2315 train_time:93702ms step_avg:60.49ms
step:1550/2315 train_time:93763ms step_avg:60.49ms
step:1551/2315 train_time:93825ms step_avg:60.49ms
step:1552/2315 train_time:93886ms step_avg:60.49ms
step:1553/2315 train_time:93947ms step_avg:60.49ms
step:1554/2315 train_time:94008ms step_avg:60.49ms
step:1555/2315 train_time:94070ms step_avg:60.50ms
step:1556/2315 train_time:94132ms step_avg:60.50ms
step:1557/2315 train_time:94194ms step_avg:60.50ms
step:1558/2315 train_time:94255ms step_avg:60.50ms
step:1559/2315 train_time:94317ms step_avg:60.50ms
step:1560/2315 train_time:94378ms step_avg:60.50ms
step:1561/2315 train_time:94439ms step_avg:60.50ms
step:1562/2315 train_time:94500ms step_avg:60.50ms
step:1563/2315 train_time:94561ms step_avg:60.50ms
step:1564/2315 train_time:94622ms step_avg:60.50ms
step:1565/2315 train_time:94682ms step_avg:60.50ms
step:1566/2315 train_time:94743ms step_avg:60.50ms
step:1567/2315 train_time:94804ms step_avg:60.50ms
step:1568/2315 train_time:94866ms step_avg:60.50ms
step:1569/2315 train_time:94926ms step_avg:60.50ms
step:1570/2315 train_time:94987ms step_avg:60.50ms
step:1571/2315 train_time:95048ms step_avg:60.50ms
step:1572/2315 train_time:95109ms step_avg:60.50ms
step:1573/2315 train_time:95171ms step_avg:60.50ms
step:1574/2315 train_time:95232ms step_avg:60.50ms
step:1575/2315 train_time:95294ms step_avg:60.50ms
step:1576/2315 train_time:95355ms step_avg:60.50ms
step:1577/2315 train_time:95417ms step_avg:60.51ms
step:1578/2315 train_time:95478ms step_avg:60.51ms
step:1579/2315 train_time:95540ms step_avg:60.51ms
step:1580/2315 train_time:95601ms step_avg:60.51ms
step:1581/2315 train_time:95662ms step_avg:60.51ms
step:1582/2315 train_time:95723ms step_avg:60.51ms
step:1583/2315 train_time:95784ms step_avg:60.51ms
step:1584/2315 train_time:95845ms step_avg:60.51ms
step:1585/2315 train_time:95906ms step_avg:60.51ms
step:1586/2315 train_time:95967ms step_avg:60.51ms
step:1587/2315 train_time:96028ms step_avg:60.51ms
step:1588/2315 train_time:96089ms step_avg:60.51ms
step:1589/2315 train_time:96150ms step_avg:60.51ms
step:1590/2315 train_time:96212ms step_avg:60.51ms
step:1591/2315 train_time:96274ms step_avg:60.51ms
step:1592/2315 train_time:96335ms step_avg:60.51ms
step:1593/2315 train_time:96397ms step_avg:60.51ms
step:1594/2315 train_time:96458ms step_avg:60.51ms
step:1595/2315 train_time:96520ms step_avg:60.51ms
step:1596/2315 train_time:96581ms step_avg:60.51ms
step:1597/2315 train_time:96642ms step_avg:60.51ms
step:1598/2315 train_time:96702ms step_avg:60.51ms
step:1599/2315 train_time:96764ms step_avg:60.52ms
step:1600/2315 train_time:96825ms step_avg:60.52ms
step:1601/2315 train_time:96886ms step_avg:60.52ms
step:1602/2315 train_time:96947ms step_avg:60.52ms
step:1603/2315 train_time:97008ms step_avg:60.52ms
step:1604/2315 train_time:97068ms step_avg:60.52ms
step:1605/2315 train_time:97130ms step_avg:60.52ms
step:1606/2315 train_time:97191ms step_avg:60.52ms
step:1607/2315 train_time:97253ms step_avg:60.52ms
step:1608/2315 train_time:97314ms step_avg:60.52ms
step:1609/2315 train_time:97376ms step_avg:60.52ms
step:1610/2315 train_time:97437ms step_avg:60.52ms
step:1611/2315 train_time:97498ms step_avg:60.52ms
step:1612/2315 train_time:97559ms step_avg:60.52ms
step:1613/2315 train_time:97621ms step_avg:60.52ms
step:1614/2315 train_time:97681ms step_avg:60.52ms
step:1615/2315 train_time:97743ms step_avg:60.52ms
step:1616/2315 train_time:97804ms step_avg:60.52ms
step:1617/2315 train_time:97864ms step_avg:60.52ms
step:1618/2315 train_time:97925ms step_avg:60.52ms
step:1619/2315 train_time:97986ms step_avg:60.52ms
step:1620/2315 train_time:98047ms step_avg:60.52ms
step:1621/2315 train_time:98108ms step_avg:60.52ms
step:1622/2315 train_time:98169ms step_avg:60.52ms
step:1623/2315 train_time:98231ms step_avg:60.52ms
step:1624/2315 train_time:98292ms step_avg:60.52ms
step:1625/2315 train_time:98354ms step_avg:60.53ms
step:1626/2315 train_time:98415ms step_avg:60.53ms
step:1627/2315 train_time:98477ms step_avg:60.53ms
step:1628/2315 train_time:98538ms step_avg:60.53ms
step:1629/2315 train_time:98599ms step_avg:60.53ms
step:1630/2315 train_time:98660ms step_avg:60.53ms
step:1631/2315 train_time:98722ms step_avg:60.53ms
step:1632/2315 train_time:98783ms step_avg:60.53ms
step:1633/2315 train_time:98844ms step_avg:60.53ms
step:1634/2315 train_time:98905ms step_avg:60.53ms
step:1635/2315 train_time:98965ms step_avg:60.53ms
step:1636/2315 train_time:99026ms step_avg:60.53ms
step:1637/2315 train_time:99087ms step_avg:60.53ms
step:1638/2315 train_time:99148ms step_avg:60.53ms
step:1639/2315 train_time:99209ms step_avg:60.53ms
step:1640/2315 train_time:99271ms step_avg:60.53ms
step:1641/2315 train_time:99333ms step_avg:60.53ms
step:1642/2315 train_time:99394ms step_avg:60.53ms
step:1643/2315 train_time:99459ms step_avg:60.54ms
step:1644/2315 train_time:99518ms step_avg:60.53ms
step:1645/2315 train_time:99579ms step_avg:60.53ms
step:1646/2315 train_time:99640ms step_avg:60.53ms
step:1647/2315 train_time:99702ms step_avg:60.54ms
step:1648/2315 train_time:99763ms step_avg:60.54ms
step:1649/2315 train_time:99824ms step_avg:60.54ms
step:1650/2315 train_time:99885ms step_avg:60.54ms
step:1651/2315 train_time:99946ms step_avg:60.54ms
step:1652/2315 train_time:100007ms step_avg:60.54ms
step:1653/2315 train_time:100067ms step_avg:60.54ms
step:1654/2315 train_time:100129ms step_avg:60.54ms
step:1655/2315 train_time:100190ms step_avg:60.54ms
step:1656/2315 train_time:100252ms step_avg:60.54ms
step:1657/2315 train_time:100314ms step_avg:60.54ms
step:1658/2315 train_time:100375ms step_avg:60.54ms
step:1659/2315 train_time:100436ms step_avg:60.54ms
step:1660/2315 train_time:100497ms step_avg:60.54ms
step:1661/2315 train_time:100559ms step_avg:60.54ms
step:1662/2315 train_time:100619ms step_avg:60.54ms
step:1663/2315 train_time:100681ms step_avg:60.54ms
step:1664/2315 train_time:100742ms step_avg:60.54ms
step:1665/2315 train_time:100803ms step_avg:60.54ms
step:1666/2315 train_time:100864ms step_avg:60.54ms
step:1667/2315 train_time:100925ms step_avg:60.54ms
step:1668/2315 train_time:100986ms step_avg:60.54ms
step:1669/2315 train_time:101047ms step_avg:60.54ms
step:1670/2315 train_time:101108ms step_avg:60.54ms
step:1671/2315 train_time:101169ms step_avg:60.54ms
step:1672/2315 train_time:101230ms step_avg:60.54ms
step:1673/2315 train_time:101293ms step_avg:60.55ms
step:1674/2315 train_time:101354ms step_avg:60.55ms
step:1675/2315 train_time:101416ms step_avg:60.55ms
step:1676/2315 train_time:101477ms step_avg:60.55ms
step:1677/2315 train_time:101539ms step_avg:60.55ms
step:1678/2315 train_time:101600ms step_avg:60.55ms
step:1679/2315 train_time:101661ms step_avg:60.55ms
step:1680/2315 train_time:101722ms step_avg:60.55ms
step:1681/2315 train_time:101784ms step_avg:60.55ms
step:1682/2315 train_time:101844ms step_avg:60.55ms
step:1683/2315 train_time:101905ms step_avg:60.55ms
step:1684/2315 train_time:101966ms step_avg:60.55ms
step:1685/2315 train_time:102027ms step_avg:60.55ms
step:1686/2315 train_time:102087ms step_avg:60.55ms
step:1687/2315 train_time:102149ms step_avg:60.55ms
step:1688/2315 train_time:102209ms step_avg:60.55ms
step:1689/2315 train_time:102271ms step_avg:60.55ms
step:1690/2315 train_time:102333ms step_avg:60.55ms
step:1691/2315 train_time:102395ms step_avg:60.55ms
step:1692/2315 train_time:102456ms step_avg:60.55ms
step:1693/2315 train_time:102518ms step_avg:60.55ms
step:1694/2315 train_time:102579ms step_avg:60.55ms
step:1695/2315 train_time:102640ms step_avg:60.55ms
step:1696/2315 train_time:102701ms step_avg:60.56ms
step:1697/2315 train_time:102763ms step_avg:60.56ms
step:1698/2315 train_time:102824ms step_avg:60.56ms
step:1699/2315 train_time:102885ms step_avg:60.56ms
step:1700/2315 train_time:102946ms step_avg:60.56ms
step:1701/2315 train_time:103007ms step_avg:60.56ms
step:1702/2315 train_time:103068ms step_avg:60.56ms
step:1703/2315 train_time:103129ms step_avg:60.56ms
step:1704/2315 train_time:103189ms step_avg:60.56ms
step:1705/2315 train_time:103250ms step_avg:60.56ms
step:1706/2315 train_time:103312ms step_avg:60.56ms
step:1707/2315 train_time:103374ms step_avg:60.56ms
step:1708/2315 train_time:103435ms step_avg:60.56ms
step:1709/2315 train_time:103497ms step_avg:60.56ms
step:1710/2315 train_time:103558ms step_avg:60.56ms
step:1711/2315 train_time:103619ms step_avg:60.56ms
step:1712/2315 train_time:103681ms step_avg:60.56ms
step:1713/2315 train_time:103742ms step_avg:60.56ms
step:1714/2315 train_time:103803ms step_avg:60.56ms
step:1715/2315 train_time:103864ms step_avg:60.56ms
step:1716/2315 train_time:103925ms step_avg:60.56ms
step:1717/2315 train_time:103986ms step_avg:60.56ms
step:1718/2315 train_time:104047ms step_avg:60.56ms
step:1719/2315 train_time:104108ms step_avg:60.56ms
step:1720/2315 train_time:104169ms step_avg:60.56ms
step:1721/2315 train_time:104230ms step_avg:60.56ms
step:1722/2315 train_time:104291ms step_avg:60.56ms
step:1723/2315 train_time:104353ms step_avg:60.56ms
step:1724/2315 train_time:104414ms step_avg:60.57ms
step:1725/2315 train_time:104476ms step_avg:60.57ms
step:1726/2315 train_time:104538ms step_avg:60.57ms
step:1727/2315 train_time:104600ms step_avg:60.57ms
step:1728/2315 train_time:104661ms step_avg:60.57ms
step:1729/2315 train_time:104722ms step_avg:60.57ms
step:1730/2315 train_time:104783ms step_avg:60.57ms
step:1731/2315 train_time:104845ms step_avg:60.57ms
step:1732/2315 train_time:104906ms step_avg:60.57ms
step:1733/2315 train_time:104967ms step_avg:60.57ms
step:1734/2315 train_time:105028ms step_avg:60.57ms
step:1735/2315 train_time:105089ms step_avg:60.57ms
step:1736/2315 train_time:105149ms step_avg:60.57ms
step:1737/2315 train_time:105211ms step_avg:60.57ms
step:1738/2315 train_time:105272ms step_avg:60.57ms
step:1739/2315 train_time:105334ms step_avg:60.57ms
step:1740/2315 train_time:105396ms step_avg:60.57ms
step:1741/2315 train_time:105458ms step_avg:60.57ms
step:1742/2315 train_time:105519ms step_avg:60.57ms
step:1743/2315 train_time:105580ms step_avg:60.57ms
step:1744/2315 train_time:105642ms step_avg:60.57ms
step:1745/2315 train_time:105703ms step_avg:60.57ms
step:1746/2315 train_time:105764ms step_avg:60.57ms
step:1747/2315 train_time:105825ms step_avg:60.58ms
step:1748/2315 train_time:105886ms step_avg:60.58ms
step:1749/2315 train_time:105947ms step_avg:60.58ms
step:1750/2315 train_time:106007ms step_avg:60.58ms
step:1750/2315 val_loss:3.3825 train_time:106070ms step_avg:60.61ms
step:1751/2315 train_time:106090ms step_avg:60.59ms
step:1752/2315 train_time:106130ms step_avg:60.58ms
step:1753/2315 train_time:106198ms step_avg:60.58ms
step:1754/2315 train_time:106263ms step_avg:60.58ms
step:1755/2315 train_time:106324ms step_avg:60.58ms
step:1756/2315 train_time:106385ms step_avg:60.58ms
step:1757/2315 train_time:106445ms step_avg:60.58ms
step:1758/2315 train_time:106505ms step_avg:60.58ms
step:1759/2315 train_time:106566ms step_avg:60.58ms
step:1760/2315 train_time:106626ms step_avg:60.58ms
step:1761/2315 train_time:106687ms step_avg:60.58ms
step:1762/2315 train_time:106747ms step_avg:60.58ms
step:1763/2315 train_time:106807ms step_avg:60.58ms
step:1764/2315 train_time:106867ms step_avg:60.58ms
step:1765/2315 train_time:106927ms step_avg:60.58ms
step:1766/2315 train_time:106990ms step_avg:60.58ms
step:1767/2315 train_time:107052ms step_avg:60.58ms
step:1768/2315 train_time:107114ms step_avg:60.58ms
step:1769/2315 train_time:107176ms step_avg:60.59ms
step:1770/2315 train_time:107238ms step_avg:60.59ms
step:1771/2315 train_time:107300ms step_avg:60.59ms
step:1772/2315 train_time:107361ms step_avg:60.59ms
step:1773/2315 train_time:107422ms step_avg:60.59ms
step:1774/2315 train_time:107483ms step_avg:60.59ms
step:1775/2315 train_time:107544ms step_avg:60.59ms
step:1776/2315 train_time:107604ms step_avg:60.59ms
step:1777/2315 train_time:107665ms step_avg:60.59ms
step:1778/2315 train_time:107725ms step_avg:60.59ms
step:1779/2315 train_time:107786ms step_avg:60.59ms
step:1780/2315 train_time:107847ms step_avg:60.59ms
step:1781/2315 train_time:107908ms step_avg:60.59ms
step:1782/2315 train_time:107969ms step_avg:60.59ms
step:1783/2315 train_time:108030ms step_avg:60.59ms
step:1784/2315 train_time:108091ms step_avg:60.59ms
step:1785/2315 train_time:108153ms step_avg:60.59ms
step:1786/2315 train_time:108214ms step_avg:60.59ms
step:1787/2315 train_time:108276ms step_avg:60.59ms
step:1788/2315 train_time:108339ms step_avg:60.59ms
step:1789/2315 train_time:108400ms step_avg:60.59ms
step:1790/2315 train_time:108461ms step_avg:60.59ms
step:1791/2315 train_time:108523ms step_avg:60.59ms
step:1792/2315 train_time:108583ms step_avg:60.59ms
step:1793/2315 train_time:108644ms step_avg:60.59ms
step:1794/2315 train_time:108704ms step_avg:60.59ms
step:1795/2315 train_time:108765ms step_avg:60.59ms
step:1796/2315 train_time:108825ms step_avg:60.59ms
step:1797/2315 train_time:108886ms step_avg:60.59ms
step:1798/2315 train_time:108947ms step_avg:60.59ms
step:1799/2315 train_time:109009ms step_avg:60.59ms
step:1800/2315 train_time:109070ms step_avg:60.59ms
step:1801/2315 train_time:109132ms step_avg:60.60ms
step:1802/2315 train_time:109194ms step_avg:60.60ms
step:1803/2315 train_time:109255ms step_avg:60.60ms
step:1804/2315 train_time:109316ms step_avg:60.60ms
step:1805/2315 train_time:109378ms step_avg:60.60ms
step:1806/2315 train_time:109439ms step_avg:60.60ms
step:1807/2315 train_time:109501ms step_avg:60.60ms
step:1808/2315 train_time:109562ms step_avg:60.60ms
step:1809/2315 train_time:109623ms step_avg:60.60ms
step:1810/2315 train_time:109684ms step_avg:60.60ms
step:1811/2315 train_time:109745ms step_avg:60.60ms
step:1812/2315 train_time:109806ms step_avg:60.60ms
step:1813/2315 train_time:109867ms step_avg:60.60ms
step:1814/2315 train_time:109929ms step_avg:60.60ms
step:1815/2315 train_time:109989ms step_avg:60.60ms
step:1816/2315 train_time:110050ms step_avg:60.60ms
step:1817/2315 train_time:110111ms step_avg:60.60ms
step:1818/2315 train_time:110172ms step_avg:60.60ms
step:1819/2315 train_time:110233ms step_avg:60.60ms
step:1820/2315 train_time:110295ms step_avg:60.60ms
step:1821/2315 train_time:110356ms step_avg:60.60ms
step:1822/2315 train_time:110418ms step_avg:60.60ms
step:1823/2315 train_time:110479ms step_avg:60.60ms
step:1824/2315 train_time:110540ms step_avg:60.60ms
step:1825/2315 train_time:110602ms step_avg:60.60ms
step:1826/2315 train_time:110663ms step_avg:60.60ms
step:1827/2315 train_time:110724ms step_avg:60.60ms
step:1828/2315 train_time:110785ms step_avg:60.60ms
step:1829/2315 train_time:110846ms step_avg:60.60ms
step:1830/2315 train_time:110907ms step_avg:60.61ms
step:1831/2315 train_time:110968ms step_avg:60.61ms
step:1832/2315 train_time:111029ms step_avg:60.61ms
step:1833/2315 train_time:111090ms step_avg:60.61ms
step:1834/2315 train_time:111150ms step_avg:60.61ms
step:1835/2315 train_time:111211ms step_avg:60.61ms
step:1836/2315 train_time:111272ms step_avg:60.61ms
step:1837/2315 train_time:111334ms step_avg:60.61ms
step:1838/2315 train_time:111396ms step_avg:60.61ms
step:1839/2315 train_time:111458ms step_avg:60.61ms
step:1840/2315 train_time:111519ms step_avg:60.61ms
step:1841/2315 train_time:111580ms step_avg:60.61ms
step:1842/2315 train_time:111642ms step_avg:60.61ms
step:1843/2315 train_time:111703ms step_avg:60.61ms
step:1844/2315 train_time:111764ms step_avg:60.61ms
step:1845/2315 train_time:111825ms step_avg:60.61ms
step:1846/2315 train_time:111886ms step_avg:60.61ms
step:1847/2315 train_time:111947ms step_avg:60.61ms
step:1848/2315 train_time:112008ms step_avg:60.61ms
step:1849/2315 train_time:112069ms step_avg:60.61ms
step:1850/2315 train_time:112130ms step_avg:60.61ms
step:1851/2315 train_time:112190ms step_avg:60.61ms
step:1852/2315 train_time:112251ms step_avg:60.61ms
step:1853/2315 train_time:112313ms step_avg:60.61ms
step:1854/2315 train_time:112374ms step_avg:60.61ms
step:1855/2315 train_time:112436ms step_avg:60.61ms
step:1856/2315 train_time:112498ms step_avg:60.61ms
step:1857/2315 train_time:112560ms step_avg:60.61ms
step:1858/2315 train_time:112621ms step_avg:60.61ms
step:1859/2315 train_time:112683ms step_avg:60.61ms
step:1860/2315 train_time:112743ms step_avg:60.61ms
step:1861/2315 train_time:112804ms step_avg:60.61ms
step:1862/2315 train_time:112865ms step_avg:60.61ms
step:1863/2315 train_time:112926ms step_avg:60.62ms
step:1864/2315 train_time:112987ms step_avg:60.62ms
step:1865/2315 train_time:113048ms step_avg:60.62ms
step:1866/2315 train_time:113108ms step_avg:60.62ms
step:1867/2315 train_time:113170ms step_avg:60.62ms
step:1868/2315 train_time:113231ms step_avg:60.62ms
step:1869/2315 train_time:113292ms step_avg:60.62ms
step:1870/2315 train_time:113353ms step_avg:60.62ms
step:1871/2315 train_time:113415ms step_avg:60.62ms
step:1872/2315 train_time:113476ms step_avg:60.62ms
step:1873/2315 train_time:113538ms step_avg:60.62ms
step:1874/2315 train_time:113600ms step_avg:60.62ms
step:1875/2315 train_time:113661ms step_avg:60.62ms
step:1876/2315 train_time:113722ms step_avg:60.62ms
step:1877/2315 train_time:113783ms step_avg:60.62ms
step:1878/2315 train_time:113845ms step_avg:60.62ms
step:1879/2315 train_time:113906ms step_avg:60.62ms
step:1880/2315 train_time:113967ms step_avg:60.62ms
step:1881/2315 train_time:114028ms step_avg:60.62ms
step:1882/2315 train_time:114089ms step_avg:60.62ms
step:1883/2315 train_time:114149ms step_avg:60.62ms
step:1884/2315 train_time:114211ms step_avg:60.62ms
step:1885/2315 train_time:114271ms step_avg:60.62ms
step:1886/2315 train_time:114332ms step_avg:60.62ms
step:1887/2315 train_time:114394ms step_avg:60.62ms
step:1888/2315 train_time:114456ms step_avg:60.62ms
step:1889/2315 train_time:114519ms step_avg:60.62ms
step:1890/2315 train_time:114580ms step_avg:60.62ms
step:1891/2315 train_time:114642ms step_avg:60.63ms
step:1892/2315 train_time:114704ms step_avg:60.63ms
step:1893/2315 train_time:114765ms step_avg:60.63ms
step:1894/2315 train_time:114826ms step_avg:60.63ms
step:1895/2315 train_time:114887ms step_avg:60.63ms
step:1896/2315 train_time:114948ms step_avg:60.63ms
step:1897/2315 train_time:115009ms step_avg:60.63ms
step:1898/2315 train_time:115069ms step_avg:60.63ms
step:1899/2315 train_time:115130ms step_avg:60.63ms
step:1900/2315 train_time:115191ms step_avg:60.63ms
step:1901/2315 train_time:115253ms step_avg:60.63ms
step:1902/2315 train_time:115314ms step_avg:60.63ms
step:1903/2315 train_time:115374ms step_avg:60.63ms
step:1904/2315 train_time:115436ms step_avg:60.63ms
step:1905/2315 train_time:115497ms step_avg:60.63ms
step:1906/2315 train_time:115559ms step_avg:60.63ms
step:1907/2315 train_time:115620ms step_avg:60.63ms
step:1908/2315 train_time:115681ms step_avg:60.63ms
step:1909/2315 train_time:115743ms step_avg:60.63ms
step:1910/2315 train_time:115805ms step_avg:60.63ms
step:1911/2315 train_time:115866ms step_avg:60.63ms
step:1912/2315 train_time:115927ms step_avg:60.63ms
step:1913/2315 train_time:115988ms step_avg:60.63ms
step:1914/2315 train_time:116049ms step_avg:60.63ms
step:1915/2315 train_time:116110ms step_avg:60.63ms
step:1916/2315 train_time:116171ms step_avg:60.63ms
step:1917/2315 train_time:116231ms step_avg:60.63ms
step:1918/2315 train_time:116292ms step_avg:60.63ms
step:1919/2315 train_time:116353ms step_avg:60.63ms
step:1920/2315 train_time:116414ms step_avg:60.63ms
step:1921/2315 train_time:116476ms step_avg:60.63ms
step:1922/2315 train_time:116538ms step_avg:60.63ms
step:1923/2315 train_time:116599ms step_avg:60.63ms
step:1924/2315 train_time:116660ms step_avg:60.63ms
step:1925/2315 train_time:116722ms step_avg:60.63ms
step:1926/2315 train_time:116783ms step_avg:60.64ms
step:1927/2315 train_time:116845ms step_avg:60.64ms
step:1928/2315 train_time:116906ms step_avg:60.64ms
step:1929/2315 train_time:116966ms step_avg:60.64ms
step:1930/2315 train_time:117027ms step_avg:60.64ms
step:1931/2315 train_time:117089ms step_avg:60.64ms
step:1932/2315 train_time:117149ms step_avg:60.64ms
step:1933/2315 train_time:117210ms step_avg:60.64ms
step:1934/2315 train_time:117271ms step_avg:60.64ms
step:1935/2315 train_time:117332ms step_avg:60.64ms
step:1936/2315 train_time:117393ms step_avg:60.64ms
step:1937/2315 train_time:117455ms step_avg:60.64ms
step:1938/2315 train_time:117517ms step_avg:60.64ms
step:1939/2315 train_time:117578ms step_avg:60.64ms
step:1940/2315 train_time:117640ms step_avg:60.64ms
step:1941/2315 train_time:117702ms step_avg:60.64ms
step:1942/2315 train_time:117764ms step_avg:60.64ms
step:1943/2315 train_time:117825ms step_avg:60.64ms
step:1944/2315 train_time:117886ms step_avg:60.64ms
step:1945/2315 train_time:117947ms step_avg:60.64ms
step:1946/2315 train_time:118008ms step_avg:60.64ms
step:1947/2315 train_time:118069ms step_avg:60.64ms
step:1948/2315 train_time:118130ms step_avg:60.64ms
step:1949/2315 train_time:118191ms step_avg:60.64ms
step:1950/2315 train_time:118251ms step_avg:60.64ms
step:1951/2315 train_time:118312ms step_avg:60.64ms
step:1952/2315 train_time:118374ms step_avg:60.64ms
step:1953/2315 train_time:118436ms step_avg:60.64ms
step:1954/2315 train_time:118498ms step_avg:60.64ms
step:1955/2315 train_time:118560ms step_avg:60.64ms
step:1956/2315 train_time:118622ms step_avg:60.65ms
step:1957/2315 train_time:118683ms step_avg:60.65ms
step:1958/2315 train_time:118744ms step_avg:60.65ms
step:1959/2315 train_time:118806ms step_avg:60.65ms
step:1960/2315 train_time:118867ms step_avg:60.65ms
step:1961/2315 train_time:118927ms step_avg:60.65ms
step:1962/2315 train_time:118988ms step_avg:60.65ms
step:1963/2315 train_time:119049ms step_avg:60.65ms
step:1964/2315 train_time:119110ms step_avg:60.65ms
step:1965/2315 train_time:119170ms step_avg:60.65ms
step:1966/2315 train_time:119232ms step_avg:60.65ms
step:1967/2315 train_time:119293ms step_avg:60.65ms
step:1968/2315 train_time:119353ms step_avg:60.65ms
step:1969/2315 train_time:119415ms step_avg:60.65ms
step:1970/2315 train_time:119476ms step_avg:60.65ms
step:1971/2315 train_time:119538ms step_avg:60.65ms
step:1972/2315 train_time:119599ms step_avg:60.65ms
step:1973/2315 train_time:119661ms step_avg:60.65ms
step:1974/2315 train_time:119722ms step_avg:60.65ms
step:1975/2315 train_time:119784ms step_avg:60.65ms
step:1976/2315 train_time:119845ms step_avg:60.65ms
step:1977/2315 train_time:119907ms step_avg:60.65ms
step:1978/2315 train_time:119967ms step_avg:60.65ms
step:1979/2315 train_time:120029ms step_avg:60.65ms
step:1980/2315 train_time:120089ms step_avg:60.65ms
step:1981/2315 train_time:120150ms step_avg:60.65ms
step:1982/2315 train_time:120211ms step_avg:60.65ms
step:1983/2315 train_time:120272ms step_avg:60.65ms
step:1984/2315 train_time:120333ms step_avg:60.65ms
step:1985/2315 train_time:120394ms step_avg:60.65ms
step:1986/2315 train_time:120456ms step_avg:60.65ms
step:1987/2315 train_time:120518ms step_avg:60.65ms
step:1988/2315 train_time:120579ms step_avg:60.65ms
step:1989/2315 train_time:120641ms step_avg:60.65ms
step:1990/2315 train_time:120702ms step_avg:60.65ms
step:1991/2315 train_time:120764ms step_avg:60.65ms
step:1992/2315 train_time:120825ms step_avg:60.66ms
step:1993/2315 train_time:120886ms step_avg:60.66ms
step:1994/2315 train_time:120948ms step_avg:60.66ms
step:1995/2315 train_time:121009ms step_avg:60.66ms
step:1996/2315 train_time:121069ms step_avg:60.66ms
step:1997/2315 train_time:121130ms step_avg:60.66ms
step:1998/2315 train_time:121191ms step_avg:60.66ms
step:1999/2315 train_time:121252ms step_avg:60.66ms
step:2000/2315 train_time:121313ms step_avg:60.66ms
step:2000/2315 val_loss:3.3325 train_time:121376ms step_avg:60.69ms
step:2001/2315 train_time:121397ms step_avg:60.67ms
step:2002/2315 train_time:121439ms step_avg:60.66ms
step:2003/2315 train_time:121504ms step_avg:60.66ms
step:2004/2315 train_time:121567ms step_avg:60.66ms
step:2005/2315 train_time:121629ms step_avg:60.66ms
step:2006/2315 train_time:121690ms step_avg:60.66ms
step:2007/2315 train_time:121751ms step_avg:60.66ms
step:2008/2315 train_time:121811ms step_avg:60.66ms
step:2009/2315 train_time:121873ms step_avg:60.66ms
step:2010/2315 train_time:121933ms step_avg:60.66ms
step:2011/2315 train_time:121994ms step_avg:60.66ms
step:2012/2315 train_time:122054ms step_avg:60.66ms
step:2013/2315 train_time:122115ms step_avg:60.66ms
step:2014/2315 train_time:122176ms step_avg:60.66ms
step:2015/2315 train_time:122237ms step_avg:60.66ms
step:2016/2315 train_time:122298ms step_avg:60.66ms
step:2017/2315 train_time:122361ms step_avg:60.66ms
step:2018/2315 train_time:122423ms step_avg:60.67ms
step:2019/2315 train_time:122486ms step_avg:60.67ms
step:2020/2315 train_time:122549ms step_avg:60.67ms
step:2021/2315 train_time:122611ms step_avg:60.67ms
step:2022/2315 train_time:122672ms step_avg:60.67ms
step:2023/2315 train_time:122732ms step_avg:60.67ms
step:2024/2315 train_time:122793ms step_avg:60.67ms
step:2025/2315 train_time:122855ms step_avg:60.67ms
step:2026/2315 train_time:122916ms step_avg:60.67ms
step:2027/2315 train_time:122976ms step_avg:60.67ms
step:2028/2315 train_time:123037ms step_avg:60.67ms
step:2029/2315 train_time:123098ms step_avg:60.67ms
step:2030/2315 train_time:123159ms step_avg:60.67ms
step:2031/2315 train_time:123220ms step_avg:60.67ms
step:2032/2315 train_time:123281ms step_avg:60.67ms
step:2033/2315 train_time:123342ms step_avg:60.67ms
step:2034/2315 train_time:123404ms step_avg:60.67ms
step:2035/2315 train_time:123467ms step_avg:60.67ms
step:2036/2315 train_time:123528ms step_avg:60.67ms
step:2037/2315 train_time:123589ms step_avg:60.67ms
step:2038/2315 train_time:123650ms step_avg:60.67ms
step:2039/2315 train_time:123712ms step_avg:60.67ms
step:2040/2315 train_time:123773ms step_avg:60.67ms
step:2041/2315 train_time:123834ms step_avg:60.67ms
step:2042/2315 train_time:123895ms step_avg:60.67ms
step:2043/2315 train_time:123956ms step_avg:60.67ms
step:2044/2315 train_time:124017ms step_avg:60.67ms
step:2045/2315 train_time:124078ms step_avg:60.67ms
step:2046/2315 train_time:124139ms step_avg:60.67ms
step:2047/2315 train_time:124200ms step_avg:60.67ms
step:2048/2315 train_time:124262ms step_avg:60.67ms
step:2049/2315 train_time:124323ms step_avg:60.67ms
step:2050/2315 train_time:124385ms step_avg:60.68ms
step:2051/2315 train_time:124447ms step_avg:60.68ms
step:2052/2315 train_time:124509ms step_avg:60.68ms
step:2053/2315 train_time:124570ms step_avg:60.68ms
step:2054/2315 train_time:124631ms step_avg:60.68ms
step:2055/2315 train_time:124693ms step_avg:60.68ms
step:2056/2315 train_time:124754ms step_avg:60.68ms
step:2057/2315 train_time:124815ms step_avg:60.68ms
step:2058/2315 train_time:124876ms step_avg:60.68ms
step:2059/2315 train_time:124938ms step_avg:60.68ms
step:2060/2315 train_time:124998ms step_avg:60.68ms
step:2061/2315 train_time:125059ms step_avg:60.68ms
step:2062/2315 train_time:125120ms step_avg:60.68ms
step:2063/2315 train_time:125182ms step_avg:60.68ms
step:2064/2315 train_time:125243ms step_avg:60.68ms
step:2065/2315 train_time:125304ms step_avg:60.68ms
step:2066/2315 train_time:125365ms step_avg:60.68ms
step:2067/2315 train_time:125427ms step_avg:60.68ms
step:2068/2315 train_time:125489ms step_avg:60.68ms
step:2069/2315 train_time:125550ms step_avg:60.68ms
step:2070/2315 train_time:125611ms step_avg:60.68ms
step:2071/2315 train_time:125672ms step_avg:60.68ms
step:2072/2315 train_time:125733ms step_avg:60.68ms
step:2073/2315 train_time:125795ms step_avg:60.68ms
step:2074/2315 train_time:125856ms step_avg:60.68ms
step:2075/2315 train_time:125917ms step_avg:60.68ms
step:2076/2315 train_time:125978ms step_avg:60.68ms
step:2077/2315 train_time:126039ms step_avg:60.68ms
step:2078/2315 train_time:126100ms step_avg:60.68ms
step:2079/2315 train_time:126161ms step_avg:60.68ms
step:2080/2315 train_time:126222ms step_avg:60.68ms
step:2081/2315 train_time:126283ms step_avg:60.68ms
step:2082/2315 train_time:126344ms step_avg:60.68ms
step:2083/2315 train_time:126406ms step_avg:60.68ms
step:2084/2315 train_time:126467ms step_avg:60.68ms
step:2085/2315 train_time:126529ms step_avg:60.69ms
step:2086/2315 train_time:126590ms step_avg:60.69ms
step:2087/2315 train_time:126651ms step_avg:60.69ms
step:2088/2315 train_time:126712ms step_avg:60.69ms
step:2089/2315 train_time:126773ms step_avg:60.69ms
step:2090/2315 train_time:126834ms step_avg:60.69ms
step:2091/2315 train_time:126895ms step_avg:60.69ms
step:2092/2315 train_time:126956ms step_avg:60.69ms
step:2093/2315 train_time:127018ms step_avg:60.69ms
step:2094/2315 train_time:127079ms step_avg:60.69ms
step:2095/2315 train_time:127141ms step_avg:60.69ms
step:2096/2315 train_time:127202ms step_avg:60.69ms
step:2097/2315 train_time:127264ms step_avg:60.69ms
step:2098/2315 train_time:127325ms step_avg:60.69ms
step:2099/2315 train_time:127386ms step_avg:60.69ms
step:2100/2315 train_time:127448ms step_avg:60.69ms
step:2101/2315 train_time:127509ms step_avg:60.69ms
step:2102/2315 train_time:127570ms step_avg:60.69ms
step:2103/2315 train_time:127631ms step_avg:60.69ms
step:2104/2315 train_time:127692ms step_avg:60.69ms
step:2105/2315 train_time:127754ms step_avg:60.69ms
step:2106/2315 train_time:127815ms step_avg:60.69ms
step:2107/2315 train_time:127876ms step_avg:60.69ms
step:2108/2315 train_time:127937ms step_avg:60.69ms
step:2109/2315 train_time:127999ms step_avg:60.69ms
step:2110/2315 train_time:128060ms step_avg:60.69ms
step:2111/2315 train_time:128121ms step_avg:60.69ms
step:2112/2315 train_time:128182ms step_avg:60.69ms
step:2113/2315 train_time:128244ms step_avg:60.69ms
step:2114/2315 train_time:128305ms step_avg:60.69ms
step:2115/2315 train_time:128366ms step_avg:60.69ms
step:2116/2315 train_time:128427ms step_avg:60.69ms
step:2117/2315 train_time:128489ms step_avg:60.69ms
step:2118/2315 train_time:128550ms step_avg:60.69ms
step:2119/2315 train_time:128611ms step_avg:60.69ms
step:2120/2315 train_time:128672ms step_avg:60.69ms
step:2121/2315 train_time:128733ms step_avg:60.69ms
step:2122/2315 train_time:128794ms step_avg:60.69ms
step:2123/2315 train_time:128856ms step_avg:60.70ms
step:2124/2315 train_time:128916ms step_avg:60.70ms
step:2125/2315 train_time:128978ms step_avg:60.70ms
step:2126/2315 train_time:129039ms step_avg:60.70ms
step:2127/2315 train_time:129100ms step_avg:60.70ms
step:2128/2315 train_time:129161ms step_avg:60.70ms
step:2129/2315 train_time:129223ms step_avg:60.70ms
step:2130/2315 train_time:129284ms step_avg:60.70ms
step:2131/2315 train_time:129345ms step_avg:60.70ms
step:2132/2315 train_time:129407ms step_avg:60.70ms
step:2133/2315 train_time:129468ms step_avg:60.70ms
step:2134/2315 train_time:129529ms step_avg:60.70ms
step:2135/2315 train_time:129590ms step_avg:60.70ms
step:2136/2315 train_time:129651ms step_avg:60.70ms
step:2137/2315 train_time:129712ms step_avg:60.70ms
step:2138/2315 train_time:129773ms step_avg:60.70ms
step:2139/2315 train_time:129834ms step_avg:60.70ms
step:2140/2315 train_time:129895ms step_avg:60.70ms
step:2141/2315 train_time:129956ms step_avg:60.70ms
step:2142/2315 train_time:130018ms step_avg:60.70ms
step:2143/2315 train_time:130080ms step_avg:60.70ms
step:2144/2315 train_time:130141ms step_avg:60.70ms
step:2145/2315 train_time:130202ms step_avg:60.70ms
step:2146/2315 train_time:130264ms step_avg:60.70ms
step:2147/2315 train_time:130325ms step_avg:60.70ms
step:2148/2315 train_time:130388ms step_avg:60.70ms
step:2149/2315 train_time:130449ms step_avg:60.70ms
step:2150/2315 train_time:130510ms step_avg:60.70ms
step:2151/2315 train_time:130571ms step_avg:60.70ms
step:2152/2315 train_time:130633ms step_avg:60.70ms
step:2153/2315 train_time:130694ms step_avg:60.70ms
step:2154/2315 train_time:130755ms step_avg:60.70ms
step:2155/2315 train_time:130816ms step_avg:60.70ms
step:2156/2315 train_time:130877ms step_avg:60.70ms
step:2157/2315 train_time:130938ms step_avg:60.70ms
step:2158/2315 train_time:130999ms step_avg:60.70ms
step:2159/2315 train_time:131062ms step_avg:60.70ms
step:2160/2315 train_time:131122ms step_avg:60.70ms
step:2161/2315 train_time:131184ms step_avg:60.71ms
step:2162/2315 train_time:131245ms step_avg:60.71ms
step:2163/2315 train_time:131307ms step_avg:60.71ms
step:2164/2315 train_time:131368ms step_avg:60.71ms
step:2165/2315 train_time:131429ms step_avg:60.71ms
step:2166/2315 train_time:131491ms step_avg:60.71ms
step:2167/2315 train_time:131552ms step_avg:60.71ms
step:2168/2315 train_time:131613ms step_avg:60.71ms
step:2169/2315 train_time:131674ms step_avg:60.71ms
step:2170/2315 train_time:131735ms step_avg:60.71ms
step:2171/2315 train_time:131797ms step_avg:60.71ms
step:2172/2315 train_time:131858ms step_avg:60.71ms
step:2173/2315 train_time:131920ms step_avg:60.71ms
step:2174/2315 train_time:131981ms step_avg:60.71ms
step:2175/2315 train_time:132042ms step_avg:60.71ms
step:2176/2315 train_time:132103ms step_avg:60.71ms
step:2177/2315 train_time:132164ms step_avg:60.71ms
step:2178/2315 train_time:132225ms step_avg:60.71ms
step:2179/2315 train_time:132287ms step_avg:60.71ms
step:2180/2315 train_time:132348ms step_avg:60.71ms
step:2181/2315 train_time:132410ms step_avg:60.71ms
step:2182/2315 train_time:132471ms step_avg:60.71ms
step:2183/2315 train_time:132531ms step_avg:60.71ms
step:2184/2315 train_time:132592ms step_avg:60.71ms
step:2185/2315 train_time:132654ms step_avg:60.71ms
step:2186/2315 train_time:132715ms step_avg:60.71ms
step:2187/2315 train_time:132777ms step_avg:60.71ms
step:2188/2315 train_time:132838ms step_avg:60.71ms
step:2189/2315 train_time:132899ms step_avg:60.71ms
step:2190/2315 train_time:132960ms step_avg:60.71ms
step:2191/2315 train_time:133022ms step_avg:60.71ms
step:2192/2315 train_time:133083ms step_avg:60.71ms
step:2193/2315 train_time:133144ms step_avg:60.71ms
step:2194/2315 train_time:133205ms step_avg:60.71ms
step:2195/2315 train_time:133266ms step_avg:60.71ms
step:2196/2315 train_time:133328ms step_avg:60.71ms
step:2197/2315 train_time:133389ms step_avg:60.71ms
step:2198/2315 train_time:133450ms step_avg:60.71ms
step:2199/2315 train_time:133511ms step_avg:60.71ms
step:2200/2315 train_time:133571ms step_avg:60.71ms
step:2201/2315 train_time:133632ms step_avg:60.71ms
step:2202/2315 train_time:133693ms step_avg:60.71ms
step:2203/2315 train_time:133755ms step_avg:60.71ms
step:2204/2315 train_time:133816ms step_avg:60.72ms
step:2205/2315 train_time:133878ms step_avg:60.72ms
step:2206/2315 train_time:133939ms step_avg:60.72ms
step:2207/2315 train_time:134001ms step_avg:60.72ms
step:2208/2315 train_time:134062ms step_avg:60.72ms
step:2209/2315 train_time:134124ms step_avg:60.72ms
step:2210/2315 train_time:134186ms step_avg:60.72ms
step:2211/2315 train_time:134247ms step_avg:60.72ms
step:2212/2315 train_time:134307ms step_avg:60.72ms
step:2213/2315 train_time:134369ms step_avg:60.72ms
step:2214/2315 train_time:134430ms step_avg:60.72ms
step:2215/2315 train_time:134491ms step_avg:60.72ms
step:2216/2315 train_time:134552ms step_avg:60.72ms
step:2217/2315 train_time:134613ms step_avg:60.72ms
step:2218/2315 train_time:134674ms step_avg:60.72ms
step:2219/2315 train_time:134734ms step_avg:60.72ms
step:2220/2315 train_time:134795ms step_avg:60.72ms
step:2221/2315 train_time:134857ms step_avg:60.72ms
step:2222/2315 train_time:134919ms step_avg:60.72ms
step:2223/2315 train_time:134981ms step_avg:60.72ms
step:2224/2315 train_time:135042ms step_avg:60.72ms
step:2225/2315 train_time:135103ms step_avg:60.72ms
step:2226/2315 train_time:135165ms step_avg:60.72ms
step:2227/2315 train_time:135226ms step_avg:60.72ms
step:2228/2315 train_time:135287ms step_avg:60.72ms
step:2229/2315 train_time:135349ms step_avg:60.72ms
step:2230/2315 train_time:135410ms step_avg:60.72ms
step:2231/2315 train_time:135471ms step_avg:60.72ms
step:2232/2315 train_time:135532ms step_avg:60.72ms
step:2233/2315 train_time:135593ms step_avg:60.72ms
step:2234/2315 train_time:135654ms step_avg:60.72ms
step:2235/2315 train_time:135715ms step_avg:60.72ms
step:2236/2315 train_time:135776ms step_avg:60.72ms
step:2237/2315 train_time:135838ms step_avg:60.72ms
step:2238/2315 train_time:135899ms step_avg:60.72ms
step:2239/2315 train_time:135960ms step_avg:60.72ms
step:2240/2315 train_time:136022ms step_avg:60.72ms
step:2241/2315 train_time:136084ms step_avg:60.72ms
step:2242/2315 train_time:136145ms step_avg:60.72ms
step:2243/2315 train_time:136206ms step_avg:60.73ms
step:2244/2315 train_time:136268ms step_avg:60.73ms
step:2245/2315 train_time:136329ms step_avg:60.73ms
step:2246/2315 train_time:136390ms step_avg:60.73ms
step:2247/2315 train_time:136451ms step_avg:60.73ms
step:2248/2315 train_time:136512ms step_avg:60.73ms
step:2249/2315 train_time:136572ms step_avg:60.73ms
step:2250/2315 train_time:136633ms step_avg:60.73ms
step:2250/2315 val_loss:3.2923 train_time:136697ms step_avg:60.75ms
step:2251/2315 train_time:136716ms step_avg:60.74ms
step:2252/2315 train_time:136758ms step_avg:60.73ms
step:2253/2315 train_time:136825ms step_avg:60.73ms
step:2254/2315 train_time:136888ms step_avg:60.73ms
step:2255/2315 train_time:136949ms step_avg:60.73ms
step:2256/2315 train_time:137011ms step_avg:60.73ms
step:2257/2315 train_time:137072ms step_avg:60.73ms
step:2258/2315 train_time:137133ms step_avg:60.73ms
step:2259/2315 train_time:137193ms step_avg:60.73ms
step:2260/2315 train_time:137253ms step_avg:60.73ms
step:2261/2315 train_time:137315ms step_avg:60.73ms
step:2262/2315 train_time:137375ms step_avg:60.73ms
step:2263/2315 train_time:137436ms step_avg:60.73ms
step:2264/2315 train_time:137496ms step_avg:60.73ms
step:2265/2315 train_time:137556ms step_avg:60.73ms
step:2266/2315 train_time:137617ms step_avg:60.73ms
step:2267/2315 train_time:137679ms step_avg:60.73ms
step:2268/2315 train_time:137741ms step_avg:60.73ms
step:2269/2315 train_time:137803ms step_avg:60.73ms
step:2270/2315 train_time:137865ms step_avg:60.73ms
step:2271/2315 train_time:137928ms step_avg:60.73ms
step:2272/2315 train_time:137990ms step_avg:60.73ms
step:2273/2315 train_time:138052ms step_avg:60.74ms
step:2274/2315 train_time:138113ms step_avg:60.74ms
step:2275/2315 train_time:138173ms step_avg:60.74ms
step:2276/2315 train_time:138234ms step_avg:60.74ms
step:2277/2315 train_time:138295ms step_avg:60.74ms
step:2278/2315 train_time:138356ms step_avg:60.74ms
step:2279/2315 train_time:138417ms step_avg:60.74ms
step:2280/2315 train_time:138477ms step_avg:60.74ms
step:2281/2315 train_time:138538ms step_avg:60.74ms
step:2282/2315 train_time:138598ms step_avg:60.74ms
step:2283/2315 train_time:138659ms step_avg:60.74ms
step:2284/2315 train_time:138721ms step_avg:60.74ms
step:2285/2315 train_time:138783ms step_avg:60.74ms
step:2286/2315 train_time:138846ms step_avg:60.74ms
step:2287/2315 train_time:138908ms step_avg:60.74ms
step:2288/2315 train_time:138969ms step_avg:60.74ms
step:2289/2315 train_time:139031ms step_avg:60.74ms
step:2290/2315 train_time:139092ms step_avg:60.74ms
step:2291/2315 train_time:139153ms step_avg:60.74ms
step:2292/2315 train_time:139214ms step_avg:60.74ms
step:2293/2315 train_time:139275ms step_avg:60.74ms
step:2294/2315 train_time:139335ms step_avg:60.74ms
step:2295/2315 train_time:139396ms step_avg:60.74ms
step:2296/2315 train_time:139456ms step_avg:60.74ms
step:2297/2315 train_time:139517ms step_avg:60.74ms
step:2298/2315 train_time:139578ms step_avg:60.74ms
step:2299/2315 train_time:139639ms step_avg:60.74ms
step:2300/2315 train_time:139700ms step_avg:60.74ms
step:2301/2315 train_time:139763ms step_avg:60.74ms
step:2302/2315 train_time:139825ms step_avg:60.74ms
step:2303/2315 train_time:139887ms step_avg:60.74ms
step:2304/2315 train_time:139949ms step_avg:60.74ms
step:2305/2315 train_time:140011ms step_avg:60.74ms
step:2306/2315 train_time:140072ms step_avg:60.74ms
step:2307/2315 train_time:140133ms step_avg:60.74ms
step:2308/2315 train_time:140194ms step_avg:60.74ms
step:2309/2315 train_time:140255ms step_avg:60.74ms
step:2310/2315 train_time:140316ms step_avg:60.74ms
step:2311/2315 train_time:140377ms step_avg:60.74ms
step:2312/2315 train_time:140438ms step_avg:60.74ms
step:2313/2315 train_time:140499ms step_avg:60.74ms
step:2314/2315 train_time:140559ms step_avg:60.74ms
step:2315/2315 train_time:140621ms step_avg:60.74ms
step:2315/2315 val_loss:3.2794 train_time:140682ms step_avg:60.77ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

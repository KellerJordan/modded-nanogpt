import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]
# polar_express_coeffs = [
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
# ]



@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    # state["second_momentum_buffer"] = torch.zeros_like(grad[...,0:1]) if param.size(-2) >= param.size(-1) else torch.zeros_like(grad[...,0:1,:])
                    # state["second_momentum_buffer"] = torch.zeros((*grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((*grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)
            
            ###################################
            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))
            ####################################


            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> (-1.5)  0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Oct 22 2025, 02:20:45) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251022+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 24 05:39:55 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000001:00:00.0 Off |                    0 |
| N/A   25C    P0            112W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000002:00:00.0 Off |                    0 |
| N/A   24C    P0            111W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000003:00:00.0 Off |                    0 |
| N/A   24C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000008:00:00.0 Off |                    0 |
| N/A   25C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000009:00:00.0 Off |                    0 |
| N/A   23C    P0            110W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   0000000A:00:00.0 Off |                    0 |
| N/A   25C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   0000000B:00:00.0 Off |                    0 |
| N/A   24C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   0000000C:00:00.0 Off |                    0 |
| N/A   24C    P0            111W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2315 train_time:94ms step_avg:94.28ms
step:2/2315 train_time:191ms step_avg:95.72ms
step:3/2315 train_time:213ms step_avg:71.11ms
step:4/2315 train_time:249ms step_avg:62.33ms
step:5/2315 train_time:307ms step_avg:61.47ms
step:6/2315 train_time:367ms step_avg:61.18ms
step:7/2315 train_time:426ms step_avg:60.88ms
step:8/2315 train_time:486ms step_avg:60.74ms
step:9/2315 train_time:545ms step_avg:60.60ms
step:10/2315 train_time:605ms step_avg:60.52ms
step:11/2315 train_time:665ms step_avg:60.44ms
step:12/2315 train_time:724ms step_avg:60.36ms
step:13/2315 train_time:784ms step_avg:60.31ms
step:14/2315 train_time:844ms step_avg:60.29ms
step:15/2315 train_time:904ms step_avg:60.26ms
step:16/2315 train_time:964ms step_avg:60.27ms
step:17/2315 train_time:1023ms step_avg:60.20ms
step:18/2315 train_time:1084ms step_avg:60.21ms
step:19/2315 train_time:1146ms step_avg:60.33ms
step:20/2315 train_time:1208ms step_avg:60.40ms
step:21/2315 train_time:1269ms step_avg:60.42ms
step:22/2315 train_time:1330ms step_avg:60.46ms
step:23/2315 train_time:1390ms step_avg:60.43ms
step:24/2315 train_time:1450ms step_avg:60.41ms
step:25/2315 train_time:1510ms step_avg:60.42ms
step:26/2315 train_time:1570ms step_avg:60.40ms
step:27/2315 train_time:1630ms step_avg:60.38ms
step:28/2315 train_time:1690ms step_avg:60.37ms
step:29/2315 train_time:1750ms step_avg:60.34ms
step:30/2315 train_time:1810ms step_avg:60.35ms
step:31/2315 train_time:1870ms step_avg:60.32ms
step:32/2315 train_time:1930ms step_avg:60.31ms
step:33/2315 train_time:1991ms step_avg:60.33ms
step:34/2315 train_time:2051ms step_avg:60.32ms
step:35/2315 train_time:2111ms step_avg:60.32ms
step:36/2315 train_time:2172ms step_avg:60.34ms
step:37/2315 train_time:2232ms step_avg:60.33ms
step:38/2315 train_time:2293ms step_avg:60.34ms
step:39/2315 train_time:2354ms step_avg:60.35ms
step:40/2315 train_time:2413ms step_avg:60.33ms
step:41/2315 train_time:2474ms step_avg:60.34ms
step:42/2315 train_time:2534ms step_avg:60.34ms
step:43/2315 train_time:2595ms step_avg:60.35ms
step:44/2315 train_time:2655ms step_avg:60.34ms
step:45/2315 train_time:2715ms step_avg:60.34ms
step:46/2315 train_time:2775ms step_avg:60.34ms
step:47/2315 train_time:2837ms step_avg:60.35ms
step:48/2315 train_time:2897ms step_avg:60.35ms
step:49/2315 train_time:2957ms step_avg:60.36ms
step:50/2315 train_time:3018ms step_avg:60.36ms
step:51/2315 train_time:3080ms step_avg:60.39ms
step:52/2315 train_time:3140ms step_avg:60.39ms
step:53/2315 train_time:3201ms step_avg:60.40ms
step:54/2315 train_time:3262ms step_avg:60.40ms
step:55/2315 train_time:3322ms step_avg:60.40ms
step:56/2315 train_time:3382ms step_avg:60.40ms
step:57/2315 train_time:3443ms step_avg:60.40ms
step:58/2315 train_time:3502ms step_avg:60.39ms
step:59/2315 train_time:3563ms step_avg:60.39ms
step:60/2315 train_time:3623ms step_avg:60.39ms
step:61/2315 train_time:3683ms step_avg:60.38ms
step:62/2315 train_time:3743ms step_avg:60.37ms
step:63/2315 train_time:3803ms step_avg:60.37ms
step:64/2315 train_time:3864ms step_avg:60.38ms
step:65/2315 train_time:3924ms step_avg:60.37ms
step:66/2315 train_time:3984ms step_avg:60.37ms
step:67/2315 train_time:4044ms step_avg:60.35ms
step:68/2315 train_time:4103ms step_avg:60.34ms
step:69/2315 train_time:4164ms step_avg:60.35ms
step:70/2315 train_time:4224ms step_avg:60.34ms
step:71/2315 train_time:4284ms step_avg:60.34ms
step:72/2315 train_time:4344ms step_avg:60.33ms
step:73/2315 train_time:4405ms step_avg:60.34ms
step:74/2315 train_time:4465ms step_avg:60.34ms
step:75/2315 train_time:4525ms step_avg:60.33ms
step:76/2315 train_time:4585ms step_avg:60.32ms
step:77/2315 train_time:4644ms step_avg:60.31ms
step:78/2315 train_time:4704ms step_avg:60.31ms
step:79/2315 train_time:4764ms step_avg:60.31ms
step:80/2315 train_time:4824ms step_avg:60.30ms
step:81/2315 train_time:4885ms step_avg:60.31ms
step:82/2315 train_time:4945ms step_avg:60.31ms
step:83/2315 train_time:5005ms step_avg:60.30ms
step:84/2315 train_time:5065ms step_avg:60.29ms
step:85/2315 train_time:5125ms step_avg:60.29ms
step:86/2315 train_time:5185ms step_avg:60.29ms
step:87/2315 train_time:5245ms step_avg:60.29ms
step:88/2315 train_time:5304ms step_avg:60.28ms
step:89/2315 train_time:5365ms step_avg:60.28ms
step:90/2315 train_time:5425ms step_avg:60.27ms
step:91/2315 train_time:5485ms step_avg:60.28ms
step:92/2315 train_time:5545ms step_avg:60.27ms
step:93/2315 train_time:5605ms step_avg:60.26ms
step:94/2315 train_time:5664ms step_avg:60.26ms
step:95/2315 train_time:5724ms step_avg:60.25ms
step:96/2315 train_time:5784ms step_avg:60.25ms
step:97/2315 train_time:5844ms step_avg:60.25ms
step:98/2315 train_time:5904ms step_avg:60.24ms
step:99/2315 train_time:5964ms step_avg:60.24ms
step:100/2315 train_time:6024ms step_avg:60.24ms
step:101/2315 train_time:6084ms step_avg:60.24ms
step:102/2315 train_time:6143ms step_avg:60.23ms
step:103/2315 train_time:6203ms step_avg:60.22ms
step:104/2315 train_time:6263ms step_avg:60.22ms
step:105/2315 train_time:6323ms step_avg:60.22ms
step:106/2315 train_time:6383ms step_avg:60.22ms
step:107/2315 train_time:6443ms step_avg:60.21ms
step:108/2315 train_time:6502ms step_avg:60.21ms
step:109/2315 train_time:6563ms step_avg:60.21ms
step:110/2315 train_time:6623ms step_avg:60.21ms
step:111/2315 train_time:6683ms step_avg:60.21ms
step:112/2315 train_time:6743ms step_avg:60.21ms
step:113/2315 train_time:6803ms step_avg:60.20ms
step:114/2315 train_time:6864ms step_avg:60.21ms
step:115/2315 train_time:6923ms step_avg:60.20ms
step:116/2315 train_time:6984ms step_avg:60.20ms
step:117/2315 train_time:7044ms step_avg:60.20ms
step:118/2315 train_time:7104ms step_avg:60.20ms
step:119/2315 train_time:7163ms step_avg:60.20ms
step:120/2315 train_time:7223ms step_avg:60.19ms
step:121/2315 train_time:7283ms step_avg:60.19ms
step:122/2315 train_time:7342ms step_avg:60.18ms
step:123/2315 train_time:7403ms step_avg:60.18ms
step:124/2315 train_time:7462ms step_avg:60.18ms
step:125/2315 train_time:7522ms step_avg:60.18ms
step:126/2315 train_time:7583ms step_avg:60.18ms
step:127/2315 train_time:7643ms step_avg:60.18ms
step:128/2315 train_time:7703ms step_avg:60.18ms
step:129/2315 train_time:7763ms step_avg:60.18ms
step:130/2315 train_time:7822ms step_avg:60.17ms
step:131/2315 train_time:7883ms step_avg:60.17ms
step:132/2315 train_time:7943ms step_avg:60.17ms
step:133/2315 train_time:8003ms step_avg:60.17ms
step:134/2315 train_time:8063ms step_avg:60.17ms
step:135/2315 train_time:8123ms step_avg:60.17ms
step:136/2315 train_time:8183ms step_avg:60.17ms
step:137/2315 train_time:8242ms step_avg:60.16ms
step:138/2315 train_time:8302ms step_avg:60.16ms
step:139/2315 train_time:8362ms step_avg:60.16ms
step:140/2315 train_time:8422ms step_avg:60.16ms
step:141/2315 train_time:8482ms step_avg:60.15ms
step:142/2315 train_time:8541ms step_avg:60.15ms
step:143/2315 train_time:8601ms step_avg:60.15ms
step:144/2315 train_time:8661ms step_avg:60.15ms
step:145/2315 train_time:8721ms step_avg:60.15ms
step:146/2315 train_time:8781ms step_avg:60.14ms
step:147/2315 train_time:8842ms step_avg:60.15ms
step:148/2315 train_time:8902ms step_avg:60.15ms
step:149/2315 train_time:8963ms step_avg:60.15ms
step:150/2315 train_time:9023ms step_avg:60.15ms
step:151/2315 train_time:9083ms step_avg:60.15ms
step:152/2315 train_time:9143ms step_avg:60.15ms
step:153/2315 train_time:9202ms step_avg:60.15ms
step:154/2315 train_time:9262ms step_avg:60.14ms
step:155/2315 train_time:9322ms step_avg:60.14ms
step:156/2315 train_time:9382ms step_avg:60.14ms
step:157/2315 train_time:9442ms step_avg:60.14ms
step:158/2315 train_time:9502ms step_avg:60.14ms
step:159/2315 train_time:9562ms step_avg:60.14ms
step:160/2315 train_time:9622ms step_avg:60.14ms
step:161/2315 train_time:9682ms step_avg:60.13ms
step:162/2315 train_time:9741ms step_avg:60.13ms
step:163/2315 train_time:9802ms step_avg:60.13ms
step:164/2315 train_time:9862ms step_avg:60.13ms
step:165/2315 train_time:9922ms step_avg:60.13ms
step:166/2315 train_time:9982ms step_avg:60.13ms
step:167/2315 train_time:10042ms step_avg:60.13ms
step:168/2315 train_time:10102ms step_avg:60.13ms
step:169/2315 train_time:10161ms step_avg:60.13ms
step:170/2315 train_time:10221ms step_avg:60.12ms
step:171/2315 train_time:10281ms step_avg:60.12ms
step:172/2315 train_time:10341ms step_avg:60.12ms
step:173/2315 train_time:10402ms step_avg:60.13ms
step:174/2315 train_time:10462ms step_avg:60.13ms
step:175/2315 train_time:10522ms step_avg:60.13ms
step:176/2315 train_time:10583ms step_avg:60.13ms
step:177/2315 train_time:10643ms step_avg:60.13ms
step:178/2315 train_time:10703ms step_avg:60.13ms
step:179/2315 train_time:10762ms step_avg:60.12ms
step:180/2315 train_time:10822ms step_avg:60.12ms
step:181/2315 train_time:10882ms step_avg:60.12ms
step:182/2315 train_time:10942ms step_avg:60.12ms
step:183/2315 train_time:11002ms step_avg:60.12ms
step:184/2315 train_time:11063ms step_avg:60.12ms
step:185/2315 train_time:11122ms step_avg:60.12ms
step:186/2315 train_time:11182ms step_avg:60.12ms
step:187/2315 train_time:11242ms step_avg:60.12ms
step:188/2315 train_time:11302ms step_avg:60.12ms
step:189/2315 train_time:11362ms step_avg:60.12ms
step:190/2315 train_time:11422ms step_avg:60.11ms
step:191/2315 train_time:11482ms step_avg:60.12ms
step:192/2315 train_time:11542ms step_avg:60.11ms
step:193/2315 train_time:11603ms step_avg:60.12ms
step:194/2315 train_time:11662ms step_avg:60.11ms
step:195/2315 train_time:11722ms step_avg:60.11ms
step:196/2315 train_time:11782ms step_avg:60.11ms
step:197/2315 train_time:11842ms step_avg:60.11ms
step:198/2315 train_time:11902ms step_avg:60.11ms
step:199/2315 train_time:11962ms step_avg:60.11ms
step:200/2315 train_time:12023ms step_avg:60.11ms
step:201/2315 train_time:12082ms step_avg:60.11ms
step:202/2315 train_time:12142ms step_avg:60.11ms
step:203/2315 train_time:12202ms step_avg:60.11ms
step:204/2315 train_time:12262ms step_avg:60.11ms
step:205/2315 train_time:12322ms step_avg:60.11ms
step:206/2315 train_time:12382ms step_avg:60.11ms
step:207/2315 train_time:12442ms step_avg:60.11ms
step:208/2315 train_time:12501ms step_avg:60.10ms
step:209/2315 train_time:12561ms step_avg:60.10ms
step:210/2315 train_time:12621ms step_avg:60.10ms
step:211/2315 train_time:12681ms step_avg:60.10ms
step:212/2315 train_time:12741ms step_avg:60.10ms
step:213/2315 train_time:12800ms step_avg:60.10ms
step:214/2315 train_time:12860ms step_avg:60.09ms
step:215/2315 train_time:12920ms step_avg:60.09ms
step:216/2315 train_time:12980ms step_avg:60.09ms
step:217/2315 train_time:13040ms step_avg:60.09ms
step:218/2315 train_time:13101ms step_avg:60.09ms
step:219/2315 train_time:13160ms step_avg:60.09ms
step:220/2315 train_time:13220ms step_avg:60.09ms
step:221/2315 train_time:13280ms step_avg:60.09ms
step:222/2315 train_time:13339ms step_avg:60.09ms
step:223/2315 train_time:13400ms step_avg:60.09ms
step:224/2315 train_time:13459ms step_avg:60.09ms
step:225/2315 train_time:13520ms step_avg:60.09ms
step:226/2315 train_time:13580ms step_avg:60.09ms
step:227/2315 train_time:13640ms step_avg:60.09ms
step:228/2315 train_time:13700ms step_avg:60.09ms
step:229/2315 train_time:13760ms step_avg:60.09ms
step:230/2315 train_time:13820ms step_avg:60.09ms
step:231/2315 train_time:13880ms step_avg:60.09ms
step:232/2315 train_time:13940ms step_avg:60.09ms
step:233/2315 train_time:14000ms step_avg:60.08ms
step:234/2315 train_time:14060ms step_avg:60.08ms
step:235/2315 train_time:14120ms step_avg:60.08ms
step:236/2315 train_time:14179ms step_avg:60.08ms
step:237/2315 train_time:14239ms step_avg:60.08ms
step:238/2315 train_time:14299ms step_avg:60.08ms
step:239/2315 train_time:14359ms step_avg:60.08ms
step:240/2315 train_time:14419ms step_avg:60.08ms
step:241/2315 train_time:14479ms step_avg:60.08ms
step:242/2315 train_time:14539ms step_avg:60.08ms
step:243/2315 train_time:14600ms step_avg:60.08ms
step:244/2315 train_time:14660ms step_avg:60.08ms
step:245/2315 train_time:14720ms step_avg:60.08ms
step:246/2315 train_time:14780ms step_avg:60.08ms
step:247/2315 train_time:14840ms step_avg:60.08ms
step:248/2315 train_time:14900ms step_avg:60.08ms
step:249/2315 train_time:14961ms step_avg:60.08ms
step:250/2315 train_time:15021ms step_avg:60.08ms
step:250/2315 val_loss:4.0645 train_time:15083ms step_avg:60.33ms
step:251/2315 train_time:15106ms step_avg:60.18ms
step:252/2315 train_time:15148ms step_avg:60.11ms
step:253/2315 train_time:15209ms step_avg:60.12ms
step:254/2315 train_time:15271ms step_avg:60.12ms
step:255/2315 train_time:15333ms step_avg:60.13ms
step:256/2315 train_time:15393ms step_avg:60.13ms
step:257/2315 train_time:15454ms step_avg:60.13ms
step:258/2315 train_time:15514ms step_avg:60.13ms
step:259/2315 train_time:15574ms step_avg:60.13ms
step:260/2315 train_time:15633ms step_avg:60.13ms
step:261/2315 train_time:15692ms step_avg:60.12ms
step:262/2315 train_time:15751ms step_avg:60.12ms
step:263/2315 train_time:15810ms step_avg:60.11ms
step:264/2315 train_time:15869ms step_avg:60.11ms
step:265/2315 train_time:15929ms step_avg:60.11ms
step:266/2315 train_time:15988ms step_avg:60.10ms
step:267/2315 train_time:16048ms step_avg:60.11ms
step:268/2315 train_time:16109ms step_avg:60.11ms
step:269/2315 train_time:16171ms step_avg:60.11ms
step:270/2315 train_time:16232ms step_avg:60.12ms
step:271/2315 train_time:16293ms step_avg:60.12ms
step:272/2315 train_time:16353ms step_avg:60.12ms
step:273/2315 train_time:16414ms step_avg:60.12ms
step:274/2315 train_time:16474ms step_avg:60.13ms
step:275/2315 train_time:16534ms step_avg:60.12ms
step:276/2315 train_time:16595ms step_avg:60.13ms
step:277/2315 train_time:16654ms step_avg:60.12ms
step:278/2315 train_time:16713ms step_avg:60.12ms
step:279/2315 train_time:16773ms step_avg:60.12ms
step:280/2315 train_time:16833ms step_avg:60.12ms
step:281/2315 train_time:16892ms step_avg:60.11ms
step:282/2315 train_time:16952ms step_avg:60.11ms
step:283/2315 train_time:17012ms step_avg:60.11ms
step:284/2315 train_time:17072ms step_avg:60.11ms
step:285/2315 train_time:17133ms step_avg:60.12ms
step:286/2315 train_time:17194ms step_avg:60.12ms
step:287/2315 train_time:17255ms step_avg:60.12ms
step:288/2315 train_time:17315ms step_avg:60.12ms
step:289/2315 train_time:17376ms step_avg:60.13ms
step:290/2315 train_time:17436ms step_avg:60.12ms
step:291/2315 train_time:17496ms step_avg:60.12ms
step:292/2315 train_time:17556ms step_avg:60.12ms
step:293/2315 train_time:17617ms step_avg:60.12ms
step:294/2315 train_time:17675ms step_avg:60.12ms
step:295/2315 train_time:17735ms step_avg:60.12ms
step:296/2315 train_time:17795ms step_avg:60.12ms
step:297/2315 train_time:17854ms step_avg:60.12ms
step:298/2315 train_time:17914ms step_avg:60.11ms
step:299/2315 train_time:17974ms step_avg:60.11ms
step:300/2315 train_time:18034ms step_avg:60.11ms
step:301/2315 train_time:18094ms step_avg:60.11ms
step:302/2315 train_time:18154ms step_avg:60.11ms
step:303/2315 train_time:18215ms step_avg:60.12ms
step:304/2315 train_time:18276ms step_avg:60.12ms
step:305/2315 train_time:18337ms step_avg:60.12ms
step:306/2315 train_time:18397ms step_avg:60.12ms
step:307/2315 train_time:18457ms step_avg:60.12ms
step:308/2315 train_time:18516ms step_avg:60.12ms
step:309/2315 train_time:18577ms step_avg:60.12ms
step:310/2315 train_time:18636ms step_avg:60.12ms
step:311/2315 train_time:18696ms step_avg:60.12ms
step:312/2315 train_time:18756ms step_avg:60.11ms
step:313/2315 train_time:18815ms step_avg:60.11ms
step:314/2315 train_time:18876ms step_avg:60.11ms
step:315/2315 train_time:18935ms step_avg:60.11ms
step:316/2315 train_time:18995ms step_avg:60.11ms
step:317/2315 train_time:19055ms step_avg:60.11ms
step:318/2315 train_time:19116ms step_avg:60.11ms
step:319/2315 train_time:19176ms step_avg:60.11ms
step:320/2315 train_time:19236ms step_avg:60.11ms
step:321/2315 train_time:19297ms step_avg:60.11ms
step:322/2315 train_time:19357ms step_avg:60.12ms
step:323/2315 train_time:19417ms step_avg:60.11ms
step:324/2315 train_time:19477ms step_avg:60.11ms
step:325/2315 train_time:19536ms step_avg:60.11ms
step:326/2315 train_time:19596ms step_avg:60.11ms
step:327/2315 train_time:19656ms step_avg:60.11ms
step:328/2315 train_time:19716ms step_avg:60.11ms
step:329/2315 train_time:19776ms step_avg:60.11ms
step:330/2315 train_time:19835ms step_avg:60.11ms
step:331/2315 train_time:19896ms step_avg:60.11ms
step:332/2315 train_time:19956ms step_avg:60.11ms
step:333/2315 train_time:20016ms step_avg:60.11ms
step:334/2315 train_time:20075ms step_avg:60.11ms
step:335/2315 train_time:20136ms step_avg:60.11ms
step:336/2315 train_time:20195ms step_avg:60.11ms
step:337/2315 train_time:20256ms step_avg:60.11ms
step:338/2315 train_time:20317ms step_avg:60.11ms
step:339/2315 train_time:20377ms step_avg:60.11ms
step:340/2315 train_time:20437ms step_avg:60.11ms
step:341/2315 train_time:20498ms step_avg:60.11ms
step:342/2315 train_time:20557ms step_avg:60.11ms
step:343/2315 train_time:20617ms step_avg:60.11ms
step:344/2315 train_time:20676ms step_avg:60.11ms
step:345/2315 train_time:20736ms step_avg:60.11ms
step:346/2315 train_time:20796ms step_avg:60.10ms
step:347/2315 train_time:20856ms step_avg:60.10ms
step:348/2315 train_time:20915ms step_avg:60.10ms
step:349/2315 train_time:20975ms step_avg:60.10ms
step:350/2315 train_time:21035ms step_avg:60.10ms
step:351/2315 train_time:21096ms step_avg:60.10ms
step:352/2315 train_time:21156ms step_avg:60.10ms
step:353/2315 train_time:21216ms step_avg:60.10ms
step:354/2315 train_time:21276ms step_avg:60.10ms
step:355/2315 train_time:21337ms step_avg:60.10ms
step:356/2315 train_time:21397ms step_avg:60.10ms
step:357/2315 train_time:21458ms step_avg:60.11ms
step:358/2315 train_time:21517ms step_avg:60.10ms
step:359/2315 train_time:21577ms step_avg:60.10ms
step:360/2315 train_time:21637ms step_avg:60.10ms
step:361/2315 train_time:21697ms step_avg:60.10ms
step:362/2315 train_time:21757ms step_avg:60.10ms
step:363/2315 train_time:21816ms step_avg:60.10ms
step:364/2315 train_time:21876ms step_avg:60.10ms
step:365/2315 train_time:21935ms step_avg:60.10ms
step:366/2315 train_time:21996ms step_avg:60.10ms
step:367/2315 train_time:22056ms step_avg:60.10ms
step:368/2315 train_time:22116ms step_avg:60.10ms
step:369/2315 train_time:22176ms step_avg:60.10ms
step:370/2315 train_time:22236ms step_avg:60.10ms
step:371/2315 train_time:22296ms step_avg:60.10ms
step:372/2315 train_time:22356ms step_avg:60.10ms
step:373/2315 train_time:22417ms step_avg:60.10ms
step:374/2315 train_time:22477ms step_avg:60.10ms
step:375/2315 train_time:22537ms step_avg:60.10ms
step:376/2315 train_time:22597ms step_avg:60.10ms
step:377/2315 train_time:22657ms step_avg:60.10ms
step:378/2315 train_time:22717ms step_avg:60.10ms
step:379/2315 train_time:22777ms step_avg:60.10ms
step:380/2315 train_time:22836ms step_avg:60.10ms
step:381/2315 train_time:22897ms step_avg:60.10ms
step:382/2315 train_time:22957ms step_avg:60.10ms
step:383/2315 train_time:23017ms step_avg:60.10ms
step:384/2315 train_time:23076ms step_avg:60.09ms
step:385/2315 train_time:23136ms step_avg:60.09ms
step:386/2315 train_time:23196ms step_avg:60.09ms
step:387/2315 train_time:23256ms step_avg:60.09ms
step:388/2315 train_time:23316ms step_avg:60.09ms
step:389/2315 train_time:23377ms step_avg:60.10ms
step:390/2315 train_time:23437ms step_avg:60.09ms
step:391/2315 train_time:23497ms step_avg:60.09ms
step:392/2315 train_time:23557ms step_avg:60.09ms
step:393/2315 train_time:23617ms step_avg:60.09ms
step:394/2315 train_time:23677ms step_avg:60.10ms
step:395/2315 train_time:23737ms step_avg:60.09ms
step:396/2315 train_time:23797ms step_avg:60.09ms
step:397/2315 train_time:23856ms step_avg:60.09ms
step:398/2315 train_time:23916ms step_avg:60.09ms
step:399/2315 train_time:23976ms step_avg:60.09ms
step:400/2315 train_time:24035ms step_avg:60.09ms
step:401/2315 train_time:24096ms step_avg:60.09ms
step:402/2315 train_time:24156ms step_avg:60.09ms
step:403/2315 train_time:24216ms step_avg:60.09ms
step:404/2315 train_time:24276ms step_avg:60.09ms
step:405/2315 train_time:24336ms step_avg:60.09ms
step:406/2315 train_time:24396ms step_avg:60.09ms
step:407/2315 train_time:24457ms step_avg:60.09ms
step:408/2315 train_time:24516ms step_avg:60.09ms
step:409/2315 train_time:24577ms step_avg:60.09ms
step:410/2315 train_time:24636ms step_avg:60.09ms
step:411/2315 train_time:24697ms step_avg:60.09ms
step:412/2315 train_time:24756ms step_avg:60.09ms
step:413/2315 train_time:24816ms step_avg:60.09ms
step:414/2315 train_time:24877ms step_avg:60.09ms
step:415/2315 train_time:24936ms step_avg:60.09ms
step:416/2315 train_time:24995ms step_avg:60.09ms
step:417/2315 train_time:25056ms step_avg:60.09ms
step:418/2315 train_time:25116ms step_avg:60.09ms
step:419/2315 train_time:25175ms step_avg:60.08ms
step:420/2315 train_time:25235ms step_avg:60.08ms
step:421/2315 train_time:25296ms step_avg:60.09ms
step:422/2315 train_time:25356ms step_avg:60.09ms
step:423/2315 train_time:25416ms step_avg:60.09ms
step:424/2315 train_time:25477ms step_avg:60.09ms
step:425/2315 train_time:25537ms step_avg:60.09ms
step:426/2315 train_time:25597ms step_avg:60.09ms
step:427/2315 train_time:25658ms step_avg:60.09ms
step:428/2315 train_time:25718ms step_avg:60.09ms
step:429/2315 train_time:25778ms step_avg:60.09ms
step:430/2315 train_time:25838ms step_avg:60.09ms
step:431/2315 train_time:25899ms step_avg:60.09ms
step:432/2315 train_time:25959ms step_avg:60.09ms
step:433/2315 train_time:26019ms step_avg:60.09ms
step:434/2315 train_time:26079ms step_avg:60.09ms
step:435/2315 train_time:26139ms step_avg:60.09ms
step:436/2315 train_time:26198ms step_avg:60.09ms
step:437/2315 train_time:26258ms step_avg:60.09ms
step:438/2315 train_time:26319ms step_avg:60.09ms
step:439/2315 train_time:26378ms step_avg:60.09ms
step:440/2315 train_time:26438ms step_avg:60.09ms
step:441/2315 train_time:26498ms step_avg:60.09ms
step:442/2315 train_time:26557ms step_avg:60.08ms
step:443/2315 train_time:26617ms step_avg:60.08ms
step:444/2315 train_time:26677ms step_avg:60.08ms
step:445/2315 train_time:26737ms step_avg:60.08ms
step:446/2315 train_time:26797ms step_avg:60.08ms
step:447/2315 train_time:26858ms step_avg:60.08ms
step:448/2315 train_time:26918ms step_avg:60.08ms
step:449/2315 train_time:26978ms step_avg:60.08ms
step:450/2315 train_time:27038ms step_avg:60.08ms
step:451/2315 train_time:27098ms step_avg:60.08ms
step:452/2315 train_time:27158ms step_avg:60.08ms
step:453/2315 train_time:27217ms step_avg:60.08ms
step:454/2315 train_time:27277ms step_avg:60.08ms
step:455/2315 train_time:27336ms step_avg:60.08ms
step:456/2315 train_time:27396ms step_avg:60.08ms
step:457/2315 train_time:27457ms step_avg:60.08ms
step:458/2315 train_time:27517ms step_avg:60.08ms
step:459/2315 train_time:27577ms step_avg:60.08ms
step:460/2315 train_time:27636ms step_avg:60.08ms
step:461/2315 train_time:27697ms step_avg:60.08ms
step:462/2315 train_time:27757ms step_avg:60.08ms
step:463/2315 train_time:27817ms step_avg:60.08ms
step:464/2315 train_time:27877ms step_avg:60.08ms
step:465/2315 train_time:27937ms step_avg:60.08ms
step:466/2315 train_time:27997ms step_avg:60.08ms
step:467/2315 train_time:28057ms step_avg:60.08ms
step:468/2315 train_time:28117ms step_avg:60.08ms
step:469/2315 train_time:28177ms step_avg:60.08ms
step:470/2315 train_time:28237ms step_avg:60.08ms
step:471/2315 train_time:28296ms step_avg:60.08ms
step:472/2315 train_time:28356ms step_avg:60.08ms
step:473/2315 train_time:28415ms step_avg:60.07ms
step:474/2315 train_time:28475ms step_avg:60.07ms
step:475/2315 train_time:28535ms step_avg:60.07ms
step:476/2315 train_time:28596ms step_avg:60.08ms
step:477/2315 train_time:28656ms step_avg:60.08ms
step:478/2315 train_time:28716ms step_avg:60.07ms
step:479/2315 train_time:28776ms step_avg:60.07ms
step:480/2315 train_time:28836ms step_avg:60.07ms
step:481/2315 train_time:28897ms step_avg:60.08ms
step:482/2315 train_time:28957ms step_avg:60.08ms
step:483/2315 train_time:29018ms step_avg:60.08ms
step:484/2315 train_time:29078ms step_avg:60.08ms
step:485/2315 train_time:29138ms step_avg:60.08ms
step:486/2315 train_time:29197ms step_avg:60.08ms
step:487/2315 train_time:29257ms step_avg:60.08ms
step:488/2315 train_time:29317ms step_avg:60.08ms
step:489/2315 train_time:29377ms step_avg:60.07ms
step:490/2315 train_time:29436ms step_avg:60.07ms
step:491/2315 train_time:29496ms step_avg:60.07ms
step:492/2315 train_time:29556ms step_avg:60.07ms
step:493/2315 train_time:29616ms step_avg:60.07ms
step:494/2315 train_time:29676ms step_avg:60.07ms
step:495/2315 train_time:29736ms step_avg:60.07ms
step:496/2315 train_time:29795ms step_avg:60.07ms
step:497/2315 train_time:29856ms step_avg:60.07ms
step:498/2315 train_time:29917ms step_avg:60.07ms
step:499/2315 train_time:29977ms step_avg:60.07ms
step:500/2315 train_time:30036ms step_avg:60.07ms
step:500/2315 val_loss:3.8098 train_time:30099ms step_avg:60.20ms
step:501/2315 train_time:30120ms step_avg:60.12ms
step:502/2315 train_time:30164ms step_avg:60.09ms
step:503/2315 train_time:30225ms step_avg:60.09ms
step:504/2315 train_time:30288ms step_avg:60.09ms
step:505/2315 train_time:30348ms step_avg:60.09ms
step:506/2315 train_time:30409ms step_avg:60.10ms
step:507/2315 train_time:30468ms step_avg:60.09ms
step:508/2315 train_time:30527ms step_avg:60.09ms
step:509/2315 train_time:30586ms step_avg:60.09ms
step:510/2315 train_time:30646ms step_avg:60.09ms
step:511/2315 train_time:31516ms step_avg:61.68ms
step:512/2315 train_time:31574ms step_avg:61.67ms
step:513/2315 train_time:31632ms step_avg:61.66ms
step:514/2315 train_time:31691ms step_avg:61.66ms
step:515/2315 train_time:31750ms step_avg:61.65ms
step:516/2315 train_time:31809ms step_avg:61.64ms
step:517/2315 train_time:31868ms step_avg:61.64ms
step:518/2315 train_time:31927ms step_avg:61.63ms
step:519/2315 train_time:31985ms step_avg:61.63ms
step:520/2315 train_time:32045ms step_avg:61.62ms
step:521/2315 train_time:32103ms step_avg:61.62ms
step:522/2315 train_time:32163ms step_avg:61.61ms
step:523/2315 train_time:32222ms step_avg:61.61ms
step:524/2315 train_time:32281ms step_avg:61.60ms
step:525/2315 train_time:32340ms step_avg:61.60ms
step:526/2315 train_time:32401ms step_avg:61.60ms
step:527/2315 train_time:32463ms step_avg:61.60ms
step:528/2315 train_time:32524ms step_avg:61.60ms
step:529/2315 train_time:32585ms step_avg:61.60ms
step:530/2315 train_time:32644ms step_avg:61.59ms
step:531/2315 train_time:32704ms step_avg:61.59ms
step:532/2315 train_time:32764ms step_avg:61.59ms
step:533/2315 train_time:32823ms step_avg:61.58ms
step:534/2315 train_time:32883ms step_avg:61.58ms
step:535/2315 train_time:32942ms step_avg:61.57ms
step:536/2315 train_time:33001ms step_avg:61.57ms
step:537/2315 train_time:33061ms step_avg:61.57ms
step:538/2315 train_time:33120ms step_avg:61.56ms
step:539/2315 train_time:33179ms step_avg:61.56ms
step:540/2315 train_time:33239ms step_avg:61.55ms
step:541/2315 train_time:33298ms step_avg:61.55ms
step:542/2315 train_time:33358ms step_avg:61.55ms
step:543/2315 train_time:33420ms step_avg:61.55ms
step:544/2315 train_time:33481ms step_avg:61.55ms
step:545/2315 train_time:33541ms step_avg:61.54ms
step:546/2315 train_time:33602ms step_avg:61.54ms
step:547/2315 train_time:33663ms step_avg:61.54ms
step:548/2315 train_time:33723ms step_avg:61.54ms
step:549/2315 train_time:33783ms step_avg:61.54ms
step:550/2315 train_time:33843ms step_avg:61.53ms
step:551/2315 train_time:33902ms step_avg:61.53ms
step:552/2315 train_time:33962ms step_avg:61.53ms
step:553/2315 train_time:34021ms step_avg:61.52ms
step:554/2315 train_time:34080ms step_avg:61.52ms
step:555/2315 train_time:34140ms step_avg:61.51ms
step:556/2315 train_time:34200ms step_avg:61.51ms
step:557/2315 train_time:34259ms step_avg:61.51ms
step:558/2315 train_time:34319ms step_avg:61.50ms
step:559/2315 train_time:34379ms step_avg:61.50ms
step:560/2315 train_time:34440ms step_avg:61.50ms
step:561/2315 train_time:34500ms step_avg:61.50ms
step:562/2315 train_time:34561ms step_avg:61.50ms
step:563/2315 train_time:34621ms step_avg:61.49ms
step:564/2315 train_time:34681ms step_avg:61.49ms
step:565/2315 train_time:34741ms step_avg:61.49ms
step:566/2315 train_time:34800ms step_avg:61.48ms
step:567/2315 train_time:34861ms step_avg:61.48ms
step:568/2315 train_time:34920ms step_avg:61.48ms
step:569/2315 train_time:34980ms step_avg:61.48ms
step:570/2315 train_time:35040ms step_avg:61.47ms
step:571/2315 train_time:35100ms step_avg:61.47ms
step:572/2315 train_time:35159ms step_avg:61.47ms
step:573/2315 train_time:35219ms step_avg:61.46ms
step:574/2315 train_time:35279ms step_avg:61.46ms
step:575/2315 train_time:35339ms step_avg:61.46ms
step:576/2315 train_time:35399ms step_avg:61.46ms
step:577/2315 train_time:35459ms step_avg:61.45ms
step:578/2315 train_time:35519ms step_avg:61.45ms
step:579/2315 train_time:35580ms step_avg:61.45ms
step:580/2315 train_time:35640ms step_avg:61.45ms
step:581/2315 train_time:35701ms step_avg:61.45ms
step:582/2315 train_time:35762ms step_avg:61.45ms
step:583/2315 train_time:35822ms step_avg:61.44ms
step:584/2315 train_time:35881ms step_avg:61.44ms
step:585/2315 train_time:35941ms step_avg:61.44ms
step:586/2315 train_time:36001ms step_avg:61.43ms
step:587/2315 train_time:36060ms step_avg:61.43ms
step:588/2315 train_time:36120ms step_avg:61.43ms
step:589/2315 train_time:36180ms step_avg:61.43ms
step:590/2315 train_time:36239ms step_avg:61.42ms
step:591/2315 train_time:36300ms step_avg:61.42ms
step:592/2315 train_time:36360ms step_avg:61.42ms
step:593/2315 train_time:36419ms step_avg:61.42ms
step:594/2315 train_time:36480ms step_avg:61.41ms
step:595/2315 train_time:36540ms step_avg:61.41ms
step:596/2315 train_time:36601ms step_avg:61.41ms
step:597/2315 train_time:36661ms step_avg:61.41ms
step:598/2315 train_time:36721ms step_avg:61.41ms
step:599/2315 train_time:36782ms step_avg:61.40ms
step:600/2315 train_time:36842ms step_avg:61.40ms
step:601/2315 train_time:36902ms step_avg:61.40ms
step:602/2315 train_time:36962ms step_avg:61.40ms
step:603/2315 train_time:37021ms step_avg:61.40ms
step:604/2315 train_time:37081ms step_avg:61.39ms
step:605/2315 train_time:37141ms step_avg:61.39ms
step:606/2315 train_time:37200ms step_avg:61.39ms
step:607/2315 train_time:37260ms step_avg:61.38ms
step:608/2315 train_time:37320ms step_avg:61.38ms
step:609/2315 train_time:37380ms step_avg:61.38ms
step:610/2315 train_time:37440ms step_avg:61.38ms
step:611/2315 train_time:37500ms step_avg:61.37ms
step:612/2315 train_time:37560ms step_avg:61.37ms
step:613/2315 train_time:37620ms step_avg:61.37ms
step:614/2315 train_time:37680ms step_avg:61.37ms
step:615/2315 train_time:37740ms step_avg:61.37ms
step:616/2315 train_time:37800ms step_avg:61.36ms
step:617/2315 train_time:37861ms step_avg:61.36ms
step:618/2315 train_time:37921ms step_avg:61.36ms
step:619/2315 train_time:37980ms step_avg:61.36ms
step:620/2315 train_time:38041ms step_avg:61.36ms
step:621/2315 train_time:38101ms step_avg:61.35ms
step:622/2315 train_time:38160ms step_avg:61.35ms
step:623/2315 train_time:38220ms step_avg:61.35ms
step:624/2315 train_time:38280ms step_avg:61.35ms
step:625/2315 train_time:38340ms step_avg:61.34ms
step:626/2315 train_time:38401ms step_avg:61.34ms
step:627/2315 train_time:38462ms step_avg:61.34ms
step:628/2315 train_time:38522ms step_avg:61.34ms
step:629/2315 train_time:38582ms step_avg:61.34ms
step:630/2315 train_time:38642ms step_avg:61.34ms
step:631/2315 train_time:38703ms step_avg:61.34ms
step:632/2315 train_time:38762ms step_avg:61.33ms
step:633/2315 train_time:38822ms step_avg:61.33ms
step:634/2315 train_time:38883ms step_avg:61.33ms
step:635/2315 train_time:38942ms step_avg:61.33ms
step:636/2315 train_time:39002ms step_avg:61.32ms
step:637/2315 train_time:39062ms step_avg:61.32ms
step:638/2315 train_time:39122ms step_avg:61.32ms
step:639/2315 train_time:39182ms step_avg:61.32ms
step:640/2315 train_time:39241ms step_avg:61.31ms
step:641/2315 train_time:39301ms step_avg:61.31ms
step:642/2315 train_time:39361ms step_avg:61.31ms
step:643/2315 train_time:39420ms step_avg:61.31ms
step:644/2315 train_time:39481ms step_avg:61.31ms
step:645/2315 train_time:39540ms step_avg:61.30ms
step:646/2315 train_time:39601ms step_avg:61.30ms
step:647/2315 train_time:39661ms step_avg:61.30ms
step:648/2315 train_time:39722ms step_avg:61.30ms
step:649/2315 train_time:39782ms step_avg:61.30ms
step:650/2315 train_time:39842ms step_avg:61.30ms
step:651/2315 train_time:39902ms step_avg:61.29ms
step:652/2315 train_time:39961ms step_avg:61.29ms
step:653/2315 train_time:40021ms step_avg:61.29ms
step:654/2315 train_time:40081ms step_avg:61.29ms
step:655/2315 train_time:40142ms step_avg:61.29ms
step:656/2315 train_time:40202ms step_avg:61.28ms
step:657/2315 train_time:40262ms step_avg:61.28ms
step:658/2315 train_time:40322ms step_avg:61.28ms
step:659/2315 train_time:40381ms step_avg:61.28ms
step:660/2315 train_time:40441ms step_avg:61.27ms
step:661/2315 train_time:40501ms step_avg:61.27ms
step:662/2315 train_time:40560ms step_avg:61.27ms
step:663/2315 train_time:40621ms step_avg:61.27ms
step:664/2315 train_time:40680ms step_avg:61.27ms
step:665/2315 train_time:40742ms step_avg:61.27ms
step:666/2315 train_time:40802ms step_avg:61.26ms
step:667/2315 train_time:40862ms step_avg:61.26ms
step:668/2315 train_time:40922ms step_avg:61.26ms
step:669/2315 train_time:40982ms step_avg:61.26ms
step:670/2315 train_time:41042ms step_avg:61.26ms
step:671/2315 train_time:41102ms step_avg:61.25ms
step:672/2315 train_time:41161ms step_avg:61.25ms
step:673/2315 train_time:41221ms step_avg:61.25ms
step:674/2315 train_time:41281ms step_avg:61.25ms
step:675/2315 train_time:41341ms step_avg:61.25ms
step:676/2315 train_time:41400ms step_avg:61.24ms
step:677/2315 train_time:41460ms step_avg:61.24ms
step:678/2315 train_time:41520ms step_avg:61.24ms
step:679/2315 train_time:41580ms step_avg:61.24ms
step:680/2315 train_time:41640ms step_avg:61.24ms
step:681/2315 train_time:41700ms step_avg:61.23ms
step:682/2315 train_time:41760ms step_avg:61.23ms
step:683/2315 train_time:41820ms step_avg:61.23ms
step:684/2315 train_time:41881ms step_avg:61.23ms
step:685/2315 train_time:41940ms step_avg:61.23ms
step:686/2315 train_time:42000ms step_avg:61.22ms
step:687/2315 train_time:42061ms step_avg:61.22ms
step:688/2315 train_time:42121ms step_avg:61.22ms
step:689/2315 train_time:42181ms step_avg:61.22ms
step:690/2315 train_time:42241ms step_avg:61.22ms
step:691/2315 train_time:42301ms step_avg:61.22ms
step:692/2315 train_time:42361ms step_avg:61.21ms
step:693/2315 train_time:42420ms step_avg:61.21ms
step:694/2315 train_time:42480ms step_avg:61.21ms
step:695/2315 train_time:42540ms step_avg:61.21ms
step:696/2315 train_time:42600ms step_avg:61.21ms
step:697/2315 train_time:42660ms step_avg:61.21ms
step:698/2315 train_time:42720ms step_avg:61.20ms
step:699/2315 train_time:42780ms step_avg:61.20ms
step:700/2315 train_time:42840ms step_avg:61.20ms
step:701/2315 train_time:42900ms step_avg:61.20ms
step:702/2315 train_time:42961ms step_avg:61.20ms
step:703/2315 train_time:43021ms step_avg:61.20ms
step:704/2315 train_time:43081ms step_avg:61.19ms
step:705/2315 train_time:43141ms step_avg:61.19ms
step:706/2315 train_time:43201ms step_avg:61.19ms
step:707/2315 train_time:43261ms step_avg:61.19ms
step:708/2315 train_time:43321ms step_avg:61.19ms
step:709/2315 train_time:43381ms step_avg:61.19ms
step:710/2315 train_time:43440ms step_avg:61.18ms
step:711/2315 train_time:43501ms step_avg:61.18ms
step:712/2315 train_time:43560ms step_avg:61.18ms
step:713/2315 train_time:43621ms step_avg:61.18ms
step:714/2315 train_time:43681ms step_avg:61.18ms
step:715/2315 train_time:43742ms step_avg:61.18ms
step:716/2315 train_time:43802ms step_avg:61.18ms
step:717/2315 train_time:43861ms step_avg:61.17ms
step:718/2315 train_time:43922ms step_avg:61.17ms
step:719/2315 train_time:43982ms step_avg:61.17ms
step:720/2315 train_time:44042ms step_avg:61.17ms
step:721/2315 train_time:44102ms step_avg:61.17ms
step:722/2315 train_time:44161ms step_avg:61.16ms
step:723/2315 train_time:44221ms step_avg:61.16ms
step:724/2315 train_time:44281ms step_avg:61.16ms
step:725/2315 train_time:44341ms step_avg:61.16ms
step:726/2315 train_time:44400ms step_avg:61.16ms
step:727/2315 train_time:44460ms step_avg:61.16ms
step:728/2315 train_time:44520ms step_avg:61.15ms
step:729/2315 train_time:44580ms step_avg:61.15ms
step:730/2315 train_time:44640ms step_avg:61.15ms
step:731/2315 train_time:44700ms step_avg:61.15ms
step:732/2315 train_time:44761ms step_avg:61.15ms
step:733/2315 train_time:44821ms step_avg:61.15ms
step:734/2315 train_time:44880ms step_avg:61.14ms
step:735/2315 train_time:44941ms step_avg:61.14ms
step:736/2315 train_time:45001ms step_avg:61.14ms
step:737/2315 train_time:45061ms step_avg:61.14ms
step:738/2315 train_time:45120ms step_avg:61.14ms
step:739/2315 train_time:45180ms step_avg:61.14ms
step:740/2315 train_time:45240ms step_avg:61.13ms
step:741/2315 train_time:45300ms step_avg:61.13ms
step:742/2315 train_time:45360ms step_avg:61.13ms
step:743/2315 train_time:45421ms step_avg:61.13ms
step:744/2315 train_time:45480ms step_avg:61.13ms
step:745/2315 train_time:45540ms step_avg:61.13ms
step:746/2315 train_time:45600ms step_avg:61.13ms
step:747/2315 train_time:45660ms step_avg:61.12ms
step:748/2315 train_time:45720ms step_avg:61.12ms
step:749/2315 train_time:45780ms step_avg:61.12ms
step:750/2315 train_time:45840ms step_avg:61.12ms
step:750/2315 val_loss:3.6820 train_time:45903ms step_avg:61.20ms
step:751/2315 train_time:45923ms step_avg:61.15ms
step:752/2315 train_time:45963ms step_avg:61.12ms
step:753/2315 train_time:46025ms step_avg:61.12ms
step:754/2315 train_time:46086ms step_avg:61.12ms
step:755/2315 train_time:46146ms step_avg:61.12ms
step:756/2315 train_time:46206ms step_avg:61.12ms
step:757/2315 train_time:46267ms step_avg:61.12ms
step:758/2315 train_time:46326ms step_avg:61.12ms
step:759/2315 train_time:46385ms step_avg:61.11ms
step:760/2315 train_time:46445ms step_avg:61.11ms
step:761/2315 train_time:46505ms step_avg:61.11ms
step:762/2315 train_time:46565ms step_avg:61.11ms
step:763/2315 train_time:46625ms step_avg:61.11ms
step:764/2315 train_time:46685ms step_avg:61.11ms
step:765/2315 train_time:46745ms step_avg:61.10ms
step:766/2315 train_time:46806ms step_avg:61.10ms
step:767/2315 train_time:46867ms step_avg:61.10ms
step:768/2315 train_time:46929ms step_avg:61.11ms
step:769/2315 train_time:46990ms step_avg:61.11ms
step:770/2315 train_time:47051ms step_avg:61.11ms
step:771/2315 train_time:47113ms step_avg:61.11ms
step:772/2315 train_time:47174ms step_avg:61.11ms
step:773/2315 train_time:47235ms step_avg:61.11ms
step:774/2315 train_time:47296ms step_avg:61.11ms
step:775/2315 train_time:47357ms step_avg:61.11ms
step:776/2315 train_time:47417ms step_avg:61.10ms
step:777/2315 train_time:47478ms step_avg:61.10ms
step:778/2315 train_time:47539ms step_avg:61.10ms
step:779/2315 train_time:47600ms step_avg:61.10ms
step:780/2315 train_time:47660ms step_avg:61.10ms
step:781/2315 train_time:47721ms step_avg:61.10ms
step:782/2315 train_time:47781ms step_avg:61.10ms
step:783/2315 train_time:47842ms step_avg:61.10ms
step:784/2315 train_time:47902ms step_avg:61.10ms
step:785/2315 train_time:47963ms step_avg:61.10ms
step:786/2315 train_time:48023ms step_avg:61.10ms
step:787/2315 train_time:48084ms step_avg:61.10ms
step:788/2315 train_time:48146ms step_avg:61.10ms
step:789/2315 train_time:48207ms step_avg:61.10ms
step:790/2315 train_time:48267ms step_avg:61.10ms
step:791/2315 train_time:48328ms step_avg:61.10ms
step:792/2315 train_time:48389ms step_avg:61.10ms
step:793/2315 train_time:48449ms step_avg:61.10ms
step:794/2315 train_time:48510ms step_avg:61.10ms
step:795/2315 train_time:48570ms step_avg:61.09ms
step:796/2315 train_time:48631ms step_avg:61.09ms
step:797/2315 train_time:48692ms step_avg:61.09ms
step:798/2315 train_time:48753ms step_avg:61.09ms
step:799/2315 train_time:48814ms step_avg:61.09ms
step:800/2315 train_time:48875ms step_avg:61.09ms
step:801/2315 train_time:48936ms step_avg:61.09ms
step:802/2315 train_time:48997ms step_avg:61.09ms
step:803/2315 train_time:49058ms step_avg:61.09ms
step:804/2315 train_time:49119ms step_avg:61.09ms
step:805/2315 train_time:49180ms step_avg:61.09ms
step:806/2315 train_time:49241ms step_avg:61.09ms
step:807/2315 train_time:49301ms step_avg:61.09ms
step:808/2315 train_time:49362ms step_avg:61.09ms
step:809/2315 train_time:49423ms step_avg:61.09ms
step:810/2315 train_time:49483ms step_avg:61.09ms
step:811/2315 train_time:49544ms step_avg:61.09ms
step:812/2315 train_time:49604ms step_avg:61.09ms
step:813/2315 train_time:49664ms step_avg:61.09ms
step:814/2315 train_time:49725ms step_avg:61.09ms
step:815/2315 train_time:49786ms step_avg:61.09ms
step:816/2315 train_time:49846ms step_avg:61.09ms
step:817/2315 train_time:49906ms step_avg:61.08ms
step:818/2315 train_time:49967ms step_avg:61.08ms
step:819/2315 train_time:50027ms step_avg:61.08ms
step:820/2315 train_time:50088ms step_avg:61.08ms
step:821/2315 train_time:50149ms step_avg:61.08ms
step:822/2315 train_time:50209ms step_avg:61.08ms
step:823/2315 train_time:50270ms step_avg:61.08ms
step:824/2315 train_time:50330ms step_avg:61.08ms
step:825/2315 train_time:50392ms step_avg:61.08ms
step:826/2315 train_time:50453ms step_avg:61.08ms
step:827/2315 train_time:50514ms step_avg:61.08ms
step:828/2315 train_time:50575ms step_avg:61.08ms
step:829/2315 train_time:50635ms step_avg:61.08ms
step:830/2315 train_time:50696ms step_avg:61.08ms
step:831/2315 train_time:50758ms step_avg:61.08ms
step:832/2315 train_time:50818ms step_avg:61.08ms
step:833/2315 train_time:50879ms step_avg:61.08ms
step:834/2315 train_time:50939ms step_avg:61.08ms
step:835/2315 train_time:51000ms step_avg:61.08ms
step:836/2315 train_time:51061ms step_avg:61.08ms
step:837/2315 train_time:51122ms step_avg:61.08ms
step:838/2315 train_time:51182ms step_avg:61.08ms
step:839/2315 train_time:51243ms step_avg:61.08ms
step:840/2315 train_time:51303ms step_avg:61.07ms
step:841/2315 train_time:51364ms step_avg:61.07ms
step:842/2315 train_time:51424ms step_avg:61.07ms
step:843/2315 train_time:51486ms step_avg:61.07ms
step:844/2315 train_time:51546ms step_avg:61.07ms
step:845/2315 train_time:51607ms step_avg:61.07ms
step:846/2315 train_time:51668ms step_avg:61.07ms
step:847/2315 train_time:51729ms step_avg:61.07ms
step:848/2315 train_time:51790ms step_avg:61.07ms
step:849/2315 train_time:51851ms step_avg:61.07ms
step:850/2315 train_time:51912ms step_avg:61.07ms
step:851/2315 train_time:51973ms step_avg:61.07ms
step:852/2315 train_time:52035ms step_avg:61.07ms
step:853/2315 train_time:52096ms step_avg:61.07ms
step:854/2315 train_time:52156ms step_avg:61.07ms
step:855/2315 train_time:52217ms step_avg:61.07ms
step:856/2315 train_time:52278ms step_avg:61.07ms
step:857/2315 train_time:52339ms step_avg:61.07ms
step:858/2315 train_time:52399ms step_avg:61.07ms
step:859/2315 train_time:52460ms step_avg:61.07ms
step:860/2315 train_time:52521ms step_avg:61.07ms
step:861/2315 train_time:52582ms step_avg:61.07ms
step:862/2315 train_time:52642ms step_avg:61.07ms
step:863/2315 train_time:52703ms step_avg:61.07ms
step:864/2315 train_time:52763ms step_avg:61.07ms
step:865/2315 train_time:52824ms step_avg:61.07ms
step:866/2315 train_time:52884ms step_avg:61.07ms
step:867/2315 train_time:52946ms step_avg:61.07ms
step:868/2315 train_time:53007ms step_avg:61.07ms
step:869/2315 train_time:53067ms step_avg:61.07ms
step:870/2315 train_time:53128ms step_avg:61.07ms
step:871/2315 train_time:53190ms step_avg:61.07ms
step:872/2315 train_time:53250ms step_avg:61.07ms
step:873/2315 train_time:53311ms step_avg:61.07ms
step:874/2315 train_time:53373ms step_avg:61.07ms
step:875/2315 train_time:53434ms step_avg:61.07ms
step:876/2315 train_time:53495ms step_avg:61.07ms
step:877/2315 train_time:53557ms step_avg:61.07ms
step:878/2315 train_time:53618ms step_avg:61.07ms
step:879/2315 train_time:53679ms step_avg:61.07ms
step:880/2315 train_time:53740ms step_avg:61.07ms
step:881/2315 train_time:53801ms step_avg:61.07ms
step:882/2315 train_time:53861ms step_avg:61.07ms
step:883/2315 train_time:53922ms step_avg:61.07ms
step:884/2315 train_time:53982ms step_avg:61.07ms
step:885/2315 train_time:54043ms step_avg:61.07ms
step:886/2315 train_time:54104ms step_avg:61.07ms
step:887/2315 train_time:54164ms step_avg:61.06ms
step:888/2315 train_time:54224ms step_avg:61.06ms
step:889/2315 train_time:54285ms step_avg:61.06ms
step:890/2315 train_time:54345ms step_avg:61.06ms
step:891/2315 train_time:54406ms step_avg:61.06ms
step:892/2315 train_time:54468ms step_avg:61.06ms
step:893/2315 train_time:54529ms step_avg:61.06ms
step:894/2315 train_time:54589ms step_avg:61.06ms
step:895/2315 train_time:54650ms step_avg:61.06ms
step:896/2315 train_time:54711ms step_avg:61.06ms
step:897/2315 train_time:54772ms step_avg:61.06ms
step:898/2315 train_time:54834ms step_avg:61.06ms
step:899/2315 train_time:54895ms step_avg:61.06ms
step:900/2315 train_time:54956ms step_avg:61.06ms
step:901/2315 train_time:55017ms step_avg:61.06ms
step:902/2315 train_time:55078ms step_avg:61.06ms
step:903/2315 train_time:55139ms step_avg:61.06ms
step:904/2315 train_time:55200ms step_avg:61.06ms
step:905/2315 train_time:55261ms step_avg:61.06ms
step:906/2315 train_time:55322ms step_avg:61.06ms
step:907/2315 train_time:55382ms step_avg:61.06ms
step:908/2315 train_time:55443ms step_avg:61.06ms
step:909/2315 train_time:55503ms step_avg:61.06ms
step:910/2315 train_time:55563ms step_avg:61.06ms
step:911/2315 train_time:55624ms step_avg:61.06ms
step:912/2315 train_time:55685ms step_avg:61.06ms
step:913/2315 train_time:55746ms step_avg:61.06ms
step:914/2315 train_time:55806ms step_avg:61.06ms
step:915/2315 train_time:55867ms step_avg:61.06ms
step:916/2315 train_time:55928ms step_avg:61.06ms
step:917/2315 train_time:55989ms step_avg:61.06ms
step:918/2315 train_time:56049ms step_avg:61.06ms
step:919/2315 train_time:56110ms step_avg:61.06ms
step:920/2315 train_time:56171ms step_avg:61.06ms
step:921/2315 train_time:56233ms step_avg:61.06ms
step:922/2315 train_time:56294ms step_avg:61.06ms
step:923/2315 train_time:56356ms step_avg:61.06ms
step:924/2315 train_time:56416ms step_avg:61.06ms
step:925/2315 train_time:56477ms step_avg:61.06ms
step:926/2315 train_time:56538ms step_avg:61.06ms
step:927/2315 train_time:56599ms step_avg:61.06ms
step:928/2315 train_time:56659ms step_avg:61.06ms
step:929/2315 train_time:56720ms step_avg:61.05ms
step:930/2315 train_time:56780ms step_avg:61.05ms
step:931/2315 train_time:56841ms step_avg:61.05ms
step:932/2315 train_time:56902ms step_avg:61.05ms
step:933/2315 train_time:56962ms step_avg:61.05ms
step:934/2315 train_time:57023ms step_avg:61.05ms
step:935/2315 train_time:57083ms step_avg:61.05ms
step:936/2315 train_time:57143ms step_avg:61.05ms
step:937/2315 train_time:57204ms step_avg:61.05ms
step:938/2315 train_time:57264ms step_avg:61.05ms
step:939/2315 train_time:57325ms step_avg:61.05ms
step:940/2315 train_time:57386ms step_avg:61.05ms
step:941/2315 train_time:57447ms step_avg:61.05ms
step:942/2315 train_time:57509ms step_avg:61.05ms
step:943/2315 train_time:57569ms step_avg:61.05ms
step:944/2315 train_time:57630ms step_avg:61.05ms
step:945/2315 train_time:57691ms step_avg:61.05ms
step:946/2315 train_time:57752ms step_avg:61.05ms
step:947/2315 train_time:57813ms step_avg:61.05ms
step:948/2315 train_time:57874ms step_avg:61.05ms
step:949/2315 train_time:57935ms step_avg:61.05ms
step:950/2315 train_time:57996ms step_avg:61.05ms
step:951/2315 train_time:58056ms step_avg:61.05ms
step:952/2315 train_time:58117ms step_avg:61.05ms
step:953/2315 train_time:58178ms step_avg:61.05ms
step:954/2315 train_time:58239ms step_avg:61.05ms
step:955/2315 train_time:58300ms step_avg:61.05ms
step:956/2315 train_time:58361ms step_avg:61.05ms
step:957/2315 train_time:58422ms step_avg:61.05ms
step:958/2315 train_time:58483ms step_avg:61.05ms
step:959/2315 train_time:58543ms step_avg:61.05ms
step:960/2315 train_time:58604ms step_avg:61.05ms
step:961/2315 train_time:58664ms step_avg:61.04ms
step:962/2315 train_time:58725ms step_avg:61.04ms
step:963/2315 train_time:58786ms step_avg:61.04ms
step:964/2315 train_time:58846ms step_avg:61.04ms
step:965/2315 train_time:58907ms step_avg:61.04ms
step:966/2315 train_time:58967ms step_avg:61.04ms
step:967/2315 train_time:59029ms step_avg:61.04ms
step:968/2315 train_time:59089ms step_avg:61.04ms
step:969/2315 train_time:59150ms step_avg:61.04ms
step:970/2315 train_time:59210ms step_avg:61.04ms
step:971/2315 train_time:59272ms step_avg:61.04ms
step:972/2315 train_time:59333ms step_avg:61.04ms
step:973/2315 train_time:59395ms step_avg:61.04ms
step:974/2315 train_time:59455ms step_avg:61.04ms
step:975/2315 train_time:59517ms step_avg:61.04ms
step:976/2315 train_time:59578ms step_avg:61.04ms
step:977/2315 train_time:59638ms step_avg:61.04ms
step:978/2315 train_time:59699ms step_avg:61.04ms
step:979/2315 train_time:59760ms step_avg:61.04ms
step:980/2315 train_time:59820ms step_avg:61.04ms
step:981/2315 train_time:59881ms step_avg:61.04ms
step:982/2315 train_time:59942ms step_avg:61.04ms
step:983/2315 train_time:60002ms step_avg:61.04ms
step:984/2315 train_time:60063ms step_avg:61.04ms
step:985/2315 train_time:60123ms step_avg:61.04ms
step:986/2315 train_time:60183ms step_avg:61.04ms
step:987/2315 train_time:60244ms step_avg:61.04ms
step:988/2315 train_time:60305ms step_avg:61.04ms
step:989/2315 train_time:60366ms step_avg:61.04ms
step:990/2315 train_time:60427ms step_avg:61.04ms
step:991/2315 train_time:60488ms step_avg:61.04ms
step:992/2315 train_time:60548ms step_avg:61.04ms
step:993/2315 train_time:60610ms step_avg:61.04ms
step:994/2315 train_time:60671ms step_avg:61.04ms
step:995/2315 train_time:60732ms step_avg:61.04ms
step:996/2315 train_time:60793ms step_avg:61.04ms
step:997/2315 train_time:60853ms step_avg:61.04ms
step:998/2315 train_time:60914ms step_avg:61.04ms
step:999/2315 train_time:60975ms step_avg:61.04ms
step:1000/2315 train_time:61035ms step_avg:61.04ms
step:1000/2315 val_loss:3.5698 train_time:61098ms step_avg:61.10ms
step:1001/2315 train_time:61122ms step_avg:61.06ms
step:1002/2315 train_time:61159ms step_avg:61.04ms
step:1003/2315 train_time:61220ms step_avg:61.04ms
step:1004/2315 train_time:61286ms step_avg:61.04ms
step:1005/2315 train_time:61347ms step_avg:61.04ms
step:1006/2315 train_time:61407ms step_avg:61.04ms
step:1007/2315 train_time:61466ms step_avg:61.04ms
step:1008/2315 train_time:61526ms step_avg:61.04ms
step:1009/2315 train_time:61586ms step_avg:61.04ms
step:1010/2315 train_time:61645ms step_avg:61.04ms
step:1011/2315 train_time:61705ms step_avg:61.03ms
step:1012/2315 train_time:61765ms step_avg:61.03ms
step:1013/2315 train_time:61824ms step_avg:61.03ms
step:1014/2315 train_time:61884ms step_avg:61.03ms
step:1015/2315 train_time:61943ms step_avg:61.03ms
step:1016/2315 train_time:62005ms step_avg:61.03ms
step:1017/2315 train_time:62068ms step_avg:61.03ms
step:1018/2315 train_time:62130ms step_avg:61.03ms
step:1019/2315 train_time:62191ms step_avg:61.03ms
step:1020/2315 train_time:62252ms step_avg:61.03ms
step:1021/2315 train_time:62312ms step_avg:61.03ms
step:1022/2315 train_time:62373ms step_avg:61.03ms
step:1023/2315 train_time:62432ms step_avg:61.03ms
step:1024/2315 train_time:62492ms step_avg:61.03ms
step:1025/2315 train_time:62552ms step_avg:61.03ms
step:1026/2315 train_time:62612ms step_avg:61.03ms
step:1027/2315 train_time:62672ms step_avg:61.02ms
step:1028/2315 train_time:62732ms step_avg:61.02ms
step:1029/2315 train_time:62793ms step_avg:61.02ms
step:1030/2315 train_time:62853ms step_avg:61.02ms
step:1031/2315 train_time:62913ms step_avg:61.02ms
step:1032/2315 train_time:62974ms step_avg:61.02ms
step:1033/2315 train_time:63035ms step_avg:61.02ms
step:1034/2315 train_time:63095ms step_avg:61.02ms
step:1035/2315 train_time:63155ms step_avg:61.02ms
step:1036/2315 train_time:63216ms step_avg:61.02ms
step:1037/2315 train_time:63276ms step_avg:61.02ms
step:1038/2315 train_time:63337ms step_avg:61.02ms
step:1039/2315 train_time:63398ms step_avg:61.02ms
step:1040/2315 train_time:63459ms step_avg:61.02ms
step:1041/2315 train_time:63519ms step_avg:61.02ms
step:1042/2315 train_time:63580ms step_avg:61.02ms
step:1043/2315 train_time:63640ms step_avg:61.02ms
step:1044/2315 train_time:63701ms step_avg:61.02ms
step:1045/2315 train_time:63762ms step_avg:61.02ms
step:1046/2315 train_time:63823ms step_avg:61.02ms
step:1047/2315 train_time:63884ms step_avg:61.02ms
step:1048/2315 train_time:63945ms step_avg:61.02ms
step:1049/2315 train_time:64006ms step_avg:61.02ms
step:1050/2315 train_time:64067ms step_avg:61.02ms
step:1051/2315 train_time:64127ms step_avg:61.02ms
step:1052/2315 train_time:64188ms step_avg:61.02ms
step:1053/2315 train_time:64248ms step_avg:61.01ms
step:1054/2315 train_time:64309ms step_avg:61.01ms
step:1055/2315 train_time:64369ms step_avg:61.01ms
step:1056/2315 train_time:64429ms step_avg:61.01ms
step:1057/2315 train_time:64490ms step_avg:61.01ms
step:1058/2315 train_time:64550ms step_avg:61.01ms
step:1059/2315 train_time:64610ms step_avg:61.01ms
step:1060/2315 train_time:64671ms step_avg:61.01ms
step:1061/2315 train_time:64731ms step_avg:61.01ms
step:1062/2315 train_time:64792ms step_avg:61.01ms
step:1063/2315 train_time:64852ms step_avg:61.01ms
step:1064/2315 train_time:64913ms step_avg:61.01ms
step:1065/2315 train_time:64973ms step_avg:61.01ms
step:1066/2315 train_time:65033ms step_avg:61.01ms
step:1067/2315 train_time:65094ms step_avg:61.01ms
step:1068/2315 train_time:65154ms step_avg:61.01ms
step:1069/2315 train_time:65214ms step_avg:61.00ms
step:1070/2315 train_time:65274ms step_avg:61.00ms
step:1071/2315 train_time:65334ms step_avg:61.00ms
step:1072/2315 train_time:65395ms step_avg:61.00ms
step:1073/2315 train_time:65455ms step_avg:61.00ms
step:1074/2315 train_time:65516ms step_avg:61.00ms
step:1075/2315 train_time:65577ms step_avg:61.00ms
step:1076/2315 train_time:65638ms step_avg:61.00ms
step:1077/2315 train_time:65699ms step_avg:61.00ms
step:1078/2315 train_time:65760ms step_avg:61.00ms
step:1079/2315 train_time:65821ms step_avg:61.00ms
step:1080/2315 train_time:65882ms step_avg:61.00ms
step:1081/2315 train_time:65942ms step_avg:61.00ms
step:1082/2315 train_time:66003ms step_avg:61.00ms
step:1083/2315 train_time:66064ms step_avg:61.00ms
step:1084/2315 train_time:66124ms step_avg:61.00ms
step:1085/2315 train_time:66185ms step_avg:61.00ms
step:1086/2315 train_time:66246ms step_avg:61.00ms
step:1087/2315 train_time:66306ms step_avg:61.00ms
step:1088/2315 train_time:66367ms step_avg:61.00ms
step:1089/2315 train_time:66428ms step_avg:61.00ms
step:1090/2315 train_time:66488ms step_avg:61.00ms
step:1091/2315 train_time:66549ms step_avg:61.00ms
step:1092/2315 train_time:66610ms step_avg:61.00ms
step:1093/2315 train_time:66670ms step_avg:61.00ms
step:1094/2315 train_time:66730ms step_avg:61.00ms
step:1095/2315 train_time:66790ms step_avg:61.00ms
step:1096/2315 train_time:66851ms step_avg:61.00ms
step:1097/2315 train_time:66912ms step_avg:61.00ms
step:1098/2315 train_time:66973ms step_avg:61.00ms
step:1099/2315 train_time:67034ms step_avg:61.00ms
step:1100/2315 train_time:67096ms step_avg:61.00ms
step:1101/2315 train_time:67156ms step_avg:61.00ms
step:1102/2315 train_time:67216ms step_avg:60.99ms
step:1103/2315 train_time:67277ms step_avg:60.99ms
step:1104/2315 train_time:67338ms step_avg:60.99ms
step:1105/2315 train_time:67399ms step_avg:60.99ms
step:1106/2315 train_time:67460ms step_avg:60.99ms
step:1107/2315 train_time:67520ms step_avg:60.99ms
step:1108/2315 train_time:67581ms step_avg:60.99ms
step:1109/2315 train_time:67643ms step_avg:60.99ms
step:1110/2315 train_time:67703ms step_avg:60.99ms
step:1111/2315 train_time:67764ms step_avg:60.99ms
step:1112/2315 train_time:67825ms step_avg:60.99ms
step:1113/2315 train_time:67885ms step_avg:60.99ms
step:1114/2315 train_time:67946ms step_avg:60.99ms
step:1115/2315 train_time:68007ms step_avg:60.99ms
step:1116/2315 train_time:68068ms step_avg:60.99ms
step:1117/2315 train_time:68129ms step_avg:60.99ms
step:1118/2315 train_time:68189ms step_avg:60.99ms
step:1119/2315 train_time:68249ms step_avg:60.99ms
step:1120/2315 train_time:68310ms step_avg:60.99ms
step:1121/2315 train_time:68370ms step_avg:60.99ms
step:1122/2315 train_time:68431ms step_avg:60.99ms
step:1123/2315 train_time:68492ms step_avg:60.99ms
step:1124/2315 train_time:68553ms step_avg:60.99ms
step:1125/2315 train_time:68614ms step_avg:60.99ms
step:1126/2315 train_time:68674ms step_avg:60.99ms
step:1127/2315 train_time:68734ms step_avg:60.99ms
step:1128/2315 train_time:68795ms step_avg:60.99ms
step:1129/2315 train_time:68855ms step_avg:60.99ms
step:1130/2315 train_time:68916ms step_avg:60.99ms
step:1131/2315 train_time:68977ms step_avg:60.99ms
step:1132/2315 train_time:69037ms step_avg:60.99ms
step:1133/2315 train_time:69097ms step_avg:60.99ms
step:1134/2315 train_time:69159ms step_avg:60.99ms
step:1135/2315 train_time:69220ms step_avg:60.99ms
step:1136/2315 train_time:69281ms step_avg:60.99ms
step:1137/2315 train_time:69342ms step_avg:60.99ms
step:1138/2315 train_time:69404ms step_avg:60.99ms
step:1139/2315 train_time:69465ms step_avg:60.99ms
step:1140/2315 train_time:69526ms step_avg:60.99ms
step:1141/2315 train_time:69586ms step_avg:60.99ms
step:1142/2315 train_time:69647ms step_avg:60.99ms
step:1143/2315 train_time:69708ms step_avg:60.99ms
step:1144/2315 train_time:69768ms step_avg:60.99ms
step:1145/2315 train_time:69829ms step_avg:60.99ms
step:1146/2315 train_time:69889ms step_avg:60.99ms
step:1147/2315 train_time:69949ms step_avg:60.98ms
step:1148/2315 train_time:70010ms step_avg:60.98ms
step:1149/2315 train_time:70070ms step_avg:60.98ms
step:1150/2315 train_time:70131ms step_avg:60.98ms
step:1151/2315 train_time:70191ms step_avg:60.98ms
step:1152/2315 train_time:70252ms step_avg:60.98ms
step:1153/2315 train_time:70313ms step_avg:60.98ms
step:1154/2315 train_time:70373ms step_avg:60.98ms
step:1155/2315 train_time:70433ms step_avg:60.98ms
step:1156/2315 train_time:70495ms step_avg:60.98ms
step:1157/2315 train_time:70555ms step_avg:60.98ms
step:1158/2315 train_time:70616ms step_avg:60.98ms
step:1159/2315 train_time:70676ms step_avg:60.98ms
step:1160/2315 train_time:70737ms step_avg:60.98ms
step:1161/2315 train_time:70798ms step_avg:60.98ms
step:1162/2315 train_time:70859ms step_avg:60.98ms
step:1163/2315 train_time:70920ms step_avg:60.98ms
step:1164/2315 train_time:70981ms step_avg:60.98ms
step:1165/2315 train_time:71042ms step_avg:60.98ms
step:1166/2315 train_time:71103ms step_avg:60.98ms
step:1167/2315 train_time:71164ms step_avg:60.98ms
step:1168/2315 train_time:71224ms step_avg:60.98ms
step:1169/2315 train_time:71285ms step_avg:60.98ms
step:1170/2315 train_time:71346ms step_avg:60.98ms
step:1171/2315 train_time:71407ms step_avg:60.98ms
step:1172/2315 train_time:71468ms step_avg:60.98ms
step:1173/2315 train_time:71528ms step_avg:60.98ms
step:1174/2315 train_time:71589ms step_avg:60.98ms
step:1175/2315 train_time:71649ms step_avg:60.98ms
step:1176/2315 train_time:71710ms step_avg:60.98ms
step:1177/2315 train_time:71770ms step_avg:60.98ms
step:1178/2315 train_time:71831ms step_avg:60.98ms
step:1179/2315 train_time:71891ms step_avg:60.98ms
step:1180/2315 train_time:71952ms step_avg:60.98ms
step:1181/2315 train_time:72012ms step_avg:60.98ms
step:1182/2315 train_time:72072ms step_avg:60.97ms
step:1183/2315 train_time:72133ms step_avg:60.97ms
step:1184/2315 train_time:72194ms step_avg:60.97ms
step:1185/2315 train_time:72254ms step_avg:60.97ms
step:1186/2315 train_time:72315ms step_avg:60.97ms
step:1187/2315 train_time:72375ms step_avg:60.97ms
step:1188/2315 train_time:72436ms step_avg:60.97ms
step:1189/2315 train_time:72497ms step_avg:60.97ms
step:1190/2315 train_time:72559ms step_avg:60.97ms
step:1191/2315 train_time:72620ms step_avg:60.97ms
step:1192/2315 train_time:72681ms step_avg:60.97ms
step:1193/2315 train_time:72742ms step_avg:60.97ms
step:1194/2315 train_time:72802ms step_avg:60.97ms
step:1195/2315 train_time:72863ms step_avg:60.97ms
step:1196/2315 train_time:72924ms step_avg:60.97ms
step:1197/2315 train_time:72985ms step_avg:60.97ms
step:1198/2315 train_time:73045ms step_avg:60.97ms
step:1199/2315 train_time:73105ms step_avg:60.97ms
step:1200/2315 train_time:73166ms step_avg:60.97ms
step:1201/2315 train_time:73227ms step_avg:60.97ms
step:1202/2315 train_time:73287ms step_avg:60.97ms
step:1203/2315 train_time:73347ms step_avg:60.97ms
step:1204/2315 train_time:73408ms step_avg:60.97ms
step:1205/2315 train_time:73468ms step_avg:60.97ms
step:1206/2315 train_time:73529ms step_avg:60.97ms
step:1207/2315 train_time:73589ms step_avg:60.97ms
step:1208/2315 train_time:73650ms step_avg:60.97ms
step:1209/2315 train_time:73711ms step_avg:60.97ms
step:1210/2315 train_time:73772ms step_avg:60.97ms
step:1211/2315 train_time:73832ms step_avg:60.97ms
step:1212/2315 train_time:73894ms step_avg:60.97ms
step:1213/2315 train_time:73954ms step_avg:60.97ms
step:1214/2315 train_time:74016ms step_avg:60.97ms
step:1215/2315 train_time:74077ms step_avg:60.97ms
step:1216/2315 train_time:74137ms step_avg:60.97ms
step:1217/2315 train_time:74197ms step_avg:60.97ms
step:1218/2315 train_time:74258ms step_avg:60.97ms
step:1219/2315 train_time:74319ms step_avg:60.97ms
step:1220/2315 train_time:74380ms step_avg:60.97ms
step:1221/2315 train_time:74440ms step_avg:60.97ms
step:1222/2315 train_time:74502ms step_avg:60.97ms
step:1223/2315 train_time:74563ms step_avg:60.97ms
step:1224/2315 train_time:74623ms step_avg:60.97ms
step:1225/2315 train_time:74684ms step_avg:60.97ms
step:1226/2315 train_time:74745ms step_avg:60.97ms
step:1227/2315 train_time:74806ms step_avg:60.97ms
step:1228/2315 train_time:74866ms step_avg:60.97ms
step:1229/2315 train_time:74927ms step_avg:60.97ms
step:1230/2315 train_time:74987ms step_avg:60.97ms
step:1231/2315 train_time:75047ms step_avg:60.96ms
step:1232/2315 train_time:75108ms step_avg:60.96ms
step:1233/2315 train_time:75168ms step_avg:60.96ms
step:1234/2315 train_time:75228ms step_avg:60.96ms
step:1235/2315 train_time:75289ms step_avg:60.96ms
step:1236/2315 train_time:75350ms step_avg:60.96ms
step:1237/2315 train_time:75409ms step_avg:60.96ms
step:1238/2315 train_time:75470ms step_avg:60.96ms
step:1239/2315 train_time:75530ms step_avg:60.96ms
step:1240/2315 train_time:75591ms step_avg:60.96ms
step:1241/2315 train_time:75652ms step_avg:60.96ms
step:1242/2315 train_time:75713ms step_avg:60.96ms
step:1243/2315 train_time:75774ms step_avg:60.96ms
step:1244/2315 train_time:75835ms step_avg:60.96ms
step:1245/2315 train_time:75895ms step_avg:60.96ms
step:1246/2315 train_time:75956ms step_avg:60.96ms
step:1247/2315 train_time:76017ms step_avg:60.96ms
step:1248/2315 train_time:76077ms step_avg:60.96ms
step:1249/2315 train_time:76138ms step_avg:60.96ms
step:1250/2315 train_time:76199ms step_avg:60.96ms
step:1250/2315 val_loss:3.5134 train_time:76262ms step_avg:61.01ms
step:1251/2315 train_time:76283ms step_avg:60.98ms
step:1252/2315 train_time:76324ms step_avg:60.96ms
step:1253/2315 train_time:76387ms step_avg:60.96ms
step:1254/2315 train_time:76450ms step_avg:60.96ms
step:1255/2315 train_time:76512ms step_avg:60.97ms
step:1256/2315 train_time:76573ms step_avg:60.97ms
step:1257/2315 train_time:76633ms step_avg:60.97ms
step:1258/2315 train_time:76693ms step_avg:60.96ms
step:1259/2315 train_time:76754ms step_avg:60.96ms
step:1260/2315 train_time:76814ms step_avg:60.96ms
step:1261/2315 train_time:76874ms step_avg:60.96ms
step:1262/2315 train_time:76934ms step_avg:60.96ms
step:1263/2315 train_time:76993ms step_avg:60.96ms
step:1264/2315 train_time:77053ms step_avg:60.96ms
step:1265/2315 train_time:77114ms step_avg:60.96ms
step:1266/2315 train_time:77175ms step_avg:60.96ms
step:1267/2315 train_time:77237ms step_avg:60.96ms
step:1268/2315 train_time:77299ms step_avg:60.96ms
step:1269/2315 train_time:77361ms step_avg:60.96ms
step:1270/2315 train_time:77422ms step_avg:60.96ms
step:1271/2315 train_time:77483ms step_avg:60.96ms
step:1272/2315 train_time:77544ms step_avg:60.96ms
step:1273/2315 train_time:77604ms step_avg:60.96ms
step:1274/2315 train_time:77665ms step_avg:60.96ms
step:1275/2315 train_time:77725ms step_avg:60.96ms
step:1276/2315 train_time:77785ms step_avg:60.96ms
step:1277/2315 train_time:77845ms step_avg:60.96ms
step:1278/2315 train_time:77906ms step_avg:60.96ms
step:1279/2315 train_time:77966ms step_avg:60.96ms
step:1280/2315 train_time:78027ms step_avg:60.96ms
step:1281/2315 train_time:78087ms step_avg:60.96ms
step:1282/2315 train_time:78147ms step_avg:60.96ms
step:1283/2315 train_time:78208ms step_avg:60.96ms
step:1284/2315 train_time:78269ms step_avg:60.96ms
step:1285/2315 train_time:78331ms step_avg:60.96ms
step:1286/2315 train_time:78393ms step_avg:60.96ms
step:1287/2315 train_time:78455ms step_avg:60.96ms
step:1288/2315 train_time:78516ms step_avg:60.96ms
step:1289/2315 train_time:78577ms step_avg:60.96ms
step:1290/2315 train_time:78637ms step_avg:60.96ms
step:1291/2315 train_time:78698ms step_avg:60.96ms
step:1292/2315 train_time:78758ms step_avg:60.96ms
step:1293/2315 train_time:78819ms step_avg:60.96ms
step:1294/2315 train_time:78879ms step_avg:60.96ms
step:1295/2315 train_time:78940ms step_avg:60.96ms
step:1296/2315 train_time:79000ms step_avg:60.96ms
step:1297/2315 train_time:79061ms step_avg:60.96ms
step:1298/2315 train_time:79122ms step_avg:60.96ms
step:1299/2315 train_time:79182ms step_avg:60.96ms
step:1300/2315 train_time:79242ms step_avg:60.96ms
step:1301/2315 train_time:79303ms step_avg:60.96ms
step:1302/2315 train_time:79363ms step_avg:60.96ms
step:1303/2315 train_time:79425ms step_avg:60.96ms
step:1304/2315 train_time:79488ms step_avg:60.96ms
step:1305/2315 train_time:79547ms step_avg:60.96ms
step:1306/2315 train_time:79608ms step_avg:60.96ms
step:1307/2315 train_time:79669ms step_avg:60.96ms
step:1308/2315 train_time:79730ms step_avg:60.96ms
step:1309/2315 train_time:79790ms step_avg:60.96ms
step:1310/2315 train_time:79851ms step_avg:60.96ms
step:1311/2315 train_time:79912ms step_avg:60.96ms
step:1312/2315 train_time:79973ms step_avg:60.96ms
step:1313/2315 train_time:80034ms step_avg:60.96ms
step:1314/2315 train_time:80095ms step_avg:60.96ms
step:1315/2315 train_time:80156ms step_avg:60.96ms
step:1316/2315 train_time:80217ms step_avg:60.95ms
step:1317/2315 train_time:80277ms step_avg:60.95ms
step:1318/2315 train_time:80338ms step_avg:60.95ms
step:1319/2315 train_time:80398ms step_avg:60.95ms
step:1320/2315 train_time:80459ms step_avg:60.95ms
step:1321/2315 train_time:80520ms step_avg:60.95ms
step:1322/2315 train_time:80580ms step_avg:60.95ms
step:1323/2315 train_time:80641ms step_avg:60.95ms
step:1324/2315 train_time:80701ms step_avg:60.95ms
step:1325/2315 train_time:80761ms step_avg:60.95ms
step:1326/2315 train_time:80822ms step_avg:60.95ms
step:1327/2315 train_time:80882ms step_avg:60.95ms
step:1328/2315 train_time:80943ms step_avg:60.95ms
step:1329/2315 train_time:81004ms step_avg:60.95ms
step:1330/2315 train_time:81065ms step_avg:60.95ms
step:1331/2315 train_time:81125ms step_avg:60.95ms
step:1332/2315 train_time:81187ms step_avg:60.95ms
step:1333/2315 train_time:81248ms step_avg:60.95ms
step:1334/2315 train_time:81308ms step_avg:60.95ms
step:1335/2315 train_time:81368ms step_avg:60.95ms
step:1336/2315 train_time:81429ms step_avg:60.95ms
step:1337/2315 train_time:81490ms step_avg:60.95ms
step:1338/2315 train_time:81551ms step_avg:60.95ms
step:1339/2315 train_time:81612ms step_avg:60.95ms
step:1340/2315 train_time:81673ms step_avg:60.95ms
step:1341/2315 train_time:81734ms step_avg:60.95ms
step:1342/2315 train_time:81794ms step_avg:60.95ms
step:1343/2315 train_time:81855ms step_avg:60.95ms
step:1344/2315 train_time:81916ms step_avg:60.95ms
step:1345/2315 train_time:81977ms step_avg:60.95ms
step:1346/2315 train_time:82037ms step_avg:60.95ms
step:1347/2315 train_time:82098ms step_avg:60.95ms
step:1348/2315 train_time:82159ms step_avg:60.95ms
step:1349/2315 train_time:82219ms step_avg:60.95ms
step:1350/2315 train_time:82280ms step_avg:60.95ms
step:1351/2315 train_time:82340ms step_avg:60.95ms
step:1352/2315 train_time:82401ms step_avg:60.95ms
step:1353/2315 train_time:82462ms step_avg:60.95ms
step:1354/2315 train_time:82523ms step_avg:60.95ms
step:1355/2315 train_time:82583ms step_avg:60.95ms
step:1356/2315 train_time:82644ms step_avg:60.95ms
step:1357/2315 train_time:82704ms step_avg:60.95ms
step:1358/2315 train_time:82765ms step_avg:60.95ms
step:1359/2315 train_time:82826ms step_avg:60.95ms
step:1360/2315 train_time:82886ms step_avg:60.95ms
step:1361/2315 train_time:82947ms step_avg:60.95ms
step:1362/2315 train_time:83007ms step_avg:60.95ms
step:1363/2315 train_time:83068ms step_avg:60.95ms
step:1364/2315 train_time:83130ms step_avg:60.95ms
step:1365/2315 train_time:83191ms step_avg:60.95ms
step:1366/2315 train_time:83253ms step_avg:60.95ms
step:1367/2315 train_time:83314ms step_avg:60.95ms
step:1368/2315 train_time:83374ms step_avg:60.95ms
step:1369/2315 train_time:83435ms step_avg:60.95ms
step:1370/2315 train_time:83496ms step_avg:60.95ms
step:1371/2315 train_time:83557ms step_avg:60.95ms
step:1372/2315 train_time:83618ms step_avg:60.95ms
step:1373/2315 train_time:83678ms step_avg:60.95ms
step:1374/2315 train_time:83739ms step_avg:60.95ms
step:1375/2315 train_time:83800ms step_avg:60.95ms
step:1376/2315 train_time:83860ms step_avg:60.94ms
step:1377/2315 train_time:83921ms step_avg:60.94ms
step:1378/2315 train_time:83982ms step_avg:60.94ms
step:1379/2315 train_time:84042ms step_avg:60.94ms
step:1380/2315 train_time:84102ms step_avg:60.94ms
step:1381/2315 train_time:84162ms step_avg:60.94ms
step:1382/2315 train_time:84223ms step_avg:60.94ms
step:1383/2315 train_time:84283ms step_avg:60.94ms
step:1384/2315 train_time:84344ms step_avg:60.94ms
step:1385/2315 train_time:84404ms step_avg:60.94ms
step:1386/2315 train_time:84466ms step_avg:60.94ms
step:1387/2315 train_time:84527ms step_avg:60.94ms
step:1388/2315 train_time:84588ms step_avg:60.94ms
step:1389/2315 train_time:84649ms step_avg:60.94ms
step:1390/2315 train_time:84709ms step_avg:60.94ms
step:1391/2315 train_time:84770ms step_avg:60.94ms
step:1392/2315 train_time:84831ms step_avg:60.94ms
step:1393/2315 train_time:84892ms step_avg:60.94ms
step:1394/2315 train_time:84953ms step_avg:60.94ms
step:1395/2315 train_time:85013ms step_avg:60.94ms
step:1396/2315 train_time:85074ms step_avg:60.94ms
step:1397/2315 train_time:85135ms step_avg:60.94ms
step:1398/2315 train_time:85196ms step_avg:60.94ms
step:1399/2315 train_time:85257ms step_avg:60.94ms
step:1400/2315 train_time:85318ms step_avg:60.94ms
step:1401/2315 train_time:85379ms step_avg:60.94ms
step:1402/2315 train_time:85440ms step_avg:60.94ms
step:1403/2315 train_time:85500ms step_avg:60.94ms
step:1404/2315 train_time:85560ms step_avg:60.94ms
step:1405/2315 train_time:85621ms step_avg:60.94ms
step:1406/2315 train_time:85681ms step_avg:60.94ms
step:1407/2315 train_time:85741ms step_avg:60.94ms
step:1408/2315 train_time:85802ms step_avg:60.94ms
step:1409/2315 train_time:85862ms step_avg:60.94ms
step:1410/2315 train_time:85923ms step_avg:60.94ms
step:1411/2315 train_time:85984ms step_avg:60.94ms
step:1412/2315 train_time:86045ms step_avg:60.94ms
step:1413/2315 train_time:86105ms step_avg:60.94ms
step:1414/2315 train_time:86166ms step_avg:60.94ms
step:1415/2315 train_time:86226ms step_avg:60.94ms
step:1416/2315 train_time:86288ms step_avg:60.94ms
step:1417/2315 train_time:86349ms step_avg:60.94ms
step:1418/2315 train_time:86410ms step_avg:60.94ms
step:1419/2315 train_time:86470ms step_avg:60.94ms
step:1420/2315 train_time:86532ms step_avg:60.94ms
step:1421/2315 train_time:86593ms step_avg:60.94ms
step:1422/2315 train_time:86654ms step_avg:60.94ms
step:1423/2315 train_time:86715ms step_avg:60.94ms
step:1424/2315 train_time:86776ms step_avg:60.94ms
step:1425/2315 train_time:86837ms step_avg:60.94ms
step:1426/2315 train_time:86897ms step_avg:60.94ms
step:1427/2315 train_time:86958ms step_avg:60.94ms
step:1428/2315 train_time:87018ms step_avg:60.94ms
step:1429/2315 train_time:87078ms step_avg:60.94ms
step:1430/2315 train_time:87139ms step_avg:60.94ms
step:1431/2315 train_time:87199ms step_avg:60.94ms
step:1432/2315 train_time:87260ms step_avg:60.94ms
step:1433/2315 train_time:87320ms step_avg:60.94ms
step:1434/2315 train_time:87381ms step_avg:60.93ms
step:1435/2315 train_time:87441ms step_avg:60.93ms
step:1436/2315 train_time:87501ms step_avg:60.93ms
step:1437/2315 train_time:87562ms step_avg:60.93ms
step:1438/2315 train_time:87624ms step_avg:60.93ms
step:1439/2315 train_time:87685ms step_avg:60.93ms
step:1440/2315 train_time:87746ms step_avg:60.94ms
step:1441/2315 train_time:87807ms step_avg:60.93ms
step:1442/2315 train_time:87867ms step_avg:60.93ms
step:1443/2315 train_time:87928ms step_avg:60.93ms
step:1444/2315 train_time:87989ms step_avg:60.93ms
step:1445/2315 train_time:88050ms step_avg:60.93ms
step:1446/2315 train_time:88111ms step_avg:60.93ms
step:1447/2315 train_time:88172ms step_avg:60.93ms
step:1448/2315 train_time:88232ms step_avg:60.93ms
step:1449/2315 train_time:88293ms step_avg:60.93ms
step:1450/2315 train_time:88355ms step_avg:60.93ms
step:1451/2315 train_time:88416ms step_avg:60.93ms
step:1452/2315 train_time:88477ms step_avg:60.93ms
step:1453/2315 train_time:88537ms step_avg:60.93ms
step:1454/2315 train_time:88598ms step_avg:60.93ms
step:1455/2315 train_time:88658ms step_avg:60.93ms
step:1456/2315 train_time:88720ms step_avg:60.93ms
step:1457/2315 train_time:88780ms step_avg:60.93ms
step:1458/2315 train_time:88841ms step_avg:60.93ms
step:1459/2315 train_time:88902ms step_avg:60.93ms
step:1460/2315 train_time:88962ms step_avg:60.93ms
step:1461/2315 train_time:89023ms step_avg:60.93ms
step:1462/2315 train_time:89084ms step_avg:60.93ms
step:1463/2315 train_time:89144ms step_avg:60.93ms
step:1464/2315 train_time:89205ms step_avg:60.93ms
step:1465/2315 train_time:89266ms step_avg:60.93ms
step:1466/2315 train_time:89327ms step_avg:60.93ms
step:1467/2315 train_time:89388ms step_avg:60.93ms
step:1468/2315 train_time:89449ms step_avg:60.93ms
step:1469/2315 train_time:89509ms step_avg:60.93ms
step:1470/2315 train_time:89571ms step_avg:60.93ms
step:1471/2315 train_time:89631ms step_avg:60.93ms
step:1472/2315 train_time:89692ms step_avg:60.93ms
step:1473/2315 train_time:89753ms step_avg:60.93ms
step:1474/2315 train_time:89815ms step_avg:60.93ms
step:1475/2315 train_time:89875ms step_avg:60.93ms
step:1476/2315 train_time:89936ms step_avg:60.93ms
step:1477/2315 train_time:89998ms step_avg:60.93ms
step:1478/2315 train_time:90058ms step_avg:60.93ms
step:1479/2315 train_time:90119ms step_avg:60.93ms
step:1480/2315 train_time:90180ms step_avg:60.93ms
step:1481/2315 train_time:90241ms step_avg:60.93ms
step:1482/2315 train_time:90301ms step_avg:60.93ms
step:1483/2315 train_time:90361ms step_avg:60.93ms
step:1484/2315 train_time:90422ms step_avg:60.93ms
step:1485/2315 train_time:90482ms step_avg:60.93ms
step:1486/2315 train_time:90543ms step_avg:60.93ms
step:1487/2315 train_time:90604ms step_avg:60.93ms
step:1488/2315 train_time:90665ms step_avg:60.93ms
step:1489/2315 train_time:90726ms step_avg:60.93ms
step:1490/2315 train_time:90786ms step_avg:60.93ms
step:1491/2315 train_time:90847ms step_avg:60.93ms
step:1492/2315 train_time:90908ms step_avg:60.93ms
step:1493/2315 train_time:90968ms step_avg:60.93ms
step:1494/2315 train_time:91029ms step_avg:60.93ms
step:1495/2315 train_time:91090ms step_avg:60.93ms
step:1496/2315 train_time:91151ms step_avg:60.93ms
step:1497/2315 train_time:91212ms step_avg:60.93ms
step:1498/2315 train_time:91273ms step_avg:60.93ms
step:1499/2315 train_time:91334ms step_avg:60.93ms
step:1500/2315 train_time:91395ms step_avg:60.93ms
step:1500/2315 val_loss:3.4512 train_time:91457ms step_avg:60.97ms
step:1501/2315 train_time:91479ms step_avg:60.95ms
step:1502/2315 train_time:91519ms step_avg:60.93ms
step:1503/2315 train_time:91585ms step_avg:60.93ms
step:1504/2315 train_time:91649ms step_avg:60.94ms
step:1505/2315 train_time:91710ms step_avg:60.94ms
step:1506/2315 train_time:91771ms step_avg:60.94ms
step:1507/2315 train_time:91832ms step_avg:60.94ms
step:1508/2315 train_time:91891ms step_avg:60.94ms
step:1509/2315 train_time:91952ms step_avg:60.94ms
step:1510/2315 train_time:92012ms step_avg:60.93ms
step:1511/2315 train_time:92071ms step_avg:60.93ms
step:1512/2315 train_time:92132ms step_avg:60.93ms
step:1513/2315 train_time:92191ms step_avg:60.93ms
step:1514/2315 train_time:92252ms step_avg:60.93ms
step:1515/2315 train_time:92311ms step_avg:60.93ms
step:1516/2315 train_time:92371ms step_avg:60.93ms
step:1517/2315 train_time:92432ms step_avg:60.93ms
step:1518/2315 train_time:92494ms step_avg:60.93ms
step:1519/2315 train_time:92556ms step_avg:60.93ms
step:1520/2315 train_time:92618ms step_avg:60.93ms
step:1521/2315 train_time:92680ms step_avg:60.93ms
step:1522/2315 train_time:92741ms step_avg:60.93ms
step:1523/2315 train_time:92802ms step_avg:60.93ms
step:1524/2315 train_time:92863ms step_avg:60.93ms
step:1525/2315 train_time:92925ms step_avg:60.93ms
step:1526/2315 train_time:92986ms step_avg:60.93ms
step:1527/2315 train_time:93048ms step_avg:60.94ms
step:1528/2315 train_time:93109ms step_avg:60.94ms
step:1529/2315 train_time:93170ms step_avg:60.94ms
step:1530/2315 train_time:93231ms step_avg:60.94ms
step:1531/2315 train_time:93291ms step_avg:60.93ms
step:1532/2315 train_time:93351ms step_avg:60.93ms
step:1533/2315 train_time:93413ms step_avg:60.93ms
step:1534/2315 train_time:93474ms step_avg:60.93ms
step:1535/2315 train_time:93536ms step_avg:60.94ms
step:1536/2315 train_time:93598ms step_avg:60.94ms
step:1537/2315 train_time:93658ms step_avg:60.94ms
step:1538/2315 train_time:93719ms step_avg:60.94ms
step:1539/2315 train_time:93780ms step_avg:60.94ms
step:1540/2315 train_time:93841ms step_avg:60.94ms
step:1541/2315 train_time:93902ms step_avg:60.94ms
step:1542/2315 train_time:93963ms step_avg:60.94ms
step:1543/2315 train_time:94025ms step_avg:60.94ms
step:1544/2315 train_time:94086ms step_avg:60.94ms
step:1545/2315 train_time:94147ms step_avg:60.94ms
step:1546/2315 train_time:94208ms step_avg:60.94ms
step:1547/2315 train_time:94269ms step_avg:60.94ms
step:1548/2315 train_time:94330ms step_avg:60.94ms
step:1549/2315 train_time:94391ms step_avg:60.94ms
step:1550/2315 train_time:94452ms step_avg:60.94ms
step:1551/2315 train_time:94513ms step_avg:60.94ms
step:1552/2315 train_time:94574ms step_avg:60.94ms
step:1553/2315 train_time:94635ms step_avg:60.94ms
step:1554/2315 train_time:94697ms step_avg:60.94ms
step:1555/2315 train_time:94758ms step_avg:60.94ms
step:1556/2315 train_time:94819ms step_avg:60.94ms
step:1557/2315 train_time:94880ms step_avg:60.94ms
step:1558/2315 train_time:94941ms step_avg:60.94ms
step:1559/2315 train_time:95001ms step_avg:60.94ms
step:1560/2315 train_time:95062ms step_avg:60.94ms
step:1561/2315 train_time:95124ms step_avg:60.94ms
step:1562/2315 train_time:95185ms step_avg:60.94ms
step:1563/2315 train_time:95246ms step_avg:60.94ms
step:1564/2315 train_time:95307ms step_avg:60.94ms
step:1565/2315 train_time:95369ms step_avg:60.94ms
step:1566/2315 train_time:95430ms step_avg:60.94ms
step:1567/2315 train_time:95491ms step_avg:60.94ms
step:1568/2315 train_time:95553ms step_avg:60.94ms
step:1569/2315 train_time:95613ms step_avg:60.94ms
step:1570/2315 train_time:95675ms step_avg:60.94ms
step:1571/2315 train_time:95735ms step_avg:60.94ms
step:1572/2315 train_time:95796ms step_avg:60.94ms
step:1573/2315 train_time:95857ms step_avg:60.94ms
step:1574/2315 train_time:95918ms step_avg:60.94ms
step:1575/2315 train_time:95979ms step_avg:60.94ms
step:1576/2315 train_time:96040ms step_avg:60.94ms
step:1577/2315 train_time:96102ms step_avg:60.94ms
step:1578/2315 train_time:96163ms step_avg:60.94ms
step:1579/2315 train_time:96224ms step_avg:60.94ms
step:1580/2315 train_time:96286ms step_avg:60.94ms
step:1581/2315 train_time:96347ms step_avg:60.94ms
step:1582/2315 train_time:96409ms step_avg:60.94ms
step:1583/2315 train_time:96471ms step_avg:60.94ms
step:1584/2315 train_time:96532ms step_avg:60.94ms
step:1585/2315 train_time:96593ms step_avg:60.94ms
step:1586/2315 train_time:96654ms step_avg:60.94ms
step:1587/2315 train_time:96715ms step_avg:60.94ms
step:1588/2315 train_time:96776ms step_avg:60.94ms
step:1589/2315 train_time:96837ms step_avg:60.94ms
step:1590/2315 train_time:96898ms step_avg:60.94ms
step:1591/2315 train_time:96959ms step_avg:60.94ms
step:1592/2315 train_time:97020ms step_avg:60.94ms
step:1593/2315 train_time:97081ms step_avg:60.94ms
step:1594/2315 train_time:97142ms step_avg:60.94ms
step:1595/2315 train_time:97203ms step_avg:60.94ms
step:1596/2315 train_time:97264ms step_avg:60.94ms
step:1597/2315 train_time:97325ms step_avg:60.94ms
step:1598/2315 train_time:97388ms step_avg:60.94ms
step:1599/2315 train_time:97450ms step_avg:60.94ms
step:1600/2315 train_time:97510ms step_avg:60.94ms
step:1601/2315 train_time:97572ms step_avg:60.94ms
step:1602/2315 train_time:97633ms step_avg:60.94ms
step:1603/2315 train_time:97694ms step_avg:60.94ms
step:1604/2315 train_time:97755ms step_avg:60.94ms
step:1605/2315 train_time:97815ms step_avg:60.94ms
step:1606/2315 train_time:97877ms step_avg:60.94ms
step:1607/2315 train_time:97938ms step_avg:60.94ms
step:1608/2315 train_time:97999ms step_avg:60.94ms
step:1609/2315 train_time:98060ms step_avg:60.94ms
step:1610/2315 train_time:98121ms step_avg:60.94ms
step:1611/2315 train_time:98182ms step_avg:60.94ms
step:1612/2315 train_time:98242ms step_avg:60.94ms
step:1613/2315 train_time:98303ms step_avg:60.94ms
step:1614/2315 train_time:98364ms step_avg:60.94ms
step:1615/2315 train_time:98426ms step_avg:60.95ms
step:1616/2315 train_time:98488ms step_avg:60.95ms
step:1617/2315 train_time:98550ms step_avg:60.95ms
step:1618/2315 train_time:98611ms step_avg:60.95ms
step:1619/2315 train_time:98672ms step_avg:60.95ms
step:1620/2315 train_time:98733ms step_avg:60.95ms
step:1621/2315 train_time:98794ms step_avg:60.95ms
step:1622/2315 train_time:98855ms step_avg:60.95ms
step:1623/2315 train_time:98916ms step_avg:60.95ms
step:1624/2315 train_time:98977ms step_avg:60.95ms
step:1625/2315 train_time:99038ms step_avg:60.95ms
step:1626/2315 train_time:99099ms step_avg:60.95ms
step:1627/2315 train_time:99161ms step_avg:60.95ms
step:1628/2315 train_time:99222ms step_avg:60.95ms
step:1629/2315 train_time:99282ms step_avg:60.95ms
step:1630/2315 train_time:99344ms step_avg:60.95ms
step:1631/2315 train_time:99405ms step_avg:60.95ms
step:1632/2315 train_time:99467ms step_avg:60.95ms
step:1633/2315 train_time:99529ms step_avg:60.95ms
step:1634/2315 train_time:99590ms step_avg:60.95ms
step:1635/2315 train_time:99651ms step_avg:60.95ms
step:1636/2315 train_time:99713ms step_avg:60.95ms
step:1637/2315 train_time:99773ms step_avg:60.95ms
step:1638/2315 train_time:99835ms step_avg:60.95ms
step:1639/2315 train_time:99896ms step_avg:60.95ms
step:1640/2315 train_time:99956ms step_avg:60.95ms
step:1641/2315 train_time:100017ms step_avg:60.95ms
step:1642/2315 train_time:100079ms step_avg:60.95ms
step:1643/2315 train_time:100139ms step_avg:60.95ms
step:1644/2315 train_time:100200ms step_avg:60.95ms
step:1645/2315 train_time:100262ms step_avg:60.95ms
step:1646/2315 train_time:100322ms step_avg:60.95ms
step:1647/2315 train_time:100383ms step_avg:60.95ms
step:1648/2315 train_time:100445ms step_avg:60.95ms
step:1649/2315 train_time:100506ms step_avg:60.95ms
step:1650/2315 train_time:100567ms step_avg:60.95ms
step:1651/2315 train_time:100629ms step_avg:60.95ms
step:1652/2315 train_time:100691ms step_avg:60.95ms
step:1653/2315 train_time:100751ms step_avg:60.95ms
step:1654/2315 train_time:100813ms step_avg:60.95ms
step:1655/2315 train_time:100874ms step_avg:60.95ms
step:1656/2315 train_time:100935ms step_avg:60.95ms
step:1657/2315 train_time:100995ms step_avg:60.95ms
step:1658/2315 train_time:101056ms step_avg:60.95ms
step:1659/2315 train_time:101117ms step_avg:60.95ms
step:1660/2315 train_time:101178ms step_avg:60.95ms
step:1661/2315 train_time:101239ms step_avg:60.95ms
step:1662/2315 train_time:101300ms step_avg:60.95ms
step:1663/2315 train_time:101361ms step_avg:60.95ms
step:1664/2315 train_time:101422ms step_avg:60.95ms
step:1665/2315 train_time:101484ms step_avg:60.95ms
step:1666/2315 train_time:101545ms step_avg:60.95ms
step:1667/2315 train_time:101606ms step_avg:60.95ms
step:1668/2315 train_time:101667ms step_avg:60.95ms
step:1669/2315 train_time:101730ms step_avg:60.95ms
step:1670/2315 train_time:101791ms step_avg:60.95ms
step:1671/2315 train_time:101852ms step_avg:60.95ms
step:1672/2315 train_time:101913ms step_avg:60.95ms
step:1673/2315 train_time:101974ms step_avg:60.95ms
step:1674/2315 train_time:102035ms step_avg:60.95ms
step:1675/2315 train_time:102096ms step_avg:60.95ms
step:1676/2315 train_time:102157ms step_avg:60.95ms
step:1677/2315 train_time:102217ms step_avg:60.95ms
step:1678/2315 train_time:102278ms step_avg:60.95ms
step:1679/2315 train_time:102340ms step_avg:60.95ms
step:1680/2315 train_time:102401ms step_avg:60.95ms
step:1681/2315 train_time:102462ms step_avg:60.95ms
step:1682/2315 train_time:102523ms step_avg:60.95ms
step:1683/2315 train_time:102584ms step_avg:60.95ms
step:1684/2315 train_time:102645ms step_avg:60.95ms
step:1685/2315 train_time:102707ms step_avg:60.95ms
step:1686/2315 train_time:102769ms step_avg:60.95ms
step:1687/2315 train_time:102830ms step_avg:60.95ms
step:1688/2315 train_time:102891ms step_avg:60.95ms
step:1689/2315 train_time:102952ms step_avg:60.95ms
step:1690/2315 train_time:103013ms step_avg:60.95ms
step:1691/2315 train_time:103074ms step_avg:60.95ms
step:1692/2315 train_time:103136ms step_avg:60.95ms
step:1693/2315 train_time:103196ms step_avg:60.95ms
step:1694/2315 train_time:103258ms step_avg:60.96ms
step:1695/2315 train_time:103319ms step_avg:60.96ms
step:1696/2315 train_time:103380ms step_avg:60.96ms
step:1697/2315 train_time:103440ms step_avg:60.95ms
step:1698/2315 train_time:103503ms step_avg:60.96ms
step:1699/2315 train_time:103562ms step_avg:60.95ms
step:1700/2315 train_time:103623ms step_avg:60.95ms
step:1701/2315 train_time:103685ms step_avg:60.96ms
step:1702/2315 train_time:103748ms step_avg:60.96ms
step:1703/2315 train_time:103809ms step_avg:60.96ms
step:1704/2315 train_time:103870ms step_avg:60.96ms
step:1705/2315 train_time:103930ms step_avg:60.96ms
step:1706/2315 train_time:103991ms step_avg:60.96ms
step:1707/2315 train_time:104053ms step_avg:60.96ms
step:1708/2315 train_time:104114ms step_avg:60.96ms
step:1709/2315 train_time:104175ms step_avg:60.96ms
step:1710/2315 train_time:104236ms step_avg:60.96ms
step:1711/2315 train_time:104297ms step_avg:60.96ms
step:1712/2315 train_time:104358ms step_avg:60.96ms
step:1713/2315 train_time:104419ms step_avg:60.96ms
step:1714/2315 train_time:104480ms step_avg:60.96ms
step:1715/2315 train_time:104541ms step_avg:60.96ms
step:1716/2315 train_time:104602ms step_avg:60.96ms
step:1717/2315 train_time:104663ms step_avg:60.96ms
step:1718/2315 train_time:104725ms step_avg:60.96ms
step:1719/2315 train_time:104787ms step_avg:60.96ms
step:1720/2315 train_time:104848ms step_avg:60.96ms
step:1721/2315 train_time:104910ms step_avg:60.96ms
step:1722/2315 train_time:104971ms step_avg:60.96ms
step:1723/2315 train_time:105032ms step_avg:60.96ms
step:1724/2315 train_time:105093ms step_avg:60.96ms
step:1725/2315 train_time:105154ms step_avg:60.96ms
step:1726/2315 train_time:105216ms step_avg:60.96ms
step:1727/2315 train_time:105277ms step_avg:60.96ms
step:1728/2315 train_time:105337ms step_avg:60.96ms
step:1729/2315 train_time:105399ms step_avg:60.96ms
step:1730/2315 train_time:105459ms step_avg:60.96ms
step:1731/2315 train_time:105520ms step_avg:60.96ms
step:1732/2315 train_time:105581ms step_avg:60.96ms
step:1733/2315 train_time:105642ms step_avg:60.96ms
step:1734/2315 train_time:105703ms step_avg:60.96ms
step:1735/2315 train_time:105765ms step_avg:60.96ms
step:1736/2315 train_time:105827ms step_avg:60.96ms
step:1737/2315 train_time:105889ms step_avg:60.96ms
step:1738/2315 train_time:105950ms step_avg:60.96ms
step:1739/2315 train_time:106012ms step_avg:60.96ms
step:1740/2315 train_time:106073ms step_avg:60.96ms
step:1741/2315 train_time:106135ms step_avg:60.96ms
step:1742/2315 train_time:106196ms step_avg:60.96ms
step:1743/2315 train_time:106257ms step_avg:60.96ms
step:1744/2315 train_time:106318ms step_avg:60.96ms
step:1745/2315 train_time:106378ms step_avg:60.96ms
step:1746/2315 train_time:106440ms step_avg:60.96ms
step:1747/2315 train_time:106500ms step_avg:60.96ms
step:1748/2315 train_time:106561ms step_avg:60.96ms
step:1749/2315 train_time:106622ms step_avg:60.96ms
step:1750/2315 train_time:106683ms step_avg:60.96ms
step:1750/2315 val_loss:3.3803 train_time:106746ms step_avg:61.00ms
step:1751/2315 train_time:106769ms step_avg:60.98ms
step:1752/2315 train_time:106807ms step_avg:60.96ms
step:1753/2315 train_time:106867ms step_avg:60.96ms
step:1754/2315 train_time:106931ms step_avg:60.96ms
step:1755/2315 train_time:106992ms step_avg:60.96ms
step:1756/2315 train_time:107052ms step_avg:60.96ms
step:1757/2315 train_time:107112ms step_avg:60.96ms
step:1758/2315 train_time:107172ms step_avg:60.96ms
step:1759/2315 train_time:107232ms step_avg:60.96ms
step:1760/2315 train_time:107292ms step_avg:60.96ms
step:1761/2315 train_time:107352ms step_avg:60.96ms
step:1762/2315 train_time:107411ms step_avg:60.96ms
step:1763/2315 train_time:107472ms step_avg:60.96ms
step:1764/2315 train_time:107531ms step_avg:60.96ms
step:1765/2315 train_time:107592ms step_avg:60.96ms
step:1766/2315 train_time:107656ms step_avg:60.96ms
step:1767/2315 train_time:107722ms step_avg:60.96ms
step:1768/2315 train_time:107784ms step_avg:60.96ms
step:1769/2315 train_time:107845ms step_avg:60.96ms
step:1770/2315 train_time:107906ms step_avg:60.96ms
step:1771/2315 train_time:107967ms step_avg:60.96ms
step:1772/2315 train_time:108028ms step_avg:60.96ms
step:1773/2315 train_time:108089ms step_avg:60.96ms
step:1774/2315 train_time:108149ms step_avg:60.96ms
step:1775/2315 train_time:108210ms step_avg:60.96ms
step:1776/2315 train_time:108270ms step_avg:60.96ms
step:1777/2315 train_time:108331ms step_avg:60.96ms
step:1778/2315 train_time:108391ms step_avg:60.96ms
step:1779/2315 train_time:108452ms step_avg:60.96ms
step:1780/2315 train_time:108512ms step_avg:60.96ms
step:1781/2315 train_time:108573ms step_avg:60.96ms
step:1782/2315 train_time:108634ms step_avg:60.96ms
step:1783/2315 train_time:108697ms step_avg:60.96ms
step:1784/2315 train_time:108759ms step_avg:60.96ms
step:1785/2315 train_time:108820ms step_avg:60.96ms
step:1786/2315 train_time:108881ms step_avg:60.96ms
step:1787/2315 train_time:108942ms step_avg:60.96ms
step:1788/2315 train_time:109004ms step_avg:60.96ms
step:1789/2315 train_time:109065ms step_avg:60.96ms
step:1790/2315 train_time:109126ms step_avg:60.96ms
step:1791/2315 train_time:109187ms step_avg:60.96ms
step:1792/2315 train_time:109248ms step_avg:60.96ms
step:1793/2315 train_time:109309ms step_avg:60.96ms
step:1794/2315 train_time:109370ms step_avg:60.96ms
step:1795/2315 train_time:109431ms step_avg:60.96ms
step:1796/2315 train_time:109491ms step_avg:60.96ms
step:1797/2315 train_time:109552ms step_avg:60.96ms
step:1798/2315 train_time:109613ms step_avg:60.96ms
step:1799/2315 train_time:109674ms step_avg:60.96ms
step:1800/2315 train_time:109736ms step_avg:60.96ms
step:1801/2315 train_time:109797ms step_avg:60.96ms
step:1802/2315 train_time:109858ms step_avg:60.96ms
step:1803/2315 train_time:109919ms step_avg:60.96ms
step:1804/2315 train_time:109980ms step_avg:60.96ms
step:1805/2315 train_time:110041ms step_avg:60.96ms
step:1806/2315 train_time:110102ms step_avg:60.96ms
step:1807/2315 train_time:110164ms step_avg:60.96ms
step:1808/2315 train_time:110225ms step_avg:60.97ms
step:1809/2315 train_time:110286ms step_avg:60.97ms
step:1810/2315 train_time:110347ms step_avg:60.97ms
step:1811/2315 train_time:110408ms step_avg:60.97ms
step:1812/2315 train_time:110469ms step_avg:60.97ms
step:1813/2315 train_time:110530ms step_avg:60.97ms
step:1814/2315 train_time:110591ms step_avg:60.97ms
step:1815/2315 train_time:110653ms step_avg:60.97ms
step:1816/2315 train_time:110714ms step_avg:60.97ms
step:1817/2315 train_time:110774ms step_avg:60.97ms
step:1818/2315 train_time:110836ms step_avg:60.97ms
step:1819/2315 train_time:110897ms step_avg:60.97ms
step:1820/2315 train_time:110958ms step_avg:60.97ms
step:1821/2315 train_time:111020ms step_avg:60.97ms
step:1822/2315 train_time:111081ms step_avg:60.97ms
step:1823/2315 train_time:111141ms step_avg:60.97ms
step:1824/2315 train_time:111202ms step_avg:60.97ms
step:1825/2315 train_time:111263ms step_avg:60.97ms
step:1826/2315 train_time:111324ms step_avg:60.97ms
step:1827/2315 train_time:111386ms step_avg:60.97ms
step:1828/2315 train_time:111448ms step_avg:60.97ms
step:1829/2315 train_time:111509ms step_avg:60.97ms
step:1830/2315 train_time:111570ms step_avg:60.97ms
step:1831/2315 train_time:111631ms step_avg:60.97ms
step:1832/2315 train_time:111692ms step_avg:60.97ms
step:1833/2315 train_time:111753ms step_avg:60.97ms
step:1834/2315 train_time:111814ms step_avg:60.97ms
step:1835/2315 train_time:111875ms step_avg:60.97ms
step:1836/2315 train_time:111936ms step_avg:60.97ms
step:1837/2315 train_time:111997ms step_avg:60.97ms
step:1838/2315 train_time:112058ms step_avg:60.97ms
step:1839/2315 train_time:112118ms step_avg:60.97ms
step:1840/2315 train_time:112179ms step_avg:60.97ms
step:1841/2315 train_time:112240ms step_avg:60.97ms
step:1842/2315 train_time:112301ms step_avg:60.97ms
step:1843/2315 train_time:112362ms step_avg:60.97ms
step:1844/2315 train_time:112423ms step_avg:60.97ms
step:1845/2315 train_time:112485ms step_avg:60.97ms
step:1846/2315 train_time:112547ms step_avg:60.97ms
step:1847/2315 train_time:112609ms step_avg:60.97ms
step:1848/2315 train_time:112670ms step_avg:60.97ms
step:1849/2315 train_time:112731ms step_avg:60.97ms
step:1850/2315 train_time:112792ms step_avg:60.97ms
step:1851/2315 train_time:112853ms step_avg:60.97ms
step:1852/2315 train_time:112914ms step_avg:60.97ms
step:1853/2315 train_time:112974ms step_avg:60.97ms
step:1854/2315 train_time:113035ms step_avg:60.97ms
step:1855/2315 train_time:113095ms step_avg:60.97ms
step:1856/2315 train_time:113157ms step_avg:60.97ms
step:1857/2315 train_time:113218ms step_avg:60.97ms
step:1858/2315 train_time:113279ms step_avg:60.97ms
step:1859/2315 train_time:113340ms step_avg:60.97ms
step:1860/2315 train_time:113401ms step_avg:60.97ms
step:1861/2315 train_time:113462ms step_avg:60.97ms
step:1862/2315 train_time:113524ms step_avg:60.97ms
step:1863/2315 train_time:113586ms step_avg:60.97ms
step:1864/2315 train_time:113647ms step_avg:60.97ms
step:1865/2315 train_time:113709ms step_avg:60.97ms
step:1866/2315 train_time:113770ms step_avg:60.97ms
step:1867/2315 train_time:113830ms step_avg:60.97ms
step:1868/2315 train_time:113891ms step_avg:60.97ms
step:1869/2315 train_time:113952ms step_avg:60.97ms
step:1870/2315 train_time:114013ms step_avg:60.97ms
step:1871/2315 train_time:114074ms step_avg:60.97ms
step:1872/2315 train_time:114135ms step_avg:60.97ms
step:1873/2315 train_time:114196ms step_avg:60.97ms
step:1874/2315 train_time:114256ms step_avg:60.97ms
step:1875/2315 train_time:114318ms step_avg:60.97ms
step:1876/2315 train_time:114378ms step_avg:60.97ms
step:1877/2315 train_time:114441ms step_avg:60.97ms
step:1878/2315 train_time:114502ms step_avg:60.97ms
step:1879/2315 train_time:114563ms step_avg:60.97ms
step:1880/2315 train_time:114624ms step_avg:60.97ms
step:1881/2315 train_time:114687ms step_avg:60.97ms
step:1882/2315 train_time:114748ms step_avg:60.97ms
step:1883/2315 train_time:114810ms step_avg:60.97ms
step:1884/2315 train_time:114870ms step_avg:60.97ms
step:1885/2315 train_time:114931ms step_avg:60.97ms
step:1886/2315 train_time:114992ms step_avg:60.97ms
step:1887/2315 train_time:115053ms step_avg:60.97ms
step:1888/2315 train_time:115114ms step_avg:60.97ms
step:1889/2315 train_time:115174ms step_avg:60.97ms
step:1890/2315 train_time:115235ms step_avg:60.97ms
step:1891/2315 train_time:115296ms step_avg:60.97ms
step:1892/2315 train_time:115357ms step_avg:60.97ms
step:1893/2315 train_time:115418ms step_avg:60.97ms
step:1894/2315 train_time:115479ms step_avg:60.97ms
step:1895/2315 train_time:115540ms step_avg:60.97ms
step:1896/2315 train_time:115601ms step_avg:60.97ms
step:1897/2315 train_time:115662ms step_avg:60.97ms
step:1898/2315 train_time:115724ms step_avg:60.97ms
step:1899/2315 train_time:115785ms step_avg:60.97ms
step:1900/2315 train_time:115847ms step_avg:60.97ms
step:1901/2315 train_time:115908ms step_avg:60.97ms
step:1902/2315 train_time:115970ms step_avg:60.97ms
step:1903/2315 train_time:116031ms step_avg:60.97ms
step:1904/2315 train_time:116092ms step_avg:60.97ms
step:1905/2315 train_time:116152ms step_avg:60.97ms
step:1906/2315 train_time:116213ms step_avg:60.97ms
step:1907/2315 train_time:116274ms step_avg:60.97ms
step:1908/2315 train_time:116334ms step_avg:60.97ms
step:1909/2315 train_time:116395ms step_avg:60.97ms
step:1910/2315 train_time:116457ms step_avg:60.97ms
step:1911/2315 train_time:116518ms step_avg:60.97ms
step:1912/2315 train_time:116579ms step_avg:60.97ms
step:1913/2315 train_time:116641ms step_avg:60.97ms
step:1914/2315 train_time:116702ms step_avg:60.97ms
step:1915/2315 train_time:116763ms step_avg:60.97ms
step:1916/2315 train_time:116824ms step_avg:60.97ms
step:1917/2315 train_time:116886ms step_avg:60.97ms
step:1918/2315 train_time:116947ms step_avg:60.97ms
step:1919/2315 train_time:117009ms step_avg:60.97ms
step:1920/2315 train_time:117070ms step_avg:60.97ms
step:1921/2315 train_time:117131ms step_avg:60.97ms
step:1922/2315 train_time:117192ms step_avg:60.97ms
step:1923/2315 train_time:117253ms step_avg:60.97ms
step:1924/2315 train_time:117314ms step_avg:60.97ms
step:1925/2315 train_time:117374ms step_avg:60.97ms
step:1926/2315 train_time:117435ms step_avg:60.97ms
step:1927/2315 train_time:117496ms step_avg:60.97ms
step:1928/2315 train_time:117557ms step_avg:60.97ms
step:1929/2315 train_time:117620ms step_avg:60.97ms
step:1930/2315 train_time:117680ms step_avg:60.97ms
step:1931/2315 train_time:117741ms step_avg:60.97ms
step:1932/2315 train_time:117802ms step_avg:60.97ms
step:1933/2315 train_time:117863ms step_avg:60.97ms
step:1934/2315 train_time:117925ms step_avg:60.97ms
step:1935/2315 train_time:117986ms step_avg:60.97ms
step:1936/2315 train_time:118048ms step_avg:60.97ms
step:1937/2315 train_time:118109ms step_avg:60.97ms
step:1938/2315 train_time:118170ms step_avg:60.98ms
step:1939/2315 train_time:118231ms step_avg:60.98ms
step:1940/2315 train_time:118292ms step_avg:60.98ms
step:1941/2315 train_time:118353ms step_avg:60.98ms
step:1942/2315 train_time:118414ms step_avg:60.98ms
step:1943/2315 train_time:118475ms step_avg:60.98ms
step:1944/2315 train_time:118536ms step_avg:60.98ms
step:1945/2315 train_time:118597ms step_avg:60.98ms
step:1946/2315 train_time:118658ms step_avg:60.98ms
step:1947/2315 train_time:118719ms step_avg:60.98ms
step:1948/2315 train_time:118780ms step_avg:60.98ms
step:1949/2315 train_time:118842ms step_avg:60.98ms
step:1950/2315 train_time:118902ms step_avg:60.98ms
step:1951/2315 train_time:118963ms step_avg:60.98ms
step:1952/2315 train_time:119025ms step_avg:60.98ms
step:1953/2315 train_time:119088ms step_avg:60.98ms
step:1954/2315 train_time:119149ms step_avg:60.98ms
step:1955/2315 train_time:119211ms step_avg:60.98ms
step:1956/2315 train_time:119272ms step_avg:60.98ms
step:1957/2315 train_time:119332ms step_avg:60.98ms
step:1958/2315 train_time:119393ms step_avg:60.98ms
step:1959/2315 train_time:119454ms step_avg:60.98ms
step:1960/2315 train_time:119515ms step_avg:60.98ms
step:1961/2315 train_time:119577ms step_avg:60.98ms
step:1962/2315 train_time:119638ms step_avg:60.98ms
step:1963/2315 train_time:119698ms step_avg:60.98ms
step:1964/2315 train_time:119759ms step_avg:60.98ms
step:1965/2315 train_time:119820ms step_avg:60.98ms
step:1966/2315 train_time:119882ms step_avg:60.98ms
step:1967/2315 train_time:119943ms step_avg:60.98ms
step:1968/2315 train_time:120005ms step_avg:60.98ms
step:1969/2315 train_time:120067ms step_avg:60.98ms
step:1970/2315 train_time:120128ms step_avg:60.98ms
step:1971/2315 train_time:120190ms step_avg:60.98ms
step:1972/2315 train_time:120251ms step_avg:60.98ms
step:1973/2315 train_time:120312ms step_avg:60.98ms
step:1974/2315 train_time:120373ms step_avg:60.98ms
step:1975/2315 train_time:120434ms step_avg:60.98ms
step:1976/2315 train_time:120495ms step_avg:60.98ms
step:1977/2315 train_time:120556ms step_avg:60.98ms
step:1978/2315 train_time:120617ms step_avg:60.98ms
step:1979/2315 train_time:120678ms step_avg:60.98ms
step:1980/2315 train_time:120738ms step_avg:60.98ms
step:1981/2315 train_time:120799ms step_avg:60.98ms
step:1982/2315 train_time:120860ms step_avg:60.98ms
step:1983/2315 train_time:120921ms step_avg:60.98ms
step:1984/2315 train_time:120982ms step_avg:60.98ms
step:1985/2315 train_time:121043ms step_avg:60.98ms
step:1986/2315 train_time:121105ms step_avg:60.98ms
step:1987/2315 train_time:121167ms step_avg:60.98ms
step:1988/2315 train_time:121229ms step_avg:60.98ms
step:1989/2315 train_time:121290ms step_avg:60.98ms
step:1990/2315 train_time:121351ms step_avg:60.98ms
step:1991/2315 train_time:121412ms step_avg:60.98ms
step:1992/2315 train_time:121473ms step_avg:60.98ms
step:1993/2315 train_time:121534ms step_avg:60.98ms
step:1994/2315 train_time:121595ms step_avg:60.98ms
step:1995/2315 train_time:121656ms step_avg:60.98ms
step:1996/2315 train_time:121717ms step_avg:60.98ms
step:1997/2315 train_time:121778ms step_avg:60.98ms
step:1998/2315 train_time:121839ms step_avg:60.98ms
step:1999/2315 train_time:121900ms step_avg:60.98ms
step:2000/2315 train_time:121960ms step_avg:60.98ms
step:2000/2315 val_loss:3.3310 train_time:122023ms step_avg:61.01ms
step:2001/2315 train_time:122047ms step_avg:60.99ms
step:2002/2315 train_time:122086ms step_avg:60.98ms
step:2003/2315 train_time:122151ms step_avg:60.98ms
step:2004/2315 train_time:122214ms step_avg:60.98ms
step:2005/2315 train_time:122274ms step_avg:60.98ms
step:2006/2315 train_time:122336ms step_avg:60.99ms
step:2007/2315 train_time:122397ms step_avg:60.99ms
step:2008/2315 train_time:122458ms step_avg:60.98ms
step:2009/2315 train_time:122519ms step_avg:60.98ms
step:2010/2315 train_time:122579ms step_avg:60.98ms
step:2011/2315 train_time:122639ms step_avg:60.98ms
step:2012/2315 train_time:122700ms step_avg:60.98ms
step:2013/2315 train_time:122760ms step_avg:60.98ms
step:2014/2315 train_time:122820ms step_avg:60.98ms
step:2015/2315 train_time:122880ms step_avg:60.98ms
step:2016/2315 train_time:122942ms step_avg:60.98ms
step:2017/2315 train_time:123003ms step_avg:60.98ms
step:2018/2315 train_time:123066ms step_avg:60.98ms
step:2019/2315 train_time:123128ms step_avg:60.98ms
step:2020/2315 train_time:123190ms step_avg:60.99ms
step:2021/2315 train_time:123252ms step_avg:60.99ms
step:2022/2315 train_time:123314ms step_avg:60.99ms
step:2023/2315 train_time:123375ms step_avg:60.99ms
step:2024/2315 train_time:123436ms step_avg:60.99ms
step:2025/2315 train_time:123497ms step_avg:60.99ms
step:2026/2315 train_time:123557ms step_avg:60.99ms
step:2027/2315 train_time:123618ms step_avg:60.99ms
step:2028/2315 train_time:123679ms step_avg:60.99ms
step:2029/2315 train_time:123739ms step_avg:60.99ms
step:2030/2315 train_time:123799ms step_avg:60.98ms
step:2031/2315 train_time:123860ms step_avg:60.98ms
step:2032/2315 train_time:123920ms step_avg:60.98ms
step:2033/2315 train_time:123981ms step_avg:60.98ms
step:2034/2315 train_time:124044ms step_avg:60.99ms
step:2035/2315 train_time:124106ms step_avg:60.99ms
step:2036/2315 train_time:124167ms step_avg:60.99ms
step:2037/2315 train_time:124229ms step_avg:60.99ms
step:2038/2315 train_time:124290ms step_avg:60.99ms
step:2039/2315 train_time:124352ms step_avg:60.99ms
step:2040/2315 train_time:124413ms step_avg:60.99ms
step:2041/2315 train_time:124474ms step_avg:60.99ms
step:2042/2315 train_time:124535ms step_avg:60.99ms
step:2043/2315 train_time:124597ms step_avg:60.99ms
step:2044/2315 train_time:124657ms step_avg:60.99ms
step:2045/2315 train_time:124718ms step_avg:60.99ms
step:2046/2315 train_time:124779ms step_avg:60.99ms
step:2047/2315 train_time:124839ms step_avg:60.99ms
step:2048/2315 train_time:124900ms step_avg:60.99ms
step:2049/2315 train_time:124960ms step_avg:60.99ms
step:2050/2315 train_time:125022ms step_avg:60.99ms
step:2051/2315 train_time:125083ms step_avg:60.99ms
step:2052/2315 train_time:125145ms step_avg:60.99ms
step:2053/2315 train_time:125205ms step_avg:60.99ms
step:2054/2315 train_time:125267ms step_avg:60.99ms
step:2055/2315 train_time:125328ms step_avg:60.99ms
step:2056/2315 train_time:125390ms step_avg:60.99ms
step:2057/2315 train_time:125451ms step_avg:60.99ms
step:2058/2315 train_time:125512ms step_avg:60.99ms
step:2059/2315 train_time:125574ms step_avg:60.99ms
step:2060/2315 train_time:125635ms step_avg:60.99ms
step:2061/2315 train_time:125696ms step_avg:60.99ms
step:2062/2315 train_time:125757ms step_avg:60.99ms
step:2063/2315 train_time:125818ms step_avg:60.99ms
step:2064/2315 train_time:125879ms step_avg:60.99ms
step:2065/2315 train_time:125940ms step_avg:60.99ms
step:2066/2315 train_time:126001ms step_avg:60.99ms
step:2067/2315 train_time:126062ms step_avg:60.99ms
step:2068/2315 train_time:126123ms step_avg:60.99ms
step:2069/2315 train_time:126184ms step_avg:60.99ms
step:2070/2315 train_time:126246ms step_avg:60.99ms
step:2071/2315 train_time:126307ms step_avg:60.99ms
step:2072/2315 train_time:126368ms step_avg:60.99ms
step:2073/2315 train_time:126429ms step_avg:60.99ms
step:2074/2315 train_time:126490ms step_avg:60.99ms
step:2075/2315 train_time:126552ms step_avg:60.99ms
step:2076/2315 train_time:126613ms step_avg:60.99ms
step:2077/2315 train_time:126674ms step_avg:60.99ms
step:2078/2315 train_time:126735ms step_avg:60.99ms
step:2079/2315 train_time:126797ms step_avg:60.99ms
step:2080/2315 train_time:126858ms step_avg:60.99ms
step:2081/2315 train_time:126919ms step_avg:60.99ms
step:2082/2315 train_time:126980ms step_avg:60.99ms
step:2083/2315 train_time:127041ms step_avg:60.99ms
step:2084/2315 train_time:127102ms step_avg:60.99ms
step:2085/2315 train_time:127163ms step_avg:60.99ms
step:2086/2315 train_time:127224ms step_avg:60.99ms
step:2087/2315 train_time:127286ms step_avg:60.99ms
step:2088/2315 train_time:127347ms step_avg:60.99ms
step:2089/2315 train_time:127408ms step_avg:60.99ms
step:2090/2315 train_time:127469ms step_avg:60.99ms
step:2091/2315 train_time:127530ms step_avg:60.99ms
step:2092/2315 train_time:127591ms step_avg:60.99ms
step:2093/2315 train_time:127653ms step_avg:60.99ms
step:2094/2315 train_time:127715ms step_avg:60.99ms
step:2095/2315 train_time:127776ms step_avg:60.99ms
step:2096/2315 train_time:127837ms step_avg:60.99ms
step:2097/2315 train_time:127898ms step_avg:60.99ms
step:2098/2315 train_time:127959ms step_avg:60.99ms
step:2099/2315 train_time:128020ms step_avg:60.99ms
step:2100/2315 train_time:128081ms step_avg:60.99ms
step:2101/2315 train_time:128143ms step_avg:60.99ms
step:2102/2315 train_time:128204ms step_avg:60.99ms
step:2103/2315 train_time:128265ms step_avg:60.99ms
step:2104/2315 train_time:128326ms step_avg:60.99ms
step:2105/2315 train_time:128387ms step_avg:60.99ms
step:2106/2315 train_time:128448ms step_avg:60.99ms
step:2107/2315 train_time:128509ms step_avg:60.99ms
step:2108/2315 train_time:128570ms step_avg:60.99ms
step:2109/2315 train_time:128632ms step_avg:60.99ms
step:2110/2315 train_time:128693ms step_avg:60.99ms
step:2111/2315 train_time:128754ms step_avg:60.99ms
step:2112/2315 train_time:128815ms step_avg:60.99ms
step:2113/2315 train_time:128876ms step_avg:60.99ms
step:2114/2315 train_time:128937ms step_avg:60.99ms
step:2115/2315 train_time:128999ms step_avg:60.99ms
step:2116/2315 train_time:129060ms step_avg:60.99ms
step:2117/2315 train_time:129121ms step_avg:60.99ms
step:2118/2315 train_time:129182ms step_avg:60.99ms
step:2119/2315 train_time:129242ms step_avg:60.99ms
step:2120/2315 train_time:129303ms step_avg:60.99ms
step:2121/2315 train_time:129364ms step_avg:60.99ms
step:2122/2315 train_time:129426ms step_avg:60.99ms
step:2123/2315 train_time:129487ms step_avg:60.99ms
step:2124/2315 train_time:129548ms step_avg:60.99ms
step:2125/2315 train_time:129609ms step_avg:60.99ms
step:2126/2315 train_time:129670ms step_avg:60.99ms
step:2127/2315 train_time:129732ms step_avg:60.99ms
step:2128/2315 train_time:129793ms step_avg:60.99ms
step:2129/2315 train_time:129855ms step_avg:60.99ms
step:2130/2315 train_time:129916ms step_avg:60.99ms
step:2131/2315 train_time:129977ms step_avg:60.99ms
step:2132/2315 train_time:130039ms step_avg:60.99ms
step:2133/2315 train_time:130100ms step_avg:60.99ms
step:2134/2315 train_time:130161ms step_avg:60.99ms
step:2135/2315 train_time:130221ms step_avg:60.99ms
step:2136/2315 train_time:130283ms step_avg:60.99ms
step:2137/2315 train_time:130343ms step_avg:60.99ms
step:2138/2315 train_time:130404ms step_avg:60.99ms
step:2139/2315 train_time:130465ms step_avg:60.99ms
step:2140/2315 train_time:130526ms step_avg:60.99ms
step:2141/2315 train_time:130587ms step_avg:60.99ms
step:2142/2315 train_time:130648ms step_avg:60.99ms
step:2143/2315 train_time:130709ms step_avg:60.99ms
step:2144/2315 train_time:130770ms step_avg:60.99ms
step:2145/2315 train_time:130832ms step_avg:60.99ms
step:2146/2315 train_time:130893ms step_avg:60.99ms
step:2147/2315 train_time:130955ms step_avg:60.99ms
step:2148/2315 train_time:131016ms step_avg:60.99ms
step:2149/2315 train_time:131078ms step_avg:60.99ms
step:2150/2315 train_time:131139ms step_avg:60.99ms
step:2151/2315 train_time:131200ms step_avg:61.00ms
step:2152/2315 train_time:131262ms step_avg:61.00ms
step:2153/2315 train_time:131323ms step_avg:61.00ms
step:2154/2315 train_time:131384ms step_avg:61.00ms
step:2155/2315 train_time:131444ms step_avg:61.00ms
step:2156/2315 train_time:131505ms step_avg:60.99ms
step:2157/2315 train_time:131567ms step_avg:61.00ms
step:2158/2315 train_time:131628ms step_avg:61.00ms
step:2159/2315 train_time:131688ms step_avg:61.00ms
step:2160/2315 train_time:131749ms step_avg:60.99ms
step:2161/2315 train_time:131811ms step_avg:61.00ms
step:2162/2315 train_time:131873ms step_avg:61.00ms
step:2163/2315 train_time:131935ms step_avg:61.00ms
step:2164/2315 train_time:131997ms step_avg:61.00ms
step:2165/2315 train_time:132059ms step_avg:61.00ms
step:2166/2315 train_time:132119ms step_avg:61.00ms
step:2167/2315 train_time:132181ms step_avg:61.00ms
step:2168/2315 train_time:132242ms step_avg:61.00ms
step:2169/2315 train_time:132303ms step_avg:61.00ms
step:2170/2315 train_time:132364ms step_avg:61.00ms
step:2171/2315 train_time:132425ms step_avg:61.00ms
step:2172/2315 train_time:132485ms step_avg:61.00ms
step:2173/2315 train_time:132546ms step_avg:61.00ms
step:2174/2315 train_time:132607ms step_avg:61.00ms
step:2175/2315 train_time:132668ms step_avg:61.00ms
step:2176/2315 train_time:132730ms step_avg:61.00ms
step:2177/2315 train_time:132791ms step_avg:61.00ms
step:2178/2315 train_time:132853ms step_avg:61.00ms
step:2179/2315 train_time:132915ms step_avg:61.00ms
step:2180/2315 train_time:132976ms step_avg:61.00ms
step:2181/2315 train_time:133037ms step_avg:61.00ms
step:2182/2315 train_time:133099ms step_avg:61.00ms
step:2183/2315 train_time:133160ms step_avg:61.00ms
step:2184/2315 train_time:133221ms step_avg:61.00ms
step:2185/2315 train_time:133282ms step_avg:61.00ms
step:2186/2315 train_time:133343ms step_avg:61.00ms
step:2187/2315 train_time:133403ms step_avg:61.00ms
step:2188/2315 train_time:133464ms step_avg:61.00ms
step:2189/2315 train_time:133525ms step_avg:61.00ms
step:2190/2315 train_time:133586ms step_avg:61.00ms
step:2191/2315 train_time:133647ms step_avg:61.00ms
step:2192/2315 train_time:133708ms step_avg:61.00ms
step:2193/2315 train_time:133769ms step_avg:61.00ms
step:2194/2315 train_time:133831ms step_avg:61.00ms
step:2195/2315 train_time:133892ms step_avg:61.00ms
step:2196/2315 train_time:133954ms step_avg:61.00ms
step:2197/2315 train_time:134016ms step_avg:61.00ms
step:2198/2315 train_time:134077ms step_avg:61.00ms
step:2199/2315 train_time:134138ms step_avg:61.00ms
step:2200/2315 train_time:134199ms step_avg:61.00ms
step:2201/2315 train_time:134260ms step_avg:61.00ms
step:2202/2315 train_time:134321ms step_avg:61.00ms
step:2203/2315 train_time:134382ms step_avg:61.00ms
step:2204/2315 train_time:134443ms step_avg:61.00ms
step:2205/2315 train_time:134504ms step_avg:61.00ms
step:2206/2315 train_time:134565ms step_avg:61.00ms
step:2207/2315 train_time:134625ms step_avg:61.00ms
step:2208/2315 train_time:134687ms step_avg:61.00ms
step:2209/2315 train_time:134747ms step_avg:61.00ms
step:2210/2315 train_time:134809ms step_avg:61.00ms
step:2211/2315 train_time:134870ms step_avg:61.00ms
step:2212/2315 train_time:134932ms step_avg:61.00ms
step:2213/2315 train_time:134994ms step_avg:61.00ms
step:2214/2315 train_time:135055ms step_avg:61.00ms
step:2215/2315 train_time:135116ms step_avg:61.00ms
step:2216/2315 train_time:135178ms step_avg:61.00ms
step:2217/2315 train_time:135239ms step_avg:61.00ms
step:2218/2315 train_time:135300ms step_avg:61.00ms
step:2219/2315 train_time:135361ms step_avg:61.00ms
step:2220/2315 train_time:135422ms step_avg:61.00ms
step:2221/2315 train_time:135482ms step_avg:61.00ms
step:2222/2315 train_time:135543ms step_avg:61.00ms
step:2223/2315 train_time:135604ms step_avg:61.00ms
step:2224/2315 train_time:135665ms step_avg:61.00ms
step:2225/2315 train_time:135726ms step_avg:61.00ms
step:2226/2315 train_time:135787ms step_avg:61.00ms
step:2227/2315 train_time:135848ms step_avg:61.00ms
step:2228/2315 train_time:135909ms step_avg:61.00ms
step:2229/2315 train_time:135971ms step_avg:61.00ms
step:2230/2315 train_time:136033ms step_avg:61.00ms
step:2231/2315 train_time:136095ms step_avg:61.00ms
step:2232/2315 train_time:136156ms step_avg:61.00ms
step:2233/2315 train_time:136217ms step_avg:61.00ms
step:2234/2315 train_time:136278ms step_avg:61.00ms
step:2235/2315 train_time:136340ms step_avg:61.00ms
step:2236/2315 train_time:136401ms step_avg:61.00ms
step:2237/2315 train_time:136461ms step_avg:61.00ms
step:2238/2315 train_time:136522ms step_avg:61.00ms
step:2239/2315 train_time:136583ms step_avg:61.00ms
step:2240/2315 train_time:136645ms step_avg:61.00ms
step:2241/2315 train_time:136705ms step_avg:61.00ms
step:2242/2315 train_time:136766ms step_avg:61.00ms
step:2243/2315 train_time:136828ms step_avg:61.00ms
step:2244/2315 train_time:136889ms step_avg:61.00ms
step:2245/2315 train_time:136950ms step_avg:61.00ms
step:2246/2315 train_time:137012ms step_avg:61.00ms
step:2247/2315 train_time:137074ms step_avg:61.00ms
step:2248/2315 train_time:137135ms step_avg:61.00ms
step:2249/2315 train_time:137196ms step_avg:61.00ms
step:2250/2315 train_time:137257ms step_avg:61.00ms
step:2250/2315 val_loss:3.2913 train_time:137321ms step_avg:61.03ms
step:2251/2315 train_time:137342ms step_avg:61.01ms
step:2252/2315 train_time:137382ms step_avg:61.00ms
step:2253/2315 train_time:137448ms step_avg:61.01ms
step:2254/2315 train_time:137513ms step_avg:61.01ms
step:2255/2315 train_time:137574ms step_avg:61.01ms
step:2256/2315 train_time:137636ms step_avg:61.01ms
step:2257/2315 train_time:137698ms step_avg:61.01ms
step:2258/2315 train_time:137759ms step_avg:61.01ms
step:2259/2315 train_time:137820ms step_avg:61.01ms
step:2260/2315 train_time:137880ms step_avg:61.01ms
step:2261/2315 train_time:137941ms step_avg:61.01ms
step:2262/2315 train_time:138002ms step_avg:61.01ms
step:2263/2315 train_time:138063ms step_avg:61.01ms
step:2264/2315 train_time:138125ms step_avg:61.01ms
step:2265/2315 train_time:138185ms step_avg:61.01ms
step:2266/2315 train_time:138245ms step_avg:61.01ms
step:2267/2315 train_time:138307ms step_avg:61.01ms
step:2268/2315 train_time:138369ms step_avg:61.01ms
step:2269/2315 train_time:138431ms step_avg:61.01ms
step:2270/2315 train_time:138493ms step_avg:61.01ms
step:2271/2315 train_time:138555ms step_avg:61.01ms
step:2272/2315 train_time:138617ms step_avg:61.01ms
step:2273/2315 train_time:138678ms step_avg:61.01ms
step:2274/2315 train_time:138739ms step_avg:61.01ms
step:2275/2315 train_time:138800ms step_avg:61.01ms
step:2276/2315 train_time:138861ms step_avg:61.01ms
step:2277/2315 train_time:138922ms step_avg:61.01ms
step:2278/2315 train_time:138983ms step_avg:61.01ms
step:2279/2315 train_time:139043ms step_avg:61.01ms
step:2280/2315 train_time:139104ms step_avg:61.01ms
step:2281/2315 train_time:139165ms step_avg:61.01ms
step:2282/2315 train_time:139226ms step_avg:61.01ms
step:2283/2315 train_time:139287ms step_avg:61.01ms
step:2284/2315 train_time:139349ms step_avg:61.01ms
step:2285/2315 train_time:139410ms step_avg:61.01ms
step:2286/2315 train_time:139471ms step_avg:61.01ms
step:2287/2315 train_time:139532ms step_avg:61.01ms
step:2288/2315 train_time:139594ms step_avg:61.01ms
step:2289/2315 train_time:139655ms step_avg:61.01ms
step:2290/2315 train_time:139716ms step_avg:61.01ms
step:2291/2315 train_time:139777ms step_avg:61.01ms
step:2292/2315 train_time:139838ms step_avg:61.01ms
step:2293/2315 train_time:139899ms step_avg:61.01ms
step:2294/2315 train_time:139960ms step_avg:61.01ms
step:2295/2315 train_time:140021ms step_avg:61.01ms
step:2296/2315 train_time:140082ms step_avg:61.01ms
step:2297/2315 train_time:140143ms step_avg:61.01ms
step:2298/2315 train_time:140204ms step_avg:61.01ms
step:2299/2315 train_time:140265ms step_avg:61.01ms
step:2300/2315 train_time:140326ms step_avg:61.01ms
step:2301/2315 train_time:140388ms step_avg:61.01ms
step:2302/2315 train_time:140450ms step_avg:61.01ms
step:2303/2315 train_time:140511ms step_avg:61.01ms
step:2304/2315 train_time:140572ms step_avg:61.01ms
step:2305/2315 train_time:140632ms step_avg:61.01ms
step:2306/2315 train_time:140693ms step_avg:61.01ms
step:2307/2315 train_time:140755ms step_avg:61.01ms
step:2308/2315 train_time:140816ms step_avg:61.01ms
step:2309/2315 train_time:140877ms step_avg:61.01ms
step:2310/2315 train_time:140938ms step_avg:61.01ms
step:2311/2315 train_time:140999ms step_avg:61.01ms
step:2312/2315 train_time:141060ms step_avg:61.01ms
step:2313/2315 train_time:141122ms step_avg:61.01ms
step:2314/2315 train_time:141183ms step_avg:61.01ms
step:2315/2315 train_time:141244ms step_avg:61.01ms
step:2315/2315 val_loss:3.2785 train_time:141306ms step_avg:61.04ms
peak memory allocated: 29512 MiB reserved: 44076 MiB

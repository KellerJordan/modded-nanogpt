import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]
# polar_express_coeffs = [
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
# ]



@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    # state["second_momentum_buffer"] = torch.zeros_like(grad[...,0:1]) if param.size(-2) >= param.size(-1) else torch.zeros_like(grad[...,0:1,:])
                    # state["second_momentum_buffer"] = torch.zeros((*grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((*grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)
            
            ###################################
            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))
            ####################################


            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> (-1.5)  0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Oct 22 2025, 02:20:45) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251022+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 24 06:51:37 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000001:00:00.0 Off |                    0 |
| N/A   35C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000002:00:00.0 Off |                    0 |
| N/A   35C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000003:00:00.0 Off |                    0 |
| N/A   30C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000008:00:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000009:00:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   0000000A:00:00.0 Off |                    0 |
| N/A   35C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   0000000C:00:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2315 train_time:81ms step_avg:81.33ms
step:2/2315 train_time:189ms step_avg:94.48ms
step:3/2315 train_time:209ms step_avg:69.74ms
step:4/2315 train_time:246ms step_avg:61.53ms
step:5/2315 train_time:304ms step_avg:60.88ms
step:6/2315 train_time:364ms step_avg:60.66ms
step:7/2315 train_time:424ms step_avg:60.56ms
step:8/2315 train_time:483ms step_avg:60.43ms
step:9/2315 train_time:543ms step_avg:60.37ms
step:10/2315 train_time:603ms step_avg:60.32ms
step:11/2315 train_time:663ms step_avg:60.31ms
step:12/2315 train_time:723ms step_avg:60.24ms
step:13/2315 train_time:783ms step_avg:60.19ms
step:14/2315 train_time:842ms step_avg:60.14ms
step:15/2315 train_time:902ms step_avg:60.12ms
step:16/2315 train_time:962ms step_avg:60.10ms
step:17/2315 train_time:1023ms step_avg:60.20ms
step:18/2315 train_time:1087ms step_avg:60.36ms
step:19/2315 train_time:1151ms step_avg:60.60ms
step:20/2315 train_time:1213ms step_avg:60.65ms
step:21/2315 train_time:1274ms step_avg:60.67ms
step:22/2315 train_time:1335ms step_avg:60.67ms
step:23/2315 train_time:1396ms step_avg:60.68ms
step:24/2315 train_time:1456ms step_avg:60.66ms
step:25/2315 train_time:1516ms step_avg:60.63ms
step:26/2315 train_time:1576ms step_avg:60.60ms
step:27/2315 train_time:1636ms step_avg:60.58ms
step:28/2315 train_time:1696ms step_avg:60.57ms
step:29/2315 train_time:1757ms step_avg:60.58ms
step:30/2315 train_time:1816ms step_avg:60.55ms
step:31/2315 train_time:1877ms step_avg:60.54ms
step:32/2315 train_time:1936ms step_avg:60.51ms
step:33/2315 train_time:1997ms step_avg:60.52ms
step:34/2315 train_time:2058ms step_avg:60.53ms
step:35/2315 train_time:2121ms step_avg:60.59ms
step:36/2315 train_time:2182ms step_avg:60.62ms
step:37/2315 train_time:2244ms step_avg:60.64ms
step:38/2315 train_time:2304ms step_avg:60.64ms
step:39/2315 train_time:2364ms step_avg:60.62ms
step:40/2315 train_time:2425ms step_avg:60.62ms
step:41/2315 train_time:2485ms step_avg:60.61ms
step:42/2315 train_time:2545ms step_avg:60.60ms
step:43/2315 train_time:2606ms step_avg:60.61ms
step:44/2315 train_time:2666ms step_avg:60.60ms
step:45/2315 train_time:2727ms step_avg:60.60ms
step:46/2315 train_time:2787ms step_avg:60.59ms
step:47/2315 train_time:2847ms step_avg:60.58ms
step:48/2315 train_time:2908ms step_avg:60.57ms
step:49/2315 train_time:2968ms step_avg:60.57ms
step:50/2315 train_time:3028ms step_avg:60.57ms
step:51/2315 train_time:3088ms step_avg:60.56ms
step:52/2315 train_time:3150ms step_avg:60.57ms
step:53/2315 train_time:3211ms step_avg:60.58ms
step:54/2315 train_time:3271ms step_avg:60.57ms
step:55/2315 train_time:3331ms step_avg:60.57ms
step:56/2315 train_time:3391ms step_avg:60.56ms
step:57/2315 train_time:3451ms step_avg:60.54ms
step:58/2315 train_time:3511ms step_avg:60.53ms
step:59/2315 train_time:3571ms step_avg:60.53ms
step:60/2315 train_time:3632ms step_avg:60.53ms
step:61/2315 train_time:3692ms step_avg:60.53ms
step:62/2315 train_time:3752ms step_avg:60.52ms
step:63/2315 train_time:3813ms step_avg:60.52ms
step:64/2315 train_time:3872ms step_avg:60.50ms
step:65/2315 train_time:3933ms step_avg:60.50ms
step:66/2315 train_time:3993ms step_avg:60.50ms
step:67/2315 train_time:4054ms step_avg:60.51ms
step:68/2315 train_time:4114ms step_avg:60.50ms
step:69/2315 train_time:4174ms step_avg:60.49ms
step:70/2315 train_time:4233ms step_avg:60.48ms
step:71/2315 train_time:4294ms step_avg:60.47ms
step:72/2315 train_time:4353ms step_avg:60.46ms
step:73/2315 train_time:4413ms step_avg:60.45ms
step:74/2315 train_time:4473ms step_avg:60.44ms
step:75/2315 train_time:4533ms step_avg:60.44ms
step:76/2315 train_time:4593ms step_avg:60.43ms
step:77/2315 train_time:4653ms step_avg:60.43ms
step:78/2315 train_time:4713ms step_avg:60.42ms
step:79/2315 train_time:4773ms step_avg:60.42ms
step:80/2315 train_time:4833ms step_avg:60.42ms
step:81/2315 train_time:4893ms step_avg:60.41ms
step:82/2315 train_time:4953ms step_avg:60.40ms
step:83/2315 train_time:5013ms step_avg:60.40ms
step:84/2315 train_time:5073ms step_avg:60.39ms
step:85/2315 train_time:5133ms step_avg:60.38ms
step:86/2315 train_time:5192ms step_avg:60.37ms
step:87/2315 train_time:5253ms step_avg:60.38ms
step:88/2315 train_time:5312ms step_avg:60.37ms
step:89/2315 train_time:5372ms step_avg:60.36ms
step:90/2315 train_time:5432ms step_avg:60.35ms
step:91/2315 train_time:5492ms step_avg:60.35ms
step:92/2315 train_time:5552ms step_avg:60.34ms
step:93/2315 train_time:5612ms step_avg:60.34ms
step:94/2315 train_time:5672ms step_avg:60.34ms
step:95/2315 train_time:5732ms step_avg:60.33ms
step:96/2315 train_time:5791ms step_avg:60.33ms
step:97/2315 train_time:5851ms step_avg:60.32ms
step:98/2315 train_time:5911ms step_avg:60.31ms
step:99/2315 train_time:5971ms step_avg:60.31ms
step:100/2315 train_time:6031ms step_avg:60.31ms
step:101/2315 train_time:6091ms step_avg:60.31ms
step:102/2315 train_time:6151ms step_avg:60.31ms
step:103/2315 train_time:6212ms step_avg:60.31ms
step:104/2315 train_time:6271ms step_avg:60.30ms
step:105/2315 train_time:6331ms step_avg:60.30ms
step:106/2315 train_time:6391ms step_avg:60.29ms
step:107/2315 train_time:6451ms step_avg:60.29ms
step:108/2315 train_time:6511ms step_avg:60.29ms
step:109/2315 train_time:6571ms step_avg:60.28ms
step:110/2315 train_time:6631ms step_avg:60.28ms
step:111/2315 train_time:6691ms step_avg:60.28ms
step:112/2315 train_time:6751ms step_avg:60.27ms
step:113/2315 train_time:6811ms step_avg:60.27ms
step:114/2315 train_time:6870ms step_avg:60.27ms
step:115/2315 train_time:6931ms step_avg:60.27ms
step:116/2315 train_time:6991ms step_avg:60.26ms
step:117/2315 train_time:7051ms step_avg:60.26ms
step:118/2315 train_time:7110ms step_avg:60.26ms
step:119/2315 train_time:7171ms step_avg:60.26ms
step:120/2315 train_time:7231ms step_avg:60.26ms
step:121/2315 train_time:7292ms step_avg:60.26ms
step:122/2315 train_time:7352ms step_avg:60.26ms
step:123/2315 train_time:7412ms step_avg:60.26ms
step:124/2315 train_time:7471ms step_avg:60.25ms
step:125/2315 train_time:7531ms step_avg:60.25ms
step:126/2315 train_time:7591ms step_avg:60.24ms
step:127/2315 train_time:7651ms step_avg:60.25ms
step:128/2315 train_time:7711ms step_avg:60.24ms
step:129/2315 train_time:7770ms step_avg:60.24ms
step:130/2315 train_time:7830ms step_avg:60.23ms
step:131/2315 train_time:7890ms step_avg:60.23ms
step:132/2315 train_time:7949ms step_avg:60.22ms
step:133/2315 train_time:8010ms step_avg:60.22ms
step:134/2315 train_time:8069ms step_avg:60.22ms
step:135/2315 train_time:8129ms step_avg:60.22ms
step:136/2315 train_time:8189ms step_avg:60.21ms
step:137/2315 train_time:8250ms step_avg:60.22ms
step:138/2315 train_time:8309ms step_avg:60.21ms
step:139/2315 train_time:8370ms step_avg:60.21ms
step:140/2315 train_time:8429ms step_avg:60.21ms
step:141/2315 train_time:8490ms step_avg:60.21ms
step:142/2315 train_time:8550ms step_avg:60.21ms
step:143/2315 train_time:8610ms step_avg:60.21ms
step:144/2315 train_time:8670ms step_avg:60.21ms
step:145/2315 train_time:8731ms step_avg:60.21ms
step:146/2315 train_time:8791ms step_avg:60.21ms
step:147/2315 train_time:8851ms step_avg:60.21ms
step:148/2315 train_time:8910ms step_avg:60.20ms
step:149/2315 train_time:8970ms step_avg:60.20ms
step:150/2315 train_time:9030ms step_avg:60.20ms
step:151/2315 train_time:9090ms step_avg:60.20ms
step:152/2315 train_time:9150ms step_avg:60.20ms
step:153/2315 train_time:9210ms step_avg:60.20ms
step:154/2315 train_time:9270ms step_avg:60.19ms
step:155/2315 train_time:9329ms step_avg:60.19ms
step:156/2315 train_time:9389ms step_avg:60.19ms
step:157/2315 train_time:9449ms step_avg:60.19ms
step:158/2315 train_time:9510ms step_avg:60.19ms
step:159/2315 train_time:9570ms step_avg:60.19ms
step:160/2315 train_time:9630ms step_avg:60.19ms
step:161/2315 train_time:9690ms step_avg:60.19ms
step:162/2315 train_time:9750ms step_avg:60.19ms
step:163/2315 train_time:9811ms step_avg:60.19ms
step:164/2315 train_time:9870ms step_avg:60.18ms
step:165/2315 train_time:9930ms step_avg:60.18ms
step:166/2315 train_time:9990ms step_avg:60.18ms
step:167/2315 train_time:10050ms step_avg:60.18ms
step:168/2315 train_time:10110ms step_avg:60.18ms
step:169/2315 train_time:10171ms step_avg:60.18ms
step:170/2315 train_time:10231ms step_avg:60.18ms
step:171/2315 train_time:10291ms step_avg:60.18ms
step:172/2315 train_time:10351ms step_avg:60.18ms
step:173/2315 train_time:10411ms step_avg:60.18ms
step:174/2315 train_time:10472ms step_avg:60.18ms
step:175/2315 train_time:10531ms step_avg:60.18ms
step:176/2315 train_time:10591ms step_avg:60.18ms
step:177/2315 train_time:10651ms step_avg:60.18ms
step:178/2315 train_time:10711ms step_avg:60.17ms
step:179/2315 train_time:10771ms step_avg:60.18ms
step:180/2315 train_time:10831ms step_avg:60.17ms
step:181/2315 train_time:10891ms step_avg:60.17ms
step:182/2315 train_time:10951ms step_avg:60.17ms
step:183/2315 train_time:11011ms step_avg:60.17ms
step:184/2315 train_time:11070ms step_avg:60.17ms
step:185/2315 train_time:11130ms step_avg:60.16ms
step:186/2315 train_time:11190ms step_avg:60.16ms
step:187/2315 train_time:11250ms step_avg:60.16ms
step:188/2315 train_time:11310ms step_avg:60.16ms
step:189/2315 train_time:11370ms step_avg:60.16ms
step:190/2315 train_time:11430ms step_avg:60.16ms
step:191/2315 train_time:11490ms step_avg:60.16ms
step:192/2315 train_time:11550ms step_avg:60.16ms
step:193/2315 train_time:11610ms step_avg:60.15ms
step:194/2315 train_time:11669ms step_avg:60.15ms
step:195/2315 train_time:11729ms step_avg:60.15ms
step:196/2315 train_time:11789ms step_avg:60.15ms
step:197/2315 train_time:11849ms step_avg:60.15ms
step:198/2315 train_time:11909ms step_avg:60.15ms
step:199/2315 train_time:11970ms step_avg:60.15ms
step:200/2315 train_time:12030ms step_avg:60.15ms
step:201/2315 train_time:12090ms step_avg:60.15ms
step:202/2315 train_time:12150ms step_avg:60.15ms
step:203/2315 train_time:12210ms step_avg:60.15ms
step:204/2315 train_time:12270ms step_avg:60.15ms
step:205/2315 train_time:12330ms step_avg:60.15ms
step:206/2315 train_time:12390ms step_avg:60.15ms
step:207/2315 train_time:12450ms step_avg:60.14ms
step:208/2315 train_time:12509ms step_avg:60.14ms
step:209/2315 train_time:12569ms step_avg:60.14ms
step:210/2315 train_time:12629ms step_avg:60.14ms
step:211/2315 train_time:12689ms step_avg:60.14ms
step:212/2315 train_time:12750ms step_avg:60.14ms
step:213/2315 train_time:12810ms step_avg:60.14ms
step:214/2315 train_time:12869ms step_avg:60.14ms
step:215/2315 train_time:12930ms step_avg:60.14ms
step:216/2315 train_time:12990ms step_avg:60.14ms
step:217/2315 train_time:13050ms step_avg:60.14ms
step:218/2315 train_time:13109ms step_avg:60.13ms
step:219/2315 train_time:13169ms step_avg:60.13ms
step:220/2315 train_time:13229ms step_avg:60.13ms
step:221/2315 train_time:13289ms step_avg:60.13ms
step:222/2315 train_time:13349ms step_avg:60.13ms
step:223/2315 train_time:13408ms step_avg:60.13ms
step:224/2315 train_time:13468ms step_avg:60.13ms
step:225/2315 train_time:13529ms step_avg:60.13ms
step:226/2315 train_time:13588ms step_avg:60.12ms
step:227/2315 train_time:13648ms step_avg:60.12ms
step:228/2315 train_time:13708ms step_avg:60.12ms
step:229/2315 train_time:13768ms step_avg:60.12ms
step:230/2315 train_time:13828ms step_avg:60.12ms
step:231/2315 train_time:13888ms step_avg:60.12ms
step:232/2315 train_time:13948ms step_avg:60.12ms
step:233/2315 train_time:14008ms step_avg:60.12ms
step:234/2315 train_time:14067ms step_avg:60.12ms
step:235/2315 train_time:14128ms step_avg:60.12ms
step:236/2315 train_time:14188ms step_avg:60.12ms
step:237/2315 train_time:14248ms step_avg:60.12ms
step:238/2315 train_time:14308ms step_avg:60.12ms
step:239/2315 train_time:14368ms step_avg:60.12ms
step:240/2315 train_time:14428ms step_avg:60.12ms
step:241/2315 train_time:14488ms step_avg:60.12ms
step:242/2315 train_time:14548ms step_avg:60.11ms
step:243/2315 train_time:14608ms step_avg:60.11ms
step:244/2315 train_time:14668ms step_avg:60.11ms
step:245/2315 train_time:14728ms step_avg:60.11ms
step:246/2315 train_time:14788ms step_avg:60.11ms
step:247/2315 train_time:14848ms step_avg:60.11ms
step:248/2315 train_time:14908ms step_avg:60.11ms
step:249/2315 train_time:14967ms step_avg:60.11ms
step:250/2315 train_time:15027ms step_avg:60.11ms
step:250/2315 val_loss:4.0778 train_time:15088ms step_avg:60.35ms
step:251/2315 train_time:15108ms step_avg:60.19ms
step:252/2315 train_time:15148ms step_avg:60.11ms
step:253/2315 train_time:15210ms step_avg:60.12ms
step:254/2315 train_time:15275ms step_avg:60.14ms
step:255/2315 train_time:15337ms step_avg:60.15ms
step:256/2315 train_time:15398ms step_avg:60.15ms
step:257/2315 train_time:15459ms step_avg:60.15ms
step:258/2315 train_time:15519ms step_avg:60.15ms
step:259/2315 train_time:15578ms step_avg:60.15ms
step:260/2315 train_time:15637ms step_avg:60.14ms
step:261/2315 train_time:15697ms step_avg:60.14ms
step:262/2315 train_time:15756ms step_avg:60.14ms
step:263/2315 train_time:15816ms step_avg:60.14ms
step:264/2315 train_time:15875ms step_avg:60.13ms
step:265/2315 train_time:15934ms step_avg:60.13ms
step:266/2315 train_time:15993ms step_avg:60.13ms
step:267/2315 train_time:16053ms step_avg:60.12ms
step:268/2315 train_time:16113ms step_avg:60.12ms
step:269/2315 train_time:16175ms step_avg:60.13ms
step:270/2315 train_time:16237ms step_avg:60.14ms
step:271/2315 train_time:16299ms step_avg:60.14ms
step:272/2315 train_time:16360ms step_avg:60.15ms
step:273/2315 train_time:16420ms step_avg:60.15ms
step:274/2315 train_time:16480ms step_avg:60.15ms
step:275/2315 train_time:16541ms step_avg:60.15ms
step:276/2315 train_time:16601ms step_avg:60.15ms
step:277/2315 train_time:16660ms step_avg:60.15ms
step:278/2315 train_time:16720ms step_avg:60.14ms
step:279/2315 train_time:16779ms step_avg:60.14ms
step:280/2315 train_time:16839ms step_avg:60.14ms
step:281/2315 train_time:16899ms step_avg:60.14ms
step:282/2315 train_time:16958ms step_avg:60.13ms
step:283/2315 train_time:17017ms step_avg:60.13ms
step:284/2315 train_time:17077ms step_avg:60.13ms
step:285/2315 train_time:17137ms step_avg:60.13ms
step:286/2315 train_time:17198ms step_avg:60.13ms
step:287/2315 train_time:17259ms step_avg:60.14ms
step:288/2315 train_time:17319ms step_avg:60.14ms
step:289/2315 train_time:17380ms step_avg:60.14ms
step:290/2315 train_time:17440ms step_avg:60.14ms
step:291/2315 train_time:17501ms step_avg:60.14ms
step:292/2315 train_time:17560ms step_avg:60.14ms
step:293/2315 train_time:17620ms step_avg:60.14ms
step:294/2315 train_time:17680ms step_avg:60.14ms
step:295/2315 train_time:17740ms step_avg:60.14ms
step:296/2315 train_time:17800ms step_avg:60.13ms
step:297/2315 train_time:17859ms step_avg:60.13ms
step:298/2315 train_time:17919ms step_avg:60.13ms
step:299/2315 train_time:17979ms step_avg:60.13ms
step:300/2315 train_time:18038ms step_avg:60.13ms
step:301/2315 train_time:18098ms step_avg:60.13ms
step:302/2315 train_time:18158ms step_avg:60.13ms
step:303/2315 train_time:18219ms step_avg:60.13ms
step:304/2315 train_time:18279ms step_avg:60.13ms
step:305/2315 train_time:18340ms step_avg:60.13ms
step:306/2315 train_time:18400ms step_avg:60.13ms
step:307/2315 train_time:18461ms step_avg:60.13ms
step:308/2315 train_time:18520ms step_avg:60.13ms
step:309/2315 train_time:18580ms step_avg:60.13ms
step:310/2315 train_time:18640ms step_avg:60.13ms
step:311/2315 train_time:18700ms step_avg:60.13ms
step:312/2315 train_time:18760ms step_avg:60.13ms
step:313/2315 train_time:18820ms step_avg:60.13ms
step:314/2315 train_time:18879ms step_avg:60.12ms
step:315/2315 train_time:18939ms step_avg:60.12ms
step:316/2315 train_time:18999ms step_avg:60.12ms
step:317/2315 train_time:19059ms step_avg:60.12ms
step:318/2315 train_time:19119ms step_avg:60.12ms
step:319/2315 train_time:19179ms step_avg:60.12ms
step:320/2315 train_time:19239ms step_avg:60.12ms
step:321/2315 train_time:19300ms step_avg:60.12ms
step:322/2315 train_time:19360ms step_avg:60.12ms
step:323/2315 train_time:19421ms step_avg:60.13ms
step:324/2315 train_time:19480ms step_avg:60.12ms
step:325/2315 train_time:19541ms step_avg:60.12ms
step:326/2315 train_time:19600ms step_avg:60.12ms
step:327/2315 train_time:19660ms step_avg:60.12ms
step:328/2315 train_time:19721ms step_avg:60.12ms
step:329/2315 train_time:19780ms step_avg:60.12ms
step:330/2315 train_time:19840ms step_avg:60.12ms
step:331/2315 train_time:19900ms step_avg:60.12ms
step:332/2315 train_time:19960ms step_avg:60.12ms
step:333/2315 train_time:20020ms step_avg:60.12ms
step:334/2315 train_time:20080ms step_avg:60.12ms
step:335/2315 train_time:20140ms step_avg:60.12ms
step:336/2315 train_time:20200ms step_avg:60.12ms
step:337/2315 train_time:20261ms step_avg:60.12ms
step:338/2315 train_time:20321ms step_avg:60.12ms
step:339/2315 train_time:20381ms step_avg:60.12ms
step:340/2315 train_time:20441ms step_avg:60.12ms
step:341/2315 train_time:20501ms step_avg:60.12ms
step:342/2315 train_time:20561ms step_avg:60.12ms
step:343/2315 train_time:20621ms step_avg:60.12ms
step:344/2315 train_time:20680ms step_avg:60.12ms
step:345/2315 train_time:20740ms step_avg:60.12ms
step:346/2315 train_time:20800ms step_avg:60.12ms
step:347/2315 train_time:20859ms step_avg:60.11ms
step:348/2315 train_time:20919ms step_avg:60.11ms
step:349/2315 train_time:20978ms step_avg:60.11ms
step:350/2315 train_time:21039ms step_avg:60.11ms
step:351/2315 train_time:21099ms step_avg:60.11ms
step:352/2315 train_time:21159ms step_avg:60.11ms
step:353/2315 train_time:21219ms step_avg:60.11ms
step:354/2315 train_time:21279ms step_avg:60.11ms
step:355/2315 train_time:21339ms step_avg:60.11ms
step:356/2315 train_time:21399ms step_avg:60.11ms
step:357/2315 train_time:21459ms step_avg:60.11ms
step:358/2315 train_time:21519ms step_avg:60.11ms
step:359/2315 train_time:21579ms step_avg:60.11ms
step:360/2315 train_time:21639ms step_avg:60.11ms
step:361/2315 train_time:21699ms step_avg:60.11ms
step:362/2315 train_time:21760ms step_avg:60.11ms
step:363/2315 train_time:21819ms step_avg:60.11ms
step:364/2315 train_time:21878ms step_avg:60.11ms
step:365/2315 train_time:21938ms step_avg:60.10ms
step:366/2315 train_time:21998ms step_avg:60.10ms
step:367/2315 train_time:22059ms step_avg:60.11ms
step:368/2315 train_time:22118ms step_avg:60.10ms
step:369/2315 train_time:22178ms step_avg:60.10ms
step:370/2315 train_time:22239ms step_avg:60.10ms
step:371/2315 train_time:22298ms step_avg:60.10ms
step:372/2315 train_time:22358ms step_avg:60.10ms
step:373/2315 train_time:22418ms step_avg:60.10ms
step:374/2315 train_time:22478ms step_avg:60.10ms
step:375/2315 train_time:22538ms step_avg:60.10ms
step:376/2315 train_time:22598ms step_avg:60.10ms
step:377/2315 train_time:22659ms step_avg:60.10ms
step:378/2315 train_time:22719ms step_avg:60.10ms
step:379/2315 train_time:22779ms step_avg:60.10ms
step:380/2315 train_time:22838ms step_avg:60.10ms
step:381/2315 train_time:22898ms step_avg:60.10ms
step:382/2315 train_time:22957ms step_avg:60.10ms
step:383/2315 train_time:23018ms step_avg:60.10ms
step:384/2315 train_time:23078ms step_avg:60.10ms
step:385/2315 train_time:23138ms step_avg:60.10ms
step:386/2315 train_time:23197ms step_avg:60.10ms
step:387/2315 train_time:23257ms step_avg:60.10ms
step:388/2315 train_time:23317ms step_avg:60.10ms
step:389/2315 train_time:23377ms step_avg:60.09ms
step:390/2315 train_time:23437ms step_avg:60.09ms
step:391/2315 train_time:23498ms step_avg:60.10ms
step:392/2315 train_time:23557ms step_avg:60.10ms
step:393/2315 train_time:23618ms step_avg:60.10ms
step:394/2315 train_time:23678ms step_avg:60.10ms
step:395/2315 train_time:23738ms step_avg:60.10ms
step:396/2315 train_time:23798ms step_avg:60.10ms
step:397/2315 train_time:23858ms step_avg:60.09ms
step:398/2315 train_time:23917ms step_avg:60.09ms
step:399/2315 train_time:23977ms step_avg:60.09ms
step:400/2315 train_time:24038ms step_avg:60.09ms
step:401/2315 train_time:24098ms step_avg:60.09ms
step:402/2315 train_time:24157ms step_avg:60.09ms
step:403/2315 train_time:24218ms step_avg:60.09ms
step:404/2315 train_time:24278ms step_avg:60.09ms
step:405/2315 train_time:24338ms step_avg:60.09ms
step:406/2315 train_time:24397ms step_avg:60.09ms
step:407/2315 train_time:24457ms step_avg:60.09ms
step:408/2315 train_time:24517ms step_avg:60.09ms
step:409/2315 train_time:24577ms step_avg:60.09ms
step:410/2315 train_time:24637ms step_avg:60.09ms
step:411/2315 train_time:24697ms step_avg:60.09ms
step:412/2315 train_time:24757ms step_avg:60.09ms
step:413/2315 train_time:24817ms step_avg:60.09ms
step:414/2315 train_time:24877ms step_avg:60.09ms
step:415/2315 train_time:24937ms step_avg:60.09ms
step:416/2315 train_time:24998ms step_avg:60.09ms
step:417/2315 train_time:25058ms step_avg:60.09ms
step:418/2315 train_time:25117ms step_avg:60.09ms
step:419/2315 train_time:25178ms step_avg:60.09ms
step:420/2315 train_time:25238ms step_avg:60.09ms
step:421/2315 train_time:25298ms step_avg:60.09ms
step:422/2315 train_time:25357ms step_avg:60.09ms
step:423/2315 train_time:25418ms step_avg:60.09ms
step:424/2315 train_time:25477ms step_avg:60.09ms
step:425/2315 train_time:25537ms step_avg:60.09ms
step:426/2315 train_time:25598ms step_avg:60.09ms
step:427/2315 train_time:25659ms step_avg:60.09ms
step:428/2315 train_time:25719ms step_avg:60.09ms
step:429/2315 train_time:25780ms step_avg:60.09ms
step:430/2315 train_time:25839ms step_avg:60.09ms
step:431/2315 train_time:25899ms step_avg:60.09ms
step:432/2315 train_time:25959ms step_avg:60.09ms
step:433/2315 train_time:26019ms step_avg:60.09ms
step:434/2315 train_time:26078ms step_avg:60.09ms
step:435/2315 train_time:26139ms step_avg:60.09ms
step:436/2315 train_time:26199ms step_avg:60.09ms
step:437/2315 train_time:26258ms step_avg:60.09ms
step:438/2315 train_time:26319ms step_avg:60.09ms
step:439/2315 train_time:26380ms step_avg:60.09ms
step:440/2315 train_time:26440ms step_avg:60.09ms
step:441/2315 train_time:26500ms step_avg:60.09ms
step:442/2315 train_time:26559ms step_avg:60.09ms
step:443/2315 train_time:26620ms step_avg:60.09ms
step:444/2315 train_time:26679ms step_avg:60.09ms
step:445/2315 train_time:26739ms step_avg:60.09ms
step:446/2315 train_time:26800ms step_avg:60.09ms
step:447/2315 train_time:26859ms step_avg:60.09ms
step:448/2315 train_time:26919ms step_avg:60.09ms
step:449/2315 train_time:26979ms step_avg:60.09ms
step:450/2315 train_time:27039ms step_avg:60.09ms
step:451/2315 train_time:27099ms step_avg:60.09ms
step:452/2315 train_time:27159ms step_avg:60.09ms
step:453/2315 train_time:27219ms step_avg:60.09ms
step:454/2315 train_time:27278ms step_avg:60.08ms
step:455/2315 train_time:27338ms step_avg:60.08ms
step:456/2315 train_time:27398ms step_avg:60.08ms
step:457/2315 train_time:27458ms step_avg:60.08ms
step:458/2315 train_time:27518ms step_avg:60.08ms
step:459/2315 train_time:27578ms step_avg:60.08ms
step:460/2315 train_time:27639ms step_avg:60.08ms
step:461/2315 train_time:27699ms step_avg:60.08ms
step:462/2315 train_time:27759ms step_avg:60.08ms
step:463/2315 train_time:27819ms step_avg:60.08ms
step:464/2315 train_time:27879ms step_avg:60.08ms
step:465/2315 train_time:27939ms step_avg:60.08ms
step:466/2315 train_time:27999ms step_avg:60.08ms
step:467/2315 train_time:28059ms step_avg:60.08ms
step:468/2315 train_time:28119ms step_avg:60.08ms
step:469/2315 train_time:28179ms step_avg:60.08ms
step:470/2315 train_time:28239ms step_avg:60.08ms
step:471/2315 train_time:28300ms step_avg:60.08ms
step:472/2315 train_time:28360ms step_avg:60.08ms
step:473/2315 train_time:28420ms step_avg:60.08ms
step:474/2315 train_time:28479ms step_avg:60.08ms
step:475/2315 train_time:28540ms step_avg:60.08ms
step:476/2315 train_time:28600ms step_avg:60.08ms
step:477/2315 train_time:28660ms step_avg:60.08ms
step:478/2315 train_time:28720ms step_avg:60.08ms
step:479/2315 train_time:28780ms step_avg:60.08ms
step:480/2315 train_time:28839ms step_avg:60.08ms
step:481/2315 train_time:28900ms step_avg:60.08ms
step:482/2315 train_time:28959ms step_avg:60.08ms
step:483/2315 train_time:29020ms step_avg:60.08ms
step:484/2315 train_time:29080ms step_avg:60.08ms
step:485/2315 train_time:29140ms step_avg:60.08ms
step:486/2315 train_time:29200ms step_avg:60.08ms
step:487/2315 train_time:29260ms step_avg:60.08ms
step:488/2315 train_time:29320ms step_avg:60.08ms
step:489/2315 train_time:29380ms step_avg:60.08ms
step:490/2315 train_time:29440ms step_avg:60.08ms
step:491/2315 train_time:29500ms step_avg:60.08ms
step:492/2315 train_time:29560ms step_avg:60.08ms
step:493/2315 train_time:29622ms step_avg:60.08ms
step:494/2315 train_time:29681ms step_avg:60.08ms
step:495/2315 train_time:29742ms step_avg:60.09ms
step:496/2315 train_time:29802ms step_avg:60.08ms
step:497/2315 train_time:29862ms step_avg:60.08ms
step:498/2315 train_time:29921ms step_avg:60.08ms
step:499/2315 train_time:29982ms step_avg:60.08ms
step:500/2315 train_time:30042ms step_avg:60.08ms
step:500/2315 val_loss:3.8133 train_time:30103ms step_avg:60.21ms
step:501/2315 train_time:30123ms step_avg:60.13ms
step:502/2315 train_time:30163ms step_avg:60.09ms
step:503/2315 train_time:30227ms step_avg:60.09ms
step:504/2315 train_time:30288ms step_avg:60.10ms
step:505/2315 train_time:30349ms step_avg:60.10ms
step:506/2315 train_time:30409ms step_avg:60.10ms
step:507/2315 train_time:30469ms step_avg:60.10ms
step:508/2315 train_time:30529ms step_avg:60.10ms
step:509/2315 train_time:30588ms step_avg:60.09ms
step:510/2315 train_time:30647ms step_avg:60.09ms
step:511/2315 train_time:30706ms step_avg:60.09ms
step:512/2315 train_time:30765ms step_avg:60.09ms
step:513/2315 train_time:30825ms step_avg:60.09ms
step:514/2315 train_time:30884ms step_avg:60.08ms
step:515/2315 train_time:30943ms step_avg:60.08ms
step:516/2315 train_time:31002ms step_avg:60.08ms
step:517/2315 train_time:31063ms step_avg:60.08ms
step:518/2315 train_time:31124ms step_avg:60.08ms
step:519/2315 train_time:31185ms step_avg:60.09ms
step:520/2315 train_time:31246ms step_avg:60.09ms
step:521/2315 train_time:31306ms step_avg:60.09ms
step:522/2315 train_time:31366ms step_avg:60.09ms
step:523/2315 train_time:31427ms step_avg:60.09ms
step:524/2315 train_time:31486ms step_avg:60.09ms
step:525/2315 train_time:31546ms step_avg:60.09ms
step:526/2315 train_time:31605ms step_avg:60.09ms
step:527/2315 train_time:31664ms step_avg:60.08ms
step:528/2315 train_time:31723ms step_avg:60.08ms
step:529/2315 train_time:31783ms step_avg:60.08ms
step:530/2315 train_time:31842ms step_avg:60.08ms
step:531/2315 train_time:31901ms step_avg:60.08ms
step:532/2315 train_time:31961ms step_avg:60.08ms
step:533/2315 train_time:32020ms step_avg:60.08ms
step:534/2315 train_time:32081ms step_avg:60.08ms
step:535/2315 train_time:32141ms step_avg:60.08ms
step:536/2315 train_time:32201ms step_avg:60.08ms
step:537/2315 train_time:32262ms step_avg:60.08ms
step:538/2315 train_time:32323ms step_avg:60.08ms
step:539/2315 train_time:32384ms step_avg:60.08ms
step:540/2315 train_time:32444ms step_avg:60.08ms
step:541/2315 train_time:32505ms step_avg:60.08ms
step:542/2315 train_time:32564ms step_avg:60.08ms
step:543/2315 train_time:32625ms step_avg:60.08ms
step:544/2315 train_time:32684ms step_avg:60.08ms
step:545/2315 train_time:32744ms step_avg:60.08ms
step:546/2315 train_time:32803ms step_avg:60.08ms
step:547/2315 train_time:32862ms step_avg:60.08ms
step:548/2315 train_time:32922ms step_avg:60.08ms
step:549/2315 train_time:32982ms step_avg:60.08ms
step:550/2315 train_time:33042ms step_avg:60.08ms
step:551/2315 train_time:33102ms step_avg:60.08ms
step:552/2315 train_time:33163ms step_avg:60.08ms
step:553/2315 train_time:33223ms step_avg:60.08ms
step:554/2315 train_time:33284ms step_avg:60.08ms
step:555/2315 train_time:33345ms step_avg:60.08ms
step:556/2315 train_time:33404ms step_avg:60.08ms
step:557/2315 train_time:33466ms step_avg:60.08ms
step:558/2315 train_time:33526ms step_avg:60.08ms
step:559/2315 train_time:33585ms step_avg:60.08ms
step:560/2315 train_time:33644ms step_avg:60.08ms
step:561/2315 train_time:33705ms step_avg:60.08ms
step:562/2315 train_time:33764ms step_avg:60.08ms
step:563/2315 train_time:33824ms step_avg:60.08ms
step:564/2315 train_time:33883ms step_avg:60.08ms
step:565/2315 train_time:33943ms step_avg:60.08ms
step:566/2315 train_time:34003ms step_avg:60.08ms
step:567/2315 train_time:34062ms step_avg:60.07ms
step:568/2315 train_time:34123ms step_avg:60.08ms
step:569/2315 train_time:34183ms step_avg:60.08ms
step:570/2315 train_time:34243ms step_avg:60.08ms
step:571/2315 train_time:34303ms step_avg:60.08ms
step:572/2315 train_time:34363ms step_avg:60.08ms
step:573/2315 train_time:34424ms step_avg:60.08ms
step:574/2315 train_time:34484ms step_avg:60.08ms
step:575/2315 train_time:34545ms step_avg:60.08ms
step:576/2315 train_time:34605ms step_avg:60.08ms
step:577/2315 train_time:34664ms step_avg:60.08ms
step:578/2315 train_time:34724ms step_avg:60.08ms
step:579/2315 train_time:34784ms step_avg:60.08ms
step:580/2315 train_time:34843ms step_avg:60.07ms
step:581/2315 train_time:34903ms step_avg:60.07ms
step:582/2315 train_time:34962ms step_avg:60.07ms
step:583/2315 train_time:35024ms step_avg:60.07ms
step:584/2315 train_time:35083ms step_avg:60.07ms
step:585/2315 train_time:35143ms step_avg:60.07ms
step:586/2315 train_time:35203ms step_avg:60.07ms
step:587/2315 train_time:35264ms step_avg:60.07ms
step:588/2315 train_time:35324ms step_avg:60.08ms
step:589/2315 train_time:35385ms step_avg:60.08ms
step:590/2315 train_time:35445ms step_avg:60.08ms
step:591/2315 train_time:35505ms step_avg:60.08ms
step:592/2315 train_time:35565ms step_avg:60.08ms
step:593/2315 train_time:35625ms step_avg:60.08ms
step:594/2315 train_time:35684ms step_avg:60.07ms
step:595/2315 train_time:35745ms step_avg:60.08ms
step:596/2315 train_time:35804ms step_avg:60.07ms
step:597/2315 train_time:35864ms step_avg:60.07ms
step:598/2315 train_time:35924ms step_avg:60.07ms
step:599/2315 train_time:35984ms step_avg:60.07ms
step:600/2315 train_time:36044ms step_avg:60.07ms
step:601/2315 train_time:36104ms step_avg:60.07ms
step:602/2315 train_time:36164ms step_avg:60.07ms
step:603/2315 train_time:36224ms step_avg:60.07ms
step:604/2315 train_time:36283ms step_avg:60.07ms
step:605/2315 train_time:36344ms step_avg:60.07ms
step:606/2315 train_time:36403ms step_avg:60.07ms
step:607/2315 train_time:36464ms step_avg:60.07ms
step:608/2315 train_time:36524ms step_avg:60.07ms
step:609/2315 train_time:36584ms step_avg:60.07ms
step:610/2315 train_time:36644ms step_avg:60.07ms
step:611/2315 train_time:36705ms step_avg:60.07ms
step:612/2315 train_time:36764ms step_avg:60.07ms
step:613/2315 train_time:36824ms step_avg:60.07ms
step:614/2315 train_time:36883ms step_avg:60.07ms
step:615/2315 train_time:36943ms step_avg:60.07ms
step:616/2315 train_time:37003ms step_avg:60.07ms
step:617/2315 train_time:37063ms step_avg:60.07ms
step:618/2315 train_time:37123ms step_avg:60.07ms
step:619/2315 train_time:37184ms step_avg:60.07ms
step:620/2315 train_time:37244ms step_avg:60.07ms
step:621/2315 train_time:37304ms step_avg:60.07ms
step:622/2315 train_time:37363ms step_avg:60.07ms
step:623/2315 train_time:37423ms step_avg:60.07ms
step:624/2315 train_time:37483ms step_avg:60.07ms
step:625/2315 train_time:37544ms step_avg:60.07ms
step:626/2315 train_time:37603ms step_avg:60.07ms
step:627/2315 train_time:37664ms step_avg:60.07ms
step:628/2315 train_time:37724ms step_avg:60.07ms
step:629/2315 train_time:37784ms step_avg:60.07ms
step:630/2315 train_time:37843ms step_avg:60.07ms
step:631/2315 train_time:37904ms step_avg:60.07ms
step:632/2315 train_time:37963ms step_avg:60.07ms
step:633/2315 train_time:38023ms step_avg:60.07ms
step:634/2315 train_time:38084ms step_avg:60.07ms
step:635/2315 train_time:38143ms step_avg:60.07ms
step:636/2315 train_time:38203ms step_avg:60.07ms
step:637/2315 train_time:38264ms step_avg:60.07ms
step:638/2315 train_time:38323ms step_avg:60.07ms
step:639/2315 train_time:38383ms step_avg:60.07ms
step:640/2315 train_time:38443ms step_avg:60.07ms
step:641/2315 train_time:38503ms step_avg:60.07ms
step:642/2315 train_time:38562ms step_avg:60.07ms
step:643/2315 train_time:38623ms step_avg:60.07ms
step:644/2315 train_time:38683ms step_avg:60.07ms
step:645/2315 train_time:38742ms step_avg:60.07ms
step:646/2315 train_time:38802ms step_avg:60.06ms
step:647/2315 train_time:38862ms step_avg:60.06ms
step:648/2315 train_time:38923ms step_avg:60.07ms
step:649/2315 train_time:38983ms step_avg:60.07ms
step:650/2315 train_time:39043ms step_avg:60.07ms
step:651/2315 train_time:39102ms step_avg:60.07ms
step:652/2315 train_time:39162ms step_avg:60.06ms
step:653/2315 train_time:39223ms step_avg:60.07ms
step:654/2315 train_time:39282ms step_avg:60.06ms
step:655/2315 train_time:39342ms step_avg:60.06ms
step:656/2315 train_time:39402ms step_avg:60.06ms
step:657/2315 train_time:39462ms step_avg:60.06ms
step:658/2315 train_time:39522ms step_avg:60.06ms
step:659/2315 train_time:39583ms step_avg:60.07ms
step:660/2315 train_time:39643ms step_avg:60.07ms
step:661/2315 train_time:39703ms step_avg:60.07ms
step:662/2315 train_time:39763ms step_avg:60.06ms
step:663/2315 train_time:39823ms step_avg:60.06ms
step:664/2315 train_time:39883ms step_avg:60.06ms
step:665/2315 train_time:39943ms step_avg:60.06ms
step:666/2315 train_time:40003ms step_avg:60.06ms
step:667/2315 train_time:40062ms step_avg:60.06ms
step:668/2315 train_time:40123ms step_avg:60.06ms
step:669/2315 train_time:40183ms step_avg:60.06ms
step:670/2315 train_time:40242ms step_avg:60.06ms
step:671/2315 train_time:40302ms step_avg:60.06ms
step:672/2315 train_time:40362ms step_avg:60.06ms
step:673/2315 train_time:40423ms step_avg:60.06ms
step:674/2315 train_time:40483ms step_avg:60.06ms
step:675/2315 train_time:40543ms step_avg:60.06ms
step:676/2315 train_time:40602ms step_avg:60.06ms
step:677/2315 train_time:40663ms step_avg:60.06ms
step:678/2315 train_time:40723ms step_avg:60.06ms
step:679/2315 train_time:40783ms step_avg:60.06ms
step:680/2315 train_time:40842ms step_avg:60.06ms
step:681/2315 train_time:40902ms step_avg:60.06ms
step:682/2315 train_time:40962ms step_avg:60.06ms
step:683/2315 train_time:41022ms step_avg:60.06ms
step:684/2315 train_time:41082ms step_avg:60.06ms
step:685/2315 train_time:41142ms step_avg:60.06ms
step:686/2315 train_time:41202ms step_avg:60.06ms
step:687/2315 train_time:41261ms step_avg:60.06ms
step:688/2315 train_time:41321ms step_avg:60.06ms
step:689/2315 train_time:41381ms step_avg:60.06ms
step:690/2315 train_time:41441ms step_avg:60.06ms
step:691/2315 train_time:41501ms step_avg:60.06ms
step:692/2315 train_time:41561ms step_avg:60.06ms
step:693/2315 train_time:41621ms step_avg:60.06ms
step:694/2315 train_time:41681ms step_avg:60.06ms
step:695/2315 train_time:41741ms step_avg:60.06ms
step:696/2315 train_time:41801ms step_avg:60.06ms
step:697/2315 train_time:41862ms step_avg:60.06ms
step:698/2315 train_time:41922ms step_avg:60.06ms
step:699/2315 train_time:41982ms step_avg:60.06ms
step:700/2315 train_time:42042ms step_avg:60.06ms
step:701/2315 train_time:42102ms step_avg:60.06ms
step:702/2315 train_time:42162ms step_avg:60.06ms
step:703/2315 train_time:42222ms step_avg:60.06ms
step:704/2315 train_time:42281ms step_avg:60.06ms
step:705/2315 train_time:42341ms step_avg:60.06ms
step:706/2315 train_time:42401ms step_avg:60.06ms
step:707/2315 train_time:42461ms step_avg:60.06ms
step:708/2315 train_time:42521ms step_avg:60.06ms
step:709/2315 train_time:42582ms step_avg:60.06ms
step:710/2315 train_time:42642ms step_avg:60.06ms
step:711/2315 train_time:42702ms step_avg:60.06ms
step:712/2315 train_time:42762ms step_avg:60.06ms
step:713/2315 train_time:42823ms step_avg:60.06ms
step:714/2315 train_time:42883ms step_avg:60.06ms
step:715/2315 train_time:42942ms step_avg:60.06ms
step:716/2315 train_time:43002ms step_avg:60.06ms
step:717/2315 train_time:43062ms step_avg:60.06ms
step:718/2315 train_time:43122ms step_avg:60.06ms
step:719/2315 train_time:43182ms step_avg:60.06ms
step:720/2315 train_time:43242ms step_avg:60.06ms
step:721/2315 train_time:43302ms step_avg:60.06ms
step:722/2315 train_time:43362ms step_avg:60.06ms
step:723/2315 train_time:43422ms step_avg:60.06ms
step:724/2315 train_time:43482ms step_avg:60.06ms
step:725/2315 train_time:43542ms step_avg:60.06ms
step:726/2315 train_time:43602ms step_avg:60.06ms
step:727/2315 train_time:43662ms step_avg:60.06ms
step:728/2315 train_time:43722ms step_avg:60.06ms
step:729/2315 train_time:43781ms step_avg:60.06ms
step:730/2315 train_time:43841ms step_avg:60.06ms
step:731/2315 train_time:43901ms step_avg:60.06ms
step:732/2315 train_time:43961ms step_avg:60.06ms
step:733/2315 train_time:44022ms step_avg:60.06ms
step:734/2315 train_time:44082ms step_avg:60.06ms
step:735/2315 train_time:44141ms step_avg:60.06ms
step:736/2315 train_time:44201ms step_avg:60.06ms
step:737/2315 train_time:44261ms step_avg:60.06ms
step:738/2315 train_time:44321ms step_avg:60.06ms
step:739/2315 train_time:44381ms step_avg:60.06ms
step:740/2315 train_time:44441ms step_avg:60.06ms
step:741/2315 train_time:44501ms step_avg:60.06ms
step:742/2315 train_time:44561ms step_avg:60.05ms
step:743/2315 train_time:44620ms step_avg:60.05ms
step:744/2315 train_time:44680ms step_avg:60.05ms
step:745/2315 train_time:44741ms step_avg:60.05ms
step:746/2315 train_time:44801ms step_avg:60.06ms
step:747/2315 train_time:44861ms step_avg:60.06ms
step:748/2315 train_time:44921ms step_avg:60.06ms
step:749/2315 train_time:44982ms step_avg:60.06ms
step:750/2315 train_time:45041ms step_avg:60.06ms
step:750/2315 val_loss:3.6825 train_time:45104ms step_avg:60.14ms
step:751/2315 train_time:45124ms step_avg:60.08ms
step:752/2315 train_time:45164ms step_avg:60.06ms
step:753/2315 train_time:45229ms step_avg:60.06ms
step:754/2315 train_time:45292ms step_avg:60.07ms
step:755/2315 train_time:45353ms step_avg:60.07ms
step:756/2315 train_time:45413ms step_avg:60.07ms
step:757/2315 train_time:45472ms step_avg:60.07ms
step:758/2315 train_time:45532ms step_avg:60.07ms
step:759/2315 train_time:45591ms step_avg:60.07ms
step:760/2315 train_time:45651ms step_avg:60.07ms
step:761/2315 train_time:45712ms step_avg:60.07ms
step:762/2315 train_time:45772ms step_avg:60.07ms
step:763/2315 train_time:45832ms step_avg:60.07ms
step:764/2315 train_time:45892ms step_avg:60.07ms
step:765/2315 train_time:45953ms step_avg:60.07ms
step:766/2315 train_time:46013ms step_avg:60.07ms
step:767/2315 train_time:46074ms step_avg:60.07ms
step:768/2315 train_time:46136ms step_avg:60.07ms
step:769/2315 train_time:46198ms step_avg:60.08ms
step:770/2315 train_time:46260ms step_avg:60.08ms
step:771/2315 train_time:46322ms step_avg:60.08ms
step:772/2315 train_time:46383ms step_avg:60.08ms
step:773/2315 train_time:46444ms step_avg:60.08ms
step:774/2315 train_time:46504ms step_avg:60.08ms
step:775/2315 train_time:46564ms step_avg:60.08ms
step:776/2315 train_time:46625ms step_avg:60.08ms
step:777/2315 train_time:46686ms step_avg:60.08ms
step:778/2315 train_time:46747ms step_avg:60.09ms
step:779/2315 train_time:46807ms step_avg:60.09ms
step:780/2315 train_time:46867ms step_avg:60.09ms
step:781/2315 train_time:46928ms step_avg:60.09ms
step:782/2315 train_time:46988ms step_avg:60.09ms
step:783/2315 train_time:47049ms step_avg:60.09ms
step:784/2315 train_time:47111ms step_avg:60.09ms
step:785/2315 train_time:47173ms step_avg:60.09ms
step:786/2315 train_time:47233ms step_avg:60.09ms
step:787/2315 train_time:47294ms step_avg:60.09ms
step:788/2315 train_time:47356ms step_avg:60.10ms
step:789/2315 train_time:47417ms step_avg:60.10ms
step:790/2315 train_time:47478ms step_avg:60.10ms
step:791/2315 train_time:47538ms step_avg:60.10ms
step:792/2315 train_time:47599ms step_avg:60.10ms
step:793/2315 train_time:47660ms step_avg:60.10ms
step:794/2315 train_time:47721ms step_avg:60.10ms
step:795/2315 train_time:47782ms step_avg:60.10ms
step:796/2315 train_time:47842ms step_avg:60.10ms
step:797/2315 train_time:47903ms step_avg:60.10ms
step:798/2315 train_time:47964ms step_avg:60.11ms
step:799/2315 train_time:48025ms step_avg:60.11ms
step:800/2315 train_time:48085ms step_avg:60.11ms
step:801/2315 train_time:48146ms step_avg:60.11ms
step:802/2315 train_time:48207ms step_avg:60.11ms
step:803/2315 train_time:48268ms step_avg:60.11ms
step:804/2315 train_time:48328ms step_avg:60.11ms
step:805/2315 train_time:48389ms step_avg:60.11ms
step:806/2315 train_time:48450ms step_avg:60.11ms
step:807/2315 train_time:48512ms step_avg:60.11ms
step:808/2315 train_time:48572ms step_avg:60.11ms
step:809/2315 train_time:48633ms step_avg:60.12ms
step:810/2315 train_time:48694ms step_avg:60.12ms
step:811/2315 train_time:48754ms step_avg:60.12ms
step:812/2315 train_time:48814ms step_avg:60.12ms
step:813/2315 train_time:48875ms step_avg:60.12ms
step:814/2315 train_time:48936ms step_avg:60.12ms
step:815/2315 train_time:48997ms step_avg:60.12ms
step:816/2315 train_time:49058ms step_avg:60.12ms
step:817/2315 train_time:49119ms step_avg:60.12ms
step:818/2315 train_time:49180ms step_avg:60.12ms
step:819/2315 train_time:49241ms step_avg:60.12ms
step:820/2315 train_time:49302ms step_avg:60.12ms
step:821/2315 train_time:49364ms step_avg:60.13ms
step:822/2315 train_time:49425ms step_avg:60.13ms
step:823/2315 train_time:49486ms step_avg:60.13ms
step:824/2315 train_time:49546ms step_avg:60.13ms
step:825/2315 train_time:49607ms step_avg:60.13ms
step:826/2315 train_time:49668ms step_avg:60.13ms
step:827/2315 train_time:49728ms step_avg:60.13ms
step:828/2315 train_time:49789ms step_avg:60.13ms
step:829/2315 train_time:49849ms step_avg:60.13ms
step:830/2315 train_time:49910ms step_avg:60.13ms
step:831/2315 train_time:49971ms step_avg:60.13ms
step:832/2315 train_time:50032ms step_avg:60.13ms
step:833/2315 train_time:50093ms step_avg:60.14ms
step:834/2315 train_time:50153ms step_avg:60.14ms
step:835/2315 train_time:50215ms step_avg:60.14ms
step:836/2315 train_time:50276ms step_avg:60.14ms
step:837/2315 train_time:50337ms step_avg:60.14ms
step:838/2315 train_time:50398ms step_avg:60.14ms
step:839/2315 train_time:50458ms step_avg:60.14ms
step:840/2315 train_time:50519ms step_avg:60.14ms
step:841/2315 train_time:50580ms step_avg:60.14ms
step:842/2315 train_time:50641ms step_avg:60.14ms
step:843/2315 train_time:50702ms step_avg:60.14ms
step:844/2315 train_time:50763ms step_avg:60.15ms
step:845/2315 train_time:50825ms step_avg:60.15ms
step:846/2315 train_time:50885ms step_avg:60.15ms
step:847/2315 train_time:50946ms step_avg:60.15ms
step:848/2315 train_time:51006ms step_avg:60.15ms
step:849/2315 train_time:51067ms step_avg:60.15ms
step:850/2315 train_time:51127ms step_avg:60.15ms
step:851/2315 train_time:51188ms step_avg:60.15ms
step:852/2315 train_time:51250ms step_avg:60.15ms
step:853/2315 train_time:51310ms step_avg:60.15ms
step:854/2315 train_time:51371ms step_avg:60.15ms
step:855/2315 train_time:51431ms step_avg:60.15ms
step:856/2315 train_time:51492ms step_avg:60.15ms
step:857/2315 train_time:51553ms step_avg:60.16ms
step:858/2315 train_time:51614ms step_avg:60.16ms
step:859/2315 train_time:51675ms step_avg:60.16ms
step:860/2315 train_time:51736ms step_avg:60.16ms
step:861/2315 train_time:51797ms step_avg:60.16ms
step:862/2315 train_time:51858ms step_avg:60.16ms
step:863/2315 train_time:51919ms step_avg:60.16ms
step:864/2315 train_time:51980ms step_avg:60.16ms
step:865/2315 train_time:52040ms step_avg:60.16ms
step:866/2315 train_time:52101ms step_avg:60.16ms
step:867/2315 train_time:52162ms step_avg:60.16ms
step:868/2315 train_time:52223ms step_avg:60.16ms
step:869/2315 train_time:52284ms step_avg:60.17ms
step:870/2315 train_time:52344ms step_avg:60.17ms
step:871/2315 train_time:52405ms step_avg:60.17ms
step:872/2315 train_time:52466ms step_avg:60.17ms
step:873/2315 train_time:52527ms step_avg:60.17ms
step:874/2315 train_time:52587ms step_avg:60.17ms
step:875/2315 train_time:52648ms step_avg:60.17ms
step:876/2315 train_time:52709ms step_avg:60.17ms
step:877/2315 train_time:52770ms step_avg:60.17ms
step:878/2315 train_time:52831ms step_avg:60.17ms
step:879/2315 train_time:52892ms step_avg:60.17ms
step:880/2315 train_time:52952ms step_avg:60.17ms
step:881/2315 train_time:53013ms step_avg:60.17ms
step:882/2315 train_time:53074ms step_avg:60.17ms
step:883/2315 train_time:53134ms step_avg:60.17ms
step:884/2315 train_time:53195ms step_avg:60.18ms
step:885/2315 train_time:53256ms step_avg:60.18ms
step:886/2315 train_time:53316ms step_avg:60.18ms
step:887/2315 train_time:53377ms step_avg:60.18ms
step:888/2315 train_time:53438ms step_avg:60.18ms
step:889/2315 train_time:53499ms step_avg:60.18ms
step:890/2315 train_time:53560ms step_avg:60.18ms
step:891/2315 train_time:53621ms step_avg:60.18ms
step:892/2315 train_time:53683ms step_avg:60.18ms
step:893/2315 train_time:53744ms step_avg:60.18ms
step:894/2315 train_time:53805ms step_avg:60.18ms
step:895/2315 train_time:53866ms step_avg:60.19ms
step:896/2315 train_time:53926ms step_avg:60.19ms
step:897/2315 train_time:53987ms step_avg:60.19ms
step:898/2315 train_time:54047ms step_avg:60.19ms
step:899/2315 train_time:54108ms step_avg:60.19ms
step:900/2315 train_time:54168ms step_avg:60.19ms
step:901/2315 train_time:54228ms step_avg:60.19ms
step:902/2315 train_time:54288ms step_avg:60.19ms
step:903/2315 train_time:54349ms step_avg:60.19ms
step:904/2315 train_time:54410ms step_avg:60.19ms
step:905/2315 train_time:54470ms step_avg:60.19ms
step:906/2315 train_time:54531ms step_avg:60.19ms
step:907/2315 train_time:54592ms step_avg:60.19ms
step:908/2315 train_time:54653ms step_avg:60.19ms
step:909/2315 train_time:54714ms step_avg:60.19ms
step:910/2315 train_time:54775ms step_avg:60.19ms
step:911/2315 train_time:54836ms step_avg:60.19ms
step:912/2315 train_time:54896ms step_avg:60.19ms
step:913/2315 train_time:54957ms step_avg:60.19ms
step:914/2315 train_time:55018ms step_avg:60.19ms
step:915/2315 train_time:55079ms step_avg:60.20ms
step:916/2315 train_time:55139ms step_avg:60.20ms
step:917/2315 train_time:55200ms step_avg:60.20ms
step:918/2315 train_time:55262ms step_avg:60.20ms
step:919/2315 train_time:55322ms step_avg:60.20ms
step:920/2315 train_time:55384ms step_avg:60.20ms
step:921/2315 train_time:55445ms step_avg:60.20ms
step:922/2315 train_time:55506ms step_avg:60.20ms
step:923/2315 train_time:55566ms step_avg:60.20ms
step:924/2315 train_time:55626ms step_avg:60.20ms
step:925/2315 train_time:55687ms step_avg:60.20ms
step:926/2315 train_time:55747ms step_avg:60.20ms
step:927/2315 train_time:55808ms step_avg:60.20ms
step:928/2315 train_time:55869ms step_avg:60.20ms
step:929/2315 train_time:55929ms step_avg:60.20ms
step:930/2315 train_time:55989ms step_avg:60.20ms
step:931/2315 train_time:56050ms step_avg:60.20ms
step:932/2315 train_time:56110ms step_avg:60.20ms
step:933/2315 train_time:56171ms step_avg:60.20ms
step:934/2315 train_time:56232ms step_avg:60.21ms
step:935/2315 train_time:56293ms step_avg:60.21ms
step:936/2315 train_time:56354ms step_avg:60.21ms
step:937/2315 train_time:56415ms step_avg:60.21ms
step:938/2315 train_time:56476ms step_avg:60.21ms
step:939/2315 train_time:56537ms step_avg:60.21ms
step:940/2315 train_time:56598ms step_avg:60.21ms
step:941/2315 train_time:56658ms step_avg:60.21ms
step:942/2315 train_time:56719ms step_avg:60.21ms
step:943/2315 train_time:56780ms step_avg:60.21ms
step:944/2315 train_time:56841ms step_avg:60.21ms
step:945/2315 train_time:56902ms step_avg:60.21ms
step:946/2315 train_time:56964ms step_avg:60.22ms
step:947/2315 train_time:57025ms step_avg:60.22ms
step:948/2315 train_time:57085ms step_avg:60.22ms
step:949/2315 train_time:57146ms step_avg:60.22ms
step:950/2315 train_time:57207ms step_avg:60.22ms
step:951/2315 train_time:57268ms step_avg:60.22ms
step:952/2315 train_time:57328ms step_avg:60.22ms
step:953/2315 train_time:57388ms step_avg:60.22ms
step:954/2315 train_time:57448ms step_avg:60.22ms
step:955/2315 train_time:57510ms step_avg:60.22ms
step:956/2315 train_time:57570ms step_avg:60.22ms
step:957/2315 train_time:57631ms step_avg:60.22ms
step:958/2315 train_time:57692ms step_avg:60.22ms
step:959/2315 train_time:57753ms step_avg:60.22ms
step:960/2315 train_time:57813ms step_avg:60.22ms
step:961/2315 train_time:57874ms step_avg:60.22ms
step:962/2315 train_time:57935ms step_avg:60.22ms
step:963/2315 train_time:57996ms step_avg:60.22ms
step:964/2315 train_time:58057ms step_avg:60.22ms
step:965/2315 train_time:58118ms step_avg:60.23ms
step:966/2315 train_time:58178ms step_avg:60.23ms
step:967/2315 train_time:58239ms step_avg:60.23ms
step:968/2315 train_time:58300ms step_avg:60.23ms
step:969/2315 train_time:58361ms step_avg:60.23ms
step:970/2315 train_time:58422ms step_avg:60.23ms
step:971/2315 train_time:58483ms step_avg:60.23ms
step:972/2315 train_time:58544ms step_avg:60.23ms
step:973/2315 train_time:58606ms step_avg:60.23ms
step:974/2315 train_time:58666ms step_avg:60.23ms
step:975/2315 train_time:58726ms step_avg:60.23ms
step:976/2315 train_time:58787ms step_avg:60.23ms
step:977/2315 train_time:58847ms step_avg:60.23ms
step:978/2315 train_time:58908ms step_avg:60.23ms
step:979/2315 train_time:58968ms step_avg:60.23ms
step:980/2315 train_time:59029ms step_avg:60.23ms
step:981/2315 train_time:59089ms step_avg:60.23ms
step:982/2315 train_time:59150ms step_avg:60.23ms
step:983/2315 train_time:59211ms step_avg:60.24ms
step:984/2315 train_time:59272ms step_avg:60.24ms
step:985/2315 train_time:59333ms step_avg:60.24ms
step:986/2315 train_time:59393ms step_avg:60.24ms
step:987/2315 train_time:59454ms step_avg:60.24ms
step:988/2315 train_time:59515ms step_avg:60.24ms
step:989/2315 train_time:59576ms step_avg:60.24ms
step:990/2315 train_time:59636ms step_avg:60.24ms
step:991/2315 train_time:59697ms step_avg:60.24ms
step:992/2315 train_time:59757ms step_avg:60.24ms
step:993/2315 train_time:59819ms step_avg:60.24ms
step:994/2315 train_time:59880ms step_avg:60.24ms
step:995/2315 train_time:59941ms step_avg:60.24ms
step:996/2315 train_time:60001ms step_avg:60.24ms
step:997/2315 train_time:60062ms step_avg:60.24ms
step:998/2315 train_time:60123ms step_avg:60.24ms
step:999/2315 train_time:60184ms step_avg:60.24ms
step:1000/2315 train_time:60245ms step_avg:60.24ms
step:1000/2315 val_loss:3.5708 train_time:60307ms step_avg:60.31ms
step:1001/2315 train_time:60327ms step_avg:60.27ms
step:1002/2315 train_time:60367ms step_avg:60.25ms
step:1003/2315 train_time:60435ms step_avg:60.25ms
step:1004/2315 train_time:60501ms step_avg:60.26ms
step:1005/2315 train_time:60562ms step_avg:60.26ms
step:1006/2315 train_time:60622ms step_avg:60.26ms
step:1007/2315 train_time:60683ms step_avg:60.26ms
step:1008/2315 train_time:60743ms step_avg:60.26ms
step:1009/2315 train_time:60803ms step_avg:60.26ms
step:1010/2315 train_time:60863ms step_avg:60.26ms
step:1011/2315 train_time:60923ms step_avg:60.26ms
step:1012/2315 train_time:60983ms step_avg:60.26ms
step:1013/2315 train_time:61043ms step_avg:60.26ms
step:1014/2315 train_time:61102ms step_avg:60.26ms
step:1015/2315 train_time:61162ms step_avg:60.26ms
step:1016/2315 train_time:61222ms step_avg:60.26ms
step:1017/2315 train_time:61284ms step_avg:60.26ms
step:1018/2315 train_time:61346ms step_avg:60.26ms
step:1019/2315 train_time:61409ms step_avg:60.26ms
step:1020/2315 train_time:61471ms step_avg:60.27ms
step:1021/2315 train_time:61533ms step_avg:60.27ms
step:1022/2315 train_time:61593ms step_avg:60.27ms
step:1023/2315 train_time:61654ms step_avg:60.27ms
step:1024/2315 train_time:61714ms step_avg:60.27ms
step:1025/2315 train_time:61774ms step_avg:60.27ms
step:1026/2315 train_time:61834ms step_avg:60.27ms
step:1027/2315 train_time:61895ms step_avg:60.27ms
step:1028/2315 train_time:61955ms step_avg:60.27ms
step:1029/2315 train_time:62015ms step_avg:60.27ms
step:1030/2315 train_time:62075ms step_avg:60.27ms
step:1031/2315 train_time:62136ms step_avg:60.27ms
step:1032/2315 train_time:62197ms step_avg:60.27ms
step:1033/2315 train_time:62259ms step_avg:60.27ms
step:1034/2315 train_time:62320ms step_avg:60.27ms
step:1035/2315 train_time:62382ms step_avg:60.27ms
step:1036/2315 train_time:62443ms step_avg:60.27ms
step:1037/2315 train_time:62504ms step_avg:60.27ms
step:1038/2315 train_time:62566ms step_avg:60.28ms
step:1039/2315 train_time:62627ms step_avg:60.28ms
step:1040/2315 train_time:62687ms step_avg:60.28ms
step:1041/2315 train_time:62748ms step_avg:60.28ms
step:1042/2315 train_time:62808ms step_avg:60.28ms
step:1043/2315 train_time:62868ms step_avg:60.28ms
step:1044/2315 train_time:62929ms step_avg:60.28ms
step:1045/2315 train_time:62990ms step_avg:60.28ms
step:1046/2315 train_time:63050ms step_avg:60.28ms
step:1047/2315 train_time:63111ms step_avg:60.28ms
step:1048/2315 train_time:63171ms step_avg:60.28ms
step:1049/2315 train_time:63232ms step_avg:60.28ms
step:1050/2315 train_time:63293ms step_avg:60.28ms
step:1051/2315 train_time:63355ms step_avg:60.28ms
step:1052/2315 train_time:63416ms step_avg:60.28ms
step:1053/2315 train_time:63478ms step_avg:60.28ms
step:1054/2315 train_time:63539ms step_avg:60.28ms
step:1055/2315 train_time:63600ms step_avg:60.28ms
step:1056/2315 train_time:63661ms step_avg:60.28ms
step:1057/2315 train_time:63722ms step_avg:60.29ms
step:1058/2315 train_time:63782ms step_avg:60.29ms
step:1059/2315 train_time:63843ms step_avg:60.29ms
step:1060/2315 train_time:63904ms step_avg:60.29ms
step:1061/2315 train_time:63965ms step_avg:60.29ms
step:1062/2315 train_time:64026ms step_avg:60.29ms
step:1063/2315 train_time:64087ms step_avg:60.29ms
step:1064/2315 train_time:64147ms step_avg:60.29ms
step:1065/2315 train_time:64208ms step_avg:60.29ms
step:1066/2315 train_time:64268ms step_avg:60.29ms
step:1067/2315 train_time:64328ms step_avg:60.29ms
step:1068/2315 train_time:64389ms step_avg:60.29ms
step:1069/2315 train_time:64450ms step_avg:60.29ms
step:1070/2315 train_time:64511ms step_avg:60.29ms
step:1071/2315 train_time:64573ms step_avg:60.29ms
step:1072/2315 train_time:64633ms step_avg:60.29ms
step:1073/2315 train_time:64694ms step_avg:60.29ms
step:1074/2315 train_time:64755ms step_avg:60.29ms
step:1075/2315 train_time:64816ms step_avg:60.29ms
step:1076/2315 train_time:64877ms step_avg:60.29ms
step:1077/2315 train_time:64938ms step_avg:60.30ms
step:1078/2315 train_time:64999ms step_avg:60.30ms
step:1079/2315 train_time:65059ms step_avg:60.30ms
step:1080/2315 train_time:65120ms step_avg:60.30ms
step:1081/2315 train_time:65181ms step_avg:60.30ms
step:1082/2315 train_time:65242ms step_avg:60.30ms
step:1083/2315 train_time:65303ms step_avg:60.30ms
step:1084/2315 train_time:65363ms step_avg:60.30ms
step:1085/2315 train_time:65425ms step_avg:60.30ms
step:1086/2315 train_time:65485ms step_avg:60.30ms
step:1087/2315 train_time:65546ms step_avg:60.30ms
step:1088/2315 train_time:65607ms step_avg:60.30ms
step:1089/2315 train_time:65667ms step_avg:60.30ms
step:1090/2315 train_time:65728ms step_avg:60.30ms
step:1091/2315 train_time:65788ms step_avg:60.30ms
step:1092/2315 train_time:65848ms step_avg:60.30ms
step:1093/2315 train_time:65909ms step_avg:60.30ms
step:1094/2315 train_time:65970ms step_avg:60.30ms
step:1095/2315 train_time:66031ms step_avg:60.30ms
step:1096/2315 train_time:66092ms step_avg:60.30ms
step:1097/2315 train_time:66153ms step_avg:60.30ms
step:1098/2315 train_time:66214ms step_avg:60.30ms
step:1099/2315 train_time:66275ms step_avg:60.30ms
step:1100/2315 train_time:66335ms step_avg:60.30ms
step:1101/2315 train_time:66396ms step_avg:60.31ms
step:1102/2315 train_time:66457ms step_avg:60.31ms
step:1103/2315 train_time:66518ms step_avg:60.31ms
step:1104/2315 train_time:66578ms step_avg:60.31ms
step:1105/2315 train_time:66639ms step_avg:60.31ms
step:1106/2315 train_time:66700ms step_avg:60.31ms
step:1107/2315 train_time:66761ms step_avg:60.31ms
step:1108/2315 train_time:66822ms step_avg:60.31ms
step:1109/2315 train_time:66883ms step_avg:60.31ms
step:1110/2315 train_time:66944ms step_avg:60.31ms
step:1111/2315 train_time:67005ms step_avg:60.31ms
step:1112/2315 train_time:67065ms step_avg:60.31ms
step:1113/2315 train_time:67127ms step_avg:60.31ms
step:1114/2315 train_time:67188ms step_avg:60.31ms
step:1115/2315 train_time:67249ms step_avg:60.31ms
step:1116/2315 train_time:67309ms step_avg:60.31ms
step:1117/2315 train_time:67369ms step_avg:60.31ms
step:1118/2315 train_time:67430ms step_avg:60.31ms
step:1119/2315 train_time:67492ms step_avg:60.31ms
step:1120/2315 train_time:67552ms step_avg:60.31ms
step:1121/2315 train_time:67613ms step_avg:60.32ms
step:1122/2315 train_time:67674ms step_avg:60.32ms
step:1123/2315 train_time:67734ms step_avg:60.32ms
step:1124/2315 train_time:67795ms step_avg:60.32ms
step:1125/2315 train_time:67856ms step_avg:60.32ms
step:1126/2315 train_time:67916ms step_avg:60.32ms
step:1127/2315 train_time:67977ms step_avg:60.32ms
step:1128/2315 train_time:68038ms step_avg:60.32ms
step:1129/2315 train_time:68099ms step_avg:60.32ms
step:1130/2315 train_time:68160ms step_avg:60.32ms
step:1131/2315 train_time:68222ms step_avg:60.32ms
step:1132/2315 train_time:68283ms step_avg:60.32ms
step:1133/2315 train_time:68344ms step_avg:60.32ms
step:1134/2315 train_time:68406ms step_avg:60.32ms
step:1135/2315 train_time:68467ms step_avg:60.32ms
step:1136/2315 train_time:68527ms step_avg:60.32ms
step:1137/2315 train_time:68587ms step_avg:60.32ms
step:1138/2315 train_time:68648ms step_avg:60.32ms
step:1139/2315 train_time:68708ms step_avg:60.32ms
step:1140/2315 train_time:68768ms step_avg:60.32ms
step:1141/2315 train_time:68829ms step_avg:60.32ms
step:1142/2315 train_time:68890ms step_avg:60.32ms
step:1143/2315 train_time:68951ms step_avg:60.32ms
step:1144/2315 train_time:69011ms step_avg:60.32ms
step:1145/2315 train_time:69073ms step_avg:60.33ms
step:1146/2315 train_time:69133ms step_avg:60.33ms
step:1147/2315 train_time:69194ms step_avg:60.33ms
step:1148/2315 train_time:69255ms step_avg:60.33ms
step:1149/2315 train_time:69316ms step_avg:60.33ms
step:1150/2315 train_time:69377ms step_avg:60.33ms
step:1151/2315 train_time:69439ms step_avg:60.33ms
step:1152/2315 train_time:69499ms step_avg:60.33ms
step:1153/2315 train_time:69560ms step_avg:60.33ms
step:1154/2315 train_time:69621ms step_avg:60.33ms
step:1155/2315 train_time:69682ms step_avg:60.33ms
step:1156/2315 train_time:69742ms step_avg:60.33ms
step:1157/2315 train_time:69804ms step_avg:60.33ms
step:1158/2315 train_time:69864ms step_avg:60.33ms
step:1159/2315 train_time:69925ms step_avg:60.33ms
step:1160/2315 train_time:69986ms step_avg:60.33ms
step:1161/2315 train_time:70047ms step_avg:60.33ms
step:1162/2315 train_time:70107ms step_avg:60.33ms
step:1163/2315 train_time:70167ms step_avg:60.33ms
step:1164/2315 train_time:70228ms step_avg:60.33ms
step:1165/2315 train_time:70289ms step_avg:60.33ms
step:1166/2315 train_time:70349ms step_avg:60.33ms
step:1167/2315 train_time:70410ms step_avg:60.33ms
step:1168/2315 train_time:70471ms step_avg:60.33ms
step:1169/2315 train_time:70532ms step_avg:60.34ms
step:1170/2315 train_time:70593ms step_avg:60.34ms
step:1171/2315 train_time:70653ms step_avg:60.34ms
step:1172/2315 train_time:70714ms step_avg:60.34ms
step:1173/2315 train_time:70775ms step_avg:60.34ms
step:1174/2315 train_time:70835ms step_avg:60.34ms
step:1175/2315 train_time:70897ms step_avg:60.34ms
step:1176/2315 train_time:70958ms step_avg:60.34ms
step:1177/2315 train_time:71018ms step_avg:60.34ms
step:1178/2315 train_time:71079ms step_avg:60.34ms
step:1179/2315 train_time:71140ms step_avg:60.34ms
step:1180/2315 train_time:71201ms step_avg:60.34ms
step:1181/2315 train_time:71262ms step_avg:60.34ms
step:1182/2315 train_time:71323ms step_avg:60.34ms
step:1183/2315 train_time:71384ms step_avg:60.34ms
step:1184/2315 train_time:71445ms step_avg:60.34ms
step:1185/2315 train_time:71506ms step_avg:60.34ms
step:1186/2315 train_time:71567ms step_avg:60.34ms
step:1187/2315 train_time:71627ms step_avg:60.34ms
step:1188/2315 train_time:71687ms step_avg:60.34ms
step:1189/2315 train_time:71748ms step_avg:60.34ms
step:1190/2315 train_time:71808ms step_avg:60.34ms
step:1191/2315 train_time:71869ms step_avg:60.34ms
step:1192/2315 train_time:71930ms step_avg:60.34ms
step:1193/2315 train_time:71991ms step_avg:60.34ms
step:1194/2315 train_time:72051ms step_avg:60.34ms
step:1195/2315 train_time:72112ms step_avg:60.34ms
step:1196/2315 train_time:72173ms step_avg:60.35ms
step:1197/2315 train_time:72234ms step_avg:60.35ms
step:1198/2315 train_time:72294ms step_avg:60.35ms
step:1199/2315 train_time:72355ms step_avg:60.35ms
step:1200/2315 train_time:72416ms step_avg:60.35ms
step:1201/2315 train_time:72477ms step_avg:60.35ms
step:1202/2315 train_time:72538ms step_avg:60.35ms
step:1203/2315 train_time:72599ms step_avg:60.35ms
step:1204/2315 train_time:72660ms step_avg:60.35ms
step:1205/2315 train_time:72721ms step_avg:60.35ms
step:1206/2315 train_time:72781ms step_avg:60.35ms
step:1207/2315 train_time:72842ms step_avg:60.35ms
step:1208/2315 train_time:72904ms step_avg:60.35ms
step:1209/2315 train_time:72965ms step_avg:60.35ms
step:1210/2315 train_time:73025ms step_avg:60.35ms
step:1211/2315 train_time:73086ms step_avg:60.35ms
step:1212/2315 train_time:73147ms step_avg:60.35ms
step:1213/2315 train_time:73207ms step_avg:60.35ms
step:1214/2315 train_time:73268ms step_avg:60.35ms
step:1215/2315 train_time:73329ms step_avg:60.35ms
step:1216/2315 train_time:73390ms step_avg:60.35ms
step:1217/2315 train_time:73451ms step_avg:60.35ms
step:1218/2315 train_time:73511ms step_avg:60.35ms
step:1219/2315 train_time:73572ms step_avg:60.35ms
step:1220/2315 train_time:73633ms step_avg:60.35ms
step:1221/2315 train_time:73694ms step_avg:60.36ms
step:1222/2315 train_time:73755ms step_avg:60.36ms
step:1223/2315 train_time:73815ms step_avg:60.36ms
step:1224/2315 train_time:73876ms step_avg:60.36ms
step:1225/2315 train_time:73937ms step_avg:60.36ms
step:1226/2315 train_time:73998ms step_avg:60.36ms
step:1227/2315 train_time:74059ms step_avg:60.36ms
step:1228/2315 train_time:74119ms step_avg:60.36ms
step:1229/2315 train_time:74180ms step_avg:60.36ms
step:1230/2315 train_time:74241ms step_avg:60.36ms
step:1231/2315 train_time:74302ms step_avg:60.36ms
step:1232/2315 train_time:74364ms step_avg:60.36ms
step:1233/2315 train_time:74425ms step_avg:60.36ms
step:1234/2315 train_time:74486ms step_avg:60.36ms
step:1235/2315 train_time:74547ms step_avg:60.36ms
step:1236/2315 train_time:74607ms step_avg:60.36ms
step:1237/2315 train_time:74668ms step_avg:60.36ms
step:1238/2315 train_time:74729ms step_avg:60.36ms
step:1239/2315 train_time:74790ms step_avg:60.36ms
step:1240/2315 train_time:74850ms step_avg:60.36ms
step:1241/2315 train_time:74911ms step_avg:60.36ms
step:1242/2315 train_time:74972ms step_avg:60.36ms
step:1243/2315 train_time:75033ms step_avg:60.36ms
step:1244/2315 train_time:75093ms step_avg:60.36ms
step:1245/2315 train_time:75154ms step_avg:60.36ms
step:1246/2315 train_time:75215ms step_avg:60.37ms
step:1247/2315 train_time:75276ms step_avg:60.37ms
step:1248/2315 train_time:75337ms step_avg:60.37ms
step:1249/2315 train_time:75398ms step_avg:60.37ms
step:1250/2315 train_time:75459ms step_avg:60.37ms
step:1250/2315 val_loss:3.5132 train_time:75521ms step_avg:60.42ms
step:1251/2315 train_time:75541ms step_avg:60.38ms
step:1252/2315 train_time:75583ms step_avg:60.37ms
step:1253/2315 train_time:75647ms step_avg:60.37ms
step:1254/2315 train_time:75710ms step_avg:60.37ms
step:1255/2315 train_time:75771ms step_avg:60.38ms
step:1256/2315 train_time:75833ms step_avg:60.38ms
step:1257/2315 train_time:75893ms step_avg:60.38ms
step:1258/2315 train_time:75953ms step_avg:60.38ms
step:1259/2315 train_time:76014ms step_avg:60.38ms
step:1260/2315 train_time:76074ms step_avg:60.38ms
step:1261/2315 train_time:76133ms step_avg:60.38ms
step:1262/2315 train_time:76193ms step_avg:60.38ms
step:1263/2315 train_time:76253ms step_avg:60.37ms
step:1264/2315 train_time:76314ms step_avg:60.37ms
step:1265/2315 train_time:76374ms step_avg:60.37ms
step:1266/2315 train_time:76434ms step_avg:60.37ms
step:1267/2315 train_time:76495ms step_avg:60.38ms
step:1268/2315 train_time:76557ms step_avg:60.38ms
step:1269/2315 train_time:76620ms step_avg:60.38ms
step:1270/2315 train_time:76681ms step_avg:60.38ms
step:1271/2315 train_time:76742ms step_avg:60.38ms
step:1272/2315 train_time:76802ms step_avg:60.38ms
step:1273/2315 train_time:76864ms step_avg:60.38ms
step:1274/2315 train_time:76924ms step_avg:60.38ms
step:1275/2315 train_time:76985ms step_avg:60.38ms
step:1276/2315 train_time:77046ms step_avg:60.38ms
step:1277/2315 train_time:77106ms step_avg:60.38ms
step:1278/2315 train_time:77166ms step_avg:60.38ms
step:1279/2315 train_time:77227ms step_avg:60.38ms
step:1280/2315 train_time:77287ms step_avg:60.38ms
step:1281/2315 train_time:77348ms step_avg:60.38ms
step:1282/2315 train_time:77409ms step_avg:60.38ms
step:1283/2315 train_time:77470ms step_avg:60.38ms
step:1284/2315 train_time:77530ms step_avg:60.38ms
step:1285/2315 train_time:77593ms step_avg:60.38ms
step:1286/2315 train_time:77654ms step_avg:60.38ms
step:1287/2315 train_time:77716ms step_avg:60.39ms
step:1288/2315 train_time:77777ms step_avg:60.39ms
step:1289/2315 train_time:77838ms step_avg:60.39ms
step:1290/2315 train_time:77899ms step_avg:60.39ms
step:1291/2315 train_time:77960ms step_avg:60.39ms
step:1292/2315 train_time:78020ms step_avg:60.39ms
step:1293/2315 train_time:78081ms step_avg:60.39ms
step:1294/2315 train_time:78141ms step_avg:60.39ms
step:1295/2315 train_time:78202ms step_avg:60.39ms
step:1296/2315 train_time:78262ms step_avg:60.39ms
step:1297/2315 train_time:78323ms step_avg:60.39ms
step:1298/2315 train_time:78383ms step_avg:60.39ms
step:1299/2315 train_time:78444ms step_avg:60.39ms
step:1300/2315 train_time:78505ms step_avg:60.39ms
step:1301/2315 train_time:78566ms step_avg:60.39ms
step:1302/2315 train_time:78628ms step_avg:60.39ms
step:1303/2315 train_time:78689ms step_avg:60.39ms
step:1304/2315 train_time:78750ms step_avg:60.39ms
step:1305/2315 train_time:78811ms step_avg:60.39ms
step:1306/2315 train_time:78871ms step_avg:60.39ms
step:1307/2315 train_time:78932ms step_avg:60.39ms
step:1308/2315 train_time:78993ms step_avg:60.39ms
step:1309/2315 train_time:79054ms step_avg:60.39ms
step:1310/2315 train_time:79114ms step_avg:60.39ms
step:1311/2315 train_time:79175ms step_avg:60.39ms
step:1312/2315 train_time:79235ms step_avg:60.39ms
step:1313/2315 train_time:79296ms step_avg:60.39ms
step:1314/2315 train_time:79357ms step_avg:60.39ms
step:1315/2315 train_time:79418ms step_avg:60.39ms
step:1316/2315 train_time:79479ms step_avg:60.39ms
step:1317/2315 train_time:79541ms step_avg:60.40ms
step:1318/2315 train_time:79601ms step_avg:60.40ms
step:1319/2315 train_time:79661ms step_avg:60.40ms
step:1320/2315 train_time:79722ms step_avg:60.40ms
step:1321/2315 train_time:79784ms step_avg:60.40ms
step:1322/2315 train_time:79844ms step_avg:60.40ms
step:1323/2315 train_time:79905ms step_avg:60.40ms
step:1324/2315 train_time:79966ms step_avg:60.40ms
step:1325/2315 train_time:80027ms step_avg:60.40ms
step:1326/2315 train_time:80087ms step_avg:60.40ms
step:1327/2315 train_time:80148ms step_avg:60.40ms
step:1328/2315 train_time:80208ms step_avg:60.40ms
step:1329/2315 train_time:80269ms step_avg:60.40ms
step:1330/2315 train_time:80329ms step_avg:60.40ms
step:1331/2315 train_time:80391ms step_avg:60.40ms
step:1332/2315 train_time:80451ms step_avg:60.40ms
step:1333/2315 train_time:80513ms step_avg:60.40ms
step:1334/2315 train_time:80574ms step_avg:60.40ms
step:1335/2315 train_time:80634ms step_avg:60.40ms
step:1336/2315 train_time:80695ms step_avg:60.40ms
step:1337/2315 train_time:80756ms step_avg:60.40ms
step:1338/2315 train_time:80816ms step_avg:60.40ms
step:1339/2315 train_time:80877ms step_avg:60.40ms
step:1340/2315 train_time:80937ms step_avg:60.40ms
step:1341/2315 train_time:80999ms step_avg:60.40ms
step:1342/2315 train_time:81059ms step_avg:60.40ms
step:1343/2315 train_time:81120ms step_avg:60.40ms
step:1344/2315 train_time:81180ms step_avg:60.40ms
step:1345/2315 train_time:81240ms step_avg:60.40ms
step:1346/2315 train_time:81301ms step_avg:60.40ms
step:1347/2315 train_time:81362ms step_avg:60.40ms
step:1348/2315 train_time:81424ms step_avg:60.40ms
step:1349/2315 train_time:81485ms step_avg:60.40ms
step:1350/2315 train_time:81545ms step_avg:60.40ms
step:1351/2315 train_time:81606ms step_avg:60.40ms
step:1352/2315 train_time:81667ms step_avg:60.40ms
step:1353/2315 train_time:81728ms step_avg:60.40ms
step:1354/2315 train_time:81789ms step_avg:60.41ms
step:1355/2315 train_time:81850ms step_avg:60.41ms
step:1356/2315 train_time:81910ms step_avg:60.41ms
step:1357/2315 train_time:81971ms step_avg:60.41ms
step:1358/2315 train_time:82032ms step_avg:60.41ms
step:1359/2315 train_time:82093ms step_avg:60.41ms
step:1360/2315 train_time:82154ms step_avg:60.41ms
step:1361/2315 train_time:82215ms step_avg:60.41ms
step:1362/2315 train_time:82275ms step_avg:60.41ms
step:1363/2315 train_time:82336ms step_avg:60.41ms
step:1364/2315 train_time:82397ms step_avg:60.41ms
step:1365/2315 train_time:82458ms step_avg:60.41ms
step:1366/2315 train_time:82518ms step_avg:60.41ms
step:1367/2315 train_time:82580ms step_avg:60.41ms
step:1368/2315 train_time:82640ms step_avg:60.41ms
step:1369/2315 train_time:82701ms step_avg:60.41ms
step:1370/2315 train_time:82762ms step_avg:60.41ms
step:1371/2315 train_time:82823ms step_avg:60.41ms
step:1372/2315 train_time:82884ms step_avg:60.41ms
step:1373/2315 train_time:82944ms step_avg:60.41ms
step:1374/2315 train_time:83005ms step_avg:60.41ms
step:1375/2315 train_time:83065ms step_avg:60.41ms
step:1376/2315 train_time:83127ms step_avg:60.41ms
step:1377/2315 train_time:83188ms step_avg:60.41ms
step:1378/2315 train_time:83248ms step_avg:60.41ms
step:1379/2315 train_time:83309ms step_avg:60.41ms
step:1380/2315 train_time:83370ms step_avg:60.41ms
step:1381/2315 train_time:83431ms step_avg:60.41ms
step:1382/2315 train_time:83492ms step_avg:60.41ms
step:1383/2315 train_time:83553ms step_avg:60.41ms
step:1384/2315 train_time:83614ms step_avg:60.41ms
step:1385/2315 train_time:83675ms step_avg:60.42ms
step:1386/2315 train_time:83736ms step_avg:60.42ms
step:1387/2315 train_time:83797ms step_avg:60.42ms
step:1388/2315 train_time:83858ms step_avg:60.42ms
step:1389/2315 train_time:83919ms step_avg:60.42ms
step:1390/2315 train_time:83979ms step_avg:60.42ms
step:1391/2315 train_time:84041ms step_avg:60.42ms
step:1392/2315 train_time:84101ms step_avg:60.42ms
step:1393/2315 train_time:84162ms step_avg:60.42ms
step:1394/2315 train_time:84223ms step_avg:60.42ms
step:1395/2315 train_time:84284ms step_avg:60.42ms
step:1396/2315 train_time:84344ms step_avg:60.42ms
step:1397/2315 train_time:84405ms step_avg:60.42ms
step:1398/2315 train_time:84466ms step_avg:60.42ms
step:1399/2315 train_time:84527ms step_avg:60.42ms
step:1400/2315 train_time:84588ms step_avg:60.42ms
step:1401/2315 train_time:84649ms step_avg:60.42ms
step:1402/2315 train_time:84710ms step_avg:60.42ms
step:1403/2315 train_time:84771ms step_avg:60.42ms
step:1404/2315 train_time:84832ms step_avg:60.42ms
step:1405/2315 train_time:84893ms step_avg:60.42ms
step:1406/2315 train_time:84953ms step_avg:60.42ms
step:1407/2315 train_time:85014ms step_avg:60.42ms
step:1408/2315 train_time:85074ms step_avg:60.42ms
step:1409/2315 train_time:85135ms step_avg:60.42ms
step:1410/2315 train_time:85197ms step_avg:60.42ms
step:1411/2315 train_time:85258ms step_avg:60.42ms
step:1412/2315 train_time:85318ms step_avg:60.42ms
step:1413/2315 train_time:85379ms step_avg:60.42ms
step:1414/2315 train_time:85440ms step_avg:60.42ms
step:1415/2315 train_time:85501ms step_avg:60.42ms
step:1416/2315 train_time:85561ms step_avg:60.42ms
step:1417/2315 train_time:85622ms step_avg:60.43ms
step:1418/2315 train_time:85683ms step_avg:60.43ms
step:1419/2315 train_time:85743ms step_avg:60.43ms
step:1420/2315 train_time:85804ms step_avg:60.43ms
step:1421/2315 train_time:85865ms step_avg:60.43ms
step:1422/2315 train_time:85926ms step_avg:60.43ms
step:1423/2315 train_time:85987ms step_avg:60.43ms
step:1424/2315 train_time:86048ms step_avg:60.43ms
step:1425/2315 train_time:86108ms step_avg:60.43ms
step:1426/2315 train_time:86169ms step_avg:60.43ms
step:1427/2315 train_time:86230ms step_avg:60.43ms
step:1428/2315 train_time:86291ms step_avg:60.43ms
step:1429/2315 train_time:86352ms step_avg:60.43ms
step:1430/2315 train_time:86412ms step_avg:60.43ms
step:1431/2315 train_time:86473ms step_avg:60.43ms
step:1432/2315 train_time:86534ms step_avg:60.43ms
step:1433/2315 train_time:86595ms step_avg:60.43ms
step:1434/2315 train_time:86656ms step_avg:60.43ms
step:1435/2315 train_time:86717ms step_avg:60.43ms
step:1436/2315 train_time:86778ms step_avg:60.43ms
step:1437/2315 train_time:86840ms step_avg:60.43ms
step:1438/2315 train_time:86900ms step_avg:60.43ms
step:1439/2315 train_time:86961ms step_avg:60.43ms
step:1440/2315 train_time:87022ms step_avg:60.43ms
step:1441/2315 train_time:87082ms step_avg:60.43ms
step:1442/2315 train_time:87143ms step_avg:60.43ms
step:1443/2315 train_time:87204ms step_avg:60.43ms
step:1444/2315 train_time:87265ms step_avg:60.43ms
step:1445/2315 train_time:87326ms step_avg:60.43ms
step:1446/2315 train_time:87386ms step_avg:60.43ms
step:1447/2315 train_time:87447ms step_avg:60.43ms
step:1448/2315 train_time:87507ms step_avg:60.43ms
step:1449/2315 train_time:87568ms step_avg:60.43ms
step:1450/2315 train_time:87628ms step_avg:60.43ms
step:1451/2315 train_time:87690ms step_avg:60.43ms
step:1452/2315 train_time:87751ms step_avg:60.43ms
step:1453/2315 train_time:87812ms step_avg:60.43ms
step:1454/2315 train_time:87873ms step_avg:60.44ms
step:1455/2315 train_time:87934ms step_avg:60.44ms
step:1456/2315 train_time:87995ms step_avg:60.44ms
step:1457/2315 train_time:88055ms step_avg:60.44ms
step:1458/2315 train_time:88116ms step_avg:60.44ms
step:1459/2315 train_time:88177ms step_avg:60.44ms
step:1460/2315 train_time:88238ms step_avg:60.44ms
step:1461/2315 train_time:88299ms step_avg:60.44ms
step:1462/2315 train_time:88359ms step_avg:60.44ms
step:1463/2315 train_time:88420ms step_avg:60.44ms
step:1464/2315 train_time:88480ms step_avg:60.44ms
step:1465/2315 train_time:88541ms step_avg:60.44ms
step:1466/2315 train_time:88602ms step_avg:60.44ms
step:1467/2315 train_time:88663ms step_avg:60.44ms
step:1468/2315 train_time:88724ms step_avg:60.44ms
step:1469/2315 train_time:88785ms step_avg:60.44ms
step:1470/2315 train_time:88845ms step_avg:60.44ms
step:1471/2315 train_time:88906ms step_avg:60.44ms
step:1472/2315 train_time:88967ms step_avg:60.44ms
step:1473/2315 train_time:89028ms step_avg:60.44ms
step:1474/2315 train_time:89089ms step_avg:60.44ms
step:1475/2315 train_time:89150ms step_avg:60.44ms
step:1476/2315 train_time:89211ms step_avg:60.44ms
step:1477/2315 train_time:89272ms step_avg:60.44ms
step:1478/2315 train_time:89333ms step_avg:60.44ms
step:1479/2315 train_time:89394ms step_avg:60.44ms
step:1480/2315 train_time:89454ms step_avg:60.44ms
step:1481/2315 train_time:89515ms step_avg:60.44ms
step:1482/2315 train_time:89576ms step_avg:60.44ms
step:1483/2315 train_time:89637ms step_avg:60.44ms
step:1484/2315 train_time:89698ms step_avg:60.44ms
step:1485/2315 train_time:89759ms step_avg:60.44ms
step:1486/2315 train_time:89820ms step_avg:60.44ms
step:1487/2315 train_time:89881ms step_avg:60.44ms
step:1488/2315 train_time:89941ms step_avg:60.44ms
step:1489/2315 train_time:90002ms step_avg:60.44ms
step:1490/2315 train_time:90062ms step_avg:60.44ms
step:1491/2315 train_time:90123ms step_avg:60.44ms
step:1492/2315 train_time:90184ms step_avg:60.44ms
step:1493/2315 train_time:90245ms step_avg:60.45ms
step:1494/2315 train_time:90305ms step_avg:60.45ms
step:1495/2315 train_time:90367ms step_avg:60.45ms
step:1496/2315 train_time:90427ms step_avg:60.45ms
step:1497/2315 train_time:90488ms step_avg:60.45ms
step:1498/2315 train_time:90549ms step_avg:60.45ms
step:1499/2315 train_time:90610ms step_avg:60.45ms
step:1500/2315 train_time:90670ms step_avg:60.45ms
step:1500/2315 val_loss:3.4492 train_time:90733ms step_avg:60.49ms
step:1501/2315 train_time:90752ms step_avg:60.46ms
step:1502/2315 train_time:90793ms step_avg:60.45ms
step:1503/2315 train_time:90858ms step_avg:60.45ms
step:1504/2315 train_time:90923ms step_avg:60.45ms
step:1505/2315 train_time:90986ms step_avg:60.46ms
step:1506/2315 train_time:91046ms step_avg:60.46ms
step:1507/2315 train_time:91107ms step_avg:60.46ms
step:1508/2315 train_time:91167ms step_avg:60.46ms
step:1509/2315 train_time:91228ms step_avg:60.46ms
step:1510/2315 train_time:91288ms step_avg:60.46ms
step:1511/2315 train_time:91347ms step_avg:60.45ms
step:1512/2315 train_time:91407ms step_avg:60.45ms
step:1513/2315 train_time:91468ms step_avg:60.45ms
step:1514/2315 train_time:91528ms step_avg:60.45ms
step:1515/2315 train_time:91587ms step_avg:60.45ms
step:1516/2315 train_time:91647ms step_avg:60.45ms
step:1517/2315 train_time:91709ms step_avg:60.45ms
step:1518/2315 train_time:91770ms step_avg:60.45ms
step:1519/2315 train_time:91834ms step_avg:60.46ms
step:1520/2315 train_time:91896ms step_avg:60.46ms
step:1521/2315 train_time:91959ms step_avg:60.46ms
step:1522/2315 train_time:92020ms step_avg:60.46ms
step:1523/2315 train_time:92082ms step_avg:60.46ms
step:1524/2315 train_time:92143ms step_avg:60.46ms
step:1525/2315 train_time:92204ms step_avg:60.46ms
step:1526/2315 train_time:92264ms step_avg:60.46ms
step:1527/2315 train_time:92326ms step_avg:60.46ms
step:1528/2315 train_time:92386ms step_avg:60.46ms
step:1529/2315 train_time:92446ms step_avg:60.46ms
step:1530/2315 train_time:92507ms step_avg:60.46ms
step:1531/2315 train_time:92568ms step_avg:60.46ms
step:1532/2315 train_time:92629ms step_avg:60.46ms
step:1533/2315 train_time:92691ms step_avg:60.46ms
step:1534/2315 train_time:92752ms step_avg:60.46ms
step:1535/2315 train_time:92814ms step_avg:60.47ms
step:1536/2315 train_time:92876ms step_avg:60.47ms
step:1537/2315 train_time:92937ms step_avg:60.47ms
step:1538/2315 train_time:92998ms step_avg:60.47ms
step:1539/2315 train_time:93059ms step_avg:60.47ms
step:1540/2315 train_time:93121ms step_avg:60.47ms
step:1541/2315 train_time:93183ms step_avg:60.47ms
step:1542/2315 train_time:93244ms step_avg:60.47ms
step:1543/2315 train_time:93305ms step_avg:60.47ms
step:1544/2315 train_time:93366ms step_avg:60.47ms
step:1545/2315 train_time:93427ms step_avg:60.47ms
step:1546/2315 train_time:93487ms step_avg:60.47ms
step:1547/2315 train_time:93549ms step_avg:60.47ms
step:1548/2315 train_time:93610ms step_avg:60.47ms
step:1549/2315 train_time:93671ms step_avg:60.47ms
step:1550/2315 train_time:93732ms step_avg:60.47ms
step:1551/2315 train_time:93793ms step_avg:60.47ms
step:1552/2315 train_time:93854ms step_avg:60.47ms
step:1553/2315 train_time:93917ms step_avg:60.47ms
step:1554/2315 train_time:93978ms step_avg:60.47ms
step:1555/2315 train_time:94040ms step_avg:60.48ms
step:1556/2315 train_time:94101ms step_avg:60.48ms
step:1557/2315 train_time:94163ms step_avg:60.48ms
step:1558/2315 train_time:94224ms step_avg:60.48ms
step:1559/2315 train_time:94285ms step_avg:60.48ms
step:1560/2315 train_time:94346ms step_avg:60.48ms
step:1561/2315 train_time:94407ms step_avg:60.48ms
step:1562/2315 train_time:94468ms step_avg:60.48ms
step:1563/2315 train_time:94529ms step_avg:60.48ms
step:1564/2315 train_time:94590ms step_avg:60.48ms
step:1565/2315 train_time:94652ms step_avg:60.48ms
step:1566/2315 train_time:94713ms step_avg:60.48ms
step:1567/2315 train_time:94774ms step_avg:60.48ms
step:1568/2315 train_time:94835ms step_avg:60.48ms
step:1569/2315 train_time:94896ms step_avg:60.48ms
step:1570/2315 train_time:94957ms step_avg:60.48ms
step:1571/2315 train_time:95019ms step_avg:60.48ms
step:1572/2315 train_time:95080ms step_avg:60.48ms
step:1573/2315 train_time:95141ms step_avg:60.48ms
step:1574/2315 train_time:95203ms step_avg:60.48ms
step:1575/2315 train_time:95265ms step_avg:60.49ms
step:1576/2315 train_time:95325ms step_avg:60.49ms
step:1577/2315 train_time:95386ms step_avg:60.49ms
step:1578/2315 train_time:95447ms step_avg:60.49ms
step:1579/2315 train_time:95508ms step_avg:60.49ms
step:1580/2315 train_time:95569ms step_avg:60.49ms
step:1581/2315 train_time:95630ms step_avg:60.49ms
step:1582/2315 train_time:95691ms step_avg:60.49ms
step:1583/2315 train_time:95752ms step_avg:60.49ms
step:1584/2315 train_time:95814ms step_avg:60.49ms
step:1585/2315 train_time:95874ms step_avg:60.49ms
step:1586/2315 train_time:95936ms step_avg:60.49ms
step:1587/2315 train_time:95997ms step_avg:60.49ms
step:1588/2315 train_time:96058ms step_avg:60.49ms
step:1589/2315 train_time:96119ms step_avg:60.49ms
step:1590/2315 train_time:96180ms step_avg:60.49ms
step:1591/2315 train_time:96241ms step_avg:60.49ms
step:1592/2315 train_time:96302ms step_avg:60.49ms
step:1593/2315 train_time:96364ms step_avg:60.49ms
step:1594/2315 train_time:96425ms step_avg:60.49ms
step:1595/2315 train_time:96486ms step_avg:60.49ms
step:1596/2315 train_time:96547ms step_avg:60.49ms
step:1597/2315 train_time:96608ms step_avg:60.49ms
step:1598/2315 train_time:96669ms step_avg:60.49ms
step:1599/2315 train_time:96730ms step_avg:60.49ms
step:1600/2315 train_time:96791ms step_avg:60.49ms
step:1601/2315 train_time:96853ms step_avg:60.50ms
step:1602/2315 train_time:96915ms step_avg:60.50ms
step:1603/2315 train_time:96975ms step_avg:60.50ms
step:1604/2315 train_time:97037ms step_avg:60.50ms
step:1605/2315 train_time:97099ms step_avg:60.50ms
step:1606/2315 train_time:97160ms step_avg:60.50ms
step:1607/2315 train_time:97220ms step_avg:60.50ms
step:1608/2315 train_time:97282ms step_avg:60.50ms
step:1609/2315 train_time:97344ms step_avg:60.50ms
step:1610/2315 train_time:97405ms step_avg:60.50ms
step:1611/2315 train_time:97466ms step_avg:60.50ms
step:1612/2315 train_time:97526ms step_avg:60.50ms
step:1613/2315 train_time:97588ms step_avg:60.50ms
step:1614/2315 train_time:97649ms step_avg:60.50ms
step:1615/2315 train_time:97711ms step_avg:60.50ms
step:1616/2315 train_time:97772ms step_avg:60.50ms
step:1617/2315 train_time:97834ms step_avg:60.50ms
step:1618/2315 train_time:97895ms step_avg:60.50ms
step:1619/2315 train_time:97956ms step_avg:60.50ms
step:1620/2315 train_time:98017ms step_avg:60.50ms
step:1621/2315 train_time:98078ms step_avg:60.50ms
step:1622/2315 train_time:98139ms step_avg:60.50ms
step:1623/2315 train_time:98200ms step_avg:60.51ms
step:1624/2315 train_time:98261ms step_avg:60.51ms
step:1625/2315 train_time:98323ms step_avg:60.51ms
step:1626/2315 train_time:98384ms step_avg:60.51ms
step:1627/2315 train_time:98444ms step_avg:60.51ms
step:1628/2315 train_time:98506ms step_avg:60.51ms
step:1629/2315 train_time:98567ms step_avg:60.51ms
step:1630/2315 train_time:98627ms step_avg:60.51ms
step:1631/2315 train_time:98689ms step_avg:60.51ms
step:1632/2315 train_time:98750ms step_avg:60.51ms
step:1633/2315 train_time:98812ms step_avg:60.51ms
step:1634/2315 train_time:98872ms step_avg:60.51ms
step:1635/2315 train_time:98934ms step_avg:60.51ms
step:1636/2315 train_time:98995ms step_avg:60.51ms
step:1637/2315 train_time:99057ms step_avg:60.51ms
step:1638/2315 train_time:99117ms step_avg:60.51ms
step:1639/2315 train_time:99178ms step_avg:60.51ms
step:1640/2315 train_time:99239ms step_avg:60.51ms
step:1641/2315 train_time:99301ms step_avg:60.51ms
step:1642/2315 train_time:99362ms step_avg:60.51ms
step:1643/2315 train_time:99424ms step_avg:60.51ms
step:1644/2315 train_time:99485ms step_avg:60.51ms
step:1645/2315 train_time:99547ms step_avg:60.51ms
step:1646/2315 train_time:99608ms step_avg:60.52ms
step:1647/2315 train_time:99669ms step_avg:60.52ms
step:1648/2315 train_time:99730ms step_avg:60.52ms
step:1649/2315 train_time:99791ms step_avg:60.52ms
step:1650/2315 train_time:99852ms step_avg:60.52ms
step:1651/2315 train_time:99914ms step_avg:60.52ms
step:1652/2315 train_time:99975ms step_avg:60.52ms
step:1653/2315 train_time:100036ms step_avg:60.52ms
step:1654/2315 train_time:100097ms step_avg:60.52ms
step:1655/2315 train_time:100159ms step_avg:60.52ms
step:1656/2315 train_time:100220ms step_avg:60.52ms
step:1657/2315 train_time:100280ms step_avg:60.52ms
step:1658/2315 train_time:100341ms step_avg:60.52ms
step:1659/2315 train_time:100403ms step_avg:60.52ms
step:1660/2315 train_time:100464ms step_avg:60.52ms
step:1661/2315 train_time:100525ms step_avg:60.52ms
step:1662/2315 train_time:100586ms step_avg:60.52ms
step:1663/2315 train_time:100648ms step_avg:60.52ms
step:1664/2315 train_time:100710ms step_avg:60.52ms
step:1665/2315 train_time:100771ms step_avg:60.52ms
step:1666/2315 train_time:100832ms step_avg:60.52ms
step:1667/2315 train_time:100894ms step_avg:60.52ms
step:1668/2315 train_time:100954ms step_avg:60.52ms
step:1669/2315 train_time:101015ms step_avg:60.52ms
step:1670/2315 train_time:101077ms step_avg:60.53ms
step:1671/2315 train_time:101138ms step_avg:60.53ms
step:1672/2315 train_time:101199ms step_avg:60.53ms
step:1673/2315 train_time:101260ms step_avg:60.53ms
step:1674/2315 train_time:101321ms step_avg:60.53ms
step:1675/2315 train_time:101383ms step_avg:60.53ms
step:1676/2315 train_time:101444ms step_avg:60.53ms
step:1677/2315 train_time:101505ms step_avg:60.53ms
step:1678/2315 train_time:101566ms step_avg:60.53ms
step:1679/2315 train_time:101628ms step_avg:60.53ms
step:1680/2315 train_time:101689ms step_avg:60.53ms
step:1681/2315 train_time:101750ms step_avg:60.53ms
step:1682/2315 train_time:101812ms step_avg:60.53ms
step:1683/2315 train_time:101873ms step_avg:60.53ms
step:1684/2315 train_time:101934ms step_avg:60.53ms
step:1685/2315 train_time:101996ms step_avg:60.53ms
step:1686/2315 train_time:102057ms step_avg:60.53ms
step:1687/2315 train_time:102118ms step_avg:60.53ms
step:1688/2315 train_time:102179ms step_avg:60.53ms
step:1689/2315 train_time:102240ms step_avg:60.53ms
step:1690/2315 train_time:102301ms step_avg:60.53ms
step:1691/2315 train_time:102363ms step_avg:60.53ms
step:1692/2315 train_time:102424ms step_avg:60.53ms
step:1693/2315 train_time:102485ms step_avg:60.53ms
step:1694/2315 train_time:102545ms step_avg:60.53ms
step:1695/2315 train_time:102607ms step_avg:60.54ms
step:1696/2315 train_time:102668ms step_avg:60.54ms
step:1697/2315 train_time:102729ms step_avg:60.54ms
step:1698/2315 train_time:102791ms step_avg:60.54ms
step:1699/2315 train_time:102853ms step_avg:60.54ms
step:1700/2315 train_time:102913ms step_avg:60.54ms
step:1701/2315 train_time:102975ms step_avg:60.54ms
step:1702/2315 train_time:103036ms step_avg:60.54ms
step:1703/2315 train_time:103097ms step_avg:60.54ms
step:1704/2315 train_time:103157ms step_avg:60.54ms
step:1705/2315 train_time:103218ms step_avg:60.54ms
step:1706/2315 train_time:103279ms step_avg:60.54ms
step:1707/2315 train_time:103341ms step_avg:60.54ms
step:1708/2315 train_time:103402ms step_avg:60.54ms
step:1709/2315 train_time:103464ms step_avg:60.54ms
step:1710/2315 train_time:103525ms step_avg:60.54ms
step:1711/2315 train_time:103586ms step_avg:60.54ms
step:1712/2315 train_time:103647ms step_avg:60.54ms
step:1713/2315 train_time:103708ms step_avg:60.54ms
step:1714/2315 train_time:103770ms step_avg:60.54ms
step:1715/2315 train_time:103832ms step_avg:60.54ms
step:1716/2315 train_time:103893ms step_avg:60.54ms
step:1717/2315 train_time:103954ms step_avg:60.54ms
step:1718/2315 train_time:104015ms step_avg:60.54ms
step:1719/2315 train_time:104076ms step_avg:60.54ms
step:1720/2315 train_time:104137ms step_avg:60.54ms
step:1721/2315 train_time:104198ms step_avg:60.55ms
step:1722/2315 train_time:104259ms step_avg:60.55ms
step:1723/2315 train_time:104320ms step_avg:60.55ms
step:1724/2315 train_time:104381ms step_avg:60.55ms
step:1725/2315 train_time:104443ms step_avg:60.55ms
step:1726/2315 train_time:104505ms step_avg:60.55ms
step:1727/2315 train_time:104567ms step_avg:60.55ms
step:1728/2315 train_time:104627ms step_avg:60.55ms
step:1729/2315 train_time:104689ms step_avg:60.55ms
step:1730/2315 train_time:104750ms step_avg:60.55ms
step:1731/2315 train_time:104812ms step_avg:60.55ms
step:1732/2315 train_time:104872ms step_avg:60.55ms
step:1733/2315 train_time:104934ms step_avg:60.55ms
step:1734/2315 train_time:104995ms step_avg:60.55ms
step:1735/2315 train_time:105056ms step_avg:60.55ms
step:1736/2315 train_time:105117ms step_avg:60.55ms
step:1737/2315 train_time:105178ms step_avg:60.55ms
step:1738/2315 train_time:105239ms step_avg:60.55ms
step:1739/2315 train_time:105300ms step_avg:60.55ms
step:1740/2315 train_time:105361ms step_avg:60.55ms
step:1741/2315 train_time:105423ms step_avg:60.55ms
step:1742/2315 train_time:105485ms step_avg:60.55ms
step:1743/2315 train_time:105546ms step_avg:60.55ms
step:1744/2315 train_time:105607ms step_avg:60.55ms
step:1745/2315 train_time:105668ms step_avg:60.55ms
step:1746/2315 train_time:105729ms step_avg:60.55ms
step:1747/2315 train_time:105791ms step_avg:60.56ms
step:1748/2315 train_time:105852ms step_avg:60.56ms
step:1749/2315 train_time:105914ms step_avg:60.56ms
step:1750/2315 train_time:105974ms step_avg:60.56ms
step:1750/2315 val_loss:3.3790 train_time:106037ms step_avg:60.59ms
step:1751/2315 train_time:106058ms step_avg:60.57ms
step:1752/2315 train_time:106099ms step_avg:60.56ms
step:1753/2315 train_time:106166ms step_avg:60.56ms
step:1754/2315 train_time:106231ms step_avg:60.56ms
step:1755/2315 train_time:106293ms step_avg:60.57ms
step:1756/2315 train_time:106353ms step_avg:60.57ms
step:1757/2315 train_time:106413ms step_avg:60.57ms
step:1758/2315 train_time:106474ms step_avg:60.57ms
step:1759/2315 train_time:106534ms step_avg:60.57ms
step:1760/2315 train_time:106594ms step_avg:60.56ms
step:1761/2315 train_time:106655ms step_avg:60.56ms
step:1762/2315 train_time:106715ms step_avg:60.56ms
step:1763/2315 train_time:106776ms step_avg:60.57ms
step:1764/2315 train_time:106836ms step_avg:60.56ms
step:1765/2315 train_time:106896ms step_avg:60.56ms
step:1766/2315 train_time:106959ms step_avg:60.57ms
step:1767/2315 train_time:107022ms step_avg:60.57ms
step:1768/2315 train_time:107084ms step_avg:60.57ms
step:1769/2315 train_time:107146ms step_avg:60.57ms
step:1770/2315 train_time:107207ms step_avg:60.57ms
step:1771/2315 train_time:107269ms step_avg:60.57ms
step:1772/2315 train_time:107330ms step_avg:60.57ms
step:1773/2315 train_time:107391ms step_avg:60.57ms
step:1774/2315 train_time:107452ms step_avg:60.57ms
step:1775/2315 train_time:107513ms step_avg:60.57ms
step:1776/2315 train_time:107574ms step_avg:60.57ms
step:1777/2315 train_time:107634ms step_avg:60.57ms
step:1778/2315 train_time:107694ms step_avg:60.57ms
step:1779/2315 train_time:107755ms step_avg:60.57ms
step:1780/2315 train_time:107816ms step_avg:60.57ms
step:1781/2315 train_time:107878ms step_avg:60.57ms
step:1782/2315 train_time:107939ms step_avg:60.57ms
step:1783/2315 train_time:108001ms step_avg:60.57ms
step:1784/2315 train_time:108063ms step_avg:60.57ms
step:1785/2315 train_time:108125ms step_avg:60.57ms
step:1786/2315 train_time:108186ms step_avg:60.57ms
step:1787/2315 train_time:108247ms step_avg:60.57ms
step:1788/2315 train_time:108308ms step_avg:60.57ms
step:1789/2315 train_time:108369ms step_avg:60.58ms
step:1790/2315 train_time:108429ms step_avg:60.58ms
step:1791/2315 train_time:108490ms step_avg:60.58ms
step:1792/2315 train_time:108551ms step_avg:60.58ms
step:1793/2315 train_time:108612ms step_avg:60.58ms
step:1794/2315 train_time:108673ms step_avg:60.58ms
step:1795/2315 train_time:108733ms step_avg:60.58ms
step:1796/2315 train_time:108794ms step_avg:60.58ms
step:1797/2315 train_time:108854ms step_avg:60.58ms
step:1798/2315 train_time:108915ms step_avg:60.58ms
step:1799/2315 train_time:108977ms step_avg:60.58ms
step:1800/2315 train_time:109039ms step_avg:60.58ms
step:1801/2315 train_time:109101ms step_avg:60.58ms
step:1802/2315 train_time:109162ms step_avg:60.58ms
step:1803/2315 train_time:109223ms step_avg:60.58ms
step:1804/2315 train_time:109285ms step_avg:60.58ms
step:1805/2315 train_time:109346ms step_avg:60.58ms
step:1806/2315 train_time:109408ms step_avg:60.58ms
step:1807/2315 train_time:109469ms step_avg:60.58ms
step:1808/2315 train_time:109530ms step_avg:60.58ms
step:1809/2315 train_time:109590ms step_avg:60.58ms
step:1810/2315 train_time:109651ms step_avg:60.58ms
step:1811/2315 train_time:109712ms step_avg:60.58ms
step:1812/2315 train_time:109773ms step_avg:60.58ms
step:1813/2315 train_time:109834ms step_avg:60.58ms
step:1814/2315 train_time:109895ms step_avg:60.58ms
step:1815/2315 train_time:109956ms step_avg:60.58ms
step:1816/2315 train_time:110017ms step_avg:60.58ms
step:1817/2315 train_time:110080ms step_avg:60.58ms
step:1818/2315 train_time:110141ms step_avg:60.58ms
step:1819/2315 train_time:110203ms step_avg:60.58ms
step:1820/2315 train_time:110263ms step_avg:60.58ms
step:1821/2315 train_time:110325ms step_avg:60.58ms
step:1822/2315 train_time:110386ms step_avg:60.59ms
step:1823/2315 train_time:110448ms step_avg:60.59ms
step:1824/2315 train_time:110509ms step_avg:60.59ms
step:1825/2315 train_time:110570ms step_avg:60.59ms
step:1826/2315 train_time:110631ms step_avg:60.59ms
step:1827/2315 train_time:110692ms step_avg:60.59ms
step:1828/2315 train_time:110753ms step_avg:60.59ms
step:1829/2315 train_time:110814ms step_avg:60.59ms
step:1830/2315 train_time:110875ms step_avg:60.59ms
step:1831/2315 train_time:110936ms step_avg:60.59ms
step:1832/2315 train_time:110997ms step_avg:60.59ms
step:1833/2315 train_time:111059ms step_avg:60.59ms
step:1834/2315 train_time:111120ms step_avg:60.59ms
step:1835/2315 train_time:111182ms step_avg:60.59ms
step:1836/2315 train_time:111242ms step_avg:60.59ms
step:1837/2315 train_time:111304ms step_avg:60.59ms
step:1838/2315 train_time:111365ms step_avg:60.59ms
step:1839/2315 train_time:111426ms step_avg:60.59ms
step:1840/2315 train_time:111488ms step_avg:60.59ms
step:1841/2315 train_time:111549ms step_avg:60.59ms
step:1842/2315 train_time:111610ms step_avg:60.59ms
step:1843/2315 train_time:111671ms step_avg:60.59ms
step:1844/2315 train_time:111732ms step_avg:60.59ms
step:1845/2315 train_time:111792ms step_avg:60.59ms
step:1846/2315 train_time:111853ms step_avg:60.59ms
step:1847/2315 train_time:111915ms step_avg:60.59ms
step:1848/2315 train_time:111976ms step_avg:60.59ms
step:1849/2315 train_time:112038ms step_avg:60.59ms
step:1850/2315 train_time:112099ms step_avg:60.59ms
step:1851/2315 train_time:112161ms step_avg:60.59ms
step:1852/2315 train_time:112222ms step_avg:60.59ms
step:1853/2315 train_time:112283ms step_avg:60.60ms
step:1854/2315 train_time:112344ms step_avg:60.60ms
step:1855/2315 train_time:112405ms step_avg:60.60ms
step:1856/2315 train_time:112466ms step_avg:60.60ms
step:1857/2315 train_time:112527ms step_avg:60.60ms
step:1858/2315 train_time:112588ms step_avg:60.60ms
step:1859/2315 train_time:112649ms step_avg:60.60ms
step:1860/2315 train_time:112710ms step_avg:60.60ms
step:1861/2315 train_time:112770ms step_avg:60.60ms
step:1862/2315 train_time:112831ms step_avg:60.60ms
step:1863/2315 train_time:112892ms step_avg:60.60ms
step:1864/2315 train_time:112953ms step_avg:60.60ms
step:1865/2315 train_time:113015ms step_avg:60.60ms
step:1866/2315 train_time:113076ms step_avg:60.60ms
step:1867/2315 train_time:113138ms step_avg:60.60ms
step:1868/2315 train_time:113200ms step_avg:60.60ms
step:1869/2315 train_time:113261ms step_avg:60.60ms
step:1870/2315 train_time:113322ms step_avg:60.60ms
step:1871/2315 train_time:113383ms step_avg:60.60ms
step:1872/2315 train_time:113444ms step_avg:60.60ms
step:1873/2315 train_time:113506ms step_avg:60.60ms
step:1874/2315 train_time:113567ms step_avg:60.60ms
step:1875/2315 train_time:113629ms step_avg:60.60ms
step:1876/2315 train_time:113690ms step_avg:60.60ms
step:1877/2315 train_time:113751ms step_avg:60.60ms
step:1878/2315 train_time:113812ms step_avg:60.60ms
step:1879/2315 train_time:113873ms step_avg:60.60ms
step:1880/2315 train_time:113934ms step_avg:60.60ms
step:1881/2315 train_time:113994ms step_avg:60.60ms
step:1882/2315 train_time:114055ms step_avg:60.60ms
step:1883/2315 train_time:114116ms step_avg:60.60ms
step:1884/2315 train_time:114178ms step_avg:60.60ms
step:1885/2315 train_time:114239ms step_avg:60.60ms
step:1886/2315 train_time:114300ms step_avg:60.60ms
step:1887/2315 train_time:114362ms step_avg:60.61ms
step:1888/2315 train_time:114423ms step_avg:60.61ms
step:1889/2315 train_time:114485ms step_avg:60.61ms
step:1890/2315 train_time:114546ms step_avg:60.61ms
step:1891/2315 train_time:114607ms step_avg:60.61ms
step:1892/2315 train_time:114668ms step_avg:60.61ms
step:1893/2315 train_time:114730ms step_avg:60.61ms
step:1894/2315 train_time:114791ms step_avg:60.61ms
step:1895/2315 train_time:114851ms step_avg:60.61ms
step:1896/2315 train_time:114912ms step_avg:60.61ms
step:1897/2315 train_time:114974ms step_avg:60.61ms
step:1898/2315 train_time:115035ms step_avg:60.61ms
step:1899/2315 train_time:115096ms step_avg:60.61ms
step:1900/2315 train_time:115157ms step_avg:60.61ms
step:1901/2315 train_time:115218ms step_avg:60.61ms
step:1902/2315 train_time:115280ms step_avg:60.61ms
step:1903/2315 train_time:115341ms step_avg:60.61ms
step:1904/2315 train_time:115402ms step_avg:60.61ms
step:1905/2315 train_time:115463ms step_avg:60.61ms
step:1906/2315 train_time:115523ms step_avg:60.61ms
step:1907/2315 train_time:115585ms step_avg:60.61ms
step:1908/2315 train_time:115646ms step_avg:60.61ms
step:1909/2315 train_time:115708ms step_avg:60.61ms
step:1910/2315 train_time:115770ms step_avg:60.61ms
step:1911/2315 train_time:115831ms step_avg:60.61ms
step:1912/2315 train_time:115892ms step_avg:60.61ms
step:1913/2315 train_time:115953ms step_avg:60.61ms
step:1914/2315 train_time:116013ms step_avg:60.61ms
step:1915/2315 train_time:116074ms step_avg:60.61ms
step:1916/2315 train_time:116135ms step_avg:60.61ms
step:1917/2315 train_time:116196ms step_avg:60.61ms
step:1918/2315 train_time:116257ms step_avg:60.61ms
step:1919/2315 train_time:116319ms step_avg:60.61ms
step:1920/2315 train_time:116381ms step_avg:60.61ms
step:1921/2315 train_time:116442ms step_avg:60.62ms
step:1922/2315 train_time:116503ms step_avg:60.62ms
step:1923/2315 train_time:116565ms step_avg:60.62ms
step:1924/2315 train_time:116626ms step_avg:60.62ms
step:1925/2315 train_time:116687ms step_avg:60.62ms
step:1926/2315 train_time:116748ms step_avg:60.62ms
step:1927/2315 train_time:116809ms step_avg:60.62ms
step:1928/2315 train_time:116871ms step_avg:60.62ms
step:1929/2315 train_time:116931ms step_avg:60.62ms
step:1930/2315 train_time:116992ms step_avg:60.62ms
step:1931/2315 train_time:117053ms step_avg:60.62ms
step:1932/2315 train_time:117114ms step_avg:60.62ms
step:1933/2315 train_time:117175ms step_avg:60.62ms
step:1934/2315 train_time:117237ms step_avg:60.62ms
step:1935/2315 train_time:117299ms step_avg:60.62ms
step:1936/2315 train_time:117360ms step_avg:60.62ms
step:1937/2315 train_time:117421ms step_avg:60.62ms
step:1938/2315 train_time:117482ms step_avg:60.62ms
step:1939/2315 train_time:117544ms step_avg:60.62ms
step:1940/2315 train_time:117605ms step_avg:60.62ms
step:1941/2315 train_time:117666ms step_avg:60.62ms
step:1942/2315 train_time:117727ms step_avg:60.62ms
step:1943/2315 train_time:117789ms step_avg:60.62ms
step:1944/2315 train_time:117850ms step_avg:60.62ms
step:1945/2315 train_time:117912ms step_avg:60.62ms
step:1946/2315 train_time:117973ms step_avg:60.62ms
step:1947/2315 train_time:118033ms step_avg:60.62ms
step:1948/2315 train_time:118094ms step_avg:60.62ms
step:1949/2315 train_time:118155ms step_avg:60.62ms
step:1950/2315 train_time:118215ms step_avg:60.62ms
step:1951/2315 train_time:118276ms step_avg:60.62ms
step:1952/2315 train_time:118338ms step_avg:60.62ms
step:1953/2315 train_time:118399ms step_avg:60.62ms
step:1954/2315 train_time:118460ms step_avg:60.62ms
step:1955/2315 train_time:118522ms step_avg:60.62ms
step:1956/2315 train_time:118583ms step_avg:60.63ms
step:1957/2315 train_time:118645ms step_avg:60.63ms
step:1958/2315 train_time:118706ms step_avg:60.63ms
step:1959/2315 train_time:118768ms step_avg:60.63ms
step:1960/2315 train_time:118829ms step_avg:60.63ms
step:1961/2315 train_time:118891ms step_avg:60.63ms
step:1962/2315 train_time:118952ms step_avg:60.63ms
step:1963/2315 train_time:119013ms step_avg:60.63ms
step:1964/2315 train_time:119074ms step_avg:60.63ms
step:1965/2315 train_time:119134ms step_avg:60.63ms
step:1966/2315 train_time:119195ms step_avg:60.63ms
step:1967/2315 train_time:119256ms step_avg:60.63ms
step:1968/2315 train_time:119317ms step_avg:60.63ms
step:1969/2315 train_time:119379ms step_avg:60.63ms
step:1970/2315 train_time:119440ms step_avg:60.63ms
step:1971/2315 train_time:119501ms step_avg:60.63ms
step:1972/2315 train_time:119562ms step_avg:60.63ms
step:1973/2315 train_time:119624ms step_avg:60.63ms
step:1974/2315 train_time:119685ms step_avg:60.63ms
step:1975/2315 train_time:119746ms step_avg:60.63ms
step:1976/2315 train_time:119808ms step_avg:60.63ms
step:1977/2315 train_time:119869ms step_avg:60.63ms
step:1978/2315 train_time:119930ms step_avg:60.63ms
step:1979/2315 train_time:119991ms step_avg:60.63ms
step:1980/2315 train_time:120052ms step_avg:60.63ms
step:1981/2315 train_time:120113ms step_avg:60.63ms
step:1982/2315 train_time:120175ms step_avg:60.63ms
step:1983/2315 train_time:120236ms step_avg:60.63ms
step:1984/2315 train_time:120296ms step_avg:60.63ms
step:1985/2315 train_time:120358ms step_avg:60.63ms
step:1986/2315 train_time:120419ms step_avg:60.63ms
step:1987/2315 train_time:120481ms step_avg:60.63ms
step:1988/2315 train_time:120542ms step_avg:60.63ms
step:1989/2315 train_time:120603ms step_avg:60.64ms
step:1990/2315 train_time:120664ms step_avg:60.64ms
step:1991/2315 train_time:120725ms step_avg:60.64ms
step:1992/2315 train_time:120786ms step_avg:60.64ms
step:1993/2315 train_time:120847ms step_avg:60.64ms
step:1994/2315 train_time:120909ms step_avg:60.64ms
step:1995/2315 train_time:120970ms step_avg:60.64ms
step:1996/2315 train_time:121031ms step_avg:60.64ms
step:1997/2315 train_time:121093ms step_avg:60.64ms
step:1998/2315 train_time:121154ms step_avg:60.64ms
step:1999/2315 train_time:121215ms step_avg:60.64ms
step:2000/2315 train_time:121276ms step_avg:60.64ms
step:2000/2315 val_loss:3.3293 train_time:121338ms step_avg:60.67ms
step:2001/2315 train_time:121358ms step_avg:60.65ms
step:2002/2315 train_time:121400ms step_avg:60.64ms
step:2003/2315 train_time:121466ms step_avg:60.64ms
step:2004/2315 train_time:121532ms step_avg:60.64ms
step:2005/2315 train_time:121593ms step_avg:60.64ms
step:2006/2315 train_time:121654ms step_avg:60.64ms
step:2007/2315 train_time:121715ms step_avg:60.65ms
step:2008/2315 train_time:121775ms step_avg:60.64ms
step:2009/2315 train_time:121836ms step_avg:60.64ms
step:2010/2315 train_time:121896ms step_avg:60.64ms
step:2011/2315 train_time:121958ms step_avg:60.65ms
step:2012/2315 train_time:122018ms step_avg:60.65ms
step:2013/2315 train_time:122079ms step_avg:60.65ms
step:2014/2315 train_time:122139ms step_avg:60.65ms
step:2015/2315 train_time:122200ms step_avg:60.65ms
step:2016/2315 train_time:122260ms step_avg:60.65ms
step:2017/2315 train_time:122323ms step_avg:60.65ms
step:2018/2315 train_time:122385ms step_avg:60.65ms
step:2019/2315 train_time:122448ms step_avg:60.65ms
step:2020/2315 train_time:122510ms step_avg:60.65ms
step:2021/2315 train_time:122573ms step_avg:60.65ms
step:2022/2315 train_time:122634ms step_avg:60.65ms
step:2023/2315 train_time:122696ms step_avg:60.65ms
step:2024/2315 train_time:122757ms step_avg:60.65ms
step:2025/2315 train_time:122818ms step_avg:60.65ms
step:2026/2315 train_time:122878ms step_avg:60.65ms
step:2027/2315 train_time:122940ms step_avg:60.65ms
step:2028/2315 train_time:123001ms step_avg:60.65ms
step:2029/2315 train_time:123061ms step_avg:60.65ms
step:2030/2315 train_time:123121ms step_avg:60.65ms
step:2031/2315 train_time:123182ms step_avg:60.65ms
step:2032/2315 train_time:123242ms step_avg:60.65ms
step:2033/2315 train_time:123304ms step_avg:60.65ms
step:2034/2315 train_time:123366ms step_avg:60.65ms
step:2035/2315 train_time:123428ms step_avg:60.65ms
step:2036/2315 train_time:123490ms step_avg:60.65ms
step:2037/2315 train_time:123551ms step_avg:60.65ms
step:2038/2315 train_time:123613ms step_avg:60.65ms
step:2039/2315 train_time:123674ms step_avg:60.65ms
step:2040/2315 train_time:123735ms step_avg:60.65ms
step:2041/2315 train_time:123796ms step_avg:60.65ms
step:2042/2315 train_time:123856ms step_avg:60.65ms
step:2043/2315 train_time:123918ms step_avg:60.65ms
step:2044/2315 train_time:123979ms step_avg:60.65ms
step:2045/2315 train_time:124040ms step_avg:60.66ms
step:2046/2315 train_time:124101ms step_avg:60.66ms
step:2047/2315 train_time:124162ms step_avg:60.66ms
step:2048/2315 train_time:124223ms step_avg:60.66ms
step:2049/2315 train_time:124284ms step_avg:60.66ms
step:2050/2315 train_time:124345ms step_avg:60.66ms
step:2051/2315 train_time:124407ms step_avg:60.66ms
step:2052/2315 train_time:124469ms step_avg:60.66ms
step:2053/2315 train_time:124530ms step_avg:60.66ms
step:2054/2315 train_time:124593ms step_avg:60.66ms
step:2055/2315 train_time:124653ms step_avg:60.66ms
step:2056/2315 train_time:124715ms step_avg:60.66ms
step:2057/2315 train_time:124776ms step_avg:60.66ms
step:2058/2315 train_time:124837ms step_avg:60.66ms
step:2059/2315 train_time:124898ms step_avg:60.66ms
step:2060/2315 train_time:124959ms step_avg:60.66ms
step:2061/2315 train_time:125020ms step_avg:60.66ms
step:2062/2315 train_time:125081ms step_avg:60.66ms
step:2063/2315 train_time:125142ms step_avg:60.66ms
step:2064/2315 train_time:125203ms step_avg:60.66ms
step:2065/2315 train_time:125265ms step_avg:60.66ms
step:2066/2315 train_time:125327ms step_avg:60.66ms
step:2067/2315 train_time:125389ms step_avg:60.66ms
step:2068/2315 train_time:125450ms step_avg:60.66ms
step:2069/2315 train_time:125511ms step_avg:60.66ms
step:2070/2315 train_time:125572ms step_avg:60.66ms
step:2071/2315 train_time:125633ms step_avg:60.66ms
step:2072/2315 train_time:125694ms step_avg:60.66ms
step:2073/2315 train_time:125755ms step_avg:60.66ms
step:2074/2315 train_time:125815ms step_avg:60.66ms
step:2075/2315 train_time:125877ms step_avg:60.66ms
step:2076/2315 train_time:125938ms step_avg:60.66ms
step:2077/2315 train_time:125999ms step_avg:60.66ms
step:2078/2315 train_time:126060ms step_avg:60.66ms
step:2079/2315 train_time:126121ms step_avg:60.66ms
step:2080/2315 train_time:126182ms step_avg:60.66ms
step:2081/2315 train_time:126244ms step_avg:60.66ms
step:2082/2315 train_time:126305ms step_avg:60.67ms
step:2083/2315 train_time:126366ms step_avg:60.67ms
step:2084/2315 train_time:126427ms step_avg:60.67ms
step:2085/2315 train_time:126489ms step_avg:60.67ms
step:2086/2315 train_time:126550ms step_avg:60.67ms
step:2087/2315 train_time:126611ms step_avg:60.67ms
step:2088/2315 train_time:126672ms step_avg:60.67ms
step:2089/2315 train_time:126733ms step_avg:60.67ms
step:2090/2315 train_time:126794ms step_avg:60.67ms
step:2091/2315 train_time:126856ms step_avg:60.67ms
step:2092/2315 train_time:126917ms step_avg:60.67ms
step:2093/2315 train_time:126978ms step_avg:60.67ms
step:2094/2315 train_time:127039ms step_avg:60.67ms
step:2095/2315 train_time:127101ms step_avg:60.67ms
step:2096/2315 train_time:127161ms step_avg:60.67ms
step:2097/2315 train_time:127223ms step_avg:60.67ms
step:2098/2315 train_time:127285ms step_avg:60.67ms
step:2099/2315 train_time:127346ms step_avg:60.67ms
step:2100/2315 train_time:127408ms step_avg:60.67ms
step:2101/2315 train_time:127469ms step_avg:60.67ms
step:2102/2315 train_time:127529ms step_avg:60.67ms
step:2103/2315 train_time:127590ms step_avg:60.67ms
step:2104/2315 train_time:127651ms step_avg:60.67ms
step:2105/2315 train_time:127712ms step_avg:60.67ms
step:2106/2315 train_time:127773ms step_avg:60.67ms
step:2107/2315 train_time:127834ms step_avg:60.67ms
step:2108/2315 train_time:127895ms step_avg:60.67ms
step:2109/2315 train_time:127957ms step_avg:60.67ms
step:2110/2315 train_time:128019ms step_avg:60.67ms
step:2111/2315 train_time:128080ms step_avg:60.67ms
step:2112/2315 train_time:128141ms step_avg:60.67ms
step:2113/2315 train_time:128202ms step_avg:60.67ms
step:2114/2315 train_time:128263ms step_avg:60.67ms
step:2115/2315 train_time:128325ms step_avg:60.67ms
step:2116/2315 train_time:128386ms step_avg:60.67ms
step:2117/2315 train_time:128448ms step_avg:60.67ms
step:2118/2315 train_time:128509ms step_avg:60.67ms
step:2119/2315 train_time:128571ms step_avg:60.68ms
step:2120/2315 train_time:128631ms step_avg:60.68ms
step:2121/2315 train_time:128693ms step_avg:60.68ms
step:2122/2315 train_time:128754ms step_avg:60.68ms
step:2123/2315 train_time:128814ms step_avg:60.68ms
step:2124/2315 train_time:128875ms step_avg:60.68ms
step:2125/2315 train_time:128937ms step_avg:60.68ms
step:2126/2315 train_time:128999ms step_avg:60.68ms
step:2127/2315 train_time:129060ms step_avg:60.68ms
step:2128/2315 train_time:129121ms step_avg:60.68ms
step:2129/2315 train_time:129182ms step_avg:60.68ms
step:2130/2315 train_time:129243ms step_avg:60.68ms
step:2131/2315 train_time:129304ms step_avg:60.68ms
step:2132/2315 train_time:129365ms step_avg:60.68ms
step:2133/2315 train_time:129427ms step_avg:60.68ms
step:2134/2315 train_time:129489ms step_avg:60.68ms
step:2135/2315 train_time:129550ms step_avg:60.68ms
step:2136/2315 train_time:129611ms step_avg:60.68ms
step:2137/2315 train_time:129672ms step_avg:60.68ms
step:2138/2315 train_time:129733ms step_avg:60.68ms
step:2139/2315 train_time:129794ms step_avg:60.68ms
step:2140/2315 train_time:129855ms step_avg:60.68ms
step:2141/2315 train_time:129917ms step_avg:60.68ms
step:2142/2315 train_time:129979ms step_avg:60.68ms
step:2143/2315 train_time:130040ms step_avg:60.68ms
step:2144/2315 train_time:130101ms step_avg:60.68ms
step:2145/2315 train_time:130162ms step_avg:60.68ms
step:2146/2315 train_time:130223ms step_avg:60.68ms
step:2147/2315 train_time:130284ms step_avg:60.68ms
step:2148/2315 train_time:130345ms step_avg:60.68ms
step:2149/2315 train_time:130407ms step_avg:60.68ms
step:2150/2315 train_time:130469ms step_avg:60.68ms
step:2151/2315 train_time:130530ms step_avg:60.68ms
step:2152/2315 train_time:130591ms step_avg:60.68ms
step:2153/2315 train_time:130652ms step_avg:60.68ms
step:2154/2315 train_time:130713ms step_avg:60.68ms
step:2155/2315 train_time:130774ms step_avg:60.68ms
step:2156/2315 train_time:130835ms step_avg:60.68ms
step:2157/2315 train_time:130896ms step_avg:60.68ms
step:2158/2315 train_time:130957ms step_avg:60.68ms
step:2159/2315 train_time:131019ms step_avg:60.69ms
step:2160/2315 train_time:131081ms step_avg:60.69ms
step:2161/2315 train_time:131143ms step_avg:60.69ms
step:2162/2315 train_time:131203ms step_avg:60.69ms
step:2163/2315 train_time:131265ms step_avg:60.69ms
step:2164/2315 train_time:131326ms step_avg:60.69ms
step:2165/2315 train_time:131388ms step_avg:60.69ms
step:2166/2315 train_time:131449ms step_avg:60.69ms
step:2167/2315 train_time:131510ms step_avg:60.69ms
step:2168/2315 train_time:131571ms step_avg:60.69ms
step:2169/2315 train_time:131632ms step_avg:60.69ms
step:2170/2315 train_time:131693ms step_avg:60.69ms
step:2171/2315 train_time:131754ms step_avg:60.69ms
step:2172/2315 train_time:131815ms step_avg:60.69ms
step:2173/2315 train_time:131876ms step_avg:60.69ms
step:2174/2315 train_time:131936ms step_avg:60.69ms
step:2175/2315 train_time:131998ms step_avg:60.69ms
step:2176/2315 train_time:132060ms step_avg:60.69ms
step:2177/2315 train_time:132122ms step_avg:60.69ms
step:2178/2315 train_time:132183ms step_avg:60.69ms
step:2179/2315 train_time:132244ms step_avg:60.69ms
step:2180/2315 train_time:132305ms step_avg:60.69ms
step:2181/2315 train_time:132367ms step_avg:60.69ms
step:2182/2315 train_time:132428ms step_avg:60.69ms
step:2183/2315 train_time:132489ms step_avg:60.69ms
step:2184/2315 train_time:132550ms step_avg:60.69ms
step:2185/2315 train_time:132611ms step_avg:60.69ms
step:2186/2315 train_time:132672ms step_avg:60.69ms
step:2187/2315 train_time:132733ms step_avg:60.69ms
step:2188/2315 train_time:132794ms step_avg:60.69ms
step:2189/2315 train_time:132855ms step_avg:60.69ms
step:2190/2315 train_time:132916ms step_avg:60.69ms
step:2191/2315 train_time:132978ms step_avg:60.69ms
step:2192/2315 train_time:133040ms step_avg:60.69ms
step:2193/2315 train_time:133102ms step_avg:60.69ms
step:2194/2315 train_time:133162ms step_avg:60.69ms
step:2195/2315 train_time:133224ms step_avg:60.69ms
step:2196/2315 train_time:133284ms step_avg:60.69ms
step:2197/2315 train_time:133346ms step_avg:60.69ms
step:2198/2315 train_time:133407ms step_avg:60.69ms
step:2199/2315 train_time:133468ms step_avg:60.69ms
step:2200/2315 train_time:133529ms step_avg:60.70ms
step:2201/2315 train_time:133590ms step_avg:60.70ms
step:2202/2315 train_time:133652ms step_avg:60.70ms
step:2203/2315 train_time:133713ms step_avg:60.70ms
step:2204/2315 train_time:133774ms step_avg:60.70ms
step:2205/2315 train_time:133835ms step_avg:60.70ms
step:2206/2315 train_time:133896ms step_avg:60.70ms
step:2207/2315 train_time:133958ms step_avg:60.70ms
step:2208/2315 train_time:134020ms step_avg:60.70ms
step:2209/2315 train_time:134081ms step_avg:60.70ms
step:2210/2315 train_time:134142ms step_avg:60.70ms
step:2211/2315 train_time:134204ms step_avg:60.70ms
step:2212/2315 train_time:134265ms step_avg:60.70ms
step:2213/2315 train_time:134326ms step_avg:60.70ms
step:2214/2315 train_time:134387ms step_avg:60.70ms
step:2215/2315 train_time:134449ms step_avg:60.70ms
step:2216/2315 train_time:134510ms step_avg:60.70ms
step:2217/2315 train_time:134571ms step_avg:60.70ms
step:2218/2315 train_time:134632ms step_avg:60.70ms
step:2219/2315 train_time:134694ms step_avg:60.70ms
step:2220/2315 train_time:134755ms step_avg:60.70ms
step:2221/2315 train_time:134815ms step_avg:60.70ms
step:2222/2315 train_time:134876ms step_avg:60.70ms
step:2223/2315 train_time:134938ms step_avg:60.70ms
step:2224/2315 train_time:134999ms step_avg:60.70ms
step:2225/2315 train_time:135060ms step_avg:60.70ms
step:2226/2315 train_time:135121ms step_avg:60.70ms
step:2227/2315 train_time:135182ms step_avg:60.70ms
step:2228/2315 train_time:135243ms step_avg:60.70ms
step:2229/2315 train_time:135305ms step_avg:60.70ms
step:2230/2315 train_time:135366ms step_avg:60.70ms
step:2231/2315 train_time:135428ms step_avg:60.70ms
step:2232/2315 train_time:135489ms step_avg:60.70ms
step:2233/2315 train_time:135550ms step_avg:60.70ms
step:2234/2315 train_time:135611ms step_avg:60.70ms
step:2235/2315 train_time:135672ms step_avg:60.70ms
step:2236/2315 train_time:135733ms step_avg:60.70ms
step:2237/2315 train_time:135794ms step_avg:60.70ms
step:2238/2315 train_time:135854ms step_avg:60.70ms
step:2239/2315 train_time:135916ms step_avg:60.70ms
step:2240/2315 train_time:135977ms step_avg:60.70ms
step:2241/2315 train_time:136038ms step_avg:60.70ms
step:2242/2315 train_time:136099ms step_avg:60.70ms
step:2243/2315 train_time:136160ms step_avg:60.70ms
step:2244/2315 train_time:136221ms step_avg:60.70ms
step:2245/2315 train_time:136283ms step_avg:60.70ms
step:2246/2315 train_time:136344ms step_avg:60.71ms
step:2247/2315 train_time:136406ms step_avg:60.71ms
step:2248/2315 train_time:136467ms step_avg:60.71ms
step:2249/2315 train_time:136529ms step_avg:60.71ms
step:2250/2315 train_time:136590ms step_avg:60.71ms
step:2250/2315 val_loss:3.2898 train_time:136652ms step_avg:60.73ms
step:2251/2315 train_time:136674ms step_avg:60.72ms
step:2252/2315 train_time:136715ms step_avg:60.71ms
step:2253/2315 train_time:136782ms step_avg:60.71ms
step:2254/2315 train_time:136845ms step_avg:60.71ms
step:2255/2315 train_time:136908ms step_avg:60.71ms
step:2256/2315 train_time:136969ms step_avg:60.71ms
step:2257/2315 train_time:137030ms step_avg:60.71ms
step:2258/2315 train_time:137090ms step_avg:60.71ms
step:2259/2315 train_time:137152ms step_avg:60.71ms
step:2260/2315 train_time:137212ms step_avg:60.71ms
step:2261/2315 train_time:137272ms step_avg:60.71ms
step:2262/2315 train_time:137332ms step_avg:60.71ms
step:2263/2315 train_time:137393ms step_avg:60.71ms
step:2264/2315 train_time:137453ms step_avg:60.71ms
step:2265/2315 train_time:137513ms step_avg:60.71ms
step:2266/2315 train_time:137574ms step_avg:60.71ms
step:2267/2315 train_time:137635ms step_avg:60.71ms
step:2268/2315 train_time:137698ms step_avg:60.71ms
step:2269/2315 train_time:137760ms step_avg:60.71ms
step:2270/2315 train_time:137822ms step_avg:60.71ms
step:2271/2315 train_time:137885ms step_avg:60.72ms
step:2272/2315 train_time:137947ms step_avg:60.72ms
step:2273/2315 train_time:138008ms step_avg:60.72ms
step:2274/2315 train_time:138070ms step_avg:60.72ms
step:2275/2315 train_time:138131ms step_avg:60.72ms
step:2276/2315 train_time:138191ms step_avg:60.72ms
step:2277/2315 train_time:138252ms step_avg:60.72ms
step:2278/2315 train_time:138312ms step_avg:60.72ms
step:2279/2315 train_time:138373ms step_avg:60.72ms
step:2280/2315 train_time:138433ms step_avg:60.72ms
step:2281/2315 train_time:138493ms step_avg:60.72ms
step:2282/2315 train_time:138553ms step_avg:60.72ms
step:2283/2315 train_time:138615ms step_avg:60.72ms
step:2284/2315 train_time:138676ms step_avg:60.72ms
step:2285/2315 train_time:138739ms step_avg:60.72ms
step:2286/2315 train_time:138801ms step_avg:60.72ms
step:2287/2315 train_time:138863ms step_avg:60.72ms
step:2288/2315 train_time:138924ms step_avg:60.72ms
step:2289/2315 train_time:138986ms step_avg:60.72ms
step:2290/2315 train_time:139048ms step_avg:60.72ms
step:2291/2315 train_time:139109ms step_avg:60.72ms
step:2292/2315 train_time:139170ms step_avg:60.72ms
step:2293/2315 train_time:139231ms step_avg:60.72ms
step:2294/2315 train_time:139291ms step_avg:60.72ms
step:2295/2315 train_time:139352ms step_avg:60.72ms
step:2296/2315 train_time:139412ms step_avg:60.72ms
step:2297/2315 train_time:139473ms step_avg:60.72ms
step:2298/2315 train_time:139534ms step_avg:60.72ms
step:2299/2315 train_time:139594ms step_avg:60.72ms
step:2300/2315 train_time:139655ms step_avg:60.72ms
step:2301/2315 train_time:139716ms step_avg:60.72ms
step:2302/2315 train_time:139778ms step_avg:60.72ms
step:2303/2315 train_time:139840ms step_avg:60.72ms
step:2304/2315 train_time:139902ms step_avg:60.72ms
step:2305/2315 train_time:139964ms step_avg:60.72ms
step:2306/2315 train_time:140025ms step_avg:60.72ms
step:2307/2315 train_time:140087ms step_avg:60.72ms
step:2308/2315 train_time:140148ms step_avg:60.72ms
step:2309/2315 train_time:140209ms step_avg:60.72ms
step:2310/2315 train_time:140270ms step_avg:60.72ms
step:2311/2315 train_time:140331ms step_avg:60.72ms
step:2312/2315 train_time:140392ms step_avg:60.72ms
step:2313/2315 train_time:140452ms step_avg:60.72ms
step:2314/2315 train_time:140513ms step_avg:60.72ms
step:2315/2315 train_time:140574ms step_avg:60.72ms
step:2315/2315 val_loss:3.2769 train_time:140635ms step_avg:60.75ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]
# polar_express_coeffs = [
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
# ]



@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    # state["second_momentum_buffer"] = torch.zeros_like(grad[...,0:1]) if param.size(-2) >= param.size(-1) else torch.zeros_like(grad[...,0:1,:])
                    # state["second_momentum_buffer"] = torch.zeros((*grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((*grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)
            
            ###################################
            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))
            ####################################


            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> (-1.5)  0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Oct 22 2025, 02:20:45) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251022+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 24 05:47:42 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000001:00:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000002:00:00.0 Off |                    0 |
| N/A   31C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000003:00:00.0 Off |                    0 |
| N/A   28C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000008:00:00.0 Off |                    0 |
| N/A   28C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000009:00:00.0 Off |                    0 |
| N/A   26C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   0000000B:00:00.0 Off |                    0 |
| N/A   28C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   0000000C:00:00.0 Off |                    0 |
| N/A   31C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2315 train_time:84ms step_avg:83.58ms
step:2/2315 train_time:185ms step_avg:92.57ms
step:3/2315 train_time:207ms step_avg:68.98ms
step:4/2315 train_time:243ms step_avg:60.63ms
step:5/2315 train_time:301ms step_avg:60.12ms
step:6/2315 train_time:360ms step_avg:60.01ms
step:7/2315 train_time:420ms step_avg:59.94ms
step:8/2315 train_time:480ms step_avg:59.94ms
step:9/2315 train_time:539ms step_avg:59.85ms
step:10/2315 train_time:598ms step_avg:59.82ms
step:11/2315 train_time:658ms step_avg:59.82ms
step:12/2315 train_time:718ms step_avg:59.82ms
step:13/2315 train_time:778ms step_avg:59.83ms
step:14/2315 train_time:838ms step_avg:59.84ms
step:15/2315 train_time:898ms step_avg:59.86ms
step:16/2315 train_time:958ms step_avg:59.84ms
step:17/2315 train_time:1018ms step_avg:59.90ms
step:18/2315 train_time:1079ms step_avg:59.96ms
step:19/2315 train_time:1143ms step_avg:60.18ms
step:20/2315 train_time:1206ms step_avg:60.29ms
step:21/2315 train_time:1267ms step_avg:60.33ms
step:22/2315 train_time:1327ms step_avg:60.34ms
step:23/2315 train_time:1388ms step_avg:60.36ms
step:24/2315 train_time:1449ms step_avg:60.36ms
step:25/2315 train_time:1510ms step_avg:60.39ms
step:26/2315 train_time:1570ms step_avg:60.39ms
step:27/2315 train_time:1630ms step_avg:60.39ms
step:28/2315 train_time:1691ms step_avg:60.38ms
step:29/2315 train_time:1751ms step_avg:60.37ms
step:30/2315 train_time:1812ms step_avg:60.38ms
step:31/2315 train_time:1871ms step_avg:60.37ms
step:32/2315 train_time:1933ms step_avg:60.39ms
step:33/2315 train_time:1993ms step_avg:60.39ms
step:34/2315 train_time:2054ms step_avg:60.40ms
step:35/2315 train_time:2115ms step_avg:60.43ms
step:36/2315 train_time:2175ms step_avg:60.42ms
step:37/2315 train_time:2236ms step_avg:60.44ms
step:38/2315 train_time:2297ms step_avg:60.45ms
step:39/2315 train_time:2357ms step_avg:60.44ms
step:40/2315 train_time:2418ms step_avg:60.45ms
step:41/2315 train_time:2479ms step_avg:60.45ms
step:42/2315 train_time:2539ms step_avg:60.46ms
step:43/2315 train_time:2600ms step_avg:60.46ms
step:44/2315 train_time:2660ms step_avg:60.44ms
step:45/2315 train_time:2719ms step_avg:60.42ms
step:46/2315 train_time:2780ms step_avg:60.43ms
step:47/2315 train_time:2840ms step_avg:60.42ms
step:48/2315 train_time:2900ms step_avg:60.41ms
step:49/2315 train_time:2959ms step_avg:60.39ms
step:50/2315 train_time:3020ms step_avg:60.39ms
step:51/2315 train_time:3079ms step_avg:60.38ms
step:52/2315 train_time:3139ms step_avg:60.37ms
step:53/2315 train_time:3200ms step_avg:60.38ms
step:54/2315 train_time:3260ms step_avg:60.37ms
step:55/2315 train_time:3321ms step_avg:60.38ms
step:56/2315 train_time:3381ms step_avg:60.38ms
step:57/2315 train_time:3442ms step_avg:60.38ms
step:58/2315 train_time:3502ms step_avg:60.38ms
step:59/2315 train_time:3563ms step_avg:60.38ms
step:60/2315 train_time:3623ms step_avg:60.38ms
step:61/2315 train_time:3684ms step_avg:60.39ms
step:62/2315 train_time:3744ms step_avg:60.39ms
step:63/2315 train_time:3805ms step_avg:60.39ms
step:64/2315 train_time:3865ms step_avg:60.39ms
step:65/2315 train_time:3925ms step_avg:60.39ms
step:66/2315 train_time:3986ms step_avg:60.39ms
step:67/2315 train_time:4047ms step_avg:60.40ms
step:68/2315 train_time:4107ms step_avg:60.40ms
step:69/2315 train_time:4168ms step_avg:60.40ms
step:70/2315 train_time:4229ms step_avg:60.41ms
step:71/2315 train_time:4289ms step_avg:60.41ms
step:72/2315 train_time:4350ms step_avg:60.42ms
step:73/2315 train_time:4411ms step_avg:60.43ms
step:74/2315 train_time:4472ms step_avg:60.43ms
step:75/2315 train_time:4533ms step_avg:60.44ms
step:76/2315 train_time:4593ms step_avg:60.43ms
step:77/2315 train_time:4654ms step_avg:60.44ms
step:78/2315 train_time:4713ms step_avg:60.42ms
step:79/2315 train_time:4773ms step_avg:60.42ms
step:80/2315 train_time:4833ms step_avg:60.41ms
step:81/2315 train_time:4892ms step_avg:60.40ms
step:82/2315 train_time:4952ms step_avg:60.39ms
step:83/2315 train_time:5013ms step_avg:60.39ms
step:84/2315 train_time:5073ms step_avg:60.39ms
step:85/2315 train_time:5134ms step_avg:60.39ms
step:86/2315 train_time:5194ms step_avg:60.39ms
step:87/2315 train_time:5254ms step_avg:60.39ms
step:88/2315 train_time:5314ms step_avg:60.38ms
step:89/2315 train_time:5374ms step_avg:60.39ms
step:90/2315 train_time:5435ms step_avg:60.38ms
step:91/2315 train_time:5494ms step_avg:60.38ms
step:92/2315 train_time:5554ms step_avg:60.37ms
step:93/2315 train_time:5615ms step_avg:60.37ms
step:94/2315 train_time:5674ms step_avg:60.37ms
step:95/2315 train_time:5735ms step_avg:60.37ms
step:96/2315 train_time:5794ms step_avg:60.36ms
step:97/2315 train_time:5854ms step_avg:60.35ms
step:98/2315 train_time:5914ms step_avg:60.34ms
step:99/2315 train_time:5973ms step_avg:60.34ms
step:100/2315 train_time:6033ms step_avg:60.33ms
step:101/2315 train_time:6093ms step_avg:60.33ms
step:102/2315 train_time:6153ms step_avg:60.32ms
step:103/2315 train_time:6213ms step_avg:60.32ms
step:104/2315 train_time:6273ms step_avg:60.32ms
step:105/2315 train_time:6333ms step_avg:60.32ms
step:106/2315 train_time:6393ms step_avg:60.31ms
step:107/2315 train_time:6453ms step_avg:60.31ms
step:108/2315 train_time:6513ms step_avg:60.31ms
step:109/2315 train_time:6573ms step_avg:60.30ms
step:110/2315 train_time:6633ms step_avg:60.30ms
step:111/2315 train_time:6693ms step_avg:60.30ms
step:112/2315 train_time:6753ms step_avg:60.30ms
step:113/2315 train_time:6813ms step_avg:60.29ms
step:114/2315 train_time:6873ms step_avg:60.29ms
step:115/2315 train_time:6933ms step_avg:60.28ms
step:116/2315 train_time:6992ms step_avg:60.28ms
step:117/2315 train_time:7052ms step_avg:60.27ms
step:118/2315 train_time:7112ms step_avg:60.27ms
step:119/2315 train_time:7172ms step_avg:60.27ms
step:120/2315 train_time:7232ms step_avg:60.26ms
step:121/2315 train_time:7291ms step_avg:60.26ms
step:122/2315 train_time:7351ms step_avg:60.26ms
step:123/2315 train_time:7412ms step_avg:60.26ms
step:124/2315 train_time:7472ms step_avg:60.25ms
step:125/2315 train_time:7532ms step_avg:60.26ms
step:126/2315 train_time:7592ms step_avg:60.25ms
step:127/2315 train_time:7652ms step_avg:60.25ms
step:128/2315 train_time:7712ms step_avg:60.25ms
step:129/2315 train_time:7772ms step_avg:60.25ms
step:130/2315 train_time:7832ms step_avg:60.25ms
step:131/2315 train_time:7892ms step_avg:60.24ms
step:132/2315 train_time:7952ms step_avg:60.24ms
step:133/2315 train_time:8012ms step_avg:60.24ms
step:134/2315 train_time:8072ms step_avg:60.24ms
step:135/2315 train_time:8132ms step_avg:60.24ms
step:136/2315 train_time:8192ms step_avg:60.24ms
step:137/2315 train_time:8252ms step_avg:60.24ms
step:138/2315 train_time:8312ms step_avg:60.23ms
step:139/2315 train_time:8372ms step_avg:60.23ms
step:140/2315 train_time:8432ms step_avg:60.23ms
step:141/2315 train_time:8492ms step_avg:60.23ms
step:142/2315 train_time:8552ms step_avg:60.23ms
step:143/2315 train_time:8612ms step_avg:60.23ms
step:144/2315 train_time:8672ms step_avg:60.22ms
step:145/2315 train_time:8732ms step_avg:60.22ms
step:146/2315 train_time:8792ms step_avg:60.22ms
step:147/2315 train_time:8852ms step_avg:60.22ms
step:148/2315 train_time:8912ms step_avg:60.22ms
step:149/2315 train_time:8973ms step_avg:60.22ms
step:150/2315 train_time:9032ms step_avg:60.22ms
step:151/2315 train_time:9092ms step_avg:60.21ms
step:152/2315 train_time:9152ms step_avg:60.21ms
step:153/2315 train_time:9212ms step_avg:60.21ms
step:154/2315 train_time:9271ms step_avg:60.20ms
step:155/2315 train_time:9331ms step_avg:60.20ms
step:156/2315 train_time:9391ms step_avg:60.20ms
step:157/2315 train_time:9452ms step_avg:60.20ms
step:158/2315 train_time:9511ms step_avg:60.20ms
step:159/2315 train_time:9572ms step_avg:60.20ms
step:160/2315 train_time:9632ms step_avg:60.20ms
step:161/2315 train_time:9692ms step_avg:60.20ms
step:162/2315 train_time:9751ms step_avg:60.19ms
step:163/2315 train_time:9812ms step_avg:60.19ms
step:164/2315 train_time:9871ms step_avg:60.19ms
step:165/2315 train_time:9931ms step_avg:60.19ms
step:166/2315 train_time:9991ms step_avg:60.19ms
step:167/2315 train_time:10051ms step_avg:60.18ms
step:168/2315 train_time:10110ms step_avg:60.18ms
step:169/2315 train_time:10171ms step_avg:60.18ms
step:170/2315 train_time:10231ms step_avg:60.18ms
step:171/2315 train_time:10291ms step_avg:60.18ms
step:172/2315 train_time:10351ms step_avg:60.18ms
step:173/2315 train_time:10411ms step_avg:60.18ms
step:174/2315 train_time:10471ms step_avg:60.18ms
step:175/2315 train_time:10531ms step_avg:60.18ms
step:176/2315 train_time:10591ms step_avg:60.17ms
step:177/2315 train_time:10651ms step_avg:60.17ms
step:178/2315 train_time:10711ms step_avg:60.17ms
step:179/2315 train_time:10771ms step_avg:60.17ms
step:180/2315 train_time:10832ms step_avg:60.18ms
step:181/2315 train_time:10892ms step_avg:60.18ms
step:182/2315 train_time:10952ms step_avg:60.17ms
step:183/2315 train_time:11012ms step_avg:60.17ms
step:184/2315 train_time:11071ms step_avg:60.17ms
step:185/2315 train_time:11131ms step_avg:60.17ms
step:186/2315 train_time:11192ms step_avg:60.17ms
step:187/2315 train_time:11251ms step_avg:60.17ms
step:188/2315 train_time:11311ms step_avg:60.16ms
step:189/2315 train_time:11371ms step_avg:60.16ms
step:190/2315 train_time:11431ms step_avg:60.16ms
step:191/2315 train_time:11491ms step_avg:60.16ms
step:192/2315 train_time:11550ms step_avg:60.16ms
step:193/2315 train_time:11610ms step_avg:60.16ms
step:194/2315 train_time:11671ms step_avg:60.16ms
step:195/2315 train_time:11731ms step_avg:60.16ms
step:196/2315 train_time:11790ms step_avg:60.16ms
step:197/2315 train_time:11851ms step_avg:60.16ms
step:198/2315 train_time:11911ms step_avg:60.16ms
step:199/2315 train_time:11971ms step_avg:60.16ms
step:200/2315 train_time:12031ms step_avg:60.15ms
step:201/2315 train_time:12091ms step_avg:60.15ms
step:202/2315 train_time:12150ms step_avg:60.15ms
step:203/2315 train_time:12210ms step_avg:60.15ms
step:204/2315 train_time:12270ms step_avg:60.15ms
step:205/2315 train_time:12329ms step_avg:60.14ms
step:206/2315 train_time:12390ms step_avg:60.15ms
step:207/2315 train_time:12450ms step_avg:60.14ms
step:208/2315 train_time:12510ms step_avg:60.14ms
step:209/2315 train_time:12571ms step_avg:60.15ms
step:210/2315 train_time:12632ms step_avg:60.15ms
step:211/2315 train_time:12691ms step_avg:60.15ms
step:212/2315 train_time:12751ms step_avg:60.14ms
step:213/2315 train_time:12811ms step_avg:60.15ms
step:214/2315 train_time:12871ms step_avg:60.14ms
step:215/2315 train_time:12931ms step_avg:60.14ms
step:216/2315 train_time:12991ms step_avg:60.14ms
step:217/2315 train_time:13051ms step_avg:60.14ms
step:218/2315 train_time:13111ms step_avg:60.14ms
step:219/2315 train_time:13171ms step_avg:60.14ms
step:220/2315 train_time:13231ms step_avg:60.14ms
step:221/2315 train_time:13290ms step_avg:60.14ms
step:222/2315 train_time:13350ms step_avg:60.13ms
step:223/2315 train_time:13410ms step_avg:60.13ms
step:224/2315 train_time:13470ms step_avg:60.13ms
step:225/2315 train_time:13531ms step_avg:60.14ms
step:226/2315 train_time:13591ms step_avg:60.14ms
step:227/2315 train_time:13652ms step_avg:60.14ms
step:228/2315 train_time:13712ms step_avg:60.14ms
step:229/2315 train_time:13772ms step_avg:60.14ms
step:230/2315 train_time:13831ms step_avg:60.14ms
step:231/2315 train_time:13891ms step_avg:60.13ms
step:232/2315 train_time:13952ms step_avg:60.14ms
step:233/2315 train_time:14011ms step_avg:60.13ms
step:234/2315 train_time:14071ms step_avg:60.13ms
step:235/2315 train_time:14131ms step_avg:60.13ms
step:236/2315 train_time:14191ms step_avg:60.13ms
step:237/2315 train_time:14251ms step_avg:60.13ms
step:238/2315 train_time:14310ms step_avg:60.13ms
step:239/2315 train_time:14370ms step_avg:60.13ms
step:240/2315 train_time:14430ms step_avg:60.13ms
step:241/2315 train_time:14491ms step_avg:60.13ms
step:242/2315 train_time:14551ms step_avg:60.13ms
step:243/2315 train_time:14611ms step_avg:60.13ms
step:244/2315 train_time:14671ms step_avg:60.13ms
step:245/2315 train_time:14731ms step_avg:60.13ms
step:246/2315 train_time:14791ms step_avg:60.13ms
step:247/2315 train_time:14851ms step_avg:60.13ms
step:248/2315 train_time:14911ms step_avg:60.12ms
step:249/2315 train_time:14971ms step_avg:60.12ms
step:250/2315 train_time:15030ms step_avg:60.12ms
step:250/2315 val_loss:4.0681 train_time:15092ms step_avg:60.37ms
step:251/2315 train_time:15115ms step_avg:60.22ms
step:252/2315 train_time:15152ms step_avg:60.13ms
step:253/2315 train_time:15214ms step_avg:60.13ms
step:254/2315 train_time:15281ms step_avg:60.16ms
step:255/2315 train_time:15344ms step_avg:60.17ms
step:256/2315 train_time:15404ms step_avg:60.17ms
step:257/2315 train_time:15464ms step_avg:60.17ms
step:258/2315 train_time:15523ms step_avg:60.17ms
step:259/2315 train_time:15583ms step_avg:60.17ms
step:260/2315 train_time:15643ms step_avg:60.17ms
step:261/2315 train_time:15702ms step_avg:60.16ms
step:262/2315 train_time:15762ms step_avg:60.16ms
step:263/2315 train_time:15821ms step_avg:60.16ms
step:264/2315 train_time:15881ms step_avg:60.15ms
step:265/2315 train_time:15940ms step_avg:60.15ms
step:266/2315 train_time:15999ms step_avg:60.15ms
step:267/2315 train_time:16059ms step_avg:60.15ms
step:268/2315 train_time:16119ms step_avg:60.14ms
step:269/2315 train_time:16180ms step_avg:60.15ms
step:270/2315 train_time:16240ms step_avg:60.15ms
step:271/2315 train_time:16302ms step_avg:60.16ms
step:272/2315 train_time:16363ms step_avg:60.16ms
step:273/2315 train_time:16424ms step_avg:60.16ms
step:274/2315 train_time:16484ms step_avg:60.16ms
step:275/2315 train_time:16543ms step_avg:60.16ms
step:276/2315 train_time:16603ms step_avg:60.16ms
step:277/2315 train_time:16663ms step_avg:60.15ms
step:278/2315 train_time:16722ms step_avg:60.15ms
step:279/2315 train_time:16781ms step_avg:60.15ms
step:280/2315 train_time:16841ms step_avg:60.15ms
step:281/2315 train_time:16901ms step_avg:60.14ms
step:282/2315 train_time:16960ms step_avg:60.14ms
step:283/2315 train_time:17020ms step_avg:60.14ms
step:284/2315 train_time:17079ms step_avg:60.14ms
step:285/2315 train_time:17140ms step_avg:60.14ms
step:286/2315 train_time:17200ms step_avg:60.14ms
step:287/2315 train_time:17262ms step_avg:60.15ms
step:288/2315 train_time:17322ms step_avg:60.15ms
step:289/2315 train_time:17383ms step_avg:60.15ms
step:290/2315 train_time:17443ms step_avg:60.15ms
step:291/2315 train_time:17503ms step_avg:60.15ms
step:292/2315 train_time:17563ms step_avg:60.15ms
step:293/2315 train_time:17622ms step_avg:60.14ms
step:294/2315 train_time:17682ms step_avg:60.14ms
step:295/2315 train_time:17741ms step_avg:60.14ms
step:296/2315 train_time:17800ms step_avg:60.13ms
step:297/2315 train_time:17859ms step_avg:60.13ms
step:298/2315 train_time:17919ms step_avg:60.13ms
step:299/2315 train_time:17978ms step_avg:60.13ms
step:300/2315 train_time:18037ms step_avg:60.12ms
step:301/2315 train_time:18097ms step_avg:60.12ms
step:302/2315 train_time:18157ms step_avg:60.12ms
step:303/2315 train_time:18216ms step_avg:60.12ms
step:304/2315 train_time:18276ms step_avg:60.12ms
step:305/2315 train_time:18336ms step_avg:60.12ms
step:306/2315 train_time:18396ms step_avg:60.12ms
step:307/2315 train_time:18456ms step_avg:60.12ms
step:308/2315 train_time:18516ms step_avg:60.12ms
step:309/2315 train_time:18576ms step_avg:60.12ms
step:310/2315 train_time:18636ms step_avg:60.12ms
step:311/2315 train_time:18695ms step_avg:60.11ms
step:312/2315 train_time:18755ms step_avg:60.11ms
step:313/2315 train_time:18814ms step_avg:60.11ms
step:314/2315 train_time:18873ms step_avg:60.11ms
step:315/2315 train_time:18933ms step_avg:60.10ms
step:316/2315 train_time:18992ms step_avg:60.10ms
step:317/2315 train_time:19052ms step_avg:60.10ms
step:318/2315 train_time:19112ms step_avg:60.10ms
step:319/2315 train_time:19171ms step_avg:60.10ms
step:320/2315 train_time:19231ms step_avg:60.10ms
step:321/2315 train_time:19292ms step_avg:60.10ms
step:322/2315 train_time:19351ms step_avg:60.10ms
step:323/2315 train_time:19411ms step_avg:60.10ms
step:324/2315 train_time:19471ms step_avg:60.10ms
step:325/2315 train_time:19532ms step_avg:60.10ms
step:326/2315 train_time:19592ms step_avg:60.10ms
step:327/2315 train_time:19651ms step_avg:60.10ms
step:328/2315 train_time:19711ms step_avg:60.09ms
step:329/2315 train_time:19771ms step_avg:60.09ms
step:330/2315 train_time:19830ms step_avg:60.09ms
step:331/2315 train_time:19891ms step_avg:60.09ms
step:332/2315 train_time:19950ms step_avg:60.09ms
step:333/2315 train_time:20010ms step_avg:60.09ms
step:334/2315 train_time:20071ms step_avg:60.09ms
step:335/2315 train_time:20130ms step_avg:60.09ms
step:336/2315 train_time:20191ms step_avg:60.09ms
step:337/2315 train_time:20251ms step_avg:60.09ms
step:338/2315 train_time:20311ms step_avg:60.09ms
step:339/2315 train_time:20371ms step_avg:60.09ms
step:340/2315 train_time:20431ms step_avg:60.09ms
step:341/2315 train_time:20491ms step_avg:60.09ms
step:342/2315 train_time:20551ms step_avg:60.09ms
step:343/2315 train_time:20611ms step_avg:60.09ms
step:344/2315 train_time:20671ms step_avg:60.09ms
step:345/2315 train_time:20730ms step_avg:60.09ms
step:346/2315 train_time:20790ms step_avg:60.09ms
step:347/2315 train_time:20850ms step_avg:60.09ms
step:348/2315 train_time:20910ms step_avg:60.09ms
step:349/2315 train_time:20971ms step_avg:60.09ms
step:350/2315 train_time:21031ms step_avg:60.09ms
step:351/2315 train_time:21091ms step_avg:60.09ms
step:352/2315 train_time:21150ms step_avg:60.09ms
step:353/2315 train_time:21211ms step_avg:60.09ms
step:354/2315 train_time:21270ms step_avg:60.08ms
step:355/2315 train_time:21330ms step_avg:60.08ms
step:356/2315 train_time:21390ms step_avg:60.08ms
step:357/2315 train_time:21450ms step_avg:60.08ms
step:358/2315 train_time:21510ms step_avg:60.08ms
step:359/2315 train_time:21571ms step_avg:60.09ms
step:360/2315 train_time:21631ms step_avg:60.09ms
step:361/2315 train_time:21691ms step_avg:60.09ms
step:362/2315 train_time:21751ms step_avg:60.08ms
step:363/2315 train_time:21810ms step_avg:60.08ms
step:364/2315 train_time:21870ms step_avg:60.08ms
step:365/2315 train_time:21930ms step_avg:60.08ms
step:366/2315 train_time:21990ms step_avg:60.08ms
step:367/2315 train_time:22050ms step_avg:60.08ms
step:368/2315 train_time:22109ms step_avg:60.08ms
step:369/2315 train_time:22170ms step_avg:60.08ms
step:370/2315 train_time:22229ms step_avg:60.08ms
step:371/2315 train_time:22289ms step_avg:60.08ms
step:372/2315 train_time:22349ms step_avg:60.08ms
step:373/2315 train_time:22410ms step_avg:60.08ms
step:374/2315 train_time:22469ms step_avg:60.08ms
step:375/2315 train_time:22530ms step_avg:60.08ms
step:376/2315 train_time:22590ms step_avg:60.08ms
step:377/2315 train_time:22650ms step_avg:60.08ms
step:378/2315 train_time:22710ms step_avg:60.08ms
step:379/2315 train_time:22770ms step_avg:60.08ms
step:380/2315 train_time:22830ms step_avg:60.08ms
step:381/2315 train_time:22889ms step_avg:60.08ms
step:382/2315 train_time:22949ms step_avg:60.08ms
step:383/2315 train_time:23009ms step_avg:60.08ms
step:384/2315 train_time:23069ms step_avg:60.07ms
step:385/2315 train_time:23129ms step_avg:60.07ms
step:386/2315 train_time:23189ms step_avg:60.08ms
step:387/2315 train_time:23249ms step_avg:60.07ms
step:388/2315 train_time:23309ms step_avg:60.07ms
step:389/2315 train_time:23369ms step_avg:60.07ms
step:390/2315 train_time:23429ms step_avg:60.07ms
step:391/2315 train_time:23489ms step_avg:60.07ms
step:392/2315 train_time:23549ms step_avg:60.07ms
step:393/2315 train_time:23609ms step_avg:60.07ms
step:394/2315 train_time:23668ms step_avg:60.07ms
step:395/2315 train_time:23729ms step_avg:60.07ms
step:396/2315 train_time:23789ms step_avg:60.07ms
step:397/2315 train_time:23850ms step_avg:60.07ms
step:398/2315 train_time:23909ms step_avg:60.07ms
step:399/2315 train_time:23969ms step_avg:60.07ms
step:400/2315 train_time:24028ms step_avg:60.07ms
step:401/2315 train_time:24089ms step_avg:60.07ms
step:402/2315 train_time:24149ms step_avg:60.07ms
step:403/2315 train_time:24208ms step_avg:60.07ms
step:404/2315 train_time:24268ms step_avg:60.07ms
step:405/2315 train_time:24328ms step_avg:60.07ms
step:406/2315 train_time:24389ms step_avg:60.07ms
step:407/2315 train_time:24450ms step_avg:60.07ms
step:408/2315 train_time:24510ms step_avg:60.07ms
step:409/2315 train_time:24569ms step_avg:60.07ms
step:410/2315 train_time:24629ms step_avg:60.07ms
step:411/2315 train_time:24689ms step_avg:60.07ms
step:412/2315 train_time:24749ms step_avg:60.07ms
step:413/2315 train_time:24810ms step_avg:60.07ms
step:414/2315 train_time:24870ms step_avg:60.07ms
step:415/2315 train_time:24930ms step_avg:60.07ms
step:416/2315 train_time:24990ms step_avg:60.07ms
step:417/2315 train_time:25050ms step_avg:60.07ms
step:418/2315 train_time:25110ms step_avg:60.07ms
step:419/2315 train_time:25170ms step_avg:60.07ms
step:420/2315 train_time:25229ms step_avg:60.07ms
step:421/2315 train_time:25289ms step_avg:60.07ms
step:422/2315 train_time:25349ms step_avg:60.07ms
step:423/2315 train_time:25409ms step_avg:60.07ms
step:424/2315 train_time:25469ms step_avg:60.07ms
step:425/2315 train_time:25529ms step_avg:60.07ms
step:426/2315 train_time:25590ms step_avg:60.07ms
step:427/2315 train_time:25650ms step_avg:60.07ms
step:428/2315 train_time:25709ms step_avg:60.07ms
step:429/2315 train_time:25769ms step_avg:60.07ms
step:430/2315 train_time:25829ms step_avg:60.07ms
step:431/2315 train_time:25889ms step_avg:60.07ms
step:432/2315 train_time:25949ms step_avg:60.07ms
step:433/2315 train_time:26009ms step_avg:60.07ms
step:434/2315 train_time:26070ms step_avg:60.07ms
step:435/2315 train_time:26130ms step_avg:60.07ms
step:436/2315 train_time:26191ms step_avg:60.07ms
step:437/2315 train_time:26250ms step_avg:60.07ms
step:438/2315 train_time:26310ms step_avg:60.07ms
step:439/2315 train_time:26370ms step_avg:60.07ms
step:440/2315 train_time:26430ms step_avg:60.07ms
step:441/2315 train_time:26490ms step_avg:60.07ms
step:442/2315 train_time:26550ms step_avg:60.07ms
step:443/2315 train_time:26610ms step_avg:60.07ms
step:444/2315 train_time:26670ms step_avg:60.07ms
step:445/2315 train_time:26730ms step_avg:60.07ms
step:446/2315 train_time:26789ms step_avg:60.07ms
step:447/2315 train_time:26849ms step_avg:60.07ms
step:448/2315 train_time:26909ms step_avg:60.07ms
step:449/2315 train_time:26970ms step_avg:60.07ms
step:450/2315 train_time:27030ms step_avg:60.07ms
step:451/2315 train_time:27090ms step_avg:60.07ms
step:452/2315 train_time:27150ms step_avg:60.07ms
step:453/2315 train_time:27210ms step_avg:60.07ms
step:454/2315 train_time:27270ms step_avg:60.07ms
step:455/2315 train_time:27331ms step_avg:60.07ms
step:456/2315 train_time:27391ms step_avg:60.07ms
step:457/2315 train_time:27451ms step_avg:60.07ms
step:458/2315 train_time:27510ms step_avg:60.07ms
step:459/2315 train_time:27570ms step_avg:60.07ms
step:460/2315 train_time:27630ms step_avg:60.07ms
step:461/2315 train_time:27691ms step_avg:60.07ms
step:462/2315 train_time:27751ms step_avg:60.07ms
step:463/2315 train_time:27811ms step_avg:60.07ms
step:464/2315 train_time:27870ms step_avg:60.07ms
step:465/2315 train_time:27930ms step_avg:60.07ms
step:466/2315 train_time:27990ms step_avg:60.06ms
step:467/2315 train_time:28051ms step_avg:60.07ms
step:468/2315 train_time:28110ms step_avg:60.06ms
step:469/2315 train_time:28171ms step_avg:60.07ms
step:470/2315 train_time:28231ms step_avg:60.07ms
step:471/2315 train_time:28291ms step_avg:60.07ms
step:472/2315 train_time:28350ms step_avg:60.06ms
step:473/2315 train_time:28410ms step_avg:60.06ms
step:474/2315 train_time:28470ms step_avg:60.06ms
step:475/2315 train_time:28530ms step_avg:60.06ms
step:476/2315 train_time:28590ms step_avg:60.06ms
step:477/2315 train_time:28650ms step_avg:60.06ms
step:478/2315 train_time:28710ms step_avg:60.06ms
step:479/2315 train_time:28770ms step_avg:60.06ms
step:480/2315 train_time:28830ms step_avg:60.06ms
step:481/2315 train_time:28890ms step_avg:60.06ms
step:482/2315 train_time:28950ms step_avg:60.06ms
step:483/2315 train_time:29010ms step_avg:60.06ms
step:484/2315 train_time:29070ms step_avg:60.06ms
step:485/2315 train_time:29130ms step_avg:60.06ms
step:486/2315 train_time:29190ms step_avg:60.06ms
step:487/2315 train_time:29250ms step_avg:60.06ms
step:488/2315 train_time:29309ms step_avg:60.06ms
step:489/2315 train_time:29369ms step_avg:60.06ms
step:490/2315 train_time:29430ms step_avg:60.06ms
step:491/2315 train_time:29490ms step_avg:60.06ms
step:492/2315 train_time:29549ms step_avg:60.06ms
step:493/2315 train_time:29610ms step_avg:60.06ms
step:494/2315 train_time:29670ms step_avg:60.06ms
step:495/2315 train_time:29730ms step_avg:60.06ms
step:496/2315 train_time:29790ms step_avg:60.06ms
step:497/2315 train_time:29849ms step_avg:60.06ms
step:498/2315 train_time:29908ms step_avg:60.06ms
step:499/2315 train_time:29968ms step_avg:60.06ms
step:500/2315 train_time:30028ms step_avg:60.06ms
step:500/2315 val_loss:3.8067 train_time:30090ms step_avg:60.18ms
step:501/2315 train_time:30112ms step_avg:60.10ms
step:502/2315 train_time:30150ms step_avg:60.06ms
step:503/2315 train_time:30210ms step_avg:60.06ms
step:504/2315 train_time:30275ms step_avg:60.07ms
step:505/2315 train_time:30337ms step_avg:60.07ms
step:506/2315 train_time:30397ms step_avg:60.07ms
step:507/2315 train_time:30457ms step_avg:60.07ms
step:508/2315 train_time:30516ms step_avg:60.07ms
step:509/2315 train_time:30576ms step_avg:60.07ms
step:510/2315 train_time:30636ms step_avg:60.07ms
step:511/2315 train_time:30695ms step_avg:60.07ms
step:512/2315 train_time:30755ms step_avg:60.07ms
step:513/2315 train_time:30814ms step_avg:60.07ms
step:514/2315 train_time:30874ms step_avg:60.07ms
step:515/2315 train_time:30933ms step_avg:60.06ms
step:516/2315 train_time:30992ms step_avg:60.06ms
step:517/2315 train_time:31053ms step_avg:60.06ms
step:518/2315 train_time:31114ms step_avg:60.07ms
step:519/2315 train_time:31174ms step_avg:60.07ms
step:520/2315 train_time:31235ms step_avg:60.07ms
step:521/2315 train_time:31295ms step_avg:60.07ms
step:522/2315 train_time:31356ms step_avg:60.07ms
step:523/2315 train_time:31416ms step_avg:60.07ms
step:524/2315 train_time:31476ms step_avg:60.07ms
step:525/2315 train_time:31536ms step_avg:60.07ms
step:526/2315 train_time:31597ms step_avg:60.07ms
step:527/2315 train_time:31656ms step_avg:60.07ms
step:528/2315 train_time:31716ms step_avg:60.07ms
step:529/2315 train_time:31775ms step_avg:60.07ms
step:530/2315 train_time:31835ms step_avg:60.07ms
step:531/2315 train_time:31894ms step_avg:60.06ms
step:532/2315 train_time:31954ms step_avg:60.06ms
step:533/2315 train_time:32014ms step_avg:60.06ms
step:534/2315 train_time:32074ms step_avg:60.06ms
step:535/2315 train_time:32134ms step_avg:60.06ms
step:536/2315 train_time:32194ms step_avg:60.06ms
step:537/2315 train_time:32255ms step_avg:60.07ms
step:538/2315 train_time:32315ms step_avg:60.07ms
step:539/2315 train_time:32375ms step_avg:60.07ms
step:540/2315 train_time:32435ms step_avg:60.07ms
step:541/2315 train_time:32496ms step_avg:60.07ms
step:542/2315 train_time:32555ms step_avg:60.06ms
step:543/2315 train_time:32615ms step_avg:60.06ms
step:544/2315 train_time:32675ms step_avg:60.06ms
step:545/2315 train_time:32735ms step_avg:60.06ms
step:546/2315 train_time:32795ms step_avg:60.06ms
step:547/2315 train_time:32855ms step_avg:60.06ms
step:548/2315 train_time:32914ms step_avg:60.06ms
step:549/2315 train_time:32974ms step_avg:60.06ms
step:550/2315 train_time:33034ms step_avg:60.06ms
step:551/2315 train_time:33094ms step_avg:60.06ms
step:552/2315 train_time:33154ms step_avg:60.06ms
step:553/2315 train_time:33214ms step_avg:60.06ms
step:554/2315 train_time:33274ms step_avg:60.06ms
step:555/2315 train_time:33335ms step_avg:60.06ms
step:556/2315 train_time:33395ms step_avg:60.06ms
step:557/2315 train_time:33455ms step_avg:60.06ms
step:558/2315 train_time:33514ms step_avg:60.06ms
step:559/2315 train_time:33575ms step_avg:60.06ms
step:560/2315 train_time:33635ms step_avg:60.06ms
step:561/2315 train_time:33695ms step_avg:60.06ms
step:562/2315 train_time:33754ms step_avg:60.06ms
step:563/2315 train_time:33814ms step_avg:60.06ms
step:564/2315 train_time:33874ms step_avg:60.06ms
step:565/2315 train_time:33934ms step_avg:60.06ms
step:566/2315 train_time:33994ms step_avg:60.06ms
step:567/2315 train_time:34054ms step_avg:60.06ms
step:568/2315 train_time:34114ms step_avg:60.06ms
step:569/2315 train_time:34175ms step_avg:60.06ms
step:570/2315 train_time:34235ms step_avg:60.06ms
step:571/2315 train_time:34296ms step_avg:60.06ms
step:572/2315 train_time:34356ms step_avg:60.06ms
step:573/2315 train_time:34416ms step_avg:60.06ms
step:574/2315 train_time:34475ms step_avg:60.06ms
step:575/2315 train_time:34535ms step_avg:60.06ms
step:576/2315 train_time:34595ms step_avg:60.06ms
step:577/2315 train_time:34655ms step_avg:60.06ms
step:578/2315 train_time:34715ms step_avg:60.06ms
step:579/2315 train_time:34775ms step_avg:60.06ms
step:580/2315 train_time:34835ms step_avg:60.06ms
step:581/2315 train_time:34895ms step_avg:60.06ms
step:582/2315 train_time:34954ms step_avg:60.06ms
step:583/2315 train_time:35014ms step_avg:60.06ms
step:584/2315 train_time:35074ms step_avg:60.06ms
step:585/2315 train_time:35135ms step_avg:60.06ms
step:586/2315 train_time:35195ms step_avg:60.06ms
step:587/2315 train_time:35255ms step_avg:60.06ms
step:588/2315 train_time:35315ms step_avg:60.06ms
step:589/2315 train_time:35375ms step_avg:60.06ms
step:590/2315 train_time:35435ms step_avg:60.06ms
step:591/2315 train_time:35496ms step_avg:60.06ms
step:592/2315 train_time:35555ms step_avg:60.06ms
step:593/2315 train_time:35615ms step_avg:60.06ms
step:594/2315 train_time:35675ms step_avg:60.06ms
step:595/2315 train_time:35735ms step_avg:60.06ms
step:596/2315 train_time:35796ms step_avg:60.06ms
step:597/2315 train_time:35855ms step_avg:60.06ms
step:598/2315 train_time:35915ms step_avg:60.06ms
step:599/2315 train_time:35975ms step_avg:60.06ms
step:600/2315 train_time:36035ms step_avg:60.06ms
step:601/2315 train_time:36095ms step_avg:60.06ms
step:602/2315 train_time:36155ms step_avg:60.06ms
step:603/2315 train_time:36215ms step_avg:60.06ms
step:604/2315 train_time:36275ms step_avg:60.06ms
step:605/2315 train_time:36335ms step_avg:60.06ms
step:606/2315 train_time:36395ms step_avg:60.06ms
step:607/2315 train_time:36455ms step_avg:60.06ms
step:608/2315 train_time:36515ms step_avg:60.06ms
step:609/2315 train_time:36576ms step_avg:60.06ms
step:610/2315 train_time:36636ms step_avg:60.06ms
step:611/2315 train_time:36696ms step_avg:60.06ms
step:612/2315 train_time:36755ms step_avg:60.06ms
step:613/2315 train_time:36815ms step_avg:60.06ms
step:614/2315 train_time:36875ms step_avg:60.06ms
step:615/2315 train_time:36936ms step_avg:60.06ms
step:616/2315 train_time:36995ms step_avg:60.06ms
step:617/2315 train_time:37055ms step_avg:60.06ms
step:618/2315 train_time:37115ms step_avg:60.06ms
step:619/2315 train_time:37175ms step_avg:60.06ms
step:620/2315 train_time:37235ms step_avg:60.06ms
step:621/2315 train_time:37295ms step_avg:60.06ms
step:622/2315 train_time:37355ms step_avg:60.06ms
step:623/2315 train_time:37415ms step_avg:60.06ms
step:624/2315 train_time:37476ms step_avg:60.06ms
step:625/2315 train_time:37536ms step_avg:60.06ms
step:626/2315 train_time:37596ms step_avg:60.06ms
step:627/2315 train_time:37656ms step_avg:60.06ms
step:628/2315 train_time:37715ms step_avg:60.06ms
step:629/2315 train_time:37775ms step_avg:60.06ms
step:630/2315 train_time:37835ms step_avg:60.06ms
step:631/2315 train_time:37895ms step_avg:60.06ms
step:632/2315 train_time:37955ms step_avg:60.06ms
step:633/2315 train_time:38015ms step_avg:60.06ms
step:634/2315 train_time:38075ms step_avg:60.06ms
step:635/2315 train_time:38135ms step_avg:60.05ms
step:636/2315 train_time:38195ms step_avg:60.06ms
step:637/2315 train_time:38256ms step_avg:60.06ms
step:638/2315 train_time:38315ms step_avg:60.06ms
step:639/2315 train_time:38375ms step_avg:60.06ms
step:640/2315 train_time:38436ms step_avg:60.06ms
step:641/2315 train_time:38496ms step_avg:60.06ms
step:642/2315 train_time:38556ms step_avg:60.06ms
step:643/2315 train_time:38617ms step_avg:60.06ms
step:644/2315 train_time:38676ms step_avg:60.06ms
step:645/2315 train_time:38736ms step_avg:60.06ms
step:646/2315 train_time:38796ms step_avg:60.06ms
step:647/2315 train_time:38855ms step_avg:60.05ms
step:648/2315 train_time:38915ms step_avg:60.05ms
step:649/2315 train_time:38975ms step_avg:60.05ms
step:650/2315 train_time:39035ms step_avg:60.05ms
step:651/2315 train_time:39095ms step_avg:60.05ms
step:652/2315 train_time:39155ms step_avg:60.05ms
step:653/2315 train_time:39214ms step_avg:60.05ms
step:654/2315 train_time:39274ms step_avg:60.05ms
step:655/2315 train_time:39334ms step_avg:60.05ms
step:656/2315 train_time:39394ms step_avg:60.05ms
step:657/2315 train_time:39454ms step_avg:60.05ms
step:658/2315 train_time:39514ms step_avg:60.05ms
step:659/2315 train_time:39574ms step_avg:60.05ms
step:660/2315 train_time:39635ms step_avg:60.05ms
step:661/2315 train_time:39694ms step_avg:60.05ms
step:662/2315 train_time:39754ms step_avg:60.05ms
step:663/2315 train_time:39814ms step_avg:60.05ms
step:664/2315 train_time:39874ms step_avg:60.05ms
step:665/2315 train_time:39934ms step_avg:60.05ms
step:666/2315 train_time:39994ms step_avg:60.05ms
step:667/2315 train_time:40054ms step_avg:60.05ms
step:668/2315 train_time:40114ms step_avg:60.05ms
step:669/2315 train_time:40174ms step_avg:60.05ms
step:670/2315 train_time:40234ms step_avg:60.05ms
step:671/2315 train_time:40294ms step_avg:60.05ms
step:672/2315 train_time:40354ms step_avg:60.05ms
step:673/2315 train_time:40415ms step_avg:60.05ms
step:674/2315 train_time:40474ms step_avg:60.05ms
step:675/2315 train_time:40536ms step_avg:60.05ms
step:676/2315 train_time:40596ms step_avg:60.05ms
step:677/2315 train_time:40655ms step_avg:60.05ms
step:678/2315 train_time:40716ms step_avg:60.05ms
step:679/2315 train_time:40776ms step_avg:60.05ms
step:680/2315 train_time:40836ms step_avg:60.05ms
step:681/2315 train_time:40896ms step_avg:60.05ms
step:682/2315 train_time:40955ms step_avg:60.05ms
step:683/2315 train_time:41015ms step_avg:60.05ms
step:684/2315 train_time:41075ms step_avg:60.05ms
step:685/2315 train_time:41135ms step_avg:60.05ms
step:686/2315 train_time:41195ms step_avg:60.05ms
step:687/2315 train_time:41255ms step_avg:60.05ms
step:688/2315 train_time:41315ms step_avg:60.05ms
step:689/2315 train_time:41375ms step_avg:60.05ms
step:690/2315 train_time:41435ms step_avg:60.05ms
step:691/2315 train_time:41496ms step_avg:60.05ms
step:692/2315 train_time:41556ms step_avg:60.05ms
step:693/2315 train_time:41616ms step_avg:60.05ms
step:694/2315 train_time:41676ms step_avg:60.05ms
step:695/2315 train_time:41736ms step_avg:60.05ms
step:696/2315 train_time:41795ms step_avg:60.05ms
step:697/2315 train_time:41855ms step_avg:60.05ms
step:698/2315 train_time:41916ms step_avg:60.05ms
step:699/2315 train_time:41976ms step_avg:60.05ms
step:700/2315 train_time:42036ms step_avg:60.05ms
step:701/2315 train_time:42096ms step_avg:60.05ms
step:702/2315 train_time:42156ms step_avg:60.05ms
step:703/2315 train_time:42216ms step_avg:60.05ms
step:704/2315 train_time:42275ms step_avg:60.05ms
step:705/2315 train_time:42336ms step_avg:60.05ms
step:706/2315 train_time:42395ms step_avg:60.05ms
step:707/2315 train_time:42455ms step_avg:60.05ms
step:708/2315 train_time:42515ms step_avg:60.05ms
step:709/2315 train_time:42575ms step_avg:60.05ms
step:710/2315 train_time:42635ms step_avg:60.05ms
step:711/2315 train_time:42695ms step_avg:60.05ms
step:712/2315 train_time:42755ms step_avg:60.05ms
step:713/2315 train_time:42814ms step_avg:60.05ms
step:714/2315 train_time:42874ms step_avg:60.05ms
step:715/2315 train_time:42935ms step_avg:60.05ms
step:716/2315 train_time:42995ms step_avg:60.05ms
step:717/2315 train_time:43055ms step_avg:60.05ms
step:718/2315 train_time:43115ms step_avg:60.05ms
step:719/2315 train_time:43176ms step_avg:60.05ms
step:720/2315 train_time:43236ms step_avg:60.05ms
step:721/2315 train_time:43296ms step_avg:60.05ms
step:722/2315 train_time:43356ms step_avg:60.05ms
step:723/2315 train_time:43416ms step_avg:60.05ms
step:724/2315 train_time:43475ms step_avg:60.05ms
step:725/2315 train_time:43535ms step_avg:60.05ms
step:726/2315 train_time:43595ms step_avg:60.05ms
step:727/2315 train_time:43655ms step_avg:60.05ms
step:728/2315 train_time:43715ms step_avg:60.05ms
step:729/2315 train_time:43775ms step_avg:60.05ms
step:730/2315 train_time:43835ms step_avg:60.05ms
step:731/2315 train_time:43895ms step_avg:60.05ms
step:732/2315 train_time:43954ms step_avg:60.05ms
step:733/2315 train_time:44015ms step_avg:60.05ms
step:734/2315 train_time:44075ms step_avg:60.05ms
step:735/2315 train_time:44135ms step_avg:60.05ms
step:736/2315 train_time:44195ms step_avg:60.05ms
step:737/2315 train_time:44256ms step_avg:60.05ms
step:738/2315 train_time:44315ms step_avg:60.05ms
step:739/2315 train_time:44375ms step_avg:60.05ms
step:740/2315 train_time:44435ms step_avg:60.05ms
step:741/2315 train_time:44495ms step_avg:60.05ms
step:742/2315 train_time:44556ms step_avg:60.05ms
step:743/2315 train_time:44616ms step_avg:60.05ms
step:744/2315 train_time:44676ms step_avg:60.05ms
step:745/2315 train_time:44736ms step_avg:60.05ms
step:746/2315 train_time:44796ms step_avg:60.05ms
step:747/2315 train_time:44856ms step_avg:60.05ms
step:748/2315 train_time:44916ms step_avg:60.05ms
step:749/2315 train_time:44976ms step_avg:60.05ms
step:750/2315 train_time:45036ms step_avg:60.05ms
step:750/2315 val_loss:3.6804 train_time:45098ms step_avg:60.13ms
step:751/2315 train_time:45121ms step_avg:60.08ms
step:752/2315 train_time:45162ms step_avg:60.06ms
step:753/2315 train_time:45223ms step_avg:60.06ms
step:754/2315 train_time:45286ms step_avg:60.06ms
step:755/2315 train_time:45346ms step_avg:60.06ms
step:756/2315 train_time:45406ms step_avg:60.06ms
step:757/2315 train_time:45465ms step_avg:60.06ms
step:758/2315 train_time:45524ms step_avg:60.06ms
step:759/2315 train_time:45584ms step_avg:60.06ms
step:760/2315 train_time:45644ms step_avg:60.06ms
step:761/2315 train_time:45703ms step_avg:60.06ms
step:762/2315 train_time:45763ms step_avg:60.06ms
step:763/2315 train_time:45822ms step_avg:60.06ms
step:764/2315 train_time:45882ms step_avg:60.06ms
step:765/2315 train_time:45942ms step_avg:60.06ms
step:766/2315 train_time:46003ms step_avg:60.06ms
step:767/2315 train_time:46065ms step_avg:60.06ms
step:768/2315 train_time:46127ms step_avg:60.06ms
step:769/2315 train_time:46189ms step_avg:60.06ms
step:770/2315 train_time:46249ms step_avg:60.06ms
step:771/2315 train_time:46310ms step_avg:60.06ms
step:772/2315 train_time:46371ms step_avg:60.07ms
step:773/2315 train_time:46432ms step_avg:60.07ms
step:774/2315 train_time:46493ms step_avg:60.07ms
step:775/2315 train_time:46554ms step_avg:60.07ms
step:776/2315 train_time:46614ms step_avg:60.07ms
step:777/2315 train_time:46675ms step_avg:60.07ms
step:778/2315 train_time:46736ms step_avg:60.07ms
step:779/2315 train_time:46796ms step_avg:60.07ms
step:780/2315 train_time:46856ms step_avg:60.07ms
step:781/2315 train_time:46917ms step_avg:60.07ms
step:782/2315 train_time:46978ms step_avg:60.07ms
step:783/2315 train_time:47039ms step_avg:60.07ms
step:784/2315 train_time:47100ms step_avg:60.08ms
step:785/2315 train_time:47161ms step_avg:60.08ms
step:786/2315 train_time:47221ms step_avg:60.08ms
step:787/2315 train_time:47282ms step_avg:60.08ms
step:788/2315 train_time:47343ms step_avg:60.08ms
step:789/2315 train_time:47404ms step_avg:60.08ms
step:790/2315 train_time:47465ms step_avg:60.08ms
step:791/2315 train_time:47525ms step_avg:60.08ms
step:792/2315 train_time:47587ms step_avg:60.08ms
step:793/2315 train_time:47648ms step_avg:60.09ms
step:794/2315 train_time:47709ms step_avg:60.09ms
step:795/2315 train_time:47769ms step_avg:60.09ms
step:796/2315 train_time:47829ms step_avg:60.09ms
step:797/2315 train_time:47891ms step_avg:60.09ms
step:798/2315 train_time:47951ms step_avg:60.09ms
step:799/2315 train_time:48012ms step_avg:60.09ms
step:800/2315 train_time:48073ms step_avg:60.09ms
step:801/2315 train_time:48134ms step_avg:60.09ms
step:802/2315 train_time:48195ms step_avg:60.09ms
step:803/2315 train_time:48256ms step_avg:60.10ms
step:804/2315 train_time:48317ms step_avg:60.10ms
step:805/2315 train_time:48379ms step_avg:60.10ms
step:806/2315 train_time:48439ms step_avg:60.10ms
step:807/2315 train_time:48500ms step_avg:60.10ms
step:808/2315 train_time:48561ms step_avg:60.10ms
step:809/2315 train_time:48622ms step_avg:60.10ms
step:810/2315 train_time:48682ms step_avg:60.10ms
step:811/2315 train_time:48742ms step_avg:60.10ms
step:812/2315 train_time:48803ms step_avg:60.10ms
step:813/2315 train_time:48863ms step_avg:60.10ms
step:814/2315 train_time:48924ms step_avg:60.10ms
step:815/2315 train_time:48985ms step_avg:60.10ms
step:816/2315 train_time:49046ms step_avg:60.11ms
step:817/2315 train_time:49107ms step_avg:60.11ms
step:818/2315 train_time:49168ms step_avg:60.11ms
step:819/2315 train_time:49229ms step_avg:60.11ms
step:820/2315 train_time:49289ms step_avg:60.11ms
step:821/2315 train_time:49350ms step_avg:60.11ms
step:822/2315 train_time:49411ms step_avg:60.11ms
step:823/2315 train_time:49472ms step_avg:60.11ms
step:824/2315 train_time:49534ms step_avg:60.11ms
step:825/2315 train_time:49595ms step_avg:60.12ms
step:826/2315 train_time:49656ms step_avg:60.12ms
step:827/2315 train_time:49717ms step_avg:60.12ms
step:828/2315 train_time:49778ms step_avg:60.12ms
step:829/2315 train_time:49838ms step_avg:60.12ms
step:830/2315 train_time:49900ms step_avg:60.12ms
step:831/2315 train_time:49960ms step_avg:60.12ms
step:832/2315 train_time:50021ms step_avg:60.12ms
step:833/2315 train_time:50081ms step_avg:60.12ms
step:834/2315 train_time:50141ms step_avg:60.12ms
step:835/2315 train_time:50202ms step_avg:60.12ms
step:836/2315 train_time:50263ms step_avg:60.12ms
step:837/2315 train_time:50324ms step_avg:60.12ms
step:838/2315 train_time:50385ms step_avg:60.13ms
step:839/2315 train_time:50446ms step_avg:60.13ms
step:840/2315 train_time:50506ms step_avg:60.13ms
step:841/2315 train_time:50567ms step_avg:60.13ms
step:842/2315 train_time:50627ms step_avg:60.13ms
step:843/2315 train_time:50688ms step_avg:60.13ms
step:844/2315 train_time:50749ms step_avg:60.13ms
step:845/2315 train_time:50810ms step_avg:60.13ms
step:846/2315 train_time:50870ms step_avg:60.13ms
step:847/2315 train_time:50931ms step_avg:60.13ms
step:848/2315 train_time:50992ms step_avg:60.13ms
step:849/2315 train_time:51053ms step_avg:60.13ms
step:850/2315 train_time:51114ms step_avg:60.13ms
step:851/2315 train_time:51175ms step_avg:60.14ms
step:852/2315 train_time:51236ms step_avg:60.14ms
step:853/2315 train_time:51297ms step_avg:60.14ms
step:854/2315 train_time:51358ms step_avg:60.14ms
step:855/2315 train_time:51422ms step_avg:60.14ms
step:856/2315 train_time:51479ms step_avg:60.14ms
step:857/2315 train_time:51539ms step_avg:60.14ms
step:858/2315 train_time:51600ms step_avg:60.14ms
step:859/2315 train_time:51661ms step_avg:60.14ms
step:860/2315 train_time:51721ms step_avg:60.14ms
step:861/2315 train_time:51782ms step_avg:60.14ms
step:862/2315 train_time:51843ms step_avg:60.14ms
step:863/2315 train_time:51904ms step_avg:60.14ms
step:864/2315 train_time:51965ms step_avg:60.14ms
step:865/2315 train_time:52026ms step_avg:60.15ms
step:866/2315 train_time:52087ms step_avg:60.15ms
step:867/2315 train_time:52148ms step_avg:60.15ms
step:868/2315 train_time:52208ms step_avg:60.15ms
step:869/2315 train_time:52269ms step_avg:60.15ms
step:870/2315 train_time:52329ms step_avg:60.15ms
step:871/2315 train_time:52390ms step_avg:60.15ms
step:872/2315 train_time:52451ms step_avg:60.15ms
step:873/2315 train_time:52512ms step_avg:60.15ms
step:874/2315 train_time:52573ms step_avg:60.15ms
step:875/2315 train_time:52634ms step_avg:60.15ms
step:876/2315 train_time:52695ms step_avg:60.15ms
step:877/2315 train_time:52756ms step_avg:60.15ms
step:878/2315 train_time:52816ms step_avg:60.16ms
step:879/2315 train_time:52878ms step_avg:60.16ms
step:880/2315 train_time:52939ms step_avg:60.16ms
step:881/2315 train_time:53001ms step_avg:60.16ms
step:882/2315 train_time:53062ms step_avg:60.16ms
step:883/2315 train_time:53122ms step_avg:60.16ms
step:884/2315 train_time:53182ms step_avg:60.16ms
step:885/2315 train_time:53242ms step_avg:60.16ms
step:886/2315 train_time:53303ms step_avg:60.16ms
step:887/2315 train_time:53364ms step_avg:60.16ms
step:888/2315 train_time:53425ms step_avg:60.16ms
step:889/2315 train_time:53485ms step_avg:60.16ms
step:890/2315 train_time:53546ms step_avg:60.16ms
step:891/2315 train_time:53606ms step_avg:60.16ms
step:892/2315 train_time:53667ms step_avg:60.16ms
step:893/2315 train_time:53727ms step_avg:60.17ms
step:894/2315 train_time:53788ms step_avg:60.17ms
step:895/2315 train_time:53849ms step_avg:60.17ms
step:896/2315 train_time:53910ms step_avg:60.17ms
step:897/2315 train_time:53971ms step_avg:60.17ms
step:898/2315 train_time:54032ms step_avg:60.17ms
step:899/2315 train_time:54093ms step_avg:60.17ms
step:900/2315 train_time:54154ms step_avg:60.17ms
step:901/2315 train_time:54215ms step_avg:60.17ms
step:902/2315 train_time:54275ms step_avg:60.17ms
step:903/2315 train_time:54337ms step_avg:60.17ms
step:904/2315 train_time:54398ms step_avg:60.17ms
step:905/2315 train_time:54458ms step_avg:60.18ms
step:906/2315 train_time:54519ms step_avg:60.18ms
step:907/2315 train_time:54580ms step_avg:60.18ms
step:908/2315 train_time:54641ms step_avg:60.18ms
step:909/2315 train_time:54702ms step_avg:60.18ms
step:910/2315 train_time:54762ms step_avg:60.18ms
step:911/2315 train_time:54823ms step_avg:60.18ms
step:912/2315 train_time:54883ms step_avg:60.18ms
step:913/2315 train_time:54944ms step_avg:60.18ms
step:914/2315 train_time:55005ms step_avg:60.18ms
step:915/2315 train_time:55066ms step_avg:60.18ms
step:916/2315 train_time:55127ms step_avg:60.18ms
step:917/2315 train_time:55186ms step_avg:60.18ms
step:918/2315 train_time:55248ms step_avg:60.18ms
step:919/2315 train_time:55309ms step_avg:60.18ms
step:920/2315 train_time:55370ms step_avg:60.18ms
step:921/2315 train_time:55430ms step_avg:60.18ms
step:922/2315 train_time:55491ms step_avg:60.19ms
step:923/2315 train_time:55552ms step_avg:60.19ms
step:924/2315 train_time:55613ms step_avg:60.19ms
step:925/2315 train_time:55674ms step_avg:60.19ms
step:926/2315 train_time:55735ms step_avg:60.19ms
step:927/2315 train_time:55795ms step_avg:60.19ms
step:928/2315 train_time:55857ms step_avg:60.19ms
step:929/2315 train_time:55917ms step_avg:60.19ms
step:930/2315 train_time:55978ms step_avg:60.19ms
step:931/2315 train_time:56039ms step_avg:60.19ms
step:932/2315 train_time:56099ms step_avg:60.19ms
step:933/2315 train_time:56160ms step_avg:60.19ms
step:934/2315 train_time:56220ms step_avg:60.19ms
step:935/2315 train_time:56281ms step_avg:60.19ms
step:936/2315 train_time:56341ms step_avg:60.19ms
step:937/2315 train_time:56402ms step_avg:60.19ms
step:938/2315 train_time:56463ms step_avg:60.20ms
step:939/2315 train_time:56524ms step_avg:60.20ms
step:940/2315 train_time:56585ms step_avg:60.20ms
step:941/2315 train_time:56646ms step_avg:60.20ms
step:942/2315 train_time:56707ms step_avg:60.20ms
step:943/2315 train_time:56767ms step_avg:60.20ms
step:944/2315 train_time:56828ms step_avg:60.20ms
step:945/2315 train_time:56888ms step_avg:60.20ms
step:946/2315 train_time:56948ms step_avg:60.20ms
step:947/2315 train_time:57009ms step_avg:60.20ms
step:948/2315 train_time:57069ms step_avg:60.20ms
step:949/2315 train_time:57130ms step_avg:60.20ms
step:950/2315 train_time:57191ms step_avg:60.20ms
step:951/2315 train_time:57252ms step_avg:60.20ms
step:952/2315 train_time:57313ms step_avg:60.20ms
step:953/2315 train_time:57374ms step_avg:60.20ms
step:954/2315 train_time:57435ms step_avg:60.20ms
step:955/2315 train_time:57496ms step_avg:60.21ms
step:956/2315 train_time:57557ms step_avg:60.21ms
step:957/2315 train_time:57618ms step_avg:60.21ms
step:958/2315 train_time:57679ms step_avg:60.21ms
step:959/2315 train_time:57740ms step_avg:60.21ms
step:960/2315 train_time:57800ms step_avg:60.21ms
step:961/2315 train_time:57861ms step_avg:60.21ms
step:962/2315 train_time:57921ms step_avg:60.21ms
step:963/2315 train_time:57982ms step_avg:60.21ms
step:964/2315 train_time:58043ms step_avg:60.21ms
step:965/2315 train_time:58104ms step_avg:60.21ms
step:966/2315 train_time:58165ms step_avg:60.21ms
step:967/2315 train_time:58226ms step_avg:60.21ms
step:968/2315 train_time:58287ms step_avg:60.21ms
step:969/2315 train_time:58348ms step_avg:60.21ms
step:970/2315 train_time:58408ms step_avg:60.21ms
step:971/2315 train_time:58469ms step_avg:60.22ms
step:972/2315 train_time:58529ms step_avg:60.22ms
step:973/2315 train_time:58590ms step_avg:60.22ms
step:974/2315 train_time:58651ms step_avg:60.22ms
step:975/2315 train_time:58712ms step_avg:60.22ms
step:976/2315 train_time:58772ms step_avg:60.22ms
step:977/2315 train_time:58834ms step_avg:60.22ms
step:978/2315 train_time:58895ms step_avg:60.22ms
step:979/2315 train_time:58956ms step_avg:60.22ms
step:980/2315 train_time:59017ms step_avg:60.22ms
step:981/2315 train_time:59078ms step_avg:60.22ms
step:982/2315 train_time:59139ms step_avg:60.22ms
step:983/2315 train_time:59199ms step_avg:60.22ms
step:984/2315 train_time:59260ms step_avg:60.22ms
step:985/2315 train_time:59321ms step_avg:60.22ms
step:986/2315 train_time:59381ms step_avg:60.22ms
step:987/2315 train_time:59441ms step_avg:60.22ms
step:988/2315 train_time:59501ms step_avg:60.22ms
step:989/2315 train_time:59562ms step_avg:60.22ms
step:990/2315 train_time:59622ms step_avg:60.22ms
step:991/2315 train_time:59683ms step_avg:60.23ms
step:992/2315 train_time:59744ms step_avg:60.23ms
step:993/2315 train_time:59805ms step_avg:60.23ms
step:994/2315 train_time:59866ms step_avg:60.23ms
step:995/2315 train_time:59928ms step_avg:60.23ms
step:996/2315 train_time:59988ms step_avg:60.23ms
step:997/2315 train_time:60049ms step_avg:60.23ms
step:998/2315 train_time:60109ms step_avg:60.23ms
step:999/2315 train_time:60170ms step_avg:60.23ms
step:1000/2315 train_time:60231ms step_avg:60.23ms
step:1000/2315 val_loss:3.5712 train_time:60294ms step_avg:60.29ms
step:1001/2315 train_time:60315ms step_avg:60.26ms
step:1002/2315 train_time:60357ms step_avg:60.24ms
step:1003/2315 train_time:60423ms step_avg:60.24ms
step:1004/2315 train_time:60486ms step_avg:60.25ms
step:1005/2315 train_time:60547ms step_avg:60.25ms
step:1006/2315 train_time:60609ms step_avg:60.25ms
step:1007/2315 train_time:60669ms step_avg:60.25ms
step:1008/2315 train_time:60729ms step_avg:60.25ms
step:1009/2315 train_time:60790ms step_avg:60.25ms
step:1010/2315 train_time:60850ms step_avg:60.25ms
step:1011/2315 train_time:60910ms step_avg:60.25ms
step:1012/2315 train_time:60970ms step_avg:60.25ms
step:1013/2315 train_time:61030ms step_avg:60.25ms
step:1014/2315 train_time:61091ms step_avg:60.25ms
step:1015/2315 train_time:61151ms step_avg:60.25ms
step:1016/2315 train_time:61211ms step_avg:60.25ms
step:1017/2315 train_time:61273ms step_avg:60.25ms
step:1018/2315 train_time:61336ms step_avg:60.25ms
step:1019/2315 train_time:61398ms step_avg:60.25ms
step:1020/2315 train_time:61461ms step_avg:60.26ms
step:1021/2315 train_time:61522ms step_avg:60.26ms
step:1022/2315 train_time:61582ms step_avg:60.26ms
step:1023/2315 train_time:61642ms step_avg:60.26ms
step:1024/2315 train_time:61702ms step_avg:60.26ms
step:1025/2315 train_time:61763ms step_avg:60.26ms
step:1026/2315 train_time:61824ms step_avg:60.26ms
step:1027/2315 train_time:61884ms step_avg:60.26ms
step:1028/2315 train_time:61944ms step_avg:60.26ms
step:1029/2315 train_time:62004ms step_avg:60.26ms
step:1030/2315 train_time:62064ms step_avg:60.26ms
step:1031/2315 train_time:62125ms step_avg:60.26ms
step:1032/2315 train_time:62186ms step_avg:60.26ms
step:1033/2315 train_time:62246ms step_avg:60.26ms
step:1034/2315 train_time:62307ms step_avg:60.26ms
step:1035/2315 train_time:62368ms step_avg:60.26ms
step:1036/2315 train_time:62430ms step_avg:60.26ms
step:1037/2315 train_time:62492ms step_avg:60.26ms
step:1038/2315 train_time:62553ms step_avg:60.26ms
step:1039/2315 train_time:62615ms step_avg:60.26ms
step:1040/2315 train_time:62675ms step_avg:60.26ms
step:1041/2315 train_time:62737ms step_avg:60.27ms
step:1042/2315 train_time:62797ms step_avg:60.27ms
step:1043/2315 train_time:62859ms step_avg:60.27ms
step:1044/2315 train_time:62919ms step_avg:60.27ms
step:1045/2315 train_time:62980ms step_avg:60.27ms
step:1046/2315 train_time:63040ms step_avg:60.27ms
step:1047/2315 train_time:63100ms step_avg:60.27ms
step:1048/2315 train_time:63161ms step_avg:60.27ms
step:1049/2315 train_time:63221ms step_avg:60.27ms
step:1050/2315 train_time:63282ms step_avg:60.27ms
step:1051/2315 train_time:63343ms step_avg:60.27ms
step:1052/2315 train_time:63404ms step_avg:60.27ms
step:1053/2315 train_time:63465ms step_avg:60.27ms
step:1054/2315 train_time:63526ms step_avg:60.27ms
step:1055/2315 train_time:63587ms step_avg:60.27ms
step:1056/2315 train_time:63647ms step_avg:60.27ms
step:1057/2315 train_time:63708ms step_avg:60.27ms
step:1058/2315 train_time:63769ms step_avg:60.27ms
step:1059/2315 train_time:63830ms step_avg:60.27ms
step:1060/2315 train_time:63891ms step_avg:60.27ms
step:1061/2315 train_time:63952ms step_avg:60.28ms
step:1062/2315 train_time:64013ms step_avg:60.28ms
step:1063/2315 train_time:64074ms step_avg:60.28ms
step:1064/2315 train_time:64134ms step_avg:60.28ms
step:1065/2315 train_time:64196ms step_avg:60.28ms
step:1066/2315 train_time:64257ms step_avg:60.28ms
step:1067/2315 train_time:64318ms step_avg:60.28ms
step:1068/2315 train_time:64378ms step_avg:60.28ms
step:1069/2315 train_time:64439ms step_avg:60.28ms
step:1070/2315 train_time:64499ms step_avg:60.28ms
step:1071/2315 train_time:64560ms step_avg:60.28ms
step:1072/2315 train_time:64621ms step_avg:60.28ms
step:1073/2315 train_time:64681ms step_avg:60.28ms
step:1074/2315 train_time:64742ms step_avg:60.28ms
step:1075/2315 train_time:64802ms step_avg:60.28ms
step:1076/2315 train_time:64864ms step_avg:60.28ms
step:1077/2315 train_time:64925ms step_avg:60.28ms
step:1078/2315 train_time:64986ms step_avg:60.28ms
step:1079/2315 train_time:65047ms step_avg:60.28ms
step:1080/2315 train_time:65107ms step_avg:60.28ms
step:1081/2315 train_time:65168ms step_avg:60.28ms
step:1082/2315 train_time:65229ms step_avg:60.29ms
step:1083/2315 train_time:65290ms step_avg:60.29ms
step:1084/2315 train_time:65351ms step_avg:60.29ms
step:1085/2315 train_time:65413ms step_avg:60.29ms
step:1086/2315 train_time:65474ms step_avg:60.29ms
step:1087/2315 train_time:65535ms step_avg:60.29ms
step:1088/2315 train_time:65596ms step_avg:60.29ms
step:1089/2315 train_time:65657ms step_avg:60.29ms
step:1090/2315 train_time:65718ms step_avg:60.29ms
step:1091/2315 train_time:65778ms step_avg:60.29ms
step:1092/2315 train_time:65838ms step_avg:60.29ms
step:1093/2315 train_time:65900ms step_avg:60.29ms
step:1094/2315 train_time:65960ms step_avg:60.29ms
step:1095/2315 train_time:66021ms step_avg:60.29ms
step:1096/2315 train_time:66082ms step_avg:60.29ms
step:1097/2315 train_time:66143ms step_avg:60.29ms
step:1098/2315 train_time:66203ms step_avg:60.29ms
step:1099/2315 train_time:66265ms step_avg:60.30ms
step:1100/2315 train_time:66326ms step_avg:60.30ms
step:1101/2315 train_time:66386ms step_avg:60.30ms
step:1102/2315 train_time:66447ms step_avg:60.30ms
step:1103/2315 train_time:66508ms step_avg:60.30ms
step:1104/2315 train_time:66569ms step_avg:60.30ms
step:1105/2315 train_time:66629ms step_avg:60.30ms
step:1106/2315 train_time:66690ms step_avg:60.30ms
step:1107/2315 train_time:66751ms step_avg:60.30ms
step:1108/2315 train_time:66812ms step_avg:60.30ms
step:1109/2315 train_time:66873ms step_avg:60.30ms
step:1110/2315 train_time:66934ms step_avg:60.30ms
step:1111/2315 train_time:66995ms step_avg:60.30ms
step:1112/2315 train_time:67056ms step_avg:60.30ms
step:1113/2315 train_time:67117ms step_avg:60.30ms
step:1114/2315 train_time:67178ms step_avg:60.30ms
step:1115/2315 train_time:67239ms step_avg:60.30ms
step:1116/2315 train_time:67300ms step_avg:60.30ms
step:1117/2315 train_time:67361ms step_avg:60.30ms
step:1118/2315 train_time:67421ms step_avg:60.30ms
step:1119/2315 train_time:67482ms step_avg:60.31ms
step:1120/2315 train_time:67543ms step_avg:60.31ms
step:1121/2315 train_time:67604ms step_avg:60.31ms
step:1122/2315 train_time:67664ms step_avg:60.31ms
step:1123/2315 train_time:67725ms step_avg:60.31ms
step:1124/2315 train_time:67785ms step_avg:60.31ms
step:1125/2315 train_time:67846ms step_avg:60.31ms
step:1126/2315 train_time:67906ms step_avg:60.31ms
step:1127/2315 train_time:67967ms step_avg:60.31ms
step:1128/2315 train_time:68028ms step_avg:60.31ms
step:1129/2315 train_time:68089ms step_avg:60.31ms
step:1130/2315 train_time:68150ms step_avg:60.31ms
step:1131/2315 train_time:68211ms step_avg:60.31ms
step:1132/2315 train_time:68272ms step_avg:60.31ms
step:1133/2315 train_time:68333ms step_avg:60.31ms
step:1134/2315 train_time:68394ms step_avg:60.31ms
step:1135/2315 train_time:68455ms step_avg:60.31ms
step:1136/2315 train_time:68516ms step_avg:60.31ms
step:1137/2315 train_time:68577ms step_avg:60.31ms
step:1138/2315 train_time:68638ms step_avg:60.31ms
step:1139/2315 train_time:68698ms step_avg:60.31ms
step:1140/2315 train_time:68759ms step_avg:60.31ms
step:1141/2315 train_time:68819ms step_avg:60.32ms
step:1142/2315 train_time:68880ms step_avg:60.32ms
step:1143/2315 train_time:68941ms step_avg:60.32ms
step:1144/2315 train_time:69002ms step_avg:60.32ms
step:1145/2315 train_time:69062ms step_avg:60.32ms
step:1146/2315 train_time:69123ms step_avg:60.32ms
step:1147/2315 train_time:69184ms step_avg:60.32ms
step:1148/2315 train_time:69245ms step_avg:60.32ms
step:1149/2315 train_time:69305ms step_avg:60.32ms
step:1150/2315 train_time:69366ms step_avg:60.32ms
step:1151/2315 train_time:69426ms step_avg:60.32ms
step:1152/2315 train_time:69486ms step_avg:60.32ms
step:1153/2315 train_time:69547ms step_avg:60.32ms
step:1154/2315 train_time:69608ms step_avg:60.32ms
step:1155/2315 train_time:69669ms step_avg:60.32ms
step:1156/2315 train_time:69730ms step_avg:60.32ms
step:1157/2315 train_time:69791ms step_avg:60.32ms
step:1158/2315 train_time:69852ms step_avg:60.32ms
step:1159/2315 train_time:69913ms step_avg:60.32ms
step:1160/2315 train_time:69974ms step_avg:60.32ms
step:1161/2315 train_time:70035ms step_avg:60.32ms
step:1162/2315 train_time:70096ms step_avg:60.32ms
step:1163/2315 train_time:70157ms step_avg:60.32ms
step:1164/2315 train_time:70217ms step_avg:60.32ms
step:1165/2315 train_time:70278ms step_avg:60.32ms
step:1166/2315 train_time:70339ms step_avg:60.33ms
step:1167/2315 train_time:70400ms step_avg:60.33ms
step:1168/2315 train_time:70460ms step_avg:60.33ms
step:1169/2315 train_time:70521ms step_avg:60.33ms
step:1170/2315 train_time:70581ms step_avg:60.33ms
step:1171/2315 train_time:70642ms step_avg:60.33ms
step:1172/2315 train_time:70703ms step_avg:60.33ms
step:1173/2315 train_time:70764ms step_avg:60.33ms
step:1174/2315 train_time:70825ms step_avg:60.33ms
step:1175/2315 train_time:70885ms step_avg:60.33ms
step:1176/2315 train_time:70946ms step_avg:60.33ms
step:1177/2315 train_time:71007ms step_avg:60.33ms
step:1178/2315 train_time:71068ms step_avg:60.33ms
step:1179/2315 train_time:71130ms step_avg:60.33ms
step:1180/2315 train_time:71190ms step_avg:60.33ms
step:1181/2315 train_time:71252ms step_avg:60.33ms
step:1182/2315 train_time:71313ms step_avg:60.33ms
step:1183/2315 train_time:71374ms step_avg:60.33ms
step:1184/2315 train_time:71434ms step_avg:60.33ms
step:1185/2315 train_time:71495ms step_avg:60.33ms
step:1186/2315 train_time:71556ms step_avg:60.33ms
step:1187/2315 train_time:71617ms step_avg:60.33ms
step:1188/2315 train_time:71678ms step_avg:60.34ms
step:1189/2315 train_time:71739ms step_avg:60.34ms
step:1190/2315 train_time:71800ms step_avg:60.34ms
step:1191/2315 train_time:71860ms step_avg:60.34ms
step:1192/2315 train_time:71921ms step_avg:60.34ms
step:1193/2315 train_time:71981ms step_avg:60.34ms
step:1194/2315 train_time:72042ms step_avg:60.34ms
step:1195/2315 train_time:72103ms step_avg:60.34ms
step:1196/2315 train_time:72164ms step_avg:60.34ms
step:1197/2315 train_time:72225ms step_avg:60.34ms
step:1198/2315 train_time:72286ms step_avg:60.34ms
step:1199/2315 train_time:72346ms step_avg:60.34ms
step:1200/2315 train_time:72408ms step_avg:60.34ms
step:1201/2315 train_time:72469ms step_avg:60.34ms
step:1202/2315 train_time:72529ms step_avg:60.34ms
step:1203/2315 train_time:72590ms step_avg:60.34ms
step:1204/2315 train_time:72651ms step_avg:60.34ms
step:1205/2315 train_time:72712ms step_avg:60.34ms
step:1206/2315 train_time:72773ms step_avg:60.34ms
step:1207/2315 train_time:72834ms step_avg:60.34ms
step:1208/2315 train_time:72895ms step_avg:60.34ms
step:1209/2315 train_time:72956ms step_avg:60.34ms
step:1210/2315 train_time:73017ms step_avg:60.34ms
step:1211/2315 train_time:73078ms step_avg:60.34ms
step:1212/2315 train_time:73138ms step_avg:60.35ms
step:1213/2315 train_time:73199ms step_avg:60.35ms
step:1214/2315 train_time:73259ms step_avg:60.35ms
step:1215/2315 train_time:73321ms step_avg:60.35ms
step:1216/2315 train_time:73381ms step_avg:60.35ms
step:1217/2315 train_time:73441ms step_avg:60.35ms
step:1218/2315 train_time:73502ms step_avg:60.35ms
step:1219/2315 train_time:73563ms step_avg:60.35ms
step:1220/2315 train_time:73624ms step_avg:60.35ms
step:1221/2315 train_time:73685ms step_avg:60.35ms
step:1222/2315 train_time:73745ms step_avg:60.35ms
step:1223/2315 train_time:73805ms step_avg:60.35ms
step:1224/2315 train_time:73866ms step_avg:60.35ms
step:1225/2315 train_time:73926ms step_avg:60.35ms
step:1226/2315 train_time:73987ms step_avg:60.35ms
step:1227/2315 train_time:74049ms step_avg:60.35ms
step:1228/2315 train_time:74111ms step_avg:60.35ms
step:1229/2315 train_time:74172ms step_avg:60.35ms
step:1230/2315 train_time:74234ms step_avg:60.35ms
step:1231/2315 train_time:74294ms step_avg:60.35ms
step:1232/2315 train_time:74355ms step_avg:60.35ms
step:1233/2315 train_time:74416ms step_avg:60.35ms
step:1234/2315 train_time:74476ms step_avg:60.35ms
step:1235/2315 train_time:74537ms step_avg:60.35ms
step:1236/2315 train_time:74598ms step_avg:60.35ms
step:1237/2315 train_time:74659ms step_avg:60.36ms
step:1238/2315 train_time:74720ms step_avg:60.36ms
step:1239/2315 train_time:74780ms step_avg:60.36ms
step:1240/2315 train_time:74840ms step_avg:60.36ms
step:1241/2315 train_time:74901ms step_avg:60.36ms
step:1242/2315 train_time:74962ms step_avg:60.36ms
step:1243/2315 train_time:75022ms step_avg:60.36ms
step:1244/2315 train_time:75083ms step_avg:60.36ms
step:1245/2315 train_time:75144ms step_avg:60.36ms
step:1246/2315 train_time:75205ms step_avg:60.36ms
step:1247/2315 train_time:75266ms step_avg:60.36ms
step:1248/2315 train_time:75328ms step_avg:60.36ms
step:1249/2315 train_time:75388ms step_avg:60.36ms
step:1250/2315 train_time:75449ms step_avg:60.36ms
step:1250/2315 val_loss:3.5127 train_time:75512ms step_avg:60.41ms
step:1251/2315 train_time:75533ms step_avg:60.38ms
step:1252/2315 train_time:75574ms step_avg:60.36ms
step:1253/2315 train_time:75640ms step_avg:60.37ms
step:1254/2315 train_time:75703ms step_avg:60.37ms
step:1255/2315 train_time:75764ms step_avg:60.37ms
step:1256/2315 train_time:75824ms step_avg:60.37ms
step:1257/2315 train_time:75885ms step_avg:60.37ms
step:1258/2315 train_time:75945ms step_avg:60.37ms
step:1259/2315 train_time:76005ms step_avg:60.37ms
step:1260/2315 train_time:76065ms step_avg:60.37ms
step:1261/2315 train_time:76126ms step_avg:60.37ms
step:1262/2315 train_time:76186ms step_avg:60.37ms
step:1263/2315 train_time:76246ms step_avg:60.37ms
step:1264/2315 train_time:76306ms step_avg:60.37ms
step:1265/2315 train_time:76366ms step_avg:60.37ms
step:1266/2315 train_time:76426ms step_avg:60.37ms
step:1267/2315 train_time:76488ms step_avg:60.37ms
step:1268/2315 train_time:76550ms step_avg:60.37ms
step:1269/2315 train_time:76614ms step_avg:60.37ms
step:1270/2315 train_time:76676ms step_avg:60.37ms
step:1271/2315 train_time:76736ms step_avg:60.37ms
step:1272/2315 train_time:76798ms step_avg:60.38ms
step:1273/2315 train_time:76858ms step_avg:60.38ms
step:1274/2315 train_time:76919ms step_avg:60.38ms
step:1275/2315 train_time:76979ms step_avg:60.38ms
step:1276/2315 train_time:77040ms step_avg:60.38ms
step:1277/2315 train_time:77101ms step_avg:60.38ms
step:1278/2315 train_time:77161ms step_avg:60.38ms
step:1279/2315 train_time:77221ms step_avg:60.38ms
step:1280/2315 train_time:77282ms step_avg:60.38ms
step:1281/2315 train_time:77343ms step_avg:60.38ms
step:1282/2315 train_time:77403ms step_avg:60.38ms
step:1283/2315 train_time:77465ms step_avg:60.38ms
step:1284/2315 train_time:77527ms step_avg:60.38ms
step:1285/2315 train_time:77589ms step_avg:60.38ms
step:1286/2315 train_time:77649ms step_avg:60.38ms
step:1287/2315 train_time:77709ms step_avg:60.38ms
step:1288/2315 train_time:77770ms step_avg:60.38ms
step:1289/2315 train_time:77830ms step_avg:60.38ms
step:1290/2315 train_time:77891ms step_avg:60.38ms
step:1291/2315 train_time:77951ms step_avg:60.38ms
step:1292/2315 train_time:78011ms step_avg:60.38ms
step:1293/2315 train_time:78072ms step_avg:60.38ms
step:1294/2315 train_time:78133ms step_avg:60.38ms
step:1295/2315 train_time:78193ms step_avg:60.38ms
step:1296/2315 train_time:78254ms step_avg:60.38ms
step:1297/2315 train_time:78314ms step_avg:60.38ms
step:1298/2315 train_time:78374ms step_avg:60.38ms
step:1299/2315 train_time:78434ms step_avg:60.38ms
step:1300/2315 train_time:78495ms step_avg:60.38ms
step:1301/2315 train_time:78556ms step_avg:60.38ms
step:1302/2315 train_time:78617ms step_avg:60.38ms
step:1303/2315 train_time:78679ms step_avg:60.38ms
step:1304/2315 train_time:78740ms step_avg:60.38ms
step:1305/2315 train_time:78801ms step_avg:60.38ms
step:1306/2315 train_time:78862ms step_avg:60.38ms
step:1307/2315 train_time:78923ms step_avg:60.38ms
step:1308/2315 train_time:78984ms step_avg:60.39ms
step:1309/2315 train_time:79045ms step_avg:60.39ms
step:1310/2315 train_time:79105ms step_avg:60.39ms
step:1311/2315 train_time:79166ms step_avg:60.39ms
step:1312/2315 train_time:79226ms step_avg:60.39ms
step:1313/2315 train_time:79287ms step_avg:60.39ms
step:1314/2315 train_time:79347ms step_avg:60.39ms
step:1315/2315 train_time:79408ms step_avg:60.39ms
step:1316/2315 train_time:79468ms step_avg:60.39ms
step:1317/2315 train_time:79529ms step_avg:60.39ms
step:1318/2315 train_time:79589ms step_avg:60.39ms
step:1319/2315 train_time:79650ms step_avg:60.39ms
step:1320/2315 train_time:79711ms step_avg:60.39ms
step:1321/2315 train_time:79772ms step_avg:60.39ms
step:1322/2315 train_time:79833ms step_avg:60.39ms
step:1323/2315 train_time:79893ms step_avg:60.39ms
step:1324/2315 train_time:79955ms step_avg:60.39ms
step:1325/2315 train_time:80015ms step_avg:60.39ms
step:1326/2315 train_time:80076ms step_avg:60.39ms
step:1327/2315 train_time:80136ms step_avg:60.39ms
step:1328/2315 train_time:80197ms step_avg:60.39ms
step:1329/2315 train_time:80258ms step_avg:60.39ms
step:1330/2315 train_time:80319ms step_avg:60.39ms
step:1331/2315 train_time:80380ms step_avg:60.39ms
step:1332/2315 train_time:80441ms step_avg:60.39ms
step:1333/2315 train_time:80502ms step_avg:60.39ms
step:1334/2315 train_time:80563ms step_avg:60.39ms
step:1335/2315 train_time:80624ms step_avg:60.39ms
step:1336/2315 train_time:80686ms step_avg:60.39ms
step:1337/2315 train_time:80746ms step_avg:60.39ms
step:1338/2315 train_time:80807ms step_avg:60.39ms
step:1339/2315 train_time:80868ms step_avg:60.39ms
step:1340/2315 train_time:80928ms step_avg:60.39ms
step:1341/2315 train_time:80988ms step_avg:60.39ms
step:1342/2315 train_time:81048ms step_avg:60.39ms
step:1343/2315 train_time:81110ms step_avg:60.39ms
step:1344/2315 train_time:81170ms step_avg:60.39ms
step:1345/2315 train_time:81230ms step_avg:60.39ms
step:1346/2315 train_time:81291ms step_avg:60.39ms
step:1347/2315 train_time:81351ms step_avg:60.39ms
step:1348/2315 train_time:81413ms step_avg:60.40ms
step:1349/2315 train_time:81473ms step_avg:60.40ms
step:1350/2315 train_time:81534ms step_avg:60.40ms
step:1351/2315 train_time:81594ms step_avg:60.40ms
step:1352/2315 train_time:81655ms step_avg:60.40ms
step:1353/2315 train_time:81715ms step_avg:60.40ms
step:1354/2315 train_time:81776ms step_avg:60.40ms
step:1355/2315 train_time:81838ms step_avg:60.40ms
step:1356/2315 train_time:81899ms step_avg:60.40ms
step:1357/2315 train_time:81960ms step_avg:60.40ms
step:1358/2315 train_time:82021ms step_avg:60.40ms
step:1359/2315 train_time:82081ms step_avg:60.40ms
step:1360/2315 train_time:82142ms step_avg:60.40ms
step:1361/2315 train_time:82203ms step_avg:60.40ms
step:1362/2315 train_time:82263ms step_avg:60.40ms
step:1363/2315 train_time:82324ms step_avg:60.40ms
step:1364/2315 train_time:82385ms step_avg:60.40ms
step:1365/2315 train_time:82446ms step_avg:60.40ms
step:1366/2315 train_time:82508ms step_avg:60.40ms
step:1367/2315 train_time:82568ms step_avg:60.40ms
step:1368/2315 train_time:82628ms step_avg:60.40ms
step:1369/2315 train_time:82689ms step_avg:60.40ms
step:1370/2315 train_time:82749ms step_avg:60.40ms
step:1371/2315 train_time:82810ms step_avg:60.40ms
step:1372/2315 train_time:82871ms step_avg:60.40ms
step:1373/2315 train_time:82932ms step_avg:60.40ms
step:1374/2315 train_time:82994ms step_avg:60.40ms
step:1375/2315 train_time:83054ms step_avg:60.40ms
step:1376/2315 train_time:83114ms step_avg:60.40ms
step:1377/2315 train_time:83175ms step_avg:60.40ms
step:1378/2315 train_time:83236ms step_avg:60.40ms
step:1379/2315 train_time:83296ms step_avg:60.40ms
step:1380/2315 train_time:83358ms step_avg:60.40ms
step:1381/2315 train_time:83419ms step_avg:60.40ms
step:1382/2315 train_time:83480ms step_avg:60.41ms
step:1383/2315 train_time:83542ms step_avg:60.41ms
step:1384/2315 train_time:83602ms step_avg:60.41ms
step:1385/2315 train_time:83663ms step_avg:60.41ms
step:1386/2315 train_time:83724ms step_avg:60.41ms
step:1387/2315 train_time:83785ms step_avg:60.41ms
step:1388/2315 train_time:83846ms step_avg:60.41ms
step:1389/2315 train_time:83906ms step_avg:60.41ms
step:1390/2315 train_time:83967ms step_avg:60.41ms
step:1391/2315 train_time:84028ms step_avg:60.41ms
step:1392/2315 train_time:84088ms step_avg:60.41ms
step:1393/2315 train_time:84149ms step_avg:60.41ms
step:1394/2315 train_time:84209ms step_avg:60.41ms
step:1395/2315 train_time:84270ms step_avg:60.41ms
step:1396/2315 train_time:84332ms step_avg:60.41ms
step:1397/2315 train_time:84393ms step_avg:60.41ms
step:1398/2315 train_time:84453ms step_avg:60.41ms
step:1399/2315 train_time:84514ms step_avg:60.41ms
step:1400/2315 train_time:84574ms step_avg:60.41ms
step:1401/2315 train_time:84635ms step_avg:60.41ms
step:1402/2315 train_time:84697ms step_avg:60.41ms
step:1403/2315 train_time:84757ms step_avg:60.41ms
step:1404/2315 train_time:84818ms step_avg:60.41ms
step:1405/2315 train_time:84879ms step_avg:60.41ms
step:1406/2315 train_time:84940ms step_avg:60.41ms
step:1407/2315 train_time:85001ms step_avg:60.41ms
step:1408/2315 train_time:85061ms step_avg:60.41ms
step:1409/2315 train_time:85122ms step_avg:60.41ms
step:1410/2315 train_time:85184ms step_avg:60.41ms
step:1411/2315 train_time:85245ms step_avg:60.41ms
step:1412/2315 train_time:85306ms step_avg:60.41ms
step:1413/2315 train_time:85367ms step_avg:60.42ms
step:1414/2315 train_time:85428ms step_avg:60.42ms
step:1415/2315 train_time:85488ms step_avg:60.42ms
step:1416/2315 train_time:85549ms step_avg:60.42ms
step:1417/2315 train_time:85609ms step_avg:60.42ms
step:1418/2315 train_time:85670ms step_avg:60.42ms
step:1419/2315 train_time:85732ms step_avg:60.42ms
step:1420/2315 train_time:85793ms step_avg:60.42ms
step:1421/2315 train_time:85853ms step_avg:60.42ms
step:1422/2315 train_time:85914ms step_avg:60.42ms
step:1423/2315 train_time:85974ms step_avg:60.42ms
step:1424/2315 train_time:86035ms step_avg:60.42ms
step:1425/2315 train_time:86096ms step_avg:60.42ms
step:1426/2315 train_time:86157ms step_avg:60.42ms
step:1427/2315 train_time:86218ms step_avg:60.42ms
step:1428/2315 train_time:86279ms step_avg:60.42ms
step:1429/2315 train_time:86340ms step_avg:60.42ms
step:1430/2315 train_time:86401ms step_avg:60.42ms
step:1431/2315 train_time:86463ms step_avg:60.42ms
step:1432/2315 train_time:86523ms step_avg:60.42ms
step:1433/2315 train_time:86584ms step_avg:60.42ms
step:1434/2315 train_time:86645ms step_avg:60.42ms
step:1435/2315 train_time:86706ms step_avg:60.42ms
step:1436/2315 train_time:86767ms step_avg:60.42ms
step:1437/2315 train_time:86828ms step_avg:60.42ms
step:1438/2315 train_time:86889ms step_avg:60.42ms
step:1439/2315 train_time:86949ms step_avg:60.42ms
step:1440/2315 train_time:87011ms step_avg:60.42ms
step:1441/2315 train_time:87071ms step_avg:60.42ms
step:1442/2315 train_time:87131ms step_avg:60.42ms
step:1443/2315 train_time:87192ms step_avg:60.42ms
step:1444/2315 train_time:87253ms step_avg:60.42ms
step:1445/2315 train_time:87314ms step_avg:60.42ms
step:1446/2315 train_time:87375ms step_avg:60.43ms
step:1447/2315 train_time:87436ms step_avg:60.43ms
step:1448/2315 train_time:87497ms step_avg:60.43ms
step:1449/2315 train_time:87558ms step_avg:60.43ms
step:1450/2315 train_time:87619ms step_avg:60.43ms
step:1451/2315 train_time:87680ms step_avg:60.43ms
step:1452/2315 train_time:87742ms step_avg:60.43ms
step:1453/2315 train_time:87802ms step_avg:60.43ms
step:1454/2315 train_time:87863ms step_avg:60.43ms
step:1455/2315 train_time:87924ms step_avg:60.43ms
step:1456/2315 train_time:87985ms step_avg:60.43ms
step:1457/2315 train_time:88046ms step_avg:60.43ms
step:1458/2315 train_time:88107ms step_avg:60.43ms
step:1459/2315 train_time:88167ms step_avg:60.43ms
step:1460/2315 train_time:88228ms step_avg:60.43ms
step:1461/2315 train_time:88288ms step_avg:60.43ms
step:1462/2315 train_time:88348ms step_avg:60.43ms
step:1463/2315 train_time:88409ms step_avg:60.43ms
step:1464/2315 train_time:88470ms step_avg:60.43ms
step:1465/2315 train_time:88531ms step_avg:60.43ms
step:1466/2315 train_time:88592ms step_avg:60.43ms
step:1467/2315 train_time:88652ms step_avg:60.43ms
step:1468/2315 train_time:88714ms step_avg:60.43ms
step:1469/2315 train_time:88775ms step_avg:60.43ms
step:1470/2315 train_time:88835ms step_avg:60.43ms
step:1471/2315 train_time:88895ms step_avg:60.43ms
step:1472/2315 train_time:88956ms step_avg:60.43ms
step:1473/2315 train_time:89017ms step_avg:60.43ms
step:1474/2315 train_time:89078ms step_avg:60.43ms
step:1475/2315 train_time:89139ms step_avg:60.43ms
step:1476/2315 train_time:89200ms step_avg:60.43ms
step:1477/2315 train_time:89261ms step_avg:60.43ms
step:1478/2315 train_time:89322ms step_avg:60.43ms
step:1479/2315 train_time:89383ms step_avg:60.43ms
step:1480/2315 train_time:89444ms step_avg:60.43ms
step:1481/2315 train_time:89504ms step_avg:60.44ms
step:1482/2315 train_time:89565ms step_avg:60.44ms
step:1483/2315 train_time:89626ms step_avg:60.44ms
step:1484/2315 train_time:89687ms step_avg:60.44ms
step:1485/2315 train_time:89747ms step_avg:60.44ms
step:1486/2315 train_time:89807ms step_avg:60.44ms
step:1487/2315 train_time:89868ms step_avg:60.44ms
step:1488/2315 train_time:89929ms step_avg:60.44ms
step:1489/2315 train_time:89990ms step_avg:60.44ms
step:1490/2315 train_time:90051ms step_avg:60.44ms
step:1491/2315 train_time:90112ms step_avg:60.44ms
step:1492/2315 train_time:90173ms step_avg:60.44ms
step:1493/2315 train_time:90235ms step_avg:60.44ms
step:1494/2315 train_time:90295ms step_avg:60.44ms
step:1495/2315 train_time:90356ms step_avg:60.44ms
step:1496/2315 train_time:90417ms step_avg:60.44ms
step:1497/2315 train_time:90478ms step_avg:60.44ms
step:1498/2315 train_time:90540ms step_avg:60.44ms
step:1499/2315 train_time:90601ms step_avg:60.44ms
step:1500/2315 train_time:90661ms step_avg:60.44ms
step:1500/2315 val_loss:3.4484 train_time:90724ms step_avg:60.48ms
step:1501/2315 train_time:90747ms step_avg:60.46ms
step:1502/2315 train_time:90785ms step_avg:60.44ms
step:1503/2315 train_time:90851ms step_avg:60.45ms
step:1504/2315 train_time:90915ms step_avg:60.45ms
step:1505/2315 train_time:90977ms step_avg:60.45ms
step:1506/2315 train_time:91037ms step_avg:60.45ms
step:1507/2315 train_time:91098ms step_avg:60.45ms
step:1508/2315 train_time:91158ms step_avg:60.45ms
step:1509/2315 train_time:91218ms step_avg:60.45ms
step:1510/2315 train_time:91278ms step_avg:60.45ms
step:1511/2315 train_time:91338ms step_avg:60.45ms
step:1512/2315 train_time:91398ms step_avg:60.45ms
step:1513/2315 train_time:91457ms step_avg:60.45ms
step:1514/2315 train_time:91517ms step_avg:60.45ms
step:1515/2315 train_time:91577ms step_avg:60.45ms
step:1516/2315 train_time:91638ms step_avg:60.45ms
step:1517/2315 train_time:91701ms step_avg:60.45ms
step:1518/2315 train_time:91763ms step_avg:60.45ms
step:1519/2315 train_time:91826ms step_avg:60.45ms
step:1520/2315 train_time:91889ms step_avg:60.45ms
step:1521/2315 train_time:91950ms step_avg:60.45ms
step:1522/2315 train_time:92011ms step_avg:60.45ms
step:1523/2315 train_time:92073ms step_avg:60.46ms
step:1524/2315 train_time:92135ms step_avg:60.46ms
step:1525/2315 train_time:92195ms step_avg:60.46ms
step:1526/2315 train_time:92256ms step_avg:60.46ms
step:1527/2315 train_time:92317ms step_avg:60.46ms
step:1528/2315 train_time:92378ms step_avg:60.46ms
step:1529/2315 train_time:92438ms step_avg:60.46ms
step:1530/2315 train_time:92500ms step_avg:60.46ms
step:1531/2315 train_time:92559ms step_avg:60.46ms
step:1532/2315 train_time:92620ms step_avg:60.46ms
step:1533/2315 train_time:92682ms step_avg:60.46ms
step:1534/2315 train_time:92743ms step_avg:60.46ms
step:1535/2315 train_time:92805ms step_avg:60.46ms
step:1536/2315 train_time:92866ms step_avg:60.46ms
step:1537/2315 train_time:92928ms step_avg:60.46ms
step:1538/2315 train_time:92989ms step_avg:60.46ms
step:1539/2315 train_time:93050ms step_avg:60.46ms
step:1540/2315 train_time:93112ms step_avg:60.46ms
step:1541/2315 train_time:93174ms step_avg:60.46ms
step:1542/2315 train_time:93236ms step_avg:60.46ms
step:1543/2315 train_time:93297ms step_avg:60.46ms
step:1544/2315 train_time:93357ms step_avg:60.46ms
step:1545/2315 train_time:93419ms step_avg:60.47ms
step:1546/2315 train_time:93480ms step_avg:60.47ms
step:1547/2315 train_time:93540ms step_avg:60.47ms
step:1548/2315 train_time:93601ms step_avg:60.47ms
step:1549/2315 train_time:93661ms step_avg:60.47ms
step:1550/2315 train_time:93722ms step_avg:60.47ms
step:1551/2315 train_time:93783ms step_avg:60.47ms
step:1552/2315 train_time:93845ms step_avg:60.47ms
step:1553/2315 train_time:93907ms step_avg:60.47ms
step:1554/2315 train_time:93968ms step_avg:60.47ms
step:1555/2315 train_time:94030ms step_avg:60.47ms
step:1556/2315 train_time:94091ms step_avg:60.47ms
step:1557/2315 train_time:94153ms step_avg:60.47ms
step:1558/2315 train_time:94215ms step_avg:60.47ms
step:1559/2315 train_time:94276ms step_avg:60.47ms
step:1560/2315 train_time:94336ms step_avg:60.47ms
step:1561/2315 train_time:94397ms step_avg:60.47ms
step:1562/2315 train_time:94459ms step_avg:60.47ms
step:1563/2315 train_time:94519ms step_avg:60.47ms
step:1564/2315 train_time:94580ms step_avg:60.47ms
step:1565/2315 train_time:94641ms step_avg:60.47ms
step:1566/2315 train_time:94702ms step_avg:60.47ms
step:1567/2315 train_time:94763ms step_avg:60.47ms
step:1568/2315 train_time:94825ms step_avg:60.47ms
step:1569/2315 train_time:94885ms step_avg:60.47ms
step:1570/2315 train_time:94946ms step_avg:60.48ms
step:1571/2315 train_time:95007ms step_avg:60.48ms
step:1572/2315 train_time:95068ms step_avg:60.48ms
step:1573/2315 train_time:95130ms step_avg:60.48ms
step:1574/2315 train_time:95191ms step_avg:60.48ms
step:1575/2315 train_time:95253ms step_avg:60.48ms
step:1576/2315 train_time:95314ms step_avg:60.48ms
step:1577/2315 train_time:95376ms step_avg:60.48ms
step:1578/2315 train_time:95437ms step_avg:60.48ms
step:1579/2315 train_time:95498ms step_avg:60.48ms
step:1580/2315 train_time:95559ms step_avg:60.48ms
step:1581/2315 train_time:95620ms step_avg:60.48ms
step:1582/2315 train_time:95682ms step_avg:60.48ms
step:1583/2315 train_time:95743ms step_avg:60.48ms
step:1584/2315 train_time:95805ms step_avg:60.48ms
step:1585/2315 train_time:95865ms step_avg:60.48ms
step:1586/2315 train_time:95926ms step_avg:60.48ms
step:1587/2315 train_time:95987ms step_avg:60.48ms
step:1588/2315 train_time:96048ms step_avg:60.48ms
step:1589/2315 train_time:96109ms step_avg:60.48ms
step:1590/2315 train_time:96171ms step_avg:60.48ms
step:1591/2315 train_time:96232ms step_avg:60.49ms
step:1592/2315 train_time:96294ms step_avg:60.49ms
step:1593/2315 train_time:96355ms step_avg:60.49ms
step:1594/2315 train_time:96417ms step_avg:60.49ms
step:1595/2315 train_time:96478ms step_avg:60.49ms
step:1596/2315 train_time:96539ms step_avg:60.49ms
step:1597/2315 train_time:96600ms step_avg:60.49ms
step:1598/2315 train_time:96661ms step_avg:60.49ms
step:1599/2315 train_time:96722ms step_avg:60.49ms
step:1600/2315 train_time:96784ms step_avg:60.49ms
step:1601/2315 train_time:96845ms step_avg:60.49ms
step:1602/2315 train_time:96906ms step_avg:60.49ms
step:1603/2315 train_time:96966ms step_avg:60.49ms
step:1604/2315 train_time:97027ms step_avg:60.49ms
step:1605/2315 train_time:97088ms step_avg:60.49ms
step:1606/2315 train_time:97149ms step_avg:60.49ms
step:1607/2315 train_time:97210ms step_avg:60.49ms
step:1608/2315 train_time:97272ms step_avg:60.49ms
step:1609/2315 train_time:97333ms step_avg:60.49ms
step:1610/2315 train_time:97395ms step_avg:60.49ms
step:1611/2315 train_time:97456ms step_avg:60.49ms
step:1612/2315 train_time:97517ms step_avg:60.49ms
step:1613/2315 train_time:97578ms step_avg:60.49ms
step:1614/2315 train_time:97639ms step_avg:60.50ms
step:1615/2315 train_time:97700ms step_avg:60.50ms
step:1616/2315 train_time:97761ms step_avg:60.50ms
step:1617/2315 train_time:97822ms step_avg:60.50ms
step:1618/2315 train_time:97883ms step_avg:60.50ms
step:1619/2315 train_time:97944ms step_avg:60.50ms
step:1620/2315 train_time:98006ms step_avg:60.50ms
step:1621/2315 train_time:98066ms step_avg:60.50ms
step:1622/2315 train_time:98127ms step_avg:60.50ms
step:1623/2315 train_time:98189ms step_avg:60.50ms
step:1624/2315 train_time:98250ms step_avg:60.50ms
step:1625/2315 train_time:98312ms step_avg:60.50ms
step:1626/2315 train_time:98374ms step_avg:60.50ms
step:1627/2315 train_time:98435ms step_avg:60.50ms
step:1628/2315 train_time:98496ms step_avg:60.50ms
step:1629/2315 train_time:98558ms step_avg:60.50ms
step:1630/2315 train_time:98619ms step_avg:60.50ms
step:1631/2315 train_time:98680ms step_avg:60.50ms
step:1632/2315 train_time:98742ms step_avg:60.50ms
step:1633/2315 train_time:98802ms step_avg:60.50ms
step:1634/2315 train_time:98863ms step_avg:60.50ms
step:1635/2315 train_time:98925ms step_avg:60.50ms
step:1636/2315 train_time:98985ms step_avg:60.50ms
step:1637/2315 train_time:99046ms step_avg:60.50ms
step:1638/2315 train_time:99107ms step_avg:60.50ms
step:1639/2315 train_time:99168ms step_avg:60.51ms
step:1640/2315 train_time:99229ms step_avg:60.51ms
step:1641/2315 train_time:99290ms step_avg:60.51ms
step:1642/2315 train_time:99351ms step_avg:60.51ms
step:1643/2315 train_time:99414ms step_avg:60.51ms
step:1644/2315 train_time:99475ms step_avg:60.51ms
step:1645/2315 train_time:99536ms step_avg:60.51ms
step:1646/2315 train_time:99597ms step_avg:60.51ms
step:1647/2315 train_time:99658ms step_avg:60.51ms
step:1648/2315 train_time:99719ms step_avg:60.51ms
step:1649/2315 train_time:99780ms step_avg:60.51ms
step:1650/2315 train_time:99841ms step_avg:60.51ms
step:1651/2315 train_time:99902ms step_avg:60.51ms
step:1652/2315 train_time:99963ms step_avg:60.51ms
step:1653/2315 train_time:100024ms step_avg:60.51ms
step:1654/2315 train_time:100085ms step_avg:60.51ms
step:1655/2315 train_time:100146ms step_avg:60.51ms
step:1656/2315 train_time:100207ms step_avg:60.51ms
step:1657/2315 train_time:100268ms step_avg:60.51ms
step:1658/2315 train_time:100330ms step_avg:60.51ms
step:1659/2315 train_time:100391ms step_avg:60.51ms
step:1660/2315 train_time:100453ms step_avg:60.51ms
step:1661/2315 train_time:100515ms step_avg:60.51ms
step:1662/2315 train_time:100576ms step_avg:60.51ms
step:1663/2315 train_time:100637ms step_avg:60.52ms
step:1664/2315 train_time:100698ms step_avg:60.52ms
step:1665/2315 train_time:100759ms step_avg:60.52ms
step:1666/2315 train_time:100821ms step_avg:60.52ms
step:1667/2315 train_time:100882ms step_avg:60.52ms
step:1668/2315 train_time:100943ms step_avg:60.52ms
step:1669/2315 train_time:101004ms step_avg:60.52ms
step:1670/2315 train_time:101064ms step_avg:60.52ms
step:1671/2315 train_time:101125ms step_avg:60.52ms
step:1672/2315 train_time:101186ms step_avg:60.52ms
step:1673/2315 train_time:101247ms step_avg:60.52ms
step:1674/2315 train_time:101309ms step_avg:60.52ms
step:1675/2315 train_time:101370ms step_avg:60.52ms
step:1676/2315 train_time:101431ms step_avg:60.52ms
step:1677/2315 train_time:101493ms step_avg:60.52ms
step:1678/2315 train_time:101554ms step_avg:60.52ms
step:1679/2315 train_time:101616ms step_avg:60.52ms
step:1680/2315 train_time:101678ms step_avg:60.52ms
step:1681/2315 train_time:101739ms step_avg:60.52ms
step:1682/2315 train_time:101800ms step_avg:60.52ms
step:1683/2315 train_time:101861ms step_avg:60.52ms
step:1684/2315 train_time:101922ms step_avg:60.52ms
step:1685/2315 train_time:101983ms step_avg:60.52ms
step:1686/2315 train_time:102044ms step_avg:60.52ms
step:1687/2315 train_time:102105ms step_avg:60.52ms
step:1688/2315 train_time:102166ms step_avg:60.52ms
step:1689/2315 train_time:102227ms step_avg:60.52ms
step:1690/2315 train_time:102288ms step_avg:60.53ms
step:1691/2315 train_time:102348ms step_avg:60.53ms
step:1692/2315 train_time:102410ms step_avg:60.53ms
step:1693/2315 train_time:102472ms step_avg:60.53ms
step:1694/2315 train_time:102533ms step_avg:60.53ms
step:1695/2315 train_time:102595ms step_avg:60.53ms
step:1696/2315 train_time:102656ms step_avg:60.53ms
step:1697/2315 train_time:102718ms step_avg:60.53ms
step:1698/2315 train_time:102779ms step_avg:60.53ms
step:1699/2315 train_time:102840ms step_avg:60.53ms
step:1700/2315 train_time:102901ms step_avg:60.53ms
step:1701/2315 train_time:102962ms step_avg:60.53ms
step:1702/2315 train_time:103024ms step_avg:60.53ms
step:1703/2315 train_time:103084ms step_avg:60.53ms
step:1704/2315 train_time:103145ms step_avg:60.53ms
step:1705/2315 train_time:103206ms step_avg:60.53ms
step:1706/2315 train_time:103267ms step_avg:60.53ms
step:1707/2315 train_time:103328ms step_avg:60.53ms
step:1708/2315 train_time:103390ms step_avg:60.53ms
step:1709/2315 train_time:103451ms step_avg:60.53ms
step:1710/2315 train_time:103513ms step_avg:60.53ms
step:1711/2315 train_time:103575ms step_avg:60.53ms
step:1712/2315 train_time:103637ms step_avg:60.54ms
step:1713/2315 train_time:103698ms step_avg:60.54ms
step:1714/2315 train_time:103759ms step_avg:60.54ms
step:1715/2315 train_time:103821ms step_avg:60.54ms
step:1716/2315 train_time:103881ms step_avg:60.54ms
step:1717/2315 train_time:103942ms step_avg:60.54ms
step:1718/2315 train_time:104003ms step_avg:60.54ms
step:1719/2315 train_time:104064ms step_avg:60.54ms
step:1720/2315 train_time:104125ms step_avg:60.54ms
step:1721/2315 train_time:104186ms step_avg:60.54ms
step:1722/2315 train_time:104247ms step_avg:60.54ms
step:1723/2315 train_time:104308ms step_avg:60.54ms
step:1724/2315 train_time:104369ms step_avg:60.54ms
step:1725/2315 train_time:104431ms step_avg:60.54ms
step:1726/2315 train_time:104492ms step_avg:60.54ms
step:1727/2315 train_time:104554ms step_avg:60.54ms
step:1728/2315 train_time:104615ms step_avg:60.54ms
step:1729/2315 train_time:104676ms step_avg:60.54ms
step:1730/2315 train_time:104737ms step_avg:60.54ms
step:1731/2315 train_time:104799ms step_avg:60.54ms
step:1732/2315 train_time:104860ms step_avg:60.54ms
step:1733/2315 train_time:104921ms step_avg:60.54ms
step:1734/2315 train_time:104983ms step_avg:60.54ms
step:1735/2315 train_time:105043ms step_avg:60.54ms
step:1736/2315 train_time:105104ms step_avg:60.54ms
step:1737/2315 train_time:105165ms step_avg:60.54ms
step:1738/2315 train_time:105226ms step_avg:60.54ms
step:1739/2315 train_time:105287ms step_avg:60.54ms
step:1740/2315 train_time:105347ms step_avg:60.54ms
step:1741/2315 train_time:105409ms step_avg:60.55ms
step:1742/2315 train_time:105471ms step_avg:60.55ms
step:1743/2315 train_time:105533ms step_avg:60.55ms
step:1744/2315 train_time:105594ms step_avg:60.55ms
step:1745/2315 train_time:105656ms step_avg:60.55ms
step:1746/2315 train_time:105716ms step_avg:60.55ms
step:1747/2315 train_time:105778ms step_avg:60.55ms
step:1748/2315 train_time:105839ms step_avg:60.55ms
step:1749/2315 train_time:105900ms step_avg:60.55ms
step:1750/2315 train_time:105961ms step_avg:60.55ms
step:1750/2315 val_loss:3.3808 train_time:106024ms step_avg:60.59ms
step:1751/2315 train_time:106046ms step_avg:60.56ms
step:1752/2315 train_time:106086ms step_avg:60.55ms
step:1753/2315 train_time:106152ms step_avg:60.55ms
step:1754/2315 train_time:106218ms step_avg:60.56ms
step:1755/2315 train_time:106280ms step_avg:60.56ms
step:1756/2315 train_time:106342ms step_avg:60.56ms
step:1757/2315 train_time:106402ms step_avg:60.56ms
step:1758/2315 train_time:106463ms step_avg:60.56ms
step:1759/2315 train_time:106523ms step_avg:60.56ms
step:1760/2315 train_time:106584ms step_avg:60.56ms
step:1761/2315 train_time:106644ms step_avg:60.56ms
step:1762/2315 train_time:106705ms step_avg:60.56ms
step:1763/2315 train_time:106765ms step_avg:60.56ms
step:1764/2315 train_time:106826ms step_avg:60.56ms
step:1765/2315 train_time:106886ms step_avg:60.56ms
step:1766/2315 train_time:106947ms step_avg:60.56ms
step:1767/2315 train_time:107010ms step_avg:60.56ms
step:1768/2315 train_time:107071ms step_avg:60.56ms
step:1769/2315 train_time:107134ms step_avg:60.56ms
step:1770/2315 train_time:107195ms step_avg:60.56ms
step:1771/2315 train_time:107257ms step_avg:60.56ms
step:1772/2315 train_time:107319ms step_avg:60.56ms
step:1773/2315 train_time:107381ms step_avg:60.56ms
step:1774/2315 train_time:107442ms step_avg:60.56ms
step:1775/2315 train_time:107503ms step_avg:60.57ms
step:1776/2315 train_time:107564ms step_avg:60.57ms
step:1777/2315 train_time:107624ms step_avg:60.57ms
step:1778/2315 train_time:107685ms step_avg:60.56ms
step:1779/2315 train_time:107745ms step_avg:60.57ms
step:1780/2315 train_time:107806ms step_avg:60.57ms
step:1781/2315 train_time:107867ms step_avg:60.57ms
step:1782/2315 train_time:107927ms step_avg:60.57ms
step:1783/2315 train_time:107990ms step_avg:60.57ms
step:1784/2315 train_time:108052ms step_avg:60.57ms
step:1785/2315 train_time:108113ms step_avg:60.57ms
step:1786/2315 train_time:108174ms step_avg:60.57ms
step:1787/2315 train_time:108236ms step_avg:60.57ms
step:1788/2315 train_time:108297ms step_avg:60.57ms
step:1789/2315 train_time:108358ms step_avg:60.57ms
step:1790/2315 train_time:108419ms step_avg:60.57ms
step:1791/2315 train_time:108480ms step_avg:60.57ms
step:1792/2315 train_time:108542ms step_avg:60.57ms
step:1793/2315 train_time:108602ms step_avg:60.57ms
step:1794/2315 train_time:108663ms step_avg:60.57ms
step:1795/2315 train_time:108724ms step_avg:60.57ms
step:1796/2315 train_time:108785ms step_avg:60.57ms
step:1797/2315 train_time:108846ms step_avg:60.57ms
step:1798/2315 train_time:108906ms step_avg:60.57ms
step:1799/2315 train_time:108968ms step_avg:60.57ms
step:1800/2315 train_time:109029ms step_avg:60.57ms
step:1801/2315 train_time:109091ms step_avg:60.57ms
step:1802/2315 train_time:109152ms step_avg:60.57ms
step:1803/2315 train_time:109213ms step_avg:60.57ms
step:1804/2315 train_time:109274ms step_avg:60.57ms
step:1805/2315 train_time:109335ms step_avg:60.57ms
step:1806/2315 train_time:109397ms step_avg:60.57ms
step:1807/2315 train_time:109458ms step_avg:60.57ms
step:1808/2315 train_time:109518ms step_avg:60.57ms
step:1809/2315 train_time:109580ms step_avg:60.57ms
step:1810/2315 train_time:109641ms step_avg:60.58ms
step:1811/2315 train_time:109702ms step_avg:60.58ms
step:1812/2315 train_time:109763ms step_avg:60.58ms
step:1813/2315 train_time:109825ms step_avg:60.58ms
step:1814/2315 train_time:109885ms step_avg:60.58ms
step:1815/2315 train_time:109946ms step_avg:60.58ms
step:1816/2315 train_time:110007ms step_avg:60.58ms
step:1817/2315 train_time:110069ms step_avg:60.58ms
step:1818/2315 train_time:110131ms step_avg:60.58ms
step:1819/2315 train_time:110192ms step_avg:60.58ms
step:1820/2315 train_time:110252ms step_avg:60.58ms
step:1821/2315 train_time:110314ms step_avg:60.58ms
step:1822/2315 train_time:110375ms step_avg:60.58ms
step:1823/2315 train_time:110436ms step_avg:60.58ms
step:1824/2315 train_time:110497ms step_avg:60.58ms
step:1825/2315 train_time:110558ms step_avg:60.58ms
step:1826/2315 train_time:110618ms step_avg:60.58ms
step:1827/2315 train_time:110679ms step_avg:60.58ms
step:1828/2315 train_time:110741ms step_avg:60.58ms
step:1829/2315 train_time:110802ms step_avg:60.58ms
step:1830/2315 train_time:110864ms step_avg:60.58ms
step:1831/2315 train_time:110925ms step_avg:60.58ms
step:1832/2315 train_time:110987ms step_avg:60.58ms
step:1833/2315 train_time:111047ms step_avg:60.58ms
step:1834/2315 train_time:111109ms step_avg:60.58ms
step:1835/2315 train_time:111170ms step_avg:60.58ms
step:1836/2315 train_time:111232ms step_avg:60.58ms
step:1837/2315 train_time:111293ms step_avg:60.58ms
step:1838/2315 train_time:111354ms step_avg:60.58ms
step:1839/2315 train_time:111415ms step_avg:60.58ms
step:1840/2315 train_time:111476ms step_avg:60.58ms
step:1841/2315 train_time:111537ms step_avg:60.58ms
step:1842/2315 train_time:111597ms step_avg:60.58ms
step:1843/2315 train_time:111658ms step_avg:60.59ms
step:1844/2315 train_time:111720ms step_avg:60.59ms
step:1845/2315 train_time:111781ms step_avg:60.59ms
step:1846/2315 train_time:111842ms step_avg:60.59ms
step:1847/2315 train_time:111904ms step_avg:60.59ms
step:1848/2315 train_time:111966ms step_avg:60.59ms
step:1849/2315 train_time:112027ms step_avg:60.59ms
step:1850/2315 train_time:112088ms step_avg:60.59ms
step:1851/2315 train_time:112150ms step_avg:60.59ms
step:1852/2315 train_time:112210ms step_avg:60.59ms
step:1853/2315 train_time:112271ms step_avg:60.59ms
step:1854/2315 train_time:112332ms step_avg:60.59ms
step:1855/2315 train_time:112393ms step_avg:60.59ms
step:1856/2315 train_time:112454ms step_avg:60.59ms
step:1857/2315 train_time:112515ms step_avg:60.59ms
step:1858/2315 train_time:112576ms step_avg:60.59ms
step:1859/2315 train_time:112637ms step_avg:60.59ms
step:1860/2315 train_time:112698ms step_avg:60.59ms
step:1861/2315 train_time:112759ms step_avg:60.59ms
step:1862/2315 train_time:112820ms step_avg:60.59ms
step:1863/2315 train_time:112881ms step_avg:60.59ms
step:1864/2315 train_time:112943ms step_avg:60.59ms
step:1865/2315 train_time:113005ms step_avg:60.59ms
step:1866/2315 train_time:113066ms step_avg:60.59ms
step:1867/2315 train_time:113127ms step_avg:60.59ms
step:1868/2315 train_time:113189ms step_avg:60.59ms
step:1869/2315 train_time:113250ms step_avg:60.59ms
step:1870/2315 train_time:113311ms step_avg:60.59ms
step:1871/2315 train_time:113371ms step_avg:60.59ms
step:1872/2315 train_time:113432ms step_avg:60.59ms
step:1873/2315 train_time:113493ms step_avg:60.59ms
step:1874/2315 train_time:113554ms step_avg:60.59ms
step:1875/2315 train_time:113614ms step_avg:60.59ms
step:1876/2315 train_time:113676ms step_avg:60.59ms
step:1877/2315 train_time:113737ms step_avg:60.60ms
step:1878/2315 train_time:113799ms step_avg:60.60ms
step:1879/2315 train_time:113859ms step_avg:60.60ms
step:1880/2315 train_time:113920ms step_avg:60.60ms
step:1881/2315 train_time:113982ms step_avg:60.60ms
step:1882/2315 train_time:114044ms step_avg:60.60ms
step:1883/2315 train_time:114106ms step_avg:60.60ms
step:1884/2315 train_time:114168ms step_avg:60.60ms
step:1885/2315 train_time:114229ms step_avg:60.60ms
step:1886/2315 train_time:114290ms step_avg:60.60ms
step:1887/2315 train_time:114351ms step_avg:60.60ms
step:1888/2315 train_time:114412ms step_avg:60.60ms
step:1889/2315 train_time:114472ms step_avg:60.60ms
step:1890/2315 train_time:114533ms step_avg:60.60ms
step:1891/2315 train_time:114594ms step_avg:60.60ms
step:1892/2315 train_time:114655ms step_avg:60.60ms
step:1893/2315 train_time:114716ms step_avg:60.60ms
step:1894/2315 train_time:114778ms step_avg:60.60ms
step:1895/2315 train_time:114839ms step_avg:60.60ms
step:1896/2315 train_time:114901ms step_avg:60.60ms
step:1897/2315 train_time:114962ms step_avg:60.60ms
step:1898/2315 train_time:115023ms step_avg:60.60ms
step:1899/2315 train_time:115085ms step_avg:60.60ms
step:1900/2315 train_time:115147ms step_avg:60.60ms
step:1901/2315 train_time:115209ms step_avg:60.60ms
step:1902/2315 train_time:115270ms step_avg:60.60ms
step:1903/2315 train_time:115331ms step_avg:60.60ms
step:1904/2315 train_time:115392ms step_avg:60.61ms
step:1905/2315 train_time:115452ms step_avg:60.60ms
step:1906/2315 train_time:115513ms step_avg:60.61ms
step:1907/2315 train_time:115574ms step_avg:60.61ms
step:1908/2315 train_time:115635ms step_avg:60.61ms
step:1909/2315 train_time:115696ms step_avg:60.61ms
step:1910/2315 train_time:115758ms step_avg:60.61ms
step:1911/2315 train_time:115818ms step_avg:60.61ms
step:1912/2315 train_time:115879ms step_avg:60.61ms
step:1913/2315 train_time:115941ms step_avg:60.61ms
step:1914/2315 train_time:116002ms step_avg:60.61ms
step:1915/2315 train_time:116063ms step_avg:60.61ms
step:1916/2315 train_time:116125ms step_avg:60.61ms
step:1917/2315 train_time:116186ms step_avg:60.61ms
step:1918/2315 train_time:116247ms step_avg:60.61ms
step:1919/2315 train_time:116308ms step_avg:60.61ms
step:1920/2315 train_time:116369ms step_avg:60.61ms
step:1921/2315 train_time:116430ms step_avg:60.61ms
step:1922/2315 train_time:116492ms step_avg:60.61ms
step:1923/2315 train_time:116552ms step_avg:60.61ms
step:1924/2315 train_time:116614ms step_avg:60.61ms
step:1925/2315 train_time:116674ms step_avg:60.61ms
step:1926/2315 train_time:116735ms step_avg:60.61ms
step:1927/2315 train_time:116797ms step_avg:60.61ms
step:1928/2315 train_time:116857ms step_avg:60.61ms
step:1929/2315 train_time:116918ms step_avg:60.61ms
step:1930/2315 train_time:116980ms step_avg:60.61ms
step:1931/2315 train_time:117041ms step_avg:60.61ms
step:1932/2315 train_time:117103ms step_avg:60.61ms
step:1933/2315 train_time:117165ms step_avg:60.61ms
step:1934/2315 train_time:117227ms step_avg:60.61ms
step:1935/2315 train_time:117289ms step_avg:60.61ms
step:1936/2315 train_time:117350ms step_avg:60.61ms
step:1937/2315 train_time:117411ms step_avg:60.61ms
step:1938/2315 train_time:117472ms step_avg:60.62ms
step:1939/2315 train_time:117533ms step_avg:60.62ms
step:1940/2315 train_time:117594ms step_avg:60.62ms
step:1941/2315 train_time:117655ms step_avg:60.62ms
step:1942/2315 train_time:117715ms step_avg:60.62ms
step:1943/2315 train_time:117776ms step_avg:60.62ms
step:1944/2315 train_time:117837ms step_avg:60.62ms
step:1945/2315 train_time:117898ms step_avg:60.62ms
step:1946/2315 train_time:117959ms step_avg:60.62ms
step:1947/2315 train_time:118020ms step_avg:60.62ms
step:1948/2315 train_time:118082ms step_avg:60.62ms
step:1949/2315 train_time:118144ms step_avg:60.62ms
step:1950/2315 train_time:118205ms step_avg:60.62ms
step:1951/2315 train_time:118267ms step_avg:60.62ms
step:1952/2315 train_time:118327ms step_avg:60.62ms
step:1953/2315 train_time:118389ms step_avg:60.62ms
step:1954/2315 train_time:118450ms step_avg:60.62ms
step:1955/2315 train_time:118511ms step_avg:60.62ms
step:1956/2315 train_time:118572ms step_avg:60.62ms
step:1957/2315 train_time:118633ms step_avg:60.62ms
step:1958/2315 train_time:118694ms step_avg:60.62ms
step:1959/2315 train_time:118755ms step_avg:60.62ms
step:1960/2315 train_time:118816ms step_avg:60.62ms
step:1961/2315 train_time:118876ms step_avg:60.62ms
step:1962/2315 train_time:118938ms step_avg:60.62ms
step:1963/2315 train_time:118999ms step_avg:60.62ms
step:1964/2315 train_time:119060ms step_avg:60.62ms
step:1965/2315 train_time:119121ms step_avg:60.62ms
step:1966/2315 train_time:119183ms step_avg:60.62ms
step:1967/2315 train_time:119245ms step_avg:60.62ms
step:1968/2315 train_time:119306ms step_avg:60.62ms
step:1969/2315 train_time:119367ms step_avg:60.62ms
step:1970/2315 train_time:119429ms step_avg:60.62ms
step:1971/2315 train_time:119490ms step_avg:60.62ms
step:1972/2315 train_time:119551ms step_avg:60.62ms
step:1973/2315 train_time:119612ms step_avg:60.62ms
step:1974/2315 train_time:119673ms step_avg:60.62ms
step:1975/2315 train_time:119734ms step_avg:60.62ms
step:1976/2315 train_time:119796ms step_avg:60.63ms
step:1977/2315 train_time:119856ms step_avg:60.63ms
step:1978/2315 train_time:119918ms step_avg:60.63ms
step:1979/2315 train_time:119978ms step_avg:60.63ms
step:1980/2315 train_time:120039ms step_avg:60.63ms
step:1981/2315 train_time:120100ms step_avg:60.63ms
step:1982/2315 train_time:120163ms step_avg:60.63ms
step:1983/2315 train_time:120224ms step_avg:60.63ms
step:1984/2315 train_time:120287ms step_avg:60.63ms
step:1985/2315 train_time:120347ms step_avg:60.63ms
step:1986/2315 train_time:120408ms step_avg:60.63ms
step:1987/2315 train_time:120470ms step_avg:60.63ms
step:1988/2315 train_time:120531ms step_avg:60.63ms
step:1989/2315 train_time:120592ms step_avg:60.63ms
step:1990/2315 train_time:120652ms step_avg:60.63ms
step:1991/2315 train_time:120713ms step_avg:60.63ms
step:1992/2315 train_time:120775ms step_avg:60.63ms
step:1993/2315 train_time:120835ms step_avg:60.63ms
step:1994/2315 train_time:120896ms step_avg:60.63ms
step:1995/2315 train_time:120957ms step_avg:60.63ms
step:1996/2315 train_time:121018ms step_avg:60.63ms
step:1997/2315 train_time:121079ms step_avg:60.63ms
step:1998/2315 train_time:121141ms step_avg:60.63ms
step:1999/2315 train_time:121203ms step_avg:60.63ms
step:2000/2315 train_time:121264ms step_avg:60.63ms
step:2000/2315 val_loss:3.3307 train_time:121328ms step_avg:60.66ms
step:2001/2315 train_time:121350ms step_avg:60.64ms
step:2002/2315 train_time:121389ms step_avg:60.63ms
step:2003/2315 train_time:121454ms step_avg:60.64ms
step:2004/2315 train_time:121519ms step_avg:60.64ms
step:2005/2315 train_time:121580ms step_avg:60.64ms
step:2006/2315 train_time:121641ms step_avg:60.64ms
step:2007/2315 train_time:121702ms step_avg:60.64ms
step:2008/2315 train_time:121762ms step_avg:60.64ms
step:2009/2315 train_time:121823ms step_avg:60.64ms
step:2010/2315 train_time:121883ms step_avg:60.64ms
step:2011/2315 train_time:121943ms step_avg:60.64ms
step:2012/2315 train_time:122004ms step_avg:60.64ms
step:2013/2315 train_time:122064ms step_avg:60.64ms
step:2014/2315 train_time:122125ms step_avg:60.64ms
step:2015/2315 train_time:122186ms step_avg:60.64ms
step:2016/2315 train_time:122247ms step_avg:60.64ms
step:2017/2315 train_time:122310ms step_avg:60.64ms
step:2018/2315 train_time:122372ms step_avg:60.64ms
step:2019/2315 train_time:122434ms step_avg:60.64ms
step:2020/2315 train_time:122496ms step_avg:60.64ms
step:2021/2315 train_time:122558ms step_avg:60.64ms
step:2022/2315 train_time:122619ms step_avg:60.64ms
step:2023/2315 train_time:122680ms step_avg:60.64ms
step:2024/2315 train_time:122741ms step_avg:60.64ms
step:2025/2315 train_time:122802ms step_avg:60.64ms
step:2026/2315 train_time:122863ms step_avg:60.64ms
step:2027/2315 train_time:122923ms step_avg:60.64ms
step:2028/2315 train_time:122983ms step_avg:60.64ms
step:2029/2315 train_time:123044ms step_avg:60.64ms
step:2030/2315 train_time:123105ms step_avg:60.64ms
step:2031/2315 train_time:123165ms step_avg:60.64ms
step:2032/2315 train_time:123227ms step_avg:60.64ms
step:2033/2315 train_time:123289ms step_avg:60.64ms
step:2034/2315 train_time:123351ms step_avg:60.64ms
step:2035/2315 train_time:123412ms step_avg:60.64ms
step:2036/2315 train_time:123473ms step_avg:60.65ms
step:2037/2315 train_time:123535ms step_avg:60.65ms
step:2038/2315 train_time:123596ms step_avg:60.65ms
step:2039/2315 train_time:123658ms step_avg:60.65ms
step:2040/2315 train_time:123718ms step_avg:60.65ms
step:2041/2315 train_time:123780ms step_avg:60.65ms
step:2042/2315 train_time:123841ms step_avg:60.65ms
step:2043/2315 train_time:123901ms step_avg:60.65ms
step:2044/2315 train_time:123961ms step_avg:60.65ms
step:2045/2315 train_time:124022ms step_avg:60.65ms
step:2046/2315 train_time:124083ms step_avg:60.65ms
step:2047/2315 train_time:124143ms step_avg:60.65ms
step:2048/2315 train_time:124204ms step_avg:60.65ms
step:2049/2315 train_time:124267ms step_avg:60.65ms
step:2050/2315 train_time:124329ms step_avg:60.65ms
step:2051/2315 train_time:124391ms step_avg:60.65ms
step:2052/2315 train_time:124453ms step_avg:60.65ms
step:2053/2315 train_time:124514ms step_avg:60.65ms
step:2054/2315 train_time:124576ms step_avg:60.65ms
step:2055/2315 train_time:124637ms step_avg:60.65ms
step:2056/2315 train_time:124698ms step_avg:60.65ms
step:2057/2315 train_time:124759ms step_avg:60.65ms
step:2058/2315 train_time:124819ms step_avg:60.65ms
step:2059/2315 train_time:124880ms step_avg:60.65ms
step:2060/2315 train_time:124941ms step_avg:60.65ms
step:2061/2315 train_time:125001ms step_avg:60.65ms
step:2062/2315 train_time:125062ms step_avg:60.65ms
step:2063/2315 train_time:125123ms step_avg:60.65ms
step:2064/2315 train_time:125184ms step_avg:60.65ms
step:2065/2315 train_time:125245ms step_avg:60.65ms
step:2066/2315 train_time:125307ms step_avg:60.65ms
step:2067/2315 train_time:125369ms step_avg:60.65ms
step:2068/2315 train_time:125431ms step_avg:60.65ms
step:2069/2315 train_time:125492ms step_avg:60.65ms
step:2070/2315 train_time:125554ms step_avg:60.65ms
step:2071/2315 train_time:125615ms step_avg:60.65ms
step:2072/2315 train_time:125675ms step_avg:60.65ms
step:2073/2315 train_time:125737ms step_avg:60.65ms
step:2074/2315 train_time:125798ms step_avg:60.65ms
step:2075/2315 train_time:125859ms step_avg:60.65ms
step:2076/2315 train_time:125919ms step_avg:60.65ms
step:2077/2315 train_time:125980ms step_avg:60.65ms
step:2078/2315 train_time:126041ms step_avg:60.65ms
step:2079/2315 train_time:126102ms step_avg:60.65ms
step:2080/2315 train_time:126163ms step_avg:60.66ms
step:2081/2315 train_time:126224ms step_avg:60.66ms
step:2082/2315 train_time:126285ms step_avg:60.66ms
step:2083/2315 train_time:126347ms step_avg:60.66ms
step:2084/2315 train_time:126409ms step_avg:60.66ms
step:2085/2315 train_time:126470ms step_avg:60.66ms
step:2086/2315 train_time:126531ms step_avg:60.66ms
step:2087/2315 train_time:126593ms step_avg:60.66ms
step:2088/2315 train_time:126654ms step_avg:60.66ms
step:2089/2315 train_time:126716ms step_avg:60.66ms
step:2090/2315 train_time:126776ms step_avg:60.66ms
step:2091/2315 train_time:126838ms step_avg:60.66ms
step:2092/2315 train_time:126899ms step_avg:60.66ms
step:2093/2315 train_time:126960ms step_avg:60.66ms
step:2094/2315 train_time:127020ms step_avg:60.66ms
step:2095/2315 train_time:127081ms step_avg:60.66ms
step:2096/2315 train_time:127142ms step_avg:60.66ms
step:2097/2315 train_time:127202ms step_avg:60.66ms
step:2098/2315 train_time:127264ms step_avg:60.66ms
step:2099/2315 train_time:127325ms step_avg:60.66ms
step:2100/2315 train_time:127386ms step_avg:60.66ms
step:2101/2315 train_time:127448ms step_avg:60.66ms
step:2102/2315 train_time:127510ms step_avg:60.66ms
step:2103/2315 train_time:127572ms step_avg:60.66ms
step:2104/2315 train_time:127633ms step_avg:60.66ms
step:2105/2315 train_time:127695ms step_avg:60.66ms
step:2106/2315 train_time:127756ms step_avg:60.66ms
step:2107/2315 train_time:127817ms step_avg:60.66ms
step:2108/2315 train_time:127879ms step_avg:60.66ms
step:2109/2315 train_time:127939ms step_avg:60.66ms
step:2110/2315 train_time:128000ms step_avg:60.66ms
step:2111/2315 train_time:128061ms step_avg:60.66ms
step:2112/2315 train_time:128122ms step_avg:60.66ms
step:2113/2315 train_time:128183ms step_avg:60.66ms
step:2114/2315 train_time:128243ms step_avg:60.66ms
step:2115/2315 train_time:128304ms step_avg:60.66ms
step:2116/2315 train_time:128365ms step_avg:60.66ms
step:2117/2315 train_time:128427ms step_avg:60.66ms
step:2118/2315 train_time:128488ms step_avg:60.66ms
step:2119/2315 train_time:128550ms step_avg:60.67ms
step:2120/2315 train_time:128611ms step_avg:60.67ms
step:2121/2315 train_time:128673ms step_avg:60.67ms
step:2122/2315 train_time:128734ms step_avg:60.67ms
step:2123/2315 train_time:128796ms step_avg:60.67ms
step:2124/2315 train_time:128857ms step_avg:60.67ms
step:2125/2315 train_time:128918ms step_avg:60.67ms
step:2126/2315 train_time:128979ms step_avg:60.67ms
step:2127/2315 train_time:129040ms step_avg:60.67ms
step:2128/2315 train_time:129101ms step_avg:60.67ms
step:2129/2315 train_time:129162ms step_avg:60.67ms
step:2130/2315 train_time:129223ms step_avg:60.67ms
step:2131/2315 train_time:129284ms step_avg:60.67ms
step:2132/2315 train_time:129345ms step_avg:60.67ms
step:2133/2315 train_time:129407ms step_avg:60.67ms
step:2134/2315 train_time:129468ms step_avg:60.67ms
step:2135/2315 train_time:129530ms step_avg:60.67ms
step:2136/2315 train_time:129592ms step_avg:60.67ms
step:2137/2315 train_time:129653ms step_avg:60.67ms
step:2138/2315 train_time:129715ms step_avg:60.67ms
step:2139/2315 train_time:129775ms step_avg:60.67ms
step:2140/2315 train_time:129836ms step_avg:60.67ms
step:2141/2315 train_time:129898ms step_avg:60.67ms
step:2142/2315 train_time:129959ms step_avg:60.67ms
step:2143/2315 train_time:130020ms step_avg:60.67ms
step:2144/2315 train_time:130081ms step_avg:60.67ms
step:2145/2315 train_time:130141ms step_avg:60.67ms
step:2146/2315 train_time:130202ms step_avg:60.67ms
step:2147/2315 train_time:130263ms step_avg:60.67ms
step:2148/2315 train_time:130324ms step_avg:60.67ms
step:2149/2315 train_time:130385ms step_avg:60.67ms
step:2150/2315 train_time:130447ms step_avg:60.67ms
step:2151/2315 train_time:130508ms step_avg:60.67ms
step:2152/2315 train_time:130570ms step_avg:60.67ms
step:2153/2315 train_time:130632ms step_avg:60.67ms
step:2154/2315 train_time:130693ms step_avg:60.67ms
step:2155/2315 train_time:130754ms step_avg:60.67ms
step:2156/2315 train_time:130815ms step_avg:60.68ms
step:2157/2315 train_time:130876ms step_avg:60.68ms
step:2158/2315 train_time:130937ms step_avg:60.68ms
step:2159/2315 train_time:130998ms step_avg:60.68ms
step:2160/2315 train_time:131059ms step_avg:60.68ms
step:2161/2315 train_time:131120ms step_avg:60.68ms
step:2162/2315 train_time:131181ms step_avg:60.68ms
step:2163/2315 train_time:131242ms step_avg:60.68ms
step:2164/2315 train_time:131303ms step_avg:60.68ms
step:2165/2315 train_time:131364ms step_avg:60.68ms
step:2166/2315 train_time:131425ms step_avg:60.68ms
step:2167/2315 train_time:131488ms step_avg:60.68ms
step:2168/2315 train_time:131549ms step_avg:60.68ms
step:2169/2315 train_time:131611ms step_avg:60.68ms
step:2170/2315 train_time:131673ms step_avg:60.68ms
step:2171/2315 train_time:131735ms step_avg:60.68ms
step:2172/2315 train_time:131796ms step_avg:60.68ms
step:2173/2315 train_time:131857ms step_avg:60.68ms
step:2174/2315 train_time:131918ms step_avg:60.68ms
step:2175/2315 train_time:131979ms step_avg:60.68ms
step:2176/2315 train_time:132039ms step_avg:60.68ms
step:2177/2315 train_time:132100ms step_avg:60.68ms
step:2178/2315 train_time:132161ms step_avg:60.68ms
step:2179/2315 train_time:132222ms step_avg:60.68ms
step:2180/2315 train_time:132283ms step_avg:60.68ms
step:2181/2315 train_time:132343ms step_avg:60.68ms
step:2182/2315 train_time:132405ms step_avg:60.68ms
step:2183/2315 train_time:132466ms step_avg:60.68ms
step:2184/2315 train_time:132528ms step_avg:60.68ms
step:2185/2315 train_time:132590ms step_avg:60.68ms
step:2186/2315 train_time:132652ms step_avg:60.68ms
step:2187/2315 train_time:132713ms step_avg:60.68ms
step:2188/2315 train_time:132774ms step_avg:60.68ms
step:2189/2315 train_time:132836ms step_avg:60.68ms
step:2190/2315 train_time:132897ms step_avg:60.68ms
step:2191/2315 train_time:132957ms step_avg:60.68ms
step:2192/2315 train_time:133018ms step_avg:60.68ms
step:2193/2315 train_time:133079ms step_avg:60.68ms
step:2194/2315 train_time:133140ms step_avg:60.68ms
step:2195/2315 train_time:133201ms step_avg:60.68ms
step:2196/2315 train_time:133262ms step_avg:60.68ms
step:2197/2315 train_time:133323ms step_avg:60.68ms
step:2198/2315 train_time:133385ms step_avg:60.68ms
step:2199/2315 train_time:133445ms step_avg:60.68ms
step:2200/2315 train_time:133506ms step_avg:60.68ms
step:2201/2315 train_time:133568ms step_avg:60.69ms
step:2202/2315 train_time:133629ms step_avg:60.69ms
step:2203/2315 train_time:133691ms step_avg:60.69ms
step:2204/2315 train_time:133753ms step_avg:60.69ms
step:2205/2315 train_time:133814ms step_avg:60.69ms
step:2206/2315 train_time:133875ms step_avg:60.69ms
step:2207/2315 train_time:133936ms step_avg:60.69ms
step:2208/2315 train_time:133997ms step_avg:60.69ms
step:2209/2315 train_time:134058ms step_avg:60.69ms
step:2210/2315 train_time:134119ms step_avg:60.69ms
step:2211/2315 train_time:134180ms step_avg:60.69ms
step:2212/2315 train_time:134241ms step_avg:60.69ms
step:2213/2315 train_time:134302ms step_avg:60.69ms
step:2214/2315 train_time:134363ms step_avg:60.69ms
step:2215/2315 train_time:134424ms step_avg:60.69ms
step:2216/2315 train_time:134485ms step_avg:60.69ms
step:2217/2315 train_time:134546ms step_avg:60.69ms
step:2218/2315 train_time:134608ms step_avg:60.69ms
step:2219/2315 train_time:134669ms step_avg:60.69ms
step:2220/2315 train_time:134731ms step_avg:60.69ms
step:2221/2315 train_time:134792ms step_avg:60.69ms
step:2222/2315 train_time:134853ms step_avg:60.69ms
step:2223/2315 train_time:134914ms step_avg:60.69ms
step:2224/2315 train_time:134976ms step_avg:60.69ms
step:2225/2315 train_time:135036ms step_avg:60.69ms
step:2226/2315 train_time:135098ms step_avg:60.69ms
step:2227/2315 train_time:135159ms step_avg:60.69ms
step:2228/2315 train_time:135220ms step_avg:60.69ms
step:2229/2315 train_time:135281ms step_avg:60.69ms
step:2230/2315 train_time:135342ms step_avg:60.69ms
step:2231/2315 train_time:135403ms step_avg:60.69ms
step:2232/2315 train_time:135464ms step_avg:60.69ms
step:2233/2315 train_time:135525ms step_avg:60.69ms
step:2234/2315 train_time:135586ms step_avg:60.69ms
step:2235/2315 train_time:135647ms step_avg:60.69ms
step:2236/2315 train_time:135709ms step_avg:60.69ms
step:2237/2315 train_time:135771ms step_avg:60.69ms
step:2238/2315 train_time:135832ms step_avg:60.69ms
step:2239/2315 train_time:135893ms step_avg:60.69ms
step:2240/2315 train_time:135954ms step_avg:60.69ms
step:2241/2315 train_time:136015ms step_avg:60.69ms
step:2242/2315 train_time:136077ms step_avg:60.69ms
step:2243/2315 train_time:136138ms step_avg:60.69ms
step:2244/2315 train_time:136199ms step_avg:60.69ms
step:2245/2315 train_time:136260ms step_avg:60.69ms
step:2246/2315 train_time:136321ms step_avg:60.70ms
step:2247/2315 train_time:136382ms step_avg:60.70ms
step:2248/2315 train_time:136443ms step_avg:60.70ms
step:2249/2315 train_time:136504ms step_avg:60.70ms
step:2250/2315 train_time:136565ms step_avg:60.70ms
step:2250/2315 val_loss:3.2916 train_time:136628ms step_avg:60.72ms
step:2251/2315 train_time:136650ms step_avg:60.71ms
step:2252/2315 train_time:136691ms step_avg:60.70ms
step:2253/2315 train_time:136757ms step_avg:60.70ms
step:2254/2315 train_time:136821ms step_avg:60.70ms
step:2255/2315 train_time:136882ms step_avg:60.70ms
step:2256/2315 train_time:136943ms step_avg:60.70ms
step:2257/2315 train_time:137003ms step_avg:60.70ms
step:2258/2315 train_time:137064ms step_avg:60.70ms
step:2259/2315 train_time:137125ms step_avg:60.70ms
step:2260/2315 train_time:137185ms step_avg:60.70ms
step:2261/2315 train_time:137246ms step_avg:60.70ms
step:2262/2315 train_time:137307ms step_avg:60.70ms
step:2263/2315 train_time:137368ms step_avg:60.70ms
step:2264/2315 train_time:137429ms step_avg:60.70ms
step:2265/2315 train_time:137490ms step_avg:60.70ms
step:2266/2315 train_time:137551ms step_avg:60.70ms
step:2267/2315 train_time:137613ms step_avg:60.70ms
step:2268/2315 train_time:137676ms step_avg:60.70ms
step:2269/2315 train_time:137737ms step_avg:60.70ms
step:2270/2315 train_time:137799ms step_avg:60.70ms
step:2271/2315 train_time:137861ms step_avg:60.70ms
step:2272/2315 train_time:137922ms step_avg:60.71ms
step:2273/2315 train_time:137983ms step_avg:60.71ms
step:2274/2315 train_time:138044ms step_avg:60.71ms
step:2275/2315 train_time:138104ms step_avg:60.71ms
step:2276/2315 train_time:138165ms step_avg:60.71ms
step:2277/2315 train_time:138226ms step_avg:60.71ms
step:2278/2315 train_time:138288ms step_avg:60.71ms
step:2279/2315 train_time:138348ms step_avg:60.71ms
step:2280/2315 train_time:138409ms step_avg:60.71ms
step:2281/2315 train_time:138470ms step_avg:60.71ms
step:2282/2315 train_time:138531ms step_avg:60.71ms
step:2283/2315 train_time:138592ms step_avg:60.71ms
step:2284/2315 train_time:138654ms step_avg:60.71ms
step:2285/2315 train_time:138716ms step_avg:60.71ms
step:2286/2315 train_time:138777ms step_avg:60.71ms
step:2287/2315 train_time:138840ms step_avg:60.71ms
step:2288/2315 train_time:138901ms step_avg:60.71ms
step:2289/2315 train_time:138962ms step_avg:60.71ms
step:2290/2315 train_time:139023ms step_avg:60.71ms
step:2291/2315 train_time:139083ms step_avg:60.71ms
step:2292/2315 train_time:139144ms step_avg:60.71ms
step:2293/2315 train_time:139205ms step_avg:60.71ms
step:2294/2315 train_time:139266ms step_avg:60.71ms
step:2295/2315 train_time:139327ms step_avg:60.71ms
step:2296/2315 train_time:139389ms step_avg:60.71ms
step:2297/2315 train_time:139449ms step_avg:60.71ms
step:2298/2315 train_time:139511ms step_avg:60.71ms
step:2299/2315 train_time:139572ms step_avg:60.71ms
step:2300/2315 train_time:139634ms step_avg:60.71ms
step:2301/2315 train_time:139696ms step_avg:60.71ms
step:2302/2315 train_time:139757ms step_avg:60.71ms
step:2303/2315 train_time:139819ms step_avg:60.71ms
step:2304/2315 train_time:139881ms step_avg:60.71ms
step:2305/2315 train_time:139941ms step_avg:60.71ms
step:2306/2315 train_time:140002ms step_avg:60.71ms
step:2307/2315 train_time:140063ms step_avg:60.71ms
step:2308/2315 train_time:140123ms step_avg:60.71ms
step:2309/2315 train_time:140184ms step_avg:60.71ms
step:2310/2315 train_time:140246ms step_avg:60.71ms
step:2311/2315 train_time:140307ms step_avg:60.71ms
step:2312/2315 train_time:140368ms step_avg:60.71ms
step:2313/2315 train_time:140429ms step_avg:60.71ms
step:2314/2315 train_time:140491ms step_avg:60.71ms
step:2315/2315 train_time:140552ms step_avg:60.71ms
step:2315/2315 val_loss:3.2788 train_time:140614ms step_avg:60.74ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]
# polar_express_coeffs = [
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
# ]



@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    # state["second_momentum_buffer"] = torch.zeros_like(grad[...,0:1]) if param.size(-2) >= param.size(-1) else torch.zeros_like(grad[...,0:1,:])
                    # state["second_momentum_buffer"] = torch.zeros((*grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((*grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)
            
            ###################################
            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))
            ####################################


            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> (-1.5)  0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Oct 22 2025, 02:20:45) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251022+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 24 05:58:23 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000001:00:00.0 Off |                    0 |
| N/A   35C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000002:00:00.0 Off |                    0 |
| N/A   35C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000003:00:00.0 Off |                    0 |
| N/A   30C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000008:00:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000009:00:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   0000000A:00:00.0 Off |                    0 |
| N/A   35C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   0000000C:00:00.0 Off |                    0 |
| N/A   34C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2315 train_time:92ms step_avg:91.96ms
step:2/2315 train_time:186ms step_avg:92.96ms
step:3/2315 train_time:208ms step_avg:69.47ms
step:4/2315 train_time:244ms step_avg:60.95ms
step:5/2315 train_time:302ms step_avg:60.43ms
step:6/2315 train_time:362ms step_avg:60.29ms
step:7/2315 train_time:421ms step_avg:60.16ms
step:8/2315 train_time:481ms step_avg:60.14ms
step:9/2315 train_time:541ms step_avg:60.07ms
step:10/2315 train_time:601ms step_avg:60.07ms
step:11/2315 train_time:661ms step_avg:60.11ms
step:12/2315 train_time:721ms step_avg:60.10ms
step:13/2315 train_time:781ms step_avg:60.05ms
step:14/2315 train_time:841ms step_avg:60.05ms
step:15/2315 train_time:901ms step_avg:60.05ms
step:16/2315 train_time:960ms step_avg:60.03ms
step:17/2315 train_time:1022ms step_avg:60.12ms
step:18/2315 train_time:1084ms step_avg:60.24ms
step:19/2315 train_time:1148ms step_avg:60.42ms
step:20/2315 train_time:1209ms step_avg:60.47ms
step:21/2315 train_time:1271ms step_avg:60.51ms
step:22/2315 train_time:1331ms step_avg:60.49ms
step:23/2315 train_time:1391ms step_avg:60.49ms
step:24/2315 train_time:1451ms step_avg:60.47ms
step:25/2315 train_time:1511ms step_avg:60.45ms
step:26/2315 train_time:1571ms step_avg:60.43ms
step:27/2315 train_time:1631ms step_avg:60.40ms
step:28/2315 train_time:1691ms step_avg:60.38ms
step:29/2315 train_time:1751ms step_avg:60.36ms
step:30/2315 train_time:1811ms step_avg:60.35ms
step:31/2315 train_time:1870ms step_avg:60.34ms
step:32/2315 train_time:1931ms step_avg:60.34ms
step:33/2315 train_time:1991ms step_avg:60.33ms
step:34/2315 train_time:2051ms step_avg:60.33ms
step:35/2315 train_time:2113ms step_avg:60.37ms
step:36/2315 train_time:2174ms step_avg:60.39ms
step:37/2315 train_time:2235ms step_avg:60.40ms
step:38/2315 train_time:2296ms step_avg:60.42ms
step:39/2315 train_time:2357ms step_avg:60.43ms
step:40/2315 train_time:2417ms step_avg:60.44ms
step:41/2315 train_time:2478ms step_avg:60.45ms
step:42/2315 train_time:2539ms step_avg:60.45ms
step:43/2315 train_time:2600ms step_avg:60.46ms
step:44/2315 train_time:2660ms step_avg:60.46ms
step:45/2315 train_time:2721ms step_avg:60.47ms
step:46/2315 train_time:2781ms step_avg:60.46ms
step:47/2315 train_time:2843ms step_avg:60.48ms
step:48/2315 train_time:2903ms step_avg:60.49ms
step:49/2315 train_time:2963ms step_avg:60.48ms
step:50/2315 train_time:3024ms step_avg:60.47ms
step:51/2315 train_time:3085ms step_avg:60.49ms
step:52/2315 train_time:3145ms step_avg:60.49ms
step:53/2315 train_time:3206ms step_avg:60.50ms
step:54/2315 train_time:3267ms step_avg:60.51ms
step:55/2315 train_time:3328ms step_avg:60.51ms
step:56/2315 train_time:3388ms step_avg:60.51ms
step:57/2315 train_time:3449ms step_avg:60.50ms
step:58/2315 train_time:3509ms step_avg:60.51ms
step:59/2315 train_time:3569ms step_avg:60.50ms
step:60/2315 train_time:3630ms step_avg:60.49ms
step:61/2315 train_time:3690ms step_avg:60.48ms
step:62/2315 train_time:3750ms step_avg:60.48ms
step:63/2315 train_time:3810ms step_avg:60.48ms
step:64/2315 train_time:3870ms step_avg:60.47ms
step:65/2315 train_time:3930ms step_avg:60.46ms
step:66/2315 train_time:3989ms step_avg:60.45ms
step:67/2315 train_time:4049ms step_avg:60.44ms
step:68/2315 train_time:4109ms step_avg:60.43ms
step:69/2315 train_time:4169ms step_avg:60.42ms
step:70/2315 train_time:4229ms step_avg:60.42ms
step:71/2315 train_time:4289ms step_avg:60.41ms
step:72/2315 train_time:4349ms step_avg:60.40ms
step:73/2315 train_time:4410ms step_avg:60.41ms
step:74/2315 train_time:4470ms step_avg:60.40ms
step:75/2315 train_time:4530ms step_avg:60.40ms
step:76/2315 train_time:4590ms step_avg:60.39ms
step:77/2315 train_time:4649ms step_avg:60.38ms
step:78/2315 train_time:4709ms step_avg:60.37ms
step:79/2315 train_time:4769ms step_avg:60.36ms
step:80/2315 train_time:4828ms step_avg:60.35ms
step:81/2315 train_time:4888ms step_avg:60.35ms
step:82/2315 train_time:4948ms step_avg:60.34ms
step:83/2315 train_time:5007ms step_avg:60.33ms
step:84/2315 train_time:5068ms step_avg:60.33ms
step:85/2315 train_time:5127ms step_avg:60.32ms
step:86/2315 train_time:5187ms step_avg:60.31ms
step:87/2315 train_time:5247ms step_avg:60.31ms
step:88/2315 train_time:5307ms step_avg:60.30ms
step:89/2315 train_time:5367ms step_avg:60.30ms
step:90/2315 train_time:5427ms step_avg:60.30ms
step:91/2315 train_time:5487ms step_avg:60.29ms
step:92/2315 train_time:5547ms step_avg:60.29ms
step:93/2315 train_time:5607ms step_avg:60.29ms
step:94/2315 train_time:5667ms step_avg:60.29ms
step:95/2315 train_time:5728ms step_avg:60.29ms
step:96/2315 train_time:5788ms step_avg:60.29ms
step:97/2315 train_time:5847ms step_avg:60.28ms
step:98/2315 train_time:5907ms step_avg:60.28ms
step:99/2315 train_time:5966ms step_avg:60.27ms
step:100/2315 train_time:6026ms step_avg:60.26ms
step:101/2315 train_time:6086ms step_avg:60.25ms
step:102/2315 train_time:6146ms step_avg:60.25ms
step:103/2315 train_time:6205ms step_avg:60.24ms
step:104/2315 train_time:6265ms step_avg:60.24ms
step:105/2315 train_time:6324ms step_avg:60.23ms
step:106/2315 train_time:6384ms step_avg:60.23ms
step:107/2315 train_time:6445ms step_avg:60.23ms
step:108/2315 train_time:6505ms step_avg:60.23ms
step:109/2315 train_time:6565ms step_avg:60.23ms
step:110/2315 train_time:6625ms step_avg:60.23ms
step:111/2315 train_time:6685ms step_avg:60.23ms
step:112/2315 train_time:6745ms step_avg:60.22ms
step:113/2315 train_time:6805ms step_avg:60.22ms
step:114/2315 train_time:6865ms step_avg:60.22ms
step:115/2315 train_time:6925ms step_avg:60.22ms
step:116/2315 train_time:6985ms step_avg:60.21ms
step:117/2315 train_time:7045ms step_avg:60.21ms
step:118/2315 train_time:7105ms step_avg:60.21ms
step:119/2315 train_time:7165ms step_avg:60.21ms
step:120/2315 train_time:7225ms step_avg:60.21ms
step:121/2315 train_time:7285ms step_avg:60.20ms
step:122/2315 train_time:7344ms step_avg:60.20ms
step:123/2315 train_time:7405ms step_avg:60.20ms
step:124/2315 train_time:7465ms step_avg:60.20ms
step:125/2315 train_time:7525ms step_avg:60.20ms
step:126/2315 train_time:7586ms step_avg:60.20ms
step:127/2315 train_time:7645ms step_avg:60.20ms
step:128/2315 train_time:7705ms step_avg:60.19ms
step:129/2315 train_time:7765ms step_avg:60.19ms
step:130/2315 train_time:7825ms step_avg:60.19ms
step:131/2315 train_time:7885ms step_avg:60.19ms
step:132/2315 train_time:7944ms step_avg:60.19ms
step:133/2315 train_time:8005ms step_avg:60.19ms
step:134/2315 train_time:8064ms step_avg:60.18ms
step:135/2315 train_time:8124ms step_avg:60.18ms
step:136/2315 train_time:8184ms step_avg:60.18ms
step:137/2315 train_time:8244ms step_avg:60.17ms
step:138/2315 train_time:8304ms step_avg:60.17ms
step:139/2315 train_time:8363ms step_avg:60.17ms
step:140/2315 train_time:8424ms step_avg:60.17ms
step:141/2315 train_time:8484ms step_avg:60.17ms
step:142/2315 train_time:8544ms step_avg:60.17ms
step:143/2315 train_time:8604ms step_avg:60.17ms
step:144/2315 train_time:8664ms step_avg:60.17ms
step:145/2315 train_time:8724ms step_avg:60.17ms
step:146/2315 train_time:8784ms step_avg:60.17ms
step:147/2315 train_time:8844ms step_avg:60.16ms
step:148/2315 train_time:8904ms step_avg:60.16ms
step:149/2315 train_time:8964ms step_avg:60.16ms
step:150/2315 train_time:9025ms step_avg:60.17ms
step:151/2315 train_time:9084ms step_avg:60.16ms
step:152/2315 train_time:9145ms step_avg:60.16ms
step:153/2315 train_time:9204ms step_avg:60.16ms
step:154/2315 train_time:9264ms step_avg:60.16ms
step:155/2315 train_time:9325ms step_avg:60.16ms
step:156/2315 train_time:9384ms step_avg:60.16ms
step:157/2315 train_time:9444ms step_avg:60.16ms
step:158/2315 train_time:9504ms step_avg:60.15ms
step:159/2315 train_time:9564ms step_avg:60.15ms
step:160/2315 train_time:9624ms step_avg:60.15ms
step:161/2315 train_time:9684ms step_avg:60.15ms
step:162/2315 train_time:9744ms step_avg:60.15ms
step:163/2315 train_time:9804ms step_avg:60.15ms
step:164/2315 train_time:9864ms step_avg:60.15ms
step:165/2315 train_time:9924ms step_avg:60.14ms
step:166/2315 train_time:9984ms step_avg:60.14ms
step:167/2315 train_time:10044ms step_avg:60.14ms
step:168/2315 train_time:10104ms step_avg:60.14ms
step:169/2315 train_time:10164ms step_avg:60.14ms
step:170/2315 train_time:10224ms step_avg:60.14ms
step:171/2315 train_time:10284ms step_avg:60.14ms
step:172/2315 train_time:10344ms step_avg:60.14ms
step:173/2315 train_time:10404ms step_avg:60.14ms
step:174/2315 train_time:10465ms step_avg:60.14ms
step:175/2315 train_time:10524ms step_avg:60.14ms
step:176/2315 train_time:10585ms step_avg:60.14ms
step:177/2315 train_time:10645ms step_avg:60.14ms
step:178/2315 train_time:10705ms step_avg:60.14ms
step:179/2315 train_time:10764ms step_avg:60.14ms
step:180/2315 train_time:10824ms step_avg:60.13ms
step:181/2315 train_time:10884ms step_avg:60.13ms
step:182/2315 train_time:10944ms step_avg:60.13ms
step:183/2315 train_time:11004ms step_avg:60.13ms
step:184/2315 train_time:11064ms step_avg:60.13ms
step:185/2315 train_time:11124ms step_avg:60.13ms
step:186/2315 train_time:11184ms step_avg:60.13ms
step:187/2315 train_time:11243ms step_avg:60.13ms
step:188/2315 train_time:11304ms step_avg:60.13ms
step:189/2315 train_time:11363ms step_avg:60.12ms
step:190/2315 train_time:11423ms step_avg:60.12ms
step:191/2315 train_time:11483ms step_avg:60.12ms
step:192/2315 train_time:11544ms step_avg:60.12ms
step:193/2315 train_time:11603ms step_avg:60.12ms
step:194/2315 train_time:11664ms step_avg:60.12ms
step:195/2315 train_time:11723ms step_avg:60.12ms
step:196/2315 train_time:11784ms step_avg:60.12ms
step:197/2315 train_time:11843ms step_avg:60.12ms
step:198/2315 train_time:11904ms step_avg:60.12ms
step:199/2315 train_time:11963ms step_avg:60.12ms
step:200/2315 train_time:12023ms step_avg:60.12ms
step:201/2315 train_time:12083ms step_avg:60.12ms
step:202/2315 train_time:12144ms step_avg:60.12ms
step:203/2315 train_time:12203ms step_avg:60.11ms
step:204/2315 train_time:12263ms step_avg:60.11ms
step:205/2315 train_time:12323ms step_avg:60.11ms
step:206/2315 train_time:12383ms step_avg:60.11ms
step:207/2315 train_time:12443ms step_avg:60.11ms
step:208/2315 train_time:12503ms step_avg:60.11ms
step:209/2315 train_time:12563ms step_avg:60.11ms
step:210/2315 train_time:12623ms step_avg:60.11ms
step:211/2315 train_time:12683ms step_avg:60.11ms
step:212/2315 train_time:12743ms step_avg:60.11ms
step:213/2315 train_time:12804ms step_avg:60.11ms
step:214/2315 train_time:12864ms step_avg:60.11ms
step:215/2315 train_time:12923ms step_avg:60.11ms
step:216/2315 train_time:12983ms step_avg:60.11ms
step:217/2315 train_time:13043ms step_avg:60.11ms
step:218/2315 train_time:13103ms step_avg:60.11ms
step:219/2315 train_time:13163ms step_avg:60.11ms
step:220/2315 train_time:13224ms step_avg:60.11ms
step:221/2315 train_time:13284ms step_avg:60.11ms
step:222/2315 train_time:13344ms step_avg:60.11ms
step:223/2315 train_time:13404ms step_avg:60.11ms
step:224/2315 train_time:13465ms step_avg:60.11ms
step:225/2315 train_time:13525ms step_avg:60.11ms
step:226/2315 train_time:13584ms step_avg:60.11ms
step:227/2315 train_time:13645ms step_avg:60.11ms
step:228/2315 train_time:13705ms step_avg:60.11ms
step:229/2315 train_time:13764ms step_avg:60.11ms
step:230/2315 train_time:13824ms step_avg:60.11ms
step:231/2315 train_time:13884ms step_avg:60.10ms
step:232/2315 train_time:13944ms step_avg:60.10ms
step:233/2315 train_time:14004ms step_avg:60.10ms
step:234/2315 train_time:14064ms step_avg:60.10ms
step:235/2315 train_time:14124ms step_avg:60.10ms
step:236/2315 train_time:14184ms step_avg:60.10ms
step:237/2315 train_time:14244ms step_avg:60.10ms
step:238/2315 train_time:14304ms step_avg:60.10ms
step:239/2315 train_time:14364ms step_avg:60.10ms
step:240/2315 train_time:14424ms step_avg:60.10ms
step:241/2315 train_time:14485ms step_avg:60.10ms
step:242/2315 train_time:14545ms step_avg:60.10ms
step:243/2315 train_time:14605ms step_avg:60.10ms
step:244/2315 train_time:14665ms step_avg:60.10ms
step:245/2315 train_time:14724ms step_avg:60.10ms
step:246/2315 train_time:14784ms step_avg:60.10ms
step:247/2315 train_time:14844ms step_avg:60.10ms
step:248/2315 train_time:14904ms step_avg:60.10ms
step:249/2315 train_time:14964ms step_avg:60.10ms
step:250/2315 train_time:15024ms step_avg:60.10ms
step:250/2315 val_loss:4.0633 train_time:15085ms step_avg:60.34ms
step:251/2315 train_time:15107ms step_avg:60.19ms
step:252/2315 train_time:15147ms step_avg:60.11ms
step:253/2315 train_time:15208ms step_avg:60.11ms
step:254/2315 train_time:15272ms step_avg:60.13ms
step:255/2315 train_time:15335ms step_avg:60.14ms
step:256/2315 train_time:15395ms step_avg:60.14ms
step:257/2315 train_time:15455ms step_avg:60.14ms
step:258/2315 train_time:15515ms step_avg:60.13ms
step:259/2315 train_time:15575ms step_avg:60.13ms
step:260/2315 train_time:15634ms step_avg:60.13ms
step:261/2315 train_time:15694ms step_avg:60.13ms
step:262/2315 train_time:15753ms step_avg:60.13ms
step:263/2315 train_time:15812ms step_avg:60.12ms
step:264/2315 train_time:15871ms step_avg:60.12ms
step:265/2315 train_time:15931ms step_avg:60.12ms
step:266/2315 train_time:15990ms step_avg:60.11ms
step:267/2315 train_time:16051ms step_avg:60.12ms
step:268/2315 train_time:16111ms step_avg:60.12ms
step:269/2315 train_time:16172ms step_avg:60.12ms
step:270/2315 train_time:16234ms step_avg:60.13ms
step:271/2315 train_time:16296ms step_avg:60.13ms
step:272/2315 train_time:16357ms step_avg:60.14ms
step:273/2315 train_time:16417ms step_avg:60.14ms
step:274/2315 train_time:16476ms step_avg:60.13ms
step:275/2315 train_time:16537ms step_avg:60.13ms
step:276/2315 train_time:16597ms step_avg:60.13ms
step:277/2315 train_time:16657ms step_avg:60.13ms
step:278/2315 train_time:16717ms step_avg:60.13ms
step:279/2315 train_time:16776ms step_avg:60.13ms
step:280/2315 train_time:16837ms step_avg:60.13ms
step:281/2315 train_time:16896ms step_avg:60.13ms
step:282/2315 train_time:16956ms step_avg:60.13ms
step:283/2315 train_time:17016ms step_avg:60.13ms
step:284/2315 train_time:17077ms step_avg:60.13ms
step:285/2315 train_time:17138ms step_avg:60.13ms
step:286/2315 train_time:17199ms step_avg:60.14ms
step:287/2315 train_time:17259ms step_avg:60.14ms
step:288/2315 train_time:17320ms step_avg:60.14ms
step:289/2315 train_time:17380ms step_avg:60.14ms
step:290/2315 train_time:17440ms step_avg:60.14ms
step:291/2315 train_time:17501ms step_avg:60.14ms
step:292/2315 train_time:17560ms step_avg:60.14ms
step:293/2315 train_time:17620ms step_avg:60.14ms
step:294/2315 train_time:17680ms step_avg:60.13ms
step:295/2315 train_time:17740ms step_avg:60.13ms
step:296/2315 train_time:17799ms step_avg:60.13ms
step:297/2315 train_time:17859ms step_avg:60.13ms
step:298/2315 train_time:17919ms step_avg:60.13ms
step:299/2315 train_time:17979ms step_avg:60.13ms
step:300/2315 train_time:18039ms step_avg:60.13ms
step:301/2315 train_time:18100ms step_avg:60.13ms
step:302/2315 train_time:18160ms step_avg:60.13ms
step:303/2315 train_time:18221ms step_avg:60.13ms
step:304/2315 train_time:18281ms step_avg:60.13ms
step:305/2315 train_time:18341ms step_avg:60.14ms
step:306/2315 train_time:18401ms step_avg:60.14ms
step:307/2315 train_time:18463ms step_avg:60.14ms
step:308/2315 train_time:18522ms step_avg:60.14ms
step:309/2315 train_time:18582ms step_avg:60.14ms
step:310/2315 train_time:18642ms step_avg:60.14ms
step:311/2315 train_time:18701ms step_avg:60.13ms
step:312/2315 train_time:18761ms step_avg:60.13ms
step:313/2315 train_time:18821ms step_avg:60.13ms
step:314/2315 train_time:18881ms step_avg:60.13ms
step:315/2315 train_time:18941ms step_avg:60.13ms
step:316/2315 train_time:19001ms step_avg:60.13ms
step:317/2315 train_time:19061ms step_avg:60.13ms
step:318/2315 train_time:19122ms step_avg:60.13ms
step:319/2315 train_time:19182ms step_avg:60.13ms
step:320/2315 train_time:19242ms step_avg:60.13ms
step:321/2315 train_time:19302ms step_avg:60.13ms
step:322/2315 train_time:19363ms step_avg:60.13ms
step:323/2315 train_time:19423ms step_avg:60.13ms
step:324/2315 train_time:19483ms step_avg:60.13ms
step:325/2315 train_time:19543ms step_avg:60.13ms
step:326/2315 train_time:19603ms step_avg:60.13ms
step:327/2315 train_time:19663ms step_avg:60.13ms
step:328/2315 train_time:19722ms step_avg:60.13ms
step:329/2315 train_time:19782ms step_avg:60.13ms
step:330/2315 train_time:19842ms step_avg:60.13ms
step:331/2315 train_time:19901ms step_avg:60.12ms
step:332/2315 train_time:19961ms step_avg:60.12ms
step:333/2315 train_time:20021ms step_avg:60.12ms
step:334/2315 train_time:20082ms step_avg:60.12ms
step:335/2315 train_time:20142ms step_avg:60.12ms
step:336/2315 train_time:20202ms step_avg:60.13ms
step:337/2315 train_time:20262ms step_avg:60.12ms
step:338/2315 train_time:20322ms step_avg:60.12ms
step:339/2315 train_time:20382ms step_avg:60.13ms
step:340/2315 train_time:20442ms step_avg:60.12ms
step:341/2315 train_time:20502ms step_avg:60.12ms
step:342/2315 train_time:20562ms step_avg:60.12ms
step:343/2315 train_time:20622ms step_avg:60.12ms
step:344/2315 train_time:20681ms step_avg:60.12ms
step:345/2315 train_time:20741ms step_avg:60.12ms
step:346/2315 train_time:20801ms step_avg:60.12ms
step:347/2315 train_time:20862ms step_avg:60.12ms
step:348/2315 train_time:20922ms step_avg:60.12ms
step:349/2315 train_time:20981ms step_avg:60.12ms
step:350/2315 train_time:21041ms step_avg:60.12ms
step:351/2315 train_time:21101ms step_avg:60.12ms
step:352/2315 train_time:21161ms step_avg:60.12ms
step:353/2315 train_time:21221ms step_avg:60.12ms
step:354/2315 train_time:21282ms step_avg:60.12ms
step:355/2315 train_time:21342ms step_avg:60.12ms
step:356/2315 train_time:21402ms step_avg:60.12ms
step:357/2315 train_time:21462ms step_avg:60.12ms
step:358/2315 train_time:21522ms step_avg:60.12ms
step:359/2315 train_time:21582ms step_avg:60.12ms
step:360/2315 train_time:21642ms step_avg:60.12ms
step:361/2315 train_time:21701ms step_avg:60.11ms
step:362/2315 train_time:21761ms step_avg:60.11ms
step:363/2315 train_time:21820ms step_avg:60.11ms
step:364/2315 train_time:21880ms step_avg:60.11ms
step:365/2315 train_time:21941ms step_avg:60.11ms
step:366/2315 train_time:22001ms step_avg:60.11ms
step:367/2315 train_time:22061ms step_avg:60.11ms
step:368/2315 train_time:22120ms step_avg:60.11ms
step:369/2315 train_time:22181ms step_avg:60.11ms
step:370/2315 train_time:22241ms step_avg:60.11ms
step:371/2315 train_time:22301ms step_avg:60.11ms
step:372/2315 train_time:22361ms step_avg:60.11ms
step:373/2315 train_time:22422ms step_avg:60.11ms
step:374/2315 train_time:22483ms step_avg:60.11ms
step:375/2315 train_time:22542ms step_avg:60.11ms
step:376/2315 train_time:22602ms step_avg:60.11ms
step:377/2315 train_time:22663ms step_avg:60.11ms
step:378/2315 train_time:22722ms step_avg:60.11ms
step:379/2315 train_time:22782ms step_avg:60.11ms
step:380/2315 train_time:22842ms step_avg:60.11ms
step:381/2315 train_time:22901ms step_avg:60.11ms
step:382/2315 train_time:22962ms step_avg:60.11ms
step:383/2315 train_time:23021ms step_avg:60.11ms
step:384/2315 train_time:23081ms step_avg:60.11ms
step:385/2315 train_time:23141ms step_avg:60.11ms
step:386/2315 train_time:23201ms step_avg:60.11ms
step:387/2315 train_time:23262ms step_avg:60.11ms
step:388/2315 train_time:23322ms step_avg:60.11ms
step:389/2315 train_time:23382ms step_avg:60.11ms
step:390/2315 train_time:23442ms step_avg:60.11ms
step:391/2315 train_time:23502ms step_avg:60.11ms
step:392/2315 train_time:23562ms step_avg:60.11ms
step:393/2315 train_time:23622ms step_avg:60.11ms
step:394/2315 train_time:23681ms step_avg:60.10ms
step:395/2315 train_time:23741ms step_avg:60.10ms
step:396/2315 train_time:23801ms step_avg:60.10ms
step:397/2315 train_time:23861ms step_avg:60.10ms
step:398/2315 train_time:23921ms step_avg:60.10ms
step:399/2315 train_time:23981ms step_avg:60.10ms
step:400/2315 train_time:24041ms step_avg:60.10ms
step:401/2315 train_time:24102ms step_avg:60.10ms
step:402/2315 train_time:24162ms step_avg:60.10ms
step:403/2315 train_time:24222ms step_avg:60.11ms
step:404/2315 train_time:24282ms step_avg:60.10ms
step:405/2315 train_time:24342ms step_avg:60.10ms
step:406/2315 train_time:24402ms step_avg:60.10ms
step:407/2315 train_time:24462ms step_avg:60.10ms
step:408/2315 train_time:24522ms step_avg:60.10ms
step:409/2315 train_time:24582ms step_avg:60.10ms
step:410/2315 train_time:24642ms step_avg:60.10ms
step:411/2315 train_time:24702ms step_avg:60.10ms
step:412/2315 train_time:24762ms step_avg:60.10ms
step:413/2315 train_time:24822ms step_avg:60.10ms
step:414/2315 train_time:24882ms step_avg:60.10ms
step:415/2315 train_time:24942ms step_avg:60.10ms
step:416/2315 train_time:25002ms step_avg:60.10ms
step:417/2315 train_time:25062ms step_avg:60.10ms
step:418/2315 train_time:25122ms step_avg:60.10ms
step:419/2315 train_time:25182ms step_avg:60.10ms
step:420/2315 train_time:25242ms step_avg:60.10ms
step:421/2315 train_time:25302ms step_avg:60.10ms
step:422/2315 train_time:25362ms step_avg:60.10ms
step:423/2315 train_time:25422ms step_avg:60.10ms
step:424/2315 train_time:25482ms step_avg:60.10ms
step:425/2315 train_time:25542ms step_avg:60.10ms
step:426/2315 train_time:25602ms step_avg:60.10ms
step:427/2315 train_time:25662ms step_avg:60.10ms
step:428/2315 train_time:25722ms step_avg:60.10ms
step:429/2315 train_time:25782ms step_avg:60.10ms
step:430/2315 train_time:25842ms step_avg:60.10ms
step:431/2315 train_time:25902ms step_avg:60.10ms
step:432/2315 train_time:25962ms step_avg:60.10ms
step:433/2315 train_time:26022ms step_avg:60.10ms
step:434/2315 train_time:26081ms step_avg:60.10ms
step:435/2315 train_time:26141ms step_avg:60.10ms
step:436/2315 train_time:26202ms step_avg:60.10ms
step:437/2315 train_time:26261ms step_avg:60.09ms
step:438/2315 train_time:26321ms step_avg:60.09ms
step:439/2315 train_time:26381ms step_avg:60.09ms
step:440/2315 train_time:26441ms step_avg:60.09ms
step:441/2315 train_time:26501ms step_avg:60.09ms
step:442/2315 train_time:26560ms step_avg:60.09ms
step:443/2315 train_time:26620ms step_avg:60.09ms
step:444/2315 train_time:26680ms step_avg:60.09ms
step:445/2315 train_time:26740ms step_avg:60.09ms
step:446/2315 train_time:26800ms step_avg:60.09ms
step:447/2315 train_time:26860ms step_avg:60.09ms
step:448/2315 train_time:26920ms step_avg:60.09ms
step:449/2315 train_time:26981ms step_avg:60.09ms
step:450/2315 train_time:27041ms step_avg:60.09ms
step:451/2315 train_time:27100ms step_avg:60.09ms
step:452/2315 train_time:27160ms step_avg:60.09ms
step:453/2315 train_time:27221ms step_avg:60.09ms
step:454/2315 train_time:27281ms step_avg:60.09ms
step:455/2315 train_time:27341ms step_avg:60.09ms
step:456/2315 train_time:27400ms step_avg:60.09ms
step:457/2315 train_time:27461ms step_avg:60.09ms
step:458/2315 train_time:27521ms step_avg:60.09ms
step:459/2315 train_time:27581ms step_avg:60.09ms
step:460/2315 train_time:27641ms step_avg:60.09ms
step:461/2315 train_time:27701ms step_avg:60.09ms
step:462/2315 train_time:27761ms step_avg:60.09ms
step:463/2315 train_time:27821ms step_avg:60.09ms
step:464/2315 train_time:27882ms step_avg:60.09ms
step:465/2315 train_time:27942ms step_avg:60.09ms
step:466/2315 train_time:28002ms step_avg:60.09ms
step:467/2315 train_time:28062ms step_avg:60.09ms
step:468/2315 train_time:28122ms step_avg:60.09ms
step:469/2315 train_time:28181ms step_avg:60.09ms
step:470/2315 train_time:28241ms step_avg:60.09ms
step:471/2315 train_time:28301ms step_avg:60.09ms
step:472/2315 train_time:28362ms step_avg:60.09ms
step:473/2315 train_time:28422ms step_avg:60.09ms
step:474/2315 train_time:28482ms step_avg:60.09ms
step:475/2315 train_time:28542ms step_avg:60.09ms
step:476/2315 train_time:28602ms step_avg:60.09ms
step:477/2315 train_time:28662ms step_avg:60.09ms
step:478/2315 train_time:28722ms step_avg:60.09ms
step:479/2315 train_time:28782ms step_avg:60.09ms
step:480/2315 train_time:28842ms step_avg:60.09ms
step:481/2315 train_time:28902ms step_avg:60.09ms
step:482/2315 train_time:28962ms step_avg:60.09ms
step:483/2315 train_time:29022ms step_avg:60.09ms
step:484/2315 train_time:29082ms step_avg:60.09ms
step:485/2315 train_time:29142ms step_avg:60.09ms
step:486/2315 train_time:29202ms step_avg:60.09ms
step:487/2315 train_time:29262ms step_avg:60.09ms
step:488/2315 train_time:29322ms step_avg:60.09ms
step:489/2315 train_time:29382ms step_avg:60.09ms
step:490/2315 train_time:29442ms step_avg:60.09ms
step:491/2315 train_time:29502ms step_avg:60.08ms
step:492/2315 train_time:29562ms step_avg:60.09ms
step:493/2315 train_time:29622ms step_avg:60.09ms
step:494/2315 train_time:29683ms step_avg:60.09ms
step:495/2315 train_time:29742ms step_avg:60.09ms
step:496/2315 train_time:29802ms step_avg:60.08ms
step:497/2315 train_time:29862ms step_avg:60.08ms
step:498/2315 train_time:29922ms step_avg:60.08ms
step:499/2315 train_time:29982ms step_avg:60.08ms
step:500/2315 train_time:30043ms step_avg:60.09ms
step:500/2315 val_loss:3.8061 train_time:30104ms step_avg:60.21ms
step:501/2315 train_time:30129ms step_avg:60.14ms
step:502/2315 train_time:30166ms step_avg:60.09ms
step:503/2315 train_time:30229ms step_avg:60.10ms
step:504/2315 train_time:30291ms step_avg:60.10ms
step:505/2315 train_time:30351ms step_avg:60.10ms
step:506/2315 train_time:30411ms step_avg:60.10ms
step:507/2315 train_time:30471ms step_avg:60.10ms
step:508/2315 train_time:30531ms step_avg:60.10ms
step:509/2315 train_time:30590ms step_avg:60.10ms
step:510/2315 train_time:30649ms step_avg:60.10ms
step:511/2315 train_time:30708ms step_avg:60.09ms
step:512/2315 train_time:30767ms step_avg:60.09ms
step:513/2315 train_time:30826ms step_avg:60.09ms
step:514/2315 train_time:30885ms step_avg:60.09ms
step:515/2315 train_time:30944ms step_avg:60.09ms
step:516/2315 train_time:31004ms step_avg:60.08ms
step:517/2315 train_time:31064ms step_avg:60.09ms
step:518/2315 train_time:31125ms step_avg:60.09ms
step:519/2315 train_time:31185ms step_avg:60.09ms
step:520/2315 train_time:31247ms step_avg:60.09ms
step:521/2315 train_time:31307ms step_avg:60.09ms
step:522/2315 train_time:31367ms step_avg:60.09ms
step:523/2315 train_time:31427ms step_avg:60.09ms
step:524/2315 train_time:31487ms step_avg:60.09ms
step:525/2315 train_time:31546ms step_avg:60.09ms
step:526/2315 train_time:31606ms step_avg:60.09ms
step:527/2315 train_time:31665ms step_avg:60.09ms
step:528/2315 train_time:31725ms step_avg:60.08ms
step:529/2315 train_time:31784ms step_avg:60.08ms
step:530/2315 train_time:31844ms step_avg:60.08ms
step:531/2315 train_time:31903ms step_avg:60.08ms
step:532/2315 train_time:31963ms step_avg:60.08ms
step:533/2315 train_time:32023ms step_avg:60.08ms
step:534/2315 train_time:32083ms step_avg:60.08ms
step:535/2315 train_time:32143ms step_avg:60.08ms
step:536/2315 train_time:32204ms step_avg:60.08ms
step:537/2315 train_time:32264ms step_avg:60.08ms
step:538/2315 train_time:32324ms step_avg:60.08ms
step:539/2315 train_time:32385ms step_avg:60.08ms
step:540/2315 train_time:32445ms step_avg:60.08ms
step:541/2315 train_time:32506ms step_avg:60.08ms
step:542/2315 train_time:32566ms step_avg:60.08ms
step:543/2315 train_time:32626ms step_avg:60.08ms
step:544/2315 train_time:32685ms step_avg:60.08ms
step:545/2315 train_time:32745ms step_avg:60.08ms
step:546/2315 train_time:32805ms step_avg:60.08ms
step:547/2315 train_time:32864ms step_avg:60.08ms
step:548/2315 train_time:32923ms step_avg:60.08ms
step:549/2315 train_time:32983ms step_avg:60.08ms
step:550/2315 train_time:33043ms step_avg:60.08ms
step:551/2315 train_time:33102ms step_avg:60.08ms
step:552/2315 train_time:33162ms step_avg:60.08ms
step:553/2315 train_time:33223ms step_avg:60.08ms
step:554/2315 train_time:33284ms step_avg:60.08ms
step:555/2315 train_time:33344ms step_avg:60.08ms
step:556/2315 train_time:33405ms step_avg:60.08ms
step:557/2315 train_time:33465ms step_avg:60.08ms
step:558/2315 train_time:33525ms step_avg:60.08ms
step:559/2315 train_time:33585ms step_avg:60.08ms
step:560/2315 train_time:33645ms step_avg:60.08ms
step:561/2315 train_time:33704ms step_avg:60.08ms
step:562/2315 train_time:33764ms step_avg:60.08ms
step:563/2315 train_time:33824ms step_avg:60.08ms
step:564/2315 train_time:33883ms step_avg:60.08ms
step:565/2315 train_time:33943ms step_avg:60.08ms
step:566/2315 train_time:34004ms step_avg:60.08ms
step:567/2315 train_time:34063ms step_avg:60.08ms
step:568/2315 train_time:34123ms step_avg:60.08ms
step:569/2315 train_time:34183ms step_avg:60.08ms
step:570/2315 train_time:34243ms step_avg:60.08ms
step:571/2315 train_time:34304ms step_avg:60.08ms
step:572/2315 train_time:34364ms step_avg:60.08ms
step:573/2315 train_time:34424ms step_avg:60.08ms
step:574/2315 train_time:34485ms step_avg:60.08ms
step:575/2315 train_time:34545ms step_avg:60.08ms
step:576/2315 train_time:34605ms step_avg:60.08ms
step:577/2315 train_time:34665ms step_avg:60.08ms
step:578/2315 train_time:34725ms step_avg:60.08ms
step:579/2315 train_time:34785ms step_avg:60.08ms
step:580/2315 train_time:34845ms step_avg:60.08ms
step:581/2315 train_time:34904ms step_avg:60.08ms
step:582/2315 train_time:34963ms step_avg:60.07ms
step:583/2315 train_time:35023ms step_avg:60.07ms
step:584/2315 train_time:35083ms step_avg:60.07ms
step:585/2315 train_time:35143ms step_avg:60.07ms
step:586/2315 train_time:35203ms step_avg:60.07ms
step:587/2315 train_time:35264ms step_avg:60.07ms
step:588/2315 train_time:35324ms step_avg:60.07ms
step:589/2315 train_time:35384ms step_avg:60.07ms
step:590/2315 train_time:35444ms step_avg:60.08ms
step:591/2315 train_time:35505ms step_avg:60.08ms
step:592/2315 train_time:35565ms step_avg:60.08ms
step:593/2315 train_time:35625ms step_avg:60.08ms
step:594/2315 train_time:35685ms step_avg:60.08ms
step:595/2315 train_time:35745ms step_avg:60.07ms
step:596/2315 train_time:35804ms step_avg:60.07ms
step:597/2315 train_time:35864ms step_avg:60.07ms
step:598/2315 train_time:35924ms step_avg:60.07ms
step:599/2315 train_time:35984ms step_avg:60.07ms
step:600/2315 train_time:36044ms step_avg:60.07ms
step:601/2315 train_time:36104ms step_avg:60.07ms
step:602/2315 train_time:36164ms step_avg:60.07ms
step:603/2315 train_time:36224ms step_avg:60.07ms
step:604/2315 train_time:36284ms step_avg:60.07ms
step:605/2315 train_time:36344ms step_avg:60.07ms
step:606/2315 train_time:36405ms step_avg:60.07ms
step:607/2315 train_time:36465ms step_avg:60.07ms
step:608/2315 train_time:36525ms step_avg:60.07ms
step:609/2315 train_time:36585ms step_avg:60.07ms
step:610/2315 train_time:36645ms step_avg:60.07ms
step:611/2315 train_time:36704ms step_avg:60.07ms
step:612/2315 train_time:36765ms step_avg:60.07ms
step:613/2315 train_time:36824ms step_avg:60.07ms
step:614/2315 train_time:36884ms step_avg:60.07ms
step:615/2315 train_time:36944ms step_avg:60.07ms
step:616/2315 train_time:37004ms step_avg:60.07ms
step:617/2315 train_time:37064ms step_avg:60.07ms
step:618/2315 train_time:37124ms step_avg:60.07ms
step:619/2315 train_time:37184ms step_avg:60.07ms
step:620/2315 train_time:37244ms step_avg:60.07ms
step:621/2315 train_time:37304ms step_avg:60.07ms
step:622/2315 train_time:37364ms step_avg:60.07ms
step:623/2315 train_time:37424ms step_avg:60.07ms
step:624/2315 train_time:37484ms step_avg:60.07ms
step:625/2315 train_time:37544ms step_avg:60.07ms
step:626/2315 train_time:37604ms step_avg:60.07ms
step:627/2315 train_time:37664ms step_avg:60.07ms
step:628/2315 train_time:37724ms step_avg:60.07ms
step:629/2315 train_time:37784ms step_avg:60.07ms
step:630/2315 train_time:37844ms step_avg:60.07ms
step:631/2315 train_time:37903ms step_avg:60.07ms
step:632/2315 train_time:37963ms step_avg:60.07ms
step:633/2315 train_time:38023ms step_avg:60.07ms
step:634/2315 train_time:38084ms step_avg:60.07ms
step:635/2315 train_time:38144ms step_avg:60.07ms
step:636/2315 train_time:38204ms step_avg:60.07ms
step:637/2315 train_time:38263ms step_avg:60.07ms
step:638/2315 train_time:38324ms step_avg:60.07ms
step:639/2315 train_time:38383ms step_avg:60.07ms
step:640/2315 train_time:38444ms step_avg:60.07ms
step:641/2315 train_time:38504ms step_avg:60.07ms
step:642/2315 train_time:38564ms step_avg:60.07ms
step:643/2315 train_time:38624ms step_avg:60.07ms
step:644/2315 train_time:38684ms step_avg:60.07ms
step:645/2315 train_time:38744ms step_avg:60.07ms
step:646/2315 train_time:38804ms step_avg:60.07ms
step:647/2315 train_time:38864ms step_avg:60.07ms
step:648/2315 train_time:38924ms step_avg:60.07ms
step:649/2315 train_time:38984ms step_avg:60.07ms
step:650/2315 train_time:39044ms step_avg:60.07ms
step:651/2315 train_time:39104ms step_avg:60.07ms
step:652/2315 train_time:39164ms step_avg:60.07ms
step:653/2315 train_time:39224ms step_avg:60.07ms
step:654/2315 train_time:39284ms step_avg:60.07ms
step:655/2315 train_time:39344ms step_avg:60.07ms
step:656/2315 train_time:39404ms step_avg:60.07ms
step:657/2315 train_time:39464ms step_avg:60.07ms
step:658/2315 train_time:39524ms step_avg:60.07ms
step:659/2315 train_time:39584ms step_avg:60.07ms
step:660/2315 train_time:39645ms step_avg:60.07ms
step:661/2315 train_time:39704ms step_avg:60.07ms
step:662/2315 train_time:39764ms step_avg:60.07ms
step:663/2315 train_time:39824ms step_avg:60.07ms
step:664/2315 train_time:39884ms step_avg:60.07ms
step:665/2315 train_time:39944ms step_avg:60.07ms
step:666/2315 train_time:40004ms step_avg:60.07ms
step:667/2315 train_time:40064ms step_avg:60.07ms
step:668/2315 train_time:40124ms step_avg:60.07ms
step:669/2315 train_time:40183ms step_avg:60.06ms
step:670/2315 train_time:40243ms step_avg:60.06ms
step:671/2315 train_time:40303ms step_avg:60.06ms
step:672/2315 train_time:40364ms step_avg:60.07ms
step:673/2315 train_time:40424ms step_avg:60.06ms
step:674/2315 train_time:40483ms step_avg:60.06ms
step:675/2315 train_time:40544ms step_avg:60.06ms
step:676/2315 train_time:40605ms step_avg:60.07ms
step:677/2315 train_time:40664ms step_avg:60.07ms
step:678/2315 train_time:40724ms step_avg:60.07ms
step:679/2315 train_time:40784ms step_avg:60.06ms
step:680/2315 train_time:40844ms step_avg:60.06ms
step:681/2315 train_time:40904ms step_avg:60.06ms
step:682/2315 train_time:40964ms step_avg:60.06ms
step:683/2315 train_time:41024ms step_avg:60.06ms
step:684/2315 train_time:41084ms step_avg:60.06ms
step:685/2315 train_time:41143ms step_avg:60.06ms
step:686/2315 train_time:41203ms step_avg:60.06ms
step:687/2315 train_time:41263ms step_avg:60.06ms
step:688/2315 train_time:41323ms step_avg:60.06ms
step:689/2315 train_time:41383ms step_avg:60.06ms
step:690/2315 train_time:41444ms step_avg:60.06ms
step:691/2315 train_time:41504ms step_avg:60.06ms
step:692/2315 train_time:41564ms step_avg:60.06ms
step:693/2315 train_time:41624ms step_avg:60.06ms
step:694/2315 train_time:41684ms step_avg:60.06ms
step:695/2315 train_time:41743ms step_avg:60.06ms
step:696/2315 train_time:41804ms step_avg:60.06ms
step:697/2315 train_time:41863ms step_avg:60.06ms
step:698/2315 train_time:41924ms step_avg:60.06ms
step:699/2315 train_time:41983ms step_avg:60.06ms
step:700/2315 train_time:42044ms step_avg:60.06ms
step:701/2315 train_time:42103ms step_avg:60.06ms
step:702/2315 train_time:42163ms step_avg:60.06ms
step:703/2315 train_time:42224ms step_avg:60.06ms
step:704/2315 train_time:42284ms step_avg:60.06ms
step:705/2315 train_time:42344ms step_avg:60.06ms
step:706/2315 train_time:42404ms step_avg:60.06ms
step:707/2315 train_time:42464ms step_avg:60.06ms
step:708/2315 train_time:42524ms step_avg:60.06ms
step:709/2315 train_time:42584ms step_avg:60.06ms
step:710/2315 train_time:42644ms step_avg:60.06ms
step:711/2315 train_time:42704ms step_avg:60.06ms
step:712/2315 train_time:42764ms step_avg:60.06ms
step:713/2315 train_time:42823ms step_avg:60.06ms
step:714/2315 train_time:42884ms step_avg:60.06ms
step:715/2315 train_time:42944ms step_avg:60.06ms
step:716/2315 train_time:43004ms step_avg:60.06ms
step:717/2315 train_time:43063ms step_avg:60.06ms
step:718/2315 train_time:43123ms step_avg:60.06ms
step:719/2315 train_time:43183ms step_avg:60.06ms
step:720/2315 train_time:43243ms step_avg:60.06ms
step:721/2315 train_time:43303ms step_avg:60.06ms
step:722/2315 train_time:43364ms step_avg:60.06ms
step:723/2315 train_time:43424ms step_avg:60.06ms
step:724/2315 train_time:43484ms step_avg:60.06ms
step:725/2315 train_time:43544ms step_avg:60.06ms
step:726/2315 train_time:43604ms step_avg:60.06ms
step:727/2315 train_time:43664ms step_avg:60.06ms
step:728/2315 train_time:43724ms step_avg:60.06ms
step:729/2315 train_time:43784ms step_avg:60.06ms
step:730/2315 train_time:43843ms step_avg:60.06ms
step:731/2315 train_time:43903ms step_avg:60.06ms
step:732/2315 train_time:43964ms step_avg:60.06ms
step:733/2315 train_time:44024ms step_avg:60.06ms
step:734/2315 train_time:44083ms step_avg:60.06ms
step:735/2315 train_time:44143ms step_avg:60.06ms
step:736/2315 train_time:44204ms step_avg:60.06ms
step:737/2315 train_time:44264ms step_avg:60.06ms
step:738/2315 train_time:44324ms step_avg:60.06ms
step:739/2315 train_time:44384ms step_avg:60.06ms
step:740/2315 train_time:44444ms step_avg:60.06ms
step:741/2315 train_time:44504ms step_avg:60.06ms
step:742/2315 train_time:44564ms step_avg:60.06ms
step:743/2315 train_time:44624ms step_avg:60.06ms
step:744/2315 train_time:44684ms step_avg:60.06ms
step:745/2315 train_time:44744ms step_avg:60.06ms
step:746/2315 train_time:44803ms step_avg:60.06ms
step:747/2315 train_time:44863ms step_avg:60.06ms
step:748/2315 train_time:44923ms step_avg:60.06ms
step:749/2315 train_time:44983ms step_avg:60.06ms
step:750/2315 train_time:45043ms step_avg:60.06ms
step:750/2315 val_loss:3.6792 train_time:45105ms step_avg:60.14ms
step:751/2315 train_time:45127ms step_avg:60.09ms
step:752/2315 train_time:45165ms step_avg:60.06ms
step:753/2315 train_time:45229ms step_avg:60.07ms
step:754/2315 train_time:45294ms step_avg:60.07ms
step:755/2315 train_time:45354ms step_avg:60.07ms
step:756/2315 train_time:45414ms step_avg:60.07ms
step:757/2315 train_time:45473ms step_avg:60.07ms
step:758/2315 train_time:45534ms step_avg:60.07ms
step:759/2315 train_time:45593ms step_avg:60.07ms
step:760/2315 train_time:45652ms step_avg:60.07ms
step:761/2315 train_time:45711ms step_avg:60.07ms
step:762/2315 train_time:45772ms step_avg:60.07ms
step:763/2315 train_time:45831ms step_avg:60.07ms
step:764/2315 train_time:45891ms step_avg:60.07ms
step:765/2315 train_time:45951ms step_avg:60.07ms
step:766/2315 train_time:46011ms step_avg:60.07ms
step:767/2315 train_time:46072ms step_avg:60.07ms
step:768/2315 train_time:46134ms step_avg:60.07ms
step:769/2315 train_time:46195ms step_avg:60.07ms
step:770/2315 train_time:46256ms step_avg:60.07ms
step:771/2315 train_time:46316ms step_avg:60.07ms
step:772/2315 train_time:46378ms step_avg:60.07ms
step:773/2315 train_time:46438ms step_avg:60.08ms
step:774/2315 train_time:46499ms step_avg:60.08ms
step:775/2315 train_time:46560ms step_avg:60.08ms
step:776/2315 train_time:46621ms step_avg:60.08ms
step:777/2315 train_time:46682ms step_avg:60.08ms
step:778/2315 train_time:46742ms step_avg:60.08ms
step:779/2315 train_time:46803ms step_avg:60.08ms
step:780/2315 train_time:46863ms step_avg:60.08ms
step:781/2315 train_time:46924ms step_avg:60.08ms
step:782/2315 train_time:46985ms step_avg:60.08ms
step:783/2315 train_time:47045ms step_avg:60.08ms
step:784/2315 train_time:47106ms step_avg:60.08ms
step:785/2315 train_time:47168ms step_avg:60.09ms
step:786/2315 train_time:47229ms step_avg:60.09ms
step:787/2315 train_time:47290ms step_avg:60.09ms
step:788/2315 train_time:47351ms step_avg:60.09ms
step:789/2315 train_time:47411ms step_avg:60.09ms
step:790/2315 train_time:47472ms step_avg:60.09ms
step:791/2315 train_time:47533ms step_avg:60.09ms
step:792/2315 train_time:47594ms step_avg:60.09ms
step:793/2315 train_time:47654ms step_avg:60.09ms
step:794/2315 train_time:47715ms step_avg:60.09ms
step:795/2315 train_time:47775ms step_avg:60.09ms
step:796/2315 train_time:47835ms step_avg:60.09ms
step:797/2315 train_time:47895ms step_avg:60.09ms
step:798/2315 train_time:47955ms step_avg:60.09ms
step:799/2315 train_time:48017ms step_avg:60.10ms
step:800/2315 train_time:48079ms step_avg:60.10ms
step:801/2315 train_time:48140ms step_avg:60.10ms
step:802/2315 train_time:48201ms step_avg:60.10ms
step:803/2315 train_time:48262ms step_avg:60.10ms
step:804/2315 train_time:48324ms step_avg:60.10ms
step:805/2315 train_time:48385ms step_avg:60.11ms
step:806/2315 train_time:48445ms step_avg:60.11ms
step:807/2315 train_time:48507ms step_avg:60.11ms
step:808/2315 train_time:48568ms step_avg:60.11ms
step:809/2315 train_time:48628ms step_avg:60.11ms
step:810/2315 train_time:48689ms step_avg:60.11ms
step:811/2315 train_time:48750ms step_avg:60.11ms
step:812/2315 train_time:48810ms step_avg:60.11ms
step:813/2315 train_time:48871ms step_avg:60.11ms
step:814/2315 train_time:48931ms step_avg:60.11ms
step:815/2315 train_time:48992ms step_avg:60.11ms
step:816/2315 train_time:49053ms step_avg:60.11ms
step:817/2315 train_time:49113ms step_avg:60.11ms
step:818/2315 train_time:49174ms step_avg:60.11ms
step:819/2315 train_time:49234ms step_avg:60.12ms
step:820/2315 train_time:49295ms step_avg:60.12ms
step:821/2315 train_time:49356ms step_avg:60.12ms
step:822/2315 train_time:49417ms step_avg:60.12ms
step:823/2315 train_time:49478ms step_avg:60.12ms
step:824/2315 train_time:49540ms step_avg:60.12ms
step:825/2315 train_time:49601ms step_avg:60.12ms
step:826/2315 train_time:49662ms step_avg:60.12ms
step:827/2315 train_time:49723ms step_avg:60.12ms
step:828/2315 train_time:49784ms step_avg:60.13ms
step:829/2315 train_time:49844ms step_avg:60.13ms
step:830/2315 train_time:49905ms step_avg:60.13ms
step:831/2315 train_time:49966ms step_avg:60.13ms
step:832/2315 train_time:50027ms step_avg:60.13ms
step:833/2315 train_time:50088ms step_avg:60.13ms
step:834/2315 train_time:50149ms step_avg:60.13ms
step:835/2315 train_time:50210ms step_avg:60.13ms
step:836/2315 train_time:50271ms step_avg:60.13ms
step:837/2315 train_time:50332ms step_avg:60.13ms
step:838/2315 train_time:50393ms step_avg:60.13ms
step:839/2315 train_time:50454ms step_avg:60.14ms
step:840/2315 train_time:50514ms step_avg:60.14ms
step:841/2315 train_time:50574ms step_avg:60.14ms
step:842/2315 train_time:50635ms step_avg:60.14ms
step:843/2315 train_time:50695ms step_avg:60.14ms
step:844/2315 train_time:50757ms step_avg:60.14ms
step:845/2315 train_time:50818ms step_avg:60.14ms
step:846/2315 train_time:50879ms step_avg:60.14ms
step:847/2315 train_time:50940ms step_avg:60.14ms
step:848/2315 train_time:51002ms step_avg:60.14ms
step:849/2315 train_time:51062ms step_avg:60.14ms
step:850/2315 train_time:51123ms step_avg:60.14ms
step:851/2315 train_time:51184ms step_avg:60.15ms
step:852/2315 train_time:51246ms step_avg:60.15ms
step:853/2315 train_time:51307ms step_avg:60.15ms
step:854/2315 train_time:51368ms step_avg:60.15ms
step:855/2315 train_time:51428ms step_avg:60.15ms
step:856/2315 train_time:51489ms step_avg:60.15ms
step:857/2315 train_time:51551ms step_avg:60.15ms
step:858/2315 train_time:51611ms step_avg:60.15ms
step:859/2315 train_time:51672ms step_avg:60.15ms
step:860/2315 train_time:51733ms step_avg:60.15ms
step:861/2315 train_time:51793ms step_avg:60.16ms
step:862/2315 train_time:51854ms step_avg:60.16ms
step:863/2315 train_time:51915ms step_avg:60.16ms
step:864/2315 train_time:51975ms step_avg:60.16ms
step:865/2315 train_time:52036ms step_avg:60.16ms
step:866/2315 train_time:52097ms step_avg:60.16ms
step:867/2315 train_time:52157ms step_avg:60.16ms
step:868/2315 train_time:52219ms step_avg:60.16ms
step:869/2315 train_time:52280ms step_avg:60.16ms
step:870/2315 train_time:52340ms step_avg:60.16ms
step:871/2315 train_time:52401ms step_avg:60.16ms
step:872/2315 train_time:52463ms step_avg:60.16ms
step:873/2315 train_time:52524ms step_avg:60.16ms
step:874/2315 train_time:52585ms step_avg:60.17ms
step:875/2315 train_time:52646ms step_avg:60.17ms
step:876/2315 train_time:52706ms step_avg:60.17ms
step:877/2315 train_time:52768ms step_avg:60.17ms
step:878/2315 train_time:52829ms step_avg:60.17ms
step:879/2315 train_time:52889ms step_avg:60.17ms
step:880/2315 train_time:52950ms step_avg:60.17ms
step:881/2315 train_time:53011ms step_avg:60.17ms
step:882/2315 train_time:53071ms step_avg:60.17ms
step:883/2315 train_time:53132ms step_avg:60.17ms
step:884/2315 train_time:53193ms step_avg:60.17ms
step:885/2315 train_time:53254ms step_avg:60.17ms
step:886/2315 train_time:53314ms step_avg:60.17ms
step:887/2315 train_time:53375ms step_avg:60.17ms
step:888/2315 train_time:53436ms step_avg:60.18ms
step:889/2315 train_time:53496ms step_avg:60.18ms
step:890/2315 train_time:53557ms step_avg:60.18ms
step:891/2315 train_time:53618ms step_avg:60.18ms
step:892/2315 train_time:53679ms step_avg:60.18ms
step:893/2315 train_time:53740ms step_avg:60.18ms
step:894/2315 train_time:53801ms step_avg:60.18ms
step:895/2315 train_time:53863ms step_avg:60.18ms
step:896/2315 train_time:53924ms step_avg:60.18ms
step:897/2315 train_time:53985ms step_avg:60.18ms
step:898/2315 train_time:54046ms step_avg:60.18ms
step:899/2315 train_time:54107ms step_avg:60.19ms
step:900/2315 train_time:54168ms step_avg:60.19ms
step:901/2315 train_time:54229ms step_avg:60.19ms
step:902/2315 train_time:54289ms step_avg:60.19ms
step:903/2315 train_time:54350ms step_avg:60.19ms
step:904/2315 train_time:54412ms step_avg:60.19ms
step:905/2315 train_time:54472ms step_avg:60.19ms
step:906/2315 train_time:54533ms step_avg:60.19ms
step:907/2315 train_time:54593ms step_avg:60.19ms
step:908/2315 train_time:54654ms step_avg:60.19ms
step:909/2315 train_time:54714ms step_avg:60.19ms
step:910/2315 train_time:54774ms step_avg:60.19ms
step:911/2315 train_time:54835ms step_avg:60.19ms
step:912/2315 train_time:54896ms step_avg:60.19ms
step:913/2315 train_time:54957ms step_avg:60.19ms
step:914/2315 train_time:55018ms step_avg:60.19ms
step:915/2315 train_time:55079ms step_avg:60.20ms
step:916/2315 train_time:55140ms step_avg:60.20ms
step:917/2315 train_time:55201ms step_avg:60.20ms
step:918/2315 train_time:55262ms step_avg:60.20ms
step:919/2315 train_time:55323ms step_avg:60.20ms
step:920/2315 train_time:55384ms step_avg:60.20ms
step:921/2315 train_time:55445ms step_avg:60.20ms
step:922/2315 train_time:55507ms step_avg:60.20ms
step:923/2315 train_time:55568ms step_avg:60.20ms
step:924/2315 train_time:55629ms step_avg:60.20ms
step:925/2315 train_time:55689ms step_avg:60.20ms
step:926/2315 train_time:55750ms step_avg:60.21ms
step:927/2315 train_time:55811ms step_avg:60.21ms
step:928/2315 train_time:55872ms step_avg:60.21ms
step:929/2315 train_time:55933ms step_avg:60.21ms
step:930/2315 train_time:55993ms step_avg:60.21ms
step:931/2315 train_time:56054ms step_avg:60.21ms
step:932/2315 train_time:56115ms step_avg:60.21ms
step:933/2315 train_time:56175ms step_avg:60.21ms
step:934/2315 train_time:56235ms step_avg:60.21ms
step:935/2315 train_time:56296ms step_avg:60.21ms
step:936/2315 train_time:56356ms step_avg:60.21ms
step:937/2315 train_time:56417ms step_avg:60.21ms
step:938/2315 train_time:56478ms step_avg:60.21ms
step:939/2315 train_time:56539ms step_avg:60.21ms
step:940/2315 train_time:56599ms step_avg:60.21ms
step:941/2315 train_time:56661ms step_avg:60.21ms
step:942/2315 train_time:56722ms step_avg:60.21ms
step:943/2315 train_time:56783ms step_avg:60.22ms
step:944/2315 train_time:56845ms step_avg:60.22ms
step:945/2315 train_time:56905ms step_avg:60.22ms
step:946/2315 train_time:56966ms step_avg:60.22ms
step:947/2315 train_time:57027ms step_avg:60.22ms
step:948/2315 train_time:57087ms step_avg:60.22ms
step:949/2315 train_time:57148ms step_avg:60.22ms
step:950/2315 train_time:57209ms step_avg:60.22ms
step:951/2315 train_time:57270ms step_avg:60.22ms
step:952/2315 train_time:57331ms step_avg:60.22ms
step:953/2315 train_time:57391ms step_avg:60.22ms
step:954/2315 train_time:57452ms step_avg:60.22ms
step:955/2315 train_time:57512ms step_avg:60.22ms
step:956/2315 train_time:57574ms step_avg:60.22ms
step:957/2315 train_time:57634ms step_avg:60.22ms
step:958/2315 train_time:57694ms step_avg:60.22ms
step:959/2315 train_time:57755ms step_avg:60.22ms
step:960/2315 train_time:57816ms step_avg:60.23ms
step:961/2315 train_time:57877ms step_avg:60.23ms
step:962/2315 train_time:57938ms step_avg:60.23ms
step:963/2315 train_time:57999ms step_avg:60.23ms
step:964/2315 train_time:58060ms step_avg:60.23ms
step:965/2315 train_time:58121ms step_avg:60.23ms
step:966/2315 train_time:58182ms step_avg:60.23ms
step:967/2315 train_time:58243ms step_avg:60.23ms
step:968/2315 train_time:58303ms step_avg:60.23ms
step:969/2315 train_time:58364ms step_avg:60.23ms
step:970/2315 train_time:58425ms step_avg:60.23ms
step:971/2315 train_time:58487ms step_avg:60.23ms
step:972/2315 train_time:58548ms step_avg:60.23ms
step:973/2315 train_time:58608ms step_avg:60.23ms
step:974/2315 train_time:58670ms step_avg:60.24ms
step:975/2315 train_time:58730ms step_avg:60.24ms
step:976/2315 train_time:58791ms step_avg:60.24ms
step:977/2315 train_time:58852ms step_avg:60.24ms
step:978/2315 train_time:58913ms step_avg:60.24ms
step:979/2315 train_time:58973ms step_avg:60.24ms
step:980/2315 train_time:59033ms step_avg:60.24ms
step:981/2315 train_time:59094ms step_avg:60.24ms
step:982/2315 train_time:59155ms step_avg:60.24ms
step:983/2315 train_time:59215ms step_avg:60.24ms
step:984/2315 train_time:59276ms step_avg:60.24ms
step:985/2315 train_time:59337ms step_avg:60.24ms
step:986/2315 train_time:59398ms step_avg:60.24ms
step:987/2315 train_time:59458ms step_avg:60.24ms
step:988/2315 train_time:59520ms step_avg:60.24ms
step:989/2315 train_time:59581ms step_avg:60.24ms
step:990/2315 train_time:59642ms step_avg:60.24ms
step:991/2315 train_time:59703ms step_avg:60.25ms
step:992/2315 train_time:59764ms step_avg:60.25ms
step:993/2315 train_time:59826ms step_avg:60.25ms
step:994/2315 train_time:59886ms step_avg:60.25ms
step:995/2315 train_time:59947ms step_avg:60.25ms
step:996/2315 train_time:60008ms step_avg:60.25ms
step:997/2315 train_time:60069ms step_avg:60.25ms
step:998/2315 train_time:60129ms step_avg:60.25ms
step:999/2315 train_time:60190ms step_avg:60.25ms
step:1000/2315 train_time:60250ms step_avg:60.25ms
step:1000/2315 val_loss:3.5667 train_time:60313ms step_avg:60.31ms
step:1001/2315 train_time:60338ms step_avg:60.28ms
step:1002/2315 train_time:60376ms step_avg:60.26ms
step:1003/2315 train_time:60438ms step_avg:60.26ms
step:1004/2315 train_time:60503ms step_avg:60.26ms
step:1005/2315 train_time:60564ms step_avg:60.26ms
step:1006/2315 train_time:60625ms step_avg:60.26ms
step:1007/2315 train_time:60685ms step_avg:60.26ms
step:1008/2315 train_time:60745ms step_avg:60.26ms
step:1009/2315 train_time:60805ms step_avg:60.26ms
step:1010/2315 train_time:60865ms step_avg:60.26ms
step:1011/2315 train_time:60926ms step_avg:60.26ms
step:1012/2315 train_time:60985ms step_avg:60.26ms
step:1013/2315 train_time:61046ms step_avg:60.26ms
step:1014/2315 train_time:61105ms step_avg:60.26ms
step:1015/2315 train_time:61165ms step_avg:60.26ms
step:1016/2315 train_time:61227ms step_avg:60.26ms
step:1017/2315 train_time:61291ms step_avg:60.27ms
step:1018/2315 train_time:61353ms step_avg:60.27ms
step:1019/2315 train_time:61415ms step_avg:60.27ms
step:1020/2315 train_time:61476ms step_avg:60.27ms
step:1021/2315 train_time:61536ms step_avg:60.27ms
step:1022/2315 train_time:61597ms step_avg:60.27ms
step:1023/2315 train_time:61658ms step_avg:60.27ms
step:1024/2315 train_time:61719ms step_avg:60.27ms
step:1025/2315 train_time:61780ms step_avg:60.27ms
step:1026/2315 train_time:61841ms step_avg:60.27ms
step:1027/2315 train_time:61901ms step_avg:60.27ms
step:1028/2315 train_time:61962ms step_avg:60.27ms
step:1029/2315 train_time:62022ms step_avg:60.27ms
step:1030/2315 train_time:62083ms step_avg:60.27ms
step:1031/2315 train_time:62142ms step_avg:60.27ms
step:1032/2315 train_time:62203ms step_avg:60.27ms
step:1033/2315 train_time:62264ms step_avg:60.28ms
step:1034/2315 train_time:62326ms step_avg:60.28ms
step:1035/2315 train_time:62388ms step_avg:60.28ms
step:1036/2315 train_time:62449ms step_avg:60.28ms
step:1037/2315 train_time:62510ms step_avg:60.28ms
step:1038/2315 train_time:62571ms step_avg:60.28ms
step:1039/2315 train_time:62631ms step_avg:60.28ms
step:1040/2315 train_time:62693ms step_avg:60.28ms
step:1041/2315 train_time:62753ms step_avg:60.28ms
step:1042/2315 train_time:62814ms step_avg:60.28ms
step:1043/2315 train_time:62874ms step_avg:60.28ms
step:1044/2315 train_time:62935ms step_avg:60.28ms
step:1045/2315 train_time:62995ms step_avg:60.28ms
step:1046/2315 train_time:63056ms step_avg:60.28ms
step:1047/2315 train_time:63116ms step_avg:60.28ms
step:1048/2315 train_time:63176ms step_avg:60.28ms
step:1049/2315 train_time:63237ms step_avg:60.28ms
step:1050/2315 train_time:63299ms step_avg:60.28ms
step:1051/2315 train_time:63359ms step_avg:60.28ms
step:1052/2315 train_time:63421ms step_avg:60.29ms
step:1053/2315 train_time:63482ms step_avg:60.29ms
step:1054/2315 train_time:63544ms step_avg:60.29ms
step:1055/2315 train_time:63605ms step_avg:60.29ms
step:1056/2315 train_time:63665ms step_avg:60.29ms
step:1057/2315 train_time:63727ms step_avg:60.29ms
step:1058/2315 train_time:63788ms step_avg:60.29ms
step:1059/2315 train_time:63848ms step_avg:60.29ms
step:1060/2315 train_time:63909ms step_avg:60.29ms
step:1061/2315 train_time:63970ms step_avg:60.29ms
step:1062/2315 train_time:64031ms step_avg:60.29ms
step:1063/2315 train_time:64091ms step_avg:60.29ms
step:1064/2315 train_time:64151ms step_avg:60.29ms
step:1065/2315 train_time:64212ms step_avg:60.29ms
step:1066/2315 train_time:64273ms step_avg:60.29ms
step:1067/2315 train_time:64334ms step_avg:60.29ms
step:1068/2315 train_time:64395ms step_avg:60.29ms
step:1069/2315 train_time:64455ms step_avg:60.29ms
step:1070/2315 train_time:64516ms step_avg:60.30ms
step:1071/2315 train_time:64577ms step_avg:60.30ms
step:1072/2315 train_time:64639ms step_avg:60.30ms
step:1073/2315 train_time:64700ms step_avg:60.30ms
step:1074/2315 train_time:64761ms step_avg:60.30ms
step:1075/2315 train_time:64822ms step_avg:60.30ms
step:1076/2315 train_time:64883ms step_avg:60.30ms
step:1077/2315 train_time:64944ms step_avg:60.30ms
step:1078/2315 train_time:65004ms step_avg:60.30ms
step:1079/2315 train_time:65065ms step_avg:60.30ms
step:1080/2315 train_time:65126ms step_avg:60.30ms
step:1081/2315 train_time:65188ms step_avg:60.30ms
step:1082/2315 train_time:65248ms step_avg:60.30ms
step:1083/2315 train_time:65309ms step_avg:60.30ms
step:1084/2315 train_time:65370ms step_avg:60.30ms
step:1085/2315 train_time:65431ms step_avg:60.31ms
step:1086/2315 train_time:65492ms step_avg:60.31ms
step:1087/2315 train_time:65553ms step_avg:60.31ms
step:1088/2315 train_time:65614ms step_avg:60.31ms
step:1089/2315 train_time:65674ms step_avg:60.31ms
step:1090/2315 train_time:65734ms step_avg:60.31ms
step:1091/2315 train_time:65795ms step_avg:60.31ms
step:1092/2315 train_time:65855ms step_avg:60.31ms
step:1093/2315 train_time:65916ms step_avg:60.31ms
step:1094/2315 train_time:65977ms step_avg:60.31ms
step:1095/2315 train_time:66039ms step_avg:60.31ms
step:1096/2315 train_time:66099ms step_avg:60.31ms
step:1097/2315 train_time:66160ms step_avg:60.31ms
step:1098/2315 train_time:66222ms step_avg:60.31ms
step:1099/2315 train_time:66283ms step_avg:60.31ms
step:1100/2315 train_time:66344ms step_avg:60.31ms
step:1101/2315 train_time:66405ms step_avg:60.31ms
step:1102/2315 train_time:66466ms step_avg:60.31ms
step:1103/2315 train_time:66527ms step_avg:60.31ms
step:1104/2315 train_time:66588ms step_avg:60.31ms
step:1105/2315 train_time:66648ms step_avg:60.32ms
step:1106/2315 train_time:66709ms step_avg:60.32ms
step:1107/2315 train_time:66770ms step_avg:60.32ms
step:1108/2315 train_time:66831ms step_avg:60.32ms
step:1109/2315 train_time:66891ms step_avg:60.32ms
step:1110/2315 train_time:66952ms step_avg:60.32ms
step:1111/2315 train_time:67013ms step_avg:60.32ms
step:1112/2315 train_time:67074ms step_avg:60.32ms
step:1113/2315 train_time:67134ms step_avg:60.32ms
step:1114/2315 train_time:67194ms step_avg:60.32ms
step:1115/2315 train_time:67255ms step_avg:60.32ms
step:1116/2315 train_time:67316ms step_avg:60.32ms
step:1117/2315 train_time:67377ms step_avg:60.32ms
step:1118/2315 train_time:67438ms step_avg:60.32ms
step:1119/2315 train_time:67499ms step_avg:60.32ms
step:1120/2315 train_time:67560ms step_avg:60.32ms
step:1121/2315 train_time:67621ms step_avg:60.32ms
step:1122/2315 train_time:67682ms step_avg:60.32ms
step:1123/2315 train_time:67743ms step_avg:60.32ms
step:1124/2315 train_time:67804ms step_avg:60.32ms
step:1125/2315 train_time:67865ms step_avg:60.32ms
step:1126/2315 train_time:67926ms step_avg:60.33ms
step:1127/2315 train_time:67988ms step_avg:60.33ms
step:1128/2315 train_time:68049ms step_avg:60.33ms
step:1129/2315 train_time:68110ms step_avg:60.33ms
step:1130/2315 train_time:68170ms step_avg:60.33ms
step:1131/2315 train_time:68231ms step_avg:60.33ms
step:1132/2315 train_time:68291ms step_avg:60.33ms
step:1133/2315 train_time:68352ms step_avg:60.33ms
step:1134/2315 train_time:68413ms step_avg:60.33ms
step:1135/2315 train_time:68474ms step_avg:60.33ms
step:1136/2315 train_time:68534ms step_avg:60.33ms
step:1137/2315 train_time:68594ms step_avg:60.33ms
step:1138/2315 train_time:68654ms step_avg:60.33ms
step:1139/2315 train_time:68715ms step_avg:60.33ms
step:1140/2315 train_time:68776ms step_avg:60.33ms
step:1141/2315 train_time:68837ms step_avg:60.33ms
step:1142/2315 train_time:68899ms step_avg:60.33ms
step:1143/2315 train_time:68960ms step_avg:60.33ms
step:1144/2315 train_time:69020ms step_avg:60.33ms
step:1145/2315 train_time:69081ms step_avg:60.33ms
step:1146/2315 train_time:69142ms step_avg:60.33ms
step:1147/2315 train_time:69203ms step_avg:60.33ms
step:1148/2315 train_time:69264ms step_avg:60.33ms
step:1149/2315 train_time:69325ms step_avg:60.34ms
step:1150/2315 train_time:69387ms step_avg:60.34ms
step:1151/2315 train_time:69448ms step_avg:60.34ms
step:1152/2315 train_time:69508ms step_avg:60.34ms
step:1153/2315 train_time:69569ms step_avg:60.34ms
step:1154/2315 train_time:69630ms step_avg:60.34ms
step:1155/2315 train_time:69690ms step_avg:60.34ms
step:1156/2315 train_time:69751ms step_avg:60.34ms
step:1157/2315 train_time:69812ms step_avg:60.34ms
step:1158/2315 train_time:69873ms step_avg:60.34ms
step:1159/2315 train_time:69933ms step_avg:60.34ms
step:1160/2315 train_time:69994ms step_avg:60.34ms
step:1161/2315 train_time:70054ms step_avg:60.34ms
step:1162/2315 train_time:70114ms step_avg:60.34ms
step:1163/2315 train_time:70175ms step_avg:60.34ms
step:1164/2315 train_time:70235ms step_avg:60.34ms
step:1165/2315 train_time:70297ms step_avg:60.34ms
step:1166/2315 train_time:70357ms step_avg:60.34ms
step:1167/2315 train_time:70418ms step_avg:60.34ms
step:1168/2315 train_time:70479ms step_avg:60.34ms
step:1169/2315 train_time:70540ms step_avg:60.34ms
step:1170/2315 train_time:70601ms step_avg:60.34ms
step:1171/2315 train_time:70662ms step_avg:60.34ms
step:1172/2315 train_time:70723ms step_avg:60.34ms
step:1173/2315 train_time:70784ms step_avg:60.34ms
step:1174/2315 train_time:70844ms step_avg:60.34ms
step:1175/2315 train_time:70906ms step_avg:60.35ms
step:1176/2315 train_time:70966ms step_avg:60.35ms
step:1177/2315 train_time:71028ms step_avg:60.35ms
step:1178/2315 train_time:71088ms step_avg:60.35ms
step:1179/2315 train_time:71149ms step_avg:60.35ms
step:1180/2315 train_time:71210ms step_avg:60.35ms
step:1181/2315 train_time:71270ms step_avg:60.35ms
step:1182/2315 train_time:71331ms step_avg:60.35ms
step:1183/2315 train_time:71391ms step_avg:60.35ms
step:1184/2315 train_time:71452ms step_avg:60.35ms
step:1185/2315 train_time:71513ms step_avg:60.35ms
step:1186/2315 train_time:71573ms step_avg:60.35ms
step:1187/2315 train_time:71634ms step_avg:60.35ms
step:1188/2315 train_time:71695ms step_avg:60.35ms
step:1189/2315 train_time:71755ms step_avg:60.35ms
step:1190/2315 train_time:71816ms step_avg:60.35ms
step:1191/2315 train_time:71877ms step_avg:60.35ms
step:1192/2315 train_time:71938ms step_avg:60.35ms
step:1193/2315 train_time:71999ms step_avg:60.35ms
step:1194/2315 train_time:72059ms step_avg:60.35ms
step:1195/2315 train_time:72120ms step_avg:60.35ms
step:1196/2315 train_time:72181ms step_avg:60.35ms
step:1197/2315 train_time:72243ms step_avg:60.35ms
step:1198/2315 train_time:72304ms step_avg:60.35ms
step:1199/2315 train_time:72365ms step_avg:60.35ms
step:1200/2315 train_time:72427ms step_avg:60.36ms
step:1201/2315 train_time:72488ms step_avg:60.36ms
step:1202/2315 train_time:72548ms step_avg:60.36ms
step:1203/2315 train_time:72609ms step_avg:60.36ms
step:1204/2315 train_time:72669ms step_avg:60.36ms
step:1205/2315 train_time:72730ms step_avg:60.36ms
step:1206/2315 train_time:72791ms step_avg:60.36ms
step:1207/2315 train_time:72852ms step_avg:60.36ms
step:1208/2315 train_time:72912ms step_avg:60.36ms
step:1209/2315 train_time:72974ms step_avg:60.36ms
step:1210/2315 train_time:73034ms step_avg:60.36ms
step:1211/2315 train_time:73094ms step_avg:60.36ms
step:1212/2315 train_time:73155ms step_avg:60.36ms
step:1213/2315 train_time:73216ms step_avg:60.36ms
step:1214/2315 train_time:73277ms step_avg:60.36ms
step:1215/2315 train_time:73338ms step_avg:60.36ms
step:1216/2315 train_time:73399ms step_avg:60.36ms
step:1217/2315 train_time:73460ms step_avg:60.36ms
step:1218/2315 train_time:73521ms step_avg:60.36ms
step:1219/2315 train_time:73583ms step_avg:60.36ms
step:1220/2315 train_time:73643ms step_avg:60.36ms
step:1221/2315 train_time:73705ms step_avg:60.36ms
step:1222/2315 train_time:73765ms step_avg:60.36ms
step:1223/2315 train_time:73826ms step_avg:60.37ms
step:1224/2315 train_time:73887ms step_avg:60.37ms
step:1225/2315 train_time:73948ms step_avg:60.37ms
step:1226/2315 train_time:74009ms step_avg:60.37ms
step:1227/2315 train_time:74070ms step_avg:60.37ms
step:1228/2315 train_time:74131ms step_avg:60.37ms
step:1229/2315 train_time:74191ms step_avg:60.37ms
step:1230/2315 train_time:74252ms step_avg:60.37ms
step:1231/2315 train_time:74312ms step_avg:60.37ms
step:1232/2315 train_time:74373ms step_avg:60.37ms
step:1233/2315 train_time:74433ms step_avg:60.37ms
step:1234/2315 train_time:74494ms step_avg:60.37ms
step:1235/2315 train_time:74555ms step_avg:60.37ms
step:1236/2315 train_time:74615ms step_avg:60.37ms
step:1237/2315 train_time:74676ms step_avg:60.37ms
step:1238/2315 train_time:74737ms step_avg:60.37ms
step:1239/2315 train_time:74798ms step_avg:60.37ms
step:1240/2315 train_time:74858ms step_avg:60.37ms
step:1241/2315 train_time:74919ms step_avg:60.37ms
step:1242/2315 train_time:74980ms step_avg:60.37ms
step:1243/2315 train_time:75042ms step_avg:60.37ms
step:1244/2315 train_time:75103ms step_avg:60.37ms
step:1245/2315 train_time:75164ms step_avg:60.37ms
step:1246/2315 train_time:75225ms step_avg:60.37ms
step:1247/2315 train_time:75286ms step_avg:60.37ms
step:1248/2315 train_time:75346ms step_avg:60.37ms
step:1249/2315 train_time:75407ms step_avg:60.37ms
step:1250/2315 train_time:75468ms step_avg:60.37ms
step:1250/2315 val_loss:3.5122 train_time:75531ms step_avg:60.42ms
step:1251/2315 train_time:75551ms step_avg:60.39ms
step:1252/2315 train_time:75592ms step_avg:60.38ms
step:1253/2315 train_time:75659ms step_avg:60.38ms
step:1254/2315 train_time:75722ms step_avg:60.38ms
step:1255/2315 train_time:75785ms step_avg:60.39ms
step:1256/2315 train_time:75845ms step_avg:60.39ms
step:1257/2315 train_time:75906ms step_avg:60.39ms
step:1258/2315 train_time:75967ms step_avg:60.39ms
step:1259/2315 train_time:76027ms step_avg:60.39ms
step:1260/2315 train_time:76086ms step_avg:60.39ms
step:1261/2315 train_time:76146ms step_avg:60.39ms
step:1262/2315 train_time:76206ms step_avg:60.39ms
step:1263/2315 train_time:76266ms step_avg:60.38ms
step:1264/2315 train_time:76326ms step_avg:60.38ms
step:1265/2315 train_time:76385ms step_avg:60.38ms
step:1266/2315 train_time:76445ms step_avg:60.38ms
step:1267/2315 train_time:76506ms step_avg:60.38ms
step:1268/2315 train_time:76568ms step_avg:60.38ms
step:1269/2315 train_time:76631ms step_avg:60.39ms
step:1270/2315 train_time:76693ms step_avg:60.39ms
step:1271/2315 train_time:76755ms step_avg:60.39ms
step:1272/2315 train_time:76817ms step_avg:60.39ms
step:1273/2315 train_time:76877ms step_avg:60.39ms
step:1274/2315 train_time:76938ms step_avg:60.39ms
step:1275/2315 train_time:76999ms step_avg:60.39ms
step:1276/2315 train_time:77060ms step_avg:60.39ms
step:1277/2315 train_time:77120ms step_avg:60.39ms
step:1278/2315 train_time:77183ms step_avg:60.39ms
step:1279/2315 train_time:77241ms step_avg:60.39ms
step:1280/2315 train_time:77302ms step_avg:60.39ms
step:1281/2315 train_time:77362ms step_avg:60.39ms
step:1282/2315 train_time:77422ms step_avg:60.39ms
step:1283/2315 train_time:77484ms step_avg:60.39ms
step:1284/2315 train_time:77545ms step_avg:60.39ms
step:1285/2315 train_time:77607ms step_avg:60.39ms
step:1286/2315 train_time:77667ms step_avg:60.39ms
step:1287/2315 train_time:77728ms step_avg:60.39ms
step:1288/2315 train_time:77789ms step_avg:60.40ms
step:1289/2315 train_time:77850ms step_avg:60.40ms
step:1290/2315 train_time:77910ms step_avg:60.40ms
step:1291/2315 train_time:77971ms step_avg:60.40ms
step:1292/2315 train_time:78032ms step_avg:60.40ms
step:1293/2315 train_time:78092ms step_avg:60.40ms
step:1294/2315 train_time:78153ms step_avg:60.40ms
step:1295/2315 train_time:78214ms step_avg:60.40ms
step:1296/2315 train_time:78275ms step_avg:60.40ms
step:1297/2315 train_time:78335ms step_avg:60.40ms
step:1298/2315 train_time:78396ms step_avg:60.40ms
step:1299/2315 train_time:78457ms step_avg:60.40ms
step:1300/2315 train_time:78518ms step_avg:60.40ms
step:1301/2315 train_time:78580ms step_avg:60.40ms
step:1302/2315 train_time:78640ms step_avg:60.40ms
step:1303/2315 train_time:78702ms step_avg:60.40ms
step:1304/2315 train_time:78762ms step_avg:60.40ms
step:1305/2315 train_time:78824ms step_avg:60.40ms
step:1306/2315 train_time:78884ms step_avg:60.40ms
step:1307/2315 train_time:78945ms step_avg:60.40ms
step:1308/2315 train_time:79005ms step_avg:60.40ms
step:1309/2315 train_time:79066ms step_avg:60.40ms
step:1310/2315 train_time:79127ms step_avg:60.40ms
step:1311/2315 train_time:79187ms step_avg:60.40ms
step:1312/2315 train_time:79247ms step_avg:60.40ms
step:1313/2315 train_time:79307ms step_avg:60.40ms
step:1314/2315 train_time:79368ms step_avg:60.40ms
step:1315/2315 train_time:79428ms step_avg:60.40ms
step:1316/2315 train_time:79489ms step_avg:60.40ms
step:1317/2315 train_time:79550ms step_avg:60.40ms
step:1318/2315 train_time:79611ms step_avg:60.40ms
step:1319/2315 train_time:79673ms step_avg:60.40ms
step:1320/2315 train_time:79734ms step_avg:60.40ms
step:1321/2315 train_time:79795ms step_avg:60.41ms
step:1322/2315 train_time:79857ms step_avg:60.41ms
step:1323/2315 train_time:79918ms step_avg:60.41ms
step:1324/2315 train_time:79979ms step_avg:60.41ms
step:1325/2315 train_time:80039ms step_avg:60.41ms
step:1326/2315 train_time:80100ms step_avg:60.41ms
step:1327/2315 train_time:80161ms step_avg:60.41ms
step:1328/2315 train_time:80222ms step_avg:60.41ms
step:1329/2315 train_time:80283ms step_avg:60.41ms
step:1330/2315 train_time:80343ms step_avg:60.41ms
step:1331/2315 train_time:80404ms step_avg:60.41ms
step:1332/2315 train_time:80464ms step_avg:60.41ms
step:1333/2315 train_time:80525ms step_avg:60.41ms
step:1334/2315 train_time:80586ms step_avg:60.41ms
step:1335/2315 train_time:80647ms step_avg:60.41ms
step:1336/2315 train_time:80708ms step_avg:60.41ms
step:1337/2315 train_time:80768ms step_avg:60.41ms
step:1338/2315 train_time:80829ms step_avg:60.41ms
step:1339/2315 train_time:80890ms step_avg:60.41ms
step:1340/2315 train_time:80951ms step_avg:60.41ms
step:1341/2315 train_time:81012ms step_avg:60.41ms
step:1342/2315 train_time:81073ms step_avg:60.41ms
step:1343/2315 train_time:81134ms step_avg:60.41ms
step:1344/2315 train_time:81195ms step_avg:60.41ms
step:1345/2315 train_time:81256ms step_avg:60.41ms
step:1346/2315 train_time:81317ms step_avg:60.41ms
step:1347/2315 train_time:81378ms step_avg:60.41ms
step:1348/2315 train_time:81439ms step_avg:60.41ms
step:1349/2315 train_time:81501ms step_avg:60.42ms
step:1350/2315 train_time:81562ms step_avg:60.42ms
step:1351/2315 train_time:81622ms step_avg:60.42ms
step:1352/2315 train_time:81684ms step_avg:60.42ms
step:1353/2315 train_time:81744ms step_avg:60.42ms
step:1354/2315 train_time:81805ms step_avg:60.42ms
step:1355/2315 train_time:81866ms step_avg:60.42ms
step:1356/2315 train_time:81926ms step_avg:60.42ms
step:1357/2315 train_time:81987ms step_avg:60.42ms
step:1358/2315 train_time:82047ms step_avg:60.42ms
step:1359/2315 train_time:82107ms step_avg:60.42ms
step:1360/2315 train_time:82168ms step_avg:60.42ms
step:1361/2315 train_time:82229ms step_avg:60.42ms
step:1362/2315 train_time:82289ms step_avg:60.42ms
step:1363/2315 train_time:82350ms step_avg:60.42ms
step:1364/2315 train_time:82410ms step_avg:60.42ms
step:1365/2315 train_time:82472ms step_avg:60.42ms
step:1366/2315 train_time:82533ms step_avg:60.42ms
step:1367/2315 train_time:82594ms step_avg:60.42ms
step:1368/2315 train_time:82656ms step_avg:60.42ms
step:1369/2315 train_time:82717ms step_avg:60.42ms
step:1370/2315 train_time:82778ms step_avg:60.42ms
step:1371/2315 train_time:82839ms step_avg:60.42ms
step:1372/2315 train_time:82900ms step_avg:60.42ms
step:1373/2315 train_time:82961ms step_avg:60.42ms
step:1374/2315 train_time:83021ms step_avg:60.42ms
step:1375/2315 train_time:83083ms step_avg:60.42ms
step:1376/2315 train_time:83143ms step_avg:60.42ms
step:1377/2315 train_time:83205ms step_avg:60.42ms
step:1378/2315 train_time:83265ms step_avg:60.42ms
step:1379/2315 train_time:83326ms step_avg:60.43ms
step:1380/2315 train_time:83386ms step_avg:60.42ms
step:1381/2315 train_time:83447ms step_avg:60.43ms
step:1382/2315 train_time:83507ms step_avg:60.43ms
step:1383/2315 train_time:83568ms step_avg:60.43ms
step:1384/2315 train_time:83629ms step_avg:60.43ms
step:1385/2315 train_time:83690ms step_avg:60.43ms
step:1386/2315 train_time:83751ms step_avg:60.43ms
step:1387/2315 train_time:83812ms step_avg:60.43ms
step:1388/2315 train_time:83873ms step_avg:60.43ms
step:1389/2315 train_time:83934ms step_avg:60.43ms
step:1390/2315 train_time:83995ms step_avg:60.43ms
step:1391/2315 train_time:84056ms step_avg:60.43ms
step:1392/2315 train_time:84117ms step_avg:60.43ms
step:1393/2315 train_time:84178ms step_avg:60.43ms
step:1394/2315 train_time:84238ms step_avg:60.43ms
step:1395/2315 train_time:84299ms step_avg:60.43ms
step:1396/2315 train_time:84360ms step_avg:60.43ms
step:1397/2315 train_time:84421ms step_avg:60.43ms
step:1398/2315 train_time:84482ms step_avg:60.43ms
step:1399/2315 train_time:84543ms step_avg:60.43ms
step:1400/2315 train_time:84604ms step_avg:60.43ms
step:1401/2315 train_time:84664ms step_avg:60.43ms
step:1402/2315 train_time:84725ms step_avg:60.43ms
step:1403/2315 train_time:84786ms step_avg:60.43ms
step:1404/2315 train_time:84847ms step_avg:60.43ms
step:1405/2315 train_time:84907ms step_avg:60.43ms
step:1406/2315 train_time:84968ms step_avg:60.43ms
step:1407/2315 train_time:85029ms step_avg:60.43ms
step:1408/2315 train_time:85089ms step_avg:60.43ms
step:1409/2315 train_time:85150ms step_avg:60.43ms
step:1410/2315 train_time:85211ms step_avg:60.43ms
step:1411/2315 train_time:85272ms step_avg:60.43ms
step:1412/2315 train_time:85333ms step_avg:60.43ms
step:1413/2315 train_time:85394ms step_avg:60.43ms
step:1414/2315 train_time:85455ms step_avg:60.44ms
step:1415/2315 train_time:85517ms step_avg:60.44ms
step:1416/2315 train_time:85577ms step_avg:60.44ms
step:1417/2315 train_time:85639ms step_avg:60.44ms
step:1418/2315 train_time:85700ms step_avg:60.44ms
step:1419/2315 train_time:85760ms step_avg:60.44ms
step:1420/2315 train_time:85821ms step_avg:60.44ms
step:1421/2315 train_time:85882ms step_avg:60.44ms
step:1422/2315 train_time:85943ms step_avg:60.44ms
step:1423/2315 train_time:86004ms step_avg:60.44ms
step:1424/2315 train_time:86064ms step_avg:60.44ms
step:1425/2315 train_time:86125ms step_avg:60.44ms
step:1426/2315 train_time:86186ms step_avg:60.44ms
step:1427/2315 train_time:86247ms step_avg:60.44ms
step:1428/2315 train_time:86307ms step_avg:60.44ms
step:1429/2315 train_time:86368ms step_avg:60.44ms
step:1430/2315 train_time:86428ms step_avg:60.44ms
step:1431/2315 train_time:86489ms step_avg:60.44ms
step:1432/2315 train_time:86549ms step_avg:60.44ms
step:1433/2315 train_time:86610ms step_avg:60.44ms
step:1434/2315 train_time:86672ms step_avg:60.44ms
step:1435/2315 train_time:86733ms step_avg:60.44ms
step:1436/2315 train_time:86794ms step_avg:60.44ms
step:1437/2315 train_time:86855ms step_avg:60.44ms
step:1438/2315 train_time:86916ms step_avg:60.44ms
step:1439/2315 train_time:86977ms step_avg:60.44ms
step:1440/2315 train_time:87038ms step_avg:60.44ms
step:1441/2315 train_time:87099ms step_avg:60.44ms
step:1442/2315 train_time:87160ms step_avg:60.44ms
step:1443/2315 train_time:87221ms step_avg:60.44ms
step:1444/2315 train_time:87281ms step_avg:60.44ms
step:1445/2315 train_time:87342ms step_avg:60.44ms
step:1446/2315 train_time:87404ms step_avg:60.45ms
step:1447/2315 train_time:87464ms step_avg:60.45ms
step:1448/2315 train_time:87524ms step_avg:60.44ms
step:1449/2315 train_time:87585ms step_avg:60.44ms
step:1450/2315 train_time:87646ms step_avg:60.45ms
step:1451/2315 train_time:87706ms step_avg:60.45ms
step:1452/2315 train_time:87767ms step_avg:60.45ms
step:1453/2315 train_time:87829ms step_avg:60.45ms
step:1454/2315 train_time:87889ms step_avg:60.45ms
step:1455/2315 train_time:87949ms step_avg:60.45ms
step:1456/2315 train_time:88010ms step_avg:60.45ms
step:1457/2315 train_time:88071ms step_avg:60.45ms
step:1458/2315 train_time:88133ms step_avg:60.45ms
step:1459/2315 train_time:88194ms step_avg:60.45ms
step:1460/2315 train_time:88255ms step_avg:60.45ms
step:1461/2315 train_time:88316ms step_avg:60.45ms
step:1462/2315 train_time:88377ms step_avg:60.45ms
step:1463/2315 train_time:88438ms step_avg:60.45ms
step:1464/2315 train_time:88499ms step_avg:60.45ms
step:1465/2315 train_time:88560ms step_avg:60.45ms
step:1466/2315 train_time:88621ms step_avg:60.45ms
step:1467/2315 train_time:88681ms step_avg:60.45ms
step:1468/2315 train_time:88742ms step_avg:60.45ms
step:1469/2315 train_time:88803ms step_avg:60.45ms
step:1470/2315 train_time:88865ms step_avg:60.45ms
step:1471/2315 train_time:88926ms step_avg:60.45ms
step:1472/2315 train_time:88987ms step_avg:60.45ms
step:1473/2315 train_time:89047ms step_avg:60.45ms
step:1474/2315 train_time:89108ms step_avg:60.45ms
step:1475/2315 train_time:89169ms step_avg:60.45ms
step:1476/2315 train_time:89229ms step_avg:60.45ms
step:1477/2315 train_time:89290ms step_avg:60.45ms
step:1478/2315 train_time:89352ms step_avg:60.45ms
step:1479/2315 train_time:89413ms step_avg:60.45ms
step:1480/2315 train_time:89474ms step_avg:60.46ms
step:1481/2315 train_time:89536ms step_avg:60.46ms
step:1482/2315 train_time:89597ms step_avg:60.46ms
step:1483/2315 train_time:89658ms step_avg:60.46ms
step:1484/2315 train_time:89718ms step_avg:60.46ms
step:1485/2315 train_time:89779ms step_avg:60.46ms
step:1486/2315 train_time:89841ms step_avg:60.46ms
step:1487/2315 train_time:89902ms step_avg:60.46ms
step:1488/2315 train_time:89963ms step_avg:60.46ms
step:1489/2315 train_time:90023ms step_avg:60.46ms
step:1490/2315 train_time:90085ms step_avg:60.46ms
step:1491/2315 train_time:90146ms step_avg:60.46ms
step:1492/2315 train_time:90207ms step_avg:60.46ms
step:1493/2315 train_time:90268ms step_avg:60.46ms
step:1494/2315 train_time:90328ms step_avg:60.46ms
step:1495/2315 train_time:90388ms step_avg:60.46ms
step:1496/2315 train_time:90449ms step_avg:60.46ms
step:1497/2315 train_time:90509ms step_avg:60.46ms
step:1498/2315 train_time:90569ms step_avg:60.46ms
step:1499/2315 train_time:90630ms step_avg:60.46ms
step:1500/2315 train_time:90691ms step_avg:60.46ms
step:1500/2315 val_loss:3.4472 train_time:90754ms step_avg:60.50ms
step:1501/2315 train_time:90777ms step_avg:60.48ms
step:1502/2315 train_time:90815ms step_avg:60.46ms
step:1503/2315 train_time:90877ms step_avg:60.46ms
step:1504/2315 train_time:90941ms step_avg:60.47ms
step:1505/2315 train_time:91002ms step_avg:60.47ms
step:1506/2315 train_time:91063ms step_avg:60.47ms
step:1507/2315 train_time:91126ms step_avg:60.47ms
step:1508/2315 train_time:91184ms step_avg:60.47ms
step:1509/2315 train_time:91245ms step_avg:60.47ms
step:1510/2315 train_time:91306ms step_avg:60.47ms
step:1511/2315 train_time:91367ms step_avg:60.47ms
step:1512/2315 train_time:91427ms step_avg:60.47ms
step:1513/2315 train_time:91488ms step_avg:60.47ms
step:1514/2315 train_time:91548ms step_avg:60.47ms
step:1515/2315 train_time:91608ms step_avg:60.47ms
step:1516/2315 train_time:91669ms step_avg:60.47ms
step:1517/2315 train_time:91730ms step_avg:60.47ms
step:1518/2315 train_time:91792ms step_avg:60.47ms
step:1519/2315 train_time:91854ms step_avg:60.47ms
step:1520/2315 train_time:91916ms step_avg:60.47ms
step:1521/2315 train_time:91978ms step_avg:60.47ms
step:1522/2315 train_time:92039ms step_avg:60.47ms
step:1523/2315 train_time:92101ms step_avg:60.47ms
step:1524/2315 train_time:92162ms step_avg:60.47ms
step:1525/2315 train_time:92223ms step_avg:60.47ms
step:1526/2315 train_time:92284ms step_avg:60.47ms
step:1527/2315 train_time:92345ms step_avg:60.47ms
step:1528/2315 train_time:92405ms step_avg:60.47ms
step:1529/2315 train_time:92466ms step_avg:60.47ms
step:1530/2315 train_time:92527ms step_avg:60.48ms
step:1531/2315 train_time:92588ms step_avg:60.48ms
step:1532/2315 train_time:92649ms step_avg:60.48ms
step:1533/2315 train_time:92711ms step_avg:60.48ms
step:1534/2315 train_time:92772ms step_avg:60.48ms
step:1535/2315 train_time:92835ms step_avg:60.48ms
step:1536/2315 train_time:92897ms step_avg:60.48ms
step:1537/2315 train_time:92958ms step_avg:60.48ms
step:1538/2315 train_time:93019ms step_avg:60.48ms
step:1539/2315 train_time:93080ms step_avg:60.48ms
step:1540/2315 train_time:93141ms step_avg:60.48ms
step:1541/2315 train_time:93202ms step_avg:60.48ms
step:1542/2315 train_time:93264ms step_avg:60.48ms
step:1543/2315 train_time:93324ms step_avg:60.48ms
step:1544/2315 train_time:93385ms step_avg:60.48ms
step:1545/2315 train_time:93446ms step_avg:60.48ms
step:1546/2315 train_time:93506ms step_avg:60.48ms
step:1547/2315 train_time:93567ms step_avg:60.48ms
step:1548/2315 train_time:93629ms step_avg:60.48ms
step:1549/2315 train_time:93690ms step_avg:60.48ms
step:1550/2315 train_time:93752ms step_avg:60.49ms
step:1551/2315 train_time:93813ms step_avg:60.49ms
step:1552/2315 train_time:93874ms step_avg:60.49ms
step:1553/2315 train_time:93936ms step_avg:60.49ms
step:1554/2315 train_time:93998ms step_avg:60.49ms
step:1555/2315 train_time:94060ms step_avg:60.49ms
step:1556/2315 train_time:94121ms step_avg:60.49ms
step:1557/2315 train_time:94182ms step_avg:60.49ms
step:1558/2315 train_time:94243ms step_avg:60.49ms
step:1559/2315 train_time:94304ms step_avg:60.49ms
step:1560/2315 train_time:94365ms step_avg:60.49ms
step:1561/2315 train_time:94426ms step_avg:60.49ms
step:1562/2315 train_time:94486ms step_avg:60.49ms
step:1563/2315 train_time:94548ms step_avg:60.49ms
step:1564/2315 train_time:94610ms step_avg:60.49ms
step:1565/2315 train_time:94672ms step_avg:60.49ms
step:1566/2315 train_time:94733ms step_avg:60.49ms
step:1567/2315 train_time:94795ms step_avg:60.49ms
step:1568/2315 train_time:94856ms step_avg:60.49ms
step:1569/2315 train_time:94917ms step_avg:60.50ms
step:1570/2315 train_time:94979ms step_avg:60.50ms
step:1571/2315 train_time:95040ms step_avg:60.50ms
step:1572/2315 train_time:95101ms step_avg:60.50ms
step:1573/2315 train_time:95162ms step_avg:60.50ms
step:1574/2315 train_time:95223ms step_avg:60.50ms
step:1575/2315 train_time:95285ms step_avg:60.50ms
step:1576/2315 train_time:95345ms step_avg:60.50ms
step:1577/2315 train_time:95406ms step_avg:60.50ms
step:1578/2315 train_time:95467ms step_avg:60.50ms
step:1579/2315 train_time:95529ms step_avg:60.50ms
step:1580/2315 train_time:95590ms step_avg:60.50ms
step:1581/2315 train_time:95651ms step_avg:60.50ms
step:1582/2315 train_time:95712ms step_avg:60.50ms
step:1583/2315 train_time:95774ms step_avg:60.50ms
step:1584/2315 train_time:95836ms step_avg:60.50ms
step:1585/2315 train_time:95897ms step_avg:60.50ms
step:1586/2315 train_time:95958ms step_avg:60.50ms
step:1587/2315 train_time:96019ms step_avg:60.50ms
step:1588/2315 train_time:96080ms step_avg:60.50ms
step:1589/2315 train_time:96141ms step_avg:60.50ms
step:1590/2315 train_time:96202ms step_avg:60.50ms
step:1591/2315 train_time:96263ms step_avg:60.50ms
step:1592/2315 train_time:96324ms step_avg:60.51ms
step:1593/2315 train_time:96385ms step_avg:60.51ms
step:1594/2315 train_time:96446ms step_avg:60.51ms
step:1595/2315 train_time:96508ms step_avg:60.51ms
step:1596/2315 train_time:96568ms step_avg:60.51ms
step:1597/2315 train_time:96630ms step_avg:60.51ms
step:1598/2315 train_time:96692ms step_avg:60.51ms
step:1599/2315 train_time:96754ms step_avg:60.51ms
step:1600/2315 train_time:96815ms step_avg:60.51ms
step:1601/2315 train_time:96877ms step_avg:60.51ms
step:1602/2315 train_time:96938ms step_avg:60.51ms
step:1603/2315 train_time:96999ms step_avg:60.51ms
step:1604/2315 train_time:97061ms step_avg:60.51ms
step:1605/2315 train_time:97121ms step_avg:60.51ms
step:1606/2315 train_time:97182ms step_avg:60.51ms
step:1607/2315 train_time:97244ms step_avg:60.51ms
step:1608/2315 train_time:97304ms step_avg:60.51ms
step:1609/2315 train_time:97365ms step_avg:60.51ms
step:1610/2315 train_time:97427ms step_avg:60.51ms
step:1611/2315 train_time:97488ms step_avg:60.51ms
step:1612/2315 train_time:97549ms step_avg:60.51ms
step:1613/2315 train_time:97610ms step_avg:60.51ms
step:1614/2315 train_time:97671ms step_avg:60.52ms
step:1615/2315 train_time:97733ms step_avg:60.52ms
step:1616/2315 train_time:97795ms step_avg:60.52ms
step:1617/2315 train_time:97856ms step_avg:60.52ms
step:1618/2315 train_time:97917ms step_avg:60.52ms
step:1619/2315 train_time:97979ms step_avg:60.52ms
step:1620/2315 train_time:98040ms step_avg:60.52ms
step:1621/2315 train_time:98101ms step_avg:60.52ms
step:1622/2315 train_time:98162ms step_avg:60.52ms
step:1623/2315 train_time:98223ms step_avg:60.52ms
step:1624/2315 train_time:98284ms step_avg:60.52ms
step:1625/2315 train_time:98345ms step_avg:60.52ms
step:1626/2315 train_time:98407ms step_avg:60.52ms
step:1627/2315 train_time:98468ms step_avg:60.52ms
step:1628/2315 train_time:98529ms step_avg:60.52ms
step:1629/2315 train_time:98591ms step_avg:60.52ms
step:1630/2315 train_time:98652ms step_avg:60.52ms
step:1631/2315 train_time:98713ms step_avg:60.52ms
step:1632/2315 train_time:98774ms step_avg:60.52ms
step:1633/2315 train_time:98835ms step_avg:60.52ms
step:1634/2315 train_time:98897ms step_avg:60.52ms
step:1635/2315 train_time:98958ms step_avg:60.52ms
step:1636/2315 train_time:99019ms step_avg:60.53ms
step:1637/2315 train_time:99080ms step_avg:60.53ms
step:1638/2315 train_time:99141ms step_avg:60.53ms
step:1639/2315 train_time:99202ms step_avg:60.53ms
step:1640/2315 train_time:99263ms step_avg:60.53ms
step:1641/2315 train_time:99324ms step_avg:60.53ms
step:1642/2315 train_time:99385ms step_avg:60.53ms
step:1643/2315 train_time:99447ms step_avg:60.53ms
step:1644/2315 train_time:99508ms step_avg:60.53ms
step:1645/2315 train_time:99569ms step_avg:60.53ms
step:1646/2315 train_time:99631ms step_avg:60.53ms
step:1647/2315 train_time:99693ms step_avg:60.53ms
step:1648/2315 train_time:99754ms step_avg:60.53ms
step:1649/2315 train_time:99815ms step_avg:60.53ms
step:1650/2315 train_time:99876ms step_avg:60.53ms
step:1651/2315 train_time:99937ms step_avg:60.53ms
step:1652/2315 train_time:99999ms step_avg:60.53ms
step:1653/2315 train_time:100060ms step_avg:60.53ms
step:1654/2315 train_time:100121ms step_avg:60.53ms
step:1655/2315 train_time:100182ms step_avg:60.53ms
step:1656/2315 train_time:100243ms step_avg:60.53ms
step:1657/2315 train_time:100304ms step_avg:60.53ms
step:1658/2315 train_time:100365ms step_avg:60.53ms
step:1659/2315 train_time:100426ms step_avg:60.53ms
step:1660/2315 train_time:100488ms step_avg:60.54ms
step:1661/2315 train_time:100550ms step_avg:60.54ms
step:1662/2315 train_time:100611ms step_avg:60.54ms
step:1663/2315 train_time:100672ms step_avg:60.54ms
step:1664/2315 train_time:100733ms step_avg:60.54ms
step:1665/2315 train_time:100795ms step_avg:60.54ms
step:1666/2315 train_time:100856ms step_avg:60.54ms
step:1667/2315 train_time:100917ms step_avg:60.54ms
step:1668/2315 train_time:100978ms step_avg:60.54ms
step:1669/2315 train_time:101039ms step_avg:60.54ms
step:1670/2315 train_time:101101ms step_avg:60.54ms
step:1671/2315 train_time:101162ms step_avg:60.54ms
step:1672/2315 train_time:101222ms step_avg:60.54ms
step:1673/2315 train_time:101284ms step_avg:60.54ms
step:1674/2315 train_time:101344ms step_avg:60.54ms
step:1675/2315 train_time:101406ms step_avg:60.54ms
step:1676/2315 train_time:101468ms step_avg:60.54ms
step:1677/2315 train_time:101529ms step_avg:60.54ms
step:1678/2315 train_time:101591ms step_avg:60.54ms
step:1679/2315 train_time:101653ms step_avg:60.54ms
step:1680/2315 train_time:101714ms step_avg:60.54ms
step:1681/2315 train_time:101775ms step_avg:60.54ms
step:1682/2315 train_time:101836ms step_avg:60.54ms
step:1683/2315 train_time:101897ms step_avg:60.55ms
step:1684/2315 train_time:101959ms step_avg:60.55ms
step:1685/2315 train_time:102020ms step_avg:60.55ms
step:1686/2315 train_time:102081ms step_avg:60.55ms
step:1687/2315 train_time:102142ms step_avg:60.55ms
step:1688/2315 train_time:102204ms step_avg:60.55ms
step:1689/2315 train_time:102264ms step_avg:60.55ms
step:1690/2315 train_time:102325ms step_avg:60.55ms
step:1691/2315 train_time:102386ms step_avg:60.55ms
step:1692/2315 train_time:102448ms step_avg:60.55ms
step:1693/2315 train_time:102510ms step_avg:60.55ms
step:1694/2315 train_time:102571ms step_avg:60.55ms
step:1695/2315 train_time:102632ms step_avg:60.55ms
step:1696/2315 train_time:102693ms step_avg:60.55ms
step:1697/2315 train_time:102755ms step_avg:60.55ms
step:1698/2315 train_time:102816ms step_avg:60.55ms
step:1699/2315 train_time:102877ms step_avg:60.55ms
step:1700/2315 train_time:102938ms step_avg:60.55ms
step:1701/2315 train_time:102999ms step_avg:60.55ms
step:1702/2315 train_time:103060ms step_avg:60.55ms
step:1703/2315 train_time:103120ms step_avg:60.55ms
step:1704/2315 train_time:103181ms step_avg:60.55ms
step:1705/2315 train_time:103242ms step_avg:60.55ms
step:1706/2315 train_time:103304ms step_avg:60.55ms
step:1707/2315 train_time:103364ms step_avg:60.55ms
step:1708/2315 train_time:103426ms step_avg:60.55ms
step:1709/2315 train_time:103488ms step_avg:60.55ms
step:1710/2315 train_time:103548ms step_avg:60.55ms
step:1711/2315 train_time:103610ms step_avg:60.56ms
step:1712/2315 train_time:103672ms step_avg:60.56ms
step:1713/2315 train_time:103734ms step_avg:60.56ms
step:1714/2315 train_time:103795ms step_avg:60.56ms
step:1715/2315 train_time:103856ms step_avg:60.56ms
step:1716/2315 train_time:103917ms step_avg:60.56ms
step:1717/2315 train_time:103979ms step_avg:60.56ms
step:1718/2315 train_time:104040ms step_avg:60.56ms
step:1719/2315 train_time:104101ms step_avg:60.56ms
step:1720/2315 train_time:104161ms step_avg:60.56ms
step:1721/2315 train_time:104223ms step_avg:60.56ms
step:1722/2315 train_time:104283ms step_avg:60.56ms
step:1723/2315 train_time:104344ms step_avg:60.56ms
step:1724/2315 train_time:104406ms step_avg:60.56ms
step:1725/2315 train_time:104467ms step_avg:60.56ms
step:1726/2315 train_time:104528ms step_avg:60.56ms
step:1727/2315 train_time:104590ms step_avg:60.56ms
step:1728/2315 train_time:104652ms step_avg:60.56ms
step:1729/2315 train_time:104714ms step_avg:60.56ms
step:1730/2315 train_time:104775ms step_avg:60.56ms
step:1731/2315 train_time:104836ms step_avg:60.56ms
step:1732/2315 train_time:104897ms step_avg:60.56ms
step:1733/2315 train_time:104958ms step_avg:60.56ms
step:1734/2315 train_time:105019ms step_avg:60.56ms
step:1735/2315 train_time:105080ms step_avg:60.56ms
step:1736/2315 train_time:105141ms step_avg:60.56ms
step:1737/2315 train_time:105202ms step_avg:60.57ms
step:1738/2315 train_time:105262ms step_avg:60.57ms
step:1739/2315 train_time:105324ms step_avg:60.57ms
step:1740/2315 train_time:105385ms step_avg:60.57ms
step:1741/2315 train_time:105447ms step_avg:60.57ms
step:1742/2315 train_time:105509ms step_avg:60.57ms
step:1743/2315 train_time:105570ms step_avg:60.57ms
step:1744/2315 train_time:105631ms step_avg:60.57ms
step:1745/2315 train_time:105693ms step_avg:60.57ms
step:1746/2315 train_time:105755ms step_avg:60.57ms
step:1747/2315 train_time:105816ms step_avg:60.57ms
step:1748/2315 train_time:105877ms step_avg:60.57ms
step:1749/2315 train_time:105938ms step_avg:60.57ms
step:1750/2315 train_time:105999ms step_avg:60.57ms
step:1750/2315 val_loss:3.3780 train_time:106062ms step_avg:60.61ms
step:1751/2315 train_time:106085ms step_avg:60.59ms
step:1752/2315 train_time:106124ms step_avg:60.57ms
step:1753/2315 train_time:106192ms step_avg:60.58ms
step:1754/2315 train_time:106256ms step_avg:60.58ms
step:1755/2315 train_time:106317ms step_avg:60.58ms
step:1756/2315 train_time:106377ms step_avg:60.58ms
step:1757/2315 train_time:106437ms step_avg:60.58ms
step:1758/2315 train_time:106498ms step_avg:60.58ms
step:1759/2315 train_time:106558ms step_avg:60.58ms
step:1760/2315 train_time:106619ms step_avg:60.58ms
step:1761/2315 train_time:106679ms step_avg:60.58ms
step:1762/2315 train_time:106739ms step_avg:60.58ms
step:1763/2315 train_time:106800ms step_avg:60.58ms
step:1764/2315 train_time:106861ms step_avg:60.58ms
step:1765/2315 train_time:106921ms step_avg:60.58ms
step:1766/2315 train_time:106984ms step_avg:60.58ms
step:1767/2315 train_time:107046ms step_avg:60.58ms
step:1768/2315 train_time:107107ms step_avg:60.58ms
step:1769/2315 train_time:107170ms step_avg:60.58ms
step:1770/2315 train_time:107232ms step_avg:60.58ms
step:1771/2315 train_time:107293ms step_avg:60.58ms
step:1772/2315 train_time:107355ms step_avg:60.58ms
step:1773/2315 train_time:107416ms step_avg:60.58ms
step:1774/2315 train_time:107476ms step_avg:60.58ms
step:1775/2315 train_time:107537ms step_avg:60.58ms
step:1776/2315 train_time:107599ms step_avg:60.58ms
step:1777/2315 train_time:107659ms step_avg:60.58ms
step:1778/2315 train_time:107719ms step_avg:60.58ms
step:1779/2315 train_time:107780ms step_avg:60.58ms
step:1780/2315 train_time:107841ms step_avg:60.58ms
step:1781/2315 train_time:107902ms step_avg:60.58ms
step:1782/2315 train_time:107963ms step_avg:60.59ms
step:1783/2315 train_time:108024ms step_avg:60.59ms
step:1784/2315 train_time:108086ms step_avg:60.59ms
step:1785/2315 train_time:108147ms step_avg:60.59ms
step:1786/2315 train_time:108208ms step_avg:60.59ms
step:1787/2315 train_time:108269ms step_avg:60.59ms
step:1788/2315 train_time:108331ms step_avg:60.59ms
step:1789/2315 train_time:108393ms step_avg:60.59ms
step:1790/2315 train_time:108453ms step_avg:60.59ms
step:1791/2315 train_time:108515ms step_avg:60.59ms
step:1792/2315 train_time:108576ms step_avg:60.59ms
step:1793/2315 train_time:108637ms step_avg:60.59ms
step:1794/2315 train_time:108698ms step_avg:60.59ms
step:1795/2315 train_time:108758ms step_avg:60.59ms
step:1796/2315 train_time:108819ms step_avg:60.59ms
step:1797/2315 train_time:108881ms step_avg:60.59ms
step:1798/2315 train_time:108942ms step_avg:60.59ms
step:1799/2315 train_time:109004ms step_avg:60.59ms
step:1800/2315 train_time:109065ms step_avg:60.59ms
step:1801/2315 train_time:109126ms step_avg:60.59ms
step:1802/2315 train_time:109188ms step_avg:60.59ms
step:1803/2315 train_time:109249ms step_avg:60.59ms
step:1804/2315 train_time:109310ms step_avg:60.59ms
step:1805/2315 train_time:109371ms step_avg:60.59ms
step:1806/2315 train_time:109432ms step_avg:60.59ms
step:1807/2315 train_time:109494ms step_avg:60.59ms
step:1808/2315 train_time:109555ms step_avg:60.59ms
step:1809/2315 train_time:109617ms step_avg:60.60ms
step:1810/2315 train_time:109678ms step_avg:60.60ms
step:1811/2315 train_time:109739ms step_avg:60.60ms
step:1812/2315 train_time:109799ms step_avg:60.60ms
step:1813/2315 train_time:109861ms step_avg:60.60ms
step:1814/2315 train_time:109922ms step_avg:60.60ms
step:1815/2315 train_time:109983ms step_avg:60.60ms
step:1816/2315 train_time:110043ms step_avg:60.60ms
step:1817/2315 train_time:110104ms step_avg:60.60ms
step:1818/2315 train_time:110165ms step_avg:60.60ms
step:1819/2315 train_time:110226ms step_avg:60.60ms
step:1820/2315 train_time:110288ms step_avg:60.60ms
step:1821/2315 train_time:110349ms step_avg:60.60ms
step:1822/2315 train_time:110410ms step_avg:60.60ms
step:1823/2315 train_time:110472ms step_avg:60.60ms
step:1824/2315 train_time:110533ms step_avg:60.60ms
step:1825/2315 train_time:110595ms step_avg:60.60ms
step:1826/2315 train_time:110656ms step_avg:60.60ms
step:1827/2315 train_time:110717ms step_avg:60.60ms
step:1828/2315 train_time:110779ms step_avg:60.60ms
step:1829/2315 train_time:110839ms step_avg:60.60ms
step:1830/2315 train_time:110900ms step_avg:60.60ms
step:1831/2315 train_time:110961ms step_avg:60.60ms
step:1832/2315 train_time:111023ms step_avg:60.60ms
step:1833/2315 train_time:111083ms step_avg:60.60ms
step:1834/2315 train_time:111144ms step_avg:60.60ms
step:1835/2315 train_time:111205ms step_avg:60.60ms
step:1836/2315 train_time:111266ms step_avg:60.60ms
step:1837/2315 train_time:111327ms step_avg:60.60ms
step:1838/2315 train_time:111388ms step_avg:60.60ms
step:1839/2315 train_time:111449ms step_avg:60.60ms
step:1840/2315 train_time:111511ms step_avg:60.60ms
step:1841/2315 train_time:111572ms step_avg:60.60ms
step:1842/2315 train_time:111633ms step_avg:60.60ms
step:1843/2315 train_time:111694ms step_avg:60.60ms
step:1844/2315 train_time:111756ms step_avg:60.60ms
step:1845/2315 train_time:111817ms step_avg:60.61ms
step:1846/2315 train_time:111878ms step_avg:60.61ms
step:1847/2315 train_time:111940ms step_avg:60.61ms
step:1848/2315 train_time:112001ms step_avg:60.61ms
step:1849/2315 train_time:112063ms step_avg:60.61ms
step:1850/2315 train_time:112124ms step_avg:60.61ms
step:1851/2315 train_time:112185ms step_avg:60.61ms
step:1852/2315 train_time:112246ms step_avg:60.61ms
step:1853/2315 train_time:112306ms step_avg:60.61ms
step:1854/2315 train_time:112367ms step_avg:60.61ms
step:1855/2315 train_time:112428ms step_avg:60.61ms
step:1856/2315 train_time:112489ms step_avg:60.61ms
step:1857/2315 train_time:112551ms step_avg:60.61ms
step:1858/2315 train_time:112613ms step_avg:60.61ms
step:1859/2315 train_time:112674ms step_avg:60.61ms
step:1860/2315 train_time:112735ms step_avg:60.61ms
step:1861/2315 train_time:112797ms step_avg:60.61ms
step:1862/2315 train_time:112858ms step_avg:60.61ms
step:1863/2315 train_time:112920ms step_avg:60.61ms
step:1864/2315 train_time:112981ms step_avg:60.61ms
step:1865/2315 train_time:113042ms step_avg:60.61ms
step:1866/2315 train_time:113103ms step_avg:60.61ms
step:1867/2315 train_time:113164ms step_avg:60.61ms
step:1868/2315 train_time:113225ms step_avg:60.61ms
step:1869/2315 train_time:113286ms step_avg:60.61ms
step:1870/2315 train_time:113346ms step_avg:60.61ms
step:1871/2315 train_time:113407ms step_avg:60.61ms
step:1872/2315 train_time:113468ms step_avg:60.61ms
step:1873/2315 train_time:113529ms step_avg:60.61ms
step:1874/2315 train_time:113590ms step_avg:60.61ms
step:1875/2315 train_time:113652ms step_avg:60.61ms
step:1876/2315 train_time:113713ms step_avg:60.61ms
step:1877/2315 train_time:113775ms step_avg:60.62ms
step:1878/2315 train_time:113837ms step_avg:60.62ms
step:1879/2315 train_time:113899ms step_avg:60.62ms
step:1880/2315 train_time:113959ms step_avg:60.62ms
step:1881/2315 train_time:114020ms step_avg:60.62ms
step:1882/2315 train_time:114081ms step_avg:60.62ms
step:1883/2315 train_time:114142ms step_avg:60.62ms
step:1884/2315 train_time:114204ms step_avg:60.62ms
step:1885/2315 train_time:114264ms step_avg:60.62ms
step:1886/2315 train_time:114325ms step_avg:60.62ms
step:1887/2315 train_time:114386ms step_avg:60.62ms
step:1888/2315 train_time:114447ms step_avg:60.62ms
step:1889/2315 train_time:114508ms step_avg:60.62ms
step:1890/2315 train_time:114568ms step_avg:60.62ms
step:1891/2315 train_time:114630ms step_avg:60.62ms
step:1892/2315 train_time:114691ms step_avg:60.62ms
step:1893/2315 train_time:114753ms step_avg:60.62ms
step:1894/2315 train_time:114815ms step_avg:60.62ms
step:1895/2315 train_time:114877ms step_avg:60.62ms
step:1896/2315 train_time:114939ms step_avg:60.62ms
step:1897/2315 train_time:115000ms step_avg:60.62ms
step:1898/2315 train_time:115062ms step_avg:60.62ms
step:1899/2315 train_time:115123ms step_avg:60.62ms
step:1900/2315 train_time:115184ms step_avg:60.62ms
step:1901/2315 train_time:115245ms step_avg:60.62ms
step:1902/2315 train_time:115306ms step_avg:60.62ms
step:1903/2315 train_time:115367ms step_avg:60.62ms
step:1904/2315 train_time:115428ms step_avg:60.62ms
step:1905/2315 train_time:115488ms step_avg:60.62ms
step:1906/2315 train_time:115548ms step_avg:60.62ms
step:1907/2315 train_time:115609ms step_avg:60.62ms
step:1908/2315 train_time:115670ms step_avg:60.62ms
step:1909/2315 train_time:115731ms step_avg:60.62ms
step:1910/2315 train_time:115792ms step_avg:60.62ms
step:1911/2315 train_time:115854ms step_avg:60.62ms
step:1912/2315 train_time:115915ms step_avg:60.63ms
step:1913/2315 train_time:115977ms step_avg:60.63ms
step:1914/2315 train_time:116039ms step_avg:60.63ms
step:1915/2315 train_time:116100ms step_avg:60.63ms
step:1916/2315 train_time:116161ms step_avg:60.63ms
step:1917/2315 train_time:116222ms step_avg:60.63ms
step:1918/2315 train_time:116283ms step_avg:60.63ms
step:1919/2315 train_time:116344ms step_avg:60.63ms
step:1920/2315 train_time:116405ms step_avg:60.63ms
step:1921/2315 train_time:116466ms step_avg:60.63ms
step:1922/2315 train_time:116527ms step_avg:60.63ms
step:1923/2315 train_time:116588ms step_avg:60.63ms
step:1924/2315 train_time:116649ms step_avg:60.63ms
step:1925/2315 train_time:116710ms step_avg:60.63ms
step:1926/2315 train_time:116772ms step_avg:60.63ms
step:1927/2315 train_time:116834ms step_avg:60.63ms
step:1928/2315 train_time:116895ms step_avg:60.63ms
step:1929/2315 train_time:116957ms step_avg:60.63ms
step:1930/2315 train_time:117018ms step_avg:60.63ms
step:1931/2315 train_time:117079ms step_avg:60.63ms
step:1932/2315 train_time:117141ms step_avg:60.63ms
step:1933/2315 train_time:117202ms step_avg:60.63ms
step:1934/2315 train_time:117263ms step_avg:60.63ms
step:1935/2315 train_time:117324ms step_avg:60.63ms
step:1936/2315 train_time:117385ms step_avg:60.63ms
step:1937/2315 train_time:117446ms step_avg:60.63ms
step:1938/2315 train_time:117507ms step_avg:60.63ms
step:1939/2315 train_time:117568ms step_avg:60.63ms
step:1940/2315 train_time:117629ms step_avg:60.63ms
step:1941/2315 train_time:117690ms step_avg:60.63ms
step:1942/2315 train_time:117751ms step_avg:60.63ms
step:1943/2315 train_time:117813ms step_avg:60.63ms
step:1944/2315 train_time:117874ms step_avg:60.63ms
step:1945/2315 train_time:117936ms step_avg:60.64ms
step:1946/2315 train_time:117997ms step_avg:60.64ms
step:1947/2315 train_time:118058ms step_avg:60.64ms
step:1948/2315 train_time:118120ms step_avg:60.64ms
step:1949/2315 train_time:118181ms step_avg:60.64ms
step:1950/2315 train_time:118243ms step_avg:60.64ms
step:1951/2315 train_time:118303ms step_avg:60.64ms
step:1952/2315 train_time:118364ms step_avg:60.64ms
step:1953/2315 train_time:118425ms step_avg:60.64ms
step:1954/2315 train_time:118486ms step_avg:60.64ms
step:1955/2315 train_time:118546ms step_avg:60.64ms
step:1956/2315 train_time:118608ms step_avg:60.64ms
step:1957/2315 train_time:118669ms step_avg:60.64ms
step:1958/2315 train_time:118730ms step_avg:60.64ms
step:1959/2315 train_time:118791ms step_avg:60.64ms
step:1960/2315 train_time:118853ms step_avg:60.64ms
step:1961/2315 train_time:118914ms step_avg:60.64ms
step:1962/2315 train_time:118975ms step_avg:60.64ms
step:1963/2315 train_time:119037ms step_avg:60.64ms
step:1964/2315 train_time:119099ms step_avg:60.64ms
step:1965/2315 train_time:119160ms step_avg:60.64ms
step:1966/2315 train_time:119221ms step_avg:60.64ms
step:1967/2315 train_time:119282ms step_avg:60.64ms
step:1968/2315 train_time:119343ms step_avg:60.64ms
step:1969/2315 train_time:119404ms step_avg:60.64ms
step:1970/2315 train_time:119465ms step_avg:60.64ms
step:1971/2315 train_time:119526ms step_avg:60.64ms
step:1972/2315 train_time:119586ms step_avg:60.64ms
step:1973/2315 train_time:119647ms step_avg:60.64ms
step:1974/2315 train_time:119708ms step_avg:60.64ms
step:1975/2315 train_time:119769ms step_avg:60.64ms
step:1976/2315 train_time:119830ms step_avg:60.64ms
step:1977/2315 train_time:119892ms step_avg:60.64ms
step:1978/2315 train_time:119954ms step_avg:60.64ms
step:1979/2315 train_time:120016ms step_avg:60.64ms
step:1980/2315 train_time:120077ms step_avg:60.64ms
step:1981/2315 train_time:120139ms step_avg:60.65ms
step:1982/2315 train_time:120201ms step_avg:60.65ms
step:1983/2315 train_time:120261ms step_avg:60.65ms
step:1984/2315 train_time:120323ms step_avg:60.65ms
step:1985/2315 train_time:120384ms step_avg:60.65ms
step:1986/2315 train_time:120445ms step_avg:60.65ms
step:1987/2315 train_time:120506ms step_avg:60.65ms
step:1988/2315 train_time:120567ms step_avg:60.65ms
step:1989/2315 train_time:120627ms step_avg:60.65ms
step:1990/2315 train_time:120688ms step_avg:60.65ms
step:1991/2315 train_time:120749ms step_avg:60.65ms
step:1992/2315 train_time:120811ms step_avg:60.65ms
step:1993/2315 train_time:120872ms step_avg:60.65ms
step:1994/2315 train_time:120933ms step_avg:60.65ms
step:1995/2315 train_time:120995ms step_avg:60.65ms
step:1996/2315 train_time:121057ms step_avg:60.65ms
step:1997/2315 train_time:121118ms step_avg:60.65ms
step:1998/2315 train_time:121179ms step_avg:60.65ms
step:1999/2315 train_time:121240ms step_avg:60.65ms
step:2000/2315 train_time:121301ms step_avg:60.65ms
step:2000/2315 val_loss:3.3288 train_time:121364ms step_avg:60.68ms
step:2001/2315 train_time:121386ms step_avg:60.66ms
step:2002/2315 train_time:121431ms step_avg:60.66ms
step:2003/2315 train_time:121497ms step_avg:60.66ms
step:2004/2315 train_time:121562ms step_avg:60.66ms
step:2005/2315 train_time:121622ms step_avg:60.66ms
step:2006/2315 train_time:121685ms step_avg:60.66ms
step:2007/2315 train_time:121746ms step_avg:60.66ms
step:2008/2315 train_time:121806ms step_avg:60.66ms
step:2009/2315 train_time:121866ms step_avg:60.66ms
step:2010/2315 train_time:121927ms step_avg:60.66ms
step:2011/2315 train_time:121987ms step_avg:60.66ms
step:2012/2315 train_time:122048ms step_avg:60.66ms
step:2013/2315 train_time:122109ms step_avg:60.66ms
step:2014/2315 train_time:122170ms step_avg:60.66ms
step:2015/2315 train_time:122230ms step_avg:60.66ms
step:2016/2315 train_time:122291ms step_avg:60.66ms
step:2017/2315 train_time:122353ms step_avg:60.66ms
step:2018/2315 train_time:122414ms step_avg:60.66ms
step:2019/2315 train_time:122477ms step_avg:60.66ms
step:2020/2315 train_time:122539ms step_avg:60.66ms
step:2021/2315 train_time:122601ms step_avg:60.66ms
step:2022/2315 train_time:122663ms step_avg:60.66ms
step:2023/2315 train_time:122724ms step_avg:60.66ms
step:2024/2315 train_time:122786ms step_avg:60.66ms
step:2025/2315 train_time:122846ms step_avg:60.66ms
step:2026/2315 train_time:122907ms step_avg:60.66ms
step:2027/2315 train_time:122967ms step_avg:60.66ms
step:2028/2315 train_time:123028ms step_avg:60.66ms
step:2029/2315 train_time:123089ms step_avg:60.66ms
step:2030/2315 train_time:123149ms step_avg:60.66ms
step:2031/2315 train_time:123210ms step_avg:60.66ms
step:2032/2315 train_time:123270ms step_avg:60.66ms
step:2033/2315 train_time:123331ms step_avg:60.66ms
step:2034/2315 train_time:123393ms step_avg:60.67ms
step:2035/2315 train_time:123454ms step_avg:60.67ms
step:2036/2315 train_time:123515ms step_avg:60.67ms
step:2037/2315 train_time:123577ms step_avg:60.67ms
step:2038/2315 train_time:123640ms step_avg:60.67ms
step:2039/2315 train_time:123702ms step_avg:60.67ms
step:2040/2315 train_time:123763ms step_avg:60.67ms
step:2041/2315 train_time:123824ms step_avg:60.67ms
step:2042/2315 train_time:123886ms step_avg:60.67ms
step:2043/2315 train_time:123946ms step_avg:60.67ms
step:2044/2315 train_time:124008ms step_avg:60.67ms
step:2045/2315 train_time:124068ms step_avg:60.67ms
step:2046/2315 train_time:124129ms step_avg:60.67ms
step:2047/2315 train_time:124189ms step_avg:60.67ms
step:2048/2315 train_time:124250ms step_avg:60.67ms
step:2049/2315 train_time:124311ms step_avg:60.67ms
step:2050/2315 train_time:124373ms step_avg:60.67ms
step:2051/2315 train_time:124434ms step_avg:60.67ms
step:2052/2315 train_time:124496ms step_avg:60.67ms
step:2053/2315 train_time:124558ms step_avg:60.67ms
step:2054/2315 train_time:124620ms step_avg:60.67ms
step:2055/2315 train_time:124682ms step_avg:60.67ms
step:2056/2315 train_time:124744ms step_avg:60.67ms
step:2057/2315 train_time:124805ms step_avg:60.67ms
step:2058/2315 train_time:124866ms step_avg:60.67ms
step:2059/2315 train_time:124927ms step_avg:60.67ms
step:2060/2315 train_time:124988ms step_avg:60.67ms
step:2061/2315 train_time:125049ms step_avg:60.67ms
step:2062/2315 train_time:125110ms step_avg:60.67ms
step:2063/2315 train_time:125170ms step_avg:60.67ms
step:2064/2315 train_time:125231ms step_avg:60.67ms
step:2065/2315 train_time:125292ms step_avg:60.67ms
step:2066/2315 train_time:125353ms step_avg:60.67ms
step:2067/2315 train_time:125414ms step_avg:60.67ms
step:2068/2315 train_time:125476ms step_avg:60.67ms
step:2069/2315 train_time:125539ms step_avg:60.68ms
step:2070/2315 train_time:125600ms step_avg:60.68ms
step:2071/2315 train_time:125662ms step_avg:60.68ms
step:2072/2315 train_time:125724ms step_avg:60.68ms
step:2073/2315 train_time:125785ms step_avg:60.68ms
step:2074/2315 train_time:125847ms step_avg:60.68ms
step:2075/2315 train_time:125908ms step_avg:60.68ms
step:2076/2315 train_time:125968ms step_avg:60.68ms
step:2077/2315 train_time:126029ms step_avg:60.68ms
step:2078/2315 train_time:126090ms step_avg:60.68ms
step:2079/2315 train_time:126151ms step_avg:60.68ms
step:2080/2315 train_time:126211ms step_avg:60.68ms
step:2081/2315 train_time:126272ms step_avg:60.68ms
step:2082/2315 train_time:126334ms step_avg:60.68ms
step:2083/2315 train_time:126395ms step_avg:60.68ms
step:2084/2315 train_time:126456ms step_avg:60.68ms
step:2085/2315 train_time:126518ms step_avg:60.68ms
step:2086/2315 train_time:126580ms step_avg:60.68ms
step:2087/2315 train_time:126641ms step_avg:60.68ms
step:2088/2315 train_time:126703ms step_avg:60.68ms
step:2089/2315 train_time:126765ms step_avg:60.68ms
step:2090/2315 train_time:126826ms step_avg:60.68ms
step:2091/2315 train_time:126888ms step_avg:60.68ms
step:2092/2315 train_time:126948ms step_avg:60.68ms
step:2093/2315 train_time:127009ms step_avg:60.68ms
step:2094/2315 train_time:127071ms step_avg:60.68ms
step:2095/2315 train_time:127132ms step_avg:60.68ms
step:2096/2315 train_time:127193ms step_avg:60.68ms
step:2097/2315 train_time:127254ms step_avg:60.68ms
step:2098/2315 train_time:127315ms step_avg:60.68ms
step:2099/2315 train_time:127375ms step_avg:60.68ms
step:2100/2315 train_time:127437ms step_avg:60.68ms
step:2101/2315 train_time:127499ms step_avg:60.68ms
step:2102/2315 train_time:127560ms step_avg:60.68ms
step:2103/2315 train_time:127622ms step_avg:60.69ms
step:2104/2315 train_time:127683ms step_avg:60.69ms
step:2105/2315 train_time:127745ms step_avg:60.69ms
step:2106/2315 train_time:127806ms step_avg:60.69ms
step:2107/2315 train_time:127868ms step_avg:60.69ms
step:2108/2315 train_time:127929ms step_avg:60.69ms
step:2109/2315 train_time:127990ms step_avg:60.69ms
step:2110/2315 train_time:128051ms step_avg:60.69ms
step:2111/2315 train_time:128112ms step_avg:60.69ms
step:2112/2315 train_time:128173ms step_avg:60.69ms
step:2113/2315 train_time:128234ms step_avg:60.69ms
step:2114/2315 train_time:128295ms step_avg:60.69ms
step:2115/2315 train_time:128356ms step_avg:60.69ms
step:2116/2315 train_time:128417ms step_avg:60.69ms
step:2117/2315 train_time:128478ms step_avg:60.69ms
step:2118/2315 train_time:128539ms step_avg:60.69ms
step:2119/2315 train_time:128601ms step_avg:60.69ms
step:2120/2315 train_time:128662ms step_avg:60.69ms
step:2121/2315 train_time:128724ms step_avg:60.69ms
step:2122/2315 train_time:128785ms step_avg:60.69ms
step:2123/2315 train_time:128847ms step_avg:60.69ms
step:2124/2315 train_time:128908ms step_avg:60.69ms
step:2125/2315 train_time:128969ms step_avg:60.69ms
step:2126/2315 train_time:129030ms step_avg:60.69ms
step:2127/2315 train_time:129091ms step_avg:60.69ms
step:2128/2315 train_time:129152ms step_avg:60.69ms
step:2129/2315 train_time:129213ms step_avg:60.69ms
step:2130/2315 train_time:129275ms step_avg:60.69ms
step:2131/2315 train_time:129336ms step_avg:60.69ms
step:2132/2315 train_time:129397ms step_avg:60.69ms
step:2133/2315 train_time:129458ms step_avg:60.69ms
step:2134/2315 train_time:129520ms step_avg:60.69ms
step:2135/2315 train_time:129581ms step_avg:60.69ms
step:2136/2315 train_time:129642ms step_avg:60.69ms
step:2137/2315 train_time:129704ms step_avg:60.69ms
step:2138/2315 train_time:129766ms step_avg:60.69ms
step:2139/2315 train_time:129827ms step_avg:60.70ms
step:2140/2315 train_time:129888ms step_avg:60.70ms
step:2141/2315 train_time:129949ms step_avg:60.70ms
step:2142/2315 train_time:130010ms step_avg:60.70ms
step:2143/2315 train_time:130071ms step_avg:60.70ms
step:2144/2315 train_time:130132ms step_avg:60.70ms
step:2145/2315 train_time:130193ms step_avg:60.70ms
step:2146/2315 train_time:130254ms step_avg:60.70ms
step:2147/2315 train_time:130316ms step_avg:60.70ms
step:2148/2315 train_time:130378ms step_avg:60.70ms
step:2149/2315 train_time:130439ms step_avg:60.70ms
step:2150/2315 train_time:130500ms step_avg:60.70ms
step:2151/2315 train_time:130562ms step_avg:60.70ms
step:2152/2315 train_time:130623ms step_avg:60.70ms
step:2153/2315 train_time:130684ms step_avg:60.70ms
step:2154/2315 train_time:130745ms step_avg:60.70ms
step:2155/2315 train_time:130806ms step_avg:60.70ms
step:2156/2315 train_time:130867ms step_avg:60.70ms
step:2157/2315 train_time:130929ms step_avg:60.70ms
step:2158/2315 train_time:130990ms step_avg:60.70ms
step:2159/2315 train_time:131051ms step_avg:60.70ms
step:2160/2315 train_time:131112ms step_avg:60.70ms
step:2161/2315 train_time:131173ms step_avg:60.70ms
step:2162/2315 train_time:131234ms step_avg:60.70ms
step:2163/2315 train_time:131296ms step_avg:60.70ms
step:2164/2315 train_time:131357ms step_avg:60.70ms
step:2165/2315 train_time:131419ms step_avg:60.70ms
step:2166/2315 train_time:131480ms step_avg:60.70ms
step:2167/2315 train_time:131542ms step_avg:60.70ms
step:2168/2315 train_time:131603ms step_avg:60.70ms
step:2169/2315 train_time:131664ms step_avg:60.70ms
step:2170/2315 train_time:131725ms step_avg:60.70ms
step:2171/2315 train_time:131786ms step_avg:60.70ms
step:2172/2315 train_time:131848ms step_avg:60.70ms
step:2173/2315 train_time:131909ms step_avg:60.70ms
step:2174/2315 train_time:131969ms step_avg:60.70ms
step:2175/2315 train_time:132031ms step_avg:60.70ms
step:2176/2315 train_time:132092ms step_avg:60.70ms
step:2177/2315 train_time:132153ms step_avg:60.70ms
step:2178/2315 train_time:132215ms step_avg:60.70ms
step:2179/2315 train_time:132276ms step_avg:60.71ms
step:2180/2315 train_time:132339ms step_avg:60.71ms
step:2181/2315 train_time:132400ms step_avg:60.71ms
step:2182/2315 train_time:132461ms step_avg:60.71ms
step:2183/2315 train_time:132522ms step_avg:60.71ms
step:2184/2315 train_time:132583ms step_avg:60.71ms
step:2185/2315 train_time:132645ms step_avg:60.71ms
step:2186/2315 train_time:132705ms step_avg:60.71ms
step:2187/2315 train_time:132767ms step_avg:60.71ms
step:2188/2315 train_time:132828ms step_avg:60.71ms
step:2189/2315 train_time:132889ms step_avg:60.71ms
step:2190/2315 train_time:132950ms step_avg:60.71ms
step:2191/2315 train_time:133011ms step_avg:60.71ms
step:2192/2315 train_time:133072ms step_avg:60.71ms
step:2193/2315 train_time:133133ms step_avg:60.71ms
step:2194/2315 train_time:133193ms step_avg:60.71ms
step:2195/2315 train_time:133254ms step_avg:60.71ms
step:2196/2315 train_time:133316ms step_avg:60.71ms
step:2197/2315 train_time:133377ms step_avg:60.71ms
step:2198/2315 train_time:133439ms step_avg:60.71ms
step:2199/2315 train_time:133500ms step_avg:60.71ms
step:2200/2315 train_time:133562ms step_avg:60.71ms
step:2201/2315 train_time:133623ms step_avg:60.71ms
step:2202/2315 train_time:133685ms step_avg:60.71ms
step:2203/2315 train_time:133746ms step_avg:60.71ms
step:2204/2315 train_time:133807ms step_avg:60.71ms
step:2205/2315 train_time:133868ms step_avg:60.71ms
step:2206/2315 train_time:133930ms step_avg:60.71ms
step:2207/2315 train_time:133991ms step_avg:60.71ms
step:2208/2315 train_time:134052ms step_avg:60.71ms
step:2209/2315 train_time:134113ms step_avg:60.71ms
step:2210/2315 train_time:134175ms step_avg:60.71ms
step:2211/2315 train_time:134235ms step_avg:60.71ms
step:2212/2315 train_time:134296ms step_avg:60.71ms
step:2213/2315 train_time:134358ms step_avg:60.71ms
step:2214/2315 train_time:134419ms step_avg:60.71ms
step:2215/2315 train_time:134481ms step_avg:60.71ms
step:2216/2315 train_time:134543ms step_avg:60.71ms
step:2217/2315 train_time:134604ms step_avg:60.71ms
step:2218/2315 train_time:134665ms step_avg:60.71ms
step:2219/2315 train_time:134726ms step_avg:60.71ms
step:2220/2315 train_time:134788ms step_avg:60.72ms
step:2221/2315 train_time:134849ms step_avg:60.72ms
step:2222/2315 train_time:134910ms step_avg:60.72ms
step:2223/2315 train_time:134971ms step_avg:60.72ms
step:2224/2315 train_time:135032ms step_avg:60.72ms
step:2225/2315 train_time:135093ms step_avg:60.72ms
step:2226/2315 train_time:135154ms step_avg:60.72ms
step:2227/2315 train_time:135215ms step_avg:60.72ms
step:2228/2315 train_time:135277ms step_avg:60.72ms
step:2229/2315 train_time:135339ms step_avg:60.72ms
step:2230/2315 train_time:135400ms step_avg:60.72ms
step:2231/2315 train_time:135461ms step_avg:60.72ms
step:2232/2315 train_time:135522ms step_avg:60.72ms
step:2233/2315 train_time:135584ms step_avg:60.72ms
step:2234/2315 train_time:135645ms step_avg:60.72ms
step:2235/2315 train_time:135706ms step_avg:60.72ms
step:2236/2315 train_time:135767ms step_avg:60.72ms
step:2237/2315 train_time:135828ms step_avg:60.72ms
step:2238/2315 train_time:135889ms step_avg:60.72ms
step:2239/2315 train_time:135950ms step_avg:60.72ms
step:2240/2315 train_time:136011ms step_avg:60.72ms
step:2241/2315 train_time:136072ms step_avg:60.72ms
step:2242/2315 train_time:136133ms step_avg:60.72ms
step:2243/2315 train_time:136194ms step_avg:60.72ms
step:2244/2315 train_time:136256ms step_avg:60.72ms
step:2245/2315 train_time:136318ms step_avg:60.72ms
step:2246/2315 train_time:136379ms step_avg:60.72ms
step:2247/2315 train_time:136440ms step_avg:60.72ms
step:2248/2315 train_time:136501ms step_avg:60.72ms
step:2249/2315 train_time:136562ms step_avg:60.72ms
step:2250/2315 train_time:136623ms step_avg:60.72ms
step:2250/2315 val_loss:3.2890 train_time:136687ms step_avg:60.75ms
step:2251/2315 train_time:136707ms step_avg:60.73ms
step:2252/2315 train_time:136753ms step_avg:60.73ms
step:2253/2315 train_time:136817ms step_avg:60.73ms
step:2254/2315 train_time:136881ms step_avg:60.73ms
step:2255/2315 train_time:136942ms step_avg:60.73ms
step:2256/2315 train_time:137004ms step_avg:60.73ms
step:2257/2315 train_time:137065ms step_avg:60.73ms
step:2258/2315 train_time:137126ms step_avg:60.73ms
step:2259/2315 train_time:137188ms step_avg:60.73ms
step:2260/2315 train_time:137248ms step_avg:60.73ms
step:2261/2315 train_time:137310ms step_avg:60.73ms
step:2262/2315 train_time:137370ms step_avg:60.73ms
step:2263/2315 train_time:137431ms step_avg:60.73ms
step:2264/2315 train_time:137492ms step_avg:60.73ms
step:2265/2315 train_time:137552ms step_avg:60.73ms
step:2266/2315 train_time:137613ms step_avg:60.73ms
step:2267/2315 train_time:137674ms step_avg:60.73ms
step:2268/2315 train_time:137735ms step_avg:60.73ms
step:2269/2315 train_time:137797ms step_avg:60.73ms
step:2270/2315 train_time:137859ms step_avg:60.73ms
step:2271/2315 train_time:137920ms step_avg:60.73ms
step:2272/2315 train_time:137983ms step_avg:60.73ms
step:2273/2315 train_time:138044ms step_avg:60.73ms
step:2274/2315 train_time:138105ms step_avg:60.73ms
step:2275/2315 train_time:138166ms step_avg:60.73ms
step:2276/2315 train_time:138228ms step_avg:60.73ms
step:2277/2315 train_time:138289ms step_avg:60.73ms
step:2278/2315 train_time:138350ms step_avg:60.73ms
step:2279/2315 train_time:138410ms step_avg:60.73ms
step:2280/2315 train_time:138471ms step_avg:60.73ms
step:2281/2315 train_time:138532ms step_avg:60.73ms
step:2282/2315 train_time:138593ms step_avg:60.73ms
step:2283/2315 train_time:138654ms step_avg:60.73ms
step:2284/2315 train_time:138715ms step_avg:60.73ms
step:2285/2315 train_time:138776ms step_avg:60.73ms
step:2286/2315 train_time:138837ms step_avg:60.73ms
step:2287/2315 train_time:138899ms step_avg:60.73ms
step:2288/2315 train_time:138961ms step_avg:60.73ms
step:2289/2315 train_time:139022ms step_avg:60.73ms
step:2290/2315 train_time:139084ms step_avg:60.74ms
step:2291/2315 train_time:139145ms step_avg:60.74ms
step:2292/2315 train_time:139206ms step_avg:60.74ms
step:2293/2315 train_time:139267ms step_avg:60.74ms
step:2294/2315 train_time:139329ms step_avg:60.74ms
step:2295/2315 train_time:139390ms step_avg:60.74ms
step:2296/2315 train_time:139450ms step_avg:60.74ms
step:2297/2315 train_time:139512ms step_avg:60.74ms
step:2298/2315 train_time:139573ms step_avg:60.74ms
step:2299/2315 train_time:139634ms step_avg:60.74ms
step:2300/2315 train_time:139695ms step_avg:60.74ms
step:2301/2315 train_time:139756ms step_avg:60.74ms
step:2302/2315 train_time:139817ms step_avg:60.74ms
step:2303/2315 train_time:139878ms step_avg:60.74ms
step:2304/2315 train_time:139940ms step_avg:60.74ms
step:2305/2315 train_time:140002ms step_avg:60.74ms
step:2306/2315 train_time:140064ms step_avg:60.74ms
step:2307/2315 train_time:140125ms step_avg:60.74ms
step:2308/2315 train_time:140186ms step_avg:60.74ms
step:2309/2315 train_time:140248ms step_avg:60.74ms
step:2310/2315 train_time:140308ms step_avg:60.74ms
step:2311/2315 train_time:140370ms step_avg:60.74ms
step:2312/2315 train_time:140430ms step_avg:60.74ms
step:2313/2315 train_time:140492ms step_avg:60.74ms
step:2314/2315 train_time:140553ms step_avg:60.74ms
step:2315/2315 train_time:140614ms step_avg:60.74ms
step:2315/2315 val_loss:3.2759 train_time:140675ms step_avg:60.77ms
peak memory allocated: 29512 MiB reserved: 44076 MiB

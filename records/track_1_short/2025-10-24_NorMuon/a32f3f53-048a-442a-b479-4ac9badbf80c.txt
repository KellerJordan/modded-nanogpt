import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]
# polar_express_coeffs = [
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
# ]



@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    # state["second_momentum_buffer"] = torch.zeros_like(grad[...,0:1]) if param.size(-2) >= param.size(-1) else torch.zeros_like(grad[...,0:1,:])
                    # state["second_momentum_buffer"] = torch.zeros((*grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((*grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)
            
            ###################################
            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))
            ####################################


            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> (-1.5)  0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Oct 22 2025, 02:20:45) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251022+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 24 06:19:41 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000001:00:00.0 Off |                    0 |
| N/A   35C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000002:00:00.0 Off |                    0 |
| N/A   35C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000003:00:00.0 Off |                    0 |
| N/A   30C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000008:00:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000009:00:00.0 Off |                    0 |
| N/A   28C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   0000000A:00:00.0 Off |                    0 |
| N/A   35C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   0000000C:00:00.0 Off |                    0 |
| N/A   34C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2315 train_time:93ms step_avg:92.53ms
step:2/2315 train_time:187ms step_avg:93.66ms
step:3/2315 train_time:210ms step_avg:69.84ms
step:4/2315 train_time:245ms step_avg:61.27ms
step:5/2315 train_time:304ms step_avg:60.73ms
step:6/2315 train_time:364ms step_avg:60.63ms
step:7/2315 train_time:423ms step_avg:60.43ms
step:8/2315 train_time:483ms step_avg:60.37ms
step:9/2315 train_time:543ms step_avg:60.31ms
step:10/2315 train_time:602ms step_avg:60.24ms
step:11/2315 train_time:662ms step_avg:60.21ms
step:12/2315 train_time:722ms step_avg:60.19ms
step:13/2315 train_time:782ms step_avg:60.15ms
step:14/2315 train_time:841ms step_avg:60.10ms
step:15/2315 train_time:901ms step_avg:60.09ms
step:16/2315 train_time:962ms step_avg:60.12ms
step:17/2315 train_time:1026ms step_avg:60.36ms
step:18/2315 train_time:1089ms step_avg:60.52ms
step:19/2315 train_time:1153ms step_avg:60.66ms
step:20/2315 train_time:1214ms step_avg:60.68ms
step:21/2315 train_time:1275ms step_avg:60.72ms
step:22/2315 train_time:1335ms step_avg:60.68ms
step:23/2315 train_time:1395ms step_avg:60.66ms
step:24/2315 train_time:1455ms step_avg:60.62ms
step:25/2315 train_time:1515ms step_avg:60.59ms
step:26/2315 train_time:1574ms step_avg:60.55ms
step:27/2315 train_time:1634ms step_avg:60.51ms
step:28/2315 train_time:1693ms step_avg:60.48ms
step:29/2315 train_time:1753ms step_avg:60.45ms
step:30/2315 train_time:1813ms step_avg:60.43ms
step:31/2315 train_time:1873ms step_avg:60.41ms
step:32/2315 train_time:1932ms step_avg:60.39ms
step:33/2315 train_time:1993ms step_avg:60.39ms
step:34/2315 train_time:2054ms step_avg:60.41ms
step:35/2315 train_time:2116ms step_avg:60.45ms
step:36/2315 train_time:2178ms step_avg:60.49ms
step:37/2315 train_time:2238ms step_avg:60.50ms
step:38/2315 train_time:2298ms step_avg:60.49ms
step:39/2315 train_time:2359ms step_avg:60.48ms
step:40/2315 train_time:2419ms step_avg:60.48ms
step:41/2315 train_time:2480ms step_avg:60.48ms
step:42/2315 train_time:2539ms step_avg:60.46ms
step:43/2315 train_time:2600ms step_avg:60.46ms
step:44/2315 train_time:2660ms step_avg:60.45ms
step:45/2315 train_time:2720ms step_avg:60.44ms
step:46/2315 train_time:2780ms step_avg:60.43ms
step:47/2315 train_time:2840ms step_avg:60.43ms
step:48/2315 train_time:2901ms step_avg:60.44ms
step:49/2315 train_time:2962ms step_avg:60.44ms
step:50/2315 train_time:3023ms step_avg:60.45ms
step:51/2315 train_time:3083ms step_avg:60.46ms
step:52/2315 train_time:3144ms step_avg:60.46ms
step:53/2315 train_time:3205ms step_avg:60.48ms
step:54/2315 train_time:3266ms step_avg:60.48ms
step:55/2315 train_time:3328ms step_avg:60.50ms
step:56/2315 train_time:3388ms step_avg:60.50ms
step:57/2315 train_time:3449ms step_avg:60.50ms
step:58/2315 train_time:3509ms step_avg:60.50ms
step:59/2315 train_time:3569ms step_avg:60.50ms
step:60/2315 train_time:3630ms step_avg:60.49ms
step:61/2315 train_time:3690ms step_avg:60.50ms
step:62/2315 train_time:3751ms step_avg:60.49ms
step:63/2315 train_time:3811ms step_avg:60.49ms
step:64/2315 train_time:3871ms step_avg:60.48ms
step:65/2315 train_time:3931ms step_avg:60.48ms
step:66/2315 train_time:3991ms step_avg:60.47ms
step:67/2315 train_time:4053ms step_avg:60.49ms
step:68/2315 train_time:4112ms step_avg:60.48ms
step:69/2315 train_time:4172ms step_avg:60.47ms
step:70/2315 train_time:4232ms step_avg:60.46ms
step:71/2315 train_time:4292ms step_avg:60.45ms
step:72/2315 train_time:4351ms step_avg:60.44ms
step:73/2315 train_time:4412ms step_avg:60.44ms
step:74/2315 train_time:4472ms step_avg:60.43ms
step:75/2315 train_time:4532ms step_avg:60.43ms
step:76/2315 train_time:4592ms step_avg:60.42ms
step:77/2315 train_time:4652ms step_avg:60.42ms
step:78/2315 train_time:4712ms step_avg:60.41ms
step:79/2315 train_time:4772ms step_avg:60.41ms
step:80/2315 train_time:4833ms step_avg:60.41ms
step:81/2315 train_time:4893ms step_avg:60.40ms
step:82/2315 train_time:4952ms step_avg:60.39ms
step:83/2315 train_time:5013ms step_avg:60.40ms
step:84/2315 train_time:5073ms step_avg:60.39ms
step:85/2315 train_time:5134ms step_avg:60.40ms
step:86/2315 train_time:5193ms step_avg:60.39ms
step:87/2315 train_time:5253ms step_avg:60.38ms
step:88/2315 train_time:5313ms step_avg:60.37ms
step:89/2315 train_time:5373ms step_avg:60.37ms
step:90/2315 train_time:5432ms step_avg:60.36ms
step:91/2315 train_time:5493ms step_avg:60.36ms
step:92/2315 train_time:5552ms step_avg:60.35ms
step:93/2315 train_time:5612ms step_avg:60.35ms
step:94/2315 train_time:5672ms step_avg:60.35ms
step:95/2315 train_time:5733ms step_avg:60.34ms
step:96/2315 train_time:5793ms step_avg:60.34ms
step:97/2315 train_time:5853ms step_avg:60.34ms
step:98/2315 train_time:5912ms step_avg:60.33ms
step:99/2315 train_time:5973ms step_avg:60.33ms
step:100/2315 train_time:6032ms step_avg:60.32ms
step:101/2315 train_time:6092ms step_avg:60.32ms
step:102/2315 train_time:6152ms step_avg:60.31ms
step:103/2315 train_time:6212ms step_avg:60.31ms
step:104/2315 train_time:6271ms step_avg:60.30ms
step:105/2315 train_time:6331ms step_avg:60.30ms
step:106/2315 train_time:6391ms step_avg:60.29ms
step:107/2315 train_time:6452ms step_avg:60.30ms
step:108/2315 train_time:6512ms step_avg:60.30ms
step:109/2315 train_time:6572ms step_avg:60.29ms
step:110/2315 train_time:6632ms step_avg:60.29ms
step:111/2315 train_time:6691ms step_avg:60.28ms
step:112/2315 train_time:6751ms step_avg:60.28ms
step:113/2315 train_time:6811ms step_avg:60.27ms
step:114/2315 train_time:6870ms step_avg:60.27ms
step:115/2315 train_time:6932ms step_avg:60.27ms
step:116/2315 train_time:6991ms step_avg:60.27ms
step:117/2315 train_time:7052ms step_avg:60.27ms
step:118/2315 train_time:7112ms step_avg:60.27ms
step:119/2315 train_time:7172ms step_avg:60.27ms
step:120/2315 train_time:7232ms step_avg:60.26ms
step:121/2315 train_time:7291ms step_avg:60.26ms
step:122/2315 train_time:7352ms step_avg:60.26ms
step:123/2315 train_time:7412ms step_avg:60.26ms
step:124/2315 train_time:7471ms step_avg:60.25ms
step:125/2315 train_time:7531ms step_avg:60.25ms
step:126/2315 train_time:7592ms step_avg:60.25ms
step:127/2315 train_time:7652ms step_avg:60.25ms
step:128/2315 train_time:7711ms step_avg:60.24ms
step:129/2315 train_time:7771ms step_avg:60.24ms
step:130/2315 train_time:7831ms step_avg:60.24ms
step:131/2315 train_time:7891ms step_avg:60.24ms
step:132/2315 train_time:7951ms step_avg:60.24ms
step:133/2315 train_time:8011ms step_avg:60.24ms
step:134/2315 train_time:8071ms step_avg:60.23ms
step:135/2315 train_time:8132ms step_avg:60.23ms
step:136/2315 train_time:8192ms step_avg:60.23ms
step:137/2315 train_time:8253ms step_avg:60.24ms
step:138/2315 train_time:8312ms step_avg:60.23ms
step:139/2315 train_time:8372ms step_avg:60.23ms
step:140/2315 train_time:8431ms step_avg:60.22ms
step:141/2315 train_time:8491ms step_avg:60.22ms
step:142/2315 train_time:8551ms step_avg:60.22ms
step:143/2315 train_time:8612ms step_avg:60.22ms
step:144/2315 train_time:8672ms step_avg:60.22ms
step:145/2315 train_time:8732ms step_avg:60.22ms
step:146/2315 train_time:8792ms step_avg:60.22ms
step:147/2315 train_time:8851ms step_avg:60.21ms
step:148/2315 train_time:8911ms step_avg:60.21ms
step:149/2315 train_time:8971ms step_avg:60.21ms
step:150/2315 train_time:9031ms step_avg:60.20ms
step:151/2315 train_time:9090ms step_avg:60.20ms
step:152/2315 train_time:9151ms step_avg:60.20ms
step:153/2315 train_time:9212ms step_avg:60.21ms
step:154/2315 train_time:9272ms step_avg:60.21ms
step:155/2315 train_time:9332ms step_avg:60.21ms
step:156/2315 train_time:9392ms step_avg:60.20ms
step:157/2315 train_time:9452ms step_avg:60.20ms
step:158/2315 train_time:9511ms step_avg:60.20ms
step:159/2315 train_time:9571ms step_avg:60.20ms
step:160/2315 train_time:9631ms step_avg:60.19ms
step:161/2315 train_time:9691ms step_avg:60.19ms
step:162/2315 train_time:9751ms step_avg:60.19ms
step:163/2315 train_time:9811ms step_avg:60.19ms
step:164/2315 train_time:9871ms step_avg:60.19ms
step:165/2315 train_time:9931ms step_avg:60.19ms
step:166/2315 train_time:9991ms step_avg:60.18ms
step:167/2315 train_time:10051ms step_avg:60.18ms
step:168/2315 train_time:10111ms step_avg:60.18ms
step:169/2315 train_time:10171ms step_avg:60.18ms
step:170/2315 train_time:10231ms step_avg:60.18ms
step:171/2315 train_time:10291ms step_avg:60.18ms
step:172/2315 train_time:10351ms step_avg:60.18ms
step:173/2315 train_time:10411ms step_avg:60.18ms
step:174/2315 train_time:10471ms step_avg:60.18ms
step:175/2315 train_time:10531ms step_avg:60.18ms
step:176/2315 train_time:10591ms step_avg:60.17ms
step:177/2315 train_time:10651ms step_avg:60.18ms
step:178/2315 train_time:10711ms step_avg:60.18ms
step:179/2315 train_time:10770ms step_avg:60.17ms
step:180/2315 train_time:10830ms step_avg:60.17ms
step:181/2315 train_time:10890ms step_avg:60.17ms
step:182/2315 train_time:10950ms step_avg:60.16ms
step:183/2315 train_time:11010ms step_avg:60.16ms
step:184/2315 train_time:11070ms step_avg:60.16ms
step:185/2315 train_time:11130ms step_avg:60.16ms
step:186/2315 train_time:11189ms step_avg:60.16ms
step:187/2315 train_time:11250ms step_avg:60.16ms
step:188/2315 train_time:11310ms step_avg:60.16ms
step:189/2315 train_time:11370ms step_avg:60.16ms
step:190/2315 train_time:11430ms step_avg:60.16ms
step:191/2315 train_time:11490ms step_avg:60.16ms
step:192/2315 train_time:11549ms step_avg:60.15ms
step:193/2315 train_time:11610ms step_avg:60.15ms
step:194/2315 train_time:11669ms step_avg:60.15ms
step:195/2315 train_time:11730ms step_avg:60.15ms
step:196/2315 train_time:11789ms step_avg:60.15ms
step:197/2315 train_time:11849ms step_avg:60.15ms
step:198/2315 train_time:11909ms step_avg:60.15ms
step:199/2315 train_time:11969ms step_avg:60.15ms
step:200/2315 train_time:12028ms step_avg:60.14ms
step:201/2315 train_time:12089ms step_avg:60.14ms
step:202/2315 train_time:12149ms step_avg:60.14ms
step:203/2315 train_time:12209ms step_avg:60.14ms
step:204/2315 train_time:12269ms step_avg:60.14ms
step:205/2315 train_time:12330ms step_avg:60.15ms
step:206/2315 train_time:12390ms step_avg:60.15ms
step:207/2315 train_time:12450ms step_avg:60.15ms
step:208/2315 train_time:12511ms step_avg:60.15ms
step:209/2315 train_time:12571ms step_avg:60.15ms
step:210/2315 train_time:12630ms step_avg:60.15ms
step:211/2315 train_time:12690ms step_avg:60.14ms
step:212/2315 train_time:12750ms step_avg:60.14ms
step:213/2315 train_time:12810ms step_avg:60.14ms
step:214/2315 train_time:12871ms step_avg:60.14ms
step:215/2315 train_time:12931ms step_avg:60.14ms
step:216/2315 train_time:12990ms step_avg:60.14ms
step:217/2315 train_time:13050ms step_avg:60.14ms
step:218/2315 train_time:13110ms step_avg:60.14ms
step:219/2315 train_time:13170ms step_avg:60.14ms
step:220/2315 train_time:13230ms step_avg:60.14ms
step:221/2315 train_time:13291ms step_avg:60.14ms
step:222/2315 train_time:13352ms step_avg:60.14ms
step:223/2315 train_time:13411ms step_avg:60.14ms
step:224/2315 train_time:13471ms step_avg:60.14ms
step:225/2315 train_time:13532ms step_avg:60.14ms
step:226/2315 train_time:13592ms step_avg:60.14ms
step:227/2315 train_time:13652ms step_avg:60.14ms
step:228/2315 train_time:13711ms step_avg:60.14ms
step:229/2315 train_time:13771ms step_avg:60.14ms
step:230/2315 train_time:13831ms step_avg:60.13ms
step:231/2315 train_time:13891ms step_avg:60.13ms
step:232/2315 train_time:13950ms step_avg:60.13ms
step:233/2315 train_time:14010ms step_avg:60.13ms
step:234/2315 train_time:14070ms step_avg:60.13ms
step:235/2315 train_time:14130ms step_avg:60.13ms
step:236/2315 train_time:14190ms step_avg:60.13ms
step:237/2315 train_time:14251ms step_avg:60.13ms
step:238/2315 train_time:14311ms step_avg:60.13ms
step:239/2315 train_time:14371ms step_avg:60.13ms
step:240/2315 train_time:14431ms step_avg:60.13ms
step:241/2315 train_time:14491ms step_avg:60.13ms
step:242/2315 train_time:14551ms step_avg:60.13ms
step:243/2315 train_time:14611ms step_avg:60.13ms
step:244/2315 train_time:14672ms step_avg:60.13ms
step:245/2315 train_time:14732ms step_avg:60.13ms
step:246/2315 train_time:14792ms step_avg:60.13ms
step:247/2315 train_time:14852ms step_avg:60.13ms
step:248/2315 train_time:14911ms step_avg:60.13ms
step:249/2315 train_time:14971ms step_avg:60.12ms
step:250/2315 train_time:15030ms step_avg:60.12ms
step:250/2315 val_loss:4.0816 train_time:15092ms step_avg:60.37ms
step:251/2315 train_time:15112ms step_avg:60.21ms
step:252/2315 train_time:15152ms step_avg:60.13ms
step:253/2315 train_time:15218ms step_avg:60.15ms
step:254/2315 train_time:15282ms step_avg:60.17ms
step:255/2315 train_time:15342ms step_avg:60.17ms
step:256/2315 train_time:15402ms step_avg:60.16ms
step:257/2315 train_time:15462ms step_avg:60.16ms
step:258/2315 train_time:15522ms step_avg:60.16ms
step:259/2315 train_time:15581ms step_avg:60.16ms
step:260/2315 train_time:15640ms step_avg:60.15ms
step:261/2315 train_time:15699ms step_avg:60.15ms
step:262/2315 train_time:15758ms step_avg:60.15ms
step:263/2315 train_time:15817ms step_avg:60.14ms
step:264/2315 train_time:15876ms step_avg:60.14ms
step:265/2315 train_time:15935ms step_avg:60.13ms
step:266/2315 train_time:15994ms step_avg:60.13ms
step:267/2315 train_time:16054ms step_avg:60.13ms
step:268/2315 train_time:16115ms step_avg:60.13ms
step:269/2315 train_time:16178ms step_avg:60.14ms
step:270/2315 train_time:16240ms step_avg:60.15ms
step:271/2315 train_time:16302ms step_avg:60.16ms
step:272/2315 train_time:16362ms step_avg:60.15ms
step:273/2315 train_time:16423ms step_avg:60.16ms
step:274/2315 train_time:16482ms step_avg:60.15ms
step:275/2315 train_time:16541ms step_avg:60.15ms
step:276/2315 train_time:16602ms step_avg:60.15ms
step:277/2315 train_time:16661ms step_avg:60.15ms
step:278/2315 train_time:16720ms step_avg:60.15ms
step:279/2315 train_time:16780ms step_avg:60.14ms
step:280/2315 train_time:16839ms step_avg:60.14ms
step:281/2315 train_time:16899ms step_avg:60.14ms
step:282/2315 train_time:16959ms step_avg:60.14ms
step:283/2315 train_time:17018ms step_avg:60.14ms
step:284/2315 train_time:17078ms step_avg:60.13ms
step:285/2315 train_time:17139ms step_avg:60.14ms
step:286/2315 train_time:17201ms step_avg:60.14ms
step:287/2315 train_time:17262ms step_avg:60.15ms
step:288/2315 train_time:17323ms step_avg:60.15ms
step:289/2315 train_time:17384ms step_avg:60.15ms
step:290/2315 train_time:17444ms step_avg:60.15ms
step:291/2315 train_time:17504ms step_avg:60.15ms
step:292/2315 train_time:17564ms step_avg:60.15ms
step:293/2315 train_time:17623ms step_avg:60.15ms
step:294/2315 train_time:17683ms step_avg:60.15ms
step:295/2315 train_time:17743ms step_avg:60.15ms
step:296/2315 train_time:17802ms step_avg:60.14ms
step:297/2315 train_time:17863ms step_avg:60.14ms
step:298/2315 train_time:17923ms step_avg:60.14ms
step:299/2315 train_time:17983ms step_avg:60.14ms
step:300/2315 train_time:18043ms step_avg:60.14ms
step:301/2315 train_time:18104ms step_avg:60.15ms
step:302/2315 train_time:18165ms step_avg:60.15ms
step:303/2315 train_time:18225ms step_avg:60.15ms
step:304/2315 train_time:18285ms step_avg:60.15ms
step:305/2315 train_time:18345ms step_avg:60.15ms
step:306/2315 train_time:18405ms step_avg:60.15ms
step:307/2315 train_time:18465ms step_avg:60.15ms
step:308/2315 train_time:18524ms step_avg:60.14ms
step:309/2315 train_time:18584ms step_avg:60.14ms
step:310/2315 train_time:18644ms step_avg:60.14ms
step:311/2315 train_time:18704ms step_avg:60.14ms
step:312/2315 train_time:18764ms step_avg:60.14ms
step:313/2315 train_time:18824ms step_avg:60.14ms
step:314/2315 train_time:18884ms step_avg:60.14ms
step:315/2315 train_time:18944ms step_avg:60.14ms
step:316/2315 train_time:19004ms step_avg:60.14ms
step:317/2315 train_time:19065ms step_avg:60.14ms
step:318/2315 train_time:19126ms step_avg:60.14ms
step:319/2315 train_time:19186ms step_avg:60.14ms
step:320/2315 train_time:19246ms step_avg:60.15ms
step:321/2315 train_time:19306ms step_avg:60.14ms
step:322/2315 train_time:19366ms step_avg:60.14ms
step:323/2315 train_time:19426ms step_avg:60.14ms
step:324/2315 train_time:19486ms step_avg:60.14ms
step:325/2315 train_time:19546ms step_avg:60.14ms
step:326/2315 train_time:19606ms step_avg:60.14ms
step:327/2315 train_time:19665ms step_avg:60.14ms
step:328/2315 train_time:19725ms step_avg:60.14ms
step:329/2315 train_time:19786ms step_avg:60.14ms
step:330/2315 train_time:19846ms step_avg:60.14ms
step:331/2315 train_time:19907ms step_avg:60.14ms
step:332/2315 train_time:19966ms step_avg:60.14ms
step:333/2315 train_time:20027ms step_avg:60.14ms
step:334/2315 train_time:20087ms step_avg:60.14ms
step:335/2315 train_time:20148ms step_avg:60.14ms
step:336/2315 train_time:20208ms step_avg:60.14ms
step:337/2315 train_time:20269ms step_avg:60.14ms
step:338/2315 train_time:20328ms step_avg:60.14ms
step:339/2315 train_time:20388ms step_avg:60.14ms
step:340/2315 train_time:20448ms step_avg:60.14ms
step:341/2315 train_time:20508ms step_avg:60.14ms
step:342/2315 train_time:20567ms step_avg:60.14ms
step:343/2315 train_time:20628ms step_avg:60.14ms
step:344/2315 train_time:20688ms step_avg:60.14ms
step:345/2315 train_time:20747ms step_avg:60.14ms
step:346/2315 train_time:20808ms step_avg:60.14ms
step:347/2315 train_time:20868ms step_avg:60.14ms
step:348/2315 train_time:20928ms step_avg:60.14ms
step:349/2315 train_time:20988ms step_avg:60.14ms
step:350/2315 train_time:21048ms step_avg:60.14ms
step:351/2315 train_time:21109ms step_avg:60.14ms
step:352/2315 train_time:21169ms step_avg:60.14ms
step:353/2315 train_time:21229ms step_avg:60.14ms
step:354/2315 train_time:21289ms step_avg:60.14ms
step:355/2315 train_time:21349ms step_avg:60.14ms
step:356/2315 train_time:21409ms step_avg:60.14ms
step:357/2315 train_time:21469ms step_avg:60.14ms
step:358/2315 train_time:21530ms step_avg:60.14ms
step:359/2315 train_time:21590ms step_avg:60.14ms
step:360/2315 train_time:21649ms step_avg:60.14ms
step:361/2315 train_time:21709ms step_avg:60.14ms
step:362/2315 train_time:21769ms step_avg:60.14ms
step:363/2315 train_time:21829ms step_avg:60.14ms
step:364/2315 train_time:21889ms step_avg:60.14ms
step:365/2315 train_time:21950ms step_avg:60.14ms
step:366/2315 train_time:22011ms step_avg:60.14ms
step:367/2315 train_time:22071ms step_avg:60.14ms
step:368/2315 train_time:22131ms step_avg:60.14ms
step:369/2315 train_time:22191ms step_avg:60.14ms
step:370/2315 train_time:22251ms step_avg:60.14ms
step:371/2315 train_time:22311ms step_avg:60.14ms
step:372/2315 train_time:22371ms step_avg:60.14ms
step:373/2315 train_time:22431ms step_avg:60.14ms
step:374/2315 train_time:22490ms step_avg:60.13ms
step:375/2315 train_time:22550ms step_avg:60.13ms
step:376/2315 train_time:22610ms step_avg:60.13ms
step:377/2315 train_time:22670ms step_avg:60.13ms
step:378/2315 train_time:22730ms step_avg:60.13ms
step:379/2315 train_time:22790ms step_avg:60.13ms
step:380/2315 train_time:22851ms step_avg:60.13ms
step:381/2315 train_time:22910ms step_avg:60.13ms
step:382/2315 train_time:22970ms step_avg:60.13ms
step:383/2315 train_time:23030ms step_avg:60.13ms
step:384/2315 train_time:23090ms step_avg:60.13ms
step:385/2315 train_time:23150ms step_avg:60.13ms
step:386/2315 train_time:23210ms step_avg:60.13ms
step:387/2315 train_time:23270ms step_avg:60.13ms
step:388/2315 train_time:23330ms step_avg:60.13ms
step:389/2315 train_time:23390ms step_avg:60.13ms
step:390/2315 train_time:23451ms step_avg:60.13ms
step:391/2315 train_time:23511ms step_avg:60.13ms
step:392/2315 train_time:23570ms step_avg:60.13ms
step:393/2315 train_time:23630ms step_avg:60.13ms
step:394/2315 train_time:23690ms step_avg:60.13ms
step:395/2315 train_time:23750ms step_avg:60.13ms
step:396/2315 train_time:23810ms step_avg:60.13ms
step:397/2315 train_time:23871ms step_avg:60.13ms
step:398/2315 train_time:23930ms step_avg:60.13ms
step:399/2315 train_time:23990ms step_avg:60.13ms
step:400/2315 train_time:24051ms step_avg:60.13ms
step:401/2315 train_time:24111ms step_avg:60.13ms
step:402/2315 train_time:24171ms step_avg:60.13ms
step:403/2315 train_time:24231ms step_avg:60.13ms
step:404/2315 train_time:24290ms step_avg:60.12ms
step:405/2315 train_time:24350ms step_avg:60.12ms
step:406/2315 train_time:24410ms step_avg:60.12ms
step:407/2315 train_time:24470ms step_avg:60.12ms
step:408/2315 train_time:24530ms step_avg:60.12ms
step:409/2315 train_time:24590ms step_avg:60.12ms
step:410/2315 train_time:24649ms step_avg:60.12ms
step:411/2315 train_time:24710ms step_avg:60.12ms
step:412/2315 train_time:24770ms step_avg:60.12ms
step:413/2315 train_time:24830ms step_avg:60.12ms
step:414/2315 train_time:24890ms step_avg:60.12ms
step:415/2315 train_time:24951ms step_avg:60.12ms
step:416/2315 train_time:25011ms step_avg:60.12ms
step:417/2315 train_time:25072ms step_avg:60.12ms
step:418/2315 train_time:25132ms step_avg:60.12ms
step:419/2315 train_time:25191ms step_avg:60.12ms
step:420/2315 train_time:25251ms step_avg:60.12ms
step:421/2315 train_time:25311ms step_avg:60.12ms
step:422/2315 train_time:25370ms step_avg:60.12ms
step:423/2315 train_time:25430ms step_avg:60.12ms
step:424/2315 train_time:25490ms step_avg:60.12ms
step:425/2315 train_time:25551ms step_avg:60.12ms
step:426/2315 train_time:25611ms step_avg:60.12ms
step:427/2315 train_time:25671ms step_avg:60.12ms
step:428/2315 train_time:25731ms step_avg:60.12ms
step:429/2315 train_time:25791ms step_avg:60.12ms
step:430/2315 train_time:25851ms step_avg:60.12ms
step:431/2315 train_time:25911ms step_avg:60.12ms
step:432/2315 train_time:25971ms step_avg:60.12ms
step:433/2315 train_time:26032ms step_avg:60.12ms
step:434/2315 train_time:26092ms step_avg:60.12ms
step:435/2315 train_time:26152ms step_avg:60.12ms
step:436/2315 train_time:26211ms step_avg:60.12ms
step:437/2315 train_time:26271ms step_avg:60.12ms
step:438/2315 train_time:26331ms step_avg:60.12ms
step:439/2315 train_time:26391ms step_avg:60.12ms
step:440/2315 train_time:26451ms step_avg:60.12ms
step:441/2315 train_time:26511ms step_avg:60.12ms
step:442/2315 train_time:26572ms step_avg:60.12ms
step:443/2315 train_time:26631ms step_avg:60.12ms
step:444/2315 train_time:26691ms step_avg:60.12ms
step:445/2315 train_time:26751ms step_avg:60.11ms
step:446/2315 train_time:26810ms step_avg:60.11ms
step:447/2315 train_time:26871ms step_avg:60.11ms
step:448/2315 train_time:26931ms step_avg:60.11ms
step:449/2315 train_time:26992ms step_avg:60.12ms
step:450/2315 train_time:27052ms step_avg:60.12ms
step:451/2315 train_time:27112ms step_avg:60.11ms
step:452/2315 train_time:27171ms step_avg:60.11ms
step:453/2315 train_time:27231ms step_avg:60.11ms
step:454/2315 train_time:27291ms step_avg:60.11ms
step:455/2315 train_time:27351ms step_avg:60.11ms
step:456/2315 train_time:27411ms step_avg:60.11ms
step:457/2315 train_time:27471ms step_avg:60.11ms
step:458/2315 train_time:27531ms step_avg:60.11ms
step:459/2315 train_time:27591ms step_avg:60.11ms
step:460/2315 train_time:27652ms step_avg:60.11ms
step:461/2315 train_time:27712ms step_avg:60.11ms
step:462/2315 train_time:27772ms step_avg:60.11ms
step:463/2315 train_time:27831ms step_avg:60.11ms
step:464/2315 train_time:27891ms step_avg:60.11ms
step:465/2315 train_time:27951ms step_avg:60.11ms
step:466/2315 train_time:28011ms step_avg:60.11ms
step:467/2315 train_time:28072ms step_avg:60.11ms
step:468/2315 train_time:28131ms step_avg:60.11ms
step:469/2315 train_time:28191ms step_avg:60.11ms
step:470/2315 train_time:28251ms step_avg:60.11ms
step:471/2315 train_time:28311ms step_avg:60.11ms
step:472/2315 train_time:28370ms step_avg:60.11ms
step:473/2315 train_time:28431ms step_avg:60.11ms
step:474/2315 train_time:28491ms step_avg:60.11ms
step:475/2315 train_time:28550ms step_avg:60.11ms
step:476/2315 train_time:28610ms step_avg:60.11ms
step:477/2315 train_time:28671ms step_avg:60.11ms
step:478/2315 train_time:28731ms step_avg:60.11ms
step:479/2315 train_time:28791ms step_avg:60.11ms
step:480/2315 train_time:28850ms step_avg:60.10ms
step:481/2315 train_time:28910ms step_avg:60.10ms
step:482/2315 train_time:28970ms step_avg:60.10ms
step:483/2315 train_time:29030ms step_avg:60.10ms
step:484/2315 train_time:29090ms step_avg:60.10ms
step:485/2315 train_time:29151ms step_avg:60.10ms
step:486/2315 train_time:29211ms step_avg:60.11ms
step:487/2315 train_time:29271ms step_avg:60.10ms
step:488/2315 train_time:29331ms step_avg:60.10ms
step:489/2315 train_time:29391ms step_avg:60.10ms
step:490/2315 train_time:29450ms step_avg:60.10ms
step:491/2315 train_time:29511ms step_avg:60.10ms
step:492/2315 train_time:29570ms step_avg:60.10ms
step:493/2315 train_time:29630ms step_avg:60.10ms
step:494/2315 train_time:29690ms step_avg:60.10ms
step:495/2315 train_time:29750ms step_avg:60.10ms
step:496/2315 train_time:29811ms step_avg:60.10ms
step:497/2315 train_time:29871ms step_avg:60.10ms
step:498/2315 train_time:29931ms step_avg:60.10ms
step:499/2315 train_time:29991ms step_avg:60.10ms
step:500/2315 train_time:30051ms step_avg:60.10ms
step:500/2315 val_loss:3.8207 train_time:30112ms step_avg:60.22ms
step:501/2315 train_time:30132ms step_avg:60.14ms
step:502/2315 train_time:30173ms step_avg:60.11ms
step:503/2315 train_time:30236ms step_avg:60.11ms
step:504/2315 train_time:30299ms step_avg:60.12ms
step:505/2315 train_time:30359ms step_avg:60.12ms
step:506/2315 train_time:30419ms step_avg:60.12ms
step:507/2315 train_time:30479ms step_avg:60.12ms
step:508/2315 train_time:30538ms step_avg:60.11ms
step:509/2315 train_time:30597ms step_avg:60.11ms
step:510/2315 train_time:30656ms step_avg:60.11ms
step:511/2315 train_time:30715ms step_avg:60.11ms
step:512/2315 train_time:30774ms step_avg:60.11ms
step:513/2315 train_time:30834ms step_avg:60.10ms
step:514/2315 train_time:30893ms step_avg:60.10ms
step:515/2315 train_time:30952ms step_avg:60.10ms
step:516/2315 train_time:31011ms step_avg:60.10ms
step:517/2315 train_time:31071ms step_avg:60.10ms
step:518/2315 train_time:31132ms step_avg:60.10ms
step:519/2315 train_time:31193ms step_avg:60.10ms
step:520/2315 train_time:31254ms step_avg:60.10ms
step:521/2315 train_time:31315ms step_avg:60.11ms
step:522/2315 train_time:31375ms step_avg:60.11ms
step:523/2315 train_time:31435ms step_avg:60.11ms
step:524/2315 train_time:31495ms step_avg:60.11ms
step:525/2315 train_time:31557ms step_avg:60.11ms
step:526/2315 train_time:31616ms step_avg:60.11ms
step:527/2315 train_time:31676ms step_avg:60.11ms
step:528/2315 train_time:31735ms step_avg:60.10ms
step:529/2315 train_time:31794ms step_avg:60.10ms
step:530/2315 train_time:31854ms step_avg:60.10ms
step:531/2315 train_time:31913ms step_avg:60.10ms
step:532/2315 train_time:31973ms step_avg:60.10ms
step:533/2315 train_time:32032ms step_avg:60.10ms
step:534/2315 train_time:32092ms step_avg:60.10ms
step:535/2315 train_time:32153ms step_avg:60.10ms
step:536/2315 train_time:32213ms step_avg:60.10ms
step:537/2315 train_time:32274ms step_avg:60.10ms
step:538/2315 train_time:32335ms step_avg:60.10ms
step:539/2315 train_time:32394ms step_avg:60.10ms
step:540/2315 train_time:32455ms step_avg:60.10ms
step:541/2315 train_time:32516ms step_avg:60.10ms
step:542/2315 train_time:32576ms step_avg:60.10ms
step:543/2315 train_time:32636ms step_avg:60.10ms
step:544/2315 train_time:32695ms step_avg:60.10ms
step:545/2315 train_time:32755ms step_avg:60.10ms
step:546/2315 train_time:32815ms step_avg:60.10ms
step:547/2315 train_time:32874ms step_avg:60.10ms
step:548/2315 train_time:32934ms step_avg:60.10ms
step:549/2315 train_time:32993ms step_avg:60.10ms
step:550/2315 train_time:33053ms step_avg:60.10ms
step:551/2315 train_time:33112ms step_avg:60.10ms
step:552/2315 train_time:33172ms step_avg:60.09ms
step:553/2315 train_time:33233ms step_avg:60.10ms
step:554/2315 train_time:33294ms step_avg:60.10ms
step:555/2315 train_time:33355ms step_avg:60.10ms
step:556/2315 train_time:33415ms step_avg:60.10ms
step:557/2315 train_time:33476ms step_avg:60.10ms
step:558/2315 train_time:33536ms step_avg:60.10ms
step:559/2315 train_time:33596ms step_avg:60.10ms
step:560/2315 train_time:33656ms step_avg:60.10ms
step:561/2315 train_time:33716ms step_avg:60.10ms
step:562/2315 train_time:33775ms step_avg:60.10ms
step:563/2315 train_time:33835ms step_avg:60.10ms
step:564/2315 train_time:33894ms step_avg:60.10ms
step:565/2315 train_time:33953ms step_avg:60.09ms
step:566/2315 train_time:34013ms step_avg:60.09ms
step:567/2315 train_time:34073ms step_avg:60.09ms
step:568/2315 train_time:34133ms step_avg:60.09ms
step:569/2315 train_time:34193ms step_avg:60.09ms
step:570/2315 train_time:34254ms step_avg:60.09ms
step:571/2315 train_time:34314ms step_avg:60.09ms
step:572/2315 train_time:34374ms step_avg:60.09ms
step:573/2315 train_time:34435ms step_avg:60.10ms
step:574/2315 train_time:34495ms step_avg:60.10ms
step:575/2315 train_time:34555ms step_avg:60.10ms
step:576/2315 train_time:34615ms step_avg:60.09ms
step:577/2315 train_time:34675ms step_avg:60.10ms
step:578/2315 train_time:34735ms step_avg:60.09ms
step:579/2315 train_time:34794ms step_avg:60.09ms
step:580/2315 train_time:34854ms step_avg:60.09ms
step:581/2315 train_time:34914ms step_avg:60.09ms
step:582/2315 train_time:34973ms step_avg:60.09ms
step:583/2315 train_time:35033ms step_avg:60.09ms
step:584/2315 train_time:35093ms step_avg:60.09ms
step:585/2315 train_time:35154ms step_avg:60.09ms
step:586/2315 train_time:35214ms step_avg:60.09ms
step:587/2315 train_time:35274ms step_avg:60.09ms
step:588/2315 train_time:35334ms step_avg:60.09ms
step:589/2315 train_time:35394ms step_avg:60.09ms
step:590/2315 train_time:35454ms step_avg:60.09ms
step:591/2315 train_time:35514ms step_avg:60.09ms
step:592/2315 train_time:35574ms step_avg:60.09ms
step:593/2315 train_time:35635ms step_avg:60.09ms
step:594/2315 train_time:35694ms step_avg:60.09ms
step:595/2315 train_time:35754ms step_avg:60.09ms
step:596/2315 train_time:35814ms step_avg:60.09ms
step:597/2315 train_time:35874ms step_avg:60.09ms
step:598/2315 train_time:35934ms step_avg:60.09ms
step:599/2315 train_time:35994ms step_avg:60.09ms
step:600/2315 train_time:36054ms step_avg:60.09ms
step:601/2315 train_time:36114ms step_avg:60.09ms
step:602/2315 train_time:36173ms step_avg:60.09ms
step:603/2315 train_time:36233ms step_avg:60.09ms
step:604/2315 train_time:36294ms step_avg:60.09ms
step:605/2315 train_time:36354ms step_avg:60.09ms
step:606/2315 train_time:36415ms step_avg:60.09ms
step:607/2315 train_time:36475ms step_avg:60.09ms
step:608/2315 train_time:36535ms step_avg:60.09ms
step:609/2315 train_time:36595ms step_avg:60.09ms
step:610/2315 train_time:36655ms step_avg:60.09ms
step:611/2315 train_time:36715ms step_avg:60.09ms
step:612/2315 train_time:36775ms step_avg:60.09ms
step:613/2315 train_time:36834ms step_avg:60.09ms
step:614/2315 train_time:36894ms step_avg:60.09ms
step:615/2315 train_time:36954ms step_avg:60.09ms
step:616/2315 train_time:37013ms step_avg:60.09ms
step:617/2315 train_time:37073ms step_avg:60.09ms
step:618/2315 train_time:37133ms step_avg:60.09ms
step:619/2315 train_time:37193ms step_avg:60.09ms
step:620/2315 train_time:37253ms step_avg:60.09ms
step:621/2315 train_time:37313ms step_avg:60.09ms
step:622/2315 train_time:37373ms step_avg:60.08ms
step:623/2315 train_time:37433ms step_avg:60.08ms
step:624/2315 train_time:37493ms step_avg:60.09ms
step:625/2315 train_time:37553ms step_avg:60.09ms
step:626/2315 train_time:37613ms step_avg:60.09ms
step:627/2315 train_time:37674ms step_avg:60.09ms
step:628/2315 train_time:37734ms step_avg:60.09ms
step:629/2315 train_time:37794ms step_avg:60.09ms
step:630/2315 train_time:37854ms step_avg:60.09ms
step:631/2315 train_time:37914ms step_avg:60.09ms
step:632/2315 train_time:37973ms step_avg:60.08ms
step:633/2315 train_time:38034ms step_avg:60.08ms
step:634/2315 train_time:38095ms step_avg:60.09ms
step:635/2315 train_time:38154ms step_avg:60.09ms
step:636/2315 train_time:38214ms step_avg:60.09ms
step:637/2315 train_time:38274ms step_avg:60.08ms
step:638/2315 train_time:38334ms step_avg:60.08ms
step:639/2315 train_time:38394ms step_avg:60.08ms
step:640/2315 train_time:38455ms step_avg:60.09ms
step:641/2315 train_time:38516ms step_avg:60.09ms
step:642/2315 train_time:38575ms step_avg:60.09ms
step:643/2315 train_time:38635ms step_avg:60.09ms
step:644/2315 train_time:38695ms step_avg:60.09ms
step:645/2315 train_time:38755ms step_avg:60.09ms
step:646/2315 train_time:38815ms step_avg:60.08ms
step:647/2315 train_time:38874ms step_avg:60.08ms
step:648/2315 train_time:38934ms step_avg:60.08ms
step:649/2315 train_time:38993ms step_avg:60.08ms
step:650/2315 train_time:39053ms step_avg:60.08ms
step:651/2315 train_time:39113ms step_avg:60.08ms
step:652/2315 train_time:39173ms step_avg:60.08ms
step:653/2315 train_time:39233ms step_avg:60.08ms
step:654/2315 train_time:39293ms step_avg:60.08ms
step:655/2315 train_time:39354ms step_avg:60.08ms
step:656/2315 train_time:39413ms step_avg:60.08ms
step:657/2315 train_time:39473ms step_avg:60.08ms
step:658/2315 train_time:39534ms step_avg:60.08ms
step:659/2315 train_time:39594ms step_avg:60.08ms
step:660/2315 train_time:39654ms step_avg:60.08ms
step:661/2315 train_time:39714ms step_avg:60.08ms
step:662/2315 train_time:39774ms step_avg:60.08ms
step:663/2315 train_time:39834ms step_avg:60.08ms
step:664/2315 train_time:39893ms step_avg:60.08ms
step:665/2315 train_time:39953ms step_avg:60.08ms
step:666/2315 train_time:40013ms step_avg:60.08ms
step:667/2315 train_time:40074ms step_avg:60.08ms
step:668/2315 train_time:40135ms step_avg:60.08ms
step:669/2315 train_time:40194ms step_avg:60.08ms
step:670/2315 train_time:40254ms step_avg:60.08ms
step:671/2315 train_time:40314ms step_avg:60.08ms
step:672/2315 train_time:40375ms step_avg:60.08ms
step:673/2315 train_time:40435ms step_avg:60.08ms
step:674/2315 train_time:40494ms step_avg:60.08ms
step:675/2315 train_time:40554ms step_avg:60.08ms
step:676/2315 train_time:40614ms step_avg:60.08ms
step:677/2315 train_time:40674ms step_avg:60.08ms
step:678/2315 train_time:40734ms step_avg:60.08ms
step:679/2315 train_time:40794ms step_avg:60.08ms
step:680/2315 train_time:40854ms step_avg:60.08ms
step:681/2315 train_time:40914ms step_avg:60.08ms
step:682/2315 train_time:40974ms step_avg:60.08ms
step:683/2315 train_time:41034ms step_avg:60.08ms
step:684/2315 train_time:41094ms step_avg:60.08ms
step:685/2315 train_time:41154ms step_avg:60.08ms
step:686/2315 train_time:41214ms step_avg:60.08ms
step:687/2315 train_time:41273ms step_avg:60.08ms
step:688/2315 train_time:41333ms step_avg:60.08ms
step:689/2315 train_time:41394ms step_avg:60.08ms
step:690/2315 train_time:41454ms step_avg:60.08ms
step:691/2315 train_time:41514ms step_avg:60.08ms
step:692/2315 train_time:41575ms step_avg:60.08ms
step:693/2315 train_time:41634ms step_avg:60.08ms
step:694/2315 train_time:41694ms step_avg:60.08ms
step:695/2315 train_time:41755ms step_avg:60.08ms
step:696/2315 train_time:41815ms step_avg:60.08ms
step:697/2315 train_time:41874ms step_avg:60.08ms
step:698/2315 train_time:41934ms step_avg:60.08ms
step:699/2315 train_time:41995ms step_avg:60.08ms
step:700/2315 train_time:42055ms step_avg:60.08ms
step:701/2315 train_time:42115ms step_avg:60.08ms
step:702/2315 train_time:42175ms step_avg:60.08ms
step:703/2315 train_time:42235ms step_avg:60.08ms
step:704/2315 train_time:42295ms step_avg:60.08ms
step:705/2315 train_time:42355ms step_avg:60.08ms
step:706/2315 train_time:42415ms step_avg:60.08ms
step:707/2315 train_time:42474ms step_avg:60.08ms
step:708/2315 train_time:42535ms step_avg:60.08ms
step:709/2315 train_time:42595ms step_avg:60.08ms
step:710/2315 train_time:42655ms step_avg:60.08ms
step:711/2315 train_time:42715ms step_avg:60.08ms
step:712/2315 train_time:42775ms step_avg:60.08ms
step:713/2315 train_time:42835ms step_avg:60.08ms
step:714/2315 train_time:42895ms step_avg:60.08ms
step:715/2315 train_time:42955ms step_avg:60.08ms
step:716/2315 train_time:43014ms step_avg:60.08ms
step:717/2315 train_time:43075ms step_avg:60.08ms
step:718/2315 train_time:43135ms step_avg:60.08ms
step:719/2315 train_time:43195ms step_avg:60.08ms
step:720/2315 train_time:43255ms step_avg:60.08ms
step:721/2315 train_time:43315ms step_avg:60.08ms
step:722/2315 train_time:43375ms step_avg:60.08ms
step:723/2315 train_time:43435ms step_avg:60.08ms
step:724/2315 train_time:43494ms step_avg:60.08ms
step:725/2315 train_time:43554ms step_avg:60.07ms
step:726/2315 train_time:43614ms step_avg:60.07ms
step:727/2315 train_time:43674ms step_avg:60.07ms
step:728/2315 train_time:43734ms step_avg:60.07ms
step:729/2315 train_time:43794ms step_avg:60.07ms
step:730/2315 train_time:43854ms step_avg:60.07ms
step:731/2315 train_time:43914ms step_avg:60.07ms
step:732/2315 train_time:43974ms step_avg:60.07ms
step:733/2315 train_time:44035ms step_avg:60.07ms
step:734/2315 train_time:44095ms step_avg:60.07ms
step:735/2315 train_time:44154ms step_avg:60.07ms
step:736/2315 train_time:44214ms step_avg:60.07ms
step:737/2315 train_time:44274ms step_avg:60.07ms
step:738/2315 train_time:44334ms step_avg:60.07ms
step:739/2315 train_time:44394ms step_avg:60.07ms
step:740/2315 train_time:44454ms step_avg:60.07ms
step:741/2315 train_time:44514ms step_avg:60.07ms
step:742/2315 train_time:44573ms step_avg:60.07ms
step:743/2315 train_time:44633ms step_avg:60.07ms
step:744/2315 train_time:44693ms step_avg:60.07ms
step:745/2315 train_time:44753ms step_avg:60.07ms
step:746/2315 train_time:44813ms step_avg:60.07ms
step:747/2315 train_time:44873ms step_avg:60.07ms
step:748/2315 train_time:44933ms step_avg:60.07ms
step:749/2315 train_time:44993ms step_avg:60.07ms
step:750/2315 train_time:45054ms step_avg:60.07ms
step:750/2315 val_loss:3.6959 train_time:45116ms step_avg:60.15ms
step:751/2315 train_time:45136ms step_avg:60.10ms
step:752/2315 train_time:45175ms step_avg:60.07ms
step:753/2315 train_time:45238ms step_avg:60.08ms
step:754/2315 train_time:45301ms step_avg:60.08ms
step:755/2315 train_time:45363ms step_avg:60.08ms
step:756/2315 train_time:45424ms step_avg:60.08ms
step:757/2315 train_time:45484ms step_avg:60.08ms
step:758/2315 train_time:45543ms step_avg:60.08ms
step:759/2315 train_time:45603ms step_avg:60.08ms
step:760/2315 train_time:45663ms step_avg:60.08ms
step:761/2315 train_time:45723ms step_avg:60.08ms
step:762/2315 train_time:45783ms step_avg:60.08ms
step:763/2315 train_time:45844ms step_avg:60.08ms
step:764/2315 train_time:45904ms step_avg:60.08ms
step:765/2315 train_time:45964ms step_avg:60.08ms
step:766/2315 train_time:46024ms step_avg:60.08ms
step:767/2315 train_time:46086ms step_avg:60.09ms
step:768/2315 train_time:46147ms step_avg:60.09ms
step:769/2315 train_time:46210ms step_avg:60.09ms
step:770/2315 train_time:46273ms step_avg:60.09ms
step:771/2315 train_time:46335ms step_avg:60.10ms
step:772/2315 train_time:46396ms step_avg:60.10ms
step:773/2315 train_time:46456ms step_avg:60.10ms
step:774/2315 train_time:46516ms step_avg:60.10ms
step:775/2315 train_time:46576ms step_avg:60.10ms
step:776/2315 train_time:46637ms step_avg:60.10ms
step:777/2315 train_time:46697ms step_avg:60.10ms
step:778/2315 train_time:46757ms step_avg:60.10ms
step:779/2315 train_time:46818ms step_avg:60.10ms
step:780/2315 train_time:46878ms step_avg:60.10ms
step:781/2315 train_time:46938ms step_avg:60.10ms
step:782/2315 train_time:46998ms step_avg:60.10ms
step:783/2315 train_time:47059ms step_avg:60.10ms
step:784/2315 train_time:47119ms step_avg:60.10ms
step:785/2315 train_time:47181ms step_avg:60.10ms
step:786/2315 train_time:47243ms step_avg:60.11ms
step:787/2315 train_time:47304ms step_avg:60.11ms
step:788/2315 train_time:47366ms step_avg:60.11ms
step:789/2315 train_time:47427ms step_avg:60.11ms
step:790/2315 train_time:47488ms step_avg:60.11ms
step:791/2315 train_time:47549ms step_avg:60.11ms
step:792/2315 train_time:47609ms step_avg:60.11ms
step:793/2315 train_time:47670ms step_avg:60.11ms
step:794/2315 train_time:47731ms step_avg:60.11ms
step:795/2315 train_time:47791ms step_avg:60.12ms
step:796/2315 train_time:47852ms step_avg:60.12ms
step:797/2315 train_time:47913ms step_avg:60.12ms
step:798/2315 train_time:47974ms step_avg:60.12ms
step:799/2315 train_time:48035ms step_avg:60.12ms
step:800/2315 train_time:48095ms step_avg:60.12ms
step:801/2315 train_time:48156ms step_avg:60.12ms
step:802/2315 train_time:48217ms step_avg:60.12ms
step:803/2315 train_time:48278ms step_avg:60.12ms
step:804/2315 train_time:48337ms step_avg:60.12ms
step:805/2315 train_time:48398ms step_avg:60.12ms
step:806/2315 train_time:48458ms step_avg:60.12ms
step:807/2315 train_time:48519ms step_avg:60.12ms
step:808/2315 train_time:48580ms step_avg:60.12ms
step:809/2315 train_time:48641ms step_avg:60.13ms
step:810/2315 train_time:48702ms step_avg:60.13ms
step:811/2315 train_time:48763ms step_avg:60.13ms
step:812/2315 train_time:48824ms step_avg:60.13ms
step:813/2315 train_time:48886ms step_avg:60.13ms
step:814/2315 train_time:48946ms step_avg:60.13ms
step:815/2315 train_time:49007ms step_avg:60.13ms
step:816/2315 train_time:49067ms step_avg:60.13ms
step:817/2315 train_time:49129ms step_avg:60.13ms
step:818/2315 train_time:49189ms step_avg:60.13ms
step:819/2315 train_time:49251ms step_avg:60.14ms
step:820/2315 train_time:49312ms step_avg:60.14ms
step:821/2315 train_time:49373ms step_avg:60.14ms
step:822/2315 train_time:49434ms step_avg:60.14ms
step:823/2315 train_time:49495ms step_avg:60.14ms
step:824/2315 train_time:49555ms step_avg:60.14ms
step:825/2315 train_time:49616ms step_avg:60.14ms
step:826/2315 train_time:49677ms step_avg:60.14ms
step:827/2315 train_time:49737ms step_avg:60.14ms
step:828/2315 train_time:49797ms step_avg:60.14ms
step:829/2315 train_time:49858ms step_avg:60.14ms
step:830/2315 train_time:49918ms step_avg:60.14ms
step:831/2315 train_time:49978ms step_avg:60.14ms
step:832/2315 train_time:50039ms step_avg:60.14ms
step:833/2315 train_time:50100ms step_avg:60.14ms
step:834/2315 train_time:50160ms step_avg:60.14ms
step:835/2315 train_time:50221ms step_avg:60.15ms
step:836/2315 train_time:50282ms step_avg:60.15ms
step:837/2315 train_time:50344ms step_avg:60.15ms
step:838/2315 train_time:50405ms step_avg:60.15ms
step:839/2315 train_time:50465ms step_avg:60.15ms
step:840/2315 train_time:50526ms step_avg:60.15ms
step:841/2315 train_time:50588ms step_avg:60.15ms
step:842/2315 train_time:50649ms step_avg:60.15ms
step:843/2315 train_time:50710ms step_avg:60.15ms
step:844/2315 train_time:50771ms step_avg:60.15ms
step:845/2315 train_time:50832ms step_avg:60.16ms
step:846/2315 train_time:50893ms step_avg:60.16ms
step:847/2315 train_time:50954ms step_avg:60.16ms
step:848/2315 train_time:51014ms step_avg:60.16ms
step:849/2315 train_time:51075ms step_avg:60.16ms
step:850/2315 train_time:51136ms step_avg:60.16ms
step:851/2315 train_time:51197ms step_avg:60.16ms
step:852/2315 train_time:51257ms step_avg:60.16ms
step:853/2315 train_time:51318ms step_avg:60.16ms
step:854/2315 train_time:51378ms step_avg:60.16ms
step:855/2315 train_time:51438ms step_avg:60.16ms
step:856/2315 train_time:51498ms step_avg:60.16ms
step:857/2315 train_time:51559ms step_avg:60.16ms
step:858/2315 train_time:51620ms step_avg:60.16ms
step:859/2315 train_time:51681ms step_avg:60.16ms
step:860/2315 train_time:51742ms step_avg:60.17ms
step:861/2315 train_time:51804ms step_avg:60.17ms
step:862/2315 train_time:51865ms step_avg:60.17ms
step:863/2315 train_time:51926ms step_avg:60.17ms
step:864/2315 train_time:51986ms step_avg:60.17ms
step:865/2315 train_time:52047ms step_avg:60.17ms
step:866/2315 train_time:52108ms step_avg:60.17ms
step:867/2315 train_time:52170ms step_avg:60.17ms
step:868/2315 train_time:52231ms step_avg:60.17ms
step:869/2315 train_time:52292ms step_avg:60.18ms
step:870/2315 train_time:52353ms step_avg:60.18ms
step:871/2315 train_time:52414ms step_avg:60.18ms
step:872/2315 train_time:52474ms step_avg:60.18ms
step:873/2315 train_time:52535ms step_avg:60.18ms
step:874/2315 train_time:52596ms step_avg:60.18ms
step:875/2315 train_time:52657ms step_avg:60.18ms
step:876/2315 train_time:52717ms step_avg:60.18ms
step:877/2315 train_time:52778ms step_avg:60.18ms
step:878/2315 train_time:52838ms step_avg:60.18ms
step:879/2315 train_time:52899ms step_avg:60.18ms
step:880/2315 train_time:52959ms step_avg:60.18ms
step:881/2315 train_time:53020ms step_avg:60.18ms
step:882/2315 train_time:53081ms step_avg:60.18ms
step:883/2315 train_time:53142ms step_avg:60.18ms
step:884/2315 train_time:53203ms step_avg:60.18ms
step:885/2315 train_time:53264ms step_avg:60.19ms
step:886/2315 train_time:53326ms step_avg:60.19ms
step:887/2315 train_time:53388ms step_avg:60.19ms
step:888/2315 train_time:53448ms step_avg:60.19ms
step:889/2315 train_time:53510ms step_avg:60.19ms
step:890/2315 train_time:53570ms step_avg:60.19ms
step:891/2315 train_time:53631ms step_avg:60.19ms
step:892/2315 train_time:53692ms step_avg:60.19ms
step:893/2315 train_time:53753ms step_avg:60.19ms
step:894/2315 train_time:53814ms step_avg:60.19ms
step:895/2315 train_time:53875ms step_avg:60.20ms
step:896/2315 train_time:53935ms step_avg:60.20ms
step:897/2315 train_time:53996ms step_avg:60.20ms
step:898/2315 train_time:54056ms step_avg:60.20ms
step:899/2315 train_time:54117ms step_avg:60.20ms
step:900/2315 train_time:54178ms step_avg:60.20ms
step:901/2315 train_time:54238ms step_avg:60.20ms
step:902/2315 train_time:54298ms step_avg:60.20ms
step:903/2315 train_time:54359ms step_avg:60.20ms
step:904/2315 train_time:54419ms step_avg:60.20ms
step:905/2315 train_time:54479ms step_avg:60.20ms
step:906/2315 train_time:54540ms step_avg:60.20ms
step:907/2315 train_time:54601ms step_avg:60.20ms
step:908/2315 train_time:54661ms step_avg:60.20ms
step:909/2315 train_time:54722ms step_avg:60.20ms
step:910/2315 train_time:54784ms step_avg:60.20ms
step:911/2315 train_time:54845ms step_avg:60.20ms
step:912/2315 train_time:54905ms step_avg:60.20ms
step:913/2315 train_time:54966ms step_avg:60.20ms
step:914/2315 train_time:55027ms step_avg:60.20ms
step:915/2315 train_time:55088ms step_avg:60.21ms
step:916/2315 train_time:55149ms step_avg:60.21ms
step:917/2315 train_time:55211ms step_avg:60.21ms
step:918/2315 train_time:55271ms step_avg:60.21ms
step:919/2315 train_time:55332ms step_avg:60.21ms
step:920/2315 train_time:55392ms step_avg:60.21ms
step:921/2315 train_time:55453ms step_avg:60.21ms
step:922/2315 train_time:55514ms step_avg:60.21ms
step:923/2315 train_time:55575ms step_avg:60.21ms
step:924/2315 train_time:55635ms step_avg:60.21ms
step:925/2315 train_time:55696ms step_avg:60.21ms
step:926/2315 train_time:55757ms step_avg:60.21ms
step:927/2315 train_time:55818ms step_avg:60.21ms
step:928/2315 train_time:55878ms step_avg:60.21ms
step:929/2315 train_time:55938ms step_avg:60.21ms
step:930/2315 train_time:55998ms step_avg:60.21ms
step:931/2315 train_time:56058ms step_avg:60.21ms
step:932/2315 train_time:56119ms step_avg:60.21ms
step:933/2315 train_time:56180ms step_avg:60.21ms
step:934/2315 train_time:56241ms step_avg:60.21ms
step:935/2315 train_time:56302ms step_avg:60.22ms
step:936/2315 train_time:56362ms step_avg:60.22ms
step:937/2315 train_time:56423ms step_avg:60.22ms
step:938/2315 train_time:56484ms step_avg:60.22ms
step:939/2315 train_time:56546ms step_avg:60.22ms
step:940/2315 train_time:56606ms step_avg:60.22ms
step:941/2315 train_time:56667ms step_avg:60.22ms
step:942/2315 train_time:56728ms step_avg:60.22ms
step:943/2315 train_time:56790ms step_avg:60.22ms
step:944/2315 train_time:56851ms step_avg:60.22ms
step:945/2315 train_time:56911ms step_avg:60.22ms
step:946/2315 train_time:56972ms step_avg:60.22ms
step:947/2315 train_time:57033ms step_avg:60.23ms
step:948/2315 train_time:57094ms step_avg:60.23ms
step:949/2315 train_time:57155ms step_avg:60.23ms
step:950/2315 train_time:57216ms step_avg:60.23ms
step:951/2315 train_time:57276ms step_avg:60.23ms
step:952/2315 train_time:57337ms step_avg:60.23ms
step:953/2315 train_time:57397ms step_avg:60.23ms
step:954/2315 train_time:57458ms step_avg:60.23ms
step:955/2315 train_time:57518ms step_avg:60.23ms
step:956/2315 train_time:57579ms step_avg:60.23ms
step:957/2315 train_time:57639ms step_avg:60.23ms
step:958/2315 train_time:57700ms step_avg:60.23ms
step:959/2315 train_time:57760ms step_avg:60.23ms
step:960/2315 train_time:57821ms step_avg:60.23ms
step:961/2315 train_time:57882ms step_avg:60.23ms
step:962/2315 train_time:57943ms step_avg:60.23ms
step:963/2315 train_time:58003ms step_avg:60.23ms
step:964/2315 train_time:58064ms step_avg:60.23ms
step:965/2315 train_time:58125ms step_avg:60.23ms
step:966/2315 train_time:58186ms step_avg:60.23ms
step:967/2315 train_time:58246ms step_avg:60.23ms
step:968/2315 train_time:58307ms step_avg:60.23ms
step:969/2315 train_time:58368ms step_avg:60.24ms
step:970/2315 train_time:58429ms step_avg:60.24ms
step:971/2315 train_time:58490ms step_avg:60.24ms
step:972/2315 train_time:58551ms step_avg:60.24ms
step:973/2315 train_time:58613ms step_avg:60.24ms
step:974/2315 train_time:58673ms step_avg:60.24ms
step:975/2315 train_time:58734ms step_avg:60.24ms
step:976/2315 train_time:58795ms step_avg:60.24ms
step:977/2315 train_time:58856ms step_avg:60.24ms
step:978/2315 train_time:58916ms step_avg:60.24ms
step:979/2315 train_time:58977ms step_avg:60.24ms
step:980/2315 train_time:59037ms step_avg:60.24ms
step:981/2315 train_time:59097ms step_avg:60.24ms
step:982/2315 train_time:59157ms step_avg:60.24ms
step:983/2315 train_time:59218ms step_avg:60.24ms
step:984/2315 train_time:59278ms step_avg:60.24ms
step:985/2315 train_time:59339ms step_avg:60.24ms
step:986/2315 train_time:59400ms step_avg:60.24ms
step:987/2315 train_time:59460ms step_avg:60.24ms
step:988/2315 train_time:59522ms step_avg:60.24ms
step:989/2315 train_time:59583ms step_avg:60.25ms
step:990/2315 train_time:59643ms step_avg:60.25ms
step:991/2315 train_time:59704ms step_avg:60.25ms
step:992/2315 train_time:59765ms step_avg:60.25ms
step:993/2315 train_time:59826ms step_avg:60.25ms
step:994/2315 train_time:59887ms step_avg:60.25ms
step:995/2315 train_time:59948ms step_avg:60.25ms
step:996/2315 train_time:60009ms step_avg:60.25ms
step:997/2315 train_time:60070ms step_avg:60.25ms
step:998/2315 train_time:60131ms step_avg:60.25ms
step:999/2315 train_time:60191ms step_avg:60.25ms
step:1000/2315 train_time:60252ms step_avg:60.25ms
step:1000/2315 val_loss:3.5778 train_time:60315ms step_avg:60.31ms
step:1001/2315 train_time:60337ms step_avg:60.28ms
step:1002/2315 train_time:60375ms step_avg:60.25ms
step:1003/2315 train_time:60438ms step_avg:60.26ms
step:1004/2315 train_time:60502ms step_avg:60.26ms
step:1005/2315 train_time:60563ms step_avg:60.26ms
step:1006/2315 train_time:60623ms step_avg:60.26ms
step:1007/2315 train_time:60683ms step_avg:60.26ms
step:1008/2315 train_time:60743ms step_avg:60.26ms
step:1009/2315 train_time:60804ms step_avg:60.26ms
step:1010/2315 train_time:60864ms step_avg:60.26ms
step:1011/2315 train_time:60924ms step_avg:60.26ms
step:1012/2315 train_time:60984ms step_avg:60.26ms
step:1013/2315 train_time:61044ms step_avg:60.26ms
step:1014/2315 train_time:61104ms step_avg:60.26ms
step:1015/2315 train_time:61164ms step_avg:60.26ms
step:1016/2315 train_time:61226ms step_avg:60.26ms
step:1017/2315 train_time:61291ms step_avg:60.27ms
step:1018/2315 train_time:61352ms step_avg:60.27ms
step:1019/2315 train_time:61414ms step_avg:60.27ms
step:1020/2315 train_time:61475ms step_avg:60.27ms
step:1021/2315 train_time:61537ms step_avg:60.27ms
step:1022/2315 train_time:61597ms step_avg:60.27ms
step:1023/2315 train_time:61657ms step_avg:60.27ms
step:1024/2315 train_time:61718ms step_avg:60.27ms
step:1025/2315 train_time:61778ms step_avg:60.27ms
step:1026/2315 train_time:61838ms step_avg:60.27ms
step:1027/2315 train_time:61898ms step_avg:60.27ms
step:1028/2315 train_time:61958ms step_avg:60.27ms
step:1029/2315 train_time:62018ms step_avg:60.27ms
step:1030/2315 train_time:62078ms step_avg:60.27ms
step:1031/2315 train_time:62138ms step_avg:60.27ms
step:1032/2315 train_time:62199ms step_avg:60.27ms
step:1033/2315 train_time:62261ms step_avg:60.27ms
step:1034/2315 train_time:62323ms step_avg:60.27ms
step:1035/2315 train_time:62385ms step_avg:60.28ms
step:1036/2315 train_time:62445ms step_avg:60.28ms
step:1037/2315 train_time:62507ms step_avg:60.28ms
step:1038/2315 train_time:62568ms step_avg:60.28ms
step:1039/2315 train_time:62629ms step_avg:60.28ms
step:1040/2315 train_time:62690ms step_avg:60.28ms
step:1041/2315 train_time:62752ms step_avg:60.28ms
step:1042/2315 train_time:62812ms step_avg:60.28ms
step:1043/2315 train_time:62873ms step_avg:60.28ms
step:1044/2315 train_time:62934ms step_avg:60.28ms
step:1045/2315 train_time:62994ms step_avg:60.28ms
step:1046/2315 train_time:63054ms step_avg:60.28ms
step:1047/2315 train_time:63115ms step_avg:60.28ms
step:1048/2315 train_time:63175ms step_avg:60.28ms
step:1049/2315 train_time:63236ms step_avg:60.28ms
step:1050/2315 train_time:63296ms step_avg:60.28ms
step:1051/2315 train_time:63357ms step_avg:60.28ms
step:1052/2315 train_time:63418ms step_avg:60.28ms
step:1053/2315 train_time:63479ms step_avg:60.28ms
step:1054/2315 train_time:63539ms step_avg:60.28ms
step:1055/2315 train_time:63601ms step_avg:60.29ms
step:1056/2315 train_time:63662ms step_avg:60.29ms
step:1057/2315 train_time:63723ms step_avg:60.29ms
step:1058/2315 train_time:63784ms step_avg:60.29ms
step:1059/2315 train_time:63845ms step_avg:60.29ms
step:1060/2315 train_time:63905ms step_avg:60.29ms
step:1061/2315 train_time:63966ms step_avg:60.29ms
step:1062/2315 train_time:64027ms step_avg:60.29ms
step:1063/2315 train_time:64089ms step_avg:60.29ms
step:1064/2315 train_time:64150ms step_avg:60.29ms
step:1065/2315 train_time:64211ms step_avg:60.29ms
step:1066/2315 train_time:64272ms step_avg:60.29ms
step:1067/2315 train_time:64333ms step_avg:60.29ms
step:1068/2315 train_time:64393ms step_avg:60.29ms
step:1069/2315 train_time:64455ms step_avg:60.29ms
step:1070/2315 train_time:64515ms step_avg:60.29ms
step:1071/2315 train_time:64576ms step_avg:60.29ms
step:1072/2315 train_time:64637ms step_avg:60.30ms
step:1073/2315 train_time:64698ms step_avg:60.30ms
step:1074/2315 train_time:64758ms step_avg:60.30ms
step:1075/2315 train_time:64819ms step_avg:60.30ms
step:1076/2315 train_time:64879ms step_avg:60.30ms
step:1077/2315 train_time:64941ms step_avg:60.30ms
step:1078/2315 train_time:65002ms step_avg:60.30ms
step:1079/2315 train_time:65063ms step_avg:60.30ms
step:1080/2315 train_time:65124ms step_avg:60.30ms
step:1081/2315 train_time:65185ms step_avg:60.30ms
step:1082/2315 train_time:65246ms step_avg:60.30ms
step:1083/2315 train_time:65307ms step_avg:60.30ms
step:1084/2315 train_time:65368ms step_avg:60.30ms
step:1085/2315 train_time:65429ms step_avg:60.30ms
step:1086/2315 train_time:65490ms step_avg:60.30ms
step:1087/2315 train_time:65551ms step_avg:60.30ms
step:1088/2315 train_time:65613ms step_avg:60.31ms
step:1089/2315 train_time:65674ms step_avg:60.31ms
step:1090/2315 train_time:65734ms step_avg:60.31ms
step:1091/2315 train_time:65795ms step_avg:60.31ms
step:1092/2315 train_time:65856ms step_avg:60.31ms
step:1093/2315 train_time:65916ms step_avg:60.31ms
step:1094/2315 train_time:65977ms step_avg:60.31ms
step:1095/2315 train_time:66037ms step_avg:60.31ms
step:1096/2315 train_time:66098ms step_avg:60.31ms
step:1097/2315 train_time:66159ms step_avg:60.31ms
step:1098/2315 train_time:66219ms step_avg:60.31ms
step:1099/2315 train_time:66281ms step_avg:60.31ms
step:1100/2315 train_time:66342ms step_avg:60.31ms
step:1101/2315 train_time:66403ms step_avg:60.31ms
step:1102/2315 train_time:66464ms step_avg:60.31ms
step:1103/2315 train_time:66524ms step_avg:60.31ms
step:1104/2315 train_time:66585ms step_avg:60.31ms
step:1105/2315 train_time:66646ms step_avg:60.31ms
step:1106/2315 train_time:66707ms step_avg:60.31ms
step:1107/2315 train_time:66768ms step_avg:60.31ms
step:1108/2315 train_time:66829ms step_avg:60.32ms
step:1109/2315 train_time:66891ms step_avg:60.32ms
step:1110/2315 train_time:66952ms step_avg:60.32ms
step:1111/2315 train_time:67013ms step_avg:60.32ms
step:1112/2315 train_time:67074ms step_avg:60.32ms
step:1113/2315 train_time:67135ms step_avg:60.32ms
step:1114/2315 train_time:67195ms step_avg:60.32ms
step:1115/2315 train_time:67256ms step_avg:60.32ms
step:1116/2315 train_time:67317ms step_avg:60.32ms
step:1117/2315 train_time:67377ms step_avg:60.32ms
step:1118/2315 train_time:67438ms step_avg:60.32ms
step:1119/2315 train_time:67499ms step_avg:60.32ms
step:1120/2315 train_time:67559ms step_avg:60.32ms
step:1121/2315 train_time:67620ms step_avg:60.32ms
step:1122/2315 train_time:67681ms step_avg:60.32ms
step:1123/2315 train_time:67742ms step_avg:60.32ms
step:1124/2315 train_time:67803ms step_avg:60.32ms
step:1125/2315 train_time:67864ms step_avg:60.32ms
step:1126/2315 train_time:67924ms step_avg:60.32ms
step:1127/2315 train_time:67986ms step_avg:60.32ms
step:1128/2315 train_time:68047ms step_avg:60.33ms
step:1129/2315 train_time:68108ms step_avg:60.33ms
step:1130/2315 train_time:68169ms step_avg:60.33ms
step:1131/2315 train_time:68230ms step_avg:60.33ms
step:1132/2315 train_time:68291ms step_avg:60.33ms
step:1133/2315 train_time:68352ms step_avg:60.33ms
step:1134/2315 train_time:68413ms step_avg:60.33ms
step:1135/2315 train_time:68475ms step_avg:60.33ms
step:1136/2315 train_time:68535ms step_avg:60.33ms
step:1137/2315 train_time:68596ms step_avg:60.33ms
step:1138/2315 train_time:68656ms step_avg:60.33ms
step:1139/2315 train_time:68716ms step_avg:60.33ms
step:1140/2315 train_time:68777ms step_avg:60.33ms
step:1141/2315 train_time:68837ms step_avg:60.33ms
step:1142/2315 train_time:68898ms step_avg:60.33ms
step:1143/2315 train_time:68959ms step_avg:60.33ms
step:1144/2315 train_time:69020ms step_avg:60.33ms
step:1145/2315 train_time:69082ms step_avg:60.33ms
step:1146/2315 train_time:69143ms step_avg:60.33ms
step:1147/2315 train_time:69204ms step_avg:60.33ms
step:1148/2315 train_time:69265ms step_avg:60.34ms
step:1149/2315 train_time:69325ms step_avg:60.34ms
step:1150/2315 train_time:69386ms step_avg:60.34ms
step:1151/2315 train_time:69447ms step_avg:60.34ms
step:1152/2315 train_time:69508ms step_avg:60.34ms
step:1153/2315 train_time:69570ms step_avg:60.34ms
step:1154/2315 train_time:69631ms step_avg:60.34ms
step:1155/2315 train_time:69692ms step_avg:60.34ms
step:1156/2315 train_time:69753ms step_avg:60.34ms
step:1157/2315 train_time:69813ms step_avg:60.34ms
step:1158/2315 train_time:69874ms step_avg:60.34ms
step:1159/2315 train_time:69935ms step_avg:60.34ms
step:1160/2315 train_time:69996ms step_avg:60.34ms
step:1161/2315 train_time:70056ms step_avg:60.34ms
step:1162/2315 train_time:70117ms step_avg:60.34ms
step:1163/2315 train_time:70178ms step_avg:60.34ms
step:1164/2315 train_time:70238ms step_avg:60.34ms
step:1165/2315 train_time:70299ms step_avg:60.34ms
step:1166/2315 train_time:70360ms step_avg:60.34ms
step:1167/2315 train_time:70421ms step_avg:60.34ms
step:1168/2315 train_time:70482ms step_avg:60.34ms
step:1169/2315 train_time:70544ms step_avg:60.35ms
step:1170/2315 train_time:70605ms step_avg:60.35ms
step:1171/2315 train_time:70666ms step_avg:60.35ms
step:1172/2315 train_time:70726ms step_avg:60.35ms
step:1173/2315 train_time:70787ms step_avg:60.35ms
step:1174/2315 train_time:70848ms step_avg:60.35ms
step:1175/2315 train_time:70910ms step_avg:60.35ms
step:1176/2315 train_time:70971ms step_avg:60.35ms
step:1177/2315 train_time:71032ms step_avg:60.35ms
step:1178/2315 train_time:71093ms step_avg:60.35ms
step:1179/2315 train_time:71154ms step_avg:60.35ms
step:1180/2315 train_time:71215ms step_avg:60.35ms
step:1181/2315 train_time:71276ms step_avg:60.35ms
step:1182/2315 train_time:71336ms step_avg:60.35ms
step:1183/2315 train_time:71396ms step_avg:60.35ms
step:1184/2315 train_time:71456ms step_avg:60.35ms
step:1185/2315 train_time:71517ms step_avg:60.35ms
step:1186/2315 train_time:71577ms step_avg:60.35ms
step:1187/2315 train_time:71638ms step_avg:60.35ms
step:1188/2315 train_time:71699ms step_avg:60.35ms
step:1189/2315 train_time:71760ms step_avg:60.35ms
step:1190/2315 train_time:71821ms step_avg:60.35ms
step:1191/2315 train_time:71883ms step_avg:60.36ms
step:1192/2315 train_time:71943ms step_avg:60.35ms
step:1193/2315 train_time:72004ms step_avg:60.36ms
step:1194/2315 train_time:72065ms step_avg:60.36ms
step:1195/2315 train_time:72126ms step_avg:60.36ms
step:1196/2315 train_time:72187ms step_avg:60.36ms
step:1197/2315 train_time:72248ms step_avg:60.36ms
step:1198/2315 train_time:72309ms step_avg:60.36ms
step:1199/2315 train_time:72370ms step_avg:60.36ms
step:1200/2315 train_time:72431ms step_avg:60.36ms
step:1201/2315 train_time:72492ms step_avg:60.36ms
step:1202/2315 train_time:72553ms step_avg:60.36ms
step:1203/2315 train_time:72614ms step_avg:60.36ms
step:1204/2315 train_time:72674ms step_avg:60.36ms
step:1205/2315 train_time:72736ms step_avg:60.36ms
step:1206/2315 train_time:72796ms step_avg:60.36ms
step:1207/2315 train_time:72857ms step_avg:60.36ms
step:1208/2315 train_time:72917ms step_avg:60.36ms
step:1209/2315 train_time:72977ms step_avg:60.36ms
step:1210/2315 train_time:73038ms step_avg:60.36ms
step:1211/2315 train_time:73099ms step_avg:60.36ms
step:1212/2315 train_time:73160ms step_avg:60.36ms
step:1213/2315 train_time:73221ms step_avg:60.36ms
step:1214/2315 train_time:73282ms step_avg:60.36ms
step:1215/2315 train_time:73343ms step_avg:60.36ms
step:1216/2315 train_time:73404ms step_avg:60.37ms
step:1217/2315 train_time:73466ms step_avg:60.37ms
step:1218/2315 train_time:73526ms step_avg:60.37ms
step:1219/2315 train_time:73587ms step_avg:60.37ms
step:1220/2315 train_time:73648ms step_avg:60.37ms
step:1221/2315 train_time:73709ms step_avg:60.37ms
step:1222/2315 train_time:73769ms step_avg:60.37ms
step:1223/2315 train_time:73830ms step_avg:60.37ms
step:1224/2315 train_time:73891ms step_avg:60.37ms
step:1225/2315 train_time:73952ms step_avg:60.37ms
step:1226/2315 train_time:74013ms step_avg:60.37ms
step:1227/2315 train_time:74074ms step_avg:60.37ms
step:1228/2315 train_time:74134ms step_avg:60.37ms
step:1229/2315 train_time:74196ms step_avg:60.37ms
step:1230/2315 train_time:74257ms step_avg:60.37ms
step:1231/2315 train_time:74317ms step_avg:60.37ms
step:1232/2315 train_time:74377ms step_avg:60.37ms
step:1233/2315 train_time:74438ms step_avg:60.37ms
step:1234/2315 train_time:74498ms step_avg:60.37ms
step:1235/2315 train_time:74560ms step_avg:60.37ms
step:1236/2315 train_time:74621ms step_avg:60.37ms
step:1237/2315 train_time:74682ms step_avg:60.37ms
step:1238/2315 train_time:74742ms step_avg:60.37ms
step:1239/2315 train_time:74803ms step_avg:60.37ms
step:1240/2315 train_time:74864ms step_avg:60.37ms
step:1241/2315 train_time:74926ms step_avg:60.38ms
step:1242/2315 train_time:74987ms step_avg:60.38ms
step:1243/2315 train_time:75049ms step_avg:60.38ms
step:1244/2315 train_time:75109ms step_avg:60.38ms
step:1245/2315 train_time:75170ms step_avg:60.38ms
step:1246/2315 train_time:75231ms step_avg:60.38ms
step:1247/2315 train_time:75292ms step_avg:60.38ms
step:1248/2315 train_time:75353ms step_avg:60.38ms
step:1249/2315 train_time:75414ms step_avg:60.38ms
step:1250/2315 train_time:75474ms step_avg:60.38ms
step:1250/2315 val_loss:3.5194 train_time:75537ms step_avg:60.43ms
step:1251/2315 train_time:75556ms step_avg:60.40ms
step:1252/2315 train_time:75598ms step_avg:60.38ms
step:1253/2315 train_time:75664ms step_avg:60.39ms
step:1254/2315 train_time:75729ms step_avg:60.39ms
step:1255/2315 train_time:75791ms step_avg:60.39ms
step:1256/2315 train_time:75851ms step_avg:60.39ms
step:1257/2315 train_time:75912ms step_avg:60.39ms
step:1258/2315 train_time:75971ms step_avg:60.39ms
step:1259/2315 train_time:76032ms step_avg:60.39ms
step:1260/2315 train_time:76092ms step_avg:60.39ms
step:1261/2315 train_time:76152ms step_avg:60.39ms
step:1262/2315 train_time:76212ms step_avg:60.39ms
step:1263/2315 train_time:76271ms step_avg:60.39ms
step:1264/2315 train_time:76331ms step_avg:60.39ms
step:1265/2315 train_time:76391ms step_avg:60.39ms
step:1266/2315 train_time:76451ms step_avg:60.39ms
step:1267/2315 train_time:76513ms step_avg:60.39ms
step:1268/2315 train_time:76576ms step_avg:60.39ms
step:1269/2315 train_time:76639ms step_avg:60.39ms
step:1270/2315 train_time:76700ms step_avg:60.39ms
step:1271/2315 train_time:76762ms step_avg:60.39ms
step:1272/2315 train_time:76823ms step_avg:60.40ms
step:1273/2315 train_time:76883ms step_avg:60.40ms
step:1274/2315 train_time:76945ms step_avg:60.40ms
step:1275/2315 train_time:77005ms step_avg:60.40ms
step:1276/2315 train_time:77065ms step_avg:60.40ms
step:1277/2315 train_time:77126ms step_avg:60.40ms
step:1278/2315 train_time:77187ms step_avg:60.40ms
step:1279/2315 train_time:77248ms step_avg:60.40ms
step:1280/2315 train_time:77308ms step_avg:60.40ms
step:1281/2315 train_time:77368ms step_avg:60.40ms
step:1282/2315 train_time:77428ms step_avg:60.40ms
step:1283/2315 train_time:77488ms step_avg:60.40ms
step:1284/2315 train_time:77550ms step_avg:60.40ms
step:1285/2315 train_time:77612ms step_avg:60.40ms
step:1286/2315 train_time:77673ms step_avg:60.40ms
step:1287/2315 train_time:77735ms step_avg:60.40ms
step:1288/2315 train_time:77796ms step_avg:60.40ms
step:1289/2315 train_time:77857ms step_avg:60.40ms
step:1290/2315 train_time:77918ms step_avg:60.40ms
step:1291/2315 train_time:77979ms step_avg:60.40ms
step:1292/2315 train_time:78040ms step_avg:60.40ms
step:1293/2315 train_time:78100ms step_avg:60.40ms
step:1294/2315 train_time:78161ms step_avg:60.40ms
step:1295/2315 train_time:78222ms step_avg:60.40ms
step:1296/2315 train_time:78283ms step_avg:60.40ms
step:1297/2315 train_time:78344ms step_avg:60.40ms
step:1298/2315 train_time:78405ms step_avg:60.40ms
step:1299/2315 train_time:78465ms step_avg:60.40ms
step:1300/2315 train_time:78526ms step_avg:60.40ms
step:1301/2315 train_time:78587ms step_avg:60.41ms
step:1302/2315 train_time:78648ms step_avg:60.41ms
step:1303/2315 train_time:78708ms step_avg:60.41ms
step:1304/2315 train_time:78769ms step_avg:60.41ms
step:1305/2315 train_time:78830ms step_avg:60.41ms
step:1306/2315 train_time:78891ms step_avg:60.41ms
step:1307/2315 train_time:78952ms step_avg:60.41ms
step:1308/2315 train_time:79014ms step_avg:60.41ms
step:1309/2315 train_time:79075ms step_avg:60.41ms
step:1310/2315 train_time:79136ms step_avg:60.41ms
step:1311/2315 train_time:79197ms step_avg:60.41ms
step:1312/2315 train_time:79258ms step_avg:60.41ms
step:1313/2315 train_time:79319ms step_avg:60.41ms
step:1314/2315 train_time:79380ms step_avg:60.41ms
step:1315/2315 train_time:79441ms step_avg:60.41ms
step:1316/2315 train_time:79502ms step_avg:60.41ms
step:1317/2315 train_time:79563ms step_avg:60.41ms
step:1318/2315 train_time:79624ms step_avg:60.41ms
step:1319/2315 train_time:79685ms step_avg:60.41ms
step:1320/2315 train_time:79746ms step_avg:60.41ms
step:1321/2315 train_time:79806ms step_avg:60.41ms
step:1322/2315 train_time:79867ms step_avg:60.41ms
step:1323/2315 train_time:79928ms step_avg:60.41ms
step:1324/2315 train_time:79988ms step_avg:60.41ms
step:1325/2315 train_time:80049ms step_avg:60.41ms
step:1326/2315 train_time:80109ms step_avg:60.41ms
step:1327/2315 train_time:80170ms step_avg:60.41ms
step:1328/2315 train_time:80230ms step_avg:60.41ms
step:1329/2315 train_time:80292ms step_avg:60.42ms
step:1330/2315 train_time:80353ms step_avg:60.42ms
step:1331/2315 train_time:80414ms step_avg:60.42ms
step:1332/2315 train_time:80474ms step_avg:60.42ms
step:1333/2315 train_time:80535ms step_avg:60.42ms
step:1334/2315 train_time:80595ms step_avg:60.42ms
step:1335/2315 train_time:80657ms step_avg:60.42ms
step:1336/2315 train_time:80717ms step_avg:60.42ms
step:1337/2315 train_time:80778ms step_avg:60.42ms
step:1338/2315 train_time:80840ms step_avg:60.42ms
step:1339/2315 train_time:80901ms step_avg:60.42ms
step:1340/2315 train_time:80961ms step_avg:60.42ms
step:1341/2315 train_time:81022ms step_avg:60.42ms
step:1342/2315 train_time:81083ms step_avg:60.42ms
step:1343/2315 train_time:81145ms step_avg:60.42ms
step:1344/2315 train_time:81205ms step_avg:60.42ms
step:1345/2315 train_time:81267ms step_avg:60.42ms
step:1346/2315 train_time:81327ms step_avg:60.42ms
step:1347/2315 train_time:81388ms step_avg:60.42ms
step:1348/2315 train_time:81448ms step_avg:60.42ms
step:1349/2315 train_time:81509ms step_avg:60.42ms
step:1350/2315 train_time:81569ms step_avg:60.42ms
step:1351/2315 train_time:81630ms step_avg:60.42ms
step:1352/2315 train_time:81690ms step_avg:60.42ms
step:1353/2315 train_time:81752ms step_avg:60.42ms
step:1354/2315 train_time:81813ms step_avg:60.42ms
step:1355/2315 train_time:81874ms step_avg:60.42ms
step:1356/2315 train_time:81935ms step_avg:60.42ms
step:1357/2315 train_time:81996ms step_avg:60.42ms
step:1358/2315 train_time:82057ms step_avg:60.42ms
step:1359/2315 train_time:82119ms step_avg:60.43ms
step:1360/2315 train_time:82180ms step_avg:60.43ms
step:1361/2315 train_time:82241ms step_avg:60.43ms
step:1362/2315 train_time:82302ms step_avg:60.43ms
step:1363/2315 train_time:82363ms step_avg:60.43ms
step:1364/2315 train_time:82424ms step_avg:60.43ms
step:1365/2315 train_time:82485ms step_avg:60.43ms
step:1366/2315 train_time:82545ms step_avg:60.43ms
step:1367/2315 train_time:82606ms step_avg:60.43ms
step:1368/2315 train_time:82667ms step_avg:60.43ms
step:1369/2315 train_time:82728ms step_avg:60.43ms
step:1370/2315 train_time:82788ms step_avg:60.43ms
step:1371/2315 train_time:82848ms step_avg:60.43ms
step:1372/2315 train_time:82909ms step_avg:60.43ms
step:1373/2315 train_time:82970ms step_avg:60.43ms
step:1374/2315 train_time:83031ms step_avg:60.43ms
step:1375/2315 train_time:83092ms step_avg:60.43ms
step:1376/2315 train_time:83153ms step_avg:60.43ms
step:1377/2315 train_time:83214ms step_avg:60.43ms
step:1378/2315 train_time:83275ms step_avg:60.43ms
step:1379/2315 train_time:83336ms step_avg:60.43ms
step:1380/2315 train_time:83397ms step_avg:60.43ms
step:1381/2315 train_time:83458ms step_avg:60.43ms
step:1382/2315 train_time:83518ms step_avg:60.43ms
step:1383/2315 train_time:83579ms step_avg:60.43ms
step:1384/2315 train_time:83641ms step_avg:60.43ms
step:1385/2315 train_time:83702ms step_avg:60.43ms
step:1386/2315 train_time:83763ms step_avg:60.43ms
step:1387/2315 train_time:83824ms step_avg:60.44ms
step:1388/2315 train_time:83885ms step_avg:60.44ms
step:1389/2315 train_time:83946ms step_avg:60.44ms
step:1390/2315 train_time:84007ms step_avg:60.44ms
step:1391/2315 train_time:84068ms step_avg:60.44ms
step:1392/2315 train_time:84128ms step_avg:60.44ms
step:1393/2315 train_time:84189ms step_avg:60.44ms
step:1394/2315 train_time:84249ms step_avg:60.44ms
step:1395/2315 train_time:84310ms step_avg:60.44ms
step:1396/2315 train_time:84371ms step_avg:60.44ms
step:1397/2315 train_time:84432ms step_avg:60.44ms
step:1398/2315 train_time:84494ms step_avg:60.44ms
step:1399/2315 train_time:84555ms step_avg:60.44ms
step:1400/2315 train_time:84616ms step_avg:60.44ms
step:1401/2315 train_time:84677ms step_avg:60.44ms
step:1402/2315 train_time:84738ms step_avg:60.44ms
step:1403/2315 train_time:84799ms step_avg:60.44ms
step:1404/2315 train_time:84860ms step_avg:60.44ms
step:1405/2315 train_time:84921ms step_avg:60.44ms
step:1406/2315 train_time:84982ms step_avg:60.44ms
step:1407/2315 train_time:85044ms step_avg:60.44ms
step:1408/2315 train_time:85104ms step_avg:60.44ms
step:1409/2315 train_time:85165ms step_avg:60.44ms
step:1410/2315 train_time:85226ms step_avg:60.44ms
step:1411/2315 train_time:85287ms step_avg:60.44ms
step:1412/2315 train_time:85347ms step_avg:60.44ms
step:1413/2315 train_time:85409ms step_avg:60.44ms
step:1414/2315 train_time:85469ms step_avg:60.44ms
step:1415/2315 train_time:85529ms step_avg:60.44ms
step:1416/2315 train_time:85590ms step_avg:60.44ms
step:1417/2315 train_time:85651ms step_avg:60.45ms
step:1418/2315 train_time:85712ms step_avg:60.45ms
step:1419/2315 train_time:85772ms step_avg:60.45ms
step:1420/2315 train_time:85834ms step_avg:60.45ms
step:1421/2315 train_time:85895ms step_avg:60.45ms
step:1422/2315 train_time:85956ms step_avg:60.45ms
step:1423/2315 train_time:86016ms step_avg:60.45ms
step:1424/2315 train_time:86077ms step_avg:60.45ms
step:1425/2315 train_time:86138ms step_avg:60.45ms
step:1426/2315 train_time:86199ms step_avg:60.45ms
step:1427/2315 train_time:86261ms step_avg:60.45ms
step:1428/2315 train_time:86322ms step_avg:60.45ms
step:1429/2315 train_time:86384ms step_avg:60.45ms
step:1430/2315 train_time:86445ms step_avg:60.45ms
step:1431/2315 train_time:86505ms step_avg:60.45ms
step:1432/2315 train_time:86566ms step_avg:60.45ms
step:1433/2315 train_time:86627ms step_avg:60.45ms
step:1434/2315 train_time:86688ms step_avg:60.45ms
step:1435/2315 train_time:86748ms step_avg:60.45ms
step:1436/2315 train_time:86808ms step_avg:60.45ms
step:1437/2315 train_time:86869ms step_avg:60.45ms
step:1438/2315 train_time:86929ms step_avg:60.45ms
step:1439/2315 train_time:86989ms step_avg:60.45ms
step:1440/2315 train_time:87050ms step_avg:60.45ms
step:1441/2315 train_time:87111ms step_avg:60.45ms
step:1442/2315 train_time:87173ms step_avg:60.45ms
step:1443/2315 train_time:87234ms step_avg:60.45ms
step:1444/2315 train_time:87296ms step_avg:60.45ms
step:1445/2315 train_time:87356ms step_avg:60.45ms
step:1446/2315 train_time:87417ms step_avg:60.45ms
step:1447/2315 train_time:87478ms step_avg:60.45ms
step:1448/2315 train_time:87539ms step_avg:60.46ms
step:1449/2315 train_time:87600ms step_avg:60.46ms
step:1450/2315 train_time:87660ms step_avg:60.46ms
step:1451/2315 train_time:87722ms step_avg:60.46ms
step:1452/2315 train_time:87782ms step_avg:60.46ms
step:1453/2315 train_time:87844ms step_avg:60.46ms
step:1454/2315 train_time:87905ms step_avg:60.46ms
step:1455/2315 train_time:87966ms step_avg:60.46ms
step:1456/2315 train_time:88026ms step_avg:60.46ms
step:1457/2315 train_time:88086ms step_avg:60.46ms
step:1458/2315 train_time:88147ms step_avg:60.46ms
step:1459/2315 train_time:88208ms step_avg:60.46ms
step:1460/2315 train_time:88268ms step_avg:60.46ms
step:1461/2315 train_time:88329ms step_avg:60.46ms
step:1462/2315 train_time:88389ms step_avg:60.46ms
step:1463/2315 train_time:88450ms step_avg:60.46ms
step:1464/2315 train_time:88512ms step_avg:60.46ms
step:1465/2315 train_time:88572ms step_avg:60.46ms
step:1466/2315 train_time:88634ms step_avg:60.46ms
step:1467/2315 train_time:88695ms step_avg:60.46ms
step:1468/2315 train_time:88755ms step_avg:60.46ms
step:1469/2315 train_time:88816ms step_avg:60.46ms
step:1470/2315 train_time:88877ms step_avg:60.46ms
step:1471/2315 train_time:88938ms step_avg:60.46ms
step:1472/2315 train_time:88999ms step_avg:60.46ms
step:1473/2315 train_time:89060ms step_avg:60.46ms
step:1474/2315 train_time:89121ms step_avg:60.46ms
step:1475/2315 train_time:89182ms step_avg:60.46ms
step:1476/2315 train_time:89243ms step_avg:60.46ms
step:1477/2315 train_time:89304ms step_avg:60.46ms
step:1478/2315 train_time:89365ms step_avg:60.46ms
step:1479/2315 train_time:89427ms step_avg:60.46ms
step:1480/2315 train_time:89488ms step_avg:60.46ms
step:1481/2315 train_time:89548ms step_avg:60.46ms
step:1482/2315 train_time:89608ms step_avg:60.46ms
step:1483/2315 train_time:89668ms step_avg:60.46ms
step:1484/2315 train_time:89729ms step_avg:60.46ms
step:1485/2315 train_time:89789ms step_avg:60.46ms
step:1486/2315 train_time:89850ms step_avg:60.46ms
step:1487/2315 train_time:89911ms step_avg:60.46ms
step:1488/2315 train_time:89972ms step_avg:60.47ms
step:1489/2315 train_time:90033ms step_avg:60.47ms
step:1490/2315 train_time:90094ms step_avg:60.47ms
step:1491/2315 train_time:90155ms step_avg:60.47ms
step:1492/2315 train_time:90215ms step_avg:60.47ms
step:1493/2315 train_time:90276ms step_avg:60.47ms
step:1494/2315 train_time:90338ms step_avg:60.47ms
step:1495/2315 train_time:90399ms step_avg:60.47ms
step:1496/2315 train_time:90461ms step_avg:60.47ms
step:1497/2315 train_time:90522ms step_avg:60.47ms
step:1498/2315 train_time:90582ms step_avg:60.47ms
step:1499/2315 train_time:90644ms step_avg:60.47ms
step:1500/2315 train_time:90705ms step_avg:60.47ms
step:1500/2315 val_loss:3.4550 train_time:90767ms step_avg:60.51ms
step:1501/2315 train_time:90787ms step_avg:60.48ms
step:1502/2315 train_time:90828ms step_avg:60.47ms
step:1503/2315 train_time:90894ms step_avg:60.48ms
step:1504/2315 train_time:90960ms step_avg:60.48ms
step:1505/2315 train_time:91021ms step_avg:60.48ms
step:1506/2315 train_time:91081ms step_avg:60.48ms
step:1507/2315 train_time:91141ms step_avg:60.48ms
step:1508/2315 train_time:91201ms step_avg:60.48ms
step:1509/2315 train_time:91261ms step_avg:60.48ms
step:1510/2315 train_time:91321ms step_avg:60.48ms
step:1511/2315 train_time:91381ms step_avg:60.48ms
step:1512/2315 train_time:91442ms step_avg:60.48ms
step:1513/2315 train_time:91501ms step_avg:60.48ms
step:1514/2315 train_time:91561ms step_avg:60.48ms
step:1515/2315 train_time:91621ms step_avg:60.48ms
step:1516/2315 train_time:91682ms step_avg:60.48ms
step:1517/2315 train_time:91744ms step_avg:60.48ms
step:1518/2315 train_time:91807ms step_avg:60.48ms
step:1519/2315 train_time:91870ms step_avg:60.48ms
step:1520/2315 train_time:91933ms step_avg:60.48ms
step:1521/2315 train_time:91994ms step_avg:60.48ms
step:1522/2315 train_time:92055ms step_avg:60.48ms
step:1523/2315 train_time:92117ms step_avg:60.48ms
step:1524/2315 train_time:92178ms step_avg:60.48ms
step:1525/2315 train_time:92239ms step_avg:60.48ms
step:1526/2315 train_time:92299ms step_avg:60.48ms
step:1527/2315 train_time:92360ms step_avg:60.48ms
step:1528/2315 train_time:92420ms step_avg:60.48ms
step:1529/2315 train_time:92481ms step_avg:60.48ms
step:1530/2315 train_time:92541ms step_avg:60.48ms
step:1531/2315 train_time:92602ms step_avg:60.48ms
step:1532/2315 train_time:92662ms step_avg:60.48ms
step:1533/2315 train_time:92724ms step_avg:60.49ms
step:1534/2315 train_time:92786ms step_avg:60.49ms
step:1535/2315 train_time:92849ms step_avg:60.49ms
step:1536/2315 train_time:92910ms step_avg:60.49ms
step:1537/2315 train_time:92971ms step_avg:60.49ms
step:1538/2315 train_time:93033ms step_avg:60.49ms
step:1539/2315 train_time:93094ms step_avg:60.49ms
step:1540/2315 train_time:93156ms step_avg:60.49ms
step:1541/2315 train_time:93217ms step_avg:60.49ms
step:1542/2315 train_time:93278ms step_avg:60.49ms
step:1543/2315 train_time:93339ms step_avg:60.49ms
step:1544/2315 train_time:93400ms step_avg:60.49ms
step:1545/2315 train_time:93461ms step_avg:60.49ms
step:1546/2315 train_time:93522ms step_avg:60.49ms
step:1547/2315 train_time:93582ms step_avg:60.49ms
step:1548/2315 train_time:93643ms step_avg:60.49ms
step:1549/2315 train_time:93704ms step_avg:60.49ms
step:1550/2315 train_time:93765ms step_avg:60.49ms
step:1551/2315 train_time:93828ms step_avg:60.50ms
step:1552/2315 train_time:93889ms step_avg:60.50ms
step:1553/2315 train_time:93951ms step_avg:60.50ms
step:1554/2315 train_time:94012ms step_avg:60.50ms
step:1555/2315 train_time:94074ms step_avg:60.50ms
step:1556/2315 train_time:94135ms step_avg:60.50ms
step:1557/2315 train_time:94197ms step_avg:60.50ms
step:1558/2315 train_time:94258ms step_avg:60.50ms
step:1559/2315 train_time:94319ms step_avg:60.50ms
step:1560/2315 train_time:94381ms step_avg:60.50ms
step:1561/2315 train_time:94441ms step_avg:60.50ms
step:1562/2315 train_time:94502ms step_avg:60.50ms
step:1563/2315 train_time:94562ms step_avg:60.50ms
step:1564/2315 train_time:94624ms step_avg:60.50ms
step:1565/2315 train_time:94684ms step_avg:60.50ms
step:1566/2315 train_time:94745ms step_avg:60.50ms
step:1567/2315 train_time:94807ms step_avg:60.50ms
step:1568/2315 train_time:94869ms step_avg:60.50ms
step:1569/2315 train_time:94931ms step_avg:60.50ms
step:1570/2315 train_time:94992ms step_avg:60.50ms
step:1571/2315 train_time:95054ms step_avg:60.51ms
step:1572/2315 train_time:95115ms step_avg:60.51ms
step:1573/2315 train_time:95177ms step_avg:60.51ms
step:1574/2315 train_time:95238ms step_avg:60.51ms
step:1575/2315 train_time:95299ms step_avg:60.51ms
step:1576/2315 train_time:95360ms step_avg:60.51ms
step:1577/2315 train_time:95421ms step_avg:60.51ms
step:1578/2315 train_time:95482ms step_avg:60.51ms
step:1579/2315 train_time:95543ms step_avg:60.51ms
step:1580/2315 train_time:95604ms step_avg:60.51ms
step:1581/2315 train_time:95665ms step_avg:60.51ms
step:1582/2315 train_time:95726ms step_avg:60.51ms
step:1583/2315 train_time:95787ms step_avg:60.51ms
step:1584/2315 train_time:95848ms step_avg:60.51ms
step:1585/2315 train_time:95909ms step_avg:60.51ms
step:1586/2315 train_time:95971ms step_avg:60.51ms
step:1587/2315 train_time:96032ms step_avg:60.51ms
step:1588/2315 train_time:96093ms step_avg:60.51ms
step:1589/2315 train_time:96155ms step_avg:60.51ms
step:1590/2315 train_time:96216ms step_avg:60.51ms
step:1591/2315 train_time:96278ms step_avg:60.51ms
step:1592/2315 train_time:96338ms step_avg:60.51ms
step:1593/2315 train_time:96400ms step_avg:60.51ms
step:1594/2315 train_time:96461ms step_avg:60.52ms
step:1595/2315 train_time:96522ms step_avg:60.52ms
step:1596/2315 train_time:96583ms step_avg:60.52ms
step:1597/2315 train_time:96644ms step_avg:60.52ms
step:1598/2315 train_time:96705ms step_avg:60.52ms
step:1599/2315 train_time:96766ms step_avg:60.52ms
step:1600/2315 train_time:96827ms step_avg:60.52ms
step:1601/2315 train_time:96889ms step_avg:60.52ms
step:1602/2315 train_time:96950ms step_avg:60.52ms
step:1603/2315 train_time:97011ms step_avg:60.52ms
step:1604/2315 train_time:97072ms step_avg:60.52ms
step:1605/2315 train_time:97135ms step_avg:60.52ms
step:1606/2315 train_time:97196ms step_avg:60.52ms
step:1607/2315 train_time:97257ms step_avg:60.52ms
step:1608/2315 train_time:97318ms step_avg:60.52ms
step:1609/2315 train_time:97379ms step_avg:60.52ms
step:1610/2315 train_time:97440ms step_avg:60.52ms
step:1611/2315 train_time:97501ms step_avg:60.52ms
step:1612/2315 train_time:97562ms step_avg:60.52ms
step:1613/2315 train_time:97624ms step_avg:60.52ms
step:1614/2315 train_time:97685ms step_avg:60.52ms
step:1615/2315 train_time:97746ms step_avg:60.52ms
step:1616/2315 train_time:97807ms step_avg:60.52ms
step:1617/2315 train_time:97868ms step_avg:60.52ms
step:1618/2315 train_time:97930ms step_avg:60.53ms
step:1619/2315 train_time:97991ms step_avg:60.53ms
step:1620/2315 train_time:98052ms step_avg:60.53ms
step:1621/2315 train_time:98114ms step_avg:60.53ms
step:1622/2315 train_time:98174ms step_avg:60.53ms
step:1623/2315 train_time:98236ms step_avg:60.53ms
step:1624/2315 train_time:98297ms step_avg:60.53ms
step:1625/2315 train_time:98358ms step_avg:60.53ms
step:1626/2315 train_time:98420ms step_avg:60.53ms
step:1627/2315 train_time:98481ms step_avg:60.53ms
step:1628/2315 train_time:98542ms step_avg:60.53ms
step:1629/2315 train_time:98603ms step_avg:60.53ms
step:1630/2315 train_time:98664ms step_avg:60.53ms
step:1631/2315 train_time:98725ms step_avg:60.53ms
step:1632/2315 train_time:98786ms step_avg:60.53ms
step:1633/2315 train_time:98848ms step_avg:60.53ms
step:1634/2315 train_time:98909ms step_avg:60.53ms
step:1635/2315 train_time:98970ms step_avg:60.53ms
step:1636/2315 train_time:99031ms step_avg:60.53ms
step:1637/2315 train_time:99092ms step_avg:60.53ms
step:1638/2315 train_time:99153ms step_avg:60.53ms
step:1639/2315 train_time:99214ms step_avg:60.53ms
step:1640/2315 train_time:99276ms step_avg:60.53ms
step:1641/2315 train_time:99337ms step_avg:60.53ms
step:1642/2315 train_time:99399ms step_avg:60.54ms
step:1643/2315 train_time:99460ms step_avg:60.54ms
step:1644/2315 train_time:99521ms step_avg:60.54ms
step:1645/2315 train_time:99583ms step_avg:60.54ms
step:1646/2315 train_time:99643ms step_avg:60.54ms
step:1647/2315 train_time:99704ms step_avg:60.54ms
step:1648/2315 train_time:99765ms step_avg:60.54ms
step:1649/2315 train_time:99826ms step_avg:60.54ms
step:1650/2315 train_time:99888ms step_avg:60.54ms
step:1651/2315 train_time:99950ms step_avg:60.54ms
step:1652/2315 train_time:100010ms step_avg:60.54ms
step:1653/2315 train_time:100072ms step_avg:60.54ms
step:1654/2315 train_time:100133ms step_avg:60.54ms
step:1655/2315 train_time:100195ms step_avg:60.54ms
step:1656/2315 train_time:100256ms step_avg:60.54ms
step:1657/2315 train_time:100318ms step_avg:60.54ms
step:1658/2315 train_time:100379ms step_avg:60.54ms
step:1659/2315 train_time:100439ms step_avg:60.54ms
step:1660/2315 train_time:100500ms step_avg:60.54ms
step:1661/2315 train_time:100562ms step_avg:60.54ms
step:1662/2315 train_time:100623ms step_avg:60.54ms
step:1663/2315 train_time:100685ms step_avg:60.54ms
step:1664/2315 train_time:100746ms step_avg:60.54ms
step:1665/2315 train_time:100807ms step_avg:60.54ms
step:1666/2315 train_time:100868ms step_avg:60.54ms
step:1667/2315 train_time:100930ms step_avg:60.55ms
step:1668/2315 train_time:100991ms step_avg:60.55ms
step:1669/2315 train_time:101052ms step_avg:60.55ms
step:1670/2315 train_time:101113ms step_avg:60.55ms
step:1671/2315 train_time:101174ms step_avg:60.55ms
step:1672/2315 train_time:101235ms step_avg:60.55ms
step:1673/2315 train_time:101296ms step_avg:60.55ms
step:1674/2315 train_time:101357ms step_avg:60.55ms
step:1675/2315 train_time:101419ms step_avg:60.55ms
step:1676/2315 train_time:101480ms step_avg:60.55ms
step:1677/2315 train_time:101541ms step_avg:60.55ms
step:1678/2315 train_time:101603ms step_avg:60.55ms
step:1679/2315 train_time:101664ms step_avg:60.55ms
step:1680/2315 train_time:101725ms step_avg:60.55ms
step:1681/2315 train_time:101786ms step_avg:60.55ms
step:1682/2315 train_time:101847ms step_avg:60.55ms
step:1683/2315 train_time:101908ms step_avg:60.55ms
step:1684/2315 train_time:101970ms step_avg:60.55ms
step:1685/2315 train_time:102032ms step_avg:60.55ms
step:1686/2315 train_time:102093ms step_avg:60.55ms
step:1687/2315 train_time:102154ms step_avg:60.55ms
step:1688/2315 train_time:102215ms step_avg:60.55ms
step:1689/2315 train_time:102276ms step_avg:60.55ms
step:1690/2315 train_time:102337ms step_avg:60.55ms
step:1691/2315 train_time:102398ms step_avg:60.55ms
step:1692/2315 train_time:102459ms step_avg:60.56ms
step:1693/2315 train_time:102520ms step_avg:60.56ms
step:1694/2315 train_time:102582ms step_avg:60.56ms
step:1695/2315 train_time:102643ms step_avg:60.56ms
step:1696/2315 train_time:102704ms step_avg:60.56ms
step:1697/2315 train_time:102765ms step_avg:60.56ms
step:1698/2315 train_time:102827ms step_avg:60.56ms
step:1699/2315 train_time:102888ms step_avg:60.56ms
step:1700/2315 train_time:102949ms step_avg:60.56ms
step:1701/2315 train_time:103011ms step_avg:60.56ms
step:1702/2315 train_time:103072ms step_avg:60.56ms
step:1703/2315 train_time:103134ms step_avg:60.56ms
step:1704/2315 train_time:103195ms step_avg:60.56ms
step:1705/2315 train_time:103256ms step_avg:60.56ms
step:1706/2315 train_time:103317ms step_avg:60.56ms
step:1707/2315 train_time:103379ms step_avg:60.56ms
step:1708/2315 train_time:103440ms step_avg:60.56ms
step:1709/2315 train_time:103501ms step_avg:60.56ms
step:1710/2315 train_time:103562ms step_avg:60.56ms
step:1711/2315 train_time:103624ms step_avg:60.56ms
step:1712/2315 train_time:103685ms step_avg:60.56ms
step:1713/2315 train_time:103746ms step_avg:60.56ms
step:1714/2315 train_time:103807ms step_avg:60.56ms
step:1715/2315 train_time:103868ms step_avg:60.56ms
step:1716/2315 train_time:103929ms step_avg:60.56ms
step:1717/2315 train_time:103990ms step_avg:60.56ms
step:1718/2315 train_time:104051ms step_avg:60.56ms
step:1719/2315 train_time:104112ms step_avg:60.57ms
step:1720/2315 train_time:104173ms step_avg:60.57ms
step:1721/2315 train_time:104235ms step_avg:60.57ms
step:1722/2315 train_time:104296ms step_avg:60.57ms
step:1723/2315 train_time:104357ms step_avg:60.57ms
step:1724/2315 train_time:104418ms step_avg:60.57ms
step:1725/2315 train_time:104480ms step_avg:60.57ms
step:1726/2315 train_time:104541ms step_avg:60.57ms
step:1727/2315 train_time:104602ms step_avg:60.57ms
step:1728/2315 train_time:104663ms step_avg:60.57ms
step:1729/2315 train_time:104725ms step_avg:60.57ms
step:1730/2315 train_time:104786ms step_avg:60.57ms
step:1731/2315 train_time:104847ms step_avg:60.57ms
step:1732/2315 train_time:104908ms step_avg:60.57ms
step:1733/2315 train_time:104969ms step_avg:60.57ms
step:1734/2315 train_time:105031ms step_avg:60.57ms
step:1735/2315 train_time:105092ms step_avg:60.57ms
step:1736/2315 train_time:105153ms step_avg:60.57ms
step:1737/2315 train_time:105214ms step_avg:60.57ms
step:1738/2315 train_time:105276ms step_avg:60.57ms
step:1739/2315 train_time:105337ms step_avg:60.57ms
step:1740/2315 train_time:105398ms step_avg:60.57ms
step:1741/2315 train_time:105460ms step_avg:60.57ms
step:1742/2315 train_time:105521ms step_avg:60.57ms
step:1743/2315 train_time:105582ms step_avg:60.57ms
step:1744/2315 train_time:105643ms step_avg:60.58ms
step:1745/2315 train_time:105704ms step_avg:60.58ms
step:1746/2315 train_time:105765ms step_avg:60.58ms
step:1747/2315 train_time:105826ms step_avg:60.58ms
step:1748/2315 train_time:105887ms step_avg:60.58ms
step:1749/2315 train_time:105949ms step_avg:60.58ms
step:1750/2315 train_time:106010ms step_avg:60.58ms
step:1750/2315 val_loss:3.3855 train_time:106072ms step_avg:60.61ms
step:1751/2315 train_time:106093ms step_avg:60.59ms
step:1752/2315 train_time:106135ms step_avg:60.58ms
step:1753/2315 train_time:106199ms step_avg:60.58ms
step:1754/2315 train_time:106262ms step_avg:60.58ms
step:1755/2315 train_time:106323ms step_avg:60.58ms
step:1756/2315 train_time:106384ms step_avg:60.58ms
step:1757/2315 train_time:106445ms step_avg:60.58ms
step:1758/2315 train_time:106506ms step_avg:60.58ms
step:1759/2315 train_time:106566ms step_avg:60.58ms
step:1760/2315 train_time:106627ms step_avg:60.58ms
step:1761/2315 train_time:106687ms step_avg:60.58ms
step:1762/2315 train_time:106748ms step_avg:60.58ms
step:1763/2315 train_time:106808ms step_avg:60.58ms
step:1764/2315 train_time:106869ms step_avg:60.58ms
step:1765/2315 train_time:106930ms step_avg:60.58ms
step:1766/2315 train_time:106991ms step_avg:60.58ms
step:1767/2315 train_time:107054ms step_avg:60.59ms
step:1768/2315 train_time:107115ms step_avg:60.59ms
step:1769/2315 train_time:107178ms step_avg:60.59ms
step:1770/2315 train_time:107239ms step_avg:60.59ms
step:1771/2315 train_time:107300ms step_avg:60.59ms
step:1772/2315 train_time:107361ms step_avg:60.59ms
step:1773/2315 train_time:107423ms step_avg:60.59ms
step:1774/2315 train_time:107484ms step_avg:60.59ms
step:1775/2315 train_time:107544ms step_avg:60.59ms
step:1776/2315 train_time:107605ms step_avg:60.59ms
step:1777/2315 train_time:107666ms step_avg:60.59ms
step:1778/2315 train_time:107727ms step_avg:60.59ms
step:1779/2315 train_time:107787ms step_avg:60.59ms
step:1780/2315 train_time:107848ms step_avg:60.59ms
step:1781/2315 train_time:107909ms step_avg:60.59ms
step:1782/2315 train_time:107970ms step_avg:60.59ms
step:1783/2315 train_time:108032ms step_avg:60.59ms
step:1784/2315 train_time:108094ms step_avg:60.59ms
step:1785/2315 train_time:108156ms step_avg:60.59ms
step:1786/2315 train_time:108217ms step_avg:60.59ms
step:1787/2315 train_time:108278ms step_avg:60.59ms
step:1788/2315 train_time:108339ms step_avg:60.59ms
step:1789/2315 train_time:108400ms step_avg:60.59ms
step:1790/2315 train_time:108461ms step_avg:60.59ms
step:1791/2315 train_time:108522ms step_avg:60.59ms
step:1792/2315 train_time:108584ms step_avg:60.59ms
step:1793/2315 train_time:108644ms step_avg:60.59ms
step:1794/2315 train_time:108706ms step_avg:60.59ms
step:1795/2315 train_time:108766ms step_avg:60.59ms
step:1796/2315 train_time:108827ms step_avg:60.59ms
step:1797/2315 train_time:108888ms step_avg:60.59ms
step:1798/2315 train_time:108949ms step_avg:60.59ms
step:1799/2315 train_time:109011ms step_avg:60.60ms
step:1800/2315 train_time:109072ms step_avg:60.60ms
step:1801/2315 train_time:109134ms step_avg:60.60ms
step:1802/2315 train_time:109195ms step_avg:60.60ms
step:1803/2315 train_time:109257ms step_avg:60.60ms
step:1804/2315 train_time:109318ms step_avg:60.60ms
step:1805/2315 train_time:109379ms step_avg:60.60ms
step:1806/2315 train_time:109440ms step_avg:60.60ms
step:1807/2315 train_time:109501ms step_avg:60.60ms
step:1808/2315 train_time:109562ms step_avg:60.60ms
step:1809/2315 train_time:109623ms step_avg:60.60ms
step:1810/2315 train_time:109684ms step_avg:60.60ms
step:1811/2315 train_time:109746ms step_avg:60.60ms
step:1812/2315 train_time:109806ms step_avg:60.60ms
step:1813/2315 train_time:109867ms step_avg:60.60ms
step:1814/2315 train_time:109928ms step_avg:60.60ms
step:1815/2315 train_time:109990ms step_avg:60.60ms
step:1816/2315 train_time:110052ms step_avg:60.60ms
step:1817/2315 train_time:110113ms step_avg:60.60ms
step:1818/2315 train_time:110175ms step_avg:60.60ms
step:1819/2315 train_time:110236ms step_avg:60.60ms
step:1820/2315 train_time:110297ms step_avg:60.60ms
step:1821/2315 train_time:110358ms step_avg:60.60ms
step:1822/2315 train_time:110419ms step_avg:60.60ms
step:1823/2315 train_time:110480ms step_avg:60.60ms
step:1824/2315 train_time:110542ms step_avg:60.60ms
step:1825/2315 train_time:110603ms step_avg:60.60ms
step:1826/2315 train_time:110664ms step_avg:60.60ms
step:1827/2315 train_time:110725ms step_avg:60.60ms
step:1828/2315 train_time:110786ms step_avg:60.61ms
step:1829/2315 train_time:110847ms step_avg:60.61ms
step:1830/2315 train_time:110908ms step_avg:60.61ms
step:1831/2315 train_time:110970ms step_avg:60.61ms
step:1832/2315 train_time:111031ms step_avg:60.61ms
step:1833/2315 train_time:111093ms step_avg:60.61ms
step:1834/2315 train_time:111155ms step_avg:60.61ms
step:1835/2315 train_time:111216ms step_avg:60.61ms
step:1836/2315 train_time:111277ms step_avg:60.61ms
step:1837/2315 train_time:111339ms step_avg:60.61ms
step:1838/2315 train_time:111400ms step_avg:60.61ms
step:1839/2315 train_time:111461ms step_avg:60.61ms
step:1840/2315 train_time:111522ms step_avg:60.61ms
step:1841/2315 train_time:111582ms step_avg:60.61ms
step:1842/2315 train_time:111643ms step_avg:60.61ms
step:1843/2315 train_time:111704ms step_avg:60.61ms
step:1844/2315 train_time:111766ms step_avg:60.61ms
step:1845/2315 train_time:111827ms step_avg:60.61ms
step:1846/2315 train_time:111887ms step_avg:60.61ms
step:1847/2315 train_time:111949ms step_avg:60.61ms
step:1848/2315 train_time:112010ms step_avg:60.61ms
step:1849/2315 train_time:112072ms step_avg:60.61ms
step:1850/2315 train_time:112133ms step_avg:60.61ms
step:1851/2315 train_time:112195ms step_avg:60.61ms
step:1852/2315 train_time:112256ms step_avg:60.61ms
step:1853/2315 train_time:112317ms step_avg:60.61ms
step:1854/2315 train_time:112378ms step_avg:60.61ms
step:1855/2315 train_time:112439ms step_avg:60.61ms
step:1856/2315 train_time:112500ms step_avg:60.61ms
step:1857/2315 train_time:112561ms step_avg:60.61ms
step:1858/2315 train_time:112622ms step_avg:60.61ms
step:1859/2315 train_time:112683ms step_avg:60.62ms
step:1860/2315 train_time:112744ms step_avg:60.62ms
step:1861/2315 train_time:112805ms step_avg:60.62ms
step:1862/2315 train_time:112866ms step_avg:60.62ms
step:1863/2315 train_time:112927ms step_avg:60.62ms
step:1864/2315 train_time:112989ms step_avg:60.62ms
step:1865/2315 train_time:113050ms step_avg:60.62ms
step:1866/2315 train_time:113111ms step_avg:60.62ms
step:1867/2315 train_time:113173ms step_avg:60.62ms
step:1868/2315 train_time:113234ms step_avg:60.62ms
step:1869/2315 train_time:113295ms step_avg:60.62ms
step:1870/2315 train_time:113357ms step_avg:60.62ms
step:1871/2315 train_time:113418ms step_avg:60.62ms
step:1872/2315 train_time:113479ms step_avg:60.62ms
step:1873/2315 train_time:113540ms step_avg:60.62ms
step:1874/2315 train_time:113601ms step_avg:60.62ms
step:1875/2315 train_time:113663ms step_avg:60.62ms
step:1876/2315 train_time:113724ms step_avg:60.62ms
step:1877/2315 train_time:113786ms step_avg:60.62ms
step:1878/2315 train_time:113846ms step_avg:60.62ms
step:1879/2315 train_time:113908ms step_avg:60.62ms
step:1880/2315 train_time:113969ms step_avg:60.62ms
step:1881/2315 train_time:114031ms step_avg:60.62ms
step:1882/2315 train_time:114092ms step_avg:60.62ms
step:1883/2315 train_time:114153ms step_avg:60.62ms
step:1884/2315 train_time:114214ms step_avg:60.62ms
step:1885/2315 train_time:114276ms step_avg:60.62ms
step:1886/2315 train_time:114336ms step_avg:60.62ms
step:1887/2315 train_time:114398ms step_avg:60.62ms
step:1888/2315 train_time:114459ms step_avg:60.62ms
step:1889/2315 train_time:114520ms step_avg:60.62ms
step:1890/2315 train_time:114581ms step_avg:60.62ms
step:1891/2315 train_time:114642ms step_avg:60.62ms
step:1892/2315 train_time:114703ms step_avg:60.63ms
step:1893/2315 train_time:114764ms step_avg:60.63ms
step:1894/2315 train_time:114825ms step_avg:60.63ms
step:1895/2315 train_time:114887ms step_avg:60.63ms
step:1896/2315 train_time:114948ms step_avg:60.63ms
step:1897/2315 train_time:115011ms step_avg:60.63ms
step:1898/2315 train_time:115071ms step_avg:60.63ms
step:1899/2315 train_time:115133ms step_avg:60.63ms
step:1900/2315 train_time:115194ms step_avg:60.63ms
step:1901/2315 train_time:115255ms step_avg:60.63ms
step:1902/2315 train_time:115316ms step_avg:60.63ms
step:1903/2315 train_time:115377ms step_avg:60.63ms
step:1904/2315 train_time:115438ms step_avg:60.63ms
step:1905/2315 train_time:115499ms step_avg:60.63ms
step:1906/2315 train_time:115559ms step_avg:60.63ms
step:1907/2315 train_time:115620ms step_avg:60.63ms
step:1908/2315 train_time:115682ms step_avg:60.63ms
step:1909/2315 train_time:115743ms step_avg:60.63ms
step:1910/2315 train_time:115803ms step_avg:60.63ms
step:1911/2315 train_time:115864ms step_avg:60.63ms
step:1912/2315 train_time:115925ms step_avg:60.63ms
step:1913/2315 train_time:115987ms step_avg:60.63ms
step:1914/2315 train_time:116048ms step_avg:60.63ms
step:1915/2315 train_time:116110ms step_avg:60.63ms
step:1916/2315 train_time:116171ms step_avg:60.63ms
step:1917/2315 train_time:116232ms step_avg:60.63ms
step:1918/2315 train_time:116294ms step_avg:60.63ms
step:1919/2315 train_time:116355ms step_avg:60.63ms
step:1920/2315 train_time:116416ms step_avg:60.63ms
step:1921/2315 train_time:116477ms step_avg:60.63ms
step:1922/2315 train_time:116538ms step_avg:60.63ms
step:1923/2315 train_time:116599ms step_avg:60.63ms
step:1924/2315 train_time:116660ms step_avg:60.63ms
step:1925/2315 train_time:116721ms step_avg:60.63ms
step:1926/2315 train_time:116781ms step_avg:60.63ms
step:1927/2315 train_time:116842ms step_avg:60.63ms
step:1928/2315 train_time:116903ms step_avg:60.63ms
step:1929/2315 train_time:116964ms step_avg:60.63ms
step:1930/2315 train_time:117026ms step_avg:60.64ms
step:1931/2315 train_time:117088ms step_avg:60.64ms
step:1932/2315 train_time:117150ms step_avg:60.64ms
step:1933/2315 train_time:117212ms step_avg:60.64ms
step:1934/2315 train_time:117273ms step_avg:60.64ms
step:1935/2315 train_time:117334ms step_avg:60.64ms
step:1936/2315 train_time:117395ms step_avg:60.64ms
step:1937/2315 train_time:117457ms step_avg:60.64ms
step:1938/2315 train_time:117518ms step_avg:60.64ms
step:1939/2315 train_time:117580ms step_avg:60.64ms
step:1940/2315 train_time:117641ms step_avg:60.64ms
step:1941/2315 train_time:117702ms step_avg:60.64ms
step:1942/2315 train_time:117763ms step_avg:60.64ms
step:1943/2315 train_time:117824ms step_avg:60.64ms
step:1944/2315 train_time:117885ms step_avg:60.64ms
step:1945/2315 train_time:117946ms step_avg:60.64ms
step:1946/2315 train_time:118007ms step_avg:60.64ms
step:1947/2315 train_time:118069ms step_avg:60.64ms
step:1948/2315 train_time:118130ms step_avg:60.64ms
step:1949/2315 train_time:118192ms step_avg:60.64ms
step:1950/2315 train_time:118253ms step_avg:60.64ms
step:1951/2315 train_time:118314ms step_avg:60.64ms
step:1952/2315 train_time:118375ms step_avg:60.64ms
step:1953/2315 train_time:118436ms step_avg:60.64ms
step:1954/2315 train_time:118497ms step_avg:60.64ms
step:1955/2315 train_time:118558ms step_avg:60.64ms
step:1956/2315 train_time:118620ms step_avg:60.64ms
step:1957/2315 train_time:118680ms step_avg:60.64ms
step:1958/2315 train_time:118741ms step_avg:60.64ms
step:1959/2315 train_time:118802ms step_avg:60.64ms
step:1960/2315 train_time:118863ms step_avg:60.64ms
step:1961/2315 train_time:118924ms step_avg:60.64ms
step:1962/2315 train_time:118985ms step_avg:60.64ms
step:1963/2315 train_time:119047ms step_avg:60.65ms
step:1964/2315 train_time:119108ms step_avg:60.65ms
step:1965/2315 train_time:119170ms step_avg:60.65ms
step:1966/2315 train_time:119231ms step_avg:60.65ms
step:1967/2315 train_time:119292ms step_avg:60.65ms
step:1968/2315 train_time:119353ms step_avg:60.65ms
step:1969/2315 train_time:119415ms step_avg:60.65ms
step:1970/2315 train_time:119476ms step_avg:60.65ms
step:1971/2315 train_time:119537ms step_avg:60.65ms
step:1972/2315 train_time:119598ms step_avg:60.65ms
step:1973/2315 train_time:119659ms step_avg:60.65ms
step:1974/2315 train_time:119720ms step_avg:60.65ms
step:1975/2315 train_time:119781ms step_avg:60.65ms
step:1976/2315 train_time:119842ms step_avg:60.65ms
step:1977/2315 train_time:119904ms step_avg:60.65ms
step:1978/2315 train_time:119965ms step_avg:60.65ms
step:1979/2315 train_time:120026ms step_avg:60.65ms
step:1980/2315 train_time:120088ms step_avg:60.65ms
step:1981/2315 train_time:120150ms step_avg:60.65ms
step:1982/2315 train_time:120211ms step_avg:60.65ms
step:1983/2315 train_time:120273ms step_avg:60.65ms
step:1984/2315 train_time:120334ms step_avg:60.65ms
step:1985/2315 train_time:120395ms step_avg:60.65ms
step:1986/2315 train_time:120456ms step_avg:60.65ms
step:1987/2315 train_time:120517ms step_avg:60.65ms
step:1988/2315 train_time:120578ms step_avg:60.65ms
step:1989/2315 train_time:120639ms step_avg:60.65ms
step:1990/2315 train_time:120700ms step_avg:60.65ms
step:1991/2315 train_time:120762ms step_avg:60.65ms
step:1992/2315 train_time:120823ms step_avg:60.65ms
step:1993/2315 train_time:120885ms step_avg:60.65ms
step:1994/2315 train_time:120945ms step_avg:60.65ms
step:1995/2315 train_time:121006ms step_avg:60.65ms
step:1996/2315 train_time:121068ms step_avg:60.66ms
step:1997/2315 train_time:121130ms step_avg:60.66ms
step:1998/2315 train_time:121191ms step_avg:60.66ms
step:1999/2315 train_time:121253ms step_avg:60.66ms
step:2000/2315 train_time:121313ms step_avg:60.66ms
step:2000/2315 val_loss:3.3348 train_time:121376ms step_avg:60.69ms
step:2001/2315 train_time:121396ms step_avg:60.67ms
step:2002/2315 train_time:121437ms step_avg:60.66ms
step:2003/2315 train_time:121505ms step_avg:60.66ms
step:2004/2315 train_time:121569ms step_avg:60.66ms
step:2005/2315 train_time:121631ms step_avg:60.66ms
step:2006/2315 train_time:121693ms step_avg:60.66ms
step:2007/2315 train_time:121755ms step_avg:60.66ms
step:2008/2315 train_time:121815ms step_avg:60.67ms
step:2009/2315 train_time:121876ms step_avg:60.67ms
step:2010/2315 train_time:121937ms step_avg:60.66ms
step:2011/2315 train_time:121997ms step_avg:60.66ms
step:2012/2315 train_time:122057ms step_avg:60.66ms
step:2013/2315 train_time:122117ms step_avg:60.66ms
step:2014/2315 train_time:122178ms step_avg:60.66ms
step:2015/2315 train_time:122238ms step_avg:60.66ms
step:2016/2315 train_time:122299ms step_avg:60.66ms
step:2017/2315 train_time:122361ms step_avg:60.66ms
step:2018/2315 train_time:122424ms step_avg:60.67ms
step:2019/2315 train_time:122488ms step_avg:60.67ms
step:2020/2315 train_time:122550ms step_avg:60.67ms
step:2021/2315 train_time:122612ms step_avg:60.67ms
step:2022/2315 train_time:122673ms step_avg:60.67ms
step:2023/2315 train_time:122735ms step_avg:60.67ms
step:2024/2315 train_time:122797ms step_avg:60.67ms
step:2025/2315 train_time:122857ms step_avg:60.67ms
step:2026/2315 train_time:122918ms step_avg:60.67ms
step:2027/2315 train_time:122978ms step_avg:60.67ms
step:2028/2315 train_time:123039ms step_avg:60.67ms
step:2029/2315 train_time:123099ms step_avg:60.67ms
step:2030/2315 train_time:123159ms step_avg:60.67ms
step:2031/2315 train_time:123220ms step_avg:60.67ms
step:2032/2315 train_time:123280ms step_avg:60.67ms
step:2033/2315 train_time:123342ms step_avg:60.67ms
step:2034/2315 train_time:123405ms step_avg:60.67ms
step:2035/2315 train_time:123467ms step_avg:60.67ms
step:2036/2315 train_time:123528ms step_avg:60.67ms
step:2037/2315 train_time:123590ms step_avg:60.67ms
step:2038/2315 train_time:123651ms step_avg:60.67ms
step:2039/2315 train_time:123713ms step_avg:60.67ms
step:2040/2315 train_time:123774ms step_avg:60.67ms
step:2041/2315 train_time:123836ms step_avg:60.67ms
step:2042/2315 train_time:123897ms step_avg:60.67ms
step:2043/2315 train_time:123958ms step_avg:60.67ms
step:2044/2315 train_time:124019ms step_avg:60.67ms
step:2045/2315 train_time:124080ms step_avg:60.67ms
step:2046/2315 train_time:124140ms step_avg:60.67ms
step:2047/2315 train_time:124201ms step_avg:60.67ms
step:2048/2315 train_time:124262ms step_avg:60.67ms
step:2049/2315 train_time:124323ms step_avg:60.68ms
step:2050/2315 train_time:124385ms step_avg:60.68ms
step:2051/2315 train_time:124448ms step_avg:60.68ms
step:2052/2315 train_time:124509ms step_avg:60.68ms
step:2053/2315 train_time:124571ms step_avg:60.68ms
step:2054/2315 train_time:124632ms step_avg:60.68ms
step:2055/2315 train_time:124693ms step_avg:60.68ms
step:2056/2315 train_time:124754ms step_avg:60.68ms
step:2057/2315 train_time:124816ms step_avg:60.68ms
step:2058/2315 train_time:124877ms step_avg:60.68ms
step:2059/2315 train_time:124938ms step_avg:60.68ms
step:2060/2315 train_time:124999ms step_avg:60.68ms
step:2061/2315 train_time:125060ms step_avg:60.68ms
step:2062/2315 train_time:125121ms step_avg:60.68ms
step:2063/2315 train_time:125182ms step_avg:60.68ms
step:2064/2315 train_time:125243ms step_avg:60.68ms
step:2065/2315 train_time:125304ms step_avg:60.68ms
step:2066/2315 train_time:125365ms step_avg:60.68ms
step:2067/2315 train_time:125426ms step_avg:60.68ms
step:2068/2315 train_time:125487ms step_avg:60.68ms
step:2069/2315 train_time:125549ms step_avg:60.68ms
step:2070/2315 train_time:125610ms step_avg:60.68ms
step:2071/2315 train_time:125672ms step_avg:60.68ms
step:2072/2315 train_time:125733ms step_avg:60.68ms
step:2073/2315 train_time:125794ms step_avg:60.68ms
step:2074/2315 train_time:125856ms step_avg:60.68ms
step:2075/2315 train_time:125917ms step_avg:60.68ms
step:2076/2315 train_time:125977ms step_avg:60.68ms
step:2077/2315 train_time:126038ms step_avg:60.68ms
step:2078/2315 train_time:126099ms step_avg:60.68ms
step:2079/2315 train_time:126160ms step_avg:60.68ms
step:2080/2315 train_time:126221ms step_avg:60.68ms
step:2081/2315 train_time:126282ms step_avg:60.68ms
step:2082/2315 train_time:126344ms step_avg:60.68ms
step:2083/2315 train_time:126406ms step_avg:60.68ms
step:2084/2315 train_time:126467ms step_avg:60.68ms
step:2085/2315 train_time:126528ms step_avg:60.69ms
step:2086/2315 train_time:126590ms step_avg:60.69ms
step:2087/2315 train_time:126651ms step_avg:60.69ms
step:2088/2315 train_time:126712ms step_avg:60.69ms
step:2089/2315 train_time:126774ms step_avg:60.69ms
step:2090/2315 train_time:126835ms step_avg:60.69ms
step:2091/2315 train_time:126896ms step_avg:60.69ms
step:2092/2315 train_time:126957ms step_avg:60.69ms
step:2093/2315 train_time:127018ms step_avg:60.69ms
step:2094/2315 train_time:127080ms step_avg:60.69ms
step:2095/2315 train_time:127141ms step_avg:60.69ms
step:2096/2315 train_time:127201ms step_avg:60.69ms
step:2097/2315 train_time:127262ms step_avg:60.69ms
step:2098/2315 train_time:127324ms step_avg:60.69ms
step:2099/2315 train_time:127386ms step_avg:60.69ms
step:2100/2315 train_time:127447ms step_avg:60.69ms
step:2101/2315 train_time:127508ms step_avg:60.69ms
step:2102/2315 train_time:127569ms step_avg:60.69ms
step:2103/2315 train_time:127631ms step_avg:60.69ms
step:2104/2315 train_time:127692ms step_avg:60.69ms
step:2105/2315 train_time:127754ms step_avg:60.69ms
step:2106/2315 train_time:127815ms step_avg:60.69ms
step:2107/2315 train_time:127876ms step_avg:60.69ms
step:2108/2315 train_time:127938ms step_avg:60.69ms
step:2109/2315 train_time:127998ms step_avg:60.69ms
step:2110/2315 train_time:128059ms step_avg:60.69ms
step:2111/2315 train_time:128121ms step_avg:60.69ms
step:2112/2315 train_time:128182ms step_avg:60.69ms
step:2113/2315 train_time:128243ms step_avg:60.69ms
step:2114/2315 train_time:128304ms step_avg:60.69ms
step:2115/2315 train_time:128365ms step_avg:60.69ms
step:2116/2315 train_time:128426ms step_avg:60.69ms
step:2117/2315 train_time:128487ms step_avg:60.69ms
step:2118/2315 train_time:128549ms step_avg:60.69ms
step:2119/2315 train_time:128611ms step_avg:60.69ms
step:2120/2315 train_time:128673ms step_avg:60.69ms
step:2121/2315 train_time:128734ms step_avg:60.69ms
step:2122/2315 train_time:128795ms step_avg:60.70ms
step:2123/2315 train_time:128856ms step_avg:60.70ms
step:2124/2315 train_time:128917ms step_avg:60.70ms
step:2125/2315 train_time:128979ms step_avg:60.70ms
step:2126/2315 train_time:129040ms step_avg:60.70ms
step:2127/2315 train_time:129101ms step_avg:60.70ms
step:2128/2315 train_time:129162ms step_avg:60.70ms
step:2129/2315 train_time:129223ms step_avg:60.70ms
step:2130/2315 train_time:129284ms step_avg:60.70ms
step:2131/2315 train_time:129345ms step_avg:60.70ms
step:2132/2315 train_time:129406ms step_avg:60.70ms
step:2133/2315 train_time:129468ms step_avg:60.70ms
step:2134/2315 train_time:129529ms step_avg:60.70ms
step:2135/2315 train_time:129591ms step_avg:60.70ms
step:2136/2315 train_time:129652ms step_avg:60.70ms
step:2137/2315 train_time:129713ms step_avg:60.70ms
step:2138/2315 train_time:129774ms step_avg:60.70ms
step:2139/2315 train_time:129835ms step_avg:60.70ms
step:2140/2315 train_time:129897ms step_avg:60.70ms
step:2141/2315 train_time:129959ms step_avg:60.70ms
step:2142/2315 train_time:130020ms step_avg:60.70ms
step:2143/2315 train_time:130081ms step_avg:60.70ms
step:2144/2315 train_time:130142ms step_avg:60.70ms
step:2145/2315 train_time:130203ms step_avg:60.70ms
step:2146/2315 train_time:130264ms step_avg:60.70ms
step:2147/2315 train_time:130325ms step_avg:60.70ms
step:2148/2315 train_time:130387ms step_avg:60.70ms
step:2149/2315 train_time:130447ms step_avg:60.70ms
step:2150/2315 train_time:130508ms step_avg:60.70ms
step:2151/2315 train_time:130570ms step_avg:60.70ms
step:2152/2315 train_time:130631ms step_avg:60.70ms
step:2153/2315 train_time:130693ms step_avg:60.70ms
step:2154/2315 train_time:130754ms step_avg:60.70ms
step:2155/2315 train_time:130816ms step_avg:60.70ms
step:2156/2315 train_time:130877ms step_avg:60.70ms
step:2157/2315 train_time:130938ms step_avg:60.70ms
step:2158/2315 train_time:130999ms step_avg:60.70ms
step:2159/2315 train_time:131060ms step_avg:60.70ms
step:2160/2315 train_time:131121ms step_avg:60.70ms
step:2161/2315 train_time:131182ms step_avg:60.70ms
step:2162/2315 train_time:131243ms step_avg:60.70ms
step:2163/2315 train_time:131305ms step_avg:60.70ms
step:2164/2315 train_time:131365ms step_avg:60.70ms
step:2165/2315 train_time:131427ms step_avg:60.71ms
step:2166/2315 train_time:131489ms step_avg:60.71ms
step:2167/2315 train_time:131551ms step_avg:60.71ms
step:2168/2315 train_time:131612ms step_avg:60.71ms
step:2169/2315 train_time:131673ms step_avg:60.71ms
step:2170/2315 train_time:131735ms step_avg:60.71ms
step:2171/2315 train_time:131796ms step_avg:60.71ms
step:2172/2315 train_time:131858ms step_avg:60.71ms
step:2173/2315 train_time:131919ms step_avg:60.71ms
step:2174/2315 train_time:131980ms step_avg:60.71ms
step:2175/2315 train_time:132041ms step_avg:60.71ms
step:2176/2315 train_time:132102ms step_avg:60.71ms
step:2177/2315 train_time:132163ms step_avg:60.71ms
step:2178/2315 train_time:132224ms step_avg:60.71ms
step:2179/2315 train_time:132285ms step_avg:60.71ms
step:2180/2315 train_time:132346ms step_avg:60.71ms
step:2181/2315 train_time:132408ms step_avg:60.71ms
step:2182/2315 train_time:132469ms step_avg:60.71ms
step:2183/2315 train_time:132530ms step_avg:60.71ms
step:2184/2315 train_time:132591ms step_avg:60.71ms
step:2185/2315 train_time:132653ms step_avg:60.71ms
step:2186/2315 train_time:132715ms step_avg:60.71ms
step:2187/2315 train_time:132776ms step_avg:60.71ms
step:2188/2315 train_time:132837ms step_avg:60.71ms
step:2189/2315 train_time:132898ms step_avg:60.71ms
step:2190/2315 train_time:132960ms step_avg:60.71ms
step:2191/2315 train_time:133021ms step_avg:60.71ms
step:2192/2315 train_time:133082ms step_avg:60.71ms
step:2193/2315 train_time:133143ms step_avg:60.71ms
step:2194/2315 train_time:133205ms step_avg:60.71ms
step:2195/2315 train_time:133266ms step_avg:60.71ms
step:2196/2315 train_time:133327ms step_avg:60.71ms
step:2197/2315 train_time:133389ms step_avg:60.71ms
step:2198/2315 train_time:133450ms step_avg:60.71ms
step:2199/2315 train_time:133512ms step_avg:60.71ms
step:2200/2315 train_time:133572ms step_avg:60.71ms
step:2201/2315 train_time:133634ms step_avg:60.71ms
step:2202/2315 train_time:133695ms step_avg:60.72ms
step:2203/2315 train_time:133756ms step_avg:60.72ms
step:2204/2315 train_time:133817ms step_avg:60.72ms
step:2205/2315 train_time:133878ms step_avg:60.72ms
step:2206/2315 train_time:133939ms step_avg:60.72ms
step:2207/2315 train_time:134000ms step_avg:60.72ms
step:2208/2315 train_time:134061ms step_avg:60.72ms
step:2209/2315 train_time:134122ms step_avg:60.72ms
step:2210/2315 train_time:134183ms step_avg:60.72ms
step:2211/2315 train_time:134245ms step_avg:60.72ms
step:2212/2315 train_time:134306ms step_avg:60.72ms
step:2213/2315 train_time:134367ms step_avg:60.72ms
step:2214/2315 train_time:134428ms step_avg:60.72ms
step:2215/2315 train_time:134490ms step_avg:60.72ms
step:2216/2315 train_time:134551ms step_avg:60.72ms
step:2217/2315 train_time:134613ms step_avg:60.72ms
step:2218/2315 train_time:134674ms step_avg:60.72ms
step:2219/2315 train_time:134735ms step_avg:60.72ms
step:2220/2315 train_time:134797ms step_avg:60.72ms
step:2221/2315 train_time:134858ms step_avg:60.72ms
step:2222/2315 train_time:134919ms step_avg:60.72ms
step:2223/2315 train_time:134980ms step_avg:60.72ms
step:2224/2315 train_time:135041ms step_avg:60.72ms
step:2225/2315 train_time:135102ms step_avg:60.72ms
step:2226/2315 train_time:135163ms step_avg:60.72ms
step:2227/2315 train_time:135225ms step_avg:60.72ms
step:2228/2315 train_time:135286ms step_avg:60.72ms
step:2229/2315 train_time:135347ms step_avg:60.72ms
step:2230/2315 train_time:135408ms step_avg:60.72ms
step:2231/2315 train_time:135470ms step_avg:60.72ms
step:2232/2315 train_time:135531ms step_avg:60.72ms
step:2233/2315 train_time:135592ms step_avg:60.72ms
step:2234/2315 train_time:135653ms step_avg:60.72ms
step:2235/2315 train_time:135715ms step_avg:60.72ms
step:2236/2315 train_time:135776ms step_avg:60.72ms
step:2237/2315 train_time:135838ms step_avg:60.72ms
step:2238/2315 train_time:135898ms step_avg:60.72ms
step:2239/2315 train_time:135960ms step_avg:60.72ms
step:2240/2315 train_time:136021ms step_avg:60.72ms
step:2241/2315 train_time:136082ms step_avg:60.72ms
step:2242/2315 train_time:136143ms step_avg:60.72ms
step:2243/2315 train_time:136204ms step_avg:60.72ms
step:2244/2315 train_time:136265ms step_avg:60.72ms
step:2245/2315 train_time:136327ms step_avg:60.72ms
step:2246/2315 train_time:136388ms step_avg:60.72ms
step:2247/2315 train_time:136449ms step_avg:60.73ms
step:2248/2315 train_time:136510ms step_avg:60.73ms
step:2249/2315 train_time:136571ms step_avg:60.73ms
step:2250/2315 train_time:136632ms step_avg:60.73ms
step:2250/2315 val_loss:3.2949 train_time:136696ms step_avg:60.75ms
step:2251/2315 train_time:136715ms step_avg:60.74ms
step:2252/2315 train_time:136758ms step_avg:60.73ms
step:2253/2315 train_time:136824ms step_avg:60.73ms
step:2254/2315 train_time:136890ms step_avg:60.73ms
step:2255/2315 train_time:136951ms step_avg:60.73ms
step:2256/2315 train_time:137013ms step_avg:60.73ms
step:2257/2315 train_time:137074ms step_avg:60.73ms
step:2258/2315 train_time:137135ms step_avg:60.73ms
step:2259/2315 train_time:137196ms step_avg:60.73ms
step:2260/2315 train_time:137256ms step_avg:60.73ms
step:2261/2315 train_time:137317ms step_avg:60.73ms
step:2262/2315 train_time:137377ms step_avg:60.73ms
step:2263/2315 train_time:137438ms step_avg:60.73ms
step:2264/2315 train_time:137499ms step_avg:60.73ms
step:2265/2315 train_time:137559ms step_avg:60.73ms
step:2266/2315 train_time:137619ms step_avg:60.73ms
step:2267/2315 train_time:137682ms step_avg:60.73ms
step:2268/2315 train_time:137744ms step_avg:60.73ms
step:2269/2315 train_time:137807ms step_avg:60.73ms
step:2270/2315 train_time:137869ms step_avg:60.74ms
step:2271/2315 train_time:137932ms step_avg:60.74ms
step:2272/2315 train_time:137993ms step_avg:60.74ms
step:2273/2315 train_time:138054ms step_avg:60.74ms
step:2274/2315 train_time:138115ms step_avg:60.74ms
step:2275/2315 train_time:138176ms step_avg:60.74ms
step:2276/2315 train_time:138237ms step_avg:60.74ms
step:2277/2315 train_time:138297ms step_avg:60.74ms
step:2278/2315 train_time:138358ms step_avg:60.74ms
step:2279/2315 train_time:138418ms step_avg:60.74ms
step:2280/2315 train_time:138479ms step_avg:60.74ms
step:2281/2315 train_time:138539ms step_avg:60.74ms
step:2282/2315 train_time:138600ms step_avg:60.74ms
step:2283/2315 train_time:138661ms step_avg:60.74ms
step:2284/2315 train_time:138722ms step_avg:60.74ms
step:2285/2315 train_time:138784ms step_avg:60.74ms
step:2286/2315 train_time:138847ms step_avg:60.74ms
step:2287/2315 train_time:138909ms step_avg:60.74ms
step:2288/2315 train_time:138971ms step_avg:60.74ms
step:2289/2315 train_time:139033ms step_avg:60.74ms
step:2290/2315 train_time:139094ms step_avg:60.74ms
step:2291/2315 train_time:139154ms step_avg:60.74ms
step:2292/2315 train_time:139215ms step_avg:60.74ms
step:2293/2315 train_time:139276ms step_avg:60.74ms
step:2294/2315 train_time:139337ms step_avg:60.74ms
step:2295/2315 train_time:139398ms step_avg:60.74ms
step:2296/2315 train_time:139458ms step_avg:60.74ms
step:2297/2315 train_time:139519ms step_avg:60.74ms
step:2298/2315 train_time:139579ms step_avg:60.74ms
step:2299/2315 train_time:139640ms step_avg:60.74ms
step:2300/2315 train_time:139701ms step_avg:60.74ms
step:2301/2315 train_time:139763ms step_avg:60.74ms
step:2302/2315 train_time:139825ms step_avg:60.74ms
step:2303/2315 train_time:139887ms step_avg:60.74ms
step:2304/2315 train_time:139949ms step_avg:60.74ms
step:2305/2315 train_time:140010ms step_avg:60.74ms
step:2306/2315 train_time:140071ms step_avg:60.74ms
step:2307/2315 train_time:140133ms step_avg:60.74ms
step:2308/2315 train_time:140194ms step_avg:60.74ms
step:2309/2315 train_time:140255ms step_avg:60.74ms
step:2310/2315 train_time:140316ms step_avg:60.74ms
step:2311/2315 train_time:140377ms step_avg:60.74ms
step:2312/2315 train_time:140438ms step_avg:60.74ms
step:2313/2315 train_time:140499ms step_avg:60.74ms
step:2314/2315 train_time:140559ms step_avg:60.74ms
step:2315/2315 train_time:140621ms step_avg:60.74ms
step:2315/2315 val_loss:3.2825 train_time:140682ms step_avg:60.77ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

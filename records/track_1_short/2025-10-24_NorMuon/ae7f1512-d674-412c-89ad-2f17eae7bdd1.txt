import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]
# polar_express_coeffs = [
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
# ]



@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    # state["second_momentum_buffer"] = torch.zeros_like(grad[...,0:1]) if param.size(-2) >= param.size(-1) else torch.zeros_like(grad[...,0:1,:])
                    # state["second_momentum_buffer"] = torch.zeros((*grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((*grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)
            
            ###################################
            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))
            ####################################


            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> (-1.5)  0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Oct 22 2025, 02:20:45) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251022+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 24 06:26:47 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000001:00:00.0 Off |                    0 |
| N/A   36C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000002:00:00.0 Off |                    0 |
| N/A   35C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000003:00:00.0 Off |                    0 |
| N/A   31C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000008:00:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000009:00:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   0000000A:00:00.0 Off |                    0 |
| N/A   36C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   0000000C:00:00.0 Off |                    0 |
| N/A   35C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2315 train_time:93ms step_avg:93.00ms
step:2/2315 train_time:188ms step_avg:94.10ms
step:3/2315 train_time:209ms step_avg:69.80ms
step:4/2315 train_time:246ms step_avg:61.56ms
step:5/2315 train_time:304ms step_avg:60.85ms
step:6/2315 train_time:364ms step_avg:60.70ms
step:7/2315 train_time:424ms step_avg:60.55ms
step:8/2315 train_time:483ms step_avg:60.43ms
step:9/2315 train_time:543ms step_avg:60.39ms
step:10/2315 train_time:603ms step_avg:60.31ms
step:11/2315 train_time:663ms step_avg:60.24ms
step:12/2315 train_time:723ms step_avg:60.22ms
step:13/2315 train_time:782ms step_avg:60.18ms
step:14/2315 train_time:842ms step_avg:60.16ms
step:15/2315 train_time:902ms step_avg:60.13ms
step:16/2315 train_time:963ms step_avg:60.17ms
step:17/2315 train_time:1024ms step_avg:60.22ms
step:18/2315 train_time:1086ms step_avg:60.35ms
step:19/2315 train_time:1150ms step_avg:60.52ms
step:20/2315 train_time:1212ms step_avg:60.60ms
step:21/2315 train_time:1273ms step_avg:60.61ms
step:22/2315 train_time:1333ms step_avg:60.60ms
step:23/2315 train_time:1394ms step_avg:60.61ms
step:24/2315 train_time:1454ms step_avg:60.60ms
step:25/2315 train_time:1514ms step_avg:60.58ms
step:26/2315 train_time:1574ms step_avg:60.55ms
step:27/2315 train_time:1635ms step_avg:60.54ms
step:28/2315 train_time:1695ms step_avg:60.53ms
step:29/2315 train_time:1755ms step_avg:60.53ms
step:30/2315 train_time:1816ms step_avg:60.54ms
step:31/2315 train_time:1876ms step_avg:60.53ms
step:32/2315 train_time:1937ms step_avg:60.53ms
step:33/2315 train_time:1998ms step_avg:60.54ms
step:34/2315 train_time:2059ms step_avg:60.55ms
step:35/2315 train_time:2121ms step_avg:60.59ms
step:36/2315 train_time:2182ms step_avg:60.60ms
step:37/2315 train_time:2243ms step_avg:60.62ms
step:38/2315 train_time:2303ms step_avg:60.61ms
step:39/2315 train_time:2364ms step_avg:60.61ms
step:40/2315 train_time:2424ms step_avg:60.61ms
step:41/2315 train_time:2485ms step_avg:60.62ms
step:42/2315 train_time:2545ms step_avg:60.60ms
step:43/2315 train_time:2605ms step_avg:60.59ms
step:44/2315 train_time:2665ms step_avg:60.58ms
step:45/2315 train_time:2726ms step_avg:60.57ms
step:46/2315 train_time:2786ms step_avg:60.56ms
step:47/2315 train_time:2846ms step_avg:60.56ms
step:48/2315 train_time:2906ms step_avg:60.55ms
step:49/2315 train_time:2967ms step_avg:60.55ms
step:50/2315 train_time:3027ms step_avg:60.55ms
step:51/2315 train_time:3089ms step_avg:60.57ms
step:52/2315 train_time:3150ms step_avg:60.58ms
step:53/2315 train_time:3211ms step_avg:60.58ms
step:54/2315 train_time:3272ms step_avg:60.59ms
step:55/2315 train_time:3333ms step_avg:60.60ms
step:56/2315 train_time:3392ms step_avg:60.58ms
step:57/2315 train_time:3453ms step_avg:60.57ms
step:58/2315 train_time:3512ms step_avg:60.56ms
step:59/2315 train_time:3573ms step_avg:60.56ms
step:60/2315 train_time:3633ms step_avg:60.55ms
step:61/2315 train_time:3694ms step_avg:60.55ms
step:62/2315 train_time:3754ms step_avg:60.55ms
step:63/2315 train_time:3815ms step_avg:60.55ms
step:64/2315 train_time:3876ms step_avg:60.56ms
step:65/2315 train_time:3936ms step_avg:60.56ms
step:66/2315 train_time:3997ms step_avg:60.57ms
step:67/2315 train_time:4058ms step_avg:60.57ms
step:68/2315 train_time:4119ms step_avg:60.57ms
step:69/2315 train_time:4179ms step_avg:60.57ms
step:70/2315 train_time:4240ms step_avg:60.57ms
step:71/2315 train_time:4300ms step_avg:60.57ms
step:72/2315 train_time:4361ms step_avg:60.57ms
step:73/2315 train_time:4421ms step_avg:60.56ms
step:74/2315 train_time:4482ms step_avg:60.57ms
step:75/2315 train_time:4542ms step_avg:60.56ms
step:76/2315 train_time:4602ms step_avg:60.55ms
step:77/2315 train_time:4663ms step_avg:60.56ms
step:78/2315 train_time:4723ms step_avg:60.56ms
step:79/2315 train_time:4783ms step_avg:60.55ms
step:80/2315 train_time:4844ms step_avg:60.55ms
step:81/2315 train_time:4904ms step_avg:60.55ms
step:82/2315 train_time:4964ms step_avg:60.54ms
step:83/2315 train_time:5024ms step_avg:60.53ms
step:84/2315 train_time:5084ms step_avg:60.52ms
step:85/2315 train_time:5144ms step_avg:60.52ms
step:86/2315 train_time:5204ms step_avg:60.51ms
step:87/2315 train_time:5264ms step_avg:60.51ms
step:88/2315 train_time:5324ms step_avg:60.50ms
step:89/2315 train_time:5384ms step_avg:60.50ms
step:90/2315 train_time:5444ms step_avg:60.49ms
step:91/2315 train_time:5505ms step_avg:60.49ms
step:92/2315 train_time:5565ms step_avg:60.49ms
step:93/2315 train_time:5625ms step_avg:60.49ms
step:94/2315 train_time:5685ms step_avg:60.48ms
step:95/2315 train_time:5745ms step_avg:60.47ms
step:96/2315 train_time:5805ms step_avg:60.47ms
step:97/2315 train_time:5866ms step_avg:60.47ms
step:98/2315 train_time:5925ms step_avg:60.46ms
step:99/2315 train_time:5985ms step_avg:60.45ms
step:100/2315 train_time:6045ms step_avg:60.45ms
step:101/2315 train_time:6104ms step_avg:60.44ms
step:102/2315 train_time:6164ms step_avg:60.43ms
step:103/2315 train_time:6224ms step_avg:60.43ms
step:104/2315 train_time:6284ms step_avg:60.42ms
step:105/2315 train_time:6344ms step_avg:60.42ms
step:106/2315 train_time:6404ms step_avg:60.42ms
step:107/2315 train_time:6464ms step_avg:60.41ms
step:108/2315 train_time:6524ms step_avg:60.41ms
step:109/2315 train_time:6585ms step_avg:60.41ms
step:110/2315 train_time:6644ms step_avg:60.40ms
step:111/2315 train_time:6705ms step_avg:60.41ms
step:112/2315 train_time:6765ms step_avg:60.40ms
step:113/2315 train_time:6825ms step_avg:60.40ms
step:114/2315 train_time:6885ms step_avg:60.39ms
step:115/2315 train_time:6945ms step_avg:60.39ms
step:116/2315 train_time:7005ms step_avg:60.39ms
step:117/2315 train_time:7065ms step_avg:60.38ms
step:118/2315 train_time:7124ms step_avg:60.38ms
step:119/2315 train_time:7185ms step_avg:60.38ms
step:120/2315 train_time:7245ms step_avg:60.37ms
step:121/2315 train_time:7304ms step_avg:60.37ms
step:122/2315 train_time:7364ms step_avg:60.36ms
step:123/2315 train_time:7424ms step_avg:60.36ms
step:124/2315 train_time:7484ms step_avg:60.35ms
step:125/2315 train_time:7544ms step_avg:60.35ms
step:126/2315 train_time:7604ms step_avg:60.35ms
step:127/2315 train_time:7664ms step_avg:60.35ms
step:128/2315 train_time:7724ms step_avg:60.34ms
step:129/2315 train_time:7784ms step_avg:60.34ms
step:130/2315 train_time:7844ms step_avg:60.34ms
step:131/2315 train_time:7904ms step_avg:60.34ms
step:132/2315 train_time:7964ms step_avg:60.33ms
step:133/2315 train_time:8025ms step_avg:60.34ms
step:134/2315 train_time:8085ms step_avg:60.33ms
step:135/2315 train_time:8145ms step_avg:60.33ms
step:136/2315 train_time:8204ms step_avg:60.32ms
step:137/2315 train_time:8264ms step_avg:60.32ms
step:138/2315 train_time:8324ms step_avg:60.32ms
step:139/2315 train_time:8384ms step_avg:60.32ms
step:140/2315 train_time:8443ms step_avg:60.31ms
step:141/2315 train_time:8503ms step_avg:60.31ms
step:142/2315 train_time:8564ms step_avg:60.31ms
step:143/2315 train_time:8624ms step_avg:60.31ms
step:144/2315 train_time:8684ms step_avg:60.31ms
step:145/2315 train_time:8745ms step_avg:60.31ms
step:146/2315 train_time:8805ms step_avg:60.31ms
step:147/2315 train_time:8865ms step_avg:60.31ms
step:148/2315 train_time:8924ms step_avg:60.30ms
step:149/2315 train_time:8984ms step_avg:60.30ms
step:150/2315 train_time:9044ms step_avg:60.29ms
step:151/2315 train_time:9104ms step_avg:60.29ms
step:152/2315 train_time:9164ms step_avg:60.29ms
step:153/2315 train_time:9224ms step_avg:60.29ms
step:154/2315 train_time:9284ms step_avg:60.28ms
step:155/2315 train_time:9344ms step_avg:60.28ms
step:156/2315 train_time:9404ms step_avg:60.28ms
step:157/2315 train_time:9464ms step_avg:60.28ms
step:158/2315 train_time:9523ms step_avg:60.27ms
step:159/2315 train_time:9584ms step_avg:60.28ms
step:160/2315 train_time:9644ms step_avg:60.27ms
step:161/2315 train_time:9704ms step_avg:60.27ms
step:162/2315 train_time:9764ms step_avg:60.27ms
step:163/2315 train_time:9824ms step_avg:60.27ms
step:164/2315 train_time:9883ms step_avg:60.26ms
step:165/2315 train_time:9944ms step_avg:60.26ms
step:166/2315 train_time:10003ms step_avg:60.26ms
step:167/2315 train_time:10063ms step_avg:60.26ms
step:168/2315 train_time:10123ms step_avg:60.26ms
step:169/2315 train_time:10184ms step_avg:60.26ms
step:170/2315 train_time:10244ms step_avg:60.26ms
step:171/2315 train_time:10303ms step_avg:60.25ms
step:172/2315 train_time:10363ms step_avg:60.25ms
step:173/2315 train_time:10423ms step_avg:60.25ms
step:174/2315 train_time:10483ms step_avg:60.25ms
step:175/2315 train_time:10543ms step_avg:60.24ms
step:176/2315 train_time:10602ms step_avg:60.24ms
step:177/2315 train_time:10662ms step_avg:60.24ms
step:178/2315 train_time:10722ms step_avg:60.24ms
step:179/2315 train_time:10783ms step_avg:60.24ms
step:180/2315 train_time:10843ms step_avg:60.24ms
step:181/2315 train_time:10903ms step_avg:60.24ms
step:182/2315 train_time:10963ms step_avg:60.24ms
step:183/2315 train_time:11023ms step_avg:60.23ms
step:184/2315 train_time:11083ms step_avg:60.23ms
step:185/2315 train_time:11143ms step_avg:60.23ms
step:186/2315 train_time:11202ms step_avg:60.23ms
step:187/2315 train_time:11262ms step_avg:60.23ms
step:188/2315 train_time:11323ms step_avg:60.23ms
step:189/2315 train_time:11382ms step_avg:60.22ms
step:190/2315 train_time:11442ms step_avg:60.22ms
step:191/2315 train_time:11502ms step_avg:60.22ms
step:192/2315 train_time:11561ms step_avg:60.22ms
step:193/2315 train_time:11622ms step_avg:60.22ms
step:194/2315 train_time:11681ms step_avg:60.21ms
step:195/2315 train_time:11741ms step_avg:60.21ms
step:196/2315 train_time:11801ms step_avg:60.21ms
step:197/2315 train_time:11862ms step_avg:60.21ms
step:198/2315 train_time:11922ms step_avg:60.21ms
step:199/2315 train_time:11982ms step_avg:60.21ms
step:200/2315 train_time:12042ms step_avg:60.21ms
step:201/2315 train_time:12103ms step_avg:60.21ms
step:202/2315 train_time:12163ms step_avg:60.21ms
step:203/2315 train_time:12223ms step_avg:60.21ms
step:204/2315 train_time:12283ms step_avg:60.21ms
step:205/2315 train_time:12343ms step_avg:60.21ms
step:206/2315 train_time:12402ms step_avg:60.21ms
step:207/2315 train_time:12462ms step_avg:60.20ms
step:208/2315 train_time:12522ms step_avg:60.20ms
step:209/2315 train_time:12582ms step_avg:60.20ms
step:210/2315 train_time:12643ms step_avg:60.20ms
step:211/2315 train_time:12703ms step_avg:60.21ms
step:212/2315 train_time:12763ms step_avg:60.20ms
step:213/2315 train_time:12823ms step_avg:60.20ms
step:214/2315 train_time:12883ms step_avg:60.20ms
step:215/2315 train_time:12943ms step_avg:60.20ms
step:216/2315 train_time:13004ms step_avg:60.20ms
step:217/2315 train_time:13064ms step_avg:60.20ms
step:218/2315 train_time:13124ms step_avg:60.20ms
step:219/2315 train_time:13184ms step_avg:60.20ms
step:220/2315 train_time:13244ms step_avg:60.20ms
step:221/2315 train_time:13304ms step_avg:60.20ms
step:222/2315 train_time:13363ms step_avg:60.20ms
step:223/2315 train_time:13423ms step_avg:60.19ms
step:224/2315 train_time:13483ms step_avg:60.19ms
step:225/2315 train_time:13543ms step_avg:60.19ms
step:226/2315 train_time:13603ms step_avg:60.19ms
step:227/2315 train_time:13663ms step_avg:60.19ms
step:228/2315 train_time:13724ms step_avg:60.19ms
step:229/2315 train_time:13784ms step_avg:60.19ms
step:230/2315 train_time:13843ms step_avg:60.19ms
step:231/2315 train_time:13903ms step_avg:60.19ms
step:232/2315 train_time:13964ms step_avg:60.19ms
step:233/2315 train_time:14023ms step_avg:60.18ms
step:234/2315 train_time:14083ms step_avg:60.18ms
step:235/2315 train_time:14143ms step_avg:60.18ms
step:236/2315 train_time:14203ms step_avg:60.18ms
step:237/2315 train_time:14263ms step_avg:60.18ms
step:238/2315 train_time:14323ms step_avg:60.18ms
step:239/2315 train_time:14383ms step_avg:60.18ms
step:240/2315 train_time:14443ms step_avg:60.18ms
step:241/2315 train_time:14502ms step_avg:60.17ms
step:242/2315 train_time:14562ms step_avg:60.17ms
step:243/2315 train_time:14622ms step_avg:60.17ms
step:244/2315 train_time:14682ms step_avg:60.17ms
step:245/2315 train_time:14743ms step_avg:60.17ms
step:246/2315 train_time:14802ms step_avg:60.17ms
step:247/2315 train_time:14863ms step_avg:60.17ms
step:248/2315 train_time:14923ms step_avg:60.17ms
step:249/2315 train_time:14983ms step_avg:60.17ms
step:250/2315 train_time:15042ms step_avg:60.17ms
step:250/2315 val_loss:4.0661 train_time:15104ms step_avg:60.42ms
step:251/2315 train_time:15123ms step_avg:60.25ms
step:252/2315 train_time:15164ms step_avg:60.17ms
step:253/2315 train_time:15226ms step_avg:60.18ms
step:254/2315 train_time:15289ms step_avg:60.19ms
step:255/2315 train_time:15351ms step_avg:60.20ms
step:256/2315 train_time:15411ms step_avg:60.20ms
step:257/2315 train_time:15471ms step_avg:60.20ms
step:258/2315 train_time:15530ms step_avg:60.20ms
step:259/2315 train_time:15590ms step_avg:60.19ms
step:260/2315 train_time:15649ms step_avg:60.19ms
step:261/2315 train_time:15708ms step_avg:60.18ms
step:262/2315 train_time:15768ms step_avg:60.18ms
step:263/2315 train_time:15827ms step_avg:60.18ms
step:264/2315 train_time:15886ms step_avg:60.17ms
step:265/2315 train_time:15945ms step_avg:60.17ms
step:266/2315 train_time:16004ms step_avg:60.17ms
step:267/2315 train_time:16063ms step_avg:60.16ms
step:268/2315 train_time:16123ms step_avg:60.16ms
step:269/2315 train_time:16183ms step_avg:60.16ms
step:270/2315 train_time:16243ms step_avg:60.16ms
step:271/2315 train_time:16304ms step_avg:60.16ms
step:272/2315 train_time:16364ms step_avg:60.16ms
step:273/2315 train_time:16425ms step_avg:60.17ms
step:274/2315 train_time:16485ms step_avg:60.17ms
step:275/2315 train_time:16545ms step_avg:60.16ms
step:276/2315 train_time:16604ms step_avg:60.16ms
step:277/2315 train_time:16665ms step_avg:60.16ms
step:278/2315 train_time:16724ms step_avg:60.16ms
step:279/2315 train_time:16784ms step_avg:60.16ms
step:280/2315 train_time:16843ms step_avg:60.15ms
step:281/2315 train_time:16903ms step_avg:60.15ms
step:282/2315 train_time:16964ms step_avg:60.15ms
step:283/2315 train_time:17023ms step_avg:60.15ms
step:284/2315 train_time:17082ms step_avg:60.15ms
step:285/2315 train_time:17142ms step_avg:60.15ms
step:286/2315 train_time:17201ms step_avg:60.14ms
step:287/2315 train_time:17262ms step_avg:60.14ms
step:288/2315 train_time:17322ms step_avg:60.15ms
step:289/2315 train_time:17383ms step_avg:60.15ms
step:290/2315 train_time:17443ms step_avg:60.15ms
step:291/2315 train_time:17503ms step_avg:60.15ms
step:292/2315 train_time:17563ms step_avg:60.15ms
step:293/2315 train_time:17623ms step_avg:60.15ms
step:294/2315 train_time:17683ms step_avg:60.15ms
step:295/2315 train_time:17743ms step_avg:60.15ms
step:296/2315 train_time:17802ms step_avg:60.14ms
step:297/2315 train_time:17862ms step_avg:60.14ms
step:298/2315 train_time:17922ms step_avg:60.14ms
step:299/2315 train_time:17982ms step_avg:60.14ms
step:300/2315 train_time:18041ms step_avg:60.14ms
step:301/2315 train_time:18101ms step_avg:60.14ms
step:302/2315 train_time:18161ms step_avg:60.14ms
step:303/2315 train_time:18221ms step_avg:60.14ms
step:304/2315 train_time:18281ms step_avg:60.13ms
step:305/2315 train_time:18341ms step_avg:60.14ms
step:306/2315 train_time:18402ms step_avg:60.14ms
step:307/2315 train_time:18462ms step_avg:60.14ms
step:308/2315 train_time:18522ms step_avg:60.14ms
step:309/2315 train_time:18582ms step_avg:60.14ms
step:310/2315 train_time:18642ms step_avg:60.14ms
step:311/2315 train_time:18703ms step_avg:60.14ms
step:312/2315 train_time:18763ms step_avg:60.14ms
step:313/2315 train_time:18823ms step_avg:60.14ms
step:314/2315 train_time:18882ms step_avg:60.13ms
step:315/2315 train_time:18942ms step_avg:60.13ms
step:316/2315 train_time:19001ms step_avg:60.13ms
step:317/2315 train_time:19061ms step_avg:60.13ms
step:318/2315 train_time:19121ms step_avg:60.13ms
step:319/2315 train_time:19180ms step_avg:60.13ms
step:320/2315 train_time:19240ms step_avg:60.13ms
step:321/2315 train_time:19301ms step_avg:60.13ms
step:322/2315 train_time:19361ms step_avg:60.13ms
step:323/2315 train_time:19421ms step_avg:60.13ms
step:324/2315 train_time:19481ms step_avg:60.13ms
step:325/2315 train_time:19541ms step_avg:60.13ms
step:326/2315 train_time:19601ms step_avg:60.13ms
step:327/2315 train_time:19662ms step_avg:60.13ms
step:328/2315 train_time:19722ms step_avg:60.13ms
step:329/2315 train_time:19782ms step_avg:60.13ms
step:330/2315 train_time:19842ms step_avg:60.13ms
step:331/2315 train_time:19902ms step_avg:60.13ms
step:332/2315 train_time:19962ms step_avg:60.13ms
step:333/2315 train_time:20022ms step_avg:60.13ms
step:334/2315 train_time:20082ms step_avg:60.12ms
step:335/2315 train_time:20141ms step_avg:60.12ms
step:336/2315 train_time:20202ms step_avg:60.12ms
step:337/2315 train_time:20261ms step_avg:60.12ms
step:338/2315 train_time:20321ms step_avg:60.12ms
step:339/2315 train_time:20382ms step_avg:60.12ms
step:340/2315 train_time:20442ms step_avg:60.12ms
step:341/2315 train_time:20502ms step_avg:60.12ms
step:342/2315 train_time:20562ms step_avg:60.12ms
step:343/2315 train_time:20622ms step_avg:60.12ms
step:344/2315 train_time:20682ms step_avg:60.12ms
step:345/2315 train_time:20742ms step_avg:60.12ms
step:346/2315 train_time:20801ms step_avg:60.12ms
step:347/2315 train_time:20861ms step_avg:60.12ms
step:348/2315 train_time:20922ms step_avg:60.12ms
step:349/2315 train_time:20981ms step_avg:60.12ms
step:350/2315 train_time:21041ms step_avg:60.12ms
step:351/2315 train_time:21101ms step_avg:60.12ms
step:352/2315 train_time:21162ms step_avg:60.12ms
step:353/2315 train_time:21222ms step_avg:60.12ms
step:354/2315 train_time:21282ms step_avg:60.12ms
step:355/2315 train_time:21342ms step_avg:60.12ms
step:356/2315 train_time:21403ms step_avg:60.12ms
step:357/2315 train_time:21463ms step_avg:60.12ms
step:358/2315 train_time:21523ms step_avg:60.12ms
step:359/2315 train_time:21584ms step_avg:60.12ms
step:360/2315 train_time:21644ms step_avg:60.12ms
step:361/2315 train_time:21703ms step_avg:60.12ms
step:362/2315 train_time:21763ms step_avg:60.12ms
step:363/2315 train_time:21823ms step_avg:60.12ms
step:364/2315 train_time:21882ms step_avg:60.12ms
step:365/2315 train_time:21942ms step_avg:60.12ms
step:366/2315 train_time:22002ms step_avg:60.11ms
step:367/2315 train_time:22062ms step_avg:60.11ms
step:368/2315 train_time:22121ms step_avg:60.11ms
step:369/2315 train_time:22182ms step_avg:60.11ms
step:370/2315 train_time:22242ms step_avg:60.11ms
step:371/2315 train_time:22302ms step_avg:60.11ms
step:372/2315 train_time:22362ms step_avg:60.11ms
step:373/2315 train_time:22422ms step_avg:60.11ms
step:374/2315 train_time:22481ms step_avg:60.11ms
step:375/2315 train_time:22542ms step_avg:60.11ms
step:376/2315 train_time:22601ms step_avg:60.11ms
step:377/2315 train_time:22661ms step_avg:60.11ms
step:378/2315 train_time:22721ms step_avg:60.11ms
step:379/2315 train_time:22782ms step_avg:60.11ms
step:380/2315 train_time:22842ms step_avg:60.11ms
step:381/2315 train_time:22902ms step_avg:60.11ms
step:382/2315 train_time:22961ms step_avg:60.11ms
step:383/2315 train_time:23021ms step_avg:60.11ms
step:384/2315 train_time:23081ms step_avg:60.11ms
step:385/2315 train_time:23141ms step_avg:60.11ms
step:386/2315 train_time:23201ms step_avg:60.11ms
step:387/2315 train_time:23262ms step_avg:60.11ms
step:388/2315 train_time:23322ms step_avg:60.11ms
step:389/2315 train_time:23382ms step_avg:60.11ms
step:390/2315 train_time:23443ms step_avg:60.11ms
step:391/2315 train_time:23503ms step_avg:60.11ms
step:392/2315 train_time:23562ms step_avg:60.11ms
step:393/2315 train_time:23622ms step_avg:60.11ms
step:394/2315 train_time:23683ms step_avg:60.11ms
step:395/2315 train_time:23743ms step_avg:60.11ms
step:396/2315 train_time:23802ms step_avg:60.11ms
step:397/2315 train_time:23863ms step_avg:60.11ms
step:398/2315 train_time:23923ms step_avg:60.11ms
step:399/2315 train_time:23983ms step_avg:60.11ms
step:400/2315 train_time:24042ms step_avg:60.11ms
step:401/2315 train_time:24102ms step_avg:60.10ms
step:402/2315 train_time:24162ms step_avg:60.10ms
step:403/2315 train_time:24222ms step_avg:60.10ms
step:404/2315 train_time:24281ms step_avg:60.10ms
step:405/2315 train_time:24342ms step_avg:60.10ms
step:406/2315 train_time:24402ms step_avg:60.10ms
step:407/2315 train_time:24462ms step_avg:60.10ms
step:408/2315 train_time:24522ms step_avg:60.10ms
step:409/2315 train_time:24583ms step_avg:60.11ms
step:410/2315 train_time:24643ms step_avg:60.11ms
step:411/2315 train_time:24703ms step_avg:60.11ms
step:412/2315 train_time:24763ms step_avg:60.10ms
step:413/2315 train_time:24823ms step_avg:60.10ms
step:414/2315 train_time:24883ms step_avg:60.10ms
step:415/2315 train_time:24942ms step_avg:60.10ms
step:416/2315 train_time:25001ms step_avg:60.10ms
step:417/2315 train_time:25062ms step_avg:60.10ms
step:418/2315 train_time:25122ms step_avg:60.10ms
step:419/2315 train_time:25182ms step_avg:60.10ms
step:420/2315 train_time:25243ms step_avg:60.10ms
step:421/2315 train_time:25302ms step_avg:60.10ms
step:422/2315 train_time:25362ms step_avg:60.10ms
step:423/2315 train_time:25422ms step_avg:60.10ms
step:424/2315 train_time:25482ms step_avg:60.10ms
step:425/2315 train_time:25542ms step_avg:60.10ms
step:426/2315 train_time:25602ms step_avg:60.10ms
step:427/2315 train_time:25662ms step_avg:60.10ms
step:428/2315 train_time:25722ms step_avg:60.10ms
step:429/2315 train_time:25783ms step_avg:60.10ms
step:430/2315 train_time:25843ms step_avg:60.10ms
step:431/2315 train_time:25903ms step_avg:60.10ms
step:432/2315 train_time:25964ms step_avg:60.10ms
step:433/2315 train_time:26023ms step_avg:60.10ms
step:434/2315 train_time:26082ms step_avg:60.10ms
step:435/2315 train_time:26142ms step_avg:60.10ms
step:436/2315 train_time:26202ms step_avg:60.10ms
step:437/2315 train_time:26262ms step_avg:60.10ms
step:438/2315 train_time:26322ms step_avg:60.10ms
step:439/2315 train_time:26382ms step_avg:60.10ms
step:440/2315 train_time:26442ms step_avg:60.10ms
step:441/2315 train_time:26502ms step_avg:60.10ms
step:442/2315 train_time:26562ms step_avg:60.09ms
step:443/2315 train_time:26623ms step_avg:60.10ms
step:444/2315 train_time:26683ms step_avg:60.10ms
step:445/2315 train_time:26744ms step_avg:60.10ms
step:446/2315 train_time:26804ms step_avg:60.10ms
step:447/2315 train_time:26864ms step_avg:60.10ms
step:448/2315 train_time:26924ms step_avg:60.10ms
step:449/2315 train_time:26984ms step_avg:60.10ms
step:450/2315 train_time:27044ms step_avg:60.10ms
step:451/2315 train_time:27103ms step_avg:60.10ms
step:452/2315 train_time:27163ms step_avg:60.09ms
step:453/2315 train_time:27223ms step_avg:60.09ms
step:454/2315 train_time:27282ms step_avg:60.09ms
step:455/2315 train_time:27342ms step_avg:60.09ms
step:456/2315 train_time:27402ms step_avg:60.09ms
step:457/2315 train_time:27462ms step_avg:60.09ms
step:458/2315 train_time:27522ms step_avg:60.09ms
step:459/2315 train_time:27583ms step_avg:60.09ms
step:460/2315 train_time:27643ms step_avg:60.09ms
step:461/2315 train_time:27704ms step_avg:60.09ms
step:462/2315 train_time:27763ms step_avg:60.09ms
step:463/2315 train_time:27824ms step_avg:60.10ms
step:464/2315 train_time:27884ms step_avg:60.09ms
step:465/2315 train_time:27943ms step_avg:60.09ms
step:466/2315 train_time:28004ms step_avg:60.09ms
step:467/2315 train_time:28064ms step_avg:60.09ms
step:468/2315 train_time:28123ms step_avg:60.09ms
step:469/2315 train_time:28183ms step_avg:60.09ms
step:470/2315 train_time:28243ms step_avg:60.09ms
step:471/2315 train_time:28303ms step_avg:60.09ms
step:472/2315 train_time:28362ms step_avg:60.09ms
step:473/2315 train_time:28423ms step_avg:60.09ms
step:474/2315 train_time:28482ms step_avg:60.09ms
step:475/2315 train_time:28542ms step_avg:60.09ms
step:476/2315 train_time:28602ms step_avg:60.09ms
step:477/2315 train_time:28662ms step_avg:60.09ms
step:478/2315 train_time:28723ms step_avg:60.09ms
step:479/2315 train_time:28783ms step_avg:60.09ms
step:480/2315 train_time:28843ms step_avg:60.09ms
step:481/2315 train_time:28903ms step_avg:60.09ms
step:482/2315 train_time:28963ms step_avg:60.09ms
step:483/2315 train_time:29024ms step_avg:60.09ms
step:484/2315 train_time:29084ms step_avg:60.09ms
step:485/2315 train_time:29143ms step_avg:60.09ms
step:486/2315 train_time:29203ms step_avg:60.09ms
step:487/2315 train_time:29263ms step_avg:60.09ms
step:488/2315 train_time:29323ms step_avg:60.09ms
step:489/2315 train_time:29382ms step_avg:60.09ms
step:490/2315 train_time:29442ms step_avg:60.09ms
step:491/2315 train_time:29503ms step_avg:60.09ms
step:492/2315 train_time:29562ms step_avg:60.09ms
step:493/2315 train_time:29623ms step_avg:60.09ms
step:494/2315 train_time:29682ms step_avg:60.09ms
step:495/2315 train_time:29742ms step_avg:60.09ms
step:496/2315 train_time:29802ms step_avg:60.08ms
step:497/2315 train_time:29863ms step_avg:60.09ms
step:498/2315 train_time:29923ms step_avg:60.09ms
step:499/2315 train_time:29983ms step_avg:60.09ms
step:500/2315 train_time:30043ms step_avg:60.09ms
step:500/2315 val_loss:3.8115 train_time:30104ms step_avg:60.21ms
step:501/2315 train_time:30124ms step_avg:60.13ms
step:502/2315 train_time:30164ms step_avg:60.09ms
step:503/2315 train_time:30228ms step_avg:60.10ms
step:504/2315 train_time:30291ms step_avg:60.10ms
step:505/2315 train_time:30351ms step_avg:60.10ms
step:506/2315 train_time:30412ms step_avg:60.10ms
step:507/2315 train_time:30472ms step_avg:60.10ms
step:508/2315 train_time:30531ms step_avg:60.10ms
step:509/2315 train_time:30591ms step_avg:60.10ms
step:510/2315 train_time:30650ms step_avg:60.10ms
step:511/2315 train_time:30710ms step_avg:60.10ms
step:512/2315 train_time:30769ms step_avg:60.10ms
step:513/2315 train_time:30828ms step_avg:60.09ms
step:514/2315 train_time:30887ms step_avg:60.09ms
step:515/2315 train_time:30947ms step_avg:60.09ms
step:516/2315 train_time:31006ms step_avg:60.09ms
step:517/2315 train_time:31066ms step_avg:60.09ms
step:518/2315 train_time:31127ms step_avg:60.09ms
step:519/2315 train_time:31189ms step_avg:60.09ms
step:520/2315 train_time:31251ms step_avg:60.10ms
step:521/2315 train_time:31313ms step_avg:60.10ms
step:522/2315 train_time:31373ms step_avg:60.10ms
step:523/2315 train_time:31433ms step_avg:60.10ms
step:524/2315 train_time:31493ms step_avg:60.10ms
step:525/2315 train_time:31553ms step_avg:60.10ms
step:526/2315 train_time:31613ms step_avg:60.10ms
step:527/2315 train_time:31673ms step_avg:60.10ms
step:528/2315 train_time:31734ms step_avg:60.10ms
step:529/2315 train_time:31794ms step_avg:60.10ms
step:530/2315 train_time:31854ms step_avg:60.10ms
step:531/2315 train_time:31914ms step_avg:60.10ms
step:532/2315 train_time:31975ms step_avg:60.10ms
step:533/2315 train_time:32035ms step_avg:60.10ms
step:534/2315 train_time:32096ms step_avg:60.10ms
step:535/2315 train_time:32157ms step_avg:60.11ms
step:536/2315 train_time:32217ms step_avg:60.11ms
step:537/2315 train_time:32278ms step_avg:60.11ms
step:538/2315 train_time:32339ms step_avg:60.11ms
step:539/2315 train_time:32399ms step_avg:60.11ms
step:540/2315 train_time:32459ms step_avg:60.11ms
step:541/2315 train_time:32519ms step_avg:60.11ms
step:542/2315 train_time:32579ms step_avg:60.11ms
step:543/2315 train_time:32640ms step_avg:60.11ms
step:544/2315 train_time:32700ms step_avg:60.11ms
step:545/2315 train_time:32759ms step_avg:60.11ms
step:546/2315 train_time:32819ms step_avg:60.11ms
step:547/2315 train_time:32880ms step_avg:60.11ms
step:548/2315 train_time:32940ms step_avg:60.11ms
step:549/2315 train_time:33000ms step_avg:60.11ms
step:550/2315 train_time:33060ms step_avg:60.11ms
step:551/2315 train_time:33121ms step_avg:60.11ms
step:552/2315 train_time:33181ms step_avg:60.11ms
step:553/2315 train_time:33241ms step_avg:60.11ms
step:554/2315 train_time:33302ms step_avg:60.11ms
step:555/2315 train_time:33362ms step_avg:60.11ms
step:556/2315 train_time:33422ms step_avg:60.11ms
step:557/2315 train_time:33482ms step_avg:60.11ms
step:558/2315 train_time:33542ms step_avg:60.11ms
step:559/2315 train_time:33602ms step_avg:60.11ms
step:560/2315 train_time:33662ms step_avg:60.11ms
step:561/2315 train_time:33722ms step_avg:60.11ms
step:562/2315 train_time:33782ms step_avg:60.11ms
step:563/2315 train_time:33842ms step_avg:60.11ms
step:564/2315 train_time:33902ms step_avg:60.11ms
step:565/2315 train_time:33962ms step_avg:60.11ms
step:566/2315 train_time:34022ms step_avg:60.11ms
step:567/2315 train_time:34082ms step_avg:60.11ms
step:568/2315 train_time:34143ms step_avg:60.11ms
step:569/2315 train_time:34203ms step_avg:60.11ms
step:570/2315 train_time:34263ms step_avg:60.11ms
step:571/2315 train_time:34323ms step_avg:60.11ms
step:572/2315 train_time:34382ms step_avg:60.11ms
step:573/2315 train_time:34442ms step_avg:60.11ms
step:574/2315 train_time:34502ms step_avg:60.11ms
step:575/2315 train_time:34562ms step_avg:60.11ms
step:576/2315 train_time:34622ms step_avg:60.11ms
step:577/2315 train_time:34682ms step_avg:60.11ms
step:578/2315 train_time:34742ms step_avg:60.11ms
step:579/2315 train_time:34803ms step_avg:60.11ms
step:580/2315 train_time:34863ms step_avg:60.11ms
step:581/2315 train_time:34923ms step_avg:60.11ms
step:582/2315 train_time:34982ms step_avg:60.11ms
step:583/2315 train_time:35042ms step_avg:60.11ms
step:584/2315 train_time:35102ms step_avg:60.11ms
step:585/2315 train_time:35162ms step_avg:60.11ms
step:586/2315 train_time:35221ms step_avg:60.10ms
step:587/2315 train_time:35281ms step_avg:60.10ms
step:588/2315 train_time:35342ms step_avg:60.10ms
step:589/2315 train_time:35402ms step_avg:60.11ms
step:590/2315 train_time:35462ms step_avg:60.11ms
step:591/2315 train_time:35522ms step_avg:60.10ms
step:592/2315 train_time:35582ms step_avg:60.10ms
step:593/2315 train_time:35642ms step_avg:60.10ms
step:594/2315 train_time:35702ms step_avg:60.10ms
step:595/2315 train_time:35762ms step_avg:60.10ms
step:596/2315 train_time:35821ms step_avg:60.10ms
step:597/2315 train_time:35882ms step_avg:60.10ms
step:598/2315 train_time:35941ms step_avg:60.10ms
step:599/2315 train_time:36002ms step_avg:60.10ms
step:600/2315 train_time:36062ms step_avg:60.10ms
step:601/2315 train_time:36122ms step_avg:60.10ms
step:602/2315 train_time:36182ms step_avg:60.10ms
step:603/2315 train_time:36243ms step_avg:60.10ms
step:604/2315 train_time:36302ms step_avg:60.10ms
step:605/2315 train_time:36363ms step_avg:60.10ms
step:606/2315 train_time:36422ms step_avg:60.10ms
step:607/2315 train_time:36483ms step_avg:60.10ms
step:608/2315 train_time:36543ms step_avg:60.10ms
step:609/2315 train_time:36603ms step_avg:60.10ms
step:610/2315 train_time:36663ms step_avg:60.10ms
step:611/2315 train_time:36723ms step_avg:60.10ms
step:612/2315 train_time:36783ms step_avg:60.10ms
step:613/2315 train_time:36843ms step_avg:60.10ms
step:614/2315 train_time:36903ms step_avg:60.10ms
step:615/2315 train_time:36963ms step_avg:60.10ms
step:616/2315 train_time:37024ms step_avg:60.10ms
step:617/2315 train_time:37084ms step_avg:60.10ms
step:618/2315 train_time:37143ms step_avg:60.10ms
step:619/2315 train_time:37203ms step_avg:60.10ms
step:620/2315 train_time:37262ms step_avg:60.10ms
step:621/2315 train_time:37324ms step_avg:60.10ms
step:622/2315 train_time:37383ms step_avg:60.10ms
step:623/2315 train_time:37443ms step_avg:60.10ms
step:624/2315 train_time:37503ms step_avg:60.10ms
step:625/2315 train_time:37563ms step_avg:60.10ms
step:626/2315 train_time:37622ms step_avg:60.10ms
step:627/2315 train_time:37682ms step_avg:60.10ms
step:628/2315 train_time:37742ms step_avg:60.10ms
step:629/2315 train_time:37802ms step_avg:60.10ms
step:630/2315 train_time:37862ms step_avg:60.10ms
step:631/2315 train_time:37922ms step_avg:60.10ms
step:632/2315 train_time:37982ms step_avg:60.10ms
step:633/2315 train_time:38043ms step_avg:60.10ms
step:634/2315 train_time:38103ms step_avg:60.10ms
step:635/2315 train_time:38163ms step_avg:60.10ms
step:636/2315 train_time:38223ms step_avg:60.10ms
step:637/2315 train_time:38282ms step_avg:60.10ms
step:638/2315 train_time:38342ms step_avg:60.10ms
step:639/2315 train_time:38402ms step_avg:60.10ms
step:640/2315 train_time:38462ms step_avg:60.10ms
step:641/2315 train_time:38523ms step_avg:60.10ms
step:642/2315 train_time:38583ms step_avg:60.10ms
step:643/2315 train_time:38643ms step_avg:60.10ms
step:644/2315 train_time:38703ms step_avg:60.10ms
step:645/2315 train_time:38763ms step_avg:60.10ms
step:646/2315 train_time:38823ms step_avg:60.10ms
step:647/2315 train_time:38882ms step_avg:60.10ms
step:648/2315 train_time:38942ms step_avg:60.10ms
step:649/2315 train_time:39002ms step_avg:60.10ms
step:650/2315 train_time:39063ms step_avg:60.10ms
step:651/2315 train_time:39122ms step_avg:60.10ms
step:652/2315 train_time:39182ms step_avg:60.09ms
step:653/2315 train_time:39242ms step_avg:60.10ms
step:654/2315 train_time:39302ms step_avg:60.10ms
step:655/2315 train_time:39363ms step_avg:60.10ms
step:656/2315 train_time:39422ms step_avg:60.10ms
step:657/2315 train_time:39483ms step_avg:60.10ms
step:658/2315 train_time:39542ms step_avg:60.09ms
step:659/2315 train_time:39602ms step_avg:60.09ms
step:660/2315 train_time:39663ms step_avg:60.10ms
step:661/2315 train_time:39723ms step_avg:60.09ms
step:662/2315 train_time:39782ms step_avg:60.09ms
step:663/2315 train_time:39842ms step_avg:60.09ms
step:664/2315 train_time:39902ms step_avg:60.09ms
step:665/2315 train_time:39963ms step_avg:60.09ms
step:666/2315 train_time:40023ms step_avg:60.09ms
step:667/2315 train_time:40083ms step_avg:60.09ms
step:668/2315 train_time:40142ms step_avg:60.09ms
step:669/2315 train_time:40203ms step_avg:60.09ms
step:670/2315 train_time:40263ms step_avg:60.09ms
step:671/2315 train_time:40323ms step_avg:60.09ms
step:672/2315 train_time:40383ms step_avg:60.09ms
step:673/2315 train_time:40443ms step_avg:60.09ms
step:674/2315 train_time:40503ms step_avg:60.09ms
step:675/2315 train_time:40562ms step_avg:60.09ms
step:676/2315 train_time:40622ms step_avg:60.09ms
step:677/2315 train_time:40682ms step_avg:60.09ms
step:678/2315 train_time:40742ms step_avg:60.09ms
step:679/2315 train_time:40802ms step_avg:60.09ms
step:680/2315 train_time:40862ms step_avg:60.09ms
step:681/2315 train_time:40922ms step_avg:60.09ms
step:682/2315 train_time:40982ms step_avg:60.09ms
step:683/2315 train_time:41042ms step_avg:60.09ms
step:684/2315 train_time:41102ms step_avg:60.09ms
step:685/2315 train_time:41162ms step_avg:60.09ms
step:686/2315 train_time:41223ms step_avg:60.09ms
step:687/2315 train_time:41283ms step_avg:60.09ms
step:688/2315 train_time:41343ms step_avg:60.09ms
step:689/2315 train_time:41403ms step_avg:60.09ms
step:690/2315 train_time:41462ms step_avg:60.09ms
step:691/2315 train_time:41523ms step_avg:60.09ms
step:692/2315 train_time:41583ms step_avg:60.09ms
step:693/2315 train_time:41643ms step_avg:60.09ms
step:694/2315 train_time:41702ms step_avg:60.09ms
step:695/2315 train_time:41763ms step_avg:60.09ms
step:696/2315 train_time:41822ms step_avg:60.09ms
step:697/2315 train_time:41882ms step_avg:60.09ms
step:698/2315 train_time:41942ms step_avg:60.09ms
step:699/2315 train_time:42002ms step_avg:60.09ms
step:700/2315 train_time:42062ms step_avg:60.09ms
step:701/2315 train_time:42122ms step_avg:60.09ms
step:702/2315 train_time:42181ms step_avg:60.09ms
step:703/2315 train_time:42242ms step_avg:60.09ms
step:704/2315 train_time:42302ms step_avg:60.09ms
step:705/2315 train_time:42362ms step_avg:60.09ms
step:706/2315 train_time:42422ms step_avg:60.09ms
step:707/2315 train_time:42482ms step_avg:60.09ms
step:708/2315 train_time:42541ms step_avg:60.09ms
step:709/2315 train_time:42601ms step_avg:60.09ms
step:710/2315 train_time:42661ms step_avg:60.09ms
step:711/2315 train_time:42721ms step_avg:60.09ms
step:712/2315 train_time:42781ms step_avg:60.09ms
step:713/2315 train_time:42841ms step_avg:60.09ms
step:714/2315 train_time:42901ms step_avg:60.09ms
step:715/2315 train_time:42961ms step_avg:60.09ms
step:716/2315 train_time:43021ms step_avg:60.09ms
step:717/2315 train_time:43081ms step_avg:60.09ms
step:718/2315 train_time:43141ms step_avg:60.09ms
step:719/2315 train_time:43202ms step_avg:60.09ms
step:720/2315 train_time:43262ms step_avg:60.09ms
step:721/2315 train_time:43322ms step_avg:60.09ms
step:722/2315 train_time:43382ms step_avg:60.09ms
step:723/2315 train_time:43442ms step_avg:60.09ms
step:724/2315 train_time:43502ms step_avg:60.09ms
step:725/2315 train_time:43562ms step_avg:60.08ms
step:726/2315 train_time:43621ms step_avg:60.08ms
step:727/2315 train_time:43682ms step_avg:60.09ms
step:728/2315 train_time:43742ms step_avg:60.09ms
step:729/2315 train_time:43802ms step_avg:60.08ms
step:730/2315 train_time:43862ms step_avg:60.08ms
step:731/2315 train_time:43922ms step_avg:60.08ms
step:732/2315 train_time:43982ms step_avg:60.08ms
step:733/2315 train_time:44042ms step_avg:60.08ms
step:734/2315 train_time:44102ms step_avg:60.08ms
step:735/2315 train_time:44162ms step_avg:60.08ms
step:736/2315 train_time:44221ms step_avg:60.08ms
step:737/2315 train_time:44281ms step_avg:60.08ms
step:738/2315 train_time:44341ms step_avg:60.08ms
step:739/2315 train_time:44401ms step_avg:60.08ms
step:740/2315 train_time:44461ms step_avg:60.08ms
step:741/2315 train_time:44521ms step_avg:60.08ms
step:742/2315 train_time:44582ms step_avg:60.08ms
step:743/2315 train_time:44642ms step_avg:60.08ms
step:744/2315 train_time:44702ms step_avg:60.08ms
step:745/2315 train_time:44761ms step_avg:60.08ms
step:746/2315 train_time:44822ms step_avg:60.08ms
step:747/2315 train_time:44882ms step_avg:60.08ms
step:748/2315 train_time:44942ms step_avg:60.08ms
step:749/2315 train_time:45002ms step_avg:60.08ms
step:750/2315 train_time:45063ms step_avg:60.08ms
step:750/2315 val_loss:3.6833 train_time:45125ms step_avg:60.17ms
step:751/2315 train_time:45145ms step_avg:60.11ms
step:752/2315 train_time:45185ms step_avg:60.09ms
step:753/2315 train_time:45248ms step_avg:60.09ms
step:754/2315 train_time:45312ms step_avg:60.10ms
step:755/2315 train_time:45372ms step_avg:60.10ms
step:756/2315 train_time:45432ms step_avg:60.10ms
step:757/2315 train_time:45492ms step_avg:60.09ms
step:758/2315 train_time:45552ms step_avg:60.09ms
step:759/2315 train_time:45612ms step_avg:60.09ms
step:760/2315 train_time:45671ms step_avg:60.09ms
step:761/2315 train_time:45732ms step_avg:60.09ms
step:762/2315 train_time:45793ms step_avg:60.10ms
step:763/2315 train_time:45854ms step_avg:60.10ms
step:764/2315 train_time:45914ms step_avg:60.10ms
step:765/2315 train_time:45974ms step_avg:60.10ms
step:766/2315 train_time:46034ms step_avg:60.10ms
step:767/2315 train_time:46095ms step_avg:60.10ms
step:768/2315 train_time:46157ms step_avg:60.10ms
step:769/2315 train_time:46219ms step_avg:60.10ms
step:770/2315 train_time:46280ms step_avg:60.10ms
step:771/2315 train_time:46342ms step_avg:60.11ms
step:772/2315 train_time:46402ms step_avg:60.11ms
step:773/2315 train_time:46463ms step_avg:60.11ms
step:774/2315 train_time:46524ms step_avg:60.11ms
step:775/2315 train_time:46585ms step_avg:60.11ms
step:776/2315 train_time:46645ms step_avg:60.11ms
step:777/2315 train_time:46706ms step_avg:60.11ms
step:778/2315 train_time:46766ms step_avg:60.11ms
step:779/2315 train_time:46827ms step_avg:60.11ms
step:780/2315 train_time:46887ms step_avg:60.11ms
step:781/2315 train_time:46948ms step_avg:60.11ms
step:782/2315 train_time:47008ms step_avg:60.11ms
step:783/2315 train_time:47068ms step_avg:60.11ms
step:784/2315 train_time:47130ms step_avg:60.11ms
step:785/2315 train_time:47192ms step_avg:60.12ms
step:786/2315 train_time:47253ms step_avg:60.12ms
step:787/2315 train_time:47314ms step_avg:60.12ms
step:788/2315 train_time:47375ms step_avg:60.12ms
step:789/2315 train_time:47436ms step_avg:60.12ms
step:790/2315 train_time:47496ms step_avg:60.12ms
step:791/2315 train_time:47557ms step_avg:60.12ms
step:792/2315 train_time:47619ms step_avg:60.12ms
step:793/2315 train_time:47680ms step_avg:60.13ms
step:794/2315 train_time:47741ms step_avg:60.13ms
step:795/2315 train_time:47801ms step_avg:60.13ms
step:796/2315 train_time:47862ms step_avg:60.13ms
step:797/2315 train_time:47924ms step_avg:60.13ms
step:798/2315 train_time:47985ms step_avg:60.13ms
step:799/2315 train_time:48046ms step_avg:60.13ms
step:800/2315 train_time:48106ms step_avg:60.13ms
step:801/2315 train_time:48167ms step_avg:60.13ms
step:802/2315 train_time:48227ms step_avg:60.13ms
step:803/2315 train_time:48287ms step_avg:60.13ms
step:804/2315 train_time:48348ms step_avg:60.13ms
step:805/2315 train_time:48410ms step_avg:60.14ms
step:806/2315 train_time:48470ms step_avg:60.14ms
step:807/2315 train_time:48532ms step_avg:60.14ms
step:808/2315 train_time:48593ms step_avg:60.14ms
step:809/2315 train_time:48654ms step_avg:60.14ms
step:810/2315 train_time:48714ms step_avg:60.14ms
step:811/2315 train_time:48776ms step_avg:60.14ms
step:812/2315 train_time:48836ms step_avg:60.14ms
step:813/2315 train_time:48897ms step_avg:60.14ms
step:814/2315 train_time:48958ms step_avg:60.14ms
step:815/2315 train_time:49019ms step_avg:60.15ms
step:816/2315 train_time:49080ms step_avg:60.15ms
step:817/2315 train_time:49141ms step_avg:60.15ms
step:818/2315 train_time:49202ms step_avg:60.15ms
step:819/2315 train_time:49263ms step_avg:60.15ms
step:820/2315 train_time:49324ms step_avg:60.15ms
step:821/2315 train_time:49386ms step_avg:60.15ms
step:822/2315 train_time:49447ms step_avg:60.15ms
step:823/2315 train_time:49508ms step_avg:60.16ms
step:824/2315 train_time:49568ms step_avg:60.15ms
step:825/2315 train_time:49629ms step_avg:60.16ms
step:826/2315 train_time:49690ms step_avg:60.16ms
step:827/2315 train_time:49751ms step_avg:60.16ms
step:828/2315 train_time:49811ms step_avg:60.16ms
step:829/2315 train_time:49872ms step_avg:60.16ms
step:830/2315 train_time:49934ms step_avg:60.16ms
step:831/2315 train_time:49996ms step_avg:60.16ms
step:832/2315 train_time:50056ms step_avg:60.16ms
step:833/2315 train_time:50116ms step_avg:60.16ms
step:834/2315 train_time:50178ms step_avg:60.16ms
step:835/2315 train_time:50238ms step_avg:60.17ms
step:836/2315 train_time:50299ms step_avg:60.17ms
step:837/2315 train_time:50360ms step_avg:60.17ms
step:838/2315 train_time:50421ms step_avg:60.17ms
step:839/2315 train_time:50482ms step_avg:60.17ms
step:840/2315 train_time:50543ms step_avg:60.17ms
step:841/2315 train_time:50604ms step_avg:60.17ms
step:842/2315 train_time:50665ms step_avg:60.17ms
step:843/2315 train_time:50726ms step_avg:60.17ms
step:844/2315 train_time:50786ms step_avg:60.17ms
step:845/2315 train_time:50847ms step_avg:60.17ms
step:846/2315 train_time:50907ms step_avg:60.17ms
step:847/2315 train_time:50968ms step_avg:60.18ms
step:848/2315 train_time:51029ms step_avg:60.18ms
step:849/2315 train_time:51090ms step_avg:60.18ms
step:850/2315 train_time:51151ms step_avg:60.18ms
step:851/2315 train_time:51212ms step_avg:60.18ms
step:852/2315 train_time:51273ms step_avg:60.18ms
step:853/2315 train_time:51333ms step_avg:60.18ms
step:854/2315 train_time:51394ms step_avg:60.18ms
step:855/2315 train_time:51455ms step_avg:60.18ms
step:856/2315 train_time:51516ms step_avg:60.18ms
step:857/2315 train_time:51577ms step_avg:60.18ms
step:858/2315 train_time:51638ms step_avg:60.18ms
step:859/2315 train_time:51700ms step_avg:60.19ms
step:860/2315 train_time:51761ms step_avg:60.19ms
step:861/2315 train_time:51821ms step_avg:60.19ms
step:862/2315 train_time:51882ms step_avg:60.19ms
step:863/2315 train_time:51943ms step_avg:60.19ms
step:864/2315 train_time:52003ms step_avg:60.19ms
step:865/2315 train_time:52064ms step_avg:60.19ms
step:866/2315 train_time:52125ms step_avg:60.19ms
step:867/2315 train_time:52186ms step_avg:60.19ms
step:868/2315 train_time:52247ms step_avg:60.19ms
step:869/2315 train_time:52307ms step_avg:60.19ms
step:870/2315 train_time:52368ms step_avg:60.19ms
step:871/2315 train_time:52429ms step_avg:60.19ms
step:872/2315 train_time:52490ms step_avg:60.20ms
step:873/2315 train_time:52552ms step_avg:60.20ms
step:874/2315 train_time:52614ms step_avg:60.20ms
step:875/2315 train_time:52675ms step_avg:60.20ms
step:876/2315 train_time:52735ms step_avg:60.20ms
step:877/2315 train_time:52796ms step_avg:60.20ms
step:878/2315 train_time:52857ms step_avg:60.20ms
step:879/2315 train_time:52918ms step_avg:60.20ms
step:880/2315 train_time:52979ms step_avg:60.20ms
step:881/2315 train_time:53040ms step_avg:60.20ms
step:882/2315 train_time:53101ms step_avg:60.20ms
step:883/2315 train_time:53161ms step_avg:60.21ms
step:884/2315 train_time:53222ms step_avg:60.21ms
step:885/2315 train_time:53283ms step_avg:60.21ms
step:886/2315 train_time:53344ms step_avg:60.21ms
step:887/2315 train_time:53405ms step_avg:60.21ms
step:888/2315 train_time:53466ms step_avg:60.21ms
step:889/2315 train_time:53527ms step_avg:60.21ms
step:890/2315 train_time:53587ms step_avg:60.21ms
step:891/2315 train_time:53647ms step_avg:60.21ms
step:892/2315 train_time:53708ms step_avg:60.21ms
step:893/2315 train_time:53769ms step_avg:60.21ms
step:894/2315 train_time:53829ms step_avg:60.21ms
step:895/2315 train_time:53891ms step_avg:60.21ms
step:896/2315 train_time:53952ms step_avg:60.21ms
step:897/2315 train_time:54013ms step_avg:60.21ms
step:898/2315 train_time:54073ms step_avg:60.22ms
step:899/2315 train_time:54134ms step_avg:60.22ms
step:900/2315 train_time:54195ms step_avg:60.22ms
step:901/2315 train_time:54256ms step_avg:60.22ms
step:902/2315 train_time:54316ms step_avg:60.22ms
step:903/2315 train_time:54378ms step_avg:60.22ms
step:904/2315 train_time:54438ms step_avg:60.22ms
step:905/2315 train_time:54500ms step_avg:60.22ms
step:906/2315 train_time:54561ms step_avg:60.22ms
step:907/2315 train_time:54622ms step_avg:60.22ms
step:908/2315 train_time:54682ms step_avg:60.22ms
step:909/2315 train_time:54743ms step_avg:60.22ms
step:910/2315 train_time:54804ms step_avg:60.22ms
step:911/2315 train_time:54865ms step_avg:60.22ms
step:912/2315 train_time:54925ms step_avg:60.22ms
step:913/2315 train_time:54986ms step_avg:60.23ms
step:914/2315 train_time:55046ms step_avg:60.23ms
step:915/2315 train_time:55107ms step_avg:60.23ms
step:916/2315 train_time:55167ms step_avg:60.23ms
step:917/2315 train_time:55229ms step_avg:60.23ms
step:918/2315 train_time:55290ms step_avg:60.23ms
step:919/2315 train_time:55351ms step_avg:60.23ms
step:920/2315 train_time:55412ms step_avg:60.23ms
step:921/2315 train_time:55473ms step_avg:60.23ms
step:922/2315 train_time:55534ms step_avg:60.23ms
step:923/2315 train_time:55595ms step_avg:60.23ms
step:924/2315 train_time:55656ms step_avg:60.23ms
step:925/2315 train_time:55716ms step_avg:60.23ms
step:926/2315 train_time:55777ms step_avg:60.23ms
step:927/2315 train_time:55838ms step_avg:60.24ms
step:928/2315 train_time:55899ms step_avg:60.24ms
step:929/2315 train_time:55960ms step_avg:60.24ms
step:930/2315 train_time:56020ms step_avg:60.24ms
step:931/2315 train_time:56082ms step_avg:60.24ms
step:932/2315 train_time:56142ms step_avg:60.24ms
step:933/2315 train_time:56203ms step_avg:60.24ms
step:934/2315 train_time:56264ms step_avg:60.24ms
step:935/2315 train_time:56325ms step_avg:60.24ms
step:936/2315 train_time:56385ms step_avg:60.24ms
step:937/2315 train_time:56446ms step_avg:60.24ms
step:938/2315 train_time:56507ms step_avg:60.24ms
step:939/2315 train_time:56567ms step_avg:60.24ms
step:940/2315 train_time:56628ms step_avg:60.24ms
step:941/2315 train_time:56689ms step_avg:60.24ms
step:942/2315 train_time:56750ms step_avg:60.24ms
step:943/2315 train_time:56810ms step_avg:60.24ms
step:944/2315 train_time:56872ms step_avg:60.25ms
step:945/2315 train_time:56933ms step_avg:60.25ms
step:946/2315 train_time:56994ms step_avg:60.25ms
step:947/2315 train_time:57055ms step_avg:60.25ms
step:948/2315 train_time:57116ms step_avg:60.25ms
step:949/2315 train_time:57177ms step_avg:60.25ms
step:950/2315 train_time:57238ms step_avg:60.25ms
step:951/2315 train_time:57299ms step_avg:60.25ms
step:952/2315 train_time:57360ms step_avg:60.25ms
step:953/2315 train_time:57421ms step_avg:60.25ms
step:954/2315 train_time:57483ms step_avg:60.25ms
step:955/2315 train_time:57543ms step_avg:60.25ms
step:956/2315 train_time:57604ms step_avg:60.26ms
step:957/2315 train_time:57664ms step_avg:60.26ms
step:958/2315 train_time:57725ms step_avg:60.26ms
step:959/2315 train_time:57786ms step_avg:60.26ms
step:960/2315 train_time:57847ms step_avg:60.26ms
step:961/2315 train_time:57907ms step_avg:60.26ms
step:962/2315 train_time:57968ms step_avg:60.26ms
step:963/2315 train_time:58029ms step_avg:60.26ms
step:964/2315 train_time:58090ms step_avg:60.26ms
step:965/2315 train_time:58152ms step_avg:60.26ms
step:966/2315 train_time:58213ms step_avg:60.26ms
step:967/2315 train_time:58273ms step_avg:60.26ms
step:968/2315 train_time:58334ms step_avg:60.26ms
step:969/2315 train_time:58396ms step_avg:60.26ms
step:970/2315 train_time:58456ms step_avg:60.26ms
step:971/2315 train_time:58517ms step_avg:60.26ms
step:972/2315 train_time:58578ms step_avg:60.27ms
step:973/2315 train_time:58639ms step_avg:60.27ms
step:974/2315 train_time:58700ms step_avg:60.27ms
step:975/2315 train_time:58761ms step_avg:60.27ms
step:976/2315 train_time:58822ms step_avg:60.27ms
step:977/2315 train_time:58883ms step_avg:60.27ms
step:978/2315 train_time:58944ms step_avg:60.27ms
step:979/2315 train_time:59005ms step_avg:60.27ms
step:980/2315 train_time:59066ms step_avg:60.27ms
step:981/2315 train_time:59127ms step_avg:60.27ms
step:982/2315 train_time:59188ms step_avg:60.27ms
step:983/2315 train_time:59248ms step_avg:60.27ms
step:984/2315 train_time:59309ms step_avg:60.27ms
step:985/2315 train_time:59369ms step_avg:60.27ms
step:986/2315 train_time:59430ms step_avg:60.27ms
step:987/2315 train_time:59492ms step_avg:60.28ms
step:988/2315 train_time:59553ms step_avg:60.28ms
step:989/2315 train_time:59614ms step_avg:60.28ms
step:990/2315 train_time:59676ms step_avg:60.28ms
step:991/2315 train_time:59737ms step_avg:60.28ms
step:992/2315 train_time:59797ms step_avg:60.28ms
step:993/2315 train_time:59858ms step_avg:60.28ms
step:994/2315 train_time:59919ms step_avg:60.28ms
step:995/2315 train_time:59981ms step_avg:60.28ms
step:996/2315 train_time:60042ms step_avg:60.28ms
step:997/2315 train_time:60103ms step_avg:60.28ms
step:998/2315 train_time:60163ms step_avg:60.28ms
step:999/2315 train_time:60224ms step_avg:60.28ms
step:1000/2315 train_time:60285ms step_avg:60.28ms
step:1000/2315 val_loss:3.5718 train_time:60348ms step_avg:60.35ms
step:1001/2315 train_time:60370ms step_avg:60.31ms
step:1002/2315 train_time:60410ms step_avg:60.29ms
step:1003/2315 train_time:60474ms step_avg:60.29ms
step:1004/2315 train_time:60541ms step_avg:60.30ms
step:1005/2315 train_time:60605ms step_avg:60.30ms
step:1006/2315 train_time:60665ms step_avg:60.30ms
step:1007/2315 train_time:60726ms step_avg:60.30ms
step:1008/2315 train_time:60786ms step_avg:60.30ms
step:1009/2315 train_time:60846ms step_avg:60.30ms
step:1010/2315 train_time:60906ms step_avg:60.30ms
step:1011/2315 train_time:60966ms step_avg:60.30ms
step:1012/2315 train_time:61026ms step_avg:60.30ms
step:1013/2315 train_time:61086ms step_avg:60.30ms
step:1014/2315 train_time:61146ms step_avg:60.30ms
step:1015/2315 train_time:61206ms step_avg:60.30ms
step:1016/2315 train_time:61266ms step_avg:60.30ms
step:1017/2315 train_time:61327ms step_avg:60.30ms
step:1018/2315 train_time:61388ms step_avg:60.30ms
step:1019/2315 train_time:61451ms step_avg:60.30ms
step:1020/2315 train_time:61512ms step_avg:60.31ms
step:1021/2315 train_time:61574ms step_avg:60.31ms
step:1022/2315 train_time:61636ms step_avg:60.31ms
step:1023/2315 train_time:61698ms step_avg:60.31ms
step:1024/2315 train_time:61758ms step_avg:60.31ms
step:1025/2315 train_time:61819ms step_avg:60.31ms
step:1026/2315 train_time:61879ms step_avg:60.31ms
step:1027/2315 train_time:61940ms step_avg:60.31ms
step:1028/2315 train_time:62001ms step_avg:60.31ms
step:1029/2315 train_time:62060ms step_avg:60.31ms
step:1030/2315 train_time:62121ms step_avg:60.31ms
step:1031/2315 train_time:62182ms step_avg:60.31ms
step:1032/2315 train_time:62242ms step_avg:60.31ms
step:1033/2315 train_time:62304ms step_avg:60.31ms
step:1034/2315 train_time:62365ms step_avg:60.31ms
step:1035/2315 train_time:62427ms step_avg:60.32ms
step:1036/2315 train_time:62489ms step_avg:60.32ms
step:1037/2315 train_time:62551ms step_avg:60.32ms
step:1038/2315 train_time:62612ms step_avg:60.32ms
step:1039/2315 train_time:62673ms step_avg:60.32ms
step:1040/2315 train_time:62734ms step_avg:60.32ms
step:1041/2315 train_time:62795ms step_avg:60.32ms
step:1042/2315 train_time:62855ms step_avg:60.32ms
step:1043/2315 train_time:62916ms step_avg:60.32ms
step:1044/2315 train_time:62977ms step_avg:60.32ms
step:1045/2315 train_time:63038ms step_avg:60.32ms
step:1046/2315 train_time:63098ms step_avg:60.32ms
step:1047/2315 train_time:63159ms step_avg:60.32ms
step:1048/2315 train_time:63220ms step_avg:60.32ms
step:1049/2315 train_time:63280ms step_avg:60.32ms
step:1050/2315 train_time:63341ms step_avg:60.33ms
step:1051/2315 train_time:63403ms step_avg:60.33ms
step:1052/2315 train_time:63464ms step_avg:60.33ms
step:1053/2315 train_time:63526ms step_avg:60.33ms
step:1054/2315 train_time:63587ms step_avg:60.33ms
step:1055/2315 train_time:63648ms step_avg:60.33ms
step:1056/2315 train_time:63708ms step_avg:60.33ms
step:1057/2315 train_time:63769ms step_avg:60.33ms
step:1058/2315 train_time:63830ms step_avg:60.33ms
step:1059/2315 train_time:63890ms step_avg:60.33ms
step:1060/2315 train_time:63950ms step_avg:60.33ms
step:1061/2315 train_time:64011ms step_avg:60.33ms
step:1062/2315 train_time:64071ms step_avg:60.33ms
step:1063/2315 train_time:64131ms step_avg:60.33ms
step:1064/2315 train_time:64192ms step_avg:60.33ms
step:1065/2315 train_time:64253ms step_avg:60.33ms
step:1066/2315 train_time:64314ms step_avg:60.33ms
step:1067/2315 train_time:64376ms step_avg:60.33ms
step:1068/2315 train_time:64437ms step_avg:60.33ms
step:1069/2315 train_time:64498ms step_avg:60.33ms
step:1070/2315 train_time:64559ms step_avg:60.34ms
step:1071/2315 train_time:64620ms step_avg:60.34ms
step:1072/2315 train_time:64681ms step_avg:60.34ms
step:1073/2315 train_time:64742ms step_avg:60.34ms
step:1074/2315 train_time:64803ms step_avg:60.34ms
step:1075/2315 train_time:64864ms step_avg:60.34ms
step:1076/2315 train_time:64924ms step_avg:60.34ms
step:1077/2315 train_time:64986ms step_avg:60.34ms
step:1078/2315 train_time:65047ms step_avg:60.34ms
step:1079/2315 train_time:65108ms step_avg:60.34ms
step:1080/2315 train_time:65168ms step_avg:60.34ms
step:1081/2315 train_time:65229ms step_avg:60.34ms
step:1082/2315 train_time:65289ms step_avg:60.34ms
step:1083/2315 train_time:65350ms step_avg:60.34ms
step:1084/2315 train_time:65412ms step_avg:60.34ms
step:1085/2315 train_time:65472ms step_avg:60.34ms
step:1086/2315 train_time:65533ms step_avg:60.34ms
step:1087/2315 train_time:65593ms step_avg:60.34ms
step:1088/2315 train_time:65654ms step_avg:60.34ms
step:1089/2315 train_time:65715ms step_avg:60.34ms
step:1090/2315 train_time:65777ms step_avg:60.35ms
step:1091/2315 train_time:65837ms step_avg:60.35ms
step:1092/2315 train_time:65898ms step_avg:60.35ms
step:1093/2315 train_time:65959ms step_avg:60.35ms
step:1094/2315 train_time:66019ms step_avg:60.35ms
step:1095/2315 train_time:66080ms step_avg:60.35ms
step:1096/2315 train_time:66141ms step_avg:60.35ms
step:1097/2315 train_time:66202ms step_avg:60.35ms
step:1098/2315 train_time:66264ms step_avg:60.35ms
step:1099/2315 train_time:66325ms step_avg:60.35ms
step:1100/2315 train_time:66386ms step_avg:60.35ms
step:1101/2315 train_time:66448ms step_avg:60.35ms
step:1102/2315 train_time:66508ms step_avg:60.35ms
step:1103/2315 train_time:66569ms step_avg:60.35ms
step:1104/2315 train_time:66630ms step_avg:60.35ms
step:1105/2315 train_time:66690ms step_avg:60.35ms
step:1106/2315 train_time:66751ms step_avg:60.35ms
step:1107/2315 train_time:66812ms step_avg:60.35ms
step:1108/2315 train_time:66873ms step_avg:60.35ms
step:1109/2315 train_time:66934ms step_avg:60.36ms
step:1110/2315 train_time:66995ms step_avg:60.36ms
step:1111/2315 train_time:67057ms step_avg:60.36ms
step:1112/2315 train_time:67117ms step_avg:60.36ms
step:1113/2315 train_time:67178ms step_avg:60.36ms
step:1114/2315 train_time:67239ms step_avg:60.36ms
step:1115/2315 train_time:67300ms step_avg:60.36ms
step:1116/2315 train_time:67362ms step_avg:60.36ms
step:1117/2315 train_time:67423ms step_avg:60.36ms
step:1118/2315 train_time:67484ms step_avg:60.36ms
step:1119/2315 train_time:67546ms step_avg:60.36ms
step:1120/2315 train_time:67607ms step_avg:60.36ms
step:1121/2315 train_time:67667ms step_avg:60.36ms
step:1122/2315 train_time:67728ms step_avg:60.36ms
step:1123/2315 train_time:67790ms step_avg:60.36ms
step:1124/2315 train_time:67850ms step_avg:60.36ms
step:1125/2315 train_time:67911ms step_avg:60.37ms
step:1126/2315 train_time:67971ms step_avg:60.37ms
step:1127/2315 train_time:68032ms step_avg:60.37ms
step:1128/2315 train_time:68093ms step_avg:60.37ms
step:1129/2315 train_time:68155ms step_avg:60.37ms
step:1130/2315 train_time:68217ms step_avg:60.37ms
step:1131/2315 train_time:68277ms step_avg:60.37ms
step:1132/2315 train_time:68338ms step_avg:60.37ms
step:1133/2315 train_time:68400ms step_avg:60.37ms
step:1134/2315 train_time:68460ms step_avg:60.37ms
step:1135/2315 train_time:68521ms step_avg:60.37ms
step:1136/2315 train_time:68582ms step_avg:60.37ms
step:1137/2315 train_time:68644ms step_avg:60.37ms
step:1138/2315 train_time:68704ms step_avg:60.37ms
step:1139/2315 train_time:68765ms step_avg:60.37ms
step:1140/2315 train_time:68826ms step_avg:60.37ms
step:1141/2315 train_time:68887ms step_avg:60.37ms
step:1142/2315 train_time:68948ms step_avg:60.38ms
step:1143/2315 train_time:69009ms step_avg:60.38ms
step:1144/2315 train_time:69070ms step_avg:60.38ms
step:1145/2315 train_time:69131ms step_avg:60.38ms
step:1146/2315 train_time:69191ms step_avg:60.38ms
step:1147/2315 train_time:69252ms step_avg:60.38ms
step:1148/2315 train_time:69312ms step_avg:60.38ms
step:1149/2315 train_time:69373ms step_avg:60.38ms
step:1150/2315 train_time:69434ms step_avg:60.38ms
step:1151/2315 train_time:69495ms step_avg:60.38ms
step:1152/2315 train_time:69556ms step_avg:60.38ms
step:1153/2315 train_time:69618ms step_avg:60.38ms
step:1154/2315 train_time:69679ms step_avg:60.38ms
step:1155/2315 train_time:69740ms step_avg:60.38ms
step:1156/2315 train_time:69801ms step_avg:60.38ms
step:1157/2315 train_time:69862ms step_avg:60.38ms
step:1158/2315 train_time:69923ms step_avg:60.38ms
step:1159/2315 train_time:69985ms step_avg:60.38ms
step:1160/2315 train_time:70046ms step_avg:60.38ms
step:1161/2315 train_time:70107ms step_avg:60.39ms
step:1162/2315 train_time:70168ms step_avg:60.39ms
step:1163/2315 train_time:70229ms step_avg:60.39ms
step:1164/2315 train_time:70289ms step_avg:60.39ms
step:1165/2315 train_time:70350ms step_avg:60.39ms
step:1166/2315 train_time:70411ms step_avg:60.39ms
step:1167/2315 train_time:70472ms step_avg:60.39ms
step:1168/2315 train_time:70533ms step_avg:60.39ms
step:1169/2315 train_time:70594ms step_avg:60.39ms
step:1170/2315 train_time:70656ms step_avg:60.39ms
step:1171/2315 train_time:70718ms step_avg:60.39ms
step:1172/2315 train_time:70778ms step_avg:60.39ms
step:1173/2315 train_time:70838ms step_avg:60.39ms
step:1174/2315 train_time:70899ms step_avg:60.39ms
step:1175/2315 train_time:70961ms step_avg:60.39ms
step:1176/2315 train_time:71022ms step_avg:60.39ms
step:1177/2315 train_time:71083ms step_avg:60.39ms
step:1178/2315 train_time:71144ms step_avg:60.39ms
step:1179/2315 train_time:71205ms step_avg:60.39ms
step:1180/2315 train_time:71266ms step_avg:60.39ms
step:1181/2315 train_time:71327ms step_avg:60.40ms
step:1182/2315 train_time:71387ms step_avg:60.40ms
step:1183/2315 train_time:71449ms step_avg:60.40ms
step:1184/2315 train_time:71509ms step_avg:60.40ms
step:1185/2315 train_time:71570ms step_avg:60.40ms
step:1186/2315 train_time:71630ms step_avg:60.40ms
step:1187/2315 train_time:71691ms step_avg:60.40ms
step:1188/2315 train_time:71752ms step_avg:60.40ms
step:1189/2315 train_time:71813ms step_avg:60.40ms
step:1190/2315 train_time:71873ms step_avg:60.40ms
step:1191/2315 train_time:71933ms step_avg:60.40ms
step:1192/2315 train_time:71996ms step_avg:60.40ms
step:1193/2315 train_time:72056ms step_avg:60.40ms
step:1194/2315 train_time:72116ms step_avg:60.40ms
step:1195/2315 train_time:72177ms step_avg:60.40ms
step:1196/2315 train_time:72239ms step_avg:60.40ms
step:1197/2315 train_time:72300ms step_avg:60.40ms
step:1198/2315 train_time:72360ms step_avg:60.40ms
step:1199/2315 train_time:72422ms step_avg:60.40ms
step:1200/2315 train_time:72483ms step_avg:60.40ms
step:1201/2315 train_time:72544ms step_avg:60.40ms
step:1202/2315 train_time:72604ms step_avg:60.40ms
step:1203/2315 train_time:72666ms step_avg:60.40ms
step:1204/2315 train_time:72727ms step_avg:60.40ms
step:1205/2315 train_time:72788ms step_avg:60.40ms
step:1206/2315 train_time:72848ms step_avg:60.40ms
step:1207/2315 train_time:72909ms step_avg:60.40ms
step:1208/2315 train_time:72969ms step_avg:60.41ms
step:1209/2315 train_time:73030ms step_avg:60.41ms
step:1210/2315 train_time:73091ms step_avg:60.41ms
step:1211/2315 train_time:73152ms step_avg:60.41ms
step:1212/2315 train_time:73213ms step_avg:60.41ms
step:1213/2315 train_time:73274ms step_avg:60.41ms
step:1214/2315 train_time:73335ms step_avg:60.41ms
step:1215/2315 train_time:73396ms step_avg:60.41ms
step:1216/2315 train_time:73457ms step_avg:60.41ms
step:1217/2315 train_time:73518ms step_avg:60.41ms
step:1218/2315 train_time:73579ms step_avg:60.41ms
step:1219/2315 train_time:73640ms step_avg:60.41ms
step:1220/2315 train_time:73701ms step_avg:60.41ms
step:1221/2315 train_time:73763ms step_avg:60.41ms
step:1222/2315 train_time:73824ms step_avg:60.41ms
step:1223/2315 train_time:73885ms step_avg:60.41ms
step:1224/2315 train_time:73945ms step_avg:60.41ms
step:1225/2315 train_time:74007ms step_avg:60.41ms
step:1226/2315 train_time:74068ms step_avg:60.41ms
step:1227/2315 train_time:74129ms step_avg:60.41ms
step:1228/2315 train_time:74189ms step_avg:60.41ms
step:1229/2315 train_time:74250ms step_avg:60.41ms
step:1230/2315 train_time:74310ms step_avg:60.41ms
step:1231/2315 train_time:74371ms step_avg:60.42ms
step:1232/2315 train_time:74432ms step_avg:60.42ms
step:1233/2315 train_time:74493ms step_avg:60.42ms
step:1234/2315 train_time:74554ms step_avg:60.42ms
step:1235/2315 train_time:74615ms step_avg:60.42ms
step:1236/2315 train_time:74676ms step_avg:60.42ms
step:1237/2315 train_time:74737ms step_avg:60.42ms
step:1238/2315 train_time:74798ms step_avg:60.42ms
step:1239/2315 train_time:74859ms step_avg:60.42ms
step:1240/2315 train_time:74919ms step_avg:60.42ms
step:1241/2315 train_time:74980ms step_avg:60.42ms
step:1242/2315 train_time:75042ms step_avg:60.42ms
step:1243/2315 train_time:75104ms step_avg:60.42ms
step:1244/2315 train_time:75164ms step_avg:60.42ms
step:1245/2315 train_time:75226ms step_avg:60.42ms
step:1246/2315 train_time:75287ms step_avg:60.42ms
step:1247/2315 train_time:75348ms step_avg:60.42ms
step:1248/2315 train_time:75408ms step_avg:60.42ms
step:1249/2315 train_time:75470ms step_avg:60.42ms
step:1250/2315 train_time:75530ms step_avg:60.42ms
step:1250/2315 val_loss:3.5145 train_time:75593ms step_avg:60.47ms
step:1251/2315 train_time:75613ms step_avg:60.44ms
step:1252/2315 train_time:75654ms step_avg:60.43ms
step:1253/2315 train_time:75719ms step_avg:60.43ms
step:1254/2315 train_time:75783ms step_avg:60.43ms
step:1255/2315 train_time:75844ms step_avg:60.43ms
step:1256/2315 train_time:75906ms step_avg:60.43ms
step:1257/2315 train_time:75966ms step_avg:60.43ms
step:1258/2315 train_time:76026ms step_avg:60.43ms
step:1259/2315 train_time:76086ms step_avg:60.43ms
step:1260/2315 train_time:76147ms step_avg:60.43ms
step:1261/2315 train_time:76207ms step_avg:60.43ms
step:1262/2315 train_time:76267ms step_avg:60.43ms
step:1263/2315 train_time:76326ms step_avg:60.43ms
step:1264/2315 train_time:76386ms step_avg:60.43ms
step:1265/2315 train_time:76446ms step_avg:60.43ms
step:1266/2315 train_time:76507ms step_avg:60.43ms
step:1267/2315 train_time:76568ms step_avg:60.43ms
step:1268/2315 train_time:76630ms step_avg:60.43ms
step:1269/2315 train_time:76692ms step_avg:60.44ms
step:1270/2315 train_time:76755ms step_avg:60.44ms
step:1271/2315 train_time:76817ms step_avg:60.44ms
step:1272/2315 train_time:76878ms step_avg:60.44ms
step:1273/2315 train_time:76938ms step_avg:60.44ms
step:1274/2315 train_time:76999ms step_avg:60.44ms
step:1275/2315 train_time:77059ms step_avg:60.44ms
step:1276/2315 train_time:77119ms step_avg:60.44ms
step:1277/2315 train_time:77180ms step_avg:60.44ms
step:1278/2315 train_time:77240ms step_avg:60.44ms
step:1279/2315 train_time:77301ms step_avg:60.44ms
step:1280/2315 train_time:77361ms step_avg:60.44ms
step:1281/2315 train_time:77422ms step_avg:60.44ms
step:1282/2315 train_time:77482ms step_avg:60.44ms
step:1283/2315 train_time:77543ms step_avg:60.44ms
step:1284/2315 train_time:77604ms step_avg:60.44ms
step:1285/2315 train_time:77666ms step_avg:60.44ms
step:1286/2315 train_time:77729ms step_avg:60.44ms
step:1287/2315 train_time:77790ms step_avg:60.44ms
step:1288/2315 train_time:77850ms step_avg:60.44ms
step:1289/2315 train_time:77911ms step_avg:60.44ms
step:1290/2315 train_time:77972ms step_avg:60.44ms
step:1291/2315 train_time:78033ms step_avg:60.44ms
step:1292/2315 train_time:78093ms step_avg:60.44ms
step:1293/2315 train_time:78153ms step_avg:60.44ms
step:1294/2315 train_time:78214ms step_avg:60.44ms
step:1295/2315 train_time:78275ms step_avg:60.44ms
step:1296/2315 train_time:78336ms step_avg:60.44ms
step:1297/2315 train_time:78397ms step_avg:60.45ms
step:1298/2315 train_time:78459ms step_avg:60.45ms
step:1299/2315 train_time:78520ms step_avg:60.45ms
step:1300/2315 train_time:78581ms step_avg:60.45ms
step:1301/2315 train_time:78642ms step_avg:60.45ms
step:1302/2315 train_time:78703ms step_avg:60.45ms
step:1303/2315 train_time:78764ms step_avg:60.45ms
step:1304/2315 train_time:78825ms step_avg:60.45ms
step:1305/2315 train_time:78887ms step_avg:60.45ms
step:1306/2315 train_time:78948ms step_avg:60.45ms
step:1307/2315 train_time:79009ms step_avg:60.45ms
step:1308/2315 train_time:79069ms step_avg:60.45ms
step:1309/2315 train_time:79130ms step_avg:60.45ms
step:1310/2315 train_time:79190ms step_avg:60.45ms
step:1311/2315 train_time:79251ms step_avg:60.45ms
step:1312/2315 train_time:79312ms step_avg:60.45ms
step:1313/2315 train_time:79373ms step_avg:60.45ms
step:1314/2315 train_time:79434ms step_avg:60.45ms
step:1315/2315 train_time:79495ms step_avg:60.45ms
step:1316/2315 train_time:79556ms step_avg:60.45ms
step:1317/2315 train_time:79618ms step_avg:60.45ms
step:1318/2315 train_time:79679ms step_avg:60.45ms
step:1319/2315 train_time:79740ms step_avg:60.46ms
step:1320/2315 train_time:79801ms step_avg:60.46ms
step:1321/2315 train_time:79862ms step_avg:60.46ms
step:1322/2315 train_time:79923ms step_avg:60.46ms
step:1323/2315 train_time:79984ms step_avg:60.46ms
step:1324/2315 train_time:80044ms step_avg:60.46ms
step:1325/2315 train_time:80105ms step_avg:60.46ms
step:1326/2315 train_time:80165ms step_avg:60.46ms
step:1327/2315 train_time:80226ms step_avg:60.46ms
step:1328/2315 train_time:80287ms step_avg:60.46ms
step:1329/2315 train_time:80348ms step_avg:60.46ms
step:1330/2315 train_time:80409ms step_avg:60.46ms
step:1331/2315 train_time:80470ms step_avg:60.46ms
step:1332/2315 train_time:80531ms step_avg:60.46ms
step:1333/2315 train_time:80593ms step_avg:60.46ms
step:1334/2315 train_time:80654ms step_avg:60.46ms
step:1335/2315 train_time:80715ms step_avg:60.46ms
step:1336/2315 train_time:80776ms step_avg:60.46ms
step:1337/2315 train_time:80837ms step_avg:60.46ms
step:1338/2315 train_time:80898ms step_avg:60.46ms
step:1339/2315 train_time:80959ms step_avg:60.46ms
step:1340/2315 train_time:81020ms step_avg:60.46ms
step:1341/2315 train_time:81080ms step_avg:60.46ms
step:1342/2315 train_time:81141ms step_avg:60.46ms
step:1343/2315 train_time:81201ms step_avg:60.46ms
step:1344/2315 train_time:81262ms step_avg:60.46ms
step:1345/2315 train_time:81323ms step_avg:60.46ms
step:1346/2315 train_time:81384ms step_avg:60.46ms
step:1347/2315 train_time:81445ms step_avg:60.46ms
step:1348/2315 train_time:81506ms step_avg:60.46ms
step:1349/2315 train_time:81568ms step_avg:60.47ms
step:1350/2315 train_time:81629ms step_avg:60.47ms
step:1351/2315 train_time:81689ms step_avg:60.47ms
step:1352/2315 train_time:81751ms step_avg:60.47ms
step:1353/2315 train_time:81812ms step_avg:60.47ms
step:1354/2315 train_time:81873ms step_avg:60.47ms
step:1355/2315 train_time:81934ms step_avg:60.47ms
step:1356/2315 train_time:81995ms step_avg:60.47ms
step:1357/2315 train_time:82056ms step_avg:60.47ms
step:1358/2315 train_time:82117ms step_avg:60.47ms
step:1359/2315 train_time:82178ms step_avg:60.47ms
step:1360/2315 train_time:82238ms step_avg:60.47ms
step:1361/2315 train_time:82299ms step_avg:60.47ms
step:1362/2315 train_time:82360ms step_avg:60.47ms
step:1363/2315 train_time:82420ms step_avg:60.47ms
step:1364/2315 train_time:82481ms step_avg:60.47ms
step:1365/2315 train_time:82542ms step_avg:60.47ms
step:1366/2315 train_time:82603ms step_avg:60.47ms
step:1367/2315 train_time:82665ms step_avg:60.47ms
step:1368/2315 train_time:82726ms step_avg:60.47ms
step:1369/2315 train_time:82787ms step_avg:60.47ms
step:1370/2315 train_time:82848ms step_avg:60.47ms
step:1371/2315 train_time:82909ms step_avg:60.47ms
step:1372/2315 train_time:82969ms step_avg:60.47ms
step:1373/2315 train_time:83030ms step_avg:60.47ms
step:1374/2315 train_time:83091ms step_avg:60.47ms
step:1375/2315 train_time:83152ms step_avg:60.47ms
step:1376/2315 train_time:83213ms step_avg:60.47ms
step:1377/2315 train_time:83274ms step_avg:60.47ms
step:1378/2315 train_time:83334ms step_avg:60.47ms
step:1379/2315 train_time:83396ms step_avg:60.48ms
step:1380/2315 train_time:83456ms step_avg:60.48ms
step:1381/2315 train_time:83518ms step_avg:60.48ms
step:1382/2315 train_time:83578ms step_avg:60.48ms
step:1383/2315 train_time:83640ms step_avg:60.48ms
step:1384/2315 train_time:83700ms step_avg:60.48ms
step:1385/2315 train_time:83761ms step_avg:60.48ms
step:1386/2315 train_time:83821ms step_avg:60.48ms
step:1387/2315 train_time:83883ms step_avg:60.48ms
step:1388/2315 train_time:83944ms step_avg:60.48ms
step:1389/2315 train_time:84005ms step_avg:60.48ms
step:1390/2315 train_time:84066ms step_avg:60.48ms
step:1391/2315 train_time:84126ms step_avg:60.48ms
step:1392/2315 train_time:84187ms step_avg:60.48ms
step:1393/2315 train_time:84248ms step_avg:60.48ms
step:1394/2315 train_time:84309ms step_avg:60.48ms
step:1395/2315 train_time:84370ms step_avg:60.48ms
step:1396/2315 train_time:84432ms step_avg:60.48ms
step:1397/2315 train_time:84493ms step_avg:60.48ms
step:1398/2315 train_time:84553ms step_avg:60.48ms
step:1399/2315 train_time:84614ms step_avg:60.48ms
step:1400/2315 train_time:84675ms step_avg:60.48ms
step:1401/2315 train_time:84736ms step_avg:60.48ms
step:1402/2315 train_time:84796ms step_avg:60.48ms
step:1403/2315 train_time:84857ms step_avg:60.48ms
step:1404/2315 train_time:84919ms step_avg:60.48ms
step:1405/2315 train_time:84980ms step_avg:60.48ms
step:1406/2315 train_time:85040ms step_avg:60.48ms
step:1407/2315 train_time:85101ms step_avg:60.48ms
step:1408/2315 train_time:85161ms step_avg:60.48ms
step:1409/2315 train_time:85222ms step_avg:60.48ms
step:1410/2315 train_time:85283ms step_avg:60.48ms
step:1411/2315 train_time:85344ms step_avg:60.48ms
step:1412/2315 train_time:85404ms step_avg:60.48ms
step:1413/2315 train_time:85465ms step_avg:60.48ms
step:1414/2315 train_time:85525ms step_avg:60.48ms
step:1415/2315 train_time:85587ms step_avg:60.49ms
step:1416/2315 train_time:85648ms step_avg:60.49ms
step:1417/2315 train_time:85709ms step_avg:60.49ms
step:1418/2315 train_time:85769ms step_avg:60.49ms
step:1419/2315 train_time:85831ms step_avg:60.49ms
step:1420/2315 train_time:85893ms step_avg:60.49ms
step:1421/2315 train_time:85954ms step_avg:60.49ms
step:1422/2315 train_time:86014ms step_avg:60.49ms
step:1423/2315 train_time:86075ms step_avg:60.49ms
step:1424/2315 train_time:86136ms step_avg:60.49ms
step:1425/2315 train_time:86196ms step_avg:60.49ms
step:1426/2315 train_time:86257ms step_avg:60.49ms
step:1427/2315 train_time:86318ms step_avg:60.49ms
step:1428/2315 train_time:86379ms step_avg:60.49ms
step:1429/2315 train_time:86440ms step_avg:60.49ms
step:1430/2315 train_time:86500ms step_avg:60.49ms
step:1431/2315 train_time:86561ms step_avg:60.49ms
step:1432/2315 train_time:86621ms step_avg:60.49ms
step:1433/2315 train_time:86682ms step_avg:60.49ms
step:1434/2315 train_time:86742ms step_avg:60.49ms
step:1435/2315 train_time:86803ms step_avg:60.49ms
step:1436/2315 train_time:86865ms step_avg:60.49ms
step:1437/2315 train_time:86927ms step_avg:60.49ms
step:1438/2315 train_time:86987ms step_avg:60.49ms
step:1439/2315 train_time:87048ms step_avg:60.49ms
step:1440/2315 train_time:87109ms step_avg:60.49ms
step:1441/2315 train_time:87170ms step_avg:60.49ms
step:1442/2315 train_time:87231ms step_avg:60.49ms
step:1443/2315 train_time:87293ms step_avg:60.49ms
step:1444/2315 train_time:87353ms step_avg:60.49ms
step:1445/2315 train_time:87415ms step_avg:60.49ms
step:1446/2315 train_time:87475ms step_avg:60.49ms
step:1447/2315 train_time:87537ms step_avg:60.50ms
step:1448/2315 train_time:87597ms step_avg:60.50ms
step:1449/2315 train_time:87658ms step_avg:60.50ms
step:1450/2315 train_time:87719ms step_avg:60.50ms
step:1451/2315 train_time:87780ms step_avg:60.50ms
step:1452/2315 train_time:87840ms step_avg:60.50ms
step:1453/2315 train_time:87901ms step_avg:60.50ms
step:1454/2315 train_time:87962ms step_avg:60.50ms
step:1455/2315 train_time:88023ms step_avg:60.50ms
step:1456/2315 train_time:88085ms step_avg:60.50ms
step:1457/2315 train_time:88146ms step_avg:60.50ms
step:1458/2315 train_time:88207ms step_avg:60.50ms
step:1459/2315 train_time:88269ms step_avg:60.50ms
step:1460/2315 train_time:88329ms step_avg:60.50ms
step:1461/2315 train_time:88390ms step_avg:60.50ms
step:1462/2315 train_time:88450ms step_avg:60.50ms
step:1463/2315 train_time:88512ms step_avg:60.50ms
step:1464/2315 train_time:88572ms step_avg:60.50ms
step:1465/2315 train_time:88633ms step_avg:60.50ms
step:1466/2315 train_time:88694ms step_avg:60.50ms
step:1467/2315 train_time:88755ms step_avg:60.50ms
step:1468/2315 train_time:88815ms step_avg:60.50ms
step:1469/2315 train_time:88876ms step_avg:60.50ms
step:1470/2315 train_time:88937ms step_avg:60.50ms
step:1471/2315 train_time:88998ms step_avg:60.50ms
step:1472/2315 train_time:89059ms step_avg:60.50ms
step:1473/2315 train_time:89120ms step_avg:60.50ms
step:1474/2315 train_time:89181ms step_avg:60.50ms
step:1475/2315 train_time:89242ms step_avg:60.50ms
step:1476/2315 train_time:89302ms step_avg:60.50ms
step:1477/2315 train_time:89363ms step_avg:60.50ms
step:1478/2315 train_time:89424ms step_avg:60.50ms
step:1479/2315 train_time:89484ms step_avg:60.50ms
step:1480/2315 train_time:89545ms step_avg:60.50ms
step:1481/2315 train_time:89607ms step_avg:60.50ms
step:1482/2315 train_time:89667ms step_avg:60.50ms
step:1483/2315 train_time:89728ms step_avg:60.50ms
step:1484/2315 train_time:89789ms step_avg:60.50ms
step:1485/2315 train_time:89850ms step_avg:60.51ms
step:1486/2315 train_time:89911ms step_avg:60.51ms
step:1487/2315 train_time:89972ms step_avg:60.51ms
step:1488/2315 train_time:90033ms step_avg:60.51ms
step:1489/2315 train_time:90094ms step_avg:60.51ms
step:1490/2315 train_time:90155ms step_avg:60.51ms
step:1491/2315 train_time:90217ms step_avg:60.51ms
step:1492/2315 train_time:90278ms step_avg:60.51ms
step:1493/2315 train_time:90338ms step_avg:60.51ms
step:1494/2315 train_time:90399ms step_avg:60.51ms
step:1495/2315 train_time:90460ms step_avg:60.51ms
step:1496/2315 train_time:90520ms step_avg:60.51ms
step:1497/2315 train_time:90581ms step_avg:60.51ms
step:1498/2315 train_time:90642ms step_avg:60.51ms
step:1499/2315 train_time:90702ms step_avg:60.51ms
step:1500/2315 train_time:90763ms step_avg:60.51ms
step:1500/2315 val_loss:3.4519 train_time:90826ms step_avg:60.55ms
step:1501/2315 train_time:90847ms step_avg:60.52ms
step:1502/2315 train_time:90886ms step_avg:60.51ms
step:1503/2315 train_time:90949ms step_avg:60.51ms
step:1504/2315 train_time:91012ms step_avg:60.51ms
step:1505/2315 train_time:91074ms step_avg:60.51ms
step:1506/2315 train_time:91135ms step_avg:60.51ms
step:1507/2315 train_time:91195ms step_avg:60.51ms
step:1508/2315 train_time:91255ms step_avg:60.51ms
step:1509/2315 train_time:91315ms step_avg:60.51ms
step:1510/2315 train_time:91375ms step_avg:60.51ms
step:1511/2315 train_time:91435ms step_avg:60.51ms
step:1512/2315 train_time:91496ms step_avg:60.51ms
step:1513/2315 train_time:91556ms step_avg:60.51ms
step:1514/2315 train_time:91616ms step_avg:60.51ms
step:1515/2315 train_time:91675ms step_avg:60.51ms
step:1516/2315 train_time:91736ms step_avg:60.51ms
step:1517/2315 train_time:91797ms step_avg:60.51ms
step:1518/2315 train_time:91859ms step_avg:60.51ms
step:1519/2315 train_time:91922ms step_avg:60.51ms
step:1520/2315 train_time:91984ms step_avg:60.52ms
step:1521/2315 train_time:92046ms step_avg:60.52ms
step:1522/2315 train_time:92107ms step_avg:60.52ms
step:1523/2315 train_time:92168ms step_avg:60.52ms
step:1524/2315 train_time:92229ms step_avg:60.52ms
step:1525/2315 train_time:92290ms step_avg:60.52ms
step:1526/2315 train_time:92351ms step_avg:60.52ms
step:1527/2315 train_time:92413ms step_avg:60.52ms
step:1528/2315 train_time:92473ms step_avg:60.52ms
step:1529/2315 train_time:92533ms step_avg:60.52ms
step:1530/2315 train_time:92594ms step_avg:60.52ms
step:1531/2315 train_time:92654ms step_avg:60.52ms
step:1532/2315 train_time:92715ms step_avg:60.52ms
step:1533/2315 train_time:92777ms step_avg:60.52ms
step:1534/2315 train_time:92839ms step_avg:60.52ms
step:1535/2315 train_time:92901ms step_avg:60.52ms
step:1536/2315 train_time:92962ms step_avg:60.52ms
step:1537/2315 train_time:93024ms step_avg:60.52ms
step:1538/2315 train_time:93086ms step_avg:60.52ms
step:1539/2315 train_time:93148ms step_avg:60.52ms
step:1540/2315 train_time:93209ms step_avg:60.53ms
step:1541/2315 train_time:93270ms step_avg:60.53ms
step:1542/2315 train_time:93331ms step_avg:60.53ms
step:1543/2315 train_time:93392ms step_avg:60.53ms
step:1544/2315 train_time:93453ms step_avg:60.53ms
step:1545/2315 train_time:93514ms step_avg:60.53ms
step:1546/2315 train_time:93575ms step_avg:60.53ms
step:1547/2315 train_time:93636ms step_avg:60.53ms
step:1548/2315 train_time:93697ms step_avg:60.53ms
step:1549/2315 train_time:93758ms step_avg:60.53ms
step:1550/2315 train_time:93818ms step_avg:60.53ms
step:1551/2315 train_time:93879ms step_avg:60.53ms
step:1552/2315 train_time:93940ms step_avg:60.53ms
step:1553/2315 train_time:94002ms step_avg:60.53ms
step:1554/2315 train_time:94064ms step_avg:60.53ms
step:1555/2315 train_time:94125ms step_avg:60.53ms
step:1556/2315 train_time:94187ms step_avg:60.53ms
step:1557/2315 train_time:94249ms step_avg:60.53ms
step:1558/2315 train_time:94309ms step_avg:60.53ms
step:1559/2315 train_time:94370ms step_avg:60.53ms
step:1560/2315 train_time:94431ms step_avg:60.53ms
step:1561/2315 train_time:94492ms step_avg:60.53ms
step:1562/2315 train_time:94553ms step_avg:60.53ms
step:1563/2315 train_time:94614ms step_avg:60.53ms
step:1564/2315 train_time:94675ms step_avg:60.53ms
step:1565/2315 train_time:94736ms step_avg:60.53ms
step:1566/2315 train_time:94797ms step_avg:60.53ms
step:1567/2315 train_time:94859ms step_avg:60.54ms
step:1568/2315 train_time:94921ms step_avg:60.54ms
step:1569/2315 train_time:94982ms step_avg:60.54ms
step:1570/2315 train_time:95043ms step_avg:60.54ms
step:1571/2315 train_time:95105ms step_avg:60.54ms
step:1572/2315 train_time:95166ms step_avg:60.54ms
step:1573/2315 train_time:95228ms step_avg:60.54ms
step:1574/2315 train_time:95289ms step_avg:60.54ms
step:1575/2315 train_time:95351ms step_avg:60.54ms
step:1576/2315 train_time:95412ms step_avg:60.54ms
step:1577/2315 train_time:95473ms step_avg:60.54ms
step:1578/2315 train_time:95533ms step_avg:60.54ms
step:1579/2315 train_time:95595ms step_avg:60.54ms
step:1580/2315 train_time:95656ms step_avg:60.54ms
step:1581/2315 train_time:95717ms step_avg:60.54ms
step:1582/2315 train_time:95778ms step_avg:60.54ms
step:1583/2315 train_time:95839ms step_avg:60.54ms
step:1584/2315 train_time:95901ms step_avg:60.54ms
step:1585/2315 train_time:95962ms step_avg:60.54ms
step:1586/2315 train_time:96023ms step_avg:60.54ms
step:1587/2315 train_time:96085ms step_avg:60.54ms
step:1588/2315 train_time:96147ms step_avg:60.55ms
step:1589/2315 train_time:96208ms step_avg:60.55ms
step:1590/2315 train_time:96269ms step_avg:60.55ms
step:1591/2315 train_time:96330ms step_avg:60.55ms
step:1592/2315 train_time:96391ms step_avg:60.55ms
step:1593/2315 train_time:96452ms step_avg:60.55ms
step:1594/2315 train_time:96513ms step_avg:60.55ms
step:1595/2315 train_time:96574ms step_avg:60.55ms
step:1596/2315 train_time:96635ms step_avg:60.55ms
step:1597/2315 train_time:96696ms step_avg:60.55ms
step:1598/2315 train_time:96757ms step_avg:60.55ms
step:1599/2315 train_time:96818ms step_avg:60.55ms
step:1600/2315 train_time:96879ms step_avg:60.55ms
step:1601/2315 train_time:96941ms step_avg:60.55ms
step:1602/2315 train_time:97002ms step_avg:60.55ms
step:1603/2315 train_time:97063ms step_avg:60.55ms
step:1604/2315 train_time:97125ms step_avg:60.55ms
step:1605/2315 train_time:97187ms step_avg:60.55ms
step:1606/2315 train_time:97248ms step_avg:60.55ms
step:1607/2315 train_time:97310ms step_avg:60.55ms
step:1608/2315 train_time:97371ms step_avg:60.55ms
step:1609/2315 train_time:97432ms step_avg:60.55ms
step:1610/2315 train_time:97493ms step_avg:60.55ms
step:1611/2315 train_time:97554ms step_avg:60.55ms
step:1612/2315 train_time:97615ms step_avg:60.55ms
step:1613/2315 train_time:97676ms step_avg:60.56ms
step:1614/2315 train_time:97737ms step_avg:60.56ms
step:1615/2315 train_time:97798ms step_avg:60.56ms
step:1616/2315 train_time:97860ms step_avg:60.56ms
step:1617/2315 train_time:97920ms step_avg:60.56ms
step:1618/2315 train_time:97981ms step_avg:60.56ms
step:1619/2315 train_time:98043ms step_avg:60.56ms
step:1620/2315 train_time:98104ms step_avg:60.56ms
step:1621/2315 train_time:98166ms step_avg:60.56ms
step:1622/2315 train_time:98228ms step_avg:60.56ms
step:1623/2315 train_time:98289ms step_avg:60.56ms
step:1624/2315 train_time:98350ms step_avg:60.56ms
step:1625/2315 train_time:98411ms step_avg:60.56ms
step:1626/2315 train_time:98472ms step_avg:60.56ms
step:1627/2315 train_time:98533ms step_avg:60.56ms
step:1628/2315 train_time:98594ms step_avg:60.56ms
step:1629/2315 train_time:98655ms step_avg:60.56ms
step:1630/2315 train_time:98716ms step_avg:60.56ms
step:1631/2315 train_time:98777ms step_avg:60.56ms
step:1632/2315 train_time:98838ms step_avg:60.56ms
step:1633/2315 train_time:98899ms step_avg:60.56ms
step:1634/2315 train_time:98960ms step_avg:60.56ms
step:1635/2315 train_time:99022ms step_avg:60.56ms
step:1636/2315 train_time:99083ms step_avg:60.56ms
step:1637/2315 train_time:99144ms step_avg:60.56ms
step:1638/2315 train_time:99205ms step_avg:60.56ms
step:1639/2315 train_time:99267ms step_avg:60.57ms
step:1640/2315 train_time:99328ms step_avg:60.57ms
step:1641/2315 train_time:99389ms step_avg:60.57ms
step:1642/2315 train_time:99450ms step_avg:60.57ms
step:1643/2315 train_time:99512ms step_avg:60.57ms
step:1644/2315 train_time:99573ms step_avg:60.57ms
step:1645/2315 train_time:99634ms step_avg:60.57ms
step:1646/2315 train_time:99694ms step_avg:60.57ms
step:1647/2315 train_time:99755ms step_avg:60.57ms
step:1648/2315 train_time:99816ms step_avg:60.57ms
step:1649/2315 train_time:99878ms step_avg:60.57ms
step:1650/2315 train_time:99939ms step_avg:60.57ms
step:1651/2315 train_time:100000ms step_avg:60.57ms
step:1652/2315 train_time:100061ms step_avg:60.57ms
step:1653/2315 train_time:100123ms step_avg:60.57ms
step:1654/2315 train_time:100184ms step_avg:60.57ms
step:1655/2315 train_time:100246ms step_avg:60.57ms
step:1656/2315 train_time:100307ms step_avg:60.57ms
step:1657/2315 train_time:100369ms step_avg:60.57ms
step:1658/2315 train_time:100429ms step_avg:60.57ms
step:1659/2315 train_time:100491ms step_avg:60.57ms
step:1660/2315 train_time:100552ms step_avg:60.57ms
step:1661/2315 train_time:100613ms step_avg:60.57ms
step:1662/2315 train_time:100674ms step_avg:60.57ms
step:1663/2315 train_time:100735ms step_avg:60.57ms
step:1664/2315 train_time:100795ms step_avg:60.57ms
step:1665/2315 train_time:100857ms step_avg:60.57ms
step:1666/2315 train_time:100918ms step_avg:60.58ms
step:1667/2315 train_time:100980ms step_avg:60.58ms
step:1668/2315 train_time:101041ms step_avg:60.58ms
step:1669/2315 train_time:101102ms step_avg:60.58ms
step:1670/2315 train_time:101163ms step_avg:60.58ms
step:1671/2315 train_time:101224ms step_avg:60.58ms
step:1672/2315 train_time:101285ms step_avg:60.58ms
step:1673/2315 train_time:101347ms step_avg:60.58ms
step:1674/2315 train_time:101408ms step_avg:60.58ms
step:1675/2315 train_time:101470ms step_avg:60.58ms
step:1676/2315 train_time:101531ms step_avg:60.58ms
step:1677/2315 train_time:101593ms step_avg:60.58ms
step:1678/2315 train_time:101654ms step_avg:60.58ms
step:1679/2315 train_time:101715ms step_avg:60.58ms
step:1680/2315 train_time:101776ms step_avg:60.58ms
step:1681/2315 train_time:101837ms step_avg:60.58ms
step:1682/2315 train_time:101898ms step_avg:60.58ms
step:1683/2315 train_time:101959ms step_avg:60.58ms
step:1684/2315 train_time:102020ms step_avg:60.58ms
step:1685/2315 train_time:102081ms step_avg:60.58ms
step:1686/2315 train_time:102142ms step_avg:60.58ms
step:1687/2315 train_time:102204ms step_avg:60.58ms
step:1688/2315 train_time:102265ms step_avg:60.58ms
step:1689/2315 train_time:102327ms step_avg:60.58ms
step:1690/2315 train_time:102388ms step_avg:60.58ms
step:1691/2315 train_time:102450ms step_avg:60.59ms
step:1692/2315 train_time:102511ms step_avg:60.59ms
step:1693/2315 train_time:102572ms step_avg:60.59ms
step:1694/2315 train_time:102633ms step_avg:60.59ms
step:1695/2315 train_time:102695ms step_avg:60.59ms
step:1696/2315 train_time:102755ms step_avg:60.59ms
step:1697/2315 train_time:102816ms step_avg:60.59ms
step:1698/2315 train_time:102877ms step_avg:60.59ms
step:1699/2315 train_time:102938ms step_avg:60.59ms
step:1700/2315 train_time:102999ms step_avg:60.59ms
step:1701/2315 train_time:103061ms step_avg:60.59ms
step:1702/2315 train_time:103122ms step_avg:60.59ms
step:1703/2315 train_time:103184ms step_avg:60.59ms
step:1704/2315 train_time:103245ms step_avg:60.59ms
step:1705/2315 train_time:103306ms step_avg:60.59ms
step:1706/2315 train_time:103367ms step_avg:60.59ms
step:1707/2315 train_time:103429ms step_avg:60.59ms
step:1708/2315 train_time:103490ms step_avg:60.59ms
step:1709/2315 train_time:103551ms step_avg:60.59ms
step:1710/2315 train_time:103613ms step_avg:60.59ms
step:1711/2315 train_time:103674ms step_avg:60.59ms
step:1712/2315 train_time:103735ms step_avg:60.59ms
step:1713/2315 train_time:103796ms step_avg:60.59ms
step:1714/2315 train_time:103857ms step_avg:60.59ms
step:1715/2315 train_time:103918ms step_avg:60.59ms
step:1716/2315 train_time:103979ms step_avg:60.59ms
step:1717/2315 train_time:104040ms step_avg:60.59ms
step:1718/2315 train_time:104101ms step_avg:60.59ms
step:1719/2315 train_time:104162ms step_avg:60.59ms
step:1720/2315 train_time:104223ms step_avg:60.59ms
step:1721/2315 train_time:104285ms step_avg:60.60ms
step:1722/2315 train_time:104346ms step_avg:60.60ms
step:1723/2315 train_time:104408ms step_avg:60.60ms
step:1724/2315 train_time:104469ms step_avg:60.60ms
step:1725/2315 train_time:104530ms step_avg:60.60ms
step:1726/2315 train_time:104592ms step_avg:60.60ms
step:1727/2315 train_time:104653ms step_avg:60.60ms
step:1728/2315 train_time:104714ms step_avg:60.60ms
step:1729/2315 train_time:104775ms step_avg:60.60ms
step:1730/2315 train_time:104836ms step_avg:60.60ms
step:1731/2315 train_time:104898ms step_avg:60.60ms
step:1732/2315 train_time:104959ms step_avg:60.60ms
step:1733/2315 train_time:105020ms step_avg:60.60ms
step:1734/2315 train_time:105081ms step_avg:60.60ms
step:1735/2315 train_time:105141ms step_avg:60.60ms
step:1736/2315 train_time:105203ms step_avg:60.60ms
step:1737/2315 train_time:105265ms step_avg:60.60ms
step:1738/2315 train_time:105326ms step_avg:60.60ms
step:1739/2315 train_time:105388ms step_avg:60.60ms
step:1740/2315 train_time:105449ms step_avg:60.60ms
step:1741/2315 train_time:105511ms step_avg:60.60ms
step:1742/2315 train_time:105573ms step_avg:60.60ms
step:1743/2315 train_time:105634ms step_avg:60.60ms
step:1744/2315 train_time:105695ms step_avg:60.60ms
step:1745/2315 train_time:105756ms step_avg:60.60ms
step:1746/2315 train_time:105817ms step_avg:60.61ms
step:1747/2315 train_time:105878ms step_avg:60.61ms
step:1748/2315 train_time:105939ms step_avg:60.61ms
step:1749/2315 train_time:106000ms step_avg:60.61ms
step:1750/2315 train_time:106061ms step_avg:60.61ms
step:1750/2315 val_loss:3.3820 train_time:106123ms step_avg:60.64ms
step:1751/2315 train_time:106144ms step_avg:60.62ms
step:1752/2315 train_time:106183ms step_avg:60.61ms
step:1753/2315 train_time:106249ms step_avg:60.61ms
step:1754/2315 train_time:106313ms step_avg:60.61ms
step:1755/2315 train_time:106375ms step_avg:60.61ms
step:1756/2315 train_time:106436ms step_avg:60.61ms
step:1757/2315 train_time:106497ms step_avg:60.61ms
step:1758/2315 train_time:106558ms step_avg:60.61ms
step:1759/2315 train_time:106619ms step_avg:60.61ms
step:1760/2315 train_time:106679ms step_avg:60.61ms
step:1761/2315 train_time:106739ms step_avg:60.61ms
step:1762/2315 train_time:106800ms step_avg:60.61ms
step:1763/2315 train_time:106860ms step_avg:60.61ms
step:1764/2315 train_time:106920ms step_avg:60.61ms
step:1765/2315 train_time:106981ms step_avg:60.61ms
step:1766/2315 train_time:107044ms step_avg:60.61ms
step:1767/2315 train_time:107107ms step_avg:60.62ms
step:1768/2315 train_time:107168ms step_avg:60.62ms
step:1769/2315 train_time:107231ms step_avg:60.62ms
step:1770/2315 train_time:107292ms step_avg:60.62ms
step:1771/2315 train_time:107354ms step_avg:60.62ms
step:1772/2315 train_time:107417ms step_avg:60.62ms
step:1773/2315 train_time:107479ms step_avg:60.62ms
step:1774/2315 train_time:107540ms step_avg:60.62ms
step:1775/2315 train_time:107601ms step_avg:60.62ms
step:1776/2315 train_time:107661ms step_avg:60.62ms
step:1777/2315 train_time:107722ms step_avg:60.62ms
step:1778/2315 train_time:107783ms step_avg:60.62ms
step:1779/2315 train_time:107844ms step_avg:60.62ms
step:1780/2315 train_time:107904ms step_avg:60.62ms
step:1781/2315 train_time:107965ms step_avg:60.62ms
step:1782/2315 train_time:108026ms step_avg:60.62ms
step:1783/2315 train_time:108089ms step_avg:60.62ms
step:1784/2315 train_time:108150ms step_avg:60.62ms
step:1785/2315 train_time:108212ms step_avg:60.62ms
step:1786/2315 train_time:108273ms step_avg:60.62ms
step:1787/2315 train_time:108335ms step_avg:60.62ms
step:1788/2315 train_time:108397ms step_avg:60.62ms
step:1789/2315 train_time:108458ms step_avg:60.63ms
step:1790/2315 train_time:108520ms step_avg:60.63ms
step:1791/2315 train_time:108581ms step_avg:60.63ms
step:1792/2315 train_time:108642ms step_avg:60.63ms
step:1793/2315 train_time:108702ms step_avg:60.63ms
step:1794/2315 train_time:108763ms step_avg:60.63ms
step:1795/2315 train_time:108825ms step_avg:60.63ms
step:1796/2315 train_time:108885ms step_avg:60.63ms
step:1797/2315 train_time:108946ms step_avg:60.63ms
step:1798/2315 train_time:109007ms step_avg:60.63ms
step:1799/2315 train_time:109068ms step_avg:60.63ms
step:1800/2315 train_time:109130ms step_avg:60.63ms
step:1801/2315 train_time:109191ms step_avg:60.63ms
step:1802/2315 train_time:109253ms step_avg:60.63ms
step:1803/2315 train_time:109314ms step_avg:60.63ms
step:1804/2315 train_time:109375ms step_avg:60.63ms
step:1805/2315 train_time:109436ms step_avg:60.63ms
step:1806/2315 train_time:109497ms step_avg:60.63ms
step:1807/2315 train_time:109558ms step_avg:60.63ms
step:1808/2315 train_time:109619ms step_avg:60.63ms
step:1809/2315 train_time:109680ms step_avg:60.63ms
step:1810/2315 train_time:109741ms step_avg:60.63ms
step:1811/2315 train_time:109802ms step_avg:60.63ms
step:1812/2315 train_time:109862ms step_avg:60.63ms
step:1813/2315 train_time:109924ms step_avg:60.63ms
step:1814/2315 train_time:109985ms step_avg:60.63ms
step:1815/2315 train_time:110047ms step_avg:60.63ms
step:1816/2315 train_time:110108ms step_avg:60.63ms
step:1817/2315 train_time:110169ms step_avg:60.63ms
step:1818/2315 train_time:110230ms step_avg:60.63ms
step:1819/2315 train_time:110291ms step_avg:60.63ms
step:1820/2315 train_time:110353ms step_avg:60.63ms
step:1821/2315 train_time:110414ms step_avg:60.63ms
step:1822/2315 train_time:110474ms step_avg:60.63ms
step:1823/2315 train_time:110536ms step_avg:60.63ms
step:1824/2315 train_time:110597ms step_avg:60.63ms
step:1825/2315 train_time:110659ms step_avg:60.63ms
step:1826/2315 train_time:110720ms step_avg:60.64ms
step:1827/2315 train_time:110781ms step_avg:60.64ms
step:1828/2315 train_time:110842ms step_avg:60.64ms
step:1829/2315 train_time:110903ms step_avg:60.64ms
step:1830/2315 train_time:110964ms step_avg:60.64ms
step:1831/2315 train_time:111025ms step_avg:60.64ms
step:1832/2315 train_time:111086ms step_avg:60.64ms
step:1833/2315 train_time:111148ms step_avg:60.64ms
step:1834/2315 train_time:111209ms step_avg:60.64ms
step:1835/2315 train_time:111270ms step_avg:60.64ms
step:1836/2315 train_time:111332ms step_avg:60.64ms
step:1837/2315 train_time:111393ms step_avg:60.64ms
step:1838/2315 train_time:111454ms step_avg:60.64ms
step:1839/2315 train_time:111515ms step_avg:60.64ms
step:1840/2315 train_time:111576ms step_avg:60.64ms
step:1841/2315 train_time:111637ms step_avg:60.64ms
step:1842/2315 train_time:111699ms step_avg:60.64ms
step:1843/2315 train_time:111761ms step_avg:60.64ms
step:1844/2315 train_time:111821ms step_avg:60.64ms
step:1845/2315 train_time:111882ms step_avg:60.64ms
step:1846/2315 train_time:111943ms step_avg:60.64ms
step:1847/2315 train_time:112004ms step_avg:60.64ms
step:1848/2315 train_time:112065ms step_avg:60.64ms
step:1849/2315 train_time:112127ms step_avg:60.64ms
step:1850/2315 train_time:112188ms step_avg:60.64ms
step:1851/2315 train_time:112250ms step_avg:60.64ms
step:1852/2315 train_time:112311ms step_avg:60.64ms
step:1853/2315 train_time:112373ms step_avg:60.64ms
step:1854/2315 train_time:112433ms step_avg:60.64ms
step:1855/2315 train_time:112494ms step_avg:60.64ms
step:1856/2315 train_time:112555ms step_avg:60.64ms
step:1857/2315 train_time:112616ms step_avg:60.64ms
step:1858/2315 train_time:112677ms step_avg:60.64ms
step:1859/2315 train_time:112739ms step_avg:60.65ms
step:1860/2315 train_time:112800ms step_avg:60.65ms
step:1861/2315 train_time:112862ms step_avg:60.65ms
step:1862/2315 train_time:112923ms step_avg:60.65ms
step:1863/2315 train_time:112984ms step_avg:60.65ms
step:1864/2315 train_time:113045ms step_avg:60.65ms
step:1865/2315 train_time:113106ms step_avg:60.65ms
step:1866/2315 train_time:113167ms step_avg:60.65ms
step:1867/2315 train_time:113229ms step_avg:60.65ms
step:1868/2315 train_time:113290ms step_avg:60.65ms
step:1869/2315 train_time:113351ms step_avg:60.65ms
step:1870/2315 train_time:113412ms step_avg:60.65ms
step:1871/2315 train_time:113474ms step_avg:60.65ms
step:1872/2315 train_time:113534ms step_avg:60.65ms
step:1873/2315 train_time:113595ms step_avg:60.65ms
step:1874/2315 train_time:113656ms step_avg:60.65ms
step:1875/2315 train_time:113717ms step_avg:60.65ms
step:1876/2315 train_time:113778ms step_avg:60.65ms
step:1877/2315 train_time:113841ms step_avg:60.65ms
step:1878/2315 train_time:113902ms step_avg:60.65ms
step:1879/2315 train_time:113963ms step_avg:60.65ms
step:1880/2315 train_time:114024ms step_avg:60.65ms
step:1881/2315 train_time:114085ms step_avg:60.65ms
step:1882/2315 train_time:114146ms step_avg:60.65ms
step:1883/2315 train_time:114208ms step_avg:60.65ms
step:1884/2315 train_time:114269ms step_avg:60.65ms
step:1885/2315 train_time:114330ms step_avg:60.65ms
step:1886/2315 train_time:114391ms step_avg:60.65ms
step:1887/2315 train_time:114452ms step_avg:60.65ms
step:1888/2315 train_time:114514ms step_avg:60.65ms
step:1889/2315 train_time:114575ms step_avg:60.65ms
step:1890/2315 train_time:114636ms step_avg:60.65ms
step:1891/2315 train_time:114697ms step_avg:60.65ms
step:1892/2315 train_time:114758ms step_avg:60.65ms
step:1893/2315 train_time:114820ms step_avg:60.66ms
step:1894/2315 train_time:114881ms step_avg:60.66ms
step:1895/2315 train_time:114942ms step_avg:60.66ms
step:1896/2315 train_time:115004ms step_avg:60.66ms
step:1897/2315 train_time:115065ms step_avg:60.66ms
step:1898/2315 train_time:115126ms step_avg:60.66ms
step:1899/2315 train_time:115188ms step_avg:60.66ms
step:1900/2315 train_time:115248ms step_avg:60.66ms
step:1901/2315 train_time:115310ms step_avg:60.66ms
step:1902/2315 train_time:115372ms step_avg:60.66ms
step:1903/2315 train_time:115432ms step_avg:60.66ms
step:1904/2315 train_time:115494ms step_avg:60.66ms
step:1905/2315 train_time:115554ms step_avg:60.66ms
step:1906/2315 train_time:115616ms step_avg:60.66ms
step:1907/2315 train_time:115677ms step_avg:60.66ms
step:1908/2315 train_time:115737ms step_avg:60.66ms
step:1909/2315 train_time:115799ms step_avg:60.66ms
step:1910/2315 train_time:115861ms step_avg:60.66ms
step:1911/2315 train_time:115922ms step_avg:60.66ms
step:1912/2315 train_time:115983ms step_avg:60.66ms
step:1913/2315 train_time:116045ms step_avg:60.66ms
step:1914/2315 train_time:116106ms step_avg:60.66ms
step:1915/2315 train_time:116167ms step_avg:60.66ms
step:1916/2315 train_time:116228ms step_avg:60.66ms
step:1917/2315 train_time:116290ms step_avg:60.66ms
step:1918/2315 train_time:116351ms step_avg:60.66ms
step:1919/2315 train_time:116412ms step_avg:60.66ms
step:1920/2315 train_time:116473ms step_avg:60.66ms
step:1921/2315 train_time:116534ms step_avg:60.66ms
step:1922/2315 train_time:116595ms step_avg:60.66ms
step:1923/2315 train_time:116657ms step_avg:60.66ms
step:1924/2315 train_time:116718ms step_avg:60.66ms
step:1925/2315 train_time:116779ms step_avg:60.66ms
step:1926/2315 train_time:116840ms step_avg:60.66ms
step:1927/2315 train_time:116902ms step_avg:60.67ms
step:1928/2315 train_time:116963ms step_avg:60.67ms
step:1929/2315 train_time:117025ms step_avg:60.67ms
step:1930/2315 train_time:117086ms step_avg:60.67ms
step:1931/2315 train_time:117147ms step_avg:60.67ms
step:1932/2315 train_time:117208ms step_avg:60.67ms
step:1933/2315 train_time:117270ms step_avg:60.67ms
step:1934/2315 train_time:117331ms step_avg:60.67ms
step:1935/2315 train_time:117393ms step_avg:60.67ms
step:1936/2315 train_time:117454ms step_avg:60.67ms
step:1937/2315 train_time:117515ms step_avg:60.67ms
step:1938/2315 train_time:117575ms step_avg:60.67ms
step:1939/2315 train_time:117636ms step_avg:60.67ms
step:1940/2315 train_time:117697ms step_avg:60.67ms
step:1941/2315 train_time:117759ms step_avg:60.67ms
step:1942/2315 train_time:117820ms step_avg:60.67ms
step:1943/2315 train_time:117882ms step_avg:60.67ms
step:1944/2315 train_time:117942ms step_avg:60.67ms
step:1945/2315 train_time:118003ms step_avg:60.67ms
step:1946/2315 train_time:118065ms step_avg:60.67ms
step:1947/2315 train_time:118126ms step_avg:60.67ms
step:1948/2315 train_time:118187ms step_avg:60.67ms
step:1949/2315 train_time:118249ms step_avg:60.67ms
step:1950/2315 train_time:118310ms step_avg:60.67ms
step:1951/2315 train_time:118371ms step_avg:60.67ms
step:1952/2315 train_time:118432ms step_avg:60.67ms
step:1953/2315 train_time:118494ms step_avg:60.67ms
step:1954/2315 train_time:118555ms step_avg:60.67ms
step:1955/2315 train_time:118616ms step_avg:60.67ms
step:1956/2315 train_time:118677ms step_avg:60.67ms
step:1957/2315 train_time:118739ms step_avg:60.67ms
step:1958/2315 train_time:118799ms step_avg:60.67ms
step:1959/2315 train_time:118861ms step_avg:60.67ms
step:1960/2315 train_time:118922ms step_avg:60.67ms
step:1961/2315 train_time:118983ms step_avg:60.67ms
step:1962/2315 train_time:119044ms step_avg:60.68ms
step:1963/2315 train_time:119106ms step_avg:60.68ms
step:1964/2315 train_time:119167ms step_avg:60.68ms
step:1965/2315 train_time:119229ms step_avg:60.68ms
step:1966/2315 train_time:119290ms step_avg:60.68ms
step:1967/2315 train_time:119351ms step_avg:60.68ms
step:1968/2315 train_time:119411ms step_avg:60.68ms
step:1969/2315 train_time:119473ms step_avg:60.68ms
step:1970/2315 train_time:119534ms step_avg:60.68ms
step:1971/2315 train_time:119595ms step_avg:60.68ms
step:1972/2315 train_time:119656ms step_avg:60.68ms
step:1973/2315 train_time:119718ms step_avg:60.68ms
step:1974/2315 train_time:119778ms step_avg:60.68ms
step:1975/2315 train_time:119839ms step_avg:60.68ms
step:1976/2315 train_time:119900ms step_avg:60.68ms
step:1977/2315 train_time:119961ms step_avg:60.68ms
step:1978/2315 train_time:120022ms step_avg:60.68ms
step:1979/2315 train_time:120084ms step_avg:60.68ms
step:1980/2315 train_time:120145ms step_avg:60.68ms
step:1981/2315 train_time:120208ms step_avg:60.68ms
step:1982/2315 train_time:120269ms step_avg:60.68ms
step:1983/2315 train_time:120330ms step_avg:60.68ms
step:1984/2315 train_time:120391ms step_avg:60.68ms
step:1985/2315 train_time:120452ms step_avg:60.68ms
step:1986/2315 train_time:120513ms step_avg:60.68ms
step:1987/2315 train_time:120574ms step_avg:60.68ms
step:1988/2315 train_time:120635ms step_avg:60.68ms
step:1989/2315 train_time:120696ms step_avg:60.68ms
step:1990/2315 train_time:120758ms step_avg:60.68ms
step:1991/2315 train_time:120818ms step_avg:60.68ms
step:1992/2315 train_time:120880ms step_avg:60.68ms
step:1993/2315 train_time:120942ms step_avg:60.68ms
step:1994/2315 train_time:121003ms step_avg:60.68ms
step:1995/2315 train_time:121064ms step_avg:60.68ms
step:1996/2315 train_time:121125ms step_avg:60.68ms
step:1997/2315 train_time:121188ms step_avg:60.68ms
step:1998/2315 train_time:121248ms step_avg:60.68ms
step:1999/2315 train_time:121310ms step_avg:60.69ms
step:2000/2315 train_time:121371ms step_avg:60.69ms
step:2000/2315 val_loss:3.3318 train_time:121434ms step_avg:60.72ms
step:2001/2315 train_time:121453ms step_avg:60.70ms
step:2002/2315 train_time:121496ms step_avg:60.69ms
step:2003/2315 train_time:121563ms step_avg:60.69ms
step:2004/2315 train_time:121626ms step_avg:60.69ms
step:2005/2315 train_time:121688ms step_avg:60.69ms
step:2006/2315 train_time:121750ms step_avg:60.69ms
step:2007/2315 train_time:121812ms step_avg:60.69ms
step:2008/2315 train_time:121873ms step_avg:60.69ms
step:2009/2315 train_time:121935ms step_avg:60.69ms
step:2010/2315 train_time:121995ms step_avg:60.69ms
step:2011/2315 train_time:122056ms step_avg:60.69ms
step:2012/2315 train_time:122116ms step_avg:60.69ms
step:2013/2315 train_time:122176ms step_avg:60.69ms
step:2014/2315 train_time:122237ms step_avg:60.69ms
step:2015/2315 train_time:122297ms step_avg:60.69ms
step:2016/2315 train_time:122358ms step_avg:60.69ms
step:2017/2315 train_time:122419ms step_avg:60.69ms
step:2018/2315 train_time:122482ms step_avg:60.69ms
step:2019/2315 train_time:122547ms step_avg:60.70ms
step:2020/2315 train_time:122608ms step_avg:60.70ms
step:2021/2315 train_time:122671ms step_avg:60.70ms
step:2022/2315 train_time:122732ms step_avg:60.70ms
step:2023/2315 train_time:122794ms step_avg:60.70ms
step:2024/2315 train_time:122855ms step_avg:60.70ms
step:2025/2315 train_time:122916ms step_avg:60.70ms
step:2026/2315 train_time:122977ms step_avg:60.70ms
step:2027/2315 train_time:123038ms step_avg:60.70ms
step:2028/2315 train_time:123098ms step_avg:60.70ms
step:2029/2315 train_time:123158ms step_avg:60.70ms
step:2030/2315 train_time:123219ms step_avg:60.70ms
step:2031/2315 train_time:123279ms step_avg:60.70ms
step:2032/2315 train_time:123340ms step_avg:60.70ms
step:2033/2315 train_time:123401ms step_avg:60.70ms
step:2034/2315 train_time:123463ms step_avg:60.70ms
step:2035/2315 train_time:123525ms step_avg:60.70ms
step:2036/2315 train_time:123587ms step_avg:60.70ms
step:2037/2315 train_time:123650ms step_avg:60.70ms
step:2038/2315 train_time:123711ms step_avg:60.70ms
step:2039/2315 train_time:123772ms step_avg:60.70ms
step:2040/2315 train_time:123833ms step_avg:60.70ms
step:2041/2315 train_time:123895ms step_avg:60.70ms
step:2042/2315 train_time:123956ms step_avg:60.70ms
step:2043/2315 train_time:124016ms step_avg:60.70ms
step:2044/2315 train_time:124078ms step_avg:60.70ms
step:2045/2315 train_time:124139ms step_avg:60.70ms
step:2046/2315 train_time:124199ms step_avg:60.70ms
step:2047/2315 train_time:124260ms step_avg:60.70ms
step:2048/2315 train_time:124320ms step_avg:60.70ms
step:2049/2315 train_time:124381ms step_avg:60.70ms
step:2050/2315 train_time:124443ms step_avg:60.70ms
step:2051/2315 train_time:124506ms step_avg:60.71ms
step:2052/2315 train_time:124568ms step_avg:60.71ms
step:2053/2315 train_time:124629ms step_avg:60.71ms
step:2054/2315 train_time:124690ms step_avg:60.71ms
step:2055/2315 train_time:124752ms step_avg:60.71ms
step:2056/2315 train_time:124813ms step_avg:60.71ms
step:2057/2315 train_time:124875ms step_avg:60.71ms
step:2058/2315 train_time:124936ms step_avg:60.71ms
step:2059/2315 train_time:124997ms step_avg:60.71ms
step:2060/2315 train_time:125057ms step_avg:60.71ms
step:2061/2315 train_time:125118ms step_avg:60.71ms
step:2062/2315 train_time:125179ms step_avg:60.71ms
step:2063/2315 train_time:125240ms step_avg:60.71ms
step:2064/2315 train_time:125300ms step_avg:60.71ms
step:2065/2315 train_time:125362ms step_avg:60.71ms
step:2066/2315 train_time:125423ms step_avg:60.71ms
step:2067/2315 train_time:125485ms step_avg:60.71ms
step:2068/2315 train_time:125548ms step_avg:60.71ms
step:2069/2315 train_time:125610ms step_avg:60.71ms
step:2070/2315 train_time:125671ms step_avg:60.71ms
step:2071/2315 train_time:125732ms step_avg:60.71ms
step:2072/2315 train_time:125792ms step_avg:60.71ms
step:2073/2315 train_time:125854ms step_avg:60.71ms
step:2074/2315 train_time:125915ms step_avg:60.71ms
step:2075/2315 train_time:125976ms step_avg:60.71ms
step:2076/2315 train_time:126037ms step_avg:60.71ms
step:2077/2315 train_time:126098ms step_avg:60.71ms
step:2078/2315 train_time:126159ms step_avg:60.71ms
step:2079/2315 train_time:126219ms step_avg:60.71ms
step:2080/2315 train_time:126280ms step_avg:60.71ms
step:2081/2315 train_time:126342ms step_avg:60.71ms
step:2082/2315 train_time:126403ms step_avg:60.71ms
step:2083/2315 train_time:126465ms step_avg:60.71ms
step:2084/2315 train_time:126527ms step_avg:60.71ms
step:2085/2315 train_time:126589ms step_avg:60.71ms
step:2086/2315 train_time:126651ms step_avg:60.71ms
step:2087/2315 train_time:126712ms step_avg:60.71ms
step:2088/2315 train_time:126772ms step_avg:60.71ms
step:2089/2315 train_time:126834ms step_avg:60.72ms
step:2090/2315 train_time:126895ms step_avg:60.72ms
step:2091/2315 train_time:126956ms step_avg:60.72ms
step:2092/2315 train_time:127017ms step_avg:60.72ms
step:2093/2315 train_time:127078ms step_avg:60.72ms
step:2094/2315 train_time:127139ms step_avg:60.72ms
step:2095/2315 train_time:127200ms step_avg:60.72ms
step:2096/2315 train_time:127261ms step_avg:60.72ms
step:2097/2315 train_time:127322ms step_avg:60.72ms
step:2098/2315 train_time:127383ms step_avg:60.72ms
step:2099/2315 train_time:127446ms step_avg:60.72ms
step:2100/2315 train_time:127507ms step_avg:60.72ms
step:2101/2315 train_time:127569ms step_avg:60.72ms
step:2102/2315 train_time:127630ms step_avg:60.72ms
step:2103/2315 train_time:127692ms step_avg:60.72ms
step:2104/2315 train_time:127753ms step_avg:60.72ms
step:2105/2315 train_time:127814ms step_avg:60.72ms
step:2106/2315 train_time:127875ms step_avg:60.72ms
step:2107/2315 train_time:127936ms step_avg:60.72ms
step:2108/2315 train_time:127997ms step_avg:60.72ms
step:2109/2315 train_time:128058ms step_avg:60.72ms
step:2110/2315 train_time:128119ms step_avg:60.72ms
step:2111/2315 train_time:128180ms step_avg:60.72ms
step:2112/2315 train_time:128242ms step_avg:60.72ms
step:2113/2315 train_time:128303ms step_avg:60.72ms
step:2114/2315 train_time:128364ms step_avg:60.72ms
step:2115/2315 train_time:128425ms step_avg:60.72ms
step:2116/2315 train_time:128486ms step_avg:60.72ms
step:2117/2315 train_time:128548ms step_avg:60.72ms
step:2118/2315 train_time:128609ms step_avg:60.72ms
step:2119/2315 train_time:128671ms step_avg:60.72ms
step:2120/2315 train_time:128732ms step_avg:60.72ms
step:2121/2315 train_time:128794ms step_avg:60.72ms
step:2122/2315 train_time:128855ms step_avg:60.72ms
step:2123/2315 train_time:128916ms step_avg:60.72ms
step:2124/2315 train_time:128977ms step_avg:60.72ms
step:2125/2315 train_time:129038ms step_avg:60.72ms
step:2126/2315 train_time:129099ms step_avg:60.72ms
step:2127/2315 train_time:129160ms step_avg:60.72ms
step:2128/2315 train_time:129222ms step_avg:60.72ms
step:2129/2315 train_time:129283ms step_avg:60.72ms
step:2130/2315 train_time:129344ms step_avg:60.72ms
step:2131/2315 train_time:129406ms step_avg:60.73ms
step:2132/2315 train_time:129467ms step_avg:60.73ms
step:2133/2315 train_time:129529ms step_avg:60.73ms
step:2134/2315 train_time:129590ms step_avg:60.73ms
step:2135/2315 train_time:129651ms step_avg:60.73ms
step:2136/2315 train_time:129712ms step_avg:60.73ms
step:2137/2315 train_time:129773ms step_avg:60.73ms
step:2138/2315 train_time:129834ms step_avg:60.73ms
step:2139/2315 train_time:129895ms step_avg:60.73ms
step:2140/2315 train_time:129957ms step_avg:60.73ms
step:2141/2315 train_time:130018ms step_avg:60.73ms
step:2142/2315 train_time:130079ms step_avg:60.73ms
step:2143/2315 train_time:130140ms step_avg:60.73ms
step:2144/2315 train_time:130201ms step_avg:60.73ms
step:2145/2315 train_time:130263ms step_avg:60.73ms
step:2146/2315 train_time:130324ms step_avg:60.73ms
step:2147/2315 train_time:130386ms step_avg:60.73ms
step:2148/2315 train_time:130448ms step_avg:60.73ms
step:2149/2315 train_time:130509ms step_avg:60.73ms
step:2150/2315 train_time:130571ms step_avg:60.73ms
step:2151/2315 train_time:130633ms step_avg:60.73ms
step:2152/2315 train_time:130694ms step_avg:60.73ms
step:2153/2315 train_time:130755ms step_avg:60.73ms
step:2154/2315 train_time:130816ms step_avg:60.73ms
step:2155/2315 train_time:130877ms step_avg:60.73ms
step:2156/2315 train_time:130939ms step_avg:60.73ms
step:2157/2315 train_time:131000ms step_avg:60.73ms
step:2158/2315 train_time:131061ms step_avg:60.73ms
step:2159/2315 train_time:131122ms step_avg:60.73ms
step:2160/2315 train_time:131183ms step_avg:60.73ms
step:2161/2315 train_time:131245ms step_avg:60.73ms
step:2162/2315 train_time:131306ms step_avg:60.73ms
step:2163/2315 train_time:131367ms step_avg:60.73ms
step:2164/2315 train_time:131428ms step_avg:60.73ms
step:2165/2315 train_time:131489ms step_avg:60.73ms
step:2166/2315 train_time:131550ms step_avg:60.73ms
step:2167/2315 train_time:131612ms step_avg:60.73ms
step:2168/2315 train_time:131673ms step_avg:60.73ms
step:2169/2315 train_time:131735ms step_avg:60.74ms
step:2170/2315 train_time:131796ms step_avg:60.74ms
step:2171/2315 train_time:131857ms step_avg:60.74ms
step:2172/2315 train_time:131918ms step_avg:60.74ms
step:2173/2315 train_time:131980ms step_avg:60.74ms
step:2174/2315 train_time:132041ms step_avg:60.74ms
step:2175/2315 train_time:132102ms step_avg:60.74ms
step:2176/2315 train_time:132163ms step_avg:60.74ms
step:2177/2315 train_time:132224ms step_avg:60.74ms
step:2178/2315 train_time:132285ms step_avg:60.74ms
step:2179/2315 train_time:132346ms step_avg:60.74ms
step:2180/2315 train_time:132407ms step_avg:60.74ms
step:2181/2315 train_time:132468ms step_avg:60.74ms
step:2182/2315 train_time:132530ms step_avg:60.74ms
step:2183/2315 train_time:132591ms step_avg:60.74ms
step:2184/2315 train_time:132652ms step_avg:60.74ms
step:2185/2315 train_time:132714ms step_avg:60.74ms
step:2186/2315 train_time:132775ms step_avg:60.74ms
step:2187/2315 train_time:132836ms step_avg:60.74ms
step:2188/2315 train_time:132897ms step_avg:60.74ms
step:2189/2315 train_time:132959ms step_avg:60.74ms
step:2190/2315 train_time:133020ms step_avg:60.74ms
step:2191/2315 train_time:133081ms step_avg:60.74ms
step:2192/2315 train_time:133143ms step_avg:60.74ms
step:2193/2315 train_time:133204ms step_avg:60.74ms
step:2194/2315 train_time:133264ms step_avg:60.74ms
step:2195/2315 train_time:133326ms step_avg:60.74ms
step:2196/2315 train_time:133387ms step_avg:60.74ms
step:2197/2315 train_time:133448ms step_avg:60.74ms
step:2198/2315 train_time:133509ms step_avg:60.74ms
step:2199/2315 train_time:133571ms step_avg:60.74ms
step:2200/2315 train_time:133632ms step_avg:60.74ms
step:2201/2315 train_time:133693ms step_avg:60.74ms
step:2202/2315 train_time:133754ms step_avg:60.74ms
step:2203/2315 train_time:133816ms step_avg:60.74ms
step:2204/2315 train_time:133876ms step_avg:60.74ms
step:2205/2315 train_time:133939ms step_avg:60.74ms
step:2206/2315 train_time:133999ms step_avg:60.74ms
step:2207/2315 train_time:134061ms step_avg:60.74ms
step:2208/2315 train_time:134122ms step_avg:60.74ms
step:2209/2315 train_time:134183ms step_avg:60.74ms
step:2210/2315 train_time:134245ms step_avg:60.74ms
step:2211/2315 train_time:134306ms step_avg:60.74ms
step:2212/2315 train_time:134367ms step_avg:60.74ms
step:2213/2315 train_time:134429ms step_avg:60.75ms
step:2214/2315 train_time:134490ms step_avg:60.75ms
step:2215/2315 train_time:134551ms step_avg:60.75ms
step:2216/2315 train_time:134613ms step_avg:60.75ms
step:2217/2315 train_time:134675ms step_avg:60.75ms
step:2218/2315 train_time:134737ms step_avg:60.75ms
step:2219/2315 train_time:134797ms step_avg:60.75ms
step:2220/2315 train_time:134859ms step_avg:60.75ms
step:2221/2315 train_time:134920ms step_avg:60.75ms
step:2222/2315 train_time:134981ms step_avg:60.75ms
step:2223/2315 train_time:135043ms step_avg:60.75ms
step:2224/2315 train_time:135104ms step_avg:60.75ms
step:2225/2315 train_time:135165ms step_avg:60.75ms
step:2226/2315 train_time:135226ms step_avg:60.75ms
step:2227/2315 train_time:135287ms step_avg:60.75ms
step:2228/2315 train_time:135348ms step_avg:60.75ms
step:2229/2315 train_time:135410ms step_avg:60.75ms
step:2230/2315 train_time:135470ms step_avg:60.75ms
step:2231/2315 train_time:135532ms step_avg:60.75ms
step:2232/2315 train_time:135593ms step_avg:60.75ms
step:2233/2315 train_time:135654ms step_avg:60.75ms
step:2234/2315 train_time:135714ms step_avg:60.75ms
step:2235/2315 train_time:135776ms step_avg:60.75ms
step:2236/2315 train_time:135837ms step_avg:60.75ms
step:2237/2315 train_time:135899ms step_avg:60.75ms
step:2238/2315 train_time:135960ms step_avg:60.75ms
step:2239/2315 train_time:136021ms step_avg:60.75ms
step:2240/2315 train_time:136081ms step_avg:60.75ms
step:2241/2315 train_time:136143ms step_avg:60.75ms
step:2242/2315 train_time:136204ms step_avg:60.75ms
step:2243/2315 train_time:136265ms step_avg:60.75ms
step:2244/2315 train_time:136327ms step_avg:60.75ms
step:2245/2315 train_time:136388ms step_avg:60.75ms
step:2246/2315 train_time:136449ms step_avg:60.75ms
step:2247/2315 train_time:136510ms step_avg:60.75ms
step:2248/2315 train_time:136571ms step_avg:60.75ms
step:2249/2315 train_time:136632ms step_avg:60.75ms
step:2250/2315 train_time:136693ms step_avg:60.75ms
step:2250/2315 val_loss:3.2923 train_time:136757ms step_avg:60.78ms
step:2251/2315 train_time:136777ms step_avg:60.76ms
step:2252/2315 train_time:136818ms step_avg:60.75ms
step:2253/2315 train_time:136883ms step_avg:60.76ms
step:2254/2315 train_time:136946ms step_avg:60.76ms
step:2255/2315 train_time:137009ms step_avg:60.76ms
step:2256/2315 train_time:137071ms step_avg:60.76ms
step:2257/2315 train_time:137131ms step_avg:60.76ms
step:2258/2315 train_time:137192ms step_avg:60.76ms
step:2259/2315 train_time:137252ms step_avg:60.76ms
step:2260/2315 train_time:137313ms step_avg:60.76ms
step:2261/2315 train_time:137374ms step_avg:60.76ms
step:2262/2315 train_time:137434ms step_avg:60.76ms
step:2263/2315 train_time:137495ms step_avg:60.76ms
step:2264/2315 train_time:137556ms step_avg:60.76ms
step:2265/2315 train_time:137617ms step_avg:60.76ms
step:2266/2315 train_time:137678ms step_avg:60.76ms
step:2267/2315 train_time:137740ms step_avg:60.76ms
step:2268/2315 train_time:137803ms step_avg:60.76ms
step:2269/2315 train_time:137865ms step_avg:60.76ms
step:2270/2315 train_time:137927ms step_avg:60.76ms
step:2271/2315 train_time:137990ms step_avg:60.76ms
step:2272/2315 train_time:138051ms step_avg:60.76ms
step:2273/2315 train_time:138113ms step_avg:60.76ms
step:2274/2315 train_time:138174ms step_avg:60.76ms
step:2275/2315 train_time:138235ms step_avg:60.76ms
step:2276/2315 train_time:138295ms step_avg:60.76ms
step:2277/2315 train_time:138356ms step_avg:60.76ms
step:2278/2315 train_time:138418ms step_avg:60.76ms
step:2279/2315 train_time:138479ms step_avg:60.76ms
step:2280/2315 train_time:138539ms step_avg:60.76ms
step:2281/2315 train_time:138600ms step_avg:60.76ms
step:2282/2315 train_time:138661ms step_avg:60.76ms
step:2283/2315 train_time:138723ms step_avg:60.76ms
step:2284/2315 train_time:138785ms step_avg:60.76ms
step:2285/2315 train_time:138846ms step_avg:60.76ms
step:2286/2315 train_time:138908ms step_avg:60.76ms
step:2287/2315 train_time:138970ms step_avg:60.77ms
step:2288/2315 train_time:139031ms step_avg:60.77ms
step:2289/2315 train_time:139093ms step_avg:60.77ms
step:2290/2315 train_time:139153ms step_avg:60.77ms
step:2291/2315 train_time:139214ms step_avg:60.77ms
step:2292/2315 train_time:139275ms step_avg:60.77ms
step:2293/2315 train_time:139336ms step_avg:60.77ms
step:2294/2315 train_time:139397ms step_avg:60.77ms
step:2295/2315 train_time:139458ms step_avg:60.77ms
step:2296/2315 train_time:139519ms step_avg:60.77ms
step:2297/2315 train_time:139580ms step_avg:60.77ms
step:2298/2315 train_time:139641ms step_avg:60.77ms
step:2299/2315 train_time:139702ms step_avg:60.77ms
step:2300/2315 train_time:139764ms step_avg:60.77ms
step:2301/2315 train_time:139826ms step_avg:60.77ms
step:2302/2315 train_time:139887ms step_avg:60.77ms
step:2303/2315 train_time:139948ms step_avg:60.77ms
step:2304/2315 train_time:140010ms step_avg:60.77ms
step:2305/2315 train_time:140072ms step_avg:60.77ms
step:2306/2315 train_time:140133ms step_avg:60.77ms
step:2307/2315 train_time:140194ms step_avg:60.77ms
step:2308/2315 train_time:140255ms step_avg:60.77ms
step:2309/2315 train_time:140316ms step_avg:60.77ms
step:2310/2315 train_time:140378ms step_avg:60.77ms
step:2311/2315 train_time:140439ms step_avg:60.77ms
step:2312/2315 train_time:140500ms step_avg:60.77ms
step:2313/2315 train_time:140560ms step_avg:60.77ms
step:2314/2315 train_time:140621ms step_avg:60.77ms
step:2315/2315 train_time:140683ms step_avg:60.77ms
step:2315/2315 val_loss:3.2797 train_time:140744ms step_avg:60.80ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

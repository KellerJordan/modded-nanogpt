import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]
# polar_express_coeffs = [
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
# ]



@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    # state["second_momentum_buffer"] = torch.zeros_like(grad[...,0:1]) if param.size(-2) >= param.size(-1) else torch.zeros_like(grad[...,0:1,:])
                    # state["second_momentum_buffer"] = torch.zeros((*grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((*grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)
            
            ###################################
            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))
            ####################################


            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Oct 22 2025, 02:20:45) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251022+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 24 06:44:31 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000001:00:00.0 Off |                    0 |
| N/A   35C    P0            119W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000002:00:00.0 Off |                    0 |
| N/A   34C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000003:00:00.0 Off |                    0 |
| N/A   30C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000008:00:00.0 Off |                    0 |
| N/A   30C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000009:00:00.0 Off |                    0 |
| N/A   28C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   0000000A:00:00.0 Off |                    0 |
| N/A   35C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   0000000C:00:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2315 train_time:93ms step_avg:92.58ms
step:2/2315 train_time:188ms step_avg:93.98ms
step:3/2315 train_time:210ms step_avg:69.88ms
step:4/2315 train_time:246ms step_avg:61.45ms
step:5/2315 train_time:304ms step_avg:60.89ms
step:6/2315 train_time:364ms step_avg:60.71ms
step:7/2315 train_time:424ms step_avg:60.55ms
step:8/2315 train_time:484ms step_avg:60.56ms
step:9/2315 train_time:544ms step_avg:60.43ms
step:10/2315 train_time:604ms step_avg:60.36ms
step:11/2315 train_time:664ms step_avg:60.33ms
step:12/2315 train_time:724ms step_avg:60.32ms
step:13/2315 train_time:784ms step_avg:60.29ms
step:14/2315 train_time:844ms step_avg:60.29ms
step:15/2315 train_time:904ms step_avg:60.29ms
step:16/2315 train_time:964ms step_avg:60.28ms
step:17/2315 train_time:1028ms step_avg:60.44ms
step:18/2315 train_time:1090ms step_avg:60.57ms
step:19/2315 train_time:1153ms step_avg:60.70ms
step:20/2315 train_time:1215ms step_avg:60.74ms
step:21/2315 train_time:1276ms step_avg:60.76ms
step:22/2315 train_time:1337ms step_avg:60.76ms
step:23/2315 train_time:1397ms step_avg:60.73ms
step:24/2315 train_time:1457ms step_avg:60.70ms
step:25/2315 train_time:1517ms step_avg:60.67ms
step:26/2315 train_time:1577ms step_avg:60.65ms
step:27/2315 train_time:1637ms step_avg:60.63ms
step:28/2315 train_time:1698ms step_avg:60.63ms
step:29/2315 train_time:1758ms step_avg:60.62ms
step:30/2315 train_time:1819ms step_avg:60.62ms
step:31/2315 train_time:1879ms step_avg:60.62ms
step:32/2315 train_time:1940ms step_avg:60.61ms
step:33/2315 train_time:2002ms step_avg:60.66ms
step:34/2315 train_time:2064ms step_avg:60.72ms
step:35/2315 train_time:2127ms step_avg:60.77ms
step:36/2315 train_time:2188ms step_avg:60.77ms
step:37/2315 train_time:2249ms step_avg:60.79ms
step:38/2315 train_time:2309ms step_avg:60.76ms
step:39/2315 train_time:2370ms step_avg:60.76ms
step:40/2315 train_time:2430ms step_avg:60.75ms
step:41/2315 train_time:2490ms step_avg:60.74ms
step:42/2315 train_time:2551ms step_avg:60.73ms
step:43/2315 train_time:2612ms step_avg:60.73ms
step:44/2315 train_time:2672ms step_avg:60.73ms
step:45/2315 train_time:2733ms step_avg:60.72ms
step:46/2315 train_time:2793ms step_avg:60.71ms
step:47/2315 train_time:2853ms step_avg:60.71ms
step:48/2315 train_time:2914ms step_avg:60.70ms
step:49/2315 train_time:2975ms step_avg:60.70ms
step:50/2315 train_time:3035ms step_avg:60.70ms
step:51/2315 train_time:3096ms step_avg:60.70ms
step:52/2315 train_time:3156ms step_avg:60.70ms
step:53/2315 train_time:3218ms step_avg:60.72ms
step:54/2315 train_time:3278ms step_avg:60.71ms
step:55/2315 train_time:3338ms step_avg:60.70ms
step:56/2315 train_time:3399ms step_avg:60.70ms
step:57/2315 train_time:3460ms step_avg:60.70ms
step:58/2315 train_time:3520ms step_avg:60.69ms
step:59/2315 train_time:3580ms step_avg:60.68ms
step:60/2315 train_time:3640ms step_avg:60.67ms
step:61/2315 train_time:3701ms step_avg:60.66ms
step:62/2315 train_time:3760ms step_avg:60.65ms
step:63/2315 train_time:3821ms step_avg:60.65ms
step:64/2315 train_time:3882ms step_avg:60.65ms
step:65/2315 train_time:3943ms step_avg:60.66ms
step:66/2315 train_time:4003ms step_avg:60.65ms
step:67/2315 train_time:4063ms step_avg:60.65ms
step:68/2315 train_time:4124ms step_avg:60.64ms
step:69/2315 train_time:4185ms step_avg:60.65ms
step:70/2315 train_time:4246ms step_avg:60.65ms
step:71/2315 train_time:4307ms step_avg:60.66ms
step:72/2315 train_time:4367ms step_avg:60.65ms
step:73/2315 train_time:4428ms step_avg:60.65ms
step:74/2315 train_time:4488ms step_avg:60.65ms
step:75/2315 train_time:4548ms step_avg:60.65ms
step:76/2315 train_time:4608ms step_avg:60.63ms
step:77/2315 train_time:4668ms step_avg:60.63ms
step:78/2315 train_time:4729ms step_avg:60.63ms
step:79/2315 train_time:4789ms step_avg:60.62ms
step:80/2315 train_time:4850ms step_avg:60.62ms
step:81/2315 train_time:4910ms step_avg:60.62ms
step:82/2315 train_time:4970ms step_avg:60.61ms
step:83/2315 train_time:5031ms step_avg:60.61ms
step:84/2315 train_time:5091ms step_avg:60.61ms
step:85/2315 train_time:5151ms step_avg:60.60ms
step:86/2315 train_time:5212ms step_avg:60.60ms
step:87/2315 train_time:5272ms step_avg:60.60ms
step:88/2315 train_time:5332ms step_avg:60.60ms
step:89/2315 train_time:5392ms step_avg:60.59ms
step:90/2315 train_time:5452ms step_avg:60.58ms
step:91/2315 train_time:5512ms step_avg:60.58ms
step:92/2315 train_time:5573ms step_avg:60.57ms
step:93/2315 train_time:5633ms step_avg:60.57ms
step:94/2315 train_time:5693ms step_avg:60.56ms
step:95/2315 train_time:5753ms step_avg:60.55ms
step:96/2315 train_time:5812ms step_avg:60.55ms
step:97/2315 train_time:5873ms step_avg:60.55ms
step:98/2315 train_time:5933ms step_avg:60.54ms
step:99/2315 train_time:5993ms step_avg:60.54ms
step:100/2315 train_time:6053ms step_avg:60.53ms
step:101/2315 train_time:6113ms step_avg:60.53ms
step:102/2315 train_time:6174ms step_avg:60.53ms
step:103/2315 train_time:6235ms step_avg:60.53ms
step:104/2315 train_time:6295ms step_avg:60.53ms
step:105/2315 train_time:6355ms step_avg:60.52ms
step:106/2315 train_time:6416ms step_avg:60.52ms
step:107/2315 train_time:6476ms step_avg:60.52ms
step:108/2315 train_time:6536ms step_avg:60.52ms
step:109/2315 train_time:6596ms step_avg:60.51ms
step:110/2315 train_time:6655ms step_avg:60.50ms
step:111/2315 train_time:6716ms step_avg:60.50ms
step:112/2315 train_time:6775ms step_avg:60.50ms
step:113/2315 train_time:6836ms step_avg:60.50ms
step:114/2315 train_time:6896ms step_avg:60.49ms
step:115/2315 train_time:6957ms step_avg:60.49ms
step:116/2315 train_time:7017ms step_avg:60.49ms
step:117/2315 train_time:7077ms step_avg:60.49ms
step:118/2315 train_time:7137ms step_avg:60.48ms
step:119/2315 train_time:7198ms step_avg:60.49ms
step:120/2315 train_time:7259ms step_avg:60.49ms
step:121/2315 train_time:7320ms step_avg:60.49ms
step:122/2315 train_time:7380ms step_avg:60.49ms
step:123/2315 train_time:7440ms step_avg:60.49ms
step:124/2315 train_time:7500ms step_avg:60.48ms
step:125/2315 train_time:7560ms step_avg:60.48ms
step:126/2315 train_time:7619ms step_avg:60.47ms
step:127/2315 train_time:7679ms step_avg:60.47ms
step:128/2315 train_time:7739ms step_avg:60.46ms
step:129/2315 train_time:7799ms step_avg:60.46ms
step:130/2315 train_time:7860ms step_avg:60.46ms
step:131/2315 train_time:7920ms step_avg:60.46ms
step:132/2315 train_time:7980ms step_avg:60.45ms
step:133/2315 train_time:8040ms step_avg:60.45ms
step:134/2315 train_time:8099ms step_avg:60.44ms
step:135/2315 train_time:8159ms step_avg:60.44ms
step:136/2315 train_time:8219ms step_avg:60.44ms
step:137/2315 train_time:8280ms step_avg:60.44ms
step:138/2315 train_time:8340ms step_avg:60.44ms
step:139/2315 train_time:8402ms step_avg:60.44ms
step:140/2315 train_time:8462ms step_avg:60.44ms
step:141/2315 train_time:8522ms step_avg:60.44ms
step:142/2315 train_time:8582ms step_avg:60.44ms
step:143/2315 train_time:8643ms step_avg:60.44ms
step:144/2315 train_time:8702ms step_avg:60.43ms
step:145/2315 train_time:8763ms step_avg:60.43ms
step:146/2315 train_time:8822ms step_avg:60.43ms
step:147/2315 train_time:8883ms step_avg:60.43ms
step:148/2315 train_time:8942ms step_avg:60.42ms
step:149/2315 train_time:9004ms step_avg:60.43ms
step:150/2315 train_time:9064ms step_avg:60.43ms
step:151/2315 train_time:9124ms step_avg:60.43ms
step:152/2315 train_time:9185ms step_avg:60.43ms
step:153/2315 train_time:9245ms step_avg:60.42ms
step:154/2315 train_time:9305ms step_avg:60.42ms
step:155/2315 train_time:9365ms step_avg:60.42ms
step:156/2315 train_time:9425ms step_avg:60.42ms
step:157/2315 train_time:9485ms step_avg:60.42ms
step:158/2315 train_time:9545ms step_avg:60.41ms
step:159/2315 train_time:9605ms step_avg:60.41ms
step:160/2315 train_time:9664ms step_avg:60.40ms
step:161/2315 train_time:9724ms step_avg:60.40ms
step:162/2315 train_time:9784ms step_avg:60.39ms
step:163/2315 train_time:9844ms step_avg:60.39ms
step:164/2315 train_time:9904ms step_avg:60.39ms
step:165/2315 train_time:9965ms step_avg:60.39ms
step:166/2315 train_time:10025ms step_avg:60.39ms
step:167/2315 train_time:10085ms step_avg:60.39ms
step:168/2315 train_time:10146ms step_avg:60.39ms
step:169/2315 train_time:10207ms step_avg:60.40ms
step:170/2315 train_time:10267ms step_avg:60.39ms
step:171/2315 train_time:10328ms step_avg:60.40ms
step:172/2315 train_time:10388ms step_avg:60.39ms
step:173/2315 train_time:10449ms step_avg:60.40ms
step:174/2315 train_time:10508ms step_avg:60.39ms
step:175/2315 train_time:10569ms step_avg:60.39ms
step:176/2315 train_time:10628ms step_avg:60.39ms
step:177/2315 train_time:10689ms step_avg:60.39ms
step:178/2315 train_time:10748ms step_avg:60.38ms
step:179/2315 train_time:10809ms step_avg:60.38ms
step:180/2315 train_time:10869ms step_avg:60.38ms
step:181/2315 train_time:10930ms step_avg:60.38ms
step:182/2315 train_time:10990ms step_avg:60.38ms
step:183/2315 train_time:11050ms step_avg:60.38ms
step:184/2315 train_time:11110ms step_avg:60.38ms
step:185/2315 train_time:11170ms step_avg:60.38ms
step:186/2315 train_time:11230ms step_avg:60.38ms
step:187/2315 train_time:11290ms step_avg:60.37ms
step:188/2315 train_time:11350ms step_avg:60.37ms
step:189/2315 train_time:11410ms step_avg:60.37ms
step:190/2315 train_time:11469ms step_avg:60.36ms
step:191/2315 train_time:11529ms step_avg:60.36ms
step:192/2315 train_time:11589ms step_avg:60.36ms
step:193/2315 train_time:11649ms step_avg:60.36ms
step:194/2315 train_time:11708ms step_avg:60.35ms
step:195/2315 train_time:11769ms step_avg:60.35ms
step:196/2315 train_time:11829ms step_avg:60.35ms
step:197/2315 train_time:11890ms step_avg:60.35ms
step:198/2315 train_time:11949ms step_avg:60.35ms
step:199/2315 train_time:12011ms step_avg:60.35ms
step:200/2315 train_time:12071ms step_avg:60.36ms
step:201/2315 train_time:12131ms step_avg:60.35ms
step:202/2315 train_time:12191ms step_avg:60.35ms
step:203/2315 train_time:12251ms step_avg:60.35ms
step:204/2315 train_time:12311ms step_avg:60.35ms
step:205/2315 train_time:12370ms step_avg:60.34ms
step:206/2315 train_time:12430ms step_avg:60.34ms
step:207/2315 train_time:12490ms step_avg:60.34ms
step:208/2315 train_time:12550ms step_avg:60.33ms
step:209/2315 train_time:12610ms step_avg:60.34ms
step:210/2315 train_time:12671ms step_avg:60.34ms
step:211/2315 train_time:12731ms step_avg:60.34ms
step:212/2315 train_time:12791ms step_avg:60.34ms
step:213/2315 train_time:12851ms step_avg:60.34ms
step:214/2315 train_time:12911ms step_avg:60.33ms
step:215/2315 train_time:12972ms step_avg:60.33ms
step:216/2315 train_time:13032ms step_avg:60.33ms
step:217/2315 train_time:13092ms step_avg:60.33ms
step:218/2315 train_time:13152ms step_avg:60.33ms
step:219/2315 train_time:13212ms step_avg:60.33ms
step:220/2315 train_time:13271ms step_avg:60.32ms
step:221/2315 train_time:13331ms step_avg:60.32ms
step:222/2315 train_time:13391ms step_avg:60.32ms
step:223/2315 train_time:13450ms step_avg:60.32ms
step:224/2315 train_time:13510ms step_avg:60.31ms
step:225/2315 train_time:13570ms step_avg:60.31ms
step:226/2315 train_time:13630ms step_avg:60.31ms
step:227/2315 train_time:13690ms step_avg:60.31ms
step:228/2315 train_time:13750ms step_avg:60.31ms
step:229/2315 train_time:13810ms step_avg:60.31ms
step:230/2315 train_time:13870ms step_avg:60.31ms
step:231/2315 train_time:13930ms step_avg:60.30ms
step:232/2315 train_time:13990ms step_avg:60.30ms
step:233/2315 train_time:14050ms step_avg:60.30ms
step:234/2315 train_time:14111ms step_avg:60.30ms
step:235/2315 train_time:14171ms step_avg:60.30ms
step:236/2315 train_time:14231ms step_avg:60.30ms
step:237/2315 train_time:14290ms step_avg:60.30ms
step:238/2315 train_time:14351ms step_avg:60.30ms
step:239/2315 train_time:14410ms step_avg:60.29ms
step:240/2315 train_time:14470ms step_avg:60.29ms
step:241/2315 train_time:14530ms step_avg:60.29ms
step:242/2315 train_time:14590ms step_avg:60.29ms
step:243/2315 train_time:14650ms step_avg:60.29ms
step:244/2315 train_time:14709ms step_avg:60.28ms
step:245/2315 train_time:14770ms step_avg:60.29ms
step:246/2315 train_time:14830ms step_avg:60.28ms
step:247/2315 train_time:14890ms step_avg:60.28ms
step:248/2315 train_time:14950ms step_avg:60.28ms
step:249/2315 train_time:15010ms step_avg:60.28ms
step:250/2315 train_time:15070ms step_avg:60.28ms
step:250/2315 val_loss:4.0708 train_time:15132ms step_avg:60.53ms
step:251/2315 train_time:15151ms step_avg:60.36ms
step:252/2315 train_time:15193ms step_avg:60.29ms
step:253/2315 train_time:15260ms step_avg:60.32ms
step:254/2315 train_time:15324ms step_avg:60.33ms
step:255/2315 train_time:15384ms step_avg:60.33ms
step:256/2315 train_time:15445ms step_avg:60.33ms
step:257/2315 train_time:15505ms step_avg:60.33ms
step:258/2315 train_time:15565ms step_avg:60.33ms
step:259/2315 train_time:15626ms step_avg:60.33ms
step:260/2315 train_time:15686ms step_avg:60.33ms
step:261/2315 train_time:15745ms step_avg:60.33ms
step:262/2315 train_time:15805ms step_avg:60.32ms
step:263/2315 train_time:15864ms step_avg:60.32ms
step:264/2315 train_time:15924ms step_avg:60.32ms
step:265/2315 train_time:15983ms step_avg:60.31ms
step:266/2315 train_time:16043ms step_avg:60.31ms
step:267/2315 train_time:16104ms step_avg:60.31ms
step:268/2315 train_time:16165ms step_avg:60.32ms
step:269/2315 train_time:16227ms step_avg:60.32ms
step:270/2315 train_time:16288ms step_avg:60.33ms
step:271/2315 train_time:16349ms step_avg:60.33ms
step:272/2315 train_time:16409ms step_avg:60.33ms
step:273/2315 train_time:16469ms step_avg:60.33ms
step:274/2315 train_time:16529ms step_avg:60.32ms
step:275/2315 train_time:16589ms step_avg:60.32ms
step:276/2315 train_time:16648ms step_avg:60.32ms
step:277/2315 train_time:16708ms step_avg:60.32ms
step:278/2315 train_time:16768ms step_avg:60.32ms
step:279/2315 train_time:16828ms step_avg:60.32ms
step:280/2315 train_time:16887ms step_avg:60.31ms
step:281/2315 train_time:16947ms step_avg:60.31ms
step:282/2315 train_time:17006ms step_avg:60.30ms
step:283/2315 train_time:17066ms step_avg:60.30ms
step:284/2315 train_time:17126ms step_avg:60.30ms
step:285/2315 train_time:17187ms step_avg:60.31ms
step:286/2315 train_time:17249ms step_avg:60.31ms
step:287/2315 train_time:17309ms step_avg:60.31ms
step:288/2315 train_time:17369ms step_avg:60.31ms
step:289/2315 train_time:17429ms step_avg:60.31ms
step:290/2315 train_time:17489ms step_avg:60.31ms
step:291/2315 train_time:17549ms step_avg:60.31ms
step:292/2315 train_time:17608ms step_avg:60.30ms
step:293/2315 train_time:17668ms step_avg:60.30ms
step:294/2315 train_time:17728ms step_avg:60.30ms
step:295/2315 train_time:17788ms step_avg:60.30ms
step:296/2315 train_time:17848ms step_avg:60.30ms
step:297/2315 train_time:17908ms step_avg:60.30ms
step:298/2315 train_time:17968ms step_avg:60.30ms
step:299/2315 train_time:18029ms step_avg:60.30ms
step:300/2315 train_time:18088ms step_avg:60.29ms
step:301/2315 train_time:18149ms step_avg:60.30ms
step:302/2315 train_time:18209ms step_avg:60.30ms
step:303/2315 train_time:18270ms step_avg:60.30ms
step:304/2315 train_time:18330ms step_avg:60.30ms
step:305/2315 train_time:18391ms step_avg:60.30ms
step:306/2315 train_time:18451ms step_avg:60.30ms
step:307/2315 train_time:18511ms step_avg:60.30ms
step:308/2315 train_time:18571ms step_avg:60.30ms
step:309/2315 train_time:18632ms step_avg:60.30ms
step:310/2315 train_time:18691ms step_avg:60.29ms
step:311/2315 train_time:18751ms step_avg:60.29ms
step:312/2315 train_time:18810ms step_avg:60.29ms
step:313/2315 train_time:18870ms step_avg:60.29ms
step:314/2315 train_time:18930ms step_avg:60.29ms
step:315/2315 train_time:18990ms step_avg:60.29ms
step:316/2315 train_time:19050ms step_avg:60.28ms
step:317/2315 train_time:19111ms step_avg:60.29ms
step:318/2315 train_time:19171ms step_avg:60.29ms
step:319/2315 train_time:19230ms step_avg:60.28ms
step:320/2315 train_time:19290ms step_avg:60.28ms
step:321/2315 train_time:19350ms step_avg:60.28ms
step:322/2315 train_time:19411ms step_avg:60.28ms
step:323/2315 train_time:19471ms step_avg:60.28ms
step:324/2315 train_time:19531ms step_avg:60.28ms
step:325/2315 train_time:19590ms step_avg:60.28ms
step:326/2315 train_time:19650ms step_avg:60.28ms
step:327/2315 train_time:19709ms step_avg:60.27ms
step:328/2315 train_time:19770ms step_avg:60.28ms
step:329/2315 train_time:19829ms step_avg:60.27ms
step:330/2315 train_time:19888ms step_avg:60.27ms
step:331/2315 train_time:19948ms step_avg:60.27ms
step:332/2315 train_time:20009ms step_avg:60.27ms
step:333/2315 train_time:20069ms step_avg:60.27ms
step:334/2315 train_time:20129ms step_avg:60.27ms
step:335/2315 train_time:20189ms step_avg:60.27ms
step:336/2315 train_time:20250ms step_avg:60.27ms
step:337/2315 train_time:20310ms step_avg:60.27ms
step:338/2315 train_time:20370ms step_avg:60.27ms
step:339/2315 train_time:20431ms step_avg:60.27ms
step:340/2315 train_time:20490ms step_avg:60.27ms
step:341/2315 train_time:20550ms step_avg:60.27ms
step:342/2315 train_time:20610ms step_avg:60.26ms
step:343/2315 train_time:20670ms step_avg:60.26ms
step:344/2315 train_time:20730ms step_avg:60.26ms
step:345/2315 train_time:20790ms step_avg:60.26ms
step:346/2315 train_time:20849ms step_avg:60.26ms
step:347/2315 train_time:20909ms step_avg:60.26ms
step:348/2315 train_time:20969ms step_avg:60.25ms
step:349/2315 train_time:21030ms step_avg:60.26ms
step:350/2315 train_time:21089ms step_avg:60.26ms
step:351/2315 train_time:21150ms step_avg:60.26ms
step:352/2315 train_time:21210ms step_avg:60.26ms
step:353/2315 train_time:21270ms step_avg:60.26ms
step:354/2315 train_time:21330ms step_avg:60.25ms
step:355/2315 train_time:21390ms step_avg:60.25ms
step:356/2315 train_time:21450ms step_avg:60.25ms
step:357/2315 train_time:21510ms step_avg:60.25ms
step:358/2315 train_time:21570ms step_avg:60.25ms
step:359/2315 train_time:21631ms step_avg:60.25ms
step:360/2315 train_time:21690ms step_avg:60.25ms
step:361/2315 train_time:21751ms step_avg:60.25ms
step:362/2315 train_time:21810ms step_avg:60.25ms
step:363/2315 train_time:21870ms step_avg:60.25ms
step:364/2315 train_time:21930ms step_avg:60.25ms
step:365/2315 train_time:21990ms step_avg:60.25ms
step:366/2315 train_time:22049ms step_avg:60.24ms
step:367/2315 train_time:22110ms step_avg:60.24ms
step:368/2315 train_time:22170ms step_avg:60.24ms
step:369/2315 train_time:22230ms step_avg:60.24ms
step:370/2315 train_time:22290ms step_avg:60.24ms
step:371/2315 train_time:22350ms step_avg:60.24ms
step:372/2315 train_time:22410ms step_avg:60.24ms
step:373/2315 train_time:22470ms step_avg:60.24ms
step:374/2315 train_time:22530ms step_avg:60.24ms
step:375/2315 train_time:22590ms step_avg:60.24ms
step:376/2315 train_time:22650ms step_avg:60.24ms
step:377/2315 train_time:22710ms step_avg:60.24ms
step:378/2315 train_time:22770ms step_avg:60.24ms
step:379/2315 train_time:22830ms step_avg:60.24ms
step:380/2315 train_time:22890ms step_avg:60.24ms
step:381/2315 train_time:22949ms step_avg:60.23ms
step:382/2315 train_time:23010ms step_avg:60.23ms
step:383/2315 train_time:23070ms step_avg:60.23ms
step:384/2315 train_time:23130ms step_avg:60.23ms
step:385/2315 train_time:23190ms step_avg:60.23ms
step:386/2315 train_time:23250ms step_avg:60.23ms
step:387/2315 train_time:23310ms step_avg:60.23ms
step:388/2315 train_time:23370ms step_avg:60.23ms
step:389/2315 train_time:23431ms step_avg:60.23ms
step:390/2315 train_time:23491ms step_avg:60.23ms
step:391/2315 train_time:23550ms step_avg:60.23ms
step:392/2315 train_time:23610ms step_avg:60.23ms
step:393/2315 train_time:23670ms step_avg:60.23ms
step:394/2315 train_time:23729ms step_avg:60.23ms
step:395/2315 train_time:23789ms step_avg:60.23ms
step:396/2315 train_time:23849ms step_avg:60.23ms
step:397/2315 train_time:23910ms step_avg:60.23ms
step:398/2315 train_time:23969ms step_avg:60.22ms
step:399/2315 train_time:24029ms step_avg:60.22ms
step:400/2315 train_time:24089ms step_avg:60.22ms
step:401/2315 train_time:24149ms step_avg:60.22ms
step:402/2315 train_time:24209ms step_avg:60.22ms
step:403/2315 train_time:24269ms step_avg:60.22ms
step:404/2315 train_time:24329ms step_avg:60.22ms
step:405/2315 train_time:24389ms step_avg:60.22ms
step:406/2315 train_time:24449ms step_avg:60.22ms
step:407/2315 train_time:24509ms step_avg:60.22ms
step:408/2315 train_time:24569ms step_avg:60.22ms
step:409/2315 train_time:24629ms step_avg:60.22ms
step:410/2315 train_time:24689ms step_avg:60.22ms
step:411/2315 train_time:24749ms step_avg:60.22ms
step:412/2315 train_time:24809ms step_avg:60.22ms
step:413/2315 train_time:24869ms step_avg:60.21ms
step:414/2315 train_time:24928ms step_avg:60.21ms
step:415/2315 train_time:24988ms step_avg:60.21ms
step:416/2315 train_time:25049ms step_avg:60.21ms
step:417/2315 train_time:25109ms step_avg:60.21ms
step:418/2315 train_time:25169ms step_avg:60.21ms
step:419/2315 train_time:25229ms step_avg:60.21ms
step:420/2315 train_time:25289ms step_avg:60.21ms
step:421/2315 train_time:25349ms step_avg:60.21ms
step:422/2315 train_time:25409ms step_avg:60.21ms
step:423/2315 train_time:25469ms step_avg:60.21ms
step:424/2315 train_time:25529ms step_avg:60.21ms
step:425/2315 train_time:25591ms step_avg:60.21ms
step:426/2315 train_time:25650ms step_avg:60.21ms
step:427/2315 train_time:25710ms step_avg:60.21ms
step:428/2315 train_time:25770ms step_avg:60.21ms
step:429/2315 train_time:25830ms step_avg:60.21ms
step:430/2315 train_time:25890ms step_avg:60.21ms
step:431/2315 train_time:25949ms step_avg:60.21ms
step:432/2315 train_time:26009ms step_avg:60.21ms
step:433/2315 train_time:26069ms step_avg:60.21ms
step:434/2315 train_time:26129ms step_avg:60.20ms
step:435/2315 train_time:26189ms step_avg:60.20ms
step:436/2315 train_time:26249ms step_avg:60.20ms
step:437/2315 train_time:26308ms step_avg:60.20ms
step:438/2315 train_time:26368ms step_avg:60.20ms
step:439/2315 train_time:26429ms step_avg:60.20ms
step:440/2315 train_time:26489ms step_avg:60.20ms
step:441/2315 train_time:26549ms step_avg:60.20ms
step:442/2315 train_time:26609ms step_avg:60.20ms
step:443/2315 train_time:26669ms step_avg:60.20ms
step:444/2315 train_time:26729ms step_avg:60.20ms
step:445/2315 train_time:26790ms step_avg:60.20ms
step:446/2315 train_time:26849ms step_avg:60.20ms
step:447/2315 train_time:26909ms step_avg:60.20ms
step:448/2315 train_time:26969ms step_avg:60.20ms
step:449/2315 train_time:27029ms step_avg:60.20ms
step:450/2315 train_time:27089ms step_avg:60.20ms
step:451/2315 train_time:27149ms step_avg:60.20ms
step:452/2315 train_time:27210ms step_avg:60.20ms
step:453/2315 train_time:27270ms step_avg:60.20ms
step:454/2315 train_time:27330ms step_avg:60.20ms
step:455/2315 train_time:27390ms step_avg:60.20ms
step:456/2315 train_time:27450ms step_avg:60.20ms
step:457/2315 train_time:27511ms step_avg:60.20ms
step:458/2315 train_time:27570ms step_avg:60.20ms
step:459/2315 train_time:27630ms step_avg:60.20ms
step:460/2315 train_time:27689ms step_avg:60.19ms
step:461/2315 train_time:27749ms step_avg:60.19ms
step:462/2315 train_time:27809ms step_avg:60.19ms
step:463/2315 train_time:27869ms step_avg:60.19ms
step:464/2315 train_time:27929ms step_avg:60.19ms
step:465/2315 train_time:27990ms step_avg:60.19ms
step:466/2315 train_time:28049ms step_avg:60.19ms
step:467/2315 train_time:28109ms step_avg:60.19ms
step:468/2315 train_time:28168ms step_avg:60.19ms
step:469/2315 train_time:28229ms step_avg:60.19ms
step:470/2315 train_time:28289ms step_avg:60.19ms
step:471/2315 train_time:28349ms step_avg:60.19ms
step:472/2315 train_time:28409ms step_avg:60.19ms
step:473/2315 train_time:28470ms step_avg:60.19ms
step:474/2315 train_time:28530ms step_avg:60.19ms
step:475/2315 train_time:28589ms step_avg:60.19ms
step:476/2315 train_time:28650ms step_avg:60.19ms
step:477/2315 train_time:28710ms step_avg:60.19ms
step:478/2315 train_time:28770ms step_avg:60.19ms
step:479/2315 train_time:28830ms step_avg:60.19ms
step:480/2315 train_time:28890ms step_avg:60.19ms
step:481/2315 train_time:28950ms step_avg:60.19ms
step:482/2315 train_time:29010ms step_avg:60.19ms
step:483/2315 train_time:29070ms step_avg:60.19ms
step:484/2315 train_time:29130ms step_avg:60.18ms
step:485/2315 train_time:29190ms step_avg:60.18ms
step:486/2315 train_time:29250ms step_avg:60.18ms
step:487/2315 train_time:29310ms step_avg:60.18ms
step:488/2315 train_time:29370ms step_avg:60.18ms
step:489/2315 train_time:29430ms step_avg:60.18ms
step:490/2315 train_time:29490ms step_avg:60.18ms
step:491/2315 train_time:29551ms step_avg:60.19ms
step:492/2315 train_time:29612ms step_avg:60.19ms
step:493/2315 train_time:29672ms step_avg:60.19ms
step:494/2315 train_time:29731ms step_avg:60.18ms
step:495/2315 train_time:29791ms step_avg:60.18ms
step:496/2315 train_time:29851ms step_avg:60.18ms
step:497/2315 train_time:29912ms step_avg:60.18ms
step:498/2315 train_time:29972ms step_avg:60.18ms
step:499/2315 train_time:30032ms step_avg:60.18ms
step:500/2315 train_time:30091ms step_avg:60.18ms
step:500/2315 val_loss:3.8103 train_time:30154ms step_avg:60.31ms
step:501/2315 train_time:30172ms step_avg:60.22ms
step:502/2315 train_time:30213ms step_avg:60.18ms
step:503/2315 train_time:30277ms step_avg:60.19ms
step:504/2315 train_time:30342ms step_avg:60.20ms
step:505/2315 train_time:30403ms step_avg:60.20ms
step:506/2315 train_time:30464ms step_avg:60.20ms
step:507/2315 train_time:30523ms step_avg:60.20ms
step:508/2315 train_time:30582ms step_avg:60.20ms
step:509/2315 train_time:30642ms step_avg:60.20ms
step:510/2315 train_time:30702ms step_avg:60.20ms
step:511/2315 train_time:30761ms step_avg:60.20ms
step:512/2315 train_time:30821ms step_avg:60.20ms
step:513/2315 train_time:30880ms step_avg:60.19ms
step:514/2315 train_time:30939ms step_avg:60.19ms
step:515/2315 train_time:30998ms step_avg:60.19ms
step:516/2315 train_time:31057ms step_avg:60.19ms
step:517/2315 train_time:31118ms step_avg:60.19ms
step:518/2315 train_time:31178ms step_avg:60.19ms
step:519/2315 train_time:31241ms step_avg:60.19ms
step:520/2315 train_time:31302ms step_avg:60.20ms
step:521/2315 train_time:31363ms step_avg:60.20ms
step:522/2315 train_time:31423ms step_avg:60.20ms
step:523/2315 train_time:31483ms step_avg:60.20ms
step:524/2315 train_time:31543ms step_avg:60.20ms
step:525/2315 train_time:31603ms step_avg:60.20ms
step:526/2315 train_time:31663ms step_avg:60.20ms
step:527/2315 train_time:31723ms step_avg:60.19ms
step:528/2315 train_time:31782ms step_avg:60.19ms
step:529/2315 train_time:31842ms step_avg:60.19ms
step:530/2315 train_time:31902ms step_avg:60.19ms
step:531/2315 train_time:31963ms step_avg:60.19ms
step:532/2315 train_time:32022ms step_avg:60.19ms
step:533/2315 train_time:32082ms step_avg:60.19ms
step:534/2315 train_time:32144ms step_avg:60.19ms
step:535/2315 train_time:32205ms step_avg:60.20ms
step:536/2315 train_time:32265ms step_avg:60.20ms
step:537/2315 train_time:32326ms step_avg:60.20ms
step:538/2315 train_time:32386ms step_avg:60.20ms
step:539/2315 train_time:32447ms step_avg:60.20ms
step:540/2315 train_time:32508ms step_avg:60.20ms
step:541/2315 train_time:32567ms step_avg:60.20ms
step:542/2315 train_time:32626ms step_avg:60.20ms
step:543/2315 train_time:32686ms step_avg:60.20ms
step:544/2315 train_time:32746ms step_avg:60.19ms
step:545/2315 train_time:32805ms step_avg:60.19ms
step:546/2315 train_time:32865ms step_avg:60.19ms
step:547/2315 train_time:32925ms step_avg:60.19ms
step:548/2315 train_time:32985ms step_avg:60.19ms
step:549/2315 train_time:33045ms step_avg:60.19ms
step:550/2315 train_time:33106ms step_avg:60.19ms
step:551/2315 train_time:33166ms step_avg:60.19ms
step:552/2315 train_time:33226ms step_avg:60.19ms
step:553/2315 train_time:33286ms step_avg:60.19ms
step:554/2315 train_time:33346ms step_avg:60.19ms
step:555/2315 train_time:33407ms step_avg:60.19ms
step:556/2315 train_time:33467ms step_avg:60.19ms
step:557/2315 train_time:33527ms step_avg:60.19ms
step:558/2315 train_time:33587ms step_avg:60.19ms
step:559/2315 train_time:33647ms step_avg:60.19ms
step:560/2315 train_time:33707ms step_avg:60.19ms
step:561/2315 train_time:33766ms step_avg:60.19ms
step:562/2315 train_time:33826ms step_avg:60.19ms
step:563/2315 train_time:33886ms step_avg:60.19ms
step:564/2315 train_time:33946ms step_avg:60.19ms
step:565/2315 train_time:34007ms step_avg:60.19ms
step:566/2315 train_time:34067ms step_avg:60.19ms
step:567/2315 train_time:34128ms step_avg:60.19ms
step:568/2315 train_time:34189ms step_avg:60.19ms
step:569/2315 train_time:34249ms step_avg:60.19ms
step:570/2315 train_time:34309ms step_avg:60.19ms
step:571/2315 train_time:34369ms step_avg:60.19ms
step:572/2315 train_time:34430ms step_avg:60.19ms
step:573/2315 train_time:34489ms step_avg:60.19ms
step:574/2315 train_time:34549ms step_avg:60.19ms
step:575/2315 train_time:34610ms step_avg:60.19ms
step:576/2315 train_time:34669ms step_avg:60.19ms
step:577/2315 train_time:34729ms step_avg:60.19ms
step:578/2315 train_time:34788ms step_avg:60.19ms
step:579/2315 train_time:34849ms step_avg:60.19ms
step:580/2315 train_time:34909ms step_avg:60.19ms
step:581/2315 train_time:34969ms step_avg:60.19ms
step:582/2315 train_time:35029ms step_avg:60.19ms
step:583/2315 train_time:35090ms step_avg:60.19ms
step:584/2315 train_time:35151ms step_avg:60.19ms
step:585/2315 train_time:35210ms step_avg:60.19ms
step:586/2315 train_time:35270ms step_avg:60.19ms
step:587/2315 train_time:35330ms step_avg:60.19ms
step:588/2315 train_time:35390ms step_avg:60.19ms
step:589/2315 train_time:35450ms step_avg:60.19ms
step:590/2315 train_time:35510ms step_avg:60.19ms
step:591/2315 train_time:35571ms step_avg:60.19ms
step:592/2315 train_time:35630ms step_avg:60.19ms
step:593/2315 train_time:35690ms step_avg:60.19ms
step:594/2315 train_time:35750ms step_avg:60.18ms
step:595/2315 train_time:35810ms step_avg:60.19ms
step:596/2315 train_time:35870ms step_avg:60.18ms
step:597/2315 train_time:35930ms step_avg:60.18ms
step:598/2315 train_time:35991ms step_avg:60.18ms
step:599/2315 train_time:36051ms step_avg:60.19ms
step:600/2315 train_time:36112ms step_avg:60.19ms
step:601/2315 train_time:36172ms step_avg:60.19ms
step:602/2315 train_time:36232ms step_avg:60.19ms
step:603/2315 train_time:36292ms step_avg:60.19ms
step:604/2315 train_time:36353ms step_avg:60.19ms
step:605/2315 train_time:36413ms step_avg:60.19ms
step:606/2315 train_time:36473ms step_avg:60.19ms
step:607/2315 train_time:36533ms step_avg:60.19ms
step:608/2315 train_time:36594ms step_avg:60.19ms
step:609/2315 train_time:36654ms step_avg:60.19ms
step:610/2315 train_time:36714ms step_avg:60.19ms
step:611/2315 train_time:36773ms step_avg:60.19ms
step:612/2315 train_time:36833ms step_avg:60.19ms
step:613/2315 train_time:36893ms step_avg:60.18ms
step:614/2315 train_time:36954ms step_avg:60.19ms
step:615/2315 train_time:37014ms step_avg:60.19ms
step:616/2315 train_time:37074ms step_avg:60.19ms
step:617/2315 train_time:37134ms step_avg:60.18ms
step:618/2315 train_time:37194ms step_avg:60.19ms
step:619/2315 train_time:37255ms step_avg:60.19ms
step:620/2315 train_time:37314ms step_avg:60.18ms
step:621/2315 train_time:37374ms step_avg:60.18ms
step:622/2315 train_time:37434ms step_avg:60.18ms
step:623/2315 train_time:37494ms step_avg:60.18ms
step:624/2315 train_time:37554ms step_avg:60.18ms
step:625/2315 train_time:37613ms step_avg:60.18ms
step:626/2315 train_time:37673ms step_avg:60.18ms
step:627/2315 train_time:37733ms step_avg:60.18ms
step:628/2315 train_time:37794ms step_avg:60.18ms
step:629/2315 train_time:37854ms step_avg:60.18ms
step:630/2315 train_time:37914ms step_avg:60.18ms
step:631/2315 train_time:37975ms step_avg:60.18ms
step:632/2315 train_time:38035ms step_avg:60.18ms
step:633/2315 train_time:38095ms step_avg:60.18ms
step:634/2315 train_time:38155ms step_avg:60.18ms
step:635/2315 train_time:38215ms step_avg:60.18ms
step:636/2315 train_time:38275ms step_avg:60.18ms
step:637/2315 train_time:38334ms step_avg:60.18ms
step:638/2315 train_time:38394ms step_avg:60.18ms
step:639/2315 train_time:38454ms step_avg:60.18ms
step:640/2315 train_time:38514ms step_avg:60.18ms
step:641/2315 train_time:38574ms step_avg:60.18ms
step:642/2315 train_time:38634ms step_avg:60.18ms
step:643/2315 train_time:38693ms step_avg:60.18ms
step:644/2315 train_time:38753ms step_avg:60.18ms
step:645/2315 train_time:38813ms step_avg:60.18ms
step:646/2315 train_time:38873ms step_avg:60.18ms
step:647/2315 train_time:38933ms step_avg:60.18ms
step:648/2315 train_time:38993ms step_avg:60.17ms
step:649/2315 train_time:39054ms step_avg:60.18ms
step:650/2315 train_time:39114ms step_avg:60.17ms
step:651/2315 train_time:39174ms step_avg:60.18ms
step:652/2315 train_time:39234ms step_avg:60.18ms
step:653/2315 train_time:39294ms step_avg:60.17ms
step:654/2315 train_time:39354ms step_avg:60.17ms
step:655/2315 train_time:39414ms step_avg:60.17ms
step:656/2315 train_time:39474ms step_avg:60.17ms
step:657/2315 train_time:39534ms step_avg:60.17ms
step:658/2315 train_time:39594ms step_avg:60.17ms
step:659/2315 train_time:39654ms step_avg:60.17ms
step:660/2315 train_time:39713ms step_avg:60.17ms
step:661/2315 train_time:39773ms step_avg:60.17ms
step:662/2315 train_time:39833ms step_avg:60.17ms
step:663/2315 train_time:39893ms step_avg:60.17ms
step:664/2315 train_time:39954ms step_avg:60.17ms
step:665/2315 train_time:40014ms step_avg:60.17ms
step:666/2315 train_time:40074ms step_avg:60.17ms
step:667/2315 train_time:40135ms step_avg:60.17ms
step:668/2315 train_time:40194ms step_avg:60.17ms
step:669/2315 train_time:40255ms step_avg:60.17ms
step:670/2315 train_time:40315ms step_avg:60.17ms
step:671/2315 train_time:40375ms step_avg:60.17ms
step:672/2315 train_time:40435ms step_avg:60.17ms
step:673/2315 train_time:40495ms step_avg:60.17ms
step:674/2315 train_time:40554ms step_avg:60.17ms
step:675/2315 train_time:40614ms step_avg:60.17ms
step:676/2315 train_time:40673ms step_avg:60.17ms
step:677/2315 train_time:40734ms step_avg:60.17ms
step:678/2315 train_time:40794ms step_avg:60.17ms
step:679/2315 train_time:40854ms step_avg:60.17ms
step:680/2315 train_time:40914ms step_avg:60.17ms
step:681/2315 train_time:40975ms step_avg:60.17ms
step:682/2315 train_time:41035ms step_avg:60.17ms
step:683/2315 train_time:41095ms step_avg:60.17ms
step:684/2315 train_time:41155ms step_avg:60.17ms
step:685/2315 train_time:41215ms step_avg:60.17ms
step:686/2315 train_time:41275ms step_avg:60.17ms
step:687/2315 train_time:41335ms step_avg:60.17ms
step:688/2315 train_time:41395ms step_avg:60.17ms
step:689/2315 train_time:41455ms step_avg:60.17ms
step:690/2315 train_time:41515ms step_avg:60.17ms
step:691/2315 train_time:41575ms step_avg:60.17ms
step:692/2315 train_time:41635ms step_avg:60.17ms
step:693/2315 train_time:41695ms step_avg:60.17ms
step:694/2315 train_time:41754ms step_avg:60.16ms
step:695/2315 train_time:41814ms step_avg:60.16ms
step:696/2315 train_time:41874ms step_avg:60.16ms
step:697/2315 train_time:41935ms step_avg:60.17ms
step:698/2315 train_time:41995ms step_avg:60.16ms
step:699/2315 train_time:42055ms step_avg:60.16ms
step:700/2315 train_time:42114ms step_avg:60.16ms
step:701/2315 train_time:42174ms step_avg:60.16ms
step:702/2315 train_time:42234ms step_avg:60.16ms
step:703/2315 train_time:42294ms step_avg:60.16ms
step:704/2315 train_time:42355ms step_avg:60.16ms
step:705/2315 train_time:42415ms step_avg:60.16ms
step:706/2315 train_time:42475ms step_avg:60.16ms
step:707/2315 train_time:42534ms step_avg:60.16ms
step:708/2315 train_time:42594ms step_avg:60.16ms
step:709/2315 train_time:42655ms step_avg:60.16ms
step:710/2315 train_time:42715ms step_avg:60.16ms
step:711/2315 train_time:42775ms step_avg:60.16ms
step:712/2315 train_time:42835ms step_avg:60.16ms
step:713/2315 train_time:42894ms step_avg:60.16ms
step:714/2315 train_time:42954ms step_avg:60.16ms
step:715/2315 train_time:43015ms step_avg:60.16ms
step:716/2315 train_time:43075ms step_avg:60.16ms
step:717/2315 train_time:43135ms step_avg:60.16ms
step:718/2315 train_time:43194ms step_avg:60.16ms
step:719/2315 train_time:43255ms step_avg:60.16ms
step:720/2315 train_time:43314ms step_avg:60.16ms
step:721/2315 train_time:43375ms step_avg:60.16ms
step:722/2315 train_time:43435ms step_avg:60.16ms
step:723/2315 train_time:43494ms step_avg:60.16ms
step:724/2315 train_time:43555ms step_avg:60.16ms
step:725/2315 train_time:43615ms step_avg:60.16ms
step:726/2315 train_time:43675ms step_avg:60.16ms
step:727/2315 train_time:43734ms step_avg:60.16ms
step:728/2315 train_time:43794ms step_avg:60.16ms
step:729/2315 train_time:43854ms step_avg:60.16ms
step:730/2315 train_time:43914ms step_avg:60.16ms
step:731/2315 train_time:43974ms step_avg:60.16ms
step:732/2315 train_time:44034ms step_avg:60.16ms
step:733/2315 train_time:44093ms step_avg:60.15ms
step:734/2315 train_time:44154ms step_avg:60.15ms
step:735/2315 train_time:44214ms step_avg:60.16ms
step:736/2315 train_time:44274ms step_avg:60.16ms
step:737/2315 train_time:44334ms step_avg:60.15ms
step:738/2315 train_time:44395ms step_avg:60.16ms
step:739/2315 train_time:44455ms step_avg:60.16ms
step:740/2315 train_time:44515ms step_avg:60.16ms
step:741/2315 train_time:44575ms step_avg:60.15ms
step:742/2315 train_time:44634ms step_avg:60.15ms
step:743/2315 train_time:44694ms step_avg:60.15ms
step:744/2315 train_time:44754ms step_avg:60.15ms
step:745/2315 train_time:44815ms step_avg:60.15ms
step:746/2315 train_time:44875ms step_avg:60.15ms
step:747/2315 train_time:44934ms step_avg:60.15ms
step:748/2315 train_time:44995ms step_avg:60.15ms
step:749/2315 train_time:45055ms step_avg:60.15ms
step:750/2315 train_time:45115ms step_avg:60.15ms
step:750/2315 val_loss:3.6842 train_time:45176ms step_avg:60.24ms
step:751/2315 train_time:45195ms step_avg:60.18ms
step:752/2315 train_time:45236ms step_avg:60.15ms
step:753/2315 train_time:45297ms step_avg:60.16ms
step:754/2315 train_time:45361ms step_avg:60.16ms
step:755/2315 train_time:45424ms step_avg:60.16ms
step:756/2315 train_time:45483ms step_avg:60.16ms
step:757/2315 train_time:45543ms step_avg:60.16ms
step:758/2315 train_time:45603ms step_avg:60.16ms
step:759/2315 train_time:45662ms step_avg:60.16ms
step:760/2315 train_time:45723ms step_avg:60.16ms
step:761/2315 train_time:45782ms step_avg:60.16ms
step:762/2315 train_time:45841ms step_avg:60.16ms
step:763/2315 train_time:45902ms step_avg:60.16ms
step:764/2315 train_time:45962ms step_avg:60.16ms
step:765/2315 train_time:46023ms step_avg:60.16ms
step:766/2315 train_time:46083ms step_avg:60.16ms
step:767/2315 train_time:46144ms step_avg:60.16ms
step:768/2315 train_time:46206ms step_avg:60.16ms
step:769/2315 train_time:46269ms step_avg:60.17ms
step:770/2315 train_time:46331ms step_avg:60.17ms
step:771/2315 train_time:46393ms step_avg:60.17ms
step:772/2315 train_time:46454ms step_avg:60.17ms
step:773/2315 train_time:46514ms step_avg:60.17ms
step:774/2315 train_time:46575ms step_avg:60.17ms
step:775/2315 train_time:46635ms step_avg:60.17ms
step:776/2315 train_time:46696ms step_avg:60.18ms
step:777/2315 train_time:46757ms step_avg:60.18ms
step:778/2315 train_time:46818ms step_avg:60.18ms
step:779/2315 train_time:46878ms step_avg:60.18ms
step:780/2315 train_time:46939ms step_avg:60.18ms
step:781/2315 train_time:46999ms step_avg:60.18ms
step:782/2315 train_time:47059ms step_avg:60.18ms
step:783/2315 train_time:47120ms step_avg:60.18ms
step:784/2315 train_time:47180ms step_avg:60.18ms
step:785/2315 train_time:47242ms step_avg:60.18ms
step:786/2315 train_time:47303ms step_avg:60.18ms
step:787/2315 train_time:47364ms step_avg:60.18ms
step:788/2315 train_time:47426ms step_avg:60.19ms
step:789/2315 train_time:47487ms step_avg:60.19ms
step:790/2315 train_time:47548ms step_avg:60.19ms
step:791/2315 train_time:47609ms step_avg:60.19ms
step:792/2315 train_time:47670ms step_avg:60.19ms
step:793/2315 train_time:47731ms step_avg:60.19ms
step:794/2315 train_time:47791ms step_avg:60.19ms
step:795/2315 train_time:47852ms step_avg:60.19ms
step:796/2315 train_time:47913ms step_avg:60.19ms
step:797/2315 train_time:47973ms step_avg:60.19ms
step:798/2315 train_time:48034ms step_avg:60.19ms
step:799/2315 train_time:48095ms step_avg:60.19ms
step:800/2315 train_time:48156ms step_avg:60.19ms
step:801/2315 train_time:48217ms step_avg:60.20ms
step:802/2315 train_time:48278ms step_avg:60.20ms
step:803/2315 train_time:48339ms step_avg:60.20ms
step:804/2315 train_time:48401ms step_avg:60.20ms
step:805/2315 train_time:48461ms step_avg:60.20ms
step:806/2315 train_time:48522ms step_avg:60.20ms
step:807/2315 train_time:48583ms step_avg:60.20ms
step:808/2315 train_time:48644ms step_avg:60.20ms
step:809/2315 train_time:48704ms step_avg:60.20ms
step:810/2315 train_time:48765ms step_avg:60.20ms
step:811/2315 train_time:48826ms step_avg:60.21ms
step:812/2315 train_time:48887ms step_avg:60.21ms
step:813/2315 train_time:48948ms step_avg:60.21ms
step:814/2315 train_time:49009ms step_avg:60.21ms
step:815/2315 train_time:49070ms step_avg:60.21ms
step:816/2315 train_time:49131ms step_avg:60.21ms
step:817/2315 train_time:49192ms step_avg:60.21ms
step:818/2315 train_time:49253ms step_avg:60.21ms
step:819/2315 train_time:49314ms step_avg:60.21ms
step:820/2315 train_time:49375ms step_avg:60.21ms
step:821/2315 train_time:49436ms step_avg:60.21ms
step:822/2315 train_time:49497ms step_avg:60.22ms
step:823/2315 train_time:49558ms step_avg:60.22ms
step:824/2315 train_time:49619ms step_avg:60.22ms
step:825/2315 train_time:49679ms step_avg:60.22ms
step:826/2315 train_time:49740ms step_avg:60.22ms
step:827/2315 train_time:49801ms step_avg:60.22ms
step:828/2315 train_time:49862ms step_avg:60.22ms
step:829/2315 train_time:49923ms step_avg:60.22ms
step:830/2315 train_time:49984ms step_avg:60.22ms
step:831/2315 train_time:50044ms step_avg:60.22ms
step:832/2315 train_time:50105ms step_avg:60.22ms
step:833/2315 train_time:50166ms step_avg:60.22ms
step:834/2315 train_time:50227ms step_avg:60.22ms
step:835/2315 train_time:50288ms step_avg:60.23ms
step:836/2315 train_time:50349ms step_avg:60.23ms
step:837/2315 train_time:50410ms step_avg:60.23ms
step:838/2315 train_time:50471ms step_avg:60.23ms
step:839/2315 train_time:50532ms step_avg:60.23ms
step:840/2315 train_time:50593ms step_avg:60.23ms
step:841/2315 train_time:50654ms step_avg:60.23ms
step:842/2315 train_time:50715ms step_avg:60.23ms
step:843/2315 train_time:50776ms step_avg:60.23ms
step:844/2315 train_time:50837ms step_avg:60.23ms
step:845/2315 train_time:50899ms step_avg:60.23ms
step:846/2315 train_time:50959ms step_avg:60.24ms
step:847/2315 train_time:51020ms step_avg:60.24ms
step:848/2315 train_time:51080ms step_avg:60.24ms
step:849/2315 train_time:51141ms step_avg:60.24ms
step:850/2315 train_time:51202ms step_avg:60.24ms
step:851/2315 train_time:51263ms step_avg:60.24ms
step:852/2315 train_time:51324ms step_avg:60.24ms
step:853/2315 train_time:51385ms step_avg:60.24ms
step:854/2315 train_time:51447ms step_avg:60.24ms
step:855/2315 train_time:51507ms step_avg:60.24ms
step:856/2315 train_time:51568ms step_avg:60.24ms
step:857/2315 train_time:51629ms step_avg:60.24ms
step:858/2315 train_time:51690ms step_avg:60.24ms
step:859/2315 train_time:51751ms step_avg:60.25ms
step:860/2315 train_time:51811ms step_avg:60.25ms
step:861/2315 train_time:51872ms step_avg:60.25ms
step:862/2315 train_time:51933ms step_avg:60.25ms
step:863/2315 train_time:51994ms step_avg:60.25ms
step:864/2315 train_time:52055ms step_avg:60.25ms
step:865/2315 train_time:52116ms step_avg:60.25ms
step:866/2315 train_time:52176ms step_avg:60.25ms
step:867/2315 train_time:52238ms step_avg:60.25ms
step:868/2315 train_time:52299ms step_avg:60.25ms
step:869/2315 train_time:52360ms step_avg:60.25ms
step:870/2315 train_time:52420ms step_avg:60.25ms
step:871/2315 train_time:52481ms step_avg:60.25ms
step:872/2315 train_time:52542ms step_avg:60.25ms
step:873/2315 train_time:52603ms step_avg:60.26ms
step:874/2315 train_time:52664ms step_avg:60.26ms
step:875/2315 train_time:52725ms step_avg:60.26ms
step:876/2315 train_time:52786ms step_avg:60.26ms
step:877/2315 train_time:52847ms step_avg:60.26ms
step:878/2315 train_time:52907ms step_avg:60.26ms
step:879/2315 train_time:52968ms step_avg:60.26ms
step:880/2315 train_time:53029ms step_avg:60.26ms
step:881/2315 train_time:53090ms step_avg:60.26ms
step:882/2315 train_time:53152ms step_avg:60.26ms
step:883/2315 train_time:53213ms step_avg:60.26ms
step:884/2315 train_time:53274ms step_avg:60.26ms
step:885/2315 train_time:53336ms step_avg:60.27ms
step:886/2315 train_time:53397ms step_avg:60.27ms
step:887/2315 train_time:53458ms step_avg:60.27ms
step:888/2315 train_time:53519ms step_avg:60.27ms
step:889/2315 train_time:53579ms step_avg:60.27ms
step:890/2315 train_time:53640ms step_avg:60.27ms
step:891/2315 train_time:53701ms step_avg:60.27ms
step:892/2315 train_time:53761ms step_avg:60.27ms
step:893/2315 train_time:53822ms step_avg:60.27ms
step:894/2315 train_time:53884ms step_avg:60.27ms
step:895/2315 train_time:53945ms step_avg:60.27ms
step:896/2315 train_time:54006ms step_avg:60.27ms
step:897/2315 train_time:54067ms step_avg:60.28ms
step:898/2315 train_time:54128ms step_avg:60.28ms
step:899/2315 train_time:54189ms step_avg:60.28ms
step:900/2315 train_time:54249ms step_avg:60.28ms
step:901/2315 train_time:54310ms step_avg:60.28ms
step:902/2315 train_time:54371ms step_avg:60.28ms
step:903/2315 train_time:54433ms step_avg:60.28ms
step:904/2315 train_time:54493ms step_avg:60.28ms
step:905/2315 train_time:54554ms step_avg:60.28ms
step:906/2315 train_time:54615ms step_avg:60.28ms
step:907/2315 train_time:54677ms step_avg:60.28ms
step:908/2315 train_time:54738ms step_avg:60.28ms
step:909/2315 train_time:54799ms step_avg:60.29ms
step:910/2315 train_time:54860ms step_avg:60.29ms
step:911/2315 train_time:54921ms step_avg:60.29ms
step:912/2315 train_time:54982ms step_avg:60.29ms
step:913/2315 train_time:55042ms step_avg:60.29ms
step:914/2315 train_time:55103ms step_avg:60.29ms
step:915/2315 train_time:55164ms step_avg:60.29ms
step:916/2315 train_time:55225ms step_avg:60.29ms
step:917/2315 train_time:55286ms step_avg:60.29ms
step:918/2315 train_time:55347ms step_avg:60.29ms
step:919/2315 train_time:55408ms step_avg:60.29ms
step:920/2315 train_time:55468ms step_avg:60.29ms
step:921/2315 train_time:55529ms step_avg:60.29ms
step:922/2315 train_time:55590ms step_avg:60.29ms
step:923/2315 train_time:55652ms step_avg:60.29ms
step:924/2315 train_time:55712ms step_avg:60.29ms
step:925/2315 train_time:55773ms step_avg:60.30ms
step:926/2315 train_time:55834ms step_avg:60.30ms
step:927/2315 train_time:55895ms step_avg:60.30ms
step:928/2315 train_time:55956ms step_avg:60.30ms
step:929/2315 train_time:56017ms step_avg:60.30ms
step:930/2315 train_time:56078ms step_avg:60.30ms
step:931/2315 train_time:56139ms step_avg:60.30ms
step:932/2315 train_time:56200ms step_avg:60.30ms
step:933/2315 train_time:56260ms step_avg:60.30ms
step:934/2315 train_time:56320ms step_avg:60.30ms
step:935/2315 train_time:56381ms step_avg:60.30ms
step:936/2315 train_time:56442ms step_avg:60.30ms
step:937/2315 train_time:56502ms step_avg:60.30ms
step:938/2315 train_time:56563ms step_avg:60.30ms
step:939/2315 train_time:56625ms step_avg:60.30ms
step:940/2315 train_time:56685ms step_avg:60.30ms
step:941/2315 train_time:56746ms step_avg:60.30ms
step:942/2315 train_time:56808ms step_avg:60.31ms
step:943/2315 train_time:56869ms step_avg:60.31ms
step:944/2315 train_time:56930ms step_avg:60.31ms
step:945/2315 train_time:56991ms step_avg:60.31ms
step:946/2315 train_time:57052ms step_avg:60.31ms
step:947/2315 train_time:57113ms step_avg:60.31ms
step:948/2315 train_time:57174ms step_avg:60.31ms
step:949/2315 train_time:57235ms step_avg:60.31ms
step:950/2315 train_time:57297ms step_avg:60.31ms
step:951/2315 train_time:57358ms step_avg:60.31ms
step:952/2315 train_time:57418ms step_avg:60.31ms
step:953/2315 train_time:57479ms step_avg:60.31ms
step:954/2315 train_time:57540ms step_avg:60.31ms
step:955/2315 train_time:57600ms step_avg:60.31ms
step:956/2315 train_time:57661ms step_avg:60.32ms
step:957/2315 train_time:57722ms step_avg:60.32ms
step:958/2315 train_time:57783ms step_avg:60.32ms
step:959/2315 train_time:57844ms step_avg:60.32ms
step:960/2315 train_time:57905ms step_avg:60.32ms
step:961/2315 train_time:57966ms step_avg:60.32ms
step:962/2315 train_time:58027ms step_avg:60.32ms
step:963/2315 train_time:58088ms step_avg:60.32ms
step:964/2315 train_time:58149ms step_avg:60.32ms
step:965/2315 train_time:58210ms step_avg:60.32ms
step:966/2315 train_time:58271ms step_avg:60.32ms
step:967/2315 train_time:58332ms step_avg:60.32ms
step:968/2315 train_time:58392ms step_avg:60.32ms
step:969/2315 train_time:58454ms step_avg:60.32ms
step:970/2315 train_time:58515ms step_avg:60.32ms
step:971/2315 train_time:58576ms step_avg:60.33ms
step:972/2315 train_time:58637ms step_avg:60.33ms
step:973/2315 train_time:58699ms step_avg:60.33ms
step:974/2315 train_time:58759ms step_avg:60.33ms
step:975/2315 train_time:58819ms step_avg:60.33ms
step:976/2315 train_time:58880ms step_avg:60.33ms
step:977/2315 train_time:58942ms step_avg:60.33ms
step:978/2315 train_time:59003ms step_avg:60.33ms
step:979/2315 train_time:59063ms step_avg:60.33ms
step:980/2315 train_time:59125ms step_avg:60.33ms
step:981/2315 train_time:59186ms step_avg:60.33ms
step:982/2315 train_time:59247ms step_avg:60.33ms
step:983/2315 train_time:59308ms step_avg:60.33ms
step:984/2315 train_time:59369ms step_avg:60.33ms
step:985/2315 train_time:59430ms step_avg:60.34ms
step:986/2315 train_time:59491ms step_avg:60.34ms
step:987/2315 train_time:59552ms step_avg:60.34ms
step:988/2315 train_time:59613ms step_avg:60.34ms
step:989/2315 train_time:59674ms step_avg:60.34ms
step:990/2315 train_time:59736ms step_avg:60.34ms
step:991/2315 train_time:59797ms step_avg:60.34ms
step:992/2315 train_time:59857ms step_avg:60.34ms
step:993/2315 train_time:59919ms step_avg:60.34ms
step:994/2315 train_time:59979ms step_avg:60.34ms
step:995/2315 train_time:60040ms step_avg:60.34ms
step:996/2315 train_time:60100ms step_avg:60.34ms
step:997/2315 train_time:60161ms step_avg:60.34ms
step:998/2315 train_time:60222ms step_avg:60.34ms
step:999/2315 train_time:60283ms step_avg:60.34ms
step:1000/2315 train_time:60344ms step_avg:60.34ms
step:1000/2315 val_loss:3.5765 train_time:60407ms step_avg:60.41ms
step:1001/2315 train_time:60427ms step_avg:60.37ms
step:1002/2315 train_time:60468ms step_avg:60.35ms
step:1003/2315 train_time:60534ms step_avg:60.35ms
step:1004/2315 train_time:60601ms step_avg:60.36ms
step:1005/2315 train_time:60664ms step_avg:60.36ms
step:1006/2315 train_time:60724ms step_avg:60.36ms
step:1007/2315 train_time:60785ms step_avg:60.36ms
step:1008/2315 train_time:60845ms step_avg:60.36ms
step:1009/2315 train_time:60905ms step_avg:60.36ms
step:1010/2315 train_time:60965ms step_avg:60.36ms
step:1011/2315 train_time:61026ms step_avg:60.36ms
step:1012/2315 train_time:61086ms step_avg:60.36ms
step:1013/2315 train_time:61146ms step_avg:60.36ms
step:1014/2315 train_time:61206ms step_avg:60.36ms
step:1015/2315 train_time:61265ms step_avg:60.36ms
step:1016/2315 train_time:61326ms step_avg:60.36ms
step:1017/2315 train_time:61387ms step_avg:60.36ms
step:1018/2315 train_time:61449ms step_avg:60.36ms
step:1019/2315 train_time:61512ms step_avg:60.37ms
step:1020/2315 train_time:61575ms step_avg:60.37ms
step:1021/2315 train_time:61636ms step_avg:60.37ms
step:1022/2315 train_time:61697ms step_avg:60.37ms
step:1023/2315 train_time:61758ms step_avg:60.37ms
step:1024/2315 train_time:61819ms step_avg:60.37ms
step:1025/2315 train_time:61880ms step_avg:60.37ms
step:1026/2315 train_time:61940ms step_avg:60.37ms
step:1027/2315 train_time:62001ms step_avg:60.37ms
step:1028/2315 train_time:62061ms step_avg:60.37ms
step:1029/2315 train_time:62122ms step_avg:60.37ms
step:1030/2315 train_time:62182ms step_avg:60.37ms
step:1031/2315 train_time:62243ms step_avg:60.37ms
step:1032/2315 train_time:62303ms step_avg:60.37ms
step:1033/2315 train_time:62364ms step_avg:60.37ms
step:1034/2315 train_time:62425ms step_avg:60.37ms
step:1035/2315 train_time:62487ms step_avg:60.37ms
step:1036/2315 train_time:62548ms step_avg:60.37ms
step:1037/2315 train_time:62610ms step_avg:60.38ms
step:1038/2315 train_time:62671ms step_avg:60.38ms
step:1039/2315 train_time:62731ms step_avg:60.38ms
step:1040/2315 train_time:62792ms step_avg:60.38ms
step:1041/2315 train_time:62853ms step_avg:60.38ms
step:1042/2315 train_time:62914ms step_avg:60.38ms
step:1043/2315 train_time:62975ms step_avg:60.38ms
step:1044/2315 train_time:63036ms step_avg:60.38ms
step:1045/2315 train_time:63097ms step_avg:60.38ms
step:1046/2315 train_time:63158ms step_avg:60.38ms
step:1047/2315 train_time:63219ms step_avg:60.38ms
step:1048/2315 train_time:63279ms step_avg:60.38ms
step:1049/2315 train_time:63340ms step_avg:60.38ms
step:1050/2315 train_time:63401ms step_avg:60.38ms
step:1051/2315 train_time:63463ms step_avg:60.38ms
step:1052/2315 train_time:63524ms step_avg:60.38ms
step:1053/2315 train_time:63585ms step_avg:60.38ms
step:1054/2315 train_time:63646ms step_avg:60.39ms
step:1055/2315 train_time:63708ms step_avg:60.39ms
step:1056/2315 train_time:63768ms step_avg:60.39ms
step:1057/2315 train_time:63829ms step_avg:60.39ms
step:1058/2315 train_time:63890ms step_avg:60.39ms
step:1059/2315 train_time:63950ms step_avg:60.39ms
step:1060/2315 train_time:64011ms step_avg:60.39ms
step:1061/2315 train_time:64072ms step_avg:60.39ms
step:1062/2315 train_time:64133ms step_avg:60.39ms
step:1063/2315 train_time:64194ms step_avg:60.39ms
step:1064/2315 train_time:64256ms step_avg:60.39ms
step:1065/2315 train_time:64317ms step_avg:60.39ms
step:1066/2315 train_time:64377ms step_avg:60.39ms
step:1067/2315 train_time:64438ms step_avg:60.39ms
step:1068/2315 train_time:64499ms step_avg:60.39ms
step:1069/2315 train_time:64560ms step_avg:60.39ms
step:1070/2315 train_time:64621ms step_avg:60.39ms
step:1071/2315 train_time:64683ms step_avg:60.39ms
step:1072/2315 train_time:64744ms step_avg:60.40ms
step:1073/2315 train_time:64805ms step_avg:60.40ms
step:1074/2315 train_time:64865ms step_avg:60.40ms
step:1075/2315 train_time:64927ms step_avg:60.40ms
step:1076/2315 train_time:64988ms step_avg:60.40ms
step:1077/2315 train_time:65049ms step_avg:60.40ms
step:1078/2315 train_time:65110ms step_avg:60.40ms
step:1079/2315 train_time:65170ms step_avg:60.40ms
step:1080/2315 train_time:65231ms step_avg:60.40ms
step:1081/2315 train_time:65292ms step_avg:60.40ms
step:1082/2315 train_time:65353ms step_avg:60.40ms
step:1083/2315 train_time:65414ms step_avg:60.40ms
step:1084/2315 train_time:65476ms step_avg:60.40ms
step:1085/2315 train_time:65537ms step_avg:60.40ms
step:1086/2315 train_time:65597ms step_avg:60.40ms
step:1087/2315 train_time:65658ms step_avg:60.40ms
step:1088/2315 train_time:65719ms step_avg:60.40ms
step:1089/2315 train_time:65781ms step_avg:60.41ms
step:1090/2315 train_time:65842ms step_avg:60.41ms
step:1091/2315 train_time:65902ms step_avg:60.41ms
step:1092/2315 train_time:65963ms step_avg:60.41ms
step:1093/2315 train_time:66024ms step_avg:60.41ms
step:1094/2315 train_time:66086ms step_avg:60.41ms
step:1095/2315 train_time:66147ms step_avg:60.41ms
step:1096/2315 train_time:66208ms step_avg:60.41ms
step:1097/2315 train_time:66268ms step_avg:60.41ms
step:1098/2315 train_time:66329ms step_avg:60.41ms
step:1099/2315 train_time:66390ms step_avg:60.41ms
step:1100/2315 train_time:66450ms step_avg:60.41ms
step:1101/2315 train_time:66511ms step_avg:60.41ms
step:1102/2315 train_time:66572ms step_avg:60.41ms
step:1103/2315 train_time:66633ms step_avg:60.41ms
step:1104/2315 train_time:66694ms step_avg:60.41ms
step:1105/2315 train_time:66755ms step_avg:60.41ms
step:1106/2315 train_time:66816ms step_avg:60.41ms
step:1107/2315 train_time:66877ms step_avg:60.41ms
step:1108/2315 train_time:66938ms step_avg:60.41ms
step:1109/2315 train_time:66999ms step_avg:60.41ms
step:1110/2315 train_time:67060ms step_avg:60.41ms
step:1111/2315 train_time:67121ms step_avg:60.42ms
step:1112/2315 train_time:67182ms step_avg:60.42ms
step:1113/2315 train_time:67243ms step_avg:60.42ms
step:1114/2315 train_time:67303ms step_avg:60.42ms
step:1115/2315 train_time:67364ms step_avg:60.42ms
step:1116/2315 train_time:67425ms step_avg:60.42ms
step:1117/2315 train_time:67486ms step_avg:60.42ms
step:1118/2315 train_time:67547ms step_avg:60.42ms
step:1119/2315 train_time:67608ms step_avg:60.42ms
step:1120/2315 train_time:67669ms step_avg:60.42ms
step:1121/2315 train_time:67729ms step_avg:60.42ms
step:1122/2315 train_time:67790ms step_avg:60.42ms
step:1123/2315 train_time:67850ms step_avg:60.42ms
step:1124/2315 train_time:67911ms step_avg:60.42ms
step:1125/2315 train_time:67973ms step_avg:60.42ms
step:1126/2315 train_time:68034ms step_avg:60.42ms
step:1127/2315 train_time:68095ms step_avg:60.42ms
step:1128/2315 train_time:68156ms step_avg:60.42ms
step:1129/2315 train_time:68218ms step_avg:60.42ms
step:1130/2315 train_time:68278ms step_avg:60.42ms
step:1131/2315 train_time:68339ms step_avg:60.42ms
step:1132/2315 train_time:68400ms step_avg:60.42ms
step:1133/2315 train_time:68461ms step_avg:60.42ms
step:1134/2315 train_time:68521ms step_avg:60.42ms
step:1135/2315 train_time:68583ms step_avg:60.43ms
step:1136/2315 train_time:68644ms step_avg:60.43ms
step:1137/2315 train_time:68705ms step_avg:60.43ms
step:1138/2315 train_time:68766ms step_avg:60.43ms
step:1139/2315 train_time:68828ms step_avg:60.43ms
step:1140/2315 train_time:68889ms step_avg:60.43ms
step:1141/2315 train_time:68949ms step_avg:60.43ms
step:1142/2315 train_time:69010ms step_avg:60.43ms
step:1143/2315 train_time:69071ms step_avg:60.43ms
step:1144/2315 train_time:69132ms step_avg:60.43ms
step:1145/2315 train_time:69193ms step_avg:60.43ms
step:1146/2315 train_time:69254ms step_avg:60.43ms
step:1147/2315 train_time:69315ms step_avg:60.43ms
step:1148/2315 train_time:69376ms step_avg:60.43ms
step:1149/2315 train_time:69437ms step_avg:60.43ms
step:1150/2315 train_time:69498ms step_avg:60.43ms
step:1151/2315 train_time:69559ms step_avg:60.43ms
step:1152/2315 train_time:69620ms step_avg:60.43ms
step:1153/2315 train_time:69682ms step_avg:60.44ms
step:1154/2315 train_time:69743ms step_avg:60.44ms
step:1155/2315 train_time:69804ms step_avg:60.44ms
step:1156/2315 train_time:69864ms step_avg:60.44ms
step:1157/2315 train_time:69926ms step_avg:60.44ms
step:1158/2315 train_time:69987ms step_avg:60.44ms
step:1159/2315 train_time:70048ms step_avg:60.44ms
step:1160/2315 train_time:70109ms step_avg:60.44ms
step:1161/2315 train_time:70169ms step_avg:60.44ms
step:1162/2315 train_time:70230ms step_avg:60.44ms
step:1163/2315 train_time:70291ms step_avg:60.44ms
step:1164/2315 train_time:70352ms step_avg:60.44ms
step:1165/2315 train_time:70412ms step_avg:60.44ms
step:1166/2315 train_time:70473ms step_avg:60.44ms
step:1167/2315 train_time:70534ms step_avg:60.44ms
step:1168/2315 train_time:70595ms step_avg:60.44ms
step:1169/2315 train_time:70657ms step_avg:60.44ms
step:1170/2315 train_time:70718ms step_avg:60.44ms
step:1171/2315 train_time:70779ms step_avg:60.44ms
step:1172/2315 train_time:70840ms step_avg:60.44ms
step:1173/2315 train_time:70902ms step_avg:60.44ms
step:1174/2315 train_time:70962ms step_avg:60.44ms
step:1175/2315 train_time:71023ms step_avg:60.45ms
step:1176/2315 train_time:71084ms step_avg:60.45ms
step:1177/2315 train_time:71145ms step_avg:60.45ms
step:1178/2315 train_time:71206ms step_avg:60.45ms
step:1179/2315 train_time:71268ms step_avg:60.45ms
step:1180/2315 train_time:71328ms step_avg:60.45ms
step:1181/2315 train_time:71390ms step_avg:60.45ms
step:1182/2315 train_time:71450ms step_avg:60.45ms
step:1183/2315 train_time:71510ms step_avg:60.45ms
step:1184/2315 train_time:71571ms step_avg:60.45ms
step:1185/2315 train_time:71632ms step_avg:60.45ms
step:1186/2315 train_time:71693ms step_avg:60.45ms
step:1187/2315 train_time:71755ms step_avg:60.45ms
step:1188/2315 train_time:71816ms step_avg:60.45ms
step:1189/2315 train_time:71877ms step_avg:60.45ms
step:1190/2315 train_time:71938ms step_avg:60.45ms
step:1191/2315 train_time:71999ms step_avg:60.45ms
step:1192/2315 train_time:72060ms step_avg:60.45ms
step:1193/2315 train_time:72121ms step_avg:60.45ms
step:1194/2315 train_time:72183ms step_avg:60.45ms
step:1195/2315 train_time:72243ms step_avg:60.45ms
step:1196/2315 train_time:72304ms step_avg:60.46ms
step:1197/2315 train_time:72366ms step_avg:60.46ms
step:1198/2315 train_time:72427ms step_avg:60.46ms
step:1199/2315 train_time:72488ms step_avg:60.46ms
step:1200/2315 train_time:72549ms step_avg:60.46ms
step:1201/2315 train_time:72609ms step_avg:60.46ms
step:1202/2315 train_time:72669ms step_avg:60.46ms
step:1203/2315 train_time:72730ms step_avg:60.46ms
step:1204/2315 train_time:72791ms step_avg:60.46ms
step:1205/2315 train_time:72851ms step_avg:60.46ms
step:1206/2315 train_time:72912ms step_avg:60.46ms
step:1207/2315 train_time:72974ms step_avg:60.46ms
step:1208/2315 train_time:73035ms step_avg:60.46ms
step:1209/2315 train_time:73096ms step_avg:60.46ms
step:1210/2315 train_time:73156ms step_avg:60.46ms
step:1211/2315 train_time:73218ms step_avg:60.46ms
step:1212/2315 train_time:73278ms step_avg:60.46ms
step:1213/2315 train_time:73340ms step_avg:60.46ms
step:1214/2315 train_time:73402ms step_avg:60.46ms
step:1215/2315 train_time:73462ms step_avg:60.46ms
step:1216/2315 train_time:73523ms step_avg:60.46ms
step:1217/2315 train_time:73584ms step_avg:60.46ms
step:1218/2315 train_time:73645ms step_avg:60.46ms
step:1219/2315 train_time:73706ms step_avg:60.46ms
step:1220/2315 train_time:73767ms step_avg:60.46ms
step:1221/2315 train_time:73828ms step_avg:60.47ms
step:1222/2315 train_time:73888ms step_avg:60.46ms
step:1223/2315 train_time:73949ms step_avg:60.47ms
step:1224/2315 train_time:74010ms step_avg:60.47ms
step:1225/2315 train_time:74071ms step_avg:60.47ms
step:1226/2315 train_time:74132ms step_avg:60.47ms
step:1227/2315 train_time:74193ms step_avg:60.47ms
step:1228/2315 train_time:74254ms step_avg:60.47ms
step:1229/2315 train_time:74315ms step_avg:60.47ms
step:1230/2315 train_time:74376ms step_avg:60.47ms
step:1231/2315 train_time:74437ms step_avg:60.47ms
step:1232/2315 train_time:74498ms step_avg:60.47ms
step:1233/2315 train_time:74559ms step_avg:60.47ms
step:1234/2315 train_time:74619ms step_avg:60.47ms
step:1235/2315 train_time:74680ms step_avg:60.47ms
step:1236/2315 train_time:74741ms step_avg:60.47ms
step:1237/2315 train_time:74802ms step_avg:60.47ms
step:1238/2315 train_time:74863ms step_avg:60.47ms
step:1239/2315 train_time:74924ms step_avg:60.47ms
step:1240/2315 train_time:74985ms step_avg:60.47ms
step:1241/2315 train_time:75047ms step_avg:60.47ms
step:1242/2315 train_time:75108ms step_avg:60.47ms
step:1243/2315 train_time:75169ms step_avg:60.47ms
step:1244/2315 train_time:75229ms step_avg:60.47ms
step:1245/2315 train_time:75290ms step_avg:60.47ms
step:1246/2315 train_time:75350ms step_avg:60.47ms
step:1247/2315 train_time:75411ms step_avg:60.47ms
step:1248/2315 train_time:75471ms step_avg:60.47ms
step:1249/2315 train_time:75532ms step_avg:60.47ms
step:1250/2315 train_time:75594ms step_avg:60.48ms
step:1250/2315 val_loss:3.5148 train_time:75656ms step_avg:60.52ms
step:1251/2315 train_time:75675ms step_avg:60.49ms
step:1252/2315 train_time:75716ms step_avg:60.48ms
step:1253/2315 train_time:75782ms step_avg:60.48ms
step:1254/2315 train_time:75844ms step_avg:60.48ms
step:1255/2315 train_time:75906ms step_avg:60.48ms
step:1256/2315 train_time:75966ms step_avg:60.48ms
step:1257/2315 train_time:76027ms step_avg:60.48ms
step:1258/2315 train_time:76087ms step_avg:60.48ms
step:1259/2315 train_time:76147ms step_avg:60.48ms
step:1260/2315 train_time:76207ms step_avg:60.48ms
step:1261/2315 train_time:76268ms step_avg:60.48ms
step:1262/2315 train_time:76328ms step_avg:60.48ms
step:1263/2315 train_time:76388ms step_avg:60.48ms
step:1264/2315 train_time:76448ms step_avg:60.48ms
step:1265/2315 train_time:76508ms step_avg:60.48ms
step:1266/2315 train_time:76568ms step_avg:60.48ms
step:1267/2315 train_time:76630ms step_avg:60.48ms
step:1268/2315 train_time:76692ms step_avg:60.48ms
step:1269/2315 train_time:76755ms step_avg:60.48ms
step:1270/2315 train_time:76816ms step_avg:60.49ms
step:1271/2315 train_time:76877ms step_avg:60.49ms
step:1272/2315 train_time:76939ms step_avg:60.49ms
step:1273/2315 train_time:76998ms step_avg:60.49ms
step:1274/2315 train_time:77059ms step_avg:60.49ms
step:1275/2315 train_time:77120ms step_avg:60.49ms
step:1276/2315 train_time:77180ms step_avg:60.49ms
step:1277/2315 train_time:77242ms step_avg:60.49ms
step:1278/2315 train_time:77302ms step_avg:60.49ms
step:1279/2315 train_time:77363ms step_avg:60.49ms
step:1280/2315 train_time:77423ms step_avg:60.49ms
step:1281/2315 train_time:77484ms step_avg:60.49ms
step:1282/2315 train_time:77544ms step_avg:60.49ms
step:1283/2315 train_time:77605ms step_avg:60.49ms
step:1284/2315 train_time:77666ms step_avg:60.49ms
step:1285/2315 train_time:77728ms step_avg:60.49ms
step:1286/2315 train_time:77789ms step_avg:60.49ms
step:1287/2315 train_time:77851ms step_avg:60.49ms
step:1288/2315 train_time:77911ms step_avg:60.49ms
step:1289/2315 train_time:77972ms step_avg:60.49ms
step:1290/2315 train_time:78033ms step_avg:60.49ms
step:1291/2315 train_time:78094ms step_avg:60.49ms
step:1292/2315 train_time:78155ms step_avg:60.49ms
step:1293/2315 train_time:78216ms step_avg:60.49ms
step:1294/2315 train_time:78276ms step_avg:60.49ms
step:1295/2315 train_time:78337ms step_avg:60.49ms
step:1296/2315 train_time:78397ms step_avg:60.49ms
step:1297/2315 train_time:78458ms step_avg:60.49ms
step:1298/2315 train_time:78519ms step_avg:60.49ms
step:1299/2315 train_time:78580ms step_avg:60.49ms
step:1300/2315 train_time:78641ms step_avg:60.49ms
step:1301/2315 train_time:78703ms step_avg:60.49ms
step:1302/2315 train_time:78764ms step_avg:60.49ms
step:1303/2315 train_time:78825ms step_avg:60.49ms
step:1304/2315 train_time:78885ms step_avg:60.49ms
step:1305/2315 train_time:78946ms step_avg:60.49ms
step:1306/2315 train_time:79007ms step_avg:60.50ms
step:1307/2315 train_time:79068ms step_avg:60.50ms
step:1308/2315 train_time:79128ms step_avg:60.50ms
step:1309/2315 train_time:79189ms step_avg:60.50ms
step:1310/2315 train_time:79250ms step_avg:60.50ms
step:1311/2315 train_time:79310ms step_avg:60.50ms
step:1312/2315 train_time:79371ms step_avg:60.50ms
step:1313/2315 train_time:79432ms step_avg:60.50ms
step:1314/2315 train_time:79493ms step_avg:60.50ms
step:1315/2315 train_time:79555ms step_avg:60.50ms
step:1316/2315 train_time:79616ms step_avg:60.50ms
step:1317/2315 train_time:79678ms step_avg:60.50ms
step:1318/2315 train_time:79738ms step_avg:60.50ms
step:1319/2315 train_time:79799ms step_avg:60.50ms
step:1320/2315 train_time:79860ms step_avg:60.50ms
step:1321/2315 train_time:79922ms step_avg:60.50ms
step:1322/2315 train_time:79982ms step_avg:60.50ms
step:1323/2315 train_time:80043ms step_avg:60.50ms
step:1324/2315 train_time:80103ms step_avg:60.50ms
step:1325/2315 train_time:80164ms step_avg:60.50ms
step:1326/2315 train_time:80225ms step_avg:60.50ms
step:1327/2315 train_time:80285ms step_avg:60.50ms
step:1328/2315 train_time:80346ms step_avg:60.50ms
step:1329/2315 train_time:80407ms step_avg:60.50ms
step:1330/2315 train_time:80468ms step_avg:60.50ms
step:1331/2315 train_time:80529ms step_avg:60.50ms
step:1332/2315 train_time:80590ms step_avg:60.50ms
step:1333/2315 train_time:80651ms step_avg:60.50ms
step:1334/2315 train_time:80712ms step_avg:60.50ms
step:1335/2315 train_time:80773ms step_avg:60.50ms
step:1336/2315 train_time:80834ms step_avg:60.50ms
step:1337/2315 train_time:80896ms step_avg:60.51ms
step:1338/2315 train_time:80957ms step_avg:60.51ms
step:1339/2315 train_time:81018ms step_avg:60.51ms
step:1340/2315 train_time:81078ms step_avg:60.51ms
step:1341/2315 train_time:81139ms step_avg:60.51ms
step:1342/2315 train_time:81200ms step_avg:60.51ms
step:1343/2315 train_time:81262ms step_avg:60.51ms
step:1344/2315 train_time:81323ms step_avg:60.51ms
step:1345/2315 train_time:81384ms step_avg:60.51ms
step:1346/2315 train_time:81444ms step_avg:60.51ms
step:1347/2315 train_time:81504ms step_avg:60.51ms
step:1348/2315 train_time:81565ms step_avg:60.51ms
step:1349/2315 train_time:81626ms step_avg:60.51ms
step:1350/2315 train_time:81686ms step_avg:60.51ms
step:1351/2315 train_time:81747ms step_avg:60.51ms
step:1352/2315 train_time:81809ms step_avg:60.51ms
step:1353/2315 train_time:81870ms step_avg:60.51ms
step:1354/2315 train_time:81931ms step_avg:60.51ms
step:1355/2315 train_time:81992ms step_avg:60.51ms
step:1356/2315 train_time:82053ms step_avg:60.51ms
step:1357/2315 train_time:82114ms step_avg:60.51ms
step:1358/2315 train_time:82175ms step_avg:60.51ms
step:1359/2315 train_time:82237ms step_avg:60.51ms
step:1360/2315 train_time:82297ms step_avg:60.51ms
step:1361/2315 train_time:82358ms step_avg:60.51ms
step:1362/2315 train_time:82419ms step_avg:60.51ms
step:1363/2315 train_time:82480ms step_avg:60.51ms
step:1364/2315 train_time:82541ms step_avg:60.51ms
step:1365/2315 train_time:82602ms step_avg:60.51ms
step:1366/2315 train_time:82663ms step_avg:60.51ms
step:1367/2315 train_time:82725ms step_avg:60.52ms
step:1368/2315 train_time:82785ms step_avg:60.52ms
step:1369/2315 train_time:82845ms step_avg:60.52ms
step:1370/2315 train_time:82906ms step_avg:60.52ms
step:1371/2315 train_time:82968ms step_avg:60.52ms
step:1372/2315 train_time:83028ms step_avg:60.52ms
step:1373/2315 train_time:83090ms step_avg:60.52ms
step:1374/2315 train_time:83151ms step_avg:60.52ms
step:1375/2315 train_time:83212ms step_avg:60.52ms
step:1376/2315 train_time:83273ms step_avg:60.52ms
step:1377/2315 train_time:83334ms step_avg:60.52ms
step:1378/2315 train_time:83394ms step_avg:60.52ms
step:1379/2315 train_time:83456ms step_avg:60.52ms
step:1380/2315 train_time:83517ms step_avg:60.52ms
step:1381/2315 train_time:83578ms step_avg:60.52ms
step:1382/2315 train_time:83639ms step_avg:60.52ms
step:1383/2315 train_time:83699ms step_avg:60.52ms
step:1384/2315 train_time:83760ms step_avg:60.52ms
step:1385/2315 train_time:83822ms step_avg:60.52ms
step:1386/2315 train_time:83883ms step_avg:60.52ms
step:1387/2315 train_time:83944ms step_avg:60.52ms
step:1388/2315 train_time:84004ms step_avg:60.52ms
step:1389/2315 train_time:84065ms step_avg:60.52ms
step:1390/2315 train_time:84126ms step_avg:60.52ms
step:1391/2315 train_time:84187ms step_avg:60.52ms
step:1392/2315 train_time:84248ms step_avg:60.52ms
step:1393/2315 train_time:84309ms step_avg:60.52ms
step:1394/2315 train_time:84371ms step_avg:60.52ms
step:1395/2315 train_time:84432ms step_avg:60.52ms
step:1396/2315 train_time:84493ms step_avg:60.52ms
step:1397/2315 train_time:84554ms step_avg:60.53ms
step:1398/2315 train_time:84615ms step_avg:60.53ms
step:1399/2315 train_time:84676ms step_avg:60.53ms
step:1400/2315 train_time:84736ms step_avg:60.53ms
step:1401/2315 train_time:84797ms step_avg:60.53ms
step:1402/2315 train_time:84858ms step_avg:60.53ms
step:1403/2315 train_time:84919ms step_avg:60.53ms
step:1404/2315 train_time:84980ms step_avg:60.53ms
step:1405/2315 train_time:85041ms step_avg:60.53ms
step:1406/2315 train_time:85102ms step_avg:60.53ms
step:1407/2315 train_time:85163ms step_avg:60.53ms
step:1408/2315 train_time:85224ms step_avg:60.53ms
step:1409/2315 train_time:85285ms step_avg:60.53ms
step:1410/2315 train_time:85345ms step_avg:60.53ms
step:1411/2315 train_time:85407ms step_avg:60.53ms
step:1412/2315 train_time:85467ms step_avg:60.53ms
step:1413/2315 train_time:85528ms step_avg:60.53ms
step:1414/2315 train_time:85589ms step_avg:60.53ms
step:1415/2315 train_time:85650ms step_avg:60.53ms
step:1416/2315 train_time:85711ms step_avg:60.53ms
step:1417/2315 train_time:85773ms step_avg:60.53ms
step:1418/2315 train_time:85833ms step_avg:60.53ms
step:1419/2315 train_time:85895ms step_avg:60.53ms
step:1420/2315 train_time:85956ms step_avg:60.53ms
step:1421/2315 train_time:86016ms step_avg:60.53ms
step:1422/2315 train_time:86078ms step_avg:60.53ms
step:1423/2315 train_time:86139ms step_avg:60.53ms
step:1424/2315 train_time:86199ms step_avg:60.53ms
step:1425/2315 train_time:86260ms step_avg:60.53ms
step:1426/2315 train_time:86322ms step_avg:60.53ms
step:1427/2315 train_time:86384ms step_avg:60.54ms
step:1428/2315 train_time:86444ms step_avg:60.54ms
step:1429/2315 train_time:86506ms step_avg:60.54ms
step:1430/2315 train_time:86566ms step_avg:60.54ms
step:1431/2315 train_time:86627ms step_avg:60.54ms
step:1432/2315 train_time:86688ms step_avg:60.54ms
step:1433/2315 train_time:86749ms step_avg:60.54ms
step:1434/2315 train_time:86810ms step_avg:60.54ms
step:1435/2315 train_time:86871ms step_avg:60.54ms
step:1436/2315 train_time:86931ms step_avg:60.54ms
step:1437/2315 train_time:86993ms step_avg:60.54ms
step:1438/2315 train_time:87054ms step_avg:60.54ms
step:1439/2315 train_time:87115ms step_avg:60.54ms
step:1440/2315 train_time:87175ms step_avg:60.54ms
step:1441/2315 train_time:87236ms step_avg:60.54ms
step:1442/2315 train_time:87297ms step_avg:60.54ms
step:1443/2315 train_time:87358ms step_avg:60.54ms
step:1444/2315 train_time:87419ms step_avg:60.54ms
step:1445/2315 train_time:87481ms step_avg:60.54ms
step:1446/2315 train_time:87542ms step_avg:60.54ms
step:1447/2315 train_time:87603ms step_avg:60.54ms
step:1448/2315 train_time:87664ms step_avg:60.54ms
step:1449/2315 train_time:87726ms step_avg:60.54ms
step:1450/2315 train_time:87786ms step_avg:60.54ms
step:1451/2315 train_time:87846ms step_avg:60.54ms
step:1452/2315 train_time:87907ms step_avg:60.54ms
step:1453/2315 train_time:87968ms step_avg:60.54ms
step:1454/2315 train_time:88029ms step_avg:60.54ms
step:1455/2315 train_time:88090ms step_avg:60.54ms
step:1456/2315 train_time:88150ms step_avg:60.54ms
step:1457/2315 train_time:88212ms step_avg:60.54ms
step:1458/2315 train_time:88272ms step_avg:60.54ms
step:1459/2315 train_time:88335ms step_avg:60.54ms
step:1460/2315 train_time:88395ms step_avg:60.54ms
step:1461/2315 train_time:88456ms step_avg:60.54ms
step:1462/2315 train_time:88517ms step_avg:60.54ms
step:1463/2315 train_time:88578ms step_avg:60.55ms
step:1464/2315 train_time:88639ms step_avg:60.55ms
step:1465/2315 train_time:88700ms step_avg:60.55ms
step:1466/2315 train_time:88760ms step_avg:60.55ms
step:1467/2315 train_time:88821ms step_avg:60.55ms
step:1468/2315 train_time:88882ms step_avg:60.55ms
step:1469/2315 train_time:88944ms step_avg:60.55ms
step:1470/2315 train_time:89004ms step_avg:60.55ms
step:1471/2315 train_time:89064ms step_avg:60.55ms
step:1472/2315 train_time:89125ms step_avg:60.55ms
step:1473/2315 train_time:89186ms step_avg:60.55ms
step:1474/2315 train_time:89246ms step_avg:60.55ms
step:1475/2315 train_time:89308ms step_avg:60.55ms
step:1476/2315 train_time:89369ms step_avg:60.55ms
step:1477/2315 train_time:89431ms step_avg:60.55ms
step:1478/2315 train_time:89491ms step_avg:60.55ms
step:1479/2315 train_time:89553ms step_avg:60.55ms
step:1480/2315 train_time:89614ms step_avg:60.55ms
step:1481/2315 train_time:89674ms step_avg:60.55ms
step:1482/2315 train_time:89734ms step_avg:60.55ms
step:1483/2315 train_time:89795ms step_avg:60.55ms
step:1484/2315 train_time:89857ms step_avg:60.55ms
step:1485/2315 train_time:89918ms step_avg:60.55ms
step:1486/2315 train_time:89979ms step_avg:60.55ms
step:1487/2315 train_time:90040ms step_avg:60.55ms
step:1488/2315 train_time:90100ms step_avg:60.55ms
step:1489/2315 train_time:90161ms step_avg:60.55ms
step:1490/2315 train_time:90223ms step_avg:60.55ms
step:1491/2315 train_time:90284ms step_avg:60.55ms
step:1492/2315 train_time:90344ms step_avg:60.55ms
step:1493/2315 train_time:90406ms step_avg:60.55ms
step:1494/2315 train_time:90467ms step_avg:60.55ms
step:1495/2315 train_time:90528ms step_avg:60.55ms
step:1496/2315 train_time:90588ms step_avg:60.55ms
step:1497/2315 train_time:90649ms step_avg:60.55ms
step:1498/2315 train_time:90710ms step_avg:60.55ms
step:1499/2315 train_time:90771ms step_avg:60.55ms
step:1500/2315 train_time:90832ms step_avg:60.55ms
step:1500/2315 val_loss:3.4525 train_time:90894ms step_avg:60.60ms
step:1501/2315 train_time:90914ms step_avg:60.57ms
step:1502/2315 train_time:90956ms step_avg:60.56ms
step:1503/2315 train_time:91020ms step_avg:60.56ms
step:1504/2315 train_time:91084ms step_avg:60.56ms
step:1505/2315 train_time:91145ms step_avg:60.56ms
step:1506/2315 train_time:91206ms step_avg:60.56ms
step:1507/2315 train_time:91266ms step_avg:60.56ms
step:1508/2315 train_time:91326ms step_avg:60.56ms
step:1509/2315 train_time:91387ms step_avg:60.56ms
step:1510/2315 train_time:91446ms step_avg:60.56ms
step:1511/2315 train_time:91506ms step_avg:60.56ms
step:1512/2315 train_time:91566ms step_avg:60.56ms
step:1513/2315 train_time:91626ms step_avg:60.56ms
step:1514/2315 train_time:91686ms step_avg:60.56ms
step:1515/2315 train_time:91746ms step_avg:60.56ms
step:1516/2315 train_time:91807ms step_avg:60.56ms
step:1517/2315 train_time:91869ms step_avg:60.56ms
step:1518/2315 train_time:91931ms step_avg:60.56ms
step:1519/2315 train_time:91993ms step_avg:60.56ms
step:1520/2315 train_time:92056ms step_avg:60.56ms
step:1521/2315 train_time:92119ms step_avg:60.56ms
step:1522/2315 train_time:92179ms step_avg:60.56ms
step:1523/2315 train_time:92241ms step_avg:60.57ms
step:1524/2315 train_time:92302ms step_avg:60.57ms
step:1525/2315 train_time:92364ms step_avg:60.57ms
step:1526/2315 train_time:92424ms step_avg:60.57ms
step:1527/2315 train_time:92485ms step_avg:60.57ms
step:1528/2315 train_time:92546ms step_avg:60.57ms
step:1529/2315 train_time:92607ms step_avg:60.57ms
step:1530/2315 train_time:92667ms step_avg:60.57ms
step:1531/2315 train_time:92728ms step_avg:60.57ms
step:1532/2315 train_time:92789ms step_avg:60.57ms
step:1533/2315 train_time:92850ms step_avg:60.57ms
step:1534/2315 train_time:92912ms step_avg:60.57ms
step:1535/2315 train_time:92975ms step_avg:60.57ms
step:1536/2315 train_time:93036ms step_avg:60.57ms
step:1537/2315 train_time:93098ms step_avg:60.57ms
step:1538/2315 train_time:93159ms step_avg:60.57ms
step:1539/2315 train_time:93220ms step_avg:60.57ms
step:1540/2315 train_time:93282ms step_avg:60.57ms
step:1541/2315 train_time:93344ms step_avg:60.57ms
step:1542/2315 train_time:93405ms step_avg:60.57ms
step:1543/2315 train_time:93466ms step_avg:60.57ms
step:1544/2315 train_time:93527ms step_avg:60.57ms
step:1545/2315 train_time:93588ms step_avg:60.57ms
step:1546/2315 train_time:93648ms step_avg:60.57ms
step:1547/2315 train_time:93709ms step_avg:60.57ms
step:1548/2315 train_time:93770ms step_avg:60.57ms
step:1549/2315 train_time:93831ms step_avg:60.58ms
step:1550/2315 train_time:93892ms step_avg:60.58ms
step:1551/2315 train_time:93954ms step_avg:60.58ms
step:1552/2315 train_time:94016ms step_avg:60.58ms
step:1553/2315 train_time:94079ms step_avg:60.58ms
step:1554/2315 train_time:94139ms step_avg:60.58ms
step:1555/2315 train_time:94201ms step_avg:60.58ms
step:1556/2315 train_time:94263ms step_avg:60.58ms
step:1557/2315 train_time:94324ms step_avg:60.58ms
step:1558/2315 train_time:94385ms step_avg:60.58ms
step:1559/2315 train_time:94447ms step_avg:60.58ms
step:1560/2315 train_time:94508ms step_avg:60.58ms
step:1561/2315 train_time:94569ms step_avg:60.58ms
step:1562/2315 train_time:94629ms step_avg:60.58ms
step:1563/2315 train_time:94690ms step_avg:60.58ms
step:1564/2315 train_time:94751ms step_avg:60.58ms
step:1565/2315 train_time:94812ms step_avg:60.58ms
step:1566/2315 train_time:94873ms step_avg:60.58ms
step:1567/2315 train_time:94934ms step_avg:60.58ms
step:1568/2315 train_time:94996ms step_avg:60.58ms
step:1569/2315 train_time:95058ms step_avg:60.59ms
step:1570/2315 train_time:95119ms step_avg:60.59ms
step:1571/2315 train_time:95181ms step_avg:60.59ms
step:1572/2315 train_time:95241ms step_avg:60.59ms
step:1573/2315 train_time:95303ms step_avg:60.59ms
step:1574/2315 train_time:95364ms step_avg:60.59ms
step:1575/2315 train_time:95425ms step_avg:60.59ms
step:1576/2315 train_time:95486ms step_avg:60.59ms
step:1577/2315 train_time:95548ms step_avg:60.59ms
step:1578/2315 train_time:95608ms step_avg:60.59ms
step:1579/2315 train_time:95669ms step_avg:60.59ms
step:1580/2315 train_time:95730ms step_avg:60.59ms
step:1581/2315 train_time:95791ms step_avg:60.59ms
step:1582/2315 train_time:95852ms step_avg:60.59ms
step:1583/2315 train_time:95913ms step_avg:60.59ms
step:1584/2315 train_time:95975ms step_avg:60.59ms
step:1585/2315 train_time:96037ms step_avg:60.59ms
step:1586/2315 train_time:96098ms step_avg:60.59ms
step:1587/2315 train_time:96160ms step_avg:60.59ms
step:1588/2315 train_time:96221ms step_avg:60.59ms
step:1589/2315 train_time:96283ms step_avg:60.59ms
step:1590/2315 train_time:96344ms step_avg:60.59ms
step:1591/2315 train_time:96404ms step_avg:60.59ms
step:1592/2315 train_time:96465ms step_avg:60.59ms
step:1593/2315 train_time:96527ms step_avg:60.59ms
step:1594/2315 train_time:96587ms step_avg:60.59ms
step:1595/2315 train_time:96649ms step_avg:60.59ms
step:1596/2315 train_time:96709ms step_avg:60.59ms
step:1597/2315 train_time:96771ms step_avg:60.60ms
step:1598/2315 train_time:96831ms step_avg:60.60ms
step:1599/2315 train_time:96893ms step_avg:60.60ms
step:1600/2315 train_time:96954ms step_avg:60.60ms
step:1601/2315 train_time:97016ms step_avg:60.60ms
step:1602/2315 train_time:97077ms step_avg:60.60ms
step:1603/2315 train_time:97139ms step_avg:60.60ms
step:1604/2315 train_time:97200ms step_avg:60.60ms
step:1605/2315 train_time:97261ms step_avg:60.60ms
step:1606/2315 train_time:97322ms step_avg:60.60ms
step:1607/2315 train_time:97384ms step_avg:60.60ms
step:1608/2315 train_time:97445ms step_avg:60.60ms
step:1609/2315 train_time:97506ms step_avg:60.60ms
step:1610/2315 train_time:97567ms step_avg:60.60ms
step:1611/2315 train_time:97628ms step_avg:60.60ms
step:1612/2315 train_time:97689ms step_avg:60.60ms
step:1613/2315 train_time:97750ms step_avg:60.60ms
step:1614/2315 train_time:97811ms step_avg:60.60ms
step:1615/2315 train_time:97872ms step_avg:60.60ms
step:1616/2315 train_time:97933ms step_avg:60.60ms
step:1617/2315 train_time:97995ms step_avg:60.60ms
step:1618/2315 train_time:98056ms step_avg:60.60ms
step:1619/2315 train_time:98118ms step_avg:60.60ms
step:1620/2315 train_time:98179ms step_avg:60.60ms
step:1621/2315 train_time:98241ms step_avg:60.61ms
step:1622/2315 train_time:98302ms step_avg:60.61ms
step:1623/2315 train_time:98363ms step_avg:60.61ms
step:1624/2315 train_time:98424ms step_avg:60.61ms
step:1625/2315 train_time:98486ms step_avg:60.61ms
step:1626/2315 train_time:98548ms step_avg:60.61ms
step:1627/2315 train_time:98609ms step_avg:60.61ms
step:1628/2315 train_time:98670ms step_avg:60.61ms
step:1629/2315 train_time:98731ms step_avg:60.61ms
step:1630/2315 train_time:98792ms step_avg:60.61ms
step:1631/2315 train_time:98852ms step_avg:60.61ms
step:1632/2315 train_time:98913ms step_avg:60.61ms
step:1633/2315 train_time:98974ms step_avg:60.61ms
step:1634/2315 train_time:99035ms step_avg:60.61ms
step:1635/2315 train_time:99097ms step_avg:60.61ms
step:1636/2315 train_time:99159ms step_avg:60.61ms
step:1637/2315 train_time:99221ms step_avg:60.61ms
step:1638/2315 train_time:99282ms step_avg:60.61ms
step:1639/2315 train_time:99343ms step_avg:60.61ms
step:1640/2315 train_time:99404ms step_avg:60.61ms
step:1641/2315 train_time:99465ms step_avg:60.61ms
step:1642/2315 train_time:99526ms step_avg:60.61ms
step:1643/2315 train_time:99587ms step_avg:60.61ms
step:1644/2315 train_time:99648ms step_avg:60.61ms
step:1645/2315 train_time:99709ms step_avg:60.61ms
step:1646/2315 train_time:99770ms step_avg:60.61ms
step:1647/2315 train_time:99831ms step_avg:60.61ms
step:1648/2315 train_time:99892ms step_avg:60.61ms
step:1649/2315 train_time:99953ms step_avg:60.61ms
step:1650/2315 train_time:100014ms step_avg:60.61ms
step:1651/2315 train_time:100075ms step_avg:60.61ms
step:1652/2315 train_time:100137ms step_avg:60.62ms
step:1653/2315 train_time:100198ms step_avg:60.62ms
step:1654/2315 train_time:100260ms step_avg:60.62ms
step:1655/2315 train_time:100322ms step_avg:60.62ms
step:1656/2315 train_time:100384ms step_avg:60.62ms
step:1657/2315 train_time:100445ms step_avg:60.62ms
step:1658/2315 train_time:100507ms step_avg:60.62ms
step:1659/2315 train_time:100567ms step_avg:60.62ms
step:1660/2315 train_time:100628ms step_avg:60.62ms
step:1661/2315 train_time:100689ms step_avg:60.62ms
step:1662/2315 train_time:100750ms step_avg:60.62ms
step:1663/2315 train_time:100812ms step_avg:60.62ms
step:1664/2315 train_time:100872ms step_avg:60.62ms
step:1665/2315 train_time:100934ms step_avg:60.62ms
step:1666/2315 train_time:100994ms step_avg:60.62ms
step:1667/2315 train_time:101056ms step_avg:60.62ms
step:1668/2315 train_time:101117ms step_avg:60.62ms
step:1669/2315 train_time:101179ms step_avg:60.62ms
step:1670/2315 train_time:101240ms step_avg:60.62ms
step:1671/2315 train_time:101302ms step_avg:60.62ms
step:1672/2315 train_time:101363ms step_avg:60.62ms
step:1673/2315 train_time:101425ms step_avg:60.62ms
step:1674/2315 train_time:101486ms step_avg:60.62ms
step:1675/2315 train_time:101548ms step_avg:60.63ms
step:1676/2315 train_time:101609ms step_avg:60.63ms
step:1677/2315 train_time:101670ms step_avg:60.63ms
step:1678/2315 train_time:101731ms step_avg:60.63ms
step:1679/2315 train_time:101792ms step_avg:60.63ms
step:1680/2315 train_time:101854ms step_avg:60.63ms
step:1681/2315 train_time:101915ms step_avg:60.63ms
step:1682/2315 train_time:101975ms step_avg:60.63ms
step:1683/2315 train_time:102037ms step_avg:60.63ms
step:1684/2315 train_time:102097ms step_avg:60.63ms
step:1685/2315 train_time:102158ms step_avg:60.63ms
step:1686/2315 train_time:102219ms step_avg:60.63ms
step:1687/2315 train_time:102281ms step_avg:60.63ms
step:1688/2315 train_time:102342ms step_avg:60.63ms
step:1689/2315 train_time:102404ms step_avg:60.63ms
step:1690/2315 train_time:102465ms step_avg:60.63ms
step:1691/2315 train_time:102526ms step_avg:60.63ms
step:1692/2315 train_time:102587ms step_avg:60.63ms
step:1693/2315 train_time:102649ms step_avg:60.63ms
step:1694/2315 train_time:102710ms step_avg:60.63ms
step:1695/2315 train_time:102771ms step_avg:60.63ms
step:1696/2315 train_time:102832ms step_avg:60.63ms
step:1697/2315 train_time:102893ms step_avg:60.63ms
step:1698/2315 train_time:102954ms step_avg:60.63ms
step:1699/2315 train_time:103015ms step_avg:60.63ms
step:1700/2315 train_time:103076ms step_avg:60.63ms
step:1701/2315 train_time:103138ms step_avg:60.63ms
step:1702/2315 train_time:103199ms step_avg:60.63ms
step:1703/2315 train_time:103260ms step_avg:60.63ms
step:1704/2315 train_time:103321ms step_avg:60.63ms
step:1705/2315 train_time:103383ms step_avg:60.63ms
step:1706/2315 train_time:103444ms step_avg:60.64ms
step:1707/2315 train_time:103506ms step_avg:60.64ms
step:1708/2315 train_time:103567ms step_avg:60.64ms
step:1709/2315 train_time:103628ms step_avg:60.64ms
step:1710/2315 train_time:103689ms step_avg:60.64ms
step:1711/2315 train_time:103750ms step_avg:60.64ms
step:1712/2315 train_time:103811ms step_avg:60.64ms
step:1713/2315 train_time:103872ms step_avg:60.64ms
step:1714/2315 train_time:103933ms step_avg:60.64ms
step:1715/2315 train_time:103995ms step_avg:60.64ms
step:1716/2315 train_time:104056ms step_avg:60.64ms
step:1717/2315 train_time:104118ms step_avg:60.64ms
step:1718/2315 train_time:104179ms step_avg:60.64ms
step:1719/2315 train_time:104240ms step_avg:60.64ms
step:1720/2315 train_time:104302ms step_avg:60.64ms
step:1721/2315 train_time:104363ms step_avg:60.64ms
step:1722/2315 train_time:104424ms step_avg:60.64ms
step:1723/2315 train_time:104486ms step_avg:60.64ms
step:1724/2315 train_time:104548ms step_avg:60.64ms
step:1725/2315 train_time:104609ms step_avg:60.64ms
step:1726/2315 train_time:104670ms step_avg:60.64ms
step:1727/2315 train_time:104732ms step_avg:60.64ms
step:1728/2315 train_time:104793ms step_avg:60.64ms
step:1729/2315 train_time:104854ms step_avg:60.64ms
step:1730/2315 train_time:104915ms step_avg:60.64ms
step:1731/2315 train_time:104976ms step_avg:60.64ms
step:1732/2315 train_time:105037ms step_avg:60.64ms
step:1733/2315 train_time:105098ms step_avg:60.65ms
step:1734/2315 train_time:105159ms step_avg:60.65ms
step:1735/2315 train_time:105220ms step_avg:60.65ms
step:1736/2315 train_time:105282ms step_avg:60.65ms
step:1737/2315 train_time:105343ms step_avg:60.65ms
step:1738/2315 train_time:105404ms step_avg:60.65ms
step:1739/2315 train_time:105466ms step_avg:60.65ms
step:1740/2315 train_time:105527ms step_avg:60.65ms
step:1741/2315 train_time:105588ms step_avg:60.65ms
step:1742/2315 train_time:105649ms step_avg:60.65ms
step:1743/2315 train_time:105710ms step_avg:60.65ms
step:1744/2315 train_time:105771ms step_avg:60.65ms
step:1745/2315 train_time:105832ms step_avg:60.65ms
step:1746/2315 train_time:105893ms step_avg:60.65ms
step:1747/2315 train_time:105954ms step_avg:60.65ms
step:1748/2315 train_time:106015ms step_avg:60.65ms
step:1749/2315 train_time:106077ms step_avg:60.65ms
step:1750/2315 train_time:106138ms step_avg:60.65ms
step:1750/2315 val_loss:3.3835 train_time:106201ms step_avg:60.69ms
step:1751/2315 train_time:106221ms step_avg:60.66ms
step:1752/2315 train_time:106266ms step_avg:60.65ms
step:1753/2315 train_time:106332ms step_avg:60.66ms
step:1754/2315 train_time:106398ms step_avg:60.66ms
step:1755/2315 train_time:106460ms step_avg:60.66ms
step:1756/2315 train_time:106523ms step_avg:60.66ms
step:1757/2315 train_time:106584ms step_avg:60.66ms
step:1758/2315 train_time:106645ms step_avg:60.66ms
step:1759/2315 train_time:106705ms step_avg:60.66ms
step:1760/2315 train_time:106766ms step_avg:60.66ms
step:1761/2315 train_time:106826ms step_avg:60.66ms
step:1762/2315 train_time:106887ms step_avg:60.66ms
step:1763/2315 train_time:106947ms step_avg:60.66ms
step:1764/2315 train_time:107008ms step_avg:60.66ms
step:1765/2315 train_time:107068ms step_avg:60.66ms
step:1766/2315 train_time:107129ms step_avg:60.66ms
step:1767/2315 train_time:107191ms step_avg:60.66ms
step:1768/2315 train_time:107253ms step_avg:60.66ms
step:1769/2315 train_time:107316ms step_avg:60.66ms
step:1770/2315 train_time:107378ms step_avg:60.67ms
step:1771/2315 train_time:107440ms step_avg:60.67ms
step:1772/2315 train_time:107502ms step_avg:60.67ms
step:1773/2315 train_time:107564ms step_avg:60.67ms
step:1774/2315 train_time:107624ms step_avg:60.67ms
step:1775/2315 train_time:107685ms step_avg:60.67ms
step:1776/2315 train_time:107746ms step_avg:60.67ms
step:1777/2315 train_time:107807ms step_avg:60.67ms
step:1778/2315 train_time:107868ms step_avg:60.67ms
step:1779/2315 train_time:107929ms step_avg:60.67ms
step:1780/2315 train_time:107990ms step_avg:60.67ms
step:1781/2315 train_time:108051ms step_avg:60.67ms
step:1782/2315 train_time:108111ms step_avg:60.67ms
step:1783/2315 train_time:108173ms step_avg:60.67ms
step:1784/2315 train_time:108234ms step_avg:60.67ms
step:1785/2315 train_time:108296ms step_avg:60.67ms
step:1786/2315 train_time:108357ms step_avg:60.67ms
step:1787/2315 train_time:108419ms step_avg:60.67ms
step:1788/2315 train_time:108480ms step_avg:60.67ms
step:1789/2315 train_time:108542ms step_avg:60.67ms
step:1790/2315 train_time:108603ms step_avg:60.67ms
step:1791/2315 train_time:108664ms step_avg:60.67ms
step:1792/2315 train_time:108726ms step_avg:60.67ms
step:1793/2315 train_time:108787ms step_avg:60.67ms
step:1794/2315 train_time:108848ms step_avg:60.67ms
step:1795/2315 train_time:108909ms step_avg:60.67ms
step:1796/2315 train_time:108970ms step_avg:60.67ms
step:1797/2315 train_time:109030ms step_avg:60.67ms
step:1798/2315 train_time:109091ms step_avg:60.67ms
step:1799/2315 train_time:109152ms step_avg:60.67ms
step:1800/2315 train_time:109214ms step_avg:60.67ms
step:1801/2315 train_time:109276ms step_avg:60.68ms
step:1802/2315 train_time:109338ms step_avg:60.68ms
step:1803/2315 train_time:109399ms step_avg:60.68ms
step:1804/2315 train_time:109461ms step_avg:60.68ms
step:1805/2315 train_time:109522ms step_avg:60.68ms
step:1806/2315 train_time:109583ms step_avg:60.68ms
step:1807/2315 train_time:109644ms step_avg:60.68ms
step:1808/2315 train_time:109705ms step_avg:60.68ms
step:1809/2315 train_time:109767ms step_avg:60.68ms
step:1810/2315 train_time:109827ms step_avg:60.68ms
step:1811/2315 train_time:109889ms step_avg:60.68ms
step:1812/2315 train_time:109949ms step_avg:60.68ms
step:1813/2315 train_time:110010ms step_avg:60.68ms
step:1814/2315 train_time:110071ms step_avg:60.68ms
step:1815/2315 train_time:110133ms step_avg:60.68ms
step:1816/2315 train_time:110194ms step_avg:60.68ms
step:1817/2315 train_time:110256ms step_avg:60.68ms
step:1818/2315 train_time:110317ms step_avg:60.68ms
step:1819/2315 train_time:110379ms step_avg:60.68ms
step:1820/2315 train_time:110440ms step_avg:60.68ms
step:1821/2315 train_time:110501ms step_avg:60.68ms
step:1822/2315 train_time:110563ms step_avg:60.68ms
step:1823/2315 train_time:110625ms step_avg:60.68ms
step:1824/2315 train_time:110685ms step_avg:60.68ms
step:1825/2315 train_time:110746ms step_avg:60.68ms
step:1826/2315 train_time:110807ms step_avg:60.68ms
step:1827/2315 train_time:110868ms step_avg:60.68ms
step:1828/2315 train_time:110929ms step_avg:60.68ms
step:1829/2315 train_time:110990ms step_avg:60.68ms
step:1830/2315 train_time:111051ms step_avg:60.68ms
step:1831/2315 train_time:111112ms step_avg:60.68ms
step:1832/2315 train_time:111173ms step_avg:60.68ms
step:1833/2315 train_time:111234ms step_avg:60.68ms
step:1834/2315 train_time:111295ms step_avg:60.68ms
step:1835/2315 train_time:111357ms step_avg:60.69ms
step:1836/2315 train_time:111418ms step_avg:60.69ms
step:1837/2315 train_time:111480ms step_avg:60.69ms
step:1838/2315 train_time:111541ms step_avg:60.69ms
step:1839/2315 train_time:111602ms step_avg:60.69ms
step:1840/2315 train_time:111664ms step_avg:60.69ms
step:1841/2315 train_time:111724ms step_avg:60.69ms
step:1842/2315 train_time:111785ms step_avg:60.69ms
step:1843/2315 train_time:111846ms step_avg:60.69ms
step:1844/2315 train_time:111907ms step_avg:60.69ms
step:1845/2315 train_time:111969ms step_avg:60.69ms
step:1846/2315 train_time:112029ms step_avg:60.69ms
step:1847/2315 train_time:112090ms step_avg:60.69ms
step:1848/2315 train_time:112151ms step_avg:60.69ms
step:1849/2315 train_time:112213ms step_avg:60.69ms
step:1850/2315 train_time:112275ms step_avg:60.69ms
step:1851/2315 train_time:112336ms step_avg:60.69ms
step:1852/2315 train_time:112397ms step_avg:60.69ms
step:1853/2315 train_time:112458ms step_avg:60.69ms
step:1854/2315 train_time:112520ms step_avg:60.69ms
step:1855/2315 train_time:112582ms step_avg:60.69ms
step:1856/2315 train_time:112643ms step_avg:60.69ms
step:1857/2315 train_time:112704ms step_avg:60.69ms
step:1858/2315 train_time:112765ms step_avg:60.69ms
step:1859/2315 train_time:112826ms step_avg:60.69ms
step:1860/2315 train_time:112887ms step_avg:60.69ms
step:1861/2315 train_time:112948ms step_avg:60.69ms
step:1862/2315 train_time:113009ms step_avg:60.69ms
step:1863/2315 train_time:113070ms step_avg:60.69ms
step:1864/2315 train_time:113131ms step_avg:60.69ms
step:1865/2315 train_time:113193ms step_avg:60.69ms
step:1866/2315 train_time:113254ms step_avg:60.69ms
step:1867/2315 train_time:113316ms step_avg:60.69ms
step:1868/2315 train_time:113378ms step_avg:60.69ms
step:1869/2315 train_time:113439ms step_avg:60.69ms
step:1870/2315 train_time:113500ms step_avg:60.69ms
step:1871/2315 train_time:113561ms step_avg:60.70ms
step:1872/2315 train_time:113622ms step_avg:60.70ms
step:1873/2315 train_time:113683ms step_avg:60.70ms
step:1874/2315 train_time:113744ms step_avg:60.70ms
step:1875/2315 train_time:113806ms step_avg:60.70ms
step:1876/2315 train_time:113867ms step_avg:60.70ms
step:1877/2315 train_time:113928ms step_avg:60.70ms
step:1878/2315 train_time:113990ms step_avg:60.70ms
step:1879/2315 train_time:114051ms step_avg:60.70ms
step:1880/2315 train_time:114111ms step_avg:60.70ms
step:1881/2315 train_time:114172ms step_avg:60.70ms
step:1882/2315 train_time:114234ms step_avg:60.70ms
step:1883/2315 train_time:114295ms step_avg:60.70ms
step:1884/2315 train_time:114356ms step_avg:60.70ms
step:1885/2315 train_time:114418ms step_avg:60.70ms
step:1886/2315 train_time:114479ms step_avg:60.70ms
step:1887/2315 train_time:114540ms step_avg:60.70ms
step:1888/2315 train_time:114601ms step_avg:60.70ms
step:1889/2315 train_time:114664ms step_avg:60.70ms
step:1890/2315 train_time:114725ms step_avg:60.70ms
step:1891/2315 train_time:114787ms step_avg:60.70ms
step:1892/2315 train_time:114848ms step_avg:60.70ms
step:1893/2315 train_time:114909ms step_avg:60.70ms
step:1894/2315 train_time:114971ms step_avg:60.70ms
step:1895/2315 train_time:115032ms step_avg:60.70ms
step:1896/2315 train_time:115093ms step_avg:60.70ms
step:1897/2315 train_time:115154ms step_avg:60.70ms
step:1898/2315 train_time:115215ms step_avg:60.70ms
step:1899/2315 train_time:115276ms step_avg:60.70ms
step:1900/2315 train_time:115338ms step_avg:60.70ms
step:1901/2315 train_time:115399ms step_avg:60.70ms
step:1902/2315 train_time:115461ms step_avg:60.71ms
step:1903/2315 train_time:115522ms step_avg:60.70ms
step:1904/2315 train_time:115583ms step_avg:60.71ms
step:1905/2315 train_time:115644ms step_avg:60.71ms
step:1906/2315 train_time:115704ms step_avg:60.71ms
step:1907/2315 train_time:115766ms step_avg:60.71ms
step:1908/2315 train_time:115827ms step_avg:60.71ms
step:1909/2315 train_time:115888ms step_avg:60.71ms
step:1910/2315 train_time:115949ms step_avg:60.71ms
step:1911/2315 train_time:116010ms step_avg:60.71ms
step:1912/2315 train_time:116071ms step_avg:60.71ms
step:1913/2315 train_time:116133ms step_avg:60.71ms
step:1914/2315 train_time:116194ms step_avg:60.71ms
step:1915/2315 train_time:116255ms step_avg:60.71ms
step:1916/2315 train_time:116316ms step_avg:60.71ms
step:1917/2315 train_time:116377ms step_avg:60.71ms
step:1918/2315 train_time:116438ms step_avg:60.71ms
step:1919/2315 train_time:116500ms step_avg:60.71ms
step:1920/2315 train_time:116561ms step_avg:60.71ms
step:1921/2315 train_time:116623ms step_avg:60.71ms
step:1922/2315 train_time:116684ms step_avg:60.71ms
step:1923/2315 train_time:116745ms step_avg:60.71ms
step:1924/2315 train_time:116806ms step_avg:60.71ms
step:1925/2315 train_time:116868ms step_avg:60.71ms
step:1926/2315 train_time:116928ms step_avg:60.71ms
step:1927/2315 train_time:116990ms step_avg:60.71ms
step:1928/2315 train_time:117051ms step_avg:60.71ms
step:1929/2315 train_time:117112ms step_avg:60.71ms
step:1930/2315 train_time:117174ms step_avg:60.71ms
step:1931/2315 train_time:117235ms step_avg:60.71ms
step:1932/2315 train_time:117296ms step_avg:60.71ms
step:1933/2315 train_time:117357ms step_avg:60.71ms
step:1934/2315 train_time:117418ms step_avg:60.71ms
step:1935/2315 train_time:117480ms step_avg:60.71ms
step:1936/2315 train_time:117541ms step_avg:60.71ms
step:1937/2315 train_time:117603ms step_avg:60.71ms
step:1938/2315 train_time:117664ms step_avg:60.71ms
step:1939/2315 train_time:117725ms step_avg:60.71ms
step:1940/2315 train_time:117786ms step_avg:60.71ms
step:1941/2315 train_time:117847ms step_avg:60.71ms
step:1942/2315 train_time:117908ms step_avg:60.71ms
step:1943/2315 train_time:117969ms step_avg:60.71ms
step:1944/2315 train_time:118030ms step_avg:60.71ms
step:1945/2315 train_time:118092ms step_avg:60.72ms
step:1946/2315 train_time:118154ms step_avg:60.72ms
step:1947/2315 train_time:118215ms step_avg:60.72ms
step:1948/2315 train_time:118276ms step_avg:60.72ms
step:1949/2315 train_time:118337ms step_avg:60.72ms
step:1950/2315 train_time:118398ms step_avg:60.72ms
step:1951/2315 train_time:118459ms step_avg:60.72ms
step:1952/2315 train_time:118520ms step_avg:60.72ms
step:1953/2315 train_time:118581ms step_avg:60.72ms
step:1954/2315 train_time:118643ms step_avg:60.72ms
step:1955/2315 train_time:118704ms step_avg:60.72ms
step:1956/2315 train_time:118765ms step_avg:60.72ms
step:1957/2315 train_time:118827ms step_avg:60.72ms
step:1958/2315 train_time:118888ms step_avg:60.72ms
step:1959/2315 train_time:118949ms step_avg:60.72ms
step:1960/2315 train_time:119010ms step_avg:60.72ms
step:1961/2315 train_time:119072ms step_avg:60.72ms
step:1962/2315 train_time:119133ms step_avg:60.72ms
step:1963/2315 train_time:119194ms step_avg:60.72ms
step:1964/2315 train_time:119255ms step_avg:60.72ms
step:1965/2315 train_time:119316ms step_avg:60.72ms
step:1966/2315 train_time:119378ms step_avg:60.72ms
step:1967/2315 train_time:119439ms step_avg:60.72ms
step:1968/2315 train_time:119500ms step_avg:60.72ms
step:1969/2315 train_time:119561ms step_avg:60.72ms
step:1970/2315 train_time:119622ms step_avg:60.72ms
step:1971/2315 train_time:119683ms step_avg:60.72ms
step:1972/2315 train_time:119744ms step_avg:60.72ms
step:1973/2315 train_time:119806ms step_avg:60.72ms
step:1974/2315 train_time:119867ms step_avg:60.72ms
step:1975/2315 train_time:119928ms step_avg:60.72ms
step:1976/2315 train_time:119989ms step_avg:60.72ms
step:1977/2315 train_time:120051ms step_avg:60.72ms
step:1978/2315 train_time:120112ms step_avg:60.72ms
step:1979/2315 train_time:120174ms step_avg:60.72ms
step:1980/2315 train_time:120234ms step_avg:60.72ms
step:1981/2315 train_time:120296ms step_avg:60.72ms
step:1982/2315 train_time:120357ms step_avg:60.72ms
step:1983/2315 train_time:120418ms step_avg:60.73ms
step:1984/2315 train_time:120479ms step_avg:60.73ms
step:1985/2315 train_time:120541ms step_avg:60.73ms
step:1986/2315 train_time:120602ms step_avg:60.73ms
step:1987/2315 train_time:120664ms step_avg:60.73ms
step:1988/2315 train_time:120725ms step_avg:60.73ms
step:1989/2315 train_time:120787ms step_avg:60.73ms
step:1990/2315 train_time:120847ms step_avg:60.73ms
step:1991/2315 train_time:120909ms step_avg:60.73ms
step:1992/2315 train_time:120970ms step_avg:60.73ms
step:1993/2315 train_time:121031ms step_avg:60.73ms
step:1994/2315 train_time:121093ms step_avg:60.73ms
step:1995/2315 train_time:121154ms step_avg:60.73ms
step:1996/2315 train_time:121216ms step_avg:60.73ms
step:1997/2315 train_time:121277ms step_avg:60.73ms
step:1998/2315 train_time:121338ms step_avg:60.73ms
step:1999/2315 train_time:121399ms step_avg:60.73ms
step:2000/2315 train_time:121460ms step_avg:60.73ms
step:2000/2315 val_loss:3.3319 train_time:121523ms step_avg:60.76ms
step:2001/2315 train_time:121543ms step_avg:60.74ms
step:2002/2315 train_time:121585ms step_avg:60.73ms
step:2003/2315 train_time:121652ms step_avg:60.73ms
step:2004/2315 train_time:121715ms step_avg:60.74ms
step:2005/2315 train_time:121777ms step_avg:60.74ms
step:2006/2315 train_time:121839ms step_avg:60.74ms
step:2007/2315 train_time:121900ms step_avg:60.74ms
step:2008/2315 train_time:121960ms step_avg:60.74ms
step:2009/2315 train_time:122021ms step_avg:60.74ms
step:2010/2315 train_time:122081ms step_avg:60.74ms
step:2011/2315 train_time:122142ms step_avg:60.74ms
step:2012/2315 train_time:122203ms step_avg:60.74ms
step:2013/2315 train_time:122263ms step_avg:60.74ms
step:2014/2315 train_time:122324ms step_avg:60.74ms
step:2015/2315 train_time:122384ms step_avg:60.74ms
step:2016/2315 train_time:122445ms step_avg:60.74ms
step:2017/2315 train_time:122506ms step_avg:60.74ms
step:2018/2315 train_time:122568ms step_avg:60.74ms
step:2019/2315 train_time:122631ms step_avg:60.74ms
step:2020/2315 train_time:122694ms step_avg:60.74ms
step:2021/2315 train_time:122756ms step_avg:60.74ms
step:2022/2315 train_time:122817ms step_avg:60.74ms
step:2023/2315 train_time:122879ms step_avg:60.74ms
step:2024/2315 train_time:122940ms step_avg:60.74ms
step:2025/2315 train_time:123001ms step_avg:60.74ms
step:2026/2315 train_time:123061ms step_avg:60.74ms
step:2027/2315 train_time:123122ms step_avg:60.74ms
step:2028/2315 train_time:123182ms step_avg:60.74ms
step:2029/2315 train_time:123243ms step_avg:60.74ms
step:2030/2315 train_time:123304ms step_avg:60.74ms
step:2031/2315 train_time:123364ms step_avg:60.74ms
step:2032/2315 train_time:123425ms step_avg:60.74ms
step:2033/2315 train_time:123486ms step_avg:60.74ms
step:2034/2315 train_time:123548ms step_avg:60.74ms
step:2035/2315 train_time:123610ms step_avg:60.74ms
step:2036/2315 train_time:123672ms step_avg:60.74ms
step:2037/2315 train_time:123734ms step_avg:60.74ms
step:2038/2315 train_time:123795ms step_avg:60.74ms
step:2039/2315 train_time:123857ms step_avg:60.74ms
step:2040/2315 train_time:123917ms step_avg:60.74ms
step:2041/2315 train_time:123979ms step_avg:60.74ms
step:2042/2315 train_time:124041ms step_avg:60.74ms
step:2043/2315 train_time:124102ms step_avg:60.74ms
step:2044/2315 train_time:124163ms step_avg:60.74ms
step:2045/2315 train_time:124224ms step_avg:60.75ms
step:2046/2315 train_time:124284ms step_avg:60.75ms
step:2047/2315 train_time:124345ms step_avg:60.75ms
step:2048/2315 train_time:124406ms step_avg:60.75ms
step:2049/2315 train_time:124467ms step_avg:60.75ms
step:2050/2315 train_time:124528ms step_avg:60.75ms
step:2051/2315 train_time:124590ms step_avg:60.75ms
step:2052/2315 train_time:124651ms step_avg:60.75ms
step:2053/2315 train_time:124713ms step_avg:60.75ms
step:2054/2315 train_time:124774ms step_avg:60.75ms
step:2055/2315 train_time:124836ms step_avg:60.75ms
step:2056/2315 train_time:124897ms step_avg:60.75ms
step:2057/2315 train_time:124959ms step_avg:60.75ms
step:2058/2315 train_time:125020ms step_avg:60.75ms
step:2059/2315 train_time:125081ms step_avg:60.75ms
step:2060/2315 train_time:125142ms step_avg:60.75ms
step:2061/2315 train_time:125203ms step_avg:60.75ms
step:2062/2315 train_time:125264ms step_avg:60.75ms
step:2063/2315 train_time:125325ms step_avg:60.75ms
step:2064/2315 train_time:125385ms step_avg:60.75ms
step:2065/2315 train_time:125447ms step_avg:60.75ms
step:2066/2315 train_time:125508ms step_avg:60.75ms
step:2067/2315 train_time:125569ms step_avg:60.75ms
step:2068/2315 train_time:125630ms step_avg:60.75ms
step:2069/2315 train_time:125692ms step_avg:60.75ms
step:2070/2315 train_time:125754ms step_avg:60.75ms
step:2071/2315 train_time:125816ms step_avg:60.75ms
step:2072/2315 train_time:125878ms step_avg:60.75ms
step:2073/2315 train_time:125939ms step_avg:60.75ms
step:2074/2315 train_time:126000ms step_avg:60.75ms
step:2075/2315 train_time:126061ms step_avg:60.75ms
step:2076/2315 train_time:126122ms step_avg:60.75ms
step:2077/2315 train_time:126183ms step_avg:60.75ms
step:2078/2315 train_time:126244ms step_avg:60.75ms
step:2079/2315 train_time:126305ms step_avg:60.75ms
step:2080/2315 train_time:126366ms step_avg:60.75ms
step:2081/2315 train_time:126426ms step_avg:60.75ms
step:2082/2315 train_time:126487ms step_avg:60.75ms
step:2083/2315 train_time:126549ms step_avg:60.75ms
step:2084/2315 train_time:126610ms step_avg:60.75ms
step:2085/2315 train_time:126671ms step_avg:60.75ms
step:2086/2315 train_time:126732ms step_avg:60.75ms
step:2087/2315 train_time:126794ms step_avg:60.75ms
step:2088/2315 train_time:126855ms step_avg:60.75ms
step:2089/2315 train_time:126917ms step_avg:60.75ms
step:2090/2315 train_time:126978ms step_avg:60.76ms
step:2091/2315 train_time:127040ms step_avg:60.76ms
step:2092/2315 train_time:127101ms step_avg:60.76ms
step:2093/2315 train_time:127162ms step_avg:60.76ms
step:2094/2315 train_time:127224ms step_avg:60.76ms
step:2095/2315 train_time:127284ms step_avg:60.76ms
step:2096/2315 train_time:127346ms step_avg:60.76ms
step:2097/2315 train_time:127407ms step_avg:60.76ms
step:2098/2315 train_time:127468ms step_avg:60.76ms
step:2099/2315 train_time:127529ms step_avg:60.76ms
step:2100/2315 train_time:127590ms step_avg:60.76ms
step:2101/2315 train_time:127651ms step_avg:60.76ms
step:2102/2315 train_time:127713ms step_avg:60.76ms
step:2103/2315 train_time:127775ms step_avg:60.76ms
step:2104/2315 train_time:127835ms step_avg:60.76ms
step:2105/2315 train_time:127897ms step_avg:60.76ms
step:2106/2315 train_time:127958ms step_avg:60.76ms
step:2107/2315 train_time:128020ms step_avg:60.76ms
step:2108/2315 train_time:128081ms step_avg:60.76ms
step:2109/2315 train_time:128142ms step_avg:60.76ms
step:2110/2315 train_time:128203ms step_avg:60.76ms
step:2111/2315 train_time:128265ms step_avg:60.76ms
step:2112/2315 train_time:128326ms step_avg:60.76ms
step:2113/2315 train_time:128387ms step_avg:60.76ms
step:2114/2315 train_time:128448ms step_avg:60.76ms
step:2115/2315 train_time:128509ms step_avg:60.76ms
step:2116/2315 train_time:128569ms step_avg:60.76ms
step:2117/2315 train_time:128630ms step_avg:60.76ms
step:2118/2315 train_time:128692ms step_avg:60.76ms
step:2119/2315 train_time:128754ms step_avg:60.76ms
step:2120/2315 train_time:128816ms step_avg:60.76ms
step:2121/2315 train_time:128877ms step_avg:60.76ms
step:2122/2315 train_time:128938ms step_avg:60.76ms
step:2123/2315 train_time:128999ms step_avg:60.76ms
step:2124/2315 train_time:129060ms step_avg:60.76ms
step:2125/2315 train_time:129122ms step_avg:60.76ms
step:2126/2315 train_time:129183ms step_avg:60.76ms
step:2127/2315 train_time:129244ms step_avg:60.76ms
step:2128/2315 train_time:129305ms step_avg:60.76ms
step:2129/2315 train_time:129366ms step_avg:60.76ms
step:2130/2315 train_time:129427ms step_avg:60.76ms
step:2131/2315 train_time:129488ms step_avg:60.76ms
step:2132/2315 train_time:129549ms step_avg:60.76ms
step:2133/2315 train_time:129610ms step_avg:60.76ms
step:2134/2315 train_time:129671ms step_avg:60.76ms
step:2135/2315 train_time:129732ms step_avg:60.76ms
step:2136/2315 train_time:129794ms step_avg:60.76ms
step:2137/2315 train_time:129856ms step_avg:60.77ms
step:2138/2315 train_time:129917ms step_avg:60.77ms
step:2139/2315 train_time:129978ms step_avg:60.77ms
step:2140/2315 train_time:130039ms step_avg:60.77ms
step:2141/2315 train_time:130100ms step_avg:60.77ms
step:2142/2315 train_time:130161ms step_avg:60.77ms
step:2143/2315 train_time:130223ms step_avg:60.77ms
step:2144/2315 train_time:130284ms step_avg:60.77ms
step:2145/2315 train_time:130345ms step_avg:60.77ms
step:2146/2315 train_time:130406ms step_avg:60.77ms
step:2147/2315 train_time:130467ms step_avg:60.77ms
step:2148/2315 train_time:130528ms step_avg:60.77ms
step:2149/2315 train_time:130589ms step_avg:60.77ms
step:2150/2315 train_time:130650ms step_avg:60.77ms
step:2151/2315 train_time:130712ms step_avg:60.77ms
step:2152/2315 train_time:130773ms step_avg:60.77ms
step:2153/2315 train_time:130835ms step_avg:60.77ms
step:2154/2315 train_time:130895ms step_avg:60.77ms
step:2155/2315 train_time:130957ms step_avg:60.77ms
step:2156/2315 train_time:131018ms step_avg:60.77ms
step:2157/2315 train_time:131079ms step_avg:60.77ms
step:2158/2315 train_time:131140ms step_avg:60.77ms
step:2159/2315 train_time:131202ms step_avg:60.77ms
step:2160/2315 train_time:131263ms step_avg:60.77ms
step:2161/2315 train_time:131324ms step_avg:60.77ms
step:2162/2315 train_time:131385ms step_avg:60.77ms
step:2163/2315 train_time:131447ms step_avg:60.77ms
step:2164/2315 train_time:131508ms step_avg:60.77ms
step:2165/2315 train_time:131568ms step_avg:60.77ms
step:2166/2315 train_time:131629ms step_avg:60.77ms
step:2167/2315 train_time:131691ms step_avg:60.77ms
step:2168/2315 train_time:131752ms step_avg:60.77ms
step:2169/2315 train_time:131813ms step_avg:60.77ms
step:2170/2315 train_time:131874ms step_avg:60.77ms
step:2171/2315 train_time:131936ms step_avg:60.77ms
step:2172/2315 train_time:131998ms step_avg:60.77ms
step:2173/2315 train_time:132059ms step_avg:60.77ms
step:2174/2315 train_time:132120ms step_avg:60.77ms
step:2175/2315 train_time:132182ms step_avg:60.77ms
step:2176/2315 train_time:132243ms step_avg:60.77ms
step:2177/2315 train_time:132304ms step_avg:60.77ms
step:2178/2315 train_time:132365ms step_avg:60.77ms
step:2179/2315 train_time:132426ms step_avg:60.77ms
step:2180/2315 train_time:132487ms step_avg:60.77ms
step:2181/2315 train_time:132548ms step_avg:60.77ms
step:2182/2315 train_time:132609ms step_avg:60.77ms
step:2183/2315 train_time:132670ms step_avg:60.77ms
step:2184/2315 train_time:132732ms step_avg:60.77ms
step:2185/2315 train_time:132793ms step_avg:60.77ms
step:2186/2315 train_time:132854ms step_avg:60.77ms
step:2187/2315 train_time:132916ms step_avg:60.78ms
step:2188/2315 train_time:132978ms step_avg:60.78ms
step:2189/2315 train_time:133039ms step_avg:60.78ms
step:2190/2315 train_time:133100ms step_avg:60.78ms
step:2191/2315 train_time:133162ms step_avg:60.78ms
step:2192/2315 train_time:133223ms step_avg:60.78ms
step:2193/2315 train_time:133284ms step_avg:60.78ms
step:2194/2315 train_time:133345ms step_avg:60.78ms
step:2195/2315 train_time:133406ms step_avg:60.78ms
step:2196/2315 train_time:133467ms step_avg:60.78ms
step:2197/2315 train_time:133528ms step_avg:60.78ms
step:2198/2315 train_time:133589ms step_avg:60.78ms
step:2199/2315 train_time:133650ms step_avg:60.78ms
step:2200/2315 train_time:133711ms step_avg:60.78ms
step:2201/2315 train_time:133773ms step_avg:60.78ms
step:2202/2315 train_time:133833ms step_avg:60.78ms
step:2203/2315 train_time:133895ms step_avg:60.78ms
step:2204/2315 train_time:133957ms step_avg:60.78ms
step:2205/2315 train_time:134018ms step_avg:60.78ms
step:2206/2315 train_time:134079ms step_avg:60.78ms
step:2207/2315 train_time:134140ms step_avg:60.78ms
step:2208/2315 train_time:134201ms step_avg:60.78ms
step:2209/2315 train_time:134263ms step_avg:60.78ms
step:2210/2315 train_time:134324ms step_avg:60.78ms
step:2211/2315 train_time:134385ms step_avg:60.78ms
step:2212/2315 train_time:134446ms step_avg:60.78ms
step:2213/2315 train_time:134507ms step_avg:60.78ms
step:2214/2315 train_time:134568ms step_avg:60.78ms
step:2215/2315 train_time:134630ms step_avg:60.78ms
step:2216/2315 train_time:134690ms step_avg:60.78ms
step:2217/2315 train_time:134751ms step_avg:60.78ms
step:2218/2315 train_time:134814ms step_avg:60.78ms
step:2219/2315 train_time:134875ms step_avg:60.78ms
step:2220/2315 train_time:134937ms step_avg:60.78ms
step:2221/2315 train_time:134998ms step_avg:60.78ms
step:2222/2315 train_time:135058ms step_avg:60.78ms
step:2223/2315 train_time:135120ms step_avg:60.78ms
step:2224/2315 train_time:135181ms step_avg:60.78ms
step:2225/2315 train_time:135242ms step_avg:60.78ms
step:2226/2315 train_time:135303ms step_avg:60.78ms
step:2227/2315 train_time:135364ms step_avg:60.78ms
step:2228/2315 train_time:135425ms step_avg:60.78ms
step:2229/2315 train_time:135487ms step_avg:60.78ms
step:2230/2315 train_time:135548ms step_avg:60.78ms
step:2231/2315 train_time:135609ms step_avg:60.78ms
step:2232/2315 train_time:135670ms step_avg:60.78ms
step:2233/2315 train_time:135731ms step_avg:60.78ms
step:2234/2315 train_time:135792ms step_avg:60.78ms
step:2235/2315 train_time:135854ms step_avg:60.78ms
step:2236/2315 train_time:135916ms step_avg:60.79ms
step:2237/2315 train_time:135978ms step_avg:60.79ms
step:2238/2315 train_time:136038ms step_avg:60.79ms
step:2239/2315 train_time:136099ms step_avg:60.79ms
step:2240/2315 train_time:136160ms step_avg:60.79ms
step:2241/2315 train_time:136222ms step_avg:60.79ms
step:2242/2315 train_time:136283ms step_avg:60.79ms
step:2243/2315 train_time:136344ms step_avg:60.79ms
step:2244/2315 train_time:136405ms step_avg:60.79ms
step:2245/2315 train_time:136466ms step_avg:60.79ms
step:2246/2315 train_time:136527ms step_avg:60.79ms
step:2247/2315 train_time:136588ms step_avg:60.79ms
step:2248/2315 train_time:136649ms step_avg:60.79ms
step:2249/2315 train_time:136710ms step_avg:60.79ms
step:2250/2315 train_time:136771ms step_avg:60.79ms
step:2250/2315 val_loss:3.2920 train_time:136834ms step_avg:60.82ms
step:2251/2315 train_time:136855ms step_avg:60.80ms
step:2252/2315 train_time:136896ms step_avg:60.79ms
step:2253/2315 train_time:136962ms step_avg:60.79ms
step:2254/2315 train_time:137026ms step_avg:60.79ms
step:2255/2315 train_time:137087ms step_avg:60.79ms
step:2256/2315 train_time:137149ms step_avg:60.79ms
step:2257/2315 train_time:137210ms step_avg:60.79ms
step:2258/2315 train_time:137271ms step_avg:60.79ms
step:2259/2315 train_time:137332ms step_avg:60.79ms
step:2260/2315 train_time:137392ms step_avg:60.79ms
step:2261/2315 train_time:137453ms step_avg:60.79ms
step:2262/2315 train_time:137514ms step_avg:60.79ms
step:2263/2315 train_time:137575ms step_avg:60.79ms
step:2264/2315 train_time:137636ms step_avg:60.79ms
step:2265/2315 train_time:137696ms step_avg:60.79ms
step:2266/2315 train_time:137757ms step_avg:60.79ms
step:2267/2315 train_time:137818ms step_avg:60.79ms
step:2268/2315 train_time:137880ms step_avg:60.79ms
step:2269/2315 train_time:137943ms step_avg:60.79ms
step:2270/2315 train_time:138005ms step_avg:60.79ms
step:2271/2315 train_time:138067ms step_avg:60.80ms
step:2272/2315 train_time:138129ms step_avg:60.80ms
step:2273/2315 train_time:138190ms step_avg:60.80ms
step:2274/2315 train_time:138251ms step_avg:60.80ms
step:2275/2315 train_time:138312ms step_avg:60.80ms
step:2276/2315 train_time:138373ms step_avg:60.80ms
step:2277/2315 train_time:138434ms step_avg:60.80ms
step:2278/2315 train_time:138495ms step_avg:60.80ms
step:2279/2315 train_time:138556ms step_avg:60.80ms
step:2280/2315 train_time:138617ms step_avg:60.80ms
step:2281/2315 train_time:138678ms step_avg:60.80ms
step:2282/2315 train_time:138738ms step_avg:60.80ms
step:2283/2315 train_time:138799ms step_avg:60.80ms
step:2284/2315 train_time:138861ms step_avg:60.80ms
step:2285/2315 train_time:138923ms step_avg:60.80ms
step:2286/2315 train_time:138984ms step_avg:60.80ms
step:2287/2315 train_time:139046ms step_avg:60.80ms
step:2288/2315 train_time:139107ms step_avg:60.80ms
step:2289/2315 train_time:139168ms step_avg:60.80ms
step:2290/2315 train_time:139229ms step_avg:60.80ms
step:2291/2315 train_time:139290ms step_avg:60.80ms
step:2292/2315 train_time:139351ms step_avg:60.80ms
step:2293/2315 train_time:139412ms step_avg:60.80ms
step:2294/2315 train_time:139473ms step_avg:60.80ms
step:2295/2315 train_time:139534ms step_avg:60.80ms
step:2296/2315 train_time:139595ms step_avg:60.80ms
step:2297/2315 train_time:139656ms step_avg:60.80ms
step:2298/2315 train_time:139717ms step_avg:60.80ms
step:2299/2315 train_time:139779ms step_avg:60.80ms
step:2300/2315 train_time:139840ms step_avg:60.80ms
step:2301/2315 train_time:139902ms step_avg:60.80ms
step:2302/2315 train_time:139963ms step_avg:60.80ms
step:2303/2315 train_time:140025ms step_avg:60.80ms
step:2304/2315 train_time:140085ms step_avg:60.80ms
step:2305/2315 train_time:140147ms step_avg:60.80ms
step:2306/2315 train_time:140208ms step_avg:60.80ms
step:2307/2315 train_time:140270ms step_avg:60.80ms
step:2308/2315 train_time:140331ms step_avg:60.80ms
step:2309/2315 train_time:140391ms step_avg:60.80ms
step:2310/2315 train_time:140452ms step_avg:60.80ms
step:2311/2315 train_time:140513ms step_avg:60.80ms
step:2312/2315 train_time:140574ms step_avg:60.80ms
step:2313/2315 train_time:140635ms step_avg:60.80ms
step:2314/2315 train_time:140696ms step_avg:60.80ms
step:2315/2315 train_time:140758ms step_avg:60.80ms
step:2315/2315 val_loss:3.2793 train_time:140820ms step_avg:60.83ms
peak memory allocated: 29512 MiB reserved: 44076 MiB

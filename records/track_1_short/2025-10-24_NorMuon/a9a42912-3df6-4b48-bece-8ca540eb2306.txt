import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]
# polar_express_coeffs = [
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
# ]



@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    # state["second_momentum_buffer"] = torch.zeros_like(grad[...,0:1]) if param.size(-2) >= param.size(-1) else torch.zeros_like(grad[...,0:1,:])
                    # state["second_momentum_buffer"] = torch.zeros((*grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((*grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)
            
            ###################################
            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))
            ####################################


            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> (-1.5)  0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Oct 22 2025, 02:20:45) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251022+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 24 06:40:58 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000001:00:00.0 Off |                    0 |
| N/A   36C    P0            118W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000002:00:00.0 Off |                    0 |
| N/A   35C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000003:00:00.0 Off |                    0 |
| N/A   30C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000008:00:00.0 Off |                    0 |
| N/A   31C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000009:00:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   0000000A:00:00.0 Off |                    0 |
| N/A   35C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   0000000C:00:00.0 Off |                    0 |
| N/A   34C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2315 train_time:89ms step_avg:88.87ms
step:2/2315 train_time:186ms step_avg:92.94ms
step:3/2315 train_time:207ms step_avg:68.96ms
step:4/2315 train_time:244ms step_avg:61.02ms
step:5/2315 train_time:302ms step_avg:60.45ms
step:6/2315 train_time:362ms step_avg:60.32ms
step:7/2315 train_time:421ms step_avg:60.17ms
step:8/2315 train_time:481ms step_avg:60.07ms
step:9/2315 train_time:540ms step_avg:60.04ms
step:10/2315 train_time:600ms step_avg:60.01ms
step:11/2315 train_time:660ms step_avg:60.02ms
step:12/2315 train_time:720ms step_avg:60.02ms
step:13/2315 train_time:780ms step_avg:59.99ms
step:14/2315 train_time:840ms step_avg:59.97ms
step:15/2315 train_time:899ms step_avg:59.95ms
step:16/2315 train_time:959ms step_avg:59.92ms
step:17/2315 train_time:1020ms step_avg:60.01ms
step:18/2315 train_time:1084ms step_avg:60.21ms
step:19/2315 train_time:1148ms step_avg:60.40ms
step:20/2315 train_time:1209ms step_avg:60.45ms
step:21/2315 train_time:1270ms step_avg:60.47ms
step:22/2315 train_time:1330ms step_avg:60.46ms
step:23/2315 train_time:1390ms step_avg:60.44ms
step:24/2315 train_time:1450ms step_avg:60.42ms
step:25/2315 train_time:1510ms step_avg:60.42ms
step:26/2315 train_time:1571ms step_avg:60.42ms
step:27/2315 train_time:1631ms step_avg:60.42ms
step:28/2315 train_time:1692ms step_avg:60.41ms
step:29/2315 train_time:1752ms step_avg:60.42ms
step:30/2315 train_time:1812ms step_avg:60.41ms
step:31/2315 train_time:1873ms step_avg:60.41ms
step:32/2315 train_time:1934ms step_avg:60.43ms
step:33/2315 train_time:1995ms step_avg:60.44ms
step:34/2315 train_time:2056ms step_avg:60.46ms
step:35/2315 train_time:2118ms step_avg:60.52ms
step:36/2315 train_time:2180ms step_avg:60.56ms
step:37/2315 train_time:2242ms step_avg:60.58ms
step:38/2315 train_time:2302ms step_avg:60.58ms
step:39/2315 train_time:2363ms step_avg:60.60ms
step:40/2315 train_time:2424ms step_avg:60.59ms
step:41/2315 train_time:2484ms step_avg:60.60ms
step:42/2315 train_time:2545ms step_avg:60.58ms
step:43/2315 train_time:2605ms step_avg:60.58ms
step:44/2315 train_time:2665ms step_avg:60.57ms
step:45/2315 train_time:2726ms step_avg:60.58ms
step:46/2315 train_time:2787ms step_avg:60.58ms
step:47/2315 train_time:2847ms step_avg:60.57ms
step:48/2315 train_time:2907ms step_avg:60.56ms
step:49/2315 train_time:2967ms step_avg:60.56ms
step:50/2315 train_time:3028ms step_avg:60.56ms
step:51/2315 train_time:3090ms step_avg:60.58ms
step:52/2315 train_time:3151ms step_avg:60.60ms
step:53/2315 train_time:3212ms step_avg:60.60ms
step:54/2315 train_time:3272ms step_avg:60.59ms
step:55/2315 train_time:3332ms step_avg:60.58ms
step:56/2315 train_time:3392ms step_avg:60.57ms
step:57/2315 train_time:3452ms step_avg:60.57ms
step:58/2315 train_time:3512ms step_avg:60.56ms
step:59/2315 train_time:3574ms step_avg:60.58ms
step:60/2315 train_time:3635ms step_avg:60.58ms
step:61/2315 train_time:3696ms step_avg:60.58ms
step:62/2315 train_time:3756ms step_avg:60.57ms
step:63/2315 train_time:3816ms step_avg:60.57ms
step:64/2315 train_time:3877ms step_avg:60.58ms
step:65/2315 train_time:3938ms step_avg:60.58ms
step:66/2315 train_time:3998ms step_avg:60.58ms
step:67/2315 train_time:4060ms step_avg:60.59ms
step:68/2315 train_time:4120ms step_avg:60.58ms
step:69/2315 train_time:4180ms step_avg:60.58ms
step:70/2315 train_time:4241ms step_avg:60.58ms
step:71/2315 train_time:4301ms step_avg:60.57ms
step:72/2315 train_time:4361ms step_avg:60.57ms
step:73/2315 train_time:4422ms step_avg:60.57ms
step:74/2315 train_time:4482ms step_avg:60.57ms
step:75/2315 train_time:4543ms step_avg:60.57ms
step:76/2315 train_time:4603ms step_avg:60.57ms
step:77/2315 train_time:4664ms step_avg:60.57ms
step:78/2315 train_time:4724ms step_avg:60.56ms
step:79/2315 train_time:4785ms step_avg:60.57ms
step:80/2315 train_time:4845ms step_avg:60.56ms
step:81/2315 train_time:4905ms step_avg:60.55ms
step:82/2315 train_time:4965ms step_avg:60.55ms
step:83/2315 train_time:5026ms step_avg:60.55ms
step:84/2315 train_time:5086ms step_avg:60.55ms
step:85/2315 train_time:5147ms step_avg:60.55ms
step:86/2315 train_time:5207ms step_avg:60.54ms
step:87/2315 train_time:5267ms step_avg:60.54ms
step:88/2315 train_time:5328ms step_avg:60.54ms
step:89/2315 train_time:5388ms step_avg:60.54ms
step:90/2315 train_time:5448ms step_avg:60.53ms
step:91/2315 train_time:5508ms step_avg:60.52ms
step:92/2315 train_time:5568ms step_avg:60.53ms
step:93/2315 train_time:5628ms step_avg:60.52ms
step:94/2315 train_time:5689ms step_avg:60.52ms
step:95/2315 train_time:5749ms step_avg:60.52ms
step:96/2315 train_time:5809ms step_avg:60.51ms
step:97/2315 train_time:5869ms step_avg:60.51ms
step:98/2315 train_time:5929ms step_avg:60.50ms
step:99/2315 train_time:5990ms step_avg:60.51ms
step:100/2315 train_time:6050ms step_avg:60.50ms
step:101/2315 train_time:6111ms step_avg:60.50ms
step:102/2315 train_time:6171ms step_avg:60.50ms
step:103/2315 train_time:6231ms step_avg:60.50ms
step:104/2315 train_time:6291ms step_avg:60.49ms
step:105/2315 train_time:6352ms step_avg:60.49ms
step:106/2315 train_time:6411ms step_avg:60.48ms
step:107/2315 train_time:6472ms step_avg:60.48ms
step:108/2315 train_time:6532ms step_avg:60.48ms
step:109/2315 train_time:6592ms step_avg:60.48ms
step:110/2315 train_time:6652ms step_avg:60.47ms
step:111/2315 train_time:6712ms step_avg:60.47ms
step:112/2315 train_time:6772ms step_avg:60.47ms
step:113/2315 train_time:6832ms step_avg:60.46ms
step:114/2315 train_time:6892ms step_avg:60.46ms
step:115/2315 train_time:6952ms step_avg:60.45ms
step:116/2315 train_time:7012ms step_avg:60.45ms
step:117/2315 train_time:7072ms step_avg:60.45ms
step:118/2315 train_time:7133ms step_avg:60.45ms
step:119/2315 train_time:7194ms step_avg:60.46ms
step:120/2315 train_time:7254ms step_avg:60.45ms
step:121/2315 train_time:7314ms step_avg:60.45ms
step:122/2315 train_time:7374ms step_avg:60.45ms
step:123/2315 train_time:7435ms step_avg:60.45ms
step:124/2315 train_time:7495ms step_avg:60.44ms
step:125/2315 train_time:7555ms step_avg:60.44ms
step:126/2315 train_time:7615ms step_avg:60.44ms
step:127/2315 train_time:7677ms step_avg:60.45ms
step:128/2315 train_time:7737ms step_avg:60.44ms
step:129/2315 train_time:7797ms step_avg:60.44ms
step:130/2315 train_time:7858ms step_avg:60.44ms
step:131/2315 train_time:7918ms step_avg:60.45ms
step:132/2315 train_time:7978ms step_avg:60.44ms
step:133/2315 train_time:8039ms step_avg:60.45ms
step:134/2315 train_time:8099ms step_avg:60.44ms
step:135/2315 train_time:8159ms step_avg:60.44ms
step:136/2315 train_time:8219ms step_avg:60.44ms
step:137/2315 train_time:8280ms step_avg:60.44ms
step:138/2315 train_time:8340ms step_avg:60.44ms
step:139/2315 train_time:8401ms step_avg:60.44ms
step:140/2315 train_time:8460ms step_avg:60.43ms
step:141/2315 train_time:8520ms step_avg:60.43ms
step:142/2315 train_time:8580ms step_avg:60.42ms
step:143/2315 train_time:8640ms step_avg:60.42ms
step:144/2315 train_time:8699ms step_avg:60.41ms
step:145/2315 train_time:8760ms step_avg:60.41ms
step:146/2315 train_time:8820ms step_avg:60.41ms
step:147/2315 train_time:8880ms step_avg:60.41ms
step:148/2315 train_time:8940ms step_avg:60.40ms
step:149/2315 train_time:9000ms step_avg:60.40ms
step:150/2315 train_time:9060ms step_avg:60.40ms
step:151/2315 train_time:9120ms step_avg:60.40ms
step:152/2315 train_time:9181ms step_avg:60.40ms
step:153/2315 train_time:9241ms step_avg:60.40ms
step:154/2315 train_time:9301ms step_avg:60.40ms
step:155/2315 train_time:9361ms step_avg:60.40ms
step:156/2315 train_time:9421ms step_avg:60.39ms
step:157/2315 train_time:9481ms step_avg:60.39ms
step:158/2315 train_time:9541ms step_avg:60.39ms
step:159/2315 train_time:9601ms step_avg:60.38ms
step:160/2315 train_time:9661ms step_avg:60.38ms
step:161/2315 train_time:9721ms step_avg:60.38ms
step:162/2315 train_time:9781ms step_avg:60.38ms
step:163/2315 train_time:9841ms step_avg:60.37ms
step:164/2315 train_time:9901ms step_avg:60.37ms
step:165/2315 train_time:9961ms step_avg:60.37ms
step:166/2315 train_time:10021ms step_avg:60.37ms
step:167/2315 train_time:10081ms step_avg:60.36ms
step:168/2315 train_time:10141ms step_avg:60.36ms
step:169/2315 train_time:10201ms step_avg:60.36ms
step:170/2315 train_time:10261ms step_avg:60.36ms
step:171/2315 train_time:10321ms step_avg:60.36ms
step:172/2315 train_time:10382ms step_avg:60.36ms
step:173/2315 train_time:10442ms step_avg:60.36ms
step:174/2315 train_time:10501ms step_avg:60.35ms
step:175/2315 train_time:10562ms step_avg:60.35ms
step:176/2315 train_time:10622ms step_avg:60.35ms
step:177/2315 train_time:10683ms step_avg:60.36ms
step:178/2315 train_time:10743ms step_avg:60.36ms
step:179/2315 train_time:10803ms step_avg:60.35ms
step:180/2315 train_time:10863ms step_avg:60.35ms
step:181/2315 train_time:10923ms step_avg:60.35ms
step:182/2315 train_time:10983ms step_avg:60.35ms
step:183/2315 train_time:11044ms step_avg:60.35ms
step:184/2315 train_time:11104ms step_avg:60.35ms
step:185/2315 train_time:11164ms step_avg:60.34ms
step:186/2315 train_time:11224ms step_avg:60.34ms
step:187/2315 train_time:11284ms step_avg:60.34ms
step:188/2315 train_time:11343ms step_avg:60.34ms
step:189/2315 train_time:11404ms step_avg:60.34ms
step:190/2315 train_time:11464ms step_avg:60.33ms
step:191/2315 train_time:11524ms step_avg:60.33ms
step:192/2315 train_time:11584ms step_avg:60.33ms
step:193/2315 train_time:11644ms step_avg:60.33ms
step:194/2315 train_time:11704ms step_avg:60.33ms
step:195/2315 train_time:11764ms step_avg:60.33ms
step:196/2315 train_time:11823ms step_avg:60.32ms
step:197/2315 train_time:11884ms step_avg:60.32ms
step:198/2315 train_time:11944ms step_avg:60.32ms
step:199/2315 train_time:12004ms step_avg:60.32ms
step:200/2315 train_time:12064ms step_avg:60.32ms
step:201/2315 train_time:12124ms step_avg:60.32ms
step:202/2315 train_time:12184ms step_avg:60.32ms
step:203/2315 train_time:12244ms step_avg:60.32ms
step:204/2315 train_time:12304ms step_avg:60.31ms
step:205/2315 train_time:12364ms step_avg:60.31ms
step:206/2315 train_time:12424ms step_avg:60.31ms
step:207/2315 train_time:12484ms step_avg:60.31ms
step:208/2315 train_time:12543ms step_avg:60.30ms
step:209/2315 train_time:12603ms step_avg:60.30ms
step:210/2315 train_time:12663ms step_avg:60.30ms
step:211/2315 train_time:12723ms step_avg:60.30ms
step:212/2315 train_time:12784ms step_avg:60.30ms
step:213/2315 train_time:12843ms step_avg:60.30ms
step:214/2315 train_time:12903ms step_avg:60.29ms
step:215/2315 train_time:12963ms step_avg:60.29ms
step:216/2315 train_time:13023ms step_avg:60.29ms
step:217/2315 train_time:13083ms step_avg:60.29ms
step:218/2315 train_time:13143ms step_avg:60.29ms
step:219/2315 train_time:13203ms step_avg:60.29ms
step:220/2315 train_time:13263ms step_avg:60.29ms
step:221/2315 train_time:13323ms step_avg:60.29ms
step:222/2315 train_time:13383ms step_avg:60.29ms
step:223/2315 train_time:13444ms step_avg:60.29ms
step:224/2315 train_time:13503ms step_avg:60.28ms
step:225/2315 train_time:13564ms step_avg:60.28ms
step:226/2315 train_time:13623ms step_avg:60.28ms
step:227/2315 train_time:13683ms step_avg:60.28ms
step:228/2315 train_time:13743ms step_avg:60.28ms
step:229/2315 train_time:13804ms step_avg:60.28ms
step:230/2315 train_time:13863ms step_avg:60.27ms
step:231/2315 train_time:13923ms step_avg:60.27ms
step:232/2315 train_time:13983ms step_avg:60.27ms
step:233/2315 train_time:14043ms step_avg:60.27ms
step:234/2315 train_time:14103ms step_avg:60.27ms
step:235/2315 train_time:14163ms step_avg:60.27ms
step:236/2315 train_time:14223ms step_avg:60.27ms
step:237/2315 train_time:14283ms step_avg:60.27ms
step:238/2315 train_time:14344ms step_avg:60.27ms
step:239/2315 train_time:14404ms step_avg:60.27ms
step:240/2315 train_time:14464ms step_avg:60.27ms
step:241/2315 train_time:14524ms step_avg:60.26ms
step:242/2315 train_time:14583ms step_avg:60.26ms
step:243/2315 train_time:14643ms step_avg:60.26ms
step:244/2315 train_time:14703ms step_avg:60.26ms
step:245/2315 train_time:14763ms step_avg:60.26ms
step:246/2315 train_time:14822ms step_avg:60.25ms
step:247/2315 train_time:14883ms step_avg:60.25ms
step:248/2315 train_time:14943ms step_avg:60.25ms
step:249/2315 train_time:15003ms step_avg:60.25ms
step:250/2315 train_time:15063ms step_avg:60.25ms
step:250/2315 val_loss:4.0711 train_time:15124ms step_avg:60.50ms
step:251/2315 train_time:15144ms step_avg:60.33ms
step:252/2315 train_time:15183ms step_avg:60.25ms
step:253/2315 train_time:15249ms step_avg:60.27ms
step:254/2315 train_time:15312ms step_avg:60.28ms
step:255/2315 train_time:15373ms step_avg:60.28ms
step:256/2315 train_time:15433ms step_avg:60.29ms
step:257/2315 train_time:15494ms step_avg:60.29ms
step:258/2315 train_time:15553ms step_avg:60.28ms
step:259/2315 train_time:15612ms step_avg:60.28ms
step:260/2315 train_time:15672ms step_avg:60.28ms
step:261/2315 train_time:15731ms step_avg:60.27ms
step:262/2315 train_time:15791ms step_avg:60.27ms
step:263/2315 train_time:15850ms step_avg:60.27ms
step:264/2315 train_time:15909ms step_avg:60.26ms
step:265/2315 train_time:15968ms step_avg:60.26ms
step:266/2315 train_time:16028ms step_avg:60.26ms
step:267/2315 train_time:16090ms step_avg:60.26ms
step:268/2315 train_time:16150ms step_avg:60.26ms
step:269/2315 train_time:16211ms step_avg:60.26ms
step:270/2315 train_time:16272ms step_avg:60.27ms
step:271/2315 train_time:16333ms step_avg:60.27ms
step:272/2315 train_time:16394ms step_avg:60.27ms
step:273/2315 train_time:16455ms step_avg:60.27ms
step:274/2315 train_time:16515ms step_avg:60.27ms
step:275/2315 train_time:16575ms step_avg:60.27ms
step:276/2315 train_time:16635ms step_avg:60.27ms
step:277/2315 train_time:16695ms step_avg:60.27ms
step:278/2315 train_time:16755ms step_avg:60.27ms
step:279/2315 train_time:16816ms step_avg:60.27ms
step:280/2315 train_time:16875ms step_avg:60.27ms
step:281/2315 train_time:16936ms step_avg:60.27ms
step:282/2315 train_time:16996ms step_avg:60.27ms
step:283/2315 train_time:17056ms step_avg:60.27ms
step:284/2315 train_time:17116ms step_avg:60.27ms
step:285/2315 train_time:17178ms step_avg:60.27ms
step:286/2315 train_time:17238ms step_avg:60.27ms
step:287/2315 train_time:17298ms step_avg:60.27ms
step:288/2315 train_time:17358ms step_avg:60.27ms
step:289/2315 train_time:17419ms step_avg:60.27ms
step:290/2315 train_time:17479ms step_avg:60.27ms
step:291/2315 train_time:17539ms step_avg:60.27ms
step:292/2315 train_time:17598ms step_avg:60.27ms
step:293/2315 train_time:17658ms step_avg:60.27ms
step:294/2315 train_time:17717ms step_avg:60.26ms
step:295/2315 train_time:17778ms step_avg:60.26ms
step:296/2315 train_time:17838ms step_avg:60.26ms
step:297/2315 train_time:17898ms step_avg:60.26ms
step:298/2315 train_time:17958ms step_avg:60.26ms
step:299/2315 train_time:18018ms step_avg:60.26ms
step:300/2315 train_time:18078ms step_avg:60.26ms
step:301/2315 train_time:18138ms step_avg:60.26ms
step:302/2315 train_time:18198ms step_avg:60.26ms
step:303/2315 train_time:18259ms step_avg:60.26ms
step:304/2315 train_time:18319ms step_avg:60.26ms
step:305/2315 train_time:18380ms step_avg:60.26ms
step:306/2315 train_time:18439ms step_avg:60.26ms
step:307/2315 train_time:18499ms step_avg:60.26ms
step:308/2315 train_time:18558ms step_avg:60.25ms
step:309/2315 train_time:18618ms step_avg:60.25ms
step:310/2315 train_time:18678ms step_avg:60.25ms
step:311/2315 train_time:18738ms step_avg:60.25ms
step:312/2315 train_time:18798ms step_avg:60.25ms
step:313/2315 train_time:18858ms step_avg:60.25ms
step:314/2315 train_time:18918ms step_avg:60.25ms
step:315/2315 train_time:18979ms step_avg:60.25ms
step:316/2315 train_time:19039ms step_avg:60.25ms
step:317/2315 train_time:19099ms step_avg:60.25ms
step:318/2315 train_time:19160ms step_avg:60.25ms
step:319/2315 train_time:19220ms step_avg:60.25ms
step:320/2315 train_time:19280ms step_avg:60.25ms
step:321/2315 train_time:19340ms step_avg:60.25ms
step:322/2315 train_time:19399ms step_avg:60.25ms
step:323/2315 train_time:19459ms step_avg:60.25ms
step:324/2315 train_time:19519ms step_avg:60.24ms
step:325/2315 train_time:19579ms step_avg:60.24ms
step:326/2315 train_time:19639ms step_avg:60.24ms
step:327/2315 train_time:19700ms step_avg:60.25ms
step:328/2315 train_time:19759ms step_avg:60.24ms
step:329/2315 train_time:19819ms step_avg:60.24ms
step:330/2315 train_time:19879ms step_avg:60.24ms
step:331/2315 train_time:19940ms step_avg:60.24ms
step:332/2315 train_time:19999ms step_avg:60.24ms
step:333/2315 train_time:20060ms step_avg:60.24ms
step:334/2315 train_time:20120ms step_avg:60.24ms
step:335/2315 train_time:20181ms step_avg:60.24ms
step:336/2315 train_time:20240ms step_avg:60.24ms
step:337/2315 train_time:20300ms step_avg:60.24ms
step:338/2315 train_time:20360ms step_avg:60.24ms
step:339/2315 train_time:20420ms step_avg:60.23ms
step:340/2315 train_time:20479ms step_avg:60.23ms
step:341/2315 train_time:20539ms step_avg:60.23ms
step:342/2315 train_time:20599ms step_avg:60.23ms
step:343/2315 train_time:20659ms step_avg:60.23ms
step:344/2315 train_time:20719ms step_avg:60.23ms
step:345/2315 train_time:20778ms step_avg:60.23ms
step:346/2315 train_time:20838ms step_avg:60.23ms
step:347/2315 train_time:20899ms step_avg:60.23ms
step:348/2315 train_time:20958ms step_avg:60.23ms
step:349/2315 train_time:21019ms step_avg:60.23ms
step:350/2315 train_time:21079ms step_avg:60.23ms
step:351/2315 train_time:21139ms step_avg:60.23ms
step:352/2315 train_time:21200ms step_avg:60.23ms
step:353/2315 train_time:21260ms step_avg:60.23ms
step:354/2315 train_time:21319ms step_avg:60.22ms
step:355/2315 train_time:21379ms step_avg:60.22ms
step:356/2315 train_time:21439ms step_avg:60.22ms
step:357/2315 train_time:21499ms step_avg:60.22ms
step:358/2315 train_time:21559ms step_avg:60.22ms
step:359/2315 train_time:21619ms step_avg:60.22ms
step:360/2315 train_time:21679ms step_avg:60.22ms
step:361/2315 train_time:21740ms step_avg:60.22ms
step:362/2315 train_time:21799ms step_avg:60.22ms
step:363/2315 train_time:21859ms step_avg:60.22ms
step:364/2315 train_time:21918ms step_avg:60.22ms
step:365/2315 train_time:21979ms step_avg:60.22ms
step:366/2315 train_time:22038ms step_avg:60.21ms
step:367/2315 train_time:22098ms step_avg:60.21ms
step:368/2315 train_time:22158ms step_avg:60.21ms
step:369/2315 train_time:22219ms step_avg:60.21ms
step:370/2315 train_time:22279ms step_avg:60.21ms
step:371/2315 train_time:22339ms step_avg:60.21ms
step:372/2315 train_time:22399ms step_avg:60.21ms
step:373/2315 train_time:22459ms step_avg:60.21ms
step:374/2315 train_time:22519ms step_avg:60.21ms
step:375/2315 train_time:22580ms step_avg:60.21ms
step:376/2315 train_time:22639ms step_avg:60.21ms
step:377/2315 train_time:22699ms step_avg:60.21ms
step:378/2315 train_time:22759ms step_avg:60.21ms
step:379/2315 train_time:22819ms step_avg:60.21ms
step:380/2315 train_time:22879ms step_avg:60.21ms
step:381/2315 train_time:22940ms step_avg:60.21ms
step:382/2315 train_time:23000ms step_avg:60.21ms
step:383/2315 train_time:23060ms step_avg:60.21ms
step:384/2315 train_time:23120ms step_avg:60.21ms
step:385/2315 train_time:23180ms step_avg:60.21ms
step:386/2315 train_time:23241ms step_avg:60.21ms
step:387/2315 train_time:23301ms step_avg:60.21ms
step:388/2315 train_time:23361ms step_avg:60.21ms
step:389/2315 train_time:23421ms step_avg:60.21ms
step:390/2315 train_time:23480ms step_avg:60.21ms
step:391/2315 train_time:23540ms step_avg:60.21ms
step:392/2315 train_time:23600ms step_avg:60.20ms
step:393/2315 train_time:23660ms step_avg:60.20ms
step:394/2315 train_time:23720ms step_avg:60.20ms
step:395/2315 train_time:23780ms step_avg:60.20ms
step:396/2315 train_time:23840ms step_avg:60.20ms
step:397/2315 train_time:23900ms step_avg:60.20ms
step:398/2315 train_time:23960ms step_avg:60.20ms
step:399/2315 train_time:24020ms step_avg:60.20ms
step:400/2315 train_time:24081ms step_avg:60.20ms
step:401/2315 train_time:24142ms step_avg:60.20ms
step:402/2315 train_time:24201ms step_avg:60.20ms
step:403/2315 train_time:24261ms step_avg:60.20ms
step:404/2315 train_time:24321ms step_avg:60.20ms
step:405/2315 train_time:24380ms step_avg:60.20ms
step:406/2315 train_time:24440ms step_avg:60.20ms
step:407/2315 train_time:24500ms step_avg:60.20ms
step:408/2315 train_time:24560ms step_avg:60.20ms
step:409/2315 train_time:24620ms step_avg:60.20ms
step:410/2315 train_time:24680ms step_avg:60.20ms
step:411/2315 train_time:24740ms step_avg:60.20ms
step:412/2315 train_time:24800ms step_avg:60.19ms
step:413/2315 train_time:24860ms step_avg:60.19ms
step:414/2315 train_time:24920ms step_avg:60.19ms
step:415/2315 train_time:24981ms step_avg:60.19ms
step:416/2315 train_time:25041ms step_avg:60.19ms
step:417/2315 train_time:25101ms step_avg:60.19ms
step:418/2315 train_time:25161ms step_avg:60.19ms
step:419/2315 train_time:25222ms step_avg:60.19ms
step:420/2315 train_time:25282ms step_avg:60.19ms
step:421/2315 train_time:25341ms step_avg:60.19ms
step:422/2315 train_time:25401ms step_avg:60.19ms
step:423/2315 train_time:25461ms step_avg:60.19ms
step:424/2315 train_time:25521ms step_avg:60.19ms
step:425/2315 train_time:25581ms step_avg:60.19ms
step:426/2315 train_time:25641ms step_avg:60.19ms
step:427/2315 train_time:25701ms step_avg:60.19ms
step:428/2315 train_time:25760ms step_avg:60.19ms
step:429/2315 train_time:25820ms step_avg:60.19ms
step:430/2315 train_time:25880ms step_avg:60.19ms
step:431/2315 train_time:25940ms step_avg:60.19ms
step:432/2315 train_time:26000ms step_avg:60.19ms
step:433/2315 train_time:26060ms step_avg:60.19ms
step:434/2315 train_time:26120ms step_avg:60.18ms
step:435/2315 train_time:26180ms step_avg:60.18ms
step:436/2315 train_time:26239ms step_avg:60.18ms
step:437/2315 train_time:26299ms step_avg:60.18ms
step:438/2315 train_time:26359ms step_avg:60.18ms
step:439/2315 train_time:26420ms step_avg:60.18ms
step:440/2315 train_time:26480ms step_avg:60.18ms
step:441/2315 train_time:26539ms step_avg:60.18ms
step:442/2315 train_time:26599ms step_avg:60.18ms
step:443/2315 train_time:26660ms step_avg:60.18ms
step:444/2315 train_time:26720ms step_avg:60.18ms
step:445/2315 train_time:26780ms step_avg:60.18ms
step:446/2315 train_time:26839ms step_avg:60.18ms
step:447/2315 train_time:26899ms step_avg:60.18ms
step:448/2315 train_time:26959ms step_avg:60.18ms
step:449/2315 train_time:27019ms step_avg:60.18ms
step:450/2315 train_time:27080ms step_avg:60.18ms
step:451/2315 train_time:27140ms step_avg:60.18ms
step:452/2315 train_time:27199ms step_avg:60.18ms
step:453/2315 train_time:27259ms step_avg:60.18ms
step:454/2315 train_time:27319ms step_avg:60.17ms
step:455/2315 train_time:27380ms step_avg:60.18ms
step:456/2315 train_time:27440ms step_avg:60.17ms
step:457/2315 train_time:27500ms step_avg:60.17ms
step:458/2315 train_time:27559ms step_avg:60.17ms
step:459/2315 train_time:27619ms step_avg:60.17ms
step:460/2315 train_time:27679ms step_avg:60.17ms
step:461/2315 train_time:27738ms step_avg:60.17ms
step:462/2315 train_time:27798ms step_avg:60.17ms
step:463/2315 train_time:27858ms step_avg:60.17ms
step:464/2315 train_time:27918ms step_avg:60.17ms
step:465/2315 train_time:27978ms step_avg:60.17ms
step:466/2315 train_time:28037ms step_avg:60.17ms
step:467/2315 train_time:28098ms step_avg:60.17ms
step:468/2315 train_time:28157ms step_avg:60.16ms
step:469/2315 train_time:28217ms step_avg:60.16ms
step:470/2315 train_time:28277ms step_avg:60.16ms
step:471/2315 train_time:28338ms step_avg:60.16ms
step:472/2315 train_time:28398ms step_avg:60.16ms
step:473/2315 train_time:28458ms step_avg:60.16ms
step:474/2315 train_time:28517ms step_avg:60.16ms
step:475/2315 train_time:28577ms step_avg:60.16ms
step:476/2315 train_time:28638ms step_avg:60.16ms
step:477/2315 train_time:28697ms step_avg:60.16ms
step:478/2315 train_time:28757ms step_avg:60.16ms
step:479/2315 train_time:28817ms step_avg:60.16ms
step:480/2315 train_time:28878ms step_avg:60.16ms
step:481/2315 train_time:28938ms step_avg:60.16ms
step:482/2315 train_time:28997ms step_avg:60.16ms
step:483/2315 train_time:29058ms step_avg:60.16ms
step:484/2315 train_time:29117ms step_avg:60.16ms
step:485/2315 train_time:29178ms step_avg:60.16ms
step:486/2315 train_time:29237ms step_avg:60.16ms
step:487/2315 train_time:29297ms step_avg:60.16ms
step:488/2315 train_time:29358ms step_avg:60.16ms
step:489/2315 train_time:29417ms step_avg:60.16ms
step:490/2315 train_time:29477ms step_avg:60.16ms
step:491/2315 train_time:29538ms step_avg:60.16ms
step:492/2315 train_time:29598ms step_avg:60.16ms
step:493/2315 train_time:29658ms step_avg:60.16ms
step:494/2315 train_time:29717ms step_avg:60.16ms
step:495/2315 train_time:29777ms step_avg:60.16ms
step:496/2315 train_time:29837ms step_avg:60.16ms
step:497/2315 train_time:29898ms step_avg:60.16ms
step:498/2315 train_time:29958ms step_avg:60.16ms
step:499/2315 train_time:30019ms step_avg:60.16ms
step:500/2315 train_time:30079ms step_avg:60.16ms
step:500/2315 val_loss:3.8088 train_time:30140ms step_avg:60.28ms
step:501/2315 train_time:30159ms step_avg:60.20ms
step:502/2315 train_time:30205ms step_avg:60.17ms
step:503/2315 train_time:30266ms step_avg:60.17ms
step:504/2315 train_time:30328ms step_avg:60.18ms
step:505/2315 train_time:30388ms step_avg:60.17ms
step:506/2315 train_time:30449ms step_avg:60.18ms
step:507/2315 train_time:30509ms step_avg:60.18ms
step:508/2315 train_time:30568ms step_avg:60.17ms
step:509/2315 train_time:30627ms step_avg:60.17ms
step:510/2315 train_time:30686ms step_avg:60.17ms
step:511/2315 train_time:30746ms step_avg:60.17ms
step:512/2315 train_time:30806ms step_avg:60.17ms
step:513/2315 train_time:30865ms step_avg:60.17ms
step:514/2315 train_time:30924ms step_avg:60.16ms
step:515/2315 train_time:30984ms step_avg:60.16ms
step:516/2315 train_time:31043ms step_avg:60.16ms
step:517/2315 train_time:31105ms step_avg:60.16ms
step:518/2315 train_time:31166ms step_avg:60.17ms
step:519/2315 train_time:31227ms step_avg:60.17ms
step:520/2315 train_time:31288ms step_avg:60.17ms
step:521/2315 train_time:31348ms step_avg:60.17ms
step:522/2315 train_time:31409ms step_avg:60.17ms
step:523/2315 train_time:31469ms step_avg:60.17ms
step:524/2315 train_time:31529ms step_avg:60.17ms
step:525/2315 train_time:31588ms step_avg:60.17ms
step:526/2315 train_time:31647ms step_avg:60.17ms
step:527/2315 train_time:31707ms step_avg:60.17ms
step:528/2315 train_time:31766ms step_avg:60.16ms
step:529/2315 train_time:31826ms step_avg:60.16ms
step:530/2315 train_time:31885ms step_avg:60.16ms
step:531/2315 train_time:31945ms step_avg:60.16ms
step:532/2315 train_time:32005ms step_avg:60.16ms
step:533/2315 train_time:32065ms step_avg:60.16ms
step:534/2315 train_time:32125ms step_avg:60.16ms
step:535/2315 train_time:32186ms step_avg:60.16ms
step:536/2315 train_time:32247ms step_avg:60.16ms
step:537/2315 train_time:32308ms step_avg:60.16ms
step:538/2315 train_time:32368ms step_avg:60.16ms
step:539/2315 train_time:32429ms step_avg:60.16ms
step:540/2315 train_time:32489ms step_avg:60.16ms
step:541/2315 train_time:32549ms step_avg:60.16ms
step:542/2315 train_time:32608ms step_avg:60.16ms
step:543/2315 train_time:32668ms step_avg:60.16ms
step:544/2315 train_time:32727ms step_avg:60.16ms
step:545/2315 train_time:32787ms step_avg:60.16ms
step:546/2315 train_time:32846ms step_avg:60.16ms
step:547/2315 train_time:32906ms step_avg:60.16ms
step:548/2315 train_time:32965ms step_avg:60.16ms
step:549/2315 train_time:33025ms step_avg:60.15ms
step:550/2315 train_time:33085ms step_avg:60.15ms
step:551/2315 train_time:33145ms step_avg:60.15ms
step:552/2315 train_time:33206ms step_avg:60.15ms
step:553/2315 train_time:33267ms step_avg:60.16ms
step:554/2315 train_time:33327ms step_avg:60.16ms
step:555/2315 train_time:33387ms step_avg:60.16ms
step:556/2315 train_time:33447ms step_avg:60.16ms
step:557/2315 train_time:33507ms step_avg:60.16ms
step:558/2315 train_time:33567ms step_avg:60.16ms
step:559/2315 train_time:33627ms step_avg:60.16ms
step:560/2315 train_time:33687ms step_avg:60.15ms
step:561/2315 train_time:33747ms step_avg:60.15ms
step:562/2315 train_time:33806ms step_avg:60.15ms
step:563/2315 train_time:33866ms step_avg:60.15ms
step:564/2315 train_time:33926ms step_avg:60.15ms
step:565/2315 train_time:33985ms step_avg:60.15ms
step:566/2315 train_time:34045ms step_avg:60.15ms
step:567/2315 train_time:34105ms step_avg:60.15ms
step:568/2315 train_time:34165ms step_avg:60.15ms
step:569/2315 train_time:34226ms step_avg:60.15ms
step:570/2315 train_time:34286ms step_avg:60.15ms
step:571/2315 train_time:34346ms step_avg:60.15ms
step:572/2315 train_time:34407ms step_avg:60.15ms
step:573/2315 train_time:34467ms step_avg:60.15ms
step:574/2315 train_time:34527ms step_avg:60.15ms
step:575/2315 train_time:34587ms step_avg:60.15ms
step:576/2315 train_time:34646ms step_avg:60.15ms
step:577/2315 train_time:34706ms step_avg:60.15ms
step:578/2315 train_time:34766ms step_avg:60.15ms
step:579/2315 train_time:34826ms step_avg:60.15ms
step:580/2315 train_time:34886ms step_avg:60.15ms
step:581/2315 train_time:34945ms step_avg:60.15ms
step:582/2315 train_time:35005ms step_avg:60.15ms
step:583/2315 train_time:35065ms step_avg:60.15ms
step:584/2315 train_time:35125ms step_avg:60.14ms
step:585/2315 train_time:35185ms step_avg:60.14ms
step:586/2315 train_time:35245ms step_avg:60.15ms
step:587/2315 train_time:35306ms step_avg:60.15ms
step:588/2315 train_time:35366ms step_avg:60.15ms
step:589/2315 train_time:35426ms step_avg:60.15ms
step:590/2315 train_time:35486ms step_avg:60.15ms
step:591/2315 train_time:35546ms step_avg:60.15ms
step:592/2315 train_time:35606ms step_avg:60.14ms
step:593/2315 train_time:35666ms step_avg:60.15ms
step:594/2315 train_time:35726ms step_avg:60.14ms
step:595/2315 train_time:35786ms step_avg:60.14ms
step:596/2315 train_time:35846ms step_avg:60.14ms
step:597/2315 train_time:35906ms step_avg:60.14ms
step:598/2315 train_time:35966ms step_avg:60.14ms
step:599/2315 train_time:36026ms step_avg:60.14ms
step:600/2315 train_time:36086ms step_avg:60.14ms
step:601/2315 train_time:36146ms step_avg:60.14ms
step:602/2315 train_time:36206ms step_avg:60.14ms
step:603/2315 train_time:36266ms step_avg:60.14ms
step:604/2315 train_time:36326ms step_avg:60.14ms
step:605/2315 train_time:36386ms step_avg:60.14ms
step:606/2315 train_time:36446ms step_avg:60.14ms
step:607/2315 train_time:36506ms step_avg:60.14ms
step:608/2315 train_time:36566ms step_avg:60.14ms
step:609/2315 train_time:36626ms step_avg:60.14ms
step:610/2315 train_time:36686ms step_avg:60.14ms
step:611/2315 train_time:36746ms step_avg:60.14ms
step:612/2315 train_time:36806ms step_avg:60.14ms
step:613/2315 train_time:36866ms step_avg:60.14ms
step:614/2315 train_time:36926ms step_avg:60.14ms
step:615/2315 train_time:36986ms step_avg:60.14ms
step:616/2315 train_time:37045ms step_avg:60.14ms
step:617/2315 train_time:37105ms step_avg:60.14ms
step:618/2315 train_time:37165ms step_avg:60.14ms
step:619/2315 train_time:37225ms step_avg:60.14ms
step:620/2315 train_time:37285ms step_avg:60.14ms
step:621/2315 train_time:37345ms step_avg:60.14ms
step:622/2315 train_time:37406ms step_avg:60.14ms
step:623/2315 train_time:37466ms step_avg:60.14ms
step:624/2315 train_time:37526ms step_avg:60.14ms
step:625/2315 train_time:37586ms step_avg:60.14ms
step:626/2315 train_time:37646ms step_avg:60.14ms
step:627/2315 train_time:37707ms step_avg:60.14ms
step:628/2315 train_time:37766ms step_avg:60.14ms
step:629/2315 train_time:37826ms step_avg:60.14ms
step:630/2315 train_time:37886ms step_avg:60.14ms
step:631/2315 train_time:37946ms step_avg:60.14ms
step:632/2315 train_time:38006ms step_avg:60.14ms
step:633/2315 train_time:38066ms step_avg:60.14ms
step:634/2315 train_time:38126ms step_avg:60.14ms
step:635/2315 train_time:38186ms step_avg:60.14ms
step:636/2315 train_time:38246ms step_avg:60.13ms
step:637/2315 train_time:38305ms step_avg:60.13ms
step:638/2315 train_time:38366ms step_avg:60.13ms
step:639/2315 train_time:38425ms step_avg:60.13ms
step:640/2315 train_time:38485ms step_avg:60.13ms
step:641/2315 train_time:38545ms step_avg:60.13ms
step:642/2315 train_time:38605ms step_avg:60.13ms
step:643/2315 train_time:38664ms step_avg:60.13ms
step:644/2315 train_time:38724ms step_avg:60.13ms
step:645/2315 train_time:38785ms step_avg:60.13ms
step:646/2315 train_time:38844ms step_avg:60.13ms
step:647/2315 train_time:38904ms step_avg:60.13ms
step:648/2315 train_time:38964ms step_avg:60.13ms
step:649/2315 train_time:39025ms step_avg:60.13ms
step:650/2315 train_time:39085ms step_avg:60.13ms
step:651/2315 train_time:39144ms step_avg:60.13ms
step:652/2315 train_time:39204ms step_avg:60.13ms
step:653/2315 train_time:39265ms step_avg:60.13ms
step:654/2315 train_time:39325ms step_avg:60.13ms
step:655/2315 train_time:39385ms step_avg:60.13ms
step:656/2315 train_time:39445ms step_avg:60.13ms
step:657/2315 train_time:39505ms step_avg:60.13ms
step:658/2315 train_time:39565ms step_avg:60.13ms
step:659/2315 train_time:39625ms step_avg:60.13ms
step:660/2315 train_time:39685ms step_avg:60.13ms
step:661/2315 train_time:39745ms step_avg:60.13ms
step:662/2315 train_time:39806ms step_avg:60.13ms
step:663/2315 train_time:39866ms step_avg:60.13ms
step:664/2315 train_time:39926ms step_avg:60.13ms
step:665/2315 train_time:39986ms step_avg:60.13ms
step:666/2315 train_time:40046ms step_avg:60.13ms
step:667/2315 train_time:40106ms step_avg:60.13ms
step:668/2315 train_time:40166ms step_avg:60.13ms
step:669/2315 train_time:40226ms step_avg:60.13ms
step:670/2315 train_time:40286ms step_avg:60.13ms
step:671/2315 train_time:40346ms step_avg:60.13ms
step:672/2315 train_time:40406ms step_avg:60.13ms
step:673/2315 train_time:40466ms step_avg:60.13ms
step:674/2315 train_time:40526ms step_avg:60.13ms
step:675/2315 train_time:40586ms step_avg:60.13ms
step:676/2315 train_time:40646ms step_avg:60.13ms
step:677/2315 train_time:40706ms step_avg:60.13ms
step:678/2315 train_time:40766ms step_avg:60.13ms
step:679/2315 train_time:40826ms step_avg:60.13ms
step:680/2315 train_time:40886ms step_avg:60.13ms
step:681/2315 train_time:40946ms step_avg:60.13ms
step:682/2315 train_time:41006ms step_avg:60.13ms
step:683/2315 train_time:41066ms step_avg:60.13ms
step:684/2315 train_time:41126ms step_avg:60.13ms
step:685/2315 train_time:41186ms step_avg:60.13ms
step:686/2315 train_time:41246ms step_avg:60.13ms
step:687/2315 train_time:41307ms step_avg:60.13ms
step:688/2315 train_time:41366ms step_avg:60.13ms
step:689/2315 train_time:41426ms step_avg:60.12ms
step:690/2315 train_time:41486ms step_avg:60.12ms
step:691/2315 train_time:41546ms step_avg:60.12ms
step:692/2315 train_time:41606ms step_avg:60.12ms
step:693/2315 train_time:41666ms step_avg:60.12ms
step:694/2315 train_time:41726ms step_avg:60.12ms
step:695/2315 train_time:41786ms step_avg:60.12ms
step:696/2315 train_time:41846ms step_avg:60.12ms
step:697/2315 train_time:41906ms step_avg:60.12ms
step:698/2315 train_time:41966ms step_avg:60.12ms
step:699/2315 train_time:42026ms step_avg:60.12ms
step:700/2315 train_time:42086ms step_avg:60.12ms
step:701/2315 train_time:42146ms step_avg:60.12ms
step:702/2315 train_time:42206ms step_avg:60.12ms
step:703/2315 train_time:42266ms step_avg:60.12ms
step:704/2315 train_time:42326ms step_avg:60.12ms
step:705/2315 train_time:42386ms step_avg:60.12ms
step:706/2315 train_time:42446ms step_avg:60.12ms
step:707/2315 train_time:42505ms step_avg:60.12ms
step:708/2315 train_time:42565ms step_avg:60.12ms
step:709/2315 train_time:42625ms step_avg:60.12ms
step:710/2315 train_time:42685ms step_avg:60.12ms
step:711/2315 train_time:42745ms step_avg:60.12ms
step:712/2315 train_time:42804ms step_avg:60.12ms
step:713/2315 train_time:42865ms step_avg:60.12ms
step:714/2315 train_time:42925ms step_avg:60.12ms
step:715/2315 train_time:42985ms step_avg:60.12ms
step:716/2315 train_time:43045ms step_avg:60.12ms
step:717/2315 train_time:43105ms step_avg:60.12ms
step:718/2315 train_time:43165ms step_avg:60.12ms
step:719/2315 train_time:43225ms step_avg:60.12ms
step:720/2315 train_time:43285ms step_avg:60.12ms
step:721/2315 train_time:43345ms step_avg:60.12ms
step:722/2315 train_time:43406ms step_avg:60.12ms
step:723/2315 train_time:43466ms step_avg:60.12ms
step:724/2315 train_time:43526ms step_avg:60.12ms
step:725/2315 train_time:43586ms step_avg:60.12ms
step:726/2315 train_time:43646ms step_avg:60.12ms
step:727/2315 train_time:43706ms step_avg:60.12ms
step:728/2315 train_time:43766ms step_avg:60.12ms
step:729/2315 train_time:43826ms step_avg:60.12ms
step:730/2315 train_time:43886ms step_avg:60.12ms
step:731/2315 train_time:43945ms step_avg:60.12ms
step:732/2315 train_time:44005ms step_avg:60.12ms
step:733/2315 train_time:44066ms step_avg:60.12ms
step:734/2315 train_time:44125ms step_avg:60.12ms
step:735/2315 train_time:44185ms step_avg:60.12ms
step:736/2315 train_time:44245ms step_avg:60.12ms
step:737/2315 train_time:44305ms step_avg:60.12ms
step:738/2315 train_time:44365ms step_avg:60.12ms
step:739/2315 train_time:44425ms step_avg:60.12ms
step:740/2315 train_time:44485ms step_avg:60.11ms
step:741/2315 train_time:44545ms step_avg:60.11ms
step:742/2315 train_time:44605ms step_avg:60.12ms
step:743/2315 train_time:44666ms step_avg:60.12ms
step:744/2315 train_time:44726ms step_avg:60.11ms
step:745/2315 train_time:44786ms step_avg:60.12ms
step:746/2315 train_time:44846ms step_avg:60.12ms
step:747/2315 train_time:44906ms step_avg:60.11ms
step:748/2315 train_time:44966ms step_avg:60.11ms
step:749/2315 train_time:45025ms step_avg:60.11ms
step:750/2315 train_time:45085ms step_avg:60.11ms
step:750/2315 val_loss:3.6811 train_time:45147ms step_avg:60.20ms
step:751/2315 train_time:45167ms step_avg:60.14ms
step:752/2315 train_time:45207ms step_avg:60.12ms
step:753/2315 train_time:45269ms step_avg:60.12ms
step:754/2315 train_time:45332ms step_avg:60.12ms
step:755/2315 train_time:45392ms step_avg:60.12ms
step:756/2315 train_time:45451ms step_avg:60.12ms
step:757/2315 train_time:45511ms step_avg:60.12ms
step:758/2315 train_time:45570ms step_avg:60.12ms
step:759/2315 train_time:45629ms step_avg:60.12ms
step:760/2315 train_time:45689ms step_avg:60.12ms
step:761/2315 train_time:45749ms step_avg:60.12ms
step:762/2315 train_time:45809ms step_avg:60.12ms
step:763/2315 train_time:45869ms step_avg:60.12ms
step:764/2315 train_time:45929ms step_avg:60.12ms
step:765/2315 train_time:45990ms step_avg:60.12ms
step:766/2315 train_time:46050ms step_avg:60.12ms
step:767/2315 train_time:46112ms step_avg:60.12ms
step:768/2315 train_time:46173ms step_avg:60.12ms
step:769/2315 train_time:46235ms step_avg:60.12ms
step:770/2315 train_time:46296ms step_avg:60.13ms
step:771/2315 train_time:46358ms step_avg:60.13ms
step:772/2315 train_time:46418ms step_avg:60.13ms
step:773/2315 train_time:46479ms step_avg:60.13ms
step:774/2315 train_time:46540ms step_avg:60.13ms
step:775/2315 train_time:46601ms step_avg:60.13ms
step:776/2315 train_time:46661ms step_avg:60.13ms
step:777/2315 train_time:46722ms step_avg:60.13ms
step:778/2315 train_time:46782ms step_avg:60.13ms
step:779/2315 train_time:46843ms step_avg:60.13ms
step:780/2315 train_time:46903ms step_avg:60.13ms
step:781/2315 train_time:46963ms step_avg:60.13ms
step:782/2315 train_time:47023ms step_avg:60.13ms
step:783/2315 train_time:47084ms step_avg:60.13ms
step:784/2315 train_time:47145ms step_avg:60.13ms
step:785/2315 train_time:47207ms step_avg:60.14ms
step:786/2315 train_time:47269ms step_avg:60.14ms
step:787/2315 train_time:47330ms step_avg:60.14ms
step:788/2315 train_time:47391ms step_avg:60.14ms
step:789/2315 train_time:47451ms step_avg:60.14ms
step:790/2315 train_time:47512ms step_avg:60.14ms
step:791/2315 train_time:47573ms step_avg:60.14ms
step:792/2315 train_time:47634ms step_avg:60.14ms
step:793/2315 train_time:47695ms step_avg:60.15ms
step:794/2315 train_time:47755ms step_avg:60.15ms
step:795/2315 train_time:47816ms step_avg:60.15ms
step:796/2315 train_time:47877ms step_avg:60.15ms
step:797/2315 train_time:47938ms step_avg:60.15ms
step:798/2315 train_time:47999ms step_avg:60.15ms
step:799/2315 train_time:48060ms step_avg:60.15ms
step:800/2315 train_time:48121ms step_avg:60.15ms
step:801/2315 train_time:48181ms step_avg:60.15ms
step:802/2315 train_time:48242ms step_avg:60.15ms
step:803/2315 train_time:48303ms step_avg:60.15ms
step:804/2315 train_time:48364ms step_avg:60.15ms
step:805/2315 train_time:48425ms step_avg:60.16ms
step:806/2315 train_time:48486ms step_avg:60.16ms
step:807/2315 train_time:48548ms step_avg:60.16ms
step:808/2315 train_time:48609ms step_avg:60.16ms
step:809/2315 train_time:48669ms step_avg:60.16ms
step:810/2315 train_time:48730ms step_avg:60.16ms
step:811/2315 train_time:48790ms step_avg:60.16ms
step:812/2315 train_time:48851ms step_avg:60.16ms
step:813/2315 train_time:48912ms step_avg:60.16ms
step:814/2315 train_time:48972ms step_avg:60.16ms
step:815/2315 train_time:49033ms step_avg:60.16ms
step:816/2315 train_time:49094ms step_avg:60.16ms
step:817/2315 train_time:49155ms step_avg:60.17ms
step:818/2315 train_time:49216ms step_avg:60.17ms
step:819/2315 train_time:49277ms step_avg:60.17ms
step:820/2315 train_time:49338ms step_avg:60.17ms
step:821/2315 train_time:49400ms step_avg:60.17ms
step:822/2315 train_time:49461ms step_avg:60.17ms
step:823/2315 train_time:49522ms step_avg:60.17ms
step:824/2315 train_time:49582ms step_avg:60.17ms
step:825/2315 train_time:49643ms step_avg:60.17ms
step:826/2315 train_time:49704ms step_avg:60.17ms
step:827/2315 train_time:49765ms step_avg:60.18ms
step:828/2315 train_time:49826ms step_avg:60.18ms
step:829/2315 train_time:49887ms step_avg:60.18ms
step:830/2315 train_time:49948ms step_avg:60.18ms
step:831/2315 train_time:50010ms step_avg:60.18ms
step:832/2315 train_time:50070ms step_avg:60.18ms
step:833/2315 train_time:50130ms step_avg:60.18ms
step:834/2315 train_time:50191ms step_avg:60.18ms
step:835/2315 train_time:50251ms step_avg:60.18ms
step:836/2315 train_time:50312ms step_avg:60.18ms
step:837/2315 train_time:50373ms step_avg:60.18ms
step:838/2315 train_time:50434ms step_avg:60.18ms
step:839/2315 train_time:50495ms step_avg:60.19ms
step:840/2315 train_time:50556ms step_avg:60.19ms
step:841/2315 train_time:50618ms step_avg:60.19ms
step:842/2315 train_time:50678ms step_avg:60.19ms
step:843/2315 train_time:50739ms step_avg:60.19ms
step:844/2315 train_time:50800ms step_avg:60.19ms
step:845/2315 train_time:50860ms step_avg:60.19ms
step:846/2315 train_time:50921ms step_avg:60.19ms
step:847/2315 train_time:50982ms step_avg:60.19ms
step:848/2315 train_time:51043ms step_avg:60.19ms
step:849/2315 train_time:51104ms step_avg:60.19ms
step:850/2315 train_time:51164ms step_avg:60.19ms
step:851/2315 train_time:51226ms step_avg:60.20ms
step:852/2315 train_time:51287ms step_avg:60.20ms
step:853/2315 train_time:51348ms step_avg:60.20ms
step:854/2315 train_time:51409ms step_avg:60.20ms
step:855/2315 train_time:51471ms step_avg:60.20ms
step:856/2315 train_time:51531ms step_avg:60.20ms
step:857/2315 train_time:51592ms step_avg:60.20ms
step:858/2315 train_time:51653ms step_avg:60.20ms
step:859/2315 train_time:51714ms step_avg:60.20ms
step:860/2315 train_time:51775ms step_avg:60.20ms
step:861/2315 train_time:51836ms step_avg:60.20ms
step:862/2315 train_time:51897ms step_avg:60.20ms
step:863/2315 train_time:51957ms step_avg:60.21ms
step:864/2315 train_time:52018ms step_avg:60.21ms
step:865/2315 train_time:52079ms step_avg:60.21ms
step:866/2315 train_time:52140ms step_avg:60.21ms
step:867/2315 train_time:52201ms step_avg:60.21ms
step:868/2315 train_time:52262ms step_avg:60.21ms
step:869/2315 train_time:52323ms step_avg:60.21ms
step:870/2315 train_time:52384ms step_avg:60.21ms
step:871/2315 train_time:52445ms step_avg:60.21ms
step:872/2315 train_time:52507ms step_avg:60.21ms
step:873/2315 train_time:52568ms step_avg:60.22ms
step:874/2315 train_time:52628ms step_avg:60.22ms
step:875/2315 train_time:52689ms step_avg:60.22ms
step:876/2315 train_time:52750ms step_avg:60.22ms
step:877/2315 train_time:52811ms step_avg:60.22ms
step:878/2315 train_time:52871ms step_avg:60.22ms
step:879/2315 train_time:52932ms step_avg:60.22ms
step:880/2315 train_time:52993ms step_avg:60.22ms
step:881/2315 train_time:53053ms step_avg:60.22ms
step:882/2315 train_time:53114ms step_avg:60.22ms
step:883/2315 train_time:53175ms step_avg:60.22ms
step:884/2315 train_time:53236ms step_avg:60.22ms
step:885/2315 train_time:53297ms step_avg:60.22ms
step:886/2315 train_time:53358ms step_avg:60.22ms
step:887/2315 train_time:53419ms step_avg:60.22ms
step:888/2315 train_time:53480ms step_avg:60.23ms
step:889/2315 train_time:53541ms step_avg:60.23ms
step:890/2315 train_time:53602ms step_avg:60.23ms
step:891/2315 train_time:53662ms step_avg:60.23ms
step:892/2315 train_time:53723ms step_avg:60.23ms
step:893/2315 train_time:53784ms step_avg:60.23ms
step:894/2315 train_time:53846ms step_avg:60.23ms
step:895/2315 train_time:53908ms step_avg:60.23ms
step:896/2315 train_time:53968ms step_avg:60.23ms
step:897/2315 train_time:54029ms step_avg:60.23ms
step:898/2315 train_time:54089ms step_avg:60.23ms
step:899/2315 train_time:54150ms step_avg:60.23ms
step:900/2315 train_time:54211ms step_avg:60.23ms
step:901/2315 train_time:54272ms step_avg:60.24ms
step:902/2315 train_time:54334ms step_avg:60.24ms
step:903/2315 train_time:54394ms step_avg:60.24ms
step:904/2315 train_time:54455ms step_avg:60.24ms
step:905/2315 train_time:54516ms step_avg:60.24ms
step:906/2315 train_time:54576ms step_avg:60.24ms
step:907/2315 train_time:54637ms step_avg:60.24ms
step:908/2315 train_time:54698ms step_avg:60.24ms
step:909/2315 train_time:54759ms step_avg:60.24ms
step:910/2315 train_time:54820ms step_avg:60.24ms
step:911/2315 train_time:54881ms step_avg:60.24ms
step:912/2315 train_time:54942ms step_avg:60.24ms
step:913/2315 train_time:55003ms step_avg:60.24ms
step:914/2315 train_time:55064ms step_avg:60.24ms
step:915/2315 train_time:55125ms step_avg:60.25ms
step:916/2315 train_time:55186ms step_avg:60.25ms
step:917/2315 train_time:55247ms step_avg:60.25ms
step:918/2315 train_time:55308ms step_avg:60.25ms
step:919/2315 train_time:55369ms step_avg:60.25ms
step:920/2315 train_time:55429ms step_avg:60.25ms
step:921/2315 train_time:55490ms step_avg:60.25ms
step:922/2315 train_time:55550ms step_avg:60.25ms
step:923/2315 train_time:55611ms step_avg:60.25ms
step:924/2315 train_time:55671ms step_avg:60.25ms
step:925/2315 train_time:55732ms step_avg:60.25ms
step:926/2315 train_time:55793ms step_avg:60.25ms
step:927/2315 train_time:55854ms step_avg:60.25ms
step:928/2315 train_time:55916ms step_avg:60.25ms
step:929/2315 train_time:55977ms step_avg:60.25ms
step:930/2315 train_time:56038ms step_avg:60.26ms
step:931/2315 train_time:56098ms step_avg:60.26ms
step:932/2315 train_time:56159ms step_avg:60.26ms
step:933/2315 train_time:56220ms step_avg:60.26ms
step:934/2315 train_time:56281ms step_avg:60.26ms
step:935/2315 train_time:56342ms step_avg:60.26ms
step:936/2315 train_time:56403ms step_avg:60.26ms
step:937/2315 train_time:56464ms step_avg:60.26ms
step:938/2315 train_time:56524ms step_avg:60.26ms
step:939/2315 train_time:56585ms step_avg:60.26ms
step:940/2315 train_time:56646ms step_avg:60.26ms
step:941/2315 train_time:56707ms step_avg:60.26ms
step:942/2315 train_time:56767ms step_avg:60.26ms
step:943/2315 train_time:56829ms step_avg:60.26ms
step:944/2315 train_time:56889ms step_avg:60.26ms
step:945/2315 train_time:56950ms step_avg:60.26ms
step:946/2315 train_time:57010ms step_avg:60.26ms
step:947/2315 train_time:57071ms step_avg:60.27ms
step:948/2315 train_time:57132ms step_avg:60.27ms
step:949/2315 train_time:57193ms step_avg:60.27ms
step:950/2315 train_time:57254ms step_avg:60.27ms
step:951/2315 train_time:57316ms step_avg:60.27ms
step:952/2315 train_time:57377ms step_avg:60.27ms
step:953/2315 train_time:57438ms step_avg:60.27ms
step:954/2315 train_time:57499ms step_avg:60.27ms
step:955/2315 train_time:57559ms step_avg:60.27ms
step:956/2315 train_time:57621ms step_avg:60.27ms
step:957/2315 train_time:57681ms step_avg:60.27ms
step:958/2315 train_time:57741ms step_avg:60.27ms
step:959/2315 train_time:57803ms step_avg:60.27ms
step:960/2315 train_time:57864ms step_avg:60.27ms
step:961/2315 train_time:57925ms step_avg:60.28ms
step:962/2315 train_time:57985ms step_avg:60.28ms
step:963/2315 train_time:58046ms step_avg:60.28ms
step:964/2315 train_time:58108ms step_avg:60.28ms
step:965/2315 train_time:58169ms step_avg:60.28ms
step:966/2315 train_time:58230ms step_avg:60.28ms
step:967/2315 train_time:58291ms step_avg:60.28ms
step:968/2315 train_time:58351ms step_avg:60.28ms
step:969/2315 train_time:58412ms step_avg:60.28ms
step:970/2315 train_time:58472ms step_avg:60.28ms
step:971/2315 train_time:58533ms step_avg:60.28ms
step:972/2315 train_time:58594ms step_avg:60.28ms
step:973/2315 train_time:58656ms step_avg:60.28ms
step:974/2315 train_time:58716ms step_avg:60.28ms
step:975/2315 train_time:58778ms step_avg:60.28ms
step:976/2315 train_time:58839ms step_avg:60.29ms
step:977/2315 train_time:58899ms step_avg:60.29ms
step:978/2315 train_time:58960ms step_avg:60.29ms
step:979/2315 train_time:59021ms step_avg:60.29ms
step:980/2315 train_time:59082ms step_avg:60.29ms
step:981/2315 train_time:59143ms step_avg:60.29ms
step:982/2315 train_time:59205ms step_avg:60.29ms
step:983/2315 train_time:59266ms step_avg:60.29ms
step:984/2315 train_time:59327ms step_avg:60.29ms
step:985/2315 train_time:59388ms step_avg:60.29ms
step:986/2315 train_time:59448ms step_avg:60.29ms
step:987/2315 train_time:59510ms step_avg:60.29ms
step:988/2315 train_time:59570ms step_avg:60.29ms
step:989/2315 train_time:59631ms step_avg:60.29ms
step:990/2315 train_time:59691ms step_avg:60.29ms
step:991/2315 train_time:59752ms step_avg:60.29ms
step:992/2315 train_time:59813ms step_avg:60.29ms
step:993/2315 train_time:59874ms step_avg:60.30ms
step:994/2315 train_time:59936ms step_avg:60.30ms
step:995/2315 train_time:59997ms step_avg:60.30ms
step:996/2315 train_time:60058ms step_avg:60.30ms
step:997/2315 train_time:60119ms step_avg:60.30ms
step:998/2315 train_time:60179ms step_avg:60.30ms
step:999/2315 train_time:60240ms step_avg:60.30ms
step:1000/2315 train_time:60301ms step_avg:60.30ms
step:1000/2315 val_loss:3.5705 train_time:60364ms step_avg:60.36ms
step:1001/2315 train_time:60383ms step_avg:60.32ms
step:1002/2315 train_time:60424ms step_avg:60.30ms
step:1003/2315 train_time:60490ms step_avg:60.31ms
step:1004/2315 train_time:60560ms step_avg:60.32ms
step:1005/2315 train_time:60620ms step_avg:60.32ms
step:1006/2315 train_time:60680ms step_avg:60.32ms
step:1007/2315 train_time:60740ms step_avg:60.32ms
step:1008/2315 train_time:60800ms step_avg:60.32ms
step:1009/2315 train_time:60861ms step_avg:60.32ms
step:1010/2315 train_time:60921ms step_avg:60.32ms
step:1011/2315 train_time:60982ms step_avg:60.32ms
step:1012/2315 train_time:61042ms step_avg:60.32ms
step:1013/2315 train_time:61102ms step_avg:60.32ms
step:1014/2315 train_time:61162ms step_avg:60.32ms
step:1015/2315 train_time:61222ms step_avg:60.32ms
step:1016/2315 train_time:61283ms step_avg:60.32ms
step:1017/2315 train_time:61344ms step_avg:60.32ms
step:1018/2315 train_time:61405ms step_avg:60.32ms
step:1019/2315 train_time:61469ms step_avg:60.32ms
step:1020/2315 train_time:61531ms step_avg:60.32ms
step:1021/2315 train_time:61594ms step_avg:60.33ms
step:1022/2315 train_time:61654ms step_avg:60.33ms
step:1023/2315 train_time:61715ms step_avg:60.33ms
step:1024/2315 train_time:61775ms step_avg:60.33ms
step:1025/2315 train_time:61836ms step_avg:60.33ms
step:1026/2315 train_time:61896ms step_avg:60.33ms
step:1027/2315 train_time:61956ms step_avg:60.33ms
step:1028/2315 train_time:62017ms step_avg:60.33ms
step:1029/2315 train_time:62077ms step_avg:60.33ms
step:1030/2315 train_time:62137ms step_avg:60.33ms
step:1031/2315 train_time:62198ms step_avg:60.33ms
step:1032/2315 train_time:62259ms step_avg:60.33ms
step:1033/2315 train_time:62320ms step_avg:60.33ms
step:1034/2315 train_time:62382ms step_avg:60.33ms
step:1035/2315 train_time:62443ms step_avg:60.33ms
step:1036/2315 train_time:62505ms step_avg:60.33ms
step:1037/2315 train_time:62567ms step_avg:60.33ms
step:1038/2315 train_time:62628ms step_avg:60.33ms
step:1039/2315 train_time:62689ms step_avg:60.34ms
step:1040/2315 train_time:62750ms step_avg:60.34ms
step:1041/2315 train_time:62811ms step_avg:60.34ms
step:1042/2315 train_time:62871ms step_avg:60.34ms
step:1043/2315 train_time:62932ms step_avg:60.34ms
step:1044/2315 train_time:62993ms step_avg:60.34ms
step:1045/2315 train_time:63053ms step_avg:60.34ms
step:1046/2315 train_time:63114ms step_avg:60.34ms
step:1047/2315 train_time:63175ms step_avg:60.34ms
step:1048/2315 train_time:63235ms step_avg:60.34ms
step:1049/2315 train_time:63296ms step_avg:60.34ms
step:1050/2315 train_time:63357ms step_avg:60.34ms
step:1051/2315 train_time:63418ms step_avg:60.34ms
step:1052/2315 train_time:63479ms step_avg:60.34ms
step:1053/2315 train_time:63541ms step_avg:60.34ms
step:1054/2315 train_time:63602ms step_avg:60.34ms
step:1055/2315 train_time:63663ms step_avg:60.34ms
step:1056/2315 train_time:63724ms step_avg:60.34ms
step:1057/2315 train_time:63785ms step_avg:60.35ms
step:1058/2315 train_time:63845ms step_avg:60.35ms
step:1059/2315 train_time:63906ms step_avg:60.35ms
step:1060/2315 train_time:63966ms step_avg:60.35ms
step:1061/2315 train_time:64027ms step_avg:60.35ms
step:1062/2315 train_time:64088ms step_avg:60.35ms
step:1063/2315 train_time:64149ms step_avg:60.35ms
step:1064/2315 train_time:64210ms step_avg:60.35ms
step:1065/2315 train_time:64271ms step_avg:60.35ms
step:1066/2315 train_time:64333ms step_avg:60.35ms
step:1067/2315 train_time:64394ms step_avg:60.35ms
step:1068/2315 train_time:64454ms step_avg:60.35ms
step:1069/2315 train_time:64516ms step_avg:60.35ms
step:1070/2315 train_time:64577ms step_avg:60.35ms
step:1071/2315 train_time:64639ms step_avg:60.35ms
step:1072/2315 train_time:64700ms step_avg:60.35ms
step:1073/2315 train_time:64761ms step_avg:60.35ms
step:1074/2315 train_time:64821ms step_avg:60.35ms
step:1075/2315 train_time:64882ms step_avg:60.36ms
step:1076/2315 train_time:64942ms step_avg:60.36ms
step:1077/2315 train_time:65003ms step_avg:60.36ms
step:1078/2315 train_time:65063ms step_avg:60.36ms
step:1079/2315 train_time:65124ms step_avg:60.36ms
step:1080/2315 train_time:65185ms step_avg:60.36ms
step:1081/2315 train_time:65246ms step_avg:60.36ms
step:1082/2315 train_time:65307ms step_avg:60.36ms
step:1083/2315 train_time:65368ms step_avg:60.36ms
step:1084/2315 train_time:65429ms step_avg:60.36ms
step:1085/2315 train_time:65490ms step_avg:60.36ms
step:1086/2315 train_time:65551ms step_avg:60.36ms
step:1087/2315 train_time:65612ms step_avg:60.36ms
step:1088/2315 train_time:65673ms step_avg:60.36ms
step:1089/2315 train_time:65734ms step_avg:60.36ms
step:1090/2315 train_time:65795ms step_avg:60.36ms
step:1091/2315 train_time:65856ms step_avg:60.36ms
step:1092/2315 train_time:65916ms step_avg:60.36ms
step:1093/2315 train_time:65977ms step_avg:60.36ms
step:1094/2315 train_time:66038ms step_avg:60.36ms
step:1095/2315 train_time:66099ms step_avg:60.36ms
step:1096/2315 train_time:66160ms step_avg:60.36ms
step:1097/2315 train_time:66221ms step_avg:60.37ms
step:1098/2315 train_time:66282ms step_avg:60.37ms
step:1099/2315 train_time:66343ms step_avg:60.37ms
step:1100/2315 train_time:66404ms step_avg:60.37ms
step:1101/2315 train_time:66465ms step_avg:60.37ms
step:1102/2315 train_time:66526ms step_avg:60.37ms
step:1103/2315 train_time:66587ms step_avg:60.37ms
step:1104/2315 train_time:66647ms step_avg:60.37ms
step:1105/2315 train_time:66709ms step_avg:60.37ms
step:1106/2315 train_time:66769ms step_avg:60.37ms
step:1107/2315 train_time:66830ms step_avg:60.37ms
step:1108/2315 train_time:66892ms step_avg:60.37ms
step:1109/2315 train_time:66953ms step_avg:60.37ms
step:1110/2315 train_time:67014ms step_avg:60.37ms
step:1111/2315 train_time:67075ms step_avg:60.37ms
step:1112/2315 train_time:67136ms step_avg:60.37ms
step:1113/2315 train_time:67198ms step_avg:60.38ms
step:1114/2315 train_time:67258ms step_avg:60.38ms
step:1115/2315 train_time:67320ms step_avg:60.38ms
step:1116/2315 train_time:67381ms step_avg:60.38ms
step:1117/2315 train_time:67442ms step_avg:60.38ms
step:1118/2315 train_time:67503ms step_avg:60.38ms
step:1119/2315 train_time:67563ms step_avg:60.38ms
step:1120/2315 train_time:67625ms step_avg:60.38ms
step:1121/2315 train_time:67686ms step_avg:60.38ms
step:1122/2315 train_time:67747ms step_avg:60.38ms
step:1123/2315 train_time:67808ms step_avg:60.38ms
step:1124/2315 train_time:67869ms step_avg:60.38ms
step:1125/2315 train_time:67930ms step_avg:60.38ms
step:1126/2315 train_time:67991ms step_avg:60.38ms
step:1127/2315 train_time:68052ms step_avg:60.38ms
step:1128/2315 train_time:68113ms step_avg:60.38ms
step:1129/2315 train_time:68174ms step_avg:60.38ms
step:1130/2315 train_time:68235ms step_avg:60.38ms
step:1131/2315 train_time:68296ms step_avg:60.39ms
step:1132/2315 train_time:68357ms step_avg:60.39ms
step:1133/2315 train_time:68418ms step_avg:60.39ms
step:1134/2315 train_time:68480ms step_avg:60.39ms
step:1135/2315 train_time:68541ms step_avg:60.39ms
step:1136/2315 train_time:68602ms step_avg:60.39ms
step:1137/2315 train_time:68663ms step_avg:60.39ms
step:1138/2315 train_time:68723ms step_avg:60.39ms
step:1139/2315 train_time:68784ms step_avg:60.39ms
step:1140/2315 train_time:68845ms step_avg:60.39ms
step:1141/2315 train_time:68906ms step_avg:60.39ms
step:1142/2315 train_time:68967ms step_avg:60.39ms
step:1143/2315 train_time:69029ms step_avg:60.39ms
step:1144/2315 train_time:69090ms step_avg:60.39ms
step:1145/2315 train_time:69151ms step_avg:60.39ms
step:1146/2315 train_time:69212ms step_avg:60.39ms
step:1147/2315 train_time:69273ms step_avg:60.39ms
step:1148/2315 train_time:69335ms step_avg:60.40ms
step:1149/2315 train_time:69395ms step_avg:60.40ms
step:1150/2315 train_time:69456ms step_avg:60.40ms
step:1151/2315 train_time:69517ms step_avg:60.40ms
step:1152/2315 train_time:69578ms step_avg:60.40ms
step:1153/2315 train_time:69639ms step_avg:60.40ms
step:1154/2315 train_time:69700ms step_avg:60.40ms
step:1155/2315 train_time:69761ms step_avg:60.40ms
step:1156/2315 train_time:69822ms step_avg:60.40ms
step:1157/2315 train_time:69883ms step_avg:60.40ms
step:1158/2315 train_time:69943ms step_avg:60.40ms
step:1159/2315 train_time:70004ms step_avg:60.40ms
step:1160/2315 train_time:70065ms step_avg:60.40ms
step:1161/2315 train_time:70126ms step_avg:60.40ms
step:1162/2315 train_time:70187ms step_avg:60.40ms
step:1163/2315 train_time:70249ms step_avg:60.40ms
step:1164/2315 train_time:70309ms step_avg:60.40ms
step:1165/2315 train_time:70371ms step_avg:60.40ms
step:1166/2315 train_time:70431ms step_avg:60.40ms
step:1167/2315 train_time:70493ms step_avg:60.41ms
step:1168/2315 train_time:70554ms step_avg:60.41ms
step:1169/2315 train_time:70615ms step_avg:60.41ms
step:1170/2315 train_time:70675ms step_avg:60.41ms
step:1171/2315 train_time:70737ms step_avg:60.41ms
step:1172/2315 train_time:70798ms step_avg:60.41ms
step:1173/2315 train_time:70859ms step_avg:60.41ms
step:1174/2315 train_time:70920ms step_avg:60.41ms
step:1175/2315 train_time:70981ms step_avg:60.41ms
step:1176/2315 train_time:71042ms step_avg:60.41ms
step:1177/2315 train_time:71103ms step_avg:60.41ms
step:1178/2315 train_time:71164ms step_avg:60.41ms
step:1179/2315 train_time:71225ms step_avg:60.41ms
step:1180/2315 train_time:71286ms step_avg:60.41ms
step:1181/2315 train_time:71347ms step_avg:60.41ms
step:1182/2315 train_time:71408ms step_avg:60.41ms
step:1183/2315 train_time:71469ms step_avg:60.41ms
step:1184/2315 train_time:71530ms step_avg:60.41ms
step:1185/2315 train_time:71591ms step_avg:60.41ms
step:1186/2315 train_time:71651ms step_avg:60.41ms
step:1187/2315 train_time:71714ms step_avg:60.42ms
step:1188/2315 train_time:71774ms step_avg:60.42ms
step:1189/2315 train_time:71835ms step_avg:60.42ms
step:1190/2315 train_time:71896ms step_avg:60.42ms
step:1191/2315 train_time:71957ms step_avg:60.42ms
step:1192/2315 train_time:72017ms step_avg:60.42ms
step:1193/2315 train_time:72078ms step_avg:60.42ms
step:1194/2315 train_time:72140ms step_avg:60.42ms
step:1195/2315 train_time:72202ms step_avg:60.42ms
step:1196/2315 train_time:72262ms step_avg:60.42ms
step:1197/2315 train_time:72323ms step_avg:60.42ms
step:1198/2315 train_time:72384ms step_avg:60.42ms
step:1199/2315 train_time:72445ms step_avg:60.42ms
step:1200/2315 train_time:72505ms step_avg:60.42ms
step:1201/2315 train_time:72566ms step_avg:60.42ms
step:1202/2315 train_time:72627ms step_avg:60.42ms
step:1203/2315 train_time:72689ms step_avg:60.42ms
step:1204/2315 train_time:72749ms step_avg:60.42ms
step:1205/2315 train_time:72811ms step_avg:60.42ms
step:1206/2315 train_time:72872ms step_avg:60.42ms
step:1207/2315 train_time:72933ms step_avg:60.42ms
step:1208/2315 train_time:72993ms step_avg:60.42ms
step:1209/2315 train_time:73054ms step_avg:60.43ms
step:1210/2315 train_time:73115ms step_avg:60.43ms
step:1211/2315 train_time:73177ms step_avg:60.43ms
step:1212/2315 train_time:73238ms step_avg:60.43ms
step:1213/2315 train_time:73300ms step_avg:60.43ms
step:1214/2315 train_time:73361ms step_avg:60.43ms
step:1215/2315 train_time:73422ms step_avg:60.43ms
step:1216/2315 train_time:73483ms step_avg:60.43ms
step:1217/2315 train_time:73543ms step_avg:60.43ms
step:1218/2315 train_time:73604ms step_avg:60.43ms
step:1219/2315 train_time:73665ms step_avg:60.43ms
step:1220/2315 train_time:73726ms step_avg:60.43ms
step:1221/2315 train_time:73787ms step_avg:60.43ms
step:1222/2315 train_time:73849ms step_avg:60.43ms
step:1223/2315 train_time:73910ms step_avg:60.43ms
step:1224/2315 train_time:73970ms step_avg:60.43ms
step:1225/2315 train_time:74032ms step_avg:60.43ms
step:1226/2315 train_time:74093ms step_avg:60.43ms
step:1227/2315 train_time:74154ms step_avg:60.44ms
step:1228/2315 train_time:74215ms step_avg:60.44ms
step:1229/2315 train_time:74276ms step_avg:60.44ms
step:1230/2315 train_time:74337ms step_avg:60.44ms
step:1231/2315 train_time:74398ms step_avg:60.44ms
step:1232/2315 train_time:74459ms step_avg:60.44ms
step:1233/2315 train_time:74521ms step_avg:60.44ms
step:1234/2315 train_time:74582ms step_avg:60.44ms
step:1235/2315 train_time:74643ms step_avg:60.44ms
step:1236/2315 train_time:74703ms step_avg:60.44ms
step:1237/2315 train_time:74763ms step_avg:60.44ms
step:1238/2315 train_time:74825ms step_avg:60.44ms
step:1239/2315 train_time:74886ms step_avg:60.44ms
step:1240/2315 train_time:74947ms step_avg:60.44ms
step:1241/2315 train_time:75009ms step_avg:60.44ms
step:1242/2315 train_time:75069ms step_avg:60.44ms
step:1243/2315 train_time:75130ms step_avg:60.44ms
step:1244/2315 train_time:75191ms step_avg:60.44ms
step:1245/2315 train_time:75252ms step_avg:60.44ms
step:1246/2315 train_time:75313ms step_avg:60.44ms
step:1247/2315 train_time:75374ms step_avg:60.44ms
step:1248/2315 train_time:75435ms step_avg:60.44ms
step:1249/2315 train_time:75496ms step_avg:60.44ms
step:1250/2315 train_time:75557ms step_avg:60.45ms
step:1250/2315 val_loss:3.5131 train_time:75620ms step_avg:60.50ms
step:1251/2315 train_time:75639ms step_avg:60.46ms
step:1252/2315 train_time:75682ms step_avg:60.45ms
step:1253/2315 train_time:75746ms step_avg:60.45ms
step:1254/2315 train_time:75809ms step_avg:60.45ms
step:1255/2315 train_time:75870ms step_avg:60.45ms
step:1256/2315 train_time:75932ms step_avg:60.46ms
step:1257/2315 train_time:75992ms step_avg:60.46ms
step:1258/2315 train_time:76053ms step_avg:60.46ms
step:1259/2315 train_time:76114ms step_avg:60.46ms
step:1260/2315 train_time:76174ms step_avg:60.46ms
step:1261/2315 train_time:76235ms step_avg:60.46ms
step:1262/2315 train_time:76295ms step_avg:60.46ms
step:1263/2315 train_time:76356ms step_avg:60.46ms
step:1264/2315 train_time:76415ms step_avg:60.46ms
step:1265/2315 train_time:76475ms step_avg:60.45ms
step:1266/2315 train_time:76535ms step_avg:60.45ms
step:1267/2315 train_time:76596ms step_avg:60.45ms
step:1268/2315 train_time:76658ms step_avg:60.46ms
step:1269/2315 train_time:76720ms step_avg:60.46ms
step:1270/2315 train_time:76782ms step_avg:60.46ms
step:1271/2315 train_time:76844ms step_avg:60.46ms
step:1272/2315 train_time:76905ms step_avg:60.46ms
step:1273/2315 train_time:76965ms step_avg:60.46ms
step:1274/2315 train_time:77026ms step_avg:60.46ms
step:1275/2315 train_time:77086ms step_avg:60.46ms
step:1276/2315 train_time:77147ms step_avg:60.46ms
step:1277/2315 train_time:77209ms step_avg:60.46ms
step:1278/2315 train_time:77269ms step_avg:60.46ms
step:1279/2315 train_time:77330ms step_avg:60.46ms
step:1280/2315 train_time:77390ms step_avg:60.46ms
step:1281/2315 train_time:77451ms step_avg:60.46ms
step:1282/2315 train_time:77511ms step_avg:60.46ms
step:1283/2315 train_time:77572ms step_avg:60.46ms
step:1284/2315 train_time:77632ms step_avg:60.46ms
step:1285/2315 train_time:77694ms step_avg:60.46ms
step:1286/2315 train_time:77756ms step_avg:60.46ms
step:1287/2315 train_time:77818ms step_avg:60.46ms
step:1288/2315 train_time:77879ms step_avg:60.46ms
step:1289/2315 train_time:77940ms step_avg:60.47ms
step:1290/2315 train_time:78000ms step_avg:60.47ms
step:1291/2315 train_time:78061ms step_avg:60.47ms
step:1292/2315 train_time:78121ms step_avg:60.47ms
step:1293/2315 train_time:78182ms step_avg:60.47ms
step:1294/2315 train_time:78243ms step_avg:60.47ms
step:1295/2315 train_time:78304ms step_avg:60.47ms
step:1296/2315 train_time:78365ms step_avg:60.47ms
step:1297/2315 train_time:78426ms step_avg:60.47ms
step:1298/2315 train_time:78487ms step_avg:60.47ms
step:1299/2315 train_time:78548ms step_avg:60.47ms
step:1300/2315 train_time:78609ms step_avg:60.47ms
step:1301/2315 train_time:78671ms step_avg:60.47ms
step:1302/2315 train_time:78731ms step_avg:60.47ms
step:1303/2315 train_time:78793ms step_avg:60.47ms
step:1304/2315 train_time:78853ms step_avg:60.47ms
step:1305/2315 train_time:78914ms step_avg:60.47ms
step:1306/2315 train_time:78975ms step_avg:60.47ms
step:1307/2315 train_time:79036ms step_avg:60.47ms
step:1308/2315 train_time:79097ms step_avg:60.47ms
step:1309/2315 train_time:79158ms step_avg:60.47ms
step:1310/2315 train_time:79219ms step_avg:60.47ms
step:1311/2315 train_time:79280ms step_avg:60.47ms
step:1312/2315 train_time:79341ms step_avg:60.47ms
step:1313/2315 train_time:79402ms step_avg:60.47ms
step:1314/2315 train_time:79462ms step_avg:60.47ms
step:1315/2315 train_time:79523ms step_avg:60.47ms
step:1316/2315 train_time:79584ms step_avg:60.47ms
step:1317/2315 train_time:79645ms step_avg:60.47ms
step:1318/2315 train_time:79706ms step_avg:60.47ms
step:1319/2315 train_time:79767ms step_avg:60.48ms
step:1320/2315 train_time:79827ms step_avg:60.48ms
step:1321/2315 train_time:79887ms step_avg:60.47ms
step:1322/2315 train_time:79949ms step_avg:60.48ms
step:1323/2315 train_time:80010ms step_avg:60.48ms
step:1324/2315 train_time:80071ms step_avg:60.48ms
step:1325/2315 train_time:80132ms step_avg:60.48ms
step:1326/2315 train_time:80192ms step_avg:60.48ms
step:1327/2315 train_time:80254ms step_avg:60.48ms
step:1328/2315 train_time:80315ms step_avg:60.48ms
step:1329/2315 train_time:80376ms step_avg:60.48ms
step:1330/2315 train_time:80437ms step_avg:60.48ms
step:1331/2315 train_time:80498ms step_avg:60.48ms
step:1332/2315 train_time:80558ms step_avg:60.48ms
step:1333/2315 train_time:80619ms step_avg:60.48ms
step:1334/2315 train_time:80680ms step_avg:60.48ms
step:1335/2315 train_time:80741ms step_avg:60.48ms
step:1336/2315 train_time:80802ms step_avg:60.48ms
step:1337/2315 train_time:80863ms step_avg:60.48ms
step:1338/2315 train_time:80924ms step_avg:60.48ms
step:1339/2315 train_time:80985ms step_avg:60.48ms
step:1340/2315 train_time:81046ms step_avg:60.48ms
step:1341/2315 train_time:81108ms step_avg:60.48ms
step:1342/2315 train_time:81169ms step_avg:60.48ms
step:1343/2315 train_time:81230ms step_avg:60.48ms
step:1344/2315 train_time:81291ms step_avg:60.48ms
step:1345/2315 train_time:81351ms step_avg:60.48ms
step:1346/2315 train_time:81412ms step_avg:60.48ms
step:1347/2315 train_time:81473ms step_avg:60.48ms
step:1348/2315 train_time:81534ms step_avg:60.48ms
step:1349/2315 train_time:81595ms step_avg:60.49ms
step:1350/2315 train_time:81655ms step_avg:60.49ms
step:1351/2315 train_time:81716ms step_avg:60.49ms
step:1352/2315 train_time:81777ms step_avg:60.49ms
step:1353/2315 train_time:81838ms step_avg:60.49ms
step:1354/2315 train_time:81899ms step_avg:60.49ms
step:1355/2315 train_time:81960ms step_avg:60.49ms
step:1356/2315 train_time:82021ms step_avg:60.49ms
step:1357/2315 train_time:82082ms step_avg:60.49ms
step:1358/2315 train_time:82143ms step_avg:60.49ms
step:1359/2315 train_time:82204ms step_avg:60.49ms
step:1360/2315 train_time:82264ms step_avg:60.49ms
step:1361/2315 train_time:82325ms step_avg:60.49ms
step:1362/2315 train_time:82387ms step_avg:60.49ms
step:1363/2315 train_time:82448ms step_avg:60.49ms
step:1364/2315 train_time:82509ms step_avg:60.49ms
step:1365/2315 train_time:82570ms step_avg:60.49ms
step:1366/2315 train_time:82630ms step_avg:60.49ms
step:1367/2315 train_time:82691ms step_avg:60.49ms
step:1368/2315 train_time:82752ms step_avg:60.49ms
step:1369/2315 train_time:82813ms step_avg:60.49ms
step:1370/2315 train_time:82873ms step_avg:60.49ms
step:1371/2315 train_time:82934ms step_avg:60.49ms
step:1372/2315 train_time:82996ms step_avg:60.49ms
step:1373/2315 train_time:83057ms step_avg:60.49ms
step:1374/2315 train_time:83118ms step_avg:60.49ms
step:1375/2315 train_time:83179ms step_avg:60.49ms
step:1376/2315 train_time:83240ms step_avg:60.49ms
step:1377/2315 train_time:83301ms step_avg:60.49ms
step:1378/2315 train_time:83362ms step_avg:60.49ms
step:1379/2315 train_time:83423ms step_avg:60.50ms
step:1380/2315 train_time:83483ms step_avg:60.50ms
step:1381/2315 train_time:83544ms step_avg:60.50ms
step:1382/2315 train_time:83605ms step_avg:60.50ms
step:1383/2315 train_time:83666ms step_avg:60.50ms
step:1384/2315 train_time:83727ms step_avg:60.50ms
step:1385/2315 train_time:83788ms step_avg:60.50ms
step:1386/2315 train_time:83849ms step_avg:60.50ms
step:1387/2315 train_time:83910ms step_avg:60.50ms
step:1388/2315 train_time:83971ms step_avg:60.50ms
step:1389/2315 train_time:84032ms step_avg:60.50ms
step:1390/2315 train_time:84092ms step_avg:60.50ms
step:1391/2315 train_time:84153ms step_avg:60.50ms
step:1392/2315 train_time:84214ms step_avg:60.50ms
step:1393/2315 train_time:84275ms step_avg:60.50ms
step:1394/2315 train_time:84336ms step_avg:60.50ms
step:1395/2315 train_time:84398ms step_avg:60.50ms
step:1396/2315 train_time:84459ms step_avg:60.50ms
step:1397/2315 train_time:84520ms step_avg:60.50ms
step:1398/2315 train_time:84581ms step_avg:60.50ms
step:1399/2315 train_time:84642ms step_avg:60.50ms
step:1400/2315 train_time:84702ms step_avg:60.50ms
step:1401/2315 train_time:84763ms step_avg:60.50ms
step:1402/2315 train_time:84824ms step_avg:60.50ms
step:1403/2315 train_time:84885ms step_avg:60.50ms
step:1404/2315 train_time:84946ms step_avg:60.50ms
step:1405/2315 train_time:85007ms step_avg:60.50ms
step:1406/2315 train_time:85068ms step_avg:60.50ms
step:1407/2315 train_time:85130ms step_avg:60.50ms
step:1408/2315 train_time:85190ms step_avg:60.50ms
step:1409/2315 train_time:85251ms step_avg:60.50ms
step:1410/2315 train_time:85312ms step_avg:60.50ms
step:1411/2315 train_time:85373ms step_avg:60.51ms
step:1412/2315 train_time:85434ms step_avg:60.51ms
step:1413/2315 train_time:85495ms step_avg:60.51ms
step:1414/2315 train_time:85556ms step_avg:60.51ms
step:1415/2315 train_time:85618ms step_avg:60.51ms
step:1416/2315 train_time:85679ms step_avg:60.51ms
step:1417/2315 train_time:85740ms step_avg:60.51ms
step:1418/2315 train_time:85800ms step_avg:60.51ms
step:1419/2315 train_time:85861ms step_avg:60.51ms
step:1420/2315 train_time:85922ms step_avg:60.51ms
step:1421/2315 train_time:85983ms step_avg:60.51ms
step:1422/2315 train_time:86043ms step_avg:60.51ms
step:1423/2315 train_time:86104ms step_avg:60.51ms
step:1424/2315 train_time:86166ms step_avg:60.51ms
step:1425/2315 train_time:86227ms step_avg:60.51ms
step:1426/2315 train_time:86288ms step_avg:60.51ms
step:1427/2315 train_time:86349ms step_avg:60.51ms
step:1428/2315 train_time:86410ms step_avg:60.51ms
step:1429/2315 train_time:86470ms step_avg:60.51ms
step:1430/2315 train_time:86531ms step_avg:60.51ms
step:1431/2315 train_time:86591ms step_avg:60.51ms
step:1432/2315 train_time:86652ms step_avg:60.51ms
step:1433/2315 train_time:86713ms step_avg:60.51ms
step:1434/2315 train_time:86774ms step_avg:60.51ms
step:1435/2315 train_time:86836ms step_avg:60.51ms
step:1436/2315 train_time:86896ms step_avg:60.51ms
step:1437/2315 train_time:86958ms step_avg:60.51ms
step:1438/2315 train_time:87019ms step_avg:60.51ms
step:1439/2315 train_time:87080ms step_avg:60.51ms
step:1440/2315 train_time:87141ms step_avg:60.51ms
step:1441/2315 train_time:87202ms step_avg:60.52ms
step:1442/2315 train_time:87262ms step_avg:60.51ms
step:1443/2315 train_time:87324ms step_avg:60.52ms
step:1444/2315 train_time:87385ms step_avg:60.52ms
step:1445/2315 train_time:87446ms step_avg:60.52ms
step:1446/2315 train_time:87507ms step_avg:60.52ms
step:1447/2315 train_time:87568ms step_avg:60.52ms
step:1448/2315 train_time:87630ms step_avg:60.52ms
step:1449/2315 train_time:87690ms step_avg:60.52ms
step:1450/2315 train_time:87750ms step_avg:60.52ms
step:1451/2315 train_time:87811ms step_avg:60.52ms
step:1452/2315 train_time:87872ms step_avg:60.52ms
step:1453/2315 train_time:87933ms step_avg:60.52ms
step:1454/2315 train_time:87994ms step_avg:60.52ms
step:1455/2315 train_time:88056ms step_avg:60.52ms
step:1456/2315 train_time:88116ms step_avg:60.52ms
step:1457/2315 train_time:88178ms step_avg:60.52ms
step:1458/2315 train_time:88239ms step_avg:60.52ms
step:1459/2315 train_time:88300ms step_avg:60.52ms
step:1460/2315 train_time:88361ms step_avg:60.52ms
step:1461/2315 train_time:88422ms step_avg:60.52ms
step:1462/2315 train_time:88482ms step_avg:60.52ms
step:1463/2315 train_time:88544ms step_avg:60.52ms
step:1464/2315 train_time:88605ms step_avg:60.52ms
step:1465/2315 train_time:88666ms step_avg:60.52ms
step:1466/2315 train_time:88727ms step_avg:60.52ms
step:1467/2315 train_time:88788ms step_avg:60.52ms
step:1468/2315 train_time:88849ms step_avg:60.52ms
step:1469/2315 train_time:88911ms step_avg:60.52ms
step:1470/2315 train_time:88971ms step_avg:60.52ms
step:1471/2315 train_time:89032ms step_avg:60.52ms
step:1472/2315 train_time:89092ms step_avg:60.52ms
step:1473/2315 train_time:89154ms step_avg:60.53ms
step:1474/2315 train_time:89215ms step_avg:60.53ms
step:1475/2315 train_time:89276ms step_avg:60.53ms
step:1476/2315 train_time:89337ms step_avg:60.53ms
step:1477/2315 train_time:89398ms step_avg:60.53ms
step:1478/2315 train_time:89459ms step_avg:60.53ms
step:1479/2315 train_time:89520ms step_avg:60.53ms
step:1480/2315 train_time:89581ms step_avg:60.53ms
step:1481/2315 train_time:89643ms step_avg:60.53ms
step:1482/2315 train_time:89703ms step_avg:60.53ms
step:1483/2315 train_time:89764ms step_avg:60.53ms
step:1484/2315 train_time:89825ms step_avg:60.53ms
step:1485/2315 train_time:89886ms step_avg:60.53ms
step:1486/2315 train_time:89947ms step_avg:60.53ms
step:1487/2315 train_time:90009ms step_avg:60.53ms
step:1488/2315 train_time:90069ms step_avg:60.53ms
step:1489/2315 train_time:90130ms step_avg:60.53ms
step:1490/2315 train_time:90190ms step_avg:60.53ms
step:1491/2315 train_time:90251ms step_avg:60.53ms
step:1492/2315 train_time:90312ms step_avg:60.53ms
step:1493/2315 train_time:90373ms step_avg:60.53ms
step:1494/2315 train_time:90433ms step_avg:60.53ms
step:1495/2315 train_time:90494ms step_avg:60.53ms
step:1496/2315 train_time:90556ms step_avg:60.53ms
step:1497/2315 train_time:90617ms step_avg:60.53ms
step:1498/2315 train_time:90678ms step_avg:60.53ms
step:1499/2315 train_time:90740ms step_avg:60.53ms
step:1500/2315 train_time:90800ms step_avg:60.53ms
step:1500/2315 val_loss:3.4517 train_time:90863ms step_avg:60.58ms
step:1501/2315 train_time:90881ms step_avg:60.55ms
step:1502/2315 train_time:90922ms step_avg:60.53ms
step:1503/2315 train_time:90987ms step_avg:60.54ms
step:1504/2315 train_time:91053ms step_avg:60.54ms
step:1505/2315 train_time:91115ms step_avg:60.54ms
step:1506/2315 train_time:91175ms step_avg:60.54ms
step:1507/2315 train_time:91236ms step_avg:60.54ms
step:1508/2315 train_time:91297ms step_avg:60.54ms
step:1509/2315 train_time:91357ms step_avg:60.54ms
step:1510/2315 train_time:91417ms step_avg:60.54ms
step:1511/2315 train_time:91477ms step_avg:60.54ms
step:1512/2315 train_time:91537ms step_avg:60.54ms
step:1513/2315 train_time:91597ms step_avg:60.54ms
step:1514/2315 train_time:91658ms step_avg:60.54ms
step:1515/2315 train_time:91717ms step_avg:60.54ms
step:1516/2315 train_time:91778ms step_avg:60.54ms
step:1517/2315 train_time:91840ms step_avg:60.54ms
step:1518/2315 train_time:91902ms step_avg:60.54ms
step:1519/2315 train_time:91965ms step_avg:60.54ms
step:1520/2315 train_time:92028ms step_avg:60.54ms
step:1521/2315 train_time:92090ms step_avg:60.55ms
step:1522/2315 train_time:92151ms step_avg:60.55ms
step:1523/2315 train_time:92213ms step_avg:60.55ms
step:1524/2315 train_time:92274ms step_avg:60.55ms
step:1525/2315 train_time:92335ms step_avg:60.55ms
step:1526/2315 train_time:92396ms step_avg:60.55ms
step:1527/2315 train_time:92457ms step_avg:60.55ms
step:1528/2315 train_time:92517ms step_avg:60.55ms
step:1529/2315 train_time:92578ms step_avg:60.55ms
step:1530/2315 train_time:92638ms step_avg:60.55ms
step:1531/2315 train_time:92699ms step_avg:60.55ms
step:1532/2315 train_time:92760ms step_avg:60.55ms
step:1533/2315 train_time:92822ms step_avg:60.55ms
step:1534/2315 train_time:92883ms step_avg:60.55ms
step:1535/2315 train_time:92945ms step_avg:60.55ms
step:1536/2315 train_time:93006ms step_avg:60.55ms
step:1537/2315 train_time:93068ms step_avg:60.55ms
step:1538/2315 train_time:93129ms step_avg:60.55ms
step:1539/2315 train_time:93190ms step_avg:60.55ms
step:1540/2315 train_time:93252ms step_avg:60.55ms
step:1541/2315 train_time:93313ms step_avg:60.55ms
step:1542/2315 train_time:93374ms step_avg:60.55ms
step:1543/2315 train_time:93436ms step_avg:60.55ms
step:1544/2315 train_time:93496ms step_avg:60.55ms
step:1545/2315 train_time:93557ms step_avg:60.55ms
step:1546/2315 train_time:93618ms step_avg:60.56ms
step:1547/2315 train_time:93679ms step_avg:60.56ms
step:1548/2315 train_time:93739ms step_avg:60.56ms
step:1549/2315 train_time:93800ms step_avg:60.56ms
step:1550/2315 train_time:93861ms step_avg:60.56ms
step:1551/2315 train_time:93923ms step_avg:60.56ms
step:1552/2315 train_time:93985ms step_avg:60.56ms
step:1553/2315 train_time:94047ms step_avg:60.56ms
step:1554/2315 train_time:94108ms step_avg:60.56ms
step:1555/2315 train_time:94170ms step_avg:60.56ms
step:1556/2315 train_time:94231ms step_avg:60.56ms
step:1557/2315 train_time:94293ms step_avg:60.56ms
step:1558/2315 train_time:94354ms step_avg:60.56ms
step:1559/2315 train_time:94415ms step_avg:60.56ms
step:1560/2315 train_time:94476ms step_avg:60.56ms
step:1561/2315 train_time:94537ms step_avg:60.56ms
step:1562/2315 train_time:94598ms step_avg:60.56ms
step:1563/2315 train_time:94659ms step_avg:60.56ms
step:1564/2315 train_time:94719ms step_avg:60.56ms
step:1565/2315 train_time:94780ms step_avg:60.56ms
step:1566/2315 train_time:94841ms step_avg:60.56ms
step:1567/2315 train_time:94902ms step_avg:60.56ms
step:1568/2315 train_time:94964ms step_avg:60.56ms
step:1569/2315 train_time:95027ms step_avg:60.57ms
step:1570/2315 train_time:95088ms step_avg:60.57ms
step:1571/2315 train_time:95150ms step_avg:60.57ms
step:1572/2315 train_time:95211ms step_avg:60.57ms
step:1573/2315 train_time:95273ms step_avg:60.57ms
step:1574/2315 train_time:95334ms step_avg:60.57ms
step:1575/2315 train_time:95395ms step_avg:60.57ms
step:1576/2315 train_time:95456ms step_avg:60.57ms
step:1577/2315 train_time:95517ms step_avg:60.57ms
step:1578/2315 train_time:95578ms step_avg:60.57ms
step:1579/2315 train_time:95639ms step_avg:60.57ms
step:1580/2315 train_time:95700ms step_avg:60.57ms
step:1581/2315 train_time:95761ms step_avg:60.57ms
step:1582/2315 train_time:95822ms step_avg:60.57ms
step:1583/2315 train_time:95883ms step_avg:60.57ms
step:1584/2315 train_time:95944ms step_avg:60.57ms
step:1585/2315 train_time:96005ms step_avg:60.57ms
step:1586/2315 train_time:96066ms step_avg:60.57ms
step:1587/2315 train_time:96128ms step_avg:60.57ms
step:1588/2315 train_time:96189ms step_avg:60.57ms
step:1589/2315 train_time:96251ms step_avg:60.57ms
step:1590/2315 train_time:96312ms step_avg:60.57ms
step:1591/2315 train_time:96373ms step_avg:60.57ms
step:1592/2315 train_time:96434ms step_avg:60.57ms
step:1593/2315 train_time:96496ms step_avg:60.57ms
step:1594/2315 train_time:96557ms step_avg:60.58ms
step:1595/2315 train_time:96618ms step_avg:60.58ms
step:1596/2315 train_time:96679ms step_avg:60.58ms
step:1597/2315 train_time:96740ms step_avg:60.58ms
step:1598/2315 train_time:96801ms step_avg:60.58ms
step:1599/2315 train_time:96862ms step_avg:60.58ms
step:1600/2315 train_time:96923ms step_avg:60.58ms
step:1601/2315 train_time:96984ms step_avg:60.58ms
step:1602/2315 train_time:97045ms step_avg:60.58ms
step:1603/2315 train_time:97107ms step_avg:60.58ms
step:1604/2315 train_time:97169ms step_avg:60.58ms
step:1605/2315 train_time:97231ms step_avg:60.58ms
step:1606/2315 train_time:97292ms step_avg:60.58ms
step:1607/2315 train_time:97353ms step_avg:60.58ms
step:1608/2315 train_time:97414ms step_avg:60.58ms
step:1609/2315 train_time:97475ms step_avg:60.58ms
step:1610/2315 train_time:97536ms step_avg:60.58ms
step:1611/2315 train_time:97597ms step_avg:60.58ms
step:1612/2315 train_time:97658ms step_avg:60.58ms
step:1613/2315 train_time:97720ms step_avg:60.58ms
step:1614/2315 train_time:97781ms step_avg:60.58ms
step:1615/2315 train_time:97843ms step_avg:60.58ms
step:1616/2315 train_time:97904ms step_avg:60.58ms
step:1617/2315 train_time:97965ms step_avg:60.58ms
step:1618/2315 train_time:98026ms step_avg:60.58ms
step:1619/2315 train_time:98087ms step_avg:60.58ms
step:1620/2315 train_time:98148ms step_avg:60.59ms
step:1621/2315 train_time:98209ms step_avg:60.59ms
step:1622/2315 train_time:98270ms step_avg:60.59ms
step:1623/2315 train_time:98332ms step_avg:60.59ms
step:1624/2315 train_time:98393ms step_avg:60.59ms
step:1625/2315 train_time:98454ms step_avg:60.59ms
step:1626/2315 train_time:98515ms step_avg:60.59ms
step:1627/2315 train_time:98576ms step_avg:60.59ms
step:1628/2315 train_time:98638ms step_avg:60.59ms
step:1629/2315 train_time:98698ms step_avg:60.59ms
step:1630/2315 train_time:98759ms step_avg:60.59ms
step:1631/2315 train_time:98821ms step_avg:60.59ms
step:1632/2315 train_time:98882ms step_avg:60.59ms
step:1633/2315 train_time:98943ms step_avg:60.59ms
step:1634/2315 train_time:99005ms step_avg:60.59ms
step:1635/2315 train_time:99067ms step_avg:60.59ms
step:1636/2315 train_time:99128ms step_avg:60.59ms
step:1637/2315 train_time:99189ms step_avg:60.59ms
step:1638/2315 train_time:99250ms step_avg:60.59ms
step:1639/2315 train_time:99312ms step_avg:60.59ms
step:1640/2315 train_time:99373ms step_avg:60.59ms
step:1641/2315 train_time:99434ms step_avg:60.59ms
step:1642/2315 train_time:99495ms step_avg:60.59ms
step:1643/2315 train_time:99556ms step_avg:60.59ms
step:1644/2315 train_time:99618ms step_avg:60.59ms
step:1645/2315 train_time:99679ms step_avg:60.60ms
step:1646/2315 train_time:99740ms step_avg:60.60ms
step:1647/2315 train_time:99801ms step_avg:60.60ms
step:1648/2315 train_time:99862ms step_avg:60.60ms
step:1649/2315 train_time:99923ms step_avg:60.60ms
step:1650/2315 train_time:99984ms step_avg:60.60ms
step:1651/2315 train_time:100045ms step_avg:60.60ms
step:1652/2315 train_time:100106ms step_avg:60.60ms
step:1653/2315 train_time:100168ms step_avg:60.60ms
step:1654/2315 train_time:100229ms step_avg:60.60ms
step:1655/2315 train_time:100290ms step_avg:60.60ms
step:1656/2315 train_time:100351ms step_avg:60.60ms
step:1657/2315 train_time:100413ms step_avg:60.60ms
step:1658/2315 train_time:100474ms step_avg:60.60ms
step:1659/2315 train_time:100535ms step_avg:60.60ms
step:1660/2315 train_time:100596ms step_avg:60.60ms
step:1661/2315 train_time:100658ms step_avg:60.60ms
step:1662/2315 train_time:100719ms step_avg:60.60ms
step:1663/2315 train_time:100781ms step_avg:60.60ms
step:1664/2315 train_time:100841ms step_avg:60.60ms
step:1665/2315 train_time:100903ms step_avg:60.60ms
step:1666/2315 train_time:100964ms step_avg:60.60ms
step:1667/2315 train_time:101025ms step_avg:60.60ms
step:1668/2315 train_time:101086ms step_avg:60.60ms
step:1669/2315 train_time:101148ms step_avg:60.60ms
step:1670/2315 train_time:101208ms step_avg:60.60ms
step:1671/2315 train_time:101270ms step_avg:60.60ms
step:1672/2315 train_time:101331ms step_avg:60.60ms
step:1673/2315 train_time:101392ms step_avg:60.61ms
step:1674/2315 train_time:101453ms step_avg:60.61ms
step:1675/2315 train_time:101515ms step_avg:60.61ms
step:1676/2315 train_time:101576ms step_avg:60.61ms
step:1677/2315 train_time:101637ms step_avg:60.61ms
step:1678/2315 train_time:101698ms step_avg:60.61ms
step:1679/2315 train_time:101759ms step_avg:60.61ms
step:1680/2315 train_time:101820ms step_avg:60.61ms
step:1681/2315 train_time:101882ms step_avg:60.61ms
step:1682/2315 train_time:101943ms step_avg:60.61ms
step:1683/2315 train_time:102004ms step_avg:60.61ms
step:1684/2315 train_time:102065ms step_avg:60.61ms
step:1685/2315 train_time:102126ms step_avg:60.61ms
step:1686/2315 train_time:102187ms step_avg:60.61ms
step:1687/2315 train_time:102249ms step_avg:60.61ms
step:1688/2315 train_time:102310ms step_avg:60.61ms
step:1689/2315 train_time:102372ms step_avg:60.61ms
step:1690/2315 train_time:102433ms step_avg:60.61ms
step:1691/2315 train_time:102495ms step_avg:60.61ms
step:1692/2315 train_time:102555ms step_avg:60.61ms
step:1693/2315 train_time:102617ms step_avg:60.61ms
step:1694/2315 train_time:102678ms step_avg:60.61ms
step:1695/2315 train_time:102740ms step_avg:60.61ms
step:1696/2315 train_time:102800ms step_avg:60.61ms
step:1697/2315 train_time:102861ms step_avg:60.61ms
step:1698/2315 train_time:102922ms step_avg:60.61ms
step:1699/2315 train_time:102983ms step_avg:60.61ms
step:1700/2315 train_time:103045ms step_avg:60.61ms
step:1701/2315 train_time:103106ms step_avg:60.62ms
step:1702/2315 train_time:103167ms step_avg:60.62ms
step:1703/2315 train_time:103229ms step_avg:60.62ms
step:1704/2315 train_time:103290ms step_avg:60.62ms
step:1705/2315 train_time:103351ms step_avg:60.62ms
step:1706/2315 train_time:103412ms step_avg:60.62ms
step:1707/2315 train_time:103474ms step_avg:60.62ms
step:1708/2315 train_time:103535ms step_avg:60.62ms
step:1709/2315 train_time:103597ms step_avg:60.62ms
step:1710/2315 train_time:103658ms step_avg:60.62ms
step:1711/2315 train_time:103719ms step_avg:60.62ms
step:1712/2315 train_time:103780ms step_avg:60.62ms
step:1713/2315 train_time:103841ms step_avg:60.62ms
step:1714/2315 train_time:103902ms step_avg:60.62ms
step:1715/2315 train_time:103963ms step_avg:60.62ms
step:1716/2315 train_time:104024ms step_avg:60.62ms
step:1717/2315 train_time:104085ms step_avg:60.62ms
step:1718/2315 train_time:104146ms step_avg:60.62ms
step:1719/2315 train_time:104208ms step_avg:60.62ms
step:1720/2315 train_time:104269ms step_avg:60.62ms
step:1721/2315 train_time:104330ms step_avg:60.62ms
step:1722/2315 train_time:104391ms step_avg:60.62ms
step:1723/2315 train_time:104453ms step_avg:60.62ms
step:1724/2315 train_time:104514ms step_avg:60.62ms
step:1725/2315 train_time:104575ms step_avg:60.62ms
step:1726/2315 train_time:104637ms step_avg:60.62ms
step:1727/2315 train_time:104698ms step_avg:60.62ms
step:1728/2315 train_time:104759ms step_avg:60.62ms
step:1729/2315 train_time:104820ms step_avg:60.62ms
step:1730/2315 train_time:104881ms step_avg:60.62ms
step:1731/2315 train_time:104943ms step_avg:60.63ms
step:1732/2315 train_time:105003ms step_avg:60.63ms
step:1733/2315 train_time:105065ms step_avg:60.63ms
step:1734/2315 train_time:105127ms step_avg:60.63ms
step:1735/2315 train_time:105188ms step_avg:60.63ms
step:1736/2315 train_time:105249ms step_avg:60.63ms
step:1737/2315 train_time:105310ms step_avg:60.63ms
step:1738/2315 train_time:105372ms step_avg:60.63ms
step:1739/2315 train_time:105433ms step_avg:60.63ms
step:1740/2315 train_time:105493ms step_avg:60.63ms
step:1741/2315 train_time:105555ms step_avg:60.63ms
step:1742/2315 train_time:105616ms step_avg:60.63ms
step:1743/2315 train_time:105679ms step_avg:60.63ms
step:1744/2315 train_time:105740ms step_avg:60.63ms
step:1745/2315 train_time:105800ms step_avg:60.63ms
step:1746/2315 train_time:105861ms step_avg:60.63ms
step:1747/2315 train_time:105922ms step_avg:60.63ms
step:1748/2315 train_time:105983ms step_avg:60.63ms
step:1749/2315 train_time:106045ms step_avg:60.63ms
step:1750/2315 train_time:106106ms step_avg:60.63ms
step:1750/2315 val_loss:3.3803 train_time:106169ms step_avg:60.67ms
step:1751/2315 train_time:106188ms step_avg:60.64ms
step:1752/2315 train_time:106231ms step_avg:60.63ms
step:1753/2315 train_time:106299ms step_avg:60.64ms
step:1754/2315 train_time:106364ms step_avg:60.64ms
step:1755/2315 train_time:106426ms step_avg:60.64ms
step:1756/2315 train_time:106487ms step_avg:60.64ms
step:1757/2315 train_time:106547ms step_avg:60.64ms
step:1758/2315 train_time:106608ms step_avg:60.64ms
step:1759/2315 train_time:106668ms step_avg:60.64ms
step:1760/2315 train_time:106728ms step_avg:60.64ms
step:1761/2315 train_time:106789ms step_avg:60.64ms
step:1762/2315 train_time:106849ms step_avg:60.64ms
step:1763/2315 train_time:106910ms step_avg:60.64ms
step:1764/2315 train_time:106971ms step_avg:60.64ms
step:1765/2315 train_time:107031ms step_avg:60.64ms
step:1766/2315 train_time:107092ms step_avg:60.64ms
step:1767/2315 train_time:107154ms step_avg:60.64ms
step:1768/2315 train_time:107216ms step_avg:60.64ms
step:1769/2315 train_time:107280ms step_avg:60.64ms
step:1770/2315 train_time:107342ms step_avg:60.65ms
step:1771/2315 train_time:107404ms step_avg:60.65ms
step:1772/2315 train_time:107465ms step_avg:60.65ms
step:1773/2315 train_time:107527ms step_avg:60.65ms
step:1774/2315 train_time:107587ms step_avg:60.65ms
step:1775/2315 train_time:107648ms step_avg:60.65ms
step:1776/2315 train_time:107709ms step_avg:60.65ms
step:1777/2315 train_time:107770ms step_avg:60.65ms
step:1778/2315 train_time:107830ms step_avg:60.65ms
step:1779/2315 train_time:107891ms step_avg:60.65ms
step:1780/2315 train_time:107952ms step_avg:60.65ms
step:1781/2315 train_time:108012ms step_avg:60.65ms
step:1782/2315 train_time:108073ms step_avg:60.65ms
step:1783/2315 train_time:108135ms step_avg:60.65ms
step:1784/2315 train_time:108197ms step_avg:60.65ms
step:1785/2315 train_time:108260ms step_avg:60.65ms
step:1786/2315 train_time:108321ms step_avg:60.65ms
step:1787/2315 train_time:108383ms step_avg:60.65ms
step:1788/2315 train_time:108444ms step_avg:60.65ms
step:1789/2315 train_time:108506ms step_avg:60.65ms
step:1790/2315 train_time:108566ms step_avg:60.65ms
step:1791/2315 train_time:108628ms step_avg:60.65ms
step:1792/2315 train_time:108689ms step_avg:60.65ms
step:1793/2315 train_time:108749ms step_avg:60.65ms
step:1794/2315 train_time:108810ms step_avg:60.65ms
step:1795/2315 train_time:108871ms step_avg:60.65ms
step:1796/2315 train_time:108932ms step_avg:60.65ms
step:1797/2315 train_time:108993ms step_avg:60.65ms
step:1798/2315 train_time:109053ms step_avg:60.65ms
step:1799/2315 train_time:109115ms step_avg:60.65ms
step:1800/2315 train_time:109176ms step_avg:60.65ms
step:1801/2315 train_time:109239ms step_avg:60.65ms
step:1802/2315 train_time:109300ms step_avg:60.66ms
step:1803/2315 train_time:109363ms step_avg:60.66ms
step:1804/2315 train_time:109425ms step_avg:60.66ms
step:1805/2315 train_time:109486ms step_avg:60.66ms
step:1806/2315 train_time:109547ms step_avg:60.66ms
step:1807/2315 train_time:109608ms step_avg:60.66ms
step:1808/2315 train_time:109669ms step_avg:60.66ms
step:1809/2315 train_time:109730ms step_avg:60.66ms
step:1810/2315 train_time:109791ms step_avg:60.66ms
step:1811/2315 train_time:109851ms step_avg:60.66ms
step:1812/2315 train_time:109912ms step_avg:60.66ms
step:1813/2315 train_time:109973ms step_avg:60.66ms
step:1814/2315 train_time:110033ms step_avg:60.66ms
step:1815/2315 train_time:110094ms step_avg:60.66ms
step:1816/2315 train_time:110155ms step_avg:60.66ms
step:1817/2315 train_time:110217ms step_avg:60.66ms
step:1818/2315 train_time:110278ms step_avg:60.66ms
step:1819/2315 train_time:110340ms step_avg:60.66ms
step:1820/2315 train_time:110401ms step_avg:60.66ms
step:1821/2315 train_time:110462ms step_avg:60.66ms
step:1822/2315 train_time:110524ms step_avg:60.66ms
step:1823/2315 train_time:110586ms step_avg:60.66ms
step:1824/2315 train_time:110646ms step_avg:60.66ms
step:1825/2315 train_time:110707ms step_avg:60.66ms
step:1826/2315 train_time:110769ms step_avg:60.66ms
step:1827/2315 train_time:110830ms step_avg:60.66ms
step:1828/2315 train_time:110890ms step_avg:60.66ms
step:1829/2315 train_time:110951ms step_avg:60.66ms
step:1830/2315 train_time:111012ms step_avg:60.66ms
step:1831/2315 train_time:111073ms step_avg:60.66ms
step:1832/2315 train_time:111135ms step_avg:60.66ms
step:1833/2315 train_time:111196ms step_avg:60.66ms
step:1834/2315 train_time:111258ms step_avg:60.66ms
step:1835/2315 train_time:111319ms step_avg:60.66ms
step:1836/2315 train_time:111381ms step_avg:60.66ms
step:1837/2315 train_time:111442ms step_avg:60.67ms
step:1838/2315 train_time:111503ms step_avg:60.67ms
step:1839/2315 train_time:111565ms step_avg:60.67ms
step:1840/2315 train_time:111626ms step_avg:60.67ms
step:1841/2315 train_time:111687ms step_avg:60.67ms
step:1842/2315 train_time:111748ms step_avg:60.67ms
step:1843/2315 train_time:111809ms step_avg:60.67ms
step:1844/2315 train_time:111870ms step_avg:60.67ms
step:1845/2315 train_time:111931ms step_avg:60.67ms
step:1846/2315 train_time:111992ms step_avg:60.67ms
step:1847/2315 train_time:112053ms step_avg:60.67ms
step:1848/2315 train_time:112114ms step_avg:60.67ms
step:1849/2315 train_time:112176ms step_avg:60.67ms
step:1850/2315 train_time:112238ms step_avg:60.67ms
step:1851/2315 train_time:112299ms step_avg:60.67ms
step:1852/2315 train_time:112360ms step_avg:60.67ms
step:1853/2315 train_time:112421ms step_avg:60.67ms
step:1854/2315 train_time:112482ms step_avg:60.67ms
step:1855/2315 train_time:112543ms step_avg:60.67ms
step:1856/2315 train_time:112604ms step_avg:60.67ms
step:1857/2315 train_time:112666ms step_avg:60.67ms
step:1858/2315 train_time:112727ms step_avg:60.67ms
step:1859/2315 train_time:112788ms step_avg:60.67ms
step:1860/2315 train_time:112849ms step_avg:60.67ms
step:1861/2315 train_time:112910ms step_avg:60.67ms
step:1862/2315 train_time:112972ms step_avg:60.67ms
step:1863/2315 train_time:113033ms step_avg:60.67ms
step:1864/2315 train_time:113094ms step_avg:60.67ms
step:1865/2315 train_time:113156ms step_avg:60.67ms
step:1866/2315 train_time:113218ms step_avg:60.67ms
step:1867/2315 train_time:113280ms step_avg:60.67ms
step:1868/2315 train_time:113341ms step_avg:60.68ms
step:1869/2315 train_time:113402ms step_avg:60.68ms
step:1870/2315 train_time:113463ms step_avg:60.68ms
step:1871/2315 train_time:113524ms step_avg:60.68ms
step:1872/2315 train_time:113585ms step_avg:60.68ms
step:1873/2315 train_time:113646ms step_avg:60.68ms
step:1874/2315 train_time:113707ms step_avg:60.68ms
step:1875/2315 train_time:113768ms step_avg:60.68ms
step:1876/2315 train_time:113830ms step_avg:60.68ms
step:1877/2315 train_time:113891ms step_avg:60.68ms
step:1878/2315 train_time:113952ms step_avg:60.68ms
step:1879/2315 train_time:114012ms step_avg:60.68ms
step:1880/2315 train_time:114073ms step_avg:60.68ms
step:1881/2315 train_time:114136ms step_avg:60.68ms
step:1882/2315 train_time:114197ms step_avg:60.68ms
step:1883/2315 train_time:114259ms step_avg:60.68ms
step:1884/2315 train_time:114320ms step_avg:60.68ms
step:1885/2315 train_time:114381ms step_avg:60.68ms
step:1886/2315 train_time:114442ms step_avg:60.68ms
step:1887/2315 train_time:114503ms step_avg:60.68ms
step:1888/2315 train_time:114564ms step_avg:60.68ms
step:1889/2315 train_time:114626ms step_avg:60.68ms
step:1890/2315 train_time:114687ms step_avg:60.68ms
step:1891/2315 train_time:114748ms step_avg:60.68ms
step:1892/2315 train_time:114809ms step_avg:60.68ms
step:1893/2315 train_time:114870ms step_avg:60.68ms
step:1894/2315 train_time:114931ms step_avg:60.68ms
step:1895/2315 train_time:114992ms step_avg:60.68ms
step:1896/2315 train_time:115052ms step_avg:60.68ms
step:1897/2315 train_time:115114ms step_avg:60.68ms
step:1898/2315 train_time:115175ms step_avg:60.68ms
step:1899/2315 train_time:115238ms step_avg:60.68ms
step:1900/2315 train_time:115299ms step_avg:60.68ms
step:1901/2315 train_time:115360ms step_avg:60.68ms
step:1902/2315 train_time:115422ms step_avg:60.68ms
step:1903/2315 train_time:115482ms step_avg:60.68ms
step:1904/2315 train_time:115544ms step_avg:60.68ms
step:1905/2315 train_time:115605ms step_avg:60.68ms
step:1906/2315 train_time:115666ms step_avg:60.69ms
step:1907/2315 train_time:115727ms step_avg:60.69ms
step:1908/2315 train_time:115788ms step_avg:60.69ms
step:1909/2315 train_time:115849ms step_avg:60.69ms
step:1910/2315 train_time:115910ms step_avg:60.69ms
step:1911/2315 train_time:115971ms step_avg:60.69ms
step:1912/2315 train_time:116032ms step_avg:60.69ms
step:1913/2315 train_time:116094ms step_avg:60.69ms
step:1914/2315 train_time:116155ms step_avg:60.69ms
step:1915/2315 train_time:116217ms step_avg:60.69ms
step:1916/2315 train_time:116278ms step_avg:60.69ms
step:1917/2315 train_time:116339ms step_avg:60.69ms
step:1918/2315 train_time:116400ms step_avg:60.69ms
step:1919/2315 train_time:116461ms step_avg:60.69ms
step:1920/2315 train_time:116522ms step_avg:60.69ms
step:1921/2315 train_time:116583ms step_avg:60.69ms
step:1922/2315 train_time:116644ms step_avg:60.69ms
step:1923/2315 train_time:116707ms step_avg:60.69ms
step:1924/2315 train_time:116768ms step_avg:60.69ms
step:1925/2315 train_time:116829ms step_avg:60.69ms
step:1926/2315 train_time:116889ms step_avg:60.69ms
step:1927/2315 train_time:116951ms step_avg:60.69ms
step:1928/2315 train_time:117012ms step_avg:60.69ms
step:1929/2315 train_time:117073ms step_avg:60.69ms
step:1930/2315 train_time:117134ms step_avg:60.69ms
step:1931/2315 train_time:117195ms step_avg:60.69ms
step:1932/2315 train_time:117258ms step_avg:60.69ms
step:1933/2315 train_time:117319ms step_avg:60.69ms
step:1934/2315 train_time:117380ms step_avg:60.69ms
step:1935/2315 train_time:117442ms step_avg:60.69ms
step:1936/2315 train_time:117502ms step_avg:60.69ms
step:1937/2315 train_time:117563ms step_avg:60.69ms
step:1938/2315 train_time:117625ms step_avg:60.69ms
step:1939/2315 train_time:117687ms step_avg:60.69ms
step:1940/2315 train_time:117747ms step_avg:60.69ms
step:1941/2315 train_time:117809ms step_avg:60.69ms
step:1942/2315 train_time:117870ms step_avg:60.69ms
step:1943/2315 train_time:117931ms step_avg:60.70ms
step:1944/2315 train_time:117992ms step_avg:60.70ms
step:1945/2315 train_time:118053ms step_avg:60.70ms
step:1946/2315 train_time:118114ms step_avg:60.70ms
step:1947/2315 train_time:118175ms step_avg:60.70ms
step:1948/2315 train_time:118237ms step_avg:60.70ms
step:1949/2315 train_time:118298ms step_avg:60.70ms
step:1950/2315 train_time:118358ms step_avg:60.70ms
step:1951/2315 train_time:118420ms step_avg:60.70ms
step:1952/2315 train_time:118481ms step_avg:60.70ms
step:1953/2315 train_time:118542ms step_avg:60.70ms
step:1954/2315 train_time:118603ms step_avg:60.70ms
step:1955/2315 train_time:118665ms step_avg:60.70ms
step:1956/2315 train_time:118726ms step_avg:60.70ms
step:1957/2315 train_time:118787ms step_avg:60.70ms
step:1958/2315 train_time:118848ms step_avg:60.70ms
step:1959/2315 train_time:118909ms step_avg:60.70ms
step:1960/2315 train_time:118971ms step_avg:60.70ms
step:1961/2315 train_time:119032ms step_avg:60.70ms
step:1962/2315 train_time:119093ms step_avg:60.70ms
step:1963/2315 train_time:119154ms step_avg:60.70ms
step:1964/2315 train_time:119216ms step_avg:60.70ms
step:1965/2315 train_time:119277ms step_avg:60.70ms
step:1966/2315 train_time:119339ms step_avg:60.70ms
step:1967/2315 train_time:119400ms step_avg:60.70ms
step:1968/2315 train_time:119461ms step_avg:60.70ms
step:1969/2315 train_time:119522ms step_avg:60.70ms
step:1970/2315 train_time:119583ms step_avg:60.70ms
step:1971/2315 train_time:119644ms step_avg:60.70ms
step:1972/2315 train_time:119705ms step_avg:60.70ms
step:1973/2315 train_time:119767ms step_avg:60.70ms
step:1974/2315 train_time:119829ms step_avg:60.70ms
step:1975/2315 train_time:119890ms step_avg:60.70ms
step:1976/2315 train_time:119951ms step_avg:60.70ms
step:1977/2315 train_time:120012ms step_avg:60.70ms
step:1978/2315 train_time:120072ms step_avg:60.70ms
step:1979/2315 train_time:120134ms step_avg:60.70ms
step:1980/2315 train_time:120194ms step_avg:60.70ms
step:1981/2315 train_time:120256ms step_avg:60.70ms
step:1982/2315 train_time:120317ms step_avg:60.71ms
step:1983/2315 train_time:120379ms step_avg:60.71ms
step:1984/2315 train_time:120440ms step_avg:60.71ms
step:1985/2315 train_time:120501ms step_avg:60.71ms
step:1986/2315 train_time:120563ms step_avg:60.71ms
step:1987/2315 train_time:120623ms step_avg:60.71ms
step:1988/2315 train_time:120685ms step_avg:60.71ms
step:1989/2315 train_time:120747ms step_avg:60.71ms
step:1990/2315 train_time:120807ms step_avg:60.71ms
step:1991/2315 train_time:120868ms step_avg:60.71ms
step:1992/2315 train_time:120929ms step_avg:60.71ms
step:1993/2315 train_time:120991ms step_avg:60.71ms
step:1994/2315 train_time:121052ms step_avg:60.71ms
step:1995/2315 train_time:121113ms step_avg:60.71ms
step:1996/2315 train_time:121174ms step_avg:60.71ms
step:1997/2315 train_time:121236ms step_avg:60.71ms
step:1998/2315 train_time:121297ms step_avg:60.71ms
step:1999/2315 train_time:121359ms step_avg:60.71ms
step:2000/2315 train_time:121420ms step_avg:60.71ms
step:2000/2315 val_loss:3.3310 train_time:121483ms step_avg:60.74ms
step:2001/2315 train_time:121502ms step_avg:60.72ms
step:2002/2315 train_time:121545ms step_avg:60.71ms
step:2003/2315 train_time:121611ms step_avg:60.71ms
step:2004/2315 train_time:121677ms step_avg:60.72ms
step:2005/2315 train_time:121740ms step_avg:60.72ms
step:2006/2315 train_time:121800ms step_avg:60.72ms
step:2007/2315 train_time:121862ms step_avg:60.72ms
step:2008/2315 train_time:121922ms step_avg:60.72ms
step:2009/2315 train_time:121983ms step_avg:60.72ms
step:2010/2315 train_time:122043ms step_avg:60.72ms
step:2011/2315 train_time:122104ms step_avg:60.72ms
step:2012/2315 train_time:122165ms step_avg:60.72ms
step:2013/2315 train_time:122225ms step_avg:60.72ms
step:2014/2315 train_time:122286ms step_avg:60.72ms
step:2015/2315 train_time:122346ms step_avg:60.72ms
step:2016/2315 train_time:122409ms step_avg:60.72ms
step:2017/2315 train_time:122471ms step_avg:60.72ms
step:2018/2315 train_time:122533ms step_avg:60.72ms
step:2019/2315 train_time:122596ms step_avg:60.72ms
step:2020/2315 train_time:122659ms step_avg:60.72ms
step:2021/2315 train_time:122721ms step_avg:60.72ms
step:2022/2315 train_time:122782ms step_avg:60.72ms
step:2023/2315 train_time:122843ms step_avg:60.72ms
step:2024/2315 train_time:122904ms step_avg:60.72ms
step:2025/2315 train_time:122965ms step_avg:60.72ms
step:2026/2315 train_time:123026ms step_avg:60.72ms
step:2027/2315 train_time:123086ms step_avg:60.72ms
step:2028/2315 train_time:123147ms step_avg:60.72ms
step:2029/2315 train_time:123207ms step_avg:60.72ms
step:2030/2315 train_time:123267ms step_avg:60.72ms
step:2031/2315 train_time:123328ms step_avg:60.72ms
step:2032/2315 train_time:123389ms step_avg:60.72ms
step:2033/2315 train_time:123450ms step_avg:60.72ms
step:2034/2315 train_time:123512ms step_avg:60.72ms
step:2035/2315 train_time:123574ms step_avg:60.72ms
step:2036/2315 train_time:123635ms step_avg:60.72ms
step:2037/2315 train_time:123697ms step_avg:60.73ms
step:2038/2315 train_time:123758ms step_avg:60.73ms
step:2039/2315 train_time:123820ms step_avg:60.73ms
step:2040/2315 train_time:123881ms step_avg:60.73ms
step:2041/2315 train_time:123942ms step_avg:60.73ms
step:2042/2315 train_time:124003ms step_avg:60.73ms
step:2043/2315 train_time:124064ms step_avg:60.73ms
step:2044/2315 train_time:124125ms step_avg:60.73ms
step:2045/2315 train_time:124186ms step_avg:60.73ms
step:2046/2315 train_time:124247ms step_avg:60.73ms
step:2047/2315 train_time:124308ms step_avg:60.73ms
step:2048/2315 train_time:124369ms step_avg:60.73ms
step:2049/2315 train_time:124430ms step_avg:60.73ms
step:2050/2315 train_time:124491ms step_avg:60.73ms
step:2051/2315 train_time:124552ms step_avg:60.73ms
step:2052/2315 train_time:124614ms step_avg:60.73ms
step:2053/2315 train_time:124676ms step_avg:60.73ms
step:2054/2315 train_time:124738ms step_avg:60.73ms
step:2055/2315 train_time:124800ms step_avg:60.73ms
step:2056/2315 train_time:124861ms step_avg:60.73ms
step:2057/2315 train_time:124922ms step_avg:60.73ms
step:2058/2315 train_time:124983ms step_avg:60.73ms
step:2059/2315 train_time:125044ms step_avg:60.73ms
step:2060/2315 train_time:125104ms step_avg:60.73ms
step:2061/2315 train_time:125165ms step_avg:60.73ms
step:2062/2315 train_time:125226ms step_avg:60.73ms
step:2063/2315 train_time:125287ms step_avg:60.73ms
step:2064/2315 train_time:125349ms step_avg:60.73ms
step:2065/2315 train_time:125411ms step_avg:60.73ms
step:2066/2315 train_time:125472ms step_avg:60.73ms
step:2067/2315 train_time:125533ms step_avg:60.73ms
step:2068/2315 train_time:125594ms step_avg:60.73ms
step:2069/2315 train_time:125655ms step_avg:60.73ms
step:2070/2315 train_time:125716ms step_avg:60.73ms
step:2071/2315 train_time:125778ms step_avg:60.73ms
step:2072/2315 train_time:125839ms step_avg:60.73ms
step:2073/2315 train_time:125900ms step_avg:60.73ms
step:2074/2315 train_time:125961ms step_avg:60.73ms
step:2075/2315 train_time:126023ms step_avg:60.73ms
step:2076/2315 train_time:126083ms step_avg:60.73ms
step:2077/2315 train_time:126144ms step_avg:60.73ms
step:2078/2315 train_time:126205ms step_avg:60.73ms
step:2079/2315 train_time:126266ms step_avg:60.73ms
step:2080/2315 train_time:126327ms step_avg:60.73ms
step:2081/2315 train_time:126388ms step_avg:60.73ms
step:2082/2315 train_time:126450ms step_avg:60.73ms
step:2083/2315 train_time:126511ms step_avg:60.73ms
step:2084/2315 train_time:126572ms step_avg:60.74ms
step:2085/2315 train_time:126633ms step_avg:60.74ms
step:2086/2315 train_time:126695ms step_avg:60.74ms
step:2087/2315 train_time:126756ms step_avg:60.74ms
step:2088/2315 train_time:126817ms step_avg:60.74ms
step:2089/2315 train_time:126879ms step_avg:60.74ms
step:2090/2315 train_time:126940ms step_avg:60.74ms
step:2091/2315 train_time:127002ms step_avg:60.74ms
step:2092/2315 train_time:127063ms step_avg:60.74ms
step:2093/2315 train_time:127125ms step_avg:60.74ms
step:2094/2315 train_time:127186ms step_avg:60.74ms
step:2095/2315 train_time:127247ms step_avg:60.74ms
step:2096/2315 train_time:127308ms step_avg:60.74ms
step:2097/2315 train_time:127369ms step_avg:60.74ms
step:2098/2315 train_time:127430ms step_avg:60.74ms
step:2099/2315 train_time:127491ms step_avg:60.74ms
step:2100/2315 train_time:127552ms step_avg:60.74ms
step:2101/2315 train_time:127614ms step_avg:60.74ms
step:2102/2315 train_time:127675ms step_avg:60.74ms
step:2103/2315 train_time:127736ms step_avg:60.74ms
step:2104/2315 train_time:127797ms step_avg:60.74ms
step:2105/2315 train_time:127858ms step_avg:60.74ms
step:2106/2315 train_time:127919ms step_avg:60.74ms
step:2107/2315 train_time:127981ms step_avg:60.74ms
step:2108/2315 train_time:128042ms step_avg:60.74ms
step:2109/2315 train_time:128104ms step_avg:60.74ms
step:2110/2315 train_time:128165ms step_avg:60.74ms
step:2111/2315 train_time:128226ms step_avg:60.74ms
step:2112/2315 train_time:128287ms step_avg:60.74ms
step:2113/2315 train_time:128348ms step_avg:60.74ms
step:2114/2315 train_time:128409ms step_avg:60.74ms
step:2115/2315 train_time:128470ms step_avg:60.74ms
step:2116/2315 train_time:128530ms step_avg:60.74ms
step:2117/2315 train_time:128592ms step_avg:60.74ms
step:2118/2315 train_time:128653ms step_avg:60.74ms
step:2119/2315 train_time:128714ms step_avg:60.74ms
step:2120/2315 train_time:128776ms step_avg:60.74ms
step:2121/2315 train_time:128837ms step_avg:60.74ms
step:2122/2315 train_time:128899ms step_avg:60.74ms
step:2123/2315 train_time:128961ms step_avg:60.74ms
step:2124/2315 train_time:129022ms step_avg:60.74ms
step:2125/2315 train_time:129083ms step_avg:60.75ms
step:2126/2315 train_time:129144ms step_avg:60.74ms
step:2127/2315 train_time:129205ms step_avg:60.75ms
step:2128/2315 train_time:129266ms step_avg:60.75ms
step:2129/2315 train_time:129328ms step_avg:60.75ms
step:2130/2315 train_time:129389ms step_avg:60.75ms
step:2131/2315 train_time:129450ms step_avg:60.75ms
step:2132/2315 train_time:129510ms step_avg:60.75ms
step:2133/2315 train_time:129572ms step_avg:60.75ms
step:2134/2315 train_time:129632ms step_avg:60.75ms
step:2135/2315 train_time:129693ms step_avg:60.75ms
step:2136/2315 train_time:129754ms step_avg:60.75ms
step:2137/2315 train_time:129816ms step_avg:60.75ms
step:2138/2315 train_time:129877ms step_avg:60.75ms
step:2139/2315 train_time:129939ms step_avg:60.75ms
step:2140/2315 train_time:130001ms step_avg:60.75ms
step:2141/2315 train_time:130063ms step_avg:60.75ms
step:2142/2315 train_time:130124ms step_avg:60.75ms
step:2143/2315 train_time:130185ms step_avg:60.75ms
step:2144/2315 train_time:130247ms step_avg:60.75ms
step:2145/2315 train_time:130308ms step_avg:60.75ms
step:2146/2315 train_time:130370ms step_avg:60.75ms
step:2147/2315 train_time:130432ms step_avg:60.75ms
step:2148/2315 train_time:130492ms step_avg:60.75ms
step:2149/2315 train_time:130554ms step_avg:60.75ms
step:2150/2315 train_time:130615ms step_avg:60.75ms
step:2151/2315 train_time:130676ms step_avg:60.75ms
step:2152/2315 train_time:130737ms step_avg:60.75ms
step:2153/2315 train_time:130798ms step_avg:60.75ms
step:2154/2315 train_time:130859ms step_avg:60.75ms
step:2155/2315 train_time:130921ms step_avg:60.75ms
step:2156/2315 train_time:130982ms step_avg:60.75ms
step:2157/2315 train_time:131043ms step_avg:60.75ms
step:2158/2315 train_time:131104ms step_avg:60.75ms
step:2159/2315 train_time:131165ms step_avg:60.75ms
step:2160/2315 train_time:131226ms step_avg:60.75ms
step:2161/2315 train_time:131288ms step_avg:60.75ms
step:2162/2315 train_time:131349ms step_avg:60.75ms
step:2163/2315 train_time:131411ms step_avg:60.75ms
step:2164/2315 train_time:131471ms step_avg:60.75ms
step:2165/2315 train_time:131533ms step_avg:60.75ms
step:2166/2315 train_time:131594ms step_avg:60.75ms
step:2167/2315 train_time:131656ms step_avg:60.75ms
step:2168/2315 train_time:131716ms step_avg:60.75ms
step:2169/2315 train_time:131777ms step_avg:60.75ms
step:2170/2315 train_time:131838ms step_avg:60.75ms
step:2171/2315 train_time:131900ms step_avg:60.76ms
step:2172/2315 train_time:131962ms step_avg:60.76ms
step:2173/2315 train_time:132023ms step_avg:60.76ms
step:2174/2315 train_time:132084ms step_avg:60.76ms
step:2175/2315 train_time:132145ms step_avg:60.76ms
step:2176/2315 train_time:132206ms step_avg:60.76ms
step:2177/2315 train_time:132267ms step_avg:60.76ms
step:2178/2315 train_time:132328ms step_avg:60.76ms
step:2179/2315 train_time:132389ms step_avg:60.76ms
step:2180/2315 train_time:132450ms step_avg:60.76ms
step:2181/2315 train_time:132512ms step_avg:60.76ms
step:2182/2315 train_time:132573ms step_avg:60.76ms
step:2183/2315 train_time:132634ms step_avg:60.76ms
step:2184/2315 train_time:132695ms step_avg:60.76ms
step:2185/2315 train_time:132756ms step_avg:60.76ms
step:2186/2315 train_time:132818ms step_avg:60.76ms
step:2187/2315 train_time:132880ms step_avg:60.76ms
step:2188/2315 train_time:132941ms step_avg:60.76ms
step:2189/2315 train_time:133002ms step_avg:60.76ms
step:2190/2315 train_time:133064ms step_avg:60.76ms
step:2191/2315 train_time:133125ms step_avg:60.76ms
step:2192/2315 train_time:133186ms step_avg:60.76ms
step:2193/2315 train_time:133247ms step_avg:60.76ms
step:2194/2315 train_time:133308ms step_avg:60.76ms
step:2195/2315 train_time:133369ms step_avg:60.76ms
step:2196/2315 train_time:133430ms step_avg:60.76ms
step:2197/2315 train_time:133492ms step_avg:60.76ms
step:2198/2315 train_time:133553ms step_avg:60.76ms
step:2199/2315 train_time:133614ms step_avg:60.76ms
step:2200/2315 train_time:133674ms step_avg:60.76ms
step:2201/2315 train_time:133735ms step_avg:60.76ms
step:2202/2315 train_time:133796ms step_avg:60.76ms
step:2203/2315 train_time:133858ms step_avg:60.76ms
step:2204/2315 train_time:133919ms step_avg:60.76ms
step:2205/2315 train_time:133981ms step_avg:60.76ms
step:2206/2315 train_time:134042ms step_avg:60.76ms
step:2207/2315 train_time:134104ms step_avg:60.76ms
step:2208/2315 train_time:134165ms step_avg:60.76ms
step:2209/2315 train_time:134226ms step_avg:60.76ms
step:2210/2315 train_time:134287ms step_avg:60.76ms
step:2211/2315 train_time:134348ms step_avg:60.76ms
step:2212/2315 train_time:134409ms step_avg:60.76ms
step:2213/2315 train_time:134471ms step_avg:60.76ms
step:2214/2315 train_time:134531ms step_avg:60.76ms
step:2215/2315 train_time:134593ms step_avg:60.76ms
step:2216/2315 train_time:134653ms step_avg:60.76ms
step:2217/2315 train_time:134715ms step_avg:60.76ms
step:2218/2315 train_time:134777ms step_avg:60.76ms
step:2219/2315 train_time:134837ms step_avg:60.76ms
step:2220/2315 train_time:134898ms step_avg:60.77ms
step:2221/2315 train_time:134960ms step_avg:60.77ms
step:2222/2315 train_time:135022ms step_avg:60.77ms
step:2223/2315 train_time:135083ms step_avg:60.77ms
step:2224/2315 train_time:135144ms step_avg:60.77ms
step:2225/2315 train_time:135205ms step_avg:60.77ms
step:2226/2315 train_time:135266ms step_avg:60.77ms
step:2227/2315 train_time:135328ms step_avg:60.77ms
step:2228/2315 train_time:135389ms step_avg:60.77ms
step:2229/2315 train_time:135449ms step_avg:60.77ms
step:2230/2315 train_time:135510ms step_avg:60.77ms
step:2231/2315 train_time:135571ms step_avg:60.77ms
step:2232/2315 train_time:135632ms step_avg:60.77ms
step:2233/2315 train_time:135693ms step_avg:60.77ms
step:2234/2315 train_time:135755ms step_avg:60.77ms
step:2235/2315 train_time:135816ms step_avg:60.77ms
step:2236/2315 train_time:135878ms step_avg:60.77ms
step:2237/2315 train_time:135940ms step_avg:60.77ms
step:2238/2315 train_time:136001ms step_avg:60.77ms
step:2239/2315 train_time:136063ms step_avg:60.77ms
step:2240/2315 train_time:136124ms step_avg:60.77ms
step:2241/2315 train_time:136186ms step_avg:60.77ms
step:2242/2315 train_time:136247ms step_avg:60.77ms
step:2243/2315 train_time:136309ms step_avg:60.77ms
step:2244/2315 train_time:136370ms step_avg:60.77ms
step:2245/2315 train_time:136430ms step_avg:60.77ms
step:2246/2315 train_time:136491ms step_avg:60.77ms
step:2247/2315 train_time:136553ms step_avg:60.77ms
step:2248/2315 train_time:136613ms step_avg:60.77ms
step:2249/2315 train_time:136674ms step_avg:60.77ms
step:2250/2315 train_time:136735ms step_avg:60.77ms
step:2250/2315 val_loss:3.2910 train_time:136798ms step_avg:60.80ms
step:2251/2315 train_time:136818ms step_avg:60.78ms
step:2252/2315 train_time:136860ms step_avg:60.77ms
step:2253/2315 train_time:136926ms step_avg:60.78ms
step:2254/2315 train_time:136989ms step_avg:60.78ms
step:2255/2315 train_time:137050ms step_avg:60.78ms
step:2256/2315 train_time:137111ms step_avg:60.78ms
step:2257/2315 train_time:137171ms step_avg:60.78ms
step:2258/2315 train_time:137231ms step_avg:60.78ms
step:2259/2315 train_time:137293ms step_avg:60.78ms
step:2260/2315 train_time:137353ms step_avg:60.78ms
step:2261/2315 train_time:137414ms step_avg:60.78ms
step:2262/2315 train_time:137474ms step_avg:60.78ms
step:2263/2315 train_time:137535ms step_avg:60.78ms
step:2264/2315 train_time:137596ms step_avg:60.78ms
step:2265/2315 train_time:137656ms step_avg:60.78ms
step:2266/2315 train_time:137717ms step_avg:60.78ms
step:2267/2315 train_time:137779ms step_avg:60.78ms
step:2268/2315 train_time:137842ms step_avg:60.78ms
step:2269/2315 train_time:137904ms step_avg:60.78ms
step:2270/2315 train_time:137966ms step_avg:60.78ms
step:2271/2315 train_time:138028ms step_avg:60.78ms
step:2272/2315 train_time:138090ms step_avg:60.78ms
step:2273/2315 train_time:138151ms step_avg:60.78ms
step:2274/2315 train_time:138211ms step_avg:60.78ms
step:2275/2315 train_time:138272ms step_avg:60.78ms
step:2276/2315 train_time:138332ms step_avg:60.78ms
step:2277/2315 train_time:138393ms step_avg:60.78ms
step:2278/2315 train_time:138454ms step_avg:60.78ms
step:2279/2315 train_time:138515ms step_avg:60.78ms
step:2280/2315 train_time:138576ms step_avg:60.78ms
step:2281/2315 train_time:138637ms step_avg:60.78ms
step:2282/2315 train_time:138697ms step_avg:60.78ms
step:2283/2315 train_time:138759ms step_avg:60.78ms
step:2284/2315 train_time:138821ms step_avg:60.78ms
step:2285/2315 train_time:138882ms step_avg:60.78ms
step:2286/2315 train_time:138944ms step_avg:60.78ms
step:2287/2315 train_time:139005ms step_avg:60.78ms
step:2288/2315 train_time:139067ms step_avg:60.78ms
step:2289/2315 train_time:139129ms step_avg:60.78ms
step:2290/2315 train_time:139189ms step_avg:60.78ms
step:2291/2315 train_time:139251ms step_avg:60.78ms
step:2292/2315 train_time:139311ms step_avg:60.78ms
step:2293/2315 train_time:139372ms step_avg:60.78ms
step:2294/2315 train_time:139433ms step_avg:60.78ms
step:2295/2315 train_time:139494ms step_avg:60.78ms
step:2296/2315 train_time:139555ms step_avg:60.78ms
step:2297/2315 train_time:139616ms step_avg:60.78ms
step:2298/2315 train_time:139677ms step_avg:60.78ms
step:2299/2315 train_time:139739ms step_avg:60.78ms
step:2300/2315 train_time:139800ms step_avg:60.78ms
step:2301/2315 train_time:139862ms step_avg:60.78ms
step:2302/2315 train_time:139923ms step_avg:60.78ms
step:2303/2315 train_time:139985ms step_avg:60.78ms
step:2304/2315 train_time:140046ms step_avg:60.78ms
step:2305/2315 train_time:140107ms step_avg:60.78ms
step:2306/2315 train_time:140169ms step_avg:60.78ms
step:2307/2315 train_time:140231ms step_avg:60.79ms
step:2308/2315 train_time:140292ms step_avg:60.79ms
step:2309/2315 train_time:140353ms step_avg:60.79ms
step:2310/2315 train_time:140414ms step_avg:60.79ms
step:2311/2315 train_time:140475ms step_avg:60.79ms
step:2312/2315 train_time:140535ms step_avg:60.79ms
step:2313/2315 train_time:140596ms step_avg:60.79ms
step:2314/2315 train_time:140657ms step_avg:60.79ms
step:2315/2315 train_time:140719ms step_avg:60.79ms
step:2315/2315 val_loss:3.2789 train_time:140781ms step_avg:60.81ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

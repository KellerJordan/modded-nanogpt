import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]
# polar_express_coeffs = [
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
# ]



@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    # state["second_momentum_buffer"] = torch.zeros_like(grad[...,0:1]) if param.size(-2) >= param.size(-1) else torch.zeros_like(grad[...,0:1,:])
                    # state["second_momentum_buffer"] = torch.zeros((*grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((*grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)
            
            ###################################
            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))
            ####################################


            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> (-1.5)  0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Oct 22 2025, 02:20:45) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251022+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 24 06:33:53 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000001:00:00.0 Off |                    0 |
| N/A   36C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000002:00:00.0 Off |                    0 |
| N/A   35C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000003:00:00.0 Off |                    0 |
| N/A   31C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000008:00:00.0 Off |                    0 |
| N/A   31C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000009:00:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   0000000A:00:00.0 Off |                    0 |
| N/A   36C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   0000000C:00:00.0 Off |                    0 |
| N/A   35C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2315 train_time:82ms step_avg:81.63ms
step:2/2315 train_time:184ms step_avg:92.12ms
step:3/2315 train_time:204ms step_avg:68.06ms
step:4/2315 train_time:241ms step_avg:60.34ms
step:5/2315 train_time:300ms step_avg:59.98ms
step:6/2315 train_time:360ms step_avg:59.97ms
step:7/2315 train_time:419ms step_avg:59.88ms
step:8/2315 train_time:479ms step_avg:59.82ms
step:9/2315 train_time:539ms step_avg:59.87ms
step:10/2315 train_time:599ms step_avg:59.87ms
step:11/2315 train_time:658ms step_avg:59.86ms
step:12/2315 train_time:718ms step_avg:59.82ms
step:13/2315 train_time:778ms step_avg:59.84ms
step:14/2315 train_time:838ms step_avg:59.82ms
step:15/2315 train_time:898ms step_avg:59.85ms
step:16/2315 train_time:958ms step_avg:59.85ms
step:17/2315 train_time:1020ms step_avg:59.97ms
step:18/2315 train_time:1083ms step_avg:60.14ms
step:19/2315 train_time:1147ms step_avg:60.37ms
step:20/2315 train_time:1208ms step_avg:60.42ms
step:21/2315 train_time:1270ms step_avg:60.46ms
step:22/2315 train_time:1331ms step_avg:60.49ms
step:23/2315 train_time:1391ms step_avg:60.46ms
step:24/2315 train_time:1451ms step_avg:60.45ms
step:25/2315 train_time:1511ms step_avg:60.45ms
step:26/2315 train_time:1572ms step_avg:60.45ms
step:27/2315 train_time:1632ms step_avg:60.44ms
step:28/2315 train_time:1692ms step_avg:60.43ms
step:29/2315 train_time:1752ms step_avg:60.43ms
step:30/2315 train_time:1813ms step_avg:60.43ms
step:31/2315 train_time:1873ms step_avg:60.41ms
step:32/2315 train_time:1933ms step_avg:60.41ms
step:33/2315 train_time:1993ms step_avg:60.41ms
step:34/2315 train_time:2054ms step_avg:60.42ms
step:35/2315 train_time:2116ms step_avg:60.45ms
step:36/2315 train_time:2177ms step_avg:60.48ms
step:37/2315 train_time:2239ms step_avg:60.50ms
step:38/2315 train_time:2300ms step_avg:60.52ms
step:39/2315 train_time:2361ms step_avg:60.53ms
step:40/2315 train_time:2421ms step_avg:60.52ms
step:41/2315 train_time:2482ms step_avg:60.54ms
step:42/2315 train_time:2543ms step_avg:60.54ms
step:43/2315 train_time:2603ms step_avg:60.54ms
step:44/2315 train_time:2665ms step_avg:60.56ms
step:45/2315 train_time:2726ms step_avg:60.57ms
step:46/2315 train_time:2786ms step_avg:60.56ms
step:47/2315 train_time:2846ms step_avg:60.55ms
step:48/2315 train_time:2906ms step_avg:60.55ms
step:49/2315 train_time:2967ms step_avg:60.55ms
step:50/2315 train_time:3027ms step_avg:60.54ms
step:51/2315 train_time:3088ms step_avg:60.54ms
step:52/2315 train_time:3148ms step_avg:60.54ms
step:53/2315 train_time:3209ms step_avg:60.54ms
step:54/2315 train_time:3269ms step_avg:60.54ms
step:55/2315 train_time:3330ms step_avg:60.54ms
step:56/2315 train_time:3390ms step_avg:60.54ms
step:57/2315 train_time:3450ms step_avg:60.53ms
step:58/2315 train_time:3511ms step_avg:60.53ms
step:59/2315 train_time:3571ms step_avg:60.53ms
step:60/2315 train_time:3632ms step_avg:60.53ms
step:61/2315 train_time:3692ms step_avg:60.53ms
step:62/2315 train_time:3752ms step_avg:60.52ms
step:63/2315 train_time:3813ms step_avg:60.52ms
step:64/2315 train_time:3873ms step_avg:60.52ms
step:65/2315 train_time:3933ms step_avg:60.52ms
step:66/2315 train_time:3994ms step_avg:60.51ms
step:67/2315 train_time:4055ms step_avg:60.52ms
step:68/2315 train_time:4115ms step_avg:60.52ms
step:69/2315 train_time:4176ms step_avg:60.52ms
step:70/2315 train_time:4236ms step_avg:60.52ms
step:71/2315 train_time:4297ms step_avg:60.52ms
step:72/2315 train_time:4357ms step_avg:60.52ms
step:73/2315 train_time:4417ms step_avg:60.51ms
step:74/2315 train_time:4477ms step_avg:60.50ms
step:75/2315 train_time:4538ms step_avg:60.50ms
step:76/2315 train_time:4598ms step_avg:60.50ms
step:77/2315 train_time:4658ms step_avg:60.49ms
step:78/2315 train_time:4718ms step_avg:60.49ms
step:79/2315 train_time:4779ms step_avg:60.50ms
step:80/2315 train_time:4840ms step_avg:60.50ms
step:81/2315 train_time:4900ms step_avg:60.50ms
step:82/2315 train_time:4961ms step_avg:60.49ms
step:83/2315 train_time:5021ms step_avg:60.50ms
step:84/2315 train_time:5081ms step_avg:60.49ms
step:85/2315 train_time:5142ms step_avg:60.50ms
step:86/2315 train_time:5203ms step_avg:60.49ms
step:87/2315 train_time:5263ms step_avg:60.50ms
step:88/2315 train_time:5324ms step_avg:60.50ms
step:89/2315 train_time:5384ms step_avg:60.49ms
step:90/2315 train_time:5444ms step_avg:60.48ms
step:91/2315 train_time:5504ms step_avg:60.48ms
step:92/2315 train_time:5564ms step_avg:60.47ms
step:93/2315 train_time:5623ms step_avg:60.47ms
step:94/2315 train_time:5683ms step_avg:60.46ms
step:95/2315 train_time:5743ms step_avg:60.45ms
step:96/2315 train_time:5803ms step_avg:60.45ms
step:97/2315 train_time:5864ms step_avg:60.46ms
step:98/2315 train_time:5924ms step_avg:60.45ms
step:99/2315 train_time:5985ms step_avg:60.45ms
step:100/2315 train_time:6045ms step_avg:60.45ms
step:101/2315 train_time:6105ms step_avg:60.44ms
step:102/2315 train_time:6164ms step_avg:60.43ms
step:103/2315 train_time:6225ms step_avg:60.43ms
step:104/2315 train_time:6284ms step_avg:60.43ms
step:105/2315 train_time:6344ms step_avg:60.42ms
step:106/2315 train_time:6404ms step_avg:60.42ms
step:107/2315 train_time:6464ms step_avg:60.41ms
step:108/2315 train_time:6524ms step_avg:60.41ms
step:109/2315 train_time:6584ms step_avg:60.40ms
step:110/2315 train_time:6644ms step_avg:60.40ms
step:111/2315 train_time:6704ms step_avg:60.40ms
step:112/2315 train_time:6764ms step_avg:60.39ms
step:113/2315 train_time:6824ms step_avg:60.39ms
step:114/2315 train_time:6884ms step_avg:60.39ms
step:115/2315 train_time:6945ms step_avg:60.39ms
step:116/2315 train_time:7005ms step_avg:60.39ms
step:117/2315 train_time:7065ms step_avg:60.38ms
step:118/2315 train_time:7125ms step_avg:60.38ms
step:119/2315 train_time:7185ms step_avg:60.38ms
step:120/2315 train_time:7246ms step_avg:60.38ms
step:121/2315 train_time:7305ms step_avg:60.37ms
step:122/2315 train_time:7365ms step_avg:60.37ms
step:123/2315 train_time:7425ms step_avg:60.37ms
step:124/2315 train_time:7485ms step_avg:60.36ms
step:125/2315 train_time:7545ms step_avg:60.36ms
step:126/2315 train_time:7605ms step_avg:60.35ms
step:127/2315 train_time:7665ms step_avg:60.35ms
step:128/2315 train_time:7725ms step_avg:60.35ms
step:129/2315 train_time:7785ms step_avg:60.35ms
step:130/2315 train_time:7845ms step_avg:60.34ms
step:131/2315 train_time:7905ms step_avg:60.34ms
step:132/2315 train_time:7965ms step_avg:60.34ms
step:133/2315 train_time:8025ms step_avg:60.34ms
step:134/2315 train_time:8085ms step_avg:60.33ms
step:135/2315 train_time:8145ms step_avg:60.33ms
step:136/2315 train_time:8204ms step_avg:60.33ms
step:137/2315 train_time:8264ms step_avg:60.32ms
step:138/2315 train_time:8324ms step_avg:60.32ms
step:139/2315 train_time:8384ms step_avg:60.32ms
step:140/2315 train_time:8444ms step_avg:60.31ms
step:141/2315 train_time:8504ms step_avg:60.31ms
step:142/2315 train_time:8564ms step_avg:60.31ms
step:143/2315 train_time:8624ms step_avg:60.31ms
step:144/2315 train_time:8684ms step_avg:60.30ms
step:145/2315 train_time:8743ms step_avg:60.30ms
step:146/2315 train_time:8804ms step_avg:60.30ms
step:147/2315 train_time:8864ms step_avg:60.30ms
step:148/2315 train_time:8924ms step_avg:60.30ms
step:149/2315 train_time:8984ms step_avg:60.29ms
step:150/2315 train_time:9043ms step_avg:60.29ms
step:151/2315 train_time:9104ms step_avg:60.29ms
step:152/2315 train_time:9164ms step_avg:60.29ms
step:153/2315 train_time:9224ms step_avg:60.29ms
step:154/2315 train_time:9283ms step_avg:60.28ms
step:155/2315 train_time:9344ms step_avg:60.28ms
step:156/2315 train_time:9404ms step_avg:60.28ms
step:157/2315 train_time:9463ms step_avg:60.28ms
step:158/2315 train_time:9523ms step_avg:60.27ms
step:159/2315 train_time:9583ms step_avg:60.27ms
step:160/2315 train_time:9643ms step_avg:60.27ms
step:161/2315 train_time:9703ms step_avg:60.27ms
step:162/2315 train_time:9763ms step_avg:60.27ms
step:163/2315 train_time:9823ms step_avg:60.27ms
step:164/2315 train_time:9883ms step_avg:60.26ms
step:165/2315 train_time:9943ms step_avg:60.26ms
step:166/2315 train_time:10003ms step_avg:60.26ms
step:167/2315 train_time:10064ms step_avg:60.26ms
step:168/2315 train_time:10124ms step_avg:60.26ms
step:169/2315 train_time:10184ms step_avg:60.26ms
step:170/2315 train_time:10243ms step_avg:60.26ms
step:171/2315 train_time:10304ms step_avg:60.26ms
step:172/2315 train_time:10364ms step_avg:60.26ms
step:173/2315 train_time:10423ms step_avg:60.25ms
step:174/2315 train_time:10483ms step_avg:60.24ms
step:175/2315 train_time:10543ms step_avg:60.25ms
step:176/2315 train_time:10602ms step_avg:60.24ms
step:177/2315 train_time:10662ms step_avg:60.24ms
step:178/2315 train_time:10722ms step_avg:60.24ms
step:179/2315 train_time:10783ms step_avg:60.24ms
step:180/2315 train_time:10844ms step_avg:60.24ms
step:181/2315 train_time:10904ms step_avg:60.24ms
step:182/2315 train_time:10963ms step_avg:60.24ms
step:183/2315 train_time:11023ms step_avg:60.24ms
step:184/2315 train_time:11083ms step_avg:60.23ms
step:185/2315 train_time:11143ms step_avg:60.23ms
step:186/2315 train_time:11202ms step_avg:60.23ms
step:187/2315 train_time:11263ms step_avg:60.23ms
step:188/2315 train_time:11324ms step_avg:60.23ms
step:189/2315 train_time:11383ms step_avg:60.23ms
step:190/2315 train_time:11444ms step_avg:60.23ms
step:191/2315 train_time:11504ms step_avg:60.23ms
step:192/2315 train_time:11564ms step_avg:60.23ms
step:193/2315 train_time:11625ms step_avg:60.23ms
step:194/2315 train_time:11684ms step_avg:60.23ms
step:195/2315 train_time:11744ms step_avg:60.23ms
step:196/2315 train_time:11804ms step_avg:60.22ms
step:197/2315 train_time:11864ms step_avg:60.22ms
step:198/2315 train_time:11924ms step_avg:60.22ms
step:199/2315 train_time:11983ms step_avg:60.22ms
step:200/2315 train_time:12043ms step_avg:60.22ms
step:201/2315 train_time:12103ms step_avg:60.21ms
step:202/2315 train_time:12162ms step_avg:60.21ms
step:203/2315 train_time:12223ms step_avg:60.21ms
step:204/2315 train_time:12282ms step_avg:60.21ms
step:205/2315 train_time:12343ms step_avg:60.21ms
step:206/2315 train_time:12402ms step_avg:60.20ms
step:207/2315 train_time:12463ms step_avg:60.21ms
step:208/2315 train_time:12523ms step_avg:60.21ms
step:209/2315 train_time:12583ms step_avg:60.21ms
step:210/2315 train_time:12643ms step_avg:60.21ms
step:211/2315 train_time:12703ms step_avg:60.20ms
step:212/2315 train_time:12763ms step_avg:60.20ms
step:213/2315 train_time:12824ms step_avg:60.20ms
step:214/2315 train_time:12883ms step_avg:60.20ms
step:215/2315 train_time:12944ms step_avg:60.20ms
step:216/2315 train_time:13004ms step_avg:60.20ms
step:217/2315 train_time:13063ms step_avg:60.20ms
step:218/2315 train_time:13123ms step_avg:60.20ms
step:219/2315 train_time:13183ms step_avg:60.20ms
step:220/2315 train_time:13242ms step_avg:60.19ms
step:221/2315 train_time:13302ms step_avg:60.19ms
step:222/2315 train_time:13363ms step_avg:60.19ms
step:223/2315 train_time:13423ms step_avg:60.19ms
step:224/2315 train_time:13483ms step_avg:60.19ms
step:225/2315 train_time:13543ms step_avg:60.19ms
step:226/2315 train_time:13603ms step_avg:60.19ms
step:227/2315 train_time:13663ms step_avg:60.19ms
step:228/2315 train_time:13722ms step_avg:60.19ms
step:229/2315 train_time:13782ms step_avg:60.18ms
step:230/2315 train_time:13842ms step_avg:60.18ms
step:231/2315 train_time:13903ms step_avg:60.19ms
step:232/2315 train_time:13963ms step_avg:60.19ms
step:233/2315 train_time:14023ms step_avg:60.18ms
step:234/2315 train_time:14083ms step_avg:60.18ms
step:235/2315 train_time:14143ms step_avg:60.18ms
step:236/2315 train_time:14202ms step_avg:60.18ms
step:237/2315 train_time:14263ms step_avg:60.18ms
step:238/2315 train_time:14323ms step_avg:60.18ms
step:239/2315 train_time:14383ms step_avg:60.18ms
step:240/2315 train_time:14443ms step_avg:60.18ms
step:241/2315 train_time:14503ms step_avg:60.18ms
step:242/2315 train_time:14563ms step_avg:60.18ms
step:243/2315 train_time:14624ms step_avg:60.18ms
step:244/2315 train_time:14684ms step_avg:60.18ms
step:245/2315 train_time:14744ms step_avg:60.18ms
step:246/2315 train_time:14803ms step_avg:60.18ms
step:247/2315 train_time:14863ms step_avg:60.18ms
step:248/2315 train_time:14923ms step_avg:60.17ms
step:249/2315 train_time:14983ms step_avg:60.17ms
step:250/2315 train_time:15043ms step_avg:60.17ms
step:250/2315 val_loss:4.0606 train_time:15105ms step_avg:60.42ms
step:251/2315 train_time:15125ms step_avg:60.26ms
step:252/2315 train_time:15165ms step_avg:60.18ms
step:253/2315 train_time:15230ms step_avg:60.20ms
step:254/2315 train_time:15294ms step_avg:60.21ms
step:255/2315 train_time:15355ms step_avg:60.22ms
step:256/2315 train_time:15415ms step_avg:60.22ms
step:257/2315 train_time:15475ms step_avg:60.21ms
step:258/2315 train_time:15534ms step_avg:60.21ms
step:259/2315 train_time:15595ms step_avg:60.21ms
step:260/2315 train_time:15654ms step_avg:60.21ms
step:261/2315 train_time:15714ms step_avg:60.21ms
step:262/2315 train_time:15773ms step_avg:60.20ms
step:263/2315 train_time:15832ms step_avg:60.20ms
step:264/2315 train_time:15892ms step_avg:60.20ms
step:265/2315 train_time:15951ms step_avg:60.19ms
step:266/2315 train_time:16012ms step_avg:60.19ms
step:267/2315 train_time:16073ms step_avg:60.20ms
step:268/2315 train_time:16134ms step_avg:60.20ms
step:269/2315 train_time:16196ms step_avg:60.21ms
step:270/2315 train_time:16258ms step_avg:60.21ms
step:271/2315 train_time:16319ms step_avg:60.22ms
step:272/2315 train_time:16379ms step_avg:60.22ms
step:273/2315 train_time:16440ms step_avg:60.22ms
step:274/2315 train_time:16499ms step_avg:60.22ms
step:275/2315 train_time:16559ms step_avg:60.21ms
step:276/2315 train_time:16618ms step_avg:60.21ms
step:277/2315 train_time:16678ms step_avg:60.21ms
step:278/2315 train_time:16737ms step_avg:60.21ms
step:279/2315 train_time:16797ms step_avg:60.21ms
step:280/2315 train_time:16857ms step_avg:60.20ms
step:281/2315 train_time:16917ms step_avg:60.20ms
step:282/2315 train_time:16977ms step_avg:60.20ms
step:283/2315 train_time:17037ms step_avg:60.20ms
step:284/2315 train_time:17097ms step_avg:60.20ms
step:285/2315 train_time:17158ms step_avg:60.20ms
step:286/2315 train_time:17218ms step_avg:60.20ms
step:287/2315 train_time:17279ms step_avg:60.21ms
step:288/2315 train_time:17340ms step_avg:60.21ms
step:289/2315 train_time:17401ms step_avg:60.21ms
step:290/2315 train_time:17460ms step_avg:60.21ms
step:291/2315 train_time:17520ms step_avg:60.21ms
step:292/2315 train_time:17580ms step_avg:60.21ms
step:293/2315 train_time:17640ms step_avg:60.20ms
step:294/2315 train_time:17699ms step_avg:60.20ms
step:295/2315 train_time:17760ms step_avg:60.20ms
step:296/2315 train_time:17819ms step_avg:60.20ms
step:297/2315 train_time:17879ms step_avg:60.20ms
step:298/2315 train_time:17938ms step_avg:60.19ms
step:299/2315 train_time:17999ms step_avg:60.20ms
step:300/2315 train_time:18059ms step_avg:60.20ms
step:301/2315 train_time:18120ms step_avg:60.20ms
step:302/2315 train_time:18180ms step_avg:60.20ms
step:303/2315 train_time:18240ms step_avg:60.20ms
step:304/2315 train_time:18300ms step_avg:60.20ms
step:305/2315 train_time:18360ms step_avg:60.20ms
step:306/2315 train_time:18419ms step_avg:60.19ms
step:307/2315 train_time:18480ms step_avg:60.19ms
step:308/2315 train_time:18539ms step_avg:60.19ms
step:309/2315 train_time:18599ms step_avg:60.19ms
step:310/2315 train_time:18659ms step_avg:60.19ms
step:311/2315 train_time:18718ms step_avg:60.19ms
step:312/2315 train_time:18778ms step_avg:60.19ms
step:313/2315 train_time:18838ms step_avg:60.18ms
step:314/2315 train_time:18897ms step_avg:60.18ms
step:315/2315 train_time:18957ms step_avg:60.18ms
step:316/2315 train_time:19017ms step_avg:60.18ms
step:317/2315 train_time:19078ms step_avg:60.18ms
step:318/2315 train_time:19138ms step_avg:60.18ms
step:319/2315 train_time:19198ms step_avg:60.18ms
step:320/2315 train_time:19258ms step_avg:60.18ms
step:321/2315 train_time:19318ms step_avg:60.18ms
step:322/2315 train_time:19378ms step_avg:60.18ms
step:323/2315 train_time:19438ms step_avg:60.18ms
step:324/2315 train_time:19499ms step_avg:60.18ms
step:325/2315 train_time:19558ms step_avg:60.18ms
step:326/2315 train_time:19618ms step_avg:60.18ms
step:327/2315 train_time:19678ms step_avg:60.18ms
step:328/2315 train_time:19738ms step_avg:60.18ms
step:329/2315 train_time:19798ms step_avg:60.18ms
step:330/2315 train_time:19857ms step_avg:60.17ms
step:331/2315 train_time:19918ms step_avg:60.17ms
step:332/2315 train_time:19977ms step_avg:60.17ms
step:333/2315 train_time:20038ms step_avg:60.17ms
step:334/2315 train_time:20097ms step_avg:60.17ms
step:335/2315 train_time:20157ms step_avg:60.17ms
step:336/2315 train_time:20217ms step_avg:60.17ms
step:337/2315 train_time:20277ms step_avg:60.17ms
step:338/2315 train_time:20337ms step_avg:60.17ms
step:339/2315 train_time:20397ms step_avg:60.17ms
step:340/2315 train_time:20457ms step_avg:60.17ms
step:341/2315 train_time:20517ms step_avg:60.17ms
step:342/2315 train_time:20576ms step_avg:60.16ms
step:343/2315 train_time:20637ms step_avg:60.17ms
step:344/2315 train_time:20696ms step_avg:60.16ms
step:345/2315 train_time:20756ms step_avg:60.16ms
step:346/2315 train_time:20816ms step_avg:60.16ms
step:347/2315 train_time:20876ms step_avg:60.16ms
step:348/2315 train_time:20936ms step_avg:60.16ms
step:349/2315 train_time:20996ms step_avg:60.16ms
step:350/2315 train_time:21056ms step_avg:60.16ms
step:351/2315 train_time:21116ms step_avg:60.16ms
step:352/2315 train_time:21176ms step_avg:60.16ms
step:353/2315 train_time:21236ms step_avg:60.16ms
step:354/2315 train_time:21296ms step_avg:60.16ms
step:355/2315 train_time:21357ms step_avg:60.16ms
step:356/2315 train_time:21416ms step_avg:60.16ms
step:357/2315 train_time:21477ms step_avg:60.16ms
step:358/2315 train_time:21537ms step_avg:60.16ms
step:359/2315 train_time:21597ms step_avg:60.16ms
step:360/2315 train_time:21657ms step_avg:60.16ms
step:361/2315 train_time:21717ms step_avg:60.16ms
step:362/2315 train_time:21776ms step_avg:60.16ms
step:363/2315 train_time:21836ms step_avg:60.15ms
step:364/2315 train_time:21896ms step_avg:60.15ms
step:365/2315 train_time:21956ms step_avg:60.15ms
step:366/2315 train_time:22016ms step_avg:60.15ms
step:367/2315 train_time:22076ms step_avg:60.15ms
step:368/2315 train_time:22135ms step_avg:60.15ms
step:369/2315 train_time:22195ms step_avg:60.15ms
step:370/2315 train_time:22255ms step_avg:60.15ms
step:371/2315 train_time:22316ms step_avg:60.15ms
step:372/2315 train_time:22376ms step_avg:60.15ms
step:373/2315 train_time:22436ms step_avg:60.15ms
step:374/2315 train_time:22496ms step_avg:60.15ms
step:375/2315 train_time:22556ms step_avg:60.15ms
step:376/2315 train_time:22616ms step_avg:60.15ms
step:377/2315 train_time:22676ms step_avg:60.15ms
step:378/2315 train_time:22736ms step_avg:60.15ms
step:379/2315 train_time:22796ms step_avg:60.15ms
step:380/2315 train_time:22855ms step_avg:60.14ms
step:381/2315 train_time:22915ms step_avg:60.14ms
step:382/2315 train_time:22974ms step_avg:60.14ms
step:383/2315 train_time:23035ms step_avg:60.14ms
step:384/2315 train_time:23095ms step_avg:60.14ms
step:385/2315 train_time:23154ms step_avg:60.14ms
step:386/2315 train_time:23214ms step_avg:60.14ms
step:387/2315 train_time:23275ms step_avg:60.14ms
step:388/2315 train_time:23335ms step_avg:60.14ms
step:389/2315 train_time:23395ms step_avg:60.14ms
step:390/2315 train_time:23455ms step_avg:60.14ms
step:391/2315 train_time:23515ms step_avg:60.14ms
step:392/2315 train_time:23576ms step_avg:60.14ms
step:393/2315 train_time:23637ms step_avg:60.15ms
step:394/2315 train_time:23697ms step_avg:60.14ms
step:395/2315 train_time:23757ms step_avg:60.14ms
step:396/2315 train_time:23816ms step_avg:60.14ms
step:397/2315 train_time:23877ms step_avg:60.14ms
step:398/2315 train_time:23937ms step_avg:60.14ms
step:399/2315 train_time:23997ms step_avg:60.14ms
step:400/2315 train_time:24057ms step_avg:60.14ms
step:401/2315 train_time:24116ms step_avg:60.14ms
step:402/2315 train_time:24176ms step_avg:60.14ms
step:403/2315 train_time:24237ms step_avg:60.14ms
step:404/2315 train_time:24297ms step_avg:60.14ms
step:405/2315 train_time:24357ms step_avg:60.14ms
step:406/2315 train_time:24417ms step_avg:60.14ms
step:407/2315 train_time:24478ms step_avg:60.14ms
step:408/2315 train_time:24538ms step_avg:60.14ms
step:409/2315 train_time:24598ms step_avg:60.14ms
step:410/2315 train_time:24657ms step_avg:60.14ms
step:411/2315 train_time:24717ms step_avg:60.14ms
step:412/2315 train_time:24777ms step_avg:60.14ms
step:413/2315 train_time:24837ms step_avg:60.14ms
step:414/2315 train_time:24898ms step_avg:60.14ms
step:415/2315 train_time:24958ms step_avg:60.14ms
step:416/2315 train_time:25017ms step_avg:60.14ms
step:417/2315 train_time:25077ms step_avg:60.14ms
step:418/2315 train_time:25137ms step_avg:60.14ms
step:419/2315 train_time:25197ms step_avg:60.14ms
step:420/2315 train_time:25257ms step_avg:60.14ms
step:421/2315 train_time:25317ms step_avg:60.14ms
step:422/2315 train_time:25377ms step_avg:60.13ms
step:423/2315 train_time:25437ms step_avg:60.13ms
step:424/2315 train_time:25497ms step_avg:60.13ms
step:425/2315 train_time:25558ms step_avg:60.14ms
step:426/2315 train_time:25618ms step_avg:60.14ms
step:427/2315 train_time:25678ms step_avg:60.14ms
step:428/2315 train_time:25738ms step_avg:60.14ms
step:429/2315 train_time:25798ms step_avg:60.13ms
step:430/2315 train_time:25858ms step_avg:60.13ms
step:431/2315 train_time:25918ms step_avg:60.13ms
step:432/2315 train_time:25978ms step_avg:60.13ms
step:433/2315 train_time:26038ms step_avg:60.13ms
step:434/2315 train_time:26098ms step_avg:60.13ms
step:435/2315 train_time:26158ms step_avg:60.13ms
step:436/2315 train_time:26218ms step_avg:60.13ms
step:437/2315 train_time:26278ms step_avg:60.13ms
step:438/2315 train_time:26338ms step_avg:60.13ms
step:439/2315 train_time:26398ms step_avg:60.13ms
step:440/2315 train_time:26458ms step_avg:60.13ms
step:441/2315 train_time:26518ms step_avg:60.13ms
step:442/2315 train_time:26578ms step_avg:60.13ms
step:443/2315 train_time:26638ms step_avg:60.13ms
step:444/2315 train_time:26698ms step_avg:60.13ms
step:445/2315 train_time:26759ms step_avg:60.13ms
step:446/2315 train_time:26819ms step_avg:60.13ms
step:447/2315 train_time:26879ms step_avg:60.13ms
step:448/2315 train_time:26939ms step_avg:60.13ms
step:449/2315 train_time:26999ms step_avg:60.13ms
step:450/2315 train_time:27059ms step_avg:60.13ms
step:451/2315 train_time:27119ms step_avg:60.13ms
step:452/2315 train_time:27178ms step_avg:60.13ms
step:453/2315 train_time:27239ms step_avg:60.13ms
step:454/2315 train_time:27298ms step_avg:60.13ms
step:455/2315 train_time:27358ms step_avg:60.13ms
step:456/2315 train_time:27417ms step_avg:60.13ms
step:457/2315 train_time:27477ms step_avg:60.12ms
step:458/2315 train_time:27537ms step_avg:60.12ms
step:459/2315 train_time:27597ms step_avg:60.12ms
step:460/2315 train_time:27656ms step_avg:60.12ms
step:461/2315 train_time:27716ms step_avg:60.12ms
step:462/2315 train_time:27776ms step_avg:60.12ms
step:463/2315 train_time:27836ms step_avg:60.12ms
step:464/2315 train_time:27896ms step_avg:60.12ms
step:465/2315 train_time:27957ms step_avg:60.12ms
step:466/2315 train_time:28017ms step_avg:60.12ms
step:467/2315 train_time:28077ms step_avg:60.12ms
step:468/2315 train_time:28137ms step_avg:60.12ms
step:469/2315 train_time:28197ms step_avg:60.12ms
step:470/2315 train_time:28257ms step_avg:60.12ms
step:471/2315 train_time:28318ms step_avg:60.12ms
step:472/2315 train_time:28378ms step_avg:60.12ms
step:473/2315 train_time:28438ms step_avg:60.12ms
step:474/2315 train_time:28498ms step_avg:60.12ms
step:475/2315 train_time:28559ms step_avg:60.12ms
step:476/2315 train_time:28618ms step_avg:60.12ms
step:477/2315 train_time:28678ms step_avg:60.12ms
step:478/2315 train_time:28739ms step_avg:60.12ms
step:479/2315 train_time:28799ms step_avg:60.12ms
step:480/2315 train_time:28859ms step_avg:60.12ms
step:481/2315 train_time:28918ms step_avg:60.12ms
step:482/2315 train_time:28978ms step_avg:60.12ms
step:483/2315 train_time:29038ms step_avg:60.12ms
step:484/2315 train_time:29098ms step_avg:60.12ms
step:485/2315 train_time:29158ms step_avg:60.12ms
step:486/2315 train_time:29218ms step_avg:60.12ms
step:487/2315 train_time:29278ms step_avg:60.12ms
step:488/2315 train_time:29337ms step_avg:60.12ms
step:489/2315 train_time:29398ms step_avg:60.12ms
step:490/2315 train_time:29457ms step_avg:60.12ms
step:491/2315 train_time:29517ms step_avg:60.12ms
step:492/2315 train_time:29577ms step_avg:60.12ms
step:493/2315 train_time:29637ms step_avg:60.12ms
step:494/2315 train_time:29697ms step_avg:60.12ms
step:495/2315 train_time:29758ms step_avg:60.12ms
step:496/2315 train_time:29818ms step_avg:60.12ms
step:497/2315 train_time:29878ms step_avg:60.12ms
step:498/2315 train_time:29938ms step_avg:60.12ms
step:499/2315 train_time:29997ms step_avg:60.12ms
step:500/2315 train_time:30057ms step_avg:60.11ms
step:500/2315 val_loss:3.8102 train_time:30119ms step_avg:60.24ms
step:501/2315 train_time:30140ms step_avg:60.16ms
step:502/2315 train_time:30178ms step_avg:60.12ms
step:503/2315 train_time:30243ms step_avg:60.13ms
step:504/2315 train_time:30306ms step_avg:60.13ms
step:505/2315 train_time:30366ms step_avg:60.13ms
step:506/2315 train_time:30425ms step_avg:60.13ms
step:507/2315 train_time:30485ms step_avg:60.13ms
step:508/2315 train_time:30545ms step_avg:60.13ms
step:509/2315 train_time:30605ms step_avg:60.13ms
step:510/2315 train_time:30664ms step_avg:60.12ms
step:511/2315 train_time:30723ms step_avg:60.12ms
step:512/2315 train_time:30782ms step_avg:60.12ms
step:513/2315 train_time:30841ms step_avg:60.12ms
step:514/2315 train_time:30900ms step_avg:60.12ms
step:515/2315 train_time:30959ms step_avg:60.11ms
step:516/2315 train_time:31018ms step_avg:60.11ms
step:517/2315 train_time:31078ms step_avg:60.11ms
step:518/2315 train_time:31139ms step_avg:60.11ms
step:519/2315 train_time:31202ms step_avg:60.12ms
step:520/2315 train_time:31264ms step_avg:60.12ms
step:521/2315 train_time:31325ms step_avg:60.12ms
step:522/2315 train_time:31385ms step_avg:60.13ms
step:523/2315 train_time:31445ms step_avg:60.12ms
step:524/2315 train_time:31505ms step_avg:60.12ms
step:525/2315 train_time:31565ms step_avg:60.12ms
step:526/2315 train_time:31625ms step_avg:60.12ms
step:527/2315 train_time:31684ms step_avg:60.12ms
step:528/2315 train_time:31743ms step_avg:60.12ms
step:529/2315 train_time:31802ms step_avg:60.12ms
step:530/2315 train_time:31862ms step_avg:60.12ms
step:531/2315 train_time:31922ms step_avg:60.12ms
step:532/2315 train_time:31981ms step_avg:60.11ms
step:533/2315 train_time:32042ms step_avg:60.12ms
step:534/2315 train_time:32102ms step_avg:60.12ms
step:535/2315 train_time:32163ms step_avg:60.12ms
step:536/2315 train_time:32224ms step_avg:60.12ms
step:537/2315 train_time:32286ms step_avg:60.12ms
step:538/2315 train_time:32347ms step_avg:60.12ms
step:539/2315 train_time:32408ms step_avg:60.13ms
step:540/2315 train_time:32468ms step_avg:60.13ms
step:541/2315 train_time:32528ms step_avg:60.12ms
step:542/2315 train_time:32587ms step_avg:60.12ms
step:543/2315 train_time:32647ms step_avg:60.12ms
step:544/2315 train_time:32706ms step_avg:60.12ms
step:545/2315 train_time:32766ms step_avg:60.12ms
step:546/2315 train_time:32826ms step_avg:60.12ms
step:547/2315 train_time:32886ms step_avg:60.12ms
step:548/2315 train_time:32946ms step_avg:60.12ms
step:549/2315 train_time:33006ms step_avg:60.12ms
step:550/2315 train_time:33066ms step_avg:60.12ms
step:551/2315 train_time:33126ms step_avg:60.12ms
step:552/2315 train_time:33187ms step_avg:60.12ms
step:553/2315 train_time:33248ms step_avg:60.12ms
step:554/2315 train_time:33308ms step_avg:60.12ms
step:555/2315 train_time:33369ms step_avg:60.12ms
step:556/2315 train_time:33428ms step_avg:60.12ms
step:557/2315 train_time:33489ms step_avg:60.12ms
step:558/2315 train_time:33548ms step_avg:60.12ms
step:559/2315 train_time:33609ms step_avg:60.12ms
step:560/2315 train_time:33668ms step_avg:60.12ms
step:561/2315 train_time:33729ms step_avg:60.12ms
step:562/2315 train_time:33789ms step_avg:60.12ms
step:563/2315 train_time:33849ms step_avg:60.12ms
step:564/2315 train_time:33909ms step_avg:60.12ms
step:565/2315 train_time:33969ms step_avg:60.12ms
step:566/2315 train_time:34029ms step_avg:60.12ms
step:567/2315 train_time:34089ms step_avg:60.12ms
step:568/2315 train_time:34149ms step_avg:60.12ms
step:569/2315 train_time:34210ms step_avg:60.12ms
step:570/2315 train_time:34270ms step_avg:60.12ms
step:571/2315 train_time:34330ms step_avg:60.12ms
step:572/2315 train_time:34390ms step_avg:60.12ms
step:573/2315 train_time:34450ms step_avg:60.12ms
step:574/2315 train_time:34509ms step_avg:60.12ms
step:575/2315 train_time:34570ms step_avg:60.12ms
step:576/2315 train_time:34629ms step_avg:60.12ms
step:577/2315 train_time:34689ms step_avg:60.12ms
step:578/2315 train_time:34750ms step_avg:60.12ms
step:579/2315 train_time:34810ms step_avg:60.12ms
step:580/2315 train_time:34870ms step_avg:60.12ms
step:581/2315 train_time:34930ms step_avg:60.12ms
step:582/2315 train_time:34990ms step_avg:60.12ms
step:583/2315 train_time:35050ms step_avg:60.12ms
step:584/2315 train_time:35110ms step_avg:60.12ms
step:585/2315 train_time:35171ms step_avg:60.12ms
step:586/2315 train_time:35231ms step_avg:60.12ms
step:587/2315 train_time:35291ms step_avg:60.12ms
step:588/2315 train_time:35351ms step_avg:60.12ms
step:589/2315 train_time:35411ms step_avg:60.12ms
step:590/2315 train_time:35471ms step_avg:60.12ms
step:591/2315 train_time:35531ms step_avg:60.12ms
step:592/2315 train_time:35591ms step_avg:60.12ms
step:593/2315 train_time:35651ms step_avg:60.12ms
step:594/2315 train_time:35711ms step_avg:60.12ms
step:595/2315 train_time:35771ms step_avg:60.12ms
step:596/2315 train_time:35831ms step_avg:60.12ms
step:597/2315 train_time:35891ms step_avg:60.12ms
step:598/2315 train_time:35951ms step_avg:60.12ms
step:599/2315 train_time:36011ms step_avg:60.12ms
step:600/2315 train_time:36071ms step_avg:60.12ms
step:601/2315 train_time:36132ms step_avg:60.12ms
step:602/2315 train_time:36191ms step_avg:60.12ms
step:603/2315 train_time:36252ms step_avg:60.12ms
step:604/2315 train_time:36311ms step_avg:60.12ms
step:605/2315 train_time:36372ms step_avg:60.12ms
step:606/2315 train_time:36431ms step_avg:60.12ms
step:607/2315 train_time:36492ms step_avg:60.12ms
step:608/2315 train_time:36551ms step_avg:60.12ms
step:609/2315 train_time:36611ms step_avg:60.12ms
step:610/2315 train_time:36671ms step_avg:60.12ms
step:611/2315 train_time:36732ms step_avg:60.12ms
step:612/2315 train_time:36792ms step_avg:60.12ms
step:613/2315 train_time:36852ms step_avg:60.12ms
step:614/2315 train_time:36912ms step_avg:60.12ms
step:615/2315 train_time:36972ms step_avg:60.12ms
step:616/2315 train_time:37032ms step_avg:60.12ms
step:617/2315 train_time:37092ms step_avg:60.12ms
step:618/2315 train_time:37152ms step_avg:60.12ms
step:619/2315 train_time:37212ms step_avg:60.12ms
step:620/2315 train_time:37271ms step_avg:60.11ms
step:621/2315 train_time:37332ms step_avg:60.12ms
step:622/2315 train_time:37392ms step_avg:60.12ms
step:623/2315 train_time:37451ms step_avg:60.11ms
step:624/2315 train_time:37511ms step_avg:60.11ms
step:625/2315 train_time:37571ms step_avg:60.11ms
step:626/2315 train_time:37631ms step_avg:60.11ms
step:627/2315 train_time:37691ms step_avg:60.11ms
step:628/2315 train_time:37751ms step_avg:60.11ms
step:629/2315 train_time:37812ms step_avg:60.11ms
step:630/2315 train_time:37872ms step_avg:60.11ms
step:631/2315 train_time:37932ms step_avg:60.11ms
step:632/2315 train_time:37992ms step_avg:60.11ms
step:633/2315 train_time:38052ms step_avg:60.11ms
step:634/2315 train_time:38112ms step_avg:60.11ms
step:635/2315 train_time:38172ms step_avg:60.11ms
step:636/2315 train_time:38232ms step_avg:60.11ms
step:637/2315 train_time:38293ms step_avg:60.11ms
step:638/2315 train_time:38352ms step_avg:60.11ms
step:639/2315 train_time:38412ms step_avg:60.11ms
step:640/2315 train_time:38472ms step_avg:60.11ms
step:641/2315 train_time:38532ms step_avg:60.11ms
step:642/2315 train_time:38592ms step_avg:60.11ms
step:643/2315 train_time:38652ms step_avg:60.11ms
step:644/2315 train_time:38712ms step_avg:60.11ms
step:645/2315 train_time:38772ms step_avg:60.11ms
step:646/2315 train_time:38833ms step_avg:60.11ms
step:647/2315 train_time:38893ms step_avg:60.11ms
step:648/2315 train_time:38953ms step_avg:60.11ms
step:649/2315 train_time:39014ms step_avg:60.11ms
step:650/2315 train_time:39073ms step_avg:60.11ms
step:651/2315 train_time:39134ms step_avg:60.11ms
step:652/2315 train_time:39194ms step_avg:60.11ms
step:653/2315 train_time:39254ms step_avg:60.11ms
step:654/2315 train_time:39314ms step_avg:60.11ms
step:655/2315 train_time:39374ms step_avg:60.11ms
step:656/2315 train_time:39434ms step_avg:60.11ms
step:657/2315 train_time:39494ms step_avg:60.11ms
step:658/2315 train_time:39554ms step_avg:60.11ms
step:659/2315 train_time:39614ms step_avg:60.11ms
step:660/2315 train_time:39674ms step_avg:60.11ms
step:661/2315 train_time:39735ms step_avg:60.11ms
step:662/2315 train_time:39796ms step_avg:60.11ms
step:663/2315 train_time:39856ms step_avg:60.11ms
step:664/2315 train_time:39915ms step_avg:60.11ms
step:665/2315 train_time:39976ms step_avg:60.11ms
step:666/2315 train_time:40036ms step_avg:60.11ms
step:667/2315 train_time:40096ms step_avg:60.11ms
step:668/2315 train_time:40156ms step_avg:60.11ms
step:669/2315 train_time:40216ms step_avg:60.11ms
step:670/2315 train_time:40275ms step_avg:60.11ms
step:671/2315 train_time:40336ms step_avg:60.11ms
step:672/2315 train_time:40396ms step_avg:60.11ms
step:673/2315 train_time:40456ms step_avg:60.11ms
step:674/2315 train_time:40516ms step_avg:60.11ms
step:675/2315 train_time:40576ms step_avg:60.11ms
step:676/2315 train_time:40636ms step_avg:60.11ms
step:677/2315 train_time:40696ms step_avg:60.11ms
step:678/2315 train_time:40756ms step_avg:60.11ms
step:679/2315 train_time:40817ms step_avg:60.11ms
step:680/2315 train_time:40876ms step_avg:60.11ms
step:681/2315 train_time:40937ms step_avg:60.11ms
step:682/2315 train_time:40996ms step_avg:60.11ms
step:683/2315 train_time:41056ms step_avg:60.11ms
step:684/2315 train_time:41116ms step_avg:60.11ms
step:685/2315 train_time:41176ms step_avg:60.11ms
step:686/2315 train_time:41236ms step_avg:60.11ms
step:687/2315 train_time:41296ms step_avg:60.11ms
step:688/2315 train_time:41356ms step_avg:60.11ms
step:689/2315 train_time:41416ms step_avg:60.11ms
step:690/2315 train_time:41476ms step_avg:60.11ms
step:691/2315 train_time:41537ms step_avg:60.11ms
step:692/2315 train_time:41596ms step_avg:60.11ms
step:693/2315 train_time:41656ms step_avg:60.11ms
step:694/2315 train_time:41715ms step_avg:60.11ms
step:695/2315 train_time:41776ms step_avg:60.11ms
step:696/2315 train_time:41836ms step_avg:60.11ms
step:697/2315 train_time:41898ms step_avg:60.11ms
step:698/2315 train_time:41957ms step_avg:60.11ms
step:699/2315 train_time:42017ms step_avg:60.11ms
step:700/2315 train_time:42077ms step_avg:60.11ms
step:701/2315 train_time:42137ms step_avg:60.11ms
step:702/2315 train_time:42197ms step_avg:60.11ms
step:703/2315 train_time:42257ms step_avg:60.11ms
step:704/2315 train_time:42318ms step_avg:60.11ms
step:705/2315 train_time:42378ms step_avg:60.11ms
step:706/2315 train_time:42438ms step_avg:60.11ms
step:707/2315 train_time:42498ms step_avg:60.11ms
step:708/2315 train_time:42558ms step_avg:60.11ms
step:709/2315 train_time:42618ms step_avg:60.11ms
step:710/2315 train_time:42679ms step_avg:60.11ms
step:711/2315 train_time:42739ms step_avg:60.11ms
step:712/2315 train_time:42799ms step_avg:60.11ms
step:713/2315 train_time:42859ms step_avg:60.11ms
step:714/2315 train_time:42919ms step_avg:60.11ms
step:715/2315 train_time:42978ms step_avg:60.11ms
step:716/2315 train_time:43038ms step_avg:60.11ms
step:717/2315 train_time:43098ms step_avg:60.11ms
step:718/2315 train_time:43158ms step_avg:60.11ms
step:719/2315 train_time:43218ms step_avg:60.11ms
step:720/2315 train_time:43277ms step_avg:60.11ms
step:721/2315 train_time:43337ms step_avg:60.11ms
step:722/2315 train_time:43397ms step_avg:60.11ms
step:723/2315 train_time:43457ms step_avg:60.11ms
step:724/2315 train_time:43518ms step_avg:60.11ms
step:725/2315 train_time:43578ms step_avg:60.11ms
step:726/2315 train_time:43638ms step_avg:60.11ms
step:727/2315 train_time:43698ms step_avg:60.11ms
step:728/2315 train_time:43758ms step_avg:60.11ms
step:729/2315 train_time:43818ms step_avg:60.11ms
step:730/2315 train_time:43878ms step_avg:60.11ms
step:731/2315 train_time:43938ms step_avg:60.11ms
step:732/2315 train_time:43997ms step_avg:60.11ms
step:733/2315 train_time:44057ms step_avg:60.11ms
step:734/2315 train_time:44117ms step_avg:60.10ms
step:735/2315 train_time:44177ms step_avg:60.10ms
step:736/2315 train_time:44237ms step_avg:60.10ms
step:737/2315 train_time:44296ms step_avg:60.10ms
step:738/2315 train_time:44356ms step_avg:60.10ms
step:739/2315 train_time:44416ms step_avg:60.10ms
step:740/2315 train_time:44477ms step_avg:60.10ms
step:741/2315 train_time:44537ms step_avg:60.10ms
step:742/2315 train_time:44598ms step_avg:60.10ms
step:743/2315 train_time:44658ms step_avg:60.10ms
step:744/2315 train_time:44718ms step_avg:60.11ms
step:745/2315 train_time:44778ms step_avg:60.10ms
step:746/2315 train_time:44839ms step_avg:60.11ms
step:747/2315 train_time:44899ms step_avg:60.11ms
step:748/2315 train_time:44959ms step_avg:60.11ms
step:749/2315 train_time:45019ms step_avg:60.11ms
step:750/2315 train_time:45079ms step_avg:60.11ms
step:750/2315 val_loss:3.6810 train_time:45140ms step_avg:60.19ms
step:751/2315 train_time:45160ms step_avg:60.13ms
step:752/2315 train_time:45201ms step_avg:60.11ms
step:753/2315 train_time:45264ms step_avg:60.11ms
step:754/2315 train_time:45327ms step_avg:60.12ms
step:755/2315 train_time:45390ms step_avg:60.12ms
step:756/2315 train_time:45450ms step_avg:60.12ms
step:757/2315 train_time:45510ms step_avg:60.12ms
step:758/2315 train_time:45569ms step_avg:60.12ms
step:759/2315 train_time:45629ms step_avg:60.12ms
step:760/2315 train_time:45688ms step_avg:60.12ms
step:761/2315 train_time:45748ms step_avg:60.12ms
step:762/2315 train_time:45808ms step_avg:60.12ms
step:763/2315 train_time:45868ms step_avg:60.12ms
step:764/2315 train_time:45929ms step_avg:60.12ms
step:765/2315 train_time:45989ms step_avg:60.12ms
step:766/2315 train_time:46049ms step_avg:60.12ms
step:767/2315 train_time:46112ms step_avg:60.12ms
step:768/2315 train_time:46174ms step_avg:60.12ms
step:769/2315 train_time:46238ms step_avg:60.13ms
step:770/2315 train_time:46299ms step_avg:60.13ms
step:771/2315 train_time:46361ms step_avg:60.13ms
step:772/2315 train_time:46421ms step_avg:60.13ms
step:773/2315 train_time:46482ms step_avg:60.13ms
step:774/2315 train_time:46543ms step_avg:60.13ms
step:775/2315 train_time:46604ms step_avg:60.13ms
step:776/2315 train_time:46663ms step_avg:60.13ms
step:777/2315 train_time:46724ms step_avg:60.13ms
step:778/2315 train_time:46784ms step_avg:60.13ms
step:779/2315 train_time:46844ms step_avg:60.13ms
step:780/2315 train_time:46904ms step_avg:60.13ms
step:781/2315 train_time:46965ms step_avg:60.13ms
step:782/2315 train_time:47026ms step_avg:60.14ms
step:783/2315 train_time:47087ms step_avg:60.14ms
step:784/2315 train_time:47149ms step_avg:60.14ms
step:785/2315 train_time:47211ms step_avg:60.14ms
step:786/2315 train_time:47272ms step_avg:60.14ms
step:787/2315 train_time:47334ms step_avg:60.14ms
step:788/2315 train_time:47395ms step_avg:60.15ms
step:789/2315 train_time:47456ms step_avg:60.15ms
step:790/2315 train_time:47516ms step_avg:60.15ms
step:791/2315 train_time:47578ms step_avg:60.15ms
step:792/2315 train_time:47638ms step_avg:60.15ms
step:793/2315 train_time:47699ms step_avg:60.15ms
step:794/2315 train_time:47759ms step_avg:60.15ms
step:795/2315 train_time:47820ms step_avg:60.15ms
step:796/2315 train_time:47880ms step_avg:60.15ms
step:797/2315 train_time:47941ms step_avg:60.15ms
step:798/2315 train_time:48001ms step_avg:60.15ms
step:799/2315 train_time:48062ms step_avg:60.15ms
step:800/2315 train_time:48123ms step_avg:60.15ms
step:801/2315 train_time:48186ms step_avg:60.16ms
step:802/2315 train_time:48247ms step_avg:60.16ms
step:803/2315 train_time:48307ms step_avg:60.16ms
step:804/2315 train_time:48368ms step_avg:60.16ms
step:805/2315 train_time:48430ms step_avg:60.16ms
step:806/2315 train_time:48490ms step_avg:60.16ms
step:807/2315 train_time:48551ms step_avg:60.16ms
step:808/2315 train_time:48611ms step_avg:60.16ms
step:809/2315 train_time:48672ms step_avg:60.16ms
step:810/2315 train_time:48733ms step_avg:60.16ms
step:811/2315 train_time:48795ms step_avg:60.17ms
step:812/2315 train_time:48856ms step_avg:60.17ms
step:813/2315 train_time:48917ms step_avg:60.17ms
step:814/2315 train_time:48977ms step_avg:60.17ms
step:815/2315 train_time:49038ms step_avg:60.17ms
step:816/2315 train_time:49099ms step_avg:60.17ms
step:817/2315 train_time:49160ms step_avg:60.17ms
step:818/2315 train_time:49221ms step_avg:60.17ms
step:819/2315 train_time:49282ms step_avg:60.17ms
step:820/2315 train_time:49342ms step_avg:60.17ms
step:821/2315 train_time:49403ms step_avg:60.17ms
step:822/2315 train_time:49464ms step_avg:60.17ms
step:823/2315 train_time:49525ms step_avg:60.18ms
step:824/2315 train_time:49586ms step_avg:60.18ms
step:825/2315 train_time:49648ms step_avg:60.18ms
step:826/2315 train_time:49708ms step_avg:60.18ms
step:827/2315 train_time:49770ms step_avg:60.18ms
step:828/2315 train_time:49831ms step_avg:60.18ms
step:829/2315 train_time:49891ms step_avg:60.18ms
step:830/2315 train_time:49952ms step_avg:60.18ms
step:831/2315 train_time:50013ms step_avg:60.18ms
step:832/2315 train_time:50074ms step_avg:60.19ms
step:833/2315 train_time:50135ms step_avg:60.19ms
step:834/2315 train_time:50196ms step_avg:60.19ms
step:835/2315 train_time:50257ms step_avg:60.19ms
step:836/2315 train_time:50317ms step_avg:60.19ms
step:837/2315 train_time:50379ms step_avg:60.19ms
step:838/2315 train_time:50440ms step_avg:60.19ms
step:839/2315 train_time:50501ms step_avg:60.19ms
step:840/2315 train_time:50562ms step_avg:60.19ms
step:841/2315 train_time:50623ms step_avg:60.19ms
step:842/2315 train_time:50683ms step_avg:60.19ms
step:843/2315 train_time:50744ms step_avg:60.19ms
step:844/2315 train_time:50805ms step_avg:60.20ms
step:845/2315 train_time:50866ms step_avg:60.20ms
step:846/2315 train_time:50926ms step_avg:60.20ms
step:847/2315 train_time:50988ms step_avg:60.20ms
step:848/2315 train_time:51048ms step_avg:60.20ms
step:849/2315 train_time:51110ms step_avg:60.20ms
step:850/2315 train_time:51170ms step_avg:60.20ms
step:851/2315 train_time:51231ms step_avg:60.20ms
step:852/2315 train_time:51292ms step_avg:60.20ms
step:853/2315 train_time:51353ms step_avg:60.20ms
step:854/2315 train_time:51415ms step_avg:60.20ms
step:855/2315 train_time:51476ms step_avg:60.21ms
step:856/2315 train_time:51537ms step_avg:60.21ms
step:857/2315 train_time:51598ms step_avg:60.21ms
step:858/2315 train_time:51659ms step_avg:60.21ms
step:859/2315 train_time:51719ms step_avg:60.21ms
step:860/2315 train_time:51781ms step_avg:60.21ms
step:861/2315 train_time:51841ms step_avg:60.21ms
step:862/2315 train_time:51902ms step_avg:60.21ms
step:863/2315 train_time:51963ms step_avg:60.21ms
step:864/2315 train_time:52023ms step_avg:60.21ms
step:865/2315 train_time:52084ms step_avg:60.21ms
step:866/2315 train_time:52145ms step_avg:60.21ms
step:867/2315 train_time:52207ms step_avg:60.22ms
step:868/2315 train_time:52269ms step_avg:60.22ms
step:869/2315 train_time:52330ms step_avg:60.22ms
step:870/2315 train_time:52391ms step_avg:60.22ms
step:871/2315 train_time:52452ms step_avg:60.22ms
step:872/2315 train_time:52513ms step_avg:60.22ms
step:873/2315 train_time:52574ms step_avg:60.22ms
step:874/2315 train_time:52635ms step_avg:60.22ms
step:875/2315 train_time:52696ms step_avg:60.22ms
step:876/2315 train_time:52757ms step_avg:60.22ms
step:877/2315 train_time:52819ms step_avg:60.23ms
step:878/2315 train_time:52879ms step_avg:60.23ms
step:879/2315 train_time:52940ms step_avg:60.23ms
step:880/2315 train_time:53000ms step_avg:60.23ms
step:881/2315 train_time:53061ms step_avg:60.23ms
step:882/2315 train_time:53122ms step_avg:60.23ms
step:883/2315 train_time:53183ms step_avg:60.23ms
step:884/2315 train_time:53243ms step_avg:60.23ms
step:885/2315 train_time:53304ms step_avg:60.23ms
step:886/2315 train_time:53365ms step_avg:60.23ms
step:887/2315 train_time:53427ms step_avg:60.23ms
step:888/2315 train_time:53488ms step_avg:60.23ms
step:889/2315 train_time:53550ms step_avg:60.24ms
step:890/2315 train_time:53610ms step_avg:60.24ms
step:891/2315 train_time:53671ms step_avg:60.24ms
step:892/2315 train_time:53731ms step_avg:60.24ms
step:893/2315 train_time:53793ms step_avg:60.24ms
step:894/2315 train_time:53854ms step_avg:60.24ms
step:895/2315 train_time:53915ms step_avg:60.24ms
step:896/2315 train_time:53976ms step_avg:60.24ms
step:897/2315 train_time:54037ms step_avg:60.24ms
step:898/2315 train_time:54098ms step_avg:60.24ms
step:899/2315 train_time:54159ms step_avg:60.24ms
step:900/2315 train_time:54220ms step_avg:60.24ms
step:901/2315 train_time:54281ms step_avg:60.24ms
step:902/2315 train_time:54341ms step_avg:60.25ms
step:903/2315 train_time:54402ms step_avg:60.25ms
step:904/2315 train_time:54463ms step_avg:60.25ms
step:905/2315 train_time:54524ms step_avg:60.25ms
step:906/2315 train_time:54586ms step_avg:60.25ms
step:907/2315 train_time:54647ms step_avg:60.25ms
step:908/2315 train_time:54708ms step_avg:60.25ms
step:909/2315 train_time:54769ms step_avg:60.25ms
step:910/2315 train_time:54830ms step_avg:60.25ms
step:911/2315 train_time:54891ms step_avg:60.25ms
step:912/2315 train_time:54952ms step_avg:60.25ms
step:913/2315 train_time:55012ms step_avg:60.25ms
step:914/2315 train_time:55074ms step_avg:60.26ms
step:915/2315 train_time:55135ms step_avg:60.26ms
step:916/2315 train_time:55195ms step_avg:60.26ms
step:917/2315 train_time:55256ms step_avg:60.26ms
step:918/2315 train_time:55317ms step_avg:60.26ms
step:919/2315 train_time:55378ms step_avg:60.26ms
step:920/2315 train_time:55438ms step_avg:60.26ms
step:921/2315 train_time:55499ms step_avg:60.26ms
step:922/2315 train_time:55560ms step_avg:60.26ms
step:923/2315 train_time:55621ms step_avg:60.26ms
step:924/2315 train_time:55682ms step_avg:60.26ms
step:925/2315 train_time:55743ms step_avg:60.26ms
step:926/2315 train_time:55804ms step_avg:60.26ms
step:927/2315 train_time:55866ms step_avg:60.27ms
step:928/2315 train_time:55927ms step_avg:60.27ms
step:929/2315 train_time:55988ms step_avg:60.27ms
step:930/2315 train_time:56048ms step_avg:60.27ms
step:931/2315 train_time:56110ms step_avg:60.27ms
step:932/2315 train_time:56170ms step_avg:60.27ms
step:933/2315 train_time:56231ms step_avg:60.27ms
step:934/2315 train_time:56292ms step_avg:60.27ms
step:935/2315 train_time:56353ms step_avg:60.27ms
step:936/2315 train_time:56414ms step_avg:60.27ms
step:937/2315 train_time:56476ms step_avg:60.27ms
step:938/2315 train_time:56537ms step_avg:60.27ms
step:939/2315 train_time:56598ms step_avg:60.27ms
step:940/2315 train_time:56659ms step_avg:60.28ms
step:941/2315 train_time:56720ms step_avg:60.28ms
step:942/2315 train_time:56780ms step_avg:60.28ms
step:943/2315 train_time:56841ms step_avg:60.28ms
step:944/2315 train_time:56901ms step_avg:60.28ms
step:945/2315 train_time:56962ms step_avg:60.28ms
step:946/2315 train_time:57022ms step_avg:60.28ms
step:947/2315 train_time:57083ms step_avg:60.28ms
step:948/2315 train_time:57145ms step_avg:60.28ms
step:949/2315 train_time:57207ms step_avg:60.28ms
step:950/2315 train_time:57268ms step_avg:60.28ms
step:951/2315 train_time:57330ms step_avg:60.28ms
step:952/2315 train_time:57391ms step_avg:60.28ms
step:953/2315 train_time:57451ms step_avg:60.28ms
step:954/2315 train_time:57512ms step_avg:60.29ms
step:955/2315 train_time:57573ms step_avg:60.29ms
step:956/2315 train_time:57634ms step_avg:60.29ms
step:957/2315 train_time:57695ms step_avg:60.29ms
step:958/2315 train_time:57756ms step_avg:60.29ms
step:959/2315 train_time:57817ms step_avg:60.29ms
step:960/2315 train_time:57877ms step_avg:60.29ms
step:961/2315 train_time:57938ms step_avg:60.29ms
step:962/2315 train_time:57999ms step_avg:60.29ms
step:963/2315 train_time:58060ms step_avg:60.29ms
step:964/2315 train_time:58121ms step_avg:60.29ms
step:965/2315 train_time:58182ms step_avg:60.29ms
step:966/2315 train_time:58242ms step_avg:60.29ms
step:967/2315 train_time:58303ms step_avg:60.29ms
step:968/2315 train_time:58364ms step_avg:60.29ms
step:969/2315 train_time:58426ms step_avg:60.29ms
step:970/2315 train_time:58487ms step_avg:60.30ms
step:971/2315 train_time:58547ms step_avg:60.30ms
step:972/2315 train_time:58608ms step_avg:60.30ms
step:973/2315 train_time:58669ms step_avg:60.30ms
step:974/2315 train_time:58730ms step_avg:60.30ms
step:975/2315 train_time:58791ms step_avg:60.30ms
step:976/2315 train_time:58852ms step_avg:60.30ms
step:977/2315 train_time:58913ms step_avg:60.30ms
step:978/2315 train_time:58974ms step_avg:60.30ms
step:979/2315 train_time:59035ms step_avg:60.30ms
step:980/2315 train_time:59095ms step_avg:60.30ms
step:981/2315 train_time:59157ms step_avg:60.30ms
step:982/2315 train_time:59218ms step_avg:60.30ms
step:983/2315 train_time:59279ms step_avg:60.30ms
step:984/2315 train_time:59339ms step_avg:60.30ms
step:985/2315 train_time:59400ms step_avg:60.30ms
step:986/2315 train_time:59461ms step_avg:60.30ms
step:987/2315 train_time:59521ms step_avg:60.31ms
step:988/2315 train_time:59582ms step_avg:60.31ms
step:989/2315 train_time:59643ms step_avg:60.31ms
step:990/2315 train_time:59703ms step_avg:60.31ms
step:991/2315 train_time:59764ms step_avg:60.31ms
step:992/2315 train_time:59825ms step_avg:60.31ms
step:993/2315 train_time:59886ms step_avg:60.31ms
step:994/2315 train_time:59947ms step_avg:60.31ms
step:995/2315 train_time:60008ms step_avg:60.31ms
step:996/2315 train_time:60069ms step_avg:60.31ms
step:997/2315 train_time:60130ms step_avg:60.31ms
step:998/2315 train_time:60190ms step_avg:60.31ms
step:999/2315 train_time:60251ms step_avg:60.31ms
step:1000/2315 train_time:60313ms step_avg:60.31ms
step:1000/2315 val_loss:3.5797 train_time:60376ms step_avg:60.38ms
step:1001/2315 train_time:60395ms step_avg:60.33ms
step:1002/2315 train_time:60436ms step_avg:60.32ms
step:1003/2315 train_time:60500ms step_avg:60.32ms
step:1004/2315 train_time:60566ms step_avg:60.33ms
step:1005/2315 train_time:60628ms step_avg:60.33ms
step:1006/2315 train_time:60688ms step_avg:60.33ms
step:1007/2315 train_time:60749ms step_avg:60.33ms
step:1008/2315 train_time:60808ms step_avg:60.33ms
step:1009/2315 train_time:60869ms step_avg:60.33ms
step:1010/2315 train_time:60929ms step_avg:60.33ms
step:1011/2315 train_time:60989ms step_avg:60.33ms
step:1012/2315 train_time:61049ms step_avg:60.33ms
step:1013/2315 train_time:61109ms step_avg:60.32ms
step:1014/2315 train_time:61169ms step_avg:60.32ms
step:1015/2315 train_time:61229ms step_avg:60.32ms
step:1016/2315 train_time:61291ms step_avg:60.33ms
step:1017/2315 train_time:61354ms step_avg:60.33ms
step:1018/2315 train_time:61416ms step_avg:60.33ms
step:1019/2315 train_time:61479ms step_avg:60.33ms
step:1020/2315 train_time:61541ms step_avg:60.33ms
step:1021/2315 train_time:61602ms step_avg:60.33ms
step:1022/2315 train_time:61662ms step_avg:60.33ms
step:1023/2315 train_time:61722ms step_avg:60.33ms
step:1024/2315 train_time:61784ms step_avg:60.34ms
step:1025/2315 train_time:61843ms step_avg:60.33ms
step:1026/2315 train_time:61903ms step_avg:60.33ms
step:1027/2315 train_time:61963ms step_avg:60.33ms
step:1028/2315 train_time:62024ms step_avg:60.33ms
step:1029/2315 train_time:62084ms step_avg:60.33ms
step:1030/2315 train_time:62144ms step_avg:60.33ms
step:1031/2315 train_time:62204ms step_avg:60.33ms
step:1032/2315 train_time:62265ms step_avg:60.33ms
step:1033/2315 train_time:62328ms step_avg:60.34ms
step:1034/2315 train_time:62389ms step_avg:60.34ms
step:1035/2315 train_time:62451ms step_avg:60.34ms
step:1036/2315 train_time:62512ms step_avg:60.34ms
step:1037/2315 train_time:62574ms step_avg:60.34ms
step:1038/2315 train_time:62635ms step_avg:60.34ms
step:1039/2315 train_time:62697ms step_avg:60.34ms
step:1040/2315 train_time:62757ms step_avg:60.34ms
step:1041/2315 train_time:62818ms step_avg:60.34ms
step:1042/2315 train_time:62878ms step_avg:60.34ms
step:1043/2315 train_time:62938ms step_avg:60.34ms
step:1044/2315 train_time:62999ms step_avg:60.34ms
step:1045/2315 train_time:63059ms step_avg:60.34ms
step:1046/2315 train_time:63119ms step_avg:60.34ms
step:1047/2315 train_time:63180ms step_avg:60.34ms
step:1048/2315 train_time:63240ms step_avg:60.34ms
step:1049/2315 train_time:63301ms step_avg:60.34ms
step:1050/2315 train_time:63362ms step_avg:60.35ms
step:1051/2315 train_time:63425ms step_avg:60.35ms
step:1052/2315 train_time:63485ms step_avg:60.35ms
step:1053/2315 train_time:63547ms step_avg:60.35ms
step:1054/2315 train_time:63607ms step_avg:60.35ms
step:1055/2315 train_time:63668ms step_avg:60.35ms
step:1056/2315 train_time:63729ms step_avg:60.35ms
step:1057/2315 train_time:63790ms step_avg:60.35ms
step:1058/2315 train_time:63851ms step_avg:60.35ms
step:1059/2315 train_time:63912ms step_avg:60.35ms
step:1060/2315 train_time:63973ms step_avg:60.35ms
step:1061/2315 train_time:64034ms step_avg:60.35ms
step:1062/2315 train_time:64094ms step_avg:60.35ms
step:1063/2315 train_time:64156ms step_avg:60.35ms
step:1064/2315 train_time:64216ms step_avg:60.35ms
step:1065/2315 train_time:64277ms step_avg:60.35ms
step:1066/2315 train_time:64338ms step_avg:60.35ms
step:1067/2315 train_time:64399ms step_avg:60.36ms
step:1068/2315 train_time:64460ms step_avg:60.36ms
step:1069/2315 train_time:64520ms step_avg:60.36ms
step:1070/2315 train_time:64581ms step_avg:60.36ms
step:1071/2315 train_time:64641ms step_avg:60.36ms
step:1072/2315 train_time:64701ms step_avg:60.36ms
step:1073/2315 train_time:64762ms step_avg:60.36ms
step:1074/2315 train_time:64823ms step_avg:60.36ms
step:1075/2315 train_time:64884ms step_avg:60.36ms
step:1076/2315 train_time:64945ms step_avg:60.36ms
step:1077/2315 train_time:65005ms step_avg:60.36ms
step:1078/2315 train_time:65066ms step_avg:60.36ms
step:1079/2315 train_time:65127ms step_avg:60.36ms
step:1080/2315 train_time:65188ms step_avg:60.36ms
step:1081/2315 train_time:65249ms step_avg:60.36ms
step:1082/2315 train_time:65309ms step_avg:60.36ms
step:1083/2315 train_time:65371ms step_avg:60.36ms
step:1084/2315 train_time:65432ms step_avg:60.36ms
step:1085/2315 train_time:65493ms step_avg:60.36ms
step:1086/2315 train_time:65554ms step_avg:60.36ms
step:1087/2315 train_time:65614ms step_avg:60.36ms
step:1088/2315 train_time:65675ms step_avg:60.36ms
step:1089/2315 train_time:65736ms step_avg:60.36ms
step:1090/2315 train_time:65797ms step_avg:60.36ms
step:1091/2315 train_time:65857ms step_avg:60.36ms
step:1092/2315 train_time:65918ms step_avg:60.36ms
step:1093/2315 train_time:65979ms step_avg:60.36ms
step:1094/2315 train_time:66040ms step_avg:60.37ms
step:1095/2315 train_time:66100ms step_avg:60.37ms
step:1096/2315 train_time:66160ms step_avg:60.37ms
step:1097/2315 train_time:66221ms step_avg:60.37ms
step:1098/2315 train_time:66282ms step_avg:60.37ms
step:1099/2315 train_time:66342ms step_avg:60.37ms
step:1100/2315 train_time:66403ms step_avg:60.37ms
step:1101/2315 train_time:66463ms step_avg:60.37ms
step:1102/2315 train_time:66525ms step_avg:60.37ms
step:1103/2315 train_time:66586ms step_avg:60.37ms
step:1104/2315 train_time:66647ms step_avg:60.37ms
step:1105/2315 train_time:66708ms step_avg:60.37ms
step:1106/2315 train_time:66769ms step_avg:60.37ms
step:1107/2315 train_time:66830ms step_avg:60.37ms
step:1108/2315 train_time:66891ms step_avg:60.37ms
step:1109/2315 train_time:66952ms step_avg:60.37ms
step:1110/2315 train_time:67013ms step_avg:60.37ms
step:1111/2315 train_time:67074ms step_avg:60.37ms
step:1112/2315 train_time:67135ms step_avg:60.37ms
step:1113/2315 train_time:67196ms step_avg:60.37ms
step:1114/2315 train_time:67256ms step_avg:60.37ms
step:1115/2315 train_time:67318ms step_avg:60.37ms
step:1116/2315 train_time:67379ms step_avg:60.38ms
step:1117/2315 train_time:67439ms step_avg:60.38ms
step:1118/2315 train_time:67499ms step_avg:60.38ms
step:1119/2315 train_time:67560ms step_avg:60.38ms
step:1120/2315 train_time:67622ms step_avg:60.38ms
step:1121/2315 train_time:67681ms step_avg:60.38ms
step:1122/2315 train_time:67742ms step_avg:60.38ms
step:1123/2315 train_time:67803ms step_avg:60.38ms
step:1124/2315 train_time:67864ms step_avg:60.38ms
step:1125/2315 train_time:67925ms step_avg:60.38ms
step:1126/2315 train_time:67987ms step_avg:60.38ms
step:1127/2315 train_time:68048ms step_avg:60.38ms
step:1128/2315 train_time:68108ms step_avg:60.38ms
step:1129/2315 train_time:68169ms step_avg:60.38ms
step:1130/2315 train_time:68229ms step_avg:60.38ms
step:1131/2315 train_time:68290ms step_avg:60.38ms
step:1132/2315 train_time:68351ms step_avg:60.38ms
step:1133/2315 train_time:68413ms step_avg:60.38ms
step:1134/2315 train_time:68474ms step_avg:60.38ms
step:1135/2315 train_time:68535ms step_avg:60.38ms
step:1136/2315 train_time:68595ms step_avg:60.38ms
step:1137/2315 train_time:68656ms step_avg:60.38ms
step:1138/2315 train_time:68717ms step_avg:60.38ms
step:1139/2315 train_time:68778ms step_avg:60.38ms
step:1140/2315 train_time:68839ms step_avg:60.39ms
step:1141/2315 train_time:68900ms step_avg:60.39ms
step:1142/2315 train_time:68960ms step_avg:60.39ms
step:1143/2315 train_time:69021ms step_avg:60.39ms
step:1144/2315 train_time:69082ms step_avg:60.39ms
step:1145/2315 train_time:69143ms step_avg:60.39ms
step:1146/2315 train_time:69203ms step_avg:60.39ms
step:1147/2315 train_time:69264ms step_avg:60.39ms
step:1148/2315 train_time:69325ms step_avg:60.39ms
step:1149/2315 train_time:69386ms step_avg:60.39ms
step:1150/2315 train_time:69447ms step_avg:60.39ms
step:1151/2315 train_time:69508ms step_avg:60.39ms
step:1152/2315 train_time:69569ms step_avg:60.39ms
step:1153/2315 train_time:69630ms step_avg:60.39ms
step:1154/2315 train_time:69691ms step_avg:60.39ms
step:1155/2315 train_time:69752ms step_avg:60.39ms
step:1156/2315 train_time:69813ms step_avg:60.39ms
step:1157/2315 train_time:69874ms step_avg:60.39ms
step:1158/2315 train_time:69935ms step_avg:60.39ms
step:1159/2315 train_time:69996ms step_avg:60.39ms
step:1160/2315 train_time:70057ms step_avg:60.39ms
step:1161/2315 train_time:70118ms step_avg:60.39ms
step:1162/2315 train_time:70178ms step_avg:60.39ms
step:1163/2315 train_time:70239ms step_avg:60.39ms
step:1164/2315 train_time:70299ms step_avg:60.39ms
step:1165/2315 train_time:70360ms step_avg:60.39ms
step:1166/2315 train_time:70420ms step_avg:60.39ms
step:1167/2315 train_time:70481ms step_avg:60.39ms
step:1168/2315 train_time:70541ms step_avg:60.40ms
step:1169/2315 train_time:70602ms step_avg:60.40ms
step:1170/2315 train_time:70663ms step_avg:60.40ms
step:1171/2315 train_time:70724ms step_avg:60.40ms
step:1172/2315 train_time:70784ms step_avg:60.40ms
step:1173/2315 train_time:70845ms step_avg:60.40ms
step:1174/2315 train_time:70907ms step_avg:60.40ms
step:1175/2315 train_time:70968ms step_avg:60.40ms
step:1176/2315 train_time:71029ms step_avg:60.40ms
step:1177/2315 train_time:71089ms step_avg:60.40ms
step:1178/2315 train_time:71150ms step_avg:60.40ms
step:1179/2315 train_time:71211ms step_avg:60.40ms
step:1180/2315 train_time:71272ms step_avg:60.40ms
step:1181/2315 train_time:71333ms step_avg:60.40ms
step:1182/2315 train_time:71393ms step_avg:60.40ms
step:1183/2315 train_time:71455ms step_avg:60.40ms
step:1184/2315 train_time:71515ms step_avg:60.40ms
step:1185/2315 train_time:71576ms step_avg:60.40ms
step:1186/2315 train_time:71637ms step_avg:60.40ms
step:1187/2315 train_time:71698ms step_avg:60.40ms
step:1188/2315 train_time:71759ms step_avg:60.40ms
step:1189/2315 train_time:71819ms step_avg:60.40ms
step:1190/2315 train_time:71880ms step_avg:60.40ms
step:1191/2315 train_time:71941ms step_avg:60.40ms
step:1192/2315 train_time:72001ms step_avg:60.40ms
step:1193/2315 train_time:72061ms step_avg:60.40ms
step:1194/2315 train_time:72122ms step_avg:60.40ms
step:1195/2315 train_time:72183ms step_avg:60.40ms
step:1196/2315 train_time:72244ms step_avg:60.40ms
step:1197/2315 train_time:72305ms step_avg:60.41ms
step:1198/2315 train_time:72366ms step_avg:60.41ms
step:1199/2315 train_time:72427ms step_avg:60.41ms
step:1200/2315 train_time:72487ms step_avg:60.41ms
step:1201/2315 train_time:72548ms step_avg:60.41ms
step:1202/2315 train_time:72609ms step_avg:60.41ms
step:1203/2315 train_time:72670ms step_avg:60.41ms
step:1204/2315 train_time:72731ms step_avg:60.41ms
step:1205/2315 train_time:72792ms step_avg:60.41ms
step:1206/2315 train_time:72852ms step_avg:60.41ms
step:1207/2315 train_time:72914ms step_avg:60.41ms
step:1208/2315 train_time:72974ms step_avg:60.41ms
step:1209/2315 train_time:73035ms step_avg:60.41ms
step:1210/2315 train_time:73096ms step_avg:60.41ms
step:1211/2315 train_time:73157ms step_avg:60.41ms
step:1212/2315 train_time:73217ms step_avg:60.41ms
step:1213/2315 train_time:73278ms step_avg:60.41ms
step:1214/2315 train_time:73340ms step_avg:60.41ms
step:1215/2315 train_time:73400ms step_avg:60.41ms
step:1216/2315 train_time:73461ms step_avg:60.41ms
step:1217/2315 train_time:73521ms step_avg:60.41ms
step:1218/2315 train_time:73581ms step_avg:60.41ms
step:1219/2315 train_time:73642ms step_avg:60.41ms
step:1220/2315 train_time:73702ms step_avg:60.41ms
step:1221/2315 train_time:73763ms step_avg:60.41ms
step:1222/2315 train_time:73824ms step_avg:60.41ms
step:1223/2315 train_time:73885ms step_avg:60.41ms
step:1224/2315 train_time:73946ms step_avg:60.41ms
step:1225/2315 train_time:74007ms step_avg:60.41ms
step:1226/2315 train_time:74068ms step_avg:60.41ms
step:1227/2315 train_time:74129ms step_avg:60.41ms
step:1228/2315 train_time:74189ms step_avg:60.41ms
step:1229/2315 train_time:74251ms step_avg:60.42ms
step:1230/2315 train_time:74311ms step_avg:60.42ms
step:1231/2315 train_time:74373ms step_avg:60.42ms
step:1232/2315 train_time:74433ms step_avg:60.42ms
step:1233/2315 train_time:74494ms step_avg:60.42ms
step:1234/2315 train_time:74555ms step_avg:60.42ms
step:1235/2315 train_time:74616ms step_avg:60.42ms
step:1236/2315 train_time:74676ms step_avg:60.42ms
step:1237/2315 train_time:74737ms step_avg:60.42ms
step:1238/2315 train_time:74798ms step_avg:60.42ms
step:1239/2315 train_time:74859ms step_avg:60.42ms
step:1240/2315 train_time:74919ms step_avg:60.42ms
step:1241/2315 train_time:74980ms step_avg:60.42ms
step:1242/2315 train_time:75041ms step_avg:60.42ms
step:1243/2315 train_time:75101ms step_avg:60.42ms
step:1244/2315 train_time:75162ms step_avg:60.42ms
step:1245/2315 train_time:75223ms step_avg:60.42ms
step:1246/2315 train_time:75284ms step_avg:60.42ms
step:1247/2315 train_time:75345ms step_avg:60.42ms
step:1248/2315 train_time:75406ms step_avg:60.42ms
step:1249/2315 train_time:75467ms step_avg:60.42ms
step:1250/2315 train_time:75527ms step_avg:60.42ms
step:1250/2315 val_loss:3.5134 train_time:75590ms step_avg:60.47ms
step:1251/2315 train_time:75609ms step_avg:60.44ms
step:1252/2315 train_time:75650ms step_avg:60.42ms
step:1253/2315 train_time:75715ms step_avg:60.43ms
step:1254/2315 train_time:75779ms step_avg:60.43ms
step:1255/2315 train_time:75841ms step_avg:60.43ms
step:1256/2315 train_time:75901ms step_avg:60.43ms
step:1257/2315 train_time:75961ms step_avg:60.43ms
step:1258/2315 train_time:76021ms step_avg:60.43ms
step:1259/2315 train_time:76081ms step_avg:60.43ms
step:1260/2315 train_time:76141ms step_avg:60.43ms
step:1261/2315 train_time:76201ms step_avg:60.43ms
step:1262/2315 train_time:76261ms step_avg:60.43ms
step:1263/2315 train_time:76321ms step_avg:60.43ms
step:1264/2315 train_time:76381ms step_avg:60.43ms
step:1265/2315 train_time:76442ms step_avg:60.43ms
step:1266/2315 train_time:76502ms step_avg:60.43ms
step:1267/2315 train_time:76563ms step_avg:60.43ms
step:1268/2315 train_time:76624ms step_avg:60.43ms
step:1269/2315 train_time:76686ms step_avg:60.43ms
step:1270/2315 train_time:76749ms step_avg:60.43ms
step:1271/2315 train_time:76810ms step_avg:60.43ms
step:1272/2315 train_time:76871ms step_avg:60.43ms
step:1273/2315 train_time:76932ms step_avg:60.43ms
step:1274/2315 train_time:76992ms step_avg:60.43ms
step:1275/2315 train_time:77053ms step_avg:60.43ms
step:1276/2315 train_time:77113ms step_avg:60.43ms
step:1277/2315 train_time:77175ms step_avg:60.43ms
step:1278/2315 train_time:77235ms step_avg:60.43ms
step:1279/2315 train_time:77296ms step_avg:60.43ms
step:1280/2315 train_time:77356ms step_avg:60.43ms
step:1281/2315 train_time:77418ms step_avg:60.44ms
step:1282/2315 train_time:77478ms step_avg:60.44ms
step:1283/2315 train_time:77539ms step_avg:60.44ms
step:1284/2315 train_time:77600ms step_avg:60.44ms
step:1285/2315 train_time:77662ms step_avg:60.44ms
step:1286/2315 train_time:77722ms step_avg:60.44ms
step:1287/2315 train_time:77784ms step_avg:60.44ms
step:1288/2315 train_time:77845ms step_avg:60.44ms
step:1289/2315 train_time:77905ms step_avg:60.44ms
step:1290/2315 train_time:77966ms step_avg:60.44ms
step:1291/2315 train_time:78027ms step_avg:60.44ms
step:1292/2315 train_time:78088ms step_avg:60.44ms
step:1293/2315 train_time:78149ms step_avg:60.44ms
step:1294/2315 train_time:78210ms step_avg:60.44ms
step:1295/2315 train_time:78270ms step_avg:60.44ms
step:1296/2315 train_time:78331ms step_avg:60.44ms
step:1297/2315 train_time:78391ms step_avg:60.44ms
step:1298/2315 train_time:78452ms step_avg:60.44ms
step:1299/2315 train_time:78513ms step_avg:60.44ms
step:1300/2315 train_time:78574ms step_avg:60.44ms
step:1301/2315 train_time:78636ms step_avg:60.44ms
step:1302/2315 train_time:78697ms step_avg:60.44ms
step:1303/2315 train_time:78759ms step_avg:60.44ms
step:1304/2315 train_time:78820ms step_avg:60.44ms
step:1305/2315 train_time:78881ms step_avg:60.45ms
step:1306/2315 train_time:78942ms step_avg:60.45ms
step:1307/2315 train_time:79003ms step_avg:60.45ms
step:1308/2315 train_time:79064ms step_avg:60.45ms
step:1309/2315 train_time:79125ms step_avg:60.45ms
step:1310/2315 train_time:79185ms step_avg:60.45ms
step:1311/2315 train_time:79246ms step_avg:60.45ms
step:1312/2315 train_time:79306ms step_avg:60.45ms
step:1313/2315 train_time:79366ms step_avg:60.45ms
step:1314/2315 train_time:79426ms step_avg:60.45ms
step:1315/2315 train_time:79488ms step_avg:60.45ms
step:1316/2315 train_time:79549ms step_avg:60.45ms
step:1317/2315 train_time:79610ms step_avg:60.45ms
step:1318/2315 train_time:79671ms step_avg:60.45ms
step:1319/2315 train_time:79733ms step_avg:60.45ms
step:1320/2315 train_time:79794ms step_avg:60.45ms
step:1321/2315 train_time:79855ms step_avg:60.45ms
step:1322/2315 train_time:79915ms step_avg:60.45ms
step:1323/2315 train_time:79977ms step_avg:60.45ms
step:1324/2315 train_time:80038ms step_avg:60.45ms
step:1325/2315 train_time:80099ms step_avg:60.45ms
step:1326/2315 train_time:80159ms step_avg:60.45ms
step:1327/2315 train_time:80221ms step_avg:60.45ms
step:1328/2315 train_time:80281ms step_avg:60.45ms
step:1329/2315 train_time:80342ms step_avg:60.45ms
step:1330/2315 train_time:80403ms step_avg:60.45ms
step:1331/2315 train_time:80464ms step_avg:60.45ms
step:1332/2315 train_time:80524ms step_avg:60.45ms
step:1333/2315 train_time:80585ms step_avg:60.45ms
step:1334/2315 train_time:80646ms step_avg:60.45ms
step:1335/2315 train_time:80707ms step_avg:60.45ms
step:1336/2315 train_time:80769ms step_avg:60.46ms
step:1337/2315 train_time:80830ms step_avg:60.46ms
step:1338/2315 train_time:80891ms step_avg:60.46ms
step:1339/2315 train_time:80952ms step_avg:60.46ms
step:1340/2315 train_time:81012ms step_avg:60.46ms
step:1341/2315 train_time:81073ms step_avg:60.46ms
step:1342/2315 train_time:81134ms step_avg:60.46ms
step:1343/2315 train_time:81195ms step_avg:60.46ms
step:1344/2315 train_time:81256ms step_avg:60.46ms
step:1345/2315 train_time:81317ms step_avg:60.46ms
step:1346/2315 train_time:81378ms step_avg:60.46ms
step:1347/2315 train_time:81439ms step_avg:60.46ms
step:1348/2315 train_time:81500ms step_avg:60.46ms
step:1349/2315 train_time:81561ms step_avg:60.46ms
step:1350/2315 train_time:81622ms step_avg:60.46ms
step:1351/2315 train_time:81683ms step_avg:60.46ms
step:1352/2315 train_time:81744ms step_avg:60.46ms
step:1353/2315 train_time:81805ms step_avg:60.46ms
step:1354/2315 train_time:81865ms step_avg:60.46ms
step:1355/2315 train_time:81925ms step_avg:60.46ms
step:1356/2315 train_time:81986ms step_avg:60.46ms
step:1357/2315 train_time:82048ms step_avg:60.46ms
step:1358/2315 train_time:82110ms step_avg:60.46ms
step:1359/2315 train_time:82170ms step_avg:60.46ms
step:1360/2315 train_time:82231ms step_avg:60.46ms
step:1361/2315 train_time:82292ms step_avg:60.46ms
step:1362/2315 train_time:82352ms step_avg:60.46ms
step:1363/2315 train_time:82413ms step_avg:60.46ms
step:1364/2315 train_time:82474ms step_avg:60.47ms
step:1365/2315 train_time:82535ms step_avg:60.47ms
step:1366/2315 train_time:82596ms step_avg:60.47ms
step:1367/2315 train_time:82658ms step_avg:60.47ms
step:1368/2315 train_time:82719ms step_avg:60.47ms
step:1369/2315 train_time:82780ms step_avg:60.47ms
step:1370/2315 train_time:82840ms step_avg:60.47ms
step:1371/2315 train_time:82902ms step_avg:60.47ms
step:1372/2315 train_time:82963ms step_avg:60.47ms
step:1373/2315 train_time:83024ms step_avg:60.47ms
step:1374/2315 train_time:83085ms step_avg:60.47ms
step:1375/2315 train_time:83146ms step_avg:60.47ms
step:1376/2315 train_time:83206ms step_avg:60.47ms
step:1377/2315 train_time:83266ms step_avg:60.47ms
step:1378/2315 train_time:83327ms step_avg:60.47ms
step:1379/2315 train_time:83388ms step_avg:60.47ms
step:1380/2315 train_time:83449ms step_avg:60.47ms
step:1381/2315 train_time:83511ms step_avg:60.47ms
step:1382/2315 train_time:83572ms step_avg:60.47ms
step:1383/2315 train_time:83633ms step_avg:60.47ms
step:1384/2315 train_time:83694ms step_avg:60.47ms
step:1385/2315 train_time:83755ms step_avg:60.47ms
step:1386/2315 train_time:83816ms step_avg:60.47ms
step:1387/2315 train_time:83877ms step_avg:60.47ms
step:1388/2315 train_time:83938ms step_avg:60.47ms
step:1389/2315 train_time:83999ms step_avg:60.47ms
step:1390/2315 train_time:84060ms step_avg:60.47ms
step:1391/2315 train_time:84121ms step_avg:60.48ms
step:1392/2315 train_time:84181ms step_avg:60.48ms
step:1393/2315 train_time:84242ms step_avg:60.48ms
step:1394/2315 train_time:84303ms step_avg:60.48ms
step:1395/2315 train_time:84364ms step_avg:60.48ms
step:1396/2315 train_time:84425ms step_avg:60.48ms
step:1397/2315 train_time:84486ms step_avg:60.48ms
step:1398/2315 train_time:84547ms step_avg:60.48ms
step:1399/2315 train_time:84609ms step_avg:60.48ms
step:1400/2315 train_time:84669ms step_avg:60.48ms
step:1401/2315 train_time:84730ms step_avg:60.48ms
step:1402/2315 train_time:84791ms step_avg:60.48ms
step:1403/2315 train_time:84852ms step_avg:60.48ms
step:1404/2315 train_time:84912ms step_avg:60.48ms
step:1405/2315 train_time:84973ms step_avg:60.48ms
step:1406/2315 train_time:85034ms step_avg:60.48ms
step:1407/2315 train_time:85095ms step_avg:60.48ms
step:1408/2315 train_time:85156ms step_avg:60.48ms
step:1409/2315 train_time:85217ms step_avg:60.48ms
step:1410/2315 train_time:85277ms step_avg:60.48ms
step:1411/2315 train_time:85340ms step_avg:60.48ms
step:1412/2315 train_time:85400ms step_avg:60.48ms
step:1413/2315 train_time:85461ms step_avg:60.48ms
step:1414/2315 train_time:85522ms step_avg:60.48ms
step:1415/2315 train_time:85584ms step_avg:60.48ms
step:1416/2315 train_time:85645ms step_avg:60.48ms
step:1417/2315 train_time:85706ms step_avg:60.48ms
step:1418/2315 train_time:85766ms step_avg:60.48ms
step:1419/2315 train_time:85827ms step_avg:60.48ms
step:1420/2315 train_time:85888ms step_avg:60.48ms
step:1421/2315 train_time:85949ms step_avg:60.48ms
step:1422/2315 train_time:86009ms step_avg:60.48ms
step:1423/2315 train_time:86071ms step_avg:60.49ms
step:1424/2315 train_time:86132ms step_avg:60.49ms
step:1425/2315 train_time:86193ms step_avg:60.49ms
step:1426/2315 train_time:86254ms step_avg:60.49ms
step:1427/2315 train_time:86315ms step_avg:60.49ms
step:1428/2315 train_time:86376ms step_avg:60.49ms
step:1429/2315 train_time:86436ms step_avg:60.49ms
step:1430/2315 train_time:86498ms step_avg:60.49ms
step:1431/2315 train_time:86559ms step_avg:60.49ms
step:1432/2315 train_time:86620ms step_avg:60.49ms
step:1433/2315 train_time:86681ms step_avg:60.49ms
step:1434/2315 train_time:86742ms step_avg:60.49ms
step:1435/2315 train_time:86803ms step_avg:60.49ms
step:1436/2315 train_time:86863ms step_avg:60.49ms
step:1437/2315 train_time:86924ms step_avg:60.49ms
step:1438/2315 train_time:86985ms step_avg:60.49ms
step:1439/2315 train_time:87046ms step_avg:60.49ms
step:1440/2315 train_time:87107ms step_avg:60.49ms
step:1441/2315 train_time:87168ms step_avg:60.49ms
step:1442/2315 train_time:87229ms step_avg:60.49ms
step:1443/2315 train_time:87291ms step_avg:60.49ms
step:1444/2315 train_time:87352ms step_avg:60.49ms
step:1445/2315 train_time:87413ms step_avg:60.49ms
step:1446/2315 train_time:87473ms step_avg:60.49ms
step:1447/2315 train_time:87534ms step_avg:60.49ms
step:1448/2315 train_time:87595ms step_avg:60.49ms
step:1449/2315 train_time:87657ms step_avg:60.49ms
step:1450/2315 train_time:87718ms step_avg:60.50ms
step:1451/2315 train_time:87779ms step_avg:60.50ms
step:1452/2315 train_time:87840ms step_avg:60.50ms
step:1453/2315 train_time:87900ms step_avg:60.50ms
step:1454/2315 train_time:87961ms step_avg:60.50ms
step:1455/2315 train_time:88022ms step_avg:60.50ms
step:1456/2315 train_time:88083ms step_avg:60.50ms
step:1457/2315 train_time:88144ms step_avg:60.50ms
step:1458/2315 train_time:88205ms step_avg:60.50ms
step:1459/2315 train_time:88266ms step_avg:60.50ms
step:1460/2315 train_time:88327ms step_avg:60.50ms
step:1461/2315 train_time:88387ms step_avg:60.50ms
step:1462/2315 train_time:88449ms step_avg:60.50ms
step:1463/2315 train_time:88510ms step_avg:60.50ms
step:1464/2315 train_time:88572ms step_avg:60.50ms
step:1465/2315 train_time:88633ms step_avg:60.50ms
step:1466/2315 train_time:88694ms step_avg:60.50ms
step:1467/2315 train_time:88755ms step_avg:60.50ms
step:1468/2315 train_time:88815ms step_avg:60.50ms
step:1469/2315 train_time:88876ms step_avg:60.50ms
step:1470/2315 train_time:88937ms step_avg:60.50ms
step:1471/2315 train_time:88999ms step_avg:60.50ms
step:1472/2315 train_time:89060ms step_avg:60.50ms
step:1473/2315 train_time:89121ms step_avg:60.50ms
step:1474/2315 train_time:89181ms step_avg:60.50ms
step:1475/2315 train_time:89242ms step_avg:60.50ms
step:1476/2315 train_time:89303ms step_avg:60.50ms
step:1477/2315 train_time:89364ms step_avg:60.50ms
step:1478/2315 train_time:89425ms step_avg:60.50ms
step:1479/2315 train_time:89486ms step_avg:60.50ms
step:1480/2315 train_time:89547ms step_avg:60.50ms
step:1481/2315 train_time:89608ms step_avg:60.50ms
step:1482/2315 train_time:89669ms step_avg:60.51ms
step:1483/2315 train_time:89730ms step_avg:60.51ms
step:1484/2315 train_time:89791ms step_avg:60.51ms
step:1485/2315 train_time:89852ms step_avg:60.51ms
step:1486/2315 train_time:89913ms step_avg:60.51ms
step:1487/2315 train_time:89974ms step_avg:60.51ms
step:1488/2315 train_time:90035ms step_avg:60.51ms
step:1489/2315 train_time:90097ms step_avg:60.51ms
step:1490/2315 train_time:90157ms step_avg:60.51ms
step:1491/2315 train_time:90218ms step_avg:60.51ms
step:1492/2315 train_time:90279ms step_avg:60.51ms
step:1493/2315 train_time:90341ms step_avg:60.51ms
step:1494/2315 train_time:90401ms step_avg:60.51ms
step:1495/2315 train_time:90462ms step_avg:60.51ms
step:1496/2315 train_time:90523ms step_avg:60.51ms
step:1497/2315 train_time:90584ms step_avg:60.51ms
step:1498/2315 train_time:90644ms step_avg:60.51ms
step:1499/2315 train_time:90705ms step_avg:60.51ms
step:1500/2315 train_time:90766ms step_avg:60.51ms
step:1500/2315 val_loss:3.4489 train_time:90829ms step_avg:60.55ms
step:1501/2315 train_time:90849ms step_avg:60.53ms
step:1502/2315 train_time:90889ms step_avg:60.51ms
step:1503/2315 train_time:90954ms step_avg:60.52ms
step:1504/2315 train_time:91018ms step_avg:60.52ms
step:1505/2315 train_time:91079ms step_avg:60.52ms
step:1506/2315 train_time:91140ms step_avg:60.52ms
step:1507/2315 train_time:91200ms step_avg:60.52ms
step:1508/2315 train_time:91260ms step_avg:60.52ms
step:1509/2315 train_time:91320ms step_avg:60.52ms
step:1510/2315 train_time:91380ms step_avg:60.52ms
step:1511/2315 train_time:91440ms step_avg:60.52ms
step:1512/2315 train_time:91500ms step_avg:60.52ms
step:1513/2315 train_time:91560ms step_avg:60.52ms
step:1514/2315 train_time:91620ms step_avg:60.52ms
step:1515/2315 train_time:91680ms step_avg:60.51ms
step:1516/2315 train_time:91740ms step_avg:60.51ms
step:1517/2315 train_time:91803ms step_avg:60.52ms
step:1518/2315 train_time:91865ms step_avg:60.52ms
step:1519/2315 train_time:91929ms step_avg:60.52ms
step:1520/2315 train_time:91991ms step_avg:60.52ms
step:1521/2315 train_time:92052ms step_avg:60.52ms
step:1522/2315 train_time:92113ms step_avg:60.52ms
step:1523/2315 train_time:92174ms step_avg:60.52ms
step:1524/2315 train_time:92235ms step_avg:60.52ms
step:1525/2315 train_time:92296ms step_avg:60.52ms
step:1526/2315 train_time:92356ms step_avg:60.52ms
step:1527/2315 train_time:92418ms step_avg:60.52ms
step:1528/2315 train_time:92478ms step_avg:60.52ms
step:1529/2315 train_time:92539ms step_avg:60.52ms
step:1530/2315 train_time:92599ms step_avg:60.52ms
step:1531/2315 train_time:92660ms step_avg:60.52ms
step:1532/2315 train_time:92721ms step_avg:60.52ms
step:1533/2315 train_time:92783ms step_avg:60.52ms
step:1534/2315 train_time:92844ms step_avg:60.52ms
step:1535/2315 train_time:92906ms step_avg:60.52ms
step:1536/2315 train_time:92967ms step_avg:60.53ms
step:1537/2315 train_time:93029ms step_avg:60.53ms
step:1538/2315 train_time:93090ms step_avg:60.53ms
step:1539/2315 train_time:93152ms step_avg:60.53ms
step:1540/2315 train_time:93213ms step_avg:60.53ms
step:1541/2315 train_time:93274ms step_avg:60.53ms
step:1542/2315 train_time:93335ms step_avg:60.53ms
step:1543/2315 train_time:93396ms step_avg:60.53ms
step:1544/2315 train_time:93456ms step_avg:60.53ms
step:1545/2315 train_time:93517ms step_avg:60.53ms
step:1546/2315 train_time:93578ms step_avg:60.53ms
step:1547/2315 train_time:93639ms step_avg:60.53ms
step:1548/2315 train_time:93700ms step_avg:60.53ms
step:1549/2315 train_time:93761ms step_avg:60.53ms
step:1550/2315 train_time:93823ms step_avg:60.53ms
step:1551/2315 train_time:93884ms step_avg:60.53ms
step:1552/2315 train_time:93945ms step_avg:60.53ms
step:1553/2315 train_time:94008ms step_avg:60.53ms
step:1554/2315 train_time:94069ms step_avg:60.53ms
step:1555/2315 train_time:94131ms step_avg:60.53ms
step:1556/2315 train_time:94192ms step_avg:60.53ms
step:1557/2315 train_time:94253ms step_avg:60.54ms
step:1558/2315 train_time:94314ms step_avg:60.54ms
step:1559/2315 train_time:94375ms step_avg:60.54ms
step:1560/2315 train_time:94437ms step_avg:60.54ms
step:1561/2315 train_time:94498ms step_avg:60.54ms
step:1562/2315 train_time:94559ms step_avg:60.54ms
step:1563/2315 train_time:94619ms step_avg:60.54ms
step:1564/2315 train_time:94681ms step_avg:60.54ms
step:1565/2315 train_time:94742ms step_avg:60.54ms
step:1566/2315 train_time:94803ms step_avg:60.54ms
step:1567/2315 train_time:94864ms step_avg:60.54ms
step:1568/2315 train_time:94926ms step_avg:60.54ms
step:1569/2315 train_time:94987ms step_avg:60.54ms
step:1570/2315 train_time:95048ms step_avg:60.54ms
step:1571/2315 train_time:95110ms step_avg:60.54ms
step:1572/2315 train_time:95171ms step_avg:60.54ms
step:1573/2315 train_time:95233ms step_avg:60.54ms
step:1574/2315 train_time:95294ms step_avg:60.54ms
step:1575/2315 train_time:95356ms step_avg:60.54ms
step:1576/2315 train_time:95417ms step_avg:60.54ms
step:1577/2315 train_time:95478ms step_avg:60.54ms
step:1578/2315 train_time:95539ms step_avg:60.54ms
step:1579/2315 train_time:95600ms step_avg:60.54ms
step:1580/2315 train_time:95661ms step_avg:60.54ms
step:1581/2315 train_time:95722ms step_avg:60.55ms
step:1582/2315 train_time:95783ms step_avg:60.55ms
step:1583/2315 train_time:95844ms step_avg:60.55ms
step:1584/2315 train_time:95905ms step_avg:60.55ms
step:1585/2315 train_time:95966ms step_avg:60.55ms
step:1586/2315 train_time:96027ms step_avg:60.55ms
step:1587/2315 train_time:96089ms step_avg:60.55ms
step:1588/2315 train_time:96151ms step_avg:60.55ms
step:1589/2315 train_time:96212ms step_avg:60.55ms
step:1590/2315 train_time:96273ms step_avg:60.55ms
step:1591/2315 train_time:96334ms step_avg:60.55ms
step:1592/2315 train_time:96395ms step_avg:60.55ms
step:1593/2315 train_time:96456ms step_avg:60.55ms
step:1594/2315 train_time:96517ms step_avg:60.55ms
step:1595/2315 train_time:96579ms step_avg:60.55ms
step:1596/2315 train_time:96640ms step_avg:60.55ms
step:1597/2315 train_time:96701ms step_avg:60.55ms
step:1598/2315 train_time:96762ms step_avg:60.55ms
step:1599/2315 train_time:96823ms step_avg:60.55ms
step:1600/2315 train_time:96885ms step_avg:60.55ms
step:1601/2315 train_time:96946ms step_avg:60.55ms
step:1602/2315 train_time:97007ms step_avg:60.55ms
step:1603/2315 train_time:97068ms step_avg:60.55ms
step:1604/2315 train_time:97129ms step_avg:60.55ms
step:1605/2315 train_time:97191ms step_avg:60.56ms
step:1606/2315 train_time:97253ms step_avg:60.56ms
step:1607/2315 train_time:97314ms step_avg:60.56ms
step:1608/2315 train_time:97374ms step_avg:60.56ms
step:1609/2315 train_time:97435ms step_avg:60.56ms
step:1610/2315 train_time:97496ms step_avg:60.56ms
step:1611/2315 train_time:97557ms step_avg:60.56ms
step:1612/2315 train_time:97617ms step_avg:60.56ms
step:1613/2315 train_time:97679ms step_avg:60.56ms
step:1614/2315 train_time:97740ms step_avg:60.56ms
step:1615/2315 train_time:97801ms step_avg:60.56ms
step:1616/2315 train_time:97862ms step_avg:60.56ms
step:1617/2315 train_time:97924ms step_avg:60.56ms
step:1618/2315 train_time:97985ms step_avg:60.56ms
step:1619/2315 train_time:98047ms step_avg:60.56ms
step:1620/2315 train_time:98108ms step_avg:60.56ms
step:1621/2315 train_time:98170ms step_avg:60.56ms
step:1622/2315 train_time:98231ms step_avg:60.56ms
step:1623/2315 train_time:98292ms step_avg:60.56ms
step:1624/2315 train_time:98353ms step_avg:60.56ms
step:1625/2315 train_time:98414ms step_avg:60.56ms
step:1626/2315 train_time:98476ms step_avg:60.56ms
step:1627/2315 train_time:98537ms step_avg:60.56ms
step:1628/2315 train_time:98598ms step_avg:60.56ms
step:1629/2315 train_time:98659ms step_avg:60.56ms
step:1630/2315 train_time:98720ms step_avg:60.56ms
step:1631/2315 train_time:98781ms step_avg:60.56ms
step:1632/2315 train_time:98842ms step_avg:60.57ms
step:1633/2315 train_time:98904ms step_avg:60.57ms
step:1634/2315 train_time:98965ms step_avg:60.57ms
step:1635/2315 train_time:99027ms step_avg:60.57ms
step:1636/2315 train_time:99088ms step_avg:60.57ms
step:1637/2315 train_time:99149ms step_avg:60.57ms
step:1638/2315 train_time:99210ms step_avg:60.57ms
step:1639/2315 train_time:99272ms step_avg:60.57ms
step:1640/2315 train_time:99333ms step_avg:60.57ms
step:1641/2315 train_time:99394ms step_avg:60.57ms
step:1642/2315 train_time:99455ms step_avg:60.57ms
step:1643/2315 train_time:99517ms step_avg:60.57ms
step:1644/2315 train_time:99577ms step_avg:60.57ms
step:1645/2315 train_time:99639ms step_avg:60.57ms
step:1646/2315 train_time:99699ms step_avg:60.57ms
step:1647/2315 train_time:99761ms step_avg:60.57ms
step:1648/2315 train_time:99822ms step_avg:60.57ms
step:1649/2315 train_time:99884ms step_avg:60.57ms
step:1650/2315 train_time:99945ms step_avg:60.57ms
step:1651/2315 train_time:100006ms step_avg:60.57ms
step:1652/2315 train_time:100067ms step_avg:60.57ms
step:1653/2315 train_time:100129ms step_avg:60.57ms
step:1654/2315 train_time:100191ms step_avg:60.57ms
step:1655/2315 train_time:100253ms step_avg:60.58ms
step:1656/2315 train_time:100314ms step_avg:60.58ms
step:1657/2315 train_time:100376ms step_avg:60.58ms
step:1658/2315 train_time:100437ms step_avg:60.58ms
step:1659/2315 train_time:100498ms step_avg:60.58ms
step:1660/2315 train_time:100558ms step_avg:60.58ms
step:1661/2315 train_time:100620ms step_avg:60.58ms
step:1662/2315 train_time:100680ms step_avg:60.58ms
step:1663/2315 train_time:100742ms step_avg:60.58ms
step:1664/2315 train_time:100803ms step_avg:60.58ms
step:1665/2315 train_time:100865ms step_avg:60.58ms
step:1666/2315 train_time:100925ms step_avg:60.58ms
step:1667/2315 train_time:100987ms step_avg:60.58ms
step:1668/2315 train_time:101048ms step_avg:60.58ms
step:1669/2315 train_time:101109ms step_avg:60.58ms
step:1670/2315 train_time:101170ms step_avg:60.58ms
step:1671/2315 train_time:101232ms step_avg:60.58ms
step:1672/2315 train_time:101294ms step_avg:60.58ms
step:1673/2315 train_time:101356ms step_avg:60.58ms
step:1674/2315 train_time:101417ms step_avg:60.58ms
step:1675/2315 train_time:101478ms step_avg:60.58ms
step:1676/2315 train_time:101539ms step_avg:60.58ms
step:1677/2315 train_time:101600ms step_avg:60.58ms
step:1678/2315 train_time:101661ms step_avg:60.58ms
step:1679/2315 train_time:101722ms step_avg:60.58ms
step:1680/2315 train_time:101784ms step_avg:60.59ms
step:1681/2315 train_time:101845ms step_avg:60.59ms
step:1682/2315 train_time:101906ms step_avg:60.59ms
step:1683/2315 train_time:101967ms step_avg:60.59ms
step:1684/2315 train_time:102029ms step_avg:60.59ms
step:1685/2315 train_time:102090ms step_avg:60.59ms
step:1686/2315 train_time:102150ms step_avg:60.59ms
step:1687/2315 train_time:102213ms step_avg:60.59ms
step:1688/2315 train_time:102273ms step_avg:60.59ms
step:1689/2315 train_time:102335ms step_avg:60.59ms
step:1690/2315 train_time:102395ms step_avg:60.59ms
step:1691/2315 train_time:102456ms step_avg:60.59ms
step:1692/2315 train_time:102517ms step_avg:60.59ms
step:1693/2315 train_time:102579ms step_avg:60.59ms
step:1694/2315 train_time:102640ms step_avg:60.59ms
step:1695/2315 train_time:102701ms step_avg:60.59ms
step:1696/2315 train_time:102762ms step_avg:60.59ms
step:1697/2315 train_time:102823ms step_avg:60.59ms
step:1698/2315 train_time:102884ms step_avg:60.59ms
step:1699/2315 train_time:102946ms step_avg:60.59ms
step:1700/2315 train_time:103007ms step_avg:60.59ms
step:1701/2315 train_time:103069ms step_avg:60.59ms
step:1702/2315 train_time:103131ms step_avg:60.59ms
step:1703/2315 train_time:103192ms step_avg:60.59ms
step:1704/2315 train_time:103253ms step_avg:60.59ms
step:1705/2315 train_time:103313ms step_avg:60.59ms
step:1706/2315 train_time:103374ms step_avg:60.59ms
step:1707/2315 train_time:103436ms step_avg:60.59ms
step:1708/2315 train_time:103497ms step_avg:60.60ms
step:1709/2315 train_time:103558ms step_avg:60.60ms
step:1710/2315 train_time:103619ms step_avg:60.60ms
step:1711/2315 train_time:103681ms step_avg:60.60ms
step:1712/2315 train_time:103742ms step_avg:60.60ms
step:1713/2315 train_time:103803ms step_avg:60.60ms
step:1714/2315 train_time:103865ms step_avg:60.60ms
step:1715/2315 train_time:103926ms step_avg:60.60ms
step:1716/2315 train_time:103987ms step_avg:60.60ms
step:1717/2315 train_time:104049ms step_avg:60.60ms
step:1718/2315 train_time:104109ms step_avg:60.60ms
step:1719/2315 train_time:104171ms step_avg:60.60ms
step:1720/2315 train_time:104232ms step_avg:60.60ms
step:1721/2315 train_time:104293ms step_avg:60.60ms
step:1722/2315 train_time:104354ms step_avg:60.60ms
step:1723/2315 train_time:104415ms step_avg:60.60ms
step:1724/2315 train_time:104477ms step_avg:60.60ms
step:1725/2315 train_time:104539ms step_avg:60.60ms
step:1726/2315 train_time:104600ms step_avg:60.60ms
step:1727/2315 train_time:104661ms step_avg:60.60ms
step:1728/2315 train_time:104722ms step_avg:60.60ms
step:1729/2315 train_time:104784ms step_avg:60.60ms
step:1730/2315 train_time:104844ms step_avg:60.60ms
step:1731/2315 train_time:104906ms step_avg:60.60ms
step:1732/2315 train_time:104967ms step_avg:60.60ms
step:1733/2315 train_time:105028ms step_avg:60.60ms
step:1734/2315 train_time:105089ms step_avg:60.61ms
step:1735/2315 train_time:105151ms step_avg:60.61ms
step:1736/2315 train_time:105213ms step_avg:60.61ms
step:1737/2315 train_time:105274ms step_avg:60.61ms
step:1738/2315 train_time:105335ms step_avg:60.61ms
step:1739/2315 train_time:105396ms step_avg:60.61ms
step:1740/2315 train_time:105457ms step_avg:60.61ms
step:1741/2315 train_time:105519ms step_avg:60.61ms
step:1742/2315 train_time:105580ms step_avg:60.61ms
step:1743/2315 train_time:105641ms step_avg:60.61ms
step:1744/2315 train_time:105702ms step_avg:60.61ms
step:1745/2315 train_time:105763ms step_avg:60.61ms
step:1746/2315 train_time:105824ms step_avg:60.61ms
step:1747/2315 train_time:105886ms step_avg:60.61ms
step:1748/2315 train_time:105947ms step_avg:60.61ms
step:1749/2315 train_time:106008ms step_avg:60.61ms
step:1750/2315 train_time:106069ms step_avg:60.61ms
step:1750/2315 val_loss:3.3793 train_time:106132ms step_avg:60.65ms
step:1751/2315 train_time:106152ms step_avg:60.62ms
step:1752/2315 train_time:106193ms step_avg:60.61ms
step:1753/2315 train_time:106262ms step_avg:60.62ms
step:1754/2315 train_time:106325ms step_avg:60.62ms
step:1755/2315 train_time:106387ms step_avg:60.62ms
step:1756/2315 train_time:106448ms step_avg:60.62ms
step:1757/2315 train_time:106509ms step_avg:60.62ms
step:1758/2315 train_time:106570ms step_avg:60.62ms
step:1759/2315 train_time:106631ms step_avg:60.62ms
step:1760/2315 train_time:106691ms step_avg:60.62ms
step:1761/2315 train_time:106752ms step_avg:60.62ms
step:1762/2315 train_time:106812ms step_avg:60.62ms
step:1763/2315 train_time:106873ms step_avg:60.62ms
step:1764/2315 train_time:106933ms step_avg:60.62ms
step:1765/2315 train_time:106994ms step_avg:60.62ms
step:1766/2315 train_time:107055ms step_avg:60.62ms
step:1767/2315 train_time:107116ms step_avg:60.62ms
step:1768/2315 train_time:107179ms step_avg:60.62ms
step:1769/2315 train_time:107243ms step_avg:60.62ms
step:1770/2315 train_time:107304ms step_avg:60.62ms
step:1771/2315 train_time:107366ms step_avg:60.62ms
step:1772/2315 train_time:107428ms step_avg:60.63ms
step:1773/2315 train_time:107490ms step_avg:60.63ms
step:1774/2315 train_time:107551ms step_avg:60.63ms
step:1775/2315 train_time:107612ms step_avg:60.63ms
step:1776/2315 train_time:107673ms step_avg:60.63ms
step:1777/2315 train_time:107734ms step_avg:60.63ms
step:1778/2315 train_time:107794ms step_avg:60.63ms
step:1779/2315 train_time:107855ms step_avg:60.63ms
step:1780/2315 train_time:107916ms step_avg:60.63ms
step:1781/2315 train_time:107977ms step_avg:60.63ms
step:1782/2315 train_time:108037ms step_avg:60.63ms
step:1783/2315 train_time:108100ms step_avg:60.63ms
step:1784/2315 train_time:108161ms step_avg:60.63ms
step:1785/2315 train_time:108223ms step_avg:60.63ms
step:1786/2315 train_time:108285ms step_avg:60.63ms
step:1787/2315 train_time:108346ms step_avg:60.63ms
step:1788/2315 train_time:108407ms step_avg:60.63ms
step:1789/2315 train_time:108470ms step_avg:60.63ms
step:1790/2315 train_time:108530ms step_avg:60.63ms
step:1791/2315 train_time:108592ms step_avg:60.63ms
step:1792/2315 train_time:108652ms step_avg:60.63ms
step:1793/2315 train_time:108713ms step_avg:60.63ms
step:1794/2315 train_time:108775ms step_avg:60.63ms
step:1795/2315 train_time:108835ms step_avg:60.63ms
step:1796/2315 train_time:108895ms step_avg:60.63ms
step:1797/2315 train_time:108956ms step_avg:60.63ms
step:1798/2315 train_time:109017ms step_avg:60.63ms
step:1799/2315 train_time:109078ms step_avg:60.63ms
step:1800/2315 train_time:109140ms step_avg:60.63ms
step:1801/2315 train_time:109202ms step_avg:60.63ms
step:1802/2315 train_time:109263ms step_avg:60.63ms
step:1803/2315 train_time:109325ms step_avg:60.63ms
step:1804/2315 train_time:109386ms step_avg:60.64ms
step:1805/2315 train_time:109447ms step_avg:60.64ms
step:1806/2315 train_time:109509ms step_avg:60.64ms
step:1807/2315 train_time:109570ms step_avg:60.64ms
step:1808/2315 train_time:109631ms step_avg:60.64ms
step:1809/2315 train_time:109693ms step_avg:60.64ms
step:1810/2315 train_time:109754ms step_avg:60.64ms
step:1811/2315 train_time:109816ms step_avg:60.64ms
step:1812/2315 train_time:109877ms step_avg:60.64ms
step:1813/2315 train_time:109938ms step_avg:60.64ms
step:1814/2315 train_time:109999ms step_avg:60.64ms
step:1815/2315 train_time:110060ms step_avg:60.64ms
step:1816/2315 train_time:110121ms step_avg:60.64ms
step:1817/2315 train_time:110182ms step_avg:60.64ms
step:1818/2315 train_time:110243ms step_avg:60.64ms
step:1819/2315 train_time:110305ms step_avg:60.64ms
step:1820/2315 train_time:110367ms step_avg:60.64ms
step:1821/2315 train_time:110428ms step_avg:60.64ms
step:1822/2315 train_time:110489ms step_avg:60.64ms
step:1823/2315 train_time:110550ms step_avg:60.64ms
step:1824/2315 train_time:110611ms step_avg:60.64ms
step:1825/2315 train_time:110673ms step_avg:60.64ms
step:1826/2315 train_time:110733ms step_avg:60.64ms
step:1827/2315 train_time:110795ms step_avg:60.64ms
step:1828/2315 train_time:110856ms step_avg:60.64ms
step:1829/2315 train_time:110917ms step_avg:60.64ms
step:1830/2315 train_time:110977ms step_avg:60.64ms
step:1831/2315 train_time:111039ms step_avg:60.64ms
step:1832/2315 train_time:111100ms step_avg:60.64ms
step:1833/2315 train_time:111161ms step_avg:60.64ms
step:1834/2315 train_time:111222ms step_avg:60.64ms
step:1835/2315 train_time:111284ms step_avg:60.65ms
step:1836/2315 train_time:111345ms step_avg:60.65ms
step:1837/2315 train_time:111407ms step_avg:60.65ms
step:1838/2315 train_time:111468ms step_avg:60.65ms
step:1839/2315 train_time:111529ms step_avg:60.65ms
step:1840/2315 train_time:111590ms step_avg:60.65ms
step:1841/2315 train_time:111651ms step_avg:60.65ms
step:1842/2315 train_time:111712ms step_avg:60.65ms
step:1843/2315 train_time:111774ms step_avg:60.65ms
step:1844/2315 train_time:111835ms step_avg:60.65ms
step:1845/2315 train_time:111896ms step_avg:60.65ms
step:1846/2315 train_time:111957ms step_avg:60.65ms
step:1847/2315 train_time:112018ms step_avg:60.65ms
step:1848/2315 train_time:112079ms step_avg:60.65ms
step:1849/2315 train_time:112140ms step_avg:60.65ms
step:1850/2315 train_time:112202ms step_avg:60.65ms
step:1851/2315 train_time:112263ms step_avg:60.65ms
step:1852/2315 train_time:112324ms step_avg:60.65ms
step:1853/2315 train_time:112386ms step_avg:60.65ms
step:1854/2315 train_time:112447ms step_avg:60.65ms
step:1855/2315 train_time:112508ms step_avg:60.65ms
step:1856/2315 train_time:112569ms step_avg:60.65ms
step:1857/2315 train_time:112630ms step_avg:60.65ms
step:1858/2315 train_time:112691ms step_avg:60.65ms
step:1859/2315 train_time:112752ms step_avg:60.65ms
step:1860/2315 train_time:112814ms step_avg:60.65ms
step:1861/2315 train_time:112875ms step_avg:60.65ms
step:1862/2315 train_time:112937ms step_avg:60.65ms
step:1863/2315 train_time:112998ms step_avg:60.65ms
step:1864/2315 train_time:113059ms step_avg:60.65ms
step:1865/2315 train_time:113120ms step_avg:60.65ms
step:1866/2315 train_time:113181ms step_avg:60.65ms
step:1867/2315 train_time:113243ms step_avg:60.66ms
step:1868/2315 train_time:113305ms step_avg:60.66ms
step:1869/2315 train_time:113366ms step_avg:60.66ms
step:1870/2315 train_time:113427ms step_avg:60.66ms
step:1871/2315 train_time:113488ms step_avg:60.66ms
step:1872/2315 train_time:113549ms step_avg:60.66ms
step:1873/2315 train_time:113610ms step_avg:60.66ms
step:1874/2315 train_time:113672ms step_avg:60.66ms
step:1875/2315 train_time:113733ms step_avg:60.66ms
step:1876/2315 train_time:113794ms step_avg:60.66ms
step:1877/2315 train_time:113855ms step_avg:60.66ms
step:1878/2315 train_time:113917ms step_avg:60.66ms
step:1879/2315 train_time:113978ms step_avg:60.66ms
step:1880/2315 train_time:114039ms step_avg:60.66ms
step:1881/2315 train_time:114100ms step_avg:60.66ms
step:1882/2315 train_time:114161ms step_avg:60.66ms
step:1883/2315 train_time:114222ms step_avg:60.66ms
step:1884/2315 train_time:114284ms step_avg:60.66ms
step:1885/2315 train_time:114345ms step_avg:60.66ms
step:1886/2315 train_time:114406ms step_avg:60.66ms
step:1887/2315 train_time:114467ms step_avg:60.66ms
step:1888/2315 train_time:114528ms step_avg:60.66ms
step:1889/2315 train_time:114590ms step_avg:60.66ms
step:1890/2315 train_time:114651ms step_avg:60.66ms
step:1891/2315 train_time:114714ms step_avg:60.66ms
step:1892/2315 train_time:114775ms step_avg:60.66ms
step:1893/2315 train_time:114836ms step_avg:60.66ms
step:1894/2315 train_time:114897ms step_avg:60.66ms
step:1895/2315 train_time:114958ms step_avg:60.66ms
step:1896/2315 train_time:115019ms step_avg:60.66ms
step:1897/2315 train_time:115081ms step_avg:60.66ms
step:1898/2315 train_time:115141ms step_avg:60.66ms
step:1899/2315 train_time:115203ms step_avg:60.66ms
step:1900/2315 train_time:115264ms step_avg:60.67ms
step:1901/2315 train_time:115325ms step_avg:60.67ms
step:1902/2315 train_time:115387ms step_avg:60.67ms
step:1903/2315 train_time:115447ms step_avg:60.67ms
step:1904/2315 train_time:115509ms step_avg:60.67ms
step:1905/2315 train_time:115569ms step_avg:60.67ms
step:1906/2315 train_time:115630ms step_avg:60.67ms
step:1907/2315 train_time:115692ms step_avg:60.67ms
step:1908/2315 train_time:115753ms step_avg:60.67ms
step:1909/2315 train_time:115815ms step_avg:60.67ms
step:1910/2315 train_time:115876ms step_avg:60.67ms
step:1911/2315 train_time:115937ms step_avg:60.67ms
step:1912/2315 train_time:115998ms step_avg:60.67ms
step:1913/2315 train_time:116059ms step_avg:60.67ms
step:1914/2315 train_time:116120ms step_avg:60.67ms
step:1915/2315 train_time:116181ms step_avg:60.67ms
step:1916/2315 train_time:116242ms step_avg:60.67ms
step:1917/2315 train_time:116304ms step_avg:60.67ms
step:1918/2315 train_time:116365ms step_avg:60.67ms
step:1919/2315 train_time:116426ms step_avg:60.67ms
step:1920/2315 train_time:116487ms step_avg:60.67ms
step:1921/2315 train_time:116548ms step_avg:60.67ms
step:1922/2315 train_time:116609ms step_avg:60.67ms
step:1923/2315 train_time:116671ms step_avg:60.67ms
step:1924/2315 train_time:116732ms step_avg:60.67ms
step:1925/2315 train_time:116793ms step_avg:60.67ms
step:1926/2315 train_time:116854ms step_avg:60.67ms
step:1927/2315 train_time:116916ms step_avg:60.67ms
step:1928/2315 train_time:116977ms step_avg:60.67ms
step:1929/2315 train_time:117038ms step_avg:60.67ms
step:1930/2315 train_time:117099ms step_avg:60.67ms
step:1931/2315 train_time:117160ms step_avg:60.67ms
step:1932/2315 train_time:117222ms step_avg:60.67ms
step:1933/2315 train_time:117283ms step_avg:60.67ms
step:1934/2315 train_time:117344ms step_avg:60.67ms
step:1935/2315 train_time:117406ms step_avg:60.67ms
step:1936/2315 train_time:117467ms step_avg:60.67ms
step:1937/2315 train_time:117528ms step_avg:60.68ms
step:1938/2315 train_time:117589ms step_avg:60.68ms
step:1939/2315 train_time:117650ms step_avg:60.68ms
step:1940/2315 train_time:117711ms step_avg:60.68ms
step:1941/2315 train_time:117773ms step_avg:60.68ms
step:1942/2315 train_time:117834ms step_avg:60.68ms
step:1943/2315 train_time:117895ms step_avg:60.68ms
step:1944/2315 train_time:117956ms step_avg:60.68ms
step:1945/2315 train_time:118018ms step_avg:60.68ms
step:1946/2315 train_time:118078ms step_avg:60.68ms
step:1947/2315 train_time:118140ms step_avg:60.68ms
step:1948/2315 train_time:118201ms step_avg:60.68ms
step:1949/2315 train_time:118263ms step_avg:60.68ms
step:1950/2315 train_time:118324ms step_avg:60.68ms
step:1951/2315 train_time:118385ms step_avg:60.68ms
step:1952/2315 train_time:118446ms step_avg:60.68ms
step:1953/2315 train_time:118507ms step_avg:60.68ms
step:1954/2315 train_time:118569ms step_avg:60.68ms
step:1955/2315 train_time:118631ms step_avg:60.68ms
step:1956/2315 train_time:118692ms step_avg:60.68ms
step:1957/2315 train_time:118753ms step_avg:60.68ms
step:1958/2315 train_time:118814ms step_avg:60.68ms
step:1959/2315 train_time:118876ms step_avg:60.68ms
step:1960/2315 train_time:118936ms step_avg:60.68ms
step:1961/2315 train_time:118998ms step_avg:60.68ms
step:1962/2315 train_time:119059ms step_avg:60.68ms
step:1963/2315 train_time:119121ms step_avg:60.68ms
step:1964/2315 train_time:119181ms step_avg:60.68ms
step:1965/2315 train_time:119243ms step_avg:60.68ms
step:1966/2315 train_time:119304ms step_avg:60.68ms
step:1967/2315 train_time:119365ms step_avg:60.68ms
step:1968/2315 train_time:119427ms step_avg:60.68ms
step:1969/2315 train_time:119488ms step_avg:60.68ms
step:1970/2315 train_time:119549ms step_avg:60.68ms
step:1971/2315 train_time:119610ms step_avg:60.69ms
step:1972/2315 train_time:119671ms step_avg:60.69ms
step:1973/2315 train_time:119733ms step_avg:60.69ms
step:1974/2315 train_time:119793ms step_avg:60.69ms
step:1975/2315 train_time:119855ms step_avg:60.69ms
step:1976/2315 train_time:119916ms step_avg:60.69ms
step:1977/2315 train_time:119977ms step_avg:60.69ms
step:1978/2315 train_time:120038ms step_avg:60.69ms
step:1979/2315 train_time:120100ms step_avg:60.69ms
step:1980/2315 train_time:120161ms step_avg:60.69ms
step:1981/2315 train_time:120222ms step_avg:60.69ms
step:1982/2315 train_time:120283ms step_avg:60.69ms
step:1983/2315 train_time:120345ms step_avg:60.69ms
step:1984/2315 train_time:120406ms step_avg:60.69ms
step:1985/2315 train_time:120467ms step_avg:60.69ms
step:1986/2315 train_time:120528ms step_avg:60.69ms
step:1987/2315 train_time:120589ms step_avg:60.69ms
step:1988/2315 train_time:120650ms step_avg:60.69ms
step:1989/2315 train_time:120712ms step_avg:60.69ms
step:1990/2315 train_time:120773ms step_avg:60.69ms
step:1991/2315 train_time:120834ms step_avg:60.69ms
step:1992/2315 train_time:120895ms step_avg:60.69ms
step:1993/2315 train_time:120957ms step_avg:60.69ms
step:1994/2315 train_time:121018ms step_avg:60.69ms
step:1995/2315 train_time:121080ms step_avg:60.69ms
step:1996/2315 train_time:121141ms step_avg:60.69ms
step:1997/2315 train_time:121202ms step_avg:60.69ms
step:1998/2315 train_time:121263ms step_avg:60.69ms
step:1999/2315 train_time:121324ms step_avg:60.69ms
step:2000/2315 train_time:121385ms step_avg:60.69ms
step:2000/2315 val_loss:3.3298 train_time:121448ms step_avg:60.72ms
step:2001/2315 train_time:121468ms step_avg:60.70ms
step:2002/2315 train_time:121510ms step_avg:60.69ms
step:2003/2315 train_time:121578ms step_avg:60.70ms
step:2004/2315 train_time:121642ms step_avg:60.70ms
step:2005/2315 train_time:121703ms step_avg:60.70ms
step:2006/2315 train_time:121764ms step_avg:60.70ms
step:2007/2315 train_time:121825ms step_avg:60.70ms
step:2008/2315 train_time:121886ms step_avg:60.70ms
step:2009/2315 train_time:121946ms step_avg:60.70ms
step:2010/2315 train_time:122007ms step_avg:60.70ms
step:2011/2315 train_time:122067ms step_avg:60.70ms
step:2012/2315 train_time:122127ms step_avg:60.70ms
step:2013/2315 train_time:122188ms step_avg:60.70ms
step:2014/2315 train_time:122248ms step_avg:60.70ms
step:2015/2315 train_time:122309ms step_avg:60.70ms
step:2016/2315 train_time:122370ms step_avg:60.70ms
step:2017/2315 train_time:122433ms step_avg:60.70ms
step:2018/2315 train_time:122496ms step_avg:60.70ms
step:2019/2315 train_time:122559ms step_avg:60.70ms
step:2020/2315 train_time:122621ms step_avg:60.70ms
step:2021/2315 train_time:122683ms step_avg:60.70ms
step:2022/2315 train_time:122744ms step_avg:60.70ms
step:2023/2315 train_time:122805ms step_avg:60.70ms
step:2024/2315 train_time:122866ms step_avg:60.70ms
step:2025/2315 train_time:122927ms step_avg:60.70ms
step:2026/2315 train_time:122988ms step_avg:60.70ms
step:2027/2315 train_time:123048ms step_avg:60.70ms
step:2028/2315 train_time:123109ms step_avg:60.70ms
step:2029/2315 train_time:123169ms step_avg:60.70ms
step:2030/2315 train_time:123229ms step_avg:60.70ms
step:2031/2315 train_time:123290ms step_avg:60.70ms
step:2032/2315 train_time:123351ms step_avg:60.70ms
step:2033/2315 train_time:123414ms step_avg:60.71ms
step:2034/2315 train_time:123476ms step_avg:60.71ms
step:2035/2315 train_time:123538ms step_avg:60.71ms
step:2036/2315 train_time:123599ms step_avg:60.71ms
step:2037/2315 train_time:123661ms step_avg:60.71ms
step:2038/2315 train_time:123722ms step_avg:60.71ms
step:2039/2315 train_time:123783ms step_avg:60.71ms
step:2040/2315 train_time:123845ms step_avg:60.71ms
step:2041/2315 train_time:123906ms step_avg:60.71ms
step:2042/2315 train_time:123967ms step_avg:60.71ms
step:2043/2315 train_time:124029ms step_avg:60.71ms
step:2044/2315 train_time:124089ms step_avg:60.71ms
step:2045/2315 train_time:124150ms step_avg:60.71ms
step:2046/2315 train_time:124211ms step_avg:60.71ms
step:2047/2315 train_time:124271ms step_avg:60.71ms
step:2048/2315 train_time:124332ms step_avg:60.71ms
step:2049/2315 train_time:124394ms step_avg:60.71ms
step:2050/2315 train_time:124456ms step_avg:60.71ms
step:2051/2315 train_time:124517ms step_avg:60.71ms
step:2052/2315 train_time:124578ms step_avg:60.71ms
step:2053/2315 train_time:124640ms step_avg:60.71ms
step:2054/2315 train_time:124702ms step_avg:60.71ms
step:2055/2315 train_time:124764ms step_avg:60.71ms
step:2056/2315 train_time:124824ms step_avg:60.71ms
step:2057/2315 train_time:124886ms step_avg:60.71ms
step:2058/2315 train_time:124946ms step_avg:60.71ms
step:2059/2315 train_time:125007ms step_avg:60.71ms
step:2060/2315 train_time:125068ms step_avg:60.71ms
step:2061/2315 train_time:125129ms step_avg:60.71ms
step:2062/2315 train_time:125190ms step_avg:60.71ms
step:2063/2315 train_time:125251ms step_avg:60.71ms
step:2064/2315 train_time:125312ms step_avg:60.71ms
step:2065/2315 train_time:125374ms step_avg:60.71ms
step:2066/2315 train_time:125436ms step_avg:60.71ms
step:2067/2315 train_time:125497ms step_avg:60.71ms
step:2068/2315 train_time:125558ms step_avg:60.71ms
step:2069/2315 train_time:125620ms step_avg:60.72ms
step:2070/2315 train_time:125682ms step_avg:60.72ms
step:2071/2315 train_time:125744ms step_avg:60.72ms
step:2072/2315 train_time:125805ms step_avg:60.72ms
step:2073/2315 train_time:125866ms step_avg:60.72ms
step:2074/2315 train_time:125927ms step_avg:60.72ms
step:2075/2315 train_time:125988ms step_avg:60.72ms
step:2076/2315 train_time:126049ms step_avg:60.72ms
step:2077/2315 train_time:126110ms step_avg:60.72ms
step:2078/2315 train_time:126171ms step_avg:60.72ms
step:2079/2315 train_time:126232ms step_avg:60.72ms
step:2080/2315 train_time:126293ms step_avg:60.72ms
step:2081/2315 train_time:126355ms step_avg:60.72ms
step:2082/2315 train_time:126416ms step_avg:60.72ms
step:2083/2315 train_time:126478ms step_avg:60.72ms
step:2084/2315 train_time:126539ms step_avg:60.72ms
step:2085/2315 train_time:126600ms step_avg:60.72ms
step:2086/2315 train_time:126661ms step_avg:60.72ms
step:2087/2315 train_time:126723ms step_avg:60.72ms
step:2088/2315 train_time:126784ms step_avg:60.72ms
step:2089/2315 train_time:126846ms step_avg:60.72ms
step:2090/2315 train_time:126907ms step_avg:60.72ms
step:2091/2315 train_time:126969ms step_avg:60.72ms
step:2092/2315 train_time:127030ms step_avg:60.72ms
step:2093/2315 train_time:127091ms step_avg:60.72ms
step:2094/2315 train_time:127152ms step_avg:60.72ms
step:2095/2315 train_time:127213ms step_avg:60.72ms
step:2096/2315 train_time:127274ms step_avg:60.72ms
step:2097/2315 train_time:127335ms step_avg:60.72ms
step:2098/2315 train_time:127396ms step_avg:60.72ms
step:2099/2315 train_time:127457ms step_avg:60.72ms
step:2100/2315 train_time:127518ms step_avg:60.72ms
step:2101/2315 train_time:127580ms step_avg:60.72ms
step:2102/2315 train_time:127642ms step_avg:60.72ms
step:2103/2315 train_time:127704ms step_avg:60.72ms
step:2104/2315 train_time:127765ms step_avg:60.72ms
step:2105/2315 train_time:127826ms step_avg:60.73ms
step:2106/2315 train_time:127887ms step_avg:60.73ms
step:2107/2315 train_time:127949ms step_avg:60.73ms
step:2108/2315 train_time:128009ms step_avg:60.73ms
step:2109/2315 train_time:128071ms step_avg:60.73ms
step:2110/2315 train_time:128132ms step_avg:60.73ms
step:2111/2315 train_time:128193ms step_avg:60.73ms
step:2112/2315 train_time:128254ms step_avg:60.73ms
step:2113/2315 train_time:128315ms step_avg:60.73ms
step:2114/2315 train_time:128376ms step_avg:60.73ms
step:2115/2315 train_time:128437ms step_avg:60.73ms
step:2116/2315 train_time:128498ms step_avg:60.73ms
step:2117/2315 train_time:128560ms step_avg:60.73ms
step:2118/2315 train_time:128621ms step_avg:60.73ms
step:2119/2315 train_time:128682ms step_avg:60.73ms
step:2120/2315 train_time:128743ms step_avg:60.73ms
step:2121/2315 train_time:128804ms step_avg:60.73ms
step:2122/2315 train_time:128866ms step_avg:60.73ms
step:2123/2315 train_time:128927ms step_avg:60.73ms
step:2124/2315 train_time:128988ms step_avg:60.73ms
step:2125/2315 train_time:129049ms step_avg:60.73ms
step:2126/2315 train_time:129110ms step_avg:60.73ms
step:2127/2315 train_time:129171ms step_avg:60.73ms
step:2128/2315 train_time:129233ms step_avg:60.73ms
step:2129/2315 train_time:129295ms step_avg:60.73ms
step:2130/2315 train_time:129356ms step_avg:60.73ms
step:2131/2315 train_time:129417ms step_avg:60.73ms
step:2132/2315 train_time:129478ms step_avg:60.73ms
step:2133/2315 train_time:129540ms step_avg:60.73ms
step:2134/2315 train_time:129601ms step_avg:60.73ms
step:2135/2315 train_time:129661ms step_avg:60.73ms
step:2136/2315 train_time:129723ms step_avg:60.73ms
step:2137/2315 train_time:129784ms step_avg:60.73ms
step:2138/2315 train_time:129845ms step_avg:60.73ms
step:2139/2315 train_time:129906ms step_avg:60.73ms
step:2140/2315 train_time:129967ms step_avg:60.73ms
step:2141/2315 train_time:130029ms step_avg:60.73ms
step:2142/2315 train_time:130091ms step_avg:60.73ms
step:2143/2315 train_time:130152ms step_avg:60.73ms
step:2144/2315 train_time:130213ms step_avg:60.73ms
step:2145/2315 train_time:130274ms step_avg:60.73ms
step:2146/2315 train_time:130335ms step_avg:60.73ms
step:2147/2315 train_time:130397ms step_avg:60.73ms
step:2148/2315 train_time:130458ms step_avg:60.73ms
step:2149/2315 train_time:130519ms step_avg:60.73ms
step:2150/2315 train_time:130580ms step_avg:60.73ms
step:2151/2315 train_time:130642ms step_avg:60.74ms
step:2152/2315 train_time:130703ms step_avg:60.74ms
step:2153/2315 train_time:130764ms step_avg:60.74ms
step:2154/2315 train_time:130826ms step_avg:60.74ms
step:2155/2315 train_time:130887ms step_avg:60.74ms
step:2156/2315 train_time:130947ms step_avg:60.74ms
step:2157/2315 train_time:131009ms step_avg:60.74ms
step:2158/2315 train_time:131069ms step_avg:60.74ms
step:2159/2315 train_time:131131ms step_avg:60.74ms
step:2160/2315 train_time:131193ms step_avg:60.74ms
step:2161/2315 train_time:131254ms step_avg:60.74ms
step:2162/2315 train_time:131315ms step_avg:60.74ms
step:2163/2315 train_time:131376ms step_avg:60.74ms
step:2164/2315 train_time:131437ms step_avg:60.74ms
step:2165/2315 train_time:131499ms step_avg:60.74ms
step:2166/2315 train_time:131560ms step_avg:60.74ms
step:2167/2315 train_time:131622ms step_avg:60.74ms
step:2168/2315 train_time:131683ms step_avg:60.74ms
step:2169/2315 train_time:131744ms step_avg:60.74ms
step:2170/2315 train_time:131805ms step_avg:60.74ms
step:2171/2315 train_time:131866ms step_avg:60.74ms
step:2172/2315 train_time:131928ms step_avg:60.74ms
step:2173/2315 train_time:131989ms step_avg:60.74ms
step:2174/2315 train_time:132050ms step_avg:60.74ms
step:2175/2315 train_time:132112ms step_avg:60.74ms
step:2176/2315 train_time:132173ms step_avg:60.74ms
step:2177/2315 train_time:132234ms step_avg:60.74ms
step:2178/2315 train_time:132295ms step_avg:60.74ms
step:2179/2315 train_time:132356ms step_avg:60.74ms
step:2180/2315 train_time:132417ms step_avg:60.74ms
step:2181/2315 train_time:132479ms step_avg:60.74ms
step:2182/2315 train_time:132541ms step_avg:60.74ms
step:2183/2315 train_time:132602ms step_avg:60.74ms
step:2184/2315 train_time:132663ms step_avg:60.74ms
step:2185/2315 train_time:132724ms step_avg:60.74ms
step:2186/2315 train_time:132785ms step_avg:60.74ms
step:2187/2315 train_time:132847ms step_avg:60.74ms
step:2188/2315 train_time:132908ms step_avg:60.74ms
step:2189/2315 train_time:132969ms step_avg:60.74ms
step:2190/2315 train_time:133030ms step_avg:60.74ms
step:2191/2315 train_time:133092ms step_avg:60.74ms
step:2192/2315 train_time:133153ms step_avg:60.75ms
step:2193/2315 train_time:133215ms step_avg:60.75ms
step:2194/2315 train_time:133275ms step_avg:60.75ms
step:2195/2315 train_time:133336ms step_avg:60.75ms
step:2196/2315 train_time:133397ms step_avg:60.75ms
step:2197/2315 train_time:133458ms step_avg:60.75ms
step:2198/2315 train_time:133520ms step_avg:60.75ms
step:2199/2315 train_time:133581ms step_avg:60.75ms
step:2200/2315 train_time:133642ms step_avg:60.75ms
step:2201/2315 train_time:133703ms step_avg:60.75ms
step:2202/2315 train_time:133764ms step_avg:60.75ms
step:2203/2315 train_time:133825ms step_avg:60.75ms
step:2204/2315 train_time:133886ms step_avg:60.75ms
step:2205/2315 train_time:133947ms step_avg:60.75ms
step:2206/2315 train_time:134008ms step_avg:60.75ms
step:2207/2315 train_time:134070ms step_avg:60.75ms
step:2208/2315 train_time:134131ms step_avg:60.75ms
step:2209/2315 train_time:134194ms step_avg:60.75ms
step:2210/2315 train_time:134255ms step_avg:60.75ms
step:2211/2315 train_time:134316ms step_avg:60.75ms
step:2212/2315 train_time:134377ms step_avg:60.75ms
step:2213/2315 train_time:134438ms step_avg:60.75ms
step:2214/2315 train_time:134499ms step_avg:60.75ms
step:2215/2315 train_time:134562ms step_avg:60.75ms
step:2216/2315 train_time:134623ms step_avg:60.75ms
step:2217/2315 train_time:134684ms step_avg:60.75ms
step:2218/2315 train_time:134746ms step_avg:60.75ms
step:2219/2315 train_time:134807ms step_avg:60.75ms
step:2220/2315 train_time:134868ms step_avg:60.75ms
step:2221/2315 train_time:134930ms step_avg:60.75ms
step:2222/2315 train_time:134991ms step_avg:60.75ms
step:2223/2315 train_time:135052ms step_avg:60.75ms
step:2224/2315 train_time:135113ms step_avg:60.75ms
step:2225/2315 train_time:135175ms step_avg:60.75ms
step:2226/2315 train_time:135236ms step_avg:60.75ms
step:2227/2315 train_time:135297ms step_avg:60.75ms
step:2228/2315 train_time:135358ms step_avg:60.75ms
step:2229/2315 train_time:135419ms step_avg:60.75ms
step:2230/2315 train_time:135480ms step_avg:60.75ms
step:2231/2315 train_time:135541ms step_avg:60.75ms
step:2232/2315 train_time:135602ms step_avg:60.75ms
step:2233/2315 train_time:135664ms step_avg:60.75ms
step:2234/2315 train_time:135724ms step_avg:60.75ms
step:2235/2315 train_time:135786ms step_avg:60.75ms
step:2236/2315 train_time:135848ms step_avg:60.75ms
step:2237/2315 train_time:135909ms step_avg:60.75ms
step:2238/2315 train_time:135969ms step_avg:60.75ms
step:2239/2315 train_time:136031ms step_avg:60.76ms
step:2240/2315 train_time:136093ms step_avg:60.76ms
step:2241/2315 train_time:136154ms step_avg:60.76ms
step:2242/2315 train_time:136215ms step_avg:60.76ms
step:2243/2315 train_time:136276ms step_avg:60.76ms
step:2244/2315 train_time:136337ms step_avg:60.76ms
step:2245/2315 train_time:136399ms step_avg:60.76ms
step:2246/2315 train_time:136461ms step_avg:60.76ms
step:2247/2315 train_time:136521ms step_avg:60.76ms
step:2248/2315 train_time:136582ms step_avg:60.76ms
step:2249/2315 train_time:136643ms step_avg:60.76ms
step:2250/2315 train_time:136704ms step_avg:60.76ms
step:2250/2315 val_loss:3.2897 train_time:136767ms step_avg:60.79ms
step:2251/2315 train_time:136787ms step_avg:60.77ms
step:2252/2315 train_time:136829ms step_avg:60.76ms
step:2253/2315 train_time:136896ms step_avg:60.76ms
step:2254/2315 train_time:136959ms step_avg:60.76ms
step:2255/2315 train_time:137021ms step_avg:60.76ms
step:2256/2315 train_time:137083ms step_avg:60.76ms
step:2257/2315 train_time:137144ms step_avg:60.76ms
step:2258/2315 train_time:137204ms step_avg:60.76ms
step:2259/2315 train_time:137266ms step_avg:60.76ms
step:2260/2315 train_time:137327ms step_avg:60.76ms
step:2261/2315 train_time:137388ms step_avg:60.76ms
step:2262/2315 train_time:137448ms step_avg:60.76ms
step:2263/2315 train_time:137509ms step_avg:60.76ms
step:2264/2315 train_time:137570ms step_avg:60.76ms
step:2265/2315 train_time:137630ms step_avg:60.76ms
step:2266/2315 train_time:137690ms step_avg:60.76ms
step:2267/2315 train_time:137753ms step_avg:60.76ms
step:2268/2315 train_time:137816ms step_avg:60.77ms
step:2269/2315 train_time:137879ms step_avg:60.77ms
step:2270/2315 train_time:137941ms step_avg:60.77ms
step:2271/2315 train_time:138004ms step_avg:60.77ms
step:2272/2315 train_time:138065ms step_avg:60.77ms
step:2273/2315 train_time:138126ms step_avg:60.77ms
step:2274/2315 train_time:138187ms step_avg:60.77ms
step:2275/2315 train_time:138248ms step_avg:60.77ms
step:2276/2315 train_time:138309ms step_avg:60.77ms
step:2277/2315 train_time:138369ms step_avg:60.77ms
step:2278/2315 train_time:138430ms step_avg:60.77ms
step:2279/2315 train_time:138491ms step_avg:60.77ms
step:2280/2315 train_time:138552ms step_avg:60.77ms
step:2281/2315 train_time:138613ms step_avg:60.77ms
step:2282/2315 train_time:138673ms step_avg:60.77ms
step:2283/2315 train_time:138734ms step_avg:60.77ms
step:2284/2315 train_time:138795ms step_avg:60.77ms
step:2285/2315 train_time:138858ms step_avg:60.77ms
step:2286/2315 train_time:138920ms step_avg:60.77ms
step:2287/2315 train_time:138982ms step_avg:60.77ms
step:2288/2315 train_time:139043ms step_avg:60.77ms
step:2289/2315 train_time:139105ms step_avg:60.77ms
step:2290/2315 train_time:139166ms step_avg:60.77ms
step:2291/2315 train_time:139227ms step_avg:60.77ms
step:2292/2315 train_time:139288ms step_avg:60.77ms
step:2293/2315 train_time:139349ms step_avg:60.77ms
step:2294/2315 train_time:139410ms step_avg:60.77ms
step:2295/2315 train_time:139470ms step_avg:60.77ms
step:2296/2315 train_time:139531ms step_avg:60.77ms
step:2297/2315 train_time:139591ms step_avg:60.77ms
step:2298/2315 train_time:139652ms step_avg:60.77ms
step:2299/2315 train_time:139714ms step_avg:60.77ms
step:2300/2315 train_time:139775ms step_avg:60.77ms
step:2301/2315 train_time:139838ms step_avg:60.77ms
step:2302/2315 train_time:139900ms step_avg:60.77ms
step:2303/2315 train_time:139961ms step_avg:60.77ms
step:2304/2315 train_time:140023ms step_avg:60.77ms
step:2305/2315 train_time:140085ms step_avg:60.77ms
step:2306/2315 train_time:140146ms step_avg:60.77ms
step:2307/2315 train_time:140207ms step_avg:60.77ms
step:2308/2315 train_time:140269ms step_avg:60.77ms
step:2309/2315 train_time:140329ms step_avg:60.77ms
step:2310/2315 train_time:140390ms step_avg:60.77ms
step:2311/2315 train_time:140451ms step_avg:60.78ms
step:2312/2315 train_time:140512ms step_avg:60.78ms
step:2313/2315 train_time:140574ms step_avg:60.78ms
step:2314/2315 train_time:140634ms step_avg:60.78ms
step:2315/2315 train_time:140696ms step_avg:60.78ms
step:2315/2315 val_loss:3.2770 train_time:140758ms step_avg:60.80ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

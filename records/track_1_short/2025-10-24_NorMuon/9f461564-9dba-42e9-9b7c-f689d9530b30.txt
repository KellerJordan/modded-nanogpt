import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]
# polar_express_coeffs = [
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
# ]



@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    # state["second_momentum_buffer"] = torch.zeros_like(grad[...,0:1]) if param.size(-2) >= param.size(-1) else torch.zeros_like(grad[...,0:1,:])
                    # state["second_momentum_buffer"] = torch.zeros((*grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((*grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)
            
            ###################################
            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))
            ####################################


            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> (-1.5)  0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Oct 22 2025, 02:20:45) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251022+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 24 05:54:50 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000001:00:00.0 Off |                    0 |
| N/A   35C    P0            118W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000002:00:00.0 Off |                    0 |
| N/A   35C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000003:00:00.0 Off |                    0 |
| N/A   30C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000008:00:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000009:00:00.0 Off |                    0 |
| N/A   28C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   0000000A:00:00.0 Off |                    0 |
| N/A   35C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   0000000C:00:00.0 Off |                    0 |
| N/A   34C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2315 train_time:88ms step_avg:87.81ms
step:2/2315 train_time:184ms step_avg:91.84ms
step:3/2315 train_time:205ms step_avg:68.33ms
step:4/2315 train_time:241ms step_avg:60.35ms
step:5/2315 train_time:300ms step_avg:59.99ms
step:6/2315 train_time:359ms step_avg:59.89ms
step:7/2315 train_time:419ms step_avg:59.91ms
step:8/2315 train_time:480ms step_avg:59.94ms
step:9/2315 train_time:539ms step_avg:59.94ms
step:10/2315 train_time:600ms step_avg:59.97ms
step:11/2315 train_time:659ms step_avg:59.95ms
step:12/2315 train_time:719ms step_avg:59.95ms
step:13/2315 train_time:779ms step_avg:59.96ms
step:14/2315 train_time:839ms step_avg:59.95ms
step:15/2315 train_time:899ms step_avg:59.92ms
step:16/2315 train_time:959ms step_avg:59.92ms
step:17/2315 train_time:1019ms step_avg:59.95ms
step:18/2315 train_time:1082ms step_avg:60.10ms
step:19/2315 train_time:1145ms step_avg:60.27ms
step:20/2315 train_time:1206ms step_avg:60.30ms
step:21/2315 train_time:1266ms step_avg:60.30ms
step:22/2315 train_time:1327ms step_avg:60.30ms
step:23/2315 train_time:1387ms step_avg:60.30ms
step:24/2315 train_time:1447ms step_avg:60.30ms
step:25/2315 train_time:1508ms step_avg:60.32ms
step:26/2315 train_time:1569ms step_avg:60.33ms
step:27/2315 train_time:1630ms step_avg:60.36ms
step:28/2315 train_time:1690ms step_avg:60.35ms
step:29/2315 train_time:1751ms step_avg:60.37ms
step:30/2315 train_time:1811ms step_avg:60.37ms
step:31/2315 train_time:1872ms step_avg:60.38ms
step:32/2315 train_time:1932ms step_avg:60.37ms
step:33/2315 train_time:1993ms step_avg:60.39ms
step:34/2315 train_time:2053ms step_avg:60.40ms
step:35/2315 train_time:2115ms step_avg:60.42ms
step:36/2315 train_time:2175ms step_avg:60.43ms
step:37/2315 train_time:2237ms step_avg:60.46ms
step:38/2315 train_time:2297ms step_avg:60.46ms
step:39/2315 train_time:2358ms step_avg:60.46ms
step:40/2315 train_time:2418ms step_avg:60.46ms
step:41/2315 train_time:2479ms step_avg:60.45ms
step:42/2315 train_time:2539ms step_avg:60.44ms
step:43/2315 train_time:2599ms step_avg:60.44ms
step:44/2315 train_time:2659ms step_avg:60.44ms
step:45/2315 train_time:2720ms step_avg:60.44ms
step:46/2315 train_time:2780ms step_avg:60.43ms
step:47/2315 train_time:2840ms step_avg:60.44ms
step:48/2315 train_time:2901ms step_avg:60.45ms
step:49/2315 train_time:2961ms step_avg:60.44ms
step:50/2315 train_time:3022ms step_avg:60.44ms
step:51/2315 train_time:3082ms step_avg:60.43ms
step:52/2315 train_time:3142ms step_avg:60.43ms
step:53/2315 train_time:3203ms step_avg:60.42ms
step:54/2315 train_time:3263ms step_avg:60.42ms
step:55/2315 train_time:3322ms step_avg:60.40ms
step:56/2315 train_time:3383ms step_avg:60.40ms
step:57/2315 train_time:3443ms step_avg:60.40ms
step:58/2315 train_time:3502ms step_avg:60.39ms
step:59/2315 train_time:3563ms step_avg:60.40ms
step:60/2315 train_time:3624ms step_avg:60.39ms
step:61/2315 train_time:3684ms step_avg:60.40ms
step:62/2315 train_time:3744ms step_avg:60.39ms
step:63/2315 train_time:3805ms step_avg:60.39ms
step:64/2315 train_time:3865ms step_avg:60.39ms
step:65/2315 train_time:3925ms step_avg:60.39ms
step:66/2315 train_time:3985ms step_avg:60.38ms
step:67/2315 train_time:4046ms step_avg:60.38ms
step:68/2315 train_time:4106ms step_avg:60.38ms
step:69/2315 train_time:4166ms step_avg:60.37ms
step:70/2315 train_time:4226ms step_avg:60.37ms
step:71/2315 train_time:4287ms step_avg:60.38ms
step:72/2315 train_time:4348ms step_avg:60.38ms
step:73/2315 train_time:4408ms step_avg:60.38ms
step:74/2315 train_time:4468ms step_avg:60.38ms
step:75/2315 train_time:4529ms step_avg:60.38ms
step:76/2315 train_time:4589ms step_avg:60.38ms
step:77/2315 train_time:4650ms step_avg:60.38ms
step:78/2315 train_time:4710ms step_avg:60.39ms
step:79/2315 train_time:4771ms step_avg:60.39ms
step:80/2315 train_time:4831ms step_avg:60.39ms
step:81/2315 train_time:4891ms step_avg:60.39ms
step:82/2315 train_time:4952ms step_avg:60.39ms
step:83/2315 train_time:5012ms step_avg:60.39ms
step:84/2315 train_time:5073ms step_avg:60.39ms
step:85/2315 train_time:5134ms step_avg:60.40ms
step:86/2315 train_time:5195ms step_avg:60.40ms
step:87/2315 train_time:5255ms step_avg:60.40ms
step:88/2315 train_time:5315ms step_avg:60.39ms
step:89/2315 train_time:5375ms step_avg:60.40ms
step:90/2315 train_time:5436ms step_avg:60.40ms
step:91/2315 train_time:5496ms step_avg:60.39ms
step:92/2315 train_time:5556ms step_avg:60.39ms
step:93/2315 train_time:5616ms step_avg:60.39ms
step:94/2315 train_time:5676ms step_avg:60.39ms
step:95/2315 train_time:5737ms step_avg:60.39ms
step:96/2315 train_time:5797ms step_avg:60.39ms
step:97/2315 train_time:5858ms step_avg:60.39ms
step:98/2315 train_time:5918ms step_avg:60.39ms
step:99/2315 train_time:5978ms step_avg:60.38ms
step:100/2315 train_time:6038ms step_avg:60.38ms
step:101/2315 train_time:6098ms step_avg:60.38ms
step:102/2315 train_time:6158ms step_avg:60.37ms
step:103/2315 train_time:6218ms step_avg:60.36ms
step:104/2315 train_time:6277ms step_avg:60.36ms
step:105/2315 train_time:6337ms step_avg:60.36ms
step:106/2315 train_time:6398ms step_avg:60.35ms
step:107/2315 train_time:6457ms step_avg:60.35ms
step:108/2315 train_time:6517ms step_avg:60.34ms
step:109/2315 train_time:6577ms step_avg:60.34ms
step:110/2315 train_time:6637ms step_avg:60.34ms
step:111/2315 train_time:6697ms step_avg:60.33ms
step:112/2315 train_time:6758ms step_avg:60.34ms
step:113/2315 train_time:6818ms step_avg:60.34ms
step:114/2315 train_time:6878ms step_avg:60.34ms
step:115/2315 train_time:6938ms step_avg:60.33ms
step:116/2315 train_time:6998ms step_avg:60.33ms
step:117/2315 train_time:7059ms step_avg:60.33ms
step:118/2315 train_time:7119ms step_avg:60.33ms
step:119/2315 train_time:7179ms step_avg:60.33ms
step:120/2315 train_time:7239ms step_avg:60.32ms
step:121/2315 train_time:7298ms step_avg:60.32ms
step:122/2315 train_time:7358ms step_avg:60.31ms
step:123/2315 train_time:7418ms step_avg:60.31ms
step:124/2315 train_time:7477ms step_avg:60.30ms
step:125/2315 train_time:7537ms step_avg:60.30ms
step:126/2315 train_time:7597ms step_avg:60.30ms
step:127/2315 train_time:7657ms step_avg:60.29ms
step:128/2315 train_time:7717ms step_avg:60.29ms
step:129/2315 train_time:7777ms step_avg:60.29ms
step:130/2315 train_time:7837ms step_avg:60.29ms
step:131/2315 train_time:7898ms step_avg:60.29ms
step:132/2315 train_time:7958ms step_avg:60.29ms
step:133/2315 train_time:8018ms step_avg:60.28ms
step:134/2315 train_time:8077ms step_avg:60.28ms
step:135/2315 train_time:8137ms step_avg:60.28ms
step:136/2315 train_time:8197ms step_avg:60.27ms
step:137/2315 train_time:8257ms step_avg:60.27ms
step:138/2315 train_time:8317ms step_avg:60.27ms
step:139/2315 train_time:8377ms step_avg:60.26ms
step:140/2315 train_time:8436ms step_avg:60.26ms
step:141/2315 train_time:8496ms step_avg:60.26ms
step:142/2315 train_time:8556ms step_avg:60.25ms
step:143/2315 train_time:8616ms step_avg:60.25ms
step:144/2315 train_time:8676ms step_avg:60.25ms
step:145/2315 train_time:8737ms step_avg:60.25ms
step:146/2315 train_time:8796ms step_avg:60.25ms
step:147/2315 train_time:8857ms step_avg:60.25ms
step:148/2315 train_time:8917ms step_avg:60.25ms
step:149/2315 train_time:8977ms step_avg:60.25ms
step:150/2315 train_time:9037ms step_avg:60.25ms
step:151/2315 train_time:9097ms step_avg:60.25ms
step:152/2315 train_time:9157ms step_avg:60.24ms
step:153/2315 train_time:9217ms step_avg:60.24ms
step:154/2315 train_time:9277ms step_avg:60.24ms
step:155/2315 train_time:9337ms step_avg:60.24ms
step:156/2315 train_time:9396ms step_avg:60.23ms
step:157/2315 train_time:9456ms step_avg:60.23ms
step:158/2315 train_time:9516ms step_avg:60.23ms
step:159/2315 train_time:9575ms step_avg:60.22ms
step:160/2315 train_time:9636ms step_avg:60.22ms
step:161/2315 train_time:9696ms step_avg:60.22ms
step:162/2315 train_time:9755ms step_avg:60.22ms
step:163/2315 train_time:9816ms step_avg:60.22ms
step:164/2315 train_time:9876ms step_avg:60.22ms
step:165/2315 train_time:9937ms step_avg:60.23ms
step:166/2315 train_time:9997ms step_avg:60.22ms
step:167/2315 train_time:10057ms step_avg:60.22ms
step:168/2315 train_time:10117ms step_avg:60.22ms
step:169/2315 train_time:10177ms step_avg:60.22ms
step:170/2315 train_time:10236ms step_avg:60.21ms
step:171/2315 train_time:10296ms step_avg:60.21ms
step:172/2315 train_time:10356ms step_avg:60.21ms
step:173/2315 train_time:10416ms step_avg:60.21ms
step:174/2315 train_time:10475ms step_avg:60.20ms
step:175/2315 train_time:10535ms step_avg:60.20ms
step:176/2315 train_time:10595ms step_avg:60.20ms
step:177/2315 train_time:10655ms step_avg:60.20ms
step:178/2315 train_time:10715ms step_avg:60.20ms
step:179/2315 train_time:10775ms step_avg:60.20ms
step:180/2315 train_time:10835ms step_avg:60.20ms
step:181/2315 train_time:10896ms step_avg:60.20ms
step:182/2315 train_time:10956ms step_avg:60.20ms
step:183/2315 train_time:11017ms step_avg:60.20ms
step:184/2315 train_time:11076ms step_avg:60.20ms
step:185/2315 train_time:11136ms step_avg:60.20ms
step:186/2315 train_time:11196ms step_avg:60.20ms
step:187/2315 train_time:11256ms step_avg:60.19ms
step:188/2315 train_time:11317ms step_avg:60.20ms
step:189/2315 train_time:11377ms step_avg:60.19ms
step:190/2315 train_time:11436ms step_avg:60.19ms
step:191/2315 train_time:11496ms step_avg:60.19ms
step:192/2315 train_time:11555ms step_avg:60.18ms
step:193/2315 train_time:11616ms step_avg:60.19ms
step:194/2315 train_time:11676ms step_avg:60.19ms
step:195/2315 train_time:11737ms step_avg:60.19ms
step:196/2315 train_time:11796ms step_avg:60.19ms
step:197/2315 train_time:11856ms step_avg:60.18ms
step:198/2315 train_time:11916ms step_avg:60.18ms
step:199/2315 train_time:11976ms step_avg:60.18ms
step:200/2315 train_time:12036ms step_avg:60.18ms
step:201/2315 train_time:12096ms step_avg:60.18ms
step:202/2315 train_time:12155ms step_avg:60.18ms
step:203/2315 train_time:12215ms step_avg:60.17ms
step:204/2315 train_time:12275ms step_avg:60.17ms
step:205/2315 train_time:12335ms step_avg:60.17ms
step:206/2315 train_time:12396ms step_avg:60.17ms
step:207/2315 train_time:12456ms step_avg:60.18ms
step:208/2315 train_time:12516ms step_avg:60.17ms
step:209/2315 train_time:12575ms step_avg:60.17ms
step:210/2315 train_time:12635ms step_avg:60.17ms
step:211/2315 train_time:12696ms step_avg:60.17ms
step:212/2315 train_time:12756ms step_avg:60.17ms
step:213/2315 train_time:12816ms step_avg:60.17ms
step:214/2315 train_time:12876ms step_avg:60.17ms
step:215/2315 train_time:12936ms step_avg:60.17ms
step:216/2315 train_time:12996ms step_avg:60.17ms
step:217/2315 train_time:13056ms step_avg:60.17ms
step:218/2315 train_time:13116ms step_avg:60.17ms
step:219/2315 train_time:13176ms step_avg:60.17ms
step:220/2315 train_time:13236ms step_avg:60.16ms
step:221/2315 train_time:13296ms step_avg:60.16ms
step:222/2315 train_time:13356ms step_avg:60.16ms
step:223/2315 train_time:13416ms step_avg:60.16ms
step:224/2315 train_time:13477ms step_avg:60.17ms
step:225/2315 train_time:13536ms step_avg:60.16ms
step:226/2315 train_time:13596ms step_avg:60.16ms
step:227/2315 train_time:13656ms step_avg:60.16ms
step:228/2315 train_time:13715ms step_avg:60.15ms
step:229/2315 train_time:13775ms step_avg:60.15ms
step:230/2315 train_time:13836ms step_avg:60.15ms
step:231/2315 train_time:13896ms step_avg:60.16ms
step:232/2315 train_time:13956ms step_avg:60.16ms
step:233/2315 train_time:14016ms step_avg:60.16ms
step:234/2315 train_time:14076ms step_avg:60.15ms
step:235/2315 train_time:14136ms step_avg:60.15ms
step:236/2315 train_time:14196ms step_avg:60.15ms
step:237/2315 train_time:14256ms step_avg:60.15ms
step:238/2315 train_time:14316ms step_avg:60.15ms
step:239/2315 train_time:14376ms step_avg:60.15ms
step:240/2315 train_time:14436ms step_avg:60.15ms
step:241/2315 train_time:14496ms step_avg:60.15ms
step:242/2315 train_time:14557ms step_avg:60.15ms
step:243/2315 train_time:14617ms step_avg:60.15ms
step:244/2315 train_time:14677ms step_avg:60.15ms
step:245/2315 train_time:14736ms step_avg:60.15ms
step:246/2315 train_time:14796ms step_avg:60.15ms
step:247/2315 train_time:14856ms step_avg:60.15ms
step:248/2315 train_time:14916ms step_avg:60.15ms
step:249/2315 train_time:14976ms step_avg:60.14ms
step:250/2315 train_time:15036ms step_avg:60.14ms
step:250/2315 val_loss:4.0846 train_time:15097ms step_avg:60.39ms
step:251/2315 train_time:15117ms step_avg:60.23ms
step:252/2315 train_time:15157ms step_avg:60.15ms
step:253/2315 train_time:15221ms step_avg:60.16ms
step:254/2315 train_time:15283ms step_avg:60.17ms
step:255/2315 train_time:15344ms step_avg:60.17ms
step:256/2315 train_time:15405ms step_avg:60.18ms
step:257/2315 train_time:15465ms step_avg:60.18ms
step:258/2315 train_time:15525ms step_avg:60.17ms
step:259/2315 train_time:15584ms step_avg:60.17ms
step:260/2315 train_time:15643ms step_avg:60.17ms
step:261/2315 train_time:15702ms step_avg:60.16ms
step:262/2315 train_time:15761ms step_avg:60.16ms
step:263/2315 train_time:15820ms step_avg:60.15ms
step:264/2315 train_time:15880ms step_avg:60.15ms
step:265/2315 train_time:15939ms step_avg:60.15ms
step:266/2315 train_time:15998ms step_avg:60.14ms
step:267/2315 train_time:16057ms step_avg:60.14ms
step:268/2315 train_time:16117ms step_avg:60.14ms
step:269/2315 train_time:16177ms step_avg:60.14ms
step:270/2315 train_time:16238ms step_avg:60.14ms
step:271/2315 train_time:16299ms step_avg:60.14ms
step:272/2315 train_time:16359ms step_avg:60.14ms
step:273/2315 train_time:16420ms step_avg:60.15ms
step:274/2315 train_time:16481ms step_avg:60.15ms
step:275/2315 train_time:16541ms step_avg:60.15ms
step:276/2315 train_time:16600ms step_avg:60.14ms
step:277/2315 train_time:16660ms step_avg:60.14ms
step:278/2315 train_time:16719ms step_avg:60.14ms
step:279/2315 train_time:16778ms step_avg:60.14ms
step:280/2315 train_time:16838ms step_avg:60.13ms
step:281/2315 train_time:16897ms step_avg:60.13ms
step:282/2315 train_time:16957ms step_avg:60.13ms
step:283/2315 train_time:17016ms step_avg:60.13ms
step:284/2315 train_time:17076ms step_avg:60.13ms
step:285/2315 train_time:17135ms step_avg:60.12ms
step:286/2315 train_time:17195ms step_avg:60.12ms
step:287/2315 train_time:17256ms step_avg:60.13ms
step:288/2315 train_time:17316ms step_avg:60.13ms
step:289/2315 train_time:17378ms step_avg:60.13ms
step:290/2315 train_time:17438ms step_avg:60.13ms
step:291/2315 train_time:17498ms step_avg:60.13ms
step:292/2315 train_time:17559ms step_avg:60.13ms
step:293/2315 train_time:17619ms step_avg:60.13ms
step:294/2315 train_time:17678ms step_avg:60.13ms
step:295/2315 train_time:17738ms step_avg:60.13ms
step:296/2315 train_time:17797ms step_avg:60.13ms
step:297/2315 train_time:17857ms step_avg:60.12ms
step:298/2315 train_time:17916ms step_avg:60.12ms
step:299/2315 train_time:17976ms step_avg:60.12ms
step:300/2315 train_time:18035ms step_avg:60.12ms
step:301/2315 train_time:18095ms step_avg:60.12ms
step:302/2315 train_time:18154ms step_avg:60.11ms
step:303/2315 train_time:18215ms step_avg:60.12ms
step:304/2315 train_time:18275ms step_avg:60.12ms
step:305/2315 train_time:18336ms step_avg:60.12ms
step:306/2315 train_time:18396ms step_avg:60.12ms
step:307/2315 train_time:18456ms step_avg:60.12ms
step:308/2315 train_time:18517ms step_avg:60.12ms
step:309/2315 train_time:18577ms step_avg:60.12ms
step:310/2315 train_time:18638ms step_avg:60.12ms
step:311/2315 train_time:18698ms step_avg:60.12ms
step:312/2315 train_time:18757ms step_avg:60.12ms
step:313/2315 train_time:18817ms step_avg:60.12ms
step:314/2315 train_time:18877ms step_avg:60.12ms
step:315/2315 train_time:18936ms step_avg:60.11ms
step:316/2315 train_time:18995ms step_avg:60.11ms
step:317/2315 train_time:19055ms step_avg:60.11ms
step:318/2315 train_time:19115ms step_avg:60.11ms
step:319/2315 train_time:19175ms step_avg:60.11ms
step:320/2315 train_time:19235ms step_avg:60.11ms
step:321/2315 train_time:19295ms step_avg:60.11ms
step:322/2315 train_time:19355ms step_avg:60.11ms
step:323/2315 train_time:19415ms step_avg:60.11ms
step:324/2315 train_time:19475ms step_avg:60.11ms
step:325/2315 train_time:19536ms step_avg:60.11ms
step:326/2315 train_time:19596ms step_avg:60.11ms
step:327/2315 train_time:19656ms step_avg:60.11ms
step:328/2315 train_time:19717ms step_avg:60.11ms
step:329/2315 train_time:19777ms step_avg:60.11ms
step:330/2315 train_time:19837ms step_avg:60.11ms
step:331/2315 train_time:19897ms step_avg:60.11ms
step:332/2315 train_time:19956ms step_avg:60.11ms
step:333/2315 train_time:20016ms step_avg:60.11ms
step:334/2315 train_time:20075ms step_avg:60.11ms
step:335/2315 train_time:20135ms step_avg:60.11ms
step:336/2315 train_time:20195ms step_avg:60.10ms
step:337/2315 train_time:20255ms step_avg:60.10ms
step:338/2315 train_time:20315ms step_avg:60.10ms
step:339/2315 train_time:20377ms step_avg:60.11ms
step:340/2315 train_time:20436ms step_avg:60.11ms
step:341/2315 train_time:20497ms step_avg:60.11ms
step:342/2315 train_time:20556ms step_avg:60.11ms
step:343/2315 train_time:20617ms step_avg:60.11ms
step:344/2315 train_time:20677ms step_avg:60.11ms
step:345/2315 train_time:20737ms step_avg:60.11ms
step:346/2315 train_time:20797ms step_avg:60.11ms
step:347/2315 train_time:20857ms step_avg:60.11ms
step:348/2315 train_time:20917ms step_avg:60.10ms
step:349/2315 train_time:20976ms step_avg:60.10ms
step:350/2315 train_time:21036ms step_avg:60.10ms
step:351/2315 train_time:21096ms step_avg:60.10ms
step:352/2315 train_time:21156ms step_avg:60.10ms
step:353/2315 train_time:21216ms step_avg:60.10ms
step:354/2315 train_time:21276ms step_avg:60.10ms
step:355/2315 train_time:21336ms step_avg:60.10ms
step:356/2315 train_time:21396ms step_avg:60.10ms
step:357/2315 train_time:21456ms step_avg:60.10ms
step:358/2315 train_time:21516ms step_avg:60.10ms
step:359/2315 train_time:21576ms step_avg:60.10ms
step:360/2315 train_time:21637ms step_avg:60.10ms
step:361/2315 train_time:21697ms step_avg:60.10ms
step:362/2315 train_time:21756ms step_avg:60.10ms
step:363/2315 train_time:21816ms step_avg:60.10ms
step:364/2315 train_time:21876ms step_avg:60.10ms
step:365/2315 train_time:21935ms step_avg:60.10ms
step:366/2315 train_time:21995ms step_avg:60.09ms
step:367/2315 train_time:22055ms step_avg:60.09ms
step:368/2315 train_time:22115ms step_avg:60.09ms
step:369/2315 train_time:22175ms step_avg:60.09ms
step:370/2315 train_time:22235ms step_avg:60.09ms
step:371/2315 train_time:22294ms step_avg:60.09ms
step:372/2315 train_time:22355ms step_avg:60.09ms
step:373/2315 train_time:22415ms step_avg:60.09ms
step:374/2315 train_time:22475ms step_avg:60.09ms
step:375/2315 train_time:22535ms step_avg:60.09ms
step:376/2315 train_time:22596ms step_avg:60.09ms
step:377/2315 train_time:22656ms step_avg:60.09ms
step:378/2315 train_time:22716ms step_avg:60.09ms
step:379/2315 train_time:22776ms step_avg:60.10ms
step:380/2315 train_time:22835ms step_avg:60.09ms
step:381/2315 train_time:22896ms step_avg:60.09ms
step:382/2315 train_time:22955ms step_avg:60.09ms
step:383/2315 train_time:23016ms step_avg:60.09ms
step:384/2315 train_time:23076ms step_avg:60.09ms
step:385/2315 train_time:23136ms step_avg:60.09ms
step:386/2315 train_time:23196ms step_avg:60.09ms
step:387/2315 train_time:23256ms step_avg:60.09ms
step:388/2315 train_time:23316ms step_avg:60.09ms
step:389/2315 train_time:23376ms step_avg:60.09ms
step:390/2315 train_time:23436ms step_avg:60.09ms
step:391/2315 train_time:23496ms step_avg:60.09ms
step:392/2315 train_time:23557ms step_avg:60.09ms
step:393/2315 train_time:23617ms step_avg:60.09ms
step:394/2315 train_time:23676ms step_avg:60.09ms
step:395/2315 train_time:23737ms step_avg:60.09ms
step:396/2315 train_time:23797ms step_avg:60.09ms
step:397/2315 train_time:23857ms step_avg:60.09ms
step:398/2315 train_time:23917ms step_avg:60.09ms
step:399/2315 train_time:23977ms step_avg:60.09ms
step:400/2315 train_time:24037ms step_avg:60.09ms
step:401/2315 train_time:24097ms step_avg:60.09ms
step:402/2315 train_time:24158ms step_avg:60.09ms
step:403/2315 train_time:24217ms step_avg:60.09ms
step:404/2315 train_time:24277ms step_avg:60.09ms
step:405/2315 train_time:24337ms step_avg:60.09ms
step:406/2315 train_time:24397ms step_avg:60.09ms
step:407/2315 train_time:24457ms step_avg:60.09ms
step:408/2315 train_time:24516ms step_avg:60.09ms
step:409/2315 train_time:24576ms step_avg:60.09ms
step:410/2315 train_time:24636ms step_avg:60.09ms
step:411/2315 train_time:24697ms step_avg:60.09ms
step:412/2315 train_time:24757ms step_avg:60.09ms
step:413/2315 train_time:24816ms step_avg:60.09ms
step:414/2315 train_time:24876ms step_avg:60.09ms
step:415/2315 train_time:24936ms step_avg:60.09ms
step:416/2315 train_time:24996ms step_avg:60.09ms
step:417/2315 train_time:25056ms step_avg:60.09ms
step:418/2315 train_time:25116ms step_avg:60.09ms
step:419/2315 train_time:25176ms step_avg:60.09ms
step:420/2315 train_time:25236ms step_avg:60.08ms
step:421/2315 train_time:25296ms step_avg:60.08ms
step:422/2315 train_time:25357ms step_avg:60.09ms
step:423/2315 train_time:25417ms step_avg:60.09ms
step:424/2315 train_time:25476ms step_avg:60.09ms
step:425/2315 train_time:25536ms step_avg:60.09ms
step:426/2315 train_time:25596ms step_avg:60.08ms
step:427/2315 train_time:25656ms step_avg:60.08ms
step:428/2315 train_time:25716ms step_avg:60.08ms
step:429/2315 train_time:25775ms step_avg:60.08ms
step:430/2315 train_time:25835ms step_avg:60.08ms
step:431/2315 train_time:25895ms step_avg:60.08ms
step:432/2315 train_time:25955ms step_avg:60.08ms
step:433/2315 train_time:26015ms step_avg:60.08ms
step:434/2315 train_time:26075ms step_avg:60.08ms
step:435/2315 train_time:26136ms step_avg:60.08ms
step:436/2315 train_time:26195ms step_avg:60.08ms
step:437/2315 train_time:26255ms step_avg:60.08ms
step:438/2315 train_time:26315ms step_avg:60.08ms
step:439/2315 train_time:26375ms step_avg:60.08ms
step:440/2315 train_time:26435ms step_avg:60.08ms
step:441/2315 train_time:26496ms step_avg:60.08ms
step:442/2315 train_time:26556ms step_avg:60.08ms
step:443/2315 train_time:26616ms step_avg:60.08ms
step:444/2315 train_time:26676ms step_avg:60.08ms
step:445/2315 train_time:26736ms step_avg:60.08ms
step:446/2315 train_time:26796ms step_avg:60.08ms
step:447/2315 train_time:26856ms step_avg:60.08ms
step:448/2315 train_time:26915ms step_avg:60.08ms
step:449/2315 train_time:26975ms step_avg:60.08ms
step:450/2315 train_time:27035ms step_avg:60.08ms
step:451/2315 train_time:27095ms step_avg:60.08ms
step:452/2315 train_time:27155ms step_avg:60.08ms
step:453/2315 train_time:27215ms step_avg:60.08ms
step:454/2315 train_time:27275ms step_avg:60.08ms
step:455/2315 train_time:27335ms step_avg:60.08ms
step:456/2315 train_time:27395ms step_avg:60.08ms
step:457/2315 train_time:27455ms step_avg:60.08ms
step:458/2315 train_time:27515ms step_avg:60.08ms
step:459/2315 train_time:27575ms step_avg:60.08ms
step:460/2315 train_time:27635ms step_avg:60.08ms
step:461/2315 train_time:27695ms step_avg:60.08ms
step:462/2315 train_time:27755ms step_avg:60.08ms
step:463/2315 train_time:27815ms step_avg:60.08ms
step:464/2315 train_time:27875ms step_avg:60.08ms
step:465/2315 train_time:27935ms step_avg:60.08ms
step:466/2315 train_time:27995ms step_avg:60.08ms
step:467/2315 train_time:28055ms step_avg:60.08ms
step:468/2315 train_time:28115ms step_avg:60.07ms
step:469/2315 train_time:28175ms step_avg:60.07ms
step:470/2315 train_time:28235ms step_avg:60.08ms
step:471/2315 train_time:28295ms step_avg:60.08ms
step:472/2315 train_time:28355ms step_avg:60.07ms
step:473/2315 train_time:28415ms step_avg:60.07ms
step:474/2315 train_time:28476ms step_avg:60.08ms
step:475/2315 train_time:28536ms step_avg:60.08ms
step:476/2315 train_time:28596ms step_avg:60.08ms
step:477/2315 train_time:28656ms step_avg:60.08ms
step:478/2315 train_time:28715ms step_avg:60.07ms
step:479/2315 train_time:28776ms step_avg:60.07ms
step:480/2315 train_time:28836ms step_avg:60.07ms
step:481/2315 train_time:28896ms step_avg:60.07ms
step:482/2315 train_time:28957ms step_avg:60.08ms
step:483/2315 train_time:29017ms step_avg:60.08ms
step:484/2315 train_time:29076ms step_avg:60.07ms
step:485/2315 train_time:29136ms step_avg:60.07ms
step:486/2315 train_time:29196ms step_avg:60.07ms
step:487/2315 train_time:29256ms step_avg:60.07ms
step:488/2315 train_time:29316ms step_avg:60.07ms
step:489/2315 train_time:29376ms step_avg:60.07ms
step:490/2315 train_time:29436ms step_avg:60.07ms
step:491/2315 train_time:29496ms step_avg:60.07ms
step:492/2315 train_time:29556ms step_avg:60.07ms
step:493/2315 train_time:29617ms step_avg:60.07ms
step:494/2315 train_time:29676ms step_avg:60.07ms
step:495/2315 train_time:29736ms step_avg:60.07ms
step:496/2315 train_time:29796ms step_avg:60.07ms
step:497/2315 train_time:29856ms step_avg:60.07ms
step:498/2315 train_time:29916ms step_avg:60.07ms
step:499/2315 train_time:29977ms step_avg:60.07ms
step:500/2315 train_time:30036ms step_avg:60.07ms
step:500/2315 val_loss:3.8202 train_time:30098ms step_avg:60.20ms
step:501/2315 train_time:30117ms step_avg:60.11ms
step:502/2315 train_time:30157ms step_avg:60.07ms
step:503/2315 train_time:30220ms step_avg:60.08ms
step:504/2315 train_time:30282ms step_avg:60.08ms
step:505/2315 train_time:30343ms step_avg:60.09ms
step:506/2315 train_time:30403ms step_avg:60.08ms
step:507/2315 train_time:30462ms step_avg:60.08ms
step:508/2315 train_time:30521ms step_avg:60.08ms
step:509/2315 train_time:30581ms step_avg:60.08ms
step:510/2315 train_time:30640ms step_avg:60.08ms
step:511/2315 train_time:30699ms step_avg:60.08ms
step:512/2315 train_time:30758ms step_avg:60.08ms
step:513/2315 train_time:30818ms step_avg:60.07ms
step:514/2315 train_time:30877ms step_avg:60.07ms
step:515/2315 train_time:30936ms step_avg:60.07ms
step:516/2315 train_time:30996ms step_avg:60.07ms
step:517/2315 train_time:31056ms step_avg:60.07ms
step:518/2315 train_time:31117ms step_avg:60.07ms
step:519/2315 train_time:31178ms step_avg:60.07ms
step:520/2315 train_time:31239ms step_avg:60.08ms
step:521/2315 train_time:31301ms step_avg:60.08ms
step:522/2315 train_time:31361ms step_avg:60.08ms
step:523/2315 train_time:31421ms step_avg:60.08ms
step:524/2315 train_time:31481ms step_avg:60.08ms
step:525/2315 train_time:31541ms step_avg:60.08ms
step:526/2315 train_time:31600ms step_avg:60.08ms
step:527/2315 train_time:31660ms step_avg:60.08ms
step:528/2315 train_time:31719ms step_avg:60.07ms
step:529/2315 train_time:31779ms step_avg:60.07ms
step:530/2315 train_time:31838ms step_avg:60.07ms
step:531/2315 train_time:31898ms step_avg:60.07ms
step:532/2315 train_time:31958ms step_avg:60.07ms
step:533/2315 train_time:32018ms step_avg:60.07ms
step:534/2315 train_time:32078ms step_avg:60.07ms
step:535/2315 train_time:32138ms step_avg:60.07ms
step:536/2315 train_time:32198ms step_avg:60.07ms
step:537/2315 train_time:32259ms step_avg:60.07ms
step:538/2315 train_time:32319ms step_avg:60.07ms
step:539/2315 train_time:32380ms step_avg:60.07ms
step:540/2315 train_time:32440ms step_avg:60.07ms
step:541/2315 train_time:32500ms step_avg:60.07ms
step:542/2315 train_time:32560ms step_avg:60.07ms
step:543/2315 train_time:32620ms step_avg:60.07ms
step:544/2315 train_time:32680ms step_avg:60.07ms
step:545/2315 train_time:32740ms step_avg:60.07ms
step:546/2315 train_time:32799ms step_avg:60.07ms
step:547/2315 train_time:32859ms step_avg:60.07ms
step:548/2315 train_time:32918ms step_avg:60.07ms
step:549/2315 train_time:32978ms step_avg:60.07ms
step:550/2315 train_time:33038ms step_avg:60.07ms
step:551/2315 train_time:33098ms step_avg:60.07ms
step:552/2315 train_time:33158ms step_avg:60.07ms
step:553/2315 train_time:33219ms step_avg:60.07ms
step:554/2315 train_time:33279ms step_avg:60.07ms
step:555/2315 train_time:33339ms step_avg:60.07ms
step:556/2315 train_time:33399ms step_avg:60.07ms
step:557/2315 train_time:33460ms step_avg:60.07ms
step:558/2315 train_time:33520ms step_avg:60.07ms
step:559/2315 train_time:33580ms step_avg:60.07ms
step:560/2315 train_time:33639ms step_avg:60.07ms
step:561/2315 train_time:33699ms step_avg:60.07ms
step:562/2315 train_time:33759ms step_avg:60.07ms
step:563/2315 train_time:33818ms step_avg:60.07ms
step:564/2315 train_time:33878ms step_avg:60.07ms
step:565/2315 train_time:33938ms step_avg:60.07ms
step:566/2315 train_time:33997ms step_avg:60.07ms
step:567/2315 train_time:34057ms step_avg:60.07ms
step:568/2315 train_time:34117ms step_avg:60.07ms
step:569/2315 train_time:34178ms step_avg:60.07ms
step:570/2315 train_time:34238ms step_avg:60.07ms
step:571/2315 train_time:34298ms step_avg:60.07ms
step:572/2315 train_time:34358ms step_avg:60.07ms
step:573/2315 train_time:34418ms step_avg:60.07ms
step:574/2315 train_time:34479ms step_avg:60.07ms
step:575/2315 train_time:34539ms step_avg:60.07ms
step:576/2315 train_time:34599ms step_avg:60.07ms
step:577/2315 train_time:34659ms step_avg:60.07ms
step:578/2315 train_time:34719ms step_avg:60.07ms
step:579/2315 train_time:34779ms step_avg:60.07ms
step:580/2315 train_time:34839ms step_avg:60.07ms
step:581/2315 train_time:34899ms step_avg:60.07ms
step:582/2315 train_time:34958ms step_avg:60.07ms
step:583/2315 train_time:35018ms step_avg:60.07ms
step:584/2315 train_time:35078ms step_avg:60.06ms
step:585/2315 train_time:35138ms step_avg:60.07ms
step:586/2315 train_time:35199ms step_avg:60.07ms
step:587/2315 train_time:35259ms step_avg:60.07ms
step:588/2315 train_time:35319ms step_avg:60.07ms
step:589/2315 train_time:35379ms step_avg:60.07ms
step:590/2315 train_time:35439ms step_avg:60.07ms
step:591/2315 train_time:35499ms step_avg:60.07ms
step:592/2315 train_time:35558ms step_avg:60.06ms
step:593/2315 train_time:35619ms step_avg:60.07ms
step:594/2315 train_time:35679ms step_avg:60.06ms
step:595/2315 train_time:35738ms step_avg:60.06ms
step:596/2315 train_time:35798ms step_avg:60.06ms
step:597/2315 train_time:35858ms step_avg:60.06ms
step:598/2315 train_time:35918ms step_avg:60.06ms
step:599/2315 train_time:35978ms step_avg:60.06ms
step:600/2315 train_time:36038ms step_avg:60.06ms
step:601/2315 train_time:36098ms step_avg:60.06ms
step:602/2315 train_time:36158ms step_avg:60.06ms
step:603/2315 train_time:36218ms step_avg:60.06ms
step:604/2315 train_time:36279ms step_avg:60.06ms
step:605/2315 train_time:36339ms step_avg:60.06ms
step:606/2315 train_time:36399ms step_avg:60.06ms
step:607/2315 train_time:36459ms step_avg:60.06ms
step:608/2315 train_time:36520ms step_avg:60.07ms
step:609/2315 train_time:36580ms step_avg:60.07ms
step:610/2315 train_time:36640ms step_avg:60.07ms
step:611/2315 train_time:36701ms step_avg:60.07ms
step:612/2315 train_time:36760ms step_avg:60.07ms
step:613/2315 train_time:36820ms step_avg:60.06ms
step:614/2315 train_time:36880ms step_avg:60.06ms
step:615/2315 train_time:36940ms step_avg:60.07ms
step:616/2315 train_time:36999ms step_avg:60.06ms
step:617/2315 train_time:37060ms step_avg:60.06ms
step:618/2315 train_time:37119ms step_avg:60.06ms
step:619/2315 train_time:37179ms step_avg:60.06ms
step:620/2315 train_time:37239ms step_avg:60.06ms
step:621/2315 train_time:37299ms step_avg:60.06ms
step:622/2315 train_time:37358ms step_avg:60.06ms
step:623/2315 train_time:37419ms step_avg:60.06ms
step:624/2315 train_time:37479ms step_avg:60.06ms
step:625/2315 train_time:37539ms step_avg:60.06ms
step:626/2315 train_time:37599ms step_avg:60.06ms
step:627/2315 train_time:37659ms step_avg:60.06ms
step:628/2315 train_time:37719ms step_avg:60.06ms
step:629/2315 train_time:37779ms step_avg:60.06ms
step:630/2315 train_time:37839ms step_avg:60.06ms
step:631/2315 train_time:37899ms step_avg:60.06ms
step:632/2315 train_time:37959ms step_avg:60.06ms
step:633/2315 train_time:38019ms step_avg:60.06ms
step:634/2315 train_time:38079ms step_avg:60.06ms
step:635/2315 train_time:38139ms step_avg:60.06ms
step:636/2315 train_time:38199ms step_avg:60.06ms
step:637/2315 train_time:38259ms step_avg:60.06ms
step:638/2315 train_time:38319ms step_avg:60.06ms
step:639/2315 train_time:38379ms step_avg:60.06ms
step:640/2315 train_time:38439ms step_avg:60.06ms
step:641/2315 train_time:38500ms step_avg:60.06ms
step:642/2315 train_time:38560ms step_avg:60.06ms
step:643/2315 train_time:38620ms step_avg:60.06ms
step:644/2315 train_time:38680ms step_avg:60.06ms
step:645/2315 train_time:38740ms step_avg:60.06ms
step:646/2315 train_time:38800ms step_avg:60.06ms
step:647/2315 train_time:38860ms step_avg:60.06ms
step:648/2315 train_time:38920ms step_avg:60.06ms
step:649/2315 train_time:38980ms step_avg:60.06ms
step:650/2315 train_time:39039ms step_avg:60.06ms
step:651/2315 train_time:39099ms step_avg:60.06ms
step:652/2315 train_time:39159ms step_avg:60.06ms
step:653/2315 train_time:39219ms step_avg:60.06ms
step:654/2315 train_time:39279ms step_avg:60.06ms
step:655/2315 train_time:39338ms step_avg:60.06ms
step:656/2315 train_time:39398ms step_avg:60.06ms
step:657/2315 train_time:39459ms step_avg:60.06ms
step:658/2315 train_time:39518ms step_avg:60.06ms
step:659/2315 train_time:39580ms step_avg:60.06ms
step:660/2315 train_time:39639ms step_avg:60.06ms
step:661/2315 train_time:39700ms step_avg:60.06ms
step:662/2315 train_time:39759ms step_avg:60.06ms
step:663/2315 train_time:39819ms step_avg:60.06ms
step:664/2315 train_time:39879ms step_avg:60.06ms
step:665/2315 train_time:39939ms step_avg:60.06ms
step:666/2315 train_time:39999ms step_avg:60.06ms
step:667/2315 train_time:40059ms step_avg:60.06ms
step:668/2315 train_time:40119ms step_avg:60.06ms
step:669/2315 train_time:40179ms step_avg:60.06ms
step:670/2315 train_time:40238ms step_avg:60.06ms
step:671/2315 train_time:40299ms step_avg:60.06ms
step:672/2315 train_time:40359ms step_avg:60.06ms
step:673/2315 train_time:40419ms step_avg:60.06ms
step:674/2315 train_time:40478ms step_avg:60.06ms
step:675/2315 train_time:40539ms step_avg:60.06ms
step:676/2315 train_time:40599ms step_avg:60.06ms
step:677/2315 train_time:40659ms step_avg:60.06ms
step:678/2315 train_time:40719ms step_avg:60.06ms
step:679/2315 train_time:40779ms step_avg:60.06ms
step:680/2315 train_time:40839ms step_avg:60.06ms
step:681/2315 train_time:40898ms step_avg:60.06ms
step:682/2315 train_time:40959ms step_avg:60.06ms
step:683/2315 train_time:41019ms step_avg:60.06ms
step:684/2315 train_time:41080ms step_avg:60.06ms
step:685/2315 train_time:41139ms step_avg:60.06ms
step:686/2315 train_time:41199ms step_avg:60.06ms
step:687/2315 train_time:41259ms step_avg:60.06ms
step:688/2315 train_time:41319ms step_avg:60.06ms
step:689/2315 train_time:41379ms step_avg:60.06ms
step:690/2315 train_time:41438ms step_avg:60.06ms
step:691/2315 train_time:41499ms step_avg:60.06ms
step:692/2315 train_time:41558ms step_avg:60.06ms
step:693/2315 train_time:41620ms step_avg:60.06ms
step:694/2315 train_time:41680ms step_avg:60.06ms
step:695/2315 train_time:41740ms step_avg:60.06ms
step:696/2315 train_time:41799ms step_avg:60.06ms
step:697/2315 train_time:41860ms step_avg:60.06ms
step:698/2315 train_time:41920ms step_avg:60.06ms
step:699/2315 train_time:41979ms step_avg:60.06ms
step:700/2315 train_time:42040ms step_avg:60.06ms
step:701/2315 train_time:42100ms step_avg:60.06ms
step:702/2315 train_time:42160ms step_avg:60.06ms
step:703/2315 train_time:42220ms step_avg:60.06ms
step:704/2315 train_time:42280ms step_avg:60.06ms
step:705/2315 train_time:42339ms step_avg:60.06ms
step:706/2315 train_time:42399ms step_avg:60.06ms
step:707/2315 train_time:42459ms step_avg:60.06ms
step:708/2315 train_time:42519ms step_avg:60.06ms
step:709/2315 train_time:42579ms step_avg:60.06ms
step:710/2315 train_time:42639ms step_avg:60.06ms
step:711/2315 train_time:42700ms step_avg:60.06ms
step:712/2315 train_time:42760ms step_avg:60.06ms
step:713/2315 train_time:42820ms step_avg:60.06ms
step:714/2315 train_time:42880ms step_avg:60.06ms
step:715/2315 train_time:42940ms step_avg:60.06ms
step:716/2315 train_time:43000ms step_avg:60.06ms
step:717/2315 train_time:43060ms step_avg:60.06ms
step:718/2315 train_time:43120ms step_avg:60.06ms
step:719/2315 train_time:43180ms step_avg:60.06ms
step:720/2315 train_time:43239ms step_avg:60.05ms
step:721/2315 train_time:43299ms step_avg:60.05ms
step:722/2315 train_time:43359ms step_avg:60.05ms
step:723/2315 train_time:43419ms step_avg:60.05ms
step:724/2315 train_time:43480ms step_avg:60.05ms
step:725/2315 train_time:43540ms step_avg:60.05ms
step:726/2315 train_time:43599ms step_avg:60.05ms
step:727/2315 train_time:43659ms step_avg:60.05ms
step:728/2315 train_time:43719ms step_avg:60.05ms
step:729/2315 train_time:43780ms step_avg:60.05ms
step:730/2315 train_time:43840ms step_avg:60.05ms
step:731/2315 train_time:43899ms step_avg:60.05ms
step:732/2315 train_time:43959ms step_avg:60.05ms
step:733/2315 train_time:44019ms step_avg:60.05ms
step:734/2315 train_time:44079ms step_avg:60.05ms
step:735/2315 train_time:44139ms step_avg:60.05ms
step:736/2315 train_time:44198ms step_avg:60.05ms
step:737/2315 train_time:44259ms step_avg:60.05ms
step:738/2315 train_time:44318ms step_avg:60.05ms
step:739/2315 train_time:44378ms step_avg:60.05ms
step:740/2315 train_time:44438ms step_avg:60.05ms
step:741/2315 train_time:44499ms step_avg:60.05ms
step:742/2315 train_time:44559ms step_avg:60.05ms
step:743/2315 train_time:44619ms step_avg:60.05ms
step:744/2315 train_time:44679ms step_avg:60.05ms
step:745/2315 train_time:44740ms step_avg:60.05ms
step:746/2315 train_time:44799ms step_avg:60.05ms
step:747/2315 train_time:44859ms step_avg:60.05ms
step:748/2315 train_time:44919ms step_avg:60.05ms
step:749/2315 train_time:44980ms step_avg:60.05ms
step:750/2315 train_time:45039ms step_avg:60.05ms
step:750/2315 val_loss:3.6883 train_time:45101ms step_avg:60.14ms
step:751/2315 train_time:45121ms step_avg:60.08ms
step:752/2315 train_time:45162ms step_avg:60.06ms
step:753/2315 train_time:45224ms step_avg:60.06ms
step:754/2315 train_time:45286ms step_avg:60.06ms
step:755/2315 train_time:45346ms step_avg:60.06ms
step:756/2315 train_time:45406ms step_avg:60.06ms
step:757/2315 train_time:45465ms step_avg:60.06ms
step:758/2315 train_time:45524ms step_avg:60.06ms
step:759/2315 train_time:45583ms step_avg:60.06ms
step:760/2315 train_time:45643ms step_avg:60.06ms
step:761/2315 train_time:45703ms step_avg:60.06ms
step:762/2315 train_time:45763ms step_avg:60.06ms
step:763/2315 train_time:45823ms step_avg:60.06ms
step:764/2315 train_time:45882ms step_avg:60.06ms
step:765/2315 train_time:45943ms step_avg:60.06ms
step:766/2315 train_time:46003ms step_avg:60.06ms
step:767/2315 train_time:46065ms step_avg:60.06ms
step:768/2315 train_time:46126ms step_avg:60.06ms
step:769/2315 train_time:46188ms step_avg:60.06ms
step:770/2315 train_time:46250ms step_avg:60.06ms
step:771/2315 train_time:46310ms step_avg:60.07ms
step:772/2315 train_time:46371ms step_avg:60.07ms
step:773/2315 train_time:46433ms step_avg:60.07ms
step:774/2315 train_time:46493ms step_avg:60.07ms
step:775/2315 train_time:46554ms step_avg:60.07ms
step:776/2315 train_time:46615ms step_avg:60.07ms
step:777/2315 train_time:46675ms step_avg:60.07ms
step:778/2315 train_time:46736ms step_avg:60.07ms
step:779/2315 train_time:46797ms step_avg:60.07ms
step:780/2315 train_time:46858ms step_avg:60.07ms
step:781/2315 train_time:46919ms step_avg:60.08ms
step:782/2315 train_time:46979ms step_avg:60.08ms
step:783/2315 train_time:47040ms step_avg:60.08ms
step:784/2315 train_time:47101ms step_avg:60.08ms
step:785/2315 train_time:47162ms step_avg:60.08ms
step:786/2315 train_time:47223ms step_avg:60.08ms
step:787/2315 train_time:47284ms step_avg:60.08ms
step:788/2315 train_time:47345ms step_avg:60.08ms
step:789/2315 train_time:47405ms step_avg:60.08ms
step:790/2315 train_time:47466ms step_avg:60.08ms
step:791/2315 train_time:47526ms step_avg:60.08ms
step:792/2315 train_time:47587ms step_avg:60.08ms
step:793/2315 train_time:47648ms step_avg:60.09ms
step:794/2315 train_time:47708ms step_avg:60.09ms
step:795/2315 train_time:47769ms step_avg:60.09ms
step:796/2315 train_time:47828ms step_avg:60.09ms
step:797/2315 train_time:47889ms step_avg:60.09ms
step:798/2315 train_time:47950ms step_avg:60.09ms
step:799/2315 train_time:48012ms step_avg:60.09ms
step:800/2315 train_time:48073ms step_avg:60.09ms
step:801/2315 train_time:48134ms step_avg:60.09ms
step:802/2315 train_time:48196ms step_avg:60.09ms
step:803/2315 train_time:48258ms step_avg:60.10ms
step:804/2315 train_time:48318ms step_avg:60.10ms
step:805/2315 train_time:48380ms step_avg:60.10ms
step:806/2315 train_time:48440ms step_avg:60.10ms
step:807/2315 train_time:48501ms step_avg:60.10ms
step:808/2315 train_time:48562ms step_avg:60.10ms
step:809/2315 train_time:48623ms step_avg:60.10ms
step:810/2315 train_time:48684ms step_avg:60.10ms
step:811/2315 train_time:48744ms step_avg:60.10ms
step:812/2315 train_time:48805ms step_avg:60.10ms
step:813/2315 train_time:48866ms step_avg:60.11ms
step:814/2315 train_time:48926ms step_avg:60.11ms
step:815/2315 train_time:48987ms step_avg:60.11ms
step:816/2315 train_time:49047ms step_avg:60.11ms
step:817/2315 train_time:49108ms step_avg:60.11ms
step:818/2315 train_time:49168ms step_avg:60.11ms
step:819/2315 train_time:49229ms step_avg:60.11ms
step:820/2315 train_time:49290ms step_avg:60.11ms
step:821/2315 train_time:49351ms step_avg:60.11ms
step:822/2315 train_time:49412ms step_avg:60.11ms
step:823/2315 train_time:49473ms step_avg:60.11ms
step:824/2315 train_time:49534ms step_avg:60.11ms
step:825/2315 train_time:49595ms step_avg:60.12ms
step:826/2315 train_time:49656ms step_avg:60.12ms
step:827/2315 train_time:49717ms step_avg:60.12ms
step:828/2315 train_time:49778ms step_avg:60.12ms
step:829/2315 train_time:49840ms step_avg:60.12ms
step:830/2315 train_time:49900ms step_avg:60.12ms
step:831/2315 train_time:49962ms step_avg:60.12ms
step:832/2315 train_time:50022ms step_avg:60.12ms
step:833/2315 train_time:50083ms step_avg:60.12ms
step:834/2315 train_time:50143ms step_avg:60.12ms
step:835/2315 train_time:50204ms step_avg:60.12ms
step:836/2315 train_time:50264ms step_avg:60.12ms
step:837/2315 train_time:50325ms step_avg:60.13ms
step:838/2315 train_time:50386ms step_avg:60.13ms
step:839/2315 train_time:50446ms step_avg:60.13ms
step:840/2315 train_time:50507ms step_avg:60.13ms
step:841/2315 train_time:50568ms step_avg:60.13ms
step:842/2315 train_time:50628ms step_avg:60.13ms
step:843/2315 train_time:50689ms step_avg:60.13ms
step:844/2315 train_time:50749ms step_avg:60.13ms
step:845/2315 train_time:50810ms step_avg:60.13ms
step:846/2315 train_time:50872ms step_avg:60.13ms
step:847/2315 train_time:50933ms step_avg:60.13ms
step:848/2315 train_time:50994ms step_avg:60.13ms
step:849/2315 train_time:51056ms step_avg:60.14ms
step:850/2315 train_time:51117ms step_avg:60.14ms
step:851/2315 train_time:51178ms step_avg:60.14ms
step:852/2315 train_time:51239ms step_avg:60.14ms
step:853/2315 train_time:51300ms step_avg:60.14ms
step:854/2315 train_time:51360ms step_avg:60.14ms
step:855/2315 train_time:51421ms step_avg:60.14ms
step:856/2315 train_time:51481ms step_avg:60.14ms
step:857/2315 train_time:51542ms step_avg:60.14ms
step:858/2315 train_time:51603ms step_avg:60.14ms
step:859/2315 train_time:51664ms step_avg:60.14ms
step:860/2315 train_time:51724ms step_avg:60.14ms
step:861/2315 train_time:51785ms step_avg:60.14ms
step:862/2315 train_time:51845ms step_avg:60.14ms
step:863/2315 train_time:51906ms step_avg:60.15ms
step:864/2315 train_time:51968ms step_avg:60.15ms
step:865/2315 train_time:52028ms step_avg:60.15ms
step:866/2315 train_time:52088ms step_avg:60.15ms
step:867/2315 train_time:52149ms step_avg:60.15ms
step:868/2315 train_time:52209ms step_avg:60.15ms
step:869/2315 train_time:52270ms step_avg:60.15ms
step:870/2315 train_time:52331ms step_avg:60.15ms
step:871/2315 train_time:52392ms step_avg:60.15ms
step:872/2315 train_time:52452ms step_avg:60.15ms
step:873/2315 train_time:52514ms step_avg:60.15ms
step:874/2315 train_time:52575ms step_avg:60.15ms
step:875/2315 train_time:52637ms step_avg:60.16ms
step:876/2315 train_time:52698ms step_avg:60.16ms
step:877/2315 train_time:52759ms step_avg:60.16ms
step:878/2315 train_time:52820ms step_avg:60.16ms
step:879/2315 train_time:52881ms step_avg:60.16ms
step:880/2315 train_time:52941ms step_avg:60.16ms
step:881/2315 train_time:53002ms step_avg:60.16ms
step:882/2315 train_time:53063ms step_avg:60.16ms
step:883/2315 train_time:53124ms step_avg:60.16ms
step:884/2315 train_time:53184ms step_avg:60.16ms
step:885/2315 train_time:53245ms step_avg:60.16ms
step:886/2315 train_time:53305ms step_avg:60.16ms
step:887/2315 train_time:53366ms step_avg:60.16ms
step:888/2315 train_time:53426ms step_avg:60.16ms
step:889/2315 train_time:53487ms step_avg:60.16ms
step:890/2315 train_time:53547ms step_avg:60.17ms
step:891/2315 train_time:53608ms step_avg:60.17ms
step:892/2315 train_time:53669ms step_avg:60.17ms
step:893/2315 train_time:53729ms step_avg:60.17ms
step:894/2315 train_time:53791ms step_avg:60.17ms
step:895/2315 train_time:53852ms step_avg:60.17ms
step:896/2315 train_time:53913ms step_avg:60.17ms
step:897/2315 train_time:53974ms step_avg:60.17ms
step:898/2315 train_time:54035ms step_avg:60.17ms
step:899/2315 train_time:54096ms step_avg:60.17ms
step:900/2315 train_time:54156ms step_avg:60.17ms
step:901/2315 train_time:54217ms step_avg:60.17ms
step:902/2315 train_time:54278ms step_avg:60.18ms
step:903/2315 train_time:54339ms step_avg:60.18ms
step:904/2315 train_time:54400ms step_avg:60.18ms
step:905/2315 train_time:54461ms step_avg:60.18ms
step:906/2315 train_time:54521ms step_avg:60.18ms
step:907/2315 train_time:54583ms step_avg:60.18ms
step:908/2315 train_time:54644ms step_avg:60.18ms
step:909/2315 train_time:54704ms step_avg:60.18ms
step:910/2315 train_time:54765ms step_avg:60.18ms
step:911/2315 train_time:54825ms step_avg:60.18ms
step:912/2315 train_time:54886ms step_avg:60.18ms
step:913/2315 train_time:54946ms step_avg:60.18ms
step:914/2315 train_time:55007ms step_avg:60.18ms
step:915/2315 train_time:55067ms step_avg:60.18ms
step:916/2315 train_time:55128ms step_avg:60.18ms
step:917/2315 train_time:55189ms step_avg:60.18ms
step:918/2315 train_time:55249ms step_avg:60.18ms
step:919/2315 train_time:55311ms step_avg:60.19ms
step:920/2315 train_time:55372ms step_avg:60.19ms
step:921/2315 train_time:55433ms step_avg:60.19ms
step:922/2315 train_time:55494ms step_avg:60.19ms
step:923/2315 train_time:55556ms step_avg:60.19ms
step:924/2315 train_time:55617ms step_avg:60.19ms
step:925/2315 train_time:55678ms step_avg:60.19ms
step:926/2315 train_time:55739ms step_avg:60.19ms
step:927/2315 train_time:55800ms step_avg:60.19ms
step:928/2315 train_time:55861ms step_avg:60.19ms
step:929/2315 train_time:55922ms step_avg:60.20ms
step:930/2315 train_time:55983ms step_avg:60.20ms
step:931/2315 train_time:56043ms step_avg:60.20ms
step:932/2315 train_time:56104ms step_avg:60.20ms
step:933/2315 train_time:56164ms step_avg:60.20ms
step:934/2315 train_time:56225ms step_avg:60.20ms
step:935/2315 train_time:56286ms step_avg:60.20ms
step:936/2315 train_time:56346ms step_avg:60.20ms
step:937/2315 train_time:56407ms step_avg:60.20ms
step:938/2315 train_time:56467ms step_avg:60.20ms
step:939/2315 train_time:56528ms step_avg:60.20ms
step:940/2315 train_time:56588ms step_avg:60.20ms
step:941/2315 train_time:56649ms step_avg:60.20ms
step:942/2315 train_time:56709ms step_avg:60.20ms
step:943/2315 train_time:56771ms step_avg:60.20ms
step:944/2315 train_time:56832ms step_avg:60.20ms
step:945/2315 train_time:56894ms step_avg:60.21ms
step:946/2315 train_time:56955ms step_avg:60.21ms
step:947/2315 train_time:57016ms step_avg:60.21ms
step:948/2315 train_time:57076ms step_avg:60.21ms
step:949/2315 train_time:57138ms step_avg:60.21ms
step:950/2315 train_time:57198ms step_avg:60.21ms
step:951/2315 train_time:57260ms step_avg:60.21ms
step:952/2315 train_time:57320ms step_avg:60.21ms
step:953/2315 train_time:57382ms step_avg:60.21ms
step:954/2315 train_time:57443ms step_avg:60.21ms
step:955/2315 train_time:57503ms step_avg:60.21ms
step:956/2315 train_time:57563ms step_avg:60.21ms
step:957/2315 train_time:57625ms step_avg:60.21ms
step:958/2315 train_time:57685ms step_avg:60.21ms
step:959/2315 train_time:57746ms step_avg:60.21ms
step:960/2315 train_time:57806ms step_avg:60.21ms
step:961/2315 train_time:57866ms step_avg:60.21ms
step:962/2315 train_time:57927ms step_avg:60.21ms
step:963/2315 train_time:57988ms step_avg:60.22ms
step:964/2315 train_time:58048ms step_avg:60.22ms
step:965/2315 train_time:58109ms step_avg:60.22ms
step:966/2315 train_time:58170ms step_avg:60.22ms
step:967/2315 train_time:58231ms step_avg:60.22ms
step:968/2315 train_time:58292ms step_avg:60.22ms
step:969/2315 train_time:58353ms step_avg:60.22ms
step:970/2315 train_time:58414ms step_avg:60.22ms
step:971/2315 train_time:58475ms step_avg:60.22ms
step:972/2315 train_time:58536ms step_avg:60.22ms
step:973/2315 train_time:58597ms step_avg:60.22ms
step:974/2315 train_time:58658ms step_avg:60.22ms
step:975/2315 train_time:58719ms step_avg:60.22ms
step:976/2315 train_time:58780ms step_avg:60.23ms
step:977/2315 train_time:58841ms step_avg:60.23ms
step:978/2315 train_time:58902ms step_avg:60.23ms
step:979/2315 train_time:58963ms step_avg:60.23ms
step:980/2315 train_time:59023ms step_avg:60.23ms
step:981/2315 train_time:59084ms step_avg:60.23ms
step:982/2315 train_time:59144ms step_avg:60.23ms
step:983/2315 train_time:59205ms step_avg:60.23ms
step:984/2315 train_time:59266ms step_avg:60.23ms
step:985/2315 train_time:59327ms step_avg:60.23ms
step:986/2315 train_time:59387ms step_avg:60.23ms
step:987/2315 train_time:59449ms step_avg:60.23ms
step:988/2315 train_time:59510ms step_avg:60.23ms
step:989/2315 train_time:59571ms step_avg:60.23ms
step:990/2315 train_time:59632ms step_avg:60.23ms
step:991/2315 train_time:59693ms step_avg:60.24ms
step:992/2315 train_time:59754ms step_avg:60.24ms
step:993/2315 train_time:59815ms step_avg:60.24ms
step:994/2315 train_time:59876ms step_avg:60.24ms
step:995/2315 train_time:59937ms step_avg:60.24ms
step:996/2315 train_time:59998ms step_avg:60.24ms
step:997/2315 train_time:60059ms step_avg:60.24ms
step:998/2315 train_time:60120ms step_avg:60.24ms
step:999/2315 train_time:60181ms step_avg:60.24ms
step:1000/2315 train_time:60242ms step_avg:60.24ms
step:1000/2315 val_loss:3.5740 train_time:60305ms step_avg:60.30ms
step:1001/2315 train_time:60325ms step_avg:60.26ms
step:1002/2315 train_time:60367ms step_avg:60.25ms
step:1003/2315 train_time:60433ms step_avg:60.25ms
step:1004/2315 train_time:60499ms step_avg:60.26ms
step:1005/2315 train_time:60560ms step_avg:60.26ms
step:1006/2315 train_time:60621ms step_avg:60.26ms
step:1007/2315 train_time:60682ms step_avg:60.26ms
step:1008/2315 train_time:60742ms step_avg:60.26ms
step:1009/2315 train_time:60802ms step_avg:60.26ms
step:1010/2315 train_time:60862ms step_avg:60.26ms
step:1011/2315 train_time:60922ms step_avg:60.26ms
step:1012/2315 train_time:60983ms step_avg:60.26ms
step:1013/2315 train_time:61042ms step_avg:60.26ms
step:1014/2315 train_time:61102ms step_avg:60.26ms
step:1015/2315 train_time:61162ms step_avg:60.26ms
step:1016/2315 train_time:61222ms step_avg:60.26ms
step:1017/2315 train_time:61284ms step_avg:60.26ms
step:1018/2315 train_time:61345ms step_avg:60.26ms
step:1019/2315 train_time:61408ms step_avg:60.26ms
step:1020/2315 train_time:61469ms step_avg:60.26ms
step:1021/2315 train_time:61530ms step_avg:60.26ms
step:1022/2315 train_time:61591ms step_avg:60.27ms
step:1023/2315 train_time:61652ms step_avg:60.27ms
step:1024/2315 train_time:61713ms step_avg:60.27ms
step:1025/2315 train_time:61773ms step_avg:60.27ms
step:1026/2315 train_time:61834ms step_avg:60.27ms
step:1027/2315 train_time:61895ms step_avg:60.27ms
step:1028/2315 train_time:61955ms step_avg:60.27ms
step:1029/2315 train_time:62016ms step_avg:60.27ms
step:1030/2315 train_time:62077ms step_avg:60.27ms
step:1031/2315 train_time:62137ms step_avg:60.27ms
step:1032/2315 train_time:62198ms step_avg:60.27ms
step:1033/2315 train_time:62260ms step_avg:60.27ms
step:1034/2315 train_time:62321ms step_avg:60.27ms
step:1035/2315 train_time:62382ms step_avg:60.27ms
step:1036/2315 train_time:62443ms step_avg:60.27ms
step:1037/2315 train_time:62504ms step_avg:60.27ms
step:1038/2315 train_time:62565ms step_avg:60.27ms
step:1039/2315 train_time:62626ms step_avg:60.27ms
step:1040/2315 train_time:62687ms step_avg:60.28ms
step:1041/2315 train_time:62748ms step_avg:60.28ms
step:1042/2315 train_time:62808ms step_avg:60.28ms
step:1043/2315 train_time:62868ms step_avg:60.28ms
step:1044/2315 train_time:62928ms step_avg:60.28ms
step:1045/2315 train_time:62989ms step_avg:60.28ms
step:1046/2315 train_time:63049ms step_avg:60.28ms
step:1047/2315 train_time:63111ms step_avg:60.28ms
step:1048/2315 train_time:63171ms step_avg:60.28ms
step:1049/2315 train_time:63233ms step_avg:60.28ms
step:1050/2315 train_time:63294ms step_avg:60.28ms
step:1051/2315 train_time:63355ms step_avg:60.28ms
step:1052/2315 train_time:63416ms step_avg:60.28ms
step:1053/2315 train_time:63478ms step_avg:60.28ms
step:1054/2315 train_time:63538ms step_avg:60.28ms
step:1055/2315 train_time:63600ms step_avg:60.28ms
step:1056/2315 train_time:63661ms step_avg:60.28ms
step:1057/2315 train_time:63723ms step_avg:60.29ms
step:1058/2315 train_time:63783ms step_avg:60.29ms
step:1059/2315 train_time:63844ms step_avg:60.29ms
step:1060/2315 train_time:63904ms step_avg:60.29ms
step:1061/2315 train_time:63964ms step_avg:60.29ms
step:1062/2315 train_time:64025ms step_avg:60.29ms
step:1063/2315 train_time:64085ms step_avg:60.29ms
step:1064/2315 train_time:64146ms step_avg:60.29ms
step:1065/2315 train_time:64207ms step_avg:60.29ms
step:1066/2315 train_time:64267ms step_avg:60.29ms
step:1067/2315 train_time:64328ms step_avg:60.29ms
step:1068/2315 train_time:64389ms step_avg:60.29ms
step:1069/2315 train_time:64449ms step_avg:60.29ms
step:1070/2315 train_time:64510ms step_avg:60.29ms
step:1071/2315 train_time:64571ms step_avg:60.29ms
step:1072/2315 train_time:64632ms step_avg:60.29ms
step:1073/2315 train_time:64694ms step_avg:60.29ms
step:1074/2315 train_time:64755ms step_avg:60.29ms
step:1075/2315 train_time:64817ms step_avg:60.29ms
step:1076/2315 train_time:64877ms step_avg:60.29ms
step:1077/2315 train_time:64938ms step_avg:60.30ms
step:1078/2315 train_time:64999ms step_avg:60.30ms
step:1079/2315 train_time:65060ms step_avg:60.30ms
step:1080/2315 train_time:65121ms step_avg:60.30ms
step:1081/2315 train_time:65182ms step_avg:60.30ms
step:1082/2315 train_time:65242ms step_avg:60.30ms
step:1083/2315 train_time:65303ms step_avg:60.30ms
step:1084/2315 train_time:65364ms step_avg:60.30ms
step:1085/2315 train_time:65425ms step_avg:60.30ms
step:1086/2315 train_time:65486ms step_avg:60.30ms
step:1087/2315 train_time:65546ms step_avg:60.30ms
step:1088/2315 train_time:65607ms step_avg:60.30ms
step:1089/2315 train_time:65668ms step_avg:60.30ms
step:1090/2315 train_time:65728ms step_avg:60.30ms
step:1091/2315 train_time:65789ms step_avg:60.30ms
step:1092/2315 train_time:65850ms step_avg:60.30ms
step:1093/2315 train_time:65911ms step_avg:60.30ms
step:1094/2315 train_time:65972ms step_avg:60.30ms
step:1095/2315 train_time:66034ms step_avg:60.31ms
step:1096/2315 train_time:66095ms step_avg:60.31ms
step:1097/2315 train_time:66157ms step_avg:60.31ms
step:1098/2315 train_time:66218ms step_avg:60.31ms
step:1099/2315 train_time:66278ms step_avg:60.31ms
step:1100/2315 train_time:66339ms step_avg:60.31ms
step:1101/2315 train_time:66401ms step_avg:60.31ms
step:1102/2315 train_time:66461ms step_avg:60.31ms
step:1103/2315 train_time:66523ms step_avg:60.31ms
step:1104/2315 train_time:66583ms step_avg:60.31ms
step:1105/2315 train_time:66644ms step_avg:60.31ms
step:1106/2315 train_time:66705ms step_avg:60.31ms
step:1107/2315 train_time:66765ms step_avg:60.31ms
step:1108/2315 train_time:66826ms step_avg:60.31ms
step:1109/2315 train_time:66887ms step_avg:60.31ms
step:1110/2315 train_time:66947ms step_avg:60.31ms
step:1111/2315 train_time:67008ms step_avg:60.31ms
step:1112/2315 train_time:67069ms step_avg:60.31ms
step:1113/2315 train_time:67130ms step_avg:60.31ms
step:1114/2315 train_time:67191ms step_avg:60.31ms
step:1115/2315 train_time:67252ms step_avg:60.32ms
step:1116/2315 train_time:67314ms step_avg:60.32ms
step:1117/2315 train_time:67376ms step_avg:60.32ms
step:1118/2315 train_time:67437ms step_avg:60.32ms
step:1119/2315 train_time:67497ms step_avg:60.32ms
step:1120/2315 train_time:67559ms step_avg:60.32ms
step:1121/2315 train_time:67620ms step_avg:60.32ms
step:1122/2315 train_time:67681ms step_avg:60.32ms
step:1123/2315 train_time:67741ms step_avg:60.32ms
step:1124/2315 train_time:67802ms step_avg:60.32ms
step:1125/2315 train_time:67863ms step_avg:60.32ms
step:1126/2315 train_time:67924ms step_avg:60.32ms
step:1127/2315 train_time:67984ms step_avg:60.32ms
step:1128/2315 train_time:68044ms step_avg:60.32ms
step:1129/2315 train_time:68105ms step_avg:60.32ms
step:1130/2315 train_time:68166ms step_avg:60.32ms
step:1131/2315 train_time:68227ms step_avg:60.32ms
step:1132/2315 train_time:68287ms step_avg:60.32ms
step:1133/2315 train_time:68348ms step_avg:60.33ms
step:1134/2315 train_time:68409ms step_avg:60.33ms
step:1135/2315 train_time:68470ms step_avg:60.33ms
step:1136/2315 train_time:68531ms step_avg:60.33ms
step:1137/2315 train_time:68591ms step_avg:60.33ms
step:1138/2315 train_time:68652ms step_avg:60.33ms
step:1139/2315 train_time:68714ms step_avg:60.33ms
step:1140/2315 train_time:68775ms step_avg:60.33ms
step:1141/2315 train_time:68836ms step_avg:60.33ms
step:1142/2315 train_time:68897ms step_avg:60.33ms
step:1143/2315 train_time:68958ms step_avg:60.33ms
step:1144/2315 train_time:69019ms step_avg:60.33ms
step:1145/2315 train_time:69080ms step_avg:60.33ms
step:1146/2315 train_time:69141ms step_avg:60.33ms
step:1147/2315 train_time:69202ms step_avg:60.33ms
step:1148/2315 train_time:69262ms step_avg:60.33ms
step:1149/2315 train_time:69324ms step_avg:60.33ms
step:1150/2315 train_time:69384ms step_avg:60.33ms
step:1151/2315 train_time:69445ms step_avg:60.33ms
step:1152/2315 train_time:69505ms step_avg:60.33ms
step:1153/2315 train_time:69566ms step_avg:60.33ms
step:1154/2315 train_time:69626ms step_avg:60.33ms
step:1155/2315 train_time:69688ms step_avg:60.34ms
step:1156/2315 train_time:69748ms step_avg:60.34ms
step:1157/2315 train_time:69809ms step_avg:60.34ms
step:1158/2315 train_time:69869ms step_avg:60.34ms
step:1159/2315 train_time:69929ms step_avg:60.34ms
step:1160/2315 train_time:69990ms step_avg:60.34ms
step:1161/2315 train_time:70052ms step_avg:60.34ms
step:1162/2315 train_time:70113ms step_avg:60.34ms
step:1163/2315 train_time:70175ms step_avg:60.34ms
step:1164/2315 train_time:70236ms step_avg:60.34ms
step:1165/2315 train_time:70297ms step_avg:60.34ms
step:1166/2315 train_time:70358ms step_avg:60.34ms
step:1167/2315 train_time:70419ms step_avg:60.34ms
step:1168/2315 train_time:70480ms step_avg:60.34ms
step:1169/2315 train_time:70540ms step_avg:60.34ms
step:1170/2315 train_time:70601ms step_avg:60.34ms
step:1171/2315 train_time:70662ms step_avg:60.34ms
step:1172/2315 train_time:70723ms step_avg:60.34ms
step:1173/2315 train_time:70784ms step_avg:60.34ms
step:1174/2315 train_time:70844ms step_avg:60.34ms
step:1175/2315 train_time:70905ms step_avg:60.34ms
step:1176/2315 train_time:70966ms step_avg:60.35ms
step:1177/2315 train_time:71026ms step_avg:60.35ms
step:1178/2315 train_time:71086ms step_avg:60.34ms
step:1179/2315 train_time:71147ms step_avg:60.35ms
step:1180/2315 train_time:71207ms step_avg:60.35ms
step:1181/2315 train_time:71268ms step_avg:60.35ms
step:1182/2315 train_time:71330ms step_avg:60.35ms
step:1183/2315 train_time:71392ms step_avg:60.35ms
step:1184/2315 train_time:71453ms step_avg:60.35ms
step:1185/2315 train_time:71514ms step_avg:60.35ms
step:1186/2315 train_time:71576ms step_avg:60.35ms
step:1187/2315 train_time:71637ms step_avg:60.35ms
step:1188/2315 train_time:71698ms step_avg:60.35ms
step:1189/2315 train_time:71758ms step_avg:60.35ms
step:1190/2315 train_time:71819ms step_avg:60.35ms
step:1191/2315 train_time:71880ms step_avg:60.35ms
step:1192/2315 train_time:71941ms step_avg:60.35ms
step:1193/2315 train_time:72002ms step_avg:60.35ms
step:1194/2315 train_time:72062ms step_avg:60.35ms
step:1195/2315 train_time:72123ms step_avg:60.35ms
step:1196/2315 train_time:72183ms step_avg:60.35ms
step:1197/2315 train_time:72244ms step_avg:60.35ms
step:1198/2315 train_time:72305ms step_avg:60.35ms
step:1199/2315 train_time:72366ms step_avg:60.36ms
step:1200/2315 train_time:72426ms step_avg:60.36ms
step:1201/2315 train_time:72487ms step_avg:60.36ms
step:1202/2315 train_time:72547ms step_avg:60.36ms
step:1203/2315 train_time:72607ms step_avg:60.36ms
step:1204/2315 train_time:72668ms step_avg:60.36ms
step:1205/2315 train_time:72729ms step_avg:60.36ms
step:1206/2315 train_time:72790ms step_avg:60.36ms
step:1207/2315 train_time:72851ms step_avg:60.36ms
step:1208/2315 train_time:72912ms step_avg:60.36ms
step:1209/2315 train_time:72973ms step_avg:60.36ms
step:1210/2315 train_time:73034ms step_avg:60.36ms
step:1211/2315 train_time:73096ms step_avg:60.36ms
step:1212/2315 train_time:73157ms step_avg:60.36ms
step:1213/2315 train_time:73219ms step_avg:60.36ms
step:1214/2315 train_time:73280ms step_avg:60.36ms
step:1215/2315 train_time:73340ms step_avg:60.36ms
step:1216/2315 train_time:73401ms step_avg:60.36ms
step:1217/2315 train_time:73462ms step_avg:60.36ms
step:1218/2315 train_time:73523ms step_avg:60.36ms
step:1219/2315 train_time:73583ms step_avg:60.36ms
step:1220/2315 train_time:73643ms step_avg:60.36ms
step:1221/2315 train_time:73705ms step_avg:60.36ms
step:1222/2315 train_time:73766ms step_avg:60.36ms
step:1223/2315 train_time:73826ms step_avg:60.36ms
step:1224/2315 train_time:73886ms step_avg:60.36ms
step:1225/2315 train_time:73948ms step_avg:60.37ms
step:1226/2315 train_time:74008ms step_avg:60.37ms
step:1227/2315 train_time:74069ms step_avg:60.37ms
step:1228/2315 train_time:74130ms step_avg:60.37ms
step:1229/2315 train_time:74191ms step_avg:60.37ms
step:1230/2315 train_time:74252ms step_avg:60.37ms
step:1231/2315 train_time:74313ms step_avg:60.37ms
step:1232/2315 train_time:74374ms step_avg:60.37ms
step:1233/2315 train_time:74435ms step_avg:60.37ms
step:1234/2315 train_time:74496ms step_avg:60.37ms
step:1235/2315 train_time:74557ms step_avg:60.37ms
step:1236/2315 train_time:74617ms step_avg:60.37ms
step:1237/2315 train_time:74678ms step_avg:60.37ms
step:1238/2315 train_time:74739ms step_avg:60.37ms
step:1239/2315 train_time:74800ms step_avg:60.37ms
step:1240/2315 train_time:74861ms step_avg:60.37ms
step:1241/2315 train_time:74922ms step_avg:60.37ms
step:1242/2315 train_time:74982ms step_avg:60.37ms
step:1243/2315 train_time:75043ms step_avg:60.37ms
step:1244/2315 train_time:75104ms step_avg:60.37ms
step:1245/2315 train_time:75165ms step_avg:60.37ms
step:1246/2315 train_time:75226ms step_avg:60.37ms
step:1247/2315 train_time:75287ms step_avg:60.37ms
step:1248/2315 train_time:75348ms step_avg:60.37ms
step:1249/2315 train_time:75408ms step_avg:60.37ms
step:1250/2315 train_time:75468ms step_avg:60.37ms
step:1250/2315 val_loss:3.5153 train_time:75530ms step_avg:60.42ms
step:1251/2315 train_time:75550ms step_avg:60.39ms
step:1252/2315 train_time:75592ms step_avg:60.38ms
step:1253/2315 train_time:75657ms step_avg:60.38ms
step:1254/2315 train_time:75720ms step_avg:60.38ms
step:1255/2315 train_time:75781ms step_avg:60.38ms
step:1256/2315 train_time:75841ms step_avg:60.38ms
step:1257/2315 train_time:75902ms step_avg:60.38ms
step:1258/2315 train_time:75961ms step_avg:60.38ms
step:1259/2315 train_time:76021ms step_avg:60.38ms
step:1260/2315 train_time:76081ms step_avg:60.38ms
step:1261/2315 train_time:76141ms step_avg:60.38ms
step:1262/2315 train_time:76201ms step_avg:60.38ms
step:1263/2315 train_time:76261ms step_avg:60.38ms
step:1264/2315 train_time:76321ms step_avg:60.38ms
step:1265/2315 train_time:76381ms step_avg:60.38ms
step:1266/2315 train_time:76440ms step_avg:60.38ms
step:1267/2315 train_time:76501ms step_avg:60.38ms
step:1268/2315 train_time:76563ms step_avg:60.38ms
step:1269/2315 train_time:76627ms step_avg:60.38ms
step:1270/2315 train_time:76690ms step_avg:60.39ms
step:1271/2315 train_time:76752ms step_avg:60.39ms
step:1272/2315 train_time:76813ms step_avg:60.39ms
step:1273/2315 train_time:76873ms step_avg:60.39ms
step:1274/2315 train_time:76935ms step_avg:60.39ms
step:1275/2315 train_time:76995ms step_avg:60.39ms
step:1276/2315 train_time:77055ms step_avg:60.39ms
step:1277/2315 train_time:77115ms step_avg:60.39ms
step:1278/2315 train_time:77175ms step_avg:60.39ms
step:1279/2315 train_time:77236ms step_avg:60.39ms
step:1280/2315 train_time:77297ms step_avg:60.39ms
step:1281/2315 train_time:77357ms step_avg:60.39ms
step:1282/2315 train_time:77417ms step_avg:60.39ms
step:1283/2315 train_time:77478ms step_avg:60.39ms
step:1284/2315 train_time:77539ms step_avg:60.39ms
step:1285/2315 train_time:77599ms step_avg:60.39ms
step:1286/2315 train_time:77660ms step_avg:60.39ms
step:1287/2315 train_time:77721ms step_avg:60.39ms
step:1288/2315 train_time:77781ms step_avg:60.39ms
step:1289/2315 train_time:77842ms step_avg:60.39ms
step:1290/2315 train_time:77903ms step_avg:60.39ms
step:1291/2315 train_time:77965ms step_avg:60.39ms
step:1292/2315 train_time:78025ms step_avg:60.39ms
step:1293/2315 train_time:78086ms step_avg:60.39ms
step:1294/2315 train_time:78147ms step_avg:60.39ms
step:1295/2315 train_time:78208ms step_avg:60.39ms
step:1296/2315 train_time:78269ms step_avg:60.39ms
step:1297/2315 train_time:78329ms step_avg:60.39ms
step:1298/2315 train_time:78390ms step_avg:60.39ms
step:1299/2315 train_time:78451ms step_avg:60.39ms
step:1300/2315 train_time:78512ms step_avg:60.39ms
step:1301/2315 train_time:78573ms step_avg:60.39ms
step:1302/2315 train_time:78634ms step_avg:60.39ms
step:1303/2315 train_time:78696ms step_avg:60.40ms
step:1304/2315 train_time:78756ms step_avg:60.40ms
step:1305/2315 train_time:78817ms step_avg:60.40ms
step:1306/2315 train_time:78878ms step_avg:60.40ms
step:1307/2315 train_time:78939ms step_avg:60.40ms
step:1308/2315 train_time:78999ms step_avg:60.40ms
step:1309/2315 train_time:79060ms step_avg:60.40ms
step:1310/2315 train_time:79120ms step_avg:60.40ms
step:1311/2315 train_time:79180ms step_avg:60.40ms
step:1312/2315 train_time:79240ms step_avg:60.40ms
step:1313/2315 train_time:79301ms step_avg:60.40ms
step:1314/2315 train_time:79361ms step_avg:60.40ms
step:1315/2315 train_time:79423ms step_avg:60.40ms
step:1316/2315 train_time:79483ms step_avg:60.40ms
step:1317/2315 train_time:79544ms step_avg:60.40ms
step:1318/2315 train_time:79606ms step_avg:60.40ms
step:1319/2315 train_time:79667ms step_avg:60.40ms
step:1320/2315 train_time:79728ms step_avg:60.40ms
step:1321/2315 train_time:79789ms step_avg:60.40ms
step:1322/2315 train_time:79850ms step_avg:60.40ms
step:1323/2315 train_time:79911ms step_avg:60.40ms
step:1324/2315 train_time:79972ms step_avg:60.40ms
step:1325/2315 train_time:80033ms step_avg:60.40ms
step:1326/2315 train_time:80094ms step_avg:60.40ms
step:1327/2315 train_time:80154ms step_avg:60.40ms
step:1328/2315 train_time:80215ms step_avg:60.40ms
step:1329/2315 train_time:80276ms step_avg:60.40ms
step:1330/2315 train_time:80336ms step_avg:60.40ms
step:1331/2315 train_time:80397ms step_avg:60.40ms
step:1332/2315 train_time:80457ms step_avg:60.40ms
step:1333/2315 train_time:80518ms step_avg:60.40ms
step:1334/2315 train_time:80579ms step_avg:60.40ms
step:1335/2315 train_time:80640ms step_avg:60.40ms
step:1336/2315 train_time:80701ms step_avg:60.40ms
step:1337/2315 train_time:80762ms step_avg:60.41ms
step:1338/2315 train_time:80822ms step_avg:60.41ms
step:1339/2315 train_time:80882ms step_avg:60.40ms
step:1340/2315 train_time:80942ms step_avg:60.40ms
step:1341/2315 train_time:81003ms step_avg:60.41ms
step:1342/2315 train_time:81064ms step_avg:60.41ms
step:1343/2315 train_time:81126ms step_avg:60.41ms
step:1344/2315 train_time:81186ms step_avg:60.41ms
step:1345/2315 train_time:81248ms step_avg:60.41ms
step:1346/2315 train_time:81309ms step_avg:60.41ms
step:1347/2315 train_time:81370ms step_avg:60.41ms
step:1348/2315 train_time:81431ms step_avg:60.41ms
step:1349/2315 train_time:81492ms step_avg:60.41ms
step:1350/2315 train_time:81553ms step_avg:60.41ms
step:1351/2315 train_time:81614ms step_avg:60.41ms
step:1352/2315 train_time:81675ms step_avg:60.41ms
step:1353/2315 train_time:81736ms step_avg:60.41ms
step:1354/2315 train_time:81796ms step_avg:60.41ms
step:1355/2315 train_time:81857ms step_avg:60.41ms
step:1356/2315 train_time:81918ms step_avg:60.41ms
step:1357/2315 train_time:81978ms step_avg:60.41ms
step:1358/2315 train_time:82039ms step_avg:60.41ms
step:1359/2315 train_time:82099ms step_avg:60.41ms
step:1360/2315 train_time:82159ms step_avg:60.41ms
step:1361/2315 train_time:82221ms step_avg:60.41ms
step:1362/2315 train_time:82281ms step_avg:60.41ms
step:1363/2315 train_time:82341ms step_avg:60.41ms
step:1364/2315 train_time:82402ms step_avg:60.41ms
step:1365/2315 train_time:82463ms step_avg:60.41ms
step:1366/2315 train_time:82524ms step_avg:60.41ms
step:1367/2315 train_time:82585ms step_avg:60.41ms
step:1368/2315 train_time:82645ms step_avg:60.41ms
step:1369/2315 train_time:82706ms step_avg:60.41ms
step:1370/2315 train_time:82766ms step_avg:60.41ms
step:1371/2315 train_time:82827ms step_avg:60.41ms
step:1372/2315 train_time:82888ms step_avg:60.41ms
step:1373/2315 train_time:82950ms step_avg:60.42ms
step:1374/2315 train_time:83011ms step_avg:60.42ms
step:1375/2315 train_time:83072ms step_avg:60.42ms
step:1376/2315 train_time:83133ms step_avg:60.42ms
step:1377/2315 train_time:83194ms step_avg:60.42ms
step:1378/2315 train_time:83254ms step_avg:60.42ms
step:1379/2315 train_time:83314ms step_avg:60.42ms
step:1380/2315 train_time:83375ms step_avg:60.42ms
step:1381/2315 train_time:83436ms step_avg:60.42ms
step:1382/2315 train_time:83496ms step_avg:60.42ms
step:1383/2315 train_time:83557ms step_avg:60.42ms
step:1384/2315 train_time:83618ms step_avg:60.42ms
step:1385/2315 train_time:83679ms step_avg:60.42ms
step:1386/2315 train_time:83740ms step_avg:60.42ms
step:1387/2315 train_time:83801ms step_avg:60.42ms
step:1388/2315 train_time:83861ms step_avg:60.42ms
step:1389/2315 train_time:83922ms step_avg:60.42ms
step:1390/2315 train_time:83982ms step_avg:60.42ms
step:1391/2315 train_time:84043ms step_avg:60.42ms
step:1392/2315 train_time:84104ms step_avg:60.42ms
step:1393/2315 train_time:84165ms step_avg:60.42ms
step:1394/2315 train_time:84226ms step_avg:60.42ms
step:1395/2315 train_time:84287ms step_avg:60.42ms
step:1396/2315 train_time:84348ms step_avg:60.42ms
step:1397/2315 train_time:84410ms step_avg:60.42ms
step:1398/2315 train_time:84471ms step_avg:60.42ms
step:1399/2315 train_time:84532ms step_avg:60.42ms
step:1400/2315 train_time:84593ms step_avg:60.42ms
step:1401/2315 train_time:84654ms step_avg:60.42ms
step:1402/2315 train_time:84714ms step_avg:60.42ms
step:1403/2315 train_time:84775ms step_avg:60.42ms
step:1404/2315 train_time:84835ms step_avg:60.42ms
step:1405/2315 train_time:84896ms step_avg:60.42ms
step:1406/2315 train_time:84957ms step_avg:60.42ms
step:1407/2315 train_time:85018ms step_avg:60.43ms
step:1408/2315 train_time:85079ms step_avg:60.43ms
step:1409/2315 train_time:85139ms step_avg:60.43ms
step:1410/2315 train_time:85199ms step_avg:60.43ms
step:1411/2315 train_time:85260ms step_avg:60.43ms
step:1412/2315 train_time:85320ms step_avg:60.43ms
step:1413/2315 train_time:85381ms step_avg:60.43ms
step:1414/2315 train_time:85442ms step_avg:60.43ms
step:1415/2315 train_time:85503ms step_avg:60.43ms
step:1416/2315 train_time:85564ms step_avg:60.43ms
step:1417/2315 train_time:85625ms step_avg:60.43ms
step:1418/2315 train_time:85687ms step_avg:60.43ms
step:1419/2315 train_time:85748ms step_avg:60.43ms
step:1420/2315 train_time:85809ms step_avg:60.43ms
step:1421/2315 train_time:85870ms step_avg:60.43ms
step:1422/2315 train_time:85931ms step_avg:60.43ms
step:1423/2315 train_time:85992ms step_avg:60.43ms
step:1424/2315 train_time:86053ms step_avg:60.43ms
step:1425/2315 train_time:86114ms step_avg:60.43ms
step:1426/2315 train_time:86174ms step_avg:60.43ms
step:1427/2315 train_time:86235ms step_avg:60.43ms
step:1428/2315 train_time:86295ms step_avg:60.43ms
step:1429/2315 train_time:86356ms step_avg:60.43ms
step:1430/2315 train_time:86417ms step_avg:60.43ms
step:1431/2315 train_time:86477ms step_avg:60.43ms
step:1432/2315 train_time:86538ms step_avg:60.43ms
step:1433/2315 train_time:86599ms step_avg:60.43ms
step:1434/2315 train_time:86660ms step_avg:60.43ms
step:1435/2315 train_time:86721ms step_avg:60.43ms
step:1436/2315 train_time:86782ms step_avg:60.43ms
step:1437/2315 train_time:86842ms step_avg:60.43ms
step:1438/2315 train_time:86903ms step_avg:60.43ms
step:1439/2315 train_time:86963ms step_avg:60.43ms
step:1440/2315 train_time:87024ms step_avg:60.43ms
step:1441/2315 train_time:87085ms step_avg:60.43ms
step:1442/2315 train_time:87146ms step_avg:60.43ms
step:1443/2315 train_time:87207ms step_avg:60.43ms
step:1444/2315 train_time:87268ms step_avg:60.43ms
step:1445/2315 train_time:87329ms step_avg:60.44ms
step:1446/2315 train_time:87391ms step_avg:60.44ms
step:1447/2315 train_time:87452ms step_avg:60.44ms
step:1448/2315 train_time:87512ms step_avg:60.44ms
step:1449/2315 train_time:87574ms step_avg:60.44ms
step:1450/2315 train_time:87634ms step_avg:60.44ms
step:1451/2315 train_time:87695ms step_avg:60.44ms
step:1452/2315 train_time:87756ms step_avg:60.44ms
step:1453/2315 train_time:87817ms step_avg:60.44ms
step:1454/2315 train_time:87877ms step_avg:60.44ms
step:1455/2315 train_time:87938ms step_avg:60.44ms
step:1456/2315 train_time:87999ms step_avg:60.44ms
step:1457/2315 train_time:88059ms step_avg:60.44ms
step:1458/2315 train_time:88120ms step_avg:60.44ms
step:1459/2315 train_time:88181ms step_avg:60.44ms
step:1460/2315 train_time:88241ms step_avg:60.44ms
step:1461/2315 train_time:88301ms step_avg:60.44ms
step:1462/2315 train_time:88362ms step_avg:60.44ms
step:1463/2315 train_time:88423ms step_avg:60.44ms
step:1464/2315 train_time:88484ms step_avg:60.44ms
step:1465/2315 train_time:88545ms step_avg:60.44ms
step:1466/2315 train_time:88606ms step_avg:60.44ms
step:1467/2315 train_time:88668ms step_avg:60.44ms
step:1468/2315 train_time:88729ms step_avg:60.44ms
step:1469/2315 train_time:88790ms step_avg:60.44ms
step:1470/2315 train_time:88851ms step_avg:60.44ms
step:1471/2315 train_time:88912ms step_avg:60.44ms
step:1472/2315 train_time:88973ms step_avg:60.44ms
step:1473/2315 train_time:89033ms step_avg:60.44ms
step:1474/2315 train_time:89094ms step_avg:60.44ms
step:1475/2315 train_time:89155ms step_avg:60.44ms
step:1476/2315 train_time:89215ms step_avg:60.44ms
step:1477/2315 train_time:89276ms step_avg:60.44ms
step:1478/2315 train_time:89338ms step_avg:60.44ms
step:1479/2315 train_time:89398ms step_avg:60.44ms
step:1480/2315 train_time:89458ms step_avg:60.44ms
step:1481/2315 train_time:89518ms step_avg:60.44ms
step:1482/2315 train_time:89579ms step_avg:60.44ms
step:1483/2315 train_time:89640ms step_avg:60.45ms
step:1484/2315 train_time:89701ms step_avg:60.45ms
step:1485/2315 train_time:89762ms step_avg:60.45ms
step:1486/2315 train_time:89823ms step_avg:60.45ms
step:1487/2315 train_time:89883ms step_avg:60.45ms
step:1488/2315 train_time:89944ms step_avg:60.45ms
step:1489/2315 train_time:90005ms step_avg:60.45ms
step:1490/2315 train_time:90066ms step_avg:60.45ms
step:1491/2315 train_time:90128ms step_avg:60.45ms
step:1492/2315 train_time:90188ms step_avg:60.45ms
step:1493/2315 train_time:90250ms step_avg:60.45ms
step:1494/2315 train_time:90311ms step_avg:60.45ms
step:1495/2315 train_time:90372ms step_avg:60.45ms
step:1496/2315 train_time:90433ms step_avg:60.45ms
step:1497/2315 train_time:90494ms step_avg:60.45ms
step:1498/2315 train_time:90554ms step_avg:60.45ms
step:1499/2315 train_time:90615ms step_avg:60.45ms
step:1500/2315 train_time:90675ms step_avg:60.45ms
step:1500/2315 val_loss:3.4504 train_time:90738ms step_avg:60.49ms
step:1501/2315 train_time:90758ms step_avg:60.46ms
step:1502/2315 train_time:90799ms step_avg:60.45ms
step:1503/2315 train_time:90863ms step_avg:60.45ms
step:1504/2315 train_time:90927ms step_avg:60.46ms
step:1505/2315 train_time:90988ms step_avg:60.46ms
step:1506/2315 train_time:91049ms step_avg:60.46ms
step:1507/2315 train_time:91109ms step_avg:60.46ms
step:1508/2315 train_time:91169ms step_avg:60.46ms
step:1509/2315 train_time:91230ms step_avg:60.46ms
step:1510/2315 train_time:91290ms step_avg:60.46ms
step:1511/2315 train_time:91350ms step_avg:60.46ms
step:1512/2315 train_time:91410ms step_avg:60.46ms
step:1513/2315 train_time:91470ms step_avg:60.46ms
step:1514/2315 train_time:91530ms step_avg:60.46ms
step:1515/2315 train_time:91589ms step_avg:60.46ms
step:1516/2315 train_time:91650ms step_avg:60.46ms
step:1517/2315 train_time:91712ms step_avg:60.46ms
step:1518/2315 train_time:91773ms step_avg:60.46ms
step:1519/2315 train_time:91834ms step_avg:60.46ms
step:1520/2315 train_time:91896ms step_avg:60.46ms
step:1521/2315 train_time:91958ms step_avg:60.46ms
step:1522/2315 train_time:92019ms step_avg:60.46ms
step:1523/2315 train_time:92081ms step_avg:60.46ms
step:1524/2315 train_time:92142ms step_avg:60.46ms
step:1525/2315 train_time:92204ms step_avg:60.46ms
step:1526/2315 train_time:92265ms step_avg:60.46ms
step:1527/2315 train_time:92326ms step_avg:60.46ms
step:1528/2315 train_time:92387ms step_avg:60.46ms
step:1529/2315 train_time:92448ms step_avg:60.46ms
step:1530/2315 train_time:92509ms step_avg:60.46ms
step:1531/2315 train_time:92569ms step_avg:60.46ms
step:1532/2315 train_time:92630ms step_avg:60.46ms
step:1533/2315 train_time:92691ms step_avg:60.46ms
step:1534/2315 train_time:92752ms step_avg:60.46ms
step:1535/2315 train_time:92814ms step_avg:60.46ms
step:1536/2315 train_time:92875ms step_avg:60.47ms
step:1537/2315 train_time:92936ms step_avg:60.47ms
step:1538/2315 train_time:92997ms step_avg:60.47ms
step:1539/2315 train_time:93058ms step_avg:60.47ms
step:1540/2315 train_time:93120ms step_avg:60.47ms
step:1541/2315 train_time:93181ms step_avg:60.47ms
step:1542/2315 train_time:93243ms step_avg:60.47ms
step:1543/2315 train_time:93304ms step_avg:60.47ms
step:1544/2315 train_time:93365ms step_avg:60.47ms
step:1545/2315 train_time:93426ms step_avg:60.47ms
step:1546/2315 train_time:93487ms step_avg:60.47ms
step:1547/2315 train_time:93548ms step_avg:60.47ms
step:1548/2315 train_time:93609ms step_avg:60.47ms
step:1549/2315 train_time:93670ms step_avg:60.47ms
step:1550/2315 train_time:93731ms step_avg:60.47ms
step:1551/2315 train_time:93793ms step_avg:60.47ms
step:1552/2315 train_time:93854ms step_avg:60.47ms
step:1553/2315 train_time:93915ms step_avg:60.47ms
step:1554/2315 train_time:93976ms step_avg:60.47ms
step:1555/2315 train_time:94037ms step_avg:60.47ms
step:1556/2315 train_time:94098ms step_avg:60.47ms
step:1557/2315 train_time:94159ms step_avg:60.47ms
step:1558/2315 train_time:94220ms step_avg:60.48ms
step:1559/2315 train_time:94282ms step_avg:60.48ms
step:1560/2315 train_time:94344ms step_avg:60.48ms
step:1561/2315 train_time:94405ms step_avg:60.48ms
step:1562/2315 train_time:94467ms step_avg:60.48ms
step:1563/2315 train_time:94528ms step_avg:60.48ms
step:1564/2315 train_time:94589ms step_avg:60.48ms
step:1565/2315 train_time:94650ms step_avg:60.48ms
step:1566/2315 train_time:94711ms step_avg:60.48ms
step:1567/2315 train_time:94772ms step_avg:60.48ms
step:1568/2315 train_time:94833ms step_avg:60.48ms
step:1569/2315 train_time:94895ms step_avg:60.48ms
step:1570/2315 train_time:94956ms step_avg:60.48ms
step:1571/2315 train_time:95017ms step_avg:60.48ms
step:1572/2315 train_time:95078ms step_avg:60.48ms
step:1573/2315 train_time:95139ms step_avg:60.48ms
step:1574/2315 train_time:95200ms step_avg:60.48ms
step:1575/2315 train_time:95262ms step_avg:60.48ms
step:1576/2315 train_time:95323ms step_avg:60.48ms
step:1577/2315 train_time:95385ms step_avg:60.48ms
step:1578/2315 train_time:95446ms step_avg:60.49ms
step:1579/2315 train_time:95508ms step_avg:60.49ms
step:1580/2315 train_time:95569ms step_avg:60.49ms
step:1581/2315 train_time:95630ms step_avg:60.49ms
step:1582/2315 train_time:95691ms step_avg:60.49ms
step:1583/2315 train_time:95752ms step_avg:60.49ms
step:1584/2315 train_time:95814ms step_avg:60.49ms
step:1585/2315 train_time:95875ms step_avg:60.49ms
step:1586/2315 train_time:95936ms step_avg:60.49ms
step:1587/2315 train_time:95997ms step_avg:60.49ms
step:1588/2315 train_time:96058ms step_avg:60.49ms
step:1589/2315 train_time:96118ms step_avg:60.49ms
step:1590/2315 train_time:96179ms step_avg:60.49ms
step:1591/2315 train_time:96240ms step_avg:60.49ms
step:1592/2315 train_time:96302ms step_avg:60.49ms
step:1593/2315 train_time:96364ms step_avg:60.49ms
step:1594/2315 train_time:96425ms step_avg:60.49ms
step:1595/2315 train_time:96487ms step_avg:60.49ms
step:1596/2315 train_time:96548ms step_avg:60.49ms
step:1597/2315 train_time:96610ms step_avg:60.49ms
step:1598/2315 train_time:96671ms step_avg:60.49ms
step:1599/2315 train_time:96732ms step_avg:60.50ms
step:1600/2315 train_time:96793ms step_avg:60.50ms
step:1601/2315 train_time:96855ms step_avg:60.50ms
step:1602/2315 train_time:96916ms step_avg:60.50ms
step:1603/2315 train_time:96977ms step_avg:60.50ms
step:1604/2315 train_time:97038ms step_avg:60.50ms
step:1605/2315 train_time:97098ms step_avg:60.50ms
step:1606/2315 train_time:97159ms step_avg:60.50ms
step:1607/2315 train_time:97220ms step_avg:60.50ms
step:1608/2315 train_time:97281ms step_avg:60.50ms
step:1609/2315 train_time:97343ms step_avg:60.50ms
step:1610/2315 train_time:97404ms step_avg:60.50ms
step:1611/2315 train_time:97466ms step_avg:60.50ms
step:1612/2315 train_time:97527ms step_avg:60.50ms
step:1613/2315 train_time:97589ms step_avg:60.50ms
step:1614/2315 train_time:97650ms step_avg:60.50ms
step:1615/2315 train_time:97712ms step_avg:60.50ms
step:1616/2315 train_time:97772ms step_avg:60.50ms
step:1617/2315 train_time:97833ms step_avg:60.50ms
step:1618/2315 train_time:97894ms step_avg:60.50ms
step:1619/2315 train_time:97955ms step_avg:60.50ms
step:1620/2315 train_time:98016ms step_avg:60.50ms
step:1621/2315 train_time:98077ms step_avg:60.50ms
step:1622/2315 train_time:98137ms step_avg:60.50ms
step:1623/2315 train_time:98198ms step_avg:60.50ms
step:1624/2315 train_time:98260ms step_avg:60.50ms
step:1625/2315 train_time:98321ms step_avg:60.51ms
step:1626/2315 train_time:98383ms step_avg:60.51ms
step:1627/2315 train_time:98444ms step_avg:60.51ms
step:1628/2315 train_time:98505ms step_avg:60.51ms
step:1629/2315 train_time:98567ms step_avg:60.51ms
step:1630/2315 train_time:98629ms step_avg:60.51ms
step:1631/2315 train_time:98691ms step_avg:60.51ms
step:1632/2315 train_time:98751ms step_avg:60.51ms
step:1633/2315 train_time:98813ms step_avg:60.51ms
step:1634/2315 train_time:98874ms step_avg:60.51ms
step:1635/2315 train_time:98935ms step_avg:60.51ms
step:1636/2315 train_time:98996ms step_avg:60.51ms
step:1637/2315 train_time:99057ms step_avg:60.51ms
step:1638/2315 train_time:99118ms step_avg:60.51ms
step:1639/2315 train_time:99178ms step_avg:60.51ms
step:1640/2315 train_time:99239ms step_avg:60.51ms
step:1641/2315 train_time:99302ms step_avg:60.51ms
step:1642/2315 train_time:99363ms step_avg:60.51ms
step:1643/2315 train_time:99425ms step_avg:60.51ms
step:1644/2315 train_time:99486ms step_avg:60.51ms
step:1645/2315 train_time:99548ms step_avg:60.52ms
step:1646/2315 train_time:99609ms step_avg:60.52ms
step:1647/2315 train_time:99671ms step_avg:60.52ms
step:1648/2315 train_time:99732ms step_avg:60.52ms
step:1649/2315 train_time:99794ms step_avg:60.52ms
step:1650/2315 train_time:99854ms step_avg:60.52ms
step:1651/2315 train_time:99915ms step_avg:60.52ms
step:1652/2315 train_time:99976ms step_avg:60.52ms
step:1653/2315 train_time:100037ms step_avg:60.52ms
step:1654/2315 train_time:100097ms step_avg:60.52ms
step:1655/2315 train_time:100159ms step_avg:60.52ms
step:1656/2315 train_time:100220ms step_avg:60.52ms
step:1657/2315 train_time:100281ms step_avg:60.52ms
step:1658/2315 train_time:100342ms step_avg:60.52ms
step:1659/2315 train_time:100404ms step_avg:60.52ms
step:1660/2315 train_time:100465ms step_avg:60.52ms
step:1661/2315 train_time:100527ms step_avg:60.52ms
step:1662/2315 train_time:100589ms step_avg:60.52ms
step:1663/2315 train_time:100651ms step_avg:60.52ms
step:1664/2315 train_time:100711ms step_avg:60.52ms
step:1665/2315 train_time:100773ms step_avg:60.52ms
step:1666/2315 train_time:100834ms step_avg:60.52ms
step:1667/2315 train_time:100895ms step_avg:60.52ms
step:1668/2315 train_time:100955ms step_avg:60.52ms
step:1669/2315 train_time:101016ms step_avg:60.52ms
step:1670/2315 train_time:101077ms step_avg:60.53ms
step:1671/2315 train_time:101138ms step_avg:60.53ms
step:1672/2315 train_time:101199ms step_avg:60.53ms
step:1673/2315 train_time:101260ms step_avg:60.53ms
step:1674/2315 train_time:101321ms step_avg:60.53ms
step:1675/2315 train_time:101382ms step_avg:60.53ms
step:1676/2315 train_time:101444ms step_avg:60.53ms
step:1677/2315 train_time:101506ms step_avg:60.53ms
step:1678/2315 train_time:101567ms step_avg:60.53ms
step:1679/2315 train_time:101629ms step_avg:60.53ms
step:1680/2315 train_time:101691ms step_avg:60.53ms
step:1681/2315 train_time:101752ms step_avg:60.53ms
step:1682/2315 train_time:101813ms step_avg:60.53ms
step:1683/2315 train_time:101874ms step_avg:60.53ms
step:1684/2315 train_time:101935ms step_avg:60.53ms
step:1685/2315 train_time:101996ms step_avg:60.53ms
step:1686/2315 train_time:102057ms step_avg:60.53ms
step:1687/2315 train_time:102118ms step_avg:60.53ms
step:1688/2315 train_time:102179ms step_avg:60.53ms
step:1689/2315 train_time:102240ms step_avg:60.53ms
step:1690/2315 train_time:102301ms step_avg:60.53ms
step:1691/2315 train_time:102362ms step_avg:60.53ms
step:1692/2315 train_time:102424ms step_avg:60.53ms
step:1693/2315 train_time:102485ms step_avg:60.53ms
step:1694/2315 train_time:102546ms step_avg:60.54ms
step:1695/2315 train_time:102608ms step_avg:60.54ms
step:1696/2315 train_time:102669ms step_avg:60.54ms
step:1697/2315 train_time:102730ms step_avg:60.54ms
step:1698/2315 train_time:102791ms step_avg:60.54ms
step:1699/2315 train_time:102852ms step_avg:60.54ms
step:1700/2315 train_time:102914ms step_avg:60.54ms
step:1701/2315 train_time:102975ms step_avg:60.54ms
step:1702/2315 train_time:103036ms step_avg:60.54ms
step:1703/2315 train_time:103096ms step_avg:60.54ms
step:1704/2315 train_time:103157ms step_avg:60.54ms
step:1705/2315 train_time:103218ms step_avg:60.54ms
step:1706/2315 train_time:103280ms step_avg:60.54ms
step:1707/2315 train_time:103342ms step_avg:60.54ms
step:1708/2315 train_time:103404ms step_avg:60.54ms
step:1709/2315 train_time:103465ms step_avg:60.54ms
step:1710/2315 train_time:103526ms step_avg:60.54ms
step:1711/2315 train_time:103588ms step_avg:60.54ms
step:1712/2315 train_time:103649ms step_avg:60.54ms
step:1713/2315 train_time:103711ms step_avg:60.54ms
step:1714/2315 train_time:103772ms step_avg:60.54ms
step:1715/2315 train_time:103833ms step_avg:60.54ms
step:1716/2315 train_time:103894ms step_avg:60.54ms
step:1717/2315 train_time:103956ms step_avg:60.54ms
step:1718/2315 train_time:104016ms step_avg:60.55ms
step:1719/2315 train_time:104078ms step_avg:60.55ms
step:1720/2315 train_time:104139ms step_avg:60.55ms
step:1721/2315 train_time:104199ms step_avg:60.55ms
step:1722/2315 train_time:104261ms step_avg:60.55ms
step:1723/2315 train_time:104322ms step_avg:60.55ms
step:1724/2315 train_time:104383ms step_avg:60.55ms
step:1725/2315 train_time:104444ms step_avg:60.55ms
step:1726/2315 train_time:104506ms step_avg:60.55ms
step:1727/2315 train_time:104567ms step_avg:60.55ms
step:1728/2315 train_time:104629ms step_avg:60.55ms
step:1729/2315 train_time:104691ms step_avg:60.55ms
step:1730/2315 train_time:104752ms step_avg:60.55ms
step:1731/2315 train_time:104814ms step_avg:60.55ms
step:1732/2315 train_time:104875ms step_avg:60.55ms
step:1733/2315 train_time:104936ms step_avg:60.55ms
step:1734/2315 train_time:104997ms step_avg:60.55ms
step:1735/2315 train_time:105058ms step_avg:60.55ms
step:1736/2315 train_time:105119ms step_avg:60.55ms
step:1737/2315 train_time:105180ms step_avg:60.55ms
step:1738/2315 train_time:105241ms step_avg:60.55ms
step:1739/2315 train_time:105302ms step_avg:60.55ms
step:1740/2315 train_time:105363ms step_avg:60.55ms
step:1741/2315 train_time:105425ms step_avg:60.55ms
step:1742/2315 train_time:105486ms step_avg:60.55ms
step:1743/2315 train_time:105548ms step_avg:60.56ms
step:1744/2315 train_time:105608ms step_avg:60.56ms
step:1745/2315 train_time:105670ms step_avg:60.56ms
step:1746/2315 train_time:105731ms step_avg:60.56ms
step:1747/2315 train_time:105792ms step_avg:60.56ms
step:1748/2315 train_time:105853ms step_avg:60.56ms
step:1749/2315 train_time:105914ms step_avg:60.56ms
step:1750/2315 train_time:105975ms step_avg:60.56ms
step:1750/2315 val_loss:3.3812 train_time:106038ms step_avg:60.59ms
step:1751/2315 train_time:106059ms step_avg:60.57ms
step:1752/2315 train_time:106100ms step_avg:60.56ms
step:1753/2315 train_time:106164ms step_avg:60.56ms
step:1754/2315 train_time:106228ms step_avg:60.56ms
step:1755/2315 train_time:106289ms step_avg:60.56ms
step:1756/2315 train_time:106351ms step_avg:60.56ms
step:1757/2315 train_time:106412ms step_avg:60.56ms
step:1758/2315 train_time:106472ms step_avg:60.56ms
step:1759/2315 train_time:106533ms step_avg:60.56ms
step:1760/2315 train_time:106594ms step_avg:60.56ms
step:1761/2315 train_time:106654ms step_avg:60.56ms
step:1762/2315 train_time:106715ms step_avg:60.56ms
step:1763/2315 train_time:106775ms step_avg:60.56ms
step:1764/2315 train_time:106836ms step_avg:60.56ms
step:1765/2315 train_time:106897ms step_avg:60.56ms
step:1766/2315 train_time:106958ms step_avg:60.56ms
step:1767/2315 train_time:107020ms step_avg:60.57ms
step:1768/2315 train_time:107083ms step_avg:60.57ms
step:1769/2315 train_time:107145ms step_avg:60.57ms
step:1770/2315 train_time:107207ms step_avg:60.57ms
step:1771/2315 train_time:107269ms step_avg:60.57ms
step:1772/2315 train_time:107330ms step_avg:60.57ms
step:1773/2315 train_time:107392ms step_avg:60.57ms
step:1774/2315 train_time:107453ms step_avg:60.57ms
step:1775/2315 train_time:107514ms step_avg:60.57ms
step:1776/2315 train_time:107574ms step_avg:60.57ms
step:1777/2315 train_time:107635ms step_avg:60.57ms
step:1778/2315 train_time:107696ms step_avg:60.57ms
step:1779/2315 train_time:107757ms step_avg:60.57ms
step:1780/2315 train_time:107817ms step_avg:60.57ms
step:1781/2315 train_time:107878ms step_avg:60.57ms
step:1782/2315 train_time:107939ms step_avg:60.57ms
step:1783/2315 train_time:108000ms step_avg:60.57ms
step:1784/2315 train_time:108062ms step_avg:60.57ms
step:1785/2315 train_time:108123ms step_avg:60.57ms
step:1786/2315 train_time:108185ms step_avg:60.57ms
step:1787/2315 train_time:108247ms step_avg:60.57ms
step:1788/2315 train_time:108308ms step_avg:60.58ms
step:1789/2315 train_time:108369ms step_avg:60.58ms
step:1790/2315 train_time:108430ms step_avg:60.58ms
step:1791/2315 train_time:108491ms step_avg:60.58ms
step:1792/2315 train_time:108553ms step_avg:60.58ms
step:1793/2315 train_time:108614ms step_avg:60.58ms
step:1794/2315 train_time:108675ms step_avg:60.58ms
step:1795/2315 train_time:108736ms step_avg:60.58ms
step:1796/2315 train_time:108797ms step_avg:60.58ms
step:1797/2315 train_time:108857ms step_avg:60.58ms
step:1798/2315 train_time:108918ms step_avg:60.58ms
step:1799/2315 train_time:108979ms step_avg:60.58ms
step:1800/2315 train_time:109040ms step_avg:60.58ms
step:1801/2315 train_time:109102ms step_avg:60.58ms
step:1802/2315 train_time:109163ms step_avg:60.58ms
step:1803/2315 train_time:109225ms step_avg:60.58ms
step:1804/2315 train_time:109286ms step_avg:60.58ms
step:1805/2315 train_time:109347ms step_avg:60.58ms
step:1806/2315 train_time:109409ms step_avg:60.58ms
step:1807/2315 train_time:109470ms step_avg:60.58ms
step:1808/2315 train_time:109530ms step_avg:60.58ms
step:1809/2315 train_time:109592ms step_avg:60.58ms
step:1810/2315 train_time:109653ms step_avg:60.58ms
step:1811/2315 train_time:109714ms step_avg:60.58ms
step:1812/2315 train_time:109775ms step_avg:60.58ms
step:1813/2315 train_time:109836ms step_avg:60.58ms
step:1814/2315 train_time:109897ms step_avg:60.58ms
step:1815/2315 train_time:109958ms step_avg:60.58ms
step:1816/2315 train_time:110019ms step_avg:60.58ms
step:1817/2315 train_time:110081ms step_avg:60.58ms
step:1818/2315 train_time:110142ms step_avg:60.58ms
step:1819/2315 train_time:110203ms step_avg:60.58ms
step:1820/2315 train_time:110264ms step_avg:60.58ms
step:1821/2315 train_time:110326ms step_avg:60.59ms
step:1822/2315 train_time:110386ms step_avg:60.59ms
step:1823/2315 train_time:110448ms step_avg:60.59ms
step:1824/2315 train_time:110509ms step_avg:60.59ms
step:1825/2315 train_time:110570ms step_avg:60.59ms
step:1826/2315 train_time:110631ms step_avg:60.59ms
step:1827/2315 train_time:110693ms step_avg:60.59ms
step:1828/2315 train_time:110754ms step_avg:60.59ms
step:1829/2315 train_time:110815ms step_avg:60.59ms
step:1830/2315 train_time:110876ms step_avg:60.59ms
step:1831/2315 train_time:110938ms step_avg:60.59ms
step:1832/2315 train_time:110999ms step_avg:60.59ms
step:1833/2315 train_time:111060ms step_avg:60.59ms
step:1834/2315 train_time:111121ms step_avg:60.59ms
step:1835/2315 train_time:111183ms step_avg:60.59ms
step:1836/2315 train_time:111244ms step_avg:60.59ms
step:1837/2315 train_time:111305ms step_avg:60.59ms
step:1838/2315 train_time:111366ms step_avg:60.59ms
step:1839/2315 train_time:111427ms step_avg:60.59ms
step:1840/2315 train_time:111488ms step_avg:60.59ms
step:1841/2315 train_time:111548ms step_avg:60.59ms
step:1842/2315 train_time:111610ms step_avg:60.59ms
step:1843/2315 train_time:111672ms step_avg:60.59ms
step:1844/2315 train_time:111732ms step_avg:60.59ms
step:1845/2315 train_time:111794ms step_avg:60.59ms
step:1846/2315 train_time:111856ms step_avg:60.59ms
step:1847/2315 train_time:111917ms step_avg:60.59ms
step:1848/2315 train_time:111979ms step_avg:60.59ms
step:1849/2315 train_time:112040ms step_avg:60.59ms
step:1850/2315 train_time:112101ms step_avg:60.60ms
step:1851/2315 train_time:112163ms step_avg:60.60ms
step:1852/2315 train_time:112224ms step_avg:60.60ms
step:1853/2315 train_time:112285ms step_avg:60.60ms
step:1854/2315 train_time:112346ms step_avg:60.60ms
step:1855/2315 train_time:112407ms step_avg:60.60ms
step:1856/2315 train_time:112468ms step_avg:60.60ms
step:1857/2315 train_time:112529ms step_avg:60.60ms
step:1858/2315 train_time:112589ms step_avg:60.60ms
step:1859/2315 train_time:112651ms step_avg:60.60ms
step:1860/2315 train_time:112712ms step_avg:60.60ms
step:1861/2315 train_time:112774ms step_avg:60.60ms
step:1862/2315 train_time:112835ms step_avg:60.60ms
step:1863/2315 train_time:112897ms step_avg:60.60ms
step:1864/2315 train_time:112957ms step_avg:60.60ms
step:1865/2315 train_time:113019ms step_avg:60.60ms
step:1866/2315 train_time:113080ms step_avg:60.60ms
step:1867/2315 train_time:113142ms step_avg:60.60ms
step:1868/2315 train_time:113203ms step_avg:60.60ms
step:1869/2315 train_time:113264ms step_avg:60.60ms
step:1870/2315 train_time:113325ms step_avg:60.60ms
step:1871/2315 train_time:113386ms step_avg:60.60ms
step:1872/2315 train_time:113446ms step_avg:60.60ms
step:1873/2315 train_time:113507ms step_avg:60.60ms
step:1874/2315 train_time:113568ms step_avg:60.60ms
step:1875/2315 train_time:113629ms step_avg:60.60ms
step:1876/2315 train_time:113690ms step_avg:60.60ms
step:1877/2315 train_time:113752ms step_avg:60.60ms
step:1878/2315 train_time:113814ms step_avg:60.60ms
step:1879/2315 train_time:113876ms step_avg:60.60ms
step:1880/2315 train_time:113938ms step_avg:60.61ms
step:1881/2315 train_time:113999ms step_avg:60.61ms
step:1882/2315 train_time:114060ms step_avg:60.61ms
step:1883/2315 train_time:114122ms step_avg:60.61ms
step:1884/2315 train_time:114183ms step_avg:60.61ms
step:1885/2315 train_time:114244ms step_avg:60.61ms
step:1886/2315 train_time:114305ms step_avg:60.61ms
step:1887/2315 train_time:114365ms step_avg:60.61ms
step:1888/2315 train_time:114426ms step_avg:60.61ms
step:1889/2315 train_time:114487ms step_avg:60.61ms
step:1890/2315 train_time:114548ms step_avg:60.61ms
step:1891/2315 train_time:114609ms step_avg:60.61ms
step:1892/2315 train_time:114670ms step_avg:60.61ms
step:1893/2315 train_time:114732ms step_avg:60.61ms
step:1894/2315 train_time:114793ms step_avg:60.61ms
step:1895/2315 train_time:114855ms step_avg:60.61ms
step:1896/2315 train_time:114916ms step_avg:60.61ms
step:1897/2315 train_time:114978ms step_avg:60.61ms
step:1898/2315 train_time:115039ms step_avg:60.61ms
step:1899/2315 train_time:115100ms step_avg:60.61ms
step:1900/2315 train_time:115162ms step_avg:60.61ms
step:1901/2315 train_time:115223ms step_avg:60.61ms
step:1902/2315 train_time:115284ms step_avg:60.61ms
step:1903/2315 train_time:115345ms step_avg:60.61ms
step:1904/2315 train_time:115406ms step_avg:60.61ms
step:1905/2315 train_time:115466ms step_avg:60.61ms
step:1906/2315 train_time:115527ms step_avg:60.61ms
step:1907/2315 train_time:115588ms step_avg:60.61ms
step:1908/2315 train_time:115649ms step_avg:60.61ms
step:1909/2315 train_time:115710ms step_avg:60.61ms
step:1910/2315 train_time:115771ms step_avg:60.61ms
step:1911/2315 train_time:115832ms step_avg:60.61ms
step:1912/2315 train_time:115893ms step_avg:60.61ms
step:1913/2315 train_time:115956ms step_avg:60.61ms
step:1914/2315 train_time:116017ms step_avg:60.61ms
step:1915/2315 train_time:116078ms step_avg:60.62ms
step:1916/2315 train_time:116139ms step_avg:60.62ms
step:1917/2315 train_time:116200ms step_avg:60.62ms
step:1918/2315 train_time:116261ms step_avg:60.62ms
step:1919/2315 train_time:116323ms step_avg:60.62ms
step:1920/2315 train_time:116384ms step_avg:60.62ms
step:1921/2315 train_time:116445ms step_avg:60.62ms
step:1922/2315 train_time:116506ms step_avg:60.62ms
step:1923/2315 train_time:116566ms step_avg:60.62ms
step:1924/2315 train_time:116628ms step_avg:60.62ms
step:1925/2315 train_time:116689ms step_avg:60.62ms
step:1926/2315 train_time:116749ms step_avg:60.62ms
step:1927/2315 train_time:116811ms step_avg:60.62ms
step:1928/2315 train_time:116872ms step_avg:60.62ms
step:1929/2315 train_time:116934ms step_avg:60.62ms
step:1930/2315 train_time:116996ms step_avg:60.62ms
step:1931/2315 train_time:117058ms step_avg:60.62ms
step:1932/2315 train_time:117119ms step_avg:60.62ms
step:1933/2315 train_time:117181ms step_avg:60.62ms
step:1934/2315 train_time:117242ms step_avg:60.62ms
step:1935/2315 train_time:117303ms step_avg:60.62ms
step:1936/2315 train_time:117364ms step_avg:60.62ms
step:1937/2315 train_time:117426ms step_avg:60.62ms
step:1938/2315 train_time:117486ms step_avg:60.62ms
step:1939/2315 train_time:117547ms step_avg:60.62ms
step:1940/2315 train_time:117608ms step_avg:60.62ms
step:1941/2315 train_time:117669ms step_avg:60.62ms
step:1942/2315 train_time:117730ms step_avg:60.62ms
step:1943/2315 train_time:117792ms step_avg:60.62ms
step:1944/2315 train_time:117853ms step_avg:60.62ms
step:1945/2315 train_time:117915ms step_avg:60.62ms
step:1946/2315 train_time:117976ms step_avg:60.62ms
step:1947/2315 train_time:118038ms step_avg:60.63ms
step:1948/2315 train_time:118099ms step_avg:60.63ms
step:1949/2315 train_time:118160ms step_avg:60.63ms
step:1950/2315 train_time:118221ms step_avg:60.63ms
step:1951/2315 train_time:118283ms step_avg:60.63ms
step:1952/2315 train_time:118344ms step_avg:60.63ms
step:1953/2315 train_time:118405ms step_avg:60.63ms
step:1954/2315 train_time:118466ms step_avg:60.63ms
step:1955/2315 train_time:118527ms step_avg:60.63ms
step:1956/2315 train_time:118587ms step_avg:60.63ms
step:1957/2315 train_time:118649ms step_avg:60.63ms
step:1958/2315 train_time:118709ms step_avg:60.63ms
step:1959/2315 train_time:118770ms step_avg:60.63ms
step:1960/2315 train_time:118832ms step_avg:60.63ms
step:1961/2315 train_time:118894ms step_avg:60.63ms
step:1962/2315 train_time:118955ms step_avg:60.63ms
step:1963/2315 train_time:119017ms step_avg:60.63ms
step:1964/2315 train_time:119078ms step_avg:60.63ms
step:1965/2315 train_time:119139ms step_avg:60.63ms
step:1966/2315 train_time:119200ms step_avg:60.63ms
step:1967/2315 train_time:119262ms step_avg:60.63ms
step:1968/2315 train_time:119323ms step_avg:60.63ms
step:1969/2315 train_time:119384ms step_avg:60.63ms
step:1970/2315 train_time:119444ms step_avg:60.63ms
step:1971/2315 train_time:119506ms step_avg:60.63ms
step:1972/2315 train_time:119567ms step_avg:60.63ms
step:1973/2315 train_time:119628ms step_avg:60.63ms
step:1974/2315 train_time:119689ms step_avg:60.63ms
step:1975/2315 train_time:119750ms step_avg:60.63ms
step:1976/2315 train_time:119811ms step_avg:60.63ms
step:1977/2315 train_time:119873ms step_avg:60.63ms
step:1978/2315 train_time:119934ms step_avg:60.63ms
step:1979/2315 train_time:119996ms step_avg:60.63ms
step:1980/2315 train_time:120057ms step_avg:60.63ms
step:1981/2315 train_time:120118ms step_avg:60.64ms
step:1982/2315 train_time:120180ms step_avg:60.64ms
step:1983/2315 train_time:120241ms step_avg:60.64ms
step:1984/2315 train_time:120302ms step_avg:60.64ms
step:1985/2315 train_time:120364ms step_avg:60.64ms
step:1986/2315 train_time:120425ms step_avg:60.64ms
step:1987/2315 train_time:120486ms step_avg:60.64ms
step:1988/2315 train_time:120546ms step_avg:60.64ms
step:1989/2315 train_time:120607ms step_avg:60.64ms
step:1990/2315 train_time:120668ms step_avg:60.64ms
step:1991/2315 train_time:120729ms step_avg:60.64ms
step:1992/2315 train_time:120790ms step_avg:60.64ms
step:1993/2315 train_time:120853ms step_avg:60.64ms
step:1994/2315 train_time:120915ms step_avg:60.64ms
step:1995/2315 train_time:120976ms step_avg:60.64ms
step:1996/2315 train_time:121037ms step_avg:60.64ms
step:1997/2315 train_time:121099ms step_avg:60.64ms
step:1998/2315 train_time:121160ms step_avg:60.64ms
step:1999/2315 train_time:121221ms step_avg:60.64ms
step:2000/2315 train_time:121282ms step_avg:60.64ms
step:2000/2315 val_loss:3.3315 train_time:121345ms step_avg:60.67ms
step:2001/2315 train_time:121366ms step_avg:60.65ms
step:2002/2315 train_time:121408ms step_avg:60.64ms
step:2003/2315 train_time:121474ms step_avg:60.65ms
step:2004/2315 train_time:121538ms step_avg:60.65ms
step:2005/2315 train_time:121600ms step_avg:60.65ms
step:2006/2315 train_time:121662ms step_avg:60.65ms
step:2007/2315 train_time:121723ms step_avg:60.65ms
step:2008/2315 train_time:121783ms step_avg:60.65ms
step:2009/2315 train_time:121844ms step_avg:60.65ms
step:2010/2315 train_time:121904ms step_avg:60.65ms
step:2011/2315 train_time:121966ms step_avg:60.65ms
step:2012/2315 train_time:122027ms step_avg:60.65ms
step:2013/2315 train_time:122087ms step_avg:60.65ms
step:2014/2315 train_time:122147ms step_avg:60.65ms
step:2015/2315 train_time:122207ms step_avg:60.65ms
step:2016/2315 train_time:122268ms step_avg:60.65ms
step:2017/2315 train_time:122329ms step_avg:60.65ms
step:2018/2315 train_time:122391ms step_avg:60.65ms
step:2019/2315 train_time:122453ms step_avg:60.65ms
step:2020/2315 train_time:122516ms step_avg:60.65ms
step:2021/2315 train_time:122579ms step_avg:60.65ms
step:2022/2315 train_time:122640ms step_avg:60.65ms
step:2023/2315 train_time:122701ms step_avg:60.65ms
step:2024/2315 train_time:122762ms step_avg:60.65ms
step:2025/2315 train_time:122823ms step_avg:60.65ms
step:2026/2315 train_time:122884ms step_avg:60.65ms
step:2027/2315 train_time:122945ms step_avg:60.65ms
step:2028/2315 train_time:123005ms step_avg:60.65ms
step:2029/2315 train_time:123066ms step_avg:60.65ms
step:2030/2315 train_time:123127ms step_avg:60.65ms
step:2031/2315 train_time:123187ms step_avg:60.65ms
step:2032/2315 train_time:123248ms step_avg:60.65ms
step:2033/2315 train_time:123309ms step_avg:60.65ms
step:2034/2315 train_time:123370ms step_avg:60.65ms
step:2035/2315 train_time:123432ms step_avg:60.65ms
step:2036/2315 train_time:123493ms step_avg:60.65ms
step:2037/2315 train_time:123555ms step_avg:60.66ms
step:2038/2315 train_time:123616ms step_avg:60.66ms
step:2039/2315 train_time:123678ms step_avg:60.66ms
step:2040/2315 train_time:123740ms step_avg:60.66ms
step:2041/2315 train_time:123801ms step_avg:60.66ms
step:2042/2315 train_time:123862ms step_avg:60.66ms
step:2043/2315 train_time:123924ms step_avg:60.66ms
step:2044/2315 train_time:123985ms step_avg:60.66ms
step:2045/2315 train_time:124046ms step_avg:60.66ms
step:2046/2315 train_time:124107ms step_avg:60.66ms
step:2047/2315 train_time:124168ms step_avg:60.66ms
step:2048/2315 train_time:124228ms step_avg:60.66ms
step:2049/2315 train_time:124290ms step_avg:60.66ms
step:2050/2315 train_time:124351ms step_avg:60.66ms
step:2051/2315 train_time:124412ms step_avg:60.66ms
step:2052/2315 train_time:124473ms step_avg:60.66ms
step:2053/2315 train_time:124535ms step_avg:60.66ms
step:2054/2315 train_time:124596ms step_avg:60.66ms
step:2055/2315 train_time:124658ms step_avg:60.66ms
step:2056/2315 train_time:124720ms step_avg:60.66ms
step:2057/2315 train_time:124782ms step_avg:60.66ms
step:2058/2315 train_time:124843ms step_avg:60.66ms
step:2059/2315 train_time:124904ms step_avg:60.66ms
step:2060/2315 train_time:124965ms step_avg:60.66ms
step:2061/2315 train_time:125026ms step_avg:60.66ms
step:2062/2315 train_time:125087ms step_avg:60.66ms
step:2063/2315 train_time:125148ms step_avg:60.66ms
step:2064/2315 train_time:125209ms step_avg:60.66ms
step:2065/2315 train_time:125270ms step_avg:60.66ms
step:2066/2315 train_time:125331ms step_avg:60.66ms
step:2067/2315 train_time:125392ms step_avg:60.66ms
step:2068/2315 train_time:125453ms step_avg:60.66ms
step:2069/2315 train_time:125514ms step_avg:60.66ms
step:2070/2315 train_time:125576ms step_avg:60.66ms
step:2071/2315 train_time:125638ms step_avg:60.67ms
step:2072/2315 train_time:125699ms step_avg:60.67ms
step:2073/2315 train_time:125761ms step_avg:60.67ms
step:2074/2315 train_time:125822ms step_avg:60.67ms
step:2075/2315 train_time:125883ms step_avg:60.67ms
step:2076/2315 train_time:125944ms step_avg:60.67ms
step:2077/2315 train_time:126005ms step_avg:60.67ms
step:2078/2315 train_time:126066ms step_avg:60.67ms
step:2079/2315 train_time:126127ms step_avg:60.67ms
step:2080/2315 train_time:126188ms step_avg:60.67ms
step:2081/2315 train_time:126248ms step_avg:60.67ms
step:2082/2315 train_time:126309ms step_avg:60.67ms
step:2083/2315 train_time:126371ms step_avg:60.67ms
step:2084/2315 train_time:126431ms step_avg:60.67ms
step:2085/2315 train_time:126493ms step_avg:60.67ms
step:2086/2315 train_time:126554ms step_avg:60.67ms
step:2087/2315 train_time:126615ms step_avg:60.67ms
step:2088/2315 train_time:126676ms step_avg:60.67ms
step:2089/2315 train_time:126738ms step_avg:60.67ms
step:2090/2315 train_time:126800ms step_avg:60.67ms
step:2091/2315 train_time:126862ms step_avg:60.67ms
step:2092/2315 train_time:126923ms step_avg:60.67ms
step:2093/2315 train_time:126985ms step_avg:60.67ms
step:2094/2315 train_time:127046ms step_avg:60.67ms
step:2095/2315 train_time:127107ms step_avg:60.67ms
step:2096/2315 train_time:127168ms step_avg:60.67ms
step:2097/2315 train_time:127229ms step_avg:60.67ms
step:2098/2315 train_time:127291ms step_avg:60.67ms
step:2099/2315 train_time:127351ms step_avg:60.67ms
step:2100/2315 train_time:127412ms step_avg:60.67ms
step:2101/2315 train_time:127473ms step_avg:60.67ms
step:2102/2315 train_time:127534ms step_avg:60.67ms
step:2103/2315 train_time:127595ms step_avg:60.67ms
step:2104/2315 train_time:127656ms step_avg:60.67ms
step:2105/2315 train_time:127718ms step_avg:60.67ms
step:2106/2315 train_time:127780ms step_avg:60.67ms
step:2107/2315 train_time:127842ms step_avg:60.67ms
step:2108/2315 train_time:127903ms step_avg:60.68ms
step:2109/2315 train_time:127965ms step_avg:60.68ms
step:2110/2315 train_time:128026ms step_avg:60.68ms
step:2111/2315 train_time:128087ms step_avg:60.68ms
step:2112/2315 train_time:128148ms step_avg:60.68ms
step:2113/2315 train_time:128209ms step_avg:60.68ms
step:2114/2315 train_time:128270ms step_avg:60.68ms
step:2115/2315 train_time:128332ms step_avg:60.68ms
step:2116/2315 train_time:128392ms step_avg:60.68ms
step:2117/2315 train_time:128453ms step_avg:60.68ms
step:2118/2315 train_time:128514ms step_avg:60.68ms
step:2119/2315 train_time:128576ms step_avg:60.68ms
step:2120/2315 train_time:128637ms step_avg:60.68ms
step:2121/2315 train_time:128699ms step_avg:60.68ms
step:2122/2315 train_time:128759ms step_avg:60.68ms
step:2123/2315 train_time:128821ms step_avg:60.68ms
step:2124/2315 train_time:128882ms step_avg:60.68ms
step:2125/2315 train_time:128944ms step_avg:60.68ms
step:2126/2315 train_time:129006ms step_avg:60.68ms
step:2127/2315 train_time:129067ms step_avg:60.68ms
step:2128/2315 train_time:129128ms step_avg:60.68ms
step:2129/2315 train_time:129189ms step_avg:60.68ms
step:2130/2315 train_time:129250ms step_avg:60.68ms
step:2131/2315 train_time:129311ms step_avg:60.68ms
step:2132/2315 train_time:129372ms step_avg:60.68ms
step:2133/2315 train_time:129433ms step_avg:60.68ms
step:2134/2315 train_time:129494ms step_avg:60.68ms
step:2135/2315 train_time:129555ms step_avg:60.68ms
step:2136/2315 train_time:129616ms step_avg:60.68ms
step:2137/2315 train_time:129678ms step_avg:60.68ms
step:2138/2315 train_time:129739ms step_avg:60.68ms
step:2139/2315 train_time:129801ms step_avg:60.68ms
step:2140/2315 train_time:129862ms step_avg:60.68ms
step:2141/2315 train_time:129924ms step_avg:60.68ms
step:2142/2315 train_time:129985ms step_avg:60.68ms
step:2143/2315 train_time:130047ms step_avg:60.68ms
step:2144/2315 train_time:130108ms step_avg:60.68ms
step:2145/2315 train_time:130169ms step_avg:60.68ms
step:2146/2315 train_time:130230ms step_avg:60.69ms
step:2147/2315 train_time:130291ms step_avg:60.69ms
step:2148/2315 train_time:130352ms step_avg:60.69ms
step:2149/2315 train_time:130414ms step_avg:60.69ms
step:2150/2315 train_time:130474ms step_avg:60.69ms
step:2151/2315 train_time:130536ms step_avg:60.69ms
step:2152/2315 train_time:130597ms step_avg:60.69ms
step:2153/2315 train_time:130658ms step_avg:60.69ms
step:2154/2315 train_time:130719ms step_avg:60.69ms
step:2155/2315 train_time:130780ms step_avg:60.69ms
step:2156/2315 train_time:130841ms step_avg:60.69ms
step:2157/2315 train_time:130903ms step_avg:60.69ms
step:2158/2315 train_time:130964ms step_avg:60.69ms
step:2159/2315 train_time:131026ms step_avg:60.69ms
step:2160/2315 train_time:131087ms step_avg:60.69ms
step:2161/2315 train_time:131149ms step_avg:60.69ms
step:2162/2315 train_time:131211ms step_avg:60.69ms
step:2163/2315 train_time:131272ms step_avg:60.69ms
step:2164/2315 train_time:131333ms step_avg:60.69ms
step:2165/2315 train_time:131394ms step_avg:60.69ms
step:2166/2315 train_time:131455ms step_avg:60.69ms
step:2167/2315 train_time:131517ms step_avg:60.69ms
step:2168/2315 train_time:131578ms step_avg:60.69ms
step:2169/2315 train_time:131639ms step_avg:60.69ms
step:2170/2315 train_time:131700ms step_avg:60.69ms
step:2171/2315 train_time:131762ms step_avg:60.69ms
step:2172/2315 train_time:131823ms step_avg:60.69ms
step:2173/2315 train_time:131884ms step_avg:60.69ms
step:2174/2315 train_time:131946ms step_avg:60.69ms
step:2175/2315 train_time:132007ms step_avg:60.69ms
step:2176/2315 train_time:132068ms step_avg:60.69ms
step:2177/2315 train_time:132129ms step_avg:60.69ms
step:2178/2315 train_time:132190ms step_avg:60.69ms
step:2179/2315 train_time:132251ms step_avg:60.69ms
step:2180/2315 train_time:132312ms step_avg:60.69ms
step:2181/2315 train_time:132374ms step_avg:60.69ms
step:2182/2315 train_time:132435ms step_avg:60.69ms
step:2183/2315 train_time:132496ms step_avg:60.69ms
step:2184/2315 train_time:132557ms step_avg:60.69ms
step:2185/2315 train_time:132618ms step_avg:60.69ms
step:2186/2315 train_time:132679ms step_avg:60.69ms
step:2187/2315 train_time:132741ms step_avg:60.70ms
step:2188/2315 train_time:132801ms step_avg:60.70ms
step:2189/2315 train_time:132863ms step_avg:60.70ms
step:2190/2315 train_time:132924ms step_avg:60.70ms
step:2191/2315 train_time:132986ms step_avg:60.70ms
step:2192/2315 train_time:133047ms step_avg:60.70ms
step:2193/2315 train_time:133108ms step_avg:60.70ms
step:2194/2315 train_time:133169ms step_avg:60.70ms
step:2195/2315 train_time:133230ms step_avg:60.70ms
step:2196/2315 train_time:133291ms step_avg:60.70ms
step:2197/2315 train_time:133352ms step_avg:60.70ms
step:2198/2315 train_time:133413ms step_avg:60.70ms
step:2199/2315 train_time:133474ms step_avg:60.70ms
step:2200/2315 train_time:133535ms step_avg:60.70ms
step:2201/2315 train_time:133597ms step_avg:60.70ms
step:2202/2315 train_time:133658ms step_avg:60.70ms
step:2203/2315 train_time:133720ms step_avg:60.70ms
step:2204/2315 train_time:133782ms step_avg:60.70ms
step:2205/2315 train_time:133844ms step_avg:60.70ms
step:2206/2315 train_time:133905ms step_avg:60.70ms
step:2207/2315 train_time:133967ms step_avg:60.70ms
step:2208/2315 train_time:134028ms step_avg:60.70ms
step:2209/2315 train_time:134089ms step_avg:60.70ms
step:2210/2315 train_time:134151ms step_avg:60.70ms
step:2211/2315 train_time:134211ms step_avg:60.70ms
step:2212/2315 train_time:134272ms step_avg:60.70ms
step:2213/2315 train_time:134333ms step_avg:60.70ms
step:2214/2315 train_time:134394ms step_avg:60.70ms
step:2215/2315 train_time:134456ms step_avg:60.70ms
step:2216/2315 train_time:134517ms step_avg:60.70ms
step:2217/2315 train_time:134578ms step_avg:60.70ms
step:2218/2315 train_time:134640ms step_avg:60.70ms
step:2219/2315 train_time:134701ms step_avg:60.70ms
step:2220/2315 train_time:134763ms step_avg:60.70ms
step:2221/2315 train_time:134824ms step_avg:60.70ms
step:2222/2315 train_time:134886ms step_avg:60.70ms
step:2223/2315 train_time:134947ms step_avg:60.70ms
step:2224/2315 train_time:135008ms step_avg:60.70ms
step:2225/2315 train_time:135069ms step_avg:60.71ms
step:2226/2315 train_time:135130ms step_avg:60.71ms
step:2227/2315 train_time:135191ms step_avg:60.71ms
step:2228/2315 train_time:135252ms step_avg:60.71ms
step:2229/2315 train_time:135313ms step_avg:60.71ms
step:2230/2315 train_time:135374ms step_avg:60.71ms
step:2231/2315 train_time:135435ms step_avg:60.71ms
step:2232/2315 train_time:135497ms step_avg:60.71ms
step:2233/2315 train_time:135558ms step_avg:60.71ms
step:2234/2315 train_time:135619ms step_avg:60.71ms
step:2235/2315 train_time:135681ms step_avg:60.71ms
step:2236/2315 train_time:135742ms step_avg:60.71ms
step:2237/2315 train_time:135804ms step_avg:60.71ms
step:2238/2315 train_time:135865ms step_avg:60.71ms
step:2239/2315 train_time:135927ms step_avg:60.71ms
step:2240/2315 train_time:135988ms step_avg:60.71ms
step:2241/2315 train_time:136049ms step_avg:60.71ms
step:2242/2315 train_time:136110ms step_avg:60.71ms
step:2243/2315 train_time:136171ms step_avg:60.71ms
step:2244/2315 train_time:136232ms step_avg:60.71ms
step:2245/2315 train_time:136293ms step_avg:60.71ms
step:2246/2315 train_time:136354ms step_avg:60.71ms
step:2247/2315 train_time:136415ms step_avg:60.71ms
step:2248/2315 train_time:136476ms step_avg:60.71ms
step:2249/2315 train_time:136539ms step_avg:60.71ms
step:2250/2315 train_time:136599ms step_avg:60.71ms
step:2250/2315 val_loss:3.2912 train_time:136662ms step_avg:60.74ms
step:2251/2315 train_time:136682ms step_avg:60.72ms
step:2252/2315 train_time:136725ms step_avg:60.71ms
step:2253/2315 train_time:136790ms step_avg:60.71ms
step:2254/2315 train_time:136855ms step_avg:60.72ms
step:2255/2315 train_time:136916ms step_avg:60.72ms
step:2256/2315 train_time:136977ms step_avg:60.72ms
step:2257/2315 train_time:137037ms step_avg:60.72ms
step:2258/2315 train_time:137098ms step_avg:60.72ms
step:2259/2315 train_time:137159ms step_avg:60.72ms
step:2260/2315 train_time:137219ms step_avg:60.72ms
step:2261/2315 train_time:137280ms step_avg:60.72ms
step:2262/2315 train_time:137341ms step_avg:60.72ms
step:2263/2315 train_time:137401ms step_avg:60.72ms
step:2264/2315 train_time:137461ms step_avg:60.72ms
step:2265/2315 train_time:137522ms step_avg:60.72ms
step:2266/2315 train_time:137583ms step_avg:60.72ms
step:2267/2315 train_time:137645ms step_avg:60.72ms
step:2268/2315 train_time:137708ms step_avg:60.72ms
step:2269/2315 train_time:137771ms step_avg:60.72ms
step:2270/2315 train_time:137832ms step_avg:60.72ms
step:2271/2315 train_time:137894ms step_avg:60.72ms
step:2272/2315 train_time:137956ms step_avg:60.72ms
step:2273/2315 train_time:138017ms step_avg:60.72ms
step:2274/2315 train_time:138078ms step_avg:60.72ms
step:2275/2315 train_time:138138ms step_avg:60.72ms
step:2276/2315 train_time:138199ms step_avg:60.72ms
step:2277/2315 train_time:138260ms step_avg:60.72ms
step:2278/2315 train_time:138321ms step_avg:60.72ms
step:2279/2315 train_time:138382ms step_avg:60.72ms
step:2280/2315 train_time:138443ms step_avg:60.72ms
step:2281/2315 train_time:138504ms step_avg:60.72ms
step:2282/2315 train_time:138565ms step_avg:60.72ms
step:2283/2315 train_time:138626ms step_avg:60.72ms
step:2284/2315 train_time:138688ms step_avg:60.72ms
step:2285/2315 train_time:138749ms step_avg:60.72ms
step:2286/2315 train_time:138811ms step_avg:60.72ms
step:2287/2315 train_time:138873ms step_avg:60.72ms
step:2288/2315 train_time:138934ms step_avg:60.72ms
step:2289/2315 train_time:138995ms step_avg:60.72ms
step:2290/2315 train_time:139056ms step_avg:60.72ms
step:2291/2315 train_time:139117ms step_avg:60.72ms
step:2292/2315 train_time:139178ms step_avg:60.72ms
step:2293/2315 train_time:139239ms step_avg:60.72ms
step:2294/2315 train_time:139300ms step_avg:60.72ms
step:2295/2315 train_time:139361ms step_avg:60.72ms
step:2296/2315 train_time:139422ms step_avg:60.72ms
step:2297/2315 train_time:139483ms step_avg:60.72ms
step:2298/2315 train_time:139544ms step_avg:60.72ms
step:2299/2315 train_time:139606ms step_avg:60.72ms
step:2300/2315 train_time:139667ms step_avg:60.72ms
step:2301/2315 train_time:139729ms step_avg:60.73ms
step:2302/2315 train_time:139790ms step_avg:60.73ms
step:2303/2315 train_time:139852ms step_avg:60.73ms
step:2304/2315 train_time:139914ms step_avg:60.73ms
step:2305/2315 train_time:139975ms step_avg:60.73ms
step:2306/2315 train_time:140036ms step_avg:60.73ms
step:2307/2315 train_time:140097ms step_avg:60.73ms
step:2308/2315 train_time:140158ms step_avg:60.73ms
step:2309/2315 train_time:140219ms step_avg:60.73ms
step:2310/2315 train_time:140280ms step_avg:60.73ms
step:2311/2315 train_time:140341ms step_avg:60.73ms
step:2312/2315 train_time:140402ms step_avg:60.73ms
step:2313/2315 train_time:140463ms step_avg:60.73ms
step:2314/2315 train_time:140524ms step_avg:60.73ms
step:2315/2315 train_time:140586ms step_avg:60.73ms
step:2315/2315 val_loss:3.2786 train_time:140648ms step_avg:60.76ms
peak memory allocated: 29512 MiB reserved: 44076 MiB

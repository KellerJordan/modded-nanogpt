import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]
# polar_express_coeffs = [
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
# ]



@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    # state["second_momentum_buffer"] = torch.zeros_like(grad[...,0:1]) if param.size(-2) >= param.size(-1) else torch.zeros_like(grad[...,0:1,:])
                    # state["second_momentum_buffer"] = torch.zeros((*grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((*grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)
            
            ###################################
            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))
            ####################################


            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> (-1.5)  0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Oct 22 2025, 02:20:45) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251022+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 24 06:16:09 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000001:00:00.0 Off |                    0 |
| N/A   35C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000002:00:00.0 Off |                    0 |
| N/A   35C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000003:00:00.0 Off |                    0 |
| N/A   30C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000008:00:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000009:00:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   0000000A:00:00.0 Off |                    0 |
| N/A   35C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   0000000C:00:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2315 train_time:82ms step_avg:81.58ms
step:2/2315 train_time:182ms step_avg:90.82ms
step:3/2315 train_time:205ms step_avg:68.22ms
step:4/2315 train_time:239ms step_avg:59.84ms
step:5/2315 train_time:298ms step_avg:59.51ms
step:6/2315 train_time:357ms step_avg:59.56ms
step:7/2315 train_time:416ms step_avg:59.49ms
step:8/2315 train_time:476ms step_avg:59.51ms
step:9/2315 train_time:536ms step_avg:59.54ms
step:10/2315 train_time:596ms step_avg:59.56ms
step:11/2315 train_time:655ms step_avg:59.58ms
step:12/2315 train_time:715ms step_avg:59.61ms
step:13/2315 train_time:775ms step_avg:59.62ms
step:14/2315 train_time:835ms step_avg:59.66ms
step:15/2315 train_time:895ms step_avg:59.64ms
step:16/2315 train_time:955ms step_avg:59.67ms
step:17/2315 train_time:1014ms step_avg:59.65ms
step:18/2315 train_time:1076ms step_avg:59.76ms
step:19/2315 train_time:1139ms step_avg:59.93ms
step:20/2315 train_time:1200ms step_avg:60.01ms
step:21/2315 train_time:1260ms step_avg:60.01ms
step:22/2315 train_time:1321ms step_avg:60.04ms
step:23/2315 train_time:1381ms step_avg:60.05ms
step:24/2315 train_time:1442ms step_avg:60.06ms
step:25/2315 train_time:1502ms step_avg:60.06ms
step:26/2315 train_time:1561ms step_avg:60.05ms
step:27/2315 train_time:1621ms step_avg:60.05ms
step:28/2315 train_time:1682ms step_avg:60.07ms
step:29/2315 train_time:1742ms step_avg:60.07ms
step:30/2315 train_time:1802ms step_avg:60.08ms
step:31/2315 train_time:1863ms step_avg:60.09ms
step:32/2315 train_time:1923ms step_avg:60.10ms
step:33/2315 train_time:1984ms step_avg:60.13ms
step:34/2315 train_time:2046ms step_avg:60.16ms
step:35/2315 train_time:2107ms step_avg:60.21ms
step:36/2315 train_time:2168ms step_avg:60.23ms
step:37/2315 train_time:2229ms step_avg:60.25ms
step:38/2315 train_time:2290ms step_avg:60.26ms
step:39/2315 train_time:2351ms step_avg:60.29ms
step:40/2315 train_time:2412ms step_avg:60.29ms
step:41/2315 train_time:2472ms step_avg:60.30ms
step:42/2315 train_time:2533ms step_avg:60.31ms
step:43/2315 train_time:2593ms step_avg:60.31ms
step:44/2315 train_time:2654ms step_avg:60.32ms
step:45/2315 train_time:2714ms step_avg:60.32ms
step:46/2315 train_time:2774ms step_avg:60.31ms
step:47/2315 train_time:2836ms step_avg:60.33ms
step:48/2315 train_time:2896ms step_avg:60.33ms
step:49/2315 train_time:2956ms step_avg:60.33ms
step:50/2315 train_time:3016ms step_avg:60.33ms
step:51/2315 train_time:3077ms step_avg:60.33ms
step:52/2315 train_time:3137ms step_avg:60.33ms
step:53/2315 train_time:3197ms step_avg:60.32ms
step:54/2315 train_time:3256ms step_avg:60.31ms
step:55/2315 train_time:3316ms step_avg:60.30ms
step:56/2315 train_time:3376ms step_avg:60.29ms
step:57/2315 train_time:3436ms step_avg:60.28ms
step:58/2315 train_time:3496ms step_avg:60.28ms
step:59/2315 train_time:3556ms step_avg:60.28ms
step:60/2315 train_time:3617ms step_avg:60.28ms
step:61/2315 train_time:3677ms step_avg:60.28ms
step:62/2315 train_time:3736ms step_avg:60.26ms
step:63/2315 train_time:3797ms step_avg:60.26ms
step:64/2315 train_time:3856ms step_avg:60.26ms
step:65/2315 train_time:3916ms step_avg:60.25ms
step:66/2315 train_time:3976ms step_avg:60.25ms
step:67/2315 train_time:4036ms step_avg:60.24ms
step:68/2315 train_time:4096ms step_avg:60.24ms
step:69/2315 train_time:4156ms step_avg:60.23ms
step:70/2315 train_time:4216ms step_avg:60.23ms
step:71/2315 train_time:4276ms step_avg:60.22ms
step:72/2315 train_time:4335ms step_avg:60.21ms
step:73/2315 train_time:4395ms step_avg:60.21ms
step:74/2315 train_time:4455ms step_avg:60.20ms
step:75/2315 train_time:4515ms step_avg:60.20ms
step:76/2315 train_time:4575ms step_avg:60.19ms
step:77/2315 train_time:4635ms step_avg:60.20ms
step:78/2315 train_time:4696ms step_avg:60.20ms
step:79/2315 train_time:4756ms step_avg:60.20ms
step:80/2315 train_time:4816ms step_avg:60.20ms
step:81/2315 train_time:4876ms step_avg:60.20ms
step:82/2315 train_time:4935ms step_avg:60.19ms
step:83/2315 train_time:4995ms step_avg:60.18ms
step:84/2315 train_time:5054ms step_avg:60.17ms
step:85/2315 train_time:5115ms step_avg:60.17ms
step:86/2315 train_time:5175ms step_avg:60.17ms
step:87/2315 train_time:5235ms step_avg:60.17ms
step:88/2315 train_time:5294ms step_avg:60.16ms
step:89/2315 train_time:5355ms step_avg:60.17ms
step:90/2315 train_time:5414ms step_avg:60.16ms
step:91/2315 train_time:5474ms step_avg:60.16ms
step:92/2315 train_time:5535ms step_avg:60.16ms
step:93/2315 train_time:5595ms step_avg:60.16ms
step:94/2315 train_time:5655ms step_avg:60.16ms
step:95/2315 train_time:5714ms step_avg:60.15ms
step:96/2315 train_time:5774ms step_avg:60.15ms
step:97/2315 train_time:5834ms step_avg:60.15ms
step:98/2315 train_time:5895ms step_avg:60.16ms
step:99/2315 train_time:5955ms step_avg:60.15ms
step:100/2315 train_time:6015ms step_avg:60.15ms
step:101/2315 train_time:6075ms step_avg:60.15ms
step:102/2315 train_time:6136ms step_avg:60.15ms
step:103/2315 train_time:6196ms step_avg:60.15ms
step:104/2315 train_time:6256ms step_avg:60.15ms
step:105/2315 train_time:6316ms step_avg:60.15ms
step:106/2315 train_time:6376ms step_avg:60.15ms
step:107/2315 train_time:6435ms step_avg:60.14ms
step:108/2315 train_time:6496ms step_avg:60.14ms
step:109/2315 train_time:6556ms step_avg:60.14ms
step:110/2315 train_time:6616ms step_avg:60.14ms
step:111/2315 train_time:6676ms step_avg:60.14ms
step:112/2315 train_time:6737ms step_avg:60.15ms
step:113/2315 train_time:6796ms step_avg:60.14ms
step:114/2315 train_time:6856ms step_avg:60.14ms
step:115/2315 train_time:6915ms step_avg:60.13ms
step:116/2315 train_time:6975ms step_avg:60.13ms
step:117/2315 train_time:7034ms step_avg:60.12ms
step:118/2315 train_time:7094ms step_avg:60.12ms
step:119/2315 train_time:7154ms step_avg:60.12ms
step:120/2315 train_time:7214ms step_avg:60.12ms
step:121/2315 train_time:7274ms step_avg:60.11ms
step:122/2315 train_time:7334ms step_avg:60.11ms
step:123/2315 train_time:7394ms step_avg:60.12ms
step:124/2315 train_time:7454ms step_avg:60.11ms
step:125/2315 train_time:7514ms step_avg:60.11ms
step:126/2315 train_time:7574ms step_avg:60.11ms
step:127/2315 train_time:7634ms step_avg:60.11ms
step:128/2315 train_time:7695ms step_avg:60.12ms
step:129/2315 train_time:7754ms step_avg:60.11ms
step:130/2315 train_time:7814ms step_avg:60.11ms
step:131/2315 train_time:7875ms step_avg:60.11ms
step:132/2315 train_time:7935ms step_avg:60.11ms
step:133/2315 train_time:7995ms step_avg:60.11ms
step:134/2315 train_time:8054ms step_avg:60.10ms
step:135/2315 train_time:8114ms step_avg:60.10ms
step:136/2315 train_time:8175ms step_avg:60.11ms
step:137/2315 train_time:8234ms step_avg:60.10ms
step:138/2315 train_time:8294ms step_avg:60.10ms
step:139/2315 train_time:8354ms step_avg:60.10ms
step:140/2315 train_time:8414ms step_avg:60.10ms
step:141/2315 train_time:8474ms step_avg:60.10ms
step:142/2315 train_time:8534ms step_avg:60.10ms
step:143/2315 train_time:8594ms step_avg:60.10ms
step:144/2315 train_time:8654ms step_avg:60.10ms
step:145/2315 train_time:8714ms step_avg:60.10ms
step:146/2315 train_time:8774ms step_avg:60.10ms
step:147/2315 train_time:8834ms step_avg:60.10ms
step:148/2315 train_time:8894ms step_avg:60.09ms
step:149/2315 train_time:8954ms step_avg:60.09ms
step:150/2315 train_time:9013ms step_avg:60.09ms
step:151/2315 train_time:9073ms step_avg:60.09ms
step:152/2315 train_time:9133ms step_avg:60.09ms
step:153/2315 train_time:9193ms step_avg:60.08ms
step:154/2315 train_time:9253ms step_avg:60.09ms
step:155/2315 train_time:9313ms step_avg:60.08ms
step:156/2315 train_time:9374ms step_avg:60.09ms
step:157/2315 train_time:9433ms step_avg:60.08ms
step:158/2315 train_time:9494ms step_avg:60.09ms
step:159/2315 train_time:9554ms step_avg:60.09ms
step:160/2315 train_time:9614ms step_avg:60.09ms
step:161/2315 train_time:9673ms step_avg:60.08ms
step:162/2315 train_time:9734ms step_avg:60.09ms
step:163/2315 train_time:9794ms step_avg:60.08ms
step:164/2315 train_time:9854ms step_avg:60.09ms
step:165/2315 train_time:9914ms step_avg:60.09ms
step:166/2315 train_time:9974ms step_avg:60.09ms
step:167/2315 train_time:10035ms step_avg:60.09ms
step:168/2315 train_time:10094ms step_avg:60.09ms
step:169/2315 train_time:10154ms step_avg:60.08ms
step:170/2315 train_time:10213ms step_avg:60.08ms
step:171/2315 train_time:10273ms step_avg:60.08ms
step:172/2315 train_time:10333ms step_avg:60.08ms
step:173/2315 train_time:10393ms step_avg:60.08ms
step:174/2315 train_time:10453ms step_avg:60.08ms
step:175/2315 train_time:10513ms step_avg:60.07ms
step:176/2315 train_time:10573ms step_avg:60.07ms
step:177/2315 train_time:10633ms step_avg:60.07ms
step:178/2315 train_time:10693ms step_avg:60.07ms
step:179/2315 train_time:10753ms step_avg:60.07ms
step:180/2315 train_time:10813ms step_avg:60.07ms
step:181/2315 train_time:10873ms step_avg:60.07ms
step:182/2315 train_time:10933ms step_avg:60.07ms
step:183/2315 train_time:10993ms step_avg:60.07ms
step:184/2315 train_time:11053ms step_avg:60.07ms
step:185/2315 train_time:11113ms step_avg:60.07ms
step:186/2315 train_time:11173ms step_avg:60.07ms
step:187/2315 train_time:11233ms step_avg:60.07ms
step:188/2315 train_time:11293ms step_avg:60.07ms
step:189/2315 train_time:11352ms step_avg:60.07ms
step:190/2315 train_time:11412ms step_avg:60.06ms
step:191/2315 train_time:11472ms step_avg:60.06ms
step:192/2315 train_time:11532ms step_avg:60.06ms
step:193/2315 train_time:11592ms step_avg:60.06ms
step:194/2315 train_time:11651ms step_avg:60.06ms
step:195/2315 train_time:11712ms step_avg:60.06ms
step:196/2315 train_time:11772ms step_avg:60.06ms
step:197/2315 train_time:11832ms step_avg:60.06ms
step:198/2315 train_time:11893ms step_avg:60.06ms
step:199/2315 train_time:11952ms step_avg:60.06ms
step:200/2315 train_time:12012ms step_avg:60.06ms
step:201/2315 train_time:12073ms step_avg:60.06ms
step:202/2315 train_time:12134ms step_avg:60.07ms
step:203/2315 train_time:12194ms step_avg:60.07ms
step:204/2315 train_time:12254ms step_avg:60.07ms
step:205/2315 train_time:12314ms step_avg:60.07ms
step:206/2315 train_time:12374ms step_avg:60.07ms
step:207/2315 train_time:12435ms step_avg:60.07ms
step:208/2315 train_time:12495ms step_avg:60.07ms
step:209/2315 train_time:12560ms step_avg:60.09ms
step:210/2315 train_time:12615ms step_avg:60.07ms
step:211/2315 train_time:12676ms step_avg:60.07ms
step:212/2315 train_time:12735ms step_avg:60.07ms
step:213/2315 train_time:12795ms step_avg:60.07ms
step:214/2315 train_time:12855ms step_avg:60.07ms
step:215/2315 train_time:12915ms step_avg:60.07ms
step:216/2315 train_time:12975ms step_avg:60.07ms
step:217/2315 train_time:13035ms step_avg:60.07ms
step:218/2315 train_time:13095ms step_avg:60.07ms
step:219/2315 train_time:13155ms step_avg:60.07ms
step:220/2315 train_time:13215ms step_avg:60.07ms
step:221/2315 train_time:13274ms step_avg:60.06ms
step:222/2315 train_time:13334ms step_avg:60.06ms
step:223/2315 train_time:13394ms step_avg:60.06ms
step:224/2315 train_time:13453ms step_avg:60.06ms
step:225/2315 train_time:13513ms step_avg:60.06ms
step:226/2315 train_time:13573ms step_avg:60.06ms
step:227/2315 train_time:13633ms step_avg:60.06ms
step:228/2315 train_time:13693ms step_avg:60.06ms
step:229/2315 train_time:13754ms step_avg:60.06ms
step:230/2315 train_time:13814ms step_avg:60.06ms
step:231/2315 train_time:13874ms step_avg:60.06ms
step:232/2315 train_time:13934ms step_avg:60.06ms
step:233/2315 train_time:13994ms step_avg:60.06ms
step:234/2315 train_time:14053ms step_avg:60.06ms
step:235/2315 train_time:14114ms step_avg:60.06ms
step:236/2315 train_time:14174ms step_avg:60.06ms
step:237/2315 train_time:14234ms step_avg:60.06ms
step:238/2315 train_time:14294ms step_avg:60.06ms
step:239/2315 train_time:14353ms step_avg:60.06ms
step:240/2315 train_time:14413ms step_avg:60.06ms
step:241/2315 train_time:14474ms step_avg:60.06ms
step:242/2315 train_time:14534ms step_avg:60.06ms
step:243/2315 train_time:14595ms step_avg:60.06ms
step:244/2315 train_time:14655ms step_avg:60.06ms
step:245/2315 train_time:14714ms step_avg:60.06ms
step:246/2315 train_time:14774ms step_avg:60.06ms
step:247/2315 train_time:14834ms step_avg:60.06ms
step:248/2315 train_time:14894ms step_avg:60.06ms
step:249/2315 train_time:14953ms step_avg:60.05ms
step:250/2315 train_time:15013ms step_avg:60.05ms
step:250/2315 val_loss:4.0764 train_time:15075ms step_avg:60.30ms
step:251/2315 train_time:15097ms step_avg:60.15ms
step:252/2315 train_time:15135ms step_avg:60.06ms
step:253/2315 train_time:15197ms step_avg:60.07ms
step:254/2315 train_time:15266ms step_avg:60.10ms
step:255/2315 train_time:15328ms step_avg:60.11ms
step:256/2315 train_time:15388ms step_avg:60.11ms
step:257/2315 train_time:15448ms step_avg:60.11ms
step:258/2315 train_time:15507ms step_avg:60.10ms
step:259/2315 train_time:15566ms step_avg:60.10ms
step:260/2315 train_time:15626ms step_avg:60.10ms
step:261/2315 train_time:15685ms step_avg:60.10ms
step:262/2315 train_time:15745ms step_avg:60.09ms
step:263/2315 train_time:15804ms step_avg:60.09ms
step:264/2315 train_time:15864ms step_avg:60.09ms
step:265/2315 train_time:15923ms step_avg:60.09ms
step:266/2315 train_time:15983ms step_avg:60.09ms
step:267/2315 train_time:16044ms step_avg:60.09ms
step:268/2315 train_time:16106ms step_avg:60.10ms
step:269/2315 train_time:16167ms step_avg:60.10ms
step:270/2315 train_time:16229ms step_avg:60.11ms
step:271/2315 train_time:16290ms step_avg:60.11ms
step:272/2315 train_time:16350ms step_avg:60.11ms
step:273/2315 train_time:16410ms step_avg:60.11ms
step:274/2315 train_time:16470ms step_avg:60.11ms
step:275/2315 train_time:16529ms step_avg:60.11ms
step:276/2315 train_time:16588ms step_avg:60.10ms
step:277/2315 train_time:16648ms step_avg:60.10ms
step:278/2315 train_time:16707ms step_avg:60.10ms
step:279/2315 train_time:16767ms step_avg:60.10ms
step:280/2315 train_time:16826ms step_avg:60.09ms
step:281/2315 train_time:16886ms step_avg:60.09ms
step:282/2315 train_time:16945ms step_avg:60.09ms
step:283/2315 train_time:17005ms step_avg:60.09ms
step:284/2315 train_time:17065ms step_avg:60.09ms
step:285/2315 train_time:17126ms step_avg:60.09ms
step:286/2315 train_time:17187ms step_avg:60.09ms
step:287/2315 train_time:17248ms step_avg:60.10ms
step:288/2315 train_time:17309ms step_avg:60.10ms
step:289/2315 train_time:17370ms step_avg:60.10ms
step:290/2315 train_time:17430ms step_avg:60.10ms
step:291/2315 train_time:17490ms step_avg:60.10ms
step:292/2315 train_time:17550ms step_avg:60.10ms
step:293/2315 train_time:17609ms step_avg:60.10ms
step:294/2315 train_time:17669ms step_avg:60.10ms
step:295/2315 train_time:17728ms step_avg:60.09ms
step:296/2315 train_time:17788ms step_avg:60.09ms
step:297/2315 train_time:17847ms step_avg:60.09ms
step:298/2315 train_time:17907ms step_avg:60.09ms
step:299/2315 train_time:17968ms step_avg:60.09ms
step:300/2315 train_time:18029ms step_avg:60.10ms
step:301/2315 train_time:18089ms step_avg:60.09ms
step:302/2315 train_time:18149ms step_avg:60.10ms
step:303/2315 train_time:18211ms step_avg:60.10ms
step:304/2315 train_time:18271ms step_avg:60.10ms
step:305/2315 train_time:18331ms step_avg:60.10ms
step:306/2315 train_time:18390ms step_avg:60.10ms
step:307/2315 train_time:18450ms step_avg:60.10ms
step:308/2315 train_time:18510ms step_avg:60.10ms
step:309/2315 train_time:18570ms step_avg:60.10ms
step:310/2315 train_time:18629ms step_avg:60.09ms
step:311/2315 train_time:18688ms step_avg:60.09ms
step:312/2315 train_time:18748ms step_avg:60.09ms
step:313/2315 train_time:18807ms step_avg:60.09ms
step:314/2315 train_time:18866ms step_avg:60.08ms
step:315/2315 train_time:18926ms step_avg:60.08ms
step:316/2315 train_time:18985ms step_avg:60.08ms
step:317/2315 train_time:19046ms step_avg:60.08ms
step:318/2315 train_time:19106ms step_avg:60.08ms
step:319/2315 train_time:19167ms step_avg:60.08ms
step:320/2315 train_time:19227ms step_avg:60.08ms
step:321/2315 train_time:19287ms step_avg:60.08ms
step:322/2315 train_time:19347ms step_avg:60.08ms
step:323/2315 train_time:19407ms step_avg:60.08ms
step:324/2315 train_time:19467ms step_avg:60.08ms
step:325/2315 train_time:19528ms step_avg:60.09ms
step:326/2315 train_time:19588ms step_avg:60.09ms
step:327/2315 train_time:19648ms step_avg:60.09ms
step:328/2315 train_time:19708ms step_avg:60.09ms
step:329/2315 train_time:19767ms step_avg:60.08ms
step:330/2315 train_time:19827ms step_avg:60.08ms
step:331/2315 train_time:19886ms step_avg:60.08ms
step:332/2315 train_time:19946ms step_avg:60.08ms
step:333/2315 train_time:20007ms step_avg:60.08ms
step:334/2315 train_time:20067ms step_avg:60.08ms
step:335/2315 train_time:20126ms step_avg:60.08ms
step:336/2315 train_time:20187ms step_avg:60.08ms
step:337/2315 train_time:20247ms step_avg:60.08ms
step:338/2315 train_time:20307ms step_avg:60.08ms
step:339/2315 train_time:20368ms step_avg:60.08ms
step:340/2315 train_time:20428ms step_avg:60.08ms
step:341/2315 train_time:20488ms step_avg:60.08ms
step:342/2315 train_time:20548ms step_avg:60.08ms
step:343/2315 train_time:20608ms step_avg:60.08ms
step:344/2315 train_time:20668ms step_avg:60.08ms
step:345/2315 train_time:20728ms step_avg:60.08ms
step:346/2315 train_time:20787ms step_avg:60.08ms
step:347/2315 train_time:20847ms step_avg:60.08ms
step:348/2315 train_time:20908ms step_avg:60.08ms
step:349/2315 train_time:20968ms step_avg:60.08ms
step:350/2315 train_time:21027ms step_avg:60.08ms
step:351/2315 train_time:21087ms step_avg:60.08ms
step:352/2315 train_time:21147ms step_avg:60.08ms
step:353/2315 train_time:21207ms step_avg:60.08ms
step:354/2315 train_time:21267ms step_avg:60.08ms
step:355/2315 train_time:21327ms step_avg:60.08ms
step:356/2315 train_time:21387ms step_avg:60.08ms
step:357/2315 train_time:21448ms step_avg:60.08ms
step:358/2315 train_time:21507ms step_avg:60.08ms
step:359/2315 train_time:21567ms step_avg:60.08ms
step:360/2315 train_time:21627ms step_avg:60.08ms
step:361/2315 train_time:21687ms step_avg:60.08ms
step:362/2315 train_time:21747ms step_avg:60.07ms
step:363/2315 train_time:21807ms step_avg:60.07ms
step:364/2315 train_time:21867ms step_avg:60.07ms
step:365/2315 train_time:21926ms step_avg:60.07ms
step:366/2315 train_time:21986ms step_avg:60.07ms
step:367/2315 train_time:22046ms step_avg:60.07ms
step:368/2315 train_time:22106ms step_avg:60.07ms
step:369/2315 train_time:22166ms step_avg:60.07ms
step:370/2315 train_time:22226ms step_avg:60.07ms
step:371/2315 train_time:22286ms step_avg:60.07ms
step:372/2315 train_time:22346ms step_avg:60.07ms
step:373/2315 train_time:22408ms step_avg:60.07ms
step:374/2315 train_time:22468ms step_avg:60.07ms
step:375/2315 train_time:22528ms step_avg:60.07ms
step:376/2315 train_time:22587ms step_avg:60.07ms
step:377/2315 train_time:22648ms step_avg:60.07ms
step:378/2315 train_time:22708ms step_avg:60.07ms
step:379/2315 train_time:22767ms step_avg:60.07ms
step:380/2315 train_time:22827ms step_avg:60.07ms
step:381/2315 train_time:22887ms step_avg:60.07ms
step:382/2315 train_time:22947ms step_avg:60.07ms
step:383/2315 train_time:23006ms step_avg:60.07ms
step:384/2315 train_time:23066ms step_avg:60.07ms
step:385/2315 train_time:23127ms step_avg:60.07ms
step:386/2315 train_time:23186ms step_avg:60.07ms
step:387/2315 train_time:23247ms step_avg:60.07ms
step:388/2315 train_time:23307ms step_avg:60.07ms
step:389/2315 train_time:23366ms step_avg:60.07ms
step:390/2315 train_time:23427ms step_avg:60.07ms
step:391/2315 train_time:23486ms step_avg:60.07ms
step:392/2315 train_time:23546ms step_avg:60.07ms
step:393/2315 train_time:23606ms step_avg:60.07ms
step:394/2315 train_time:23666ms step_avg:60.07ms
step:395/2315 train_time:23726ms step_avg:60.07ms
step:396/2315 train_time:23786ms step_avg:60.07ms
step:397/2315 train_time:23847ms step_avg:60.07ms
step:398/2315 train_time:23906ms step_avg:60.07ms
step:399/2315 train_time:23966ms step_avg:60.07ms
step:400/2315 train_time:24026ms step_avg:60.07ms
step:401/2315 train_time:24085ms step_avg:60.06ms
step:402/2315 train_time:24145ms step_avg:60.06ms
step:403/2315 train_time:24206ms step_avg:60.06ms
step:404/2315 train_time:24266ms step_avg:60.06ms
step:405/2315 train_time:24326ms step_avg:60.07ms
step:406/2315 train_time:24386ms step_avg:60.06ms
step:407/2315 train_time:24446ms step_avg:60.06ms
step:408/2315 train_time:24507ms step_avg:60.07ms
step:409/2315 train_time:24567ms step_avg:60.07ms
step:410/2315 train_time:24627ms step_avg:60.07ms
step:411/2315 train_time:24687ms step_avg:60.07ms
step:412/2315 train_time:24748ms step_avg:60.07ms
step:413/2315 train_time:24808ms step_avg:60.07ms
step:414/2315 train_time:24868ms step_avg:60.07ms
step:415/2315 train_time:24927ms step_avg:60.07ms
step:416/2315 train_time:24987ms step_avg:60.06ms
step:417/2315 train_time:25047ms step_avg:60.06ms
step:418/2315 train_time:25107ms step_avg:60.06ms
step:419/2315 train_time:25168ms step_avg:60.07ms
step:420/2315 train_time:25228ms step_avg:60.07ms
step:421/2315 train_time:25288ms step_avg:60.07ms
step:422/2315 train_time:25347ms step_avg:60.06ms
step:423/2315 train_time:25407ms step_avg:60.06ms
step:424/2315 train_time:25467ms step_avg:60.06ms
step:425/2315 train_time:25527ms step_avg:60.06ms
step:426/2315 train_time:25587ms step_avg:60.06ms
step:427/2315 train_time:25647ms step_avg:60.06ms
step:428/2315 train_time:25707ms step_avg:60.06ms
step:429/2315 train_time:25768ms step_avg:60.06ms
step:430/2315 train_time:25829ms step_avg:60.07ms
step:431/2315 train_time:25888ms step_avg:60.07ms
step:432/2315 train_time:25948ms step_avg:60.06ms
step:433/2315 train_time:26007ms step_avg:60.06ms
step:434/2315 train_time:26068ms step_avg:60.06ms
step:435/2315 train_time:26127ms step_avg:60.06ms
step:436/2315 train_time:26187ms step_avg:60.06ms
step:437/2315 train_time:26246ms step_avg:60.06ms
step:438/2315 train_time:26306ms step_avg:60.06ms
step:439/2315 train_time:26366ms step_avg:60.06ms
step:440/2315 train_time:26426ms step_avg:60.06ms
step:441/2315 train_time:26486ms step_avg:60.06ms
step:442/2315 train_time:26546ms step_avg:60.06ms
step:443/2315 train_time:26607ms step_avg:60.06ms
step:444/2315 train_time:26667ms step_avg:60.06ms
step:445/2315 train_time:26727ms step_avg:60.06ms
step:446/2315 train_time:26787ms step_avg:60.06ms
step:447/2315 train_time:26847ms step_avg:60.06ms
step:448/2315 train_time:26907ms step_avg:60.06ms
step:449/2315 train_time:26967ms step_avg:60.06ms
step:450/2315 train_time:27027ms step_avg:60.06ms
step:451/2315 train_time:27088ms step_avg:60.06ms
step:452/2315 train_time:27148ms step_avg:60.06ms
step:453/2315 train_time:27208ms step_avg:60.06ms
step:454/2315 train_time:27268ms step_avg:60.06ms
step:455/2315 train_time:27328ms step_avg:60.06ms
step:456/2315 train_time:27387ms step_avg:60.06ms
step:457/2315 train_time:27447ms step_avg:60.06ms
step:458/2315 train_time:27507ms step_avg:60.06ms
step:459/2315 train_time:27567ms step_avg:60.06ms
step:460/2315 train_time:27628ms step_avg:60.06ms
step:461/2315 train_time:27688ms step_avg:60.06ms
step:462/2315 train_time:27748ms step_avg:60.06ms
step:463/2315 train_time:27808ms step_avg:60.06ms
step:464/2315 train_time:27868ms step_avg:60.06ms
step:465/2315 train_time:27927ms step_avg:60.06ms
step:466/2315 train_time:27987ms step_avg:60.06ms
step:467/2315 train_time:28047ms step_avg:60.06ms
step:468/2315 train_time:28107ms step_avg:60.06ms
step:469/2315 train_time:28167ms step_avg:60.06ms
step:470/2315 train_time:28227ms step_avg:60.06ms
step:471/2315 train_time:28287ms step_avg:60.06ms
step:472/2315 train_time:28347ms step_avg:60.06ms
step:473/2315 train_time:28406ms step_avg:60.06ms
step:474/2315 train_time:28467ms step_avg:60.06ms
step:475/2315 train_time:28526ms step_avg:60.06ms
step:476/2315 train_time:28586ms step_avg:60.06ms
step:477/2315 train_time:28646ms step_avg:60.06ms
step:478/2315 train_time:28706ms step_avg:60.06ms
step:479/2315 train_time:28767ms step_avg:60.06ms
step:480/2315 train_time:28827ms step_avg:60.06ms
step:481/2315 train_time:28887ms step_avg:60.06ms
step:482/2315 train_time:28947ms step_avg:60.06ms
step:483/2315 train_time:29007ms step_avg:60.06ms
step:484/2315 train_time:29066ms step_avg:60.05ms
step:485/2315 train_time:29126ms step_avg:60.05ms
step:486/2315 train_time:29187ms step_avg:60.05ms
step:487/2315 train_time:29247ms step_avg:60.06ms
step:488/2315 train_time:29307ms step_avg:60.05ms
step:489/2315 train_time:29366ms step_avg:60.05ms
step:490/2315 train_time:29426ms step_avg:60.05ms
step:491/2315 train_time:29487ms step_avg:60.06ms
step:492/2315 train_time:29547ms step_avg:60.06ms
step:493/2315 train_time:29607ms step_avg:60.05ms
step:494/2315 train_time:29667ms step_avg:60.05ms
step:495/2315 train_time:29727ms step_avg:60.05ms
step:496/2315 train_time:29787ms step_avg:60.05ms
step:497/2315 train_time:29848ms step_avg:60.06ms
step:498/2315 train_time:29908ms step_avg:60.06ms
step:499/2315 train_time:29967ms step_avg:60.05ms
step:500/2315 train_time:30027ms step_avg:60.05ms
step:500/2315 val_loss:3.8143 train_time:30089ms step_avg:60.18ms
step:501/2315 train_time:30112ms step_avg:60.10ms
step:502/2315 train_time:30149ms step_avg:60.06ms
step:503/2315 train_time:30214ms step_avg:60.07ms
step:504/2315 train_time:30276ms step_avg:60.07ms
step:505/2315 train_time:30337ms step_avg:60.07ms
step:506/2315 train_time:30397ms step_avg:60.07ms
step:507/2315 train_time:30456ms step_avg:60.07ms
step:508/2315 train_time:30516ms step_avg:60.07ms
step:509/2315 train_time:30575ms step_avg:60.07ms
step:510/2315 train_time:30634ms step_avg:60.07ms
step:511/2315 train_time:30693ms step_avg:60.06ms
step:512/2315 train_time:30752ms step_avg:60.06ms
step:513/2315 train_time:30811ms step_avg:60.06ms
step:514/2315 train_time:30871ms step_avg:60.06ms
step:515/2315 train_time:30930ms step_avg:60.06ms
step:516/2315 train_time:30989ms step_avg:60.06ms
step:517/2315 train_time:31049ms step_avg:60.06ms
step:518/2315 train_time:31109ms step_avg:60.06ms
step:519/2315 train_time:31171ms step_avg:60.06ms
step:520/2315 train_time:31232ms step_avg:60.06ms
step:521/2315 train_time:31293ms step_avg:60.06ms
step:522/2315 train_time:31354ms step_avg:60.07ms
step:523/2315 train_time:31415ms step_avg:60.07ms
step:524/2315 train_time:31474ms step_avg:60.07ms
step:525/2315 train_time:31535ms step_avg:60.07ms
step:526/2315 train_time:31594ms step_avg:60.06ms
step:527/2315 train_time:31653ms step_avg:60.06ms
step:528/2315 train_time:31713ms step_avg:60.06ms
step:529/2315 train_time:31772ms step_avg:60.06ms
step:530/2315 train_time:31831ms step_avg:60.06ms
step:531/2315 train_time:31890ms step_avg:60.06ms
step:532/2315 train_time:31950ms step_avg:60.06ms
step:533/2315 train_time:32009ms step_avg:60.05ms
step:534/2315 train_time:32069ms step_avg:60.05ms
step:535/2315 train_time:32129ms step_avg:60.05ms
step:536/2315 train_time:32190ms step_avg:60.06ms
step:537/2315 train_time:32251ms step_avg:60.06ms
step:538/2315 train_time:32312ms step_avg:60.06ms
step:539/2315 train_time:32373ms step_avg:60.06ms
step:540/2315 train_time:32434ms step_avg:60.06ms
step:541/2315 train_time:32494ms step_avg:60.06ms
step:542/2315 train_time:32554ms step_avg:60.06ms
step:543/2315 train_time:32614ms step_avg:60.06ms
step:544/2315 train_time:32674ms step_avg:60.06ms
step:545/2315 train_time:32733ms step_avg:60.06ms
step:546/2315 train_time:32793ms step_avg:60.06ms
step:547/2315 train_time:32852ms step_avg:60.06ms
step:548/2315 train_time:32911ms step_avg:60.06ms
step:549/2315 train_time:32971ms step_avg:60.06ms
step:550/2315 train_time:33031ms step_avg:60.06ms
step:551/2315 train_time:33092ms step_avg:60.06ms
step:552/2315 train_time:33152ms step_avg:60.06ms
step:553/2315 train_time:33213ms step_avg:60.06ms
step:554/2315 train_time:33273ms step_avg:60.06ms
step:555/2315 train_time:33332ms step_avg:60.06ms
step:556/2315 train_time:33393ms step_avg:60.06ms
step:557/2315 train_time:33453ms step_avg:60.06ms
step:558/2315 train_time:33514ms step_avg:60.06ms
step:559/2315 train_time:33574ms step_avg:60.06ms
step:560/2315 train_time:33633ms step_avg:60.06ms
step:561/2315 train_time:33693ms step_avg:60.06ms
step:562/2315 train_time:33753ms step_avg:60.06ms
step:563/2315 train_time:33813ms step_avg:60.06ms
step:564/2315 train_time:33872ms step_avg:60.06ms
step:565/2315 train_time:33932ms step_avg:60.06ms
step:566/2315 train_time:33991ms step_avg:60.05ms
step:567/2315 train_time:34051ms step_avg:60.05ms
step:568/2315 train_time:34111ms step_avg:60.06ms
step:569/2315 train_time:34172ms step_avg:60.06ms
step:570/2315 train_time:34232ms step_avg:60.06ms
step:571/2315 train_time:34293ms step_avg:60.06ms
step:572/2315 train_time:34353ms step_avg:60.06ms
step:573/2315 train_time:34413ms step_avg:60.06ms
step:574/2315 train_time:34474ms step_avg:60.06ms
step:575/2315 train_time:34534ms step_avg:60.06ms
step:576/2315 train_time:34594ms step_avg:60.06ms
step:577/2315 train_time:34654ms step_avg:60.06ms
step:578/2315 train_time:34713ms step_avg:60.06ms
step:579/2315 train_time:34773ms step_avg:60.06ms
step:580/2315 train_time:34832ms step_avg:60.06ms
step:581/2315 train_time:34892ms step_avg:60.05ms
step:582/2315 train_time:34952ms step_avg:60.05ms
step:583/2315 train_time:35011ms step_avg:60.05ms
step:584/2315 train_time:35071ms step_avg:60.05ms
step:585/2315 train_time:35130ms step_avg:60.05ms
step:586/2315 train_time:35191ms step_avg:60.05ms
step:587/2315 train_time:35251ms step_avg:60.05ms
step:588/2315 train_time:35312ms step_avg:60.05ms
step:589/2315 train_time:35372ms step_avg:60.05ms
step:590/2315 train_time:35432ms step_avg:60.05ms
step:591/2315 train_time:35492ms step_avg:60.05ms
step:592/2315 train_time:35553ms step_avg:60.06ms
step:593/2315 train_time:35613ms step_avg:60.06ms
step:594/2315 train_time:35673ms step_avg:60.06ms
step:595/2315 train_time:35733ms step_avg:60.06ms
step:596/2315 train_time:35792ms step_avg:60.05ms
step:597/2315 train_time:35852ms step_avg:60.05ms
step:598/2315 train_time:35912ms step_avg:60.05ms
step:599/2315 train_time:35971ms step_avg:60.05ms
step:600/2315 train_time:36032ms step_avg:60.05ms
step:601/2315 train_time:36092ms step_avg:60.05ms
step:602/2315 train_time:36152ms step_avg:60.05ms
step:603/2315 train_time:36212ms step_avg:60.05ms
step:604/2315 train_time:36272ms step_avg:60.05ms
step:605/2315 train_time:36332ms step_avg:60.05ms
step:606/2315 train_time:36392ms step_avg:60.05ms
step:607/2315 train_time:36452ms step_avg:60.05ms
step:608/2315 train_time:36512ms step_avg:60.05ms
step:609/2315 train_time:36572ms step_avg:60.05ms
step:610/2315 train_time:36632ms step_avg:60.05ms
step:611/2315 train_time:36692ms step_avg:60.05ms
step:612/2315 train_time:36752ms step_avg:60.05ms
step:613/2315 train_time:36812ms step_avg:60.05ms
step:614/2315 train_time:36872ms step_avg:60.05ms
step:615/2315 train_time:36932ms step_avg:60.05ms
step:616/2315 train_time:36992ms step_avg:60.05ms
step:617/2315 train_time:37051ms step_avg:60.05ms
step:618/2315 train_time:37111ms step_avg:60.05ms
step:619/2315 train_time:37171ms step_avg:60.05ms
step:620/2315 train_time:37231ms step_avg:60.05ms
step:621/2315 train_time:37291ms step_avg:60.05ms
step:622/2315 train_time:37351ms step_avg:60.05ms
step:623/2315 train_time:37411ms step_avg:60.05ms
step:624/2315 train_time:37471ms step_avg:60.05ms
step:625/2315 train_time:37532ms step_avg:60.05ms
step:626/2315 train_time:37592ms step_avg:60.05ms
step:627/2315 train_time:37652ms step_avg:60.05ms
step:628/2315 train_time:37711ms step_avg:60.05ms
step:629/2315 train_time:37771ms step_avg:60.05ms
step:630/2315 train_time:37832ms step_avg:60.05ms
step:631/2315 train_time:37892ms step_avg:60.05ms
step:632/2315 train_time:37952ms step_avg:60.05ms
step:633/2315 train_time:38012ms step_avg:60.05ms
step:634/2315 train_time:38073ms step_avg:60.05ms
step:635/2315 train_time:38132ms step_avg:60.05ms
step:636/2315 train_time:38193ms step_avg:60.05ms
step:637/2315 train_time:38253ms step_avg:60.05ms
step:638/2315 train_time:38314ms step_avg:60.05ms
step:639/2315 train_time:38373ms step_avg:60.05ms
step:640/2315 train_time:38433ms step_avg:60.05ms
step:641/2315 train_time:38493ms step_avg:60.05ms
step:642/2315 train_time:38553ms step_avg:60.05ms
step:643/2315 train_time:38614ms step_avg:60.05ms
step:644/2315 train_time:38674ms step_avg:60.05ms
step:645/2315 train_time:38734ms step_avg:60.05ms
step:646/2315 train_time:38795ms step_avg:60.05ms
step:647/2315 train_time:38854ms step_avg:60.05ms
step:648/2315 train_time:38913ms step_avg:60.05ms
step:649/2315 train_time:38973ms step_avg:60.05ms
step:650/2315 train_time:39034ms step_avg:60.05ms
step:651/2315 train_time:39093ms step_avg:60.05ms
step:652/2315 train_time:39153ms step_avg:60.05ms
step:653/2315 train_time:39213ms step_avg:60.05ms
step:654/2315 train_time:39274ms step_avg:60.05ms
step:655/2315 train_time:39334ms step_avg:60.05ms
step:656/2315 train_time:39394ms step_avg:60.05ms
step:657/2315 train_time:39453ms step_avg:60.05ms
step:658/2315 train_time:39513ms step_avg:60.05ms
step:659/2315 train_time:39573ms step_avg:60.05ms
step:660/2315 train_time:39633ms step_avg:60.05ms
step:661/2315 train_time:39693ms step_avg:60.05ms
step:662/2315 train_time:39753ms step_avg:60.05ms
step:663/2315 train_time:39813ms step_avg:60.05ms
step:664/2315 train_time:39872ms step_avg:60.05ms
step:665/2315 train_time:39932ms step_avg:60.05ms
step:666/2315 train_time:39993ms step_avg:60.05ms
step:667/2315 train_time:40053ms step_avg:60.05ms
step:668/2315 train_time:40113ms step_avg:60.05ms
step:669/2315 train_time:40173ms step_avg:60.05ms
step:670/2315 train_time:40234ms step_avg:60.05ms
step:671/2315 train_time:40294ms step_avg:60.05ms
step:672/2315 train_time:40355ms step_avg:60.05ms
step:673/2315 train_time:40415ms step_avg:60.05ms
step:674/2315 train_time:40474ms step_avg:60.05ms
step:675/2315 train_time:40534ms step_avg:60.05ms
step:676/2315 train_time:40594ms step_avg:60.05ms
step:677/2315 train_time:40654ms step_avg:60.05ms
step:678/2315 train_time:40714ms step_avg:60.05ms
step:679/2315 train_time:40773ms step_avg:60.05ms
step:680/2315 train_time:40833ms step_avg:60.05ms
step:681/2315 train_time:40893ms step_avg:60.05ms
step:682/2315 train_time:40953ms step_avg:60.05ms
step:683/2315 train_time:41013ms step_avg:60.05ms
step:684/2315 train_time:41073ms step_avg:60.05ms
step:685/2315 train_time:41133ms step_avg:60.05ms
step:686/2315 train_time:41193ms step_avg:60.05ms
step:687/2315 train_time:41252ms step_avg:60.05ms
step:688/2315 train_time:41313ms step_avg:60.05ms
step:689/2315 train_time:41373ms step_avg:60.05ms
step:690/2315 train_time:41432ms step_avg:60.05ms
step:691/2315 train_time:41493ms step_avg:60.05ms
step:692/2315 train_time:41553ms step_avg:60.05ms
step:693/2315 train_time:41613ms step_avg:60.05ms
step:694/2315 train_time:41673ms step_avg:60.05ms
step:695/2315 train_time:41732ms step_avg:60.05ms
step:696/2315 train_time:41792ms step_avg:60.05ms
step:697/2315 train_time:41852ms step_avg:60.05ms
step:698/2315 train_time:41912ms step_avg:60.05ms
step:699/2315 train_time:41972ms step_avg:60.05ms
step:700/2315 train_time:42032ms step_avg:60.05ms
step:701/2315 train_time:42092ms step_avg:60.05ms
step:702/2315 train_time:42152ms step_avg:60.05ms
step:703/2315 train_time:42212ms step_avg:60.05ms
step:704/2315 train_time:42273ms step_avg:60.05ms
step:705/2315 train_time:42333ms step_avg:60.05ms
step:706/2315 train_time:42392ms step_avg:60.05ms
step:707/2315 train_time:42453ms step_avg:60.05ms
step:708/2315 train_time:42513ms step_avg:60.05ms
step:709/2315 train_time:42572ms step_avg:60.05ms
step:710/2315 train_time:42632ms step_avg:60.04ms
step:711/2315 train_time:42692ms step_avg:60.04ms
step:712/2315 train_time:42753ms step_avg:60.05ms
step:713/2315 train_time:42813ms step_avg:60.05ms
step:714/2315 train_time:42872ms step_avg:60.05ms
step:715/2315 train_time:42932ms step_avg:60.05ms
step:716/2315 train_time:42992ms step_avg:60.05ms
step:717/2315 train_time:43052ms step_avg:60.04ms
step:718/2315 train_time:43112ms step_avg:60.04ms
step:719/2315 train_time:43172ms step_avg:60.04ms
step:720/2315 train_time:43232ms step_avg:60.04ms
step:721/2315 train_time:43292ms step_avg:60.04ms
step:722/2315 train_time:43353ms step_avg:60.05ms
step:723/2315 train_time:43412ms step_avg:60.04ms
step:724/2315 train_time:43473ms step_avg:60.05ms
step:725/2315 train_time:43533ms step_avg:60.05ms
step:726/2315 train_time:43593ms step_avg:60.04ms
step:727/2315 train_time:43652ms step_avg:60.04ms
step:728/2315 train_time:43713ms step_avg:60.05ms
step:729/2315 train_time:43773ms step_avg:60.04ms
step:730/2315 train_time:43832ms step_avg:60.04ms
step:731/2315 train_time:43892ms step_avg:60.04ms
step:732/2315 train_time:43952ms step_avg:60.04ms
step:733/2315 train_time:44012ms step_avg:60.04ms
step:734/2315 train_time:44072ms step_avg:60.04ms
step:735/2315 train_time:44132ms step_avg:60.04ms
step:736/2315 train_time:44193ms step_avg:60.04ms
step:737/2315 train_time:44253ms step_avg:60.04ms
step:738/2315 train_time:44313ms step_avg:60.04ms
step:739/2315 train_time:44373ms step_avg:60.04ms
step:740/2315 train_time:44433ms step_avg:60.04ms
step:741/2315 train_time:44493ms step_avg:60.04ms
step:742/2315 train_time:44553ms step_avg:60.04ms
step:743/2315 train_time:44613ms step_avg:60.04ms
step:744/2315 train_time:44673ms step_avg:60.04ms
step:745/2315 train_time:44732ms step_avg:60.04ms
step:746/2315 train_time:44792ms step_avg:60.04ms
step:747/2315 train_time:44852ms step_avg:60.04ms
step:748/2315 train_time:44913ms step_avg:60.04ms
step:749/2315 train_time:44972ms step_avg:60.04ms
step:750/2315 train_time:45032ms step_avg:60.04ms
step:750/2315 val_loss:3.6927 train_time:45094ms step_avg:60.13ms
step:751/2315 train_time:45114ms step_avg:60.07ms
step:752/2315 train_time:45155ms step_avg:60.05ms
step:753/2315 train_time:45219ms step_avg:60.05ms
step:754/2315 train_time:45282ms step_avg:60.06ms
step:755/2315 train_time:45342ms step_avg:60.06ms
step:756/2315 train_time:45401ms step_avg:60.05ms
step:757/2315 train_time:45461ms step_avg:60.05ms
step:758/2315 train_time:45521ms step_avg:60.05ms
step:759/2315 train_time:45579ms step_avg:60.05ms
step:760/2315 train_time:45638ms step_avg:60.05ms
step:761/2315 train_time:45698ms step_avg:60.05ms
step:762/2315 train_time:45758ms step_avg:60.05ms
step:763/2315 train_time:45818ms step_avg:60.05ms
step:764/2315 train_time:45877ms step_avg:60.05ms
step:765/2315 train_time:45937ms step_avg:60.05ms
step:766/2315 train_time:45997ms step_avg:60.05ms
step:767/2315 train_time:46058ms step_avg:60.05ms
step:768/2315 train_time:46118ms step_avg:60.05ms
step:769/2315 train_time:46179ms step_avg:60.05ms
step:770/2315 train_time:46241ms step_avg:60.05ms
step:771/2315 train_time:46302ms step_avg:60.05ms
step:772/2315 train_time:46363ms step_avg:60.06ms
step:773/2315 train_time:46424ms step_avg:60.06ms
step:774/2315 train_time:46485ms step_avg:60.06ms
step:775/2315 train_time:46545ms step_avg:60.06ms
step:776/2315 train_time:46606ms step_avg:60.06ms
step:777/2315 train_time:46666ms step_avg:60.06ms
step:778/2315 train_time:46726ms step_avg:60.06ms
step:779/2315 train_time:46786ms step_avg:60.06ms
step:780/2315 train_time:46847ms step_avg:60.06ms
step:781/2315 train_time:46907ms step_avg:60.06ms
step:782/2315 train_time:46968ms step_avg:60.06ms
step:783/2315 train_time:47028ms step_avg:60.06ms
step:784/2315 train_time:47089ms step_avg:60.06ms
step:785/2315 train_time:47151ms step_avg:60.06ms
step:786/2315 train_time:47212ms step_avg:60.07ms
step:787/2315 train_time:47273ms step_avg:60.07ms
step:788/2315 train_time:47334ms step_avg:60.07ms
step:789/2315 train_time:47396ms step_avg:60.07ms
step:790/2315 train_time:47457ms step_avg:60.07ms
step:791/2315 train_time:47517ms step_avg:60.07ms
step:792/2315 train_time:47577ms step_avg:60.07ms
step:793/2315 train_time:47639ms step_avg:60.07ms
step:794/2315 train_time:47699ms step_avg:60.07ms
step:795/2315 train_time:47759ms step_avg:60.07ms
step:796/2315 train_time:47819ms step_avg:60.07ms
step:797/2315 train_time:47879ms step_avg:60.07ms
step:798/2315 train_time:47940ms step_avg:60.07ms
step:799/2315 train_time:48000ms step_avg:60.07ms
step:800/2315 train_time:48059ms step_avg:60.07ms
step:801/2315 train_time:48120ms step_avg:60.07ms
step:802/2315 train_time:48180ms step_avg:60.07ms
step:803/2315 train_time:48240ms step_avg:60.08ms
step:804/2315 train_time:48301ms step_avg:60.08ms
step:805/2315 train_time:48362ms step_avg:60.08ms
step:806/2315 train_time:48423ms step_avg:60.08ms
step:807/2315 train_time:48484ms step_avg:60.08ms
step:808/2315 train_time:48544ms step_avg:60.08ms
step:809/2315 train_time:48606ms step_avg:60.08ms
step:810/2315 train_time:48667ms step_avg:60.08ms
step:811/2315 train_time:48727ms step_avg:60.08ms
step:812/2315 train_time:48787ms step_avg:60.08ms
step:813/2315 train_time:48848ms step_avg:60.08ms
step:814/2315 train_time:48908ms step_avg:60.08ms
step:815/2315 train_time:48969ms step_avg:60.08ms
step:816/2315 train_time:49030ms step_avg:60.09ms
step:817/2315 train_time:49091ms step_avg:60.09ms
step:818/2315 train_time:49151ms step_avg:60.09ms
step:819/2315 train_time:49212ms step_avg:60.09ms
step:820/2315 train_time:49273ms step_avg:60.09ms
step:821/2315 train_time:49334ms step_avg:60.09ms
step:822/2315 train_time:49395ms step_avg:60.09ms
step:823/2315 train_time:49456ms step_avg:60.09ms
step:824/2315 train_time:49516ms step_avg:60.09ms
step:825/2315 train_time:49578ms step_avg:60.09ms
step:826/2315 train_time:49639ms step_avg:60.10ms
step:827/2315 train_time:49700ms step_avg:60.10ms
step:828/2315 train_time:49760ms step_avg:60.10ms
step:829/2315 train_time:49821ms step_avg:60.10ms
step:830/2315 train_time:49881ms step_avg:60.10ms
step:831/2315 train_time:49941ms step_avg:60.10ms
step:832/2315 train_time:50001ms step_avg:60.10ms
step:833/2315 train_time:50061ms step_avg:60.10ms
step:834/2315 train_time:50121ms step_avg:60.10ms
step:835/2315 train_time:50182ms step_avg:60.10ms
step:836/2315 train_time:50242ms step_avg:60.10ms
step:837/2315 train_time:50304ms step_avg:60.10ms
step:838/2315 train_time:50364ms step_avg:60.10ms
step:839/2315 train_time:50425ms step_avg:60.10ms
step:840/2315 train_time:50486ms step_avg:60.10ms
step:841/2315 train_time:50548ms step_avg:60.11ms
step:842/2315 train_time:50609ms step_avg:60.11ms
step:843/2315 train_time:50670ms step_avg:60.11ms
step:844/2315 train_time:50731ms step_avg:60.11ms
step:845/2315 train_time:50792ms step_avg:60.11ms
step:846/2315 train_time:50853ms step_avg:60.11ms
step:847/2315 train_time:50914ms step_avg:60.11ms
step:848/2315 train_time:50975ms step_avg:60.11ms
step:849/2315 train_time:51035ms step_avg:60.11ms
step:850/2315 train_time:51096ms step_avg:60.11ms
step:851/2315 train_time:51157ms step_avg:60.11ms
step:852/2315 train_time:51217ms step_avg:60.11ms
step:853/2315 train_time:51278ms step_avg:60.11ms
step:854/2315 train_time:51338ms step_avg:60.12ms
step:855/2315 train_time:51398ms step_avg:60.12ms
step:856/2315 train_time:51459ms step_avg:60.12ms
step:857/2315 train_time:51520ms step_avg:60.12ms
step:858/2315 train_time:51580ms step_avg:60.12ms
step:859/2315 train_time:51640ms step_avg:60.12ms
step:860/2315 train_time:51700ms step_avg:60.12ms
step:861/2315 train_time:51761ms step_avg:60.12ms
step:862/2315 train_time:51821ms step_avg:60.12ms
step:863/2315 train_time:51882ms step_avg:60.12ms
step:864/2315 train_time:51942ms step_avg:60.12ms
step:865/2315 train_time:52003ms step_avg:60.12ms
step:866/2315 train_time:52063ms step_avg:60.12ms
step:867/2315 train_time:52124ms step_avg:60.12ms
step:868/2315 train_time:52184ms step_avg:60.12ms
step:869/2315 train_time:52244ms step_avg:60.12ms
step:870/2315 train_time:52305ms step_avg:60.12ms
step:871/2315 train_time:52366ms step_avg:60.12ms
step:872/2315 train_time:52426ms step_avg:60.12ms
step:873/2315 train_time:52487ms step_avg:60.12ms
step:874/2315 train_time:52547ms step_avg:60.12ms
step:875/2315 train_time:52608ms step_avg:60.12ms
step:876/2315 train_time:52669ms step_avg:60.12ms
step:877/2315 train_time:52731ms step_avg:60.13ms
step:878/2315 train_time:52791ms step_avg:60.13ms
step:879/2315 train_time:52853ms step_avg:60.13ms
step:880/2315 train_time:52913ms step_avg:60.13ms
step:881/2315 train_time:52975ms step_avg:60.13ms
step:882/2315 train_time:53035ms step_avg:60.13ms
step:883/2315 train_time:53096ms step_avg:60.13ms
step:884/2315 train_time:53158ms step_avg:60.13ms
step:885/2315 train_time:53218ms step_avg:60.13ms
step:886/2315 train_time:53278ms step_avg:60.13ms
step:887/2315 train_time:53339ms step_avg:60.13ms
step:888/2315 train_time:53399ms step_avg:60.13ms
step:889/2315 train_time:53460ms step_avg:60.13ms
step:890/2315 train_time:53520ms step_avg:60.13ms
step:891/2315 train_time:53580ms step_avg:60.13ms
step:892/2315 train_time:53639ms step_avg:60.13ms
step:893/2315 train_time:53700ms step_avg:60.13ms
step:894/2315 train_time:53759ms step_avg:60.13ms
step:895/2315 train_time:53819ms step_avg:60.13ms
step:896/2315 train_time:53880ms step_avg:60.13ms
step:897/2315 train_time:53940ms step_avg:60.13ms
step:898/2315 train_time:54001ms step_avg:60.13ms
step:899/2315 train_time:54061ms step_avg:60.13ms
step:900/2315 train_time:54121ms step_avg:60.13ms
step:901/2315 train_time:54182ms step_avg:60.14ms
step:902/2315 train_time:54243ms step_avg:60.14ms
step:903/2315 train_time:54303ms step_avg:60.14ms
step:904/2315 train_time:54364ms step_avg:60.14ms
step:905/2315 train_time:54424ms step_avg:60.14ms
step:906/2315 train_time:54485ms step_avg:60.14ms
step:907/2315 train_time:54545ms step_avg:60.14ms
step:908/2315 train_time:54606ms step_avg:60.14ms
step:909/2315 train_time:54667ms step_avg:60.14ms
step:910/2315 train_time:54727ms step_avg:60.14ms
step:911/2315 train_time:54787ms step_avg:60.14ms
step:912/2315 train_time:54848ms step_avg:60.14ms
step:913/2315 train_time:54909ms step_avg:60.14ms
step:914/2315 train_time:54970ms step_avg:60.14ms
step:915/2315 train_time:55031ms step_avg:60.14ms
step:916/2315 train_time:55092ms step_avg:60.14ms
step:917/2315 train_time:55153ms step_avg:60.15ms
step:918/2315 train_time:55214ms step_avg:60.15ms
step:919/2315 train_time:55275ms step_avg:60.15ms
step:920/2315 train_time:55336ms step_avg:60.15ms
step:921/2315 train_time:55398ms step_avg:60.15ms
step:922/2315 train_time:55459ms step_avg:60.15ms
step:923/2315 train_time:55519ms step_avg:60.15ms
step:924/2315 train_time:55579ms step_avg:60.15ms
step:925/2315 train_time:55639ms step_avg:60.15ms
step:926/2315 train_time:55700ms step_avg:60.15ms
step:927/2315 train_time:55760ms step_avg:60.15ms
step:928/2315 train_time:55820ms step_avg:60.15ms
step:929/2315 train_time:55881ms step_avg:60.15ms
step:930/2315 train_time:55941ms step_avg:60.15ms
step:931/2315 train_time:56001ms step_avg:60.15ms
step:932/2315 train_time:56061ms step_avg:60.15ms
step:933/2315 train_time:56122ms step_avg:60.15ms
step:934/2315 train_time:56182ms step_avg:60.15ms
step:935/2315 train_time:56244ms step_avg:60.15ms
step:936/2315 train_time:56305ms step_avg:60.15ms
step:937/2315 train_time:56366ms step_avg:60.16ms
step:938/2315 train_time:56427ms step_avg:60.16ms
step:939/2315 train_time:56487ms step_avg:60.16ms
step:940/2315 train_time:56547ms step_avg:60.16ms
step:941/2315 train_time:56609ms step_avg:60.16ms
step:942/2315 train_time:56669ms step_avg:60.16ms
step:943/2315 train_time:56730ms step_avg:60.16ms
step:944/2315 train_time:56791ms step_avg:60.16ms
step:945/2315 train_time:56852ms step_avg:60.16ms
step:946/2315 train_time:56912ms step_avg:60.16ms
step:947/2315 train_time:56973ms step_avg:60.16ms
step:948/2315 train_time:57034ms step_avg:60.16ms
step:949/2315 train_time:57095ms step_avg:60.16ms
step:950/2315 train_time:57156ms step_avg:60.16ms
step:951/2315 train_time:57217ms step_avg:60.16ms
step:952/2315 train_time:57278ms step_avg:60.17ms
step:953/2315 train_time:57339ms step_avg:60.17ms
step:954/2315 train_time:57400ms step_avg:60.17ms
step:955/2315 train_time:57459ms step_avg:60.17ms
step:956/2315 train_time:57520ms step_avg:60.17ms
step:957/2315 train_time:57580ms step_avg:60.17ms
step:958/2315 train_time:57640ms step_avg:60.17ms
step:959/2315 train_time:57700ms step_avg:60.17ms
step:960/2315 train_time:57760ms step_avg:60.17ms
step:961/2315 train_time:57821ms step_avg:60.17ms
step:962/2315 train_time:57881ms step_avg:60.17ms
step:963/2315 train_time:57941ms step_avg:60.17ms
step:964/2315 train_time:58001ms step_avg:60.17ms
step:965/2315 train_time:58062ms step_avg:60.17ms
step:966/2315 train_time:58123ms step_avg:60.17ms
step:967/2315 train_time:58184ms step_avg:60.17ms
step:968/2315 train_time:58244ms step_avg:60.17ms
step:969/2315 train_time:58305ms step_avg:60.17ms
step:970/2315 train_time:58366ms step_avg:60.17ms
step:971/2315 train_time:58427ms step_avg:60.17ms
step:972/2315 train_time:58487ms step_avg:60.17ms
step:973/2315 train_time:58548ms step_avg:60.17ms
step:974/2315 train_time:58608ms step_avg:60.17ms
step:975/2315 train_time:58668ms step_avg:60.17ms
step:976/2315 train_time:58728ms step_avg:60.17ms
step:977/2315 train_time:58789ms step_avg:60.17ms
step:978/2315 train_time:58850ms step_avg:60.17ms
step:979/2315 train_time:58911ms step_avg:60.17ms
step:980/2315 train_time:58973ms step_avg:60.18ms
step:981/2315 train_time:59035ms step_avg:60.18ms
step:982/2315 train_time:59096ms step_avg:60.18ms
step:983/2315 train_time:59156ms step_avg:60.18ms
step:984/2315 train_time:59217ms step_avg:60.18ms
step:985/2315 train_time:59277ms step_avg:60.18ms
step:986/2315 train_time:59338ms step_avg:60.18ms
step:987/2315 train_time:59399ms step_avg:60.18ms
step:988/2315 train_time:59460ms step_avg:60.18ms
step:989/2315 train_time:59520ms step_avg:60.18ms
step:990/2315 train_time:59580ms step_avg:60.18ms
step:991/2315 train_time:59640ms step_avg:60.18ms
step:992/2315 train_time:59700ms step_avg:60.18ms
step:993/2315 train_time:59760ms step_avg:60.18ms
step:994/2315 train_time:59820ms step_avg:60.18ms
step:995/2315 train_time:59881ms step_avg:60.18ms
step:996/2315 train_time:59941ms step_avg:60.18ms
step:997/2315 train_time:60002ms step_avg:60.18ms
step:998/2315 train_time:60063ms step_avg:60.18ms
step:999/2315 train_time:60123ms step_avg:60.18ms
step:1000/2315 train_time:60183ms step_avg:60.18ms
step:1000/2315 val_loss:3.5711 train_time:60246ms step_avg:60.25ms
step:1001/2315 train_time:60267ms step_avg:60.21ms
step:1002/2315 train_time:60308ms step_avg:60.19ms
step:1003/2315 train_time:60376ms step_avg:60.19ms
step:1004/2315 train_time:60439ms step_avg:60.20ms
step:1005/2315 train_time:60500ms step_avg:60.20ms
step:1006/2315 train_time:60561ms step_avg:60.20ms
step:1007/2315 train_time:60622ms step_avg:60.20ms
step:1008/2315 train_time:60682ms step_avg:60.20ms
step:1009/2315 train_time:60743ms step_avg:60.20ms
step:1010/2315 train_time:60803ms step_avg:60.20ms
step:1011/2315 train_time:60863ms step_avg:60.20ms
step:1012/2315 train_time:60923ms step_avg:60.20ms
step:1013/2315 train_time:60984ms step_avg:60.20ms
step:1014/2315 train_time:61044ms step_avg:60.20ms
step:1015/2315 train_time:61104ms step_avg:60.20ms
step:1016/2315 train_time:61165ms step_avg:60.20ms
step:1017/2315 train_time:61228ms step_avg:60.20ms
step:1018/2315 train_time:61289ms step_avg:60.21ms
step:1019/2315 train_time:61351ms step_avg:60.21ms
step:1020/2315 train_time:61412ms step_avg:60.21ms
step:1021/2315 train_time:61474ms step_avg:60.21ms
step:1022/2315 train_time:61534ms step_avg:60.21ms
step:1023/2315 train_time:61595ms step_avg:60.21ms
step:1024/2315 train_time:61655ms step_avg:60.21ms
step:1025/2315 train_time:61715ms step_avg:60.21ms
step:1026/2315 train_time:61775ms step_avg:60.21ms
step:1027/2315 train_time:61836ms step_avg:60.21ms
step:1028/2315 train_time:61897ms step_avg:60.21ms
step:1029/2315 train_time:61959ms step_avg:60.21ms
step:1030/2315 train_time:62019ms step_avg:60.21ms
step:1031/2315 train_time:62079ms step_avg:60.21ms
step:1032/2315 train_time:62140ms step_avg:60.21ms
step:1033/2315 train_time:62201ms step_avg:60.21ms
step:1034/2315 train_time:62262ms step_avg:60.21ms
step:1035/2315 train_time:62324ms step_avg:60.22ms
step:1036/2315 train_time:62386ms step_avg:60.22ms
step:1037/2315 train_time:62447ms step_avg:60.22ms
step:1038/2315 train_time:62508ms step_avg:60.22ms
step:1039/2315 train_time:62569ms step_avg:60.22ms
step:1040/2315 train_time:62630ms step_avg:60.22ms
step:1041/2315 train_time:62690ms step_avg:60.22ms
step:1042/2315 train_time:62750ms step_avg:60.22ms
step:1043/2315 train_time:62811ms step_avg:60.22ms
step:1044/2315 train_time:62871ms step_avg:60.22ms
step:1045/2315 train_time:62932ms step_avg:60.22ms
step:1046/2315 train_time:62992ms step_avg:60.22ms
step:1047/2315 train_time:63053ms step_avg:60.22ms
step:1048/2315 train_time:63114ms step_avg:60.22ms
step:1049/2315 train_time:63174ms step_avg:60.22ms
step:1050/2315 train_time:63235ms step_avg:60.22ms
step:1051/2315 train_time:63296ms step_avg:60.22ms
step:1052/2315 train_time:63358ms step_avg:60.23ms
step:1053/2315 train_time:63419ms step_avg:60.23ms
step:1054/2315 train_time:63480ms step_avg:60.23ms
step:1055/2315 train_time:63541ms step_avg:60.23ms
step:1056/2315 train_time:63602ms step_avg:60.23ms
step:1057/2315 train_time:63664ms step_avg:60.23ms
step:1058/2315 train_time:63724ms step_avg:60.23ms
step:1059/2315 train_time:63785ms step_avg:60.23ms
step:1060/2315 train_time:63846ms step_avg:60.23ms
step:1061/2315 train_time:63907ms step_avg:60.23ms
step:1062/2315 train_time:63968ms step_avg:60.23ms
step:1063/2315 train_time:64027ms step_avg:60.23ms
step:1064/2315 train_time:64088ms step_avg:60.23ms
step:1065/2315 train_time:64149ms step_avg:60.23ms
step:1066/2315 train_time:64209ms step_avg:60.23ms
step:1067/2315 train_time:64269ms step_avg:60.23ms
step:1068/2315 train_time:64330ms step_avg:60.23ms
step:1069/2315 train_time:64390ms step_avg:60.23ms
step:1070/2315 train_time:64450ms step_avg:60.23ms
step:1071/2315 train_time:64511ms step_avg:60.23ms
step:1072/2315 train_time:64573ms step_avg:60.24ms
step:1073/2315 train_time:64634ms step_avg:60.24ms
step:1074/2315 train_time:64694ms step_avg:60.24ms
step:1075/2315 train_time:64755ms step_avg:60.24ms
step:1076/2315 train_time:64816ms step_avg:60.24ms
step:1077/2315 train_time:64876ms step_avg:60.24ms
step:1078/2315 train_time:64936ms step_avg:60.24ms
step:1079/2315 train_time:64997ms step_avg:60.24ms
step:1080/2315 train_time:65058ms step_avg:60.24ms
step:1081/2315 train_time:65119ms step_avg:60.24ms
step:1082/2315 train_time:65180ms step_avg:60.24ms
step:1083/2315 train_time:65241ms step_avg:60.24ms
step:1084/2315 train_time:65301ms step_avg:60.24ms
step:1085/2315 train_time:65362ms step_avg:60.24ms
step:1086/2315 train_time:65423ms step_avg:60.24ms
step:1087/2315 train_time:65485ms step_avg:60.24ms
step:1088/2315 train_time:65546ms step_avg:60.24ms
step:1089/2315 train_time:65607ms step_avg:60.25ms
step:1090/2315 train_time:65668ms step_avg:60.25ms
step:1091/2315 train_time:65728ms step_avg:60.25ms
step:1092/2315 train_time:65788ms step_avg:60.25ms
step:1093/2315 train_time:65849ms step_avg:60.25ms
step:1094/2315 train_time:65909ms step_avg:60.25ms
step:1095/2315 train_time:65970ms step_avg:60.25ms
step:1096/2315 train_time:66030ms step_avg:60.25ms
step:1097/2315 train_time:66091ms step_avg:60.25ms
step:1098/2315 train_time:66151ms step_avg:60.25ms
step:1099/2315 train_time:66212ms step_avg:60.25ms
step:1100/2315 train_time:66272ms step_avg:60.25ms
step:1101/2315 train_time:66333ms step_avg:60.25ms
step:1102/2315 train_time:66394ms step_avg:60.25ms
step:1103/2315 train_time:66455ms step_avg:60.25ms
step:1104/2315 train_time:66516ms step_avg:60.25ms
step:1105/2315 train_time:66577ms step_avg:60.25ms
step:1106/2315 train_time:66638ms step_avg:60.25ms
step:1107/2315 train_time:66699ms step_avg:60.25ms
step:1108/2315 train_time:66760ms step_avg:60.25ms
step:1109/2315 train_time:66821ms step_avg:60.25ms
step:1110/2315 train_time:66882ms step_avg:60.25ms
step:1111/2315 train_time:66943ms step_avg:60.25ms
step:1112/2315 train_time:67004ms step_avg:60.26ms
step:1113/2315 train_time:67065ms step_avg:60.26ms
step:1114/2315 train_time:67125ms step_avg:60.26ms
step:1115/2315 train_time:67186ms step_avg:60.26ms
step:1116/2315 train_time:67247ms step_avg:60.26ms
step:1117/2315 train_time:67307ms step_avg:60.26ms
step:1118/2315 train_time:67368ms step_avg:60.26ms
step:1119/2315 train_time:67428ms step_avg:60.26ms
step:1120/2315 train_time:67489ms step_avg:60.26ms
step:1121/2315 train_time:67549ms step_avg:60.26ms
step:1122/2315 train_time:67610ms step_avg:60.26ms
step:1123/2315 train_time:67671ms step_avg:60.26ms
step:1124/2315 train_time:67732ms step_avg:60.26ms
step:1125/2315 train_time:67793ms step_avg:60.26ms
step:1126/2315 train_time:67854ms step_avg:60.26ms
step:1127/2315 train_time:67914ms step_avg:60.26ms
step:1128/2315 train_time:67975ms step_avg:60.26ms
step:1129/2315 train_time:68036ms step_avg:60.26ms
step:1130/2315 train_time:68097ms step_avg:60.26ms
step:1131/2315 train_time:68158ms step_avg:60.26ms
step:1132/2315 train_time:68218ms step_avg:60.26ms
step:1133/2315 train_time:68279ms step_avg:60.26ms
step:1134/2315 train_time:68340ms step_avg:60.26ms
step:1135/2315 train_time:68402ms step_avg:60.27ms
step:1136/2315 train_time:68462ms step_avg:60.27ms
step:1137/2315 train_time:68523ms step_avg:60.27ms
step:1138/2315 train_time:68584ms step_avg:60.27ms
step:1139/2315 train_time:68645ms step_avg:60.27ms
step:1140/2315 train_time:68706ms step_avg:60.27ms
step:1141/2315 train_time:68767ms step_avg:60.27ms
step:1142/2315 train_time:68827ms step_avg:60.27ms
step:1143/2315 train_time:68888ms step_avg:60.27ms
step:1144/2315 train_time:68949ms step_avg:60.27ms
step:1145/2315 train_time:69009ms step_avg:60.27ms
step:1146/2315 train_time:69069ms step_avg:60.27ms
step:1147/2315 train_time:69130ms step_avg:60.27ms
step:1148/2315 train_time:69189ms step_avg:60.27ms
step:1149/2315 train_time:69250ms step_avg:60.27ms
step:1150/2315 train_time:69310ms step_avg:60.27ms
step:1151/2315 train_time:69371ms step_avg:60.27ms
step:1152/2315 train_time:69432ms step_avg:60.27ms
step:1153/2315 train_time:69494ms step_avg:60.27ms
step:1154/2315 train_time:69555ms step_avg:60.27ms
step:1155/2315 train_time:69615ms step_avg:60.27ms
step:1156/2315 train_time:69676ms step_avg:60.27ms
step:1157/2315 train_time:69738ms step_avg:60.27ms
step:1158/2315 train_time:69799ms step_avg:60.28ms
step:1159/2315 train_time:69859ms step_avg:60.28ms
step:1160/2315 train_time:69920ms step_avg:60.28ms
step:1161/2315 train_time:69980ms step_avg:60.28ms
step:1162/2315 train_time:70041ms step_avg:60.28ms
step:1163/2315 train_time:70102ms step_avg:60.28ms
step:1164/2315 train_time:70163ms step_avg:60.28ms
step:1165/2315 train_time:70224ms step_avg:60.28ms
step:1166/2315 train_time:70284ms step_avg:60.28ms
step:1167/2315 train_time:70345ms step_avg:60.28ms
step:1168/2315 train_time:70407ms step_avg:60.28ms
step:1169/2315 train_time:70467ms step_avg:60.28ms
step:1170/2315 train_time:70528ms step_avg:60.28ms
step:1171/2315 train_time:70588ms step_avg:60.28ms
step:1172/2315 train_time:70649ms step_avg:60.28ms
step:1173/2315 train_time:70709ms step_avg:60.28ms
step:1174/2315 train_time:70769ms step_avg:60.28ms
step:1175/2315 train_time:70830ms step_avg:60.28ms
step:1176/2315 train_time:70891ms step_avg:60.28ms
step:1177/2315 train_time:70951ms step_avg:60.28ms
step:1178/2315 train_time:71011ms step_avg:60.28ms
step:1179/2315 train_time:71072ms step_avg:60.28ms
step:1180/2315 train_time:71133ms step_avg:60.28ms
step:1181/2315 train_time:71193ms step_avg:60.28ms
step:1182/2315 train_time:71254ms step_avg:60.28ms
step:1183/2315 train_time:71315ms step_avg:60.28ms
step:1184/2315 train_time:71375ms step_avg:60.28ms
step:1185/2315 train_time:71436ms step_avg:60.28ms
step:1186/2315 train_time:71498ms step_avg:60.28ms
step:1187/2315 train_time:71559ms step_avg:60.29ms
step:1188/2315 train_time:71619ms step_avg:60.29ms
step:1189/2315 train_time:71680ms step_avg:60.29ms
step:1190/2315 train_time:71741ms step_avg:60.29ms
step:1191/2315 train_time:71802ms step_avg:60.29ms
step:1192/2315 train_time:71863ms step_avg:60.29ms
step:1193/2315 train_time:71924ms step_avg:60.29ms
step:1194/2315 train_time:71984ms step_avg:60.29ms
step:1195/2315 train_time:72046ms step_avg:60.29ms
step:1196/2315 train_time:72107ms step_avg:60.29ms
step:1197/2315 train_time:72168ms step_avg:60.29ms
step:1198/2315 train_time:72228ms step_avg:60.29ms
step:1199/2315 train_time:72289ms step_avg:60.29ms
step:1200/2315 train_time:72349ms step_avg:60.29ms
step:1201/2315 train_time:72409ms step_avg:60.29ms
step:1202/2315 train_time:72469ms step_avg:60.29ms
step:1203/2315 train_time:72530ms step_avg:60.29ms
step:1204/2315 train_time:72590ms step_avg:60.29ms
step:1205/2315 train_time:72651ms step_avg:60.29ms
step:1206/2315 train_time:72712ms step_avg:60.29ms
step:1207/2315 train_time:72773ms step_avg:60.29ms
step:1208/2315 train_time:72834ms step_avg:60.29ms
step:1209/2315 train_time:72895ms step_avg:60.29ms
step:1210/2315 train_time:72956ms step_avg:60.29ms
step:1211/2315 train_time:73018ms step_avg:60.30ms
step:1212/2315 train_time:73078ms step_avg:60.30ms
step:1213/2315 train_time:73139ms step_avg:60.30ms
step:1214/2315 train_time:73200ms step_avg:60.30ms
step:1215/2315 train_time:73261ms step_avg:60.30ms
step:1216/2315 train_time:73322ms step_avg:60.30ms
step:1217/2315 train_time:73383ms step_avg:60.30ms
step:1218/2315 train_time:73444ms step_avg:60.30ms
step:1219/2315 train_time:73504ms step_avg:60.30ms
step:1220/2315 train_time:73565ms step_avg:60.30ms
step:1221/2315 train_time:73627ms step_avg:60.30ms
step:1222/2315 train_time:73688ms step_avg:60.30ms
step:1223/2315 train_time:73749ms step_avg:60.30ms
step:1224/2315 train_time:73809ms step_avg:60.30ms
step:1225/2315 train_time:73870ms step_avg:60.30ms
step:1226/2315 train_time:73930ms step_avg:60.30ms
step:1227/2315 train_time:73990ms step_avg:60.30ms
step:1228/2315 train_time:74051ms step_avg:60.30ms
step:1229/2315 train_time:74112ms step_avg:60.30ms
step:1230/2315 train_time:74172ms step_avg:60.30ms
step:1231/2315 train_time:74233ms step_avg:60.30ms
step:1232/2315 train_time:74293ms step_avg:60.30ms
step:1233/2315 train_time:74355ms step_avg:60.30ms
step:1234/2315 train_time:74416ms step_avg:60.30ms
step:1235/2315 train_time:74477ms step_avg:60.31ms
step:1236/2315 train_time:74537ms step_avg:60.31ms
step:1237/2315 train_time:74599ms step_avg:60.31ms
step:1238/2315 train_time:74660ms step_avg:60.31ms
step:1239/2315 train_time:74721ms step_avg:60.31ms
step:1240/2315 train_time:74782ms step_avg:60.31ms
step:1241/2315 train_time:74843ms step_avg:60.31ms
step:1242/2315 train_time:74904ms step_avg:60.31ms
step:1243/2315 train_time:74965ms step_avg:60.31ms
step:1244/2315 train_time:75026ms step_avg:60.31ms
step:1245/2315 train_time:75087ms step_avg:60.31ms
step:1246/2315 train_time:75148ms step_avg:60.31ms
step:1247/2315 train_time:75208ms step_avg:60.31ms
step:1248/2315 train_time:75269ms step_avg:60.31ms
step:1249/2315 train_time:75329ms step_avg:60.31ms
step:1250/2315 train_time:75389ms step_avg:60.31ms
step:1250/2315 val_loss:3.5140 train_time:75450ms step_avg:60.36ms
step:1251/2315 train_time:75472ms step_avg:60.33ms
step:1252/2315 train_time:75513ms step_avg:60.31ms
step:1253/2315 train_time:75578ms step_avg:60.32ms
step:1254/2315 train_time:75641ms step_avg:60.32ms
step:1255/2315 train_time:75701ms step_avg:60.32ms
step:1256/2315 train_time:75762ms step_avg:60.32ms
step:1257/2315 train_time:75823ms step_avg:60.32ms
step:1258/2315 train_time:75883ms step_avg:60.32ms
step:1259/2315 train_time:75944ms step_avg:60.32ms
step:1260/2315 train_time:76004ms step_avg:60.32ms
step:1261/2315 train_time:76064ms step_avg:60.32ms
step:1262/2315 train_time:76124ms step_avg:60.32ms
step:1263/2315 train_time:76183ms step_avg:60.32ms
step:1264/2315 train_time:76243ms step_avg:60.32ms
step:1265/2315 train_time:76303ms step_avg:60.32ms
step:1266/2315 train_time:76363ms step_avg:60.32ms
step:1267/2315 train_time:76423ms step_avg:60.32ms
step:1268/2315 train_time:76486ms step_avg:60.32ms
step:1269/2315 train_time:76549ms step_avg:60.32ms
step:1270/2315 train_time:76612ms step_avg:60.32ms
step:1271/2315 train_time:76673ms step_avg:60.33ms
step:1272/2315 train_time:76735ms step_avg:60.33ms
step:1273/2315 train_time:76795ms step_avg:60.33ms
step:1274/2315 train_time:76856ms step_avg:60.33ms
step:1275/2315 train_time:76916ms step_avg:60.33ms
step:1276/2315 train_time:76977ms step_avg:60.33ms
step:1277/2315 train_time:77038ms step_avg:60.33ms
step:1278/2315 train_time:77098ms step_avg:60.33ms
step:1279/2315 train_time:77159ms step_avg:60.33ms
step:1280/2315 train_time:77219ms step_avg:60.33ms
step:1281/2315 train_time:77279ms step_avg:60.33ms
step:1282/2315 train_time:77339ms step_avg:60.33ms
step:1283/2315 train_time:77400ms step_avg:60.33ms
step:1284/2315 train_time:77461ms step_avg:60.33ms
step:1285/2315 train_time:77522ms step_avg:60.33ms
step:1286/2315 train_time:77583ms step_avg:60.33ms
step:1287/2315 train_time:77643ms step_avg:60.33ms
step:1288/2315 train_time:77704ms step_avg:60.33ms
step:1289/2315 train_time:77765ms step_avg:60.33ms
step:1290/2315 train_time:77827ms step_avg:60.33ms
step:1291/2315 train_time:77888ms step_avg:60.33ms
step:1292/2315 train_time:77949ms step_avg:60.33ms
step:1293/2315 train_time:78009ms step_avg:60.33ms
step:1294/2315 train_time:78070ms step_avg:60.33ms
step:1295/2315 train_time:78131ms step_avg:60.33ms
step:1296/2315 train_time:78191ms step_avg:60.33ms
step:1297/2315 train_time:78252ms step_avg:60.33ms
step:1298/2315 train_time:78313ms step_avg:60.33ms
step:1299/2315 train_time:78373ms step_avg:60.33ms
step:1300/2315 train_time:78434ms step_avg:60.33ms
step:1301/2315 train_time:78495ms step_avg:60.33ms
step:1302/2315 train_time:78556ms step_avg:60.34ms
step:1303/2315 train_time:78618ms step_avg:60.34ms
step:1304/2315 train_time:78679ms step_avg:60.34ms
step:1305/2315 train_time:78740ms step_avg:60.34ms
step:1306/2315 train_time:78800ms step_avg:60.34ms
step:1307/2315 train_time:78861ms step_avg:60.34ms
step:1308/2315 train_time:78922ms step_avg:60.34ms
step:1309/2315 train_time:78982ms step_avg:60.34ms
step:1310/2315 train_time:79043ms step_avg:60.34ms
step:1311/2315 train_time:79103ms step_avg:60.34ms
step:1312/2315 train_time:79163ms step_avg:60.34ms
step:1313/2315 train_time:79224ms step_avg:60.34ms
step:1314/2315 train_time:79285ms step_avg:60.34ms
step:1315/2315 train_time:79346ms step_avg:60.34ms
step:1316/2315 train_time:79406ms step_avg:60.34ms
step:1317/2315 train_time:79467ms step_avg:60.34ms
step:1318/2315 train_time:79529ms step_avg:60.34ms
step:1319/2315 train_time:79590ms step_avg:60.34ms
step:1320/2315 train_time:79651ms step_avg:60.34ms
step:1321/2315 train_time:79712ms step_avg:60.34ms
step:1322/2315 train_time:79772ms step_avg:60.34ms
step:1323/2315 train_time:79834ms step_avg:60.34ms
step:1324/2315 train_time:79895ms step_avg:60.34ms
step:1325/2315 train_time:79956ms step_avg:60.34ms
step:1326/2315 train_time:80016ms step_avg:60.34ms
step:1327/2315 train_time:80077ms step_avg:60.34ms
step:1328/2315 train_time:80138ms step_avg:60.34ms
step:1329/2315 train_time:80199ms step_avg:60.35ms
step:1330/2315 train_time:80260ms step_avg:60.35ms
step:1331/2315 train_time:80320ms step_avg:60.35ms
step:1332/2315 train_time:80381ms step_avg:60.35ms
step:1333/2315 train_time:80441ms step_avg:60.35ms
step:1334/2315 train_time:80501ms step_avg:60.35ms
step:1335/2315 train_time:80562ms step_avg:60.35ms
step:1336/2315 train_time:80622ms step_avg:60.35ms
step:1337/2315 train_time:80682ms step_avg:60.35ms
step:1338/2315 train_time:80743ms step_avg:60.35ms
step:1339/2315 train_time:80804ms step_avg:60.35ms
step:1340/2315 train_time:80865ms step_avg:60.35ms
step:1341/2315 train_time:80926ms step_avg:60.35ms
step:1342/2315 train_time:80986ms step_avg:60.35ms
step:1343/2315 train_time:81048ms step_avg:60.35ms
step:1344/2315 train_time:81108ms step_avg:60.35ms
step:1345/2315 train_time:81168ms step_avg:60.35ms
step:1346/2315 train_time:81229ms step_avg:60.35ms
step:1347/2315 train_time:81290ms step_avg:60.35ms
step:1348/2315 train_time:81351ms step_avg:60.35ms
step:1349/2315 train_time:81411ms step_avg:60.35ms
step:1350/2315 train_time:81472ms step_avg:60.35ms
step:1351/2315 train_time:81533ms step_avg:60.35ms
step:1352/2315 train_time:81594ms step_avg:60.35ms
step:1353/2315 train_time:81655ms step_avg:60.35ms
step:1354/2315 train_time:81717ms step_avg:60.35ms
step:1355/2315 train_time:81778ms step_avg:60.35ms
step:1356/2315 train_time:81838ms step_avg:60.35ms
step:1357/2315 train_time:81900ms step_avg:60.35ms
step:1358/2315 train_time:81961ms step_avg:60.35ms
step:1359/2315 train_time:82022ms step_avg:60.35ms
step:1360/2315 train_time:82082ms step_avg:60.35ms
step:1361/2315 train_time:82142ms step_avg:60.35ms
step:1362/2315 train_time:82202ms step_avg:60.35ms
step:1363/2315 train_time:82263ms step_avg:60.35ms
step:1364/2315 train_time:82322ms step_avg:60.35ms
step:1365/2315 train_time:82383ms step_avg:60.35ms
step:1366/2315 train_time:82443ms step_avg:60.35ms
step:1367/2315 train_time:82504ms step_avg:60.35ms
step:1368/2315 train_time:82564ms step_avg:60.35ms
step:1369/2315 train_time:82625ms step_avg:60.35ms
step:1370/2315 train_time:82686ms step_avg:60.35ms
step:1371/2315 train_time:82747ms step_avg:60.36ms
step:1372/2315 train_time:82808ms step_avg:60.36ms
step:1373/2315 train_time:82870ms step_avg:60.36ms
step:1374/2315 train_time:82931ms step_avg:60.36ms
step:1375/2315 train_time:82992ms step_avg:60.36ms
step:1376/2315 train_time:83052ms step_avg:60.36ms
step:1377/2315 train_time:83113ms step_avg:60.36ms
step:1378/2315 train_time:83174ms step_avg:60.36ms
step:1379/2315 train_time:83235ms step_avg:60.36ms
step:1380/2315 train_time:83296ms step_avg:60.36ms
step:1381/2315 train_time:83357ms step_avg:60.36ms
step:1382/2315 train_time:83417ms step_avg:60.36ms
step:1383/2315 train_time:83478ms step_avg:60.36ms
step:1384/2315 train_time:83539ms step_avg:60.36ms
step:1385/2315 train_time:83599ms step_avg:60.36ms
step:1386/2315 train_time:83660ms step_avg:60.36ms
step:1387/2315 train_time:83721ms step_avg:60.36ms
step:1388/2315 train_time:83782ms step_avg:60.36ms
step:1389/2315 train_time:83843ms step_avg:60.36ms
step:1390/2315 train_time:83903ms step_avg:60.36ms
step:1391/2315 train_time:83964ms step_avg:60.36ms
step:1392/2315 train_time:84025ms step_avg:60.36ms
step:1393/2315 train_time:84085ms step_avg:60.36ms
step:1394/2315 train_time:84146ms step_avg:60.36ms
step:1395/2315 train_time:84207ms step_avg:60.36ms
step:1396/2315 train_time:84268ms step_avg:60.36ms
step:1397/2315 train_time:84329ms step_avg:60.36ms
step:1398/2315 train_time:84390ms step_avg:60.36ms
step:1399/2315 train_time:84450ms step_avg:60.36ms
step:1400/2315 train_time:84511ms step_avg:60.37ms
step:1401/2315 train_time:84572ms step_avg:60.37ms
step:1402/2315 train_time:84633ms step_avg:60.37ms
step:1403/2315 train_time:84693ms step_avg:60.37ms
step:1404/2315 train_time:84755ms step_avg:60.37ms
step:1405/2315 train_time:84816ms step_avg:60.37ms
step:1406/2315 train_time:84877ms step_avg:60.37ms
step:1407/2315 train_time:84938ms step_avg:60.37ms
step:1408/2315 train_time:84999ms step_avg:60.37ms
step:1409/2315 train_time:85060ms step_avg:60.37ms
step:1410/2315 train_time:85120ms step_avg:60.37ms
step:1411/2315 train_time:85181ms step_avg:60.37ms
step:1412/2315 train_time:85242ms step_avg:60.37ms
step:1413/2315 train_time:85302ms step_avg:60.37ms
step:1414/2315 train_time:85363ms step_avg:60.37ms
step:1415/2315 train_time:85423ms step_avg:60.37ms
step:1416/2315 train_time:85483ms step_avg:60.37ms
step:1417/2315 train_time:85544ms step_avg:60.37ms
step:1418/2315 train_time:85604ms step_avg:60.37ms
step:1419/2315 train_time:85665ms step_avg:60.37ms
step:1420/2315 train_time:85726ms step_avg:60.37ms
step:1421/2315 train_time:85787ms step_avg:60.37ms
step:1422/2315 train_time:85848ms step_avg:60.37ms
step:1423/2315 train_time:85910ms step_avg:60.37ms
step:1424/2315 train_time:85972ms step_avg:60.37ms
step:1425/2315 train_time:86033ms step_avg:60.37ms
step:1426/2315 train_time:86093ms step_avg:60.37ms
step:1427/2315 train_time:86154ms step_avg:60.37ms
step:1428/2315 train_time:86215ms step_avg:60.37ms
step:1429/2315 train_time:86276ms step_avg:60.37ms
step:1430/2315 train_time:86337ms step_avg:60.38ms
step:1431/2315 train_time:86398ms step_avg:60.38ms
step:1432/2315 train_time:86458ms step_avg:60.38ms
step:1433/2315 train_time:86519ms step_avg:60.38ms
step:1434/2315 train_time:86580ms step_avg:60.38ms
step:1435/2315 train_time:86641ms step_avg:60.38ms
step:1436/2315 train_time:86701ms step_avg:60.38ms
step:1437/2315 train_time:86762ms step_avg:60.38ms
step:1438/2315 train_time:86822ms step_avg:60.38ms
step:1439/2315 train_time:86883ms step_avg:60.38ms
step:1440/2315 train_time:86943ms step_avg:60.38ms
step:1441/2315 train_time:87004ms step_avg:60.38ms
step:1442/2315 train_time:87065ms step_avg:60.38ms
step:1443/2315 train_time:87126ms step_avg:60.38ms
step:1444/2315 train_time:87187ms step_avg:60.38ms
step:1445/2315 train_time:87249ms step_avg:60.38ms
step:1446/2315 train_time:87310ms step_avg:60.38ms
step:1447/2315 train_time:87371ms step_avg:60.38ms
step:1448/2315 train_time:87432ms step_avg:60.38ms
step:1449/2315 train_time:87493ms step_avg:60.38ms
step:1450/2315 train_time:87554ms step_avg:60.38ms
step:1451/2315 train_time:87615ms step_avg:60.38ms
step:1452/2315 train_time:87676ms step_avg:60.38ms
step:1453/2315 train_time:87737ms step_avg:60.38ms
step:1454/2315 train_time:87797ms step_avg:60.38ms
step:1455/2315 train_time:87858ms step_avg:60.38ms
step:1456/2315 train_time:87919ms step_avg:60.38ms
step:1457/2315 train_time:87980ms step_avg:60.38ms
step:1458/2315 train_time:88041ms step_avg:60.38ms
step:1459/2315 train_time:88102ms step_avg:60.38ms
step:1460/2315 train_time:88162ms step_avg:60.39ms
step:1461/2315 train_time:88223ms step_avg:60.39ms
step:1462/2315 train_time:88283ms step_avg:60.39ms
step:1463/2315 train_time:88343ms step_avg:60.39ms
step:1464/2315 train_time:88403ms step_avg:60.38ms
step:1465/2315 train_time:88465ms step_avg:60.39ms
step:1466/2315 train_time:88525ms step_avg:60.39ms
step:1467/2315 train_time:88586ms step_avg:60.39ms
step:1468/2315 train_time:88647ms step_avg:60.39ms
step:1469/2315 train_time:88707ms step_avg:60.39ms
step:1470/2315 train_time:88769ms step_avg:60.39ms
step:1471/2315 train_time:88830ms step_avg:60.39ms
step:1472/2315 train_time:88890ms step_avg:60.39ms
step:1473/2315 train_time:88951ms step_avg:60.39ms
step:1474/2315 train_time:89011ms step_avg:60.39ms
step:1475/2315 train_time:89072ms step_avg:60.39ms
step:1476/2315 train_time:89133ms step_avg:60.39ms
step:1477/2315 train_time:89195ms step_avg:60.39ms
step:1478/2315 train_time:89255ms step_avg:60.39ms
step:1479/2315 train_time:89316ms step_avg:60.39ms
step:1480/2315 train_time:89377ms step_avg:60.39ms
step:1481/2315 train_time:89438ms step_avg:60.39ms
step:1482/2315 train_time:89498ms step_avg:60.39ms
step:1483/2315 train_time:89559ms step_avg:60.39ms
step:1484/2315 train_time:89620ms step_avg:60.39ms
step:1485/2315 train_time:89681ms step_avg:60.39ms
step:1486/2315 train_time:89741ms step_avg:60.39ms
step:1487/2315 train_time:89802ms step_avg:60.39ms
step:1488/2315 train_time:89862ms step_avg:60.39ms
step:1489/2315 train_time:89922ms step_avg:60.39ms
step:1490/2315 train_time:89982ms step_avg:60.39ms
step:1491/2315 train_time:90042ms step_avg:60.39ms
step:1492/2315 train_time:90102ms step_avg:60.39ms
step:1493/2315 train_time:90163ms step_avg:60.39ms
step:1494/2315 train_time:90223ms step_avg:60.39ms
step:1495/2315 train_time:90284ms step_avg:60.39ms
step:1496/2315 train_time:90345ms step_avg:60.39ms
step:1497/2315 train_time:90406ms step_avg:60.39ms
step:1498/2315 train_time:90466ms step_avg:60.39ms
step:1499/2315 train_time:90527ms step_avg:60.39ms
step:1500/2315 train_time:90587ms step_avg:60.39ms
step:1500/2315 val_loss:3.4520 train_time:90649ms step_avg:60.43ms
step:1501/2315 train_time:90670ms step_avg:60.41ms
step:1502/2315 train_time:90710ms step_avg:60.39ms
step:1503/2315 train_time:90778ms step_avg:60.40ms
step:1504/2315 train_time:90840ms step_avg:60.40ms
step:1505/2315 train_time:90901ms step_avg:60.40ms
step:1506/2315 train_time:90962ms step_avg:60.40ms
step:1507/2315 train_time:91022ms step_avg:60.40ms
step:1508/2315 train_time:91082ms step_avg:60.40ms
step:1509/2315 train_time:91141ms step_avg:60.40ms
step:1510/2315 train_time:91201ms step_avg:60.40ms
step:1511/2315 train_time:91261ms step_avg:60.40ms
step:1512/2315 train_time:91321ms step_avg:60.40ms
step:1513/2315 train_time:91381ms step_avg:60.40ms
step:1514/2315 train_time:91441ms step_avg:60.40ms
step:1515/2315 train_time:91501ms step_avg:60.40ms
step:1516/2315 train_time:91561ms step_avg:60.40ms
step:1517/2315 train_time:91623ms step_avg:60.40ms
step:1518/2315 train_time:91686ms step_avg:60.40ms
step:1519/2315 train_time:91749ms step_avg:60.40ms
step:1520/2315 train_time:91812ms step_avg:60.40ms
step:1521/2315 train_time:91874ms step_avg:60.40ms
step:1522/2315 train_time:91934ms step_avg:60.40ms
step:1523/2315 train_time:91996ms step_avg:60.40ms
step:1524/2315 train_time:92057ms step_avg:60.40ms
step:1525/2315 train_time:92118ms step_avg:60.41ms
step:1526/2315 train_time:92178ms step_avg:60.41ms
step:1527/2315 train_time:92239ms step_avg:60.41ms
step:1528/2315 train_time:92300ms step_avg:60.41ms
step:1529/2315 train_time:92360ms step_avg:60.41ms
step:1530/2315 train_time:92421ms step_avg:60.41ms
step:1531/2315 train_time:92481ms step_avg:60.41ms
step:1532/2315 train_time:92541ms step_avg:60.41ms
step:1533/2315 train_time:92603ms step_avg:60.41ms
step:1534/2315 train_time:92664ms step_avg:60.41ms
step:1535/2315 train_time:92727ms step_avg:60.41ms
step:1536/2315 train_time:92788ms step_avg:60.41ms
step:1537/2315 train_time:92850ms step_avg:60.41ms
step:1538/2315 train_time:92911ms step_avg:60.41ms
step:1539/2315 train_time:92973ms step_avg:60.41ms
step:1540/2315 train_time:93034ms step_avg:60.41ms
step:1541/2315 train_time:93096ms step_avg:60.41ms
step:1542/2315 train_time:93157ms step_avg:60.41ms
step:1543/2315 train_time:93218ms step_avg:60.41ms
step:1544/2315 train_time:93279ms step_avg:60.41ms
step:1545/2315 train_time:93340ms step_avg:60.41ms
step:1546/2315 train_time:93401ms step_avg:60.41ms
step:1547/2315 train_time:93462ms step_avg:60.41ms
step:1548/2315 train_time:93522ms step_avg:60.41ms
step:1549/2315 train_time:93583ms step_avg:60.42ms
step:1550/2315 train_time:93644ms step_avg:60.42ms
step:1551/2315 train_time:93705ms step_avg:60.42ms
step:1552/2315 train_time:93766ms step_avg:60.42ms
step:1553/2315 train_time:93828ms step_avg:60.42ms
step:1554/2315 train_time:93889ms step_avg:60.42ms
step:1555/2315 train_time:93951ms step_avg:60.42ms
step:1556/2315 train_time:94012ms step_avg:60.42ms
step:1557/2315 train_time:94075ms step_avg:60.42ms
step:1558/2315 train_time:94136ms step_avg:60.42ms
step:1559/2315 train_time:94197ms step_avg:60.42ms
step:1560/2315 train_time:94258ms step_avg:60.42ms
step:1561/2315 train_time:94319ms step_avg:60.42ms
step:1562/2315 train_time:94380ms step_avg:60.42ms
step:1563/2315 train_time:94441ms step_avg:60.42ms
step:1564/2315 train_time:94502ms step_avg:60.42ms
step:1565/2315 train_time:94563ms step_avg:60.42ms
step:1566/2315 train_time:94624ms step_avg:60.42ms
step:1567/2315 train_time:94685ms step_avg:60.42ms
step:1568/2315 train_time:94746ms step_avg:60.42ms
step:1569/2315 train_time:94807ms step_avg:60.43ms
step:1570/2315 train_time:94868ms step_avg:60.43ms
step:1571/2315 train_time:94930ms step_avg:60.43ms
step:1572/2315 train_time:94991ms step_avg:60.43ms
step:1573/2315 train_time:95053ms step_avg:60.43ms
step:1574/2315 train_time:95115ms step_avg:60.43ms
step:1575/2315 train_time:95176ms step_avg:60.43ms
step:1576/2315 train_time:95238ms step_avg:60.43ms
step:1577/2315 train_time:95299ms step_avg:60.43ms
step:1578/2315 train_time:95360ms step_avg:60.43ms
step:1579/2315 train_time:95421ms step_avg:60.43ms
step:1580/2315 train_time:95482ms step_avg:60.43ms
step:1581/2315 train_time:95542ms step_avg:60.43ms
step:1582/2315 train_time:95603ms step_avg:60.43ms
step:1583/2315 train_time:95664ms step_avg:60.43ms
step:1584/2315 train_time:95726ms step_avg:60.43ms
step:1585/2315 train_time:95787ms step_avg:60.43ms
step:1586/2315 train_time:95848ms step_avg:60.43ms
step:1587/2315 train_time:95909ms step_avg:60.43ms
step:1588/2315 train_time:95970ms step_avg:60.43ms
step:1589/2315 train_time:96032ms step_avg:60.44ms
step:1590/2315 train_time:96094ms step_avg:60.44ms
step:1591/2315 train_time:96155ms step_avg:60.44ms
step:1592/2315 train_time:96216ms step_avg:60.44ms
step:1593/2315 train_time:96279ms step_avg:60.44ms
step:1594/2315 train_time:96340ms step_avg:60.44ms
step:1595/2315 train_time:96401ms step_avg:60.44ms
step:1596/2315 train_time:96462ms step_avg:60.44ms
step:1597/2315 train_time:96523ms step_avg:60.44ms
step:1598/2315 train_time:96583ms step_avg:60.44ms
step:1599/2315 train_time:96644ms step_avg:60.44ms
step:1600/2315 train_time:96706ms step_avg:60.44ms
step:1601/2315 train_time:96767ms step_avg:60.44ms
step:1602/2315 train_time:96827ms step_avg:60.44ms
step:1603/2315 train_time:96888ms step_avg:60.44ms
step:1604/2315 train_time:96949ms step_avg:60.44ms
step:1605/2315 train_time:97012ms step_avg:60.44ms
step:1606/2315 train_time:97073ms step_avg:60.44ms
step:1607/2315 train_time:97134ms step_avg:60.44ms
step:1608/2315 train_time:97196ms step_avg:60.44ms
step:1609/2315 train_time:97257ms step_avg:60.45ms
step:1610/2315 train_time:97319ms step_avg:60.45ms
step:1611/2315 train_time:97379ms step_avg:60.45ms
step:1612/2315 train_time:97440ms step_avg:60.45ms
step:1613/2315 train_time:97501ms step_avg:60.45ms
step:1614/2315 train_time:97562ms step_avg:60.45ms
step:1615/2315 train_time:97623ms step_avg:60.45ms
step:1616/2315 train_time:97684ms step_avg:60.45ms
step:1617/2315 train_time:97745ms step_avg:60.45ms
step:1618/2315 train_time:97805ms step_avg:60.45ms
step:1619/2315 train_time:97867ms step_avg:60.45ms
step:1620/2315 train_time:97928ms step_avg:60.45ms
step:1621/2315 train_time:97989ms step_avg:60.45ms
step:1622/2315 train_time:98050ms step_avg:60.45ms
step:1623/2315 train_time:98112ms step_avg:60.45ms
step:1624/2315 train_time:98173ms step_avg:60.45ms
step:1625/2315 train_time:98235ms step_avg:60.45ms
step:1626/2315 train_time:98296ms step_avg:60.45ms
step:1627/2315 train_time:98358ms step_avg:60.45ms
step:1628/2315 train_time:98419ms step_avg:60.45ms
step:1629/2315 train_time:98480ms step_avg:60.45ms
step:1630/2315 train_time:98541ms step_avg:60.45ms
step:1631/2315 train_time:98602ms step_avg:60.45ms
step:1632/2315 train_time:98662ms step_avg:60.45ms
step:1633/2315 train_time:98723ms step_avg:60.45ms
step:1634/2315 train_time:98784ms step_avg:60.46ms
step:1635/2315 train_time:98845ms step_avg:60.46ms
step:1636/2315 train_time:98906ms step_avg:60.46ms
step:1637/2315 train_time:98968ms step_avg:60.46ms
step:1638/2315 train_time:99029ms step_avg:60.46ms
step:1639/2315 train_time:99091ms step_avg:60.46ms
step:1640/2315 train_time:99153ms step_avg:60.46ms
step:1641/2315 train_time:99214ms step_avg:60.46ms
step:1642/2315 train_time:99275ms step_avg:60.46ms
step:1643/2315 train_time:99337ms step_avg:60.46ms
step:1644/2315 train_time:99398ms step_avg:60.46ms
step:1645/2315 train_time:99458ms step_avg:60.46ms
step:1646/2315 train_time:99519ms step_avg:60.46ms
step:1647/2315 train_time:99580ms step_avg:60.46ms
step:1648/2315 train_time:99641ms step_avg:60.46ms
step:1649/2315 train_time:99702ms step_avg:60.46ms
step:1650/2315 train_time:99763ms step_avg:60.46ms
step:1651/2315 train_time:99823ms step_avg:60.46ms
step:1652/2315 train_time:99885ms step_avg:60.46ms
step:1653/2315 train_time:99946ms step_avg:60.46ms
step:1654/2315 train_time:100007ms step_avg:60.46ms
step:1655/2315 train_time:100068ms step_avg:60.46ms
step:1656/2315 train_time:100130ms step_avg:60.46ms
step:1657/2315 train_time:100192ms step_avg:60.47ms
step:1658/2315 train_time:100253ms step_avg:60.47ms
step:1659/2315 train_time:100315ms step_avg:60.47ms
step:1660/2315 train_time:100377ms step_avg:60.47ms
step:1661/2315 train_time:100437ms step_avg:60.47ms
step:1662/2315 train_time:100498ms step_avg:60.47ms
step:1663/2315 train_time:100559ms step_avg:60.47ms
step:1664/2315 train_time:100621ms step_avg:60.47ms
step:1665/2315 train_time:100681ms step_avg:60.47ms
step:1666/2315 train_time:100743ms step_avg:60.47ms
step:1667/2315 train_time:100803ms step_avg:60.47ms
step:1668/2315 train_time:100864ms step_avg:60.47ms
step:1669/2315 train_time:100926ms step_avg:60.47ms
step:1670/2315 train_time:100987ms step_avg:60.47ms
step:1671/2315 train_time:101048ms step_avg:60.47ms
step:1672/2315 train_time:101109ms step_avg:60.47ms
step:1673/2315 train_time:101172ms step_avg:60.47ms
step:1674/2315 train_time:101233ms step_avg:60.47ms
step:1675/2315 train_time:101294ms step_avg:60.47ms
step:1676/2315 train_time:101356ms step_avg:60.47ms
step:1677/2315 train_time:101417ms step_avg:60.48ms
step:1678/2315 train_time:101478ms step_avg:60.48ms
step:1679/2315 train_time:101539ms step_avg:60.48ms
step:1680/2315 train_time:101601ms step_avg:60.48ms
step:1681/2315 train_time:101662ms step_avg:60.48ms
step:1682/2315 train_time:101722ms step_avg:60.48ms
step:1683/2315 train_time:101783ms step_avg:60.48ms
step:1684/2315 train_time:101844ms step_avg:60.48ms
step:1685/2315 train_time:101906ms step_avg:60.48ms
step:1686/2315 train_time:101967ms step_avg:60.48ms
step:1687/2315 train_time:102028ms step_avg:60.48ms
step:1688/2315 train_time:102089ms step_avg:60.48ms
step:1689/2315 train_time:102151ms step_avg:60.48ms
step:1690/2315 train_time:102212ms step_avg:60.48ms
step:1691/2315 train_time:102274ms step_avg:60.48ms
step:1692/2315 train_time:102335ms step_avg:60.48ms
step:1693/2315 train_time:102396ms step_avg:60.48ms
step:1694/2315 train_time:102458ms step_avg:60.48ms
step:1695/2315 train_time:102519ms step_avg:60.48ms
step:1696/2315 train_time:102580ms step_avg:60.48ms
step:1697/2315 train_time:102641ms step_avg:60.48ms
step:1698/2315 train_time:102702ms step_avg:60.48ms
step:1699/2315 train_time:102763ms step_avg:60.48ms
step:1700/2315 train_time:102824ms step_avg:60.48ms
step:1701/2315 train_time:102885ms step_avg:60.49ms
step:1702/2315 train_time:102946ms step_avg:60.49ms
step:1703/2315 train_time:103008ms step_avg:60.49ms
step:1704/2315 train_time:103069ms step_avg:60.49ms
step:1705/2315 train_time:103131ms step_avg:60.49ms
step:1706/2315 train_time:103192ms step_avg:60.49ms
step:1707/2315 train_time:103254ms step_avg:60.49ms
step:1708/2315 train_time:103315ms step_avg:60.49ms
step:1709/2315 train_time:103376ms step_avg:60.49ms
step:1710/2315 train_time:103437ms step_avg:60.49ms
step:1711/2315 train_time:103498ms step_avg:60.49ms
step:1712/2315 train_time:103559ms step_avg:60.49ms
step:1713/2315 train_time:103620ms step_avg:60.49ms
step:1714/2315 train_time:103682ms step_avg:60.49ms
step:1715/2315 train_time:103743ms step_avg:60.49ms
step:1716/2315 train_time:103804ms step_avg:60.49ms
step:1717/2315 train_time:103865ms step_avg:60.49ms
step:1718/2315 train_time:103925ms step_avg:60.49ms
step:1719/2315 train_time:103987ms step_avg:60.49ms
step:1720/2315 train_time:104048ms step_avg:60.49ms
step:1721/2315 train_time:104109ms step_avg:60.49ms
step:1722/2315 train_time:104171ms step_avg:60.49ms
step:1723/2315 train_time:104233ms step_avg:60.50ms
step:1724/2315 train_time:104294ms step_avg:60.50ms
step:1725/2315 train_time:104355ms step_avg:60.50ms
step:1726/2315 train_time:104416ms step_avg:60.50ms
step:1727/2315 train_time:104478ms step_avg:60.50ms
step:1728/2315 train_time:104539ms step_avg:60.50ms
step:1729/2315 train_time:104600ms step_avg:60.50ms
step:1730/2315 train_time:104661ms step_avg:60.50ms
step:1731/2315 train_time:104722ms step_avg:60.50ms
step:1732/2315 train_time:104783ms step_avg:60.50ms
step:1733/2315 train_time:104844ms step_avg:60.50ms
step:1734/2315 train_time:104905ms step_avg:60.50ms
step:1735/2315 train_time:104967ms step_avg:60.50ms
step:1736/2315 train_time:105027ms step_avg:60.50ms
step:1737/2315 train_time:105089ms step_avg:60.50ms
step:1738/2315 train_time:105150ms step_avg:60.50ms
step:1739/2315 train_time:105212ms step_avg:60.50ms
step:1740/2315 train_time:105274ms step_avg:60.50ms
step:1741/2315 train_time:105336ms step_avg:60.50ms
step:1742/2315 train_time:105397ms step_avg:60.50ms
step:1743/2315 train_time:105459ms step_avg:60.50ms
step:1744/2315 train_time:105519ms step_avg:60.50ms
step:1745/2315 train_time:105580ms step_avg:60.50ms
step:1746/2315 train_time:105642ms step_avg:60.51ms
step:1747/2315 train_time:105702ms step_avg:60.50ms
step:1748/2315 train_time:105763ms step_avg:60.51ms
step:1749/2315 train_time:105824ms step_avg:60.51ms
step:1750/2315 train_time:105884ms step_avg:60.51ms
step:1750/2315 val_loss:3.3830 train_time:105947ms step_avg:60.54ms
step:1751/2315 train_time:105969ms step_avg:60.52ms
step:1752/2315 train_time:106008ms step_avg:60.51ms
step:1753/2315 train_time:106073ms step_avg:60.51ms
step:1754/2315 train_time:106136ms step_avg:60.51ms
step:1755/2315 train_time:106197ms step_avg:60.51ms
step:1756/2315 train_time:106257ms step_avg:60.51ms
step:1757/2315 train_time:106317ms step_avg:60.51ms
step:1758/2315 train_time:106378ms step_avg:60.51ms
step:1759/2315 train_time:106438ms step_avg:60.51ms
step:1760/2315 train_time:106499ms step_avg:60.51ms
step:1761/2315 train_time:106560ms step_avg:60.51ms
step:1762/2315 train_time:106621ms step_avg:60.51ms
step:1763/2315 train_time:106681ms step_avg:60.51ms
step:1764/2315 train_time:106741ms step_avg:60.51ms
step:1765/2315 train_time:106801ms step_avg:60.51ms
step:1766/2315 train_time:106862ms step_avg:60.51ms
step:1767/2315 train_time:106925ms step_avg:60.51ms
step:1768/2315 train_time:106987ms step_avg:60.51ms
step:1769/2315 train_time:107049ms step_avg:60.51ms
step:1770/2315 train_time:107111ms step_avg:60.51ms
step:1771/2315 train_time:107172ms step_avg:60.52ms
step:1772/2315 train_time:107233ms step_avg:60.52ms
step:1773/2315 train_time:107294ms step_avg:60.52ms
step:1774/2315 train_time:107355ms step_avg:60.52ms
step:1775/2315 train_time:107415ms step_avg:60.52ms
step:1776/2315 train_time:107476ms step_avg:60.52ms
step:1777/2315 train_time:107537ms step_avg:60.52ms
step:1778/2315 train_time:107597ms step_avg:60.52ms
step:1779/2315 train_time:107658ms step_avg:60.52ms
step:1780/2315 train_time:107718ms step_avg:60.52ms
step:1781/2315 train_time:107779ms step_avg:60.52ms
step:1782/2315 train_time:107840ms step_avg:60.52ms
step:1783/2315 train_time:107901ms step_avg:60.52ms
step:1784/2315 train_time:107964ms step_avg:60.52ms
step:1785/2315 train_time:108027ms step_avg:60.52ms
step:1786/2315 train_time:108088ms step_avg:60.52ms
step:1787/2315 train_time:108150ms step_avg:60.52ms
step:1788/2315 train_time:108212ms step_avg:60.52ms
step:1789/2315 train_time:108273ms step_avg:60.52ms
step:1790/2315 train_time:108333ms step_avg:60.52ms
step:1791/2315 train_time:108394ms step_avg:60.52ms
step:1792/2315 train_time:108455ms step_avg:60.52ms
step:1793/2315 train_time:108516ms step_avg:60.52ms
step:1794/2315 train_time:108577ms step_avg:60.52ms
step:1795/2315 train_time:108637ms step_avg:60.52ms
step:1796/2315 train_time:108697ms step_avg:60.52ms
step:1797/2315 train_time:108758ms step_avg:60.52ms
step:1798/2315 train_time:108819ms step_avg:60.52ms
step:1799/2315 train_time:108880ms step_avg:60.52ms
step:1800/2315 train_time:108941ms step_avg:60.52ms
step:1801/2315 train_time:109002ms step_avg:60.52ms
step:1802/2315 train_time:109065ms step_avg:60.52ms
step:1803/2315 train_time:109127ms step_avg:60.53ms
step:1804/2315 train_time:109188ms step_avg:60.53ms
step:1805/2315 train_time:109250ms step_avg:60.53ms
step:1806/2315 train_time:109312ms step_avg:60.53ms
step:1807/2315 train_time:109373ms step_avg:60.53ms
step:1808/2315 train_time:109434ms step_avg:60.53ms
step:1809/2315 train_time:109495ms step_avg:60.53ms
step:1810/2315 train_time:109556ms step_avg:60.53ms
step:1811/2315 train_time:109616ms step_avg:60.53ms
step:1812/2315 train_time:109676ms step_avg:60.53ms
step:1813/2315 train_time:109737ms step_avg:60.53ms
step:1814/2315 train_time:109798ms step_avg:60.53ms
step:1815/2315 train_time:109858ms step_avg:60.53ms
step:1816/2315 train_time:109920ms step_avg:60.53ms
step:1817/2315 train_time:109980ms step_avg:60.53ms
step:1818/2315 train_time:110042ms step_avg:60.53ms
step:1819/2315 train_time:110103ms step_avg:60.53ms
step:1820/2315 train_time:110164ms step_avg:60.53ms
step:1821/2315 train_time:110226ms step_avg:60.53ms
step:1822/2315 train_time:110287ms step_avg:60.53ms
step:1823/2315 train_time:110349ms step_avg:60.53ms
step:1824/2315 train_time:110410ms step_avg:60.53ms
step:1825/2315 train_time:110471ms step_avg:60.53ms
step:1826/2315 train_time:110532ms step_avg:60.53ms
step:1827/2315 train_time:110593ms step_avg:60.53ms
step:1828/2315 train_time:110655ms step_avg:60.53ms
step:1829/2315 train_time:110715ms step_avg:60.53ms
step:1830/2315 train_time:110776ms step_avg:60.53ms
step:1831/2315 train_time:110837ms step_avg:60.53ms
step:1832/2315 train_time:110898ms step_avg:60.53ms
step:1833/2315 train_time:110958ms step_avg:60.53ms
step:1834/2315 train_time:111019ms step_avg:60.53ms
step:1835/2315 train_time:111081ms step_avg:60.53ms
step:1836/2315 train_time:111142ms step_avg:60.53ms
step:1837/2315 train_time:111204ms step_avg:60.54ms
step:1838/2315 train_time:111266ms step_avg:60.54ms
step:1839/2315 train_time:111327ms step_avg:60.54ms
step:1840/2315 train_time:111388ms step_avg:60.54ms
step:1841/2315 train_time:111450ms step_avg:60.54ms
step:1842/2315 train_time:111511ms step_avg:60.54ms
step:1843/2315 train_time:111572ms step_avg:60.54ms
step:1844/2315 train_time:111632ms step_avg:60.54ms
step:1845/2315 train_time:111694ms step_avg:60.54ms
step:1846/2315 train_time:111756ms step_avg:60.54ms
step:1847/2315 train_time:111817ms step_avg:60.54ms
step:1848/2315 train_time:111877ms step_avg:60.54ms
step:1849/2315 train_time:111938ms step_avg:60.54ms
step:1850/2315 train_time:111999ms step_avg:60.54ms
step:1851/2315 train_time:112060ms step_avg:60.54ms
step:1852/2315 train_time:112121ms step_avg:60.54ms
step:1853/2315 train_time:112182ms step_avg:60.54ms
step:1854/2315 train_time:112244ms step_avg:60.54ms
step:1855/2315 train_time:112306ms step_avg:60.54ms
step:1856/2315 train_time:112366ms step_avg:60.54ms
step:1857/2315 train_time:112428ms step_avg:60.54ms
step:1858/2315 train_time:112490ms step_avg:60.54ms
step:1859/2315 train_time:112551ms step_avg:60.54ms
step:1860/2315 train_time:112612ms step_avg:60.54ms
step:1861/2315 train_time:112673ms step_avg:60.54ms
step:1862/2315 train_time:112734ms step_avg:60.54ms
step:1863/2315 train_time:112795ms step_avg:60.55ms
step:1864/2315 train_time:112856ms step_avg:60.55ms
step:1865/2315 train_time:112917ms step_avg:60.55ms
step:1866/2315 train_time:112978ms step_avg:60.55ms
step:1867/2315 train_time:113039ms step_avg:60.55ms
step:1868/2315 train_time:113100ms step_avg:60.55ms
step:1869/2315 train_time:113161ms step_avg:60.55ms
step:1870/2315 train_time:113222ms step_avg:60.55ms
step:1871/2315 train_time:113283ms step_avg:60.55ms
step:1872/2315 train_time:113344ms step_avg:60.55ms
step:1873/2315 train_time:113405ms step_avg:60.55ms
step:1874/2315 train_time:113466ms step_avg:60.55ms
step:1875/2315 train_time:113528ms step_avg:60.55ms
step:1876/2315 train_time:113589ms step_avg:60.55ms
step:1877/2315 train_time:113650ms step_avg:60.55ms
step:1878/2315 train_time:113712ms step_avg:60.55ms
step:1879/2315 train_time:113773ms step_avg:60.55ms
step:1880/2315 train_time:113834ms step_avg:60.55ms
step:1881/2315 train_time:113895ms step_avg:60.55ms
step:1882/2315 train_time:113956ms step_avg:60.55ms
step:1883/2315 train_time:114017ms step_avg:60.55ms
step:1884/2315 train_time:114078ms step_avg:60.55ms
step:1885/2315 train_time:114139ms step_avg:60.55ms
step:1886/2315 train_time:114199ms step_avg:60.55ms
step:1887/2315 train_time:114261ms step_avg:60.55ms
step:1888/2315 train_time:114322ms step_avg:60.55ms
step:1889/2315 train_time:114383ms step_avg:60.55ms
step:1890/2315 train_time:114444ms step_avg:60.55ms
step:1891/2315 train_time:114505ms step_avg:60.55ms
step:1892/2315 train_time:114566ms step_avg:60.55ms
step:1893/2315 train_time:114627ms step_avg:60.55ms
step:1894/2315 train_time:114689ms step_avg:60.55ms
step:1895/2315 train_time:114751ms step_avg:60.55ms
step:1896/2315 train_time:114812ms step_avg:60.55ms
step:1897/2315 train_time:114873ms step_avg:60.56ms
step:1898/2315 train_time:114934ms step_avg:60.56ms
step:1899/2315 train_time:114996ms step_avg:60.56ms
step:1900/2315 train_time:115057ms step_avg:60.56ms
step:1901/2315 train_time:115119ms step_avg:60.56ms
step:1902/2315 train_time:115180ms step_avg:60.56ms
step:1903/2315 train_time:115240ms step_avg:60.56ms
step:1904/2315 train_time:115301ms step_avg:60.56ms
step:1905/2315 train_time:115363ms step_avg:60.56ms
step:1906/2315 train_time:115424ms step_avg:60.56ms
step:1907/2315 train_time:115485ms step_avg:60.56ms
step:1908/2315 train_time:115546ms step_avg:60.56ms
step:1909/2315 train_time:115607ms step_avg:60.56ms
step:1910/2315 train_time:115668ms step_avg:60.56ms
step:1911/2315 train_time:115729ms step_avg:60.56ms
step:1912/2315 train_time:115791ms step_avg:60.56ms
step:1913/2315 train_time:115852ms step_avg:60.56ms
step:1914/2315 train_time:115913ms step_avg:60.56ms
step:1915/2315 train_time:115974ms step_avg:60.56ms
step:1916/2315 train_time:116035ms step_avg:60.56ms
step:1917/2315 train_time:116096ms step_avg:60.56ms
step:1918/2315 train_time:116157ms step_avg:60.56ms
step:1919/2315 train_time:116218ms step_avg:60.56ms
step:1920/2315 train_time:116279ms step_avg:60.56ms
step:1921/2315 train_time:116339ms step_avg:60.56ms
step:1922/2315 train_time:116400ms step_avg:60.56ms
step:1923/2315 train_time:116461ms step_avg:60.56ms
step:1924/2315 train_time:116523ms step_avg:60.56ms
step:1925/2315 train_time:116586ms step_avg:60.56ms
step:1926/2315 train_time:116646ms step_avg:60.56ms
step:1927/2315 train_time:116707ms step_avg:60.56ms
step:1928/2315 train_time:116769ms step_avg:60.56ms
step:1929/2315 train_time:116830ms step_avg:60.57ms
step:1930/2315 train_time:116892ms step_avg:60.57ms
step:1931/2315 train_time:116953ms step_avg:60.57ms
step:1932/2315 train_time:117014ms step_avg:60.57ms
step:1933/2315 train_time:117076ms step_avg:60.57ms
step:1934/2315 train_time:117137ms step_avg:60.57ms
step:1935/2315 train_time:117198ms step_avg:60.57ms
step:1936/2315 train_time:117259ms step_avg:60.57ms
step:1937/2315 train_time:117319ms step_avg:60.57ms
step:1938/2315 train_time:117380ms step_avg:60.57ms
step:1939/2315 train_time:117441ms step_avg:60.57ms
step:1940/2315 train_time:117502ms step_avg:60.57ms
step:1941/2315 train_time:117563ms step_avg:60.57ms
step:1942/2315 train_time:117624ms step_avg:60.57ms
step:1943/2315 train_time:117685ms step_avg:60.57ms
step:1944/2315 train_time:117746ms step_avg:60.57ms
step:1945/2315 train_time:117808ms step_avg:60.57ms
step:1946/2315 train_time:117870ms step_avg:60.57ms
step:1947/2315 train_time:117932ms step_avg:60.57ms
step:1948/2315 train_time:117993ms step_avg:60.57ms
step:1949/2315 train_time:118054ms step_avg:60.57ms
step:1950/2315 train_time:118115ms step_avg:60.57ms
step:1951/2315 train_time:118177ms step_avg:60.57ms
step:1952/2315 train_time:118238ms step_avg:60.57ms
step:1953/2315 train_time:118299ms step_avg:60.57ms
step:1954/2315 train_time:118359ms step_avg:60.57ms
step:1955/2315 train_time:118420ms step_avg:60.57ms
step:1956/2315 train_time:118481ms step_avg:60.57ms
step:1957/2315 train_time:118542ms step_avg:60.57ms
step:1958/2315 train_time:118604ms step_avg:60.57ms
step:1959/2315 train_time:118665ms step_avg:60.57ms
step:1960/2315 train_time:118726ms step_avg:60.57ms
step:1961/2315 train_time:118787ms step_avg:60.57ms
step:1962/2315 train_time:118848ms step_avg:60.57ms
step:1963/2315 train_time:118910ms step_avg:60.58ms
step:1964/2315 train_time:118972ms step_avg:60.58ms
step:1965/2315 train_time:119032ms step_avg:60.58ms
step:1966/2315 train_time:119093ms step_avg:60.58ms
step:1967/2315 train_time:119155ms step_avg:60.58ms
step:1968/2315 train_time:119216ms step_avg:60.58ms
step:1969/2315 train_time:119277ms step_avg:60.58ms
step:1970/2315 train_time:119339ms step_avg:60.58ms
step:1971/2315 train_time:119400ms step_avg:60.58ms
step:1972/2315 train_time:119461ms step_avg:60.58ms
step:1973/2315 train_time:119522ms step_avg:60.58ms
step:1974/2315 train_time:119582ms step_avg:60.58ms
step:1975/2315 train_time:119643ms step_avg:60.58ms
step:1976/2315 train_time:119704ms step_avg:60.58ms
step:1977/2315 train_time:119765ms step_avg:60.58ms
step:1978/2315 train_time:119826ms step_avg:60.58ms
step:1979/2315 train_time:119887ms step_avg:60.58ms
step:1980/2315 train_time:119949ms step_avg:60.58ms
step:1981/2315 train_time:120011ms step_avg:60.58ms
step:1982/2315 train_time:120072ms step_avg:60.58ms
step:1983/2315 train_time:120133ms step_avg:60.58ms
step:1984/2315 train_time:120194ms step_avg:60.58ms
step:1985/2315 train_time:120256ms step_avg:60.58ms
step:1986/2315 train_time:120317ms step_avg:60.58ms
step:1987/2315 train_time:120377ms step_avg:60.58ms
step:1988/2315 train_time:120438ms step_avg:60.58ms
step:1989/2315 train_time:120499ms step_avg:60.58ms
step:1990/2315 train_time:120559ms step_avg:60.58ms
step:1991/2315 train_time:120620ms step_avg:60.58ms
step:1992/2315 train_time:120682ms step_avg:60.58ms
step:1993/2315 train_time:120744ms step_avg:60.58ms
step:1994/2315 train_time:120805ms step_avg:60.58ms
step:1995/2315 train_time:120866ms step_avg:60.58ms
step:1996/2315 train_time:120928ms step_avg:60.58ms
step:1997/2315 train_time:120989ms step_avg:60.59ms
step:1998/2315 train_time:121050ms step_avg:60.59ms
step:1999/2315 train_time:121112ms step_avg:60.59ms
step:2000/2315 train_time:121173ms step_avg:60.59ms
step:2000/2315 val_loss:3.3302 train_time:121236ms step_avg:60.62ms
step:2001/2315 train_time:121257ms step_avg:60.60ms
step:2002/2315 train_time:121300ms step_avg:60.59ms
step:2003/2315 train_time:121364ms step_avg:60.59ms
step:2004/2315 train_time:121428ms step_avg:60.59ms
step:2005/2315 train_time:121489ms step_avg:60.59ms
step:2006/2315 train_time:121551ms step_avg:60.59ms
step:2007/2315 train_time:121613ms step_avg:60.59ms
step:2008/2315 train_time:121673ms step_avg:60.59ms
step:2009/2315 train_time:121734ms step_avg:60.59ms
step:2010/2315 train_time:121795ms step_avg:60.59ms
step:2011/2315 train_time:121856ms step_avg:60.59ms
step:2012/2315 train_time:121917ms step_avg:60.59ms
step:2013/2315 train_time:121977ms step_avg:60.59ms
step:2014/2315 train_time:122038ms step_avg:60.59ms
step:2015/2315 train_time:122098ms step_avg:60.59ms
step:2016/2315 train_time:122158ms step_avg:60.59ms
step:2017/2315 train_time:122220ms step_avg:60.59ms
step:2018/2315 train_time:122281ms step_avg:60.60ms
step:2019/2315 train_time:122343ms step_avg:60.60ms
step:2020/2315 train_time:122406ms step_avg:60.60ms
step:2021/2315 train_time:122467ms step_avg:60.60ms
step:2022/2315 train_time:122528ms step_avg:60.60ms
step:2023/2315 train_time:122590ms step_avg:60.60ms
step:2024/2315 train_time:122651ms step_avg:60.60ms
step:2025/2315 train_time:122713ms step_avg:60.60ms
step:2026/2315 train_time:122774ms step_avg:60.60ms
step:2027/2315 train_time:122835ms step_avg:60.60ms
step:2028/2315 train_time:122896ms step_avg:60.60ms
step:2029/2315 train_time:122957ms step_avg:60.60ms
step:2030/2315 train_time:123017ms step_avg:60.60ms
step:2031/2315 train_time:123078ms step_avg:60.60ms
step:2032/2315 train_time:123139ms step_avg:60.60ms
step:2033/2315 train_time:123200ms step_avg:60.60ms
step:2034/2315 train_time:123260ms step_avg:60.60ms
step:2035/2315 train_time:123322ms step_avg:60.60ms
step:2036/2315 train_time:123383ms step_avg:60.60ms
step:2037/2315 train_time:123444ms step_avg:60.60ms
step:2038/2315 train_time:123505ms step_avg:60.60ms
step:2039/2315 train_time:123567ms step_avg:60.60ms
step:2040/2315 train_time:123628ms step_avg:60.60ms
step:2041/2315 train_time:123689ms step_avg:60.60ms
step:2042/2315 train_time:123750ms step_avg:60.60ms
step:2043/2315 train_time:123812ms step_avg:60.60ms
step:2044/2315 train_time:123873ms step_avg:60.60ms
step:2045/2315 train_time:123934ms step_avg:60.60ms
step:2046/2315 train_time:123995ms step_avg:60.60ms
step:2047/2315 train_time:124057ms step_avg:60.60ms
step:2048/2315 train_time:124117ms step_avg:60.60ms
step:2049/2315 train_time:124179ms step_avg:60.60ms
step:2050/2315 train_time:124240ms step_avg:60.60ms
step:2051/2315 train_time:124301ms step_avg:60.60ms
step:2052/2315 train_time:124362ms step_avg:60.61ms
step:2053/2315 train_time:124423ms step_avg:60.61ms
step:2054/2315 train_time:124484ms step_avg:60.61ms
step:2055/2315 train_time:124545ms step_avg:60.61ms
step:2056/2315 train_time:124607ms step_avg:60.61ms
step:2057/2315 train_time:124669ms step_avg:60.61ms
step:2058/2315 train_time:124730ms step_avg:60.61ms
step:2059/2315 train_time:124791ms step_avg:60.61ms
step:2060/2315 train_time:124852ms step_avg:60.61ms
step:2061/2315 train_time:124913ms step_avg:60.61ms
step:2062/2315 train_time:124975ms step_avg:60.61ms
step:2063/2315 train_time:125036ms step_avg:60.61ms
step:2064/2315 train_time:125097ms step_avg:60.61ms
step:2065/2315 train_time:125158ms step_avg:60.61ms
step:2066/2315 train_time:125220ms step_avg:60.61ms
step:2067/2315 train_time:125281ms step_avg:60.61ms
step:2068/2315 train_time:125342ms step_avg:60.61ms
step:2069/2315 train_time:125403ms step_avg:60.61ms
step:2070/2315 train_time:125464ms step_avg:60.61ms
step:2071/2315 train_time:125525ms step_avg:60.61ms
step:2072/2315 train_time:125586ms step_avg:60.61ms
step:2073/2315 train_time:125648ms step_avg:60.61ms
step:2074/2315 train_time:125709ms step_avg:60.61ms
step:2075/2315 train_time:125770ms step_avg:60.61ms
step:2076/2315 train_time:125832ms step_avg:60.61ms
step:2077/2315 train_time:125893ms step_avg:60.61ms
step:2078/2315 train_time:125954ms step_avg:60.61ms
step:2079/2315 train_time:126015ms step_avg:60.61ms
step:2080/2315 train_time:126076ms step_avg:60.61ms
step:2081/2315 train_time:126137ms step_avg:60.61ms
step:2082/2315 train_time:126199ms step_avg:60.61ms
step:2083/2315 train_time:126260ms step_avg:60.61ms
step:2084/2315 train_time:126322ms step_avg:60.61ms
step:2085/2315 train_time:126383ms step_avg:60.62ms
step:2086/2315 train_time:126444ms step_avg:60.62ms
step:2087/2315 train_time:126504ms step_avg:60.62ms
step:2088/2315 train_time:126566ms step_avg:60.62ms
step:2089/2315 train_time:126628ms step_avg:60.62ms
step:2090/2315 train_time:126689ms step_avg:60.62ms
step:2091/2315 train_time:126750ms step_avg:60.62ms
step:2092/2315 train_time:126811ms step_avg:60.62ms
step:2093/2315 train_time:126872ms step_avg:60.62ms
step:2094/2315 train_time:126934ms step_avg:60.62ms
step:2095/2315 train_time:126995ms step_avg:60.62ms
step:2096/2315 train_time:127056ms step_avg:60.62ms
step:2097/2315 train_time:127117ms step_avg:60.62ms
step:2098/2315 train_time:127178ms step_avg:60.62ms
step:2099/2315 train_time:127240ms step_avg:60.62ms
step:2100/2315 train_time:127300ms step_avg:60.62ms
step:2101/2315 train_time:127362ms step_avg:60.62ms
step:2102/2315 train_time:127423ms step_avg:60.62ms
step:2103/2315 train_time:127484ms step_avg:60.62ms
step:2104/2315 train_time:127546ms step_avg:60.62ms
step:2105/2315 train_time:127608ms step_avg:60.62ms
step:2106/2315 train_time:127669ms step_avg:60.62ms
step:2107/2315 train_time:127730ms step_avg:60.62ms
step:2108/2315 train_time:127791ms step_avg:60.62ms
step:2109/2315 train_time:127853ms step_avg:60.62ms
step:2110/2315 train_time:127914ms step_avg:60.62ms
step:2111/2315 train_time:127975ms step_avg:60.62ms
step:2112/2315 train_time:128036ms step_avg:60.62ms
step:2113/2315 train_time:128097ms step_avg:60.62ms
step:2114/2315 train_time:128158ms step_avg:60.62ms
step:2115/2315 train_time:128220ms step_avg:60.62ms
step:2116/2315 train_time:128280ms step_avg:60.62ms
step:2117/2315 train_time:128341ms step_avg:60.62ms
step:2118/2315 train_time:128402ms step_avg:60.62ms
step:2119/2315 train_time:128463ms step_avg:60.62ms
step:2120/2315 train_time:128524ms step_avg:60.62ms
step:2121/2315 train_time:128586ms step_avg:60.63ms
step:2122/2315 train_time:128647ms step_avg:60.63ms
step:2123/2315 train_time:128709ms step_avg:60.63ms
step:2124/2315 train_time:128770ms step_avg:60.63ms
step:2125/2315 train_time:128832ms step_avg:60.63ms
step:2126/2315 train_time:128892ms step_avg:60.63ms
step:2127/2315 train_time:128953ms step_avg:60.63ms
step:2128/2315 train_time:129015ms step_avg:60.63ms
step:2129/2315 train_time:129076ms step_avg:60.63ms
step:2130/2315 train_time:129138ms step_avg:60.63ms
step:2131/2315 train_time:129199ms step_avg:60.63ms
step:2132/2315 train_time:129260ms step_avg:60.63ms
step:2133/2315 train_time:129321ms step_avg:60.63ms
step:2134/2315 train_time:129382ms step_avg:60.63ms
step:2135/2315 train_time:129444ms step_avg:60.63ms
step:2136/2315 train_time:129506ms step_avg:60.63ms
step:2137/2315 train_time:129566ms step_avg:60.63ms
step:2138/2315 train_time:129627ms step_avg:60.63ms
step:2139/2315 train_time:129689ms step_avg:60.63ms
step:2140/2315 train_time:129750ms step_avg:60.63ms
step:2141/2315 train_time:129811ms step_avg:60.63ms
step:2142/2315 train_time:129872ms step_avg:60.63ms
step:2143/2315 train_time:129934ms step_avg:60.63ms
step:2144/2315 train_time:129995ms step_avg:60.63ms
step:2145/2315 train_time:130056ms step_avg:60.63ms
step:2146/2315 train_time:130117ms step_avg:60.63ms
step:2147/2315 train_time:130179ms step_avg:60.63ms
step:2148/2315 train_time:130239ms step_avg:60.63ms
step:2149/2315 train_time:130301ms step_avg:60.63ms
step:2150/2315 train_time:130362ms step_avg:60.63ms
step:2151/2315 train_time:130424ms step_avg:60.63ms
step:2152/2315 train_time:130485ms step_avg:60.63ms
step:2153/2315 train_time:130546ms step_avg:60.63ms
step:2154/2315 train_time:130607ms step_avg:60.63ms
step:2155/2315 train_time:130668ms step_avg:60.64ms
step:2156/2315 train_time:130729ms step_avg:60.64ms
step:2157/2315 train_time:130790ms step_avg:60.64ms
step:2158/2315 train_time:130852ms step_avg:60.64ms
step:2159/2315 train_time:130913ms step_avg:60.64ms
step:2160/2315 train_time:130975ms step_avg:60.64ms
step:2161/2315 train_time:131036ms step_avg:60.64ms
step:2162/2315 train_time:131097ms step_avg:60.64ms
step:2163/2315 train_time:131158ms step_avg:60.64ms
step:2164/2315 train_time:131219ms step_avg:60.64ms
step:2165/2315 train_time:131281ms step_avg:60.64ms
step:2166/2315 train_time:131342ms step_avg:60.64ms
step:2167/2315 train_time:131403ms step_avg:60.64ms
step:2168/2315 train_time:131464ms step_avg:60.64ms
step:2169/2315 train_time:131525ms step_avg:60.64ms
step:2170/2315 train_time:131586ms step_avg:60.64ms
step:2171/2315 train_time:131649ms step_avg:60.64ms
step:2172/2315 train_time:131710ms step_avg:60.64ms
step:2173/2315 train_time:131771ms step_avg:60.64ms
step:2174/2315 train_time:131832ms step_avg:60.64ms
step:2175/2315 train_time:131894ms step_avg:60.64ms
step:2176/2315 train_time:131956ms step_avg:60.64ms
step:2177/2315 train_time:132017ms step_avg:60.64ms
step:2178/2315 train_time:132078ms step_avg:60.64ms
step:2179/2315 train_time:132139ms step_avg:60.64ms
step:2180/2315 train_time:132200ms step_avg:60.64ms
step:2181/2315 train_time:132262ms step_avg:60.64ms
step:2182/2315 train_time:132323ms step_avg:60.64ms
step:2183/2315 train_time:132384ms step_avg:60.64ms
step:2184/2315 train_time:132445ms step_avg:60.64ms
step:2185/2315 train_time:132506ms step_avg:60.64ms
step:2186/2315 train_time:132568ms step_avg:60.64ms
step:2187/2315 train_time:132629ms step_avg:60.64ms
step:2188/2315 train_time:132690ms step_avg:60.64ms
step:2189/2315 train_time:132751ms step_avg:60.64ms
step:2190/2315 train_time:132813ms step_avg:60.65ms
step:2191/2315 train_time:132875ms step_avg:60.65ms
step:2192/2315 train_time:132936ms step_avg:60.65ms
step:2193/2315 train_time:132997ms step_avg:60.65ms
step:2194/2315 train_time:133058ms step_avg:60.65ms
step:2195/2315 train_time:133119ms step_avg:60.65ms
step:2196/2315 train_time:133180ms step_avg:60.65ms
step:2197/2315 train_time:133241ms step_avg:60.65ms
step:2198/2315 train_time:133302ms step_avg:60.65ms
step:2199/2315 train_time:133363ms step_avg:60.65ms
step:2200/2315 train_time:133424ms step_avg:60.65ms
step:2201/2315 train_time:133486ms step_avg:60.65ms
step:2202/2315 train_time:133547ms step_avg:60.65ms
step:2203/2315 train_time:133609ms step_avg:60.65ms
step:2204/2315 train_time:133670ms step_avg:60.65ms
step:2205/2315 train_time:133731ms step_avg:60.65ms
step:2206/2315 train_time:133792ms step_avg:60.65ms
step:2207/2315 train_time:133855ms step_avg:60.65ms
step:2208/2315 train_time:133916ms step_avg:60.65ms
step:2209/2315 train_time:133977ms step_avg:60.65ms
step:2210/2315 train_time:134038ms step_avg:60.65ms
step:2211/2315 train_time:134099ms step_avg:60.65ms
step:2212/2315 train_time:134160ms step_avg:60.65ms
step:2213/2315 train_time:134221ms step_avg:60.65ms
step:2214/2315 train_time:134282ms step_avg:60.65ms
step:2215/2315 train_time:134344ms step_avg:60.65ms
step:2216/2315 train_time:134405ms step_avg:60.65ms
step:2217/2315 train_time:134466ms step_avg:60.65ms
step:2218/2315 train_time:134527ms step_avg:60.65ms
step:2219/2315 train_time:134588ms step_avg:60.65ms
step:2220/2315 train_time:134649ms step_avg:60.65ms
step:2221/2315 train_time:134710ms step_avg:60.65ms
step:2222/2315 train_time:134771ms step_avg:60.65ms
step:2223/2315 train_time:134833ms step_avg:60.65ms
step:2224/2315 train_time:134895ms step_avg:60.65ms
step:2225/2315 train_time:134956ms step_avg:60.65ms
step:2226/2315 train_time:135017ms step_avg:60.65ms
step:2227/2315 train_time:135078ms step_avg:60.65ms
step:2228/2315 train_time:135139ms step_avg:60.65ms
step:2229/2315 train_time:135200ms step_avg:60.66ms
step:2230/2315 train_time:135261ms step_avg:60.66ms
step:2231/2315 train_time:135321ms step_avg:60.66ms
step:2232/2315 train_time:135382ms step_avg:60.66ms
step:2233/2315 train_time:135444ms step_avg:60.66ms
step:2234/2315 train_time:135504ms step_avg:60.66ms
step:2235/2315 train_time:135565ms step_avg:60.66ms
step:2236/2315 train_time:135626ms step_avg:60.66ms
step:2237/2315 train_time:135689ms step_avg:60.66ms
step:2238/2315 train_time:135750ms step_avg:60.66ms
step:2239/2315 train_time:135811ms step_avg:60.66ms
step:2240/2315 train_time:135873ms step_avg:60.66ms
step:2241/2315 train_time:135935ms step_avg:60.66ms
step:2242/2315 train_time:135996ms step_avg:60.66ms
step:2243/2315 train_time:136057ms step_avg:60.66ms
step:2244/2315 train_time:136118ms step_avg:60.66ms
step:2245/2315 train_time:136179ms step_avg:60.66ms
step:2246/2315 train_time:136240ms step_avg:60.66ms
step:2247/2315 train_time:136301ms step_avg:60.66ms
step:2248/2315 train_time:136362ms step_avg:60.66ms
step:2249/2315 train_time:136423ms step_avg:60.66ms
step:2250/2315 train_time:136483ms step_avg:60.66ms
step:2250/2315 val_loss:3.2905 train_time:136546ms step_avg:60.69ms
step:2251/2315 train_time:136570ms step_avg:60.67ms
step:2252/2315 train_time:136609ms step_avg:60.66ms
step:2253/2315 train_time:136672ms step_avg:60.66ms
step:2254/2315 train_time:136736ms step_avg:60.66ms
step:2255/2315 train_time:136797ms step_avg:60.66ms
step:2256/2315 train_time:136858ms step_avg:60.66ms
step:2257/2315 train_time:136919ms step_avg:60.66ms
step:2258/2315 train_time:136979ms step_avg:60.66ms
step:2259/2315 train_time:137041ms step_avg:60.66ms
step:2260/2315 train_time:137101ms step_avg:60.66ms
step:2261/2315 train_time:137162ms step_avg:60.66ms
step:2262/2315 train_time:137222ms step_avg:60.66ms
step:2263/2315 train_time:137283ms step_avg:60.66ms
step:2264/2315 train_time:137343ms step_avg:60.66ms
step:2265/2315 train_time:137403ms step_avg:60.66ms
step:2266/2315 train_time:137464ms step_avg:60.66ms
step:2267/2315 train_time:137525ms step_avg:60.66ms
step:2268/2315 train_time:137587ms step_avg:60.66ms
step:2269/2315 train_time:137649ms step_avg:60.67ms
step:2270/2315 train_time:137712ms step_avg:60.67ms
step:2271/2315 train_time:137774ms step_avg:60.67ms
step:2272/2315 train_time:137835ms step_avg:60.67ms
step:2273/2315 train_time:137897ms step_avg:60.67ms
step:2274/2315 train_time:137957ms step_avg:60.67ms
step:2275/2315 train_time:138018ms step_avg:60.67ms
step:2276/2315 train_time:138080ms step_avg:60.67ms
step:2277/2315 train_time:138140ms step_avg:60.67ms
step:2278/2315 train_time:138201ms step_avg:60.67ms
step:2279/2315 train_time:138262ms step_avg:60.67ms
step:2280/2315 train_time:138322ms step_avg:60.67ms
step:2281/2315 train_time:138382ms step_avg:60.67ms
step:2282/2315 train_time:138443ms step_avg:60.67ms
step:2283/2315 train_time:138504ms step_avg:60.67ms
step:2284/2315 train_time:138565ms step_avg:60.67ms
step:2285/2315 train_time:138627ms step_avg:60.67ms
step:2286/2315 train_time:138689ms step_avg:60.67ms
step:2287/2315 train_time:138751ms step_avg:60.67ms
step:2288/2315 train_time:138812ms step_avg:60.67ms
step:2289/2315 train_time:138874ms step_avg:60.67ms
step:2290/2315 train_time:138935ms step_avg:60.67ms
step:2291/2315 train_time:138996ms step_avg:60.67ms
step:2292/2315 train_time:139057ms step_avg:60.67ms
step:2293/2315 train_time:139119ms step_avg:60.67ms
step:2294/2315 train_time:139179ms step_avg:60.67ms
step:2295/2315 train_time:139240ms step_avg:60.67ms
step:2296/2315 train_time:139301ms step_avg:60.67ms
step:2297/2315 train_time:139362ms step_avg:60.67ms
step:2298/2315 train_time:139423ms step_avg:60.67ms
step:2299/2315 train_time:139484ms step_avg:60.67ms
step:2300/2315 train_time:139545ms step_avg:60.67ms
step:2301/2315 train_time:139606ms step_avg:60.67ms
step:2302/2315 train_time:139668ms step_avg:60.67ms
step:2303/2315 train_time:139729ms step_avg:60.67ms
step:2304/2315 train_time:139790ms step_avg:60.67ms
step:2305/2315 train_time:139852ms step_avg:60.67ms
step:2306/2315 train_time:139913ms step_avg:60.67ms
step:2307/2315 train_time:139976ms step_avg:60.67ms
step:2308/2315 train_time:140037ms step_avg:60.67ms
step:2309/2315 train_time:140098ms step_avg:60.67ms
step:2310/2315 train_time:140159ms step_avg:60.67ms
step:2311/2315 train_time:140220ms step_avg:60.68ms
step:2312/2315 train_time:140281ms step_avg:60.68ms
step:2313/2315 train_time:140342ms step_avg:60.68ms
step:2314/2315 train_time:140402ms step_avg:60.68ms
step:2315/2315 train_time:140464ms step_avg:60.68ms
step:2315/2315 val_loss:3.2783 train_time:140525ms step_avg:60.70ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

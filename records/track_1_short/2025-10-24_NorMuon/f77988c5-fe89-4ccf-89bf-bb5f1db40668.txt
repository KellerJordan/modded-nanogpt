import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]
# polar_express_coeffs = [
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
# ]



@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    # state["second_momentum_buffer"] = torch.zeros_like(grad[...,0:1]) if param.size(-2) >= param.size(-1) else torch.zeros_like(grad[...,0:1,:])
                    # state["second_momentum_buffer"] = torch.zeros((*grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((*grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)
            
            ###################################
            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))
            ####################################


            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Oct 22 2025, 02:20:45) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251022+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 24 06:30:20 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000001:00:00.0 Off |                    0 |
| N/A   36C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000002:00:00.0 Off |                    0 |
| N/A   35C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000003:00:00.0 Off |                    0 |
| N/A   31C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000008:00:00.0 Off |                    0 |
| N/A   31C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000009:00:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   0000000A:00:00.0 Off |                    0 |
| N/A   36C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   0000000B:00:00.0 Off |                    0 |
| N/A   31C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   0000000C:00:00.0 Off |                    0 |
| N/A   35C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2315 train_time:88ms step_avg:88.50ms
step:2/2315 train_time:185ms step_avg:92.42ms
step:3/2315 train_time:206ms step_avg:68.65ms
step:4/2315 train_time:242ms step_avg:60.60ms
step:5/2315 train_time:301ms step_avg:60.25ms
step:6/2315 train_time:361ms step_avg:60.11ms
step:7/2315 train_time:421ms step_avg:60.07ms
step:8/2315 train_time:481ms step_avg:60.12ms
step:9/2315 train_time:540ms step_avg:60.05ms
step:10/2315 train_time:600ms step_avg:60.02ms
step:11/2315 train_time:660ms step_avg:60.01ms
step:12/2315 train_time:720ms step_avg:59.99ms
step:13/2315 train_time:780ms step_avg:59.97ms
step:14/2315 train_time:839ms step_avg:59.95ms
step:15/2315 train_time:899ms step_avg:59.94ms
step:16/2315 train_time:959ms step_avg:59.93ms
step:17/2315 train_time:1021ms step_avg:60.05ms
step:18/2315 train_time:1085ms step_avg:60.29ms
step:19/2315 train_time:1150ms step_avg:60.53ms
step:20/2315 train_time:1212ms step_avg:60.59ms
step:21/2315 train_time:1272ms step_avg:60.59ms
step:22/2315 train_time:1333ms step_avg:60.57ms
step:23/2315 train_time:1393ms step_avg:60.55ms
step:24/2315 train_time:1453ms step_avg:60.54ms
step:25/2315 train_time:1513ms step_avg:60.52ms
step:26/2315 train_time:1573ms step_avg:60.51ms
step:27/2315 train_time:1634ms step_avg:60.52ms
step:28/2315 train_time:1694ms step_avg:60.51ms
step:29/2315 train_time:1755ms step_avg:60.50ms
step:30/2315 train_time:1814ms step_avg:60.47ms
step:31/2315 train_time:1874ms step_avg:60.45ms
step:32/2315 train_time:1935ms step_avg:60.47ms
step:33/2315 train_time:1996ms step_avg:60.48ms
step:34/2315 train_time:2057ms step_avg:60.51ms
step:35/2315 train_time:2120ms step_avg:60.56ms
step:36/2315 train_time:2181ms step_avg:60.59ms
step:37/2315 train_time:2242ms step_avg:60.59ms
step:38/2315 train_time:2303ms step_avg:60.61ms
step:39/2315 train_time:2364ms step_avg:60.62ms
step:40/2315 train_time:2424ms step_avg:60.61ms
step:41/2315 train_time:2485ms step_avg:60.62ms
step:42/2315 train_time:2546ms step_avg:60.61ms
step:43/2315 train_time:2606ms step_avg:60.60ms
step:44/2315 train_time:2666ms step_avg:60.60ms
step:45/2315 train_time:2727ms step_avg:60.60ms
step:46/2315 train_time:2788ms step_avg:60.60ms
step:47/2315 train_time:2848ms step_avg:60.59ms
step:48/2315 train_time:2908ms step_avg:60.59ms
step:49/2315 train_time:2969ms step_avg:60.59ms
step:50/2315 train_time:3029ms step_avg:60.58ms
step:51/2315 train_time:3089ms step_avg:60.58ms
step:52/2315 train_time:3149ms step_avg:60.56ms
step:53/2315 train_time:3210ms step_avg:60.56ms
step:54/2315 train_time:3270ms step_avg:60.55ms
step:55/2315 train_time:3330ms step_avg:60.55ms
step:56/2315 train_time:3391ms step_avg:60.55ms
step:57/2315 train_time:3451ms step_avg:60.54ms
step:58/2315 train_time:3511ms step_avg:60.53ms
step:59/2315 train_time:3571ms step_avg:60.53ms
step:60/2315 train_time:3632ms step_avg:60.53ms
step:61/2315 train_time:3692ms step_avg:60.52ms
step:62/2315 train_time:3751ms step_avg:60.51ms
step:63/2315 train_time:3811ms step_avg:60.49ms
step:64/2315 train_time:3871ms step_avg:60.48ms
step:65/2315 train_time:3931ms step_avg:60.48ms
step:66/2315 train_time:3991ms step_avg:60.46ms
step:67/2315 train_time:4051ms step_avg:60.46ms
step:68/2315 train_time:4111ms step_avg:60.45ms
step:69/2315 train_time:4172ms step_avg:60.46ms
step:70/2315 train_time:4232ms step_avg:60.46ms
step:71/2315 train_time:4292ms step_avg:60.46ms
step:72/2315 train_time:4353ms step_avg:60.46ms
step:73/2315 train_time:4413ms step_avg:60.45ms
step:74/2315 train_time:4473ms step_avg:60.45ms
step:75/2315 train_time:4534ms step_avg:60.45ms
step:76/2315 train_time:4594ms step_avg:60.45ms
step:77/2315 train_time:4654ms step_avg:60.44ms
step:78/2315 train_time:4714ms step_avg:60.43ms
step:79/2315 train_time:4773ms step_avg:60.42ms
step:80/2315 train_time:4833ms step_avg:60.42ms
step:81/2315 train_time:4894ms step_avg:60.41ms
step:82/2315 train_time:4953ms step_avg:60.41ms
step:83/2315 train_time:5013ms step_avg:60.40ms
step:84/2315 train_time:5073ms step_avg:60.39ms
step:85/2315 train_time:5133ms step_avg:60.39ms
step:86/2315 train_time:5194ms step_avg:60.39ms
step:87/2315 train_time:5254ms step_avg:60.39ms
step:88/2315 train_time:5314ms step_avg:60.38ms
step:89/2315 train_time:5374ms step_avg:60.38ms
step:90/2315 train_time:5435ms step_avg:60.39ms
step:91/2315 train_time:5496ms step_avg:60.39ms
step:92/2315 train_time:5556ms step_avg:60.39ms
step:93/2315 train_time:5616ms step_avg:60.38ms
step:94/2315 train_time:5676ms step_avg:60.38ms
step:95/2315 train_time:5736ms step_avg:60.38ms
step:96/2315 train_time:5796ms step_avg:60.37ms
step:97/2315 train_time:5856ms step_avg:60.37ms
step:98/2315 train_time:5916ms step_avg:60.37ms
step:99/2315 train_time:5976ms step_avg:60.37ms
step:100/2315 train_time:6036ms step_avg:60.36ms
step:101/2315 train_time:6097ms step_avg:60.36ms
step:102/2315 train_time:6157ms step_avg:60.36ms
step:103/2315 train_time:6218ms step_avg:60.37ms
step:104/2315 train_time:6279ms step_avg:60.37ms
step:105/2315 train_time:6340ms step_avg:60.38ms
step:106/2315 train_time:6400ms step_avg:60.38ms
step:107/2315 train_time:6460ms step_avg:60.38ms
step:108/2315 train_time:6521ms step_avg:60.38ms
step:109/2315 train_time:6581ms step_avg:60.37ms
step:110/2315 train_time:6641ms step_avg:60.38ms
step:111/2315 train_time:6702ms step_avg:60.38ms
step:112/2315 train_time:6762ms step_avg:60.37ms
step:113/2315 train_time:6822ms step_avg:60.37ms
step:114/2315 train_time:6882ms step_avg:60.37ms
step:115/2315 train_time:6943ms step_avg:60.38ms
step:116/2315 train_time:7004ms step_avg:60.38ms
step:117/2315 train_time:7064ms step_avg:60.38ms
step:118/2315 train_time:7124ms step_avg:60.38ms
step:119/2315 train_time:7184ms step_avg:60.37ms
step:120/2315 train_time:7245ms step_avg:60.37ms
step:121/2315 train_time:7305ms step_avg:60.37ms
step:122/2315 train_time:7365ms step_avg:60.37ms
step:123/2315 train_time:7426ms step_avg:60.37ms
step:124/2315 train_time:7486ms step_avg:60.37ms
step:125/2315 train_time:7546ms step_avg:60.37ms
step:126/2315 train_time:7606ms step_avg:60.36ms
step:127/2315 train_time:7666ms step_avg:60.37ms
step:128/2315 train_time:7726ms step_avg:60.36ms
step:129/2315 train_time:7787ms step_avg:60.37ms
step:130/2315 train_time:7847ms step_avg:60.36ms
step:131/2315 train_time:7907ms step_avg:60.36ms
step:132/2315 train_time:7967ms step_avg:60.35ms
step:133/2315 train_time:8027ms step_avg:60.35ms
step:134/2315 train_time:8087ms step_avg:60.35ms
step:135/2315 train_time:8148ms step_avg:60.36ms
step:136/2315 train_time:8208ms step_avg:60.36ms
step:137/2315 train_time:8269ms step_avg:60.36ms
step:138/2315 train_time:8329ms step_avg:60.35ms
step:139/2315 train_time:8389ms step_avg:60.36ms
step:140/2315 train_time:8449ms step_avg:60.35ms
step:141/2315 train_time:8509ms step_avg:60.35ms
step:142/2315 train_time:8569ms step_avg:60.34ms
step:143/2315 train_time:8628ms step_avg:60.34ms
step:144/2315 train_time:8688ms step_avg:60.33ms
step:145/2315 train_time:8748ms step_avg:60.33ms
step:146/2315 train_time:8808ms step_avg:60.33ms
step:147/2315 train_time:8868ms step_avg:60.33ms
step:148/2315 train_time:8927ms step_avg:60.32ms
step:149/2315 train_time:8987ms step_avg:60.32ms
step:150/2315 train_time:9047ms step_avg:60.32ms
step:151/2315 train_time:9108ms step_avg:60.32ms
step:152/2315 train_time:9167ms step_avg:60.31ms
step:153/2315 train_time:9228ms step_avg:60.31ms
step:154/2315 train_time:9288ms step_avg:60.31ms
step:155/2315 train_time:9348ms step_avg:60.31ms
step:156/2315 train_time:9408ms step_avg:60.31ms
step:157/2315 train_time:9468ms step_avg:60.31ms
step:158/2315 train_time:9528ms step_avg:60.30ms
step:159/2315 train_time:9588ms step_avg:60.30ms
step:160/2315 train_time:9648ms step_avg:60.30ms
step:161/2315 train_time:9708ms step_avg:60.30ms
step:162/2315 train_time:9768ms step_avg:60.30ms
step:163/2315 train_time:9828ms step_avg:60.30ms
step:164/2315 train_time:9888ms step_avg:60.29ms
step:165/2315 train_time:9948ms step_avg:60.29ms
step:166/2315 train_time:10008ms step_avg:60.29ms
step:167/2315 train_time:10068ms step_avg:60.29ms
step:168/2315 train_time:10128ms step_avg:60.29ms
step:169/2315 train_time:10188ms step_avg:60.29ms
step:170/2315 train_time:10249ms step_avg:60.29ms
step:171/2315 train_time:10309ms step_avg:60.29ms
step:172/2315 train_time:10368ms step_avg:60.28ms
step:173/2315 train_time:10428ms step_avg:60.28ms
step:174/2315 train_time:10488ms step_avg:60.28ms
step:175/2315 train_time:10548ms step_avg:60.28ms
step:176/2315 train_time:10608ms step_avg:60.27ms
step:177/2315 train_time:10668ms step_avg:60.27ms
step:178/2315 train_time:10728ms step_avg:60.27ms
step:179/2315 train_time:10788ms step_avg:60.27ms
step:180/2315 train_time:10848ms step_avg:60.27ms
step:181/2315 train_time:10908ms step_avg:60.26ms
step:182/2315 train_time:10967ms step_avg:60.26ms
step:183/2315 train_time:11027ms step_avg:60.26ms
step:184/2315 train_time:11087ms step_avg:60.26ms
step:185/2315 train_time:11148ms step_avg:60.26ms
step:186/2315 train_time:11208ms step_avg:60.26ms
step:187/2315 train_time:11267ms step_avg:60.25ms
step:188/2315 train_time:11327ms step_avg:60.25ms
step:189/2315 train_time:11388ms step_avg:60.25ms
step:190/2315 train_time:11448ms step_avg:60.25ms
step:191/2315 train_time:11508ms step_avg:60.25ms
step:192/2315 train_time:11568ms step_avg:60.25ms
step:193/2315 train_time:11628ms step_avg:60.25ms
step:194/2315 train_time:11689ms step_avg:60.25ms
step:195/2315 train_time:11748ms step_avg:60.25ms
step:196/2315 train_time:11808ms step_avg:60.24ms
step:197/2315 train_time:11868ms step_avg:60.24ms
step:198/2315 train_time:11927ms step_avg:60.24ms
step:199/2315 train_time:11987ms step_avg:60.24ms
step:200/2315 train_time:12047ms step_avg:60.23ms
step:201/2315 train_time:12107ms step_avg:60.23ms
step:202/2315 train_time:12167ms step_avg:60.23ms
step:203/2315 train_time:12227ms step_avg:60.23ms
step:204/2315 train_time:12286ms step_avg:60.23ms
step:205/2315 train_time:12347ms step_avg:60.23ms
step:206/2315 train_time:12407ms step_avg:60.23ms
step:207/2315 train_time:12467ms step_avg:60.23ms
step:208/2315 train_time:12527ms step_avg:60.23ms
step:209/2315 train_time:12587ms step_avg:60.23ms
step:210/2315 train_time:12647ms step_avg:60.22ms
step:211/2315 train_time:12707ms step_avg:60.22ms
step:212/2315 train_time:12767ms step_avg:60.22ms
step:213/2315 train_time:12827ms step_avg:60.22ms
step:214/2315 train_time:12887ms step_avg:60.22ms
step:215/2315 train_time:12948ms step_avg:60.22ms
step:216/2315 train_time:13008ms step_avg:60.22ms
step:217/2315 train_time:13068ms step_avg:60.22ms
step:218/2315 train_time:13127ms step_avg:60.22ms
step:219/2315 train_time:13187ms step_avg:60.22ms
step:220/2315 train_time:13247ms step_avg:60.21ms
step:221/2315 train_time:13307ms step_avg:60.21ms
step:222/2315 train_time:13367ms step_avg:60.21ms
step:223/2315 train_time:13427ms step_avg:60.21ms
step:224/2315 train_time:13488ms step_avg:60.21ms
step:225/2315 train_time:13547ms step_avg:60.21ms
step:226/2315 train_time:13606ms step_avg:60.21ms
step:227/2315 train_time:13667ms step_avg:60.20ms
step:228/2315 train_time:13726ms step_avg:60.20ms
step:229/2315 train_time:13787ms step_avg:60.21ms
step:230/2315 train_time:13847ms step_avg:60.20ms
step:231/2315 train_time:13907ms step_avg:60.21ms
step:232/2315 train_time:13967ms step_avg:60.20ms
step:233/2315 train_time:14027ms step_avg:60.20ms
step:234/2315 train_time:14087ms step_avg:60.20ms
step:235/2315 train_time:14148ms step_avg:60.20ms
step:236/2315 train_time:14207ms step_avg:60.20ms
step:237/2315 train_time:14267ms step_avg:60.20ms
step:238/2315 train_time:14327ms step_avg:60.20ms
step:239/2315 train_time:14387ms step_avg:60.20ms
step:240/2315 train_time:14447ms step_avg:60.19ms
step:241/2315 train_time:14507ms step_avg:60.19ms
step:242/2315 train_time:14566ms step_avg:60.19ms
step:243/2315 train_time:14627ms step_avg:60.19ms
step:244/2315 train_time:14687ms step_avg:60.19ms
step:245/2315 train_time:14747ms step_avg:60.19ms
step:246/2315 train_time:14807ms step_avg:60.19ms
step:247/2315 train_time:14866ms step_avg:60.19ms
step:248/2315 train_time:14927ms step_avg:60.19ms
step:249/2315 train_time:14987ms step_avg:60.19ms
step:250/2315 train_time:15047ms step_avg:60.19ms
step:250/2315 val_loss:4.0709 train_time:15108ms step_avg:60.43ms
step:251/2315 train_time:15127ms step_avg:60.27ms
step:252/2315 train_time:15170ms step_avg:60.20ms
step:253/2315 train_time:15236ms step_avg:60.22ms
step:254/2315 train_time:15299ms step_avg:60.23ms
step:255/2315 train_time:15360ms step_avg:60.23ms
step:256/2315 train_time:15419ms step_avg:60.23ms
step:257/2315 train_time:15479ms step_avg:60.23ms
step:258/2315 train_time:15539ms step_avg:60.23ms
step:259/2315 train_time:15599ms step_avg:60.23ms
step:260/2315 train_time:15658ms step_avg:60.22ms
step:261/2315 train_time:15718ms step_avg:60.22ms
step:262/2315 train_time:15779ms step_avg:60.22ms
step:263/2315 train_time:15838ms step_avg:60.22ms
step:264/2315 train_time:15898ms step_avg:60.22ms
step:265/2315 train_time:15957ms step_avg:60.22ms
step:266/2315 train_time:16017ms step_avg:60.21ms
step:267/2315 train_time:16078ms step_avg:60.22ms
step:268/2315 train_time:16140ms step_avg:60.22ms
step:269/2315 train_time:16202ms step_avg:60.23ms
step:270/2315 train_time:16263ms step_avg:60.23ms
step:271/2315 train_time:16324ms step_avg:60.24ms
step:272/2315 train_time:16384ms step_avg:60.24ms
step:273/2315 train_time:16444ms step_avg:60.23ms
step:274/2315 train_time:16504ms step_avg:60.23ms
step:275/2315 train_time:16564ms step_avg:60.23ms
step:276/2315 train_time:16623ms step_avg:60.23ms
step:277/2315 train_time:16683ms step_avg:60.23ms
step:278/2315 train_time:16742ms step_avg:60.22ms
step:279/2315 train_time:16801ms step_avg:60.22ms
step:280/2315 train_time:16860ms step_avg:60.22ms
step:281/2315 train_time:16921ms step_avg:60.22ms
step:282/2315 train_time:16980ms step_avg:60.21ms
step:283/2315 train_time:17041ms step_avg:60.22ms
step:284/2315 train_time:17101ms step_avg:60.21ms
step:285/2315 train_time:17163ms step_avg:60.22ms
step:286/2315 train_time:17223ms step_avg:60.22ms
step:287/2315 train_time:17284ms step_avg:60.22ms
step:288/2315 train_time:17344ms step_avg:60.22ms
step:289/2315 train_time:17404ms step_avg:60.22ms
step:290/2315 train_time:17464ms step_avg:60.22ms
step:291/2315 train_time:17524ms step_avg:60.22ms
step:292/2315 train_time:17583ms step_avg:60.22ms
step:293/2315 train_time:17643ms step_avg:60.22ms
step:294/2315 train_time:17703ms step_avg:60.21ms
step:295/2315 train_time:17762ms step_avg:60.21ms
step:296/2315 train_time:17822ms step_avg:60.21ms
step:297/2315 train_time:17881ms step_avg:60.21ms
step:298/2315 train_time:17941ms step_avg:60.20ms
step:299/2315 train_time:18001ms step_avg:60.20ms
step:300/2315 train_time:18062ms step_avg:60.21ms
step:301/2315 train_time:18122ms step_avg:60.21ms
step:302/2315 train_time:18183ms step_avg:60.21ms
step:303/2315 train_time:18244ms step_avg:60.21ms
step:304/2315 train_time:18304ms step_avg:60.21ms
step:305/2315 train_time:18364ms step_avg:60.21ms
step:306/2315 train_time:18424ms step_avg:60.21ms
step:307/2315 train_time:18484ms step_avg:60.21ms
step:308/2315 train_time:18544ms step_avg:60.21ms
step:309/2315 train_time:18603ms step_avg:60.20ms
step:310/2315 train_time:18663ms step_avg:60.20ms
step:311/2315 train_time:18723ms step_avg:60.20ms
step:312/2315 train_time:18782ms step_avg:60.20ms
step:313/2315 train_time:18841ms step_avg:60.20ms
step:314/2315 train_time:18901ms step_avg:60.20ms
step:315/2315 train_time:18961ms step_avg:60.19ms
step:316/2315 train_time:19021ms step_avg:60.19ms
step:317/2315 train_time:19081ms step_avg:60.19ms
step:318/2315 train_time:19142ms step_avg:60.19ms
step:319/2315 train_time:19202ms step_avg:60.19ms
step:320/2315 train_time:19263ms step_avg:60.20ms
step:321/2315 train_time:19322ms step_avg:60.19ms
step:322/2315 train_time:19383ms step_avg:60.19ms
step:323/2315 train_time:19443ms step_avg:60.19ms
step:324/2315 train_time:19503ms step_avg:60.20ms
step:325/2315 train_time:19563ms step_avg:60.19ms
step:326/2315 train_time:19623ms step_avg:60.19ms
step:327/2315 train_time:19682ms step_avg:60.19ms
step:328/2315 train_time:19742ms step_avg:60.19ms
step:329/2315 train_time:19801ms step_avg:60.19ms
step:330/2315 train_time:19860ms step_avg:60.18ms
step:331/2315 train_time:19920ms step_avg:60.18ms
step:332/2315 train_time:19980ms step_avg:60.18ms
step:333/2315 train_time:20040ms step_avg:60.18ms
step:334/2315 train_time:20100ms step_avg:60.18ms
step:335/2315 train_time:20160ms step_avg:60.18ms
step:336/2315 train_time:20220ms step_avg:60.18ms
step:337/2315 train_time:20281ms step_avg:60.18ms
step:338/2315 train_time:20341ms step_avg:60.18ms
step:339/2315 train_time:20402ms step_avg:60.18ms
step:340/2315 train_time:20462ms step_avg:60.18ms
step:341/2315 train_time:20523ms step_avg:60.18ms
step:342/2315 train_time:20583ms step_avg:60.18ms
step:343/2315 train_time:20643ms step_avg:60.18ms
step:344/2315 train_time:20702ms step_avg:60.18ms
step:345/2315 train_time:20762ms step_avg:60.18ms
step:346/2315 train_time:20821ms step_avg:60.18ms
step:347/2315 train_time:20881ms step_avg:60.17ms
step:348/2315 train_time:20940ms step_avg:60.17ms
step:349/2315 train_time:21000ms step_avg:60.17ms
step:350/2315 train_time:21060ms step_avg:60.17ms
step:351/2315 train_time:21121ms step_avg:60.17ms
step:352/2315 train_time:21180ms step_avg:60.17ms
step:353/2315 train_time:21240ms step_avg:60.17ms
step:354/2315 train_time:21301ms step_avg:60.17ms
step:355/2315 train_time:21362ms step_avg:60.17ms
step:356/2315 train_time:21422ms step_avg:60.17ms
step:357/2315 train_time:21483ms step_avg:60.18ms
step:358/2315 train_time:21543ms step_avg:60.17ms
step:359/2315 train_time:21602ms step_avg:60.17ms
step:360/2315 train_time:21663ms step_avg:60.17ms
step:361/2315 train_time:21723ms step_avg:60.17ms
step:362/2315 train_time:21783ms step_avg:60.17ms
step:363/2315 train_time:21842ms step_avg:60.17ms
step:364/2315 train_time:21901ms step_avg:60.17ms
step:365/2315 train_time:21961ms step_avg:60.17ms
step:366/2315 train_time:22021ms step_avg:60.17ms
step:367/2315 train_time:22081ms step_avg:60.17ms
step:368/2315 train_time:22141ms step_avg:60.16ms
step:369/2315 train_time:22201ms step_avg:60.17ms
step:370/2315 train_time:22261ms step_avg:60.17ms
step:371/2315 train_time:22323ms step_avg:60.17ms
step:372/2315 train_time:22382ms step_avg:60.17ms
step:373/2315 train_time:22443ms step_avg:60.17ms
step:374/2315 train_time:22503ms step_avg:60.17ms
step:375/2315 train_time:22563ms step_avg:60.17ms
step:376/2315 train_time:22623ms step_avg:60.17ms
step:377/2315 train_time:22684ms step_avg:60.17ms
step:378/2315 train_time:22744ms step_avg:60.17ms
step:379/2315 train_time:22804ms step_avg:60.17ms
step:380/2315 train_time:22863ms step_avg:60.17ms
step:381/2315 train_time:22923ms step_avg:60.17ms
step:382/2315 train_time:22983ms step_avg:60.17ms
step:383/2315 train_time:23043ms step_avg:60.16ms
step:384/2315 train_time:23102ms step_avg:60.16ms
step:385/2315 train_time:23162ms step_avg:60.16ms
step:386/2315 train_time:23222ms step_avg:60.16ms
step:387/2315 train_time:23283ms step_avg:60.16ms
step:388/2315 train_time:23343ms step_avg:60.16ms
step:389/2315 train_time:23403ms step_avg:60.16ms
step:390/2315 train_time:23463ms step_avg:60.16ms
step:391/2315 train_time:23524ms step_avg:60.16ms
step:392/2315 train_time:23583ms step_avg:60.16ms
step:393/2315 train_time:23643ms step_avg:60.16ms
step:394/2315 train_time:23703ms step_avg:60.16ms
step:395/2315 train_time:23763ms step_avg:60.16ms
step:396/2315 train_time:23822ms step_avg:60.16ms
step:397/2315 train_time:23882ms step_avg:60.16ms
step:398/2315 train_time:23942ms step_avg:60.16ms
step:399/2315 train_time:24002ms step_avg:60.16ms
step:400/2315 train_time:24061ms step_avg:60.15ms
step:401/2315 train_time:24121ms step_avg:60.15ms
step:402/2315 train_time:24181ms step_avg:60.15ms
step:403/2315 train_time:24241ms step_avg:60.15ms
step:404/2315 train_time:24301ms step_avg:60.15ms
step:405/2315 train_time:24361ms step_avg:60.15ms
step:406/2315 train_time:24421ms step_avg:60.15ms
step:407/2315 train_time:24481ms step_avg:60.15ms
step:408/2315 train_time:24541ms step_avg:60.15ms
step:409/2315 train_time:24602ms step_avg:60.15ms
step:410/2315 train_time:24662ms step_avg:60.15ms
step:411/2315 train_time:24722ms step_avg:60.15ms
step:412/2315 train_time:24782ms step_avg:60.15ms
step:413/2315 train_time:24843ms step_avg:60.15ms
step:414/2315 train_time:24903ms step_avg:60.15ms
step:415/2315 train_time:24963ms step_avg:60.15ms
step:416/2315 train_time:25023ms step_avg:60.15ms
step:417/2315 train_time:25083ms step_avg:60.15ms
step:418/2315 train_time:25143ms step_avg:60.15ms
step:419/2315 train_time:25202ms step_avg:60.15ms
step:420/2315 train_time:25262ms step_avg:60.15ms
step:421/2315 train_time:25322ms step_avg:60.15ms
step:422/2315 train_time:25382ms step_avg:60.15ms
step:423/2315 train_time:25442ms step_avg:60.15ms
step:424/2315 train_time:25502ms step_avg:60.15ms
step:425/2315 train_time:25562ms step_avg:60.14ms
step:426/2315 train_time:25621ms step_avg:60.14ms
step:427/2315 train_time:25681ms step_avg:60.14ms
step:428/2315 train_time:25741ms step_avg:60.14ms
step:429/2315 train_time:25801ms step_avg:60.14ms
step:430/2315 train_time:25861ms step_avg:60.14ms
step:431/2315 train_time:25922ms step_avg:60.14ms
step:432/2315 train_time:25981ms step_avg:60.14ms
step:433/2315 train_time:26041ms step_avg:60.14ms
step:434/2315 train_time:26101ms step_avg:60.14ms
step:435/2315 train_time:26161ms step_avg:60.14ms
step:436/2315 train_time:26221ms step_avg:60.14ms
step:437/2315 train_time:26281ms step_avg:60.14ms
step:438/2315 train_time:26340ms step_avg:60.14ms
step:439/2315 train_time:26401ms step_avg:60.14ms
step:440/2315 train_time:26461ms step_avg:60.14ms
step:441/2315 train_time:26522ms step_avg:60.14ms
step:442/2315 train_time:26581ms step_avg:60.14ms
step:443/2315 train_time:26642ms step_avg:60.14ms
step:444/2315 train_time:26701ms step_avg:60.14ms
step:445/2315 train_time:26762ms step_avg:60.14ms
step:446/2315 train_time:26822ms step_avg:60.14ms
step:447/2315 train_time:26882ms step_avg:60.14ms
step:448/2315 train_time:26942ms step_avg:60.14ms
step:449/2315 train_time:27002ms step_avg:60.14ms
step:450/2315 train_time:27062ms step_avg:60.14ms
step:451/2315 train_time:27121ms step_avg:60.14ms
step:452/2315 train_time:27181ms step_avg:60.14ms
step:453/2315 train_time:27241ms step_avg:60.14ms
step:454/2315 train_time:27301ms step_avg:60.13ms
step:455/2315 train_time:27361ms step_avg:60.13ms
step:456/2315 train_time:27421ms step_avg:60.13ms
step:457/2315 train_time:27482ms step_avg:60.14ms
step:458/2315 train_time:27542ms step_avg:60.13ms
step:459/2315 train_time:27602ms step_avg:60.13ms
step:460/2315 train_time:27662ms step_avg:60.13ms
step:461/2315 train_time:27722ms step_avg:60.13ms
step:462/2315 train_time:27782ms step_avg:60.13ms
step:463/2315 train_time:27842ms step_avg:60.13ms
step:464/2315 train_time:27902ms step_avg:60.13ms
step:465/2315 train_time:27963ms step_avg:60.13ms
step:466/2315 train_time:28023ms step_avg:60.13ms
step:467/2315 train_time:28083ms step_avg:60.14ms
step:468/2315 train_time:28143ms step_avg:60.13ms
step:469/2315 train_time:28203ms step_avg:60.13ms
step:470/2315 train_time:28262ms step_avg:60.13ms
step:471/2315 train_time:28322ms step_avg:60.13ms
step:472/2315 train_time:28383ms step_avg:60.13ms
step:473/2315 train_time:28442ms step_avg:60.13ms
step:474/2315 train_time:28503ms step_avg:60.13ms
step:475/2315 train_time:28563ms step_avg:60.13ms
step:476/2315 train_time:28624ms step_avg:60.13ms
step:477/2315 train_time:28684ms step_avg:60.13ms
step:478/2315 train_time:28744ms step_avg:60.13ms
step:479/2315 train_time:28803ms step_avg:60.13ms
step:480/2315 train_time:28863ms step_avg:60.13ms
step:481/2315 train_time:28923ms step_avg:60.13ms
step:482/2315 train_time:28983ms step_avg:60.13ms
step:483/2315 train_time:29043ms step_avg:60.13ms
step:484/2315 train_time:29103ms step_avg:60.13ms
step:485/2315 train_time:29164ms step_avg:60.13ms
step:486/2315 train_time:29223ms step_avg:60.13ms
step:487/2315 train_time:29283ms step_avg:60.13ms
step:488/2315 train_time:29343ms step_avg:60.13ms
step:489/2315 train_time:29403ms step_avg:60.13ms
step:490/2315 train_time:29463ms step_avg:60.13ms
step:491/2315 train_time:29524ms step_avg:60.13ms
step:492/2315 train_time:29583ms step_avg:60.13ms
step:493/2315 train_time:29644ms step_avg:60.13ms
step:494/2315 train_time:29703ms step_avg:60.13ms
step:495/2315 train_time:29763ms step_avg:60.13ms
step:496/2315 train_time:29824ms step_avg:60.13ms
step:497/2315 train_time:29883ms step_avg:60.13ms
step:498/2315 train_time:29943ms step_avg:60.13ms
step:499/2315 train_time:30003ms step_avg:60.13ms
step:500/2315 train_time:30063ms step_avg:60.13ms
step:500/2315 val_loss:3.8104 train_time:30126ms step_avg:60.25ms
step:501/2315 train_time:30145ms step_avg:60.17ms
step:502/2315 train_time:30188ms step_avg:60.14ms
step:503/2315 train_time:30251ms step_avg:60.14ms
step:504/2315 train_time:30314ms step_avg:60.15ms
step:505/2315 train_time:30376ms step_avg:60.15ms
step:506/2315 train_time:30435ms step_avg:60.15ms
step:507/2315 train_time:30495ms step_avg:60.15ms
step:508/2315 train_time:30554ms step_avg:60.15ms
step:509/2315 train_time:30615ms step_avg:60.15ms
step:510/2315 train_time:30674ms step_avg:60.14ms
step:511/2315 train_time:30734ms step_avg:60.14ms
step:512/2315 train_time:30793ms step_avg:60.14ms
step:513/2315 train_time:30852ms step_avg:60.14ms
step:514/2315 train_time:30911ms step_avg:60.14ms
step:515/2315 train_time:30970ms step_avg:60.14ms
step:516/2315 train_time:31029ms step_avg:60.13ms
step:517/2315 train_time:31091ms step_avg:60.14ms
step:518/2315 train_time:31151ms step_avg:60.14ms
step:519/2315 train_time:31213ms step_avg:60.14ms
step:520/2315 train_time:31274ms step_avg:60.14ms
step:521/2315 train_time:31335ms step_avg:60.14ms
step:522/2315 train_time:31396ms step_avg:60.14ms
step:523/2315 train_time:31456ms step_avg:60.15ms
step:524/2315 train_time:31516ms step_avg:60.15ms
step:525/2315 train_time:31576ms step_avg:60.14ms
step:526/2315 train_time:31635ms step_avg:60.14ms
step:527/2315 train_time:31695ms step_avg:60.14ms
step:528/2315 train_time:31754ms step_avg:60.14ms
step:529/2315 train_time:31814ms step_avg:60.14ms
step:530/2315 train_time:31873ms step_avg:60.14ms
step:531/2315 train_time:31933ms step_avg:60.14ms
step:532/2315 train_time:31992ms step_avg:60.14ms
step:533/2315 train_time:32052ms step_avg:60.14ms
step:534/2315 train_time:32113ms step_avg:60.14ms
step:535/2315 train_time:32173ms step_avg:60.14ms
step:536/2315 train_time:32234ms step_avg:60.14ms
step:537/2315 train_time:32295ms step_avg:60.14ms
step:538/2315 train_time:32355ms step_avg:60.14ms
step:539/2315 train_time:32416ms step_avg:60.14ms
step:540/2315 train_time:32477ms step_avg:60.14ms
step:541/2315 train_time:32536ms step_avg:60.14ms
step:542/2315 train_time:32596ms step_avg:60.14ms
step:543/2315 train_time:32656ms step_avg:60.14ms
step:544/2315 train_time:32716ms step_avg:60.14ms
step:545/2315 train_time:32776ms step_avg:60.14ms
step:546/2315 train_time:32835ms step_avg:60.14ms
step:547/2315 train_time:32895ms step_avg:60.14ms
step:548/2315 train_time:32955ms step_avg:60.14ms
step:549/2315 train_time:33015ms step_avg:60.14ms
step:550/2315 train_time:33075ms step_avg:60.14ms
step:551/2315 train_time:33136ms step_avg:60.14ms
step:552/2315 train_time:33196ms step_avg:60.14ms
step:553/2315 train_time:33256ms step_avg:60.14ms
step:554/2315 train_time:33317ms step_avg:60.14ms
step:555/2315 train_time:33378ms step_avg:60.14ms
step:556/2315 train_time:33437ms step_avg:60.14ms
step:557/2315 train_time:33498ms step_avg:60.14ms
step:558/2315 train_time:33557ms step_avg:60.14ms
step:559/2315 train_time:33618ms step_avg:60.14ms
step:560/2315 train_time:33677ms step_avg:60.14ms
step:561/2315 train_time:33737ms step_avg:60.14ms
step:562/2315 train_time:33796ms step_avg:60.14ms
step:563/2315 train_time:33856ms step_avg:60.14ms
step:564/2315 train_time:33916ms step_avg:60.14ms
step:565/2315 train_time:33977ms step_avg:60.14ms
step:566/2315 train_time:34037ms step_avg:60.14ms
step:567/2315 train_time:34098ms step_avg:60.14ms
step:568/2315 train_time:34158ms step_avg:60.14ms
step:569/2315 train_time:34218ms step_avg:60.14ms
step:570/2315 train_time:34278ms step_avg:60.14ms
step:571/2315 train_time:34339ms step_avg:60.14ms
step:572/2315 train_time:34399ms step_avg:60.14ms
step:573/2315 train_time:34459ms step_avg:60.14ms
step:574/2315 train_time:34519ms step_avg:60.14ms
step:575/2315 train_time:34579ms step_avg:60.14ms
step:576/2315 train_time:34639ms step_avg:60.14ms
step:577/2315 train_time:34699ms step_avg:60.14ms
step:578/2315 train_time:34759ms step_avg:60.14ms
step:579/2315 train_time:34819ms step_avg:60.14ms
step:580/2315 train_time:34879ms step_avg:60.14ms
step:581/2315 train_time:34939ms step_avg:60.14ms
step:582/2315 train_time:34999ms step_avg:60.14ms
step:583/2315 train_time:35060ms step_avg:60.14ms
step:584/2315 train_time:35120ms step_avg:60.14ms
step:585/2315 train_time:35180ms step_avg:60.14ms
step:586/2315 train_time:35240ms step_avg:60.14ms
step:587/2315 train_time:35301ms step_avg:60.14ms
step:588/2315 train_time:35361ms step_avg:60.14ms
step:589/2315 train_time:35422ms step_avg:60.14ms
step:590/2315 train_time:35482ms step_avg:60.14ms
step:591/2315 train_time:35542ms step_avg:60.14ms
step:592/2315 train_time:35602ms step_avg:60.14ms
step:593/2315 train_time:35662ms step_avg:60.14ms
step:594/2315 train_time:35721ms step_avg:60.14ms
step:595/2315 train_time:35781ms step_avg:60.14ms
step:596/2315 train_time:35840ms step_avg:60.14ms
step:597/2315 train_time:35901ms step_avg:60.14ms
step:598/2315 train_time:35962ms step_avg:60.14ms
step:599/2315 train_time:36022ms step_avg:60.14ms
step:600/2315 train_time:36081ms step_avg:60.14ms
step:601/2315 train_time:36142ms step_avg:60.14ms
step:602/2315 train_time:36202ms step_avg:60.14ms
step:603/2315 train_time:36262ms step_avg:60.14ms
step:604/2315 train_time:36323ms step_avg:60.14ms
step:605/2315 train_time:36383ms step_avg:60.14ms
step:606/2315 train_time:36443ms step_avg:60.14ms
step:607/2315 train_time:36503ms step_avg:60.14ms
step:608/2315 train_time:36563ms step_avg:60.14ms
step:609/2315 train_time:36623ms step_avg:60.14ms
step:610/2315 train_time:36683ms step_avg:60.14ms
step:611/2315 train_time:36743ms step_avg:60.14ms
step:612/2315 train_time:36803ms step_avg:60.14ms
step:613/2315 train_time:36864ms step_avg:60.14ms
step:614/2315 train_time:36924ms step_avg:60.14ms
step:615/2315 train_time:36984ms step_avg:60.14ms
step:616/2315 train_time:37044ms step_avg:60.14ms
step:617/2315 train_time:37105ms step_avg:60.14ms
step:618/2315 train_time:37165ms step_avg:60.14ms
step:619/2315 train_time:37226ms step_avg:60.14ms
step:620/2315 train_time:37286ms step_avg:60.14ms
step:621/2315 train_time:37347ms step_avg:60.14ms
step:622/2315 train_time:37407ms step_avg:60.14ms
step:623/2315 train_time:37466ms step_avg:60.14ms
step:624/2315 train_time:37527ms step_avg:60.14ms
step:625/2315 train_time:37587ms step_avg:60.14ms
step:626/2315 train_time:37646ms step_avg:60.14ms
step:627/2315 train_time:37706ms step_avg:60.14ms
step:628/2315 train_time:37767ms step_avg:60.14ms
step:629/2315 train_time:37827ms step_avg:60.14ms
step:630/2315 train_time:37887ms step_avg:60.14ms
step:631/2315 train_time:37947ms step_avg:60.14ms
step:632/2315 train_time:38006ms step_avg:60.14ms
step:633/2315 train_time:38067ms step_avg:60.14ms
step:634/2315 train_time:38127ms step_avg:60.14ms
step:635/2315 train_time:38187ms step_avg:60.14ms
step:636/2315 train_time:38247ms step_avg:60.14ms
step:637/2315 train_time:38307ms step_avg:60.14ms
step:638/2315 train_time:38367ms step_avg:60.14ms
step:639/2315 train_time:38427ms step_avg:60.14ms
step:640/2315 train_time:38487ms step_avg:60.14ms
step:641/2315 train_time:38547ms step_avg:60.14ms
step:642/2315 train_time:38606ms step_avg:60.13ms
step:643/2315 train_time:38666ms step_avg:60.13ms
step:644/2315 train_time:38726ms step_avg:60.13ms
step:645/2315 train_time:38786ms step_avg:60.13ms
step:646/2315 train_time:38846ms step_avg:60.13ms
step:647/2315 train_time:38906ms step_avg:60.13ms
step:648/2315 train_time:38966ms step_avg:60.13ms
step:649/2315 train_time:39026ms step_avg:60.13ms
step:650/2315 train_time:39085ms step_avg:60.13ms
step:651/2315 train_time:39146ms step_avg:60.13ms
step:652/2315 train_time:39206ms step_avg:60.13ms
step:653/2315 train_time:39266ms step_avg:60.13ms
step:654/2315 train_time:39326ms step_avg:60.13ms
step:655/2315 train_time:39387ms step_avg:60.13ms
step:656/2315 train_time:39448ms step_avg:60.13ms
step:657/2315 train_time:39508ms step_avg:60.13ms
step:658/2315 train_time:39568ms step_avg:60.13ms
step:659/2315 train_time:39627ms step_avg:60.13ms
step:660/2315 train_time:39686ms step_avg:60.13ms
step:661/2315 train_time:39746ms step_avg:60.13ms
step:662/2315 train_time:39806ms step_avg:60.13ms
step:663/2315 train_time:39866ms step_avg:60.13ms
step:664/2315 train_time:39925ms step_avg:60.13ms
step:665/2315 train_time:39985ms step_avg:60.13ms
step:666/2315 train_time:40046ms step_avg:60.13ms
step:667/2315 train_time:40106ms step_avg:60.13ms
step:668/2315 train_time:40166ms step_avg:60.13ms
step:669/2315 train_time:40226ms step_avg:60.13ms
step:670/2315 train_time:40285ms step_avg:60.13ms
step:671/2315 train_time:40346ms step_avg:60.13ms
step:672/2315 train_time:40406ms step_avg:60.13ms
step:673/2315 train_time:40467ms step_avg:60.13ms
step:674/2315 train_time:40527ms step_avg:60.13ms
step:675/2315 train_time:40587ms step_avg:60.13ms
step:676/2315 train_time:40647ms step_avg:60.13ms
step:677/2315 train_time:40707ms step_avg:60.13ms
step:678/2315 train_time:40766ms step_avg:60.13ms
step:679/2315 train_time:40826ms step_avg:60.13ms
step:680/2315 train_time:40886ms step_avg:60.13ms
step:681/2315 train_time:40946ms step_avg:60.13ms
step:682/2315 train_time:41006ms step_avg:60.13ms
step:683/2315 train_time:41066ms step_avg:60.13ms
step:684/2315 train_time:41126ms step_avg:60.13ms
step:685/2315 train_time:41186ms step_avg:60.13ms
step:686/2315 train_time:41245ms step_avg:60.12ms
step:687/2315 train_time:41306ms step_avg:60.12ms
step:688/2315 train_time:41366ms step_avg:60.12ms
step:689/2315 train_time:41426ms step_avg:60.13ms
step:690/2315 train_time:41487ms step_avg:60.13ms
step:691/2315 train_time:41547ms step_avg:60.13ms
step:692/2315 train_time:41607ms step_avg:60.13ms
step:693/2315 train_time:41667ms step_avg:60.13ms
step:694/2315 train_time:41727ms step_avg:60.13ms
step:695/2315 train_time:41787ms step_avg:60.13ms
step:696/2315 train_time:41847ms step_avg:60.12ms
step:697/2315 train_time:41907ms step_avg:60.13ms
step:698/2315 train_time:41968ms step_avg:60.13ms
step:699/2315 train_time:42027ms step_avg:60.13ms
step:700/2315 train_time:42087ms step_avg:60.12ms
step:701/2315 train_time:42147ms step_avg:60.12ms
step:702/2315 train_time:42207ms step_avg:60.12ms
step:703/2315 train_time:42267ms step_avg:60.12ms
step:704/2315 train_time:42328ms step_avg:60.12ms
step:705/2315 train_time:42387ms step_avg:60.12ms
step:706/2315 train_time:42447ms step_avg:60.12ms
step:707/2315 train_time:42507ms step_avg:60.12ms
step:708/2315 train_time:42567ms step_avg:60.12ms
step:709/2315 train_time:42626ms step_avg:60.12ms
step:710/2315 train_time:42686ms step_avg:60.12ms
step:711/2315 train_time:42747ms step_avg:60.12ms
step:712/2315 train_time:42807ms step_avg:60.12ms
step:713/2315 train_time:42866ms step_avg:60.12ms
step:714/2315 train_time:42927ms step_avg:60.12ms
step:715/2315 train_time:42987ms step_avg:60.12ms
step:716/2315 train_time:43046ms step_avg:60.12ms
step:717/2315 train_time:43106ms step_avg:60.12ms
step:718/2315 train_time:43166ms step_avg:60.12ms
step:719/2315 train_time:43226ms step_avg:60.12ms
step:720/2315 train_time:43286ms step_avg:60.12ms
step:721/2315 train_time:43346ms step_avg:60.12ms
step:722/2315 train_time:43406ms step_avg:60.12ms
step:723/2315 train_time:43466ms step_avg:60.12ms
step:724/2315 train_time:43526ms step_avg:60.12ms
step:725/2315 train_time:43586ms step_avg:60.12ms
step:726/2315 train_time:43646ms step_avg:60.12ms
step:727/2315 train_time:43706ms step_avg:60.12ms
step:728/2315 train_time:43767ms step_avg:60.12ms
step:729/2315 train_time:43826ms step_avg:60.12ms
step:730/2315 train_time:43886ms step_avg:60.12ms
step:731/2315 train_time:43946ms step_avg:60.12ms
step:732/2315 train_time:44005ms step_avg:60.12ms
step:733/2315 train_time:44066ms step_avg:60.12ms
step:734/2315 train_time:44127ms step_avg:60.12ms
step:735/2315 train_time:44186ms step_avg:60.12ms
step:736/2315 train_time:44246ms step_avg:60.12ms
step:737/2315 train_time:44306ms step_avg:60.12ms
step:738/2315 train_time:44367ms step_avg:60.12ms
step:739/2315 train_time:44426ms step_avg:60.12ms
step:740/2315 train_time:44486ms step_avg:60.12ms
step:741/2315 train_time:44546ms step_avg:60.12ms
step:742/2315 train_time:44606ms step_avg:60.12ms
step:743/2315 train_time:44666ms step_avg:60.12ms
step:744/2315 train_time:44726ms step_avg:60.12ms
step:745/2315 train_time:44786ms step_avg:60.12ms
step:746/2315 train_time:44846ms step_avg:60.11ms
step:747/2315 train_time:44906ms step_avg:60.11ms
step:748/2315 train_time:44966ms step_avg:60.12ms
step:749/2315 train_time:45027ms step_avg:60.12ms
step:750/2315 train_time:45087ms step_avg:60.12ms
step:750/2315 val_loss:3.6845 train_time:45149ms step_avg:60.20ms
step:751/2315 train_time:45168ms step_avg:60.14ms
step:752/2315 train_time:45208ms step_avg:60.12ms
step:753/2315 train_time:45272ms step_avg:60.12ms
step:754/2315 train_time:45336ms step_avg:60.13ms
step:755/2315 train_time:45396ms step_avg:60.13ms
step:756/2315 train_time:45456ms step_avg:60.13ms
step:757/2315 train_time:45516ms step_avg:60.13ms
step:758/2315 train_time:45575ms step_avg:60.13ms
step:759/2315 train_time:45634ms step_avg:60.12ms
step:760/2315 train_time:45694ms step_avg:60.12ms
step:761/2315 train_time:45753ms step_avg:60.12ms
step:762/2315 train_time:45813ms step_avg:60.12ms
step:763/2315 train_time:45873ms step_avg:60.12ms
step:764/2315 train_time:45933ms step_avg:60.12ms
step:765/2315 train_time:45994ms step_avg:60.12ms
step:766/2315 train_time:46054ms step_avg:60.12ms
step:767/2315 train_time:46116ms step_avg:60.12ms
step:768/2315 train_time:46177ms step_avg:60.13ms
step:769/2315 train_time:46239ms step_avg:60.13ms
step:770/2315 train_time:46301ms step_avg:60.13ms
step:771/2315 train_time:46362ms step_avg:60.13ms
step:772/2315 train_time:46423ms step_avg:60.13ms
step:773/2315 train_time:46484ms step_avg:60.13ms
step:774/2315 train_time:46545ms step_avg:60.14ms
step:775/2315 train_time:46605ms step_avg:60.14ms
step:776/2315 train_time:46666ms step_avg:60.14ms
step:777/2315 train_time:46727ms step_avg:60.14ms
step:778/2315 train_time:46787ms step_avg:60.14ms
step:779/2315 train_time:46848ms step_avg:60.14ms
step:780/2315 train_time:46908ms step_avg:60.14ms
step:781/2315 train_time:46968ms step_avg:60.14ms
step:782/2315 train_time:47029ms step_avg:60.14ms
step:783/2315 train_time:47090ms step_avg:60.14ms
step:784/2315 train_time:47151ms step_avg:60.14ms
step:785/2315 train_time:47212ms step_avg:60.14ms
step:786/2315 train_time:47273ms step_avg:60.14ms
step:787/2315 train_time:47334ms step_avg:60.14ms
step:788/2315 train_time:47394ms step_avg:60.15ms
step:789/2315 train_time:47455ms step_avg:60.15ms
step:790/2315 train_time:47516ms step_avg:60.15ms
step:791/2315 train_time:47576ms step_avg:60.15ms
step:792/2315 train_time:47637ms step_avg:60.15ms
step:793/2315 train_time:47698ms step_avg:60.15ms
step:794/2315 train_time:47759ms step_avg:60.15ms
step:795/2315 train_time:47819ms step_avg:60.15ms
step:796/2315 train_time:47880ms step_avg:60.15ms
step:797/2315 train_time:47940ms step_avg:60.15ms
step:798/2315 train_time:48001ms step_avg:60.15ms
step:799/2315 train_time:48062ms step_avg:60.15ms
step:800/2315 train_time:48123ms step_avg:60.15ms
step:801/2315 train_time:48184ms step_avg:60.16ms
step:802/2315 train_time:48246ms step_avg:60.16ms
step:803/2315 train_time:48307ms step_avg:60.16ms
step:804/2315 train_time:48368ms step_avg:60.16ms
step:805/2315 train_time:48429ms step_avg:60.16ms
step:806/2315 train_time:48490ms step_avg:60.16ms
step:807/2315 train_time:48551ms step_avg:60.16ms
step:808/2315 train_time:48611ms step_avg:60.16ms
step:809/2315 train_time:48672ms step_avg:60.16ms
step:810/2315 train_time:48733ms step_avg:60.16ms
step:811/2315 train_time:48794ms step_avg:60.16ms
step:812/2315 train_time:48854ms step_avg:60.16ms
step:813/2315 train_time:48914ms step_avg:60.16ms
step:814/2315 train_time:48974ms step_avg:60.17ms
step:815/2315 train_time:49035ms step_avg:60.17ms
step:816/2315 train_time:49095ms step_avg:60.17ms
step:817/2315 train_time:49156ms step_avg:60.17ms
step:818/2315 train_time:49217ms step_avg:60.17ms
step:819/2315 train_time:49278ms step_avg:60.17ms
step:820/2315 train_time:49339ms step_avg:60.17ms
step:821/2315 train_time:49400ms step_avg:60.17ms
step:822/2315 train_time:49461ms step_avg:60.17ms
step:823/2315 train_time:49522ms step_avg:60.17ms
step:824/2315 train_time:49583ms step_avg:60.17ms
step:825/2315 train_time:49644ms step_avg:60.18ms
step:826/2315 train_time:49706ms step_avg:60.18ms
step:827/2315 train_time:49767ms step_avg:60.18ms
step:828/2315 train_time:49828ms step_avg:60.18ms
step:829/2315 train_time:49889ms step_avg:60.18ms
step:830/2315 train_time:49949ms step_avg:60.18ms
step:831/2315 train_time:50010ms step_avg:60.18ms
step:832/2315 train_time:50071ms step_avg:60.18ms
step:833/2315 train_time:50132ms step_avg:60.18ms
step:834/2315 train_time:50193ms step_avg:60.18ms
step:835/2315 train_time:50253ms step_avg:60.18ms
step:836/2315 train_time:50313ms step_avg:60.18ms
step:837/2315 train_time:50374ms step_avg:60.18ms
step:838/2315 train_time:50435ms step_avg:60.18ms
step:839/2315 train_time:50496ms step_avg:60.19ms
step:840/2315 train_time:50556ms step_avg:60.19ms
step:841/2315 train_time:50618ms step_avg:60.19ms
step:842/2315 train_time:50679ms step_avg:60.19ms
step:843/2315 train_time:50740ms step_avg:60.19ms
step:844/2315 train_time:50800ms step_avg:60.19ms
step:845/2315 train_time:50862ms step_avg:60.19ms
step:846/2315 train_time:50922ms step_avg:60.19ms
step:847/2315 train_time:50984ms step_avg:60.19ms
step:848/2315 train_time:51044ms step_avg:60.19ms
step:849/2315 train_time:51106ms step_avg:60.20ms
step:850/2315 train_time:51166ms step_avg:60.20ms
step:851/2315 train_time:51228ms step_avg:60.20ms
step:852/2315 train_time:51289ms step_avg:60.20ms
step:853/2315 train_time:51350ms step_avg:60.20ms
step:854/2315 train_time:51410ms step_avg:60.20ms
step:855/2315 train_time:51471ms step_avg:60.20ms
step:856/2315 train_time:51532ms step_avg:60.20ms
step:857/2315 train_time:51593ms step_avg:60.20ms
step:858/2315 train_time:51654ms step_avg:60.20ms
step:859/2315 train_time:51714ms step_avg:60.20ms
step:860/2315 train_time:51775ms step_avg:60.20ms
step:861/2315 train_time:51836ms step_avg:60.20ms
step:862/2315 train_time:51896ms step_avg:60.20ms
step:863/2315 train_time:51957ms step_avg:60.21ms
step:864/2315 train_time:52018ms step_avg:60.21ms
step:865/2315 train_time:52079ms step_avg:60.21ms
step:866/2315 train_time:52139ms step_avg:60.21ms
step:867/2315 train_time:52201ms step_avg:60.21ms
step:868/2315 train_time:52262ms step_avg:60.21ms
step:869/2315 train_time:52324ms step_avg:60.21ms
step:870/2315 train_time:52385ms step_avg:60.21ms
step:871/2315 train_time:52446ms step_avg:60.21ms
step:872/2315 train_time:52508ms step_avg:60.22ms
step:873/2315 train_time:52569ms step_avg:60.22ms
step:874/2315 train_time:52629ms step_avg:60.22ms
step:875/2315 train_time:52690ms step_avg:60.22ms
step:876/2315 train_time:52751ms step_avg:60.22ms
step:877/2315 train_time:52812ms step_avg:60.22ms
step:878/2315 train_time:52872ms step_avg:60.22ms
step:879/2315 train_time:52933ms step_avg:60.22ms
step:880/2315 train_time:52993ms step_avg:60.22ms
step:881/2315 train_time:53054ms step_avg:60.22ms
step:882/2315 train_time:53114ms step_avg:60.22ms
step:883/2315 train_time:53176ms step_avg:60.22ms
step:884/2315 train_time:53237ms step_avg:60.22ms
step:885/2315 train_time:53298ms step_avg:60.22ms
step:886/2315 train_time:53359ms step_avg:60.22ms
step:887/2315 train_time:53420ms step_avg:60.23ms
step:888/2315 train_time:53481ms step_avg:60.23ms
step:889/2315 train_time:53541ms step_avg:60.23ms
step:890/2315 train_time:53602ms step_avg:60.23ms
step:891/2315 train_time:53663ms step_avg:60.23ms
step:892/2315 train_time:53724ms step_avg:60.23ms
step:893/2315 train_time:53786ms step_avg:60.23ms
step:894/2315 train_time:53847ms step_avg:60.23ms
step:895/2315 train_time:53908ms step_avg:60.23ms
step:896/2315 train_time:53968ms step_avg:60.23ms
step:897/2315 train_time:54029ms step_avg:60.23ms
step:898/2315 train_time:54091ms step_avg:60.23ms
step:899/2315 train_time:54151ms step_avg:60.23ms
step:900/2315 train_time:54212ms step_avg:60.24ms
step:901/2315 train_time:54273ms step_avg:60.24ms
step:902/2315 train_time:54334ms step_avg:60.24ms
step:903/2315 train_time:54395ms step_avg:60.24ms
step:904/2315 train_time:54455ms step_avg:60.24ms
step:905/2315 train_time:54516ms step_avg:60.24ms
step:906/2315 train_time:54577ms step_avg:60.24ms
step:907/2315 train_time:54637ms step_avg:60.24ms
step:908/2315 train_time:54698ms step_avg:60.24ms
step:909/2315 train_time:54760ms step_avg:60.24ms
step:910/2315 train_time:54821ms step_avg:60.24ms
step:911/2315 train_time:54882ms step_avg:60.24ms
step:912/2315 train_time:54942ms step_avg:60.24ms
step:913/2315 train_time:55004ms step_avg:60.24ms
step:914/2315 train_time:55065ms step_avg:60.25ms
step:915/2315 train_time:55126ms step_avg:60.25ms
step:916/2315 train_time:55187ms step_avg:60.25ms
step:917/2315 train_time:55248ms step_avg:60.25ms
step:918/2315 train_time:55308ms step_avg:60.25ms
step:919/2315 train_time:55369ms step_avg:60.25ms
step:920/2315 train_time:55430ms step_avg:60.25ms
step:921/2315 train_time:55490ms step_avg:60.25ms
step:922/2315 train_time:55551ms step_avg:60.25ms
step:923/2315 train_time:55612ms step_avg:60.25ms
step:924/2315 train_time:55672ms step_avg:60.25ms
step:925/2315 train_time:55733ms step_avg:60.25ms
step:926/2315 train_time:55794ms step_avg:60.25ms
step:927/2315 train_time:55854ms step_avg:60.25ms
step:928/2315 train_time:55915ms step_avg:60.25ms
step:929/2315 train_time:55976ms step_avg:60.25ms
step:930/2315 train_time:56037ms step_avg:60.26ms
step:931/2315 train_time:56098ms step_avg:60.26ms
step:932/2315 train_time:56160ms step_avg:60.26ms
step:933/2315 train_time:56220ms step_avg:60.26ms
step:934/2315 train_time:56281ms step_avg:60.26ms
step:935/2315 train_time:56342ms step_avg:60.26ms
step:936/2315 train_time:56402ms step_avg:60.26ms
step:937/2315 train_time:56464ms step_avg:60.26ms
step:938/2315 train_time:56525ms step_avg:60.26ms
step:939/2315 train_time:56586ms step_avg:60.26ms
step:940/2315 train_time:56647ms step_avg:60.26ms
step:941/2315 train_time:56709ms step_avg:60.26ms
step:942/2315 train_time:56769ms step_avg:60.26ms
step:943/2315 train_time:56830ms step_avg:60.27ms
step:944/2315 train_time:56891ms step_avg:60.27ms
step:945/2315 train_time:56952ms step_avg:60.27ms
step:946/2315 train_time:57013ms step_avg:60.27ms
step:947/2315 train_time:57074ms step_avg:60.27ms
step:948/2315 train_time:57134ms step_avg:60.27ms
step:949/2315 train_time:57194ms step_avg:60.27ms
step:950/2315 train_time:57255ms step_avg:60.27ms
step:951/2315 train_time:57316ms step_avg:60.27ms
step:952/2315 train_time:57377ms step_avg:60.27ms
step:953/2315 train_time:57438ms step_avg:60.27ms
step:954/2315 train_time:57500ms step_avg:60.27ms
step:955/2315 train_time:57560ms step_avg:60.27ms
step:956/2315 train_time:57622ms step_avg:60.27ms
step:957/2315 train_time:57682ms step_avg:60.27ms
step:958/2315 train_time:57743ms step_avg:60.27ms
step:959/2315 train_time:57804ms step_avg:60.28ms
step:960/2315 train_time:57866ms step_avg:60.28ms
step:961/2315 train_time:57927ms step_avg:60.28ms
step:962/2315 train_time:57987ms step_avg:60.28ms
step:963/2315 train_time:58048ms step_avg:60.28ms
step:964/2315 train_time:58109ms step_avg:60.28ms
step:965/2315 train_time:58169ms step_avg:60.28ms
step:966/2315 train_time:58230ms step_avg:60.28ms
step:967/2315 train_time:58291ms step_avg:60.28ms
step:968/2315 train_time:58352ms step_avg:60.28ms
step:969/2315 train_time:58413ms step_avg:60.28ms
step:970/2315 train_time:58474ms step_avg:60.28ms
step:971/2315 train_time:58534ms step_avg:60.28ms
step:972/2315 train_time:58595ms step_avg:60.28ms
step:973/2315 train_time:58655ms step_avg:60.28ms
step:974/2315 train_time:58716ms step_avg:60.28ms
step:975/2315 train_time:58777ms step_avg:60.28ms
step:976/2315 train_time:58838ms step_avg:60.28ms
step:977/2315 train_time:58899ms step_avg:60.29ms
step:978/2315 train_time:58960ms step_avg:60.29ms
step:979/2315 train_time:59021ms step_avg:60.29ms
step:980/2315 train_time:59082ms step_avg:60.29ms
step:981/2315 train_time:59144ms step_avg:60.29ms
step:982/2315 train_time:59205ms step_avg:60.29ms
step:983/2315 train_time:59266ms step_avg:60.29ms
step:984/2315 train_time:59327ms step_avg:60.29ms
step:985/2315 train_time:59388ms step_avg:60.29ms
step:986/2315 train_time:59449ms step_avg:60.29ms
step:987/2315 train_time:59510ms step_avg:60.29ms
step:988/2315 train_time:59571ms step_avg:60.29ms
step:989/2315 train_time:59631ms step_avg:60.29ms
step:990/2315 train_time:59692ms step_avg:60.30ms
step:991/2315 train_time:59753ms step_avg:60.30ms
step:992/2315 train_time:59814ms step_avg:60.30ms
step:993/2315 train_time:59874ms step_avg:60.30ms
step:994/2315 train_time:59934ms step_avg:60.30ms
step:995/2315 train_time:59995ms step_avg:60.30ms
step:996/2315 train_time:60056ms step_avg:60.30ms
step:997/2315 train_time:60117ms step_avg:60.30ms
step:998/2315 train_time:60178ms step_avg:60.30ms
step:999/2315 train_time:60239ms step_avg:60.30ms
step:1000/2315 train_time:60299ms step_avg:60.30ms
step:1000/2315 val_loss:3.5722 train_time:60362ms step_avg:60.36ms
step:1001/2315 train_time:60383ms step_avg:60.32ms
step:1002/2315 train_time:60422ms step_avg:60.30ms
step:1003/2315 train_time:60486ms step_avg:60.30ms
step:1004/2315 train_time:60551ms step_avg:60.31ms
step:1005/2315 train_time:60613ms step_avg:60.31ms
step:1006/2315 train_time:60674ms step_avg:60.31ms
step:1007/2315 train_time:60734ms step_avg:60.31ms
step:1008/2315 train_time:60794ms step_avg:60.31ms
step:1009/2315 train_time:60854ms step_avg:60.31ms
step:1010/2315 train_time:60914ms step_avg:60.31ms
step:1011/2315 train_time:60974ms step_avg:60.31ms
step:1012/2315 train_time:61034ms step_avg:60.31ms
step:1013/2315 train_time:61094ms step_avg:60.31ms
step:1014/2315 train_time:61155ms step_avg:60.31ms
step:1015/2315 train_time:61215ms step_avg:60.31ms
step:1016/2315 train_time:61277ms step_avg:60.31ms
step:1017/2315 train_time:61340ms step_avg:60.32ms
step:1018/2315 train_time:61402ms step_avg:60.32ms
step:1019/2315 train_time:61463ms step_avg:60.32ms
step:1020/2315 train_time:61525ms step_avg:60.32ms
step:1021/2315 train_time:61587ms step_avg:60.32ms
step:1022/2315 train_time:61648ms step_avg:60.32ms
step:1023/2315 train_time:61709ms step_avg:60.32ms
step:1024/2315 train_time:61769ms step_avg:60.32ms
step:1025/2315 train_time:61830ms step_avg:60.32ms
step:1026/2315 train_time:61891ms step_avg:60.32ms
step:1027/2315 train_time:61951ms step_avg:60.32ms
step:1028/2315 train_time:62012ms step_avg:60.32ms
step:1029/2315 train_time:62072ms step_avg:60.32ms
step:1030/2315 train_time:62133ms step_avg:60.32ms
step:1031/2315 train_time:62194ms step_avg:60.32ms
step:1032/2315 train_time:62255ms step_avg:60.32ms
step:1033/2315 train_time:62317ms step_avg:60.33ms
step:1034/2315 train_time:62378ms step_avg:60.33ms
step:1035/2315 train_time:62440ms step_avg:60.33ms
step:1036/2315 train_time:62501ms step_avg:60.33ms
step:1037/2315 train_time:62563ms step_avg:60.33ms
step:1038/2315 train_time:62623ms step_avg:60.33ms
step:1039/2315 train_time:62684ms step_avg:60.33ms
step:1040/2315 train_time:62746ms step_avg:60.33ms
step:1041/2315 train_time:62808ms step_avg:60.33ms
step:1042/2315 train_time:62868ms step_avg:60.33ms
step:1043/2315 train_time:62929ms step_avg:60.33ms
step:1044/2315 train_time:62990ms step_avg:60.33ms
step:1045/2315 train_time:63051ms step_avg:60.34ms
step:1046/2315 train_time:63111ms step_avg:60.34ms
step:1047/2315 train_time:63171ms step_avg:60.34ms
step:1048/2315 train_time:63232ms step_avg:60.34ms
step:1049/2315 train_time:63295ms step_avg:60.34ms
step:1050/2315 train_time:63356ms step_avg:60.34ms
step:1051/2315 train_time:63417ms step_avg:60.34ms
step:1052/2315 train_time:63478ms step_avg:60.34ms
step:1053/2315 train_time:63539ms step_avg:60.34ms
step:1054/2315 train_time:63600ms step_avg:60.34ms
step:1055/2315 train_time:63661ms step_avg:60.34ms
step:1056/2315 train_time:63721ms step_avg:60.34ms
step:1057/2315 train_time:63782ms step_avg:60.34ms
step:1058/2315 train_time:63843ms step_avg:60.34ms
step:1059/2315 train_time:63904ms step_avg:60.34ms
step:1060/2315 train_time:63965ms step_avg:60.34ms
step:1061/2315 train_time:64026ms step_avg:60.35ms
step:1062/2315 train_time:64087ms step_avg:60.35ms
step:1063/2315 train_time:64149ms step_avg:60.35ms
step:1064/2315 train_time:64209ms step_avg:60.35ms
step:1065/2315 train_time:64270ms step_avg:60.35ms
step:1066/2315 train_time:64331ms step_avg:60.35ms
step:1067/2315 train_time:64393ms step_avg:60.35ms
step:1068/2315 train_time:64454ms step_avg:60.35ms
step:1069/2315 train_time:64515ms step_avg:60.35ms
step:1070/2315 train_time:64576ms step_avg:60.35ms
step:1071/2315 train_time:64638ms step_avg:60.35ms
step:1072/2315 train_time:64699ms step_avg:60.35ms
step:1073/2315 train_time:64760ms step_avg:60.35ms
step:1074/2315 train_time:64820ms step_avg:60.35ms
step:1075/2315 train_time:64881ms step_avg:60.35ms
step:1076/2315 train_time:64942ms step_avg:60.35ms
step:1077/2315 train_time:65002ms step_avg:60.35ms
step:1078/2315 train_time:65063ms step_avg:60.35ms
step:1079/2315 train_time:65123ms step_avg:60.35ms
step:1080/2315 train_time:65185ms step_avg:60.36ms
step:1081/2315 train_time:65246ms step_avg:60.36ms
step:1082/2315 train_time:65307ms step_avg:60.36ms
step:1083/2315 train_time:65368ms step_avg:60.36ms
step:1084/2315 train_time:65430ms step_avg:60.36ms
step:1085/2315 train_time:65491ms step_avg:60.36ms
step:1086/2315 train_time:65552ms step_avg:60.36ms
step:1087/2315 train_time:65613ms step_avg:60.36ms
step:1088/2315 train_time:65674ms step_avg:60.36ms
step:1089/2315 train_time:65736ms step_avg:60.36ms
step:1090/2315 train_time:65797ms step_avg:60.36ms
step:1091/2315 train_time:65858ms step_avg:60.36ms
step:1092/2315 train_time:65919ms step_avg:60.37ms
step:1093/2315 train_time:65980ms step_avg:60.37ms
step:1094/2315 train_time:66040ms step_avg:60.37ms
step:1095/2315 train_time:66101ms step_avg:60.37ms
step:1096/2315 train_time:66161ms step_avg:60.37ms
step:1097/2315 train_time:66222ms step_avg:60.37ms
step:1098/2315 train_time:66283ms step_avg:60.37ms
step:1099/2315 train_time:66344ms step_avg:60.37ms
step:1100/2315 train_time:66406ms step_avg:60.37ms
step:1101/2315 train_time:66467ms step_avg:60.37ms
step:1102/2315 train_time:66528ms step_avg:60.37ms
step:1103/2315 train_time:66589ms step_avg:60.37ms
step:1104/2315 train_time:66650ms step_avg:60.37ms
step:1105/2315 train_time:66712ms step_avg:60.37ms
step:1106/2315 train_time:66773ms step_avg:60.37ms
step:1107/2315 train_time:66834ms step_avg:60.37ms
step:1108/2315 train_time:66894ms step_avg:60.37ms
step:1109/2315 train_time:66955ms step_avg:60.37ms
step:1110/2315 train_time:67016ms step_avg:60.37ms
step:1111/2315 train_time:67077ms step_avg:60.38ms
step:1112/2315 train_time:67139ms step_avg:60.38ms
step:1113/2315 train_time:67199ms step_avg:60.38ms
step:1114/2315 train_time:67260ms step_avg:60.38ms
step:1115/2315 train_time:67321ms step_avg:60.38ms
step:1116/2315 train_time:67382ms step_avg:60.38ms
step:1117/2315 train_time:67443ms step_avg:60.38ms
step:1118/2315 train_time:67504ms step_avg:60.38ms
step:1119/2315 train_time:67566ms step_avg:60.38ms
step:1120/2315 train_time:67628ms step_avg:60.38ms
step:1121/2315 train_time:67689ms step_avg:60.38ms
step:1122/2315 train_time:67750ms step_avg:60.38ms
step:1123/2315 train_time:67811ms step_avg:60.38ms
step:1124/2315 train_time:67871ms step_avg:60.38ms
step:1125/2315 train_time:67932ms step_avg:60.38ms
step:1126/2315 train_time:67993ms step_avg:60.38ms
step:1127/2315 train_time:68054ms step_avg:60.39ms
step:1128/2315 train_time:68115ms step_avg:60.39ms
step:1129/2315 train_time:68176ms step_avg:60.39ms
step:1130/2315 train_time:68237ms step_avg:60.39ms
step:1131/2315 train_time:68298ms step_avg:60.39ms
step:1132/2315 train_time:68359ms step_avg:60.39ms
step:1133/2315 train_time:68420ms step_avg:60.39ms
step:1134/2315 train_time:68481ms step_avg:60.39ms
step:1135/2315 train_time:68542ms step_avg:60.39ms
step:1136/2315 train_time:68602ms step_avg:60.39ms
step:1137/2315 train_time:68663ms step_avg:60.39ms
step:1138/2315 train_time:68724ms step_avg:60.39ms
step:1139/2315 train_time:68786ms step_avg:60.39ms
step:1140/2315 train_time:68847ms step_avg:60.39ms
step:1141/2315 train_time:68908ms step_avg:60.39ms
step:1142/2315 train_time:68968ms step_avg:60.39ms
step:1143/2315 train_time:69030ms step_avg:60.39ms
step:1144/2315 train_time:69091ms step_avg:60.39ms
step:1145/2315 train_time:69152ms step_avg:60.39ms
step:1146/2315 train_time:69214ms step_avg:60.40ms
step:1147/2315 train_time:69275ms step_avg:60.40ms
step:1148/2315 train_time:69335ms step_avg:60.40ms
step:1149/2315 train_time:69396ms step_avg:60.40ms
step:1150/2315 train_time:69457ms step_avg:60.40ms
step:1151/2315 train_time:69518ms step_avg:60.40ms
step:1152/2315 train_time:69579ms step_avg:60.40ms
step:1153/2315 train_time:69640ms step_avg:60.40ms
step:1154/2315 train_time:69701ms step_avg:60.40ms
step:1155/2315 train_time:69762ms step_avg:60.40ms
step:1156/2315 train_time:69822ms step_avg:60.40ms
step:1157/2315 train_time:69883ms step_avg:60.40ms
step:1158/2315 train_time:69944ms step_avg:60.40ms
step:1159/2315 train_time:70006ms step_avg:60.40ms
step:1160/2315 train_time:70067ms step_avg:60.40ms
step:1161/2315 train_time:70128ms step_avg:60.40ms
step:1162/2315 train_time:70189ms step_avg:60.40ms
step:1163/2315 train_time:70250ms step_avg:60.40ms
step:1164/2315 train_time:70310ms step_avg:60.40ms
step:1165/2315 train_time:70371ms step_avg:60.40ms
step:1166/2315 train_time:70433ms step_avg:60.41ms
step:1167/2315 train_time:70494ms step_avg:60.41ms
step:1168/2315 train_time:70555ms step_avg:60.41ms
step:1169/2315 train_time:70616ms step_avg:60.41ms
step:1170/2315 train_time:70676ms step_avg:60.41ms
step:1171/2315 train_time:70738ms step_avg:60.41ms
step:1172/2315 train_time:70799ms step_avg:60.41ms
step:1173/2315 train_time:70860ms step_avg:60.41ms
step:1174/2315 train_time:70920ms step_avg:60.41ms
step:1175/2315 train_time:70981ms step_avg:60.41ms
step:1176/2315 train_time:71041ms step_avg:60.41ms
step:1177/2315 train_time:71103ms step_avg:60.41ms
step:1178/2315 train_time:71164ms step_avg:60.41ms
step:1179/2315 train_time:71224ms step_avg:60.41ms
step:1180/2315 train_time:71286ms step_avg:60.41ms
step:1181/2315 train_time:71348ms step_avg:60.41ms
step:1182/2315 train_time:71408ms step_avg:60.41ms
step:1183/2315 train_time:71469ms step_avg:60.41ms
step:1184/2315 train_time:71530ms step_avg:60.41ms
step:1185/2315 train_time:71591ms step_avg:60.41ms
step:1186/2315 train_time:71652ms step_avg:60.41ms
step:1187/2315 train_time:71714ms step_avg:60.42ms
step:1188/2315 train_time:71775ms step_avg:60.42ms
step:1189/2315 train_time:71836ms step_avg:60.42ms
step:1190/2315 train_time:71897ms step_avg:60.42ms
step:1191/2315 train_time:71957ms step_avg:60.42ms
step:1192/2315 train_time:72018ms step_avg:60.42ms
step:1193/2315 train_time:72078ms step_avg:60.42ms
step:1194/2315 train_time:72139ms step_avg:60.42ms
step:1195/2315 train_time:72201ms step_avg:60.42ms
step:1196/2315 train_time:72261ms step_avg:60.42ms
step:1197/2315 train_time:72322ms step_avg:60.42ms
step:1198/2315 train_time:72383ms step_avg:60.42ms
step:1199/2315 train_time:72445ms step_avg:60.42ms
step:1200/2315 train_time:72506ms step_avg:60.42ms
step:1201/2315 train_time:72568ms step_avg:60.42ms
step:1202/2315 train_time:72628ms step_avg:60.42ms
step:1203/2315 train_time:72689ms step_avg:60.42ms
step:1204/2315 train_time:72751ms step_avg:60.42ms
step:1205/2315 train_time:72812ms step_avg:60.42ms
step:1206/2315 train_time:72873ms step_avg:60.43ms
step:1207/2315 train_time:72934ms step_avg:60.43ms
step:1208/2315 train_time:72995ms step_avg:60.43ms
step:1209/2315 train_time:73056ms step_avg:60.43ms
step:1210/2315 train_time:73116ms step_avg:60.43ms
step:1211/2315 train_time:73177ms step_avg:60.43ms
step:1212/2315 train_time:73238ms step_avg:60.43ms
step:1213/2315 train_time:73299ms step_avg:60.43ms
step:1214/2315 train_time:73360ms step_avg:60.43ms
step:1215/2315 train_time:73421ms step_avg:60.43ms
step:1216/2315 train_time:73481ms step_avg:60.43ms
step:1217/2315 train_time:73543ms step_avg:60.43ms
step:1218/2315 train_time:73603ms step_avg:60.43ms
step:1219/2315 train_time:73665ms step_avg:60.43ms
step:1220/2315 train_time:73725ms step_avg:60.43ms
step:1221/2315 train_time:73787ms step_avg:60.43ms
step:1222/2315 train_time:73848ms step_avg:60.43ms
step:1223/2315 train_time:73908ms step_avg:60.43ms
step:1224/2315 train_time:73969ms step_avg:60.43ms
step:1225/2315 train_time:74030ms step_avg:60.43ms
step:1226/2315 train_time:74091ms step_avg:60.43ms
step:1227/2315 train_time:74153ms step_avg:60.43ms
step:1228/2315 train_time:74215ms step_avg:60.44ms
step:1229/2315 train_time:74276ms step_avg:60.44ms
step:1230/2315 train_time:74336ms step_avg:60.44ms
step:1231/2315 train_time:74398ms step_avg:60.44ms
step:1232/2315 train_time:74458ms step_avg:60.44ms
step:1233/2315 train_time:74519ms step_avg:60.44ms
step:1234/2315 train_time:74580ms step_avg:60.44ms
step:1235/2315 train_time:74641ms step_avg:60.44ms
step:1236/2315 train_time:74702ms step_avg:60.44ms
step:1237/2315 train_time:74762ms step_avg:60.44ms
step:1238/2315 train_time:74823ms step_avg:60.44ms
step:1239/2315 train_time:74884ms step_avg:60.44ms
step:1240/2315 train_time:74945ms step_avg:60.44ms
step:1241/2315 train_time:75007ms step_avg:60.44ms
step:1242/2315 train_time:75068ms step_avg:60.44ms
step:1243/2315 train_time:75129ms step_avg:60.44ms
step:1244/2315 train_time:75190ms step_avg:60.44ms
step:1245/2315 train_time:75251ms step_avg:60.44ms
step:1246/2315 train_time:75312ms step_avg:60.44ms
step:1247/2315 train_time:75373ms step_avg:60.44ms
step:1248/2315 train_time:75434ms step_avg:60.44ms
step:1249/2315 train_time:75495ms step_avg:60.44ms
step:1250/2315 train_time:75556ms step_avg:60.44ms
step:1250/2315 val_loss:3.5148 train_time:75618ms step_avg:60.49ms
step:1251/2315 train_time:75638ms step_avg:60.46ms
step:1252/2315 train_time:75679ms step_avg:60.45ms
step:1253/2315 train_time:75744ms step_avg:60.45ms
step:1254/2315 train_time:75807ms step_avg:60.45ms
step:1255/2315 train_time:75868ms step_avg:60.45ms
step:1256/2315 train_time:75929ms step_avg:60.45ms
step:1257/2315 train_time:75989ms step_avg:60.45ms
step:1258/2315 train_time:76049ms step_avg:60.45ms
step:1259/2315 train_time:76109ms step_avg:60.45ms
step:1260/2315 train_time:76169ms step_avg:60.45ms
step:1261/2315 train_time:76230ms step_avg:60.45ms
step:1262/2315 train_time:76290ms step_avg:60.45ms
step:1263/2315 train_time:76350ms step_avg:60.45ms
step:1264/2315 train_time:76410ms step_avg:60.45ms
step:1265/2315 train_time:76470ms step_avg:60.45ms
step:1266/2315 train_time:76531ms step_avg:60.45ms
step:1267/2315 train_time:76593ms step_avg:60.45ms
step:1268/2315 train_time:76656ms step_avg:60.45ms
step:1269/2315 train_time:76719ms step_avg:60.46ms
step:1270/2315 train_time:76781ms step_avg:60.46ms
step:1271/2315 train_time:76841ms step_avg:60.46ms
step:1272/2315 train_time:76903ms step_avg:60.46ms
step:1273/2315 train_time:76964ms step_avg:60.46ms
step:1274/2315 train_time:77025ms step_avg:60.46ms
step:1275/2315 train_time:77085ms step_avg:60.46ms
step:1276/2315 train_time:77146ms step_avg:60.46ms
step:1277/2315 train_time:77207ms step_avg:60.46ms
step:1278/2315 train_time:77267ms step_avg:60.46ms
step:1279/2315 train_time:77328ms step_avg:60.46ms
step:1280/2315 train_time:77388ms step_avg:60.46ms
step:1281/2315 train_time:77449ms step_avg:60.46ms
step:1282/2315 train_time:77510ms step_avg:60.46ms
step:1283/2315 train_time:77571ms step_avg:60.46ms
step:1284/2315 train_time:77631ms step_avg:60.46ms
step:1285/2315 train_time:77693ms step_avg:60.46ms
step:1286/2315 train_time:77755ms step_avg:60.46ms
step:1287/2315 train_time:77816ms step_avg:60.46ms
step:1288/2315 train_time:77877ms step_avg:60.46ms
step:1289/2315 train_time:77938ms step_avg:60.46ms
step:1290/2315 train_time:77999ms step_avg:60.46ms
step:1291/2315 train_time:78060ms step_avg:60.46ms
step:1292/2315 train_time:78121ms step_avg:60.47ms
step:1293/2315 train_time:78182ms step_avg:60.47ms
step:1294/2315 train_time:78243ms step_avg:60.47ms
step:1295/2315 train_time:78304ms step_avg:60.47ms
step:1296/2315 train_time:78364ms step_avg:60.47ms
step:1297/2315 train_time:78426ms step_avg:60.47ms
step:1298/2315 train_time:78486ms step_avg:60.47ms
step:1299/2315 train_time:78547ms step_avg:60.47ms
step:1300/2315 train_time:78609ms step_avg:60.47ms
step:1301/2315 train_time:78670ms step_avg:60.47ms
step:1302/2315 train_time:78731ms step_avg:60.47ms
step:1303/2315 train_time:78792ms step_avg:60.47ms
step:1304/2315 train_time:78853ms step_avg:60.47ms
step:1305/2315 train_time:78914ms step_avg:60.47ms
step:1306/2315 train_time:78975ms step_avg:60.47ms
step:1307/2315 train_time:79036ms step_avg:60.47ms
step:1308/2315 train_time:79098ms step_avg:60.47ms
step:1309/2315 train_time:79159ms step_avg:60.47ms
step:1310/2315 train_time:79220ms step_avg:60.47ms
step:1311/2315 train_time:79281ms step_avg:60.47ms
step:1312/2315 train_time:79342ms step_avg:60.47ms
step:1313/2315 train_time:79403ms step_avg:60.47ms
step:1314/2315 train_time:79464ms step_avg:60.47ms
step:1315/2315 train_time:79525ms step_avg:60.47ms
step:1316/2315 train_time:79585ms step_avg:60.48ms
step:1317/2315 train_time:79647ms step_avg:60.48ms
step:1318/2315 train_time:79708ms step_avg:60.48ms
step:1319/2315 train_time:79769ms step_avg:60.48ms
step:1320/2315 train_time:79829ms step_avg:60.48ms
step:1321/2315 train_time:79890ms step_avg:60.48ms
step:1322/2315 train_time:79950ms step_avg:60.48ms
step:1323/2315 train_time:80011ms step_avg:60.48ms
step:1324/2315 train_time:80072ms step_avg:60.48ms
step:1325/2315 train_time:80133ms step_avg:60.48ms
step:1326/2315 train_time:80194ms step_avg:60.48ms
step:1327/2315 train_time:80255ms step_avg:60.48ms
step:1328/2315 train_time:80317ms step_avg:60.48ms
step:1329/2315 train_time:80379ms step_avg:60.48ms
step:1330/2315 train_time:80439ms step_avg:60.48ms
step:1331/2315 train_time:80500ms step_avg:60.48ms
step:1332/2315 train_time:80561ms step_avg:60.48ms
step:1333/2315 train_time:80622ms step_avg:60.48ms
step:1334/2315 train_time:80683ms step_avg:60.48ms
step:1335/2315 train_time:80745ms step_avg:60.48ms
step:1336/2315 train_time:80806ms step_avg:60.48ms
step:1337/2315 train_time:80867ms step_avg:60.48ms
step:1338/2315 train_time:80928ms step_avg:60.48ms
step:1339/2315 train_time:80989ms step_avg:60.48ms
step:1340/2315 train_time:81050ms step_avg:60.48ms
step:1341/2315 train_time:81111ms step_avg:60.49ms
step:1342/2315 train_time:81171ms step_avg:60.49ms
step:1343/2315 train_time:81232ms step_avg:60.49ms
step:1344/2315 train_time:81293ms step_avg:60.49ms
step:1345/2315 train_time:81354ms step_avg:60.49ms
step:1346/2315 train_time:81416ms step_avg:60.49ms
step:1347/2315 train_time:81477ms step_avg:60.49ms
step:1348/2315 train_time:81538ms step_avg:60.49ms
step:1349/2315 train_time:81599ms step_avg:60.49ms
step:1350/2315 train_time:81660ms step_avg:60.49ms
step:1351/2315 train_time:81721ms step_avg:60.49ms
step:1352/2315 train_time:81782ms step_avg:60.49ms
step:1353/2315 train_time:81844ms step_avg:60.49ms
step:1354/2315 train_time:81905ms step_avg:60.49ms
step:1355/2315 train_time:81966ms step_avg:60.49ms
step:1356/2315 train_time:82027ms step_avg:60.49ms
step:1357/2315 train_time:82087ms step_avg:60.49ms
step:1358/2315 train_time:82148ms step_avg:60.49ms
step:1359/2315 train_time:82209ms step_avg:60.49ms
step:1360/2315 train_time:82270ms step_avg:60.49ms
step:1361/2315 train_time:82331ms step_avg:60.49ms
step:1362/2315 train_time:82391ms step_avg:60.49ms
step:1363/2315 train_time:82452ms step_avg:60.49ms
step:1364/2315 train_time:82514ms step_avg:60.49ms
step:1365/2315 train_time:82575ms step_avg:60.49ms
step:1366/2315 train_time:82636ms step_avg:60.49ms
step:1367/2315 train_time:82697ms step_avg:60.50ms
step:1368/2315 train_time:82757ms step_avg:60.50ms
step:1369/2315 train_time:82819ms step_avg:60.50ms
step:1370/2315 train_time:82880ms step_avg:60.50ms
step:1371/2315 train_time:82941ms step_avg:60.50ms
step:1372/2315 train_time:83002ms step_avg:60.50ms
step:1373/2315 train_time:83064ms step_avg:60.50ms
step:1374/2315 train_time:83124ms step_avg:60.50ms
step:1375/2315 train_time:83185ms step_avg:60.50ms
step:1376/2315 train_time:83246ms step_avg:60.50ms
step:1377/2315 train_time:83307ms step_avg:60.50ms
step:1378/2315 train_time:83367ms step_avg:60.50ms
step:1379/2315 train_time:83428ms step_avg:60.50ms
step:1380/2315 train_time:83489ms step_avg:60.50ms
step:1381/2315 train_time:83551ms step_avg:60.50ms
step:1382/2315 train_time:83611ms step_avg:60.50ms
step:1383/2315 train_time:83672ms step_avg:60.50ms
step:1384/2315 train_time:83732ms step_avg:60.50ms
step:1385/2315 train_time:83793ms step_avg:60.50ms
step:1386/2315 train_time:83854ms step_avg:60.50ms
step:1387/2315 train_time:83916ms step_avg:60.50ms
step:1388/2315 train_time:83977ms step_avg:60.50ms
step:1389/2315 train_time:84039ms step_avg:60.50ms
step:1390/2315 train_time:84100ms step_avg:60.50ms
step:1391/2315 train_time:84161ms step_avg:60.50ms
step:1392/2315 train_time:84222ms step_avg:60.50ms
step:1393/2315 train_time:84284ms step_avg:60.51ms
step:1394/2315 train_time:84345ms step_avg:60.51ms
step:1395/2315 train_time:84406ms step_avg:60.51ms
step:1396/2315 train_time:84467ms step_avg:60.51ms
step:1397/2315 train_time:84528ms step_avg:60.51ms
step:1398/2315 train_time:84589ms step_avg:60.51ms
step:1399/2315 train_time:84649ms step_avg:60.51ms
step:1400/2315 train_time:84711ms step_avg:60.51ms
step:1401/2315 train_time:84771ms step_avg:60.51ms
step:1402/2315 train_time:84832ms step_avg:60.51ms
step:1403/2315 train_time:84892ms step_avg:60.51ms
step:1404/2315 train_time:84953ms step_avg:60.51ms
step:1405/2315 train_time:85014ms step_avg:60.51ms
step:1406/2315 train_time:85076ms step_avg:60.51ms
step:1407/2315 train_time:85137ms step_avg:60.51ms
step:1408/2315 train_time:85197ms step_avg:60.51ms
step:1409/2315 train_time:85258ms step_avg:60.51ms
step:1410/2315 train_time:85319ms step_avg:60.51ms
step:1411/2315 train_time:85380ms step_avg:60.51ms
step:1412/2315 train_time:85442ms step_avg:60.51ms
step:1413/2315 train_time:85503ms step_avg:60.51ms
step:1414/2315 train_time:85564ms step_avg:60.51ms
step:1415/2315 train_time:85625ms step_avg:60.51ms
step:1416/2315 train_time:85686ms step_avg:60.51ms
step:1417/2315 train_time:85748ms step_avg:60.51ms
step:1418/2315 train_time:85808ms step_avg:60.51ms
step:1419/2315 train_time:85870ms step_avg:60.51ms
step:1420/2315 train_time:85930ms step_avg:60.51ms
step:1421/2315 train_time:85991ms step_avg:60.51ms
step:1422/2315 train_time:86051ms step_avg:60.51ms
step:1423/2315 train_time:86112ms step_avg:60.51ms
step:1424/2315 train_time:86174ms step_avg:60.52ms
step:1425/2315 train_time:86236ms step_avg:60.52ms
step:1426/2315 train_time:86296ms step_avg:60.52ms
step:1427/2315 train_time:86358ms step_avg:60.52ms
step:1428/2315 train_time:86419ms step_avg:60.52ms
step:1429/2315 train_time:86480ms step_avg:60.52ms
step:1430/2315 train_time:86541ms step_avg:60.52ms
step:1431/2315 train_time:86602ms step_avg:60.52ms
step:1432/2315 train_time:86663ms step_avg:60.52ms
step:1433/2315 train_time:86724ms step_avg:60.52ms
step:1434/2315 train_time:86785ms step_avg:60.52ms
step:1435/2315 train_time:86847ms step_avg:60.52ms
step:1436/2315 train_time:86908ms step_avg:60.52ms
step:1437/2315 train_time:86968ms step_avg:60.52ms
step:1438/2315 train_time:87029ms step_avg:60.52ms
step:1439/2315 train_time:87090ms step_avg:60.52ms
step:1440/2315 train_time:87150ms step_avg:60.52ms
step:1441/2315 train_time:87211ms step_avg:60.52ms
step:1442/2315 train_time:87272ms step_avg:60.52ms
step:1443/2315 train_time:87334ms step_avg:60.52ms
step:1444/2315 train_time:87396ms step_avg:60.52ms
step:1445/2315 train_time:87457ms step_avg:60.52ms
step:1446/2315 train_time:87518ms step_avg:60.52ms
step:1447/2315 train_time:87579ms step_avg:60.52ms
step:1448/2315 train_time:87639ms step_avg:60.52ms
step:1449/2315 train_time:87700ms step_avg:60.52ms
step:1450/2315 train_time:87761ms step_avg:60.52ms
step:1451/2315 train_time:87823ms step_avg:60.53ms
step:1452/2315 train_time:87884ms step_avg:60.53ms
step:1453/2315 train_time:87945ms step_avg:60.53ms
step:1454/2315 train_time:88006ms step_avg:60.53ms
step:1455/2315 train_time:88067ms step_avg:60.53ms
step:1456/2315 train_time:88128ms step_avg:60.53ms
step:1457/2315 train_time:88189ms step_avg:60.53ms
step:1458/2315 train_time:88249ms step_avg:60.53ms
step:1459/2315 train_time:88311ms step_avg:60.53ms
step:1460/2315 train_time:88371ms step_avg:60.53ms
step:1461/2315 train_time:88432ms step_avg:60.53ms
step:1462/2315 train_time:88492ms step_avg:60.53ms
step:1463/2315 train_time:88554ms step_avg:60.53ms
step:1464/2315 train_time:88616ms step_avg:60.53ms
step:1465/2315 train_time:88677ms step_avg:60.53ms
step:1466/2315 train_time:88738ms step_avg:60.53ms
step:1467/2315 train_time:88799ms step_avg:60.53ms
step:1468/2315 train_time:88859ms step_avg:60.53ms
step:1469/2315 train_time:88920ms step_avg:60.53ms
step:1470/2315 train_time:88981ms step_avg:60.53ms
step:1471/2315 train_time:89042ms step_avg:60.53ms
step:1472/2315 train_time:89103ms step_avg:60.53ms
step:1473/2315 train_time:89164ms step_avg:60.53ms
step:1474/2315 train_time:89226ms step_avg:60.53ms
step:1475/2315 train_time:89288ms step_avg:60.53ms
step:1476/2315 train_time:89349ms step_avg:60.53ms
step:1477/2315 train_time:89410ms step_avg:60.53ms
step:1478/2315 train_time:89470ms step_avg:60.53ms
step:1479/2315 train_time:89532ms step_avg:60.54ms
step:1480/2315 train_time:89592ms step_avg:60.54ms
step:1481/2315 train_time:89653ms step_avg:60.54ms
step:1482/2315 train_time:89715ms step_avg:60.54ms
step:1483/2315 train_time:89777ms step_avg:60.54ms
step:1484/2315 train_time:89837ms step_avg:60.54ms
step:1485/2315 train_time:89898ms step_avg:60.54ms
step:1486/2315 train_time:89959ms step_avg:60.54ms
step:1487/2315 train_time:90020ms step_avg:60.54ms
step:1488/2315 train_time:90081ms step_avg:60.54ms
step:1489/2315 train_time:90142ms step_avg:60.54ms
step:1490/2315 train_time:90203ms step_avg:60.54ms
step:1491/2315 train_time:90265ms step_avg:60.54ms
step:1492/2315 train_time:90325ms step_avg:60.54ms
step:1493/2315 train_time:90386ms step_avg:60.54ms
step:1494/2315 train_time:90447ms step_avg:60.54ms
step:1495/2315 train_time:90509ms step_avg:60.54ms
step:1496/2315 train_time:90569ms step_avg:60.54ms
step:1497/2315 train_time:90630ms step_avg:60.54ms
step:1498/2315 train_time:90691ms step_avg:60.54ms
step:1499/2315 train_time:90751ms step_avg:60.54ms
step:1500/2315 train_time:90812ms step_avg:60.54ms
step:1500/2315 val_loss:3.4482 train_time:90875ms step_avg:60.58ms
step:1501/2315 train_time:90894ms step_avg:60.56ms
step:1502/2315 train_time:90935ms step_avg:60.54ms
step:1503/2315 train_time:91001ms step_avg:60.55ms
step:1504/2315 train_time:91065ms step_avg:60.55ms
step:1505/2315 train_time:91127ms step_avg:60.55ms
step:1506/2315 train_time:91188ms step_avg:60.55ms
step:1507/2315 train_time:91249ms step_avg:60.55ms
step:1508/2315 train_time:91309ms step_avg:60.55ms
step:1509/2315 train_time:91370ms step_avg:60.55ms
step:1510/2315 train_time:91430ms step_avg:60.55ms
step:1511/2315 train_time:91490ms step_avg:60.55ms
step:1512/2315 train_time:91550ms step_avg:60.55ms
step:1513/2315 train_time:91610ms step_avg:60.55ms
step:1514/2315 train_time:91670ms step_avg:60.55ms
step:1515/2315 train_time:91731ms step_avg:60.55ms
step:1516/2315 train_time:91791ms step_avg:60.55ms
step:1517/2315 train_time:91852ms step_avg:60.55ms
step:1518/2315 train_time:91914ms step_avg:60.55ms
step:1519/2315 train_time:91977ms step_avg:60.55ms
step:1520/2315 train_time:92040ms step_avg:60.55ms
step:1521/2315 train_time:92102ms step_avg:60.55ms
step:1522/2315 train_time:92163ms step_avg:60.55ms
step:1523/2315 train_time:92225ms step_avg:60.55ms
step:1524/2315 train_time:92286ms step_avg:60.55ms
step:1525/2315 train_time:92347ms step_avg:60.56ms
step:1526/2315 train_time:92408ms step_avg:60.56ms
step:1527/2315 train_time:92469ms step_avg:60.56ms
step:1528/2315 train_time:92529ms step_avg:60.56ms
step:1529/2315 train_time:92590ms step_avg:60.56ms
step:1530/2315 train_time:92651ms step_avg:60.56ms
step:1531/2315 train_time:92711ms step_avg:60.56ms
step:1532/2315 train_time:92772ms step_avg:60.56ms
step:1533/2315 train_time:92834ms step_avg:60.56ms
step:1534/2315 train_time:92896ms step_avg:60.56ms
step:1535/2315 train_time:92958ms step_avg:60.56ms
step:1536/2315 train_time:93020ms step_avg:60.56ms
step:1537/2315 train_time:93082ms step_avg:60.56ms
step:1538/2315 train_time:93143ms step_avg:60.56ms
step:1539/2315 train_time:93204ms step_avg:60.56ms
step:1540/2315 train_time:93266ms step_avg:60.56ms
step:1541/2315 train_time:93327ms step_avg:60.56ms
step:1542/2315 train_time:93388ms step_avg:60.56ms
step:1543/2315 train_time:93449ms step_avg:60.56ms
step:1544/2315 train_time:93509ms step_avg:60.56ms
step:1545/2315 train_time:93570ms step_avg:60.56ms
step:1546/2315 train_time:93631ms step_avg:60.56ms
step:1547/2315 train_time:93692ms step_avg:60.56ms
step:1548/2315 train_time:93752ms step_avg:60.56ms
step:1549/2315 train_time:93813ms step_avg:60.56ms
step:1550/2315 train_time:93875ms step_avg:60.56ms
step:1551/2315 train_time:93936ms step_avg:60.56ms
step:1552/2315 train_time:93998ms step_avg:60.57ms
step:1553/2315 train_time:94060ms step_avg:60.57ms
step:1554/2315 train_time:94121ms step_avg:60.57ms
step:1555/2315 train_time:94183ms step_avg:60.57ms
step:1556/2315 train_time:94245ms step_avg:60.57ms
step:1557/2315 train_time:94306ms step_avg:60.57ms
step:1558/2315 train_time:94367ms step_avg:60.57ms
step:1559/2315 train_time:94429ms step_avg:60.57ms
step:1560/2315 train_time:94489ms step_avg:60.57ms
step:1561/2315 train_time:94550ms step_avg:60.57ms
step:1562/2315 train_time:94611ms step_avg:60.57ms
step:1563/2315 train_time:94672ms step_avg:60.57ms
step:1564/2315 train_time:94733ms step_avg:60.57ms
step:1565/2315 train_time:94794ms step_avg:60.57ms
step:1566/2315 train_time:94855ms step_avg:60.57ms
step:1567/2315 train_time:94917ms step_avg:60.57ms
step:1568/2315 train_time:94978ms step_avg:60.57ms
step:1569/2315 train_time:95039ms step_avg:60.57ms
step:1570/2315 train_time:95100ms step_avg:60.57ms
step:1571/2315 train_time:95162ms step_avg:60.57ms
step:1572/2315 train_time:95223ms step_avg:60.57ms
step:1573/2315 train_time:95285ms step_avg:60.58ms
step:1574/2315 train_time:95346ms step_avg:60.58ms
step:1575/2315 train_time:95408ms step_avg:60.58ms
step:1576/2315 train_time:95469ms step_avg:60.58ms
step:1577/2315 train_time:95530ms step_avg:60.58ms
step:1578/2315 train_time:95591ms step_avg:60.58ms
step:1579/2315 train_time:95652ms step_avg:60.58ms
step:1580/2315 train_time:95713ms step_avg:60.58ms
step:1581/2315 train_time:95774ms step_avg:60.58ms
step:1582/2315 train_time:95835ms step_avg:60.58ms
step:1583/2315 train_time:95896ms step_avg:60.58ms
step:1584/2315 train_time:95958ms step_avg:60.58ms
step:1585/2315 train_time:96019ms step_avg:60.58ms
step:1586/2315 train_time:96081ms step_avg:60.58ms
step:1587/2315 train_time:96142ms step_avg:60.58ms
step:1588/2315 train_time:96204ms step_avg:60.58ms
step:1589/2315 train_time:96266ms step_avg:60.58ms
step:1590/2315 train_time:96327ms step_avg:60.58ms
step:1591/2315 train_time:96388ms step_avg:60.58ms
step:1592/2315 train_time:96449ms step_avg:60.58ms
step:1593/2315 train_time:96510ms step_avg:60.58ms
step:1594/2315 train_time:96571ms step_avg:60.58ms
step:1595/2315 train_time:96632ms step_avg:60.58ms
step:1596/2315 train_time:96693ms step_avg:60.58ms
step:1597/2315 train_time:96754ms step_avg:60.58ms
step:1598/2315 train_time:96814ms step_avg:60.58ms
step:1599/2315 train_time:96876ms step_avg:60.59ms
step:1600/2315 train_time:96937ms step_avg:60.59ms
step:1601/2315 train_time:96999ms step_avg:60.59ms
step:1602/2315 train_time:97060ms step_avg:60.59ms
step:1603/2315 train_time:97122ms step_avg:60.59ms
step:1604/2315 train_time:97184ms step_avg:60.59ms
step:1605/2315 train_time:97245ms step_avg:60.59ms
step:1606/2315 train_time:97306ms step_avg:60.59ms
step:1607/2315 train_time:97368ms step_avg:60.59ms
step:1608/2315 train_time:97429ms step_avg:60.59ms
step:1609/2315 train_time:97489ms step_avg:60.59ms
step:1610/2315 train_time:97550ms step_avg:60.59ms
step:1611/2315 train_time:97611ms step_avg:60.59ms
step:1612/2315 train_time:97672ms step_avg:60.59ms
step:1613/2315 train_time:97734ms step_avg:60.59ms
step:1614/2315 train_time:97795ms step_avg:60.59ms
step:1615/2315 train_time:97856ms step_avg:60.59ms
step:1616/2315 train_time:97917ms step_avg:60.59ms
step:1617/2315 train_time:97979ms step_avg:60.59ms
step:1618/2315 train_time:98040ms step_avg:60.59ms
step:1619/2315 train_time:98102ms step_avg:60.59ms
step:1620/2315 train_time:98163ms step_avg:60.59ms
step:1621/2315 train_time:98224ms step_avg:60.59ms
step:1622/2315 train_time:98285ms step_avg:60.60ms
step:1623/2315 train_time:98347ms step_avg:60.60ms
step:1624/2315 train_time:98408ms step_avg:60.60ms
step:1625/2315 train_time:98469ms step_avg:60.60ms
step:1626/2315 train_time:98530ms step_avg:60.60ms
step:1627/2315 train_time:98591ms step_avg:60.60ms
step:1628/2315 train_time:98652ms step_avg:60.60ms
step:1629/2315 train_time:98713ms step_avg:60.60ms
step:1630/2315 train_time:98774ms step_avg:60.60ms
step:1631/2315 train_time:98835ms step_avg:60.60ms
step:1632/2315 train_time:98896ms step_avg:60.60ms
step:1633/2315 train_time:98958ms step_avg:60.60ms
step:1634/2315 train_time:99020ms step_avg:60.60ms
step:1635/2315 train_time:99082ms step_avg:60.60ms
step:1636/2315 train_time:99143ms step_avg:60.60ms
step:1637/2315 train_time:99205ms step_avg:60.60ms
step:1638/2315 train_time:99266ms step_avg:60.60ms
step:1639/2315 train_time:99327ms step_avg:60.60ms
step:1640/2315 train_time:99388ms step_avg:60.60ms
step:1641/2315 train_time:99449ms step_avg:60.60ms
step:1642/2315 train_time:99510ms step_avg:60.60ms
step:1643/2315 train_time:99571ms step_avg:60.60ms
step:1644/2315 train_time:99631ms step_avg:60.60ms
step:1645/2315 train_time:99692ms step_avg:60.60ms
step:1646/2315 train_time:99753ms step_avg:60.60ms
step:1647/2315 train_time:99814ms step_avg:60.60ms
step:1648/2315 train_time:99876ms step_avg:60.60ms
step:1649/2315 train_time:99938ms step_avg:60.61ms
step:1650/2315 train_time:99999ms step_avg:60.61ms
step:1651/2315 train_time:100060ms step_avg:60.61ms
step:1652/2315 train_time:100121ms step_avg:60.61ms
step:1653/2315 train_time:100183ms step_avg:60.61ms
step:1654/2315 train_time:100244ms step_avg:60.61ms
step:1655/2315 train_time:100305ms step_avg:60.61ms
step:1656/2315 train_time:100366ms step_avg:60.61ms
step:1657/2315 train_time:100427ms step_avg:60.61ms
step:1658/2315 train_time:100488ms step_avg:60.61ms
step:1659/2315 train_time:100550ms step_avg:60.61ms
step:1660/2315 train_time:100611ms step_avg:60.61ms
step:1661/2315 train_time:100671ms step_avg:60.61ms
step:1662/2315 train_time:100732ms step_avg:60.61ms
step:1663/2315 train_time:100793ms step_avg:60.61ms
step:1664/2315 train_time:100854ms step_avg:60.61ms
step:1665/2315 train_time:100916ms step_avg:60.61ms
step:1666/2315 train_time:100978ms step_avg:60.61ms
step:1667/2315 train_time:101040ms step_avg:60.61ms
step:1668/2315 train_time:101101ms step_avg:60.61ms
step:1669/2315 train_time:101162ms step_avg:60.61ms
step:1670/2315 train_time:101223ms step_avg:60.61ms
step:1671/2315 train_time:101285ms step_avg:60.61ms
step:1672/2315 train_time:101346ms step_avg:60.61ms
step:1673/2315 train_time:101407ms step_avg:60.61ms
step:1674/2315 train_time:101468ms step_avg:60.61ms
step:1675/2315 train_time:101529ms step_avg:60.61ms
step:1676/2315 train_time:101590ms step_avg:60.61ms
step:1677/2315 train_time:101651ms step_avg:60.61ms
step:1678/2315 train_time:101712ms step_avg:60.61ms
step:1679/2315 train_time:101773ms step_avg:60.62ms
step:1680/2315 train_time:101835ms step_avg:60.62ms
step:1681/2315 train_time:101896ms step_avg:60.62ms
step:1682/2315 train_time:101957ms step_avg:60.62ms
step:1683/2315 train_time:102019ms step_avg:60.62ms
step:1684/2315 train_time:102081ms step_avg:60.62ms
step:1685/2315 train_time:102142ms step_avg:60.62ms
step:1686/2315 train_time:102203ms step_avg:60.62ms
step:1687/2315 train_time:102265ms step_avg:60.62ms
step:1688/2315 train_time:102326ms step_avg:60.62ms
step:1689/2315 train_time:102387ms step_avg:60.62ms
step:1690/2315 train_time:102448ms step_avg:60.62ms
step:1691/2315 train_time:102509ms step_avg:60.62ms
step:1692/2315 train_time:102570ms step_avg:60.62ms
step:1693/2315 train_time:102631ms step_avg:60.62ms
step:1694/2315 train_time:102692ms step_avg:60.62ms
step:1695/2315 train_time:102753ms step_avg:60.62ms
step:1696/2315 train_time:102814ms step_avg:60.62ms
step:1697/2315 train_time:102875ms step_avg:60.62ms
step:1698/2315 train_time:102936ms step_avg:60.62ms
step:1699/2315 train_time:102998ms step_avg:60.62ms
step:1700/2315 train_time:103060ms step_avg:60.62ms
step:1701/2315 train_time:103121ms step_avg:60.62ms
step:1702/2315 train_time:103182ms step_avg:60.62ms
step:1703/2315 train_time:103244ms step_avg:60.62ms
step:1704/2315 train_time:103304ms step_avg:60.62ms
step:1705/2315 train_time:103365ms step_avg:60.62ms
step:1706/2315 train_time:103426ms step_avg:60.62ms
step:1707/2315 train_time:103488ms step_avg:60.63ms
step:1708/2315 train_time:103548ms step_avg:60.63ms
step:1709/2315 train_time:103610ms step_avg:60.63ms
step:1710/2315 train_time:103671ms step_avg:60.63ms
step:1711/2315 train_time:103732ms step_avg:60.63ms
step:1712/2315 train_time:103793ms step_avg:60.63ms
step:1713/2315 train_time:103854ms step_avg:60.63ms
step:1714/2315 train_time:103915ms step_avg:60.63ms
step:1715/2315 train_time:103977ms step_avg:60.63ms
step:1716/2315 train_time:104039ms step_avg:60.63ms
step:1717/2315 train_time:104100ms step_avg:60.63ms
step:1718/2315 train_time:104161ms step_avg:60.63ms
step:1719/2315 train_time:104223ms step_avg:60.63ms
step:1720/2315 train_time:104284ms step_avg:60.63ms
step:1721/2315 train_time:104345ms step_avg:60.63ms
step:1722/2315 train_time:104405ms step_avg:60.63ms
step:1723/2315 train_time:104467ms step_avg:60.63ms
step:1724/2315 train_time:104528ms step_avg:60.63ms
step:1725/2315 train_time:104589ms step_avg:60.63ms
step:1726/2315 train_time:104650ms step_avg:60.63ms
step:1727/2315 train_time:104712ms step_avg:60.63ms
step:1728/2315 train_time:104773ms step_avg:60.63ms
step:1729/2315 train_time:104834ms step_avg:60.63ms
step:1730/2315 train_time:104895ms step_avg:60.63ms
step:1731/2315 train_time:104957ms step_avg:60.63ms
step:1732/2315 train_time:105018ms step_avg:60.63ms
step:1733/2315 train_time:105080ms step_avg:60.63ms
step:1734/2315 train_time:105141ms step_avg:60.63ms
step:1735/2315 train_time:105202ms step_avg:60.64ms
step:1736/2315 train_time:105263ms step_avg:60.64ms
step:1737/2315 train_time:105324ms step_avg:60.64ms
step:1738/2315 train_time:105386ms step_avg:60.64ms
step:1739/2315 train_time:105447ms step_avg:60.64ms
step:1740/2315 train_time:105508ms step_avg:60.64ms
step:1741/2315 train_time:105569ms step_avg:60.64ms
step:1742/2315 train_time:105630ms step_avg:60.64ms
step:1743/2315 train_time:105691ms step_avg:60.64ms
step:1744/2315 train_time:105753ms step_avg:60.64ms
step:1745/2315 train_time:105814ms step_avg:60.64ms
step:1746/2315 train_time:105875ms step_avg:60.64ms
step:1747/2315 train_time:105936ms step_avg:60.64ms
step:1748/2315 train_time:105998ms step_avg:60.64ms
step:1749/2315 train_time:106059ms step_avg:60.64ms
step:1750/2315 train_time:106120ms step_avg:60.64ms
step:1750/2315 val_loss:3.3788 train_time:106184ms step_avg:60.68ms
step:1751/2315 train_time:106204ms step_avg:60.65ms
step:1752/2315 train_time:106246ms step_avg:60.64ms
step:1753/2315 train_time:106311ms step_avg:60.65ms
step:1754/2315 train_time:106376ms step_avg:60.65ms
step:1755/2315 train_time:106438ms step_avg:60.65ms
step:1756/2315 train_time:106499ms step_avg:60.65ms
step:1757/2315 train_time:106560ms step_avg:60.65ms
step:1758/2315 train_time:106621ms step_avg:60.65ms
step:1759/2315 train_time:106681ms step_avg:60.65ms
step:1760/2315 train_time:106742ms step_avg:60.65ms
step:1761/2315 train_time:106802ms step_avg:60.65ms
step:1762/2315 train_time:106862ms step_avg:60.65ms
step:1763/2315 train_time:106923ms step_avg:60.65ms
step:1764/2315 train_time:106983ms step_avg:60.65ms
step:1765/2315 train_time:107044ms step_avg:60.65ms
step:1766/2315 train_time:107105ms step_avg:60.65ms
step:1767/2315 train_time:107167ms step_avg:60.65ms
step:1768/2315 train_time:107230ms step_avg:60.65ms
step:1769/2315 train_time:107292ms step_avg:60.65ms
step:1770/2315 train_time:107354ms step_avg:60.65ms
step:1771/2315 train_time:107417ms step_avg:60.65ms
step:1772/2315 train_time:107478ms step_avg:60.65ms
step:1773/2315 train_time:107539ms step_avg:60.65ms
step:1774/2315 train_time:107599ms step_avg:60.65ms
step:1775/2315 train_time:107660ms step_avg:60.65ms
step:1776/2315 train_time:107721ms step_avg:60.65ms
step:1777/2315 train_time:107782ms step_avg:60.65ms
step:1778/2315 train_time:107843ms step_avg:60.65ms
step:1779/2315 train_time:107904ms step_avg:60.65ms
step:1780/2315 train_time:107964ms step_avg:60.65ms
step:1781/2315 train_time:108025ms step_avg:60.65ms
step:1782/2315 train_time:108086ms step_avg:60.65ms
step:1783/2315 train_time:108147ms step_avg:60.65ms
step:1784/2315 train_time:108208ms step_avg:60.65ms
step:1785/2315 train_time:108271ms step_avg:60.66ms
step:1786/2315 train_time:108333ms step_avg:60.66ms
step:1787/2315 train_time:108396ms step_avg:60.66ms
step:1788/2315 train_time:108457ms step_avg:60.66ms
step:1789/2315 train_time:108519ms step_avg:60.66ms
step:1790/2315 train_time:108580ms step_avg:60.66ms
step:1791/2315 train_time:108641ms step_avg:60.66ms
step:1792/2315 train_time:108701ms step_avg:60.66ms
step:1793/2315 train_time:108762ms step_avg:60.66ms
step:1794/2315 train_time:108823ms step_avg:60.66ms
step:1795/2315 train_time:108884ms step_avg:60.66ms
step:1796/2315 train_time:108945ms step_avg:60.66ms
step:1797/2315 train_time:109006ms step_avg:60.66ms
step:1798/2315 train_time:109066ms step_avg:60.66ms
step:1799/2315 train_time:109128ms step_avg:60.66ms
step:1800/2315 train_time:109190ms step_avg:60.66ms
step:1801/2315 train_time:109251ms step_avg:60.66ms
step:1802/2315 train_time:109312ms step_avg:60.66ms
step:1803/2315 train_time:109374ms step_avg:60.66ms
step:1804/2315 train_time:109435ms step_avg:60.66ms
step:1805/2315 train_time:109497ms step_avg:60.66ms
step:1806/2315 train_time:109558ms step_avg:60.66ms
step:1807/2315 train_time:109620ms step_avg:60.66ms
step:1808/2315 train_time:109681ms step_avg:60.66ms
step:1809/2315 train_time:109742ms step_avg:60.66ms
step:1810/2315 train_time:109803ms step_avg:60.66ms
step:1811/2315 train_time:109864ms step_avg:60.67ms
step:1812/2315 train_time:109925ms step_avg:60.67ms
step:1813/2315 train_time:109986ms step_avg:60.67ms
step:1814/2315 train_time:110047ms step_avg:60.67ms
step:1815/2315 train_time:110108ms step_avg:60.67ms
step:1816/2315 train_time:110169ms step_avg:60.67ms
step:1817/2315 train_time:110231ms step_avg:60.67ms
step:1818/2315 train_time:110291ms step_avg:60.67ms
step:1819/2315 train_time:110353ms step_avg:60.67ms
step:1820/2315 train_time:110414ms step_avg:60.67ms
step:1821/2315 train_time:110476ms step_avg:60.67ms
step:1822/2315 train_time:110538ms step_avg:60.67ms
step:1823/2315 train_time:110599ms step_avg:60.67ms
step:1824/2315 train_time:110660ms step_avg:60.67ms
step:1825/2315 train_time:110721ms step_avg:60.67ms
step:1826/2315 train_time:110783ms step_avg:60.67ms
step:1827/2315 train_time:110844ms step_avg:60.67ms
step:1828/2315 train_time:110905ms step_avg:60.67ms
step:1829/2315 train_time:110965ms step_avg:60.67ms
step:1830/2315 train_time:111026ms step_avg:60.67ms
step:1831/2315 train_time:111087ms step_avg:60.67ms
step:1832/2315 train_time:111148ms step_avg:60.67ms
step:1833/2315 train_time:111210ms step_avg:60.67ms
step:1834/2315 train_time:111271ms step_avg:60.67ms
step:1835/2315 train_time:111333ms step_avg:60.67ms
step:1836/2315 train_time:111394ms step_avg:60.67ms
step:1837/2315 train_time:111456ms step_avg:60.67ms
step:1838/2315 train_time:111517ms step_avg:60.67ms
step:1839/2315 train_time:111579ms step_avg:60.67ms
step:1840/2315 train_time:111640ms step_avg:60.67ms
step:1841/2315 train_time:111702ms step_avg:60.67ms
step:1842/2315 train_time:111762ms step_avg:60.67ms
step:1843/2315 train_time:111824ms step_avg:60.68ms
step:1844/2315 train_time:111885ms step_avg:60.68ms
step:1845/2315 train_time:111946ms step_avg:60.68ms
step:1846/2315 train_time:112007ms step_avg:60.68ms
step:1847/2315 train_time:112068ms step_avg:60.68ms
step:1848/2315 train_time:112129ms step_avg:60.68ms
step:1849/2315 train_time:112191ms step_avg:60.68ms
step:1850/2315 train_time:112252ms step_avg:60.68ms
step:1851/2315 train_time:112314ms step_avg:60.68ms
step:1852/2315 train_time:112375ms step_avg:60.68ms
step:1853/2315 train_time:112437ms step_avg:60.68ms
step:1854/2315 train_time:112498ms step_avg:60.68ms
step:1855/2315 train_time:112560ms step_avg:60.68ms
step:1856/2315 train_time:112621ms step_avg:60.68ms
step:1857/2315 train_time:112683ms step_avg:60.68ms
step:1858/2315 train_time:112744ms step_avg:60.68ms
step:1859/2315 train_time:112805ms step_avg:60.68ms
step:1860/2315 train_time:112866ms step_avg:60.68ms
step:1861/2315 train_time:112927ms step_avg:60.68ms
step:1862/2315 train_time:112988ms step_avg:60.68ms
step:1863/2315 train_time:113049ms step_avg:60.68ms
step:1864/2315 train_time:113110ms step_avg:60.68ms
step:1865/2315 train_time:113171ms step_avg:60.68ms
step:1866/2315 train_time:113232ms step_avg:60.68ms
step:1867/2315 train_time:113294ms step_avg:60.68ms
step:1868/2315 train_time:113355ms step_avg:60.68ms
step:1869/2315 train_time:113417ms step_avg:60.68ms
step:1870/2315 train_time:113478ms step_avg:60.68ms
step:1871/2315 train_time:113539ms step_avg:60.68ms
step:1872/2315 train_time:113600ms step_avg:60.68ms
step:1873/2315 train_time:113662ms step_avg:60.68ms
step:1874/2315 train_time:113723ms step_avg:60.68ms
step:1875/2315 train_time:113784ms step_avg:60.68ms
step:1876/2315 train_time:113845ms step_avg:60.68ms
step:1877/2315 train_time:113906ms step_avg:60.69ms
step:1878/2315 train_time:113968ms step_avg:60.69ms
step:1879/2315 train_time:114028ms step_avg:60.69ms
step:1880/2315 train_time:114089ms step_avg:60.69ms
step:1881/2315 train_time:114151ms step_avg:60.69ms
step:1882/2315 train_time:114212ms step_avg:60.69ms
step:1883/2315 train_time:114273ms step_avg:60.69ms
step:1884/2315 train_time:114334ms step_avg:60.69ms
step:1885/2315 train_time:114396ms step_avg:60.69ms
step:1886/2315 train_time:114457ms step_avg:60.69ms
step:1887/2315 train_time:114519ms step_avg:60.69ms
step:1888/2315 train_time:114580ms step_avg:60.69ms
step:1889/2315 train_time:114642ms step_avg:60.69ms
step:1890/2315 train_time:114703ms step_avg:60.69ms
step:1891/2315 train_time:114764ms step_avg:60.69ms
step:1892/2315 train_time:114825ms step_avg:60.69ms
step:1893/2315 train_time:114886ms step_avg:60.69ms
step:1894/2315 train_time:114947ms step_avg:60.69ms
step:1895/2315 train_time:115009ms step_avg:60.69ms
step:1896/2315 train_time:115069ms step_avg:60.69ms
step:1897/2315 train_time:115131ms step_avg:60.69ms
step:1898/2315 train_time:115191ms step_avg:60.69ms
step:1899/2315 train_time:115253ms step_avg:60.69ms
step:1900/2315 train_time:115314ms step_avg:60.69ms
step:1901/2315 train_time:115376ms step_avg:60.69ms
step:1902/2315 train_time:115437ms step_avg:60.69ms
step:1903/2315 train_time:115498ms step_avg:60.69ms
step:1904/2315 train_time:115559ms step_avg:60.69ms
step:1905/2315 train_time:115620ms step_avg:60.69ms
step:1906/2315 train_time:115681ms step_avg:60.69ms
step:1907/2315 train_time:115742ms step_avg:60.69ms
step:1908/2315 train_time:115803ms step_avg:60.69ms
step:1909/2315 train_time:115864ms step_avg:60.69ms
step:1910/2315 train_time:115926ms step_avg:60.69ms
step:1911/2315 train_time:115987ms step_avg:60.69ms
step:1912/2315 train_time:116048ms step_avg:60.69ms
step:1913/2315 train_time:116110ms step_avg:60.70ms
step:1914/2315 train_time:116171ms step_avg:60.70ms
step:1915/2315 train_time:116232ms step_avg:60.70ms
step:1916/2315 train_time:116293ms step_avg:60.70ms
step:1917/2315 train_time:116355ms step_avg:60.70ms
step:1918/2315 train_time:116416ms step_avg:60.70ms
step:1919/2315 train_time:116478ms step_avg:60.70ms
step:1920/2315 train_time:116539ms step_avg:60.70ms
step:1921/2315 train_time:116600ms step_avg:60.70ms
step:1922/2315 train_time:116661ms step_avg:60.70ms
step:1923/2315 train_time:116723ms step_avg:60.70ms
step:1924/2315 train_time:116783ms step_avg:60.70ms
step:1925/2315 train_time:116845ms step_avg:60.70ms
step:1926/2315 train_time:116905ms step_avg:60.70ms
step:1927/2315 train_time:116966ms step_avg:60.70ms
step:1928/2315 train_time:117027ms step_avg:60.70ms
step:1929/2315 train_time:117088ms step_avg:60.70ms
step:1930/2315 train_time:117150ms step_avg:60.70ms
step:1931/2315 train_time:117212ms step_avg:60.70ms
step:1932/2315 train_time:117273ms step_avg:60.70ms
step:1933/2315 train_time:117335ms step_avg:60.70ms
step:1934/2315 train_time:117396ms step_avg:60.70ms
step:1935/2315 train_time:117458ms step_avg:60.70ms
step:1936/2315 train_time:117519ms step_avg:60.70ms
step:1937/2315 train_time:117580ms step_avg:60.70ms
step:1938/2315 train_time:117642ms step_avg:60.70ms
step:1939/2315 train_time:117703ms step_avg:60.70ms
step:1940/2315 train_time:117764ms step_avg:60.70ms
step:1941/2315 train_time:117825ms step_avg:60.70ms
step:1942/2315 train_time:117886ms step_avg:60.70ms
step:1943/2315 train_time:117948ms step_avg:60.70ms
step:1944/2315 train_time:118009ms step_avg:60.70ms
step:1945/2315 train_time:118070ms step_avg:60.70ms
step:1946/2315 train_time:118131ms step_avg:60.70ms
step:1947/2315 train_time:118192ms step_avg:60.70ms
step:1948/2315 train_time:118253ms step_avg:60.70ms
step:1949/2315 train_time:118314ms step_avg:60.71ms
step:1950/2315 train_time:118376ms step_avg:60.71ms
step:1951/2315 train_time:118437ms step_avg:60.71ms
step:1952/2315 train_time:118498ms step_avg:60.71ms
step:1953/2315 train_time:118560ms step_avg:60.71ms
step:1954/2315 train_time:118621ms step_avg:60.71ms
step:1955/2315 train_time:118682ms step_avg:60.71ms
step:1956/2315 train_time:118743ms step_avg:60.71ms
step:1957/2315 train_time:118804ms step_avg:60.71ms
step:1958/2315 train_time:118865ms step_avg:60.71ms
step:1959/2315 train_time:118927ms step_avg:60.71ms
step:1960/2315 train_time:118988ms step_avg:60.71ms
step:1961/2315 train_time:119049ms step_avg:60.71ms
step:1962/2315 train_time:119111ms step_avg:60.71ms
step:1963/2315 train_time:119172ms step_avg:60.71ms
step:1964/2315 train_time:119233ms step_avg:60.71ms
step:1965/2315 train_time:119295ms step_avg:60.71ms
step:1966/2315 train_time:119355ms step_avg:60.71ms
step:1967/2315 train_time:119417ms step_avg:60.71ms
step:1968/2315 train_time:119478ms step_avg:60.71ms
step:1969/2315 train_time:119540ms step_avg:60.71ms
step:1970/2315 train_time:119601ms step_avg:60.71ms
step:1971/2315 train_time:119662ms step_avg:60.71ms
step:1972/2315 train_time:119723ms step_avg:60.71ms
step:1973/2315 train_time:119785ms step_avg:60.71ms
step:1974/2315 train_time:119846ms step_avg:60.71ms
step:1975/2315 train_time:119907ms step_avg:60.71ms
step:1976/2315 train_time:119967ms step_avg:60.71ms
step:1977/2315 train_time:120029ms step_avg:60.71ms
step:1978/2315 train_time:120090ms step_avg:60.71ms
step:1979/2315 train_time:120152ms step_avg:60.71ms
step:1980/2315 train_time:120213ms step_avg:60.71ms
step:1981/2315 train_time:120274ms step_avg:60.71ms
step:1982/2315 train_time:120335ms step_avg:60.71ms
step:1983/2315 train_time:120397ms step_avg:60.71ms
step:1984/2315 train_time:120458ms step_avg:60.71ms
step:1985/2315 train_time:120519ms step_avg:60.71ms
step:1986/2315 train_time:120580ms step_avg:60.71ms
step:1987/2315 train_time:120641ms step_avg:60.72ms
step:1988/2315 train_time:120702ms step_avg:60.72ms
step:1989/2315 train_time:120764ms step_avg:60.72ms
step:1990/2315 train_time:120825ms step_avg:60.72ms
step:1991/2315 train_time:120887ms step_avg:60.72ms
step:1992/2315 train_time:120947ms step_avg:60.72ms
step:1993/2315 train_time:121008ms step_avg:60.72ms
step:1994/2315 train_time:121070ms step_avg:60.72ms
step:1995/2315 train_time:121132ms step_avg:60.72ms
step:1996/2315 train_time:121193ms step_avg:60.72ms
step:1997/2315 train_time:121254ms step_avg:60.72ms
step:1998/2315 train_time:121314ms step_avg:60.72ms
step:1999/2315 train_time:121376ms step_avg:60.72ms
step:2000/2315 train_time:121437ms step_avg:60.72ms
step:2000/2315 val_loss:3.3290 train_time:121501ms step_avg:60.75ms
step:2001/2315 train_time:121520ms step_avg:60.73ms
step:2002/2315 train_time:121563ms step_avg:60.72ms
step:2003/2315 train_time:121628ms step_avg:60.72ms
step:2004/2315 train_time:121692ms step_avg:60.72ms
step:2005/2315 train_time:121754ms step_avg:60.73ms
step:2006/2315 train_time:121815ms step_avg:60.73ms
step:2007/2315 train_time:121876ms step_avg:60.73ms
step:2008/2315 train_time:121937ms step_avg:60.73ms
step:2009/2315 train_time:121998ms step_avg:60.73ms
step:2010/2315 train_time:122059ms step_avg:60.73ms
step:2011/2315 train_time:122120ms step_avg:60.73ms
step:2012/2315 train_time:122181ms step_avg:60.73ms
step:2013/2315 train_time:122242ms step_avg:60.73ms
step:2014/2315 train_time:122303ms step_avg:60.73ms
step:2015/2315 train_time:122363ms step_avg:60.73ms
step:2016/2315 train_time:122424ms step_avg:60.73ms
step:2017/2315 train_time:122487ms step_avg:60.73ms
step:2018/2315 train_time:122550ms step_avg:60.73ms
step:2019/2315 train_time:122612ms step_avg:60.73ms
step:2020/2315 train_time:122674ms step_avg:60.73ms
step:2021/2315 train_time:122736ms step_avg:60.73ms
step:2022/2315 train_time:122798ms step_avg:60.73ms
step:2023/2315 train_time:122859ms step_avg:60.73ms
step:2024/2315 train_time:122920ms step_avg:60.73ms
step:2025/2315 train_time:122981ms step_avg:60.73ms
step:2026/2315 train_time:123041ms step_avg:60.73ms
step:2027/2315 train_time:123103ms step_avg:60.73ms
step:2028/2315 train_time:123164ms step_avg:60.73ms
step:2029/2315 train_time:123224ms step_avg:60.73ms
step:2030/2315 train_time:123284ms step_avg:60.73ms
step:2031/2315 train_time:123345ms step_avg:60.73ms
step:2032/2315 train_time:123406ms step_avg:60.73ms
step:2033/2315 train_time:123468ms step_avg:60.73ms
step:2034/2315 train_time:123530ms step_avg:60.73ms
step:2035/2315 train_time:123592ms step_avg:60.73ms
step:2036/2315 train_time:123654ms step_avg:60.73ms
step:2037/2315 train_time:123715ms step_avg:60.73ms
step:2038/2315 train_time:123776ms step_avg:60.73ms
step:2039/2315 train_time:123839ms step_avg:60.73ms
step:2040/2315 train_time:123900ms step_avg:60.74ms
step:2041/2315 train_time:123960ms step_avg:60.74ms
step:2042/2315 train_time:124022ms step_avg:60.74ms
step:2043/2315 train_time:124083ms step_avg:60.74ms
step:2044/2315 train_time:124143ms step_avg:60.74ms
step:2045/2315 train_time:124204ms step_avg:60.74ms
step:2046/2315 train_time:124265ms step_avg:60.74ms
step:2047/2315 train_time:124326ms step_avg:60.74ms
step:2048/2315 train_time:124387ms step_avg:60.74ms
step:2049/2315 train_time:124449ms step_avg:60.74ms
step:2050/2315 train_time:124510ms step_avg:60.74ms
step:2051/2315 train_time:124573ms step_avg:60.74ms
step:2052/2315 train_time:124635ms step_avg:60.74ms
step:2053/2315 train_time:124697ms step_avg:60.74ms
step:2054/2315 train_time:124758ms step_avg:60.74ms
step:2055/2315 train_time:124820ms step_avg:60.74ms
step:2056/2315 train_time:124881ms step_avg:60.74ms
step:2057/2315 train_time:124943ms step_avg:60.74ms
step:2058/2315 train_time:125003ms step_avg:60.74ms
step:2059/2315 train_time:125065ms step_avg:60.74ms
step:2060/2315 train_time:125126ms step_avg:60.74ms
step:2061/2315 train_time:125187ms step_avg:60.74ms
step:2062/2315 train_time:125248ms step_avg:60.74ms
step:2063/2315 train_time:125308ms step_avg:60.74ms
step:2064/2315 train_time:125369ms step_avg:60.74ms
step:2065/2315 train_time:125430ms step_avg:60.74ms
step:2066/2315 train_time:125491ms step_avg:60.74ms
step:2067/2315 train_time:125553ms step_avg:60.74ms
step:2068/2315 train_time:125614ms step_avg:60.74ms
step:2069/2315 train_time:125676ms step_avg:60.74ms
step:2070/2315 train_time:125737ms step_avg:60.74ms
step:2071/2315 train_time:125799ms step_avg:60.74ms
step:2072/2315 train_time:125860ms step_avg:60.74ms
step:2073/2315 train_time:125922ms step_avg:60.74ms
step:2074/2315 train_time:125983ms step_avg:60.74ms
step:2075/2315 train_time:126044ms step_avg:60.74ms
step:2076/2315 train_time:126105ms step_avg:60.74ms
step:2077/2315 train_time:126166ms step_avg:60.74ms
step:2078/2315 train_time:126227ms step_avg:60.74ms
step:2079/2315 train_time:126289ms step_avg:60.74ms
step:2080/2315 train_time:126349ms step_avg:60.74ms
step:2081/2315 train_time:126411ms step_avg:60.75ms
step:2082/2315 train_time:126472ms step_avg:60.75ms
step:2083/2315 train_time:126533ms step_avg:60.75ms
step:2084/2315 train_time:126595ms step_avg:60.75ms
step:2085/2315 train_time:126656ms step_avg:60.75ms
step:2086/2315 train_time:126717ms step_avg:60.75ms
step:2087/2315 train_time:126779ms step_avg:60.75ms
step:2088/2315 train_time:126840ms step_avg:60.75ms
step:2089/2315 train_time:126902ms step_avg:60.75ms
step:2090/2315 train_time:126963ms step_avg:60.75ms
step:2091/2315 train_time:127024ms step_avg:60.75ms
step:2092/2315 train_time:127085ms step_avg:60.75ms
step:2093/2315 train_time:127147ms step_avg:60.75ms
step:2094/2315 train_time:127208ms step_avg:60.75ms
step:2095/2315 train_time:127269ms step_avg:60.75ms
step:2096/2315 train_time:127330ms step_avg:60.75ms
step:2097/2315 train_time:127391ms step_avg:60.75ms
step:2098/2315 train_time:127452ms step_avg:60.75ms
step:2099/2315 train_time:127513ms step_avg:60.75ms
step:2100/2315 train_time:127574ms step_avg:60.75ms
step:2101/2315 train_time:127635ms step_avg:60.75ms
step:2102/2315 train_time:127696ms step_avg:60.75ms
step:2103/2315 train_time:127758ms step_avg:60.75ms
step:2104/2315 train_time:127820ms step_avg:60.75ms
step:2105/2315 train_time:127881ms step_avg:60.75ms
step:2106/2315 train_time:127942ms step_avg:60.75ms
step:2107/2315 train_time:128005ms step_avg:60.75ms
step:2108/2315 train_time:128066ms step_avg:60.75ms
step:2109/2315 train_time:128127ms step_avg:60.75ms
step:2110/2315 train_time:128188ms step_avg:60.75ms
step:2111/2315 train_time:128249ms step_avg:60.75ms
step:2112/2315 train_time:128310ms step_avg:60.75ms
step:2113/2315 train_time:128371ms step_avg:60.75ms
step:2114/2315 train_time:128432ms step_avg:60.75ms
step:2115/2315 train_time:128494ms step_avg:60.75ms
step:2116/2315 train_time:128555ms step_avg:60.75ms
step:2117/2315 train_time:128616ms step_avg:60.75ms
step:2118/2315 train_time:128677ms step_avg:60.75ms
step:2119/2315 train_time:128739ms step_avg:60.75ms
step:2120/2315 train_time:128800ms step_avg:60.75ms
step:2121/2315 train_time:128861ms step_avg:60.75ms
step:2122/2315 train_time:128923ms step_avg:60.76ms
step:2123/2315 train_time:128984ms step_avg:60.76ms
step:2124/2315 train_time:129045ms step_avg:60.76ms
step:2125/2315 train_time:129107ms step_avg:60.76ms
step:2126/2315 train_time:129167ms step_avg:60.76ms
step:2127/2315 train_time:129229ms step_avg:60.76ms
step:2128/2315 train_time:129289ms step_avg:60.76ms
step:2129/2315 train_time:129351ms step_avg:60.76ms
step:2130/2315 train_time:129412ms step_avg:60.76ms
step:2131/2315 train_time:129473ms step_avg:60.76ms
step:2132/2315 train_time:129534ms step_avg:60.76ms
step:2133/2315 train_time:129595ms step_avg:60.76ms
step:2134/2315 train_time:129657ms step_avg:60.76ms
step:2135/2315 train_time:129719ms step_avg:60.76ms
step:2136/2315 train_time:129780ms step_avg:60.76ms
step:2137/2315 train_time:129841ms step_avg:60.76ms
step:2138/2315 train_time:129902ms step_avg:60.76ms
step:2139/2315 train_time:129963ms step_avg:60.76ms
step:2140/2315 train_time:130025ms step_avg:60.76ms
step:2141/2315 train_time:130087ms step_avg:60.76ms
step:2142/2315 train_time:130148ms step_avg:60.76ms
step:2143/2315 train_time:130209ms step_avg:60.76ms
step:2144/2315 train_time:130270ms step_avg:60.76ms
step:2145/2315 train_time:130332ms step_avg:60.76ms
step:2146/2315 train_time:130393ms step_avg:60.76ms
step:2147/2315 train_time:130454ms step_avg:60.76ms
step:2148/2315 train_time:130515ms step_avg:60.76ms
step:2149/2315 train_time:130577ms step_avg:60.76ms
step:2150/2315 train_time:130639ms step_avg:60.76ms
step:2151/2315 train_time:130700ms step_avg:60.76ms
step:2152/2315 train_time:130761ms step_avg:60.76ms
step:2153/2315 train_time:130822ms step_avg:60.76ms
step:2154/2315 train_time:130883ms step_avg:60.76ms
step:2155/2315 train_time:130944ms step_avg:60.76ms
step:2156/2315 train_time:131005ms step_avg:60.76ms
step:2157/2315 train_time:131067ms step_avg:60.76ms
step:2158/2315 train_time:131128ms step_avg:60.76ms
step:2159/2315 train_time:131189ms step_avg:60.76ms
step:2160/2315 train_time:131250ms step_avg:60.76ms
step:2161/2315 train_time:131312ms step_avg:60.76ms
step:2162/2315 train_time:131373ms step_avg:60.76ms
step:2163/2315 train_time:131434ms step_avg:60.76ms
step:2164/2315 train_time:131495ms step_avg:60.76ms
step:2165/2315 train_time:131557ms step_avg:60.77ms
step:2166/2315 train_time:131618ms step_avg:60.77ms
step:2167/2315 train_time:131679ms step_avg:60.77ms
step:2168/2315 train_time:131740ms step_avg:60.77ms
step:2169/2315 train_time:131801ms step_avg:60.77ms
step:2170/2315 train_time:131863ms step_avg:60.77ms
step:2171/2315 train_time:131925ms step_avg:60.77ms
step:2172/2315 train_time:131987ms step_avg:60.77ms
step:2173/2315 train_time:132048ms step_avg:60.77ms
step:2174/2315 train_time:132109ms step_avg:60.77ms
step:2175/2315 train_time:132170ms step_avg:60.77ms
step:2176/2315 train_time:132231ms step_avg:60.77ms
step:2177/2315 train_time:132292ms step_avg:60.77ms
step:2178/2315 train_time:132353ms step_avg:60.77ms
step:2179/2315 train_time:132414ms step_avg:60.77ms
step:2180/2315 train_time:132476ms step_avg:60.77ms
step:2181/2315 train_time:132537ms step_avg:60.77ms
step:2182/2315 train_time:132598ms step_avg:60.77ms
step:2183/2315 train_time:132660ms step_avg:60.77ms
step:2184/2315 train_time:132721ms step_avg:60.77ms
step:2185/2315 train_time:132782ms step_avg:60.77ms
step:2186/2315 train_time:132843ms step_avg:60.77ms
step:2187/2315 train_time:132905ms step_avg:60.77ms
step:2188/2315 train_time:132966ms step_avg:60.77ms
step:2189/2315 train_time:133027ms step_avg:60.77ms
step:2190/2315 train_time:133088ms step_avg:60.77ms
step:2191/2315 train_time:133150ms step_avg:60.77ms
step:2192/2315 train_time:133211ms step_avg:60.77ms
step:2193/2315 train_time:133273ms step_avg:60.77ms
step:2194/2315 train_time:133334ms step_avg:60.77ms
step:2195/2315 train_time:133395ms step_avg:60.77ms
step:2196/2315 train_time:133456ms step_avg:60.77ms
step:2197/2315 train_time:133518ms step_avg:60.77ms
step:2198/2315 train_time:133579ms step_avg:60.77ms
step:2199/2315 train_time:133640ms step_avg:60.77ms
step:2200/2315 train_time:133701ms step_avg:60.77ms
step:2201/2315 train_time:133762ms step_avg:60.77ms
step:2202/2315 train_time:133824ms step_avg:60.77ms
step:2203/2315 train_time:133885ms step_avg:60.77ms
step:2204/2315 train_time:133946ms step_avg:60.77ms
step:2205/2315 train_time:134008ms step_avg:60.77ms
step:2206/2315 train_time:134069ms step_avg:60.77ms
step:2207/2315 train_time:134130ms step_avg:60.77ms
step:2208/2315 train_time:134191ms step_avg:60.78ms
step:2209/2315 train_time:134253ms step_avg:60.78ms
step:2210/2315 train_time:134314ms step_avg:60.78ms
step:2211/2315 train_time:134375ms step_avg:60.78ms
step:2212/2315 train_time:134436ms step_avg:60.78ms
step:2213/2315 train_time:134498ms step_avg:60.78ms
step:2214/2315 train_time:134559ms step_avg:60.78ms
step:2215/2315 train_time:134620ms step_avg:60.78ms
step:2216/2315 train_time:134682ms step_avg:60.78ms
step:2217/2315 train_time:134743ms step_avg:60.78ms
step:2218/2315 train_time:134805ms step_avg:60.78ms
step:2219/2315 train_time:134865ms step_avg:60.78ms
step:2220/2315 train_time:134927ms step_avg:60.78ms
step:2221/2315 train_time:134988ms step_avg:60.78ms
step:2222/2315 train_time:135049ms step_avg:60.78ms
step:2223/2315 train_time:135110ms step_avg:60.78ms
step:2224/2315 train_time:135172ms step_avg:60.78ms
step:2225/2315 train_time:135233ms step_avg:60.78ms
step:2226/2315 train_time:135294ms step_avg:60.78ms
step:2227/2315 train_time:135355ms step_avg:60.78ms
step:2228/2315 train_time:135417ms step_avg:60.78ms
step:2229/2315 train_time:135478ms step_avg:60.78ms
step:2230/2315 train_time:135539ms step_avg:60.78ms
step:2231/2315 train_time:135600ms step_avg:60.78ms
step:2232/2315 train_time:135661ms step_avg:60.78ms
step:2233/2315 train_time:135722ms step_avg:60.78ms
step:2234/2315 train_time:135783ms step_avg:60.78ms
step:2235/2315 train_time:135845ms step_avg:60.78ms
step:2236/2315 train_time:135906ms step_avg:60.78ms
step:2237/2315 train_time:135968ms step_avg:60.78ms
step:2238/2315 train_time:136029ms step_avg:60.78ms
step:2239/2315 train_time:136091ms step_avg:60.78ms
step:2240/2315 train_time:136152ms step_avg:60.78ms
step:2241/2315 train_time:136213ms step_avg:60.78ms
step:2242/2315 train_time:136275ms step_avg:60.78ms
step:2243/2315 train_time:136336ms step_avg:60.78ms
step:2244/2315 train_time:136397ms step_avg:60.78ms
step:2245/2315 train_time:136459ms step_avg:60.78ms
step:2246/2315 train_time:136520ms step_avg:60.78ms
step:2247/2315 train_time:136581ms step_avg:60.78ms
step:2248/2315 train_time:136642ms step_avg:60.78ms
step:2249/2315 train_time:136704ms step_avg:60.78ms
step:2250/2315 train_time:136765ms step_avg:60.78ms
step:2250/2315 val_loss:3.2891 train_time:136828ms step_avg:60.81ms
step:2251/2315 train_time:136848ms step_avg:60.79ms
step:2252/2315 train_time:136889ms step_avg:60.79ms
step:2253/2315 train_time:136954ms step_avg:60.79ms
step:2254/2315 train_time:137019ms step_avg:60.79ms
step:2255/2315 train_time:137081ms step_avg:60.79ms
step:2256/2315 train_time:137142ms step_avg:60.79ms
step:2257/2315 train_time:137203ms step_avg:60.79ms
step:2258/2315 train_time:137263ms step_avg:60.79ms
step:2259/2315 train_time:137324ms step_avg:60.79ms
step:2260/2315 train_time:137384ms step_avg:60.79ms
step:2261/2315 train_time:137446ms step_avg:60.79ms
step:2262/2315 train_time:137507ms step_avg:60.79ms
step:2263/2315 train_time:137567ms step_avg:60.79ms
step:2264/2315 train_time:137628ms step_avg:60.79ms
step:2265/2315 train_time:137688ms step_avg:60.79ms
step:2266/2315 train_time:137750ms step_avg:60.79ms
step:2267/2315 train_time:137812ms step_avg:60.79ms
step:2268/2315 train_time:137874ms step_avg:60.79ms
step:2269/2315 train_time:137936ms step_avg:60.79ms
step:2270/2315 train_time:137999ms step_avg:60.79ms
step:2271/2315 train_time:138061ms step_avg:60.79ms
step:2272/2315 train_time:138122ms step_avg:60.79ms
step:2273/2315 train_time:138183ms step_avg:60.79ms
step:2274/2315 train_time:138244ms step_avg:60.79ms
step:2275/2315 train_time:138305ms step_avg:60.79ms
step:2276/2315 train_time:138365ms step_avg:60.79ms
step:2277/2315 train_time:138426ms step_avg:60.79ms
step:2278/2315 train_time:138487ms step_avg:60.79ms
step:2279/2315 train_time:138548ms step_avg:60.79ms
step:2280/2315 train_time:138609ms step_avg:60.79ms
step:2281/2315 train_time:138670ms step_avg:60.79ms
step:2282/2315 train_time:138730ms step_avg:60.79ms
step:2283/2315 train_time:138792ms step_avg:60.79ms
step:2284/2315 train_time:138854ms step_avg:60.79ms
step:2285/2315 train_time:138915ms step_avg:60.79ms
step:2286/2315 train_time:138977ms step_avg:60.79ms
step:2287/2315 train_time:139039ms step_avg:60.80ms
step:2288/2315 train_time:139100ms step_avg:60.80ms
step:2289/2315 train_time:139161ms step_avg:60.80ms
step:2290/2315 train_time:139222ms step_avg:60.80ms
step:2291/2315 train_time:139284ms step_avg:60.80ms
step:2292/2315 train_time:139346ms step_avg:60.80ms
step:2293/2315 train_time:139407ms step_avg:60.80ms
step:2294/2315 train_time:139467ms step_avg:60.80ms
step:2295/2315 train_time:139528ms step_avg:60.80ms
step:2296/2315 train_time:139589ms step_avg:60.80ms
step:2297/2315 train_time:139651ms step_avg:60.80ms
step:2298/2315 train_time:139712ms step_avg:60.80ms
step:2299/2315 train_time:139774ms step_avg:60.80ms
step:2300/2315 train_time:139835ms step_avg:60.80ms
step:2301/2315 train_time:139896ms step_avg:60.80ms
step:2302/2315 train_time:139957ms step_avg:60.80ms
step:2303/2315 train_time:140020ms step_avg:60.80ms
step:2304/2315 train_time:140081ms step_avg:60.80ms
step:2305/2315 train_time:140143ms step_avg:60.80ms
step:2306/2315 train_time:140203ms step_avg:60.80ms
step:2307/2315 train_time:140265ms step_avg:60.80ms
step:2308/2315 train_time:140326ms step_avg:60.80ms
step:2309/2315 train_time:140387ms step_avg:60.80ms
step:2310/2315 train_time:140448ms step_avg:60.80ms
step:2311/2315 train_time:140509ms step_avg:60.80ms
step:2312/2315 train_time:140570ms step_avg:60.80ms
step:2313/2315 train_time:140631ms step_avg:60.80ms
step:2314/2315 train_time:140692ms step_avg:60.80ms
step:2315/2315 train_time:140754ms step_avg:60.80ms
step:2315/2315 val_loss:3.2764 train_time:140816ms step_avg:60.83ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

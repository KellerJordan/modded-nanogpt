import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]
# polar_express_coeffs = [
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
# ]



@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    # state["second_momentum_buffer"] = torch.zeros_like(grad[...,0:1]) if param.size(-2) >= param.size(-1) else torch.zeros_like(grad[...,0:1,:])
                    # state["second_momentum_buffer"] = torch.zeros((*grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((*grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)
            
            ###################################
            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))
            ####################################


            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> (-1.5)  0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Oct 22 2025, 02:20:45) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251022+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 24 06:01:57 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000001:00:00.0 Off |                    0 |
| N/A   36C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000002:00:00.0 Off |                    0 |
| N/A   35C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000003:00:00.0 Off |                    0 |
| N/A   31C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000008:00:00.0 Off |                    0 |
| N/A   31C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000009:00:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   0000000A:00:00.0 Off |                    0 |
| N/A   36C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   0000000C:00:00.0 Off |                    0 |
| N/A   35C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2315 train_time:84ms step_avg:84.35ms
step:2/2315 train_time:181ms step_avg:90.68ms
step:3/2315 train_time:202ms step_avg:67.38ms
step:4/2315 train_time:239ms step_avg:59.67ms
step:5/2315 train_time:297ms step_avg:59.44ms
step:6/2315 train_time:357ms step_avg:59.48ms
step:7/2315 train_time:416ms step_avg:59.46ms
step:8/2315 train_time:476ms step_avg:59.54ms
step:9/2315 train_time:536ms step_avg:59.55ms
step:10/2315 train_time:595ms step_avg:59.54ms
step:11/2315 train_time:655ms step_avg:59.55ms
step:12/2315 train_time:715ms step_avg:59.56ms
step:13/2315 train_time:774ms step_avg:59.57ms
step:14/2315 train_time:834ms step_avg:59.57ms
step:15/2315 train_time:894ms step_avg:59.58ms
step:16/2315 train_time:954ms step_avg:59.61ms
step:17/2315 train_time:1016ms step_avg:59.74ms
step:18/2315 train_time:1079ms step_avg:59.96ms
step:19/2315 train_time:1145ms step_avg:60.25ms
step:20/2315 train_time:1206ms step_avg:60.32ms
step:21/2315 train_time:1266ms step_avg:60.31ms
step:22/2315 train_time:1326ms step_avg:60.29ms
step:23/2315 train_time:1387ms step_avg:60.30ms
step:24/2315 train_time:1447ms step_avg:60.29ms
step:25/2315 train_time:1507ms step_avg:60.29ms
step:26/2315 train_time:1567ms step_avg:60.27ms
step:27/2315 train_time:1627ms step_avg:60.27ms
step:28/2315 train_time:1688ms step_avg:60.28ms
step:29/2315 train_time:1748ms step_avg:60.28ms
step:30/2315 train_time:1808ms step_avg:60.27ms
step:31/2315 train_time:1868ms step_avg:60.27ms
step:32/2315 train_time:1929ms step_avg:60.27ms
step:33/2315 train_time:1990ms step_avg:60.29ms
step:34/2315 train_time:2050ms step_avg:60.30ms
step:35/2315 train_time:2111ms step_avg:60.32ms
step:36/2315 train_time:2171ms step_avg:60.32ms
step:37/2315 train_time:2232ms step_avg:60.31ms
step:38/2315 train_time:2292ms step_avg:60.31ms
step:39/2315 train_time:2351ms step_avg:60.29ms
step:40/2315 train_time:2411ms step_avg:60.27ms
step:41/2315 train_time:2471ms step_avg:60.26ms
step:42/2315 train_time:2530ms step_avg:60.25ms
step:43/2315 train_time:2590ms step_avg:60.24ms
step:44/2315 train_time:2650ms step_avg:60.23ms
step:45/2315 train_time:2710ms step_avg:60.22ms
step:46/2315 train_time:2770ms step_avg:60.21ms
step:47/2315 train_time:2830ms step_avg:60.22ms
step:48/2315 train_time:2890ms step_avg:60.21ms
step:49/2315 train_time:2950ms step_avg:60.21ms
step:50/2315 train_time:3011ms step_avg:60.21ms
step:51/2315 train_time:3071ms step_avg:60.21ms
step:52/2315 train_time:3131ms step_avg:60.20ms
step:53/2315 train_time:3191ms step_avg:60.20ms
step:54/2315 train_time:3251ms step_avg:60.20ms
step:55/2315 train_time:3311ms step_avg:60.19ms
step:56/2315 train_time:3371ms step_avg:60.19ms
step:57/2315 train_time:3430ms step_avg:60.18ms
step:58/2315 train_time:3490ms step_avg:60.17ms
step:59/2315 train_time:3550ms step_avg:60.17ms
step:60/2315 train_time:3610ms step_avg:60.17ms
step:61/2315 train_time:3670ms step_avg:60.16ms
step:62/2315 train_time:3729ms step_avg:60.15ms
step:63/2315 train_time:3789ms step_avg:60.14ms
step:64/2315 train_time:3850ms step_avg:60.15ms
step:65/2315 train_time:3910ms step_avg:60.15ms
step:66/2315 train_time:3970ms step_avg:60.15ms
step:67/2315 train_time:4030ms step_avg:60.15ms
step:68/2315 train_time:4090ms step_avg:60.15ms
step:69/2315 train_time:4150ms step_avg:60.15ms
step:70/2315 train_time:4210ms step_avg:60.15ms
step:71/2315 train_time:4270ms step_avg:60.14ms
step:72/2315 train_time:4330ms step_avg:60.14ms
step:73/2315 train_time:4390ms step_avg:60.14ms
step:74/2315 train_time:4450ms step_avg:60.14ms
step:75/2315 train_time:4510ms step_avg:60.13ms
step:76/2315 train_time:4570ms step_avg:60.13ms
step:77/2315 train_time:4629ms step_avg:60.12ms
step:78/2315 train_time:4689ms step_avg:60.11ms
step:79/2315 train_time:4748ms step_avg:60.10ms
step:80/2315 train_time:4808ms step_avg:60.10ms
step:81/2315 train_time:4868ms step_avg:60.10ms
step:82/2315 train_time:4929ms step_avg:60.11ms
step:83/2315 train_time:4989ms step_avg:60.11ms
step:84/2315 train_time:5050ms step_avg:60.12ms
step:85/2315 train_time:5110ms step_avg:60.12ms
step:86/2315 train_time:5170ms step_avg:60.12ms
step:87/2315 train_time:5230ms step_avg:60.12ms
step:88/2315 train_time:5290ms step_avg:60.11ms
step:89/2315 train_time:5350ms step_avg:60.12ms
step:90/2315 train_time:5410ms step_avg:60.11ms
step:91/2315 train_time:5470ms step_avg:60.11ms
step:92/2315 train_time:5529ms step_avg:60.10ms
step:93/2315 train_time:5589ms step_avg:60.10ms
step:94/2315 train_time:5649ms step_avg:60.09ms
step:95/2315 train_time:5708ms step_avg:60.09ms
step:96/2315 train_time:5768ms step_avg:60.08ms
step:97/2315 train_time:5828ms step_avg:60.08ms
step:98/2315 train_time:5888ms step_avg:60.08ms
step:99/2315 train_time:5948ms step_avg:60.08ms
step:100/2315 train_time:6008ms step_avg:60.08ms
step:101/2315 train_time:6068ms step_avg:60.08ms
step:102/2315 train_time:6129ms step_avg:60.09ms
step:103/2315 train_time:6189ms step_avg:60.08ms
step:104/2315 train_time:6249ms step_avg:60.08ms
step:105/2315 train_time:6309ms step_avg:60.09ms
step:106/2315 train_time:6369ms step_avg:60.09ms
step:107/2315 train_time:6429ms step_avg:60.09ms
step:108/2315 train_time:6489ms step_avg:60.08ms
step:109/2315 train_time:6549ms step_avg:60.08ms
step:110/2315 train_time:6608ms step_avg:60.08ms
step:111/2315 train_time:6668ms step_avg:60.07ms
step:112/2315 train_time:6728ms step_avg:60.07ms
step:113/2315 train_time:6787ms step_avg:60.07ms
step:114/2315 train_time:6847ms step_avg:60.06ms
step:115/2315 train_time:6907ms step_avg:60.06ms
step:116/2315 train_time:6967ms step_avg:60.06ms
step:117/2315 train_time:7028ms step_avg:60.07ms
step:118/2315 train_time:7087ms step_avg:60.06ms
step:119/2315 train_time:7148ms step_avg:60.06ms
step:120/2315 train_time:7207ms step_avg:60.06ms
step:121/2315 train_time:7267ms step_avg:60.06ms
step:122/2315 train_time:7328ms step_avg:60.06ms
step:123/2315 train_time:7388ms step_avg:60.07ms
step:124/2315 train_time:7448ms step_avg:60.06ms
step:125/2315 train_time:7509ms step_avg:60.07ms
step:126/2315 train_time:7569ms step_avg:60.07ms
step:127/2315 train_time:7628ms step_avg:60.06ms
step:128/2315 train_time:7688ms step_avg:60.06ms
step:129/2315 train_time:7747ms step_avg:60.06ms
step:130/2315 train_time:7807ms step_avg:60.05ms
step:131/2315 train_time:7867ms step_avg:60.05ms
step:132/2315 train_time:7927ms step_avg:60.05ms
step:133/2315 train_time:7987ms step_avg:60.05ms
step:134/2315 train_time:8047ms step_avg:60.05ms
step:135/2315 train_time:8107ms step_avg:60.05ms
step:136/2315 train_time:8167ms step_avg:60.05ms
step:137/2315 train_time:8227ms step_avg:60.05ms
step:138/2315 train_time:8288ms step_avg:60.05ms
step:139/2315 train_time:8349ms step_avg:60.06ms
step:140/2315 train_time:8409ms step_avg:60.06ms
step:141/2315 train_time:8469ms step_avg:60.06ms
step:142/2315 train_time:8529ms step_avg:60.06ms
step:143/2315 train_time:8589ms step_avg:60.06ms
step:144/2315 train_time:8648ms step_avg:60.06ms
step:145/2315 train_time:8708ms step_avg:60.05ms
step:146/2315 train_time:8768ms step_avg:60.05ms
step:147/2315 train_time:8827ms step_avg:60.05ms
step:148/2315 train_time:8887ms step_avg:60.05ms
step:149/2315 train_time:8947ms step_avg:60.05ms
step:150/2315 train_time:9007ms step_avg:60.05ms
step:151/2315 train_time:9067ms step_avg:60.05ms
step:152/2315 train_time:9127ms step_avg:60.05ms
step:153/2315 train_time:9187ms step_avg:60.05ms
step:154/2315 train_time:9247ms step_avg:60.04ms
step:155/2315 train_time:9307ms step_avg:60.05ms
step:156/2315 train_time:9367ms step_avg:60.05ms
step:157/2315 train_time:9428ms step_avg:60.05ms
step:158/2315 train_time:9487ms step_avg:60.05ms
step:159/2315 train_time:9547ms step_avg:60.05ms
step:160/2315 train_time:9607ms step_avg:60.04ms
step:161/2315 train_time:9667ms step_avg:60.04ms
step:162/2315 train_time:9726ms step_avg:60.04ms
step:163/2315 train_time:9786ms step_avg:60.04ms
step:164/2315 train_time:9845ms step_avg:60.03ms
step:165/2315 train_time:9906ms step_avg:60.03ms
step:166/2315 train_time:9966ms step_avg:60.04ms
step:167/2315 train_time:10026ms step_avg:60.04ms
step:168/2315 train_time:10086ms step_avg:60.04ms
step:169/2315 train_time:10147ms step_avg:60.04ms
step:170/2315 train_time:10206ms step_avg:60.04ms
step:171/2315 train_time:10266ms step_avg:60.04ms
step:172/2315 train_time:10327ms step_avg:60.04ms
step:173/2315 train_time:10387ms step_avg:60.04ms
step:174/2315 train_time:10448ms step_avg:60.04ms
step:175/2315 train_time:10508ms step_avg:60.05ms
step:176/2315 train_time:10568ms step_avg:60.05ms
step:177/2315 train_time:10629ms step_avg:60.05ms
step:178/2315 train_time:10688ms step_avg:60.05ms
step:179/2315 train_time:10748ms step_avg:60.05ms
step:180/2315 train_time:10808ms step_avg:60.04ms
step:181/2315 train_time:10868ms step_avg:60.04ms
step:182/2315 train_time:10927ms step_avg:60.04ms
step:183/2315 train_time:10987ms step_avg:60.04ms
step:184/2315 train_time:11047ms step_avg:60.04ms
step:185/2315 train_time:11108ms step_avg:60.04ms
step:186/2315 train_time:11168ms step_avg:60.04ms
step:187/2315 train_time:11228ms step_avg:60.04ms
step:188/2315 train_time:11289ms step_avg:60.05ms
step:189/2315 train_time:11348ms step_avg:60.04ms
step:190/2315 train_time:11408ms step_avg:60.04ms
step:191/2315 train_time:11468ms step_avg:60.04ms
step:192/2315 train_time:11528ms step_avg:60.04ms
step:193/2315 train_time:11588ms step_avg:60.04ms
step:194/2315 train_time:11648ms step_avg:60.04ms
step:195/2315 train_time:11708ms step_avg:60.04ms
step:196/2315 train_time:11768ms step_avg:60.04ms
step:197/2315 train_time:11828ms step_avg:60.04ms
step:198/2315 train_time:11888ms step_avg:60.04ms
step:199/2315 train_time:11948ms step_avg:60.04ms
step:200/2315 train_time:12007ms step_avg:60.04ms
step:201/2315 train_time:12067ms step_avg:60.04ms
step:202/2315 train_time:12127ms step_avg:60.04ms
step:203/2315 train_time:12187ms step_avg:60.03ms
step:204/2315 train_time:12247ms step_avg:60.03ms
step:205/2315 train_time:12306ms step_avg:60.03ms
step:206/2315 train_time:12366ms step_avg:60.03ms
step:207/2315 train_time:12426ms step_avg:60.03ms
step:208/2315 train_time:12487ms step_avg:60.03ms
step:209/2315 train_time:12547ms step_avg:60.03ms
step:210/2315 train_time:12607ms step_avg:60.03ms
step:211/2315 train_time:12666ms step_avg:60.03ms
step:212/2315 train_time:12726ms step_avg:60.03ms
step:213/2315 train_time:12786ms step_avg:60.03ms
step:214/2315 train_time:12846ms step_avg:60.03ms
step:215/2315 train_time:12906ms step_avg:60.03ms
step:216/2315 train_time:12966ms step_avg:60.03ms
step:217/2315 train_time:13026ms step_avg:60.03ms
step:218/2315 train_time:13086ms step_avg:60.03ms
step:219/2315 train_time:13146ms step_avg:60.03ms
step:220/2315 train_time:13206ms step_avg:60.03ms
step:221/2315 train_time:13266ms step_avg:60.03ms
step:222/2315 train_time:13326ms step_avg:60.03ms
step:223/2315 train_time:13386ms step_avg:60.03ms
step:224/2315 train_time:13446ms step_avg:60.03ms
step:225/2315 train_time:13507ms step_avg:60.03ms
step:226/2315 train_time:13567ms step_avg:60.03ms
step:227/2315 train_time:13627ms step_avg:60.03ms
step:228/2315 train_time:13687ms step_avg:60.03ms
step:229/2315 train_time:13747ms step_avg:60.03ms
step:230/2315 train_time:13807ms step_avg:60.03ms
step:231/2315 train_time:13867ms step_avg:60.03ms
step:232/2315 train_time:13927ms step_avg:60.03ms
step:233/2315 train_time:13987ms step_avg:60.03ms
step:234/2315 train_time:14047ms step_avg:60.03ms
step:235/2315 train_time:14107ms step_avg:60.03ms
step:236/2315 train_time:14167ms step_avg:60.03ms
step:237/2315 train_time:14227ms step_avg:60.03ms
step:238/2315 train_time:14287ms step_avg:60.03ms
step:239/2315 train_time:14347ms step_avg:60.03ms
step:240/2315 train_time:14407ms step_avg:60.03ms
step:241/2315 train_time:14467ms step_avg:60.03ms
step:242/2315 train_time:14527ms step_avg:60.03ms
step:243/2315 train_time:14587ms step_avg:60.03ms
step:244/2315 train_time:14646ms step_avg:60.03ms
step:245/2315 train_time:14707ms step_avg:60.03ms
step:246/2315 train_time:14767ms step_avg:60.03ms
step:247/2315 train_time:14827ms step_avg:60.03ms
step:248/2315 train_time:14887ms step_avg:60.03ms
step:249/2315 train_time:14947ms step_avg:60.03ms
step:250/2315 train_time:15007ms step_avg:60.03ms
step:250/2315 val_loss:4.0734 train_time:15068ms step_avg:60.27ms
step:251/2315 train_time:15089ms step_avg:60.11ms
step:252/2315 train_time:15128ms step_avg:60.03ms
step:253/2315 train_time:15192ms step_avg:60.05ms
step:254/2315 train_time:15256ms step_avg:60.06ms
step:255/2315 train_time:15318ms step_avg:60.07ms
step:256/2315 train_time:15378ms step_avg:60.07ms
step:257/2315 train_time:15438ms step_avg:60.07ms
step:258/2315 train_time:15498ms step_avg:60.07ms
step:259/2315 train_time:15557ms step_avg:60.06ms
step:260/2315 train_time:15616ms step_avg:60.06ms
step:261/2315 train_time:15675ms step_avg:60.06ms
step:262/2315 train_time:15734ms step_avg:60.05ms
step:263/2315 train_time:15793ms step_avg:60.05ms
step:264/2315 train_time:15852ms step_avg:60.05ms
step:265/2315 train_time:15911ms step_avg:60.04ms
step:266/2315 train_time:15970ms step_avg:60.04ms
step:267/2315 train_time:16029ms step_avg:60.04ms
step:268/2315 train_time:16089ms step_avg:60.03ms
step:269/2315 train_time:16149ms step_avg:60.03ms
step:270/2315 train_time:16210ms step_avg:60.04ms
step:271/2315 train_time:16272ms step_avg:60.04ms
step:272/2315 train_time:16332ms step_avg:60.04ms
step:273/2315 train_time:16392ms step_avg:60.04ms
step:274/2315 train_time:16452ms step_avg:60.04ms
step:275/2315 train_time:16512ms step_avg:60.04ms
step:276/2315 train_time:16572ms step_avg:60.04ms
step:277/2315 train_time:16632ms step_avg:60.04ms
step:278/2315 train_time:16691ms step_avg:60.04ms
step:279/2315 train_time:16751ms step_avg:60.04ms
step:280/2315 train_time:16810ms step_avg:60.04ms
step:281/2315 train_time:16869ms step_avg:60.03ms
step:282/2315 train_time:16929ms step_avg:60.03ms
step:283/2315 train_time:16988ms step_avg:60.03ms
step:284/2315 train_time:17047ms step_avg:60.03ms
step:285/2315 train_time:17108ms step_avg:60.03ms
step:286/2315 train_time:17168ms step_avg:60.03ms
step:287/2315 train_time:17228ms step_avg:60.03ms
step:288/2315 train_time:17288ms step_avg:60.03ms
step:289/2315 train_time:17349ms step_avg:60.03ms
step:290/2315 train_time:17410ms step_avg:60.03ms
step:291/2315 train_time:17470ms step_avg:60.03ms
step:292/2315 train_time:17530ms step_avg:60.03ms
step:293/2315 train_time:17590ms step_avg:60.04ms
step:294/2315 train_time:17650ms step_avg:60.04ms
step:295/2315 train_time:17711ms step_avg:60.04ms
step:296/2315 train_time:17770ms step_avg:60.03ms
step:297/2315 train_time:17830ms step_avg:60.03ms
step:298/2315 train_time:17889ms step_avg:60.03ms
step:299/2315 train_time:17948ms step_avg:60.03ms
step:300/2315 train_time:18008ms step_avg:60.03ms
step:301/2315 train_time:18067ms step_avg:60.02ms
step:302/2315 train_time:18128ms step_avg:60.03ms
step:303/2315 train_time:18188ms step_avg:60.03ms
step:304/2315 train_time:18248ms step_avg:60.03ms
step:305/2315 train_time:18308ms step_avg:60.03ms
step:306/2315 train_time:18369ms step_avg:60.03ms
step:307/2315 train_time:18430ms step_avg:60.03ms
step:308/2315 train_time:18490ms step_avg:60.03ms
step:309/2315 train_time:18549ms step_avg:60.03ms
step:310/2315 train_time:18610ms step_avg:60.03ms
step:311/2315 train_time:18669ms step_avg:60.03ms
step:312/2315 train_time:18729ms step_avg:60.03ms
step:313/2315 train_time:18789ms step_avg:60.03ms
step:314/2315 train_time:18849ms step_avg:60.03ms
step:315/2315 train_time:18908ms step_avg:60.03ms
step:316/2315 train_time:18968ms step_avg:60.02ms
step:317/2315 train_time:19028ms step_avg:60.03ms
step:318/2315 train_time:19089ms step_avg:60.03ms
step:319/2315 train_time:19148ms step_avg:60.03ms
step:320/2315 train_time:19208ms step_avg:60.03ms
step:321/2315 train_time:19268ms step_avg:60.02ms
step:322/2315 train_time:19328ms step_avg:60.02ms
step:323/2315 train_time:19388ms step_avg:60.02ms
step:324/2315 train_time:19448ms step_avg:60.02ms
step:325/2315 train_time:19509ms step_avg:60.03ms
step:326/2315 train_time:19569ms step_avg:60.03ms
step:327/2315 train_time:19629ms step_avg:60.03ms
step:328/2315 train_time:19689ms step_avg:60.03ms
step:329/2315 train_time:19750ms step_avg:60.03ms
step:330/2315 train_time:19810ms step_avg:60.03ms
step:331/2315 train_time:19869ms step_avg:60.03ms
step:332/2315 train_time:19929ms step_avg:60.03ms
step:333/2315 train_time:19988ms step_avg:60.03ms
step:334/2315 train_time:20048ms step_avg:60.02ms
step:335/2315 train_time:20108ms step_avg:60.02ms
step:336/2315 train_time:20169ms step_avg:60.03ms
step:337/2315 train_time:20229ms step_avg:60.03ms
step:338/2315 train_time:20288ms step_avg:60.02ms
step:339/2315 train_time:20348ms step_avg:60.02ms
step:340/2315 train_time:20408ms step_avg:60.02ms
step:341/2315 train_time:20468ms step_avg:60.02ms
step:342/2315 train_time:20528ms step_avg:60.02ms
step:343/2315 train_time:20588ms step_avg:60.02ms
step:344/2315 train_time:20649ms step_avg:60.02ms
step:345/2315 train_time:20709ms step_avg:60.02ms
step:346/2315 train_time:20768ms step_avg:60.02ms
step:347/2315 train_time:20829ms step_avg:60.02ms
step:348/2315 train_time:20888ms step_avg:60.02ms
step:349/2315 train_time:20948ms step_avg:60.02ms
step:350/2315 train_time:21008ms step_avg:60.02ms
step:351/2315 train_time:21067ms step_avg:60.02ms
step:352/2315 train_time:21127ms step_avg:60.02ms
step:353/2315 train_time:21187ms step_avg:60.02ms
step:354/2315 train_time:21247ms step_avg:60.02ms
step:355/2315 train_time:21308ms step_avg:60.02ms
step:356/2315 train_time:21367ms step_avg:60.02ms
step:357/2315 train_time:21427ms step_avg:60.02ms
step:358/2315 train_time:21488ms step_avg:60.02ms
step:359/2315 train_time:21549ms step_avg:60.02ms
step:360/2315 train_time:21609ms step_avg:60.02ms
step:361/2315 train_time:21669ms step_avg:60.02ms
step:362/2315 train_time:21729ms step_avg:60.02ms
step:363/2315 train_time:21788ms step_avg:60.02ms
step:364/2315 train_time:21848ms step_avg:60.02ms
step:365/2315 train_time:21908ms step_avg:60.02ms
step:366/2315 train_time:21967ms step_avg:60.02ms
step:367/2315 train_time:22028ms step_avg:60.02ms
step:368/2315 train_time:22088ms step_avg:60.02ms
step:369/2315 train_time:22148ms step_avg:60.02ms
step:370/2315 train_time:22208ms step_avg:60.02ms
step:371/2315 train_time:22268ms step_avg:60.02ms
step:372/2315 train_time:22328ms step_avg:60.02ms
step:373/2315 train_time:22388ms step_avg:60.02ms
step:374/2315 train_time:22448ms step_avg:60.02ms
step:375/2315 train_time:22508ms step_avg:60.02ms
step:376/2315 train_time:22568ms step_avg:60.02ms
step:377/2315 train_time:22629ms step_avg:60.02ms
step:378/2315 train_time:22689ms step_avg:60.02ms
step:379/2315 train_time:22748ms step_avg:60.02ms
step:380/2315 train_time:22808ms step_avg:60.02ms
step:381/2315 train_time:22868ms step_avg:60.02ms
step:382/2315 train_time:22928ms step_avg:60.02ms
step:383/2315 train_time:22988ms step_avg:60.02ms
step:384/2315 train_time:23048ms step_avg:60.02ms
step:385/2315 train_time:23107ms step_avg:60.02ms
step:386/2315 train_time:23168ms step_avg:60.02ms
step:387/2315 train_time:23228ms step_avg:60.02ms
step:388/2315 train_time:23288ms step_avg:60.02ms
step:389/2315 train_time:23348ms step_avg:60.02ms
step:390/2315 train_time:23408ms step_avg:60.02ms
step:391/2315 train_time:23468ms step_avg:60.02ms
step:392/2315 train_time:23528ms step_avg:60.02ms
step:393/2315 train_time:23588ms step_avg:60.02ms
step:394/2315 train_time:23648ms step_avg:60.02ms
step:395/2315 train_time:23708ms step_avg:60.02ms
step:396/2315 train_time:23768ms step_avg:60.02ms
step:397/2315 train_time:23828ms step_avg:60.02ms
step:398/2315 train_time:23888ms step_avg:60.02ms
step:399/2315 train_time:23949ms step_avg:60.02ms
step:400/2315 train_time:24009ms step_avg:60.02ms
step:401/2315 train_time:24068ms step_avg:60.02ms
step:402/2315 train_time:24128ms step_avg:60.02ms
step:403/2315 train_time:24188ms step_avg:60.02ms
step:404/2315 train_time:24248ms step_avg:60.02ms
step:405/2315 train_time:24309ms step_avg:60.02ms
step:406/2315 train_time:24368ms step_avg:60.02ms
step:407/2315 train_time:24428ms step_avg:60.02ms
step:408/2315 train_time:24489ms step_avg:60.02ms
step:409/2315 train_time:24549ms step_avg:60.02ms
step:410/2315 train_time:24609ms step_avg:60.02ms
step:411/2315 train_time:24669ms step_avg:60.02ms
step:412/2315 train_time:24729ms step_avg:60.02ms
step:413/2315 train_time:24789ms step_avg:60.02ms
step:414/2315 train_time:24849ms step_avg:60.02ms
step:415/2315 train_time:24909ms step_avg:60.02ms
step:416/2315 train_time:24969ms step_avg:60.02ms
step:417/2315 train_time:25029ms step_avg:60.02ms
step:418/2315 train_time:25089ms step_avg:60.02ms
step:419/2315 train_time:25149ms step_avg:60.02ms
step:420/2315 train_time:25209ms step_avg:60.02ms
step:421/2315 train_time:25269ms step_avg:60.02ms
step:422/2315 train_time:25329ms step_avg:60.02ms
step:423/2315 train_time:25389ms step_avg:60.02ms
step:424/2315 train_time:25449ms step_avg:60.02ms
step:425/2315 train_time:25509ms step_avg:60.02ms
step:426/2315 train_time:25569ms step_avg:60.02ms
step:427/2315 train_time:25629ms step_avg:60.02ms
step:428/2315 train_time:25689ms step_avg:60.02ms
step:429/2315 train_time:25749ms step_avg:60.02ms
step:430/2315 train_time:25810ms step_avg:60.02ms
step:431/2315 train_time:25870ms step_avg:60.02ms
step:432/2315 train_time:25930ms step_avg:60.02ms
step:433/2315 train_time:25989ms step_avg:60.02ms
step:434/2315 train_time:26049ms step_avg:60.02ms
step:435/2315 train_time:26109ms step_avg:60.02ms
step:436/2315 train_time:26169ms step_avg:60.02ms
step:437/2315 train_time:26229ms step_avg:60.02ms
step:438/2315 train_time:26288ms step_avg:60.02ms
step:439/2315 train_time:26349ms step_avg:60.02ms
step:440/2315 train_time:26409ms step_avg:60.02ms
step:441/2315 train_time:26469ms step_avg:60.02ms
step:442/2315 train_time:26529ms step_avg:60.02ms
step:443/2315 train_time:26589ms step_avg:60.02ms
step:444/2315 train_time:26649ms step_avg:60.02ms
step:445/2315 train_time:26709ms step_avg:60.02ms
step:446/2315 train_time:26769ms step_avg:60.02ms
step:447/2315 train_time:26829ms step_avg:60.02ms
step:448/2315 train_time:26889ms step_avg:60.02ms
step:449/2315 train_time:26949ms step_avg:60.02ms
step:450/2315 train_time:27009ms step_avg:60.02ms
step:451/2315 train_time:27069ms step_avg:60.02ms
step:452/2315 train_time:27129ms step_avg:60.02ms
step:453/2315 train_time:27189ms step_avg:60.02ms
step:454/2315 train_time:27249ms step_avg:60.02ms
step:455/2315 train_time:27309ms step_avg:60.02ms
step:456/2315 train_time:27369ms step_avg:60.02ms
step:457/2315 train_time:27429ms step_avg:60.02ms
step:458/2315 train_time:27489ms step_avg:60.02ms
step:459/2315 train_time:27548ms step_avg:60.02ms
step:460/2315 train_time:27608ms step_avg:60.02ms
step:461/2315 train_time:27668ms step_avg:60.02ms
step:462/2315 train_time:27728ms step_avg:60.02ms
step:463/2315 train_time:27788ms step_avg:60.02ms
step:464/2315 train_time:27848ms step_avg:60.02ms
step:465/2315 train_time:27908ms step_avg:60.02ms
step:466/2315 train_time:27968ms step_avg:60.02ms
step:467/2315 train_time:28028ms step_avg:60.02ms
step:468/2315 train_time:28088ms step_avg:60.02ms
step:469/2315 train_time:28148ms step_avg:60.02ms
step:470/2315 train_time:28208ms step_avg:60.02ms
step:471/2315 train_time:28268ms step_avg:60.02ms
step:472/2315 train_time:28328ms step_avg:60.02ms
step:473/2315 train_time:28387ms step_avg:60.02ms
step:474/2315 train_time:28447ms step_avg:60.01ms
step:475/2315 train_time:28507ms step_avg:60.02ms
step:476/2315 train_time:28568ms step_avg:60.02ms
step:477/2315 train_time:28628ms step_avg:60.02ms
step:478/2315 train_time:28688ms step_avg:60.02ms
step:479/2315 train_time:28748ms step_avg:60.02ms
step:480/2315 train_time:28809ms step_avg:60.02ms
step:481/2315 train_time:28869ms step_avg:60.02ms
step:482/2315 train_time:28928ms step_avg:60.02ms
step:483/2315 train_time:28988ms step_avg:60.02ms
step:484/2315 train_time:29048ms step_avg:60.02ms
step:485/2315 train_time:29109ms step_avg:60.02ms
step:486/2315 train_time:29169ms step_avg:60.02ms
step:487/2315 train_time:29229ms step_avg:60.02ms
step:488/2315 train_time:29289ms step_avg:60.02ms
step:489/2315 train_time:29349ms step_avg:60.02ms
step:490/2315 train_time:29409ms step_avg:60.02ms
step:491/2315 train_time:29468ms step_avg:60.02ms
step:492/2315 train_time:29528ms step_avg:60.02ms
step:493/2315 train_time:29588ms step_avg:60.02ms
step:494/2315 train_time:29648ms step_avg:60.02ms
step:495/2315 train_time:29708ms step_avg:60.02ms
step:496/2315 train_time:29768ms step_avg:60.02ms
step:497/2315 train_time:29827ms step_avg:60.01ms
step:498/2315 train_time:29887ms step_avg:60.01ms
step:499/2315 train_time:29947ms step_avg:60.01ms
step:500/2315 train_time:30007ms step_avg:60.01ms
step:500/2315 val_loss:3.8149 train_time:30070ms step_avg:60.14ms
step:501/2315 train_time:30089ms step_avg:60.06ms
step:502/2315 train_time:30130ms step_avg:60.02ms
step:503/2315 train_time:30195ms step_avg:60.03ms
step:504/2315 train_time:30258ms step_avg:60.04ms
step:505/2315 train_time:30318ms step_avg:60.04ms
step:506/2315 train_time:30379ms step_avg:60.04ms
step:507/2315 train_time:30439ms step_avg:60.04ms
step:508/2315 train_time:30498ms step_avg:60.04ms
step:509/2315 train_time:30557ms step_avg:60.03ms
step:510/2315 train_time:30616ms step_avg:60.03ms
step:511/2315 train_time:30676ms step_avg:60.03ms
step:512/2315 train_time:30735ms step_avg:60.03ms
step:513/2315 train_time:30795ms step_avg:60.03ms
step:514/2315 train_time:30854ms step_avg:60.03ms
step:515/2315 train_time:30913ms step_avg:60.03ms
step:516/2315 train_time:30973ms step_avg:60.02ms
step:517/2315 train_time:31034ms step_avg:60.03ms
step:518/2315 train_time:31094ms step_avg:60.03ms
step:519/2315 train_time:31155ms step_avg:60.03ms
step:520/2315 train_time:31216ms step_avg:60.03ms
step:521/2315 train_time:31277ms step_avg:60.03ms
step:522/2315 train_time:31338ms step_avg:60.03ms
step:523/2315 train_time:31398ms step_avg:60.03ms
step:524/2315 train_time:31458ms step_avg:60.03ms
step:525/2315 train_time:31517ms step_avg:60.03ms
step:526/2315 train_time:31577ms step_avg:60.03ms
step:527/2315 train_time:31636ms step_avg:60.03ms
step:528/2315 train_time:31696ms step_avg:60.03ms
step:529/2315 train_time:31755ms step_avg:60.03ms
step:530/2315 train_time:31815ms step_avg:60.03ms
step:531/2315 train_time:31874ms step_avg:60.03ms
step:532/2315 train_time:31934ms step_avg:60.03ms
step:533/2315 train_time:31994ms step_avg:60.03ms
step:534/2315 train_time:32054ms step_avg:60.03ms
step:535/2315 train_time:32114ms step_avg:60.03ms
step:536/2315 train_time:32175ms step_avg:60.03ms
step:537/2315 train_time:32236ms step_avg:60.03ms
step:538/2315 train_time:32296ms step_avg:60.03ms
step:539/2315 train_time:32357ms step_avg:60.03ms
step:540/2315 train_time:32417ms step_avg:60.03ms
step:541/2315 train_time:32478ms step_avg:60.03ms
step:542/2315 train_time:32537ms step_avg:60.03ms
step:543/2315 train_time:32596ms step_avg:60.03ms
step:544/2315 train_time:32655ms step_avg:60.03ms
step:545/2315 train_time:32715ms step_avg:60.03ms
step:546/2315 train_time:32775ms step_avg:60.03ms
step:547/2315 train_time:32835ms step_avg:60.03ms
step:548/2315 train_time:32895ms step_avg:60.03ms
step:549/2315 train_time:32954ms step_avg:60.03ms
step:550/2315 train_time:33013ms step_avg:60.02ms
step:551/2315 train_time:33073ms step_avg:60.02ms
step:552/2315 train_time:33134ms step_avg:60.02ms
step:553/2315 train_time:33194ms step_avg:60.02ms
step:554/2315 train_time:33255ms step_avg:60.03ms
step:555/2315 train_time:33316ms step_avg:60.03ms
step:556/2315 train_time:33376ms step_avg:60.03ms
step:557/2315 train_time:33437ms step_avg:60.03ms
step:558/2315 train_time:33497ms step_avg:60.03ms
step:559/2315 train_time:33557ms step_avg:60.03ms
step:560/2315 train_time:33616ms step_avg:60.03ms
step:561/2315 train_time:33676ms step_avg:60.03ms
step:562/2315 train_time:33735ms step_avg:60.03ms
step:563/2315 train_time:33795ms step_avg:60.03ms
step:564/2315 train_time:33854ms step_avg:60.02ms
step:565/2315 train_time:33914ms step_avg:60.03ms
step:566/2315 train_time:33975ms step_avg:60.03ms
step:567/2315 train_time:34034ms step_avg:60.03ms
step:568/2315 train_time:34094ms step_avg:60.02ms
step:569/2315 train_time:34154ms step_avg:60.02ms
step:570/2315 train_time:34215ms step_avg:60.03ms
step:571/2315 train_time:34275ms step_avg:60.03ms
step:572/2315 train_time:34336ms step_avg:60.03ms
step:573/2315 train_time:34397ms step_avg:60.03ms
step:574/2315 train_time:34457ms step_avg:60.03ms
step:575/2315 train_time:34517ms step_avg:60.03ms
step:576/2315 train_time:34576ms step_avg:60.03ms
step:577/2315 train_time:34636ms step_avg:60.03ms
step:578/2315 train_time:34696ms step_avg:60.03ms
step:579/2315 train_time:34756ms step_avg:60.03ms
step:580/2315 train_time:34815ms step_avg:60.03ms
step:581/2315 train_time:34875ms step_avg:60.03ms
step:582/2315 train_time:34935ms step_avg:60.03ms
step:583/2315 train_time:34995ms step_avg:60.03ms
step:584/2315 train_time:35055ms step_avg:60.02ms
step:585/2315 train_time:35114ms step_avg:60.02ms
step:586/2315 train_time:35175ms step_avg:60.02ms
step:587/2315 train_time:35235ms step_avg:60.03ms
step:588/2315 train_time:35295ms step_avg:60.03ms
step:589/2315 train_time:35356ms step_avg:60.03ms
step:590/2315 train_time:35416ms step_avg:60.03ms
step:591/2315 train_time:35476ms step_avg:60.03ms
step:592/2315 train_time:35536ms step_avg:60.03ms
step:593/2315 train_time:35597ms step_avg:60.03ms
step:594/2315 train_time:35656ms step_avg:60.03ms
step:595/2315 train_time:35717ms step_avg:60.03ms
step:596/2315 train_time:35776ms step_avg:60.03ms
step:597/2315 train_time:35836ms step_avg:60.03ms
step:598/2315 train_time:35896ms step_avg:60.03ms
step:599/2315 train_time:35955ms step_avg:60.03ms
step:600/2315 train_time:36015ms step_avg:60.02ms
step:601/2315 train_time:36075ms step_avg:60.02ms
step:602/2315 train_time:36134ms step_avg:60.02ms
step:603/2315 train_time:36195ms step_avg:60.02ms
step:604/2315 train_time:36255ms step_avg:60.02ms
step:605/2315 train_time:36316ms step_avg:60.03ms
step:606/2315 train_time:36377ms step_avg:60.03ms
step:607/2315 train_time:36436ms step_avg:60.03ms
step:608/2315 train_time:36496ms step_avg:60.03ms
step:609/2315 train_time:36556ms step_avg:60.03ms
step:610/2315 train_time:36615ms step_avg:60.02ms
step:611/2315 train_time:36676ms step_avg:60.03ms
step:612/2315 train_time:36736ms step_avg:60.03ms
step:613/2315 train_time:36796ms step_avg:60.03ms
step:614/2315 train_time:36855ms step_avg:60.02ms
step:615/2315 train_time:36915ms step_avg:60.02ms
step:616/2315 train_time:36975ms step_avg:60.02ms
step:617/2315 train_time:37035ms step_avg:60.02ms
step:618/2315 train_time:37095ms step_avg:60.02ms
step:619/2315 train_time:37155ms step_avg:60.02ms
step:620/2315 train_time:37215ms step_avg:60.02ms
step:621/2315 train_time:37275ms step_avg:60.02ms
step:622/2315 train_time:37335ms step_avg:60.02ms
step:623/2315 train_time:37395ms step_avg:60.02ms
step:624/2315 train_time:37455ms step_avg:60.02ms
step:625/2315 train_time:37516ms step_avg:60.02ms
step:626/2315 train_time:37575ms step_avg:60.02ms
step:627/2315 train_time:37636ms step_avg:60.02ms
step:628/2315 train_time:37695ms step_avg:60.02ms
step:629/2315 train_time:37755ms step_avg:60.02ms
step:630/2315 train_time:37816ms step_avg:60.02ms
step:631/2315 train_time:37876ms step_avg:60.03ms
step:632/2315 train_time:37936ms step_avg:60.02ms
step:633/2315 train_time:37996ms step_avg:60.03ms
step:634/2315 train_time:38057ms step_avg:60.03ms
step:635/2315 train_time:38116ms step_avg:60.02ms
step:636/2315 train_time:38176ms step_avg:60.02ms
step:637/2315 train_time:38235ms step_avg:60.02ms
step:638/2315 train_time:38295ms step_avg:60.02ms
step:639/2315 train_time:38355ms step_avg:60.02ms
step:640/2315 train_time:38415ms step_avg:60.02ms
step:641/2315 train_time:38475ms step_avg:60.02ms
step:642/2315 train_time:38536ms step_avg:60.02ms
step:643/2315 train_time:38596ms step_avg:60.02ms
step:644/2315 train_time:38656ms step_avg:60.02ms
step:645/2315 train_time:38715ms step_avg:60.02ms
step:646/2315 train_time:38775ms step_avg:60.02ms
step:647/2315 train_time:38835ms step_avg:60.02ms
step:648/2315 train_time:38895ms step_avg:60.02ms
step:649/2315 train_time:38955ms step_avg:60.02ms
step:650/2315 train_time:39016ms step_avg:60.02ms
step:651/2315 train_time:39075ms step_avg:60.02ms
step:652/2315 train_time:39135ms step_avg:60.02ms
step:653/2315 train_time:39195ms step_avg:60.02ms
step:654/2315 train_time:39255ms step_avg:60.02ms
step:655/2315 train_time:39315ms step_avg:60.02ms
step:656/2315 train_time:39375ms step_avg:60.02ms
step:657/2315 train_time:39435ms step_avg:60.02ms
step:658/2315 train_time:39495ms step_avg:60.02ms
step:659/2315 train_time:39556ms step_avg:60.02ms
step:660/2315 train_time:39617ms step_avg:60.03ms
step:661/2315 train_time:39676ms step_avg:60.02ms
step:662/2315 train_time:39736ms step_avg:60.02ms
step:663/2315 train_time:39796ms step_avg:60.02ms
step:664/2315 train_time:39856ms step_avg:60.02ms
step:665/2315 train_time:39916ms step_avg:60.02ms
step:666/2315 train_time:39975ms step_avg:60.02ms
step:667/2315 train_time:40035ms step_avg:60.02ms
step:668/2315 train_time:40095ms step_avg:60.02ms
step:669/2315 train_time:40155ms step_avg:60.02ms
step:670/2315 train_time:40214ms step_avg:60.02ms
step:671/2315 train_time:40275ms step_avg:60.02ms
step:672/2315 train_time:40335ms step_avg:60.02ms
step:673/2315 train_time:40395ms step_avg:60.02ms
step:674/2315 train_time:40455ms step_avg:60.02ms
step:675/2315 train_time:40515ms step_avg:60.02ms
step:676/2315 train_time:40575ms step_avg:60.02ms
step:677/2315 train_time:40635ms step_avg:60.02ms
step:678/2315 train_time:40695ms step_avg:60.02ms
step:679/2315 train_time:40755ms step_avg:60.02ms
step:680/2315 train_time:40815ms step_avg:60.02ms
step:681/2315 train_time:40876ms step_avg:60.02ms
step:682/2315 train_time:40936ms step_avg:60.02ms
step:683/2315 train_time:40996ms step_avg:60.02ms
step:684/2315 train_time:41055ms step_avg:60.02ms
step:685/2315 train_time:41115ms step_avg:60.02ms
step:686/2315 train_time:41175ms step_avg:60.02ms
step:687/2315 train_time:41235ms step_avg:60.02ms
step:688/2315 train_time:41295ms step_avg:60.02ms
step:689/2315 train_time:41355ms step_avg:60.02ms
step:690/2315 train_time:41415ms step_avg:60.02ms
step:691/2315 train_time:41476ms step_avg:60.02ms
step:692/2315 train_time:41536ms step_avg:60.02ms
step:693/2315 train_time:41595ms step_avg:60.02ms
step:694/2315 train_time:41655ms step_avg:60.02ms
step:695/2315 train_time:41715ms step_avg:60.02ms
step:696/2315 train_time:41775ms step_avg:60.02ms
step:697/2315 train_time:41835ms step_avg:60.02ms
step:698/2315 train_time:41895ms step_avg:60.02ms
step:699/2315 train_time:41955ms step_avg:60.02ms
step:700/2315 train_time:42015ms step_avg:60.02ms
step:701/2315 train_time:42075ms step_avg:60.02ms
step:702/2315 train_time:42135ms step_avg:60.02ms
step:703/2315 train_time:42195ms step_avg:60.02ms
step:704/2315 train_time:42255ms step_avg:60.02ms
step:705/2315 train_time:42315ms step_avg:60.02ms
step:706/2315 train_time:42375ms step_avg:60.02ms
step:707/2315 train_time:42435ms step_avg:60.02ms
step:708/2315 train_time:42495ms step_avg:60.02ms
step:709/2315 train_time:42556ms step_avg:60.02ms
step:710/2315 train_time:42615ms step_avg:60.02ms
step:711/2315 train_time:42675ms step_avg:60.02ms
step:712/2315 train_time:42735ms step_avg:60.02ms
step:713/2315 train_time:42795ms step_avg:60.02ms
step:714/2315 train_time:42855ms step_avg:60.02ms
step:715/2315 train_time:42915ms step_avg:60.02ms
step:716/2315 train_time:42976ms step_avg:60.02ms
step:717/2315 train_time:43035ms step_avg:60.02ms
step:718/2315 train_time:43096ms step_avg:60.02ms
step:719/2315 train_time:43156ms step_avg:60.02ms
step:720/2315 train_time:43216ms step_avg:60.02ms
step:721/2315 train_time:43276ms step_avg:60.02ms
step:722/2315 train_time:43335ms step_avg:60.02ms
step:723/2315 train_time:43395ms step_avg:60.02ms
step:724/2315 train_time:43456ms step_avg:60.02ms
step:725/2315 train_time:43516ms step_avg:60.02ms
step:726/2315 train_time:43576ms step_avg:60.02ms
step:727/2315 train_time:43636ms step_avg:60.02ms
step:728/2315 train_time:43697ms step_avg:60.02ms
step:729/2315 train_time:43757ms step_avg:60.02ms
step:730/2315 train_time:43816ms step_avg:60.02ms
step:731/2315 train_time:43876ms step_avg:60.02ms
step:732/2315 train_time:43936ms step_avg:60.02ms
step:733/2315 train_time:43996ms step_avg:60.02ms
step:734/2315 train_time:44056ms step_avg:60.02ms
step:735/2315 train_time:44115ms step_avg:60.02ms
step:736/2315 train_time:44176ms step_avg:60.02ms
step:737/2315 train_time:44235ms step_avg:60.02ms
step:738/2315 train_time:44295ms step_avg:60.02ms
step:739/2315 train_time:44355ms step_avg:60.02ms
step:740/2315 train_time:44415ms step_avg:60.02ms
step:741/2315 train_time:44475ms step_avg:60.02ms
step:742/2315 train_time:44535ms step_avg:60.02ms
step:743/2315 train_time:44596ms step_avg:60.02ms
step:744/2315 train_time:44656ms step_avg:60.02ms
step:745/2315 train_time:44717ms step_avg:60.02ms
step:746/2315 train_time:44776ms step_avg:60.02ms
step:747/2315 train_time:44837ms step_avg:60.02ms
step:748/2315 train_time:44897ms step_avg:60.02ms
step:749/2315 train_time:44957ms step_avg:60.02ms
step:750/2315 train_time:45016ms step_avg:60.02ms
step:750/2315 val_loss:3.6826 train_time:45078ms step_avg:60.10ms
step:751/2315 train_time:45098ms step_avg:60.05ms
step:752/2315 train_time:45138ms step_avg:60.02ms
step:753/2315 train_time:45199ms step_avg:60.03ms
step:754/2315 train_time:45261ms step_avg:60.03ms
step:755/2315 train_time:45321ms step_avg:60.03ms
step:756/2315 train_time:45381ms step_avg:60.03ms
step:757/2315 train_time:45440ms step_avg:60.03ms
step:758/2315 train_time:45499ms step_avg:60.03ms
step:759/2315 train_time:45558ms step_avg:60.02ms
step:760/2315 train_time:45618ms step_avg:60.02ms
step:761/2315 train_time:45677ms step_avg:60.02ms
step:762/2315 train_time:45737ms step_avg:60.02ms
step:763/2315 train_time:45797ms step_avg:60.02ms
step:764/2315 train_time:45857ms step_avg:60.02ms
step:765/2315 train_time:45917ms step_avg:60.02ms
step:766/2315 train_time:45977ms step_avg:60.02ms
step:767/2315 train_time:46039ms step_avg:60.02ms
step:768/2315 train_time:46100ms step_avg:60.03ms
step:769/2315 train_time:46161ms step_avg:60.03ms
step:770/2315 train_time:46221ms step_avg:60.03ms
step:771/2315 train_time:46282ms step_avg:60.03ms
step:772/2315 train_time:46343ms step_avg:60.03ms
step:773/2315 train_time:46404ms step_avg:60.03ms
step:774/2315 train_time:46465ms step_avg:60.03ms
step:775/2315 train_time:46526ms step_avg:60.03ms
step:776/2315 train_time:46586ms step_avg:60.03ms
step:777/2315 train_time:46647ms step_avg:60.03ms
step:778/2315 train_time:46707ms step_avg:60.03ms
step:779/2315 train_time:46768ms step_avg:60.04ms
step:780/2315 train_time:46829ms step_avg:60.04ms
step:781/2315 train_time:46891ms step_avg:60.04ms
step:782/2315 train_time:46951ms step_avg:60.04ms
step:783/2315 train_time:47012ms step_avg:60.04ms
step:784/2315 train_time:47073ms step_avg:60.04ms
step:785/2315 train_time:47134ms step_avg:60.04ms
step:786/2315 train_time:47195ms step_avg:60.04ms
step:787/2315 train_time:47257ms step_avg:60.05ms
step:788/2315 train_time:47318ms step_avg:60.05ms
step:789/2315 train_time:47379ms step_avg:60.05ms
step:790/2315 train_time:47439ms step_avg:60.05ms
step:791/2315 train_time:47500ms step_avg:60.05ms
step:792/2315 train_time:47560ms step_avg:60.05ms
step:793/2315 train_time:47621ms step_avg:60.05ms
step:794/2315 train_time:47681ms step_avg:60.05ms
step:795/2315 train_time:47742ms step_avg:60.05ms
step:796/2315 train_time:47802ms step_avg:60.05ms
step:797/2315 train_time:47862ms step_avg:60.05ms
step:798/2315 train_time:47923ms step_avg:60.05ms
step:799/2315 train_time:47984ms step_avg:60.05ms
step:800/2315 train_time:48045ms step_avg:60.06ms
step:801/2315 train_time:48106ms step_avg:60.06ms
step:802/2315 train_time:48167ms step_avg:60.06ms
step:803/2315 train_time:48228ms step_avg:60.06ms
step:804/2315 train_time:48289ms step_avg:60.06ms
step:805/2315 train_time:48351ms step_avg:60.06ms
step:806/2315 train_time:48411ms step_avg:60.06ms
step:807/2315 train_time:48473ms step_avg:60.07ms
step:808/2315 train_time:48534ms step_avg:60.07ms
step:809/2315 train_time:48595ms step_avg:60.07ms
step:810/2315 train_time:48655ms step_avg:60.07ms
step:811/2315 train_time:48716ms step_avg:60.07ms
step:812/2315 train_time:48777ms step_avg:60.07ms
step:813/2315 train_time:48838ms step_avg:60.07ms
step:814/2315 train_time:48899ms step_avg:60.07ms
step:815/2315 train_time:48959ms step_avg:60.07ms
step:816/2315 train_time:49019ms step_avg:60.07ms
step:817/2315 train_time:49080ms step_avg:60.07ms
step:818/2315 train_time:49140ms step_avg:60.07ms
step:819/2315 train_time:49201ms step_avg:60.07ms
step:820/2315 train_time:49261ms step_avg:60.07ms
step:821/2315 train_time:49322ms step_avg:60.08ms
step:822/2315 train_time:49383ms step_avg:60.08ms
step:823/2315 train_time:49443ms step_avg:60.08ms
step:824/2315 train_time:49504ms step_avg:60.08ms
step:825/2315 train_time:49564ms step_avg:60.08ms
step:826/2315 train_time:49625ms step_avg:60.08ms
step:827/2315 train_time:49687ms step_avg:60.08ms
step:828/2315 train_time:49748ms step_avg:60.08ms
step:829/2315 train_time:49809ms step_avg:60.08ms
step:830/2315 train_time:49870ms step_avg:60.08ms
step:831/2315 train_time:49931ms step_avg:60.09ms
step:832/2315 train_time:49992ms step_avg:60.09ms
step:833/2315 train_time:50053ms step_avg:60.09ms
step:834/2315 train_time:50113ms step_avg:60.09ms
step:835/2315 train_time:50175ms step_avg:60.09ms
step:836/2315 train_time:50235ms step_avg:60.09ms
step:837/2315 train_time:50296ms step_avg:60.09ms
step:838/2315 train_time:50356ms step_avg:60.09ms
step:839/2315 train_time:50417ms step_avg:60.09ms
step:840/2315 train_time:50478ms step_avg:60.09ms
step:841/2315 train_time:50539ms step_avg:60.09ms
step:842/2315 train_time:50599ms step_avg:60.09ms
step:843/2315 train_time:50660ms step_avg:60.10ms
step:844/2315 train_time:50720ms step_avg:60.10ms
step:845/2315 train_time:50781ms step_avg:60.10ms
step:846/2315 train_time:50842ms step_avg:60.10ms
step:847/2315 train_time:50902ms step_avg:60.10ms
step:848/2315 train_time:50962ms step_avg:60.10ms
step:849/2315 train_time:51023ms step_avg:60.10ms
step:850/2315 train_time:51083ms step_avg:60.10ms
step:851/2315 train_time:51144ms step_avg:60.10ms
step:852/2315 train_time:51205ms step_avg:60.10ms
step:853/2315 train_time:51267ms step_avg:60.10ms
step:854/2315 train_time:51327ms step_avg:60.10ms
step:855/2315 train_time:51388ms step_avg:60.10ms
step:856/2315 train_time:51448ms step_avg:60.10ms
step:857/2315 train_time:51510ms step_avg:60.10ms
step:858/2315 train_time:51571ms step_avg:60.11ms
step:859/2315 train_time:51632ms step_avg:60.11ms
step:860/2315 train_time:51693ms step_avg:60.11ms
step:861/2315 train_time:51754ms step_avg:60.11ms
step:862/2315 train_time:51815ms step_avg:60.11ms
step:863/2315 train_time:51876ms step_avg:60.11ms
step:864/2315 train_time:51936ms step_avg:60.11ms
step:865/2315 train_time:51997ms step_avg:60.11ms
step:866/2315 train_time:52057ms step_avg:60.11ms
step:867/2315 train_time:52118ms step_avg:60.11ms
step:868/2315 train_time:52179ms step_avg:60.11ms
step:869/2315 train_time:52240ms step_avg:60.11ms
step:870/2315 train_time:52300ms step_avg:60.11ms
step:871/2315 train_time:52361ms step_avg:60.12ms
step:872/2315 train_time:52421ms step_avg:60.12ms
step:873/2315 train_time:52482ms step_avg:60.12ms
step:874/2315 train_time:52542ms step_avg:60.12ms
step:875/2315 train_time:52603ms step_avg:60.12ms
step:876/2315 train_time:52663ms step_avg:60.12ms
step:877/2315 train_time:52725ms step_avg:60.12ms
step:878/2315 train_time:52786ms step_avg:60.12ms
step:879/2315 train_time:52847ms step_avg:60.12ms
step:880/2315 train_time:52908ms step_avg:60.12ms
step:881/2315 train_time:52969ms step_avg:60.12ms
step:882/2315 train_time:53030ms step_avg:60.12ms
step:883/2315 train_time:53091ms step_avg:60.13ms
step:884/2315 train_time:53152ms step_avg:60.13ms
step:885/2315 train_time:53213ms step_avg:60.13ms
step:886/2315 train_time:53274ms step_avg:60.13ms
step:887/2315 train_time:53335ms step_avg:60.13ms
step:888/2315 train_time:53396ms step_avg:60.13ms
step:889/2315 train_time:53457ms step_avg:60.13ms
step:890/2315 train_time:53517ms step_avg:60.13ms
step:891/2315 train_time:53578ms step_avg:60.13ms
step:892/2315 train_time:53639ms step_avg:60.13ms
step:893/2315 train_time:53699ms step_avg:60.13ms
step:894/2315 train_time:53760ms step_avg:60.13ms
step:895/2315 train_time:53821ms step_avg:60.13ms
step:896/2315 train_time:53881ms step_avg:60.13ms
step:897/2315 train_time:53941ms step_avg:60.13ms
step:898/2315 train_time:54001ms step_avg:60.14ms
step:899/2315 train_time:54062ms step_avg:60.14ms
step:900/2315 train_time:54122ms step_avg:60.14ms
step:901/2315 train_time:54183ms step_avg:60.14ms
step:902/2315 train_time:54243ms step_avg:60.14ms
step:903/2315 train_time:54304ms step_avg:60.14ms
step:904/2315 train_time:54365ms step_avg:60.14ms
step:905/2315 train_time:54426ms step_avg:60.14ms
step:906/2315 train_time:54487ms step_avg:60.14ms
step:907/2315 train_time:54548ms step_avg:60.14ms
step:908/2315 train_time:54609ms step_avg:60.14ms
step:909/2315 train_time:54671ms step_avg:60.14ms
step:910/2315 train_time:54731ms step_avg:60.14ms
step:911/2315 train_time:54792ms step_avg:60.15ms
step:912/2315 train_time:54853ms step_avg:60.15ms
step:913/2315 train_time:54915ms step_avg:60.15ms
step:914/2315 train_time:54975ms step_avg:60.15ms
step:915/2315 train_time:55036ms step_avg:60.15ms
step:916/2315 train_time:55097ms step_avg:60.15ms
step:917/2315 train_time:55158ms step_avg:60.15ms
step:918/2315 train_time:55218ms step_avg:60.15ms
step:919/2315 train_time:55279ms step_avg:60.15ms
step:920/2315 train_time:55339ms step_avg:60.15ms
step:921/2315 train_time:55399ms step_avg:60.15ms
step:922/2315 train_time:55460ms step_avg:60.15ms
step:923/2315 train_time:55521ms step_avg:60.15ms
step:924/2315 train_time:55581ms step_avg:60.15ms
step:925/2315 train_time:55642ms step_avg:60.15ms
step:926/2315 train_time:55702ms step_avg:60.15ms
step:927/2315 train_time:55762ms step_avg:60.15ms
step:928/2315 train_time:55822ms step_avg:60.15ms
step:929/2315 train_time:55883ms step_avg:60.15ms
step:930/2315 train_time:55943ms step_avg:60.15ms
step:931/2315 train_time:56004ms step_avg:60.15ms
step:932/2315 train_time:56064ms step_avg:60.15ms
step:933/2315 train_time:56126ms step_avg:60.16ms
step:934/2315 train_time:56186ms step_avg:60.16ms
step:935/2315 train_time:56248ms step_avg:60.16ms
step:936/2315 train_time:56308ms step_avg:60.16ms
step:937/2315 train_time:56369ms step_avg:60.16ms
step:938/2315 train_time:56429ms step_avg:60.16ms
step:939/2315 train_time:56491ms step_avg:60.16ms
step:940/2315 train_time:56551ms step_avg:60.16ms
step:941/2315 train_time:56613ms step_avg:60.16ms
step:942/2315 train_time:56673ms step_avg:60.16ms
step:943/2315 train_time:56734ms step_avg:60.16ms
step:944/2315 train_time:56795ms step_avg:60.16ms
step:945/2315 train_time:56856ms step_avg:60.16ms
step:946/2315 train_time:56916ms step_avg:60.17ms
step:947/2315 train_time:56977ms step_avg:60.17ms
step:948/2315 train_time:57038ms step_avg:60.17ms
step:949/2315 train_time:57099ms step_avg:60.17ms
step:950/2315 train_time:57160ms step_avg:60.17ms
step:951/2315 train_time:57221ms step_avg:60.17ms
step:952/2315 train_time:57281ms step_avg:60.17ms
step:953/2315 train_time:57342ms step_avg:60.17ms
step:954/2315 train_time:57403ms step_avg:60.17ms
step:955/2315 train_time:57462ms step_avg:60.17ms
step:956/2315 train_time:57523ms step_avg:60.17ms
step:957/2315 train_time:57583ms step_avg:60.17ms
step:958/2315 train_time:57644ms step_avg:60.17ms
step:959/2315 train_time:57704ms step_avg:60.17ms
step:960/2315 train_time:57765ms step_avg:60.17ms
step:961/2315 train_time:57826ms step_avg:60.17ms
step:962/2315 train_time:57888ms step_avg:60.17ms
step:963/2315 train_time:57949ms step_avg:60.18ms
step:964/2315 train_time:58010ms step_avg:60.18ms
step:965/2315 train_time:58071ms step_avg:60.18ms
step:966/2315 train_time:58132ms step_avg:60.18ms
step:967/2315 train_time:58193ms step_avg:60.18ms
step:968/2315 train_time:58254ms step_avg:60.18ms
step:969/2315 train_time:58315ms step_avg:60.18ms
step:970/2315 train_time:58375ms step_avg:60.18ms
step:971/2315 train_time:58436ms step_avg:60.18ms
step:972/2315 train_time:58497ms step_avg:60.18ms
step:973/2315 train_time:58558ms step_avg:60.18ms
step:974/2315 train_time:58618ms step_avg:60.18ms
step:975/2315 train_time:58679ms step_avg:60.18ms
step:976/2315 train_time:58739ms step_avg:60.18ms
step:977/2315 train_time:58801ms step_avg:60.18ms
step:978/2315 train_time:58861ms step_avg:60.18ms
step:979/2315 train_time:58921ms step_avg:60.19ms
step:980/2315 train_time:58981ms step_avg:60.19ms
step:981/2315 train_time:59042ms step_avg:60.19ms
step:982/2315 train_time:59102ms step_avg:60.19ms
step:983/2315 train_time:59163ms step_avg:60.19ms
step:984/2315 train_time:59224ms step_avg:60.19ms
step:985/2315 train_time:59285ms step_avg:60.19ms
step:986/2315 train_time:59346ms step_avg:60.19ms
step:987/2315 train_time:59407ms step_avg:60.19ms
step:988/2315 train_time:59468ms step_avg:60.19ms
step:989/2315 train_time:59529ms step_avg:60.19ms
step:990/2315 train_time:59590ms step_avg:60.19ms
step:991/2315 train_time:59652ms step_avg:60.19ms
step:992/2315 train_time:59713ms step_avg:60.19ms
step:993/2315 train_time:59773ms step_avg:60.19ms
step:994/2315 train_time:59834ms step_avg:60.20ms
step:995/2315 train_time:59895ms step_avg:60.20ms
step:996/2315 train_time:59956ms step_avg:60.20ms
step:997/2315 train_time:60017ms step_avg:60.20ms
step:998/2315 train_time:60077ms step_avg:60.20ms
step:999/2315 train_time:60139ms step_avg:60.20ms
step:1000/2315 train_time:60200ms step_avg:60.20ms
step:1000/2315 val_loss:3.5707 train_time:60262ms step_avg:60.26ms
step:1001/2315 train_time:60282ms step_avg:60.22ms
step:1002/2315 train_time:60323ms step_avg:60.20ms
step:1003/2315 train_time:60390ms step_avg:60.21ms
step:1004/2315 train_time:60453ms step_avg:60.21ms
step:1005/2315 train_time:60514ms step_avg:60.21ms
step:1006/2315 train_time:60575ms step_avg:60.21ms
step:1007/2315 train_time:60635ms step_avg:60.21ms
step:1008/2315 train_time:60695ms step_avg:60.21ms
step:1009/2315 train_time:60756ms step_avg:60.21ms
step:1010/2315 train_time:60816ms step_avg:60.21ms
step:1011/2315 train_time:60876ms step_avg:60.21ms
step:1012/2315 train_time:60936ms step_avg:60.21ms
step:1013/2315 train_time:60996ms step_avg:60.21ms
step:1014/2315 train_time:61056ms step_avg:60.21ms
step:1015/2315 train_time:61116ms step_avg:60.21ms
step:1016/2315 train_time:61177ms step_avg:60.21ms
step:1017/2315 train_time:61241ms step_avg:60.22ms
step:1018/2315 train_time:61303ms step_avg:60.22ms
step:1019/2315 train_time:61365ms step_avg:60.22ms
step:1020/2315 train_time:61427ms step_avg:60.22ms
step:1021/2315 train_time:61487ms step_avg:60.22ms
step:1022/2315 train_time:61547ms step_avg:60.22ms
step:1023/2315 train_time:61608ms step_avg:60.22ms
step:1024/2315 train_time:61668ms step_avg:60.22ms
step:1025/2315 train_time:61728ms step_avg:60.22ms
step:1026/2315 train_time:61788ms step_avg:60.22ms
step:1027/2315 train_time:61847ms step_avg:60.22ms
step:1028/2315 train_time:61907ms step_avg:60.22ms
step:1029/2315 train_time:61968ms step_avg:60.22ms
step:1030/2315 train_time:62028ms step_avg:60.22ms
step:1031/2315 train_time:62088ms step_avg:60.22ms
step:1032/2315 train_time:62149ms step_avg:60.22ms
step:1033/2315 train_time:62211ms step_avg:60.22ms
step:1034/2315 train_time:62273ms step_avg:60.23ms
step:1035/2315 train_time:62334ms step_avg:60.23ms
step:1036/2315 train_time:62396ms step_avg:60.23ms
step:1037/2315 train_time:62458ms step_avg:60.23ms
step:1038/2315 train_time:62519ms step_avg:60.23ms
step:1039/2315 train_time:62580ms step_avg:60.23ms
step:1040/2315 train_time:62640ms step_avg:60.23ms
step:1041/2315 train_time:62702ms step_avg:60.23ms
step:1042/2315 train_time:62762ms step_avg:60.23ms
step:1043/2315 train_time:62823ms step_avg:60.23ms
step:1044/2315 train_time:62883ms step_avg:60.23ms
step:1045/2315 train_time:62943ms step_avg:60.23ms
step:1046/2315 train_time:63004ms step_avg:60.23ms
step:1047/2315 train_time:63064ms step_avg:60.23ms
step:1048/2315 train_time:63124ms step_avg:60.23ms
step:1049/2315 train_time:63185ms step_avg:60.23ms
step:1050/2315 train_time:63245ms step_avg:60.23ms
step:1051/2315 train_time:63305ms step_avg:60.23ms
step:1052/2315 train_time:63366ms step_avg:60.23ms
step:1053/2315 train_time:63426ms step_avg:60.23ms
step:1054/2315 train_time:63487ms step_avg:60.23ms
step:1055/2315 train_time:63547ms step_avg:60.23ms
step:1056/2315 train_time:63608ms step_avg:60.23ms
step:1057/2315 train_time:63669ms step_avg:60.24ms
step:1058/2315 train_time:63730ms step_avg:60.24ms
step:1059/2315 train_time:63790ms step_avg:60.24ms
step:1060/2315 train_time:63850ms step_avg:60.24ms
step:1061/2315 train_time:63911ms step_avg:60.24ms
step:1062/2315 train_time:63971ms step_avg:60.24ms
step:1063/2315 train_time:64032ms step_avg:60.24ms
step:1064/2315 train_time:64092ms step_avg:60.24ms
step:1065/2315 train_time:64153ms step_avg:60.24ms
step:1066/2315 train_time:64213ms step_avg:60.24ms
step:1067/2315 train_time:64274ms step_avg:60.24ms
step:1068/2315 train_time:64336ms step_avg:60.24ms
step:1069/2315 train_time:64396ms step_avg:60.24ms
step:1070/2315 train_time:64457ms step_avg:60.24ms
step:1071/2315 train_time:64518ms step_avg:60.24ms
step:1072/2315 train_time:64579ms step_avg:60.24ms
step:1073/2315 train_time:64640ms step_avg:60.24ms
step:1074/2315 train_time:64701ms step_avg:60.24ms
step:1075/2315 train_time:64762ms step_avg:60.24ms
step:1076/2315 train_time:64823ms step_avg:60.24ms
step:1077/2315 train_time:64883ms step_avg:60.24ms
step:1078/2315 train_time:64944ms step_avg:60.24ms
step:1079/2315 train_time:65004ms step_avg:60.24ms
step:1080/2315 train_time:65064ms step_avg:60.24ms
step:1081/2315 train_time:65125ms step_avg:60.24ms
step:1082/2315 train_time:65185ms step_avg:60.25ms
step:1083/2315 train_time:65245ms step_avg:60.24ms
step:1084/2315 train_time:65305ms step_avg:60.24ms
step:1085/2315 train_time:65366ms step_avg:60.24ms
step:1086/2315 train_time:65426ms step_avg:60.24ms
step:1087/2315 train_time:65486ms step_avg:60.24ms
step:1088/2315 train_time:65546ms step_avg:60.24ms
step:1089/2315 train_time:65607ms step_avg:60.25ms
step:1090/2315 train_time:65667ms step_avg:60.25ms
step:1091/2315 train_time:65728ms step_avg:60.25ms
step:1092/2315 train_time:65789ms step_avg:60.25ms
step:1093/2315 train_time:65850ms step_avg:60.25ms
step:1094/2315 train_time:65910ms step_avg:60.25ms
step:1095/2315 train_time:65971ms step_avg:60.25ms
step:1096/2315 train_time:66031ms step_avg:60.25ms
step:1097/2315 train_time:66092ms step_avg:60.25ms
step:1098/2315 train_time:66152ms step_avg:60.25ms
step:1099/2315 train_time:66213ms step_avg:60.25ms
step:1100/2315 train_time:66274ms step_avg:60.25ms
step:1101/2315 train_time:66335ms step_avg:60.25ms
step:1102/2315 train_time:66396ms step_avg:60.25ms
step:1103/2315 train_time:66457ms step_avg:60.25ms
step:1104/2315 train_time:66518ms step_avg:60.25ms
step:1105/2315 train_time:66579ms step_avg:60.25ms
step:1106/2315 train_time:66640ms step_avg:60.25ms
step:1107/2315 train_time:66702ms step_avg:60.25ms
step:1108/2315 train_time:66763ms step_avg:60.26ms
step:1109/2315 train_time:66824ms step_avg:60.26ms
step:1110/2315 train_time:66884ms step_avg:60.26ms
step:1111/2315 train_time:66945ms step_avg:60.26ms
step:1112/2315 train_time:67005ms step_avg:60.26ms
step:1113/2315 train_time:67066ms step_avg:60.26ms
step:1114/2315 train_time:67126ms step_avg:60.26ms
step:1115/2315 train_time:67186ms step_avg:60.26ms
step:1116/2315 train_time:67246ms step_avg:60.26ms
step:1117/2315 train_time:67307ms step_avg:60.26ms
step:1118/2315 train_time:67367ms step_avg:60.26ms
step:1119/2315 train_time:67427ms step_avg:60.26ms
step:1120/2315 train_time:67488ms step_avg:60.26ms
step:1121/2315 train_time:67549ms step_avg:60.26ms
step:1122/2315 train_time:67609ms step_avg:60.26ms
step:1123/2315 train_time:67671ms step_avg:60.26ms
step:1124/2315 train_time:67732ms step_avg:60.26ms
step:1125/2315 train_time:67793ms step_avg:60.26ms
step:1126/2315 train_time:67854ms step_avg:60.26ms
step:1127/2315 train_time:67915ms step_avg:60.26ms
step:1128/2315 train_time:67976ms step_avg:60.26ms
step:1129/2315 train_time:68037ms step_avg:60.26ms
step:1130/2315 train_time:68098ms step_avg:60.26ms
step:1131/2315 train_time:68159ms step_avg:60.26ms
step:1132/2315 train_time:68219ms step_avg:60.26ms
step:1133/2315 train_time:68281ms step_avg:60.27ms
step:1134/2315 train_time:68341ms step_avg:60.27ms
step:1135/2315 train_time:68402ms step_avg:60.27ms
step:1136/2315 train_time:68463ms step_avg:60.27ms
step:1137/2315 train_time:68524ms step_avg:60.27ms
step:1138/2315 train_time:68584ms step_avg:60.27ms
step:1139/2315 train_time:68645ms step_avg:60.27ms
step:1140/2315 train_time:68705ms step_avg:60.27ms
step:1141/2315 train_time:68766ms step_avg:60.27ms
step:1142/2315 train_time:68826ms step_avg:60.27ms
step:1143/2315 train_time:68886ms step_avg:60.27ms
step:1144/2315 train_time:68947ms step_avg:60.27ms
step:1145/2315 train_time:69007ms step_avg:60.27ms
step:1146/2315 train_time:69067ms step_avg:60.27ms
step:1147/2315 train_time:69128ms step_avg:60.27ms
step:1148/2315 train_time:69189ms step_avg:60.27ms
step:1149/2315 train_time:69250ms step_avg:60.27ms
step:1150/2315 train_time:69311ms step_avg:60.27ms
step:1151/2315 train_time:69371ms step_avg:60.27ms
step:1152/2315 train_time:69432ms step_avg:60.27ms
step:1153/2315 train_time:69493ms step_avg:60.27ms
step:1154/2315 train_time:69554ms step_avg:60.27ms
step:1155/2315 train_time:69615ms step_avg:60.27ms
step:1156/2315 train_time:69676ms step_avg:60.27ms
step:1157/2315 train_time:69737ms step_avg:60.27ms
step:1158/2315 train_time:69797ms step_avg:60.27ms
step:1159/2315 train_time:69859ms step_avg:60.28ms
step:1160/2315 train_time:69920ms step_avg:60.28ms
step:1161/2315 train_time:69980ms step_avg:60.28ms
step:1162/2315 train_time:70041ms step_avg:60.28ms
step:1163/2315 train_time:70102ms step_avg:60.28ms
step:1164/2315 train_time:70163ms step_avg:60.28ms
step:1165/2315 train_time:70224ms step_avg:60.28ms
step:1166/2315 train_time:70284ms step_avg:60.28ms
step:1167/2315 train_time:70344ms step_avg:60.28ms
step:1168/2315 train_time:70405ms step_avg:60.28ms
step:1169/2315 train_time:70465ms step_avg:60.28ms
step:1170/2315 train_time:70525ms step_avg:60.28ms
step:1171/2315 train_time:70585ms step_avg:60.28ms
step:1172/2315 train_time:70645ms step_avg:60.28ms
step:1173/2315 train_time:70705ms step_avg:60.28ms
step:1174/2315 train_time:70765ms step_avg:60.28ms
step:1175/2315 train_time:70826ms step_avg:60.28ms
step:1176/2315 train_time:70886ms step_avg:60.28ms
step:1177/2315 train_time:70947ms step_avg:60.28ms
step:1178/2315 train_time:71007ms step_avg:60.28ms
step:1179/2315 train_time:71068ms step_avg:60.28ms
step:1180/2315 train_time:71129ms step_avg:60.28ms
step:1181/2315 train_time:71190ms step_avg:60.28ms
step:1182/2315 train_time:71250ms step_avg:60.28ms
step:1183/2315 train_time:71311ms step_avg:60.28ms
step:1184/2315 train_time:71372ms step_avg:60.28ms
step:1185/2315 train_time:71433ms step_avg:60.28ms
step:1186/2315 train_time:71493ms step_avg:60.28ms
step:1187/2315 train_time:71554ms step_avg:60.28ms
step:1188/2315 train_time:71614ms step_avg:60.28ms
step:1189/2315 train_time:71675ms step_avg:60.28ms
step:1190/2315 train_time:71736ms step_avg:60.28ms
step:1191/2315 train_time:71797ms step_avg:60.28ms
step:1192/2315 train_time:71857ms step_avg:60.28ms
step:1193/2315 train_time:71918ms step_avg:60.28ms
step:1194/2315 train_time:71979ms step_avg:60.28ms
step:1195/2315 train_time:72041ms step_avg:60.29ms
step:1196/2315 train_time:72102ms step_avg:60.29ms
step:1197/2315 train_time:72163ms step_avg:60.29ms
step:1198/2315 train_time:72223ms step_avg:60.29ms
step:1199/2315 train_time:72285ms step_avg:60.29ms
step:1200/2315 train_time:72345ms step_avg:60.29ms
step:1201/2315 train_time:72406ms step_avg:60.29ms
step:1202/2315 train_time:72466ms step_avg:60.29ms
step:1203/2315 train_time:72526ms step_avg:60.29ms
step:1204/2315 train_time:72586ms step_avg:60.29ms
step:1205/2315 train_time:72647ms step_avg:60.29ms
step:1206/2315 train_time:72707ms step_avg:60.29ms
step:1207/2315 train_time:72767ms step_avg:60.29ms
step:1208/2315 train_time:72828ms step_avg:60.29ms
step:1209/2315 train_time:72889ms step_avg:60.29ms
step:1210/2315 train_time:72949ms step_avg:60.29ms
step:1211/2315 train_time:73010ms step_avg:60.29ms
step:1212/2315 train_time:73071ms step_avg:60.29ms
step:1213/2315 train_time:73132ms step_avg:60.29ms
step:1214/2315 train_time:73192ms step_avg:60.29ms
step:1215/2315 train_time:73254ms step_avg:60.29ms
step:1216/2315 train_time:73314ms step_avg:60.29ms
step:1217/2315 train_time:73376ms step_avg:60.29ms
step:1218/2315 train_time:73437ms step_avg:60.29ms
step:1219/2315 train_time:73498ms step_avg:60.29ms
step:1220/2315 train_time:73559ms step_avg:60.29ms
step:1221/2315 train_time:73620ms step_avg:60.29ms
step:1222/2315 train_time:73681ms step_avg:60.30ms
step:1223/2315 train_time:73741ms step_avg:60.30ms
step:1224/2315 train_time:73802ms step_avg:60.30ms
step:1225/2315 train_time:73863ms step_avg:60.30ms
step:1226/2315 train_time:73924ms step_avg:60.30ms
step:1227/2315 train_time:73984ms step_avg:60.30ms
step:1228/2315 train_time:74045ms step_avg:60.30ms
step:1229/2315 train_time:74106ms step_avg:60.30ms
step:1230/2315 train_time:74166ms step_avg:60.30ms
step:1231/2315 train_time:74227ms step_avg:60.30ms
step:1232/2315 train_time:74287ms step_avg:60.30ms
step:1233/2315 train_time:74347ms step_avg:60.30ms
step:1234/2315 train_time:74407ms step_avg:60.30ms
step:1235/2315 train_time:74468ms step_avg:60.30ms
step:1236/2315 train_time:74528ms step_avg:60.30ms
step:1237/2315 train_time:74589ms step_avg:60.30ms
step:1238/2315 train_time:74650ms step_avg:60.30ms
step:1239/2315 train_time:74711ms step_avg:60.30ms
step:1240/2315 train_time:74772ms step_avg:60.30ms
step:1241/2315 train_time:74834ms step_avg:60.30ms
step:1242/2315 train_time:74894ms step_avg:60.30ms
step:1243/2315 train_time:74956ms step_avg:60.30ms
step:1244/2315 train_time:75017ms step_avg:60.30ms
step:1245/2315 train_time:75078ms step_avg:60.30ms
step:1246/2315 train_time:75138ms step_avg:60.30ms
step:1247/2315 train_time:75199ms step_avg:60.30ms
step:1248/2315 train_time:75260ms step_avg:60.30ms
step:1249/2315 train_time:75321ms step_avg:60.31ms
step:1250/2315 train_time:75382ms step_avg:60.31ms
step:1250/2315 val_loss:3.5130 train_time:75444ms step_avg:60.36ms
step:1251/2315 train_time:75464ms step_avg:60.32ms
step:1252/2315 train_time:75505ms step_avg:60.31ms
step:1253/2315 train_time:75572ms step_avg:60.31ms
step:1254/2315 train_time:75635ms step_avg:60.32ms
step:1255/2315 train_time:75696ms step_avg:60.32ms
step:1256/2315 train_time:75758ms step_avg:60.32ms
step:1257/2315 train_time:75818ms step_avg:60.32ms
step:1258/2315 train_time:75878ms step_avg:60.32ms
step:1259/2315 train_time:75938ms step_avg:60.32ms
step:1260/2315 train_time:75998ms step_avg:60.32ms
step:1261/2315 train_time:76058ms step_avg:60.32ms
step:1262/2315 train_time:76117ms step_avg:60.31ms
step:1263/2315 train_time:76177ms step_avg:60.31ms
step:1264/2315 train_time:76236ms step_avg:60.31ms
step:1265/2315 train_time:76296ms step_avg:60.31ms
step:1266/2315 train_time:76356ms step_avg:60.31ms
step:1267/2315 train_time:76418ms step_avg:60.31ms
step:1268/2315 train_time:76479ms step_avg:60.31ms
step:1269/2315 train_time:76541ms step_avg:60.32ms
step:1270/2315 train_time:76603ms step_avg:60.32ms
step:1271/2315 train_time:76663ms step_avg:60.32ms
step:1272/2315 train_time:76725ms step_avg:60.32ms
step:1273/2315 train_time:76786ms step_avg:60.32ms
step:1274/2315 train_time:76847ms step_avg:60.32ms
step:1275/2315 train_time:76908ms step_avg:60.32ms
step:1276/2315 train_time:76969ms step_avg:60.32ms
step:1277/2315 train_time:77029ms step_avg:60.32ms
step:1278/2315 train_time:77089ms step_avg:60.32ms
step:1279/2315 train_time:77150ms step_avg:60.32ms
step:1280/2315 train_time:77211ms step_avg:60.32ms
step:1281/2315 train_time:77272ms step_avg:60.32ms
step:1282/2315 train_time:77332ms step_avg:60.32ms
step:1283/2315 train_time:77393ms step_avg:60.32ms
step:1284/2315 train_time:77454ms step_avg:60.32ms
step:1285/2315 train_time:77515ms step_avg:60.32ms
step:1286/2315 train_time:77577ms step_avg:60.32ms
step:1287/2315 train_time:77639ms step_avg:60.33ms
step:1288/2315 train_time:77699ms step_avg:60.33ms
step:1289/2315 train_time:77760ms step_avg:60.33ms
step:1290/2315 train_time:77820ms step_avg:60.33ms
step:1291/2315 train_time:77880ms step_avg:60.33ms
step:1292/2315 train_time:77940ms step_avg:60.33ms
step:1293/2315 train_time:78001ms step_avg:60.33ms
step:1294/2315 train_time:78061ms step_avg:60.33ms
step:1295/2315 train_time:78123ms step_avg:60.33ms
step:1296/2315 train_time:78183ms step_avg:60.33ms
step:1297/2315 train_time:78244ms step_avg:60.33ms
step:1298/2315 train_time:78305ms step_avg:60.33ms
step:1299/2315 train_time:78366ms step_avg:60.33ms
step:1300/2315 train_time:78428ms step_avg:60.33ms
step:1301/2315 train_time:78489ms step_avg:60.33ms
step:1302/2315 train_time:78550ms step_avg:60.33ms
step:1303/2315 train_time:78611ms step_avg:60.33ms
step:1304/2315 train_time:78672ms step_avg:60.33ms
step:1305/2315 train_time:78733ms step_avg:60.33ms
step:1306/2315 train_time:78794ms step_avg:60.33ms
step:1307/2315 train_time:78855ms step_avg:60.33ms
step:1308/2315 train_time:78916ms step_avg:60.33ms
step:1309/2315 train_time:78977ms step_avg:60.33ms
step:1310/2315 train_time:79037ms step_avg:60.33ms
step:1311/2315 train_time:79098ms step_avg:60.33ms
step:1312/2315 train_time:79158ms step_avg:60.33ms
step:1313/2315 train_time:79219ms step_avg:60.33ms
step:1314/2315 train_time:79279ms step_avg:60.33ms
step:1315/2315 train_time:79339ms step_avg:60.33ms
step:1316/2315 train_time:79399ms step_avg:60.33ms
step:1317/2315 train_time:79460ms step_avg:60.33ms
step:1318/2315 train_time:79520ms step_avg:60.33ms
step:1319/2315 train_time:79581ms step_avg:60.33ms
step:1320/2315 train_time:79643ms step_avg:60.34ms
step:1321/2315 train_time:79704ms step_avg:60.34ms
step:1322/2315 train_time:79765ms step_avg:60.34ms
step:1323/2315 train_time:79827ms step_avg:60.34ms
step:1324/2315 train_time:79888ms step_avg:60.34ms
step:1325/2315 train_time:79949ms step_avg:60.34ms
step:1326/2315 train_time:80010ms step_avg:60.34ms
step:1327/2315 train_time:80071ms step_avg:60.34ms
step:1328/2315 train_time:80131ms step_avg:60.34ms
step:1329/2315 train_time:80192ms step_avg:60.34ms
step:1330/2315 train_time:80253ms step_avg:60.34ms
step:1331/2315 train_time:80314ms step_avg:60.34ms
step:1332/2315 train_time:80375ms step_avg:60.34ms
step:1333/2315 train_time:80436ms step_avg:60.34ms
step:1334/2315 train_time:80496ms step_avg:60.34ms
step:1335/2315 train_time:80557ms step_avg:60.34ms
step:1336/2315 train_time:80618ms step_avg:60.34ms
step:1337/2315 train_time:80679ms step_avg:60.34ms
step:1338/2315 train_time:80739ms step_avg:60.34ms
step:1339/2315 train_time:80800ms step_avg:60.34ms
step:1340/2315 train_time:80859ms step_avg:60.34ms
step:1341/2315 train_time:80921ms step_avg:60.34ms
step:1342/2315 train_time:80981ms step_avg:60.34ms
step:1343/2315 train_time:81043ms step_avg:60.34ms
step:1344/2315 train_time:81103ms step_avg:60.34ms
step:1345/2315 train_time:81164ms step_avg:60.35ms
step:1346/2315 train_time:81225ms step_avg:60.35ms
step:1347/2315 train_time:81286ms step_avg:60.35ms
step:1348/2315 train_time:81346ms step_avg:60.35ms
step:1349/2315 train_time:81408ms step_avg:60.35ms
step:1350/2315 train_time:81468ms step_avg:60.35ms
step:1351/2315 train_time:81530ms step_avg:60.35ms
step:1352/2315 train_time:81591ms step_avg:60.35ms
step:1353/2315 train_time:81652ms step_avg:60.35ms
step:1354/2315 train_time:81713ms step_avg:60.35ms
step:1355/2315 train_time:81774ms step_avg:60.35ms
step:1356/2315 train_time:81835ms step_avg:60.35ms
step:1357/2315 train_time:81896ms step_avg:60.35ms
step:1358/2315 train_time:81956ms step_avg:60.35ms
step:1359/2315 train_time:82016ms step_avg:60.35ms
step:1360/2315 train_time:82077ms step_avg:60.35ms
step:1361/2315 train_time:82137ms step_avg:60.35ms
step:1362/2315 train_time:82198ms step_avg:60.35ms
step:1363/2315 train_time:82258ms step_avg:60.35ms
step:1364/2315 train_time:82319ms step_avg:60.35ms
step:1365/2315 train_time:82379ms step_avg:60.35ms
step:1366/2315 train_time:82439ms step_avg:60.35ms
step:1367/2315 train_time:82500ms step_avg:60.35ms
step:1368/2315 train_time:82560ms step_avg:60.35ms
step:1369/2315 train_time:82620ms step_avg:60.35ms
step:1370/2315 train_time:82680ms step_avg:60.35ms
step:1371/2315 train_time:82741ms step_avg:60.35ms
step:1372/2315 train_time:82802ms step_avg:60.35ms
step:1373/2315 train_time:82864ms step_avg:60.35ms
step:1374/2315 train_time:82926ms step_avg:60.35ms
step:1375/2315 train_time:82986ms step_avg:60.35ms
step:1376/2315 train_time:83047ms step_avg:60.35ms
step:1377/2315 train_time:83108ms step_avg:60.35ms
step:1378/2315 train_time:83169ms step_avg:60.35ms
step:1379/2315 train_time:83230ms step_avg:60.36ms
step:1380/2315 train_time:83291ms step_avg:60.36ms
step:1381/2315 train_time:83351ms step_avg:60.36ms
step:1382/2315 train_time:83412ms step_avg:60.36ms
step:1383/2315 train_time:83473ms step_avg:60.36ms
step:1384/2315 train_time:83534ms step_avg:60.36ms
step:1385/2315 train_time:83595ms step_avg:60.36ms
step:1386/2315 train_time:83656ms step_avg:60.36ms
step:1387/2315 train_time:83716ms step_avg:60.36ms
step:1388/2315 train_time:83777ms step_avg:60.36ms
step:1389/2315 train_time:83838ms step_avg:60.36ms
step:1390/2315 train_time:83899ms step_avg:60.36ms
step:1391/2315 train_time:83959ms step_avg:60.36ms
step:1392/2315 train_time:84019ms step_avg:60.36ms
step:1393/2315 train_time:84080ms step_avg:60.36ms
step:1394/2315 train_time:84140ms step_avg:60.36ms
step:1395/2315 train_time:84202ms step_avg:60.36ms
step:1396/2315 train_time:84263ms step_avg:60.36ms
step:1397/2315 train_time:84324ms step_avg:60.36ms
step:1398/2315 train_time:84385ms step_avg:60.36ms
step:1399/2315 train_time:84445ms step_avg:60.36ms
step:1400/2315 train_time:84506ms step_avg:60.36ms
step:1401/2315 train_time:84568ms step_avg:60.36ms
step:1402/2315 train_time:84629ms step_avg:60.36ms
step:1403/2315 train_time:84690ms step_avg:60.36ms
step:1404/2315 train_time:84751ms step_avg:60.36ms
step:1405/2315 train_time:84812ms step_avg:60.36ms
step:1406/2315 train_time:84873ms step_avg:60.36ms
step:1407/2315 train_time:84934ms step_avg:60.37ms
step:1408/2315 train_time:84994ms step_avg:60.37ms
step:1409/2315 train_time:85055ms step_avg:60.37ms
step:1410/2315 train_time:85115ms step_avg:60.37ms
step:1411/2315 train_time:85177ms step_avg:60.37ms
step:1412/2315 train_time:85237ms step_avg:60.37ms
step:1413/2315 train_time:85298ms step_avg:60.37ms
step:1414/2315 train_time:85358ms step_avg:60.37ms
step:1415/2315 train_time:85419ms step_avg:60.37ms
step:1416/2315 train_time:85479ms step_avg:60.37ms
step:1417/2315 train_time:85539ms step_avg:60.37ms
step:1418/2315 train_time:85600ms step_avg:60.37ms
step:1419/2315 train_time:85660ms step_avg:60.37ms
step:1420/2315 train_time:85720ms step_avg:60.37ms
step:1421/2315 train_time:85782ms step_avg:60.37ms
step:1422/2315 train_time:85843ms step_avg:60.37ms
step:1423/2315 train_time:85905ms step_avg:60.37ms
step:1424/2315 train_time:85966ms step_avg:60.37ms
step:1425/2315 train_time:86028ms step_avg:60.37ms
step:1426/2315 train_time:86089ms step_avg:60.37ms
step:1427/2315 train_time:86150ms step_avg:60.37ms
step:1428/2315 train_time:86210ms step_avg:60.37ms
step:1429/2315 train_time:86271ms step_avg:60.37ms
step:1430/2315 train_time:86332ms step_avg:60.37ms
step:1431/2315 train_time:86394ms step_avg:60.37ms
step:1432/2315 train_time:86454ms step_avg:60.37ms
step:1433/2315 train_time:86515ms step_avg:60.37ms
step:1434/2315 train_time:86575ms step_avg:60.37ms
step:1435/2315 train_time:86637ms step_avg:60.37ms
step:1436/2315 train_time:86697ms step_avg:60.37ms
step:1437/2315 train_time:86758ms step_avg:60.37ms
step:1438/2315 train_time:86818ms step_avg:60.37ms
step:1439/2315 train_time:86879ms step_avg:60.37ms
step:1440/2315 train_time:86939ms step_avg:60.37ms
step:1441/2315 train_time:86999ms step_avg:60.37ms
step:1442/2315 train_time:87059ms step_avg:60.37ms
step:1443/2315 train_time:87120ms step_avg:60.37ms
step:1444/2315 train_time:87180ms step_avg:60.37ms
step:1445/2315 train_time:87241ms step_avg:60.37ms
step:1446/2315 train_time:87302ms step_avg:60.37ms
step:1447/2315 train_time:87363ms step_avg:60.38ms
step:1448/2315 train_time:87424ms step_avg:60.38ms
step:1449/2315 train_time:87485ms step_avg:60.38ms
step:1450/2315 train_time:87546ms step_avg:60.38ms
step:1451/2315 train_time:87607ms step_avg:60.38ms
step:1452/2315 train_time:87667ms step_avg:60.38ms
step:1453/2315 train_time:87729ms step_avg:60.38ms
step:1454/2315 train_time:87790ms step_avg:60.38ms
step:1455/2315 train_time:87851ms step_avg:60.38ms
step:1456/2315 train_time:87912ms step_avg:60.38ms
step:1457/2315 train_time:87973ms step_avg:60.38ms
step:1458/2315 train_time:88034ms step_avg:60.38ms
step:1459/2315 train_time:88094ms step_avg:60.38ms
step:1460/2315 train_time:88155ms step_avg:60.38ms
step:1461/2315 train_time:88216ms step_avg:60.38ms
step:1462/2315 train_time:88276ms step_avg:60.38ms
step:1463/2315 train_time:88337ms step_avg:60.38ms
step:1464/2315 train_time:88398ms step_avg:60.38ms
step:1465/2315 train_time:88459ms step_avg:60.38ms
step:1466/2315 train_time:88519ms step_avg:60.38ms
step:1467/2315 train_time:88580ms step_avg:60.38ms
step:1468/2315 train_time:88640ms step_avg:60.38ms
step:1469/2315 train_time:88700ms step_avg:60.38ms
step:1470/2315 train_time:88761ms step_avg:60.38ms
step:1471/2315 train_time:88823ms step_avg:60.38ms
step:1472/2315 train_time:88883ms step_avg:60.38ms
step:1473/2315 train_time:88944ms step_avg:60.38ms
step:1474/2315 train_time:89005ms step_avg:60.38ms
step:1475/2315 train_time:89066ms step_avg:60.38ms
step:1476/2315 train_time:89127ms step_avg:60.38ms
step:1477/2315 train_time:89189ms step_avg:60.39ms
step:1478/2315 train_time:89249ms step_avg:60.39ms
step:1479/2315 train_time:89310ms step_avg:60.39ms
step:1480/2315 train_time:89371ms step_avg:60.39ms
step:1481/2315 train_time:89433ms step_avg:60.39ms
step:1482/2315 train_time:89493ms step_avg:60.39ms
step:1483/2315 train_time:89554ms step_avg:60.39ms
step:1484/2315 train_time:89615ms step_avg:60.39ms
step:1485/2315 train_time:89675ms step_avg:60.39ms
step:1486/2315 train_time:89736ms step_avg:60.39ms
step:1487/2315 train_time:89797ms step_avg:60.39ms
step:1488/2315 train_time:89858ms step_avg:60.39ms
step:1489/2315 train_time:89919ms step_avg:60.39ms
step:1490/2315 train_time:89979ms step_avg:60.39ms
step:1491/2315 train_time:90040ms step_avg:60.39ms
step:1492/2315 train_time:90100ms step_avg:60.39ms
step:1493/2315 train_time:90161ms step_avg:60.39ms
step:1494/2315 train_time:90222ms step_avg:60.39ms
step:1495/2315 train_time:90284ms step_avg:60.39ms
step:1496/2315 train_time:90345ms step_avg:60.39ms
step:1497/2315 train_time:90407ms step_avg:60.39ms
step:1498/2315 train_time:90468ms step_avg:60.39ms
step:1499/2315 train_time:90529ms step_avg:60.39ms
step:1500/2315 train_time:90590ms step_avg:60.39ms
step:1500/2315 val_loss:3.4494 train_time:90653ms step_avg:60.44ms
step:1501/2315 train_time:90672ms step_avg:60.41ms
step:1502/2315 train_time:90714ms step_avg:60.40ms
step:1503/2315 train_time:90779ms step_avg:60.40ms
step:1504/2315 train_time:90843ms step_avg:60.40ms
step:1505/2315 train_time:90904ms step_avg:60.40ms
step:1506/2315 train_time:90964ms step_avg:60.40ms
step:1507/2315 train_time:91025ms step_avg:60.40ms
step:1508/2315 train_time:91085ms step_avg:60.40ms
step:1509/2315 train_time:91146ms step_avg:60.40ms
step:1510/2315 train_time:91205ms step_avg:60.40ms
step:1511/2315 train_time:91265ms step_avg:60.40ms
step:1512/2315 train_time:91325ms step_avg:60.40ms
step:1513/2315 train_time:91386ms step_avg:60.40ms
step:1514/2315 train_time:91446ms step_avg:60.40ms
step:1515/2315 train_time:91506ms step_avg:60.40ms
step:1516/2315 train_time:91567ms step_avg:60.40ms
step:1517/2315 train_time:91628ms step_avg:60.40ms
step:1518/2315 train_time:91689ms step_avg:60.40ms
step:1519/2315 train_time:91751ms step_avg:60.40ms
step:1520/2315 train_time:91812ms step_avg:60.40ms
step:1521/2315 train_time:91874ms step_avg:60.40ms
step:1522/2315 train_time:91935ms step_avg:60.40ms
step:1523/2315 train_time:91997ms step_avg:60.41ms
step:1524/2315 train_time:92059ms step_avg:60.41ms
step:1525/2315 train_time:92120ms step_avg:60.41ms
step:1526/2315 train_time:92181ms step_avg:60.41ms
step:1527/2315 train_time:92242ms step_avg:60.41ms
step:1528/2315 train_time:92303ms step_avg:60.41ms
step:1529/2315 train_time:92363ms step_avg:60.41ms
step:1530/2315 train_time:92425ms step_avg:60.41ms
step:1531/2315 train_time:92485ms step_avg:60.41ms
step:1532/2315 train_time:92547ms step_avg:60.41ms
step:1533/2315 train_time:92607ms step_avg:60.41ms
step:1534/2315 train_time:92668ms step_avg:60.41ms
step:1535/2315 train_time:92730ms step_avg:60.41ms
step:1536/2315 train_time:92791ms step_avg:60.41ms
step:1537/2315 train_time:92852ms step_avg:60.41ms
step:1538/2315 train_time:92914ms step_avg:60.41ms
step:1539/2315 train_time:92975ms step_avg:60.41ms
step:1540/2315 train_time:93036ms step_avg:60.41ms
step:1541/2315 train_time:93098ms step_avg:60.41ms
step:1542/2315 train_time:93159ms step_avg:60.41ms
step:1543/2315 train_time:93220ms step_avg:60.41ms
step:1544/2315 train_time:93281ms step_avg:60.42ms
step:1545/2315 train_time:93343ms step_avg:60.42ms
step:1546/2315 train_time:93404ms step_avg:60.42ms
step:1547/2315 train_time:93464ms step_avg:60.42ms
step:1548/2315 train_time:93525ms step_avg:60.42ms
step:1549/2315 train_time:93586ms step_avg:60.42ms
step:1550/2315 train_time:93647ms step_avg:60.42ms
step:1551/2315 train_time:93709ms step_avg:60.42ms
step:1552/2315 train_time:93770ms step_avg:60.42ms
step:1553/2315 train_time:93832ms step_avg:60.42ms
step:1554/2315 train_time:93893ms step_avg:60.42ms
step:1555/2315 train_time:93955ms step_avg:60.42ms
step:1556/2315 train_time:94015ms step_avg:60.42ms
step:1557/2315 train_time:94077ms step_avg:60.42ms
step:1558/2315 train_time:94138ms step_avg:60.42ms
step:1559/2315 train_time:94199ms step_avg:60.42ms
step:1560/2315 train_time:94260ms step_avg:60.42ms
step:1561/2315 train_time:94321ms step_avg:60.42ms
step:1562/2315 train_time:94383ms step_avg:60.42ms
step:1563/2315 train_time:94444ms step_avg:60.42ms
step:1564/2315 train_time:94505ms step_avg:60.42ms
step:1565/2315 train_time:94566ms step_avg:60.43ms
step:1566/2315 train_time:94627ms step_avg:60.43ms
step:1567/2315 train_time:94688ms step_avg:60.43ms
step:1568/2315 train_time:94749ms step_avg:60.43ms
step:1569/2315 train_time:94811ms step_avg:60.43ms
step:1570/2315 train_time:94871ms step_avg:60.43ms
step:1571/2315 train_time:94932ms step_avg:60.43ms
step:1572/2315 train_time:94994ms step_avg:60.43ms
step:1573/2315 train_time:95055ms step_avg:60.43ms
step:1574/2315 train_time:95116ms step_avg:60.43ms
step:1575/2315 train_time:95177ms step_avg:60.43ms
step:1576/2315 train_time:95238ms step_avg:60.43ms
step:1577/2315 train_time:95300ms step_avg:60.43ms
step:1578/2315 train_time:95361ms step_avg:60.43ms
step:1579/2315 train_time:95423ms step_avg:60.43ms
step:1580/2315 train_time:95484ms step_avg:60.43ms
step:1581/2315 train_time:95545ms step_avg:60.43ms
step:1582/2315 train_time:95606ms step_avg:60.43ms
step:1583/2315 train_time:95668ms step_avg:60.43ms
step:1584/2315 train_time:95729ms step_avg:60.44ms
step:1585/2315 train_time:95790ms step_avg:60.44ms
step:1586/2315 train_time:95852ms step_avg:60.44ms
step:1587/2315 train_time:95913ms step_avg:60.44ms
step:1588/2315 train_time:95974ms step_avg:60.44ms
step:1589/2315 train_time:96035ms step_avg:60.44ms
step:1590/2315 train_time:96096ms step_avg:60.44ms
step:1591/2315 train_time:96157ms step_avg:60.44ms
step:1592/2315 train_time:96218ms step_avg:60.44ms
step:1593/2315 train_time:96280ms step_avg:60.44ms
step:1594/2315 train_time:96341ms step_avg:60.44ms
step:1595/2315 train_time:96403ms step_avg:60.44ms
step:1596/2315 train_time:96463ms step_avg:60.44ms
step:1597/2315 train_time:96524ms step_avg:60.44ms
step:1598/2315 train_time:96585ms step_avg:60.44ms
step:1599/2315 train_time:96647ms step_avg:60.44ms
step:1600/2315 train_time:96708ms step_avg:60.44ms
step:1601/2315 train_time:96769ms step_avg:60.44ms
step:1602/2315 train_time:96830ms step_avg:60.44ms
step:1603/2315 train_time:96891ms step_avg:60.44ms
step:1604/2315 train_time:96952ms step_avg:60.44ms
step:1605/2315 train_time:97013ms step_avg:60.44ms
step:1606/2315 train_time:97074ms step_avg:60.44ms
step:1607/2315 train_time:97135ms step_avg:60.44ms
step:1608/2315 train_time:97197ms step_avg:60.45ms
step:1609/2315 train_time:97258ms step_avg:60.45ms
step:1610/2315 train_time:97319ms step_avg:60.45ms
step:1611/2315 train_time:97381ms step_avg:60.45ms
step:1612/2315 train_time:97441ms step_avg:60.45ms
step:1613/2315 train_time:97503ms step_avg:60.45ms
step:1614/2315 train_time:97564ms step_avg:60.45ms
step:1615/2315 train_time:97625ms step_avg:60.45ms
step:1616/2315 train_time:97687ms step_avg:60.45ms
step:1617/2315 train_time:97749ms step_avg:60.45ms
step:1618/2315 train_time:97810ms step_avg:60.45ms
step:1619/2315 train_time:97871ms step_avg:60.45ms
step:1620/2315 train_time:97932ms step_avg:60.45ms
step:1621/2315 train_time:97993ms step_avg:60.45ms
step:1622/2315 train_time:98054ms step_avg:60.45ms
step:1623/2315 train_time:98115ms step_avg:60.45ms
step:1624/2315 train_time:98176ms step_avg:60.45ms
step:1625/2315 train_time:98237ms step_avg:60.45ms
step:1626/2315 train_time:98299ms step_avg:60.45ms
step:1627/2315 train_time:98360ms step_avg:60.45ms
step:1628/2315 train_time:98422ms step_avg:60.46ms
step:1629/2315 train_time:98484ms step_avg:60.46ms
step:1630/2315 train_time:98544ms step_avg:60.46ms
step:1631/2315 train_time:98606ms step_avg:60.46ms
step:1632/2315 train_time:98667ms step_avg:60.46ms
step:1633/2315 train_time:98729ms step_avg:60.46ms
step:1634/2315 train_time:98790ms step_avg:60.46ms
step:1635/2315 train_time:98851ms step_avg:60.46ms
step:1636/2315 train_time:98912ms step_avg:60.46ms
step:1637/2315 train_time:98973ms step_avg:60.46ms
step:1638/2315 train_time:99033ms step_avg:60.46ms
step:1639/2315 train_time:99094ms step_avg:60.46ms
step:1640/2315 train_time:99155ms step_avg:60.46ms
step:1641/2315 train_time:99217ms step_avg:60.46ms
step:1642/2315 train_time:99277ms step_avg:60.46ms
step:1643/2315 train_time:99339ms step_avg:60.46ms
step:1644/2315 train_time:99401ms step_avg:60.46ms
step:1645/2315 train_time:99462ms step_avg:60.46ms
step:1646/2315 train_time:99524ms step_avg:60.46ms
step:1647/2315 train_time:99585ms step_avg:60.46ms
step:1648/2315 train_time:99646ms step_avg:60.46ms
step:1649/2315 train_time:99708ms step_avg:60.47ms
step:1650/2315 train_time:99769ms step_avg:60.47ms
step:1651/2315 train_time:99830ms step_avg:60.47ms
step:1652/2315 train_time:99891ms step_avg:60.47ms
step:1653/2315 train_time:99952ms step_avg:60.47ms
step:1654/2315 train_time:100013ms step_avg:60.47ms
step:1655/2315 train_time:100074ms step_avg:60.47ms
step:1656/2315 train_time:100134ms step_avg:60.47ms
step:1657/2315 train_time:100195ms step_avg:60.47ms
step:1658/2315 train_time:100257ms step_avg:60.47ms
step:1659/2315 train_time:100319ms step_avg:60.47ms
step:1660/2315 train_time:100380ms step_avg:60.47ms
step:1661/2315 train_time:100442ms step_avg:60.47ms
step:1662/2315 train_time:100503ms step_avg:60.47ms
step:1663/2315 train_time:100564ms step_avg:60.47ms
step:1664/2315 train_time:100625ms step_avg:60.47ms
step:1665/2315 train_time:100686ms step_avg:60.47ms
step:1666/2315 train_time:100747ms step_avg:60.47ms
step:1667/2315 train_time:100809ms step_avg:60.47ms
step:1668/2315 train_time:100870ms step_avg:60.47ms
step:1669/2315 train_time:100931ms step_avg:60.47ms
step:1670/2315 train_time:100992ms step_avg:60.47ms
step:1671/2315 train_time:101053ms step_avg:60.47ms
step:1672/2315 train_time:101114ms step_avg:60.47ms
step:1673/2315 train_time:101175ms step_avg:60.48ms
step:1674/2315 train_time:101237ms step_avg:60.48ms
step:1675/2315 train_time:101299ms step_avg:60.48ms
step:1676/2315 train_time:101360ms step_avg:60.48ms
step:1677/2315 train_time:101421ms step_avg:60.48ms
step:1678/2315 train_time:101483ms step_avg:60.48ms
step:1679/2315 train_time:101545ms step_avg:60.48ms
step:1680/2315 train_time:101606ms step_avg:60.48ms
step:1681/2315 train_time:101667ms step_avg:60.48ms
step:1682/2315 train_time:101728ms step_avg:60.48ms
step:1683/2315 train_time:101789ms step_avg:60.48ms
step:1684/2315 train_time:101850ms step_avg:60.48ms
step:1685/2315 train_time:101911ms step_avg:60.48ms
step:1686/2315 train_time:101972ms step_avg:60.48ms
step:1687/2315 train_time:102033ms step_avg:60.48ms
step:1688/2315 train_time:102094ms step_avg:60.48ms
step:1689/2315 train_time:102155ms step_avg:60.48ms
step:1690/2315 train_time:102216ms step_avg:60.48ms
step:1691/2315 train_time:102278ms step_avg:60.48ms
step:1692/2315 train_time:102339ms step_avg:60.48ms
step:1693/2315 train_time:102401ms step_avg:60.49ms
step:1694/2315 train_time:102463ms step_avg:60.49ms
step:1695/2315 train_time:102524ms step_avg:60.49ms
step:1696/2315 train_time:102585ms step_avg:60.49ms
step:1697/2315 train_time:102646ms step_avg:60.49ms
step:1698/2315 train_time:102707ms step_avg:60.49ms
step:1699/2315 train_time:102768ms step_avg:60.49ms
step:1700/2315 train_time:102830ms step_avg:60.49ms
step:1701/2315 train_time:102891ms step_avg:60.49ms
step:1702/2315 train_time:102951ms step_avg:60.49ms
step:1703/2315 train_time:103012ms step_avg:60.49ms
step:1704/2315 train_time:103073ms step_avg:60.49ms
step:1705/2315 train_time:103134ms step_avg:60.49ms
step:1706/2315 train_time:103195ms step_avg:60.49ms
step:1707/2315 train_time:103257ms step_avg:60.49ms
step:1708/2315 train_time:103319ms step_avg:60.49ms
step:1709/2315 train_time:103380ms step_avg:60.49ms
step:1710/2315 train_time:103441ms step_avg:60.49ms
step:1711/2315 train_time:103503ms step_avg:60.49ms
step:1712/2315 train_time:103564ms step_avg:60.49ms
step:1713/2315 train_time:103625ms step_avg:60.49ms
step:1714/2315 train_time:103686ms step_avg:60.49ms
step:1715/2315 train_time:103747ms step_avg:60.49ms
step:1716/2315 train_time:103808ms step_avg:60.49ms
step:1717/2315 train_time:103870ms step_avg:60.49ms
step:1718/2315 train_time:103930ms step_avg:60.49ms
step:1719/2315 train_time:103991ms step_avg:60.50ms
step:1720/2315 train_time:104052ms step_avg:60.50ms
step:1721/2315 train_time:104113ms step_avg:60.50ms
step:1722/2315 train_time:104174ms step_avg:60.50ms
step:1723/2315 train_time:104235ms step_avg:60.50ms
step:1724/2315 train_time:104297ms step_avg:60.50ms
step:1725/2315 train_time:104358ms step_avg:60.50ms
step:1726/2315 train_time:104420ms step_avg:60.50ms
step:1727/2315 train_time:104482ms step_avg:60.50ms
step:1728/2315 train_time:104543ms step_avg:60.50ms
step:1729/2315 train_time:104605ms step_avg:60.50ms
step:1730/2315 train_time:104665ms step_avg:60.50ms
step:1731/2315 train_time:104727ms step_avg:60.50ms
step:1732/2315 train_time:104787ms step_avg:60.50ms
step:1733/2315 train_time:104849ms step_avg:60.50ms
step:1734/2315 train_time:104910ms step_avg:60.50ms
step:1735/2315 train_time:104970ms step_avg:60.50ms
step:1736/2315 train_time:105031ms step_avg:60.50ms
step:1737/2315 train_time:105092ms step_avg:60.50ms
step:1738/2315 train_time:105153ms step_avg:60.50ms
step:1739/2315 train_time:105215ms step_avg:60.50ms
step:1740/2315 train_time:105275ms step_avg:60.50ms
step:1741/2315 train_time:105337ms step_avg:60.50ms
step:1742/2315 train_time:105399ms step_avg:60.50ms
step:1743/2315 train_time:105460ms step_avg:60.50ms
step:1744/2315 train_time:105521ms step_avg:60.51ms
step:1745/2315 train_time:105583ms step_avg:60.51ms
step:1746/2315 train_time:105644ms step_avg:60.51ms
step:1747/2315 train_time:105706ms step_avg:60.51ms
step:1748/2315 train_time:105768ms step_avg:60.51ms
step:1749/2315 train_time:105829ms step_avg:60.51ms
step:1750/2315 train_time:105890ms step_avg:60.51ms
step:1750/2315 val_loss:3.3802 train_time:105953ms step_avg:60.54ms
step:1751/2315 train_time:105973ms step_avg:60.52ms
step:1752/2315 train_time:106014ms step_avg:60.51ms
step:1753/2315 train_time:106079ms step_avg:60.51ms
step:1754/2315 train_time:106145ms step_avg:60.52ms
step:1755/2315 train_time:106207ms step_avg:60.52ms
step:1756/2315 train_time:106268ms step_avg:60.52ms
step:1757/2315 train_time:106328ms step_avg:60.52ms
step:1758/2315 train_time:106389ms step_avg:60.52ms
step:1759/2315 train_time:106449ms step_avg:60.52ms
step:1760/2315 train_time:106509ms step_avg:60.52ms
step:1761/2315 train_time:106570ms step_avg:60.52ms
step:1762/2315 train_time:106630ms step_avg:60.52ms
step:1763/2315 train_time:106690ms step_avg:60.52ms
step:1764/2315 train_time:106750ms step_avg:60.52ms
step:1765/2315 train_time:106810ms step_avg:60.52ms
step:1766/2315 train_time:106871ms step_avg:60.52ms
step:1767/2315 train_time:106933ms step_avg:60.52ms
step:1768/2315 train_time:106995ms step_avg:60.52ms
step:1769/2315 train_time:107057ms step_avg:60.52ms
step:1770/2315 train_time:107119ms step_avg:60.52ms
step:1771/2315 train_time:107181ms step_avg:60.52ms
step:1772/2315 train_time:107242ms step_avg:60.52ms
step:1773/2315 train_time:107304ms step_avg:60.52ms
step:1774/2315 train_time:107365ms step_avg:60.52ms
step:1775/2315 train_time:107427ms step_avg:60.52ms
step:1776/2315 train_time:107488ms step_avg:60.52ms
step:1777/2315 train_time:107548ms step_avg:60.52ms
step:1778/2315 train_time:107609ms step_avg:60.52ms
step:1779/2315 train_time:107670ms step_avg:60.52ms
step:1780/2315 train_time:107730ms step_avg:60.52ms
step:1781/2315 train_time:107790ms step_avg:60.52ms
step:1782/2315 train_time:107850ms step_avg:60.52ms
step:1783/2315 train_time:107912ms step_avg:60.52ms
step:1784/2315 train_time:107973ms step_avg:60.52ms
step:1785/2315 train_time:108034ms step_avg:60.52ms
step:1786/2315 train_time:108096ms step_avg:60.52ms
step:1787/2315 train_time:108158ms step_avg:60.52ms
step:1788/2315 train_time:108220ms step_avg:60.53ms
step:1789/2315 train_time:108281ms step_avg:60.53ms
step:1790/2315 train_time:108342ms step_avg:60.53ms
step:1791/2315 train_time:108404ms step_avg:60.53ms
step:1792/2315 train_time:108466ms step_avg:60.53ms
step:1793/2315 train_time:108527ms step_avg:60.53ms
step:1794/2315 train_time:108587ms step_avg:60.53ms
step:1795/2315 train_time:108648ms step_avg:60.53ms
step:1796/2315 train_time:108709ms step_avg:60.53ms
step:1797/2315 train_time:108769ms step_avg:60.53ms
step:1798/2315 train_time:108830ms step_avg:60.53ms
step:1799/2315 train_time:108892ms step_avg:60.53ms
step:1800/2315 train_time:108952ms step_avg:60.53ms
step:1801/2315 train_time:109014ms step_avg:60.53ms
step:1802/2315 train_time:109075ms step_avg:60.53ms
step:1803/2315 train_time:109137ms step_avg:60.53ms
step:1804/2315 train_time:109199ms step_avg:60.53ms
step:1805/2315 train_time:109260ms step_avg:60.53ms
step:1806/2315 train_time:109321ms step_avg:60.53ms
step:1807/2315 train_time:109383ms step_avg:60.53ms
step:1808/2315 train_time:109445ms step_avg:60.53ms
step:1809/2315 train_time:109506ms step_avg:60.53ms
step:1810/2315 train_time:109567ms step_avg:60.53ms
step:1811/2315 train_time:109627ms step_avg:60.53ms
step:1812/2315 train_time:109688ms step_avg:60.53ms
step:1813/2315 train_time:109750ms step_avg:60.53ms
step:1814/2315 train_time:109811ms step_avg:60.54ms
step:1815/2315 train_time:109872ms step_avg:60.54ms
step:1816/2315 train_time:109933ms step_avg:60.54ms
step:1817/2315 train_time:109994ms step_avg:60.54ms
step:1818/2315 train_time:110055ms step_avg:60.54ms
step:1819/2315 train_time:110116ms step_avg:60.54ms
step:1820/2315 train_time:110177ms step_avg:60.54ms
step:1821/2315 train_time:110239ms step_avg:60.54ms
step:1822/2315 train_time:110300ms step_avg:60.54ms
step:1823/2315 train_time:110362ms step_avg:60.54ms
step:1824/2315 train_time:110423ms step_avg:60.54ms
step:1825/2315 train_time:110484ms step_avg:60.54ms
step:1826/2315 train_time:110546ms step_avg:60.54ms
step:1827/2315 train_time:110607ms step_avg:60.54ms
step:1828/2315 train_time:110667ms step_avg:60.54ms
step:1829/2315 train_time:110728ms step_avg:60.54ms
step:1830/2315 train_time:110789ms step_avg:60.54ms
step:1831/2315 train_time:110851ms step_avg:60.54ms
step:1832/2315 train_time:110912ms step_avg:60.54ms
step:1833/2315 train_time:110973ms step_avg:60.54ms
step:1834/2315 train_time:111034ms step_avg:60.54ms
step:1835/2315 train_time:111094ms step_avg:60.54ms
step:1836/2315 train_time:111155ms step_avg:60.54ms
step:1837/2315 train_time:111217ms step_avg:60.54ms
step:1838/2315 train_time:111278ms step_avg:60.54ms
step:1839/2315 train_time:111339ms step_avg:60.54ms
step:1840/2315 train_time:111400ms step_avg:60.54ms
step:1841/2315 train_time:111462ms step_avg:60.54ms
step:1842/2315 train_time:111524ms step_avg:60.54ms
step:1843/2315 train_time:111585ms step_avg:60.55ms
step:1844/2315 train_time:111646ms step_avg:60.55ms
step:1845/2315 train_time:111708ms step_avg:60.55ms
step:1846/2315 train_time:111769ms step_avg:60.55ms
step:1847/2315 train_time:111830ms step_avg:60.55ms
step:1848/2315 train_time:111890ms step_avg:60.55ms
step:1849/2315 train_time:111952ms step_avg:60.55ms
step:1850/2315 train_time:112013ms step_avg:60.55ms
step:1851/2315 train_time:112074ms step_avg:60.55ms
step:1852/2315 train_time:112135ms step_avg:60.55ms
step:1853/2315 train_time:112196ms step_avg:60.55ms
step:1854/2315 train_time:112258ms step_avg:60.55ms
step:1855/2315 train_time:112319ms step_avg:60.55ms
step:1856/2315 train_time:112380ms step_avg:60.55ms
step:1857/2315 train_time:112442ms step_avg:60.55ms
step:1858/2315 train_time:112503ms step_avg:60.55ms
step:1859/2315 train_time:112564ms step_avg:60.55ms
step:1860/2315 train_time:112625ms step_avg:60.55ms
step:1861/2315 train_time:112687ms step_avg:60.55ms
step:1862/2315 train_time:112748ms step_avg:60.55ms
step:1863/2315 train_time:112809ms step_avg:60.55ms
step:1864/2315 train_time:112870ms step_avg:60.55ms
step:1865/2315 train_time:112932ms step_avg:60.55ms
step:1866/2315 train_time:112992ms step_avg:60.55ms
step:1867/2315 train_time:113054ms step_avg:60.55ms
step:1868/2315 train_time:113115ms step_avg:60.55ms
step:1869/2315 train_time:113176ms step_avg:60.55ms
step:1870/2315 train_time:113236ms step_avg:60.55ms
step:1871/2315 train_time:113298ms step_avg:60.55ms
step:1872/2315 train_time:113359ms step_avg:60.55ms
step:1873/2315 train_time:113421ms step_avg:60.56ms
step:1874/2315 train_time:113482ms step_avg:60.56ms
step:1875/2315 train_time:113543ms step_avg:60.56ms
step:1876/2315 train_time:113604ms step_avg:60.56ms
step:1877/2315 train_time:113665ms step_avg:60.56ms
step:1878/2315 train_time:113726ms step_avg:60.56ms
step:1879/2315 train_time:113788ms step_avg:60.56ms
step:1880/2315 train_time:113849ms step_avg:60.56ms
step:1881/2315 train_time:113910ms step_avg:60.56ms
step:1882/2315 train_time:113971ms step_avg:60.56ms
step:1883/2315 train_time:114032ms step_avg:60.56ms
step:1884/2315 train_time:114094ms step_avg:60.56ms
step:1885/2315 train_time:114155ms step_avg:60.56ms
step:1886/2315 train_time:114216ms step_avg:60.56ms
step:1887/2315 train_time:114276ms step_avg:60.56ms
step:1888/2315 train_time:114337ms step_avg:60.56ms
step:1889/2315 train_time:114399ms step_avg:60.56ms
step:1890/2315 train_time:114461ms step_avg:60.56ms
step:1891/2315 train_time:114523ms step_avg:60.56ms
step:1892/2315 train_time:114584ms step_avg:60.56ms
step:1893/2315 train_time:114645ms step_avg:60.56ms
step:1894/2315 train_time:114707ms step_avg:60.56ms
step:1895/2315 train_time:114768ms step_avg:60.56ms
step:1896/2315 train_time:114829ms step_avg:60.56ms
step:1897/2315 train_time:114891ms step_avg:60.56ms
step:1898/2315 train_time:114952ms step_avg:60.56ms
step:1899/2315 train_time:115013ms step_avg:60.57ms
step:1900/2315 train_time:115074ms step_avg:60.57ms
step:1901/2315 train_time:115135ms step_avg:60.57ms
step:1902/2315 train_time:115197ms step_avg:60.57ms
step:1903/2315 train_time:115257ms step_avg:60.57ms
step:1904/2315 train_time:115318ms step_avg:60.57ms
step:1905/2315 train_time:115379ms step_avg:60.57ms
step:1906/2315 train_time:115440ms step_avg:60.57ms
step:1907/2315 train_time:115502ms step_avg:60.57ms
step:1908/2315 train_time:115563ms step_avg:60.57ms
step:1909/2315 train_time:115625ms step_avg:60.57ms
step:1910/2315 train_time:115686ms step_avg:60.57ms
step:1911/2315 train_time:115747ms step_avg:60.57ms
step:1912/2315 train_time:115808ms step_avg:60.57ms
step:1913/2315 train_time:115870ms step_avg:60.57ms
step:1914/2315 train_time:115930ms step_avg:60.57ms
step:1915/2315 train_time:115992ms step_avg:60.57ms
step:1916/2315 train_time:116053ms step_avg:60.57ms
step:1917/2315 train_time:116114ms step_avg:60.57ms
step:1918/2315 train_time:116174ms step_avg:60.57ms
step:1919/2315 train_time:116235ms step_avg:60.57ms
step:1920/2315 train_time:116296ms step_avg:60.57ms
step:1921/2315 train_time:116357ms step_avg:60.57ms
step:1922/2315 train_time:116418ms step_avg:60.57ms
step:1923/2315 train_time:116479ms step_avg:60.57ms
step:1924/2315 train_time:116540ms step_avg:60.57ms
step:1925/2315 train_time:116602ms step_avg:60.57ms
step:1926/2315 train_time:116663ms step_avg:60.57ms
step:1927/2315 train_time:116725ms step_avg:60.57ms
step:1928/2315 train_time:116786ms step_avg:60.57ms
step:1929/2315 train_time:116848ms step_avg:60.57ms
step:1930/2315 train_time:116909ms step_avg:60.57ms
step:1931/2315 train_time:116970ms step_avg:60.57ms
step:1932/2315 train_time:117031ms step_avg:60.58ms
step:1933/2315 train_time:117093ms step_avg:60.58ms
step:1934/2315 train_time:117153ms step_avg:60.58ms
step:1935/2315 train_time:117214ms step_avg:60.58ms
step:1936/2315 train_time:117276ms step_avg:60.58ms
step:1937/2315 train_time:117336ms step_avg:60.58ms
step:1938/2315 train_time:117398ms step_avg:60.58ms
step:1939/2315 train_time:117459ms step_avg:60.58ms
step:1940/2315 train_time:117520ms step_avg:60.58ms
step:1941/2315 train_time:117581ms step_avg:60.58ms
step:1942/2315 train_time:117642ms step_avg:60.58ms
step:1943/2315 train_time:117704ms step_avg:60.58ms
step:1944/2315 train_time:117765ms step_avg:60.58ms
step:1945/2315 train_time:117827ms step_avg:60.58ms
step:1946/2315 train_time:117889ms step_avg:60.58ms
step:1947/2315 train_time:117950ms step_avg:60.58ms
step:1948/2315 train_time:118011ms step_avg:60.58ms
step:1949/2315 train_time:118072ms step_avg:60.58ms
step:1950/2315 train_time:118133ms step_avg:60.58ms
step:1951/2315 train_time:118194ms step_avg:60.58ms
step:1952/2315 train_time:118255ms step_avg:60.58ms
step:1953/2315 train_time:118315ms step_avg:60.58ms
step:1954/2315 train_time:118376ms step_avg:60.58ms
step:1955/2315 train_time:118437ms step_avg:60.58ms
step:1956/2315 train_time:118498ms step_avg:60.58ms
step:1957/2315 train_time:118559ms step_avg:60.58ms
step:1958/2315 train_time:118621ms step_avg:60.58ms
step:1959/2315 train_time:118684ms step_avg:60.58ms
step:1960/2315 train_time:118744ms step_avg:60.58ms
step:1961/2315 train_time:118807ms step_avg:60.58ms
step:1962/2315 train_time:118868ms step_avg:60.58ms
step:1963/2315 train_time:118930ms step_avg:60.59ms
step:1964/2315 train_time:118992ms step_avg:60.59ms
step:1965/2315 train_time:119052ms step_avg:60.59ms
step:1966/2315 train_time:119113ms step_avg:60.59ms
step:1967/2315 train_time:119174ms step_avg:60.59ms
step:1968/2315 train_time:119235ms step_avg:60.59ms
step:1969/2315 train_time:119296ms step_avg:60.59ms
step:1970/2315 train_time:119357ms step_avg:60.59ms
step:1971/2315 train_time:119418ms step_avg:60.59ms
step:1972/2315 train_time:119479ms step_avg:60.59ms
step:1973/2315 train_time:119541ms step_avg:60.59ms
step:1974/2315 train_time:119602ms step_avg:60.59ms
step:1975/2315 train_time:119663ms step_avg:60.59ms
step:1976/2315 train_time:119724ms step_avg:60.59ms
step:1977/2315 train_time:119786ms step_avg:60.59ms
step:1978/2315 train_time:119847ms step_avg:60.59ms
step:1979/2315 train_time:119909ms step_avg:60.59ms
step:1980/2315 train_time:119970ms step_avg:60.59ms
step:1981/2315 train_time:120032ms step_avg:60.59ms
step:1982/2315 train_time:120093ms step_avg:60.59ms
step:1983/2315 train_time:120154ms step_avg:60.59ms
step:1984/2315 train_time:120215ms step_avg:60.59ms
step:1985/2315 train_time:120276ms step_avg:60.59ms
step:1986/2315 train_time:120337ms step_avg:60.59ms
step:1987/2315 train_time:120398ms step_avg:60.59ms
step:1988/2315 train_time:120459ms step_avg:60.59ms
step:1989/2315 train_time:120520ms step_avg:60.59ms
step:1990/2315 train_time:120581ms step_avg:60.59ms
step:1991/2315 train_time:120642ms step_avg:60.59ms
step:1992/2315 train_time:120704ms step_avg:60.59ms
step:1993/2315 train_time:120765ms step_avg:60.59ms
step:1994/2315 train_time:120827ms step_avg:60.60ms
step:1995/2315 train_time:120889ms step_avg:60.60ms
step:1996/2315 train_time:120950ms step_avg:60.60ms
step:1997/2315 train_time:121012ms step_avg:60.60ms
step:1998/2315 train_time:121073ms step_avg:60.60ms
step:1999/2315 train_time:121134ms step_avg:60.60ms
step:2000/2315 train_time:121195ms step_avg:60.60ms
step:2000/2315 val_loss:3.3301 train_time:121258ms step_avg:60.63ms
step:2001/2315 train_time:121277ms step_avg:60.61ms
step:2002/2315 train_time:121320ms step_avg:60.60ms
step:2003/2315 train_time:121385ms step_avg:60.60ms
step:2004/2315 train_time:121449ms step_avg:60.60ms
step:2005/2315 train_time:121511ms step_avg:60.60ms
step:2006/2315 train_time:121572ms step_avg:60.60ms
step:2007/2315 train_time:121634ms step_avg:60.60ms
step:2008/2315 train_time:121695ms step_avg:60.60ms
step:2009/2315 train_time:121756ms step_avg:60.61ms
step:2010/2315 train_time:121817ms step_avg:60.61ms
step:2011/2315 train_time:121878ms step_avg:60.61ms
step:2012/2315 train_time:121938ms step_avg:60.61ms
step:2013/2315 train_time:121998ms step_avg:60.61ms
step:2014/2315 train_time:122059ms step_avg:60.61ms
step:2015/2315 train_time:122120ms step_avg:60.61ms
step:2016/2315 train_time:122181ms step_avg:60.61ms
step:2017/2315 train_time:122242ms step_avg:60.61ms
step:2018/2315 train_time:122304ms step_avg:60.61ms
step:2019/2315 train_time:122366ms step_avg:60.61ms
step:2020/2315 train_time:122427ms step_avg:60.61ms
step:2021/2315 train_time:122489ms step_avg:60.61ms
step:2022/2315 train_time:122550ms step_avg:60.61ms
step:2023/2315 train_time:122612ms step_avg:60.61ms
step:2024/2315 train_time:122674ms step_avg:60.61ms
step:2025/2315 train_time:122736ms step_avg:60.61ms
step:2026/2315 train_time:122796ms step_avg:60.61ms
step:2027/2315 train_time:122857ms step_avg:60.61ms
step:2028/2315 train_time:122918ms step_avg:60.61ms
step:2029/2315 train_time:122978ms step_avg:60.61ms
step:2030/2315 train_time:123039ms step_avg:60.61ms
step:2031/2315 train_time:123100ms step_avg:60.61ms
step:2032/2315 train_time:123160ms step_avg:60.61ms
step:2033/2315 train_time:123222ms step_avg:60.61ms
step:2034/2315 train_time:123284ms step_avg:60.61ms
step:2035/2315 train_time:123345ms step_avg:60.61ms
step:2036/2315 train_time:123407ms step_avg:60.61ms
step:2037/2315 train_time:123468ms step_avg:60.61ms
step:2038/2315 train_time:123529ms step_avg:60.61ms
step:2039/2315 train_time:123591ms step_avg:60.61ms
step:2040/2315 train_time:123652ms step_avg:60.61ms
step:2041/2315 train_time:123714ms step_avg:60.61ms
step:2042/2315 train_time:123775ms step_avg:60.61ms
step:2043/2315 train_time:123837ms step_avg:60.62ms
step:2044/2315 train_time:123898ms step_avg:60.62ms
step:2045/2315 train_time:123959ms step_avg:60.62ms
step:2046/2315 train_time:124020ms step_avg:60.62ms
step:2047/2315 train_time:124080ms step_avg:60.62ms
step:2048/2315 train_time:124141ms step_avg:60.62ms
step:2049/2315 train_time:124202ms step_avg:60.62ms
step:2050/2315 train_time:124264ms step_avg:60.62ms
step:2051/2315 train_time:124325ms step_avg:60.62ms
step:2052/2315 train_time:124386ms step_avg:60.62ms
step:2053/2315 train_time:124447ms step_avg:60.62ms
step:2054/2315 train_time:124508ms step_avg:60.62ms
step:2055/2315 train_time:124569ms step_avg:60.62ms
step:2056/2315 train_time:124631ms step_avg:60.62ms
step:2057/2315 train_time:124693ms step_avg:60.62ms
step:2058/2315 train_time:124754ms step_avg:60.62ms
step:2059/2315 train_time:124815ms step_avg:60.62ms
step:2060/2315 train_time:124876ms step_avg:60.62ms
step:2061/2315 train_time:124937ms step_avg:60.62ms
step:2062/2315 train_time:124998ms step_avg:60.62ms
step:2063/2315 train_time:125059ms step_avg:60.62ms
step:2064/2315 train_time:125120ms step_avg:60.62ms
step:2065/2315 train_time:125181ms step_avg:60.62ms
step:2066/2315 train_time:125242ms step_avg:60.62ms
step:2067/2315 train_time:125303ms step_avg:60.62ms
step:2068/2315 train_time:125364ms step_avg:60.62ms
step:2069/2315 train_time:125425ms step_avg:60.62ms
step:2070/2315 train_time:125487ms step_avg:60.62ms
step:2071/2315 train_time:125547ms step_avg:60.62ms
step:2072/2315 train_time:125609ms step_avg:60.62ms
step:2073/2315 train_time:125671ms step_avg:60.62ms
step:2074/2315 train_time:125732ms step_avg:60.62ms
step:2075/2315 train_time:125794ms step_avg:60.62ms
step:2076/2315 train_time:125855ms step_avg:60.62ms
step:2077/2315 train_time:125917ms step_avg:60.62ms
step:2078/2315 train_time:125978ms step_avg:60.62ms
step:2079/2315 train_time:126039ms step_avg:60.62ms
step:2080/2315 train_time:126100ms step_avg:60.63ms
step:2081/2315 train_time:126162ms step_avg:60.63ms
step:2082/2315 train_time:126223ms step_avg:60.63ms
step:2083/2315 train_time:126284ms step_avg:60.63ms
step:2084/2315 train_time:126345ms step_avg:60.63ms
step:2085/2315 train_time:126406ms step_avg:60.63ms
step:2086/2315 train_time:126467ms step_avg:60.63ms
step:2087/2315 train_time:126528ms step_avg:60.63ms
step:2088/2315 train_time:126589ms step_avg:60.63ms
step:2089/2315 train_time:126650ms step_avg:60.63ms
step:2090/2315 train_time:126712ms step_avg:60.63ms
step:2091/2315 train_time:126773ms step_avg:60.63ms
step:2092/2315 train_time:126834ms step_avg:60.63ms
step:2093/2315 train_time:126895ms step_avg:60.63ms
step:2094/2315 train_time:126957ms step_avg:60.63ms
step:2095/2315 train_time:127018ms step_avg:60.63ms
step:2096/2315 train_time:127079ms step_avg:60.63ms
step:2097/2315 train_time:127140ms step_avg:60.63ms
step:2098/2315 train_time:127201ms step_avg:60.63ms
step:2099/2315 train_time:127262ms step_avg:60.63ms
step:2100/2315 train_time:127323ms step_avg:60.63ms
step:2101/2315 train_time:127384ms step_avg:60.63ms
step:2102/2315 train_time:127445ms step_avg:60.63ms
step:2103/2315 train_time:127506ms step_avg:60.63ms
step:2104/2315 train_time:127567ms step_avg:60.63ms
step:2105/2315 train_time:127628ms step_avg:60.63ms
step:2106/2315 train_time:127690ms step_avg:60.63ms
step:2107/2315 train_time:127752ms step_avg:60.63ms
step:2108/2315 train_time:127813ms step_avg:60.63ms
step:2109/2315 train_time:127875ms step_avg:60.63ms
step:2110/2315 train_time:127936ms step_avg:60.63ms
step:2111/2315 train_time:127998ms step_avg:60.63ms
step:2112/2315 train_time:128059ms step_avg:60.63ms
step:2113/2315 train_time:128120ms step_avg:60.63ms
step:2114/2315 train_time:128181ms step_avg:60.63ms
step:2115/2315 train_time:128243ms step_avg:60.63ms
step:2116/2315 train_time:128303ms step_avg:60.63ms
step:2117/2315 train_time:128364ms step_avg:60.63ms
step:2118/2315 train_time:128425ms step_avg:60.64ms
step:2119/2315 train_time:128486ms step_avg:60.64ms
step:2120/2315 train_time:128547ms step_avg:60.64ms
step:2121/2315 train_time:128608ms step_avg:60.64ms
step:2122/2315 train_time:128669ms step_avg:60.64ms
step:2123/2315 train_time:128730ms step_avg:60.64ms
step:2124/2315 train_time:128792ms step_avg:60.64ms
step:2125/2315 train_time:128854ms step_avg:60.64ms
step:2126/2315 train_time:128914ms step_avg:60.64ms
step:2127/2315 train_time:128976ms step_avg:60.64ms
step:2128/2315 train_time:129037ms step_avg:60.64ms
step:2129/2315 train_time:129099ms step_avg:60.64ms
step:2130/2315 train_time:129160ms step_avg:60.64ms
step:2131/2315 train_time:129222ms step_avg:60.64ms
step:2132/2315 train_time:129283ms step_avg:60.64ms
step:2133/2315 train_time:129344ms step_avg:60.64ms
step:2134/2315 train_time:129405ms step_avg:60.64ms
step:2135/2315 train_time:129466ms step_avg:60.64ms
step:2136/2315 train_time:129526ms step_avg:60.64ms
step:2137/2315 train_time:129588ms step_avg:60.64ms
step:2138/2315 train_time:129649ms step_avg:60.64ms
step:2139/2315 train_time:129711ms step_avg:60.64ms
step:2140/2315 train_time:129772ms step_avg:60.64ms
step:2141/2315 train_time:129833ms step_avg:60.64ms
step:2142/2315 train_time:129895ms step_avg:60.64ms
step:2143/2315 train_time:129956ms step_avg:60.64ms
step:2144/2315 train_time:130017ms step_avg:60.64ms
step:2145/2315 train_time:130079ms step_avg:60.64ms
step:2146/2315 train_time:130139ms step_avg:60.64ms
step:2147/2315 train_time:130201ms step_avg:60.64ms
step:2148/2315 train_time:130261ms step_avg:60.64ms
step:2149/2315 train_time:130323ms step_avg:60.64ms
step:2150/2315 train_time:130385ms step_avg:60.64ms
step:2151/2315 train_time:130446ms step_avg:60.64ms
step:2152/2315 train_time:130507ms step_avg:60.64ms
step:2153/2315 train_time:130568ms step_avg:60.64ms
step:2154/2315 train_time:130629ms step_avg:60.64ms
step:2155/2315 train_time:130690ms step_avg:60.64ms
step:2156/2315 train_time:130751ms step_avg:60.65ms
step:2157/2315 train_time:130812ms step_avg:60.65ms
step:2158/2315 train_time:130873ms step_avg:60.65ms
step:2159/2315 train_time:130935ms step_avg:60.65ms
step:2160/2315 train_time:130997ms step_avg:60.65ms
step:2161/2315 train_time:131059ms step_avg:60.65ms
step:2162/2315 train_time:131119ms step_avg:60.65ms
step:2163/2315 train_time:131181ms step_avg:60.65ms
step:2164/2315 train_time:131242ms step_avg:60.65ms
step:2165/2315 train_time:131303ms step_avg:60.65ms
step:2166/2315 train_time:131365ms step_avg:60.65ms
step:2167/2315 train_time:131426ms step_avg:60.65ms
step:2168/2315 train_time:131487ms step_avg:60.65ms
step:2169/2315 train_time:131548ms step_avg:60.65ms
step:2170/2315 train_time:131609ms step_avg:60.65ms
step:2171/2315 train_time:131670ms step_avg:60.65ms
step:2172/2315 train_time:131731ms step_avg:60.65ms
step:2173/2315 train_time:131792ms step_avg:60.65ms
step:2174/2315 train_time:131853ms step_avg:60.65ms
step:2175/2315 train_time:131915ms step_avg:60.65ms
step:2176/2315 train_time:131976ms step_avg:60.65ms
step:2177/2315 train_time:132037ms step_avg:60.65ms
step:2178/2315 train_time:132099ms step_avg:60.65ms
step:2179/2315 train_time:132161ms step_avg:60.65ms
step:2180/2315 train_time:132222ms step_avg:60.65ms
step:2181/2315 train_time:132283ms step_avg:60.65ms
step:2182/2315 train_time:132344ms step_avg:60.65ms
step:2183/2315 train_time:132405ms step_avg:60.65ms
step:2184/2315 train_time:132466ms step_avg:60.65ms
step:2185/2315 train_time:132526ms step_avg:60.65ms
step:2186/2315 train_time:132587ms step_avg:60.65ms
step:2187/2315 train_time:132648ms step_avg:60.65ms
step:2188/2315 train_time:132709ms step_avg:60.65ms
step:2189/2315 train_time:132771ms step_avg:60.65ms
step:2190/2315 train_time:132832ms step_avg:60.65ms
step:2191/2315 train_time:132894ms step_avg:60.65ms
step:2192/2315 train_time:132955ms step_avg:60.65ms
step:2193/2315 train_time:133016ms step_avg:60.65ms
step:2194/2315 train_time:133077ms step_avg:60.65ms
step:2195/2315 train_time:133139ms step_avg:60.66ms
step:2196/2315 train_time:133200ms step_avg:60.66ms
step:2197/2315 train_time:133261ms step_avg:60.66ms
step:2198/2315 train_time:133322ms step_avg:60.66ms
step:2199/2315 train_time:133383ms step_avg:60.66ms
step:2200/2315 train_time:133444ms step_avg:60.66ms
step:2201/2315 train_time:133506ms step_avg:60.66ms
step:2202/2315 train_time:133567ms step_avg:60.66ms
step:2203/2315 train_time:133628ms step_avg:60.66ms
step:2204/2315 train_time:133689ms step_avg:60.66ms
step:2205/2315 train_time:133750ms step_avg:60.66ms
step:2206/2315 train_time:133811ms step_avg:60.66ms
step:2207/2315 train_time:133873ms step_avg:60.66ms
step:2208/2315 train_time:133934ms step_avg:60.66ms
step:2209/2315 train_time:133995ms step_avg:60.66ms
step:2210/2315 train_time:134056ms step_avg:60.66ms
step:2211/2315 train_time:134118ms step_avg:60.66ms
step:2212/2315 train_time:134179ms step_avg:60.66ms
step:2213/2315 train_time:134241ms step_avg:60.66ms
step:2214/2315 train_time:134301ms step_avg:60.66ms
step:2215/2315 train_time:134363ms step_avg:60.66ms
step:2216/2315 train_time:134424ms step_avg:60.66ms
step:2217/2315 train_time:134485ms step_avg:60.66ms
step:2218/2315 train_time:134546ms step_avg:60.66ms
step:2219/2315 train_time:134607ms step_avg:60.66ms
step:2220/2315 train_time:134668ms step_avg:60.66ms
step:2221/2315 train_time:134730ms step_avg:60.66ms
step:2222/2315 train_time:134791ms step_avg:60.66ms
step:2223/2315 train_time:134852ms step_avg:60.66ms
step:2224/2315 train_time:134913ms step_avg:60.66ms
step:2225/2315 train_time:134975ms step_avg:60.66ms
step:2226/2315 train_time:135036ms step_avg:60.66ms
step:2227/2315 train_time:135098ms step_avg:60.66ms
step:2228/2315 train_time:135159ms step_avg:60.66ms
step:2229/2315 train_time:135220ms step_avg:60.66ms
step:2230/2315 train_time:135281ms step_avg:60.66ms
step:2231/2315 train_time:135342ms step_avg:60.66ms
step:2232/2315 train_time:135403ms step_avg:60.66ms
step:2233/2315 train_time:135464ms step_avg:60.66ms
step:2234/2315 train_time:135525ms step_avg:60.66ms
step:2235/2315 train_time:135586ms step_avg:60.66ms
step:2236/2315 train_time:135647ms step_avg:60.66ms
step:2237/2315 train_time:135708ms step_avg:60.66ms
step:2238/2315 train_time:135768ms step_avg:60.67ms
step:2239/2315 train_time:135830ms step_avg:60.67ms
step:2240/2315 train_time:135892ms step_avg:60.67ms
step:2241/2315 train_time:135954ms step_avg:60.67ms
step:2242/2315 train_time:136015ms step_avg:60.67ms
step:2243/2315 train_time:136077ms step_avg:60.67ms
step:2244/2315 train_time:136138ms step_avg:60.67ms
step:2245/2315 train_time:136199ms step_avg:60.67ms
step:2246/2315 train_time:136260ms step_avg:60.67ms
step:2247/2315 train_time:136322ms step_avg:60.67ms
step:2248/2315 train_time:136383ms step_avg:60.67ms
step:2249/2315 train_time:136444ms step_avg:60.67ms
step:2250/2315 train_time:136505ms step_avg:60.67ms
step:2250/2315 val_loss:3.2903 train_time:136568ms step_avg:60.70ms
step:2251/2315 train_time:136589ms step_avg:60.68ms
step:2252/2315 train_time:136629ms step_avg:60.67ms
step:2253/2315 train_time:136694ms step_avg:60.67ms
step:2254/2315 train_time:136759ms step_avg:60.67ms
step:2255/2315 train_time:136823ms step_avg:60.68ms
step:2256/2315 train_time:136885ms step_avg:60.68ms
step:2257/2315 train_time:136947ms step_avg:60.68ms
step:2258/2315 train_time:137008ms step_avg:60.68ms
step:2259/2315 train_time:137069ms step_avg:60.68ms
step:2260/2315 train_time:137129ms step_avg:60.68ms
step:2261/2315 train_time:137190ms step_avg:60.68ms
step:2262/2315 train_time:137250ms step_avg:60.68ms
step:2263/2315 train_time:137311ms step_avg:60.68ms
step:2264/2315 train_time:137371ms step_avg:60.68ms
step:2265/2315 train_time:137431ms step_avg:60.68ms
step:2266/2315 train_time:137492ms step_avg:60.68ms
step:2267/2315 train_time:137554ms step_avg:60.68ms
step:2268/2315 train_time:137616ms step_avg:60.68ms
step:2269/2315 train_time:137678ms step_avg:60.68ms
step:2270/2315 train_time:137740ms step_avg:60.68ms
step:2271/2315 train_time:137802ms step_avg:60.68ms
step:2272/2315 train_time:137864ms step_avg:60.68ms
step:2273/2315 train_time:137925ms step_avg:60.68ms
step:2274/2315 train_time:137986ms step_avg:60.68ms
step:2275/2315 train_time:138048ms step_avg:60.68ms
step:2276/2315 train_time:138108ms step_avg:60.68ms
step:2277/2315 train_time:138169ms step_avg:60.68ms
step:2278/2315 train_time:138230ms step_avg:60.68ms
step:2279/2315 train_time:138290ms step_avg:60.68ms
step:2280/2315 train_time:138351ms step_avg:60.68ms
step:2281/2315 train_time:138411ms step_avg:60.68ms
step:2282/2315 train_time:138471ms step_avg:60.68ms
step:2283/2315 train_time:138532ms step_avg:60.68ms
step:2284/2315 train_time:138593ms step_avg:60.68ms
step:2285/2315 train_time:138654ms step_avg:60.68ms
step:2286/2315 train_time:138717ms step_avg:60.68ms
step:2287/2315 train_time:138779ms step_avg:60.68ms
step:2288/2315 train_time:138840ms step_avg:60.68ms
step:2289/2315 train_time:138902ms step_avg:60.68ms
step:2290/2315 train_time:138963ms step_avg:60.68ms
step:2291/2315 train_time:139025ms step_avg:60.68ms
step:2292/2315 train_time:139085ms step_avg:60.68ms
step:2293/2315 train_time:139147ms step_avg:60.68ms
step:2294/2315 train_time:139208ms step_avg:60.68ms
step:2295/2315 train_time:139269ms step_avg:60.68ms
step:2296/2315 train_time:139330ms step_avg:60.68ms
step:2297/2315 train_time:139391ms step_avg:60.68ms
step:2298/2315 train_time:139452ms step_avg:60.68ms
step:2299/2315 train_time:139513ms step_avg:60.68ms
step:2300/2315 train_time:139574ms step_avg:60.68ms
step:2301/2315 train_time:139635ms step_avg:60.68ms
step:2302/2315 train_time:139696ms step_avg:60.68ms
step:2303/2315 train_time:139758ms step_avg:60.69ms
step:2304/2315 train_time:139820ms step_avg:60.69ms
step:2305/2315 train_time:139883ms step_avg:60.69ms
step:2306/2315 train_time:139943ms step_avg:60.69ms
step:2307/2315 train_time:140005ms step_avg:60.69ms
step:2308/2315 train_time:140066ms step_avg:60.69ms
step:2309/2315 train_time:140128ms step_avg:60.69ms
step:2310/2315 train_time:140188ms step_avg:60.69ms
step:2311/2315 train_time:140250ms step_avg:60.69ms
step:2312/2315 train_time:140311ms step_avg:60.69ms
step:2313/2315 train_time:140372ms step_avg:60.69ms
step:2314/2315 train_time:140433ms step_avg:60.69ms
step:2315/2315 train_time:140494ms step_avg:60.69ms
step:2315/2315 val_loss:3.2775 train_time:140555ms step_avg:60.72ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

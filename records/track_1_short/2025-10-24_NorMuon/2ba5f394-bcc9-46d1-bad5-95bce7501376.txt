import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]
# polar_express_coeffs = [
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
# ]



@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    # state["second_momentum_buffer"] = torch.zeros_like(grad[...,0:1]) if param.size(-2) >= param.size(-1) else torch.zeros_like(grad[...,0:1,:])
                    # state["second_momentum_buffer"] = torch.zeros((*grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((*grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)
            
            ###################################
            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))
            ####################################


            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> (-1.5)  0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Oct 22 2025, 02:20:45) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251022+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 24 06:09:03 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000001:00:00.0 Off |                    0 |
| N/A   36C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000002:00:00.0 Off |                    0 |
| N/A   35C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000003:00:00.0 Off |                    0 |
| N/A   31C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000008:00:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000009:00:00.0 Off |                    0 |
| N/A   29C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   0000000A:00:00.0 Off |                    0 |
| N/A   35C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   0000000C:00:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2315 train_time:83ms step_avg:82.54ms
step:2/2315 train_time:179ms step_avg:89.67ms
step:3/2315 train_time:200ms step_avg:66.60ms
step:4/2315 train_time:237ms step_avg:59.25ms
step:5/2315 train_time:295ms step_avg:59.07ms
step:6/2315 train_time:355ms step_avg:59.13ms
step:7/2315 train_time:415ms step_avg:59.31ms
step:8/2315 train_time:475ms step_avg:59.37ms
step:9/2315 train_time:534ms step_avg:59.38ms
step:10/2315 train_time:594ms step_avg:59.39ms
step:11/2315 train_time:654ms step_avg:59.47ms
step:12/2315 train_time:714ms step_avg:59.50ms
step:13/2315 train_time:774ms step_avg:59.53ms
step:14/2315 train_time:833ms step_avg:59.53ms
step:15/2315 train_time:893ms step_avg:59.55ms
step:16/2315 train_time:953ms step_avg:59.56ms
step:17/2315 train_time:1016ms step_avg:59.76ms
step:18/2315 train_time:1081ms step_avg:60.04ms
step:19/2315 train_time:1145ms step_avg:60.25ms
step:20/2315 train_time:1206ms step_avg:60.29ms
step:21/2315 train_time:1266ms step_avg:60.30ms
step:22/2315 train_time:1326ms step_avg:60.28ms
step:23/2315 train_time:1386ms step_avg:60.27ms
step:24/2315 train_time:1446ms step_avg:60.25ms
step:25/2315 train_time:1506ms step_avg:60.24ms
step:26/2315 train_time:1565ms step_avg:60.21ms
step:27/2315 train_time:1625ms step_avg:60.19ms
step:28/2315 train_time:1685ms step_avg:60.18ms
step:29/2315 train_time:1746ms step_avg:60.20ms
step:30/2315 train_time:1806ms step_avg:60.18ms
step:31/2315 train_time:1866ms step_avg:60.19ms
step:32/2315 train_time:1926ms step_avg:60.19ms
step:33/2315 train_time:1987ms step_avg:60.22ms
step:34/2315 train_time:2047ms step_avg:60.21ms
step:35/2315 train_time:2108ms step_avg:60.23ms
step:36/2315 train_time:2168ms step_avg:60.22ms
step:37/2315 train_time:2229ms step_avg:60.24ms
step:38/2315 train_time:2289ms step_avg:60.25ms
step:39/2315 train_time:2350ms step_avg:60.26ms
step:40/2315 train_time:2410ms step_avg:60.25ms
step:41/2315 train_time:2470ms step_avg:60.25ms
step:42/2315 train_time:2530ms step_avg:60.23ms
step:43/2315 train_time:2590ms step_avg:60.23ms
step:44/2315 train_time:2650ms step_avg:60.24ms
step:45/2315 train_time:2711ms step_avg:60.24ms
step:46/2315 train_time:2770ms step_avg:60.23ms
step:47/2315 train_time:2830ms step_avg:60.21ms
step:48/2315 train_time:2890ms step_avg:60.21ms
step:49/2315 train_time:2951ms step_avg:60.22ms
step:50/2315 train_time:3011ms step_avg:60.22ms
step:51/2315 train_time:3072ms step_avg:60.23ms
step:52/2315 train_time:3132ms step_avg:60.23ms
step:53/2315 train_time:3193ms step_avg:60.24ms
step:54/2315 train_time:3254ms step_avg:60.25ms
step:55/2315 train_time:3314ms step_avg:60.26ms
step:56/2315 train_time:3375ms step_avg:60.26ms
step:57/2315 train_time:3435ms step_avg:60.27ms
step:58/2315 train_time:3495ms step_avg:60.27ms
step:59/2315 train_time:3556ms step_avg:60.27ms
step:60/2315 train_time:3616ms step_avg:60.27ms
step:61/2315 train_time:3676ms step_avg:60.27ms
step:62/2315 train_time:3736ms step_avg:60.27ms
step:63/2315 train_time:3798ms step_avg:60.29ms
step:64/2315 train_time:3859ms step_avg:60.29ms
step:65/2315 train_time:3920ms step_avg:60.31ms
step:66/2315 train_time:3980ms step_avg:60.31ms
step:67/2315 train_time:4042ms step_avg:60.32ms
step:68/2315 train_time:4102ms step_avg:60.33ms
step:69/2315 train_time:4162ms step_avg:60.33ms
step:70/2315 train_time:4223ms step_avg:60.33ms
step:71/2315 train_time:4284ms step_avg:60.33ms
step:72/2315 train_time:4344ms step_avg:60.33ms
step:73/2315 train_time:4404ms step_avg:60.33ms
step:74/2315 train_time:4464ms step_avg:60.32ms
step:75/2315 train_time:4525ms step_avg:60.33ms
step:76/2315 train_time:4584ms step_avg:60.32ms
step:77/2315 train_time:4644ms step_avg:60.32ms
step:78/2315 train_time:4704ms step_avg:60.31ms
step:79/2315 train_time:4764ms step_avg:60.30ms
step:80/2315 train_time:4824ms step_avg:60.30ms
step:81/2315 train_time:4885ms step_avg:60.31ms
step:82/2315 train_time:4945ms step_avg:60.30ms
step:83/2315 train_time:5005ms step_avg:60.30ms
step:84/2315 train_time:5065ms step_avg:60.30ms
step:85/2315 train_time:5125ms step_avg:60.29ms
step:86/2315 train_time:5185ms step_avg:60.29ms
step:87/2315 train_time:5246ms step_avg:60.29ms
step:88/2315 train_time:5305ms step_avg:60.29ms
step:89/2315 train_time:5366ms step_avg:60.29ms
step:90/2315 train_time:5426ms step_avg:60.28ms
step:91/2315 train_time:5486ms step_avg:60.29ms
step:92/2315 train_time:5546ms step_avg:60.28ms
step:93/2315 train_time:5606ms step_avg:60.27ms
step:94/2315 train_time:5665ms step_avg:60.26ms
step:95/2315 train_time:5724ms step_avg:60.26ms
step:96/2315 train_time:5784ms step_avg:60.25ms
step:97/2315 train_time:5844ms step_avg:60.25ms
step:98/2315 train_time:5904ms step_avg:60.24ms
step:99/2315 train_time:5964ms step_avg:60.24ms
step:100/2315 train_time:6024ms step_avg:60.24ms
step:101/2315 train_time:6084ms step_avg:60.24ms
step:102/2315 train_time:6144ms step_avg:60.24ms
step:103/2315 train_time:6205ms step_avg:60.24ms
step:104/2315 train_time:6265ms step_avg:60.24ms
step:105/2315 train_time:6325ms step_avg:60.24ms
step:106/2315 train_time:6385ms step_avg:60.24ms
step:107/2315 train_time:6445ms step_avg:60.23ms
step:108/2315 train_time:6505ms step_avg:60.23ms
step:109/2315 train_time:6565ms step_avg:60.23ms
step:110/2315 train_time:6624ms step_avg:60.22ms
step:111/2315 train_time:6684ms step_avg:60.22ms
step:112/2315 train_time:6744ms step_avg:60.22ms
step:113/2315 train_time:6804ms step_avg:60.21ms
step:114/2315 train_time:6863ms step_avg:60.20ms
step:115/2315 train_time:6924ms step_avg:60.20ms
step:116/2315 train_time:6983ms step_avg:60.20ms
step:117/2315 train_time:7043ms step_avg:60.20ms
step:118/2315 train_time:7103ms step_avg:60.19ms
step:119/2315 train_time:7163ms step_avg:60.19ms
step:120/2315 train_time:7223ms step_avg:60.19ms
step:121/2315 train_time:7284ms step_avg:60.20ms
step:122/2315 train_time:7344ms step_avg:60.19ms
step:123/2315 train_time:7404ms step_avg:60.19ms
step:124/2315 train_time:7464ms step_avg:60.19ms
step:125/2315 train_time:7523ms step_avg:60.19ms
step:126/2315 train_time:7583ms step_avg:60.18ms
step:127/2315 train_time:7643ms step_avg:60.18ms
step:128/2315 train_time:7704ms step_avg:60.19ms
step:129/2315 train_time:7764ms step_avg:60.19ms
step:130/2315 train_time:7824ms step_avg:60.18ms
step:131/2315 train_time:7885ms step_avg:60.19ms
step:132/2315 train_time:7945ms step_avg:60.19ms
step:133/2315 train_time:8005ms step_avg:60.18ms
step:134/2315 train_time:8064ms step_avg:60.18ms
step:135/2315 train_time:8124ms step_avg:60.18ms
step:136/2315 train_time:8184ms step_avg:60.18ms
step:137/2315 train_time:8244ms step_avg:60.18ms
step:138/2315 train_time:8304ms step_avg:60.17ms
step:139/2315 train_time:8364ms step_avg:60.17ms
step:140/2315 train_time:8423ms step_avg:60.17ms
step:141/2315 train_time:8483ms step_avg:60.17ms
step:142/2315 train_time:8543ms step_avg:60.16ms
step:143/2315 train_time:8603ms step_avg:60.16ms
step:144/2315 train_time:8663ms step_avg:60.16ms
step:145/2315 train_time:8724ms step_avg:60.16ms
step:146/2315 train_time:8783ms step_avg:60.16ms
step:147/2315 train_time:8843ms step_avg:60.16ms
step:148/2315 train_time:8903ms step_avg:60.16ms
step:149/2315 train_time:8963ms step_avg:60.16ms
step:150/2315 train_time:9023ms step_avg:60.16ms
step:151/2315 train_time:9083ms step_avg:60.16ms
step:152/2315 train_time:9143ms step_avg:60.15ms
step:153/2315 train_time:9203ms step_avg:60.15ms
step:154/2315 train_time:9264ms step_avg:60.15ms
step:155/2315 train_time:9323ms step_avg:60.15ms
step:156/2315 train_time:9383ms step_avg:60.15ms
step:157/2315 train_time:9444ms step_avg:60.15ms
step:158/2315 train_time:9504ms step_avg:60.15ms
step:159/2315 train_time:9564ms step_avg:60.15ms
step:160/2315 train_time:9623ms step_avg:60.15ms
step:161/2315 train_time:9683ms step_avg:60.15ms
step:162/2315 train_time:9744ms step_avg:60.15ms
step:163/2315 train_time:9804ms step_avg:60.14ms
step:164/2315 train_time:9863ms step_avg:60.14ms
step:165/2315 train_time:9923ms step_avg:60.14ms
step:166/2315 train_time:9983ms step_avg:60.14ms
step:167/2315 train_time:10043ms step_avg:60.14ms
step:168/2315 train_time:10103ms step_avg:60.14ms
step:169/2315 train_time:10164ms step_avg:60.14ms
step:170/2315 train_time:10224ms step_avg:60.14ms
step:171/2315 train_time:10284ms step_avg:60.14ms
step:172/2315 train_time:10344ms step_avg:60.14ms
step:173/2315 train_time:10404ms step_avg:60.14ms
step:174/2315 train_time:10464ms step_avg:60.14ms
step:175/2315 train_time:10524ms step_avg:60.14ms
step:176/2315 train_time:10584ms step_avg:60.14ms
step:177/2315 train_time:10644ms step_avg:60.14ms
step:178/2315 train_time:10704ms step_avg:60.14ms
step:179/2315 train_time:10764ms step_avg:60.13ms
step:180/2315 train_time:10824ms step_avg:60.13ms
step:181/2315 train_time:10884ms step_avg:60.13ms
step:182/2315 train_time:10944ms step_avg:60.13ms
step:183/2315 train_time:11004ms step_avg:60.13ms
step:184/2315 train_time:11064ms step_avg:60.13ms
step:185/2315 train_time:11124ms step_avg:60.13ms
step:186/2315 train_time:11183ms step_avg:60.13ms
step:187/2315 train_time:11244ms step_avg:60.13ms
step:188/2315 train_time:11304ms step_avg:60.13ms
step:189/2315 train_time:11364ms step_avg:60.13ms
step:190/2315 train_time:11424ms step_avg:60.12ms
step:191/2315 train_time:11484ms step_avg:60.13ms
step:192/2315 train_time:11544ms step_avg:60.12ms
step:193/2315 train_time:11604ms step_avg:60.12ms
step:194/2315 train_time:11664ms step_avg:60.12ms
step:195/2315 train_time:11723ms step_avg:60.12ms
step:196/2315 train_time:11783ms step_avg:60.12ms
step:197/2315 train_time:11843ms step_avg:60.12ms
step:198/2315 train_time:11903ms step_avg:60.12ms
step:199/2315 train_time:11963ms step_avg:60.12ms
step:200/2315 train_time:12023ms step_avg:60.11ms
step:201/2315 train_time:12083ms step_avg:60.11ms
step:202/2315 train_time:12142ms step_avg:60.11ms
step:203/2315 train_time:12202ms step_avg:60.11ms
step:204/2315 train_time:12262ms step_avg:60.11ms
step:205/2315 train_time:12322ms step_avg:60.11ms
step:206/2315 train_time:12382ms step_avg:60.11ms
step:207/2315 train_time:12442ms step_avg:60.11ms
step:208/2315 train_time:12503ms step_avg:60.11ms
step:209/2315 train_time:12563ms step_avg:60.11ms
step:210/2315 train_time:12623ms step_avg:60.11ms
step:211/2315 train_time:12683ms step_avg:60.11ms
step:212/2315 train_time:12743ms step_avg:60.11ms
step:213/2315 train_time:12803ms step_avg:60.11ms
step:214/2315 train_time:12863ms step_avg:60.11ms
step:215/2315 train_time:12923ms step_avg:60.11ms
step:216/2315 train_time:12984ms step_avg:60.11ms
step:217/2315 train_time:13044ms step_avg:60.11ms
step:218/2315 train_time:13103ms step_avg:60.11ms
step:219/2315 train_time:13163ms step_avg:60.11ms
step:220/2315 train_time:13224ms step_avg:60.11ms
step:221/2315 train_time:13284ms step_avg:60.11ms
step:222/2315 train_time:13344ms step_avg:60.11ms
step:223/2315 train_time:13404ms step_avg:60.11ms
step:224/2315 train_time:13464ms step_avg:60.11ms
step:225/2315 train_time:13525ms step_avg:60.11ms
step:226/2315 train_time:13585ms step_avg:60.11ms
step:227/2315 train_time:13645ms step_avg:60.11ms
step:228/2315 train_time:13704ms step_avg:60.11ms
step:229/2315 train_time:13764ms step_avg:60.11ms
step:230/2315 train_time:13824ms step_avg:60.11ms
step:231/2315 train_time:13884ms step_avg:60.10ms
step:232/2315 train_time:13944ms step_avg:60.10ms
step:233/2315 train_time:14004ms step_avg:60.10ms
step:234/2315 train_time:14064ms step_avg:60.10ms
step:235/2315 train_time:14124ms step_avg:60.10ms
step:236/2315 train_time:14184ms step_avg:60.10ms
step:237/2315 train_time:14244ms step_avg:60.10ms
step:238/2315 train_time:14304ms step_avg:60.10ms
step:239/2315 train_time:14364ms step_avg:60.10ms
step:240/2315 train_time:14423ms step_avg:60.10ms
step:241/2315 train_time:14483ms step_avg:60.10ms
step:242/2315 train_time:14543ms step_avg:60.10ms
step:243/2315 train_time:14604ms step_avg:60.10ms
step:244/2315 train_time:14664ms step_avg:60.10ms
step:245/2315 train_time:14724ms step_avg:60.10ms
step:246/2315 train_time:14784ms step_avg:60.10ms
step:247/2315 train_time:14844ms step_avg:60.10ms
step:248/2315 train_time:14905ms step_avg:60.10ms
step:249/2315 train_time:14964ms step_avg:60.10ms
step:250/2315 train_time:15024ms step_avg:60.10ms
step:250/2315 val_loss:4.0770 train_time:15085ms step_avg:60.34ms
step:251/2315 train_time:15106ms step_avg:60.18ms
step:252/2315 train_time:15145ms step_avg:60.10ms
step:253/2315 train_time:15207ms step_avg:60.11ms
step:254/2315 train_time:15274ms step_avg:60.13ms
step:255/2315 train_time:15335ms step_avg:60.14ms
step:256/2315 train_time:15395ms step_avg:60.14ms
step:257/2315 train_time:15455ms step_avg:60.14ms
step:258/2315 train_time:15514ms step_avg:60.13ms
step:259/2315 train_time:15574ms step_avg:60.13ms
step:260/2315 train_time:15634ms step_avg:60.13ms
step:261/2315 train_time:15693ms step_avg:60.13ms
step:262/2315 train_time:15752ms step_avg:60.12ms
step:263/2315 train_time:15811ms step_avg:60.12ms
step:264/2315 train_time:15870ms step_avg:60.11ms
step:265/2315 train_time:15929ms step_avg:60.11ms
step:266/2315 train_time:15988ms step_avg:60.10ms
step:267/2315 train_time:16047ms step_avg:60.10ms
step:268/2315 train_time:16107ms step_avg:60.10ms
step:269/2315 train_time:16167ms step_avg:60.10ms
step:270/2315 train_time:16227ms step_avg:60.10ms
step:271/2315 train_time:16288ms step_avg:60.10ms
step:272/2315 train_time:16348ms step_avg:60.10ms
step:273/2315 train_time:16408ms step_avg:60.10ms
step:274/2315 train_time:16467ms step_avg:60.10ms
step:275/2315 train_time:16527ms step_avg:60.10ms
step:276/2315 train_time:16587ms step_avg:60.10ms
step:277/2315 train_time:16647ms step_avg:60.10ms
step:278/2315 train_time:16706ms step_avg:60.09ms
step:279/2315 train_time:16766ms step_avg:60.09ms
step:280/2315 train_time:16826ms step_avg:60.09ms
step:281/2315 train_time:16885ms step_avg:60.09ms
step:282/2315 train_time:16945ms step_avg:60.09ms
step:283/2315 train_time:17005ms step_avg:60.09ms
step:284/2315 train_time:17065ms step_avg:60.09ms
step:285/2315 train_time:17125ms step_avg:60.09ms
step:286/2315 train_time:17185ms step_avg:60.09ms
step:287/2315 train_time:17245ms step_avg:60.09ms
step:288/2315 train_time:17305ms step_avg:60.09ms
step:289/2315 train_time:17366ms step_avg:60.09ms
step:290/2315 train_time:17426ms step_avg:60.09ms
step:291/2315 train_time:17486ms step_avg:60.09ms
step:292/2315 train_time:17545ms step_avg:60.09ms
step:293/2315 train_time:17605ms step_avg:60.09ms
step:294/2315 train_time:17665ms step_avg:60.08ms
step:295/2315 train_time:17725ms step_avg:60.08ms
step:296/2315 train_time:17785ms step_avg:60.08ms
step:297/2315 train_time:17845ms step_avg:60.08ms
step:298/2315 train_time:17904ms step_avg:60.08ms
step:299/2315 train_time:17964ms step_avg:60.08ms
step:300/2315 train_time:18023ms step_avg:60.08ms
step:301/2315 train_time:18084ms step_avg:60.08ms
step:302/2315 train_time:18144ms step_avg:60.08ms
step:303/2315 train_time:18204ms step_avg:60.08ms
step:304/2315 train_time:18264ms step_avg:60.08ms
step:305/2315 train_time:18324ms step_avg:60.08ms
step:306/2315 train_time:18385ms step_avg:60.08ms
step:307/2315 train_time:18445ms step_avg:60.08ms
step:308/2315 train_time:18505ms step_avg:60.08ms
step:309/2315 train_time:18565ms step_avg:60.08ms
step:310/2315 train_time:18624ms step_avg:60.08ms
step:311/2315 train_time:18685ms step_avg:60.08ms
step:312/2315 train_time:18744ms step_avg:60.08ms
step:313/2315 train_time:18804ms step_avg:60.08ms
step:314/2315 train_time:18864ms step_avg:60.08ms
step:315/2315 train_time:18924ms step_avg:60.07ms
step:316/2315 train_time:18983ms step_avg:60.07ms
step:317/2315 train_time:19044ms step_avg:60.07ms
step:318/2315 train_time:19104ms step_avg:60.08ms
step:319/2315 train_time:19165ms step_avg:60.08ms
step:320/2315 train_time:19225ms step_avg:60.08ms
step:321/2315 train_time:19285ms step_avg:60.08ms
step:322/2315 train_time:19345ms step_avg:60.08ms
step:323/2315 train_time:19406ms step_avg:60.08ms
step:324/2315 train_time:19466ms step_avg:60.08ms
step:325/2315 train_time:19526ms step_avg:60.08ms
step:326/2315 train_time:19586ms step_avg:60.08ms
step:327/2315 train_time:19646ms step_avg:60.08ms
step:328/2315 train_time:19705ms step_avg:60.08ms
step:329/2315 train_time:19765ms step_avg:60.08ms
step:330/2315 train_time:19825ms step_avg:60.08ms
step:331/2315 train_time:19885ms step_avg:60.08ms
step:332/2315 train_time:19944ms step_avg:60.07ms
step:333/2315 train_time:20005ms step_avg:60.07ms
step:334/2315 train_time:20065ms step_avg:60.07ms
step:335/2315 train_time:20125ms step_avg:60.07ms
step:336/2315 train_time:20185ms step_avg:60.07ms
step:337/2315 train_time:20245ms step_avg:60.07ms
step:338/2315 train_time:20305ms step_avg:60.07ms
step:339/2315 train_time:20366ms step_avg:60.08ms
step:340/2315 train_time:20426ms step_avg:60.08ms
step:341/2315 train_time:20486ms step_avg:60.08ms
step:342/2315 train_time:20546ms step_avg:60.08ms
step:343/2315 train_time:20606ms step_avg:60.08ms
step:344/2315 train_time:20666ms step_avg:60.08ms
step:345/2315 train_time:20725ms step_avg:60.07ms
step:346/2315 train_time:20785ms step_avg:60.07ms
step:347/2315 train_time:20845ms step_avg:60.07ms
step:348/2315 train_time:20906ms step_avg:60.07ms
step:349/2315 train_time:20965ms step_avg:60.07ms
step:350/2315 train_time:21025ms step_avg:60.07ms
step:351/2315 train_time:21085ms step_avg:60.07ms
step:352/2315 train_time:21144ms step_avg:60.07ms
step:353/2315 train_time:21205ms step_avg:60.07ms
step:354/2315 train_time:21264ms step_avg:60.07ms
step:355/2315 train_time:21325ms step_avg:60.07ms
step:356/2315 train_time:21385ms step_avg:60.07ms
step:357/2315 train_time:21446ms step_avg:60.07ms
step:358/2315 train_time:21506ms step_avg:60.07ms
step:359/2315 train_time:21565ms step_avg:60.07ms
step:360/2315 train_time:21625ms step_avg:60.07ms
step:361/2315 train_time:21685ms step_avg:60.07ms
step:362/2315 train_time:21745ms step_avg:60.07ms
step:363/2315 train_time:21805ms step_avg:60.07ms
step:364/2315 train_time:21864ms step_avg:60.07ms
step:365/2315 train_time:21924ms step_avg:60.07ms
step:366/2315 train_time:21984ms step_avg:60.06ms
step:367/2315 train_time:22044ms step_avg:60.06ms
step:368/2315 train_time:22104ms step_avg:60.06ms
step:369/2315 train_time:22164ms step_avg:60.06ms
step:370/2315 train_time:22224ms step_avg:60.06ms
step:371/2315 train_time:22284ms step_avg:60.06ms
step:372/2315 train_time:22344ms step_avg:60.06ms
step:373/2315 train_time:22404ms step_avg:60.07ms
step:374/2315 train_time:22464ms step_avg:60.06ms
step:375/2315 train_time:22524ms step_avg:60.06ms
step:376/2315 train_time:22584ms step_avg:60.07ms
step:377/2315 train_time:22645ms step_avg:60.07ms
step:378/2315 train_time:22704ms step_avg:60.06ms
step:379/2315 train_time:22764ms step_avg:60.06ms
step:380/2315 train_time:22824ms step_avg:60.06ms
step:381/2315 train_time:22884ms step_avg:60.06ms
step:382/2315 train_time:22944ms step_avg:60.06ms
step:383/2315 train_time:23005ms step_avg:60.06ms
step:384/2315 train_time:23065ms step_avg:60.06ms
step:385/2315 train_time:23125ms step_avg:60.07ms
step:386/2315 train_time:23185ms step_avg:60.06ms
step:387/2315 train_time:23245ms step_avg:60.07ms
step:388/2315 train_time:23306ms step_avg:60.07ms
step:389/2315 train_time:23365ms step_avg:60.07ms
step:390/2315 train_time:23426ms step_avg:60.07ms
step:391/2315 train_time:23486ms step_avg:60.07ms
step:392/2315 train_time:23547ms step_avg:60.07ms
step:393/2315 train_time:23606ms step_avg:60.07ms
step:394/2315 train_time:23666ms step_avg:60.07ms
step:395/2315 train_time:23726ms step_avg:60.07ms
step:396/2315 train_time:23786ms step_avg:60.07ms
step:397/2315 train_time:23845ms step_avg:60.06ms
step:398/2315 train_time:23906ms step_avg:60.06ms
step:399/2315 train_time:23965ms step_avg:60.06ms
step:400/2315 train_time:24026ms step_avg:60.06ms
step:401/2315 train_time:24085ms step_avg:60.06ms
step:402/2315 train_time:24145ms step_avg:60.06ms
step:403/2315 train_time:24205ms step_avg:60.06ms
step:404/2315 train_time:24265ms step_avg:60.06ms
step:405/2315 train_time:24325ms step_avg:60.06ms
step:406/2315 train_time:24385ms step_avg:60.06ms
step:407/2315 train_time:24445ms step_avg:60.06ms
step:408/2315 train_time:24504ms step_avg:60.06ms
step:409/2315 train_time:24565ms step_avg:60.06ms
step:410/2315 train_time:24624ms step_avg:60.06ms
step:411/2315 train_time:24685ms step_avg:60.06ms
step:412/2315 train_time:24745ms step_avg:60.06ms
step:413/2315 train_time:24805ms step_avg:60.06ms
step:414/2315 train_time:24865ms step_avg:60.06ms
step:415/2315 train_time:24924ms step_avg:60.06ms
step:416/2315 train_time:24984ms step_avg:60.06ms
step:417/2315 train_time:25044ms step_avg:60.06ms
step:418/2315 train_time:25104ms step_avg:60.06ms
step:419/2315 train_time:25164ms step_avg:60.06ms
step:420/2315 train_time:25223ms step_avg:60.06ms
step:421/2315 train_time:25283ms step_avg:60.06ms
step:422/2315 train_time:25344ms step_avg:60.06ms
step:423/2315 train_time:25405ms step_avg:60.06ms
step:424/2315 train_time:25464ms step_avg:60.06ms
step:425/2315 train_time:25525ms step_avg:60.06ms
step:426/2315 train_time:25585ms step_avg:60.06ms
step:427/2315 train_time:25645ms step_avg:60.06ms
step:428/2315 train_time:25705ms step_avg:60.06ms
step:429/2315 train_time:25765ms step_avg:60.06ms
step:430/2315 train_time:25826ms step_avg:60.06ms
step:431/2315 train_time:25886ms step_avg:60.06ms
step:432/2315 train_time:25946ms step_avg:60.06ms
step:433/2315 train_time:26006ms step_avg:60.06ms
step:434/2315 train_time:26066ms step_avg:60.06ms
step:435/2315 train_time:26126ms step_avg:60.06ms
step:436/2315 train_time:26185ms step_avg:60.06ms
step:437/2315 train_time:26245ms step_avg:60.06ms
step:438/2315 train_time:26305ms step_avg:60.06ms
step:439/2315 train_time:26365ms step_avg:60.06ms
step:440/2315 train_time:26425ms step_avg:60.06ms
step:441/2315 train_time:26485ms step_avg:60.06ms
step:442/2315 train_time:26545ms step_avg:60.06ms
step:443/2315 train_time:26606ms step_avg:60.06ms
step:444/2315 train_time:26665ms step_avg:60.06ms
step:445/2315 train_time:26725ms step_avg:60.06ms
step:446/2315 train_time:26786ms step_avg:60.06ms
step:447/2315 train_time:26846ms step_avg:60.06ms
step:448/2315 train_time:26906ms step_avg:60.06ms
step:449/2315 train_time:26966ms step_avg:60.06ms
step:450/2315 train_time:27025ms step_avg:60.06ms
step:451/2315 train_time:27085ms step_avg:60.06ms
step:452/2315 train_time:27145ms step_avg:60.06ms
step:453/2315 train_time:27205ms step_avg:60.05ms
step:454/2315 train_time:27264ms step_avg:60.05ms
step:455/2315 train_time:27325ms step_avg:60.05ms
step:456/2315 train_time:27385ms step_avg:60.05ms
step:457/2315 train_time:27446ms step_avg:60.06ms
step:458/2315 train_time:27505ms step_avg:60.05ms
step:459/2315 train_time:27566ms step_avg:60.06ms
step:460/2315 train_time:27627ms step_avg:60.06ms
step:461/2315 train_time:27686ms step_avg:60.06ms
step:462/2315 train_time:27746ms step_avg:60.06ms
step:463/2315 train_time:27806ms step_avg:60.06ms
step:464/2315 train_time:27866ms step_avg:60.06ms
step:465/2315 train_time:27926ms step_avg:60.06ms
step:466/2315 train_time:27986ms step_avg:60.06ms
step:467/2315 train_time:28046ms step_avg:60.06ms
step:468/2315 train_time:28105ms step_avg:60.05ms
step:469/2315 train_time:28165ms step_avg:60.05ms
step:470/2315 train_time:28225ms step_avg:60.05ms
step:471/2315 train_time:28285ms step_avg:60.05ms
step:472/2315 train_time:28345ms step_avg:60.05ms
step:473/2315 train_time:28405ms step_avg:60.05ms
step:474/2315 train_time:28465ms step_avg:60.05ms
step:475/2315 train_time:28525ms step_avg:60.05ms
step:476/2315 train_time:28585ms step_avg:60.05ms
step:477/2315 train_time:28644ms step_avg:60.05ms
step:478/2315 train_time:28704ms step_avg:60.05ms
step:479/2315 train_time:28765ms step_avg:60.05ms
step:480/2315 train_time:28825ms step_avg:60.05ms
step:481/2315 train_time:28885ms step_avg:60.05ms
step:482/2315 train_time:28945ms step_avg:60.05ms
step:483/2315 train_time:29005ms step_avg:60.05ms
step:484/2315 train_time:29065ms step_avg:60.05ms
step:485/2315 train_time:29125ms step_avg:60.05ms
step:486/2315 train_time:29186ms step_avg:60.05ms
step:487/2315 train_time:29246ms step_avg:60.05ms
step:488/2315 train_time:29306ms step_avg:60.05ms
step:489/2315 train_time:29366ms step_avg:60.05ms
step:490/2315 train_time:29425ms step_avg:60.05ms
step:491/2315 train_time:29486ms step_avg:60.05ms
step:492/2315 train_time:29545ms step_avg:60.05ms
step:493/2315 train_time:29606ms step_avg:60.05ms
step:494/2315 train_time:29665ms step_avg:60.05ms
step:495/2315 train_time:29725ms step_avg:60.05ms
step:496/2315 train_time:29785ms step_avg:60.05ms
step:497/2315 train_time:29845ms step_avg:60.05ms
step:498/2315 train_time:29905ms step_avg:60.05ms
step:499/2315 train_time:29965ms step_avg:60.05ms
step:500/2315 train_time:30025ms step_avg:60.05ms
step:500/2315 val_loss:3.8108 train_time:30088ms step_avg:60.18ms
step:501/2315 train_time:30109ms step_avg:60.10ms
step:502/2315 train_time:30148ms step_avg:60.06ms
step:503/2315 train_time:30212ms step_avg:60.06ms
step:504/2315 train_time:30276ms step_avg:60.07ms
step:505/2315 train_time:30336ms step_avg:60.07ms
step:506/2315 train_time:30396ms step_avg:60.07ms
step:507/2315 train_time:30455ms step_avg:60.07ms
step:508/2315 train_time:30514ms step_avg:60.07ms
step:509/2315 train_time:30574ms step_avg:60.07ms
step:510/2315 train_time:30633ms step_avg:60.06ms
step:511/2315 train_time:30692ms step_avg:60.06ms
step:512/2315 train_time:30751ms step_avg:60.06ms
step:513/2315 train_time:30810ms step_avg:60.06ms
step:514/2315 train_time:30870ms step_avg:60.06ms
step:515/2315 train_time:30929ms step_avg:60.06ms
step:516/2315 train_time:30988ms step_avg:60.05ms
step:517/2315 train_time:31048ms step_avg:60.05ms
step:518/2315 train_time:31108ms step_avg:60.05ms
step:519/2315 train_time:31169ms step_avg:60.06ms
step:520/2315 train_time:31230ms step_avg:60.06ms
step:521/2315 train_time:31290ms step_avg:60.06ms
step:522/2315 train_time:31352ms step_avg:60.06ms
step:523/2315 train_time:31412ms step_avg:60.06ms
step:524/2315 train_time:31471ms step_avg:60.06ms
step:525/2315 train_time:31531ms step_avg:60.06ms
step:526/2315 train_time:31590ms step_avg:60.06ms
step:527/2315 train_time:31650ms step_avg:60.06ms
step:528/2315 train_time:31709ms step_avg:60.06ms
step:529/2315 train_time:31769ms step_avg:60.05ms
step:530/2315 train_time:31828ms step_avg:60.05ms
step:531/2315 train_time:31888ms step_avg:60.05ms
step:532/2315 train_time:31947ms step_avg:60.05ms
step:533/2315 train_time:32006ms step_avg:60.05ms
step:534/2315 train_time:32066ms step_avg:60.05ms
step:535/2315 train_time:32126ms step_avg:60.05ms
step:536/2315 train_time:32186ms step_avg:60.05ms
step:537/2315 train_time:32247ms step_avg:60.05ms
step:538/2315 train_time:32307ms step_avg:60.05ms
step:539/2315 train_time:32368ms step_avg:60.05ms
step:540/2315 train_time:32428ms step_avg:60.05ms
step:541/2315 train_time:32489ms step_avg:60.05ms
step:542/2315 train_time:32548ms step_avg:60.05ms
step:543/2315 train_time:32608ms step_avg:60.05ms
step:544/2315 train_time:32668ms step_avg:60.05ms
step:545/2315 train_time:32728ms step_avg:60.05ms
step:546/2315 train_time:32788ms step_avg:60.05ms
step:547/2315 train_time:32848ms step_avg:60.05ms
step:548/2315 train_time:32907ms step_avg:60.05ms
step:549/2315 train_time:32967ms step_avg:60.05ms
step:550/2315 train_time:33027ms step_avg:60.05ms
step:551/2315 train_time:33087ms step_avg:60.05ms
step:552/2315 train_time:33147ms step_avg:60.05ms
step:553/2315 train_time:33208ms step_avg:60.05ms
step:554/2315 train_time:33268ms step_avg:60.05ms
step:555/2315 train_time:33329ms step_avg:60.05ms
step:556/2315 train_time:33388ms step_avg:60.05ms
step:557/2315 train_time:33449ms step_avg:60.05ms
step:558/2315 train_time:33509ms step_avg:60.05ms
step:559/2315 train_time:33569ms step_avg:60.05ms
step:560/2315 train_time:33629ms step_avg:60.05ms
step:561/2315 train_time:33688ms step_avg:60.05ms
step:562/2315 train_time:33747ms step_avg:60.05ms
step:563/2315 train_time:33807ms step_avg:60.05ms
step:564/2315 train_time:33867ms step_avg:60.05ms
step:565/2315 train_time:33927ms step_avg:60.05ms
step:566/2315 train_time:33987ms step_avg:60.05ms
step:567/2315 train_time:34047ms step_avg:60.05ms
step:568/2315 train_time:34106ms step_avg:60.05ms
step:569/2315 train_time:34168ms step_avg:60.05ms
step:570/2315 train_time:34228ms step_avg:60.05ms
step:571/2315 train_time:34288ms step_avg:60.05ms
step:572/2315 train_time:34348ms step_avg:60.05ms
step:573/2315 train_time:34409ms step_avg:60.05ms
step:574/2315 train_time:34469ms step_avg:60.05ms
step:575/2315 train_time:34530ms step_avg:60.05ms
step:576/2315 train_time:34589ms step_avg:60.05ms
step:577/2315 train_time:34649ms step_avg:60.05ms
step:578/2315 train_time:34708ms step_avg:60.05ms
step:579/2315 train_time:34768ms step_avg:60.05ms
step:580/2315 train_time:34827ms step_avg:60.05ms
step:581/2315 train_time:34887ms step_avg:60.05ms
step:582/2315 train_time:34947ms step_avg:60.05ms
step:583/2315 train_time:35008ms step_avg:60.05ms
step:584/2315 train_time:35067ms step_avg:60.05ms
step:585/2315 train_time:35127ms step_avg:60.05ms
step:586/2315 train_time:35187ms step_avg:60.05ms
step:587/2315 train_time:35247ms step_avg:60.05ms
step:588/2315 train_time:35307ms step_avg:60.05ms
step:589/2315 train_time:35368ms step_avg:60.05ms
step:590/2315 train_time:35428ms step_avg:60.05ms
step:591/2315 train_time:35489ms step_avg:60.05ms
step:592/2315 train_time:35549ms step_avg:60.05ms
step:593/2315 train_time:35608ms step_avg:60.05ms
step:594/2315 train_time:35669ms step_avg:60.05ms
step:595/2315 train_time:35729ms step_avg:60.05ms
step:596/2315 train_time:35788ms step_avg:60.05ms
step:597/2315 train_time:35849ms step_avg:60.05ms
step:598/2315 train_time:35908ms step_avg:60.05ms
step:599/2315 train_time:35967ms step_avg:60.05ms
step:600/2315 train_time:36027ms step_avg:60.05ms
step:601/2315 train_time:36087ms step_avg:60.05ms
step:602/2315 train_time:36147ms step_avg:60.04ms
step:603/2315 train_time:36207ms step_avg:60.05ms
step:604/2315 train_time:36267ms step_avg:60.05ms
step:605/2315 train_time:36327ms step_avg:60.05ms
step:606/2315 train_time:36387ms step_avg:60.05ms
step:607/2315 train_time:36448ms step_avg:60.05ms
step:608/2315 train_time:36508ms step_avg:60.05ms
step:609/2315 train_time:36568ms step_avg:60.05ms
step:610/2315 train_time:36628ms step_avg:60.05ms
step:611/2315 train_time:36688ms step_avg:60.05ms
step:612/2315 train_time:36748ms step_avg:60.05ms
step:613/2315 train_time:36808ms step_avg:60.05ms
step:614/2315 train_time:36868ms step_avg:60.05ms
step:615/2315 train_time:36929ms step_avg:60.05ms
step:616/2315 train_time:36988ms step_avg:60.05ms
step:617/2315 train_time:37048ms step_avg:60.05ms
step:618/2315 train_time:37107ms step_avg:60.04ms
step:619/2315 train_time:37167ms step_avg:60.04ms
step:620/2315 train_time:37228ms step_avg:60.04ms
step:621/2315 train_time:37288ms step_avg:60.04ms
step:622/2315 train_time:37348ms step_avg:60.04ms
step:623/2315 train_time:37408ms step_avg:60.04ms
step:624/2315 train_time:37467ms step_avg:60.04ms
step:625/2315 train_time:37528ms step_avg:60.04ms
step:626/2315 train_time:37588ms step_avg:60.04ms
step:627/2315 train_time:37648ms step_avg:60.04ms
step:628/2315 train_time:37708ms step_avg:60.04ms
step:629/2315 train_time:37768ms step_avg:60.04ms
step:630/2315 train_time:37828ms step_avg:60.05ms
step:631/2315 train_time:37888ms step_avg:60.05ms
step:632/2315 train_time:37948ms step_avg:60.04ms
step:633/2315 train_time:38008ms step_avg:60.04ms
step:634/2315 train_time:38068ms step_avg:60.04ms
step:635/2315 train_time:38127ms step_avg:60.04ms
step:636/2315 train_time:38188ms step_avg:60.04ms
step:637/2315 train_time:38248ms step_avg:60.04ms
step:638/2315 train_time:38308ms step_avg:60.04ms
step:639/2315 train_time:38367ms step_avg:60.04ms
step:640/2315 train_time:38427ms step_avg:60.04ms
step:641/2315 train_time:38487ms step_avg:60.04ms
step:642/2315 train_time:38547ms step_avg:60.04ms
step:643/2315 train_time:38608ms step_avg:60.04ms
step:644/2315 train_time:38668ms step_avg:60.04ms
step:645/2315 train_time:38728ms step_avg:60.04ms
step:646/2315 train_time:38788ms step_avg:60.04ms
step:647/2315 train_time:38849ms step_avg:60.04ms
step:648/2315 train_time:38908ms step_avg:60.04ms
step:649/2315 train_time:38968ms step_avg:60.04ms
step:650/2315 train_time:39029ms step_avg:60.04ms
step:651/2315 train_time:39089ms step_avg:60.04ms
step:652/2315 train_time:39148ms step_avg:60.04ms
step:653/2315 train_time:39208ms step_avg:60.04ms
step:654/2315 train_time:39268ms step_avg:60.04ms
step:655/2315 train_time:39328ms step_avg:60.04ms
step:656/2315 train_time:39389ms step_avg:60.04ms
step:657/2315 train_time:39449ms step_avg:60.04ms
step:658/2315 train_time:39508ms step_avg:60.04ms
step:659/2315 train_time:39568ms step_avg:60.04ms
step:660/2315 train_time:39628ms step_avg:60.04ms
step:661/2315 train_time:39688ms step_avg:60.04ms
step:662/2315 train_time:39748ms step_avg:60.04ms
step:663/2315 train_time:39809ms step_avg:60.04ms
step:664/2315 train_time:39869ms step_avg:60.04ms
step:665/2315 train_time:39928ms step_avg:60.04ms
step:666/2315 train_time:39988ms step_avg:60.04ms
step:667/2315 train_time:40048ms step_avg:60.04ms
step:668/2315 train_time:40108ms step_avg:60.04ms
step:669/2315 train_time:40168ms step_avg:60.04ms
step:670/2315 train_time:40227ms step_avg:60.04ms
step:671/2315 train_time:40287ms step_avg:60.04ms
step:672/2315 train_time:40347ms step_avg:60.04ms
step:673/2315 train_time:40407ms step_avg:60.04ms
step:674/2315 train_time:40467ms step_avg:60.04ms
step:675/2315 train_time:40527ms step_avg:60.04ms
step:676/2315 train_time:40587ms step_avg:60.04ms
step:677/2315 train_time:40648ms step_avg:60.04ms
step:678/2315 train_time:40707ms step_avg:60.04ms
step:679/2315 train_time:40767ms step_avg:60.04ms
step:680/2315 train_time:40828ms step_avg:60.04ms
step:681/2315 train_time:40888ms step_avg:60.04ms
step:682/2315 train_time:40947ms step_avg:60.04ms
step:683/2315 train_time:41007ms step_avg:60.04ms
step:684/2315 train_time:41068ms step_avg:60.04ms
step:685/2315 train_time:41129ms step_avg:60.04ms
step:686/2315 train_time:41188ms step_avg:60.04ms
step:687/2315 train_time:41248ms step_avg:60.04ms
step:688/2315 train_time:41308ms step_avg:60.04ms
step:689/2315 train_time:41368ms step_avg:60.04ms
step:690/2315 train_time:41429ms step_avg:60.04ms
step:691/2315 train_time:41489ms step_avg:60.04ms
step:692/2315 train_time:41549ms step_avg:60.04ms
step:693/2315 train_time:41609ms step_avg:60.04ms
step:694/2315 train_time:41669ms step_avg:60.04ms
step:695/2315 train_time:41729ms step_avg:60.04ms
step:696/2315 train_time:41789ms step_avg:60.04ms
step:697/2315 train_time:41849ms step_avg:60.04ms
step:698/2315 train_time:41908ms step_avg:60.04ms
step:699/2315 train_time:41968ms step_avg:60.04ms
step:700/2315 train_time:42028ms step_avg:60.04ms
step:701/2315 train_time:42088ms step_avg:60.04ms
step:702/2315 train_time:42148ms step_avg:60.04ms
step:703/2315 train_time:42207ms step_avg:60.04ms
step:704/2315 train_time:42267ms step_avg:60.04ms
step:705/2315 train_time:42327ms step_avg:60.04ms
step:706/2315 train_time:42388ms step_avg:60.04ms
step:707/2315 train_time:42448ms step_avg:60.04ms
step:708/2315 train_time:42508ms step_avg:60.04ms
step:709/2315 train_time:42569ms step_avg:60.04ms
step:710/2315 train_time:42628ms step_avg:60.04ms
step:711/2315 train_time:42688ms step_avg:60.04ms
step:712/2315 train_time:42748ms step_avg:60.04ms
step:713/2315 train_time:42808ms step_avg:60.04ms
step:714/2315 train_time:42868ms step_avg:60.04ms
step:715/2315 train_time:42927ms step_avg:60.04ms
step:716/2315 train_time:42987ms step_avg:60.04ms
step:717/2315 train_time:43047ms step_avg:60.04ms
step:718/2315 train_time:43107ms step_avg:60.04ms
step:719/2315 train_time:43167ms step_avg:60.04ms
step:720/2315 train_time:43226ms step_avg:60.04ms
step:721/2315 train_time:43286ms step_avg:60.04ms
step:722/2315 train_time:43346ms step_avg:60.04ms
step:723/2315 train_time:43406ms step_avg:60.04ms
step:724/2315 train_time:43466ms step_avg:60.04ms
step:725/2315 train_time:43527ms step_avg:60.04ms
step:726/2315 train_time:43587ms step_avg:60.04ms
step:727/2315 train_time:43647ms step_avg:60.04ms
step:728/2315 train_time:43708ms step_avg:60.04ms
step:729/2315 train_time:43767ms step_avg:60.04ms
step:730/2315 train_time:43827ms step_avg:60.04ms
step:731/2315 train_time:43887ms step_avg:60.04ms
step:732/2315 train_time:43948ms step_avg:60.04ms
step:733/2315 train_time:44009ms step_avg:60.04ms
step:734/2315 train_time:44069ms step_avg:60.04ms
step:735/2315 train_time:44129ms step_avg:60.04ms
step:736/2315 train_time:44188ms step_avg:60.04ms
step:737/2315 train_time:44248ms step_avg:60.04ms
step:738/2315 train_time:44308ms step_avg:60.04ms
step:739/2315 train_time:44367ms step_avg:60.04ms
step:740/2315 train_time:44428ms step_avg:60.04ms
step:741/2315 train_time:44488ms step_avg:60.04ms
step:742/2315 train_time:44547ms step_avg:60.04ms
step:743/2315 train_time:44609ms step_avg:60.04ms
step:744/2315 train_time:44669ms step_avg:60.04ms
step:745/2315 train_time:44728ms step_avg:60.04ms
step:746/2315 train_time:44788ms step_avg:60.04ms
step:747/2315 train_time:44848ms step_avg:60.04ms
step:748/2315 train_time:44908ms step_avg:60.04ms
step:749/2315 train_time:44968ms step_avg:60.04ms
step:750/2315 train_time:45028ms step_avg:60.04ms
step:750/2315 val_loss:3.6835 train_time:45090ms step_avg:60.12ms
step:751/2315 train_time:45109ms step_avg:60.07ms
step:752/2315 train_time:45152ms step_avg:60.04ms
step:753/2315 train_time:45213ms step_avg:60.04ms
step:754/2315 train_time:45275ms step_avg:60.05ms
step:755/2315 train_time:45336ms step_avg:60.05ms
step:756/2315 train_time:45395ms step_avg:60.05ms
step:757/2315 train_time:45455ms step_avg:60.05ms
step:758/2315 train_time:45514ms step_avg:60.05ms
step:759/2315 train_time:45573ms step_avg:60.04ms
step:760/2315 train_time:45632ms step_avg:60.04ms
step:761/2315 train_time:45692ms step_avg:60.04ms
step:762/2315 train_time:45752ms step_avg:60.04ms
step:763/2315 train_time:45811ms step_avg:60.04ms
step:764/2315 train_time:45871ms step_avg:60.04ms
step:765/2315 train_time:45931ms step_avg:60.04ms
step:766/2315 train_time:45991ms step_avg:60.04ms
step:767/2315 train_time:46053ms step_avg:60.04ms
step:768/2315 train_time:46114ms step_avg:60.04ms
step:769/2315 train_time:46175ms step_avg:60.05ms
step:770/2315 train_time:46236ms step_avg:60.05ms
step:771/2315 train_time:46297ms step_avg:60.05ms
step:772/2315 train_time:46358ms step_avg:60.05ms
step:773/2315 train_time:46419ms step_avg:60.05ms
step:774/2315 train_time:46479ms step_avg:60.05ms
step:775/2315 train_time:46540ms step_avg:60.05ms
step:776/2315 train_time:46600ms step_avg:60.05ms
step:777/2315 train_time:46660ms step_avg:60.05ms
step:778/2315 train_time:46721ms step_avg:60.05ms
step:779/2315 train_time:46782ms step_avg:60.05ms
step:780/2315 train_time:46842ms step_avg:60.05ms
step:781/2315 train_time:46903ms step_avg:60.06ms
step:782/2315 train_time:46964ms step_avg:60.06ms
step:783/2315 train_time:47025ms step_avg:60.06ms
step:784/2315 train_time:47087ms step_avg:60.06ms
step:785/2315 train_time:47147ms step_avg:60.06ms
step:786/2315 train_time:47208ms step_avg:60.06ms
step:787/2315 train_time:47269ms step_avg:60.06ms
step:788/2315 train_time:47330ms step_avg:60.06ms
step:789/2315 train_time:47391ms step_avg:60.07ms
step:790/2315 train_time:47453ms step_avg:60.07ms
step:791/2315 train_time:47513ms step_avg:60.07ms
step:792/2315 train_time:47574ms step_avg:60.07ms
step:793/2315 train_time:47634ms step_avg:60.07ms
step:794/2315 train_time:47695ms step_avg:60.07ms
step:795/2315 train_time:47755ms step_avg:60.07ms
step:796/2315 train_time:47815ms step_avg:60.07ms
step:797/2315 train_time:47875ms step_avg:60.07ms
step:798/2315 train_time:47934ms step_avg:60.07ms
step:799/2315 train_time:47995ms step_avg:60.07ms
step:800/2315 train_time:48055ms step_avg:60.07ms
step:801/2315 train_time:48116ms step_avg:60.07ms
step:802/2315 train_time:48176ms step_avg:60.07ms
step:803/2315 train_time:48237ms step_avg:60.07ms
step:804/2315 train_time:48298ms step_avg:60.07ms
step:805/2315 train_time:48359ms step_avg:60.07ms
step:806/2315 train_time:48420ms step_avg:60.07ms
step:807/2315 train_time:48481ms step_avg:60.08ms
step:808/2315 train_time:48541ms step_avg:60.08ms
step:809/2315 train_time:48601ms step_avg:60.08ms
step:810/2315 train_time:48662ms step_avg:60.08ms
step:811/2315 train_time:48723ms step_avg:60.08ms
step:812/2315 train_time:48783ms step_avg:60.08ms
step:813/2315 train_time:48843ms step_avg:60.08ms
step:814/2315 train_time:48904ms step_avg:60.08ms
step:815/2315 train_time:48964ms step_avg:60.08ms
step:816/2315 train_time:49025ms step_avg:60.08ms
step:817/2315 train_time:49086ms step_avg:60.08ms
step:818/2315 train_time:49147ms step_avg:60.08ms
step:819/2315 train_time:49208ms step_avg:60.08ms
step:820/2315 train_time:49269ms step_avg:60.08ms
step:821/2315 train_time:49331ms step_avg:60.09ms
step:822/2315 train_time:49392ms step_avg:60.09ms
step:823/2315 train_time:49453ms step_avg:60.09ms
step:824/2315 train_time:49513ms step_avg:60.09ms
step:825/2315 train_time:49574ms step_avg:60.09ms
step:826/2315 train_time:49634ms step_avg:60.09ms
step:827/2315 train_time:49694ms step_avg:60.09ms
step:828/2315 train_time:49754ms step_avg:60.09ms
step:829/2315 train_time:49814ms step_avg:60.09ms
step:830/2315 train_time:49874ms step_avg:60.09ms
step:831/2315 train_time:49934ms step_avg:60.09ms
step:832/2315 train_time:49994ms step_avg:60.09ms
step:833/2315 train_time:50054ms step_avg:60.09ms
step:834/2315 train_time:50114ms step_avg:60.09ms
step:835/2315 train_time:50174ms step_avg:60.09ms
step:836/2315 train_time:50235ms step_avg:60.09ms
step:837/2315 train_time:50296ms step_avg:60.09ms
step:838/2315 train_time:50356ms step_avg:60.09ms
step:839/2315 train_time:50417ms step_avg:60.09ms
step:840/2315 train_time:50478ms step_avg:60.09ms
step:841/2315 train_time:50539ms step_avg:60.09ms
step:842/2315 train_time:50600ms step_avg:60.09ms
step:843/2315 train_time:50660ms step_avg:60.09ms
step:844/2315 train_time:50720ms step_avg:60.09ms
step:845/2315 train_time:50781ms step_avg:60.10ms
step:846/2315 train_time:50841ms step_avg:60.10ms
step:847/2315 train_time:50902ms step_avg:60.10ms
step:848/2315 train_time:50962ms step_avg:60.10ms
step:849/2315 train_time:51023ms step_avg:60.10ms
step:850/2315 train_time:51084ms step_avg:60.10ms
step:851/2315 train_time:51144ms step_avg:60.10ms
step:852/2315 train_time:51206ms step_avg:60.10ms
step:853/2315 train_time:51267ms step_avg:60.10ms
step:854/2315 train_time:51327ms step_avg:60.10ms
step:855/2315 train_time:51388ms step_avg:60.10ms
step:856/2315 train_time:51449ms step_avg:60.10ms
step:857/2315 train_time:51510ms step_avg:60.11ms
step:858/2315 train_time:51571ms step_avg:60.11ms
step:859/2315 train_time:51632ms step_avg:60.11ms
step:860/2315 train_time:51692ms step_avg:60.11ms
step:861/2315 train_time:51754ms step_avg:60.11ms
step:862/2315 train_time:51814ms step_avg:60.11ms
step:863/2315 train_time:51874ms step_avg:60.11ms
step:864/2315 train_time:51934ms step_avg:60.11ms
step:865/2315 train_time:51995ms step_avg:60.11ms
step:866/2315 train_time:52055ms step_avg:60.11ms
step:867/2315 train_time:52115ms step_avg:60.11ms
step:868/2315 train_time:52175ms step_avg:60.11ms
step:869/2315 train_time:52235ms step_avg:60.11ms
step:870/2315 train_time:52295ms step_avg:60.11ms
step:871/2315 train_time:52355ms step_avg:60.11ms
step:872/2315 train_time:52415ms step_avg:60.11ms
step:873/2315 train_time:52476ms step_avg:60.11ms
step:874/2315 train_time:52536ms step_avg:60.11ms
step:875/2315 train_time:52596ms step_avg:60.11ms
step:876/2315 train_time:52657ms step_avg:60.11ms
step:877/2315 train_time:52718ms step_avg:60.11ms
step:878/2315 train_time:52779ms step_avg:60.11ms
step:879/2315 train_time:52839ms step_avg:60.11ms
step:880/2315 train_time:52900ms step_avg:60.11ms
step:881/2315 train_time:52960ms step_avg:60.11ms
step:882/2315 train_time:53020ms step_avg:60.11ms
step:883/2315 train_time:53081ms step_avg:60.11ms
step:884/2315 train_time:53141ms step_avg:60.11ms
step:885/2315 train_time:53202ms step_avg:60.11ms
step:886/2315 train_time:53262ms step_avg:60.12ms
step:887/2315 train_time:53324ms step_avg:60.12ms
step:888/2315 train_time:53385ms step_avg:60.12ms
step:889/2315 train_time:53446ms step_avg:60.12ms
step:890/2315 train_time:53506ms step_avg:60.12ms
step:891/2315 train_time:53568ms step_avg:60.12ms
step:892/2315 train_time:53629ms step_avg:60.12ms
step:893/2315 train_time:53690ms step_avg:60.12ms
step:894/2315 train_time:53751ms step_avg:60.12ms
step:895/2315 train_time:53813ms step_avg:60.13ms
step:896/2315 train_time:53874ms step_avg:60.13ms
step:897/2315 train_time:53934ms step_avg:60.13ms
step:898/2315 train_time:53995ms step_avg:60.13ms
step:899/2315 train_time:54055ms step_avg:60.13ms
step:900/2315 train_time:54115ms step_avg:60.13ms
step:901/2315 train_time:54176ms step_avg:60.13ms
step:902/2315 train_time:54235ms step_avg:60.13ms
step:903/2315 train_time:54295ms step_avg:60.13ms
step:904/2315 train_time:54355ms step_avg:60.13ms
step:905/2315 train_time:54415ms step_avg:60.13ms
step:906/2315 train_time:54476ms step_avg:60.13ms
step:907/2315 train_time:54536ms step_avg:60.13ms
step:908/2315 train_time:54596ms step_avg:60.13ms
step:909/2315 train_time:54657ms step_avg:60.13ms
step:910/2315 train_time:54717ms step_avg:60.13ms
step:911/2315 train_time:54778ms step_avg:60.13ms
step:912/2315 train_time:54839ms step_avg:60.13ms
step:913/2315 train_time:54900ms step_avg:60.13ms
step:914/2315 train_time:54961ms step_avg:60.13ms
step:915/2315 train_time:55021ms step_avg:60.13ms
step:916/2315 train_time:55082ms step_avg:60.13ms
step:917/2315 train_time:55143ms step_avg:60.13ms
step:918/2315 train_time:55203ms step_avg:60.13ms
step:919/2315 train_time:55264ms step_avg:60.14ms
step:920/2315 train_time:55325ms step_avg:60.14ms
step:921/2315 train_time:55386ms step_avg:60.14ms
step:922/2315 train_time:55446ms step_avg:60.14ms
step:923/2315 train_time:55507ms step_avg:60.14ms
step:924/2315 train_time:55568ms step_avg:60.14ms
step:925/2315 train_time:55629ms step_avg:60.14ms
step:926/2315 train_time:55690ms step_avg:60.14ms
step:927/2315 train_time:55751ms step_avg:60.14ms
step:928/2315 train_time:55812ms step_avg:60.14ms
step:929/2315 train_time:55873ms step_avg:60.14ms
step:930/2315 train_time:55934ms step_avg:60.14ms
step:931/2315 train_time:55994ms step_avg:60.14ms
step:932/2315 train_time:56054ms step_avg:60.14ms
step:933/2315 train_time:56115ms step_avg:60.14ms
step:934/2315 train_time:56175ms step_avg:60.14ms
step:935/2315 train_time:56234ms step_avg:60.14ms
step:936/2315 train_time:56294ms step_avg:60.14ms
step:937/2315 train_time:56354ms step_avg:60.14ms
step:938/2315 train_time:56414ms step_avg:60.14ms
step:939/2315 train_time:56474ms step_avg:60.14ms
step:940/2315 train_time:56534ms step_avg:60.14ms
step:941/2315 train_time:56595ms step_avg:60.14ms
step:942/2315 train_time:56655ms step_avg:60.14ms
step:943/2315 train_time:56716ms step_avg:60.14ms
step:944/2315 train_time:56776ms step_avg:60.14ms
step:945/2315 train_time:56836ms step_avg:60.14ms
step:946/2315 train_time:56896ms step_avg:60.14ms
step:947/2315 train_time:56957ms step_avg:60.14ms
step:948/2315 train_time:57017ms step_avg:60.14ms
step:949/2315 train_time:57078ms step_avg:60.14ms
step:950/2315 train_time:57138ms step_avg:60.15ms
step:951/2315 train_time:57198ms step_avg:60.15ms
step:952/2315 train_time:57258ms step_avg:60.15ms
step:953/2315 train_time:57319ms step_avg:60.15ms
step:954/2315 train_time:57380ms step_avg:60.15ms
step:955/2315 train_time:57440ms step_avg:60.15ms
step:956/2315 train_time:57501ms step_avg:60.15ms
step:957/2315 train_time:57562ms step_avg:60.15ms
step:958/2315 train_time:57623ms step_avg:60.15ms
step:959/2315 train_time:57684ms step_avg:60.15ms
step:960/2315 train_time:57745ms step_avg:60.15ms
step:961/2315 train_time:57806ms step_avg:60.15ms
step:962/2315 train_time:57866ms step_avg:60.15ms
step:963/2315 train_time:57927ms step_avg:60.15ms
step:964/2315 train_time:57988ms step_avg:60.15ms
step:965/2315 train_time:58049ms step_avg:60.15ms
step:966/2315 train_time:58110ms step_avg:60.15ms
step:967/2315 train_time:58171ms step_avg:60.16ms
step:968/2315 train_time:58232ms step_avg:60.16ms
step:969/2315 train_time:58293ms step_avg:60.16ms
step:970/2315 train_time:58353ms step_avg:60.16ms
step:971/2315 train_time:58414ms step_avg:60.16ms
step:972/2315 train_time:58474ms step_avg:60.16ms
step:973/2315 train_time:58535ms step_avg:60.16ms
step:974/2315 train_time:58595ms step_avg:60.16ms
step:975/2315 train_time:58655ms step_avg:60.16ms
step:976/2315 train_time:58715ms step_avg:60.16ms
step:977/2315 train_time:58775ms step_avg:60.16ms
step:978/2315 train_time:58835ms step_avg:60.16ms
step:979/2315 train_time:58895ms step_avg:60.16ms
step:980/2315 train_time:58955ms step_avg:60.16ms
step:981/2315 train_time:59016ms step_avg:60.16ms
step:982/2315 train_time:59076ms step_avg:60.16ms
step:983/2315 train_time:59136ms step_avg:60.16ms
step:984/2315 train_time:59197ms step_avg:60.16ms
step:985/2315 train_time:59258ms step_avg:60.16ms
step:986/2315 train_time:59318ms step_avg:60.16ms
step:987/2315 train_time:59378ms step_avg:60.16ms
step:988/2315 train_time:59438ms step_avg:60.16ms
step:989/2315 train_time:59499ms step_avg:60.16ms
step:990/2315 train_time:59560ms step_avg:60.16ms
step:991/2315 train_time:59620ms step_avg:60.16ms
step:992/2315 train_time:59681ms step_avg:60.16ms
step:993/2315 train_time:59742ms step_avg:60.16ms
step:994/2315 train_time:59802ms step_avg:60.16ms
step:995/2315 train_time:59863ms step_avg:60.16ms
step:996/2315 train_time:59924ms step_avg:60.17ms
step:997/2315 train_time:59985ms step_avg:60.17ms
step:998/2315 train_time:60046ms step_avg:60.17ms
step:999/2315 train_time:60107ms step_avg:60.17ms
step:1000/2315 train_time:60168ms step_avg:60.17ms
step:1000/2315 val_loss:3.5729 train_time:60231ms step_avg:60.23ms
step:1001/2315 train_time:60251ms step_avg:60.19ms
step:1002/2315 train_time:60291ms step_avg:60.17ms
step:1003/2315 train_time:60358ms step_avg:60.18ms
step:1004/2315 train_time:60426ms step_avg:60.18ms
step:1005/2315 train_time:60487ms step_avg:60.19ms
step:1006/2315 train_time:60547ms step_avg:60.19ms
step:1007/2315 train_time:60607ms step_avg:60.19ms
step:1008/2315 train_time:60667ms step_avg:60.19ms
step:1009/2315 train_time:60728ms step_avg:60.19ms
step:1010/2315 train_time:60788ms step_avg:60.19ms
step:1011/2315 train_time:60848ms step_avg:60.19ms
step:1012/2315 train_time:60908ms step_avg:60.19ms
step:1013/2315 train_time:60969ms step_avg:60.19ms
step:1014/2315 train_time:61029ms step_avg:60.19ms
step:1015/2315 train_time:61090ms step_avg:60.19ms
step:1016/2315 train_time:61150ms step_avg:60.19ms
step:1017/2315 train_time:61211ms step_avg:60.19ms
step:1018/2315 train_time:61273ms step_avg:60.19ms
step:1019/2315 train_time:61337ms step_avg:60.19ms
step:1020/2315 train_time:61399ms step_avg:60.20ms
step:1021/2315 train_time:61461ms step_avg:60.20ms
step:1022/2315 train_time:61521ms step_avg:60.20ms
step:1023/2315 train_time:61581ms step_avg:60.20ms
step:1024/2315 train_time:61641ms step_avg:60.20ms
step:1025/2315 train_time:61702ms step_avg:60.20ms
step:1026/2315 train_time:61763ms step_avg:60.20ms
step:1027/2315 train_time:61823ms step_avg:60.20ms
step:1028/2315 train_time:61883ms step_avg:60.20ms
step:1029/2315 train_time:61943ms step_avg:60.20ms
step:1030/2315 train_time:62004ms step_avg:60.20ms
step:1031/2315 train_time:62064ms step_avg:60.20ms
step:1032/2315 train_time:62125ms step_avg:60.20ms
step:1033/2315 train_time:62186ms step_avg:60.20ms
step:1034/2315 train_time:62247ms step_avg:60.20ms
step:1035/2315 train_time:62309ms step_avg:60.20ms
step:1036/2315 train_time:62371ms step_avg:60.20ms
step:1037/2315 train_time:62432ms step_avg:60.20ms
step:1038/2315 train_time:62494ms step_avg:60.21ms
step:1039/2315 train_time:62555ms step_avg:60.21ms
step:1040/2315 train_time:62615ms step_avg:60.21ms
step:1041/2315 train_time:62676ms step_avg:60.21ms
step:1042/2315 train_time:62737ms step_avg:60.21ms
step:1043/2315 train_time:62797ms step_avg:60.21ms
step:1044/2315 train_time:62858ms step_avg:60.21ms
step:1045/2315 train_time:62918ms step_avg:60.21ms
step:1046/2315 train_time:62978ms step_avg:60.21ms
step:1047/2315 train_time:63039ms step_avg:60.21ms
step:1048/2315 train_time:63099ms step_avg:60.21ms
step:1049/2315 train_time:63160ms step_avg:60.21ms
step:1050/2315 train_time:63220ms step_avg:60.21ms
step:1051/2315 train_time:63281ms step_avg:60.21ms
step:1052/2315 train_time:63341ms step_avg:60.21ms
step:1053/2315 train_time:63403ms step_avg:60.21ms
step:1054/2315 train_time:63464ms step_avg:60.21ms
step:1055/2315 train_time:63526ms step_avg:60.21ms
step:1056/2315 train_time:63587ms step_avg:60.22ms
step:1057/2315 train_time:63648ms step_avg:60.22ms
step:1058/2315 train_time:63710ms step_avg:60.22ms
step:1059/2315 train_time:63771ms step_avg:60.22ms
step:1060/2315 train_time:63832ms step_avg:60.22ms
step:1061/2315 train_time:63893ms step_avg:60.22ms
step:1062/2315 train_time:63953ms step_avg:60.22ms
step:1063/2315 train_time:64014ms step_avg:60.22ms
step:1064/2315 train_time:64075ms step_avg:60.22ms
step:1065/2315 train_time:64136ms step_avg:60.22ms
step:1066/2315 train_time:64197ms step_avg:60.22ms
step:1067/2315 train_time:64258ms step_avg:60.22ms
step:1068/2315 train_time:64318ms step_avg:60.22ms
step:1069/2315 train_time:64379ms step_avg:60.22ms
step:1070/2315 train_time:64439ms step_avg:60.22ms
step:1071/2315 train_time:64500ms step_avg:60.22ms
step:1072/2315 train_time:64560ms step_avg:60.22ms
step:1073/2315 train_time:64620ms step_avg:60.22ms
step:1074/2315 train_time:64681ms step_avg:60.22ms
step:1075/2315 train_time:64742ms step_avg:60.22ms
step:1076/2315 train_time:64802ms step_avg:60.23ms
step:1077/2315 train_time:64863ms step_avg:60.23ms
step:1078/2315 train_time:64924ms step_avg:60.23ms
step:1079/2315 train_time:64984ms step_avg:60.23ms
step:1080/2315 train_time:65045ms step_avg:60.23ms
step:1081/2315 train_time:65106ms step_avg:60.23ms
step:1082/2315 train_time:65167ms step_avg:60.23ms
step:1083/2315 train_time:65229ms step_avg:60.23ms
step:1084/2315 train_time:65289ms step_avg:60.23ms
step:1085/2315 train_time:65350ms step_avg:60.23ms
step:1086/2315 train_time:65411ms step_avg:60.23ms
step:1087/2315 train_time:65473ms step_avg:60.23ms
step:1088/2315 train_time:65534ms step_avg:60.23ms
step:1089/2315 train_time:65595ms step_avg:60.23ms
step:1090/2315 train_time:65655ms step_avg:60.23ms
step:1091/2315 train_time:65716ms step_avg:60.24ms
step:1092/2315 train_time:65777ms step_avg:60.24ms
step:1093/2315 train_time:65838ms step_avg:60.24ms
step:1094/2315 train_time:65898ms step_avg:60.24ms
step:1095/2315 train_time:65959ms step_avg:60.24ms
step:1096/2315 train_time:66019ms step_avg:60.24ms
step:1097/2315 train_time:66080ms step_avg:60.24ms
step:1098/2315 train_time:66140ms step_avg:60.24ms
step:1099/2315 train_time:66200ms step_avg:60.24ms
step:1100/2315 train_time:66260ms step_avg:60.24ms
step:1101/2315 train_time:66321ms step_avg:60.24ms
step:1102/2315 train_time:66381ms step_avg:60.24ms
step:1103/2315 train_time:66442ms step_avg:60.24ms
step:1104/2315 train_time:66503ms step_avg:60.24ms
step:1105/2315 train_time:66564ms step_avg:60.24ms
step:1106/2315 train_time:66625ms step_avg:60.24ms
step:1107/2315 train_time:66687ms step_avg:60.24ms
step:1108/2315 train_time:66747ms step_avg:60.24ms
step:1109/2315 train_time:66809ms step_avg:60.24ms
step:1110/2315 train_time:66870ms step_avg:60.24ms
step:1111/2315 train_time:66930ms step_avg:60.24ms
step:1112/2315 train_time:66991ms step_avg:60.24ms
step:1113/2315 train_time:67052ms step_avg:60.24ms
step:1114/2315 train_time:67113ms step_avg:60.24ms
step:1115/2315 train_time:67174ms step_avg:60.25ms
step:1116/2315 train_time:67234ms step_avg:60.25ms
step:1117/2315 train_time:67295ms step_avg:60.25ms
step:1118/2315 train_time:67356ms step_avg:60.25ms
step:1119/2315 train_time:67417ms step_avg:60.25ms
step:1120/2315 train_time:67478ms step_avg:60.25ms
step:1121/2315 train_time:67539ms step_avg:60.25ms
step:1122/2315 train_time:67599ms step_avg:60.25ms
step:1123/2315 train_time:67659ms step_avg:60.25ms
step:1124/2315 train_time:67720ms step_avg:60.25ms
step:1125/2315 train_time:67780ms step_avg:60.25ms
step:1126/2315 train_time:67840ms step_avg:60.25ms
step:1127/2315 train_time:67901ms step_avg:60.25ms
step:1128/2315 train_time:67961ms step_avg:60.25ms
step:1129/2315 train_time:68022ms step_avg:60.25ms
step:1130/2315 train_time:68082ms step_avg:60.25ms
step:1131/2315 train_time:68143ms step_avg:60.25ms
step:1132/2315 train_time:68204ms step_avg:60.25ms
step:1133/2315 train_time:68266ms step_avg:60.25ms
step:1134/2315 train_time:68327ms step_avg:60.25ms
step:1135/2315 train_time:68388ms step_avg:60.25ms
step:1136/2315 train_time:68449ms step_avg:60.25ms
step:1137/2315 train_time:68510ms step_avg:60.25ms
step:1138/2315 train_time:68570ms step_avg:60.26ms
step:1139/2315 train_time:68631ms step_avg:60.26ms
step:1140/2315 train_time:68692ms step_avg:60.26ms
step:1141/2315 train_time:68753ms step_avg:60.26ms
step:1142/2315 train_time:68814ms step_avg:60.26ms
step:1143/2315 train_time:68875ms step_avg:60.26ms
step:1144/2315 train_time:68936ms step_avg:60.26ms
step:1145/2315 train_time:68996ms step_avg:60.26ms
step:1146/2315 train_time:69057ms step_avg:60.26ms
step:1147/2315 train_time:69118ms step_avg:60.26ms
step:1148/2315 train_time:69179ms step_avg:60.26ms
step:1149/2315 train_time:69239ms step_avg:60.26ms
step:1150/2315 train_time:69300ms step_avg:60.26ms
step:1151/2315 train_time:69360ms step_avg:60.26ms
step:1152/2315 train_time:69420ms step_avg:60.26ms
step:1153/2315 train_time:69480ms step_avg:60.26ms
step:1154/2315 train_time:69541ms step_avg:60.26ms
step:1155/2315 train_time:69601ms step_avg:60.26ms
step:1156/2315 train_time:69661ms step_avg:60.26ms
step:1157/2315 train_time:69722ms step_avg:60.26ms
step:1158/2315 train_time:69783ms step_avg:60.26ms
step:1159/2315 train_time:69844ms step_avg:60.26ms
step:1160/2315 train_time:69905ms step_avg:60.26ms
step:1161/2315 train_time:69967ms step_avg:60.26ms
step:1162/2315 train_time:70027ms step_avg:60.26ms
step:1163/2315 train_time:70089ms step_avg:60.27ms
step:1164/2315 train_time:70149ms step_avg:60.27ms
step:1165/2315 train_time:70211ms step_avg:60.27ms
step:1166/2315 train_time:70271ms step_avg:60.27ms
step:1167/2315 train_time:70332ms step_avg:60.27ms
step:1168/2315 train_time:70392ms step_avg:60.27ms
step:1169/2315 train_time:70454ms step_avg:60.27ms
step:1170/2315 train_time:70515ms step_avg:60.27ms
step:1171/2315 train_time:70576ms step_avg:60.27ms
step:1172/2315 train_time:70636ms step_avg:60.27ms
step:1173/2315 train_time:70697ms step_avg:60.27ms
step:1174/2315 train_time:70758ms step_avg:60.27ms
step:1175/2315 train_time:70818ms step_avg:60.27ms
step:1176/2315 train_time:70879ms step_avg:60.27ms
step:1177/2315 train_time:70939ms step_avg:60.27ms
step:1178/2315 train_time:71000ms step_avg:60.27ms
step:1179/2315 train_time:71060ms step_avg:60.27ms
step:1180/2315 train_time:71121ms step_avg:60.27ms
step:1181/2315 train_time:71182ms step_avg:60.27ms
step:1182/2315 train_time:71242ms step_avg:60.27ms
step:1183/2315 train_time:71303ms step_avg:60.27ms
step:1184/2315 train_time:71363ms step_avg:60.27ms
step:1185/2315 train_time:71425ms step_avg:60.27ms
step:1186/2315 train_time:71485ms step_avg:60.27ms
step:1187/2315 train_time:71547ms step_avg:60.28ms
step:1188/2315 train_time:71608ms step_avg:60.28ms
step:1189/2315 train_time:71669ms step_avg:60.28ms
step:1190/2315 train_time:71730ms step_avg:60.28ms
step:1191/2315 train_time:71791ms step_avg:60.28ms
step:1192/2315 train_time:71852ms step_avg:60.28ms
step:1193/2315 train_time:71913ms step_avg:60.28ms
step:1194/2315 train_time:71973ms step_avg:60.28ms
step:1195/2315 train_time:72034ms step_avg:60.28ms
step:1196/2315 train_time:72095ms step_avg:60.28ms
step:1197/2315 train_time:72156ms step_avg:60.28ms
step:1198/2315 train_time:72217ms step_avg:60.28ms
step:1199/2315 train_time:72278ms step_avg:60.28ms
step:1200/2315 train_time:72338ms step_avg:60.28ms
step:1201/2315 train_time:72399ms step_avg:60.28ms
step:1202/2315 train_time:72459ms step_avg:60.28ms
step:1203/2315 train_time:72520ms step_avg:60.28ms
step:1204/2315 train_time:72580ms step_avg:60.28ms
step:1205/2315 train_time:72640ms step_avg:60.28ms
step:1206/2315 train_time:72701ms step_avg:60.28ms
step:1207/2315 train_time:72762ms step_avg:60.28ms
step:1208/2315 train_time:72822ms step_avg:60.28ms
step:1209/2315 train_time:72883ms step_avg:60.28ms
step:1210/2315 train_time:72944ms step_avg:60.28ms
step:1211/2315 train_time:73005ms step_avg:60.29ms
step:1212/2315 train_time:73067ms step_avg:60.29ms
step:1213/2315 train_time:73128ms step_avg:60.29ms
step:1214/2315 train_time:73189ms step_avg:60.29ms
step:1215/2315 train_time:73250ms step_avg:60.29ms
step:1216/2315 train_time:73311ms step_avg:60.29ms
step:1217/2315 train_time:73372ms step_avg:60.29ms
step:1218/2315 train_time:73433ms step_avg:60.29ms
step:1219/2315 train_time:73493ms step_avg:60.29ms
step:1220/2315 train_time:73554ms step_avg:60.29ms
step:1221/2315 train_time:73615ms step_avg:60.29ms
step:1222/2315 train_time:73676ms step_avg:60.29ms
step:1223/2315 train_time:73736ms step_avg:60.29ms
step:1224/2315 train_time:73797ms step_avg:60.29ms
step:1225/2315 train_time:73858ms step_avg:60.29ms
step:1226/2315 train_time:73919ms step_avg:60.29ms
step:1227/2315 train_time:73980ms step_avg:60.29ms
step:1228/2315 train_time:74040ms step_avg:60.29ms
step:1229/2315 train_time:74100ms step_avg:60.29ms
step:1230/2315 train_time:74160ms step_avg:60.29ms
step:1231/2315 train_time:74220ms step_avg:60.29ms
step:1232/2315 train_time:74281ms step_avg:60.29ms
step:1233/2315 train_time:74342ms step_avg:60.29ms
step:1234/2315 train_time:74403ms step_avg:60.29ms
step:1235/2315 train_time:74464ms step_avg:60.29ms
step:1236/2315 train_time:74524ms step_avg:60.29ms
step:1237/2315 train_time:74585ms step_avg:60.30ms
step:1238/2315 train_time:74646ms step_avg:60.30ms
step:1239/2315 train_time:74707ms step_avg:60.30ms
step:1240/2315 train_time:74768ms step_avg:60.30ms
step:1241/2315 train_time:74829ms step_avg:60.30ms
step:1242/2315 train_time:74890ms step_avg:60.30ms
step:1243/2315 train_time:74951ms step_avg:60.30ms
step:1244/2315 train_time:75012ms step_avg:60.30ms
step:1245/2315 train_time:75074ms step_avg:60.30ms
step:1246/2315 train_time:75135ms step_avg:60.30ms
step:1247/2315 train_time:75196ms step_avg:60.30ms
step:1248/2315 train_time:75257ms step_avg:60.30ms
step:1249/2315 train_time:75318ms step_avg:60.30ms
step:1250/2315 train_time:75378ms step_avg:60.30ms
step:1250/2315 val_loss:3.5153 train_time:75441ms step_avg:60.35ms
step:1251/2315 train_time:75460ms step_avg:60.32ms
step:1252/2315 train_time:75503ms step_avg:60.31ms
step:1253/2315 train_time:75567ms step_avg:60.31ms
step:1254/2315 train_time:75632ms step_avg:60.31ms
step:1255/2315 train_time:75693ms step_avg:60.31ms
step:1256/2315 train_time:75753ms step_avg:60.31ms
step:1257/2315 train_time:75813ms step_avg:60.31ms
step:1258/2315 train_time:75873ms step_avg:60.31ms
step:1259/2315 train_time:75933ms step_avg:60.31ms
step:1260/2315 train_time:75993ms step_avg:60.31ms
step:1261/2315 train_time:76052ms step_avg:60.31ms
step:1262/2315 train_time:76112ms step_avg:60.31ms
step:1263/2315 train_time:76172ms step_avg:60.31ms
step:1264/2315 train_time:76232ms step_avg:60.31ms
step:1265/2315 train_time:76292ms step_avg:60.31ms
step:1266/2315 train_time:76352ms step_avg:60.31ms
step:1267/2315 train_time:76413ms step_avg:60.31ms
step:1268/2315 train_time:76474ms step_avg:60.31ms
step:1269/2315 train_time:76535ms step_avg:60.31ms
step:1270/2315 train_time:76597ms step_avg:60.31ms
step:1271/2315 train_time:76657ms step_avg:60.31ms
step:1272/2315 train_time:76719ms step_avg:60.31ms
step:1273/2315 train_time:76780ms step_avg:60.31ms
step:1274/2315 train_time:76840ms step_avg:60.31ms
step:1275/2315 train_time:76901ms step_avg:60.31ms
step:1276/2315 train_time:76961ms step_avg:60.31ms
step:1277/2315 train_time:77022ms step_avg:60.31ms
step:1278/2315 train_time:77082ms step_avg:60.31ms
step:1279/2315 train_time:77143ms step_avg:60.31ms
step:1280/2315 train_time:77203ms step_avg:60.31ms
step:1281/2315 train_time:77264ms step_avg:60.32ms
step:1282/2315 train_time:77325ms step_avg:60.32ms
step:1283/2315 train_time:77385ms step_avg:60.32ms
step:1284/2315 train_time:77446ms step_avg:60.32ms
step:1285/2315 train_time:77508ms step_avg:60.32ms
step:1286/2315 train_time:77569ms step_avg:60.32ms
step:1287/2315 train_time:77630ms step_avg:60.32ms
step:1288/2315 train_time:77692ms step_avg:60.32ms
step:1289/2315 train_time:77753ms step_avg:60.32ms
step:1290/2315 train_time:77813ms step_avg:60.32ms
step:1291/2315 train_time:77874ms step_avg:60.32ms
step:1292/2315 train_time:77934ms step_avg:60.32ms
step:1293/2315 train_time:77995ms step_avg:60.32ms
step:1294/2315 train_time:78055ms step_avg:60.32ms
step:1295/2315 train_time:78114ms step_avg:60.32ms
step:1296/2315 train_time:78174ms step_avg:60.32ms
step:1297/2315 train_time:78234ms step_avg:60.32ms
step:1298/2315 train_time:78294ms step_avg:60.32ms
step:1299/2315 train_time:78354ms step_avg:60.32ms
step:1300/2315 train_time:78415ms step_avg:60.32ms
step:1301/2315 train_time:78476ms step_avg:60.32ms
step:1302/2315 train_time:78536ms step_avg:60.32ms
step:1303/2315 train_time:78598ms step_avg:60.32ms
step:1304/2315 train_time:78658ms step_avg:60.32ms
step:1305/2315 train_time:78719ms step_avg:60.32ms
step:1306/2315 train_time:78780ms step_avg:60.32ms
step:1307/2315 train_time:78841ms step_avg:60.32ms
step:1308/2315 train_time:78902ms step_avg:60.32ms
step:1309/2315 train_time:78962ms step_avg:60.32ms
step:1310/2315 train_time:79023ms step_avg:60.32ms
step:1311/2315 train_time:79083ms step_avg:60.32ms
step:1312/2315 train_time:79144ms step_avg:60.32ms
step:1313/2315 train_time:79205ms step_avg:60.32ms
step:1314/2315 train_time:79265ms step_avg:60.32ms
step:1315/2315 train_time:79326ms step_avg:60.32ms
step:1316/2315 train_time:79386ms step_avg:60.32ms
step:1317/2315 train_time:79448ms step_avg:60.32ms
step:1318/2315 train_time:79509ms step_avg:60.33ms
step:1319/2315 train_time:79570ms step_avg:60.33ms
step:1320/2315 train_time:79630ms step_avg:60.33ms
step:1321/2315 train_time:79691ms step_avg:60.33ms
step:1322/2315 train_time:79752ms step_avg:60.33ms
step:1323/2315 train_time:79813ms step_avg:60.33ms
step:1324/2315 train_time:79873ms step_avg:60.33ms
step:1325/2315 train_time:79934ms step_avg:60.33ms
step:1326/2315 train_time:79995ms step_avg:60.33ms
step:1327/2315 train_time:80054ms step_avg:60.33ms
step:1328/2315 train_time:80115ms step_avg:60.33ms
step:1329/2315 train_time:80175ms step_avg:60.33ms
step:1330/2315 train_time:80235ms step_avg:60.33ms
step:1331/2315 train_time:80296ms step_avg:60.33ms
step:1332/2315 train_time:80357ms step_avg:60.33ms
step:1333/2315 train_time:80417ms step_avg:60.33ms
step:1334/2315 train_time:80478ms step_avg:60.33ms
step:1335/2315 train_time:80539ms step_avg:60.33ms
step:1336/2315 train_time:80601ms step_avg:60.33ms
step:1337/2315 train_time:80663ms step_avg:60.33ms
step:1338/2315 train_time:80724ms step_avg:60.33ms
step:1339/2315 train_time:80786ms step_avg:60.33ms
step:1340/2315 train_time:80846ms step_avg:60.33ms
step:1341/2315 train_time:80907ms step_avg:60.33ms
step:1342/2315 train_time:80967ms step_avg:60.33ms
step:1343/2315 train_time:81029ms step_avg:60.33ms
step:1344/2315 train_time:81089ms step_avg:60.33ms
step:1345/2315 train_time:81150ms step_avg:60.33ms
step:1346/2315 train_time:81211ms step_avg:60.33ms
step:1347/2315 train_time:81272ms step_avg:60.34ms
step:1348/2315 train_time:81332ms step_avg:60.34ms
step:1349/2315 train_time:81393ms step_avg:60.34ms
step:1350/2315 train_time:81453ms step_avg:60.34ms
step:1351/2315 train_time:81514ms step_avg:60.34ms
step:1352/2315 train_time:81574ms step_avg:60.34ms
step:1353/2315 train_time:81635ms step_avg:60.34ms
step:1354/2315 train_time:81695ms step_avg:60.34ms
step:1355/2315 train_time:81756ms step_avg:60.34ms
step:1356/2315 train_time:81816ms step_avg:60.34ms
step:1357/2315 train_time:81877ms step_avg:60.34ms
step:1358/2315 train_time:81938ms step_avg:60.34ms
step:1359/2315 train_time:82000ms step_avg:60.34ms
step:1360/2315 train_time:82060ms step_avg:60.34ms
step:1361/2315 train_time:82122ms step_avg:60.34ms
step:1362/2315 train_time:82182ms step_avg:60.34ms
step:1363/2315 train_time:82243ms step_avg:60.34ms
step:1364/2315 train_time:82304ms step_avg:60.34ms
step:1365/2315 train_time:82365ms step_avg:60.34ms
step:1366/2315 train_time:82426ms step_avg:60.34ms
step:1367/2315 train_time:82487ms step_avg:60.34ms
step:1368/2315 train_time:82548ms step_avg:60.34ms
step:1369/2315 train_time:82609ms step_avg:60.34ms
step:1370/2315 train_time:82670ms step_avg:60.34ms
step:1371/2315 train_time:82731ms step_avg:60.34ms
step:1372/2315 train_time:82792ms step_avg:60.34ms
step:1373/2315 train_time:82853ms step_avg:60.34ms
step:1374/2315 train_time:82913ms step_avg:60.34ms
step:1375/2315 train_time:82974ms step_avg:60.34ms
step:1376/2315 train_time:83035ms step_avg:60.35ms
step:1377/2315 train_time:83095ms step_avg:60.35ms
step:1378/2315 train_time:83156ms step_avg:60.35ms
step:1379/2315 train_time:83216ms step_avg:60.35ms
step:1380/2315 train_time:83276ms step_avg:60.34ms
step:1381/2315 train_time:83337ms step_avg:60.35ms
step:1382/2315 train_time:83397ms step_avg:60.35ms
step:1383/2315 train_time:83458ms step_avg:60.35ms
step:1384/2315 train_time:83519ms step_avg:60.35ms
step:1385/2315 train_time:83579ms step_avg:60.35ms
step:1386/2315 train_time:83641ms step_avg:60.35ms
step:1387/2315 train_time:83702ms step_avg:60.35ms
step:1388/2315 train_time:83763ms step_avg:60.35ms
step:1389/2315 train_time:83824ms step_avg:60.35ms
step:1390/2315 train_time:83884ms step_avg:60.35ms
step:1391/2315 train_time:83945ms step_avg:60.35ms
step:1392/2315 train_time:84006ms step_avg:60.35ms
step:1393/2315 train_time:84067ms step_avg:60.35ms
step:1394/2315 train_time:84128ms step_avg:60.35ms
step:1395/2315 train_time:84189ms step_avg:60.35ms
step:1396/2315 train_time:84249ms step_avg:60.35ms
step:1397/2315 train_time:84311ms step_avg:60.35ms
step:1398/2315 train_time:84371ms step_avg:60.35ms
step:1399/2315 train_time:84432ms step_avg:60.35ms
step:1400/2315 train_time:84492ms step_avg:60.35ms
step:1401/2315 train_time:84553ms step_avg:60.35ms
step:1402/2315 train_time:84613ms step_avg:60.35ms
step:1403/2315 train_time:84673ms step_avg:60.35ms
step:1404/2315 train_time:84734ms step_avg:60.35ms
step:1405/2315 train_time:84794ms step_avg:60.35ms
step:1406/2315 train_time:84854ms step_avg:60.35ms
step:1407/2315 train_time:84914ms step_avg:60.35ms
step:1408/2315 train_time:84974ms step_avg:60.35ms
step:1409/2315 train_time:85034ms step_avg:60.35ms
step:1410/2315 train_time:85095ms step_avg:60.35ms
step:1411/2315 train_time:85155ms step_avg:60.35ms
step:1412/2315 train_time:85216ms step_avg:60.35ms
step:1413/2315 train_time:85277ms step_avg:60.35ms
step:1414/2315 train_time:85338ms step_avg:60.35ms
step:1415/2315 train_time:85399ms step_avg:60.35ms
step:1416/2315 train_time:85460ms step_avg:60.35ms
step:1417/2315 train_time:85520ms step_avg:60.35ms
step:1418/2315 train_time:85580ms step_avg:60.35ms
step:1419/2315 train_time:85641ms step_avg:60.35ms
step:1420/2315 train_time:85702ms step_avg:60.35ms
step:1421/2315 train_time:85762ms step_avg:60.35ms
step:1422/2315 train_time:85823ms step_avg:60.35ms
step:1423/2315 train_time:85884ms step_avg:60.35ms
step:1424/2315 train_time:85945ms step_avg:60.35ms
step:1425/2315 train_time:86006ms step_avg:60.36ms
step:1426/2315 train_time:86067ms step_avg:60.36ms
step:1427/2315 train_time:86129ms step_avg:60.36ms
step:1428/2315 train_time:86189ms step_avg:60.36ms
step:1429/2315 train_time:86251ms step_avg:60.36ms
step:1430/2315 train_time:86312ms step_avg:60.36ms
step:1431/2315 train_time:86373ms step_avg:60.36ms
step:1432/2315 train_time:86434ms step_avg:60.36ms
step:1433/2315 train_time:86494ms step_avg:60.36ms
step:1434/2315 train_time:86555ms step_avg:60.36ms
step:1435/2315 train_time:86615ms step_avg:60.36ms
step:1436/2315 train_time:86675ms step_avg:60.36ms
step:1437/2315 train_time:86735ms step_avg:60.36ms
step:1438/2315 train_time:86795ms step_avg:60.36ms
step:1439/2315 train_time:86856ms step_avg:60.36ms
step:1440/2315 train_time:86917ms step_avg:60.36ms
step:1441/2315 train_time:86977ms step_avg:60.36ms
step:1442/2315 train_time:87039ms step_avg:60.36ms
step:1443/2315 train_time:87100ms step_avg:60.36ms
step:1444/2315 train_time:87161ms step_avg:60.36ms
step:1445/2315 train_time:87223ms step_avg:60.36ms
step:1446/2315 train_time:87284ms step_avg:60.36ms
step:1447/2315 train_time:87345ms step_avg:60.36ms
step:1448/2315 train_time:87405ms step_avg:60.36ms
step:1449/2315 train_time:87466ms step_avg:60.36ms
step:1450/2315 train_time:87526ms step_avg:60.36ms
step:1451/2315 train_time:87587ms step_avg:60.36ms
step:1452/2315 train_time:87648ms step_avg:60.36ms
step:1453/2315 train_time:87709ms step_avg:60.36ms
step:1454/2315 train_time:87769ms step_avg:60.36ms
step:1455/2315 train_time:87831ms step_avg:60.36ms
step:1456/2315 train_time:87891ms step_avg:60.36ms
step:1457/2315 train_time:87952ms step_avg:60.37ms
step:1458/2315 train_time:88012ms step_avg:60.37ms
step:1459/2315 train_time:88073ms step_avg:60.37ms
step:1460/2315 train_time:88133ms step_avg:60.37ms
step:1461/2315 train_time:88194ms step_avg:60.37ms
step:1462/2315 train_time:88254ms step_avg:60.36ms
step:1463/2315 train_time:88314ms step_avg:60.36ms
step:1464/2315 train_time:88374ms step_avg:60.36ms
step:1465/2315 train_time:88434ms step_avg:60.36ms
step:1466/2315 train_time:88494ms step_avg:60.36ms
step:1467/2315 train_time:88554ms step_avg:60.36ms
step:1468/2315 train_time:88615ms step_avg:60.36ms
step:1469/2315 train_time:88675ms step_avg:60.36ms
step:1470/2315 train_time:88736ms step_avg:60.36ms
step:1471/2315 train_time:88797ms step_avg:60.36ms
step:1472/2315 train_time:88858ms step_avg:60.37ms
step:1473/2315 train_time:88919ms step_avg:60.37ms
step:1474/2315 train_time:88980ms step_avg:60.37ms
step:1475/2315 train_time:89041ms step_avg:60.37ms
step:1476/2315 train_time:89102ms step_avg:60.37ms
step:1477/2315 train_time:89162ms step_avg:60.37ms
step:1478/2315 train_time:89224ms step_avg:60.37ms
step:1479/2315 train_time:89284ms step_avg:60.37ms
step:1480/2315 train_time:89345ms step_avg:60.37ms
step:1481/2315 train_time:89406ms step_avg:60.37ms
step:1482/2315 train_time:89466ms step_avg:60.37ms
step:1483/2315 train_time:89527ms step_avg:60.37ms
step:1484/2315 train_time:89587ms step_avg:60.37ms
step:1485/2315 train_time:89649ms step_avg:60.37ms
step:1486/2315 train_time:89709ms step_avg:60.37ms
step:1487/2315 train_time:89770ms step_avg:60.37ms
step:1488/2315 train_time:89831ms step_avg:60.37ms
step:1489/2315 train_time:89892ms step_avg:60.37ms
step:1490/2315 train_time:89953ms step_avg:60.37ms
step:1491/2315 train_time:90014ms step_avg:60.37ms
step:1492/2315 train_time:90074ms step_avg:60.37ms
step:1493/2315 train_time:90135ms step_avg:60.37ms
step:1494/2315 train_time:90194ms step_avg:60.37ms
step:1495/2315 train_time:90255ms step_avg:60.37ms
step:1496/2315 train_time:90315ms step_avg:60.37ms
step:1497/2315 train_time:90375ms step_avg:60.37ms
step:1498/2315 train_time:90435ms step_avg:60.37ms
step:1499/2315 train_time:90496ms step_avg:60.37ms
step:1500/2315 train_time:90557ms step_avg:60.37ms
step:1500/2315 val_loss:3.4506 train_time:90620ms step_avg:60.41ms
step:1501/2315 train_time:90639ms step_avg:60.39ms
step:1502/2315 train_time:90681ms step_avg:60.37ms
step:1503/2315 train_time:90749ms step_avg:60.38ms
step:1504/2315 train_time:90814ms step_avg:60.38ms
step:1505/2315 train_time:90874ms step_avg:60.38ms
step:1506/2315 train_time:90935ms step_avg:60.38ms
step:1507/2315 train_time:90995ms step_avg:60.38ms
step:1508/2315 train_time:91055ms step_avg:60.38ms
step:1509/2315 train_time:91115ms step_avg:60.38ms
step:1510/2315 train_time:91175ms step_avg:60.38ms
step:1511/2315 train_time:91235ms step_avg:60.38ms
step:1512/2315 train_time:91296ms step_avg:60.38ms
step:1513/2315 train_time:91356ms step_avg:60.38ms
step:1514/2315 train_time:91417ms step_avg:60.38ms
step:1515/2315 train_time:91477ms step_avg:60.38ms
step:1516/2315 train_time:91538ms step_avg:60.38ms
step:1517/2315 train_time:91599ms step_avg:60.38ms
step:1518/2315 train_time:91661ms step_avg:60.38ms
step:1519/2315 train_time:91725ms step_avg:60.39ms
step:1520/2315 train_time:91787ms step_avg:60.39ms
step:1521/2315 train_time:91850ms step_avg:60.39ms
step:1522/2315 train_time:91911ms step_avg:60.39ms
step:1523/2315 train_time:91972ms step_avg:60.39ms
step:1524/2315 train_time:92033ms step_avg:60.39ms
step:1525/2315 train_time:92093ms step_avg:60.39ms
step:1526/2315 train_time:92154ms step_avg:60.39ms
step:1527/2315 train_time:92214ms step_avg:60.39ms
step:1528/2315 train_time:92275ms step_avg:60.39ms
step:1529/2315 train_time:92336ms step_avg:60.39ms
step:1530/2315 train_time:92396ms step_avg:60.39ms
step:1531/2315 train_time:92457ms step_avg:60.39ms
step:1532/2315 train_time:92519ms step_avg:60.39ms
step:1533/2315 train_time:92581ms step_avg:60.39ms
step:1534/2315 train_time:92643ms step_avg:60.39ms
step:1535/2315 train_time:92705ms step_avg:60.39ms
step:1536/2315 train_time:92767ms step_avg:60.40ms
step:1537/2315 train_time:92829ms step_avg:60.40ms
step:1538/2315 train_time:92890ms step_avg:60.40ms
step:1539/2315 train_time:92951ms step_avg:60.40ms
step:1540/2315 train_time:93012ms step_avg:60.40ms
step:1541/2315 train_time:93073ms step_avg:60.40ms
step:1542/2315 train_time:93134ms step_avg:60.40ms
step:1543/2315 train_time:93195ms step_avg:60.40ms
step:1544/2315 train_time:93256ms step_avg:60.40ms
step:1545/2315 train_time:93317ms step_avg:60.40ms
step:1546/2315 train_time:93377ms step_avg:60.40ms
step:1547/2315 train_time:93438ms step_avg:60.40ms
step:1548/2315 train_time:93500ms step_avg:60.40ms
step:1549/2315 train_time:93562ms step_avg:60.40ms
step:1550/2315 train_time:93623ms step_avg:60.40ms
step:1551/2315 train_time:93685ms step_avg:60.40ms
step:1552/2315 train_time:93746ms step_avg:60.40ms
step:1553/2315 train_time:93808ms step_avg:60.40ms
step:1554/2315 train_time:93869ms step_avg:60.40ms
step:1555/2315 train_time:93930ms step_avg:60.40ms
step:1556/2315 train_time:93991ms step_avg:60.41ms
step:1557/2315 train_time:94052ms step_avg:60.41ms
step:1558/2315 train_time:94113ms step_avg:60.41ms
step:1559/2315 train_time:94173ms step_avg:60.41ms
step:1560/2315 train_time:94234ms step_avg:60.41ms
step:1561/2315 train_time:94295ms step_avg:60.41ms
step:1562/2315 train_time:94355ms step_avg:60.41ms
step:1563/2315 train_time:94416ms step_avg:60.41ms
step:1564/2315 train_time:94478ms step_avg:60.41ms
step:1565/2315 train_time:94540ms step_avg:60.41ms
step:1566/2315 train_time:94601ms step_avg:60.41ms
step:1567/2315 train_time:94662ms step_avg:60.41ms
step:1568/2315 train_time:94724ms step_avg:60.41ms
step:1569/2315 train_time:94786ms step_avg:60.41ms
step:1570/2315 train_time:94847ms step_avg:60.41ms
step:1571/2315 train_time:94908ms step_avg:60.41ms
step:1572/2315 train_time:94969ms step_avg:60.41ms
step:1573/2315 train_time:95031ms step_avg:60.41ms
step:1574/2315 train_time:95092ms step_avg:60.41ms
step:1575/2315 train_time:95153ms step_avg:60.41ms
step:1576/2315 train_time:95213ms step_avg:60.41ms
step:1577/2315 train_time:95274ms step_avg:60.41ms
step:1578/2315 train_time:95335ms step_avg:60.41ms
step:1579/2315 train_time:95396ms step_avg:60.42ms
step:1580/2315 train_time:95457ms step_avg:60.42ms
step:1581/2315 train_time:95518ms step_avg:60.42ms
step:1582/2315 train_time:95579ms step_avg:60.42ms
step:1583/2315 train_time:95642ms step_avg:60.42ms
step:1584/2315 train_time:95704ms step_avg:60.42ms
step:1585/2315 train_time:95765ms step_avg:60.42ms
step:1586/2315 train_time:95826ms step_avg:60.42ms
step:1587/2315 train_time:95888ms step_avg:60.42ms
step:1588/2315 train_time:95949ms step_avg:60.42ms
step:1589/2315 train_time:96010ms step_avg:60.42ms
step:1590/2315 train_time:96072ms step_avg:60.42ms
step:1591/2315 train_time:96132ms step_avg:60.42ms
step:1592/2315 train_time:96194ms step_avg:60.42ms
step:1593/2315 train_time:96254ms step_avg:60.42ms
step:1594/2315 train_time:96315ms step_avg:60.42ms
step:1595/2315 train_time:96376ms step_avg:60.42ms
step:1596/2315 train_time:96437ms step_avg:60.42ms
step:1597/2315 train_time:96499ms step_avg:60.43ms
step:1598/2315 train_time:96560ms step_avg:60.43ms
step:1599/2315 train_time:96623ms step_avg:60.43ms
step:1600/2315 train_time:96683ms step_avg:60.43ms
step:1601/2315 train_time:96745ms step_avg:60.43ms
step:1602/2315 train_time:96806ms step_avg:60.43ms
step:1603/2315 train_time:96867ms step_avg:60.43ms
step:1604/2315 train_time:96929ms step_avg:60.43ms
step:1605/2315 train_time:96990ms step_avg:60.43ms
step:1606/2315 train_time:97051ms step_avg:60.43ms
step:1607/2315 train_time:97112ms step_avg:60.43ms
step:1608/2315 train_time:97173ms step_avg:60.43ms
step:1609/2315 train_time:97234ms step_avg:60.43ms
step:1610/2315 train_time:97295ms step_avg:60.43ms
step:1611/2315 train_time:97356ms step_avg:60.43ms
step:1612/2315 train_time:97416ms step_avg:60.43ms
step:1613/2315 train_time:97478ms step_avg:60.43ms
step:1614/2315 train_time:97540ms step_avg:60.43ms
step:1615/2315 train_time:97601ms step_avg:60.43ms
step:1616/2315 train_time:97662ms step_avg:60.43ms
step:1617/2315 train_time:97724ms step_avg:60.44ms
step:1618/2315 train_time:97785ms step_avg:60.44ms
step:1619/2315 train_time:97847ms step_avg:60.44ms
step:1620/2315 train_time:97908ms step_avg:60.44ms
step:1621/2315 train_time:97969ms step_avg:60.44ms
step:1622/2315 train_time:98031ms step_avg:60.44ms
step:1623/2315 train_time:98092ms step_avg:60.44ms
step:1624/2315 train_time:98153ms step_avg:60.44ms
step:1625/2315 train_time:98213ms step_avg:60.44ms
step:1626/2315 train_time:98274ms step_avg:60.44ms
step:1627/2315 train_time:98335ms step_avg:60.44ms
step:1628/2315 train_time:98396ms step_avg:60.44ms
step:1629/2315 train_time:98457ms step_avg:60.44ms
step:1630/2315 train_time:98518ms step_avg:60.44ms
step:1631/2315 train_time:98580ms step_avg:60.44ms
step:1632/2315 train_time:98642ms step_avg:60.44ms
step:1633/2315 train_time:98703ms step_avg:60.44ms
step:1634/2315 train_time:98764ms step_avg:60.44ms
step:1635/2315 train_time:98826ms step_avg:60.44ms
step:1636/2315 train_time:98888ms step_avg:60.44ms
step:1637/2315 train_time:98949ms step_avg:60.45ms
step:1638/2315 train_time:99010ms step_avg:60.45ms
step:1639/2315 train_time:99072ms step_avg:60.45ms
step:1640/2315 train_time:99133ms step_avg:60.45ms
step:1641/2315 train_time:99194ms step_avg:60.45ms
step:1642/2315 train_time:99255ms step_avg:60.45ms
step:1643/2315 train_time:99316ms step_avg:60.45ms
step:1644/2315 train_time:99376ms step_avg:60.45ms
step:1645/2315 train_time:99438ms step_avg:60.45ms
step:1646/2315 train_time:99499ms step_avg:60.45ms
step:1647/2315 train_time:99561ms step_avg:60.45ms
step:1648/2315 train_time:99622ms step_avg:60.45ms
step:1649/2315 train_time:99683ms step_avg:60.45ms
step:1650/2315 train_time:99744ms step_avg:60.45ms
step:1651/2315 train_time:99806ms step_avg:60.45ms
step:1652/2315 train_time:99867ms step_avg:60.45ms
step:1653/2315 train_time:99929ms step_avg:60.45ms
step:1654/2315 train_time:99989ms step_avg:60.45ms
step:1655/2315 train_time:100050ms step_avg:60.45ms
step:1656/2315 train_time:100111ms step_avg:60.45ms
step:1657/2315 train_time:100172ms step_avg:60.45ms
step:1658/2315 train_time:100233ms step_avg:60.45ms
step:1659/2315 train_time:100294ms step_avg:60.45ms
step:1660/2315 train_time:100355ms step_avg:60.46ms
step:1661/2315 train_time:100416ms step_avg:60.46ms
step:1662/2315 train_time:100477ms step_avg:60.46ms
step:1663/2315 train_time:100539ms step_avg:60.46ms
step:1664/2315 train_time:100599ms step_avg:60.46ms
step:1665/2315 train_time:100660ms step_avg:60.46ms
step:1666/2315 train_time:100721ms step_avg:60.46ms
step:1667/2315 train_time:100783ms step_avg:60.46ms
step:1668/2315 train_time:100844ms step_avg:60.46ms
step:1669/2315 train_time:100907ms step_avg:60.46ms
step:1670/2315 train_time:100968ms step_avg:60.46ms
step:1671/2315 train_time:101029ms step_avg:60.46ms
step:1672/2315 train_time:101090ms step_avg:60.46ms
step:1673/2315 train_time:101151ms step_avg:60.46ms
step:1674/2315 train_time:101212ms step_avg:60.46ms
step:1675/2315 train_time:101273ms step_avg:60.46ms
step:1676/2315 train_time:101334ms step_avg:60.46ms
step:1677/2315 train_time:101395ms step_avg:60.46ms
step:1678/2315 train_time:101457ms step_avg:60.46ms
step:1679/2315 train_time:101518ms step_avg:60.46ms
step:1680/2315 train_time:101580ms step_avg:60.46ms
step:1681/2315 train_time:101642ms step_avg:60.46ms
step:1682/2315 train_time:101702ms step_avg:60.47ms
step:1683/2315 train_time:101764ms step_avg:60.47ms
step:1684/2315 train_time:101825ms step_avg:60.47ms
step:1685/2315 train_time:101887ms step_avg:60.47ms
step:1686/2315 train_time:101949ms step_avg:60.47ms
step:1687/2315 train_time:102010ms step_avg:60.47ms
step:1688/2315 train_time:102071ms step_avg:60.47ms
step:1689/2315 train_time:102132ms step_avg:60.47ms
step:1690/2315 train_time:102193ms step_avg:60.47ms
step:1691/2315 train_time:102254ms step_avg:60.47ms
step:1692/2315 train_time:102315ms step_avg:60.47ms
step:1693/2315 train_time:102376ms step_avg:60.47ms
step:1694/2315 train_time:102437ms step_avg:60.47ms
step:1695/2315 train_time:102498ms step_avg:60.47ms
step:1696/2315 train_time:102560ms step_avg:60.47ms
step:1697/2315 train_time:102621ms step_avg:60.47ms
step:1698/2315 train_time:102682ms step_avg:60.47ms
step:1699/2315 train_time:102745ms step_avg:60.47ms
step:1700/2315 train_time:102806ms step_avg:60.47ms
step:1701/2315 train_time:102868ms step_avg:60.47ms
step:1702/2315 train_time:102929ms step_avg:60.48ms
step:1703/2315 train_time:102990ms step_avg:60.48ms
step:1704/2315 train_time:103051ms step_avg:60.48ms
step:1705/2315 train_time:103111ms step_avg:60.48ms
step:1706/2315 train_time:103172ms step_avg:60.48ms
step:1707/2315 train_time:103234ms step_avg:60.48ms
step:1708/2315 train_time:103294ms step_avg:60.48ms
step:1709/2315 train_time:103355ms step_avg:60.48ms
step:1710/2315 train_time:103416ms step_avg:60.48ms
step:1711/2315 train_time:103477ms step_avg:60.48ms
step:1712/2315 train_time:103539ms step_avg:60.48ms
step:1713/2315 train_time:103600ms step_avg:60.48ms
step:1714/2315 train_time:103661ms step_avg:60.48ms
step:1715/2315 train_time:103723ms step_avg:60.48ms
step:1716/2315 train_time:103784ms step_avg:60.48ms
step:1717/2315 train_time:103845ms step_avg:60.48ms
step:1718/2315 train_time:103906ms step_avg:60.48ms
step:1719/2315 train_time:103968ms step_avg:60.48ms
step:1720/2315 train_time:104029ms step_avg:60.48ms
step:1721/2315 train_time:104090ms step_avg:60.48ms
step:1722/2315 train_time:104152ms step_avg:60.48ms
step:1723/2315 train_time:104213ms step_avg:60.48ms
step:1724/2315 train_time:104274ms step_avg:60.48ms
step:1725/2315 train_time:104335ms step_avg:60.48ms
step:1726/2315 train_time:104396ms step_avg:60.48ms
step:1727/2315 train_time:104457ms step_avg:60.48ms
step:1728/2315 train_time:104517ms step_avg:60.48ms
step:1729/2315 train_time:104579ms step_avg:60.49ms
step:1730/2315 train_time:104641ms step_avg:60.49ms
step:1731/2315 train_time:104703ms step_avg:60.49ms
step:1732/2315 train_time:104764ms step_avg:60.49ms
step:1733/2315 train_time:104826ms step_avg:60.49ms
step:1734/2315 train_time:104887ms step_avg:60.49ms
step:1735/2315 train_time:104949ms step_avg:60.49ms
step:1736/2315 train_time:105010ms step_avg:60.49ms
step:1737/2315 train_time:105071ms step_avg:60.49ms
step:1738/2315 train_time:105132ms step_avg:60.49ms
step:1739/2315 train_time:105193ms step_avg:60.49ms
step:1740/2315 train_time:105254ms step_avg:60.49ms
step:1741/2315 train_time:105315ms step_avg:60.49ms
step:1742/2315 train_time:105376ms step_avg:60.49ms
step:1743/2315 train_time:105437ms step_avg:60.49ms
step:1744/2315 train_time:105498ms step_avg:60.49ms
step:1745/2315 train_time:105559ms step_avg:60.49ms
step:1746/2315 train_time:105621ms step_avg:60.49ms
step:1747/2315 train_time:105683ms step_avg:60.49ms
step:1748/2315 train_time:105744ms step_avg:60.49ms
step:1749/2315 train_time:105806ms step_avg:60.50ms
step:1750/2315 train_time:105868ms step_avg:60.50ms
step:1750/2315 val_loss:3.3814 train_time:105931ms step_avg:60.53ms
step:1751/2315 train_time:105951ms step_avg:60.51ms
step:1752/2315 train_time:105994ms step_avg:60.50ms
step:1753/2315 train_time:106062ms step_avg:60.50ms
step:1754/2315 train_time:106130ms step_avg:60.51ms
step:1755/2315 train_time:106191ms step_avg:60.51ms
step:1756/2315 train_time:106251ms step_avg:60.51ms
step:1757/2315 train_time:106312ms step_avg:60.51ms
step:1758/2315 train_time:106372ms step_avg:60.51ms
step:1759/2315 train_time:106433ms step_avg:60.51ms
step:1760/2315 train_time:106493ms step_avg:60.51ms
step:1761/2315 train_time:106553ms step_avg:60.51ms
step:1762/2315 train_time:106613ms step_avg:60.51ms
step:1763/2315 train_time:106674ms step_avg:60.51ms
step:1764/2315 train_time:106734ms step_avg:60.51ms
step:1765/2315 train_time:106794ms step_avg:60.51ms
step:1766/2315 train_time:106854ms step_avg:60.51ms
step:1767/2315 train_time:106917ms step_avg:60.51ms
step:1768/2315 train_time:106979ms step_avg:60.51ms
step:1769/2315 train_time:107044ms step_avg:60.51ms
step:1770/2315 train_time:107106ms step_avg:60.51ms
step:1771/2315 train_time:107168ms step_avg:60.51ms
step:1772/2315 train_time:107229ms step_avg:60.51ms
step:1773/2315 train_time:107290ms step_avg:60.51ms
step:1774/2315 train_time:107350ms step_avg:60.51ms
step:1775/2315 train_time:107411ms step_avg:60.51ms
step:1776/2315 train_time:107472ms step_avg:60.51ms
step:1777/2315 train_time:107532ms step_avg:60.51ms
step:1778/2315 train_time:107593ms step_avg:60.51ms
step:1779/2315 train_time:107653ms step_avg:60.51ms
step:1780/2315 train_time:107714ms step_avg:60.51ms
step:1781/2315 train_time:107774ms step_avg:60.51ms
step:1782/2315 train_time:107835ms step_avg:60.51ms
step:1783/2315 train_time:107896ms step_avg:60.51ms
step:1784/2315 train_time:107958ms step_avg:60.51ms
step:1785/2315 train_time:108022ms step_avg:60.52ms
step:1786/2315 train_time:108084ms step_avg:60.52ms
step:1787/2315 train_time:108146ms step_avg:60.52ms
step:1788/2315 train_time:108207ms step_avg:60.52ms
step:1789/2315 train_time:108269ms step_avg:60.52ms
step:1790/2315 train_time:108330ms step_avg:60.52ms
step:1791/2315 train_time:108391ms step_avg:60.52ms
step:1792/2315 train_time:108451ms step_avg:60.52ms
step:1793/2315 train_time:108512ms step_avg:60.52ms
step:1794/2315 train_time:108573ms step_avg:60.52ms
step:1795/2315 train_time:108633ms step_avg:60.52ms
step:1796/2315 train_time:108694ms step_avg:60.52ms
step:1797/2315 train_time:108754ms step_avg:60.52ms
step:1798/2315 train_time:108816ms step_avg:60.52ms
step:1799/2315 train_time:108878ms step_avg:60.52ms
step:1800/2315 train_time:108940ms step_avg:60.52ms
step:1801/2315 train_time:109002ms step_avg:60.52ms
step:1802/2315 train_time:109063ms step_avg:60.52ms
step:1803/2315 train_time:109125ms step_avg:60.52ms
step:1804/2315 train_time:109187ms step_avg:60.53ms
step:1805/2315 train_time:109248ms step_avg:60.53ms
step:1806/2315 train_time:109309ms step_avg:60.53ms
step:1807/2315 train_time:109370ms step_avg:60.53ms
step:1808/2315 train_time:109431ms step_avg:60.53ms
step:1809/2315 train_time:109492ms step_avg:60.53ms
step:1810/2315 train_time:109552ms step_avg:60.53ms
step:1811/2315 train_time:109613ms step_avg:60.53ms
step:1812/2315 train_time:109673ms step_avg:60.53ms
step:1813/2315 train_time:109734ms step_avg:60.53ms
step:1814/2315 train_time:109794ms step_avg:60.53ms
step:1815/2315 train_time:109856ms step_avg:60.53ms
step:1816/2315 train_time:109918ms step_avg:60.53ms
step:1817/2315 train_time:109979ms step_avg:60.53ms
step:1818/2315 train_time:110041ms step_avg:60.53ms
step:1819/2315 train_time:110103ms step_avg:60.53ms
step:1820/2315 train_time:110164ms step_avg:60.53ms
step:1821/2315 train_time:110226ms step_avg:60.53ms
step:1822/2315 train_time:110286ms step_avg:60.53ms
step:1823/2315 train_time:110347ms step_avg:60.53ms
step:1824/2315 train_time:110409ms step_avg:60.53ms
step:1825/2315 train_time:110469ms step_avg:60.53ms
step:1826/2315 train_time:110530ms step_avg:60.53ms
step:1827/2315 train_time:110591ms step_avg:60.53ms
step:1828/2315 train_time:110651ms step_avg:60.53ms
step:1829/2315 train_time:110712ms step_avg:60.53ms
step:1830/2315 train_time:110772ms step_avg:60.53ms
step:1831/2315 train_time:110834ms step_avg:60.53ms
step:1832/2315 train_time:110895ms step_avg:60.53ms
step:1833/2315 train_time:110956ms step_avg:60.53ms
step:1834/2315 train_time:111017ms step_avg:60.53ms
step:1835/2315 train_time:111080ms step_avg:60.53ms
step:1836/2315 train_time:111141ms step_avg:60.53ms
step:1837/2315 train_time:111203ms step_avg:60.54ms
step:1838/2315 train_time:111264ms step_avg:60.54ms
step:1839/2315 train_time:111326ms step_avg:60.54ms
step:1840/2315 train_time:111387ms step_avg:60.54ms
step:1841/2315 train_time:111448ms step_avg:60.54ms
step:1842/2315 train_time:111508ms step_avg:60.54ms
step:1843/2315 train_time:111569ms step_avg:60.54ms
step:1844/2315 train_time:111629ms step_avg:60.54ms
step:1845/2315 train_time:111690ms step_avg:60.54ms
step:1846/2315 train_time:111751ms step_avg:60.54ms
step:1847/2315 train_time:111811ms step_avg:60.54ms
step:1848/2315 train_time:111872ms step_avg:60.54ms
step:1849/2315 train_time:111933ms step_avg:60.54ms
step:1850/2315 train_time:111995ms step_avg:60.54ms
step:1851/2315 train_time:112058ms step_avg:60.54ms
step:1852/2315 train_time:112119ms step_avg:60.54ms
step:1853/2315 train_time:112181ms step_avg:60.54ms
step:1854/2315 train_time:112242ms step_avg:60.54ms
step:1855/2315 train_time:112304ms step_avg:60.54ms
step:1856/2315 train_time:112365ms step_avg:60.54ms
step:1857/2315 train_time:112426ms step_avg:60.54ms
step:1858/2315 train_time:112487ms step_avg:60.54ms
step:1859/2315 train_time:112549ms step_avg:60.54ms
step:1860/2315 train_time:112609ms step_avg:60.54ms
step:1861/2315 train_time:112670ms step_avg:60.54ms
step:1862/2315 train_time:112730ms step_avg:60.54ms
step:1863/2315 train_time:112791ms step_avg:60.54ms
step:1864/2315 train_time:112851ms step_avg:60.54ms
step:1865/2315 train_time:112912ms step_avg:60.54ms
step:1866/2315 train_time:112973ms step_avg:60.54ms
step:1867/2315 train_time:113035ms step_avg:60.54ms
step:1868/2315 train_time:113097ms step_avg:60.54ms
step:1869/2315 train_time:113160ms step_avg:60.55ms
step:1870/2315 train_time:113221ms step_avg:60.55ms
step:1871/2315 train_time:113282ms step_avg:60.55ms
step:1872/2315 train_time:113343ms step_avg:60.55ms
step:1873/2315 train_time:113404ms step_avg:60.55ms
step:1874/2315 train_time:113465ms step_avg:60.55ms
step:1875/2315 train_time:113526ms step_avg:60.55ms
step:1876/2315 train_time:113587ms step_avg:60.55ms
step:1877/2315 train_time:113648ms step_avg:60.55ms
step:1878/2315 train_time:113709ms step_avg:60.55ms
step:1879/2315 train_time:113770ms step_avg:60.55ms
step:1880/2315 train_time:113831ms step_avg:60.55ms
step:1881/2315 train_time:113892ms step_avg:60.55ms
step:1882/2315 train_time:113953ms step_avg:60.55ms
step:1883/2315 train_time:114014ms step_avg:60.55ms
step:1884/2315 train_time:114075ms step_avg:60.55ms
step:1885/2315 train_time:114136ms step_avg:60.55ms
step:1886/2315 train_time:114197ms step_avg:60.55ms
step:1887/2315 train_time:114259ms step_avg:60.55ms
step:1888/2315 train_time:114321ms step_avg:60.55ms
step:1889/2315 train_time:114383ms step_avg:60.55ms
step:1890/2315 train_time:114444ms step_avg:60.55ms
step:1891/2315 train_time:114506ms step_avg:60.55ms
step:1892/2315 train_time:114567ms step_avg:60.55ms
step:1893/2315 train_time:114627ms step_avg:60.55ms
step:1894/2315 train_time:114688ms step_avg:60.55ms
step:1895/2315 train_time:114749ms step_avg:60.55ms
step:1896/2315 train_time:114810ms step_avg:60.55ms
step:1897/2315 train_time:114871ms step_avg:60.55ms
step:1898/2315 train_time:114932ms step_avg:60.55ms
step:1899/2315 train_time:114992ms step_avg:60.55ms
step:1900/2315 train_time:115053ms step_avg:60.55ms
step:1901/2315 train_time:115115ms step_avg:60.55ms
step:1902/2315 train_time:115177ms step_avg:60.56ms
step:1903/2315 train_time:115237ms step_avg:60.56ms
step:1904/2315 train_time:115299ms step_avg:60.56ms
step:1905/2315 train_time:115360ms step_avg:60.56ms
step:1906/2315 train_time:115421ms step_avg:60.56ms
step:1907/2315 train_time:115483ms step_avg:60.56ms
step:1908/2315 train_time:115544ms step_avg:60.56ms
step:1909/2315 train_time:115605ms step_avg:60.56ms
step:1910/2315 train_time:115667ms step_avg:60.56ms
step:1911/2315 train_time:115728ms step_avg:60.56ms
step:1912/2315 train_time:115789ms step_avg:60.56ms
step:1913/2315 train_time:115850ms step_avg:60.56ms
step:1914/2315 train_time:115910ms step_avg:60.56ms
step:1915/2315 train_time:115971ms step_avg:60.56ms
step:1916/2315 train_time:116031ms step_avg:60.56ms
step:1917/2315 train_time:116092ms step_avg:60.56ms
step:1918/2315 train_time:116153ms step_avg:60.56ms
step:1919/2315 train_time:116216ms step_avg:60.56ms
step:1920/2315 train_time:116277ms step_avg:60.56ms
step:1921/2315 train_time:116338ms step_avg:60.56ms
step:1922/2315 train_time:116401ms step_avg:60.56ms
step:1923/2315 train_time:116463ms step_avg:60.56ms
step:1924/2315 train_time:116525ms step_avg:60.56ms
step:1925/2315 train_time:116586ms step_avg:60.56ms
step:1926/2315 train_time:116647ms step_avg:60.56ms
step:1927/2315 train_time:116708ms step_avg:60.56ms
step:1928/2315 train_time:116769ms step_avg:60.56ms
step:1929/2315 train_time:116830ms step_avg:60.56ms
step:1930/2315 train_time:116891ms step_avg:60.57ms
step:1931/2315 train_time:116952ms step_avg:60.57ms
step:1932/2315 train_time:117013ms step_avg:60.57ms
step:1933/2315 train_time:117073ms step_avg:60.57ms
step:1934/2315 train_time:117134ms step_avg:60.57ms
step:1935/2315 train_time:117195ms step_avg:60.57ms
step:1936/2315 train_time:117256ms step_avg:60.57ms
step:1937/2315 train_time:117318ms step_avg:60.57ms
step:1938/2315 train_time:117379ms step_avg:60.57ms
step:1939/2315 train_time:117441ms step_avg:60.57ms
step:1940/2315 train_time:117503ms step_avg:60.57ms
step:1941/2315 train_time:117564ms step_avg:60.57ms
step:1942/2315 train_time:117625ms step_avg:60.57ms
step:1943/2315 train_time:117686ms step_avg:60.57ms
step:1944/2315 train_time:117747ms step_avg:60.57ms
step:1945/2315 train_time:117808ms step_avg:60.57ms
step:1946/2315 train_time:117869ms step_avg:60.57ms
step:1947/2315 train_time:117930ms step_avg:60.57ms
step:1948/2315 train_time:117991ms step_avg:60.57ms
step:1949/2315 train_time:118051ms step_avg:60.57ms
step:1950/2315 train_time:118112ms step_avg:60.57ms
step:1951/2315 train_time:118173ms step_avg:60.57ms
step:1952/2315 train_time:118235ms step_avg:60.57ms
step:1953/2315 train_time:118296ms step_avg:60.57ms
step:1954/2315 train_time:118358ms step_avg:60.57ms
step:1955/2315 train_time:118419ms step_avg:60.57ms
step:1956/2315 train_time:118481ms step_avg:60.57ms
step:1957/2315 train_time:118542ms step_avg:60.57ms
step:1958/2315 train_time:118603ms step_avg:60.57ms
step:1959/2315 train_time:118665ms step_avg:60.57ms
step:1960/2315 train_time:118726ms step_avg:60.57ms
step:1961/2315 train_time:118787ms step_avg:60.57ms
step:1962/2315 train_time:118847ms step_avg:60.57ms
step:1963/2315 train_time:118908ms step_avg:60.57ms
step:1964/2315 train_time:118969ms step_avg:60.58ms
step:1965/2315 train_time:119030ms step_avg:60.58ms
step:1966/2315 train_time:119091ms step_avg:60.58ms
step:1967/2315 train_time:119151ms step_avg:60.58ms
step:1968/2315 train_time:119213ms step_avg:60.58ms
step:1969/2315 train_time:119274ms step_avg:60.58ms
step:1970/2315 train_time:119334ms step_avg:60.58ms
step:1971/2315 train_time:119396ms step_avg:60.58ms
step:1972/2315 train_time:119458ms step_avg:60.58ms
step:1973/2315 train_time:119521ms step_avg:60.58ms
step:1974/2315 train_time:119582ms step_avg:60.58ms
step:1975/2315 train_time:119643ms step_avg:60.58ms
step:1976/2315 train_time:119705ms step_avg:60.58ms
step:1977/2315 train_time:119766ms step_avg:60.58ms
step:1978/2315 train_time:119827ms step_avg:60.58ms
step:1979/2315 train_time:119888ms step_avg:60.58ms
step:1980/2315 train_time:119949ms step_avg:60.58ms
step:1981/2315 train_time:120010ms step_avg:60.58ms
step:1982/2315 train_time:120071ms step_avg:60.58ms
step:1983/2315 train_time:120132ms step_avg:60.58ms
step:1984/2315 train_time:120193ms step_avg:60.58ms
step:1985/2315 train_time:120253ms step_avg:60.58ms
step:1986/2315 train_time:120314ms step_avg:60.58ms
step:1987/2315 train_time:120375ms step_avg:60.58ms
step:1988/2315 train_time:120437ms step_avg:60.58ms
step:1989/2315 train_time:120498ms step_avg:60.58ms
step:1990/2315 train_time:120560ms step_avg:60.58ms
step:1991/2315 train_time:120623ms step_avg:60.58ms
step:1992/2315 train_time:120684ms step_avg:60.58ms
step:1993/2315 train_time:120745ms step_avg:60.58ms
step:1994/2315 train_time:120806ms step_avg:60.58ms
step:1995/2315 train_time:120868ms step_avg:60.59ms
step:1996/2315 train_time:120929ms step_avg:60.59ms
step:1997/2315 train_time:120990ms step_avg:60.59ms
step:1998/2315 train_time:121051ms step_avg:60.59ms
step:1999/2315 train_time:121112ms step_avg:60.59ms
step:2000/2315 train_time:121173ms step_avg:60.59ms
step:2000/2315 val_loss:3.3313 train_time:121235ms step_avg:60.62ms
step:2001/2315 train_time:121254ms step_avg:60.60ms
step:2002/2315 train_time:121300ms step_avg:60.59ms
step:2003/2315 train_time:121363ms step_avg:60.59ms
step:2004/2315 train_time:121427ms step_avg:60.59ms
step:2005/2315 train_time:121491ms step_avg:60.59ms
step:2006/2315 train_time:121552ms step_avg:60.59ms
step:2007/2315 train_time:121614ms step_avg:60.59ms
step:2008/2315 train_time:121674ms step_avg:60.59ms
step:2009/2315 train_time:121734ms step_avg:60.59ms
step:2010/2315 train_time:121794ms step_avg:60.59ms
step:2011/2315 train_time:121855ms step_avg:60.59ms
step:2012/2315 train_time:121915ms step_avg:60.59ms
step:2013/2315 train_time:121977ms step_avg:60.59ms
step:2014/2315 train_time:122038ms step_avg:60.59ms
step:2015/2315 train_time:122099ms step_avg:60.60ms
step:2016/2315 train_time:122161ms step_avg:60.60ms
step:2017/2315 train_time:122223ms step_avg:60.60ms
step:2018/2315 train_time:122284ms step_avg:60.60ms
step:2019/2315 train_time:122348ms step_avg:60.60ms
step:2020/2315 train_time:122410ms step_avg:60.60ms
step:2021/2315 train_time:122472ms step_avg:60.60ms
step:2022/2315 train_time:122533ms step_avg:60.60ms
step:2023/2315 train_time:122594ms step_avg:60.60ms
step:2024/2315 train_time:122655ms step_avg:60.60ms
step:2025/2315 train_time:122716ms step_avg:60.60ms
step:2026/2315 train_time:122776ms step_avg:60.60ms
step:2027/2315 train_time:122837ms step_avg:60.60ms
step:2028/2315 train_time:122897ms step_avg:60.60ms
step:2029/2315 train_time:122958ms step_avg:60.60ms
step:2030/2315 train_time:123018ms step_avg:60.60ms
step:2031/2315 train_time:123079ms step_avg:60.60ms
step:2032/2315 train_time:123141ms step_avg:60.60ms
step:2033/2315 train_time:123202ms step_avg:60.60ms
step:2034/2315 train_time:123264ms step_avg:60.60ms
step:2035/2315 train_time:123327ms step_avg:60.60ms
step:2036/2315 train_time:123388ms step_avg:60.60ms
step:2037/2315 train_time:123450ms step_avg:60.60ms
step:2038/2315 train_time:123511ms step_avg:60.60ms
step:2039/2315 train_time:123572ms step_avg:60.60ms
step:2040/2315 train_time:123633ms step_avg:60.60ms
step:2041/2315 train_time:123695ms step_avg:60.60ms
step:2042/2315 train_time:123755ms step_avg:60.60ms
step:2043/2315 train_time:123816ms step_avg:60.61ms
step:2044/2315 train_time:123877ms step_avg:60.61ms
step:2045/2315 train_time:123938ms step_avg:60.61ms
step:2046/2315 train_time:123999ms step_avg:60.61ms
step:2047/2315 train_time:124061ms step_avg:60.61ms
step:2048/2315 train_time:124122ms step_avg:60.61ms
step:2049/2315 train_time:124184ms step_avg:60.61ms
step:2050/2315 train_time:124245ms step_avg:60.61ms
step:2051/2315 train_time:124307ms step_avg:60.61ms
step:2052/2315 train_time:124368ms step_avg:60.61ms
step:2053/2315 train_time:124430ms step_avg:60.61ms
step:2054/2315 train_time:124492ms step_avg:60.61ms
step:2055/2315 train_time:124553ms step_avg:60.61ms
step:2056/2315 train_time:124614ms step_avg:60.61ms
step:2057/2315 train_time:124675ms step_avg:60.61ms
step:2058/2315 train_time:124736ms step_avg:60.61ms
step:2059/2315 train_time:124797ms step_avg:60.61ms
step:2060/2315 train_time:124858ms step_avg:60.61ms
step:2061/2315 train_time:124919ms step_avg:60.61ms
step:2062/2315 train_time:124980ms step_avg:60.61ms
step:2063/2315 train_time:125041ms step_avg:60.61ms
step:2064/2315 train_time:125102ms step_avg:60.61ms
step:2065/2315 train_time:125163ms step_avg:60.61ms
step:2066/2315 train_time:125224ms step_avg:60.61ms
step:2067/2315 train_time:125285ms step_avg:60.61ms
step:2068/2315 train_time:125346ms step_avg:60.61ms
step:2069/2315 train_time:125408ms step_avg:60.61ms
step:2070/2315 train_time:125470ms step_avg:60.61ms
step:2071/2315 train_time:125531ms step_avg:60.61ms
step:2072/2315 train_time:125592ms step_avg:60.61ms
step:2073/2315 train_time:125653ms step_avg:60.61ms
step:2074/2315 train_time:125714ms step_avg:60.61ms
step:2075/2315 train_time:125775ms step_avg:60.61ms
step:2076/2315 train_time:125836ms step_avg:60.61ms
step:2077/2315 train_time:125897ms step_avg:60.61ms
step:2078/2315 train_time:125958ms step_avg:60.62ms
step:2079/2315 train_time:126019ms step_avg:60.62ms
step:2080/2315 train_time:126080ms step_avg:60.62ms
step:2081/2315 train_time:126143ms step_avg:60.62ms
step:2082/2315 train_time:126204ms step_avg:60.62ms
step:2083/2315 train_time:126266ms step_avg:60.62ms
step:2084/2315 train_time:126327ms step_avg:60.62ms
step:2085/2315 train_time:126389ms step_avg:60.62ms
step:2086/2315 train_time:126449ms step_avg:60.62ms
step:2087/2315 train_time:126511ms step_avg:60.62ms
step:2088/2315 train_time:126572ms step_avg:60.62ms
step:2089/2315 train_time:126633ms step_avg:60.62ms
step:2090/2315 train_time:126693ms step_avg:60.62ms
step:2091/2315 train_time:126755ms step_avg:60.62ms
step:2092/2315 train_time:126816ms step_avg:60.62ms
step:2093/2315 train_time:126876ms step_avg:60.62ms
step:2094/2315 train_time:126937ms step_avg:60.62ms
step:2095/2315 train_time:126999ms step_avg:60.62ms
step:2096/2315 train_time:127060ms step_avg:60.62ms
step:2097/2315 train_time:127122ms step_avg:60.62ms
step:2098/2315 train_time:127183ms step_avg:60.62ms
step:2099/2315 train_time:127244ms step_avg:60.62ms
step:2100/2315 train_time:127305ms step_avg:60.62ms
step:2101/2315 train_time:127366ms step_avg:60.62ms
step:2102/2315 train_time:127427ms step_avg:60.62ms
step:2103/2315 train_time:127489ms step_avg:60.62ms
step:2104/2315 train_time:127550ms step_avg:60.62ms
step:2105/2315 train_time:127612ms step_avg:60.62ms
step:2106/2315 train_time:127673ms step_avg:60.62ms
step:2107/2315 train_time:127734ms step_avg:60.62ms
step:2108/2315 train_time:127796ms step_avg:60.62ms
step:2109/2315 train_time:127857ms step_avg:60.62ms
step:2110/2315 train_time:127917ms step_avg:60.62ms
step:2111/2315 train_time:127978ms step_avg:60.62ms
step:2112/2315 train_time:128040ms step_avg:60.62ms
step:2113/2315 train_time:128102ms step_avg:60.63ms
step:2114/2315 train_time:128163ms step_avg:60.63ms
step:2115/2315 train_time:128224ms step_avg:60.63ms
step:2116/2315 train_time:128285ms step_avg:60.63ms
step:2117/2315 train_time:128348ms step_avg:60.63ms
step:2118/2315 train_time:128409ms step_avg:60.63ms
step:2119/2315 train_time:128470ms step_avg:60.63ms
step:2120/2315 train_time:128531ms step_avg:60.63ms
step:2121/2315 train_time:128592ms step_avg:60.63ms
step:2122/2315 train_time:128653ms step_avg:60.63ms
step:2123/2315 train_time:128714ms step_avg:60.63ms
step:2124/2315 train_time:128775ms step_avg:60.63ms
step:2125/2315 train_time:128836ms step_avg:60.63ms
step:2126/2315 train_time:128897ms step_avg:60.63ms
step:2127/2315 train_time:128958ms step_avg:60.63ms
step:2128/2315 train_time:129019ms step_avg:60.63ms
step:2129/2315 train_time:129081ms step_avg:60.63ms
step:2130/2315 train_time:129142ms step_avg:60.63ms
step:2131/2315 train_time:129204ms step_avg:60.63ms
step:2132/2315 train_time:129265ms step_avg:60.63ms
step:2133/2315 train_time:129327ms step_avg:60.63ms
step:2134/2315 train_time:129388ms step_avg:60.63ms
step:2135/2315 train_time:129449ms step_avg:60.63ms
step:2136/2315 train_time:129510ms step_avg:60.63ms
step:2137/2315 train_time:129572ms step_avg:60.63ms
step:2138/2315 train_time:129633ms step_avg:60.63ms
step:2139/2315 train_time:129694ms step_avg:60.63ms
step:2140/2315 train_time:129755ms step_avg:60.63ms
step:2141/2315 train_time:129816ms step_avg:60.63ms
step:2142/2315 train_time:129877ms step_avg:60.63ms
step:2143/2315 train_time:129938ms step_avg:60.63ms
step:2144/2315 train_time:129999ms step_avg:60.63ms
step:2145/2315 train_time:130061ms step_avg:60.63ms
step:2146/2315 train_time:130121ms step_avg:60.63ms
step:2147/2315 train_time:130183ms step_avg:60.63ms
step:2148/2315 train_time:130244ms step_avg:60.64ms
step:2149/2315 train_time:130306ms step_avg:60.64ms
step:2150/2315 train_time:130368ms step_avg:60.64ms
step:2151/2315 train_time:130430ms step_avg:60.64ms
step:2152/2315 train_time:130490ms step_avg:60.64ms
step:2153/2315 train_time:130552ms step_avg:60.64ms
step:2154/2315 train_time:130612ms step_avg:60.64ms
step:2155/2315 train_time:130674ms step_avg:60.64ms
step:2156/2315 train_time:130735ms step_avg:60.64ms
step:2157/2315 train_time:130796ms step_avg:60.64ms
step:2158/2315 train_time:130857ms step_avg:60.64ms
step:2159/2315 train_time:130918ms step_avg:60.64ms
step:2160/2315 train_time:130979ms step_avg:60.64ms
step:2161/2315 train_time:131040ms step_avg:60.64ms
step:2162/2315 train_time:131101ms step_avg:60.64ms
step:2163/2315 train_time:131162ms step_avg:60.64ms
step:2164/2315 train_time:131223ms step_avg:60.64ms
step:2165/2315 train_time:131285ms step_avg:60.64ms
step:2166/2315 train_time:131347ms step_avg:60.64ms
step:2167/2315 train_time:131409ms step_avg:60.64ms
step:2168/2315 train_time:131470ms step_avg:60.64ms
step:2169/2315 train_time:131531ms step_avg:60.64ms
step:2170/2315 train_time:131592ms step_avg:60.64ms
step:2171/2315 train_time:131653ms step_avg:60.64ms
step:2172/2315 train_time:131714ms step_avg:60.64ms
step:2173/2315 train_time:131775ms step_avg:60.64ms
step:2174/2315 train_time:131836ms step_avg:60.64ms
step:2175/2315 train_time:131898ms step_avg:60.64ms
step:2176/2315 train_time:131958ms step_avg:60.64ms
step:2177/2315 train_time:132020ms step_avg:60.64ms
step:2178/2315 train_time:132082ms step_avg:60.64ms
step:2179/2315 train_time:132143ms step_avg:60.64ms
step:2180/2315 train_time:132205ms step_avg:60.64ms
step:2181/2315 train_time:132266ms step_avg:60.64ms
step:2182/2315 train_time:132328ms step_avg:60.65ms
step:2183/2315 train_time:132389ms step_avg:60.65ms
step:2184/2315 train_time:132450ms step_avg:60.65ms
step:2185/2315 train_time:132511ms step_avg:60.65ms
step:2186/2315 train_time:132572ms step_avg:60.65ms
step:2187/2315 train_time:132634ms step_avg:60.65ms
step:2188/2315 train_time:132695ms step_avg:60.65ms
step:2189/2315 train_time:132756ms step_avg:60.65ms
step:2190/2315 train_time:132817ms step_avg:60.65ms
step:2191/2315 train_time:132879ms step_avg:60.65ms
step:2192/2315 train_time:132940ms step_avg:60.65ms
step:2193/2315 train_time:133001ms step_avg:60.65ms
step:2194/2315 train_time:133062ms step_avg:60.65ms
step:2195/2315 train_time:133124ms step_avg:60.65ms
step:2196/2315 train_time:133185ms step_avg:60.65ms
step:2197/2315 train_time:133247ms step_avg:60.65ms
step:2198/2315 train_time:133308ms step_avg:60.65ms
step:2199/2315 train_time:133369ms step_avg:60.65ms
step:2200/2315 train_time:133430ms step_avg:60.65ms
step:2201/2315 train_time:133491ms step_avg:60.65ms
step:2202/2315 train_time:133552ms step_avg:60.65ms
step:2203/2315 train_time:133614ms step_avg:60.65ms
step:2204/2315 train_time:133675ms step_avg:60.65ms
step:2205/2315 train_time:133736ms step_avg:60.65ms
step:2206/2315 train_time:133797ms step_avg:60.65ms
step:2207/2315 train_time:133858ms step_avg:60.65ms
step:2208/2315 train_time:133919ms step_avg:60.65ms
step:2209/2315 train_time:133980ms step_avg:60.65ms
step:2210/2315 train_time:134042ms step_avg:60.65ms
step:2211/2315 train_time:134104ms step_avg:60.65ms
step:2212/2315 train_time:134165ms step_avg:60.65ms
step:2213/2315 train_time:134226ms step_avg:60.65ms
step:2214/2315 train_time:134287ms step_avg:60.65ms
step:2215/2315 train_time:134350ms step_avg:60.65ms
step:2216/2315 train_time:134411ms step_avg:60.65ms
step:2217/2315 train_time:134472ms step_avg:60.66ms
step:2218/2315 train_time:134534ms step_avg:60.66ms
step:2219/2315 train_time:134595ms step_avg:60.66ms
step:2220/2315 train_time:134656ms step_avg:60.66ms
step:2221/2315 train_time:134717ms step_avg:60.66ms
step:2222/2315 train_time:134778ms step_avg:60.66ms
step:2223/2315 train_time:134840ms step_avg:60.66ms
step:2224/2315 train_time:134901ms step_avg:60.66ms
step:2225/2315 train_time:134962ms step_avg:60.66ms
step:2226/2315 train_time:135023ms step_avg:60.66ms
step:2227/2315 train_time:135085ms step_avg:60.66ms
step:2228/2315 train_time:135145ms step_avg:60.66ms
step:2229/2315 train_time:135206ms step_avg:60.66ms
step:2230/2315 train_time:135268ms step_avg:60.66ms
step:2231/2315 train_time:135329ms step_avg:60.66ms
step:2232/2315 train_time:135390ms step_avg:60.66ms
step:2233/2315 train_time:135452ms step_avg:60.66ms
step:2234/2315 train_time:135513ms step_avg:60.66ms
step:2235/2315 train_time:135574ms step_avg:60.66ms
step:2236/2315 train_time:135636ms step_avg:60.66ms
step:2237/2315 train_time:135697ms step_avg:60.66ms
step:2238/2315 train_time:135758ms step_avg:60.66ms
step:2239/2315 train_time:135819ms step_avg:60.66ms
step:2240/2315 train_time:135879ms step_avg:60.66ms
step:2241/2315 train_time:135940ms step_avg:60.66ms
step:2242/2315 train_time:136002ms step_avg:60.66ms
step:2243/2315 train_time:136064ms step_avg:60.66ms
step:2244/2315 train_time:136126ms step_avg:60.66ms
step:2245/2315 train_time:136188ms step_avg:60.66ms
step:2246/2315 train_time:136249ms step_avg:60.66ms
step:2247/2315 train_time:136310ms step_avg:60.66ms
step:2248/2315 train_time:136371ms step_avg:60.66ms
step:2249/2315 train_time:136433ms step_avg:60.66ms
step:2250/2315 train_time:136494ms step_avg:60.66ms
step:2250/2315 val_loss:3.2915 train_time:136557ms step_avg:60.69ms
step:2251/2315 train_time:136576ms step_avg:60.67ms
step:2252/2315 train_time:136619ms step_avg:60.67ms
step:2253/2315 train_time:136684ms step_avg:60.67ms
step:2254/2315 train_time:136747ms step_avg:60.67ms
step:2255/2315 train_time:136808ms step_avg:60.67ms
step:2256/2315 train_time:136871ms step_avg:60.67ms
step:2257/2315 train_time:136932ms step_avg:60.67ms
step:2258/2315 train_time:136992ms step_avg:60.67ms
step:2259/2315 train_time:137054ms step_avg:60.67ms
step:2260/2315 train_time:137115ms step_avg:60.67ms
step:2261/2315 train_time:137176ms step_avg:60.67ms
step:2262/2315 train_time:137236ms step_avg:60.67ms
step:2263/2315 train_time:137297ms step_avg:60.67ms
step:2264/2315 train_time:137357ms step_avg:60.67ms
step:2265/2315 train_time:137417ms step_avg:60.67ms
step:2266/2315 train_time:137478ms step_avg:60.67ms
step:2267/2315 train_time:137539ms step_avg:60.67ms
step:2268/2315 train_time:137601ms step_avg:60.67ms
step:2269/2315 train_time:137663ms step_avg:60.67ms
step:2270/2315 train_time:137725ms step_avg:60.67ms
step:2271/2315 train_time:137787ms step_avg:60.67ms
step:2272/2315 train_time:137849ms step_avg:60.67ms
step:2273/2315 train_time:137910ms step_avg:60.67ms
step:2274/2315 train_time:137971ms step_avg:60.67ms
step:2275/2315 train_time:138032ms step_avg:60.67ms
step:2276/2315 train_time:138093ms step_avg:60.67ms
step:2277/2315 train_time:138153ms step_avg:60.67ms
step:2278/2315 train_time:138214ms step_avg:60.67ms
step:2279/2315 train_time:138275ms step_avg:60.67ms
step:2280/2315 train_time:138336ms step_avg:60.67ms
step:2281/2315 train_time:138397ms step_avg:60.67ms
step:2282/2315 train_time:138457ms step_avg:60.67ms
step:2283/2315 train_time:138519ms step_avg:60.67ms
step:2284/2315 train_time:138580ms step_avg:60.67ms
step:2285/2315 train_time:138642ms step_avg:60.67ms
step:2286/2315 train_time:138703ms step_avg:60.68ms
step:2287/2315 train_time:138765ms step_avg:60.68ms
step:2288/2315 train_time:138826ms step_avg:60.68ms
step:2289/2315 train_time:138887ms step_avg:60.68ms
step:2290/2315 train_time:138948ms step_avg:60.68ms
step:2291/2315 train_time:139009ms step_avg:60.68ms
step:2292/2315 train_time:139070ms step_avg:60.68ms
step:2293/2315 train_time:139132ms step_avg:60.68ms
step:2294/2315 train_time:139193ms step_avg:60.68ms
step:2295/2315 train_time:139254ms step_avg:60.68ms
step:2296/2315 train_time:139315ms step_avg:60.68ms
step:2297/2315 train_time:139376ms step_avg:60.68ms
step:2298/2315 train_time:139437ms step_avg:60.68ms
step:2299/2315 train_time:139499ms step_avg:60.68ms
step:2300/2315 train_time:139560ms step_avg:60.68ms
step:2301/2315 train_time:139621ms step_avg:60.68ms
step:2302/2315 train_time:139682ms step_avg:60.68ms
step:2303/2315 train_time:139743ms step_avg:60.68ms
step:2304/2315 train_time:139804ms step_avg:60.68ms
step:2305/2315 train_time:139866ms step_avg:60.68ms
step:2306/2315 train_time:139927ms step_avg:60.68ms
step:2307/2315 train_time:139989ms step_avg:60.68ms
step:2308/2315 train_time:140050ms step_avg:60.68ms
step:2309/2315 train_time:140111ms step_avg:60.68ms
step:2310/2315 train_time:140173ms step_avg:60.68ms
step:2311/2315 train_time:140234ms step_avg:60.68ms
step:2312/2315 train_time:140295ms step_avg:60.68ms
step:2313/2315 train_time:140356ms step_avg:60.68ms
step:2314/2315 train_time:140416ms step_avg:60.68ms
step:2315/2315 train_time:140478ms step_avg:60.68ms
step:2315/2315 val_loss:3.2786 train_time:140540ms step_avg:60.71ms
peak memory allocated: 29512 MiB reserved: 44076 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]
# polar_express_coeffs = [
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
# ]



@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    # state["second_momentum_buffer"] = torch.zeros_like(grad[...,0:1]) if param.size(-2) >= param.size(-1) else torch.zeros_like(grad[...,0:1,:])
                    # state["second_momentum_buffer"] = torch.zeros((*grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((*grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)
            
            ###################################
            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))
            ####################################


            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> (-1.5)  0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Oct 22 2025, 02:20:45) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251022+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 24 06:48:04 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000001:00:00.0 Off |                    0 |
| N/A   35C    P0            118W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000002:00:00.0 Off |                    0 |
| N/A   35C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000003:00:00.0 Off |                    0 |
| N/A   30C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000008:00:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000009:00:00.0 Off |                    0 |
| N/A   28C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   0000000A:00:00.0 Off |                    0 |
| N/A   35C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   0000000C:00:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2315 train_time:89ms step_avg:88.74ms
step:2/2315 train_time:185ms step_avg:92.32ms
step:3/2315 train_time:204ms step_avg:68.10ms
step:4/2315 train_time:243ms step_avg:60.81ms
step:5/2315 train_time:301ms step_avg:60.28ms
step:6/2315 train_time:361ms step_avg:60.13ms
step:7/2315 train_time:420ms step_avg:59.96ms
step:8/2315 train_time:479ms step_avg:59.91ms
step:9/2315 train_time:539ms step_avg:59.92ms
step:10/2315 train_time:599ms step_avg:59.90ms
step:11/2315 train_time:659ms step_avg:59.91ms
step:12/2315 train_time:719ms step_avg:59.89ms
step:13/2315 train_time:779ms step_avg:59.89ms
step:14/2315 train_time:838ms step_avg:59.87ms
step:15/2315 train_time:899ms step_avg:59.90ms
step:16/2315 train_time:958ms step_avg:59.90ms
step:17/2315 train_time:1021ms step_avg:60.04ms
step:18/2315 train_time:1085ms step_avg:60.25ms
step:19/2315 train_time:1148ms step_avg:60.43ms
step:20/2315 train_time:1210ms step_avg:60.48ms
step:21/2315 train_time:1271ms step_avg:60.51ms
step:22/2315 train_time:1331ms step_avg:60.50ms
step:23/2315 train_time:1391ms step_avg:60.47ms
step:24/2315 train_time:1451ms step_avg:60.46ms
step:25/2315 train_time:1512ms step_avg:60.47ms
step:26/2315 train_time:1571ms step_avg:60.44ms
step:27/2315 train_time:1632ms step_avg:60.43ms
step:28/2315 train_time:1692ms step_avg:60.43ms
step:29/2315 train_time:1752ms step_avg:60.42ms
step:30/2315 train_time:1813ms step_avg:60.43ms
step:31/2315 train_time:1873ms step_avg:60.41ms
step:32/2315 train_time:1933ms step_avg:60.41ms
step:33/2315 train_time:1995ms step_avg:60.45ms
step:34/2315 train_time:2057ms step_avg:60.49ms
step:35/2315 train_time:2118ms step_avg:60.50ms
step:36/2315 train_time:2178ms step_avg:60.50ms
step:37/2315 train_time:2238ms step_avg:60.49ms
step:38/2315 train_time:2298ms step_avg:60.48ms
step:39/2315 train_time:2359ms step_avg:60.50ms
step:40/2315 train_time:2419ms step_avg:60.48ms
step:41/2315 train_time:2480ms step_avg:60.48ms
step:42/2315 train_time:2540ms step_avg:60.47ms
step:43/2315 train_time:2601ms step_avg:60.49ms
step:44/2315 train_time:2661ms step_avg:60.48ms
step:45/2315 train_time:2722ms step_avg:60.48ms
step:46/2315 train_time:2782ms step_avg:60.47ms
step:47/2315 train_time:2842ms step_avg:60.47ms
step:48/2315 train_time:2902ms step_avg:60.47ms
step:49/2315 train_time:2964ms step_avg:60.48ms
step:50/2315 train_time:3025ms step_avg:60.50ms
step:51/2315 train_time:3086ms step_avg:60.52ms
step:52/2315 train_time:3147ms step_avg:60.51ms
step:53/2315 train_time:3207ms step_avg:60.51ms
step:54/2315 train_time:3267ms step_avg:60.50ms
step:55/2315 train_time:3329ms step_avg:60.52ms
step:56/2315 train_time:3389ms step_avg:60.52ms
step:57/2315 train_time:3450ms step_avg:60.52ms
step:58/2315 train_time:3511ms step_avg:60.53ms
step:59/2315 train_time:3572ms step_avg:60.53ms
step:60/2315 train_time:3632ms step_avg:60.53ms
step:61/2315 train_time:3692ms step_avg:60.52ms
step:62/2315 train_time:3752ms step_avg:60.52ms
step:63/2315 train_time:3813ms step_avg:60.52ms
step:64/2315 train_time:3873ms step_avg:60.52ms
step:65/2315 train_time:3934ms step_avg:60.52ms
step:66/2315 train_time:3994ms step_avg:60.51ms
step:67/2315 train_time:4054ms step_avg:60.51ms
step:68/2315 train_time:4114ms step_avg:60.50ms
step:69/2315 train_time:4175ms step_avg:60.50ms
step:70/2315 train_time:4234ms step_avg:60.49ms
step:71/2315 train_time:4295ms step_avg:60.49ms
step:72/2315 train_time:4355ms step_avg:60.48ms
step:73/2315 train_time:4415ms step_avg:60.48ms
step:74/2315 train_time:4475ms step_avg:60.48ms
step:75/2315 train_time:4536ms step_avg:60.48ms
step:76/2315 train_time:4597ms step_avg:60.48ms
step:77/2315 train_time:4657ms step_avg:60.48ms
step:78/2315 train_time:4717ms step_avg:60.47ms
step:79/2315 train_time:4777ms step_avg:60.47ms
step:80/2315 train_time:4836ms step_avg:60.45ms
step:81/2315 train_time:4897ms step_avg:60.45ms
step:82/2315 train_time:4957ms step_avg:60.45ms
step:83/2315 train_time:5017ms step_avg:60.44ms
step:84/2315 train_time:5077ms step_avg:60.44ms
step:85/2315 train_time:5137ms step_avg:60.43ms
step:86/2315 train_time:5197ms step_avg:60.43ms
step:87/2315 train_time:5257ms step_avg:60.42ms
step:88/2315 train_time:5316ms step_avg:60.41ms
step:89/2315 train_time:5376ms step_avg:60.41ms
step:90/2315 train_time:5436ms step_avg:60.40ms
step:91/2315 train_time:5496ms step_avg:60.40ms
step:92/2315 train_time:5557ms step_avg:60.40ms
step:93/2315 train_time:5617ms step_avg:60.40ms
step:94/2315 train_time:5677ms step_avg:60.39ms
step:95/2315 train_time:5737ms step_avg:60.39ms
step:96/2315 train_time:5797ms step_avg:60.39ms
step:97/2315 train_time:5857ms step_avg:60.38ms
step:98/2315 train_time:5917ms step_avg:60.38ms
step:99/2315 train_time:5977ms step_avg:60.37ms
step:100/2315 train_time:6037ms step_avg:60.37ms
step:101/2315 train_time:6097ms step_avg:60.37ms
step:102/2315 train_time:6157ms step_avg:60.36ms
step:103/2315 train_time:6217ms step_avg:60.36ms
step:104/2315 train_time:6277ms step_avg:60.36ms
step:105/2315 train_time:6337ms step_avg:60.35ms
step:106/2315 train_time:6397ms step_avg:60.35ms
step:107/2315 train_time:6457ms step_avg:60.34ms
step:108/2315 train_time:6517ms step_avg:60.34ms
step:109/2315 train_time:6577ms step_avg:60.34ms
step:110/2315 train_time:6636ms step_avg:60.33ms
step:111/2315 train_time:6697ms step_avg:60.33ms
step:112/2315 train_time:6756ms step_avg:60.33ms
step:113/2315 train_time:6816ms step_avg:60.32ms
step:114/2315 train_time:6876ms step_avg:60.32ms
step:115/2315 train_time:6937ms step_avg:60.32ms
step:116/2315 train_time:6997ms step_avg:60.32ms
step:117/2315 train_time:7057ms step_avg:60.32ms
step:118/2315 train_time:7117ms step_avg:60.31ms
step:119/2315 train_time:7177ms step_avg:60.31ms
step:120/2315 train_time:7237ms step_avg:60.31ms
step:121/2315 train_time:7296ms step_avg:60.30ms
step:122/2315 train_time:7356ms step_avg:60.30ms
step:123/2315 train_time:7416ms step_avg:60.29ms
step:124/2315 train_time:7476ms step_avg:60.29ms
step:125/2315 train_time:7537ms step_avg:60.30ms
step:126/2315 train_time:7597ms step_avg:60.29ms
step:127/2315 train_time:7657ms step_avg:60.29ms
step:128/2315 train_time:7717ms step_avg:60.29ms
step:129/2315 train_time:7777ms step_avg:60.29ms
step:130/2315 train_time:7837ms step_avg:60.29ms
step:131/2315 train_time:7897ms step_avg:60.28ms
step:132/2315 train_time:7957ms step_avg:60.28ms
step:133/2315 train_time:8017ms step_avg:60.28ms
step:134/2315 train_time:8077ms step_avg:60.28ms
step:135/2315 train_time:8137ms step_avg:60.27ms
step:136/2315 train_time:8196ms step_avg:60.27ms
step:137/2315 train_time:8257ms step_avg:60.27ms
step:138/2315 train_time:8317ms step_avg:60.27ms
step:139/2315 train_time:8377ms step_avg:60.27ms
step:140/2315 train_time:8437ms step_avg:60.26ms
step:141/2315 train_time:8497ms step_avg:60.26ms
step:142/2315 train_time:8556ms step_avg:60.26ms
step:143/2315 train_time:8616ms step_avg:60.25ms
step:144/2315 train_time:8676ms step_avg:60.25ms
step:145/2315 train_time:8736ms step_avg:60.25ms
step:146/2315 train_time:8796ms step_avg:60.25ms
step:147/2315 train_time:8856ms step_avg:60.25ms
step:148/2315 train_time:8916ms step_avg:60.24ms
step:149/2315 train_time:8976ms step_avg:60.24ms
step:150/2315 train_time:9036ms step_avg:60.24ms
step:151/2315 train_time:9096ms step_avg:60.24ms
step:152/2315 train_time:9155ms step_avg:60.23ms
step:153/2315 train_time:9216ms step_avg:60.24ms
step:154/2315 train_time:9276ms step_avg:60.23ms
step:155/2315 train_time:9336ms step_avg:60.23ms
step:156/2315 train_time:9395ms step_avg:60.23ms
step:157/2315 train_time:9456ms step_avg:60.23ms
step:158/2315 train_time:9515ms step_avg:60.22ms
step:159/2315 train_time:9575ms step_avg:60.22ms
step:160/2315 train_time:9635ms step_avg:60.22ms
step:161/2315 train_time:9695ms step_avg:60.22ms
step:162/2315 train_time:9755ms step_avg:60.22ms
step:163/2315 train_time:9816ms step_avg:60.22ms
step:164/2315 train_time:9875ms step_avg:60.21ms
step:165/2315 train_time:9935ms step_avg:60.21ms
step:166/2315 train_time:9994ms step_avg:60.21ms
step:167/2315 train_time:10054ms step_avg:60.20ms
step:168/2315 train_time:10114ms step_avg:60.20ms
step:169/2315 train_time:10174ms step_avg:60.20ms
step:170/2315 train_time:10234ms step_avg:60.20ms
step:171/2315 train_time:10294ms step_avg:60.20ms
step:172/2315 train_time:10355ms step_avg:60.20ms
step:173/2315 train_time:10415ms step_avg:60.20ms
step:174/2315 train_time:10474ms step_avg:60.20ms
step:175/2315 train_time:10534ms step_avg:60.20ms
step:176/2315 train_time:10594ms step_avg:60.19ms
step:177/2315 train_time:10654ms step_avg:60.19ms
step:178/2315 train_time:10714ms step_avg:60.19ms
step:179/2315 train_time:10774ms step_avg:60.19ms
step:180/2315 train_time:10834ms step_avg:60.19ms
step:181/2315 train_time:10894ms step_avg:60.19ms
step:182/2315 train_time:10954ms step_avg:60.19ms
step:183/2315 train_time:11015ms step_avg:60.19ms
step:184/2315 train_time:11075ms step_avg:60.19ms
step:185/2315 train_time:11134ms step_avg:60.19ms
step:186/2315 train_time:11194ms step_avg:60.18ms
step:187/2315 train_time:11254ms step_avg:60.18ms
step:188/2315 train_time:11315ms step_avg:60.19ms
step:189/2315 train_time:11375ms step_avg:60.18ms
step:190/2315 train_time:11434ms step_avg:60.18ms
step:191/2315 train_time:11495ms step_avg:60.18ms
step:192/2315 train_time:11555ms step_avg:60.18ms
step:193/2315 train_time:11614ms step_avg:60.18ms
step:194/2315 train_time:11674ms step_avg:60.18ms
step:195/2315 train_time:11734ms step_avg:60.17ms
step:196/2315 train_time:11795ms step_avg:60.18ms
step:197/2315 train_time:11855ms step_avg:60.18ms
step:198/2315 train_time:11915ms step_avg:60.18ms
step:199/2315 train_time:11975ms step_avg:60.18ms
step:200/2315 train_time:12035ms step_avg:60.17ms
step:201/2315 train_time:12095ms step_avg:60.17ms
step:202/2315 train_time:12155ms step_avg:60.17ms
step:203/2315 train_time:12215ms step_avg:60.17ms
step:204/2315 train_time:12275ms step_avg:60.17ms
step:205/2315 train_time:12335ms step_avg:60.17ms
step:206/2315 train_time:12394ms step_avg:60.17ms
step:207/2315 train_time:12455ms step_avg:60.17ms
step:208/2315 train_time:12514ms step_avg:60.16ms
step:209/2315 train_time:12574ms step_avg:60.16ms
step:210/2315 train_time:12634ms step_avg:60.16ms
step:211/2315 train_time:12694ms step_avg:60.16ms
step:212/2315 train_time:12755ms step_avg:60.16ms
step:213/2315 train_time:12814ms step_avg:60.16ms
step:214/2315 train_time:12874ms step_avg:60.16ms
step:215/2315 train_time:12933ms step_avg:60.15ms
step:216/2315 train_time:12993ms step_avg:60.15ms
step:217/2315 train_time:13053ms step_avg:60.15ms
step:218/2315 train_time:13114ms step_avg:60.15ms
step:219/2315 train_time:13174ms step_avg:60.15ms
step:220/2315 train_time:13233ms step_avg:60.15ms
step:221/2315 train_time:13293ms step_avg:60.15ms
step:222/2315 train_time:13354ms step_avg:60.15ms
step:223/2315 train_time:13414ms step_avg:60.15ms
step:224/2315 train_time:13473ms step_avg:60.15ms
step:225/2315 train_time:13534ms step_avg:60.15ms
step:226/2315 train_time:13594ms step_avg:60.15ms
step:227/2315 train_time:13654ms step_avg:60.15ms
step:228/2315 train_time:13714ms step_avg:60.15ms
step:229/2315 train_time:13774ms step_avg:60.15ms
step:230/2315 train_time:13833ms step_avg:60.15ms
step:231/2315 train_time:13894ms step_avg:60.15ms
step:232/2315 train_time:13954ms step_avg:60.15ms
step:233/2315 train_time:14014ms step_avg:60.15ms
step:234/2315 train_time:14074ms step_avg:60.15ms
step:235/2315 train_time:14134ms step_avg:60.15ms
step:236/2315 train_time:14194ms step_avg:60.15ms
step:237/2315 train_time:14254ms step_avg:60.15ms
step:238/2315 train_time:14314ms step_avg:60.14ms
step:239/2315 train_time:14374ms step_avg:60.14ms
step:240/2315 train_time:14434ms step_avg:60.14ms
step:241/2315 train_time:14493ms step_avg:60.14ms
step:242/2315 train_time:14553ms step_avg:60.14ms
step:243/2315 train_time:14613ms step_avg:60.14ms
step:244/2315 train_time:14673ms step_avg:60.13ms
step:245/2315 train_time:14732ms step_avg:60.13ms
step:246/2315 train_time:14793ms step_avg:60.13ms
step:247/2315 train_time:14852ms step_avg:60.13ms
step:248/2315 train_time:14912ms step_avg:60.13ms
step:249/2315 train_time:14972ms step_avg:60.13ms
step:250/2315 train_time:15032ms step_avg:60.13ms
step:250/2315 val_loss:4.0707 train_time:15095ms step_avg:60.38ms
step:251/2315 train_time:15115ms step_avg:60.22ms
step:252/2315 train_time:15154ms step_avg:60.14ms
step:253/2315 train_time:15220ms step_avg:60.16ms
step:254/2315 train_time:15285ms step_avg:60.18ms
step:255/2315 train_time:15346ms step_avg:60.18ms
step:256/2315 train_time:15406ms step_avg:60.18ms
step:257/2315 train_time:15465ms step_avg:60.18ms
step:258/2315 train_time:15524ms step_avg:60.17ms
step:259/2315 train_time:15584ms step_avg:60.17ms
step:260/2315 train_time:15643ms step_avg:60.17ms
step:261/2315 train_time:15702ms step_avg:60.16ms
step:262/2315 train_time:15761ms step_avg:60.16ms
step:263/2315 train_time:15820ms step_avg:60.15ms
step:264/2315 train_time:15880ms step_avg:60.15ms
step:265/2315 train_time:15938ms step_avg:60.14ms
step:266/2315 train_time:15998ms step_avg:60.14ms
step:267/2315 train_time:16058ms step_avg:60.14ms
step:268/2315 train_time:16118ms step_avg:60.14ms
step:269/2315 train_time:16180ms step_avg:60.15ms
step:270/2315 train_time:16242ms step_avg:60.15ms
step:271/2315 train_time:16304ms step_avg:60.16ms
step:272/2315 train_time:16364ms step_avg:60.16ms
step:273/2315 train_time:16425ms step_avg:60.16ms
step:274/2315 train_time:16484ms step_avg:60.16ms
step:275/2315 train_time:16544ms step_avg:60.16ms
step:276/2315 train_time:16603ms step_avg:60.16ms
step:277/2315 train_time:16663ms step_avg:60.15ms
step:278/2315 train_time:16722ms step_avg:60.15ms
step:279/2315 train_time:16782ms step_avg:60.15ms
step:280/2315 train_time:16841ms step_avg:60.15ms
step:281/2315 train_time:16901ms step_avg:60.15ms
step:282/2315 train_time:16961ms step_avg:60.15ms
step:283/2315 train_time:17022ms step_avg:60.15ms
step:284/2315 train_time:17083ms step_avg:60.15ms
step:285/2315 train_time:17143ms step_avg:60.15ms
step:286/2315 train_time:17204ms step_avg:60.15ms
step:287/2315 train_time:17265ms step_avg:60.16ms
step:288/2315 train_time:17325ms step_avg:60.16ms
step:289/2315 train_time:17386ms step_avg:60.16ms
step:290/2315 train_time:17446ms step_avg:60.16ms
step:291/2315 train_time:17505ms step_avg:60.16ms
step:292/2315 train_time:17565ms step_avg:60.15ms
step:293/2315 train_time:17624ms step_avg:60.15ms
step:294/2315 train_time:17683ms step_avg:60.15ms
step:295/2315 train_time:17743ms step_avg:60.14ms
step:296/2315 train_time:17802ms step_avg:60.14ms
step:297/2315 train_time:17862ms step_avg:60.14ms
step:298/2315 train_time:17921ms step_avg:60.14ms
step:299/2315 train_time:17982ms step_avg:60.14ms
step:300/2315 train_time:18042ms step_avg:60.14ms
step:301/2315 train_time:18103ms step_avg:60.14ms
step:302/2315 train_time:18163ms step_avg:60.14ms
step:303/2315 train_time:18223ms step_avg:60.14ms
step:304/2315 train_time:18284ms step_avg:60.14ms
step:305/2315 train_time:18344ms step_avg:60.14ms
step:306/2315 train_time:18403ms step_avg:60.14ms
step:307/2315 train_time:18464ms step_avg:60.14ms
step:308/2315 train_time:18523ms step_avg:60.14ms
step:309/2315 train_time:18583ms step_avg:60.14ms
step:310/2315 train_time:18643ms step_avg:60.14ms
step:311/2315 train_time:18702ms step_avg:60.14ms
step:312/2315 train_time:18762ms step_avg:60.13ms
step:313/2315 train_time:18821ms step_avg:60.13ms
step:314/2315 train_time:18880ms step_avg:60.13ms
step:315/2315 train_time:18940ms step_avg:60.13ms
step:316/2315 train_time:19000ms step_avg:60.13ms
step:317/2315 train_time:19060ms step_avg:60.13ms
step:318/2315 train_time:19121ms step_avg:60.13ms
step:319/2315 train_time:19181ms step_avg:60.13ms
step:320/2315 train_time:19241ms step_avg:60.13ms
step:321/2315 train_time:19301ms step_avg:60.13ms
step:322/2315 train_time:19361ms step_avg:60.13ms
step:323/2315 train_time:19422ms step_avg:60.13ms
step:324/2315 train_time:19482ms step_avg:60.13ms
step:325/2315 train_time:19542ms step_avg:60.13ms
step:326/2315 train_time:19602ms step_avg:60.13ms
step:327/2315 train_time:19662ms step_avg:60.13ms
step:328/2315 train_time:19722ms step_avg:60.13ms
step:329/2315 train_time:19782ms step_avg:60.13ms
step:330/2315 train_time:19841ms step_avg:60.12ms
step:331/2315 train_time:19901ms step_avg:60.12ms
step:332/2315 train_time:19961ms step_avg:60.12ms
step:333/2315 train_time:20021ms step_avg:60.12ms
step:334/2315 train_time:20081ms step_avg:60.12ms
step:335/2315 train_time:20141ms step_avg:60.12ms
step:336/2315 train_time:20201ms step_avg:60.12ms
step:337/2315 train_time:20262ms step_avg:60.12ms
step:338/2315 train_time:20321ms step_avg:60.12ms
step:339/2315 train_time:20382ms step_avg:60.12ms
step:340/2315 train_time:20442ms step_avg:60.12ms
step:341/2315 train_time:20502ms step_avg:60.12ms
step:342/2315 train_time:20563ms step_avg:60.12ms
step:343/2315 train_time:20622ms step_avg:60.12ms
step:344/2315 train_time:20682ms step_avg:60.12ms
step:345/2315 train_time:20742ms step_avg:60.12ms
step:346/2315 train_time:20801ms step_avg:60.12ms
step:347/2315 train_time:20862ms step_avg:60.12ms
step:348/2315 train_time:20921ms step_avg:60.12ms
step:349/2315 train_time:20982ms step_avg:60.12ms
step:350/2315 train_time:21042ms step_avg:60.12ms
step:351/2315 train_time:21101ms step_avg:60.12ms
step:352/2315 train_time:21161ms step_avg:60.12ms
step:353/2315 train_time:21222ms step_avg:60.12ms
step:354/2315 train_time:21283ms step_avg:60.12ms
step:355/2315 train_time:21343ms step_avg:60.12ms
step:356/2315 train_time:21403ms step_avg:60.12ms
step:357/2315 train_time:21463ms step_avg:60.12ms
step:358/2315 train_time:21524ms step_avg:60.12ms
step:359/2315 train_time:21584ms step_avg:60.12ms
step:360/2315 train_time:21644ms step_avg:60.12ms
step:361/2315 train_time:21704ms step_avg:60.12ms
step:362/2315 train_time:21763ms step_avg:60.12ms
step:363/2315 train_time:21823ms step_avg:60.12ms
step:364/2315 train_time:21883ms step_avg:60.12ms
step:365/2315 train_time:21943ms step_avg:60.12ms
step:366/2315 train_time:22003ms step_avg:60.12ms
step:367/2315 train_time:22063ms step_avg:60.12ms
step:368/2315 train_time:22123ms step_avg:60.12ms
step:369/2315 train_time:22183ms step_avg:60.12ms
step:370/2315 train_time:22243ms step_avg:60.12ms
step:371/2315 train_time:22303ms step_avg:60.12ms
step:372/2315 train_time:22363ms step_avg:60.12ms
step:373/2315 train_time:22423ms step_avg:60.12ms
step:374/2315 train_time:22483ms step_avg:60.12ms
step:375/2315 train_time:22543ms step_avg:60.12ms
step:376/2315 train_time:22603ms step_avg:60.12ms
step:377/2315 train_time:22663ms step_avg:60.11ms
step:378/2315 train_time:22723ms step_avg:60.11ms
step:379/2315 train_time:22783ms step_avg:60.11ms
step:380/2315 train_time:22842ms step_avg:60.11ms
step:381/2315 train_time:22902ms step_avg:60.11ms
step:382/2315 train_time:22962ms step_avg:60.11ms
step:383/2315 train_time:23022ms step_avg:60.11ms
step:384/2315 train_time:23082ms step_avg:60.11ms
step:385/2315 train_time:23142ms step_avg:60.11ms
step:386/2315 train_time:23202ms step_avg:60.11ms
step:387/2315 train_time:23262ms step_avg:60.11ms
step:388/2315 train_time:23322ms step_avg:60.11ms
step:389/2315 train_time:23382ms step_avg:60.11ms
step:390/2315 train_time:23442ms step_avg:60.11ms
step:391/2315 train_time:23502ms step_avg:60.11ms
step:392/2315 train_time:23562ms step_avg:60.11ms
step:393/2315 train_time:23623ms step_avg:60.11ms
step:394/2315 train_time:23682ms step_avg:60.11ms
step:395/2315 train_time:23742ms step_avg:60.11ms
step:396/2315 train_time:23802ms step_avg:60.11ms
step:397/2315 train_time:23862ms step_avg:60.11ms
step:398/2315 train_time:23922ms step_avg:60.11ms
step:399/2315 train_time:23983ms step_avg:60.11ms
step:400/2315 train_time:24042ms step_avg:60.11ms
step:401/2315 train_time:24102ms step_avg:60.11ms
step:402/2315 train_time:24162ms step_avg:60.10ms
step:403/2315 train_time:24222ms step_avg:60.10ms
step:404/2315 train_time:24282ms step_avg:60.10ms
step:405/2315 train_time:24342ms step_avg:60.10ms
step:406/2315 train_time:24402ms step_avg:60.10ms
step:407/2315 train_time:24462ms step_avg:60.10ms
step:408/2315 train_time:24522ms step_avg:60.10ms
step:409/2315 train_time:24582ms step_avg:60.10ms
step:410/2315 train_time:24642ms step_avg:60.10ms
step:411/2315 train_time:24703ms step_avg:60.10ms
step:412/2315 train_time:24762ms step_avg:60.10ms
step:413/2315 train_time:24823ms step_avg:60.10ms
step:414/2315 train_time:24883ms step_avg:60.10ms
step:415/2315 train_time:24943ms step_avg:60.10ms
step:416/2315 train_time:25003ms step_avg:60.10ms
step:417/2315 train_time:25062ms step_avg:60.10ms
step:418/2315 train_time:25122ms step_avg:60.10ms
step:419/2315 train_time:25182ms step_avg:60.10ms
step:420/2315 train_time:25242ms step_avg:60.10ms
step:421/2315 train_time:25302ms step_avg:60.10ms
step:422/2315 train_time:25362ms step_avg:60.10ms
step:423/2315 train_time:25422ms step_avg:60.10ms
step:424/2315 train_time:25483ms step_avg:60.10ms
step:425/2315 train_time:25543ms step_avg:60.10ms
step:426/2315 train_time:25602ms step_avg:60.10ms
step:427/2315 train_time:25663ms step_avg:60.10ms
step:428/2315 train_time:25723ms step_avg:60.10ms
step:429/2315 train_time:25783ms step_avg:60.10ms
step:430/2315 train_time:25843ms step_avg:60.10ms
step:431/2315 train_time:25903ms step_avg:60.10ms
step:432/2315 train_time:25963ms step_avg:60.10ms
step:433/2315 train_time:26023ms step_avg:60.10ms
step:434/2315 train_time:26083ms step_avg:60.10ms
step:435/2315 train_time:26144ms step_avg:60.10ms
step:436/2315 train_time:26203ms step_avg:60.10ms
step:437/2315 train_time:26263ms step_avg:60.10ms
step:438/2315 train_time:26323ms step_avg:60.10ms
step:439/2315 train_time:26383ms step_avg:60.10ms
step:440/2315 train_time:26443ms step_avg:60.10ms
step:441/2315 train_time:26503ms step_avg:60.10ms
step:442/2315 train_time:26562ms step_avg:60.10ms
step:443/2315 train_time:26622ms step_avg:60.10ms
step:444/2315 train_time:26682ms step_avg:60.09ms
step:445/2315 train_time:26742ms step_avg:60.10ms
step:446/2315 train_time:26802ms step_avg:60.09ms
step:447/2315 train_time:26862ms step_avg:60.09ms
step:448/2315 train_time:26922ms step_avg:60.09ms
step:449/2315 train_time:26982ms step_avg:60.09ms
step:450/2315 train_time:27042ms step_avg:60.09ms
step:451/2315 train_time:27102ms step_avg:60.09ms
step:452/2315 train_time:27161ms step_avg:60.09ms
step:453/2315 train_time:27221ms step_avg:60.09ms
step:454/2315 train_time:27281ms step_avg:60.09ms
step:455/2315 train_time:27341ms step_avg:60.09ms
step:456/2315 train_time:27401ms step_avg:60.09ms
step:457/2315 train_time:27460ms step_avg:60.09ms
step:458/2315 train_time:27520ms step_avg:60.09ms
step:459/2315 train_time:27581ms step_avg:60.09ms
step:460/2315 train_time:27640ms step_avg:60.09ms
step:461/2315 train_time:27701ms step_avg:60.09ms
step:462/2315 train_time:27761ms step_avg:60.09ms
step:463/2315 train_time:27821ms step_avg:60.09ms
step:464/2315 train_time:27880ms step_avg:60.09ms
step:465/2315 train_time:27941ms step_avg:60.09ms
step:466/2315 train_time:28000ms step_avg:60.09ms
step:467/2315 train_time:28060ms step_avg:60.09ms
step:468/2315 train_time:28120ms step_avg:60.09ms
step:469/2315 train_time:28180ms step_avg:60.09ms
step:470/2315 train_time:28240ms step_avg:60.09ms
step:471/2315 train_time:28300ms step_avg:60.08ms
step:472/2315 train_time:28360ms step_avg:60.08ms
step:473/2315 train_time:28420ms step_avg:60.08ms
step:474/2315 train_time:28480ms step_avg:60.08ms
step:475/2315 train_time:28540ms step_avg:60.08ms
step:476/2315 train_time:28600ms step_avg:60.08ms
step:477/2315 train_time:28660ms step_avg:60.08ms
step:478/2315 train_time:28720ms step_avg:60.08ms
step:479/2315 train_time:28779ms step_avg:60.08ms
step:480/2315 train_time:28839ms step_avg:60.08ms
step:481/2315 train_time:28899ms step_avg:60.08ms
step:482/2315 train_time:28959ms step_avg:60.08ms
step:483/2315 train_time:29018ms step_avg:60.08ms
step:484/2315 train_time:29079ms step_avg:60.08ms
step:485/2315 train_time:29139ms step_avg:60.08ms
step:486/2315 train_time:29199ms step_avg:60.08ms
step:487/2315 train_time:29259ms step_avg:60.08ms
step:488/2315 train_time:29319ms step_avg:60.08ms
step:489/2315 train_time:29379ms step_avg:60.08ms
step:490/2315 train_time:29439ms step_avg:60.08ms
step:491/2315 train_time:29499ms step_avg:60.08ms
step:492/2315 train_time:29558ms step_avg:60.08ms
step:493/2315 train_time:29618ms step_avg:60.08ms
step:494/2315 train_time:29678ms step_avg:60.08ms
step:495/2315 train_time:29738ms step_avg:60.08ms
step:496/2315 train_time:29798ms step_avg:60.08ms
step:497/2315 train_time:29858ms step_avg:60.08ms
step:498/2315 train_time:29917ms step_avg:60.08ms
step:499/2315 train_time:29977ms step_avg:60.07ms
step:500/2315 train_time:30037ms step_avg:60.07ms
step:500/2315 val_loss:3.8161 train_time:30099ms step_avg:60.20ms
step:501/2315 train_time:30119ms step_avg:60.12ms
step:502/2315 train_time:30161ms step_avg:60.08ms
step:503/2315 train_time:30224ms step_avg:60.09ms
step:504/2315 train_time:30287ms step_avg:60.09ms
step:505/2315 train_time:30347ms step_avg:60.09ms
step:506/2315 train_time:30407ms step_avg:60.09ms
step:507/2315 train_time:30466ms step_avg:60.09ms
step:508/2315 train_time:30525ms step_avg:60.09ms
step:509/2315 train_time:30585ms step_avg:60.09ms
step:510/2315 train_time:30645ms step_avg:60.09ms
step:511/2315 train_time:30705ms step_avg:60.09ms
step:512/2315 train_time:30765ms step_avg:60.09ms
step:513/2315 train_time:30825ms step_avg:60.09ms
step:514/2315 train_time:30884ms step_avg:60.09ms
step:515/2315 train_time:30944ms step_avg:60.08ms
step:516/2315 train_time:31003ms step_avg:60.08ms
step:517/2315 train_time:31064ms step_avg:60.09ms
step:518/2315 train_time:31125ms step_avg:60.09ms
step:519/2315 train_time:31186ms step_avg:60.09ms
step:520/2315 train_time:31247ms step_avg:60.09ms
step:521/2315 train_time:31308ms step_avg:60.09ms
step:522/2315 train_time:31368ms step_avg:60.09ms
step:523/2315 train_time:31428ms step_avg:60.09ms
step:524/2315 train_time:31487ms step_avg:60.09ms
step:525/2315 train_time:31546ms step_avg:60.09ms
step:526/2315 train_time:31606ms step_avg:60.09ms
step:527/2315 train_time:31665ms step_avg:60.09ms
step:528/2315 train_time:31725ms step_avg:60.09ms
step:529/2315 train_time:31786ms step_avg:60.09ms
step:530/2315 train_time:31845ms step_avg:60.08ms
step:531/2315 train_time:31904ms step_avg:60.08ms
step:532/2315 train_time:31964ms step_avg:60.08ms
step:533/2315 train_time:32024ms step_avg:60.08ms
step:534/2315 train_time:32085ms step_avg:60.08ms
step:535/2315 train_time:32146ms step_avg:60.09ms
step:536/2315 train_time:32206ms step_avg:60.09ms
step:537/2315 train_time:32266ms step_avg:60.09ms
step:538/2315 train_time:32327ms step_avg:60.09ms
step:539/2315 train_time:32387ms step_avg:60.09ms
step:540/2315 train_time:32447ms step_avg:60.09ms
step:541/2315 train_time:32508ms step_avg:60.09ms
step:542/2315 train_time:32568ms step_avg:60.09ms
step:543/2315 train_time:32627ms step_avg:60.09ms
step:544/2315 train_time:32687ms step_avg:60.09ms
step:545/2315 train_time:32746ms step_avg:60.08ms
step:546/2315 train_time:32806ms step_avg:60.08ms
step:547/2315 train_time:32866ms step_avg:60.08ms
step:548/2315 train_time:32926ms step_avg:60.08ms
step:549/2315 train_time:32986ms step_avg:60.08ms
step:550/2315 train_time:33045ms step_avg:60.08ms
step:551/2315 train_time:33106ms step_avg:60.08ms
step:552/2315 train_time:33166ms step_avg:60.08ms
step:553/2315 train_time:33227ms step_avg:60.08ms
step:554/2315 train_time:33286ms step_avg:60.08ms
step:555/2315 train_time:33347ms step_avg:60.08ms
step:556/2315 train_time:33407ms step_avg:60.08ms
step:557/2315 train_time:33467ms step_avg:60.08ms
step:558/2315 train_time:33526ms step_avg:60.08ms
step:559/2315 train_time:33586ms step_avg:60.08ms
step:560/2315 train_time:33646ms step_avg:60.08ms
step:561/2315 train_time:33706ms step_avg:60.08ms
step:562/2315 train_time:33766ms step_avg:60.08ms
step:563/2315 train_time:33825ms step_avg:60.08ms
step:564/2315 train_time:33885ms step_avg:60.08ms
step:565/2315 train_time:33945ms step_avg:60.08ms
step:566/2315 train_time:34005ms step_avg:60.08ms
step:567/2315 train_time:34065ms step_avg:60.08ms
step:568/2315 train_time:34125ms step_avg:60.08ms
step:569/2315 train_time:34186ms step_avg:60.08ms
step:570/2315 train_time:34246ms step_avg:60.08ms
step:571/2315 train_time:34306ms step_avg:60.08ms
step:572/2315 train_time:34366ms step_avg:60.08ms
step:573/2315 train_time:34426ms step_avg:60.08ms
step:574/2315 train_time:34486ms step_avg:60.08ms
step:575/2315 train_time:34546ms step_avg:60.08ms
step:576/2315 train_time:34605ms step_avg:60.08ms
step:577/2315 train_time:34665ms step_avg:60.08ms
step:578/2315 train_time:34725ms step_avg:60.08ms
step:579/2315 train_time:34785ms step_avg:60.08ms
step:580/2315 train_time:34845ms step_avg:60.08ms
step:581/2315 train_time:34905ms step_avg:60.08ms
step:582/2315 train_time:34964ms step_avg:60.08ms
step:583/2315 train_time:35025ms step_avg:60.08ms
step:584/2315 train_time:35084ms step_avg:60.08ms
step:585/2315 train_time:35144ms step_avg:60.08ms
step:586/2315 train_time:35205ms step_avg:60.08ms
step:587/2315 train_time:35266ms step_avg:60.08ms
step:588/2315 train_time:35326ms step_avg:60.08ms
step:589/2315 train_time:35386ms step_avg:60.08ms
step:590/2315 train_time:35447ms step_avg:60.08ms
step:591/2315 train_time:35506ms step_avg:60.08ms
step:592/2315 train_time:35566ms step_avg:60.08ms
step:593/2315 train_time:35626ms step_avg:60.08ms
step:594/2315 train_time:35686ms step_avg:60.08ms
step:595/2315 train_time:35746ms step_avg:60.08ms
step:596/2315 train_time:35806ms step_avg:60.08ms
step:597/2315 train_time:35866ms step_avg:60.08ms
step:598/2315 train_time:35926ms step_avg:60.08ms
step:599/2315 train_time:35986ms step_avg:60.08ms
step:600/2315 train_time:36045ms step_avg:60.08ms
step:601/2315 train_time:36105ms step_avg:60.08ms
step:602/2315 train_time:36165ms step_avg:60.08ms
step:603/2315 train_time:36225ms step_avg:60.08ms
step:604/2315 train_time:36285ms step_avg:60.08ms
step:605/2315 train_time:36346ms step_avg:60.08ms
step:606/2315 train_time:36406ms step_avg:60.08ms
step:607/2315 train_time:36466ms step_avg:60.08ms
step:608/2315 train_time:36526ms step_avg:60.08ms
step:609/2315 train_time:36586ms step_avg:60.07ms
step:610/2315 train_time:36645ms step_avg:60.07ms
step:611/2315 train_time:36706ms step_avg:60.08ms
step:612/2315 train_time:36765ms step_avg:60.07ms
step:613/2315 train_time:36826ms step_avg:60.07ms
step:614/2315 train_time:36885ms step_avg:60.07ms
step:615/2315 train_time:36945ms step_avg:60.07ms
step:616/2315 train_time:37005ms step_avg:60.07ms
step:617/2315 train_time:37065ms step_avg:60.07ms
step:618/2315 train_time:37125ms step_avg:60.07ms
step:619/2315 train_time:37185ms step_avg:60.07ms
step:620/2315 train_time:37245ms step_avg:60.07ms
step:621/2315 train_time:37305ms step_avg:60.07ms
step:622/2315 train_time:37365ms step_avg:60.07ms
step:623/2315 train_time:37425ms step_avg:60.07ms
step:624/2315 train_time:37485ms step_avg:60.07ms
step:625/2315 train_time:37545ms step_avg:60.07ms
step:626/2315 train_time:37605ms step_avg:60.07ms
step:627/2315 train_time:37665ms step_avg:60.07ms
step:628/2315 train_time:37725ms step_avg:60.07ms
step:629/2315 train_time:37785ms step_avg:60.07ms
step:630/2315 train_time:37844ms step_avg:60.07ms
step:631/2315 train_time:37904ms step_avg:60.07ms
step:632/2315 train_time:37964ms step_avg:60.07ms
step:633/2315 train_time:38024ms step_avg:60.07ms
step:634/2315 train_time:38085ms step_avg:60.07ms
step:635/2315 train_time:38144ms step_avg:60.07ms
step:636/2315 train_time:38204ms step_avg:60.07ms
step:637/2315 train_time:38264ms step_avg:60.07ms
step:638/2315 train_time:38324ms step_avg:60.07ms
step:639/2315 train_time:38383ms step_avg:60.07ms
step:640/2315 train_time:38443ms step_avg:60.07ms
step:641/2315 train_time:38503ms step_avg:60.07ms
step:642/2315 train_time:38563ms step_avg:60.07ms
step:643/2315 train_time:38623ms step_avg:60.07ms
step:644/2315 train_time:38683ms step_avg:60.07ms
step:645/2315 train_time:38743ms step_avg:60.07ms
step:646/2315 train_time:38803ms step_avg:60.07ms
step:647/2315 train_time:38864ms step_avg:60.07ms
step:648/2315 train_time:38924ms step_avg:60.07ms
step:649/2315 train_time:38985ms step_avg:60.07ms
step:650/2315 train_time:39044ms step_avg:60.07ms
step:651/2315 train_time:39104ms step_avg:60.07ms
step:652/2315 train_time:39163ms step_avg:60.07ms
step:653/2315 train_time:39224ms step_avg:60.07ms
step:654/2315 train_time:39284ms step_avg:60.07ms
step:655/2315 train_time:39344ms step_avg:60.07ms
step:656/2315 train_time:39403ms step_avg:60.07ms
step:657/2315 train_time:39463ms step_avg:60.07ms
step:658/2315 train_time:39523ms step_avg:60.07ms
step:659/2315 train_time:39584ms step_avg:60.07ms
step:660/2315 train_time:39644ms step_avg:60.07ms
step:661/2315 train_time:39704ms step_avg:60.07ms
step:662/2315 train_time:39764ms step_avg:60.07ms
step:663/2315 train_time:39824ms step_avg:60.07ms
step:664/2315 train_time:39884ms step_avg:60.07ms
step:665/2315 train_time:39945ms step_avg:60.07ms
step:666/2315 train_time:40005ms step_avg:60.07ms
step:667/2315 train_time:40065ms step_avg:60.07ms
step:668/2315 train_time:40125ms step_avg:60.07ms
step:669/2315 train_time:40185ms step_avg:60.07ms
step:670/2315 train_time:40245ms step_avg:60.07ms
step:671/2315 train_time:40305ms step_avg:60.07ms
step:672/2315 train_time:40365ms step_avg:60.07ms
step:673/2315 train_time:40425ms step_avg:60.07ms
step:674/2315 train_time:40485ms step_avg:60.07ms
step:675/2315 train_time:40545ms step_avg:60.07ms
step:676/2315 train_time:40604ms step_avg:60.07ms
step:677/2315 train_time:40665ms step_avg:60.07ms
step:678/2315 train_time:40725ms step_avg:60.07ms
step:679/2315 train_time:40786ms step_avg:60.07ms
step:680/2315 train_time:40846ms step_avg:60.07ms
step:681/2315 train_time:40905ms step_avg:60.07ms
step:682/2315 train_time:40965ms step_avg:60.07ms
step:683/2315 train_time:41026ms step_avg:60.07ms
step:684/2315 train_time:41085ms step_avg:60.07ms
step:685/2315 train_time:41145ms step_avg:60.07ms
step:686/2315 train_time:41205ms step_avg:60.06ms
step:687/2315 train_time:41265ms step_avg:60.07ms
step:688/2315 train_time:41325ms step_avg:60.07ms
step:689/2315 train_time:41385ms step_avg:60.07ms
step:690/2315 train_time:41445ms step_avg:60.07ms
step:691/2315 train_time:41505ms step_avg:60.06ms
step:692/2315 train_time:41565ms step_avg:60.07ms
step:693/2315 train_time:41626ms step_avg:60.07ms
step:694/2315 train_time:41685ms step_avg:60.07ms
step:695/2315 train_time:41745ms step_avg:60.07ms
step:696/2315 train_time:41805ms step_avg:60.06ms
step:697/2315 train_time:41865ms step_avg:60.07ms
step:698/2315 train_time:41926ms step_avg:60.07ms
step:699/2315 train_time:41987ms step_avg:60.07ms
step:700/2315 train_time:42046ms step_avg:60.07ms
step:701/2315 train_time:42106ms step_avg:60.07ms
step:702/2315 train_time:42166ms step_avg:60.07ms
step:703/2315 train_time:42226ms step_avg:60.07ms
step:704/2315 train_time:42286ms step_avg:60.07ms
step:705/2315 train_time:42346ms step_avg:60.06ms
step:706/2315 train_time:42406ms step_avg:60.06ms
step:707/2315 train_time:42466ms step_avg:60.06ms
step:708/2315 train_time:42526ms step_avg:60.06ms
step:709/2315 train_time:42586ms step_avg:60.06ms
step:710/2315 train_time:42645ms step_avg:60.06ms
step:711/2315 train_time:42705ms step_avg:60.06ms
step:712/2315 train_time:42766ms step_avg:60.06ms
step:713/2315 train_time:42826ms step_avg:60.06ms
step:714/2315 train_time:42886ms step_avg:60.06ms
step:715/2315 train_time:42946ms step_avg:60.06ms
step:716/2315 train_time:43006ms step_avg:60.06ms
step:717/2315 train_time:43067ms step_avg:60.07ms
step:718/2315 train_time:43127ms step_avg:60.07ms
step:719/2315 train_time:43187ms step_avg:60.06ms
step:720/2315 train_time:43246ms step_avg:60.06ms
step:721/2315 train_time:43306ms step_avg:60.06ms
step:722/2315 train_time:43365ms step_avg:60.06ms
step:723/2315 train_time:43425ms step_avg:60.06ms
step:724/2315 train_time:43485ms step_avg:60.06ms
step:725/2315 train_time:43545ms step_avg:60.06ms
step:726/2315 train_time:43605ms step_avg:60.06ms
step:727/2315 train_time:43666ms step_avg:60.06ms
step:728/2315 train_time:43726ms step_avg:60.06ms
step:729/2315 train_time:43786ms step_avg:60.06ms
step:730/2315 train_time:43845ms step_avg:60.06ms
step:731/2315 train_time:43905ms step_avg:60.06ms
step:732/2315 train_time:43965ms step_avg:60.06ms
step:733/2315 train_time:44026ms step_avg:60.06ms
step:734/2315 train_time:44085ms step_avg:60.06ms
step:735/2315 train_time:44146ms step_avg:60.06ms
step:736/2315 train_time:44205ms step_avg:60.06ms
step:737/2315 train_time:44265ms step_avg:60.06ms
step:738/2315 train_time:44325ms step_avg:60.06ms
step:739/2315 train_time:44385ms step_avg:60.06ms
step:740/2315 train_time:44445ms step_avg:60.06ms
step:741/2315 train_time:44505ms step_avg:60.06ms
step:742/2315 train_time:44565ms step_avg:60.06ms
step:743/2315 train_time:44625ms step_avg:60.06ms
step:744/2315 train_time:44685ms step_avg:60.06ms
step:745/2315 train_time:44746ms step_avg:60.06ms
step:746/2315 train_time:44806ms step_avg:60.06ms
step:747/2315 train_time:44866ms step_avg:60.06ms
step:748/2315 train_time:44926ms step_avg:60.06ms
step:749/2315 train_time:44986ms step_avg:60.06ms
step:750/2315 train_time:45046ms step_avg:60.06ms
step:750/2315 val_loss:3.6858 train_time:45108ms step_avg:60.14ms
step:751/2315 train_time:45128ms step_avg:60.09ms
step:752/2315 train_time:45168ms step_avg:60.06ms
step:753/2315 train_time:45229ms step_avg:60.07ms
step:754/2315 train_time:45291ms step_avg:60.07ms
step:755/2315 train_time:45351ms step_avg:60.07ms
step:756/2315 train_time:45411ms step_avg:60.07ms
step:757/2315 train_time:45471ms step_avg:60.07ms
step:758/2315 train_time:45530ms step_avg:60.07ms
step:759/2315 train_time:45590ms step_avg:60.07ms
step:760/2315 train_time:45649ms step_avg:60.06ms
step:761/2315 train_time:45709ms step_avg:60.06ms
step:762/2315 train_time:45769ms step_avg:60.06ms
step:763/2315 train_time:45829ms step_avg:60.06ms
step:764/2315 train_time:45890ms step_avg:60.07ms
step:765/2315 train_time:45951ms step_avg:60.07ms
step:766/2315 train_time:46011ms step_avg:60.07ms
step:767/2315 train_time:46072ms step_avg:60.07ms
step:768/2315 train_time:46134ms step_avg:60.07ms
step:769/2315 train_time:46196ms step_avg:60.07ms
step:770/2315 train_time:46257ms step_avg:60.07ms
step:771/2315 train_time:46318ms step_avg:60.07ms
step:772/2315 train_time:46380ms step_avg:60.08ms
step:773/2315 train_time:46440ms step_avg:60.08ms
step:774/2315 train_time:46501ms step_avg:60.08ms
step:775/2315 train_time:46561ms step_avg:60.08ms
step:776/2315 train_time:46622ms step_avg:60.08ms
step:777/2315 train_time:46682ms step_avg:60.08ms
step:778/2315 train_time:46743ms step_avg:60.08ms
step:779/2315 train_time:46804ms step_avg:60.08ms
step:780/2315 train_time:46864ms step_avg:60.08ms
step:781/2315 train_time:46925ms step_avg:60.08ms
step:782/2315 train_time:46986ms step_avg:60.08ms
step:783/2315 train_time:47046ms step_avg:60.08ms
step:784/2315 train_time:47108ms step_avg:60.09ms
step:785/2315 train_time:47170ms step_avg:60.09ms
step:786/2315 train_time:47230ms step_avg:60.09ms
step:787/2315 train_time:47291ms step_avg:60.09ms
step:788/2315 train_time:47352ms step_avg:60.09ms
step:789/2315 train_time:47413ms step_avg:60.09ms
step:790/2315 train_time:47474ms step_avg:60.09ms
step:791/2315 train_time:47535ms step_avg:60.09ms
step:792/2315 train_time:47595ms step_avg:60.09ms
step:793/2315 train_time:47656ms step_avg:60.10ms
step:794/2315 train_time:47716ms step_avg:60.10ms
step:795/2315 train_time:47777ms step_avg:60.10ms
step:796/2315 train_time:47838ms step_avg:60.10ms
step:797/2315 train_time:47899ms step_avg:60.10ms
step:798/2315 train_time:47959ms step_avg:60.10ms
step:799/2315 train_time:48020ms step_avg:60.10ms
step:800/2315 train_time:48081ms step_avg:60.10ms
step:801/2315 train_time:48142ms step_avg:60.10ms
step:802/2315 train_time:48203ms step_avg:60.10ms
step:803/2315 train_time:48265ms step_avg:60.11ms
step:804/2315 train_time:48325ms step_avg:60.11ms
step:805/2315 train_time:48385ms step_avg:60.11ms
step:806/2315 train_time:48446ms step_avg:60.11ms
step:807/2315 train_time:48507ms step_avg:60.11ms
step:808/2315 train_time:48568ms step_avg:60.11ms
step:809/2315 train_time:48629ms step_avg:60.11ms
step:810/2315 train_time:48690ms step_avg:60.11ms
step:811/2315 train_time:48751ms step_avg:60.11ms
step:812/2315 train_time:48811ms step_avg:60.11ms
step:813/2315 train_time:48872ms step_avg:60.11ms
step:814/2315 train_time:48933ms step_avg:60.11ms
step:815/2315 train_time:48994ms step_avg:60.12ms
step:816/2315 train_time:49055ms step_avg:60.12ms
step:817/2315 train_time:49116ms step_avg:60.12ms
step:818/2315 train_time:49177ms step_avg:60.12ms
step:819/2315 train_time:49239ms step_avg:60.12ms
step:820/2315 train_time:49300ms step_avg:60.12ms
step:821/2315 train_time:49361ms step_avg:60.12ms
step:822/2315 train_time:49421ms step_avg:60.12ms
step:823/2315 train_time:49482ms step_avg:60.12ms
step:824/2315 train_time:49543ms step_avg:60.12ms
step:825/2315 train_time:49604ms step_avg:60.13ms
step:826/2315 train_time:49665ms step_avg:60.13ms
step:827/2315 train_time:49727ms step_avg:60.13ms
step:828/2315 train_time:49788ms step_avg:60.13ms
step:829/2315 train_time:49849ms step_avg:60.13ms
step:830/2315 train_time:49909ms step_avg:60.13ms
step:831/2315 train_time:49970ms step_avg:60.13ms
step:832/2315 train_time:50030ms step_avg:60.13ms
step:833/2315 train_time:50091ms step_avg:60.13ms
step:834/2315 train_time:50151ms step_avg:60.13ms
step:835/2315 train_time:50212ms step_avg:60.13ms
step:836/2315 train_time:50273ms step_avg:60.14ms
step:837/2315 train_time:50335ms step_avg:60.14ms
step:838/2315 train_time:50396ms step_avg:60.14ms
step:839/2315 train_time:50458ms step_avg:60.14ms
step:840/2315 train_time:50519ms step_avg:60.14ms
step:841/2315 train_time:50580ms step_avg:60.14ms
step:842/2315 train_time:50641ms step_avg:60.14ms
step:843/2315 train_time:50702ms step_avg:60.14ms
step:844/2315 train_time:50763ms step_avg:60.15ms
step:845/2315 train_time:50824ms step_avg:60.15ms
step:846/2315 train_time:50885ms step_avg:60.15ms
step:847/2315 train_time:50946ms step_avg:60.15ms
step:848/2315 train_time:51007ms step_avg:60.15ms
step:849/2315 train_time:51068ms step_avg:60.15ms
step:850/2315 train_time:51129ms step_avg:60.15ms
step:851/2315 train_time:51190ms step_avg:60.15ms
step:852/2315 train_time:51250ms step_avg:60.15ms
step:853/2315 train_time:51310ms step_avg:60.15ms
step:854/2315 train_time:51370ms step_avg:60.15ms
step:855/2315 train_time:51431ms step_avg:60.15ms
step:856/2315 train_time:51491ms step_avg:60.15ms
step:857/2315 train_time:51553ms step_avg:60.16ms
step:858/2315 train_time:51614ms step_avg:60.16ms
step:859/2315 train_time:51675ms step_avg:60.16ms
step:860/2315 train_time:51736ms step_avg:60.16ms
step:861/2315 train_time:51797ms step_avg:60.16ms
step:862/2315 train_time:51857ms step_avg:60.16ms
step:863/2315 train_time:51918ms step_avg:60.16ms
step:864/2315 train_time:51979ms step_avg:60.16ms
step:865/2315 train_time:52040ms step_avg:60.16ms
step:866/2315 train_time:52101ms step_avg:60.16ms
step:867/2315 train_time:52162ms step_avg:60.16ms
step:868/2315 train_time:52223ms step_avg:60.16ms
step:869/2315 train_time:52284ms step_avg:60.17ms
step:870/2315 train_time:52345ms step_avg:60.17ms
step:871/2315 train_time:52407ms step_avg:60.17ms
step:872/2315 train_time:52468ms step_avg:60.17ms
step:873/2315 train_time:52529ms step_avg:60.17ms
step:874/2315 train_time:52589ms step_avg:60.17ms
step:875/2315 train_time:52650ms step_avg:60.17ms
step:876/2315 train_time:52710ms step_avg:60.17ms
step:877/2315 train_time:52771ms step_avg:60.17ms
step:878/2315 train_time:52832ms step_avg:60.17ms
step:879/2315 train_time:52894ms step_avg:60.17ms
step:880/2315 train_time:52954ms step_avg:60.18ms
step:881/2315 train_time:53015ms step_avg:60.18ms
step:882/2315 train_time:53076ms step_avg:60.18ms
step:883/2315 train_time:53137ms step_avg:60.18ms
step:884/2315 train_time:53198ms step_avg:60.18ms
step:885/2315 train_time:53258ms step_avg:60.18ms
step:886/2315 train_time:53319ms step_avg:60.18ms
step:887/2315 train_time:53380ms step_avg:60.18ms
step:888/2315 train_time:53441ms step_avg:60.18ms
step:889/2315 train_time:53502ms step_avg:60.18ms
step:890/2315 train_time:53562ms step_avg:60.18ms
step:891/2315 train_time:53623ms step_avg:60.18ms
step:892/2315 train_time:53684ms step_avg:60.18ms
step:893/2315 train_time:53745ms step_avg:60.18ms
step:894/2315 train_time:53806ms step_avg:60.19ms
step:895/2315 train_time:53867ms step_avg:60.19ms
step:896/2315 train_time:53928ms step_avg:60.19ms
step:897/2315 train_time:53988ms step_avg:60.19ms
step:898/2315 train_time:54048ms step_avg:60.19ms
step:899/2315 train_time:54109ms step_avg:60.19ms
step:900/2315 train_time:54170ms step_avg:60.19ms
step:901/2315 train_time:54231ms step_avg:60.19ms
step:902/2315 train_time:54291ms step_avg:60.19ms
step:903/2315 train_time:54353ms step_avg:60.19ms
step:904/2315 train_time:54414ms step_avg:60.19ms
step:905/2315 train_time:54474ms step_avg:60.19ms
step:906/2315 train_time:54535ms step_avg:60.19ms
step:907/2315 train_time:54595ms step_avg:60.19ms
step:908/2315 train_time:54656ms step_avg:60.19ms
step:909/2315 train_time:54718ms step_avg:60.20ms
step:910/2315 train_time:54778ms step_avg:60.20ms
step:911/2315 train_time:54839ms step_avg:60.20ms
step:912/2315 train_time:54900ms step_avg:60.20ms
step:913/2315 train_time:54961ms step_avg:60.20ms
step:914/2315 train_time:55022ms step_avg:60.20ms
step:915/2315 train_time:55083ms step_avg:60.20ms
step:916/2315 train_time:55144ms step_avg:60.20ms
step:917/2315 train_time:55205ms step_avg:60.20ms
step:918/2315 train_time:55266ms step_avg:60.20ms
step:919/2315 train_time:55327ms step_avg:60.20ms
step:920/2315 train_time:55388ms step_avg:60.20ms
step:921/2315 train_time:55449ms step_avg:60.21ms
step:922/2315 train_time:55509ms step_avg:60.21ms
step:923/2315 train_time:55570ms step_avg:60.21ms
step:924/2315 train_time:55630ms step_avg:60.21ms
step:925/2315 train_time:55691ms step_avg:60.21ms
step:926/2315 train_time:55752ms step_avg:60.21ms
step:927/2315 train_time:55813ms step_avg:60.21ms
step:928/2315 train_time:55874ms step_avg:60.21ms
step:929/2315 train_time:55935ms step_avg:60.21ms
step:930/2315 train_time:55996ms step_avg:60.21ms
step:931/2315 train_time:56057ms step_avg:60.21ms
step:932/2315 train_time:56118ms step_avg:60.21ms
step:933/2315 train_time:56179ms step_avg:60.21ms
step:934/2315 train_time:56240ms step_avg:60.21ms
step:935/2315 train_time:56301ms step_avg:60.21ms
step:936/2315 train_time:56362ms step_avg:60.22ms
step:937/2315 train_time:56422ms step_avg:60.22ms
step:938/2315 train_time:56483ms step_avg:60.22ms
step:939/2315 train_time:56543ms step_avg:60.22ms
step:940/2315 train_time:56605ms step_avg:60.22ms
step:941/2315 train_time:56665ms step_avg:60.22ms
step:942/2315 train_time:56727ms step_avg:60.22ms
step:943/2315 train_time:56788ms step_avg:60.22ms
step:944/2315 train_time:56848ms step_avg:60.22ms
step:945/2315 train_time:56909ms step_avg:60.22ms
step:946/2315 train_time:56969ms step_avg:60.22ms
step:947/2315 train_time:57030ms step_avg:60.22ms
step:948/2315 train_time:57090ms step_avg:60.22ms
step:949/2315 train_time:57151ms step_avg:60.22ms
step:950/2315 train_time:57213ms step_avg:60.22ms
step:951/2315 train_time:57274ms step_avg:60.22ms
step:952/2315 train_time:57335ms step_avg:60.23ms
step:953/2315 train_time:57396ms step_avg:60.23ms
step:954/2315 train_time:57457ms step_avg:60.23ms
step:955/2315 train_time:57517ms step_avg:60.23ms
step:956/2315 train_time:57578ms step_avg:60.23ms
step:957/2315 train_time:57639ms step_avg:60.23ms
step:958/2315 train_time:57700ms step_avg:60.23ms
step:959/2315 train_time:57761ms step_avg:60.23ms
step:960/2315 train_time:57821ms step_avg:60.23ms
step:961/2315 train_time:57882ms step_avg:60.23ms
step:962/2315 train_time:57943ms step_avg:60.23ms
step:963/2315 train_time:58004ms step_avg:60.23ms
step:964/2315 train_time:58065ms step_avg:60.23ms
step:965/2315 train_time:58125ms step_avg:60.23ms
step:966/2315 train_time:58186ms step_avg:60.23ms
step:967/2315 train_time:58247ms step_avg:60.23ms
step:968/2315 train_time:58308ms step_avg:60.24ms
step:969/2315 train_time:58369ms step_avg:60.24ms
step:970/2315 train_time:58430ms step_avg:60.24ms
step:971/2315 train_time:58490ms step_avg:60.24ms
step:972/2315 train_time:58551ms step_avg:60.24ms
step:973/2315 train_time:58612ms step_avg:60.24ms
step:974/2315 train_time:58672ms step_avg:60.24ms
step:975/2315 train_time:58734ms step_avg:60.24ms
step:976/2315 train_time:58795ms step_avg:60.24ms
step:977/2315 train_time:58855ms step_avg:60.24ms
step:978/2315 train_time:58916ms step_avg:60.24ms
step:979/2315 train_time:58977ms step_avg:60.24ms
step:980/2315 train_time:59037ms step_avg:60.24ms
step:981/2315 train_time:59099ms step_avg:60.24ms
step:982/2315 train_time:59160ms step_avg:60.24ms
step:983/2315 train_time:59220ms step_avg:60.24ms
step:984/2315 train_time:59281ms step_avg:60.25ms
step:985/2315 train_time:59343ms step_avg:60.25ms
step:986/2315 train_time:59403ms step_avg:60.25ms
step:987/2315 train_time:59464ms step_avg:60.25ms
step:988/2315 train_time:59526ms step_avg:60.25ms
step:989/2315 train_time:59587ms step_avg:60.25ms
step:990/2315 train_time:59647ms step_avg:60.25ms
step:991/2315 train_time:59708ms step_avg:60.25ms
step:992/2315 train_time:59769ms step_avg:60.25ms
step:993/2315 train_time:59830ms step_avg:60.25ms
step:994/2315 train_time:59890ms step_avg:60.25ms
step:995/2315 train_time:59951ms step_avg:60.25ms
step:996/2315 train_time:60011ms step_avg:60.25ms
step:997/2315 train_time:60072ms step_avg:60.25ms
step:998/2315 train_time:60133ms step_avg:60.25ms
step:999/2315 train_time:60194ms step_avg:60.25ms
step:1000/2315 train_time:60255ms step_avg:60.25ms
step:1000/2315 val_loss:3.5743 train_time:60318ms step_avg:60.32ms
step:1001/2315 train_time:60338ms step_avg:60.28ms
step:1002/2315 train_time:60380ms step_avg:60.26ms
step:1003/2315 train_time:60446ms step_avg:60.26ms
step:1004/2315 train_time:60511ms step_avg:60.27ms
step:1005/2315 train_time:60573ms step_avg:60.27ms
step:1006/2315 train_time:60633ms step_avg:60.27ms
step:1007/2315 train_time:60693ms step_avg:60.27ms
step:1008/2315 train_time:60753ms step_avg:60.27ms
step:1009/2315 train_time:60813ms step_avg:60.27ms
step:1010/2315 train_time:60872ms step_avg:60.27ms
step:1011/2315 train_time:60932ms step_avg:60.27ms
step:1012/2315 train_time:60992ms step_avg:60.27ms
step:1013/2315 train_time:61052ms step_avg:60.27ms
step:1014/2315 train_time:61111ms step_avg:60.27ms
step:1015/2315 train_time:61171ms step_avg:60.27ms
step:1016/2315 train_time:61234ms step_avg:60.27ms
step:1017/2315 train_time:61298ms step_avg:60.27ms
step:1018/2315 train_time:61360ms step_avg:60.27ms
step:1019/2315 train_time:61421ms step_avg:60.28ms
step:1020/2315 train_time:61483ms step_avg:60.28ms
step:1021/2315 train_time:61544ms step_avg:60.28ms
step:1022/2315 train_time:61605ms step_avg:60.28ms
step:1023/2315 train_time:61666ms step_avg:60.28ms
step:1024/2315 train_time:61727ms step_avg:60.28ms
step:1025/2315 train_time:61788ms step_avg:60.28ms
step:1026/2315 train_time:61848ms step_avg:60.28ms
step:1027/2315 train_time:61908ms step_avg:60.28ms
step:1028/2315 train_time:61968ms step_avg:60.28ms
step:1029/2315 train_time:62028ms step_avg:60.28ms
step:1030/2315 train_time:62089ms step_avg:60.28ms
step:1031/2315 train_time:62149ms step_avg:60.28ms
step:1032/2315 train_time:62209ms step_avg:60.28ms
step:1033/2315 train_time:62271ms step_avg:60.28ms
step:1034/2315 train_time:62333ms step_avg:60.28ms
step:1035/2315 train_time:62395ms step_avg:60.28ms
step:1036/2315 train_time:62456ms step_avg:60.29ms
step:1037/2315 train_time:62518ms step_avg:60.29ms
step:1038/2315 train_time:62579ms step_avg:60.29ms
step:1039/2315 train_time:62640ms step_avg:60.29ms
step:1040/2315 train_time:62701ms step_avg:60.29ms
step:1041/2315 train_time:62762ms step_avg:60.29ms
step:1042/2315 train_time:62822ms step_avg:60.29ms
step:1043/2315 train_time:62883ms step_avg:60.29ms
step:1044/2315 train_time:62943ms step_avg:60.29ms
step:1045/2315 train_time:63004ms step_avg:60.29ms
step:1046/2315 train_time:63065ms step_avg:60.29ms
step:1047/2315 train_time:63125ms step_avg:60.29ms
step:1048/2315 train_time:63186ms step_avg:60.29ms
step:1049/2315 train_time:63247ms step_avg:60.29ms
step:1050/2315 train_time:63308ms step_avg:60.29ms
step:1051/2315 train_time:63370ms step_avg:60.29ms
step:1052/2315 train_time:63431ms step_avg:60.30ms
step:1053/2315 train_time:63492ms step_avg:60.30ms
step:1054/2315 train_time:63553ms step_avg:60.30ms
step:1055/2315 train_time:63615ms step_avg:60.30ms
step:1056/2315 train_time:63675ms step_avg:60.30ms
step:1057/2315 train_time:63737ms step_avg:60.30ms
step:1058/2315 train_time:63798ms step_avg:60.30ms
step:1059/2315 train_time:63859ms step_avg:60.30ms
step:1060/2315 train_time:63920ms step_avg:60.30ms
step:1061/2315 train_time:63980ms step_avg:60.30ms
step:1062/2315 train_time:64040ms step_avg:60.30ms
step:1063/2315 train_time:64101ms step_avg:60.30ms
step:1064/2315 train_time:64161ms step_avg:60.30ms
step:1065/2315 train_time:64222ms step_avg:60.30ms
step:1066/2315 train_time:64284ms step_avg:60.30ms
step:1067/2315 train_time:64345ms step_avg:60.30ms
step:1068/2315 train_time:64406ms step_avg:60.31ms
step:1069/2315 train_time:64467ms step_avg:60.31ms
step:1070/2315 train_time:64528ms step_avg:60.31ms
step:1071/2315 train_time:64589ms step_avg:60.31ms
step:1072/2315 train_time:64650ms step_avg:60.31ms
step:1073/2315 train_time:64712ms step_avg:60.31ms
step:1074/2315 train_time:64773ms step_avg:60.31ms
step:1075/2315 train_time:64834ms step_avg:60.31ms
step:1076/2315 train_time:64895ms step_avg:60.31ms
step:1077/2315 train_time:64955ms step_avg:60.31ms
step:1078/2315 train_time:65016ms step_avg:60.31ms
step:1079/2315 train_time:65076ms step_avg:60.31ms
step:1080/2315 train_time:65137ms step_avg:60.31ms
step:1081/2315 train_time:65197ms step_avg:60.31ms
step:1082/2315 train_time:65258ms step_avg:60.31ms
step:1083/2315 train_time:65319ms step_avg:60.31ms
step:1084/2315 train_time:65380ms step_avg:60.31ms
step:1085/2315 train_time:65441ms step_avg:60.31ms
step:1086/2315 train_time:65502ms step_avg:60.31ms
step:1087/2315 train_time:65562ms step_avg:60.31ms
step:1088/2315 train_time:65623ms step_avg:60.32ms
step:1089/2315 train_time:65685ms step_avg:60.32ms
step:1090/2315 train_time:65746ms step_avg:60.32ms
step:1091/2315 train_time:65807ms step_avg:60.32ms
step:1092/2315 train_time:65868ms step_avg:60.32ms
step:1093/2315 train_time:65929ms step_avg:60.32ms
step:1094/2315 train_time:65990ms step_avg:60.32ms
step:1095/2315 train_time:66051ms step_avg:60.32ms
step:1096/2315 train_time:66111ms step_avg:60.32ms
step:1097/2315 train_time:66172ms step_avg:60.32ms
step:1098/2315 train_time:66233ms step_avg:60.32ms
step:1099/2315 train_time:66295ms step_avg:60.32ms
step:1100/2315 train_time:66355ms step_avg:60.32ms
step:1101/2315 train_time:66416ms step_avg:60.32ms
step:1102/2315 train_time:66478ms step_avg:60.32ms
step:1103/2315 train_time:66539ms step_avg:60.33ms
step:1104/2315 train_time:66599ms step_avg:60.33ms
step:1105/2315 train_time:66660ms step_avg:60.33ms
step:1106/2315 train_time:66721ms step_avg:60.33ms
step:1107/2315 train_time:66782ms step_avg:60.33ms
step:1108/2315 train_time:66843ms step_avg:60.33ms
step:1109/2315 train_time:66904ms step_avg:60.33ms
step:1110/2315 train_time:66965ms step_avg:60.33ms
step:1111/2315 train_time:67026ms step_avg:60.33ms
step:1112/2315 train_time:67087ms step_avg:60.33ms
step:1113/2315 train_time:67148ms step_avg:60.33ms
step:1114/2315 train_time:67208ms step_avg:60.33ms
step:1115/2315 train_time:67269ms step_avg:60.33ms
step:1116/2315 train_time:67330ms step_avg:60.33ms
step:1117/2315 train_time:67391ms step_avg:60.33ms
step:1118/2315 train_time:67452ms step_avg:60.33ms
step:1119/2315 train_time:67514ms step_avg:60.33ms
step:1120/2315 train_time:67575ms step_avg:60.33ms
step:1121/2315 train_time:67636ms step_avg:60.34ms
step:1122/2315 train_time:67697ms step_avg:60.34ms
step:1123/2315 train_time:67758ms step_avg:60.34ms
step:1124/2315 train_time:67819ms step_avg:60.34ms
step:1125/2315 train_time:67880ms step_avg:60.34ms
step:1126/2315 train_time:67941ms step_avg:60.34ms
step:1127/2315 train_time:68001ms step_avg:60.34ms
step:1128/2315 train_time:68062ms step_avg:60.34ms
step:1129/2315 train_time:68123ms step_avg:60.34ms
step:1130/2315 train_time:68184ms step_avg:60.34ms
step:1131/2315 train_time:68245ms step_avg:60.34ms
step:1132/2315 train_time:68307ms step_avg:60.34ms
step:1133/2315 train_time:68367ms step_avg:60.34ms
step:1134/2315 train_time:68429ms step_avg:60.34ms
step:1135/2315 train_time:68490ms step_avg:60.34ms
step:1136/2315 train_time:68550ms step_avg:60.34ms
step:1137/2315 train_time:68611ms step_avg:60.34ms
step:1138/2315 train_time:68672ms step_avg:60.34ms
step:1139/2315 train_time:68733ms step_avg:60.35ms
step:1140/2315 train_time:68795ms step_avg:60.35ms
step:1141/2315 train_time:68856ms step_avg:60.35ms
step:1142/2315 train_time:68916ms step_avg:60.35ms
step:1143/2315 train_time:68978ms step_avg:60.35ms
step:1144/2315 train_time:69040ms step_avg:60.35ms
step:1145/2315 train_time:69101ms step_avg:60.35ms
step:1146/2315 train_time:69161ms step_avg:60.35ms
step:1147/2315 train_time:69221ms step_avg:60.35ms
step:1148/2315 train_time:69282ms step_avg:60.35ms
step:1149/2315 train_time:69344ms step_avg:60.35ms
step:1150/2315 train_time:69404ms step_avg:60.35ms
step:1151/2315 train_time:69466ms step_avg:60.35ms
step:1152/2315 train_time:69526ms step_avg:60.35ms
step:1153/2315 train_time:69587ms step_avg:60.35ms
step:1154/2315 train_time:69648ms step_avg:60.35ms
step:1155/2315 train_time:69709ms step_avg:60.35ms
step:1156/2315 train_time:69770ms step_avg:60.36ms
step:1157/2315 train_time:69831ms step_avg:60.36ms
step:1158/2315 train_time:69892ms step_avg:60.36ms
step:1159/2315 train_time:69953ms step_avg:60.36ms
step:1160/2315 train_time:70014ms step_avg:60.36ms
step:1161/2315 train_time:70075ms step_avg:60.36ms
step:1162/2315 train_time:70137ms step_avg:60.36ms
step:1163/2315 train_time:70199ms step_avg:60.36ms
step:1164/2315 train_time:70260ms step_avg:60.36ms
step:1165/2315 train_time:70320ms step_avg:60.36ms
step:1166/2315 train_time:70381ms step_avg:60.36ms
step:1167/2315 train_time:70442ms step_avg:60.36ms
step:1168/2315 train_time:70502ms step_avg:60.36ms
step:1169/2315 train_time:70563ms step_avg:60.36ms
step:1170/2315 train_time:70624ms step_avg:60.36ms
step:1171/2315 train_time:70685ms step_avg:60.36ms
step:1172/2315 train_time:70746ms step_avg:60.36ms
step:1173/2315 train_time:70807ms step_avg:60.36ms
step:1174/2315 train_time:70868ms step_avg:60.36ms
step:1175/2315 train_time:70930ms step_avg:60.37ms
step:1176/2315 train_time:70991ms step_avg:60.37ms
step:1177/2315 train_time:71052ms step_avg:60.37ms
step:1178/2315 train_time:71112ms step_avg:60.37ms
step:1179/2315 train_time:71175ms step_avg:60.37ms
step:1180/2315 train_time:71235ms step_avg:60.37ms
step:1181/2315 train_time:71296ms step_avg:60.37ms
step:1182/2315 train_time:71357ms step_avg:60.37ms
step:1183/2315 train_time:71418ms step_avg:60.37ms
step:1184/2315 train_time:71479ms step_avg:60.37ms
step:1185/2315 train_time:71540ms step_avg:60.37ms
step:1186/2315 train_time:71601ms step_avg:60.37ms
step:1187/2315 train_time:71661ms step_avg:60.37ms
step:1188/2315 train_time:71722ms step_avg:60.37ms
step:1189/2315 train_time:71783ms step_avg:60.37ms
step:1190/2315 train_time:71844ms step_avg:60.37ms
step:1191/2315 train_time:71905ms step_avg:60.37ms
step:1192/2315 train_time:71966ms step_avg:60.37ms
step:1193/2315 train_time:72028ms step_avg:60.38ms
step:1194/2315 train_time:72089ms step_avg:60.38ms
step:1195/2315 train_time:72150ms step_avg:60.38ms
step:1196/2315 train_time:72211ms step_avg:60.38ms
step:1197/2315 train_time:72272ms step_avg:60.38ms
step:1198/2315 train_time:72333ms step_avg:60.38ms
step:1199/2315 train_time:72393ms step_avg:60.38ms
step:1200/2315 train_time:72454ms step_avg:60.38ms
step:1201/2315 train_time:72515ms step_avg:60.38ms
step:1202/2315 train_time:72576ms step_avg:60.38ms
step:1203/2315 train_time:72637ms step_avg:60.38ms
step:1204/2315 train_time:72698ms step_avg:60.38ms
step:1205/2315 train_time:72759ms step_avg:60.38ms
step:1206/2315 train_time:72821ms step_avg:60.38ms
step:1207/2315 train_time:72882ms step_avg:60.38ms
step:1208/2315 train_time:72942ms step_avg:60.38ms
step:1209/2315 train_time:73004ms step_avg:60.38ms
step:1210/2315 train_time:73065ms step_avg:60.38ms
step:1211/2315 train_time:73126ms step_avg:60.38ms
step:1212/2315 train_time:73187ms step_avg:60.39ms
step:1213/2315 train_time:73249ms step_avg:60.39ms
step:1214/2315 train_time:73310ms step_avg:60.39ms
step:1215/2315 train_time:73371ms step_avg:60.39ms
step:1216/2315 train_time:73431ms step_avg:60.39ms
step:1217/2315 train_time:73492ms step_avg:60.39ms
step:1218/2315 train_time:73552ms step_avg:60.39ms
step:1219/2315 train_time:73614ms step_avg:60.39ms
step:1220/2315 train_time:73675ms step_avg:60.39ms
step:1221/2315 train_time:73737ms step_avg:60.39ms
step:1222/2315 train_time:73798ms step_avg:60.39ms
step:1223/2315 train_time:73859ms step_avg:60.39ms
step:1224/2315 train_time:73919ms step_avg:60.39ms
step:1225/2315 train_time:73980ms step_avg:60.39ms
step:1226/2315 train_time:74040ms step_avg:60.39ms
step:1227/2315 train_time:74101ms step_avg:60.39ms
step:1228/2315 train_time:74162ms step_avg:60.39ms
step:1229/2315 train_time:74223ms step_avg:60.39ms
step:1230/2315 train_time:74284ms step_avg:60.39ms
step:1231/2315 train_time:74346ms step_avg:60.39ms
step:1232/2315 train_time:74406ms step_avg:60.39ms
step:1233/2315 train_time:74468ms step_avg:60.40ms
step:1234/2315 train_time:74528ms step_avg:60.40ms
step:1235/2315 train_time:74589ms step_avg:60.40ms
step:1236/2315 train_time:74650ms step_avg:60.40ms
step:1237/2315 train_time:74711ms step_avg:60.40ms
step:1238/2315 train_time:74772ms step_avg:60.40ms
step:1239/2315 train_time:74833ms step_avg:60.40ms
step:1240/2315 train_time:74894ms step_avg:60.40ms
step:1241/2315 train_time:74955ms step_avg:60.40ms
step:1242/2315 train_time:75016ms step_avg:60.40ms
step:1243/2315 train_time:75077ms step_avg:60.40ms
step:1244/2315 train_time:75138ms step_avg:60.40ms
step:1245/2315 train_time:75200ms step_avg:60.40ms
step:1246/2315 train_time:75260ms step_avg:60.40ms
step:1247/2315 train_time:75321ms step_avg:60.40ms
step:1248/2315 train_time:75382ms step_avg:60.40ms
step:1249/2315 train_time:75444ms step_avg:60.40ms
step:1250/2315 train_time:75504ms step_avg:60.40ms
step:1250/2315 val_loss:3.5142 train_time:75567ms step_avg:60.45ms
step:1251/2315 train_time:75586ms step_avg:60.42ms
step:1252/2315 train_time:75628ms step_avg:60.41ms
step:1253/2315 train_time:75693ms step_avg:60.41ms
step:1254/2315 train_time:75756ms step_avg:60.41ms
step:1255/2315 train_time:75817ms step_avg:60.41ms
step:1256/2315 train_time:75878ms step_avg:60.41ms
step:1257/2315 train_time:75939ms step_avg:60.41ms
step:1258/2315 train_time:76000ms step_avg:60.41ms
step:1259/2315 train_time:76060ms step_avg:60.41ms
step:1260/2315 train_time:76121ms step_avg:60.41ms
step:1261/2315 train_time:76181ms step_avg:60.41ms
step:1262/2315 train_time:76241ms step_avg:60.41ms
step:1263/2315 train_time:76301ms step_avg:60.41ms
step:1264/2315 train_time:76361ms step_avg:60.41ms
step:1265/2315 train_time:76421ms step_avg:60.41ms
step:1266/2315 train_time:76482ms step_avg:60.41ms
step:1267/2315 train_time:76542ms step_avg:60.41ms
step:1268/2315 train_time:76604ms step_avg:60.41ms
step:1269/2315 train_time:76667ms step_avg:60.42ms
step:1270/2315 train_time:76729ms step_avg:60.42ms
step:1271/2315 train_time:76790ms step_avg:60.42ms
step:1272/2315 train_time:76851ms step_avg:60.42ms
step:1273/2315 train_time:76911ms step_avg:60.42ms
step:1274/2315 train_time:76972ms step_avg:60.42ms
step:1275/2315 train_time:77032ms step_avg:60.42ms
step:1276/2315 train_time:77092ms step_avg:60.42ms
step:1277/2315 train_time:77153ms step_avg:60.42ms
step:1278/2315 train_time:77214ms step_avg:60.42ms
step:1279/2315 train_time:77274ms step_avg:60.42ms
step:1280/2315 train_time:77335ms step_avg:60.42ms
step:1281/2315 train_time:77396ms step_avg:60.42ms
step:1282/2315 train_time:77456ms step_avg:60.42ms
step:1283/2315 train_time:77517ms step_avg:60.42ms
step:1284/2315 train_time:77578ms step_avg:60.42ms
step:1285/2315 train_time:77641ms step_avg:60.42ms
step:1286/2315 train_time:77703ms step_avg:60.42ms
step:1287/2315 train_time:77765ms step_avg:60.42ms
step:1288/2315 train_time:77826ms step_avg:60.42ms
step:1289/2315 train_time:77886ms step_avg:60.42ms
step:1290/2315 train_time:77947ms step_avg:60.42ms
step:1291/2315 train_time:78007ms step_avg:60.42ms
step:1292/2315 train_time:78068ms step_avg:60.42ms
step:1293/2315 train_time:78129ms step_avg:60.42ms
step:1294/2315 train_time:78190ms step_avg:60.42ms
step:1295/2315 train_time:78250ms step_avg:60.42ms
step:1296/2315 train_time:78311ms step_avg:60.42ms
step:1297/2315 train_time:78371ms step_avg:60.43ms
step:1298/2315 train_time:78432ms step_avg:60.43ms
step:1299/2315 train_time:78493ms step_avg:60.43ms
step:1300/2315 train_time:78554ms step_avg:60.43ms
step:1301/2315 train_time:78615ms step_avg:60.43ms
step:1302/2315 train_time:78676ms step_avg:60.43ms
step:1303/2315 train_time:78738ms step_avg:60.43ms
step:1304/2315 train_time:78798ms step_avg:60.43ms
step:1305/2315 train_time:78860ms step_avg:60.43ms
step:1306/2315 train_time:78921ms step_avg:60.43ms
step:1307/2315 train_time:78982ms step_avg:60.43ms
step:1308/2315 train_time:79042ms step_avg:60.43ms
step:1309/2315 train_time:79103ms step_avg:60.43ms
step:1310/2315 train_time:79164ms step_avg:60.43ms
step:1311/2315 train_time:79225ms step_avg:60.43ms
step:1312/2315 train_time:79286ms step_avg:60.43ms
step:1313/2315 train_time:79347ms step_avg:60.43ms
step:1314/2315 train_time:79407ms step_avg:60.43ms
step:1315/2315 train_time:79468ms step_avg:60.43ms
step:1316/2315 train_time:79530ms step_avg:60.43ms
step:1317/2315 train_time:79592ms step_avg:60.43ms
step:1318/2315 train_time:79652ms step_avg:60.43ms
step:1319/2315 train_time:79714ms step_avg:60.43ms
step:1320/2315 train_time:79774ms step_avg:60.43ms
step:1321/2315 train_time:79835ms step_avg:60.44ms
step:1322/2315 train_time:79897ms step_avg:60.44ms
step:1323/2315 train_time:79958ms step_avg:60.44ms
step:1324/2315 train_time:80019ms step_avg:60.44ms
step:1325/2315 train_time:80080ms step_avg:60.44ms
step:1326/2315 train_time:80141ms step_avg:60.44ms
step:1327/2315 train_time:80202ms step_avg:60.44ms
step:1328/2315 train_time:80263ms step_avg:60.44ms
step:1329/2315 train_time:80324ms step_avg:60.44ms
step:1330/2315 train_time:80385ms step_avg:60.44ms
step:1331/2315 train_time:80446ms step_avg:60.44ms
step:1332/2315 train_time:80507ms step_avg:60.44ms
step:1333/2315 train_time:80568ms step_avg:60.44ms
step:1334/2315 train_time:80629ms step_avg:60.44ms
step:1335/2315 train_time:80691ms step_avg:60.44ms
step:1336/2315 train_time:80751ms step_avg:60.44ms
step:1337/2315 train_time:80813ms step_avg:60.44ms
step:1338/2315 train_time:80873ms step_avg:60.44ms
step:1339/2315 train_time:80934ms step_avg:60.44ms
step:1340/2315 train_time:80994ms step_avg:60.44ms
step:1341/2315 train_time:81055ms step_avg:60.44ms
step:1342/2315 train_time:81116ms step_avg:60.44ms
step:1343/2315 train_time:81177ms step_avg:60.44ms
step:1344/2315 train_time:81238ms step_avg:60.44ms
step:1345/2315 train_time:81299ms step_avg:60.45ms
step:1346/2315 train_time:81360ms step_avg:60.45ms
step:1347/2315 train_time:81421ms step_avg:60.45ms
step:1348/2315 train_time:81482ms step_avg:60.45ms
step:1349/2315 train_time:81544ms step_avg:60.45ms
step:1350/2315 train_time:81604ms step_avg:60.45ms
step:1351/2315 train_time:81665ms step_avg:60.45ms
step:1352/2315 train_time:81726ms step_avg:60.45ms
step:1353/2315 train_time:81787ms step_avg:60.45ms
step:1354/2315 train_time:81848ms step_avg:60.45ms
step:1355/2315 train_time:81909ms step_avg:60.45ms
step:1356/2315 train_time:81971ms step_avg:60.45ms
step:1357/2315 train_time:82032ms step_avg:60.45ms
step:1358/2315 train_time:82093ms step_avg:60.45ms
step:1359/2315 train_time:82154ms step_avg:60.45ms
step:1360/2315 train_time:82215ms step_avg:60.45ms
step:1361/2315 train_time:82276ms step_avg:60.45ms
step:1362/2315 train_time:82337ms step_avg:60.45ms
step:1363/2315 train_time:82398ms step_avg:60.45ms
step:1364/2315 train_time:82458ms step_avg:60.45ms
step:1365/2315 train_time:82519ms step_avg:60.45ms
step:1366/2315 train_time:82580ms step_avg:60.45ms
step:1367/2315 train_time:82641ms step_avg:60.45ms
step:1368/2315 train_time:82702ms step_avg:60.45ms
step:1369/2315 train_time:82763ms step_avg:60.46ms
step:1370/2315 train_time:82823ms step_avg:60.46ms
step:1371/2315 train_time:82885ms step_avg:60.46ms
step:1372/2315 train_time:82946ms step_avg:60.46ms
step:1373/2315 train_time:83007ms step_avg:60.46ms
step:1374/2315 train_time:83067ms step_avg:60.46ms
step:1375/2315 train_time:83129ms step_avg:60.46ms
step:1376/2315 train_time:83190ms step_avg:60.46ms
step:1377/2315 train_time:83252ms step_avg:60.46ms
step:1378/2315 train_time:83312ms step_avg:60.46ms
step:1379/2315 train_time:83373ms step_avg:60.46ms
step:1380/2315 train_time:83433ms step_avg:60.46ms
step:1381/2315 train_time:83493ms step_avg:60.46ms
step:1382/2315 train_time:83555ms step_avg:60.46ms
step:1383/2315 train_time:83616ms step_avg:60.46ms
step:1384/2315 train_time:83677ms step_avg:60.46ms
step:1385/2315 train_time:83738ms step_avg:60.46ms
step:1386/2315 train_time:83799ms step_avg:60.46ms
step:1387/2315 train_time:83860ms step_avg:60.46ms
step:1388/2315 train_time:83921ms step_avg:60.46ms
step:1389/2315 train_time:83983ms step_avg:60.46ms
step:1390/2315 train_time:84043ms step_avg:60.46ms
step:1391/2315 train_time:84104ms step_avg:60.46ms
step:1392/2315 train_time:84165ms step_avg:60.46ms
step:1393/2315 train_time:84226ms step_avg:60.46ms
step:1394/2315 train_time:84287ms step_avg:60.46ms
step:1395/2315 train_time:84347ms step_avg:60.46ms
step:1396/2315 train_time:84408ms step_avg:60.46ms
step:1397/2315 train_time:84469ms step_avg:60.46ms
step:1398/2315 train_time:84530ms step_avg:60.47ms
step:1399/2315 train_time:84592ms step_avg:60.47ms
step:1400/2315 train_time:84652ms step_avg:60.47ms
step:1401/2315 train_time:84713ms step_avg:60.47ms
step:1402/2315 train_time:84773ms step_avg:60.47ms
step:1403/2315 train_time:84834ms step_avg:60.47ms
step:1404/2315 train_time:84895ms step_avg:60.47ms
step:1405/2315 train_time:84956ms step_avg:60.47ms
step:1406/2315 train_time:85017ms step_avg:60.47ms
step:1407/2315 train_time:85078ms step_avg:60.47ms
step:1408/2315 train_time:85140ms step_avg:60.47ms
step:1409/2315 train_time:85201ms step_avg:60.47ms
step:1410/2315 train_time:85262ms step_avg:60.47ms
step:1411/2315 train_time:85324ms step_avg:60.47ms
step:1412/2315 train_time:85384ms step_avg:60.47ms
step:1413/2315 train_time:85445ms step_avg:60.47ms
step:1414/2315 train_time:85506ms step_avg:60.47ms
step:1415/2315 train_time:85567ms step_avg:60.47ms
step:1416/2315 train_time:85628ms step_avg:60.47ms
step:1417/2315 train_time:85689ms step_avg:60.47ms
step:1418/2315 train_time:85749ms step_avg:60.47ms
step:1419/2315 train_time:85810ms step_avg:60.47ms
step:1420/2315 train_time:85872ms step_avg:60.47ms
step:1421/2315 train_time:85932ms step_avg:60.47ms
step:1422/2315 train_time:85993ms step_avg:60.47ms
step:1423/2315 train_time:86053ms step_avg:60.47ms
step:1424/2315 train_time:86115ms step_avg:60.47ms
step:1425/2315 train_time:86176ms step_avg:60.47ms
step:1426/2315 train_time:86236ms step_avg:60.47ms
step:1427/2315 train_time:86297ms step_avg:60.47ms
step:1428/2315 train_time:86358ms step_avg:60.47ms
step:1429/2315 train_time:86419ms step_avg:60.48ms
step:1430/2315 train_time:86480ms step_avg:60.48ms
step:1431/2315 train_time:86542ms step_avg:60.48ms
step:1432/2315 train_time:86602ms step_avg:60.48ms
step:1433/2315 train_time:86664ms step_avg:60.48ms
step:1434/2315 train_time:86724ms step_avg:60.48ms
step:1435/2315 train_time:86785ms step_avg:60.48ms
step:1436/2315 train_time:86845ms step_avg:60.48ms
step:1437/2315 train_time:86906ms step_avg:60.48ms
step:1438/2315 train_time:86968ms step_avg:60.48ms
step:1439/2315 train_time:87029ms step_avg:60.48ms
step:1440/2315 train_time:87090ms step_avg:60.48ms
step:1441/2315 train_time:87151ms step_avg:60.48ms
step:1442/2315 train_time:87212ms step_avg:60.48ms
step:1443/2315 train_time:87273ms step_avg:60.48ms
step:1444/2315 train_time:87333ms step_avg:60.48ms
step:1445/2315 train_time:87394ms step_avg:60.48ms
step:1446/2315 train_time:87455ms step_avg:60.48ms
step:1447/2315 train_time:87516ms step_avg:60.48ms
step:1448/2315 train_time:87577ms step_avg:60.48ms
step:1449/2315 train_time:87639ms step_avg:60.48ms
step:1450/2315 train_time:87700ms step_avg:60.48ms
step:1451/2315 train_time:87761ms step_avg:60.48ms
step:1452/2315 train_time:87821ms step_avg:60.48ms
step:1453/2315 train_time:87882ms step_avg:60.48ms
step:1454/2315 train_time:87943ms step_avg:60.48ms
step:1455/2315 train_time:88004ms step_avg:60.48ms
step:1456/2315 train_time:88065ms step_avg:60.48ms
step:1457/2315 train_time:88126ms step_avg:60.48ms
step:1458/2315 train_time:88186ms step_avg:60.48ms
step:1459/2315 train_time:88247ms step_avg:60.48ms
step:1460/2315 train_time:88309ms step_avg:60.49ms
step:1461/2315 train_time:88370ms step_avg:60.49ms
step:1462/2315 train_time:88430ms step_avg:60.49ms
step:1463/2315 train_time:88491ms step_avg:60.49ms
step:1464/2315 train_time:88552ms step_avg:60.49ms
step:1465/2315 train_time:88612ms step_avg:60.49ms
step:1466/2315 train_time:88673ms step_avg:60.49ms
step:1467/2315 train_time:88734ms step_avg:60.49ms
step:1468/2315 train_time:88795ms step_avg:60.49ms
step:1469/2315 train_time:88856ms step_avg:60.49ms
step:1470/2315 train_time:88917ms step_avg:60.49ms
step:1471/2315 train_time:88978ms step_avg:60.49ms
step:1472/2315 train_time:89039ms step_avg:60.49ms
step:1473/2315 train_time:89099ms step_avg:60.49ms
step:1474/2315 train_time:89160ms step_avg:60.49ms
step:1475/2315 train_time:89221ms step_avg:60.49ms
step:1476/2315 train_time:89282ms step_avg:60.49ms
step:1477/2315 train_time:89343ms step_avg:60.49ms
step:1478/2315 train_time:89404ms step_avg:60.49ms
step:1479/2315 train_time:89465ms step_avg:60.49ms
step:1480/2315 train_time:89526ms step_avg:60.49ms
step:1481/2315 train_time:89588ms step_avg:60.49ms
step:1482/2315 train_time:89648ms step_avg:60.49ms
step:1483/2315 train_time:89710ms step_avg:60.49ms
step:1484/2315 train_time:89771ms step_avg:60.49ms
step:1485/2315 train_time:89832ms step_avg:60.49ms
step:1486/2315 train_time:89892ms step_avg:60.49ms
step:1487/2315 train_time:89954ms step_avg:60.49ms
step:1488/2315 train_time:90015ms step_avg:60.49ms
step:1489/2315 train_time:90076ms step_avg:60.49ms
step:1490/2315 train_time:90136ms step_avg:60.49ms
step:1491/2315 train_time:90197ms step_avg:60.49ms
step:1492/2315 train_time:90258ms step_avg:60.49ms
step:1493/2315 train_time:90319ms step_avg:60.50ms
step:1494/2315 train_time:90380ms step_avg:60.50ms
step:1495/2315 train_time:90441ms step_avg:60.50ms
step:1496/2315 train_time:90502ms step_avg:60.50ms
step:1497/2315 train_time:90563ms step_avg:60.50ms
step:1498/2315 train_time:90625ms step_avg:60.50ms
step:1499/2315 train_time:90686ms step_avg:60.50ms
step:1500/2315 train_time:90747ms step_avg:60.50ms
step:1500/2315 val_loss:3.4521 train_time:90809ms step_avg:60.54ms
step:1501/2315 train_time:90829ms step_avg:60.51ms
step:1502/2315 train_time:90871ms step_avg:60.50ms
step:1503/2315 train_time:90934ms step_avg:60.50ms
step:1504/2315 train_time:90998ms step_avg:60.50ms
step:1505/2315 train_time:91059ms step_avg:60.50ms
step:1506/2315 train_time:91120ms step_avg:60.50ms
step:1507/2315 train_time:91181ms step_avg:60.51ms
step:1508/2315 train_time:91242ms step_avg:60.51ms
step:1509/2315 train_time:91303ms step_avg:60.51ms
step:1510/2315 train_time:91363ms step_avg:60.51ms
step:1511/2315 train_time:91423ms step_avg:60.51ms
step:1512/2315 train_time:91484ms step_avg:60.50ms
step:1513/2315 train_time:91543ms step_avg:60.50ms
step:1514/2315 train_time:91603ms step_avg:60.50ms
step:1515/2315 train_time:91663ms step_avg:60.50ms
step:1516/2315 train_time:91724ms step_avg:60.50ms
step:1517/2315 train_time:91786ms step_avg:60.50ms
step:1518/2315 train_time:91846ms step_avg:60.50ms
step:1519/2315 train_time:91908ms step_avg:60.51ms
step:1520/2315 train_time:91971ms step_avg:60.51ms
step:1521/2315 train_time:92033ms step_avg:60.51ms
step:1522/2315 train_time:92094ms step_avg:60.51ms
step:1523/2315 train_time:92156ms step_avg:60.51ms
step:1524/2315 train_time:92217ms step_avg:60.51ms
step:1525/2315 train_time:92278ms step_avg:60.51ms
step:1526/2315 train_time:92339ms step_avg:60.51ms
step:1527/2315 train_time:92400ms step_avg:60.51ms
step:1528/2315 train_time:92461ms step_avg:60.51ms
step:1529/2315 train_time:92522ms step_avg:60.51ms
step:1530/2315 train_time:92582ms step_avg:60.51ms
step:1531/2315 train_time:92643ms step_avg:60.51ms
step:1532/2315 train_time:92704ms step_avg:60.51ms
step:1533/2315 train_time:92766ms step_avg:60.51ms
step:1534/2315 train_time:92827ms step_avg:60.51ms
step:1535/2315 train_time:92888ms step_avg:60.51ms
step:1536/2315 train_time:92949ms step_avg:60.51ms
step:1537/2315 train_time:93011ms step_avg:60.51ms
step:1538/2315 train_time:93072ms step_avg:60.52ms
step:1539/2315 train_time:93134ms step_avg:60.52ms
step:1540/2315 train_time:93194ms step_avg:60.52ms
step:1541/2315 train_time:93256ms step_avg:60.52ms
step:1542/2315 train_time:93317ms step_avg:60.52ms
step:1543/2315 train_time:93378ms step_avg:60.52ms
step:1544/2315 train_time:93439ms step_avg:60.52ms
step:1545/2315 train_time:93501ms step_avg:60.52ms
step:1546/2315 train_time:93562ms step_avg:60.52ms
step:1547/2315 train_time:93623ms step_avg:60.52ms
step:1548/2315 train_time:93684ms step_avg:60.52ms
step:1549/2315 train_time:93745ms step_avg:60.52ms
step:1550/2315 train_time:93806ms step_avg:60.52ms
step:1551/2315 train_time:93867ms step_avg:60.52ms
step:1552/2315 train_time:93928ms step_avg:60.52ms
step:1553/2315 train_time:93990ms step_avg:60.52ms
step:1554/2315 train_time:94051ms step_avg:60.52ms
step:1555/2315 train_time:94112ms step_avg:60.52ms
step:1556/2315 train_time:94174ms step_avg:60.52ms
step:1557/2315 train_time:94235ms step_avg:60.52ms
step:1558/2315 train_time:94296ms step_avg:60.52ms
step:1559/2315 train_time:94357ms step_avg:60.52ms
step:1560/2315 train_time:94418ms step_avg:60.52ms
step:1561/2315 train_time:94480ms step_avg:60.53ms
step:1562/2315 train_time:94541ms step_avg:60.53ms
step:1563/2315 train_time:94602ms step_avg:60.53ms
step:1564/2315 train_time:94664ms step_avg:60.53ms
step:1565/2315 train_time:94725ms step_avg:60.53ms
step:1566/2315 train_time:94786ms step_avg:60.53ms
step:1567/2315 train_time:94847ms step_avg:60.53ms
step:1568/2315 train_time:94908ms step_avg:60.53ms
step:1569/2315 train_time:94969ms step_avg:60.53ms
step:1570/2315 train_time:95031ms step_avg:60.53ms
step:1571/2315 train_time:95092ms step_avg:60.53ms
step:1572/2315 train_time:95153ms step_avg:60.53ms
step:1573/2315 train_time:95214ms step_avg:60.53ms
step:1574/2315 train_time:95275ms step_avg:60.53ms
step:1575/2315 train_time:95336ms step_avg:60.53ms
step:1576/2315 train_time:95397ms step_avg:60.53ms
step:1577/2315 train_time:95459ms step_avg:60.53ms
step:1578/2315 train_time:95520ms step_avg:60.53ms
step:1579/2315 train_time:95581ms step_avg:60.53ms
step:1580/2315 train_time:95642ms step_avg:60.53ms
step:1581/2315 train_time:95703ms step_avg:60.53ms
step:1582/2315 train_time:95765ms step_avg:60.53ms
step:1583/2315 train_time:95827ms step_avg:60.54ms
step:1584/2315 train_time:95889ms step_avg:60.54ms
step:1585/2315 train_time:95949ms step_avg:60.54ms
step:1586/2315 train_time:96011ms step_avg:60.54ms
step:1587/2315 train_time:96071ms step_avg:60.54ms
step:1588/2315 train_time:96132ms step_avg:60.54ms
step:1589/2315 train_time:96193ms step_avg:60.54ms
step:1590/2315 train_time:96255ms step_avg:60.54ms
step:1591/2315 train_time:96316ms step_avg:60.54ms
step:1592/2315 train_time:96377ms step_avg:60.54ms
step:1593/2315 train_time:96438ms step_avg:60.54ms
step:1594/2315 train_time:96499ms step_avg:60.54ms
step:1595/2315 train_time:96560ms step_avg:60.54ms
step:1596/2315 train_time:96621ms step_avg:60.54ms
step:1597/2315 train_time:96683ms step_avg:60.54ms
step:1598/2315 train_time:96744ms step_avg:60.54ms
step:1599/2315 train_time:96807ms step_avg:60.54ms
step:1600/2315 train_time:96868ms step_avg:60.54ms
step:1601/2315 train_time:96929ms step_avg:60.54ms
step:1602/2315 train_time:96990ms step_avg:60.54ms
step:1603/2315 train_time:97051ms step_avg:60.54ms
step:1604/2315 train_time:97112ms step_avg:60.54ms
step:1605/2315 train_time:97173ms step_avg:60.54ms
step:1606/2315 train_time:97234ms step_avg:60.54ms
step:1607/2315 train_time:97296ms step_avg:60.55ms
step:1608/2315 train_time:97357ms step_avg:60.55ms
step:1609/2315 train_time:97419ms step_avg:60.55ms
step:1610/2315 train_time:97480ms step_avg:60.55ms
step:1611/2315 train_time:97541ms step_avg:60.55ms
step:1612/2315 train_time:97602ms step_avg:60.55ms
step:1613/2315 train_time:97663ms step_avg:60.55ms
step:1614/2315 train_time:97724ms step_avg:60.55ms
step:1615/2315 train_time:97785ms step_avg:60.55ms
step:1616/2315 train_time:97846ms step_avg:60.55ms
step:1617/2315 train_time:97908ms step_avg:60.55ms
step:1618/2315 train_time:97969ms step_avg:60.55ms
step:1619/2315 train_time:98030ms step_avg:60.55ms
step:1620/2315 train_time:98091ms step_avg:60.55ms
step:1621/2315 train_time:98152ms step_avg:60.55ms
step:1622/2315 train_time:98213ms step_avg:60.55ms
step:1623/2315 train_time:98275ms step_avg:60.55ms
step:1624/2315 train_time:98336ms step_avg:60.55ms
step:1625/2315 train_time:98398ms step_avg:60.55ms
step:1626/2315 train_time:98458ms step_avg:60.55ms
step:1627/2315 train_time:98519ms step_avg:60.55ms
step:1628/2315 train_time:98580ms step_avg:60.55ms
step:1629/2315 train_time:98641ms step_avg:60.55ms
step:1630/2315 train_time:98702ms step_avg:60.55ms
step:1631/2315 train_time:98764ms step_avg:60.55ms
step:1632/2315 train_time:98825ms step_avg:60.55ms
step:1633/2315 train_time:98887ms step_avg:60.56ms
step:1634/2315 train_time:98948ms step_avg:60.56ms
step:1635/2315 train_time:99009ms step_avg:60.56ms
step:1636/2315 train_time:99070ms step_avg:60.56ms
step:1637/2315 train_time:99131ms step_avg:60.56ms
step:1638/2315 train_time:99192ms step_avg:60.56ms
step:1639/2315 train_time:99253ms step_avg:60.56ms
step:1640/2315 train_time:99315ms step_avg:60.56ms
step:1641/2315 train_time:99376ms step_avg:60.56ms
step:1642/2315 train_time:99437ms step_avg:60.56ms
step:1643/2315 train_time:99499ms step_avg:60.56ms
step:1644/2315 train_time:99560ms step_avg:60.56ms
step:1645/2315 train_time:99620ms step_avg:60.56ms
step:1646/2315 train_time:99682ms step_avg:60.56ms
step:1647/2315 train_time:99744ms step_avg:60.56ms
step:1648/2315 train_time:99805ms step_avg:60.56ms
step:1649/2315 train_time:99867ms step_avg:60.56ms
step:1650/2315 train_time:99928ms step_avg:60.56ms
step:1651/2315 train_time:99989ms step_avg:60.56ms
step:1652/2315 train_time:100050ms step_avg:60.56ms
step:1653/2315 train_time:100111ms step_avg:60.56ms
step:1654/2315 train_time:100172ms step_avg:60.56ms
step:1655/2315 train_time:100233ms step_avg:60.56ms
step:1656/2315 train_time:100294ms step_avg:60.56ms
step:1657/2315 train_time:100356ms step_avg:60.56ms
step:1658/2315 train_time:100417ms step_avg:60.56ms
step:1659/2315 train_time:100478ms step_avg:60.57ms
step:1660/2315 train_time:100539ms step_avg:60.57ms
step:1661/2315 train_time:100601ms step_avg:60.57ms
step:1662/2315 train_time:100661ms step_avg:60.57ms
step:1663/2315 train_time:100723ms step_avg:60.57ms
step:1664/2315 train_time:100784ms step_avg:60.57ms
step:1665/2315 train_time:100847ms step_avg:60.57ms
step:1666/2315 train_time:100908ms step_avg:60.57ms
step:1667/2315 train_time:100969ms step_avg:60.57ms
step:1668/2315 train_time:101030ms step_avg:60.57ms
step:1669/2315 train_time:101091ms step_avg:60.57ms
step:1670/2315 train_time:101152ms step_avg:60.57ms
step:1671/2315 train_time:101214ms step_avg:60.57ms
step:1672/2315 train_time:101275ms step_avg:60.57ms
step:1673/2315 train_time:101336ms step_avg:60.57ms
step:1674/2315 train_time:101397ms step_avg:60.57ms
step:1675/2315 train_time:101459ms step_avg:60.57ms
step:1676/2315 train_time:101520ms step_avg:60.57ms
step:1677/2315 train_time:101582ms step_avg:60.57ms
step:1678/2315 train_time:101643ms step_avg:60.57ms
step:1679/2315 train_time:101704ms step_avg:60.57ms
step:1680/2315 train_time:101766ms step_avg:60.57ms
step:1681/2315 train_time:101827ms step_avg:60.57ms
step:1682/2315 train_time:101888ms step_avg:60.58ms
step:1683/2315 train_time:101948ms step_avg:60.58ms
step:1684/2315 train_time:102009ms step_avg:60.58ms
step:1685/2315 train_time:102070ms step_avg:60.58ms
step:1686/2315 train_time:102131ms step_avg:60.58ms
step:1687/2315 train_time:102193ms step_avg:60.58ms
step:1688/2315 train_time:102253ms step_avg:60.58ms
step:1689/2315 train_time:102315ms step_avg:60.58ms
step:1690/2315 train_time:102376ms step_avg:60.58ms
step:1691/2315 train_time:102438ms step_avg:60.58ms
step:1692/2315 train_time:102499ms step_avg:60.58ms
step:1693/2315 train_time:102560ms step_avg:60.58ms
step:1694/2315 train_time:102622ms step_avg:60.58ms
step:1695/2315 train_time:102683ms step_avg:60.58ms
step:1696/2315 train_time:102744ms step_avg:60.58ms
step:1697/2315 train_time:102806ms step_avg:60.58ms
step:1698/2315 train_time:102867ms step_avg:60.58ms
step:1699/2315 train_time:102928ms step_avg:60.58ms
step:1700/2315 train_time:102989ms step_avg:60.58ms
step:1701/2315 train_time:103050ms step_avg:60.58ms
step:1702/2315 train_time:103111ms step_avg:60.58ms
step:1703/2315 train_time:103172ms step_avg:60.58ms
step:1704/2315 train_time:103232ms step_avg:60.58ms
step:1705/2315 train_time:103293ms step_avg:60.58ms
step:1706/2315 train_time:103355ms step_avg:60.58ms
step:1707/2315 train_time:103417ms step_avg:60.58ms
step:1708/2315 train_time:103478ms step_avg:60.58ms
step:1709/2315 train_time:103540ms step_avg:60.58ms
step:1710/2315 train_time:103600ms step_avg:60.59ms
step:1711/2315 train_time:103662ms step_avg:60.59ms
step:1712/2315 train_time:103723ms step_avg:60.59ms
step:1713/2315 train_time:103784ms step_avg:60.59ms
step:1714/2315 train_time:103846ms step_avg:60.59ms
step:1715/2315 train_time:103908ms step_avg:60.59ms
step:1716/2315 train_time:103969ms step_avg:60.59ms
step:1717/2315 train_time:104030ms step_avg:60.59ms
step:1718/2315 train_time:104091ms step_avg:60.59ms
step:1719/2315 train_time:104153ms step_avg:60.59ms
step:1720/2315 train_time:104213ms step_avg:60.59ms
step:1721/2315 train_time:104274ms step_avg:60.59ms
step:1722/2315 train_time:104335ms step_avg:60.59ms
step:1723/2315 train_time:104397ms step_avg:60.59ms
step:1724/2315 train_time:104458ms step_avg:60.59ms
step:1725/2315 train_time:104519ms step_avg:60.59ms
step:1726/2315 train_time:104580ms step_avg:60.59ms
step:1727/2315 train_time:104642ms step_avg:60.59ms
step:1728/2315 train_time:104703ms step_avg:60.59ms
step:1729/2315 train_time:104764ms step_avg:60.59ms
step:1730/2315 train_time:104825ms step_avg:60.59ms
step:1731/2315 train_time:104887ms step_avg:60.59ms
step:1732/2315 train_time:104947ms step_avg:60.59ms
step:1733/2315 train_time:105009ms step_avg:60.59ms
step:1734/2315 train_time:105070ms step_avg:60.59ms
step:1735/2315 train_time:105131ms step_avg:60.59ms
step:1736/2315 train_time:105192ms step_avg:60.59ms
step:1737/2315 train_time:105254ms step_avg:60.60ms
step:1738/2315 train_time:105315ms step_avg:60.60ms
step:1739/2315 train_time:105376ms step_avg:60.60ms
step:1740/2315 train_time:105437ms step_avg:60.60ms
step:1741/2315 train_time:105498ms step_avg:60.60ms
step:1742/2315 train_time:105559ms step_avg:60.60ms
step:1743/2315 train_time:105621ms step_avg:60.60ms
step:1744/2315 train_time:105682ms step_avg:60.60ms
step:1745/2315 train_time:105744ms step_avg:60.60ms
step:1746/2315 train_time:105806ms step_avg:60.60ms
step:1747/2315 train_time:105867ms step_avg:60.60ms
step:1748/2315 train_time:105928ms step_avg:60.60ms
step:1749/2315 train_time:105989ms step_avg:60.60ms
step:1750/2315 train_time:106050ms step_avg:60.60ms
step:1750/2315 val_loss:3.3808 train_time:106113ms step_avg:60.64ms
step:1751/2315 train_time:106133ms step_avg:60.61ms
step:1752/2315 train_time:106179ms step_avg:60.60ms
step:1753/2315 train_time:106248ms step_avg:60.61ms
step:1754/2315 train_time:106310ms step_avg:60.61ms
step:1755/2315 train_time:106371ms step_avg:60.61ms
step:1756/2315 train_time:106432ms step_avg:60.61ms
step:1757/2315 train_time:106493ms step_avg:60.61ms
step:1758/2315 train_time:106554ms step_avg:60.61ms
step:1759/2315 train_time:106616ms step_avg:60.61ms
step:1760/2315 train_time:106676ms step_avg:60.61ms
step:1761/2315 train_time:106736ms step_avg:60.61ms
step:1762/2315 train_time:106797ms step_avg:60.61ms
step:1763/2315 train_time:106858ms step_avg:60.61ms
step:1764/2315 train_time:106918ms step_avg:60.61ms
step:1765/2315 train_time:106979ms step_avg:60.61ms
step:1766/2315 train_time:107041ms step_avg:60.61ms
step:1767/2315 train_time:107103ms step_avg:60.61ms
step:1768/2315 train_time:107166ms step_avg:60.61ms
step:1769/2315 train_time:107229ms step_avg:60.62ms
step:1770/2315 train_time:107290ms step_avg:60.62ms
step:1771/2315 train_time:107352ms step_avg:60.62ms
step:1772/2315 train_time:107413ms step_avg:60.62ms
step:1773/2315 train_time:107474ms step_avg:60.62ms
step:1774/2315 train_time:107535ms step_avg:60.62ms
step:1775/2315 train_time:107595ms step_avg:60.62ms
step:1776/2315 train_time:107656ms step_avg:60.62ms
step:1777/2315 train_time:107717ms step_avg:60.62ms
step:1778/2315 train_time:107778ms step_avg:60.62ms
step:1779/2315 train_time:107839ms step_avg:60.62ms
step:1780/2315 train_time:107899ms step_avg:60.62ms
step:1781/2315 train_time:107961ms step_avg:60.62ms
step:1782/2315 train_time:108022ms step_avg:60.62ms
step:1783/2315 train_time:108084ms step_avg:60.62ms
step:1784/2315 train_time:108146ms step_avg:60.62ms
step:1785/2315 train_time:108208ms step_avg:60.62ms
step:1786/2315 train_time:108269ms step_avg:60.62ms
step:1787/2315 train_time:108331ms step_avg:60.62ms
step:1788/2315 train_time:108392ms step_avg:60.62ms
step:1789/2315 train_time:108453ms step_avg:60.62ms
step:1790/2315 train_time:108515ms step_avg:60.62ms
step:1791/2315 train_time:108576ms step_avg:60.62ms
step:1792/2315 train_time:108636ms step_avg:60.62ms
step:1793/2315 train_time:108697ms step_avg:60.62ms
step:1794/2315 train_time:108758ms step_avg:60.62ms
step:1795/2315 train_time:108819ms step_avg:60.62ms
step:1796/2315 train_time:108880ms step_avg:60.62ms
step:1797/2315 train_time:108941ms step_avg:60.62ms
step:1798/2315 train_time:109002ms step_avg:60.62ms
step:1799/2315 train_time:109063ms step_avg:60.62ms
step:1800/2315 train_time:109124ms step_avg:60.62ms
step:1801/2315 train_time:109186ms step_avg:60.63ms
step:1802/2315 train_time:109247ms step_avg:60.63ms
step:1803/2315 train_time:109309ms step_avg:60.63ms
step:1804/2315 train_time:109370ms step_avg:60.63ms
step:1805/2315 train_time:109432ms step_avg:60.63ms
step:1806/2315 train_time:109493ms step_avg:60.63ms
step:1807/2315 train_time:109554ms step_avg:60.63ms
step:1808/2315 train_time:109615ms step_avg:60.63ms
step:1809/2315 train_time:109676ms step_avg:60.63ms
step:1810/2315 train_time:109737ms step_avg:60.63ms
step:1811/2315 train_time:109799ms step_avg:60.63ms
step:1812/2315 train_time:109859ms step_avg:60.63ms
step:1813/2315 train_time:109921ms step_avg:60.63ms
step:1814/2315 train_time:109981ms step_avg:60.63ms
step:1815/2315 train_time:110042ms step_avg:60.63ms
step:1816/2315 train_time:110104ms step_avg:60.63ms
step:1817/2315 train_time:110166ms step_avg:60.63ms
step:1818/2315 train_time:110227ms step_avg:60.63ms
step:1819/2315 train_time:110288ms step_avg:60.63ms
step:1820/2315 train_time:110350ms step_avg:60.63ms
step:1821/2315 train_time:110411ms step_avg:60.63ms
step:1822/2315 train_time:110472ms step_avg:60.63ms
step:1823/2315 train_time:110533ms step_avg:60.63ms
step:1824/2315 train_time:110594ms step_avg:60.63ms
step:1825/2315 train_time:110656ms step_avg:60.63ms
step:1826/2315 train_time:110718ms step_avg:60.63ms
step:1827/2315 train_time:110778ms step_avg:60.63ms
step:1828/2315 train_time:110839ms step_avg:60.63ms
step:1829/2315 train_time:110900ms step_avg:60.63ms
step:1830/2315 train_time:110961ms step_avg:60.63ms
step:1831/2315 train_time:111022ms step_avg:60.63ms
step:1832/2315 train_time:111083ms step_avg:60.63ms
step:1833/2315 train_time:111144ms step_avg:60.64ms
step:1834/2315 train_time:111205ms step_avg:60.64ms
step:1835/2315 train_time:111267ms step_avg:60.64ms
step:1836/2315 train_time:111328ms step_avg:60.64ms
step:1837/2315 train_time:111390ms step_avg:60.64ms
step:1838/2315 train_time:111451ms step_avg:60.64ms
step:1839/2315 train_time:111513ms step_avg:60.64ms
step:1840/2315 train_time:111574ms step_avg:60.64ms
step:1841/2315 train_time:111635ms step_avg:60.64ms
step:1842/2315 train_time:111696ms step_avg:60.64ms
step:1843/2315 train_time:111757ms step_avg:60.64ms
step:1844/2315 train_time:111819ms step_avg:60.64ms
step:1845/2315 train_time:111880ms step_avg:60.64ms
step:1846/2315 train_time:111940ms step_avg:60.64ms
step:1847/2315 train_time:112002ms step_avg:60.64ms
step:1848/2315 train_time:112063ms step_avg:60.64ms
step:1849/2315 train_time:112125ms step_avg:60.64ms
step:1850/2315 train_time:112186ms step_avg:60.64ms
step:1851/2315 train_time:112247ms step_avg:60.64ms
step:1852/2315 train_time:112308ms step_avg:60.64ms
step:1853/2315 train_time:112370ms step_avg:60.64ms
step:1854/2315 train_time:112431ms step_avg:60.64ms
step:1855/2315 train_time:112492ms step_avg:60.64ms
step:1856/2315 train_time:112553ms step_avg:60.64ms
step:1857/2315 train_time:112615ms step_avg:60.64ms
step:1858/2315 train_time:112676ms step_avg:60.64ms
step:1859/2315 train_time:112737ms step_avg:60.64ms
step:1860/2315 train_time:112797ms step_avg:60.64ms
step:1861/2315 train_time:112859ms step_avg:60.64ms
step:1862/2315 train_time:112920ms step_avg:60.64ms
step:1863/2315 train_time:112981ms step_avg:60.64ms
step:1864/2315 train_time:113042ms step_avg:60.65ms
step:1865/2315 train_time:113104ms step_avg:60.65ms
step:1866/2315 train_time:113165ms step_avg:60.65ms
step:1867/2315 train_time:113227ms step_avg:60.65ms
step:1868/2315 train_time:113288ms step_avg:60.65ms
step:1869/2315 train_time:113349ms step_avg:60.65ms
step:1870/2315 train_time:113410ms step_avg:60.65ms
step:1871/2315 train_time:113471ms step_avg:60.65ms
step:1872/2315 train_time:113532ms step_avg:60.65ms
step:1873/2315 train_time:113594ms step_avg:60.65ms
step:1874/2315 train_time:113655ms step_avg:60.65ms
step:1875/2315 train_time:113717ms step_avg:60.65ms
step:1876/2315 train_time:113779ms step_avg:60.65ms
step:1877/2315 train_time:113840ms step_avg:60.65ms
step:1878/2315 train_time:113901ms step_avg:60.65ms
step:1879/2315 train_time:113962ms step_avg:60.65ms
step:1880/2315 train_time:114023ms step_avg:60.65ms
step:1881/2315 train_time:114084ms step_avg:60.65ms
step:1882/2315 train_time:114145ms step_avg:60.65ms
step:1883/2315 train_time:114206ms step_avg:60.65ms
step:1884/2315 train_time:114267ms step_avg:60.65ms
step:1885/2315 train_time:114328ms step_avg:60.65ms
step:1886/2315 train_time:114389ms step_avg:60.65ms
step:1887/2315 train_time:114451ms step_avg:60.65ms
step:1888/2315 train_time:114513ms step_avg:60.65ms
step:1889/2315 train_time:114575ms step_avg:60.65ms
step:1890/2315 train_time:114635ms step_avg:60.65ms
step:1891/2315 train_time:114697ms step_avg:60.65ms
step:1892/2315 train_time:114758ms step_avg:60.65ms
step:1893/2315 train_time:114819ms step_avg:60.65ms
step:1894/2315 train_time:114880ms step_avg:60.65ms
step:1895/2315 train_time:114941ms step_avg:60.65ms
step:1896/2315 train_time:115002ms step_avg:60.65ms
step:1897/2315 train_time:115063ms step_avg:60.66ms
step:1898/2315 train_time:115124ms step_avg:60.66ms
step:1899/2315 train_time:115186ms step_avg:60.66ms
step:1900/2315 train_time:115247ms step_avg:60.66ms
step:1901/2315 train_time:115308ms step_avg:60.66ms
step:1902/2315 train_time:115369ms step_avg:60.66ms
step:1903/2315 train_time:115431ms step_avg:60.66ms
step:1904/2315 train_time:115492ms step_avg:60.66ms
step:1905/2315 train_time:115553ms step_avg:60.66ms
step:1906/2315 train_time:115614ms step_avg:60.66ms
step:1907/2315 train_time:115676ms step_avg:60.66ms
step:1908/2315 train_time:115737ms step_avg:60.66ms
step:1909/2315 train_time:115798ms step_avg:60.66ms
step:1910/2315 train_time:115859ms step_avg:60.66ms
step:1911/2315 train_time:115921ms step_avg:60.66ms
step:1912/2315 train_time:115982ms step_avg:60.66ms
step:1913/2315 train_time:116043ms step_avg:60.66ms
step:1914/2315 train_time:116104ms step_avg:60.66ms
step:1915/2315 train_time:116165ms step_avg:60.66ms
step:1916/2315 train_time:116226ms step_avg:60.66ms
step:1917/2315 train_time:116287ms step_avg:60.66ms
step:1918/2315 train_time:116348ms step_avg:60.66ms
step:1919/2315 train_time:116409ms step_avg:60.66ms
step:1920/2315 train_time:116470ms step_avg:60.66ms
step:1921/2315 train_time:116532ms step_avg:60.66ms
step:1922/2315 train_time:116593ms step_avg:60.66ms
step:1923/2315 train_time:116655ms step_avg:60.66ms
step:1924/2315 train_time:116716ms step_avg:60.66ms
step:1925/2315 train_time:116777ms step_avg:60.66ms
step:1926/2315 train_time:116838ms step_avg:60.66ms
step:1927/2315 train_time:116899ms step_avg:60.66ms
step:1928/2315 train_time:116960ms step_avg:60.66ms
step:1929/2315 train_time:117022ms step_avg:60.66ms
step:1930/2315 train_time:117083ms step_avg:60.66ms
step:1931/2315 train_time:117145ms step_avg:60.67ms
step:1932/2315 train_time:117207ms step_avg:60.67ms
step:1933/2315 train_time:117268ms step_avg:60.67ms
step:1934/2315 train_time:117329ms step_avg:60.67ms
step:1935/2315 train_time:117390ms step_avg:60.67ms
step:1936/2315 train_time:117451ms step_avg:60.67ms
step:1937/2315 train_time:117512ms step_avg:60.67ms
step:1938/2315 train_time:117573ms step_avg:60.67ms
step:1939/2315 train_time:117635ms step_avg:60.67ms
step:1940/2315 train_time:117696ms step_avg:60.67ms
step:1941/2315 train_time:117757ms step_avg:60.67ms
step:1942/2315 train_time:117818ms step_avg:60.67ms
step:1943/2315 train_time:117879ms step_avg:60.67ms
step:1944/2315 train_time:117940ms step_avg:60.67ms
step:1945/2315 train_time:118002ms step_avg:60.67ms
step:1946/2315 train_time:118062ms step_avg:60.67ms
step:1947/2315 train_time:118124ms step_avg:60.67ms
step:1948/2315 train_time:118186ms step_avg:60.67ms
step:1949/2315 train_time:118247ms step_avg:60.67ms
step:1950/2315 train_time:118308ms step_avg:60.67ms
step:1951/2315 train_time:118369ms step_avg:60.67ms
step:1952/2315 train_time:118430ms step_avg:60.67ms
step:1953/2315 train_time:118491ms step_avg:60.67ms
step:1954/2315 train_time:118552ms step_avg:60.67ms
step:1955/2315 train_time:118614ms step_avg:60.67ms
step:1956/2315 train_time:118675ms step_avg:60.67ms
step:1957/2315 train_time:118736ms step_avg:60.67ms
step:1958/2315 train_time:118797ms step_avg:60.67ms
step:1959/2315 train_time:118859ms step_avg:60.67ms
step:1960/2315 train_time:118920ms step_avg:60.67ms
step:1961/2315 train_time:118981ms step_avg:60.67ms
step:1962/2315 train_time:119042ms step_avg:60.67ms
step:1963/2315 train_time:119103ms step_avg:60.67ms
step:1964/2315 train_time:119164ms step_avg:60.67ms
step:1965/2315 train_time:119226ms step_avg:60.67ms
step:1966/2315 train_time:119287ms step_avg:60.67ms
step:1967/2315 train_time:119348ms step_avg:60.68ms
step:1968/2315 train_time:119409ms step_avg:60.68ms
step:1969/2315 train_time:119470ms step_avg:60.68ms
step:1970/2315 train_time:119531ms step_avg:60.68ms
step:1971/2315 train_time:119593ms step_avg:60.68ms
step:1972/2315 train_time:119654ms step_avg:60.68ms
step:1973/2315 train_time:119716ms step_avg:60.68ms
step:1974/2315 train_time:119777ms step_avg:60.68ms
step:1975/2315 train_time:119838ms step_avg:60.68ms
step:1976/2315 train_time:119899ms step_avg:60.68ms
step:1977/2315 train_time:119960ms step_avg:60.68ms
step:1978/2315 train_time:120022ms step_avg:60.68ms
step:1979/2315 train_time:120083ms step_avg:60.68ms
step:1980/2315 train_time:120144ms step_avg:60.68ms
step:1981/2315 train_time:120206ms step_avg:60.68ms
step:1982/2315 train_time:120267ms step_avg:60.68ms
step:1983/2315 train_time:120328ms step_avg:60.68ms
step:1984/2315 train_time:120389ms step_avg:60.68ms
step:1985/2315 train_time:120451ms step_avg:60.68ms
step:1986/2315 train_time:120512ms step_avg:60.68ms
step:1987/2315 train_time:120573ms step_avg:60.68ms
step:1988/2315 train_time:120635ms step_avg:60.68ms
step:1989/2315 train_time:120696ms step_avg:60.68ms
step:1990/2315 train_time:120757ms step_avg:60.68ms
step:1991/2315 train_time:120818ms step_avg:60.68ms
step:1992/2315 train_time:120879ms step_avg:60.68ms
step:1993/2315 train_time:120940ms step_avg:60.68ms
step:1994/2315 train_time:121002ms step_avg:60.68ms
step:1995/2315 train_time:121063ms step_avg:60.68ms
step:1996/2315 train_time:121124ms step_avg:60.68ms
step:1997/2315 train_time:121186ms step_avg:60.68ms
step:1998/2315 train_time:121246ms step_avg:60.68ms
step:1999/2315 train_time:121308ms step_avg:60.68ms
step:2000/2315 train_time:121369ms step_avg:60.68ms
step:2000/2315 val_loss:3.3314 train_time:121432ms step_avg:60.72ms
step:2001/2315 train_time:121451ms step_avg:60.70ms
step:2002/2315 train_time:121494ms step_avg:60.69ms
step:2003/2315 train_time:121559ms step_avg:60.69ms
step:2004/2315 train_time:121624ms step_avg:60.69ms
step:2005/2315 train_time:121686ms step_avg:60.69ms
step:2006/2315 train_time:121748ms step_avg:60.69ms
step:2007/2315 train_time:121809ms step_avg:60.69ms
step:2008/2315 train_time:121870ms step_avg:60.69ms
step:2009/2315 train_time:121931ms step_avg:60.69ms
step:2010/2315 train_time:121991ms step_avg:60.69ms
step:2011/2315 train_time:122052ms step_avg:60.69ms
step:2012/2315 train_time:122113ms step_avg:60.69ms
step:2013/2315 train_time:122173ms step_avg:60.69ms
step:2014/2315 train_time:122234ms step_avg:60.69ms
step:2015/2315 train_time:122295ms step_avg:60.69ms
step:2016/2315 train_time:122355ms step_avg:60.69ms
step:2017/2315 train_time:122417ms step_avg:60.69ms
step:2018/2315 train_time:122479ms step_avg:60.69ms
step:2019/2315 train_time:122542ms step_avg:60.69ms
step:2020/2315 train_time:122604ms step_avg:60.69ms
step:2021/2315 train_time:122666ms step_avg:60.70ms
step:2022/2315 train_time:122727ms step_avg:60.70ms
step:2023/2315 train_time:122789ms step_avg:60.70ms
step:2024/2315 train_time:122850ms step_avg:60.70ms
step:2025/2315 train_time:122911ms step_avg:60.70ms
step:2026/2315 train_time:122972ms step_avg:60.70ms
step:2027/2315 train_time:123033ms step_avg:60.70ms
step:2028/2315 train_time:123094ms step_avg:60.70ms
step:2029/2315 train_time:123154ms step_avg:60.70ms
step:2030/2315 train_time:123215ms step_avg:60.70ms
step:2031/2315 train_time:123276ms step_avg:60.70ms
step:2032/2315 train_time:123337ms step_avg:60.70ms
step:2033/2315 train_time:123399ms step_avg:60.70ms
step:2034/2315 train_time:123460ms step_avg:60.70ms
step:2035/2315 train_time:123521ms step_avg:60.70ms
step:2036/2315 train_time:123583ms step_avg:60.70ms
step:2037/2315 train_time:123645ms step_avg:60.70ms
step:2038/2315 train_time:123706ms step_avg:60.70ms
step:2039/2315 train_time:123768ms step_avg:60.70ms
step:2040/2315 train_time:123829ms step_avg:60.70ms
step:2041/2315 train_time:123890ms step_avg:60.70ms
step:2042/2315 train_time:123951ms step_avg:60.70ms
step:2043/2315 train_time:124012ms step_avg:60.70ms
step:2044/2315 train_time:124073ms step_avg:60.70ms
step:2045/2315 train_time:124134ms step_avg:60.70ms
step:2046/2315 train_time:124194ms step_avg:60.70ms
step:2047/2315 train_time:124255ms step_avg:60.70ms
step:2048/2315 train_time:124316ms step_avg:60.70ms
step:2049/2315 train_time:124377ms step_avg:60.70ms
step:2050/2315 train_time:124439ms step_avg:60.70ms
step:2051/2315 train_time:124501ms step_avg:60.70ms
step:2052/2315 train_time:124562ms step_avg:60.70ms
step:2053/2315 train_time:124623ms step_avg:60.70ms
step:2054/2315 train_time:124685ms step_avg:60.70ms
step:2055/2315 train_time:124746ms step_avg:60.70ms
step:2056/2315 train_time:124807ms step_avg:60.70ms
step:2057/2315 train_time:124869ms step_avg:60.70ms
step:2058/2315 train_time:124930ms step_avg:60.70ms
step:2059/2315 train_time:124991ms step_avg:60.70ms
step:2060/2315 train_time:125052ms step_avg:60.71ms
step:2061/2315 train_time:125113ms step_avg:60.71ms
step:2062/2315 train_time:125174ms step_avg:60.71ms
step:2063/2315 train_time:125236ms step_avg:60.71ms
step:2064/2315 train_time:125297ms step_avg:60.71ms
step:2065/2315 train_time:125358ms step_avg:60.71ms
step:2066/2315 train_time:125420ms step_avg:60.71ms
step:2067/2315 train_time:125481ms step_avg:60.71ms
step:2068/2315 train_time:125542ms step_avg:60.71ms
step:2069/2315 train_time:125604ms step_avg:60.71ms
step:2070/2315 train_time:125665ms step_avg:60.71ms
step:2071/2315 train_time:125727ms step_avg:60.71ms
step:2072/2315 train_time:125788ms step_avg:60.71ms
step:2073/2315 train_time:125850ms step_avg:60.71ms
step:2074/2315 train_time:125911ms step_avg:60.71ms
step:2075/2315 train_time:125972ms step_avg:60.71ms
step:2076/2315 train_time:126033ms step_avg:60.71ms
step:2077/2315 train_time:126094ms step_avg:60.71ms
step:2078/2315 train_time:126155ms step_avg:60.71ms
step:2079/2315 train_time:126216ms step_avg:60.71ms
step:2080/2315 train_time:126277ms step_avg:60.71ms
step:2081/2315 train_time:126339ms step_avg:60.71ms
step:2082/2315 train_time:126400ms step_avg:60.71ms
step:2083/2315 train_time:126461ms step_avg:60.71ms
step:2084/2315 train_time:126522ms step_avg:60.71ms
step:2085/2315 train_time:126584ms step_avg:60.71ms
step:2086/2315 train_time:126645ms step_avg:60.71ms
step:2087/2315 train_time:126706ms step_avg:60.71ms
step:2088/2315 train_time:126767ms step_avg:60.71ms
step:2089/2315 train_time:126829ms step_avg:60.71ms
step:2090/2315 train_time:126889ms step_avg:60.71ms
step:2091/2315 train_time:126951ms step_avg:60.71ms
step:2092/2315 train_time:127012ms step_avg:60.71ms
step:2093/2315 train_time:127073ms step_avg:60.71ms
step:2094/2315 train_time:127134ms step_avg:60.71ms
step:2095/2315 train_time:127195ms step_avg:60.71ms
step:2096/2315 train_time:127257ms step_avg:60.71ms
step:2097/2315 train_time:127318ms step_avg:60.71ms
step:2098/2315 train_time:127379ms step_avg:60.71ms
step:2099/2315 train_time:127441ms step_avg:60.71ms
step:2100/2315 train_time:127502ms step_avg:60.72ms
step:2101/2315 train_time:127563ms step_avg:60.72ms
step:2102/2315 train_time:127625ms step_avg:60.72ms
step:2103/2315 train_time:127686ms step_avg:60.72ms
step:2104/2315 train_time:127747ms step_avg:60.72ms
step:2105/2315 train_time:127808ms step_avg:60.72ms
step:2106/2315 train_time:127870ms step_avg:60.72ms
step:2107/2315 train_time:127932ms step_avg:60.72ms
step:2108/2315 train_time:127993ms step_avg:60.72ms
step:2109/2315 train_time:128054ms step_avg:60.72ms
step:2110/2315 train_time:128115ms step_avg:60.72ms
step:2111/2315 train_time:128176ms step_avg:60.72ms
step:2112/2315 train_time:128237ms step_avg:60.72ms
step:2113/2315 train_time:128298ms step_avg:60.72ms
step:2114/2315 train_time:128359ms step_avg:60.72ms
step:2115/2315 train_time:128420ms step_avg:60.72ms
step:2116/2315 train_time:128481ms step_avg:60.72ms
step:2117/2315 train_time:128542ms step_avg:60.72ms
step:2118/2315 train_time:128603ms step_avg:60.72ms
step:2119/2315 train_time:128665ms step_avg:60.72ms
step:2120/2315 train_time:128726ms step_avg:60.72ms
step:2121/2315 train_time:128787ms step_avg:60.72ms
step:2122/2315 train_time:128848ms step_avg:60.72ms
step:2123/2315 train_time:128910ms step_avg:60.72ms
step:2124/2315 train_time:128970ms step_avg:60.72ms
step:2125/2315 train_time:129032ms step_avg:60.72ms
step:2126/2315 train_time:129093ms step_avg:60.72ms
step:2127/2315 train_time:129154ms step_avg:60.72ms
step:2128/2315 train_time:129215ms step_avg:60.72ms
step:2129/2315 train_time:129277ms step_avg:60.72ms
step:2130/2315 train_time:129339ms step_avg:60.72ms
step:2131/2315 train_time:129400ms step_avg:60.72ms
step:2132/2315 train_time:129461ms step_avg:60.72ms
step:2133/2315 train_time:129522ms step_avg:60.72ms
step:2134/2315 train_time:129583ms step_avg:60.72ms
step:2135/2315 train_time:129645ms step_avg:60.72ms
step:2136/2315 train_time:129706ms step_avg:60.72ms
step:2137/2315 train_time:129767ms step_avg:60.72ms
step:2138/2315 train_time:129829ms step_avg:60.72ms
step:2139/2315 train_time:129890ms step_avg:60.72ms
step:2140/2315 train_time:129951ms step_avg:60.72ms
step:2141/2315 train_time:130012ms step_avg:60.73ms
step:2142/2315 train_time:130074ms step_avg:60.73ms
step:2143/2315 train_time:130135ms step_avg:60.73ms
step:2144/2315 train_time:130196ms step_avg:60.73ms
step:2145/2315 train_time:130258ms step_avg:60.73ms
step:2146/2315 train_time:130319ms step_avg:60.73ms
step:2147/2315 train_time:130380ms step_avg:60.73ms
step:2148/2315 train_time:130441ms step_avg:60.73ms
step:2149/2315 train_time:130502ms step_avg:60.73ms
step:2150/2315 train_time:130563ms step_avg:60.73ms
step:2151/2315 train_time:130625ms step_avg:60.73ms
step:2152/2315 train_time:130686ms step_avg:60.73ms
step:2153/2315 train_time:130747ms step_avg:60.73ms
step:2154/2315 train_time:130808ms step_avg:60.73ms
step:2155/2315 train_time:130870ms step_avg:60.73ms
step:2156/2315 train_time:130930ms step_avg:60.73ms
step:2157/2315 train_time:130991ms step_avg:60.73ms
step:2158/2315 train_time:131052ms step_avg:60.73ms
step:2159/2315 train_time:131114ms step_avg:60.73ms
step:2160/2315 train_time:131175ms step_avg:60.73ms
step:2161/2315 train_time:131236ms step_avg:60.73ms
step:2162/2315 train_time:131297ms step_avg:60.73ms
step:2163/2315 train_time:131359ms step_avg:60.73ms
step:2164/2315 train_time:131420ms step_avg:60.73ms
step:2165/2315 train_time:131482ms step_avg:60.73ms
step:2166/2315 train_time:131543ms step_avg:60.73ms
step:2167/2315 train_time:131604ms step_avg:60.73ms
step:2168/2315 train_time:131665ms step_avg:60.73ms
step:2169/2315 train_time:131726ms step_avg:60.73ms
step:2170/2315 train_time:131788ms step_avg:60.73ms
step:2171/2315 train_time:131850ms step_avg:60.73ms
step:2172/2315 train_time:131911ms step_avg:60.73ms
step:2173/2315 train_time:131972ms step_avg:60.73ms
step:2174/2315 train_time:132033ms step_avg:60.73ms
step:2175/2315 train_time:132094ms step_avg:60.73ms
step:2176/2315 train_time:132155ms step_avg:60.73ms
step:2177/2315 train_time:132216ms step_avg:60.73ms
step:2178/2315 train_time:132278ms step_avg:60.73ms
step:2179/2315 train_time:132339ms step_avg:60.73ms
step:2180/2315 train_time:132400ms step_avg:60.73ms
step:2181/2315 train_time:132461ms step_avg:60.73ms
step:2182/2315 train_time:132523ms step_avg:60.73ms
step:2183/2315 train_time:132584ms step_avg:60.73ms
step:2184/2315 train_time:132645ms step_avg:60.73ms
step:2185/2315 train_time:132706ms step_avg:60.74ms
step:2186/2315 train_time:132767ms step_avg:60.74ms
step:2187/2315 train_time:132829ms step_avg:60.74ms
step:2188/2315 train_time:132890ms step_avg:60.74ms
step:2189/2315 train_time:132951ms step_avg:60.74ms
step:2190/2315 train_time:133012ms step_avg:60.74ms
step:2191/2315 train_time:133074ms step_avg:60.74ms
step:2192/2315 train_time:133135ms step_avg:60.74ms
step:2193/2315 train_time:133196ms step_avg:60.74ms
step:2194/2315 train_time:133257ms step_avg:60.74ms
step:2195/2315 train_time:133319ms step_avg:60.74ms
step:2196/2315 train_time:133380ms step_avg:60.74ms
step:2197/2315 train_time:133441ms step_avg:60.74ms
step:2198/2315 train_time:133502ms step_avg:60.74ms
step:2199/2315 train_time:133563ms step_avg:60.74ms
step:2200/2315 train_time:133624ms step_avg:60.74ms
step:2201/2315 train_time:133685ms step_avg:60.74ms
step:2202/2315 train_time:133747ms step_avg:60.74ms
step:2203/2315 train_time:133809ms step_avg:60.74ms
step:2204/2315 train_time:133870ms step_avg:60.74ms
step:2205/2315 train_time:133931ms step_avg:60.74ms
step:2206/2315 train_time:133993ms step_avg:60.74ms
step:2207/2315 train_time:134054ms step_avg:60.74ms
step:2208/2315 train_time:134115ms step_avg:60.74ms
step:2209/2315 train_time:134176ms step_avg:60.74ms
step:2210/2315 train_time:134238ms step_avg:60.74ms
step:2211/2315 train_time:134299ms step_avg:60.74ms
step:2212/2315 train_time:134360ms step_avg:60.74ms
step:2213/2315 train_time:134421ms step_avg:60.74ms
step:2214/2315 train_time:134483ms step_avg:60.74ms
step:2215/2315 train_time:134545ms step_avg:60.74ms
step:2216/2315 train_time:134606ms step_avg:60.74ms
step:2217/2315 train_time:134667ms step_avg:60.74ms
step:2218/2315 train_time:134729ms step_avg:60.74ms
step:2219/2315 train_time:134790ms step_avg:60.74ms
step:2220/2315 train_time:134851ms step_avg:60.74ms
step:2221/2315 train_time:134913ms step_avg:60.74ms
step:2222/2315 train_time:134974ms step_avg:60.74ms
step:2223/2315 train_time:135035ms step_avg:60.74ms
step:2224/2315 train_time:135096ms step_avg:60.74ms
step:2225/2315 train_time:135157ms step_avg:60.74ms
step:2226/2315 train_time:135218ms step_avg:60.74ms
step:2227/2315 train_time:135280ms step_avg:60.75ms
step:2228/2315 train_time:135341ms step_avg:60.75ms
step:2229/2315 train_time:135401ms step_avg:60.75ms
step:2230/2315 train_time:135462ms step_avg:60.75ms
step:2231/2315 train_time:135524ms step_avg:60.75ms
step:2232/2315 train_time:135585ms step_avg:60.75ms
step:2233/2315 train_time:135646ms step_avg:60.75ms
step:2234/2315 train_time:135707ms step_avg:60.75ms
step:2235/2315 train_time:135770ms step_avg:60.75ms
step:2236/2315 train_time:135831ms step_avg:60.75ms
step:2237/2315 train_time:135892ms step_avg:60.75ms
step:2238/2315 train_time:135953ms step_avg:60.75ms
step:2239/2315 train_time:136015ms step_avg:60.75ms
step:2240/2315 train_time:136076ms step_avg:60.75ms
step:2241/2315 train_time:136137ms step_avg:60.75ms
step:2242/2315 train_time:136198ms step_avg:60.75ms
step:2243/2315 train_time:136259ms step_avg:60.75ms
step:2244/2315 train_time:136321ms step_avg:60.75ms
step:2245/2315 train_time:136381ms step_avg:60.75ms
step:2246/2315 train_time:136443ms step_avg:60.75ms
step:2247/2315 train_time:136503ms step_avg:60.75ms
step:2248/2315 train_time:136564ms step_avg:60.75ms
step:2249/2315 train_time:136625ms step_avg:60.75ms
step:2250/2315 train_time:136686ms step_avg:60.75ms
step:2250/2315 val_loss:3.2917 train_time:136750ms step_avg:60.78ms
step:2251/2315 train_time:136770ms step_avg:60.76ms
step:2252/2315 train_time:136812ms step_avg:60.75ms
step:2253/2315 train_time:136878ms step_avg:60.75ms
step:2254/2315 train_time:136942ms step_avg:60.75ms
step:2255/2315 train_time:137003ms step_avg:60.76ms
step:2256/2315 train_time:137064ms step_avg:60.76ms
step:2257/2315 train_time:137125ms step_avg:60.76ms
step:2258/2315 train_time:137185ms step_avg:60.76ms
step:2259/2315 train_time:137247ms step_avg:60.76ms
step:2260/2315 train_time:137307ms step_avg:60.76ms
step:2261/2315 train_time:137368ms step_avg:60.76ms
step:2262/2315 train_time:137429ms step_avg:60.76ms
step:2263/2315 train_time:137489ms step_avg:60.76ms
step:2264/2315 train_time:137551ms step_avg:60.76ms
step:2265/2315 train_time:137611ms step_avg:60.76ms
step:2266/2315 train_time:137672ms step_avg:60.76ms
step:2267/2315 train_time:137733ms step_avg:60.76ms
step:2268/2315 train_time:137795ms step_avg:60.76ms
step:2269/2315 train_time:137858ms step_avg:60.76ms
step:2270/2315 train_time:137920ms step_avg:60.76ms
step:2271/2315 train_time:137982ms step_avg:60.76ms
step:2272/2315 train_time:138044ms step_avg:60.76ms
step:2273/2315 train_time:138105ms step_avg:60.76ms
step:2274/2315 train_time:138165ms step_avg:60.76ms
step:2275/2315 train_time:138226ms step_avg:60.76ms
step:2276/2315 train_time:138287ms step_avg:60.76ms
step:2277/2315 train_time:138347ms step_avg:60.76ms
step:2278/2315 train_time:138408ms step_avg:60.76ms
step:2279/2315 train_time:138470ms step_avg:60.76ms
step:2280/2315 train_time:138531ms step_avg:60.76ms
step:2281/2315 train_time:138592ms step_avg:60.76ms
step:2282/2315 train_time:138652ms step_avg:60.76ms
step:2283/2315 train_time:138714ms step_avg:60.76ms
step:2284/2315 train_time:138775ms step_avg:60.76ms
step:2285/2315 train_time:138837ms step_avg:60.76ms
step:2286/2315 train_time:138899ms step_avg:60.76ms
step:2287/2315 train_time:138961ms step_avg:60.76ms
step:2288/2315 train_time:139023ms step_avg:60.76ms
step:2289/2315 train_time:139084ms step_avg:60.76ms
step:2290/2315 train_time:139145ms step_avg:60.76ms
step:2291/2315 train_time:139207ms step_avg:60.76ms
step:2292/2315 train_time:139268ms step_avg:60.76ms
step:2293/2315 train_time:139329ms step_avg:60.76ms
step:2294/2315 train_time:139390ms step_avg:60.76ms
step:2295/2315 train_time:139452ms step_avg:60.76ms
step:2296/2315 train_time:139513ms step_avg:60.76ms
step:2297/2315 train_time:139573ms step_avg:60.76ms
step:2298/2315 train_time:139634ms step_avg:60.76ms
step:2299/2315 train_time:139695ms step_avg:60.76ms
step:2300/2315 train_time:139757ms step_avg:60.76ms
step:2301/2315 train_time:139819ms step_avg:60.76ms
step:2302/2315 train_time:139880ms step_avg:60.76ms
step:2303/2315 train_time:139942ms step_avg:60.77ms
step:2304/2315 train_time:140003ms step_avg:60.77ms
step:2305/2315 train_time:140065ms step_avg:60.77ms
step:2306/2315 train_time:140126ms step_avg:60.77ms
step:2307/2315 train_time:140187ms step_avg:60.77ms
step:2308/2315 train_time:140247ms step_avg:60.77ms
step:2309/2315 train_time:140309ms step_avg:60.77ms
step:2310/2315 train_time:140370ms step_avg:60.77ms
step:2311/2315 train_time:140430ms step_avg:60.77ms
step:2312/2315 train_time:140491ms step_avg:60.77ms
step:2313/2315 train_time:140552ms step_avg:60.77ms
step:2314/2315 train_time:140613ms step_avg:60.77ms
step:2315/2315 train_time:140675ms step_avg:60.77ms
step:2315/2315 val_loss:3.2786 train_time:140736ms step_avg:60.79ms
peak memory allocated: 29512 MiB reserved: 44076 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]
# polar_express_coeffs = [
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
# ]



@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    # state["second_momentum_buffer"] = torch.zeros_like(grad[...,0:1]) if param.size(-2) >= param.size(-1) else torch.zeros_like(grad[...,0:1,:])
                    # state["second_momentum_buffer"] = torch.zeros((*grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((*grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)
            
            ###################################
            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))
            ####################################


            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> (-1.5)  0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Oct 22 2025, 02:20:45) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251022+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 24 06:23:14 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000001:00:00.0 Off |                    0 |
| N/A   35C    P0            118W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000002:00:00.0 Off |                    0 |
| N/A   35C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000003:00:00.0 Off |                    0 |
| N/A   30C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000008:00:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000009:00:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   0000000A:00:00.0 Off |                    0 |
| N/A   35C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   0000000C:00:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2315 train_time:88ms step_avg:87.85ms
step:2/2315 train_time:182ms step_avg:91.14ms
step:3/2315 train_time:204ms step_avg:68.05ms
step:4/2315 train_time:240ms step_avg:59.99ms
step:5/2315 train_time:298ms step_avg:59.70ms
step:6/2315 train_time:358ms step_avg:59.64ms
step:7/2315 train_time:418ms step_avg:59.69ms
step:8/2315 train_time:477ms step_avg:59.67ms
step:9/2315 train_time:537ms step_avg:59.71ms
step:10/2315 train_time:597ms step_avg:59.69ms
step:11/2315 train_time:657ms step_avg:59.70ms
step:12/2315 train_time:716ms step_avg:59.69ms
step:13/2315 train_time:776ms step_avg:59.73ms
step:14/2315 train_time:836ms step_avg:59.72ms
step:15/2315 train_time:897ms step_avg:59.78ms
step:16/2315 train_time:956ms step_avg:59.77ms
step:17/2315 train_time:1019ms step_avg:59.96ms
step:18/2315 train_time:1082ms step_avg:60.13ms
step:19/2315 train_time:1147ms step_avg:60.36ms
step:20/2315 train_time:1209ms step_avg:60.43ms
step:21/2315 train_time:1269ms step_avg:60.45ms
step:22/2315 train_time:1330ms step_avg:60.44ms
step:23/2315 train_time:1391ms step_avg:60.47ms
step:24/2315 train_time:1451ms step_avg:60.45ms
step:25/2315 train_time:1511ms step_avg:60.43ms
step:26/2315 train_time:1571ms step_avg:60.41ms
step:27/2315 train_time:1631ms step_avg:60.39ms
step:28/2315 train_time:1690ms step_avg:60.37ms
step:29/2315 train_time:1750ms step_avg:60.35ms
step:30/2315 train_time:1810ms step_avg:60.35ms
step:31/2315 train_time:1870ms step_avg:60.34ms
step:32/2315 train_time:1930ms step_avg:60.32ms
step:33/2315 train_time:1991ms step_avg:60.34ms
step:34/2315 train_time:2053ms step_avg:60.37ms
step:35/2315 train_time:2114ms step_avg:60.41ms
step:36/2315 train_time:2176ms step_avg:60.44ms
step:37/2315 train_time:2238ms step_avg:60.48ms
step:38/2315 train_time:2298ms step_avg:60.48ms
step:39/2315 train_time:2359ms step_avg:60.50ms
step:40/2315 train_time:2420ms step_avg:60.49ms
step:41/2315 train_time:2480ms step_avg:60.49ms
step:42/2315 train_time:2541ms step_avg:60.50ms
step:43/2315 train_time:2602ms step_avg:60.51ms
step:44/2315 train_time:2662ms step_avg:60.51ms
step:45/2315 train_time:2723ms step_avg:60.51ms
step:46/2315 train_time:2783ms step_avg:60.49ms
step:47/2315 train_time:2845ms step_avg:60.52ms
step:48/2315 train_time:2905ms step_avg:60.51ms
step:49/2315 train_time:2965ms step_avg:60.52ms
step:50/2315 train_time:3025ms step_avg:60.51ms
step:51/2315 train_time:3086ms step_avg:60.51ms
step:52/2315 train_time:3146ms step_avg:60.50ms
step:53/2315 train_time:3206ms step_avg:60.49ms
step:54/2315 train_time:3266ms step_avg:60.48ms
step:55/2315 train_time:3326ms step_avg:60.47ms
step:56/2315 train_time:3386ms step_avg:60.46ms
step:57/2315 train_time:3446ms step_avg:60.46ms
step:58/2315 train_time:3507ms step_avg:60.46ms
step:59/2315 train_time:3567ms step_avg:60.46ms
step:60/2315 train_time:3627ms step_avg:60.46ms
step:61/2315 train_time:3689ms step_avg:60.47ms
step:62/2315 train_time:3749ms step_avg:60.47ms
step:63/2315 train_time:3810ms step_avg:60.47ms
step:64/2315 train_time:3870ms step_avg:60.46ms
step:65/2315 train_time:3930ms step_avg:60.46ms
step:66/2315 train_time:3990ms step_avg:60.45ms
step:67/2315 train_time:4050ms step_avg:60.45ms
step:68/2315 train_time:4110ms step_avg:60.44ms
step:69/2315 train_time:4170ms step_avg:60.43ms
step:70/2315 train_time:4229ms step_avg:60.42ms
step:71/2315 train_time:4290ms step_avg:60.42ms
step:72/2315 train_time:4349ms step_avg:60.41ms
step:73/2315 train_time:4409ms step_avg:60.40ms
step:74/2315 train_time:4469ms step_avg:60.39ms
step:75/2315 train_time:4530ms step_avg:60.40ms
step:76/2315 train_time:4591ms step_avg:60.41ms
step:77/2315 train_time:4652ms step_avg:60.41ms
step:78/2315 train_time:4712ms step_avg:60.41ms
step:79/2315 train_time:4772ms step_avg:60.40ms
step:80/2315 train_time:4832ms step_avg:60.40ms
step:81/2315 train_time:4893ms step_avg:60.40ms
step:82/2315 train_time:4953ms step_avg:60.40ms
step:83/2315 train_time:5013ms step_avg:60.40ms
step:84/2315 train_time:5073ms step_avg:60.39ms
step:85/2315 train_time:5133ms step_avg:60.39ms
step:86/2315 train_time:5193ms step_avg:60.38ms
step:87/2315 train_time:5253ms step_avg:60.38ms
step:88/2315 train_time:5313ms step_avg:60.37ms
step:89/2315 train_time:5373ms step_avg:60.37ms
step:90/2315 train_time:5433ms step_avg:60.37ms
step:91/2315 train_time:5494ms step_avg:60.38ms
step:92/2315 train_time:5555ms step_avg:60.38ms
step:93/2315 train_time:5616ms step_avg:60.38ms
step:94/2315 train_time:5676ms step_avg:60.38ms
step:95/2315 train_time:5737ms step_avg:60.39ms
step:96/2315 train_time:5797ms step_avg:60.39ms
step:97/2315 train_time:5858ms step_avg:60.39ms
step:98/2315 train_time:5918ms step_avg:60.39ms
step:99/2315 train_time:5978ms step_avg:60.38ms
step:100/2315 train_time:6038ms step_avg:60.38ms
step:101/2315 train_time:6099ms step_avg:60.38ms
step:102/2315 train_time:6159ms step_avg:60.38ms
step:103/2315 train_time:6219ms step_avg:60.38ms
step:104/2315 train_time:6279ms step_avg:60.38ms
step:105/2315 train_time:6340ms step_avg:60.38ms
step:106/2315 train_time:6401ms step_avg:60.38ms
step:107/2315 train_time:6462ms step_avg:60.39ms
step:108/2315 train_time:6521ms step_avg:60.38ms
step:109/2315 train_time:6582ms step_avg:60.38ms
step:110/2315 train_time:6642ms step_avg:60.38ms
step:111/2315 train_time:6703ms step_avg:60.38ms
step:112/2315 train_time:6762ms step_avg:60.38ms
step:113/2315 train_time:6822ms step_avg:60.37ms
step:114/2315 train_time:6882ms step_avg:60.37ms
step:115/2315 train_time:6943ms step_avg:60.38ms
step:116/2315 train_time:7004ms step_avg:60.38ms
step:117/2315 train_time:7064ms step_avg:60.38ms
step:118/2315 train_time:7124ms step_avg:60.37ms
step:119/2315 train_time:7183ms step_avg:60.36ms
step:120/2315 train_time:7243ms step_avg:60.36ms
step:121/2315 train_time:7303ms step_avg:60.36ms
step:122/2315 train_time:7363ms step_avg:60.35ms
step:123/2315 train_time:7424ms step_avg:60.36ms
step:124/2315 train_time:7484ms step_avg:60.36ms
step:125/2315 train_time:7544ms step_avg:60.36ms
step:126/2315 train_time:7604ms step_avg:60.35ms
step:127/2315 train_time:7665ms step_avg:60.35ms
step:128/2315 train_time:7724ms step_avg:60.35ms
step:129/2315 train_time:7784ms step_avg:60.34ms
step:130/2315 train_time:7844ms step_avg:60.34ms
step:131/2315 train_time:7904ms step_avg:60.33ms
step:132/2315 train_time:7963ms step_avg:60.33ms
step:133/2315 train_time:8024ms step_avg:60.33ms
step:134/2315 train_time:8083ms step_avg:60.32ms
step:135/2315 train_time:8143ms step_avg:60.32ms
step:136/2315 train_time:8203ms step_avg:60.31ms
step:137/2315 train_time:8263ms step_avg:60.31ms
step:138/2315 train_time:8323ms step_avg:60.31ms
step:139/2315 train_time:8383ms step_avg:60.31ms
step:140/2315 train_time:8443ms step_avg:60.30ms
step:141/2315 train_time:8503ms step_avg:60.30ms
step:142/2315 train_time:8562ms step_avg:60.30ms
step:143/2315 train_time:8623ms step_avg:60.30ms
step:144/2315 train_time:8684ms step_avg:60.30ms
step:145/2315 train_time:8744ms step_avg:60.30ms
step:146/2315 train_time:8804ms step_avg:60.30ms
step:147/2315 train_time:8864ms step_avg:60.30ms
step:148/2315 train_time:8924ms step_avg:60.29ms
step:149/2315 train_time:8984ms step_avg:60.30ms
step:150/2315 train_time:9044ms step_avg:60.29ms
step:151/2315 train_time:9104ms step_avg:60.29ms
step:152/2315 train_time:9164ms step_avg:60.29ms
step:153/2315 train_time:9224ms step_avg:60.28ms
step:154/2315 train_time:9283ms step_avg:60.28ms
step:155/2315 train_time:9344ms step_avg:60.28ms
step:156/2315 train_time:9404ms step_avg:60.28ms
step:157/2315 train_time:9463ms step_avg:60.28ms
step:158/2315 train_time:9523ms step_avg:60.27ms
step:159/2315 train_time:9583ms step_avg:60.27ms
step:160/2315 train_time:9643ms step_avg:60.27ms
step:161/2315 train_time:9704ms step_avg:60.27ms
step:162/2315 train_time:9764ms step_avg:60.27ms
step:163/2315 train_time:9825ms step_avg:60.27ms
step:164/2315 train_time:9884ms step_avg:60.27ms
step:165/2315 train_time:9944ms step_avg:60.27ms
step:166/2315 train_time:10004ms step_avg:60.26ms
step:167/2315 train_time:10064ms step_avg:60.26ms
step:168/2315 train_time:10124ms step_avg:60.26ms
step:169/2315 train_time:10184ms step_avg:60.26ms
step:170/2315 train_time:10243ms step_avg:60.26ms
step:171/2315 train_time:10303ms step_avg:60.25ms
step:172/2315 train_time:10363ms step_avg:60.25ms
step:173/2315 train_time:10423ms step_avg:60.25ms
step:174/2315 train_time:10483ms step_avg:60.25ms
step:175/2315 train_time:10543ms step_avg:60.24ms
step:176/2315 train_time:10603ms step_avg:60.24ms
step:177/2315 train_time:10663ms step_avg:60.24ms
step:178/2315 train_time:10723ms step_avg:60.24ms
step:179/2315 train_time:10783ms step_avg:60.24ms
step:180/2315 train_time:10843ms step_avg:60.24ms
step:181/2315 train_time:10904ms step_avg:60.24ms
step:182/2315 train_time:10964ms step_avg:60.24ms
step:183/2315 train_time:11024ms step_avg:60.24ms
step:184/2315 train_time:11084ms step_avg:60.24ms
step:185/2315 train_time:11143ms step_avg:60.23ms
step:186/2315 train_time:11204ms step_avg:60.23ms
step:187/2315 train_time:11264ms step_avg:60.23ms
step:188/2315 train_time:11325ms step_avg:60.24ms
step:189/2315 train_time:11385ms step_avg:60.24ms
step:190/2315 train_time:11444ms step_avg:60.23ms
step:191/2315 train_time:11504ms step_avg:60.23ms
step:192/2315 train_time:11564ms step_avg:60.23ms
step:193/2315 train_time:11624ms step_avg:60.23ms
step:194/2315 train_time:11684ms step_avg:60.22ms
step:195/2315 train_time:11744ms step_avg:60.22ms
step:196/2315 train_time:11804ms step_avg:60.22ms
step:197/2315 train_time:11864ms step_avg:60.22ms
step:198/2315 train_time:11924ms step_avg:60.22ms
step:199/2315 train_time:11984ms step_avg:60.22ms
step:200/2315 train_time:12044ms step_avg:60.22ms
step:201/2315 train_time:12103ms step_avg:60.22ms
step:202/2315 train_time:12163ms step_avg:60.21ms
step:203/2315 train_time:12223ms step_avg:60.21ms
step:204/2315 train_time:12282ms step_avg:60.21ms
step:205/2315 train_time:12343ms step_avg:60.21ms
step:206/2315 train_time:12402ms step_avg:60.21ms
step:207/2315 train_time:12462ms step_avg:60.20ms
step:208/2315 train_time:12523ms step_avg:60.20ms
step:209/2315 train_time:12583ms step_avg:60.20ms
step:210/2315 train_time:12643ms step_avg:60.20ms
step:211/2315 train_time:12702ms step_avg:60.20ms
step:212/2315 train_time:12762ms step_avg:60.20ms
step:213/2315 train_time:12822ms step_avg:60.20ms
step:214/2315 train_time:12882ms step_avg:60.20ms
step:215/2315 train_time:12942ms step_avg:60.20ms
step:216/2315 train_time:13002ms step_avg:60.20ms
step:217/2315 train_time:13062ms step_avg:60.20ms
step:218/2315 train_time:13122ms step_avg:60.19ms
step:219/2315 train_time:13183ms step_avg:60.19ms
step:220/2315 train_time:13242ms step_avg:60.19ms
step:221/2315 train_time:13302ms step_avg:60.19ms
step:222/2315 train_time:13362ms step_avg:60.19ms
step:223/2315 train_time:13422ms step_avg:60.19ms
step:224/2315 train_time:13482ms step_avg:60.19ms
step:225/2315 train_time:13542ms step_avg:60.19ms
step:226/2315 train_time:13602ms step_avg:60.18ms
step:227/2315 train_time:13662ms step_avg:60.19ms
step:228/2315 train_time:13722ms step_avg:60.18ms
step:229/2315 train_time:13782ms step_avg:60.18ms
step:230/2315 train_time:13843ms step_avg:60.19ms
step:231/2315 train_time:13903ms step_avg:60.18ms
step:232/2315 train_time:13963ms step_avg:60.19ms
step:233/2315 train_time:14023ms step_avg:60.18ms
step:234/2315 train_time:14083ms step_avg:60.18ms
step:235/2315 train_time:14143ms step_avg:60.18ms
step:236/2315 train_time:14203ms step_avg:60.18ms
step:237/2315 train_time:14263ms step_avg:60.18ms
step:238/2315 train_time:14323ms step_avg:60.18ms
step:239/2315 train_time:14383ms step_avg:60.18ms
step:240/2315 train_time:14444ms step_avg:60.18ms
step:241/2315 train_time:14504ms step_avg:60.18ms
step:242/2315 train_time:14563ms step_avg:60.18ms
step:243/2315 train_time:14623ms step_avg:60.18ms
step:244/2315 train_time:14682ms step_avg:60.17ms
step:245/2315 train_time:14742ms step_avg:60.17ms
step:246/2315 train_time:14802ms step_avg:60.17ms
step:247/2315 train_time:14862ms step_avg:60.17ms
step:248/2315 train_time:14922ms step_avg:60.17ms
step:249/2315 train_time:14982ms step_avg:60.17ms
step:250/2315 train_time:15043ms step_avg:60.17ms
step:250/2315 val_loss:4.0656 train_time:15105ms step_avg:60.42ms
step:251/2315 train_time:15124ms step_avg:60.25ms
step:252/2315 train_time:15165ms step_avg:60.18ms
step:253/2315 train_time:15226ms step_avg:60.18ms
step:254/2315 train_time:15289ms step_avg:60.19ms
step:255/2315 train_time:15351ms step_avg:60.20ms
step:256/2315 train_time:15412ms step_avg:60.20ms
step:257/2315 train_time:15473ms step_avg:60.21ms
step:258/2315 train_time:15532ms step_avg:60.20ms
step:259/2315 train_time:15592ms step_avg:60.20ms
step:260/2315 train_time:15651ms step_avg:60.20ms
step:261/2315 train_time:15710ms step_avg:60.19ms
step:262/2315 train_time:15769ms step_avg:60.19ms
step:263/2315 train_time:15828ms step_avg:60.18ms
step:264/2315 train_time:15887ms step_avg:60.18ms
step:265/2315 train_time:15946ms step_avg:60.17ms
step:266/2315 train_time:16005ms step_avg:60.17ms
step:267/2315 train_time:16065ms step_avg:60.17ms
step:268/2315 train_time:16125ms step_avg:60.17ms
step:269/2315 train_time:16185ms step_avg:60.17ms
step:270/2315 train_time:16247ms step_avg:60.17ms
step:271/2315 train_time:16308ms step_avg:60.18ms
step:272/2315 train_time:16368ms step_avg:60.18ms
step:273/2315 train_time:16429ms step_avg:60.18ms
step:274/2315 train_time:16489ms step_avg:60.18ms
step:275/2315 train_time:16550ms step_avg:60.18ms
step:276/2315 train_time:16609ms step_avg:60.18ms
step:277/2315 train_time:16668ms step_avg:60.17ms
step:278/2315 train_time:16727ms step_avg:60.17ms
step:279/2315 train_time:16787ms step_avg:60.17ms
step:280/2315 train_time:16846ms step_avg:60.16ms
step:281/2315 train_time:16905ms step_avg:60.16ms
step:282/2315 train_time:16964ms step_avg:60.16ms
step:283/2315 train_time:17023ms step_avg:60.15ms
step:284/2315 train_time:17082ms step_avg:60.15ms
step:285/2315 train_time:17142ms step_avg:60.15ms
step:286/2315 train_time:17202ms step_avg:60.15ms
step:287/2315 train_time:17262ms step_avg:60.15ms
step:288/2315 train_time:17323ms step_avg:60.15ms
step:289/2315 train_time:17383ms step_avg:60.15ms
step:290/2315 train_time:17443ms step_avg:60.15ms
step:291/2315 train_time:17504ms step_avg:60.15ms
step:292/2315 train_time:17564ms step_avg:60.15ms
step:293/2315 train_time:17624ms step_avg:60.15ms
step:294/2315 train_time:17684ms step_avg:60.15ms
step:295/2315 train_time:17744ms step_avg:60.15ms
step:296/2315 train_time:17804ms step_avg:60.15ms
step:297/2315 train_time:17864ms step_avg:60.15ms
step:298/2315 train_time:17923ms step_avg:60.14ms
step:299/2315 train_time:17982ms step_avg:60.14ms
step:300/2315 train_time:18043ms step_avg:60.14ms
step:301/2315 train_time:18102ms step_avg:60.14ms
step:302/2315 train_time:18162ms step_avg:60.14ms
step:303/2315 train_time:18222ms step_avg:60.14ms
step:304/2315 train_time:18282ms step_avg:60.14ms
step:305/2315 train_time:18342ms step_avg:60.14ms
step:306/2315 train_time:18403ms step_avg:60.14ms
step:307/2315 train_time:18463ms step_avg:60.14ms
step:308/2315 train_time:18523ms step_avg:60.14ms
step:309/2315 train_time:18583ms step_avg:60.14ms
step:310/2315 train_time:18643ms step_avg:60.14ms
step:311/2315 train_time:18704ms step_avg:60.14ms
step:312/2315 train_time:18763ms step_avg:60.14ms
step:313/2315 train_time:18823ms step_avg:60.14ms
step:314/2315 train_time:18883ms step_avg:60.14ms
step:315/2315 train_time:18942ms step_avg:60.13ms
step:316/2315 train_time:19001ms step_avg:60.13ms
step:317/2315 train_time:19061ms step_avg:60.13ms
step:318/2315 train_time:19122ms step_avg:60.13ms
step:319/2315 train_time:19183ms step_avg:60.13ms
step:320/2315 train_time:19243ms step_avg:60.13ms
step:321/2315 train_time:19303ms step_avg:60.13ms
step:322/2315 train_time:19363ms step_avg:60.13ms
step:323/2315 train_time:19423ms step_avg:60.13ms
step:324/2315 train_time:19483ms step_avg:60.13ms
step:325/2315 train_time:19544ms step_avg:60.13ms
step:326/2315 train_time:19604ms step_avg:60.14ms
step:327/2315 train_time:19664ms step_avg:60.13ms
step:328/2315 train_time:19724ms step_avg:60.13ms
step:329/2315 train_time:19783ms step_avg:60.13ms
step:330/2315 train_time:19843ms step_avg:60.13ms
step:331/2315 train_time:19903ms step_avg:60.13ms
step:332/2315 train_time:19963ms step_avg:60.13ms
step:333/2315 train_time:20023ms step_avg:60.13ms
step:334/2315 train_time:20082ms step_avg:60.13ms
step:335/2315 train_time:20143ms step_avg:60.13ms
step:336/2315 train_time:20202ms step_avg:60.13ms
step:337/2315 train_time:20263ms step_avg:60.13ms
step:338/2315 train_time:20322ms step_avg:60.13ms
step:339/2315 train_time:20383ms step_avg:60.13ms
step:340/2315 train_time:20443ms step_avg:60.13ms
step:341/2315 train_time:20503ms step_avg:60.13ms
step:342/2315 train_time:20564ms step_avg:60.13ms
step:343/2315 train_time:20624ms step_avg:60.13ms
step:344/2315 train_time:20684ms step_avg:60.13ms
step:345/2315 train_time:20744ms step_avg:60.13ms
step:346/2315 train_time:20803ms step_avg:60.12ms
step:347/2315 train_time:20863ms step_avg:60.12ms
step:348/2315 train_time:20922ms step_avg:60.12ms
step:349/2315 train_time:20982ms step_avg:60.12ms
step:350/2315 train_time:21041ms step_avg:60.12ms
step:351/2315 train_time:21101ms step_avg:60.12ms
step:352/2315 train_time:21161ms step_avg:60.12ms
step:353/2315 train_time:21221ms step_avg:60.12ms
step:354/2315 train_time:21281ms step_avg:60.12ms
step:355/2315 train_time:21342ms step_avg:60.12ms
step:356/2315 train_time:21401ms step_avg:60.12ms
step:357/2315 train_time:21461ms step_avg:60.12ms
step:358/2315 train_time:21522ms step_avg:60.12ms
step:359/2315 train_time:21582ms step_avg:60.12ms
step:360/2315 train_time:21642ms step_avg:60.12ms
step:361/2315 train_time:21703ms step_avg:60.12ms
step:362/2315 train_time:21763ms step_avg:60.12ms
step:363/2315 train_time:21823ms step_avg:60.12ms
step:364/2315 train_time:21883ms step_avg:60.12ms
step:365/2315 train_time:21943ms step_avg:60.12ms
step:366/2315 train_time:22002ms step_avg:60.11ms
step:367/2315 train_time:22062ms step_avg:60.11ms
step:368/2315 train_time:22121ms step_avg:60.11ms
step:369/2315 train_time:22182ms step_avg:60.11ms
step:370/2315 train_time:22241ms step_avg:60.11ms
step:371/2315 train_time:22302ms step_avg:60.11ms
step:372/2315 train_time:22362ms step_avg:60.11ms
step:373/2315 train_time:22422ms step_avg:60.11ms
step:374/2315 train_time:22482ms step_avg:60.11ms
step:375/2315 train_time:22542ms step_avg:60.11ms
step:376/2315 train_time:22602ms step_avg:60.11ms
step:377/2315 train_time:22663ms step_avg:60.11ms
step:378/2315 train_time:22723ms step_avg:60.11ms
step:379/2315 train_time:22784ms step_avg:60.12ms
step:380/2315 train_time:22844ms step_avg:60.12ms
step:381/2315 train_time:22904ms step_avg:60.12ms
step:382/2315 train_time:22964ms step_avg:60.12ms
step:383/2315 train_time:23024ms step_avg:60.12ms
step:384/2315 train_time:23084ms step_avg:60.11ms
step:385/2315 train_time:23143ms step_avg:60.11ms
step:386/2315 train_time:23203ms step_avg:60.11ms
step:387/2315 train_time:23263ms step_avg:60.11ms
step:388/2315 train_time:23322ms step_avg:60.11ms
step:389/2315 train_time:23382ms step_avg:60.11ms
step:390/2315 train_time:23441ms step_avg:60.11ms
step:391/2315 train_time:23501ms step_avg:60.11ms
step:392/2315 train_time:23561ms step_avg:60.10ms
step:393/2315 train_time:23621ms step_avg:60.11ms
step:394/2315 train_time:23682ms step_avg:60.11ms
step:395/2315 train_time:23742ms step_avg:60.11ms
step:396/2315 train_time:23802ms step_avg:60.11ms
step:397/2315 train_time:23863ms step_avg:60.11ms
step:398/2315 train_time:23923ms step_avg:60.11ms
step:399/2315 train_time:23984ms step_avg:60.11ms
step:400/2315 train_time:24043ms step_avg:60.11ms
step:401/2315 train_time:24103ms step_avg:60.11ms
step:402/2315 train_time:24163ms step_avg:60.11ms
step:403/2315 train_time:24223ms step_avg:60.11ms
step:404/2315 train_time:24283ms step_avg:60.11ms
step:405/2315 train_time:24343ms step_avg:60.11ms
step:406/2315 train_time:24402ms step_avg:60.10ms
step:407/2315 train_time:24462ms step_avg:60.10ms
step:408/2315 train_time:24523ms step_avg:60.11ms
step:409/2315 train_time:24583ms step_avg:60.11ms
step:410/2315 train_time:24644ms step_avg:60.11ms
step:411/2315 train_time:24704ms step_avg:60.11ms
step:412/2315 train_time:24763ms step_avg:60.10ms
step:413/2315 train_time:24824ms step_avg:60.11ms
step:414/2315 train_time:24883ms step_avg:60.10ms
step:415/2315 train_time:24943ms step_avg:60.10ms
step:416/2315 train_time:25002ms step_avg:60.10ms
step:417/2315 train_time:25062ms step_avg:60.10ms
step:418/2315 train_time:25122ms step_avg:60.10ms
step:419/2315 train_time:25182ms step_avg:60.10ms
step:420/2315 train_time:25242ms step_avg:60.10ms
step:421/2315 train_time:25302ms step_avg:60.10ms
step:422/2315 train_time:25362ms step_avg:60.10ms
step:423/2315 train_time:25422ms step_avg:60.10ms
step:424/2315 train_time:25483ms step_avg:60.10ms
step:425/2315 train_time:25544ms step_avg:60.10ms
step:426/2315 train_time:25603ms step_avg:60.10ms
step:427/2315 train_time:25663ms step_avg:60.10ms
step:428/2315 train_time:25723ms step_avg:60.10ms
step:429/2315 train_time:25783ms step_avg:60.10ms
step:430/2315 train_time:25844ms step_avg:60.10ms
step:431/2315 train_time:25903ms step_avg:60.10ms
step:432/2315 train_time:25963ms step_avg:60.10ms
step:433/2315 train_time:26023ms step_avg:60.10ms
step:434/2315 train_time:26083ms step_avg:60.10ms
step:435/2315 train_time:26143ms step_avg:60.10ms
step:436/2315 train_time:26203ms step_avg:60.10ms
step:437/2315 train_time:26263ms step_avg:60.10ms
step:438/2315 train_time:26323ms step_avg:60.10ms
step:439/2315 train_time:26383ms step_avg:60.10ms
step:440/2315 train_time:26443ms step_avg:60.10ms
step:441/2315 train_time:26503ms step_avg:60.10ms
step:442/2315 train_time:26562ms step_avg:60.10ms
step:443/2315 train_time:26623ms step_avg:60.10ms
step:444/2315 train_time:26682ms step_avg:60.10ms
step:445/2315 train_time:26743ms step_avg:60.10ms
step:446/2315 train_time:26803ms step_avg:60.10ms
step:447/2315 train_time:26863ms step_avg:60.10ms
step:448/2315 train_time:26923ms step_avg:60.10ms
step:449/2315 train_time:26983ms step_avg:60.10ms
step:450/2315 train_time:27042ms step_avg:60.09ms
step:451/2315 train_time:27103ms step_avg:60.09ms
step:452/2315 train_time:27162ms step_avg:60.09ms
step:453/2315 train_time:27222ms step_avg:60.09ms
step:454/2315 train_time:27282ms step_avg:60.09ms
step:455/2315 train_time:27341ms step_avg:60.09ms
step:456/2315 train_time:27401ms step_avg:60.09ms
step:457/2315 train_time:27462ms step_avg:60.09ms
step:458/2315 train_time:27522ms step_avg:60.09ms
step:459/2315 train_time:27581ms step_avg:60.09ms
step:460/2315 train_time:27641ms step_avg:60.09ms
step:461/2315 train_time:27702ms step_avg:60.09ms
step:462/2315 train_time:27762ms step_avg:60.09ms
step:463/2315 train_time:27823ms step_avg:60.09ms
step:464/2315 train_time:27882ms step_avg:60.09ms
step:465/2315 train_time:27942ms step_avg:60.09ms
step:466/2315 train_time:28002ms step_avg:60.09ms
step:467/2315 train_time:28062ms step_avg:60.09ms
step:468/2315 train_time:28122ms step_avg:60.09ms
step:469/2315 train_time:28182ms step_avg:60.09ms
step:470/2315 train_time:28241ms step_avg:60.09ms
step:471/2315 train_time:28302ms step_avg:60.09ms
step:472/2315 train_time:28361ms step_avg:60.09ms
step:473/2315 train_time:28421ms step_avg:60.09ms
step:474/2315 train_time:28481ms step_avg:60.09ms
step:475/2315 train_time:28542ms step_avg:60.09ms
step:476/2315 train_time:28602ms step_avg:60.09ms
step:477/2315 train_time:28662ms step_avg:60.09ms
step:478/2315 train_time:28722ms step_avg:60.09ms
step:479/2315 train_time:28783ms step_avg:60.09ms
step:480/2315 train_time:28842ms step_avg:60.09ms
step:481/2315 train_time:28902ms step_avg:60.09ms
step:482/2315 train_time:28962ms step_avg:60.09ms
step:483/2315 train_time:29022ms step_avg:60.09ms
step:484/2315 train_time:29082ms step_avg:60.09ms
step:485/2315 train_time:29142ms step_avg:60.09ms
step:486/2315 train_time:29202ms step_avg:60.09ms
step:487/2315 train_time:29262ms step_avg:60.09ms
step:488/2315 train_time:29322ms step_avg:60.09ms
step:489/2315 train_time:29383ms step_avg:60.09ms
step:490/2315 train_time:29442ms step_avg:60.09ms
step:491/2315 train_time:29502ms step_avg:60.09ms
step:492/2315 train_time:29562ms step_avg:60.09ms
step:493/2315 train_time:29622ms step_avg:60.09ms
step:494/2315 train_time:29682ms step_avg:60.08ms
step:495/2315 train_time:29743ms step_avg:60.09ms
step:496/2315 train_time:29803ms step_avg:60.09ms
step:497/2315 train_time:29863ms step_avg:60.09ms
step:498/2315 train_time:29922ms step_avg:60.09ms
step:499/2315 train_time:29983ms step_avg:60.09ms
step:500/2315 train_time:30043ms step_avg:60.09ms
step:500/2315 val_loss:3.8211 train_time:30105ms step_avg:60.21ms
step:501/2315 train_time:30124ms step_avg:60.13ms
step:502/2315 train_time:30166ms step_avg:60.09ms
step:503/2315 train_time:30230ms step_avg:60.10ms
step:504/2315 train_time:30292ms step_avg:60.10ms
step:505/2315 train_time:30353ms step_avg:60.10ms
step:506/2315 train_time:30412ms step_avg:60.10ms
step:507/2315 train_time:30472ms step_avg:60.10ms
step:508/2315 train_time:30531ms step_avg:60.10ms
step:509/2315 train_time:30591ms step_avg:60.10ms
step:510/2315 train_time:30650ms step_avg:60.10ms
step:511/2315 train_time:30709ms step_avg:60.10ms
step:512/2315 train_time:30769ms step_avg:60.10ms
step:513/2315 train_time:30828ms step_avg:60.09ms
step:514/2315 train_time:30887ms step_avg:60.09ms
step:515/2315 train_time:30945ms step_avg:60.09ms
step:516/2315 train_time:31005ms step_avg:60.09ms
step:517/2315 train_time:31065ms step_avg:60.09ms
step:518/2315 train_time:31126ms step_avg:60.09ms
step:519/2315 train_time:31187ms step_avg:60.09ms
step:520/2315 train_time:31248ms step_avg:60.09ms
step:521/2315 train_time:31308ms step_avg:60.09ms
step:522/2315 train_time:31369ms step_avg:60.09ms
step:523/2315 train_time:31429ms step_avg:60.09ms
step:524/2315 train_time:31489ms step_avg:60.09ms
step:525/2315 train_time:31549ms step_avg:60.09ms
step:526/2315 train_time:31608ms step_avg:60.09ms
step:527/2315 train_time:31668ms step_avg:60.09ms
step:528/2315 train_time:31727ms step_avg:60.09ms
step:529/2315 train_time:31786ms step_avg:60.09ms
step:530/2315 train_time:31846ms step_avg:60.09ms
step:531/2315 train_time:31905ms step_avg:60.08ms
step:532/2315 train_time:31964ms step_avg:60.08ms
step:533/2315 train_time:32024ms step_avg:60.08ms
step:534/2315 train_time:32083ms step_avg:60.08ms
step:535/2315 train_time:32144ms step_avg:60.08ms
step:536/2315 train_time:32204ms step_avg:60.08ms
step:537/2315 train_time:32265ms step_avg:60.08ms
step:538/2315 train_time:32325ms step_avg:60.08ms
step:539/2315 train_time:32386ms step_avg:60.08ms
step:540/2315 train_time:32446ms step_avg:60.08ms
step:541/2315 train_time:32506ms step_avg:60.08ms
step:542/2315 train_time:32565ms step_avg:60.08ms
step:543/2315 train_time:32625ms step_avg:60.08ms
step:544/2315 train_time:32685ms step_avg:60.08ms
step:545/2315 train_time:32744ms step_avg:60.08ms
step:546/2315 train_time:32805ms step_avg:60.08ms
step:547/2315 train_time:32864ms step_avg:60.08ms
step:548/2315 train_time:32924ms step_avg:60.08ms
step:549/2315 train_time:32983ms step_avg:60.08ms
step:550/2315 train_time:33042ms step_avg:60.08ms
step:551/2315 train_time:33103ms step_avg:60.08ms
step:552/2315 train_time:33163ms step_avg:60.08ms
step:553/2315 train_time:33223ms step_avg:60.08ms
step:554/2315 train_time:33283ms step_avg:60.08ms
step:555/2315 train_time:33344ms step_avg:60.08ms
step:556/2315 train_time:33405ms step_avg:60.08ms
step:557/2315 train_time:33465ms step_avg:60.08ms
step:558/2315 train_time:33525ms step_avg:60.08ms
step:559/2315 train_time:33586ms step_avg:60.08ms
step:560/2315 train_time:33645ms step_avg:60.08ms
step:561/2315 train_time:33705ms step_avg:60.08ms
step:562/2315 train_time:33765ms step_avg:60.08ms
step:563/2315 train_time:33825ms step_avg:60.08ms
step:564/2315 train_time:33884ms step_avg:60.08ms
step:565/2315 train_time:33944ms step_avg:60.08ms
step:566/2315 train_time:34003ms step_avg:60.08ms
step:567/2315 train_time:34063ms step_avg:60.08ms
step:568/2315 train_time:34123ms step_avg:60.07ms
step:569/2315 train_time:34183ms step_avg:60.08ms
step:570/2315 train_time:34243ms step_avg:60.08ms
step:571/2315 train_time:34304ms step_avg:60.08ms
step:572/2315 train_time:34364ms step_avg:60.08ms
step:573/2315 train_time:34425ms step_avg:60.08ms
step:574/2315 train_time:34485ms step_avg:60.08ms
step:575/2315 train_time:34545ms step_avg:60.08ms
step:576/2315 train_time:34606ms step_avg:60.08ms
step:577/2315 train_time:34666ms step_avg:60.08ms
step:578/2315 train_time:34726ms step_avg:60.08ms
step:579/2315 train_time:34785ms step_avg:60.08ms
step:580/2315 train_time:34845ms step_avg:60.08ms
step:581/2315 train_time:34904ms step_avg:60.08ms
step:582/2315 train_time:34964ms step_avg:60.08ms
step:583/2315 train_time:35023ms step_avg:60.07ms
step:584/2315 train_time:35083ms step_avg:60.07ms
step:585/2315 train_time:35143ms step_avg:60.07ms
step:586/2315 train_time:35203ms step_avg:60.07ms
step:587/2315 train_time:35263ms step_avg:60.07ms
step:588/2315 train_time:35323ms step_avg:60.07ms
step:589/2315 train_time:35384ms step_avg:60.08ms
step:590/2315 train_time:35444ms step_avg:60.08ms
step:591/2315 train_time:35504ms step_avg:60.07ms
step:592/2315 train_time:35564ms step_avg:60.07ms
step:593/2315 train_time:35624ms step_avg:60.07ms
step:594/2315 train_time:35684ms step_avg:60.07ms
step:595/2315 train_time:35744ms step_avg:60.07ms
step:596/2315 train_time:35804ms step_avg:60.07ms
step:597/2315 train_time:35864ms step_avg:60.07ms
step:598/2315 train_time:35924ms step_avg:60.07ms
step:599/2315 train_time:35984ms step_avg:60.07ms
step:600/2315 train_time:36044ms step_avg:60.07ms
step:601/2315 train_time:36104ms step_avg:60.07ms
step:602/2315 train_time:36164ms step_avg:60.07ms
step:603/2315 train_time:36224ms step_avg:60.07ms
step:604/2315 train_time:36284ms step_avg:60.07ms
step:605/2315 train_time:36344ms step_avg:60.07ms
step:606/2315 train_time:36404ms step_avg:60.07ms
step:607/2315 train_time:36465ms step_avg:60.07ms
step:608/2315 train_time:36525ms step_avg:60.07ms
step:609/2315 train_time:36585ms step_avg:60.07ms
step:610/2315 train_time:36645ms step_avg:60.07ms
step:611/2315 train_time:36705ms step_avg:60.07ms
step:612/2315 train_time:36765ms step_avg:60.07ms
step:613/2315 train_time:36825ms step_avg:60.07ms
step:614/2315 train_time:36884ms step_avg:60.07ms
step:615/2315 train_time:36944ms step_avg:60.07ms
step:616/2315 train_time:37004ms step_avg:60.07ms
step:617/2315 train_time:37063ms step_avg:60.07ms
step:618/2315 train_time:37124ms step_avg:60.07ms
step:619/2315 train_time:37183ms step_avg:60.07ms
step:620/2315 train_time:37243ms step_avg:60.07ms
step:621/2315 train_time:37304ms step_avg:60.07ms
step:622/2315 train_time:37364ms step_avg:60.07ms
step:623/2315 train_time:37424ms step_avg:60.07ms
step:624/2315 train_time:37484ms step_avg:60.07ms
step:625/2315 train_time:37545ms step_avg:60.07ms
step:626/2315 train_time:37605ms step_avg:60.07ms
step:627/2315 train_time:37665ms step_avg:60.07ms
step:628/2315 train_time:37725ms step_avg:60.07ms
step:629/2315 train_time:37785ms step_avg:60.07ms
step:630/2315 train_time:37845ms step_avg:60.07ms
step:631/2315 train_time:37904ms step_avg:60.07ms
step:632/2315 train_time:37964ms step_avg:60.07ms
step:633/2315 train_time:38024ms step_avg:60.07ms
step:634/2315 train_time:38084ms step_avg:60.07ms
step:635/2315 train_time:38143ms step_avg:60.07ms
step:636/2315 train_time:38203ms step_avg:60.07ms
step:637/2315 train_time:38263ms step_avg:60.07ms
step:638/2315 train_time:38323ms step_avg:60.07ms
step:639/2315 train_time:38383ms step_avg:60.07ms
step:640/2315 train_time:38443ms step_avg:60.07ms
step:641/2315 train_time:38503ms step_avg:60.07ms
step:642/2315 train_time:38563ms step_avg:60.07ms
step:643/2315 train_time:38623ms step_avg:60.07ms
step:644/2315 train_time:38683ms step_avg:60.07ms
step:645/2315 train_time:38743ms step_avg:60.07ms
step:646/2315 train_time:38803ms step_avg:60.07ms
step:647/2315 train_time:38863ms step_avg:60.07ms
step:648/2315 train_time:38924ms step_avg:60.07ms
step:649/2315 train_time:38984ms step_avg:60.07ms
step:650/2315 train_time:39043ms step_avg:60.07ms
step:651/2315 train_time:39104ms step_avg:60.07ms
step:652/2315 train_time:39165ms step_avg:60.07ms
step:653/2315 train_time:39224ms step_avg:60.07ms
step:654/2315 train_time:39284ms step_avg:60.07ms
step:655/2315 train_time:39344ms step_avg:60.07ms
step:656/2315 train_time:39405ms step_avg:60.07ms
step:657/2315 train_time:39465ms step_avg:60.07ms
step:658/2315 train_time:39525ms step_avg:60.07ms
step:659/2315 train_time:39585ms step_avg:60.07ms
step:660/2315 train_time:39644ms step_avg:60.07ms
step:661/2315 train_time:39704ms step_avg:60.07ms
step:662/2315 train_time:39764ms step_avg:60.07ms
step:663/2315 train_time:39824ms step_avg:60.07ms
step:664/2315 train_time:39884ms step_avg:60.07ms
step:665/2315 train_time:39944ms step_avg:60.07ms
step:666/2315 train_time:40003ms step_avg:60.06ms
step:667/2315 train_time:40064ms step_avg:60.07ms
step:668/2315 train_time:40124ms step_avg:60.07ms
step:669/2315 train_time:40184ms step_avg:60.07ms
step:670/2315 train_time:40243ms step_avg:60.06ms
step:671/2315 train_time:40303ms step_avg:60.06ms
step:672/2315 train_time:40363ms step_avg:60.06ms
step:673/2315 train_time:40423ms step_avg:60.06ms
step:674/2315 train_time:40484ms step_avg:60.06ms
step:675/2315 train_time:40545ms step_avg:60.07ms
step:676/2315 train_time:40605ms step_avg:60.07ms
step:677/2315 train_time:40665ms step_avg:60.07ms
step:678/2315 train_time:40724ms step_avg:60.07ms
step:679/2315 train_time:40784ms step_avg:60.07ms
step:680/2315 train_time:40844ms step_avg:60.06ms
step:681/2315 train_time:40904ms step_avg:60.06ms
step:682/2315 train_time:40963ms step_avg:60.06ms
step:683/2315 train_time:41023ms step_avg:60.06ms
step:684/2315 train_time:41083ms step_avg:60.06ms
step:685/2315 train_time:41143ms step_avg:60.06ms
step:686/2315 train_time:41203ms step_avg:60.06ms
step:687/2315 train_time:41263ms step_avg:60.06ms
step:688/2315 train_time:41323ms step_avg:60.06ms
step:689/2315 train_time:41384ms step_avg:60.06ms
step:690/2315 train_time:41444ms step_avg:60.06ms
step:691/2315 train_time:41504ms step_avg:60.06ms
step:692/2315 train_time:41564ms step_avg:60.06ms
step:693/2315 train_time:41624ms step_avg:60.06ms
step:694/2315 train_time:41684ms step_avg:60.06ms
step:695/2315 train_time:41744ms step_avg:60.06ms
step:696/2315 train_time:41803ms step_avg:60.06ms
step:697/2315 train_time:41863ms step_avg:60.06ms
step:698/2315 train_time:41923ms step_avg:60.06ms
step:699/2315 train_time:41982ms step_avg:60.06ms
step:700/2315 train_time:42042ms step_avg:60.06ms
step:701/2315 train_time:42103ms step_avg:60.06ms
step:702/2315 train_time:42163ms step_avg:60.06ms
step:703/2315 train_time:42223ms step_avg:60.06ms
step:704/2315 train_time:42283ms step_avg:60.06ms
step:705/2315 train_time:42343ms step_avg:60.06ms
step:706/2315 train_time:42403ms step_avg:60.06ms
step:707/2315 train_time:42463ms step_avg:60.06ms
step:708/2315 train_time:42523ms step_avg:60.06ms
step:709/2315 train_time:42584ms step_avg:60.06ms
step:710/2315 train_time:42643ms step_avg:60.06ms
step:711/2315 train_time:42704ms step_avg:60.06ms
step:712/2315 train_time:42764ms step_avg:60.06ms
step:713/2315 train_time:42825ms step_avg:60.06ms
step:714/2315 train_time:42885ms step_avg:60.06ms
step:715/2315 train_time:42944ms step_avg:60.06ms
step:716/2315 train_time:43004ms step_avg:60.06ms
step:717/2315 train_time:43064ms step_avg:60.06ms
step:718/2315 train_time:43124ms step_avg:60.06ms
step:719/2315 train_time:43185ms step_avg:60.06ms
step:720/2315 train_time:43244ms step_avg:60.06ms
step:721/2315 train_time:43304ms step_avg:60.06ms
step:722/2315 train_time:43364ms step_avg:60.06ms
step:723/2315 train_time:43424ms step_avg:60.06ms
step:724/2315 train_time:43485ms step_avg:60.06ms
step:725/2315 train_time:43545ms step_avg:60.06ms
step:726/2315 train_time:43605ms step_avg:60.06ms
step:727/2315 train_time:43664ms step_avg:60.06ms
step:728/2315 train_time:43724ms step_avg:60.06ms
step:729/2315 train_time:43784ms step_avg:60.06ms
step:730/2315 train_time:43844ms step_avg:60.06ms
step:731/2315 train_time:43904ms step_avg:60.06ms
step:732/2315 train_time:43965ms step_avg:60.06ms
step:733/2315 train_time:44025ms step_avg:60.06ms
step:734/2315 train_time:44085ms step_avg:60.06ms
step:735/2315 train_time:44144ms step_avg:60.06ms
step:736/2315 train_time:44204ms step_avg:60.06ms
step:737/2315 train_time:44264ms step_avg:60.06ms
step:738/2315 train_time:44324ms step_avg:60.06ms
step:739/2315 train_time:44384ms step_avg:60.06ms
step:740/2315 train_time:44444ms step_avg:60.06ms
step:741/2315 train_time:44504ms step_avg:60.06ms
step:742/2315 train_time:44564ms step_avg:60.06ms
step:743/2315 train_time:44624ms step_avg:60.06ms
step:744/2315 train_time:44684ms step_avg:60.06ms
step:745/2315 train_time:44744ms step_avg:60.06ms
step:746/2315 train_time:44804ms step_avg:60.06ms
step:747/2315 train_time:44864ms step_avg:60.06ms
step:748/2315 train_time:44924ms step_avg:60.06ms
step:749/2315 train_time:44984ms step_avg:60.06ms
step:750/2315 train_time:45044ms step_avg:60.06ms
step:750/2315 val_loss:3.6786 train_time:45106ms step_avg:60.14ms
step:751/2315 train_time:45125ms step_avg:60.09ms
step:752/2315 train_time:45166ms step_avg:60.06ms
step:753/2315 train_time:45229ms step_avg:60.06ms
step:754/2315 train_time:45291ms step_avg:60.07ms
step:755/2315 train_time:45352ms step_avg:60.07ms
step:756/2315 train_time:45412ms step_avg:60.07ms
step:757/2315 train_time:45471ms step_avg:60.07ms
step:758/2315 train_time:45530ms step_avg:60.07ms
step:759/2315 train_time:45590ms step_avg:60.07ms
step:760/2315 train_time:45649ms step_avg:60.06ms
step:761/2315 train_time:45710ms step_avg:60.07ms
step:762/2315 train_time:45770ms step_avg:60.07ms
step:763/2315 train_time:45830ms step_avg:60.07ms
step:764/2315 train_time:45891ms step_avg:60.07ms
step:765/2315 train_time:45951ms step_avg:60.07ms
step:766/2315 train_time:46012ms step_avg:60.07ms
step:767/2315 train_time:46074ms step_avg:60.07ms
step:768/2315 train_time:46136ms step_avg:60.07ms
step:769/2315 train_time:46197ms step_avg:60.07ms
step:770/2315 train_time:46258ms step_avg:60.08ms
step:771/2315 train_time:46319ms step_avg:60.08ms
step:772/2315 train_time:46380ms step_avg:60.08ms
step:773/2315 train_time:46442ms step_avg:60.08ms
step:774/2315 train_time:46502ms step_avg:60.08ms
step:775/2315 train_time:46563ms step_avg:60.08ms
step:776/2315 train_time:46624ms step_avg:60.08ms
step:777/2315 train_time:46684ms step_avg:60.08ms
step:778/2315 train_time:46744ms step_avg:60.08ms
step:779/2315 train_time:46805ms step_avg:60.08ms
step:780/2315 train_time:46865ms step_avg:60.08ms
step:781/2315 train_time:46926ms step_avg:60.08ms
step:782/2315 train_time:46986ms step_avg:60.08ms
step:783/2315 train_time:47047ms step_avg:60.09ms
step:784/2315 train_time:47107ms step_avg:60.09ms
step:785/2315 train_time:47168ms step_avg:60.09ms
step:786/2315 train_time:47229ms step_avg:60.09ms
step:787/2315 train_time:47289ms step_avg:60.09ms
step:788/2315 train_time:47350ms step_avg:60.09ms
step:789/2315 train_time:47410ms step_avg:60.09ms
step:790/2315 train_time:47471ms step_avg:60.09ms
step:791/2315 train_time:47532ms step_avg:60.09ms
step:792/2315 train_time:47593ms step_avg:60.09ms
step:793/2315 train_time:47653ms step_avg:60.09ms
step:794/2315 train_time:47714ms step_avg:60.09ms
step:795/2315 train_time:47775ms step_avg:60.09ms
step:796/2315 train_time:47835ms step_avg:60.09ms
step:797/2315 train_time:47896ms step_avg:60.10ms
step:798/2315 train_time:47957ms step_avg:60.10ms
step:799/2315 train_time:48018ms step_avg:60.10ms
step:800/2315 train_time:48079ms step_avg:60.10ms
step:801/2315 train_time:48140ms step_avg:60.10ms
step:802/2315 train_time:48201ms step_avg:60.10ms
step:803/2315 train_time:48262ms step_avg:60.10ms
step:804/2315 train_time:48323ms step_avg:60.10ms
step:805/2315 train_time:48384ms step_avg:60.10ms
step:806/2315 train_time:48445ms step_avg:60.11ms
step:807/2315 train_time:48506ms step_avg:60.11ms
step:808/2315 train_time:48567ms step_avg:60.11ms
step:809/2315 train_time:48627ms step_avg:60.11ms
step:810/2315 train_time:48687ms step_avg:60.11ms
step:811/2315 train_time:48748ms step_avg:60.11ms
step:812/2315 train_time:48808ms step_avg:60.11ms
step:813/2315 train_time:48869ms step_avg:60.11ms
step:814/2315 train_time:48930ms step_avg:60.11ms
step:815/2315 train_time:48991ms step_avg:60.11ms
step:816/2315 train_time:49051ms step_avg:60.11ms
step:817/2315 train_time:49113ms step_avg:60.11ms
step:818/2315 train_time:49175ms step_avg:60.12ms
step:819/2315 train_time:49236ms step_avg:60.12ms
step:820/2315 train_time:49297ms step_avg:60.12ms
step:821/2315 train_time:49358ms step_avg:60.12ms
step:822/2315 train_time:49419ms step_avg:60.12ms
step:823/2315 train_time:49480ms step_avg:60.12ms
step:824/2315 train_time:49541ms step_avg:60.12ms
step:825/2315 train_time:49602ms step_avg:60.12ms
step:826/2315 train_time:49663ms step_avg:60.12ms
step:827/2315 train_time:49724ms step_avg:60.13ms
step:828/2315 train_time:49785ms step_avg:60.13ms
step:829/2315 train_time:49845ms step_avg:60.13ms
step:830/2315 train_time:49906ms step_avg:60.13ms
step:831/2315 train_time:49967ms step_avg:60.13ms
step:832/2315 train_time:50027ms step_avg:60.13ms
step:833/2315 train_time:50088ms step_avg:60.13ms
step:834/2315 train_time:50149ms step_avg:60.13ms
step:835/2315 train_time:50210ms step_avg:60.13ms
step:836/2315 train_time:50271ms step_avg:60.13ms
step:837/2315 train_time:50332ms step_avg:60.13ms
step:838/2315 train_time:50394ms step_avg:60.14ms
step:839/2315 train_time:50455ms step_avg:60.14ms
step:840/2315 train_time:50516ms step_avg:60.14ms
step:841/2315 train_time:50576ms step_avg:60.14ms
step:842/2315 train_time:50636ms step_avg:60.14ms
step:843/2315 train_time:50697ms step_avg:60.14ms
step:844/2315 train_time:50758ms step_avg:60.14ms
step:845/2315 train_time:50819ms step_avg:60.14ms
step:846/2315 train_time:50880ms step_avg:60.14ms
step:847/2315 train_time:50942ms step_avg:60.14ms
step:848/2315 train_time:51002ms step_avg:60.14ms
step:849/2315 train_time:51063ms step_avg:60.15ms
step:850/2315 train_time:51124ms step_avg:60.15ms
step:851/2315 train_time:51185ms step_avg:60.15ms
step:852/2315 train_time:51246ms step_avg:60.15ms
step:853/2315 train_time:51307ms step_avg:60.15ms
step:854/2315 train_time:51367ms step_avg:60.15ms
step:855/2315 train_time:51428ms step_avg:60.15ms
step:856/2315 train_time:51488ms step_avg:60.15ms
step:857/2315 train_time:51549ms step_avg:60.15ms
step:858/2315 train_time:51610ms step_avg:60.15ms
step:859/2315 train_time:51671ms step_avg:60.15ms
step:860/2315 train_time:51733ms step_avg:60.15ms
step:861/2315 train_time:51794ms step_avg:60.16ms
step:862/2315 train_time:51854ms step_avg:60.16ms
step:863/2315 train_time:51916ms step_avg:60.16ms
step:864/2315 train_time:51976ms step_avg:60.16ms
step:865/2315 train_time:52037ms step_avg:60.16ms
step:866/2315 train_time:52098ms step_avg:60.16ms
step:867/2315 train_time:52160ms step_avg:60.16ms
step:868/2315 train_time:52221ms step_avg:60.16ms
step:869/2315 train_time:52282ms step_avg:60.16ms
step:870/2315 train_time:52343ms step_avg:60.16ms
step:871/2315 train_time:52404ms step_avg:60.17ms
step:872/2315 train_time:52465ms step_avg:60.17ms
step:873/2315 train_time:52525ms step_avg:60.17ms
step:874/2315 train_time:52586ms step_avg:60.17ms
step:875/2315 train_time:52647ms step_avg:60.17ms
step:876/2315 train_time:52707ms step_avg:60.17ms
step:877/2315 train_time:52768ms step_avg:60.17ms
step:878/2315 train_time:52829ms step_avg:60.17ms
step:879/2315 train_time:52889ms step_avg:60.17ms
step:880/2315 train_time:52951ms step_avg:60.17ms
step:881/2315 train_time:53013ms step_avg:60.17ms
step:882/2315 train_time:53073ms step_avg:60.17ms
step:883/2315 train_time:53135ms step_avg:60.18ms
step:884/2315 train_time:53195ms step_avg:60.18ms
step:885/2315 train_time:53257ms step_avg:60.18ms
step:886/2315 train_time:53318ms step_avg:60.18ms
step:887/2315 train_time:53379ms step_avg:60.18ms
step:888/2315 train_time:53439ms step_avg:60.18ms
step:889/2315 train_time:53501ms step_avg:60.18ms
step:890/2315 train_time:53562ms step_avg:60.18ms
step:891/2315 train_time:53624ms step_avg:60.18ms
step:892/2315 train_time:53684ms step_avg:60.18ms
step:893/2315 train_time:53745ms step_avg:60.18ms
step:894/2315 train_time:53806ms step_avg:60.19ms
step:895/2315 train_time:53867ms step_avg:60.19ms
step:896/2315 train_time:53928ms step_avg:60.19ms
step:897/2315 train_time:53989ms step_avg:60.19ms
step:898/2315 train_time:54049ms step_avg:60.19ms
step:899/2315 train_time:54110ms step_avg:60.19ms
step:900/2315 train_time:54170ms step_avg:60.19ms
step:901/2315 train_time:54232ms step_avg:60.19ms
step:902/2315 train_time:54292ms step_avg:60.19ms
step:903/2315 train_time:54354ms step_avg:60.19ms
step:904/2315 train_time:54415ms step_avg:60.19ms
step:905/2315 train_time:54476ms step_avg:60.19ms
step:906/2315 train_time:54537ms step_avg:60.20ms
step:907/2315 train_time:54598ms step_avg:60.20ms
step:908/2315 train_time:54659ms step_avg:60.20ms
step:909/2315 train_time:54720ms step_avg:60.20ms
step:910/2315 train_time:54780ms step_avg:60.20ms
step:911/2315 train_time:54842ms step_avg:60.20ms
step:912/2315 train_time:54903ms step_avg:60.20ms
step:913/2315 train_time:54963ms step_avg:60.20ms
step:914/2315 train_time:55025ms step_avg:60.20ms
step:915/2315 train_time:55086ms step_avg:60.20ms
step:916/2315 train_time:55146ms step_avg:60.20ms
step:917/2315 train_time:55207ms step_avg:60.20ms
step:918/2315 train_time:55268ms step_avg:60.20ms
step:919/2315 train_time:55329ms step_avg:60.21ms
step:920/2315 train_time:55389ms step_avg:60.21ms
step:921/2315 train_time:55450ms step_avg:60.21ms
step:922/2315 train_time:55510ms step_avg:60.21ms
step:923/2315 train_time:55571ms step_avg:60.21ms
step:924/2315 train_time:55633ms step_avg:60.21ms
step:925/2315 train_time:55694ms step_avg:60.21ms
step:926/2315 train_time:55755ms step_avg:60.21ms
step:927/2315 train_time:55815ms step_avg:60.21ms
step:928/2315 train_time:55875ms step_avg:60.21ms
step:929/2315 train_time:55936ms step_avg:60.21ms
step:930/2315 train_time:55997ms step_avg:60.21ms
step:931/2315 train_time:56058ms step_avg:60.21ms
step:932/2315 train_time:56120ms step_avg:60.21ms
step:933/2315 train_time:56181ms step_avg:60.22ms
step:934/2315 train_time:56242ms step_avg:60.22ms
step:935/2315 train_time:56303ms step_avg:60.22ms
step:936/2315 train_time:56364ms step_avg:60.22ms
step:937/2315 train_time:56425ms step_avg:60.22ms
step:938/2315 train_time:56485ms step_avg:60.22ms
step:939/2315 train_time:56546ms step_avg:60.22ms
step:940/2315 train_time:56606ms step_avg:60.22ms
step:941/2315 train_time:56667ms step_avg:60.22ms
step:942/2315 train_time:56727ms step_avg:60.22ms
step:943/2315 train_time:56787ms step_avg:60.22ms
step:944/2315 train_time:56848ms step_avg:60.22ms
step:945/2315 train_time:56908ms step_avg:60.22ms
step:946/2315 train_time:56969ms step_avg:60.22ms
step:947/2315 train_time:57031ms step_avg:60.22ms
step:948/2315 train_time:57091ms step_avg:60.22ms
step:949/2315 train_time:57152ms step_avg:60.22ms
step:950/2315 train_time:57214ms step_avg:60.22ms
step:951/2315 train_time:57275ms step_avg:60.23ms
step:952/2315 train_time:57336ms step_avg:60.23ms
step:953/2315 train_time:57397ms step_avg:60.23ms
step:954/2315 train_time:57458ms step_avg:60.23ms
step:955/2315 train_time:57519ms step_avg:60.23ms
step:956/2315 train_time:57580ms step_avg:60.23ms
step:957/2315 train_time:57641ms step_avg:60.23ms
step:958/2315 train_time:57702ms step_avg:60.23ms
step:959/2315 train_time:57763ms step_avg:60.23ms
step:960/2315 train_time:57823ms step_avg:60.23ms
step:961/2315 train_time:57884ms step_avg:60.23ms
step:962/2315 train_time:57945ms step_avg:60.23ms
step:963/2315 train_time:58006ms step_avg:60.23ms
step:964/2315 train_time:58067ms step_avg:60.24ms
step:965/2315 train_time:58127ms step_avg:60.24ms
step:966/2315 train_time:58188ms step_avg:60.24ms
step:967/2315 train_time:58248ms step_avg:60.24ms
step:968/2315 train_time:58309ms step_avg:60.24ms
step:969/2315 train_time:58370ms step_avg:60.24ms
step:970/2315 train_time:58431ms step_avg:60.24ms
step:971/2315 train_time:58492ms step_avg:60.24ms
step:972/2315 train_time:58553ms step_avg:60.24ms
step:973/2315 train_time:58614ms step_avg:60.24ms
step:974/2315 train_time:58675ms step_avg:60.24ms
step:975/2315 train_time:58736ms step_avg:60.24ms
step:976/2315 train_time:58797ms step_avg:60.24ms
step:977/2315 train_time:58858ms step_avg:60.24ms
step:978/2315 train_time:58919ms step_avg:60.24ms
step:979/2315 train_time:58980ms step_avg:60.25ms
step:980/2315 train_time:59042ms step_avg:60.25ms
step:981/2315 train_time:59104ms step_avg:60.25ms
step:982/2315 train_time:59164ms step_avg:60.25ms
step:983/2315 train_time:59225ms step_avg:60.25ms
step:984/2315 train_time:59286ms step_avg:60.25ms
step:985/2315 train_time:59346ms step_avg:60.25ms
step:986/2315 train_time:59407ms step_avg:60.25ms
step:987/2315 train_time:59468ms step_avg:60.25ms
step:988/2315 train_time:59528ms step_avg:60.25ms
step:989/2315 train_time:59589ms step_avg:60.25ms
step:990/2315 train_time:59650ms step_avg:60.25ms
step:991/2315 train_time:59710ms step_avg:60.25ms
step:992/2315 train_time:59771ms step_avg:60.25ms
step:993/2315 train_time:59832ms step_avg:60.25ms
step:994/2315 train_time:59893ms step_avg:60.25ms
step:995/2315 train_time:59954ms step_avg:60.26ms
step:996/2315 train_time:60015ms step_avg:60.26ms
step:997/2315 train_time:60077ms step_avg:60.26ms
step:998/2315 train_time:60137ms step_avg:60.26ms
step:999/2315 train_time:60199ms step_avg:60.26ms
step:1000/2315 train_time:60260ms step_avg:60.26ms
step:1000/2315 val_loss:3.5678 train_time:60323ms step_avg:60.32ms
step:1001/2315 train_time:60343ms step_avg:60.28ms
step:1002/2315 train_time:60382ms step_avg:60.26ms
step:1003/2315 train_time:60446ms step_avg:60.27ms
step:1004/2315 train_time:60511ms step_avg:60.27ms
step:1005/2315 train_time:60573ms step_avg:60.27ms
step:1006/2315 train_time:60634ms step_avg:60.27ms
step:1007/2315 train_time:60694ms step_avg:60.27ms
step:1008/2315 train_time:60754ms step_avg:60.27ms
step:1009/2315 train_time:60814ms step_avg:60.27ms
step:1010/2315 train_time:60874ms step_avg:60.27ms
step:1011/2315 train_time:60934ms step_avg:60.27ms
step:1012/2315 train_time:60994ms step_avg:60.27ms
step:1013/2315 train_time:61053ms step_avg:60.27ms
step:1014/2315 train_time:61113ms step_avg:60.27ms
step:1015/2315 train_time:61173ms step_avg:60.27ms
step:1016/2315 train_time:61234ms step_avg:60.27ms
step:1017/2315 train_time:61296ms step_avg:60.27ms
step:1018/2315 train_time:61358ms step_avg:60.27ms
step:1019/2315 train_time:61421ms step_avg:60.28ms
step:1020/2315 train_time:61483ms step_avg:60.28ms
step:1021/2315 train_time:61544ms step_avg:60.28ms
step:1022/2315 train_time:61605ms step_avg:60.28ms
step:1023/2315 train_time:61666ms step_avg:60.28ms
step:1024/2315 train_time:61725ms step_avg:60.28ms
step:1025/2315 train_time:61786ms step_avg:60.28ms
step:1026/2315 train_time:61846ms step_avg:60.28ms
step:1027/2315 train_time:61907ms step_avg:60.28ms
step:1028/2315 train_time:61967ms step_avg:60.28ms
step:1029/2315 train_time:62027ms step_avg:60.28ms
step:1030/2315 train_time:62087ms step_avg:60.28ms
step:1031/2315 train_time:62147ms step_avg:60.28ms
step:1032/2315 train_time:62208ms step_avg:60.28ms
step:1033/2315 train_time:62268ms step_avg:60.28ms
step:1034/2315 train_time:62330ms step_avg:60.28ms
step:1035/2315 train_time:62391ms step_avg:60.28ms
step:1036/2315 train_time:62453ms step_avg:60.28ms
step:1037/2315 train_time:62515ms step_avg:60.28ms
step:1038/2315 train_time:62575ms step_avg:60.28ms
step:1039/2315 train_time:62636ms step_avg:60.29ms
step:1040/2315 train_time:62697ms step_avg:60.29ms
step:1041/2315 train_time:62758ms step_avg:60.29ms
step:1042/2315 train_time:62819ms step_avg:60.29ms
step:1043/2315 train_time:62879ms step_avg:60.29ms
step:1044/2315 train_time:62940ms step_avg:60.29ms
step:1045/2315 train_time:63001ms step_avg:60.29ms
step:1046/2315 train_time:63061ms step_avg:60.29ms
step:1047/2315 train_time:63122ms step_avg:60.29ms
step:1048/2315 train_time:63183ms step_avg:60.29ms
step:1049/2315 train_time:63245ms step_avg:60.29ms
step:1050/2315 train_time:63306ms step_avg:60.29ms
step:1051/2315 train_time:63367ms step_avg:60.29ms
step:1052/2315 train_time:63428ms step_avg:60.29ms
step:1053/2315 train_time:63488ms step_avg:60.29ms
step:1054/2315 train_time:63548ms step_avg:60.29ms
step:1055/2315 train_time:63610ms step_avg:60.29ms
step:1056/2315 train_time:63670ms step_avg:60.29ms
step:1057/2315 train_time:63731ms step_avg:60.29ms
step:1058/2315 train_time:63792ms step_avg:60.29ms
step:1059/2315 train_time:63853ms step_avg:60.30ms
step:1060/2315 train_time:63914ms step_avg:60.30ms
step:1061/2315 train_time:63974ms step_avg:60.30ms
step:1062/2315 train_time:64034ms step_avg:60.30ms
step:1063/2315 train_time:64096ms step_avg:60.30ms
step:1064/2315 train_time:64156ms step_avg:60.30ms
step:1065/2315 train_time:64217ms step_avg:60.30ms
step:1066/2315 train_time:64278ms step_avg:60.30ms
step:1067/2315 train_time:64339ms step_avg:60.30ms
step:1068/2315 train_time:64400ms step_avg:60.30ms
step:1069/2315 train_time:64461ms step_avg:60.30ms
step:1070/2315 train_time:64522ms step_avg:60.30ms
step:1071/2315 train_time:64583ms step_avg:60.30ms
step:1072/2315 train_time:64643ms step_avg:60.30ms
step:1073/2315 train_time:64705ms step_avg:60.30ms
step:1074/2315 train_time:64766ms step_avg:60.30ms
step:1075/2315 train_time:64827ms step_avg:60.30ms
step:1076/2315 train_time:64887ms step_avg:60.30ms
step:1077/2315 train_time:64948ms step_avg:60.30ms
step:1078/2315 train_time:65008ms step_avg:60.30ms
step:1079/2315 train_time:65069ms step_avg:60.31ms
step:1080/2315 train_time:65130ms step_avg:60.31ms
step:1081/2315 train_time:65191ms step_avg:60.31ms
step:1082/2315 train_time:65252ms step_avg:60.31ms
step:1083/2315 train_time:65313ms step_avg:60.31ms
step:1084/2315 train_time:65373ms step_avg:60.31ms
step:1085/2315 train_time:65435ms step_avg:60.31ms
step:1086/2315 train_time:65495ms step_avg:60.31ms
step:1087/2315 train_time:65556ms step_avg:60.31ms
step:1088/2315 train_time:65616ms step_avg:60.31ms
step:1089/2315 train_time:65677ms step_avg:60.31ms
step:1090/2315 train_time:65738ms step_avg:60.31ms
step:1091/2315 train_time:65799ms step_avg:60.31ms
step:1092/2315 train_time:65860ms step_avg:60.31ms
step:1093/2315 train_time:65921ms step_avg:60.31ms
step:1094/2315 train_time:65982ms step_avg:60.31ms
step:1095/2315 train_time:66043ms step_avg:60.31ms
step:1096/2315 train_time:66104ms step_avg:60.31ms
step:1097/2315 train_time:66165ms step_avg:60.31ms
step:1098/2315 train_time:66225ms step_avg:60.31ms
step:1099/2315 train_time:66286ms step_avg:60.32ms
step:1100/2315 train_time:66347ms step_avg:60.32ms
step:1101/2315 train_time:66408ms step_avg:60.32ms
step:1102/2315 train_time:66468ms step_avg:60.32ms
step:1103/2315 train_time:66529ms step_avg:60.32ms
step:1104/2315 train_time:66589ms step_avg:60.32ms
step:1105/2315 train_time:66650ms step_avg:60.32ms
step:1106/2315 train_time:66712ms step_avg:60.32ms
step:1107/2315 train_time:66773ms step_avg:60.32ms
step:1108/2315 train_time:66834ms step_avg:60.32ms
step:1109/2315 train_time:66894ms step_avg:60.32ms
step:1110/2315 train_time:66954ms step_avg:60.32ms
step:1111/2315 train_time:67015ms step_avg:60.32ms
step:1112/2315 train_time:67076ms step_avg:60.32ms
step:1113/2315 train_time:67137ms step_avg:60.32ms
step:1114/2315 train_time:67198ms step_avg:60.32ms
step:1115/2315 train_time:67260ms step_avg:60.32ms
step:1116/2315 train_time:67320ms step_avg:60.32ms
step:1117/2315 train_time:67381ms step_avg:60.32ms
step:1118/2315 train_time:67442ms step_avg:60.32ms
step:1119/2315 train_time:67503ms step_avg:60.32ms
step:1120/2315 train_time:67564ms step_avg:60.33ms
step:1121/2315 train_time:67625ms step_avg:60.33ms
step:1122/2315 train_time:67686ms step_avg:60.33ms
step:1123/2315 train_time:67747ms step_avg:60.33ms
step:1124/2315 train_time:67807ms step_avg:60.33ms
step:1125/2315 train_time:67868ms step_avg:60.33ms
step:1126/2315 train_time:67928ms step_avg:60.33ms
step:1127/2315 train_time:67989ms step_avg:60.33ms
step:1128/2315 train_time:68050ms step_avg:60.33ms
step:1129/2315 train_time:68111ms step_avg:60.33ms
step:1130/2315 train_time:68172ms step_avg:60.33ms
step:1131/2315 train_time:68234ms step_avg:60.33ms
step:1132/2315 train_time:68295ms step_avg:60.33ms
step:1133/2315 train_time:68356ms step_avg:60.33ms
step:1134/2315 train_time:68416ms step_avg:60.33ms
step:1135/2315 train_time:68477ms step_avg:60.33ms
step:1136/2315 train_time:68538ms step_avg:60.33ms
step:1137/2315 train_time:68599ms step_avg:60.33ms
step:1138/2315 train_time:68659ms step_avg:60.33ms
step:1139/2315 train_time:68721ms step_avg:60.33ms
step:1140/2315 train_time:68781ms step_avg:60.33ms
step:1141/2315 train_time:68843ms step_avg:60.34ms
step:1142/2315 train_time:68903ms step_avg:60.34ms
step:1143/2315 train_time:68964ms step_avg:60.34ms
step:1144/2315 train_time:69025ms step_avg:60.34ms
step:1145/2315 train_time:69087ms step_avg:60.34ms
step:1146/2315 train_time:69147ms step_avg:60.34ms
step:1147/2315 train_time:69208ms step_avg:60.34ms
step:1148/2315 train_time:69268ms step_avg:60.34ms
step:1149/2315 train_time:69329ms step_avg:60.34ms
step:1150/2315 train_time:69390ms step_avg:60.34ms
step:1151/2315 train_time:69450ms step_avg:60.34ms
step:1152/2315 train_time:69511ms step_avg:60.34ms
step:1153/2315 train_time:69572ms step_avg:60.34ms
step:1154/2315 train_time:69634ms step_avg:60.34ms
step:1155/2315 train_time:69695ms step_avg:60.34ms
step:1156/2315 train_time:69755ms step_avg:60.34ms
step:1157/2315 train_time:69816ms step_avg:60.34ms
step:1158/2315 train_time:69877ms step_avg:60.34ms
step:1159/2315 train_time:69938ms step_avg:60.34ms
step:1160/2315 train_time:69999ms step_avg:60.34ms
step:1161/2315 train_time:70060ms step_avg:60.34ms
step:1162/2315 train_time:70121ms step_avg:60.35ms
step:1163/2315 train_time:70183ms step_avg:60.35ms
step:1164/2315 train_time:70243ms step_avg:60.35ms
step:1165/2315 train_time:70304ms step_avg:60.35ms
step:1166/2315 train_time:70365ms step_avg:60.35ms
step:1167/2315 train_time:70426ms step_avg:60.35ms
step:1168/2315 train_time:70486ms step_avg:60.35ms
step:1169/2315 train_time:70547ms step_avg:60.35ms
step:1170/2315 train_time:70607ms step_avg:60.35ms
step:1171/2315 train_time:70668ms step_avg:60.35ms
step:1172/2315 train_time:70728ms step_avg:60.35ms
step:1173/2315 train_time:70789ms step_avg:60.35ms
step:1174/2315 train_time:70850ms step_avg:60.35ms
step:1175/2315 train_time:70912ms step_avg:60.35ms
step:1176/2315 train_time:70974ms step_avg:60.35ms
step:1177/2315 train_time:71035ms step_avg:60.35ms
step:1178/2315 train_time:71096ms step_avg:60.35ms
step:1179/2315 train_time:71157ms step_avg:60.35ms
step:1180/2315 train_time:71217ms step_avg:60.35ms
step:1181/2315 train_time:71278ms step_avg:60.35ms
step:1182/2315 train_time:71339ms step_avg:60.35ms
step:1183/2315 train_time:71401ms step_avg:60.36ms
step:1184/2315 train_time:71461ms step_avg:60.36ms
step:1185/2315 train_time:71522ms step_avg:60.36ms
step:1186/2315 train_time:71583ms step_avg:60.36ms
step:1187/2315 train_time:71644ms step_avg:60.36ms
step:1188/2315 train_time:71705ms step_avg:60.36ms
step:1189/2315 train_time:71767ms step_avg:60.36ms
step:1190/2315 train_time:71827ms step_avg:60.36ms
step:1191/2315 train_time:71888ms step_avg:60.36ms
step:1192/2315 train_time:71949ms step_avg:60.36ms
step:1193/2315 train_time:72009ms step_avg:60.36ms
step:1194/2315 train_time:72069ms step_avg:60.36ms
step:1195/2315 train_time:72131ms step_avg:60.36ms
step:1196/2315 train_time:72191ms step_avg:60.36ms
step:1197/2315 train_time:72252ms step_avg:60.36ms
step:1198/2315 train_time:72314ms step_avg:60.36ms
step:1199/2315 train_time:72376ms step_avg:60.36ms
step:1200/2315 train_time:72436ms step_avg:60.36ms
step:1201/2315 train_time:72497ms step_avg:60.36ms
step:1202/2315 train_time:72557ms step_avg:60.36ms
step:1203/2315 train_time:72618ms step_avg:60.36ms
step:1204/2315 train_time:72679ms step_avg:60.36ms
step:1205/2315 train_time:72741ms step_avg:60.37ms
step:1206/2315 train_time:72802ms step_avg:60.37ms
step:1207/2315 train_time:72863ms step_avg:60.37ms
step:1208/2315 train_time:72923ms step_avg:60.37ms
step:1209/2315 train_time:72985ms step_avg:60.37ms
step:1210/2315 train_time:73045ms step_avg:60.37ms
step:1211/2315 train_time:73107ms step_avg:60.37ms
step:1212/2315 train_time:73167ms step_avg:60.37ms
step:1213/2315 train_time:73228ms step_avg:60.37ms
step:1214/2315 train_time:73289ms step_avg:60.37ms
step:1215/2315 train_time:73349ms step_avg:60.37ms
step:1216/2315 train_time:73411ms step_avg:60.37ms
step:1217/2315 train_time:73472ms step_avg:60.37ms
step:1218/2315 train_time:73532ms step_avg:60.37ms
step:1219/2315 train_time:73594ms step_avg:60.37ms
step:1220/2315 train_time:73655ms step_avg:60.37ms
step:1221/2315 train_time:73716ms step_avg:60.37ms
step:1222/2315 train_time:73776ms step_avg:60.37ms
step:1223/2315 train_time:73837ms step_avg:60.37ms
step:1224/2315 train_time:73898ms step_avg:60.37ms
step:1225/2315 train_time:73959ms step_avg:60.37ms
step:1226/2315 train_time:74020ms step_avg:60.38ms
step:1227/2315 train_time:74082ms step_avg:60.38ms
step:1228/2315 train_time:74143ms step_avg:60.38ms
step:1229/2315 train_time:74204ms step_avg:60.38ms
step:1230/2315 train_time:74265ms step_avg:60.38ms
step:1231/2315 train_time:74326ms step_avg:60.38ms
step:1232/2315 train_time:74387ms step_avg:60.38ms
step:1233/2315 train_time:74448ms step_avg:60.38ms
step:1234/2315 train_time:74508ms step_avg:60.38ms
step:1235/2315 train_time:74568ms step_avg:60.38ms
step:1236/2315 train_time:74628ms step_avg:60.38ms
step:1237/2315 train_time:74689ms step_avg:60.38ms
step:1238/2315 train_time:74750ms step_avg:60.38ms
step:1239/2315 train_time:74811ms step_avg:60.38ms
step:1240/2315 train_time:74872ms step_avg:60.38ms
step:1241/2315 train_time:74934ms step_avg:60.38ms
step:1242/2315 train_time:74995ms step_avg:60.38ms
step:1243/2315 train_time:75056ms step_avg:60.38ms
step:1244/2315 train_time:75117ms step_avg:60.38ms
step:1245/2315 train_time:75178ms step_avg:60.38ms
step:1246/2315 train_time:75239ms step_avg:60.38ms
step:1247/2315 train_time:75301ms step_avg:60.39ms
step:1248/2315 train_time:75361ms step_avg:60.39ms
step:1249/2315 train_time:75423ms step_avg:60.39ms
step:1250/2315 train_time:75484ms step_avg:60.39ms
step:1250/2315 val_loss:3.5099 train_time:75546ms step_avg:60.44ms
step:1251/2315 train_time:75565ms step_avg:60.40ms
step:1252/2315 train_time:75608ms step_avg:60.39ms
step:1253/2315 train_time:75671ms step_avg:60.39ms
step:1254/2315 train_time:75737ms step_avg:60.40ms
step:1255/2315 train_time:75798ms step_avg:60.40ms
step:1256/2315 train_time:75858ms step_avg:60.40ms
step:1257/2315 train_time:75919ms step_avg:60.40ms
step:1258/2315 train_time:75979ms step_avg:60.40ms
step:1259/2315 train_time:76039ms step_avg:60.40ms
step:1260/2315 train_time:76099ms step_avg:60.40ms
step:1261/2315 train_time:76159ms step_avg:60.40ms
step:1262/2315 train_time:76219ms step_avg:60.40ms
step:1263/2315 train_time:76279ms step_avg:60.39ms
step:1264/2315 train_time:76339ms step_avg:60.39ms
step:1265/2315 train_time:76398ms step_avg:60.39ms
step:1266/2315 train_time:76458ms step_avg:60.39ms
step:1267/2315 train_time:76519ms step_avg:60.39ms
step:1268/2315 train_time:76581ms step_avg:60.40ms
step:1269/2315 train_time:76644ms step_avg:60.40ms
step:1270/2315 train_time:76706ms step_avg:60.40ms
step:1271/2315 train_time:76767ms step_avg:60.40ms
step:1272/2315 train_time:76829ms step_avg:60.40ms
step:1273/2315 train_time:76890ms step_avg:60.40ms
step:1274/2315 train_time:76951ms step_avg:60.40ms
step:1275/2315 train_time:77012ms step_avg:60.40ms
step:1276/2315 train_time:77073ms step_avg:60.40ms
step:1277/2315 train_time:77134ms step_avg:60.40ms
step:1278/2315 train_time:77194ms step_avg:60.40ms
step:1279/2315 train_time:77255ms step_avg:60.40ms
step:1280/2315 train_time:77315ms step_avg:60.40ms
step:1281/2315 train_time:77376ms step_avg:60.40ms
step:1282/2315 train_time:77436ms step_avg:60.40ms
step:1283/2315 train_time:77496ms step_avg:60.40ms
step:1284/2315 train_time:77556ms step_avg:60.40ms
step:1285/2315 train_time:77617ms step_avg:60.40ms
step:1286/2315 train_time:77678ms step_avg:60.40ms
step:1287/2315 train_time:77739ms step_avg:60.40ms
step:1288/2315 train_time:77800ms step_avg:60.40ms
step:1289/2315 train_time:77862ms step_avg:60.41ms
step:1290/2315 train_time:77923ms step_avg:60.41ms
step:1291/2315 train_time:77985ms step_avg:60.41ms
step:1292/2315 train_time:78046ms step_avg:60.41ms
step:1293/2315 train_time:78106ms step_avg:60.41ms
step:1294/2315 train_time:78166ms step_avg:60.41ms
step:1295/2315 train_time:78228ms step_avg:60.41ms
step:1296/2315 train_time:78289ms step_avg:60.41ms
step:1297/2315 train_time:78350ms step_avg:60.41ms
step:1298/2315 train_time:78410ms step_avg:60.41ms
step:1299/2315 train_time:78471ms step_avg:60.41ms
step:1300/2315 train_time:78532ms step_avg:60.41ms
step:1301/2315 train_time:78593ms step_avg:60.41ms
step:1302/2315 train_time:78653ms step_avg:60.41ms
step:1303/2315 train_time:78714ms step_avg:60.41ms
step:1304/2315 train_time:78775ms step_avg:60.41ms
step:1305/2315 train_time:78836ms step_avg:60.41ms
step:1306/2315 train_time:78896ms step_avg:60.41ms
step:1307/2315 train_time:78957ms step_avg:60.41ms
step:1308/2315 train_time:79018ms step_avg:60.41ms
step:1309/2315 train_time:79079ms step_avg:60.41ms
step:1310/2315 train_time:79139ms step_avg:60.41ms
step:1311/2315 train_time:79200ms step_avg:60.41ms
step:1312/2315 train_time:79261ms step_avg:60.41ms
step:1313/2315 train_time:79323ms step_avg:60.41ms
step:1314/2315 train_time:79384ms step_avg:60.41ms
step:1315/2315 train_time:79446ms step_avg:60.42ms
step:1316/2315 train_time:79506ms step_avg:60.41ms
step:1317/2315 train_time:79566ms step_avg:60.41ms
step:1318/2315 train_time:79627ms step_avg:60.42ms
step:1319/2315 train_time:79689ms step_avg:60.42ms
step:1320/2315 train_time:79749ms step_avg:60.42ms
step:1321/2315 train_time:79811ms step_avg:60.42ms
step:1322/2315 train_time:79871ms step_avg:60.42ms
step:1323/2315 train_time:79933ms step_avg:60.42ms
step:1324/2315 train_time:79994ms step_avg:60.42ms
step:1325/2315 train_time:80055ms step_avg:60.42ms
step:1326/2315 train_time:80116ms step_avg:60.42ms
step:1327/2315 train_time:80176ms step_avg:60.42ms
step:1328/2315 train_time:80236ms step_avg:60.42ms
step:1329/2315 train_time:80297ms step_avg:60.42ms
step:1330/2315 train_time:80358ms step_avg:60.42ms
step:1331/2315 train_time:80419ms step_avg:60.42ms
step:1332/2315 train_time:80479ms step_avg:60.42ms
step:1333/2315 train_time:80540ms step_avg:60.42ms
step:1334/2315 train_time:80601ms step_avg:60.42ms
step:1335/2315 train_time:80662ms step_avg:60.42ms
step:1336/2315 train_time:80723ms step_avg:60.42ms
step:1337/2315 train_time:80785ms step_avg:60.42ms
step:1338/2315 train_time:80845ms step_avg:60.42ms
step:1339/2315 train_time:80906ms step_avg:60.42ms
step:1340/2315 train_time:80967ms step_avg:60.42ms
step:1341/2315 train_time:81028ms step_avg:60.42ms
step:1342/2315 train_time:81089ms step_avg:60.42ms
step:1343/2315 train_time:81150ms step_avg:60.42ms
step:1344/2315 train_time:81211ms step_avg:60.42ms
step:1345/2315 train_time:81272ms step_avg:60.42ms
step:1346/2315 train_time:81333ms step_avg:60.43ms
step:1347/2315 train_time:81394ms step_avg:60.43ms
step:1348/2315 train_time:81455ms step_avg:60.43ms
step:1349/2315 train_time:81515ms step_avg:60.43ms
step:1350/2315 train_time:81576ms step_avg:60.43ms
step:1351/2315 train_time:81636ms step_avg:60.43ms
step:1352/2315 train_time:81697ms step_avg:60.43ms
step:1353/2315 train_time:81758ms step_avg:60.43ms
step:1354/2315 train_time:81819ms step_avg:60.43ms
step:1355/2315 train_time:81880ms step_avg:60.43ms
step:1356/2315 train_time:81941ms step_avg:60.43ms
step:1357/2315 train_time:82003ms step_avg:60.43ms
step:1358/2315 train_time:82064ms step_avg:60.43ms
step:1359/2315 train_time:82126ms step_avg:60.43ms
step:1360/2315 train_time:82186ms step_avg:60.43ms
step:1361/2315 train_time:82247ms step_avg:60.43ms
step:1362/2315 train_time:82309ms step_avg:60.43ms
step:1363/2315 train_time:82369ms step_avg:60.43ms
step:1364/2315 train_time:82430ms step_avg:60.43ms
step:1365/2315 train_time:82491ms step_avg:60.43ms
step:1366/2315 train_time:82551ms step_avg:60.43ms
step:1367/2315 train_time:82612ms step_avg:60.43ms
step:1368/2315 train_time:82672ms step_avg:60.43ms
step:1369/2315 train_time:82734ms step_avg:60.43ms
step:1370/2315 train_time:82795ms step_avg:60.43ms
step:1371/2315 train_time:82856ms step_avg:60.43ms
step:1372/2315 train_time:82916ms step_avg:60.43ms
step:1373/2315 train_time:82977ms step_avg:60.44ms
step:1374/2315 train_time:83038ms step_avg:60.43ms
step:1375/2315 train_time:83098ms step_avg:60.43ms
step:1376/2315 train_time:83159ms step_avg:60.44ms
step:1377/2315 train_time:83220ms step_avg:60.44ms
step:1378/2315 train_time:83281ms step_avg:60.44ms
step:1379/2315 train_time:83343ms step_avg:60.44ms
step:1380/2315 train_time:83403ms step_avg:60.44ms
step:1381/2315 train_time:83465ms step_avg:60.44ms
step:1382/2315 train_time:83525ms step_avg:60.44ms
step:1383/2315 train_time:83586ms step_avg:60.44ms
step:1384/2315 train_time:83647ms step_avg:60.44ms
step:1385/2315 train_time:83709ms step_avg:60.44ms
step:1386/2315 train_time:83770ms step_avg:60.44ms
step:1387/2315 train_time:83831ms step_avg:60.44ms
step:1388/2315 train_time:83892ms step_avg:60.44ms
step:1389/2315 train_time:83953ms step_avg:60.44ms
step:1390/2315 train_time:84014ms step_avg:60.44ms
step:1391/2315 train_time:84075ms step_avg:60.44ms
step:1392/2315 train_time:84136ms step_avg:60.44ms
step:1393/2315 train_time:84197ms step_avg:60.44ms
step:1394/2315 train_time:84257ms step_avg:60.44ms
step:1395/2315 train_time:84318ms step_avg:60.44ms
step:1396/2315 train_time:84378ms step_avg:60.44ms
step:1397/2315 train_time:84439ms step_avg:60.44ms
step:1398/2315 train_time:84500ms step_avg:60.44ms
step:1399/2315 train_time:84561ms step_avg:60.44ms
step:1400/2315 train_time:84622ms step_avg:60.44ms
step:1401/2315 train_time:84684ms step_avg:60.45ms
step:1402/2315 train_time:84745ms step_avg:60.45ms
step:1403/2315 train_time:84806ms step_avg:60.45ms
step:1404/2315 train_time:84867ms step_avg:60.45ms
step:1405/2315 train_time:84928ms step_avg:60.45ms
step:1406/2315 train_time:84989ms step_avg:60.45ms
step:1407/2315 train_time:85050ms step_avg:60.45ms
step:1408/2315 train_time:85111ms step_avg:60.45ms
step:1409/2315 train_time:85173ms step_avg:60.45ms
step:1410/2315 train_time:85233ms step_avg:60.45ms
step:1411/2315 train_time:85294ms step_avg:60.45ms
step:1412/2315 train_time:85355ms step_avg:60.45ms
step:1413/2315 train_time:85416ms step_avg:60.45ms
step:1414/2315 train_time:85477ms step_avg:60.45ms
step:1415/2315 train_time:85538ms step_avg:60.45ms
step:1416/2315 train_time:85598ms step_avg:60.45ms
step:1417/2315 train_time:85659ms step_avg:60.45ms
step:1418/2315 train_time:85720ms step_avg:60.45ms
step:1419/2315 train_time:85782ms step_avg:60.45ms
step:1420/2315 train_time:85843ms step_avg:60.45ms
step:1421/2315 train_time:85905ms step_avg:60.45ms
step:1422/2315 train_time:85966ms step_avg:60.45ms
step:1423/2315 train_time:86027ms step_avg:60.45ms
step:1424/2315 train_time:86087ms step_avg:60.45ms
step:1425/2315 train_time:86148ms step_avg:60.45ms
step:1426/2315 train_time:86209ms step_avg:60.45ms
step:1427/2315 train_time:86270ms step_avg:60.46ms
step:1428/2315 train_time:86330ms step_avg:60.46ms
step:1429/2315 train_time:86392ms step_avg:60.46ms
step:1430/2315 train_time:86453ms step_avg:60.46ms
step:1431/2315 train_time:86514ms step_avg:60.46ms
step:1432/2315 train_time:86575ms step_avg:60.46ms
step:1433/2315 train_time:86636ms step_avg:60.46ms
step:1434/2315 train_time:86697ms step_avg:60.46ms
step:1435/2315 train_time:86757ms step_avg:60.46ms
step:1436/2315 train_time:86818ms step_avg:60.46ms
step:1437/2315 train_time:86879ms step_avg:60.46ms
step:1438/2315 train_time:86940ms step_avg:60.46ms
step:1439/2315 train_time:87002ms step_avg:60.46ms
step:1440/2315 train_time:87063ms step_avg:60.46ms
step:1441/2315 train_time:87124ms step_avg:60.46ms
step:1442/2315 train_time:87185ms step_avg:60.46ms
step:1443/2315 train_time:87246ms step_avg:60.46ms
step:1444/2315 train_time:87306ms step_avg:60.46ms
step:1445/2315 train_time:87367ms step_avg:60.46ms
step:1446/2315 train_time:87428ms step_avg:60.46ms
step:1447/2315 train_time:87489ms step_avg:60.46ms
step:1448/2315 train_time:87550ms step_avg:60.46ms
step:1449/2315 train_time:87612ms step_avg:60.46ms
step:1450/2315 train_time:87672ms step_avg:60.46ms
step:1451/2315 train_time:87733ms step_avg:60.46ms
step:1452/2315 train_time:87794ms step_avg:60.46ms
step:1453/2315 train_time:87856ms step_avg:60.46ms
step:1454/2315 train_time:87916ms step_avg:60.46ms
step:1455/2315 train_time:87977ms step_avg:60.47ms
step:1456/2315 train_time:88038ms step_avg:60.47ms
step:1457/2315 train_time:88098ms step_avg:60.47ms
step:1458/2315 train_time:88159ms step_avg:60.47ms
step:1459/2315 train_time:88220ms step_avg:60.47ms
step:1460/2315 train_time:88281ms step_avg:60.47ms
step:1461/2315 train_time:88343ms step_avg:60.47ms
step:1462/2315 train_time:88404ms step_avg:60.47ms
step:1463/2315 train_time:88466ms step_avg:60.47ms
step:1464/2315 train_time:88526ms step_avg:60.47ms
step:1465/2315 train_time:88587ms step_avg:60.47ms
step:1466/2315 train_time:88648ms step_avg:60.47ms
step:1467/2315 train_time:88710ms step_avg:60.47ms
step:1468/2315 train_time:88771ms step_avg:60.47ms
step:1469/2315 train_time:88832ms step_avg:60.47ms
step:1470/2315 train_time:88893ms step_avg:60.47ms
step:1471/2315 train_time:88954ms step_avg:60.47ms
step:1472/2315 train_time:89014ms step_avg:60.47ms
step:1473/2315 train_time:89075ms step_avg:60.47ms
step:1474/2315 train_time:89136ms step_avg:60.47ms
step:1475/2315 train_time:89197ms step_avg:60.47ms
step:1476/2315 train_time:89257ms step_avg:60.47ms
step:1477/2315 train_time:89318ms step_avg:60.47ms
step:1478/2315 train_time:89378ms step_avg:60.47ms
step:1479/2315 train_time:89439ms step_avg:60.47ms
step:1480/2315 train_time:89501ms step_avg:60.47ms
step:1481/2315 train_time:89561ms step_avg:60.47ms
step:1482/2315 train_time:89622ms step_avg:60.47ms
step:1483/2315 train_time:89683ms step_avg:60.47ms
step:1484/2315 train_time:89745ms step_avg:60.47ms
step:1485/2315 train_time:89806ms step_avg:60.48ms
step:1486/2315 train_time:89866ms step_avg:60.48ms
step:1487/2315 train_time:89927ms step_avg:60.48ms
step:1488/2315 train_time:89987ms step_avg:60.48ms
step:1489/2315 train_time:90048ms step_avg:60.48ms
step:1490/2315 train_time:90109ms step_avg:60.48ms
step:1491/2315 train_time:90171ms step_avg:60.48ms
step:1492/2315 train_time:90232ms step_avg:60.48ms
step:1493/2315 train_time:90292ms step_avg:60.48ms
step:1494/2315 train_time:90353ms step_avg:60.48ms
step:1495/2315 train_time:90415ms step_avg:60.48ms
step:1496/2315 train_time:90476ms step_avg:60.48ms
step:1497/2315 train_time:90536ms step_avg:60.48ms
step:1498/2315 train_time:90597ms step_avg:60.48ms
step:1499/2315 train_time:90657ms step_avg:60.48ms
step:1500/2315 train_time:90717ms step_avg:60.48ms
step:1500/2315 val_loss:3.4477 train_time:90780ms step_avg:60.52ms
step:1501/2315 train_time:90802ms step_avg:60.49ms
step:1502/2315 train_time:90842ms step_avg:60.48ms
step:1503/2315 train_time:90906ms step_avg:60.48ms
step:1504/2315 train_time:90971ms step_avg:60.49ms
step:1505/2315 train_time:91032ms step_avg:60.49ms
step:1506/2315 train_time:91093ms step_avg:60.49ms
step:1507/2315 train_time:91153ms step_avg:60.49ms
step:1508/2315 train_time:91213ms step_avg:60.49ms
step:1509/2315 train_time:91273ms step_avg:60.49ms
step:1510/2315 train_time:91332ms step_avg:60.49ms
step:1511/2315 train_time:91392ms step_avg:60.48ms
step:1512/2315 train_time:91452ms step_avg:60.48ms
step:1513/2315 train_time:91512ms step_avg:60.48ms
step:1514/2315 train_time:91572ms step_avg:60.48ms
step:1515/2315 train_time:91632ms step_avg:60.48ms
step:1516/2315 train_time:91692ms step_avg:60.48ms
step:1517/2315 train_time:91753ms step_avg:60.48ms
step:1518/2315 train_time:91815ms step_avg:60.48ms
step:1519/2315 train_time:91878ms step_avg:60.49ms
step:1520/2315 train_time:91940ms step_avg:60.49ms
step:1521/2315 train_time:92001ms step_avg:60.49ms
step:1522/2315 train_time:92062ms step_avg:60.49ms
step:1523/2315 train_time:92124ms step_avg:60.49ms
step:1524/2315 train_time:92186ms step_avg:60.49ms
step:1525/2315 train_time:92247ms step_avg:60.49ms
step:1526/2315 train_time:92308ms step_avg:60.49ms
step:1527/2315 train_time:92369ms step_avg:60.49ms
step:1528/2315 train_time:92430ms step_avg:60.49ms
step:1529/2315 train_time:92490ms step_avg:60.49ms
step:1530/2315 train_time:92551ms step_avg:60.49ms
step:1531/2315 train_time:92612ms step_avg:60.49ms
step:1532/2315 train_time:92672ms step_avg:60.49ms
step:1533/2315 train_time:92733ms step_avg:60.49ms
step:1534/2315 train_time:92794ms step_avg:60.49ms
step:1535/2315 train_time:92856ms step_avg:60.49ms
step:1536/2315 train_time:92917ms step_avg:60.49ms
step:1537/2315 train_time:92979ms step_avg:60.49ms
step:1538/2315 train_time:93042ms step_avg:60.50ms
step:1539/2315 train_time:93103ms step_avg:60.50ms
step:1540/2315 train_time:93165ms step_avg:60.50ms
step:1541/2315 train_time:93227ms step_avg:60.50ms
step:1542/2315 train_time:93288ms step_avg:60.50ms
step:1543/2315 train_time:93349ms step_avg:60.50ms
step:1544/2315 train_time:93410ms step_avg:60.50ms
step:1545/2315 train_time:93471ms step_avg:60.50ms
step:1546/2315 train_time:93532ms step_avg:60.50ms
step:1547/2315 train_time:93593ms step_avg:60.50ms
step:1548/2315 train_time:93653ms step_avg:60.50ms
step:1549/2315 train_time:93713ms step_avg:60.50ms
step:1550/2315 train_time:93775ms step_avg:60.50ms
step:1551/2315 train_time:93836ms step_avg:60.50ms
step:1552/2315 train_time:93897ms step_avg:60.50ms
step:1553/2315 train_time:93959ms step_avg:60.50ms
step:1554/2315 train_time:94021ms step_avg:60.50ms
step:1555/2315 train_time:94084ms step_avg:60.50ms
step:1556/2315 train_time:94146ms step_avg:60.50ms
step:1557/2315 train_time:94207ms step_avg:60.51ms
step:1558/2315 train_time:94268ms step_avg:60.51ms
step:1559/2315 train_time:94329ms step_avg:60.51ms
step:1560/2315 train_time:94390ms step_avg:60.51ms
step:1561/2315 train_time:94451ms step_avg:60.51ms
step:1562/2315 train_time:94512ms step_avg:60.51ms
step:1563/2315 train_time:94573ms step_avg:60.51ms
step:1564/2315 train_time:94634ms step_avg:60.51ms
step:1565/2315 train_time:94695ms step_avg:60.51ms
step:1566/2315 train_time:94756ms step_avg:60.51ms
step:1567/2315 train_time:94817ms step_avg:60.51ms
step:1568/2315 train_time:94879ms step_avg:60.51ms
step:1569/2315 train_time:94941ms step_avg:60.51ms
step:1570/2315 train_time:95002ms step_avg:60.51ms
step:1571/2315 train_time:95064ms step_avg:60.51ms
step:1572/2315 train_time:95125ms step_avg:60.51ms
step:1573/2315 train_time:95186ms step_avg:60.51ms
step:1574/2315 train_time:95247ms step_avg:60.51ms
step:1575/2315 train_time:95308ms step_avg:60.51ms
step:1576/2315 train_time:95369ms step_avg:60.51ms
step:1577/2315 train_time:95430ms step_avg:60.51ms
step:1578/2315 train_time:95491ms step_avg:60.51ms
step:1579/2315 train_time:95552ms step_avg:60.51ms
step:1580/2315 train_time:95614ms step_avg:60.52ms
step:1581/2315 train_time:95675ms step_avg:60.52ms
step:1582/2315 train_time:95736ms step_avg:60.52ms
step:1583/2315 train_time:95797ms step_avg:60.52ms
step:1584/2315 train_time:95859ms step_avg:60.52ms
step:1585/2315 train_time:95921ms step_avg:60.52ms
step:1586/2315 train_time:95983ms step_avg:60.52ms
step:1587/2315 train_time:96043ms step_avg:60.52ms
step:1588/2315 train_time:96104ms step_avg:60.52ms
step:1589/2315 train_time:96166ms step_avg:60.52ms
step:1590/2315 train_time:96227ms step_avg:60.52ms
step:1591/2315 train_time:96289ms step_avg:60.52ms
step:1592/2315 train_time:96349ms step_avg:60.52ms
step:1593/2315 train_time:96410ms step_avg:60.52ms
step:1594/2315 train_time:96471ms step_avg:60.52ms
step:1595/2315 train_time:96533ms step_avg:60.52ms
step:1596/2315 train_time:96594ms step_avg:60.52ms
step:1597/2315 train_time:96654ms step_avg:60.52ms
step:1598/2315 train_time:96715ms step_avg:60.52ms
step:1599/2315 train_time:96776ms step_avg:60.52ms
step:1600/2315 train_time:96838ms step_avg:60.52ms
step:1601/2315 train_time:96899ms step_avg:60.52ms
step:1602/2315 train_time:96960ms step_avg:60.52ms
step:1603/2315 train_time:97022ms step_avg:60.53ms
step:1604/2315 train_time:97083ms step_avg:60.53ms
step:1605/2315 train_time:97144ms step_avg:60.53ms
step:1606/2315 train_time:97205ms step_avg:60.53ms
step:1607/2315 train_time:97267ms step_avg:60.53ms
step:1608/2315 train_time:97328ms step_avg:60.53ms
step:1609/2315 train_time:97390ms step_avg:60.53ms
step:1610/2315 train_time:97450ms step_avg:60.53ms
step:1611/2315 train_time:97512ms step_avg:60.53ms
step:1612/2315 train_time:97573ms step_avg:60.53ms
step:1613/2315 train_time:97634ms step_avg:60.53ms
step:1614/2315 train_time:97695ms step_avg:60.53ms
step:1615/2315 train_time:97756ms step_avg:60.53ms
step:1616/2315 train_time:97817ms step_avg:60.53ms
step:1617/2315 train_time:97879ms step_avg:60.53ms
step:1618/2315 train_time:97940ms step_avg:60.53ms
step:1619/2315 train_time:98002ms step_avg:60.53ms
step:1620/2315 train_time:98063ms step_avg:60.53ms
step:1621/2315 train_time:98124ms step_avg:60.53ms
step:1622/2315 train_time:98185ms step_avg:60.53ms
step:1623/2315 train_time:98247ms step_avg:60.53ms
step:1624/2315 train_time:98308ms step_avg:60.53ms
step:1625/2315 train_time:98370ms step_avg:60.54ms
step:1626/2315 train_time:98431ms step_avg:60.54ms
step:1627/2315 train_time:98492ms step_avg:60.54ms
step:1628/2315 train_time:98553ms step_avg:60.54ms
step:1629/2315 train_time:98614ms step_avg:60.54ms
step:1630/2315 train_time:98675ms step_avg:60.54ms
step:1631/2315 train_time:98736ms step_avg:60.54ms
step:1632/2315 train_time:98797ms step_avg:60.54ms
step:1633/2315 train_time:98857ms step_avg:60.54ms
step:1634/2315 train_time:98919ms step_avg:60.54ms
step:1635/2315 train_time:98981ms step_avg:60.54ms
step:1636/2315 train_time:99042ms step_avg:60.54ms
step:1637/2315 train_time:99103ms step_avg:60.54ms
step:1638/2315 train_time:99164ms step_avg:60.54ms
step:1639/2315 train_time:99226ms step_avg:60.54ms
step:1640/2315 train_time:99287ms step_avg:60.54ms
step:1641/2315 train_time:99348ms step_avg:60.54ms
step:1642/2315 train_time:99409ms step_avg:60.54ms
step:1643/2315 train_time:99471ms step_avg:60.54ms
step:1644/2315 train_time:99532ms step_avg:60.54ms
step:1645/2315 train_time:99592ms step_avg:60.54ms
step:1646/2315 train_time:99653ms step_avg:60.54ms
step:1647/2315 train_time:99715ms step_avg:60.54ms
step:1648/2315 train_time:99776ms step_avg:60.54ms
step:1649/2315 train_time:99836ms step_avg:60.54ms
step:1650/2315 train_time:99898ms step_avg:60.54ms
step:1651/2315 train_time:99959ms step_avg:60.54ms
step:1652/2315 train_time:100020ms step_avg:60.55ms
step:1653/2315 train_time:100082ms step_avg:60.55ms
step:1654/2315 train_time:100143ms step_avg:60.55ms
step:1655/2315 train_time:100204ms step_avg:60.55ms
step:1656/2315 train_time:100265ms step_avg:60.55ms
step:1657/2315 train_time:100327ms step_avg:60.55ms
step:1658/2315 train_time:100388ms step_avg:60.55ms
step:1659/2315 train_time:100449ms step_avg:60.55ms
step:1660/2315 train_time:100510ms step_avg:60.55ms
step:1661/2315 train_time:100571ms step_avg:60.55ms
step:1662/2315 train_time:100632ms step_avg:60.55ms
step:1663/2315 train_time:100694ms step_avg:60.55ms
step:1664/2315 train_time:100754ms step_avg:60.55ms
step:1665/2315 train_time:100816ms step_avg:60.55ms
step:1666/2315 train_time:100877ms step_avg:60.55ms
step:1667/2315 train_time:100938ms step_avg:60.55ms
step:1668/2315 train_time:101000ms step_avg:60.55ms
step:1669/2315 train_time:101062ms step_avg:60.55ms
step:1670/2315 train_time:101123ms step_avg:60.55ms
step:1671/2315 train_time:101184ms step_avg:60.55ms
step:1672/2315 train_time:101245ms step_avg:60.55ms
step:1673/2315 train_time:101307ms step_avg:60.55ms
step:1674/2315 train_time:101368ms step_avg:60.55ms
step:1675/2315 train_time:101429ms step_avg:60.55ms
step:1676/2315 train_time:101490ms step_avg:60.55ms
step:1677/2315 train_time:101552ms step_avg:60.56ms
step:1678/2315 train_time:101613ms step_avg:60.56ms
step:1679/2315 train_time:101674ms step_avg:60.56ms
step:1680/2315 train_time:101735ms step_avg:60.56ms
step:1681/2315 train_time:101797ms step_avg:60.56ms
step:1682/2315 train_time:101858ms step_avg:60.56ms
step:1683/2315 train_time:101919ms step_avg:60.56ms
step:1684/2315 train_time:101980ms step_avg:60.56ms
step:1685/2315 train_time:102042ms step_avg:60.56ms
step:1686/2315 train_time:102103ms step_avg:60.56ms
step:1687/2315 train_time:102164ms step_avg:60.56ms
step:1688/2315 train_time:102224ms step_avg:60.56ms
step:1689/2315 train_time:102286ms step_avg:60.56ms
step:1690/2315 train_time:102347ms step_avg:60.56ms
step:1691/2315 train_time:102408ms step_avg:60.56ms
step:1692/2315 train_time:102470ms step_avg:60.56ms
step:1693/2315 train_time:102531ms step_avg:60.56ms
step:1694/2315 train_time:102592ms step_avg:60.56ms
step:1695/2315 train_time:102653ms step_avg:60.56ms
step:1696/2315 train_time:102714ms step_avg:60.56ms
step:1697/2315 train_time:102775ms step_avg:60.56ms
step:1698/2315 train_time:102836ms step_avg:60.56ms
step:1699/2315 train_time:102897ms step_avg:60.56ms
step:1700/2315 train_time:102959ms step_avg:60.56ms
step:1701/2315 train_time:103021ms step_avg:60.56ms
step:1702/2315 train_time:103083ms step_avg:60.57ms
step:1703/2315 train_time:103144ms step_avg:60.57ms
step:1704/2315 train_time:103205ms step_avg:60.57ms
step:1705/2315 train_time:103267ms step_avg:60.57ms
step:1706/2315 train_time:103328ms step_avg:60.57ms
step:1707/2315 train_time:103389ms step_avg:60.57ms
step:1708/2315 train_time:103450ms step_avg:60.57ms
step:1709/2315 train_time:103512ms step_avg:60.57ms
step:1710/2315 train_time:103573ms step_avg:60.57ms
step:1711/2315 train_time:103635ms step_avg:60.57ms
step:1712/2315 train_time:103696ms step_avg:60.57ms
step:1713/2315 train_time:103757ms step_avg:60.57ms
step:1714/2315 train_time:103818ms step_avg:60.57ms
step:1715/2315 train_time:103880ms step_avg:60.57ms
step:1716/2315 train_time:103941ms step_avg:60.57ms
step:1717/2315 train_time:104003ms step_avg:60.57ms
step:1718/2315 train_time:104063ms step_avg:60.57ms
step:1719/2315 train_time:104125ms step_avg:60.57ms
step:1720/2315 train_time:104186ms step_avg:60.57ms
step:1721/2315 train_time:104247ms step_avg:60.57ms
step:1722/2315 train_time:104308ms step_avg:60.57ms
step:1723/2315 train_time:104369ms step_avg:60.57ms
step:1724/2315 train_time:104430ms step_avg:60.57ms
step:1725/2315 train_time:104491ms step_avg:60.57ms
step:1726/2315 train_time:104552ms step_avg:60.57ms
step:1727/2315 train_time:104614ms step_avg:60.58ms
step:1728/2315 train_time:104675ms step_avg:60.58ms
step:1729/2315 train_time:104736ms step_avg:60.58ms
step:1730/2315 train_time:104798ms step_avg:60.58ms
step:1731/2315 train_time:104859ms step_avg:60.58ms
step:1732/2315 train_time:104920ms step_avg:60.58ms
step:1733/2315 train_time:104981ms step_avg:60.58ms
step:1734/2315 train_time:105042ms step_avg:60.58ms
step:1735/2315 train_time:105103ms step_avg:60.58ms
step:1736/2315 train_time:105164ms step_avg:60.58ms
step:1737/2315 train_time:105226ms step_avg:60.58ms
step:1738/2315 train_time:105287ms step_avg:60.58ms
step:1739/2315 train_time:105348ms step_avg:60.58ms
step:1740/2315 train_time:105409ms step_avg:60.58ms
step:1741/2315 train_time:105471ms step_avg:60.58ms
step:1742/2315 train_time:105532ms step_avg:60.58ms
step:1743/2315 train_time:105593ms step_avg:60.58ms
step:1744/2315 train_time:105654ms step_avg:60.58ms
step:1745/2315 train_time:105715ms step_avg:60.58ms
step:1746/2315 train_time:105776ms step_avg:60.58ms
step:1747/2315 train_time:105836ms step_avg:60.58ms
step:1748/2315 train_time:105898ms step_avg:60.58ms
step:1749/2315 train_time:105959ms step_avg:60.58ms
step:1750/2315 train_time:106020ms step_avg:60.58ms
step:1750/2315 val_loss:3.3780 train_time:106083ms step_avg:60.62ms
step:1751/2315 train_time:106104ms step_avg:60.60ms
step:1752/2315 train_time:106143ms step_avg:60.58ms
step:1753/2315 train_time:106210ms step_avg:60.59ms
step:1754/2315 train_time:106276ms step_avg:60.59ms
step:1755/2315 train_time:106336ms step_avg:60.59ms
step:1756/2315 train_time:106397ms step_avg:60.59ms
step:1757/2315 train_time:106458ms step_avg:60.59ms
step:1758/2315 train_time:106519ms step_avg:60.59ms
step:1759/2315 train_time:106580ms step_avg:60.59ms
step:1760/2315 train_time:106640ms step_avg:60.59ms
step:1761/2315 train_time:106701ms step_avg:60.59ms
step:1762/2315 train_time:106761ms step_avg:60.59ms
step:1763/2315 train_time:106822ms step_avg:60.59ms
step:1764/2315 train_time:106882ms step_avg:60.59ms
step:1765/2315 train_time:106943ms step_avg:60.59ms
step:1766/2315 train_time:107007ms step_avg:60.59ms
step:1767/2315 train_time:107070ms step_avg:60.59ms
step:1768/2315 train_time:107132ms step_avg:60.60ms
step:1769/2315 train_time:107196ms step_avg:60.60ms
step:1770/2315 train_time:107257ms step_avg:60.60ms
step:1771/2315 train_time:107319ms step_avg:60.60ms
step:1772/2315 train_time:107380ms step_avg:60.60ms
step:1773/2315 train_time:107441ms step_avg:60.60ms
step:1774/2315 train_time:107502ms step_avg:60.60ms
step:1775/2315 train_time:107563ms step_avg:60.60ms
step:1776/2315 train_time:107623ms step_avg:60.60ms
step:1777/2315 train_time:107684ms step_avg:60.60ms
step:1778/2315 train_time:107745ms step_avg:60.60ms
step:1779/2315 train_time:107806ms step_avg:60.60ms
step:1780/2315 train_time:107867ms step_avg:60.60ms
step:1781/2315 train_time:107927ms step_avg:60.60ms
step:1782/2315 train_time:107989ms step_avg:60.60ms
step:1783/2315 train_time:108050ms step_avg:60.60ms
step:1784/2315 train_time:108112ms step_avg:60.60ms
step:1785/2315 train_time:108175ms step_avg:60.60ms
step:1786/2315 train_time:108237ms step_avg:60.60ms
step:1787/2315 train_time:108298ms step_avg:60.60ms
step:1788/2315 train_time:108359ms step_avg:60.60ms
step:1789/2315 train_time:108421ms step_avg:60.60ms
step:1790/2315 train_time:108482ms step_avg:60.60ms
step:1791/2315 train_time:108543ms step_avg:60.60ms
step:1792/2315 train_time:108604ms step_avg:60.60ms
step:1793/2315 train_time:108665ms step_avg:60.60ms
step:1794/2315 train_time:108725ms step_avg:60.60ms
step:1795/2315 train_time:108786ms step_avg:60.61ms
step:1796/2315 train_time:108847ms step_avg:60.61ms
step:1797/2315 train_time:108908ms step_avg:60.61ms
step:1798/2315 train_time:108969ms step_avg:60.61ms
step:1799/2315 train_time:109030ms step_avg:60.61ms
step:1800/2315 train_time:109091ms step_avg:60.61ms
step:1801/2315 train_time:109153ms step_avg:60.61ms
step:1802/2315 train_time:109214ms step_avg:60.61ms
step:1803/2315 train_time:109275ms step_avg:60.61ms
step:1804/2315 train_time:109337ms step_avg:60.61ms
step:1805/2315 train_time:109398ms step_avg:60.61ms
step:1806/2315 train_time:109459ms step_avg:60.61ms
step:1807/2315 train_time:109520ms step_avg:60.61ms
step:1808/2315 train_time:109581ms step_avg:60.61ms
step:1809/2315 train_time:109642ms step_avg:60.61ms
step:1810/2315 train_time:109703ms step_avg:60.61ms
step:1811/2315 train_time:109765ms step_avg:60.61ms
step:1812/2315 train_time:109826ms step_avg:60.61ms
step:1813/2315 train_time:109887ms step_avg:60.61ms
step:1814/2315 train_time:109948ms step_avg:60.61ms
step:1815/2315 train_time:110010ms step_avg:60.61ms
step:1816/2315 train_time:110071ms step_avg:60.61ms
step:1817/2315 train_time:110132ms step_avg:60.61ms
step:1818/2315 train_time:110193ms step_avg:60.61ms
step:1819/2315 train_time:110255ms step_avg:60.61ms
step:1820/2315 train_time:110316ms step_avg:60.61ms
step:1821/2315 train_time:110377ms step_avg:60.61ms
step:1822/2315 train_time:110437ms step_avg:60.61ms
step:1823/2315 train_time:110499ms step_avg:60.61ms
step:1824/2315 train_time:110560ms step_avg:60.61ms
step:1825/2315 train_time:110621ms step_avg:60.61ms
step:1826/2315 train_time:110682ms step_avg:60.61ms
step:1827/2315 train_time:110743ms step_avg:60.61ms
step:1828/2315 train_time:110804ms step_avg:60.61ms
step:1829/2315 train_time:110865ms step_avg:60.62ms
step:1830/2315 train_time:110926ms step_avg:60.62ms
step:1831/2315 train_time:110987ms step_avg:60.62ms
step:1832/2315 train_time:111049ms step_avg:60.62ms
step:1833/2315 train_time:111111ms step_avg:60.62ms
step:1834/2315 train_time:111172ms step_avg:60.62ms
step:1835/2315 train_time:111233ms step_avg:60.62ms
step:1836/2315 train_time:111295ms step_avg:60.62ms
step:1837/2315 train_time:111355ms step_avg:60.62ms
step:1838/2315 train_time:111417ms step_avg:60.62ms
step:1839/2315 train_time:111478ms step_avg:60.62ms
step:1840/2315 train_time:111538ms step_avg:60.62ms
step:1841/2315 train_time:111599ms step_avg:60.62ms
step:1842/2315 train_time:111661ms step_avg:60.62ms
step:1843/2315 train_time:111723ms step_avg:60.62ms
step:1844/2315 train_time:111784ms step_avg:60.62ms
step:1845/2315 train_time:111845ms step_avg:60.62ms
step:1846/2315 train_time:111906ms step_avg:60.62ms
step:1847/2315 train_time:111968ms step_avg:60.62ms
step:1848/2315 train_time:112029ms step_avg:60.62ms
step:1849/2315 train_time:112090ms step_avg:60.62ms
step:1850/2315 train_time:112151ms step_avg:60.62ms
step:1851/2315 train_time:112213ms step_avg:60.62ms
step:1852/2315 train_time:112274ms step_avg:60.62ms
step:1853/2315 train_time:112335ms step_avg:60.62ms
step:1854/2315 train_time:112397ms step_avg:60.62ms
step:1855/2315 train_time:112458ms step_avg:60.62ms
step:1856/2315 train_time:112518ms step_avg:60.62ms
step:1857/2315 train_time:112580ms step_avg:60.62ms
step:1858/2315 train_time:112641ms step_avg:60.62ms
step:1859/2315 train_time:112702ms step_avg:60.62ms
step:1860/2315 train_time:112762ms step_avg:60.62ms
step:1861/2315 train_time:112824ms step_avg:60.63ms
step:1862/2315 train_time:112885ms step_avg:60.63ms
step:1863/2315 train_time:112946ms step_avg:60.63ms
step:1864/2315 train_time:113007ms step_avg:60.63ms
step:1865/2315 train_time:113070ms step_avg:60.63ms
step:1866/2315 train_time:113131ms step_avg:60.63ms
step:1867/2315 train_time:113192ms step_avg:60.63ms
step:1868/2315 train_time:113253ms step_avg:60.63ms
step:1869/2315 train_time:113315ms step_avg:60.63ms
step:1870/2315 train_time:113376ms step_avg:60.63ms
step:1871/2315 train_time:113437ms step_avg:60.63ms
step:1872/2315 train_time:113498ms step_avg:60.63ms
step:1873/2315 train_time:113559ms step_avg:60.63ms
step:1874/2315 train_time:113620ms step_avg:60.63ms
step:1875/2315 train_time:113681ms step_avg:60.63ms
step:1876/2315 train_time:113742ms step_avg:60.63ms
step:1877/2315 train_time:113804ms step_avg:60.63ms
step:1878/2315 train_time:113865ms step_avg:60.63ms
step:1879/2315 train_time:113927ms step_avg:60.63ms
step:1880/2315 train_time:113988ms step_avg:60.63ms
step:1881/2315 train_time:114050ms step_avg:60.63ms
step:1882/2315 train_time:114111ms step_avg:60.63ms
step:1883/2315 train_time:114172ms step_avg:60.63ms
step:1884/2315 train_time:114234ms step_avg:60.63ms
step:1885/2315 train_time:114295ms step_avg:60.63ms
step:1886/2315 train_time:114356ms step_avg:60.63ms
step:1887/2315 train_time:114417ms step_avg:60.63ms
step:1888/2315 train_time:114478ms step_avg:60.63ms
step:1889/2315 train_time:114540ms step_avg:60.64ms
step:1890/2315 train_time:114601ms step_avg:60.64ms
step:1891/2315 train_time:114662ms step_avg:60.64ms
step:1892/2315 train_time:114723ms step_avg:60.64ms
step:1893/2315 train_time:114784ms step_avg:60.64ms
step:1894/2315 train_time:114845ms step_avg:60.64ms
step:1895/2315 train_time:114907ms step_avg:60.64ms
step:1896/2315 train_time:114968ms step_avg:60.64ms
step:1897/2315 train_time:115029ms step_avg:60.64ms
step:1898/2315 train_time:115091ms step_avg:60.64ms
step:1899/2315 train_time:115152ms step_avg:60.64ms
step:1900/2315 train_time:115213ms step_avg:60.64ms
step:1901/2315 train_time:115275ms step_avg:60.64ms
step:1902/2315 train_time:115336ms step_avg:60.64ms
step:1903/2315 train_time:115397ms step_avg:60.64ms
step:1904/2315 train_time:115458ms step_avg:60.64ms
step:1905/2315 train_time:115518ms step_avg:60.64ms
step:1906/2315 train_time:115579ms step_avg:60.64ms
step:1907/2315 train_time:115640ms step_avg:60.64ms
step:1908/2315 train_time:115701ms step_avg:60.64ms
step:1909/2315 train_time:115764ms step_avg:60.64ms
step:1910/2315 train_time:115825ms step_avg:60.64ms
step:1911/2315 train_time:115886ms step_avg:60.64ms
step:1912/2315 train_time:115948ms step_avg:60.64ms
step:1913/2315 train_time:116009ms step_avg:60.64ms
step:1914/2315 train_time:116070ms step_avg:60.64ms
step:1915/2315 train_time:116132ms step_avg:60.64ms
step:1916/2315 train_time:116193ms step_avg:60.64ms
step:1917/2315 train_time:116254ms step_avg:60.64ms
step:1918/2315 train_time:116315ms step_avg:60.64ms
step:1919/2315 train_time:116377ms step_avg:60.64ms
step:1920/2315 train_time:116437ms step_avg:60.64ms
step:1921/2315 train_time:116498ms step_avg:60.64ms
step:1922/2315 train_time:116560ms step_avg:60.64ms
step:1923/2315 train_time:116622ms step_avg:60.65ms
step:1924/2315 train_time:116683ms step_avg:60.65ms
step:1925/2315 train_time:116744ms step_avg:60.65ms
step:1926/2315 train_time:116805ms step_avg:60.65ms
step:1927/2315 train_time:116867ms step_avg:60.65ms
step:1928/2315 train_time:116927ms step_avg:60.65ms
step:1929/2315 train_time:116989ms step_avg:60.65ms
step:1930/2315 train_time:117050ms step_avg:60.65ms
step:1931/2315 train_time:117111ms step_avg:60.65ms
step:1932/2315 train_time:117172ms step_avg:60.65ms
step:1933/2315 train_time:117234ms step_avg:60.65ms
step:1934/2315 train_time:117295ms step_avg:60.65ms
step:1935/2315 train_time:117355ms step_avg:60.65ms
step:1936/2315 train_time:117417ms step_avg:60.65ms
step:1937/2315 train_time:117478ms step_avg:60.65ms
step:1938/2315 train_time:117539ms step_avg:60.65ms
step:1939/2315 train_time:117600ms step_avg:60.65ms
step:1940/2315 train_time:117661ms step_avg:60.65ms
step:1941/2315 train_time:117722ms step_avg:60.65ms
step:1942/2315 train_time:117784ms step_avg:60.65ms
step:1943/2315 train_time:117845ms step_avg:60.65ms
step:1944/2315 train_time:117906ms step_avg:60.65ms
step:1945/2315 train_time:117967ms step_avg:60.65ms
step:1946/2315 train_time:118028ms step_avg:60.65ms
step:1947/2315 train_time:118091ms step_avg:60.65ms
step:1948/2315 train_time:118152ms step_avg:60.65ms
step:1949/2315 train_time:118213ms step_avg:60.65ms
step:1950/2315 train_time:118274ms step_avg:60.65ms
step:1951/2315 train_time:118335ms step_avg:60.65ms
step:1952/2315 train_time:118396ms step_avg:60.65ms
step:1953/2315 train_time:118457ms step_avg:60.65ms
step:1954/2315 train_time:118519ms step_avg:60.65ms
step:1955/2315 train_time:118580ms step_avg:60.65ms
step:1956/2315 train_time:118641ms step_avg:60.65ms
step:1957/2315 train_time:118703ms step_avg:60.66ms
step:1958/2315 train_time:118764ms step_avg:60.66ms
step:1959/2315 train_time:118826ms step_avg:60.66ms
step:1960/2315 train_time:118886ms step_avg:60.66ms
step:1961/2315 train_time:118948ms step_avg:60.66ms
step:1962/2315 train_time:119009ms step_avg:60.66ms
step:1963/2315 train_time:119070ms step_avg:60.66ms
step:1964/2315 train_time:119131ms step_avg:60.66ms
step:1965/2315 train_time:119193ms step_avg:60.66ms
step:1966/2315 train_time:119254ms step_avg:60.66ms
step:1967/2315 train_time:119315ms step_avg:60.66ms
step:1968/2315 train_time:119376ms step_avg:60.66ms
step:1969/2315 train_time:119438ms step_avg:60.66ms
step:1970/2315 train_time:119499ms step_avg:60.66ms
step:1971/2315 train_time:119560ms step_avg:60.66ms
step:1972/2315 train_time:119621ms step_avg:60.66ms
step:1973/2315 train_time:119683ms step_avg:60.66ms
step:1974/2315 train_time:119743ms step_avg:60.66ms
step:1975/2315 train_time:119804ms step_avg:60.66ms
step:1976/2315 train_time:119866ms step_avg:60.66ms
step:1977/2315 train_time:119927ms step_avg:60.66ms
step:1978/2315 train_time:119988ms step_avg:60.66ms
step:1979/2315 train_time:120049ms step_avg:60.66ms
step:1980/2315 train_time:120110ms step_avg:60.66ms
step:1981/2315 train_time:120172ms step_avg:60.66ms
step:1982/2315 train_time:120233ms step_avg:60.66ms
step:1983/2315 train_time:120294ms step_avg:60.66ms
step:1984/2315 train_time:120355ms step_avg:60.66ms
step:1985/2315 train_time:120416ms step_avg:60.66ms
step:1986/2315 train_time:120477ms step_avg:60.66ms
step:1987/2315 train_time:120539ms step_avg:60.66ms
step:1988/2315 train_time:120600ms step_avg:60.66ms
step:1989/2315 train_time:120662ms step_avg:60.66ms
step:1990/2315 train_time:120722ms step_avg:60.66ms
step:1991/2315 train_time:120784ms step_avg:60.66ms
step:1992/2315 train_time:120845ms step_avg:60.66ms
step:1993/2315 train_time:120906ms step_avg:60.67ms
step:1994/2315 train_time:120968ms step_avg:60.67ms
step:1995/2315 train_time:121029ms step_avg:60.67ms
step:1996/2315 train_time:121091ms step_avg:60.67ms
step:1997/2315 train_time:121152ms step_avg:60.67ms
step:1998/2315 train_time:121213ms step_avg:60.67ms
step:1999/2315 train_time:121275ms step_avg:60.67ms
step:2000/2315 train_time:121336ms step_avg:60.67ms
step:2000/2315 val_loss:3.3289 train_time:121399ms step_avg:60.70ms
step:2001/2315 train_time:121419ms step_avg:60.68ms
step:2002/2315 train_time:121461ms step_avg:60.67ms
step:2003/2315 train_time:121528ms step_avg:60.67ms
step:2004/2315 train_time:121591ms step_avg:60.67ms
step:2005/2315 train_time:121653ms step_avg:60.67ms
step:2006/2315 train_time:121715ms step_avg:60.68ms
step:2007/2315 train_time:121775ms step_avg:60.68ms
step:2008/2315 train_time:121835ms step_avg:60.67ms
step:2009/2315 train_time:121896ms step_avg:60.68ms
step:2010/2315 train_time:121957ms step_avg:60.68ms
step:2011/2315 train_time:122018ms step_avg:60.68ms
step:2012/2315 train_time:122078ms step_avg:60.67ms
step:2013/2315 train_time:122138ms step_avg:60.67ms
step:2014/2315 train_time:122199ms step_avg:60.67ms
step:2015/2315 train_time:122259ms step_avg:60.67ms
step:2016/2315 train_time:122320ms step_avg:60.67ms
step:2017/2315 train_time:122383ms step_avg:60.68ms
step:2018/2315 train_time:122446ms step_avg:60.68ms
step:2019/2315 train_time:122510ms step_avg:60.68ms
step:2020/2315 train_time:122572ms step_avg:60.68ms
step:2021/2315 train_time:122634ms step_avg:60.68ms
step:2022/2315 train_time:122695ms step_avg:60.68ms
step:2023/2315 train_time:122757ms step_avg:60.68ms
step:2024/2315 train_time:122817ms step_avg:60.68ms
step:2025/2315 train_time:122878ms step_avg:60.68ms
step:2026/2315 train_time:122938ms step_avg:60.68ms
step:2027/2315 train_time:122999ms step_avg:60.68ms
step:2028/2315 train_time:123060ms step_avg:60.68ms
step:2029/2315 train_time:123120ms step_avg:60.68ms
step:2030/2315 train_time:123181ms step_avg:60.68ms
step:2031/2315 train_time:123241ms step_avg:60.68ms
step:2032/2315 train_time:123302ms step_avg:60.68ms
step:2033/2315 train_time:123363ms step_avg:60.68ms
step:2034/2315 train_time:123425ms step_avg:60.68ms
step:2035/2315 train_time:123488ms step_avg:60.68ms
step:2036/2315 train_time:123549ms step_avg:60.68ms
step:2037/2315 train_time:123610ms step_avg:60.68ms
step:2038/2315 train_time:123672ms step_avg:60.68ms
step:2039/2315 train_time:123733ms step_avg:60.68ms
step:2040/2315 train_time:123794ms step_avg:60.68ms
step:2041/2315 train_time:123855ms step_avg:60.68ms
step:2042/2315 train_time:123916ms step_avg:60.68ms
step:2043/2315 train_time:123977ms step_avg:60.68ms
step:2044/2315 train_time:124038ms step_avg:60.68ms
step:2045/2315 train_time:124099ms step_avg:60.68ms
step:2046/2315 train_time:124160ms step_avg:60.68ms
step:2047/2315 train_time:124221ms step_avg:60.68ms
step:2048/2315 train_time:124281ms step_avg:60.68ms
step:2049/2315 train_time:124343ms step_avg:60.68ms
step:2050/2315 train_time:124405ms step_avg:60.69ms
step:2051/2315 train_time:124467ms step_avg:60.69ms
step:2052/2315 train_time:124528ms step_avg:60.69ms
step:2053/2315 train_time:124590ms step_avg:60.69ms
step:2054/2315 train_time:124651ms step_avg:60.69ms
step:2055/2315 train_time:124713ms step_avg:60.69ms
step:2056/2315 train_time:124774ms step_avg:60.69ms
step:2057/2315 train_time:124835ms step_avg:60.69ms
step:2058/2315 train_time:124896ms step_avg:60.69ms
step:2059/2315 train_time:124957ms step_avg:60.69ms
step:2060/2315 train_time:125018ms step_avg:60.69ms
step:2061/2315 train_time:125079ms step_avg:60.69ms
step:2062/2315 train_time:125140ms step_avg:60.69ms
step:2063/2315 train_time:125200ms step_avg:60.69ms
step:2064/2315 train_time:125261ms step_avg:60.69ms
step:2065/2315 train_time:125323ms step_avg:60.69ms
step:2066/2315 train_time:125384ms step_avg:60.69ms
step:2067/2315 train_time:125446ms step_avg:60.69ms
step:2068/2315 train_time:125508ms step_avg:60.69ms
step:2069/2315 train_time:125569ms step_avg:60.69ms
step:2070/2315 train_time:125631ms step_avg:60.69ms
step:2071/2315 train_time:125693ms step_avg:60.69ms
step:2072/2315 train_time:125754ms step_avg:60.69ms
step:2073/2315 train_time:125815ms step_avg:60.69ms
step:2074/2315 train_time:125876ms step_avg:60.69ms
step:2075/2315 train_time:125937ms step_avg:60.69ms
step:2076/2315 train_time:125998ms step_avg:60.69ms
step:2077/2315 train_time:126059ms step_avg:60.69ms
step:2078/2315 train_time:126120ms step_avg:60.69ms
step:2079/2315 train_time:126181ms step_avg:60.69ms
step:2080/2315 train_time:126241ms step_avg:60.69ms
step:2081/2315 train_time:126303ms step_avg:60.69ms
step:2082/2315 train_time:126364ms step_avg:60.69ms
step:2083/2315 train_time:126425ms step_avg:60.69ms
step:2084/2315 train_time:126486ms step_avg:60.69ms
step:2085/2315 train_time:126548ms step_avg:60.69ms
step:2086/2315 train_time:126609ms step_avg:60.69ms
step:2087/2315 train_time:126670ms step_avg:60.69ms
step:2088/2315 train_time:126731ms step_avg:60.70ms
step:2089/2315 train_time:126793ms step_avg:60.70ms
step:2090/2315 train_time:126854ms step_avg:60.70ms
step:2091/2315 train_time:126916ms step_avg:60.70ms
step:2092/2315 train_time:126977ms step_avg:60.70ms
step:2093/2315 train_time:127038ms step_avg:60.70ms
step:2094/2315 train_time:127099ms step_avg:60.70ms
step:2095/2315 train_time:127160ms step_avg:60.70ms
step:2096/2315 train_time:127221ms step_avg:60.70ms
step:2097/2315 train_time:127283ms step_avg:60.70ms
step:2098/2315 train_time:127344ms step_avg:60.70ms
step:2099/2315 train_time:127405ms step_avg:60.70ms
step:2100/2315 train_time:127466ms step_avg:60.70ms
step:2101/2315 train_time:127528ms step_avg:60.70ms
step:2102/2315 train_time:127589ms step_avg:60.70ms
step:2103/2315 train_time:127650ms step_avg:60.70ms
step:2104/2315 train_time:127712ms step_avg:60.70ms
step:2105/2315 train_time:127774ms step_avg:60.70ms
step:2106/2315 train_time:127834ms step_avg:60.70ms
step:2107/2315 train_time:127896ms step_avg:60.70ms
step:2108/2315 train_time:127957ms step_avg:60.70ms
step:2109/2315 train_time:128018ms step_avg:60.70ms
step:2110/2315 train_time:128079ms step_avg:60.70ms
step:2111/2315 train_time:128140ms step_avg:60.70ms
step:2112/2315 train_time:128201ms step_avg:60.70ms
step:2113/2315 train_time:128262ms step_avg:60.70ms
step:2114/2315 train_time:128324ms step_avg:60.70ms
step:2115/2315 train_time:128385ms step_avg:60.70ms
step:2116/2315 train_time:128446ms step_avg:60.70ms
step:2117/2315 train_time:128508ms step_avg:60.70ms
step:2118/2315 train_time:128568ms step_avg:60.70ms
step:2119/2315 train_time:128630ms step_avg:60.70ms
step:2120/2315 train_time:128691ms step_avg:60.70ms
step:2121/2315 train_time:128753ms step_avg:60.70ms
step:2122/2315 train_time:128814ms step_avg:60.70ms
step:2123/2315 train_time:128876ms step_avg:60.70ms
step:2124/2315 train_time:128937ms step_avg:60.70ms
step:2125/2315 train_time:128998ms step_avg:60.70ms
step:2126/2315 train_time:129059ms step_avg:60.70ms
step:2127/2315 train_time:129121ms step_avg:60.71ms
step:2128/2315 train_time:129181ms step_avg:60.71ms
step:2129/2315 train_time:129243ms step_avg:60.71ms
step:2130/2315 train_time:129304ms step_avg:60.71ms
step:2131/2315 train_time:129365ms step_avg:60.71ms
step:2132/2315 train_time:129426ms step_avg:60.71ms
step:2133/2315 train_time:129488ms step_avg:60.71ms
step:2134/2315 train_time:129548ms step_avg:60.71ms
step:2135/2315 train_time:129609ms step_avg:60.71ms
step:2136/2315 train_time:129670ms step_avg:60.71ms
step:2137/2315 train_time:129732ms step_avg:60.71ms
step:2138/2315 train_time:129793ms step_avg:60.71ms
step:2139/2315 train_time:129855ms step_avg:60.71ms
step:2140/2315 train_time:129916ms step_avg:60.71ms
step:2141/2315 train_time:129978ms step_avg:60.71ms
step:2142/2315 train_time:130039ms step_avg:60.71ms
step:2143/2315 train_time:130101ms step_avg:60.71ms
step:2144/2315 train_time:130162ms step_avg:60.71ms
step:2145/2315 train_time:130223ms step_avg:60.71ms
step:2146/2315 train_time:130284ms step_avg:60.71ms
step:2147/2315 train_time:130345ms step_avg:60.71ms
step:2148/2315 train_time:130406ms step_avg:60.71ms
step:2149/2315 train_time:130468ms step_avg:60.71ms
step:2150/2315 train_time:130528ms step_avg:60.71ms
step:2151/2315 train_time:130589ms step_avg:60.71ms
step:2152/2315 train_time:130650ms step_avg:60.71ms
step:2153/2315 train_time:130712ms step_avg:60.71ms
step:2154/2315 train_time:130774ms step_avg:60.71ms
step:2155/2315 train_time:130836ms step_avg:60.71ms
step:2156/2315 train_time:130897ms step_avg:60.71ms
step:2157/2315 train_time:130958ms step_avg:60.71ms
step:2158/2315 train_time:131019ms step_avg:60.71ms
step:2159/2315 train_time:131080ms step_avg:60.71ms
step:2160/2315 train_time:131141ms step_avg:60.71ms
step:2161/2315 train_time:131203ms step_avg:60.71ms
step:2162/2315 train_time:131264ms step_avg:60.71ms
step:2163/2315 train_time:131325ms step_avg:60.71ms
step:2164/2315 train_time:131386ms step_avg:60.71ms
step:2165/2315 train_time:131448ms step_avg:60.71ms
step:2166/2315 train_time:131508ms step_avg:60.71ms
step:2167/2315 train_time:131570ms step_avg:60.72ms
step:2168/2315 train_time:131631ms step_avg:60.72ms
step:2169/2315 train_time:131692ms step_avg:60.72ms
step:2170/2315 train_time:131754ms step_avg:60.72ms
step:2171/2315 train_time:131815ms step_avg:60.72ms
step:2172/2315 train_time:131877ms step_avg:60.72ms
step:2173/2315 train_time:131938ms step_avg:60.72ms
step:2174/2315 train_time:131999ms step_avg:60.72ms
step:2175/2315 train_time:132060ms step_avg:60.72ms
step:2176/2315 train_time:132120ms step_avg:60.72ms
step:2177/2315 train_time:132181ms step_avg:60.72ms
step:2178/2315 train_time:132242ms step_avg:60.72ms
step:2179/2315 train_time:132304ms step_avg:60.72ms
step:2180/2315 train_time:132365ms step_avg:60.72ms
step:2181/2315 train_time:132426ms step_avg:60.72ms
step:2182/2315 train_time:132487ms step_avg:60.72ms
step:2183/2315 train_time:132548ms step_avg:60.72ms
step:2184/2315 train_time:132610ms step_avg:60.72ms
step:2185/2315 train_time:132671ms step_avg:60.72ms
step:2186/2315 train_time:132733ms step_avg:60.72ms
step:2187/2315 train_time:132795ms step_avg:60.72ms
step:2188/2315 train_time:132856ms step_avg:60.72ms
step:2189/2315 train_time:132917ms step_avg:60.72ms
step:2190/2315 train_time:132978ms step_avg:60.72ms
step:2191/2315 train_time:133039ms step_avg:60.72ms
step:2192/2315 train_time:133100ms step_avg:60.72ms
step:2193/2315 train_time:133162ms step_avg:60.72ms
step:2194/2315 train_time:133223ms step_avg:60.72ms
step:2195/2315 train_time:133284ms step_avg:60.72ms
step:2196/2315 train_time:133345ms step_avg:60.72ms
step:2197/2315 train_time:133406ms step_avg:60.72ms
step:2198/2315 train_time:133467ms step_avg:60.72ms
step:2199/2315 train_time:133528ms step_avg:60.72ms
step:2200/2315 train_time:133589ms step_avg:60.72ms
step:2201/2315 train_time:133651ms step_avg:60.72ms
step:2202/2315 train_time:133712ms step_avg:60.72ms
step:2203/2315 train_time:133774ms step_avg:60.72ms
step:2204/2315 train_time:133836ms step_avg:60.72ms
step:2205/2315 train_time:133897ms step_avg:60.72ms
step:2206/2315 train_time:133958ms step_avg:60.72ms
step:2207/2315 train_time:134020ms step_avg:60.72ms
step:2208/2315 train_time:134081ms step_avg:60.72ms
step:2209/2315 train_time:134142ms step_avg:60.73ms
step:2210/2315 train_time:134202ms step_avg:60.73ms
step:2211/2315 train_time:134263ms step_avg:60.73ms
step:2212/2315 train_time:134325ms step_avg:60.73ms
step:2213/2315 train_time:134386ms step_avg:60.73ms
step:2214/2315 train_time:134446ms step_avg:60.73ms
step:2215/2315 train_time:134508ms step_avg:60.73ms
step:2216/2315 train_time:134570ms step_avg:60.73ms
step:2217/2315 train_time:134631ms step_avg:60.73ms
step:2218/2315 train_time:134692ms step_avg:60.73ms
step:2219/2315 train_time:134754ms step_avg:60.73ms
step:2220/2315 train_time:134816ms step_avg:60.73ms
step:2221/2315 train_time:134877ms step_avg:60.73ms
step:2222/2315 train_time:134938ms step_avg:60.73ms
step:2223/2315 train_time:135000ms step_avg:60.73ms
step:2224/2315 train_time:135061ms step_avg:60.73ms
step:2225/2315 train_time:135122ms step_avg:60.73ms
step:2226/2315 train_time:135183ms step_avg:60.73ms
step:2227/2315 train_time:135244ms step_avg:60.73ms
step:2228/2315 train_time:135305ms step_avg:60.73ms
step:2229/2315 train_time:135366ms step_avg:60.73ms
step:2230/2315 train_time:135427ms step_avg:60.73ms
step:2231/2315 train_time:135489ms step_avg:60.73ms
step:2232/2315 train_time:135549ms step_avg:60.73ms
step:2233/2315 train_time:135611ms step_avg:60.73ms
step:2234/2315 train_time:135672ms step_avg:60.73ms
step:2235/2315 train_time:135734ms step_avg:60.73ms
step:2236/2315 train_time:135795ms step_avg:60.73ms
step:2237/2315 train_time:135857ms step_avg:60.73ms
step:2238/2315 train_time:135918ms step_avg:60.73ms
step:2239/2315 train_time:135979ms step_avg:60.73ms
step:2240/2315 train_time:136040ms step_avg:60.73ms
step:2241/2315 train_time:136101ms step_avg:60.73ms
step:2242/2315 train_time:136162ms step_avg:60.73ms
step:2243/2315 train_time:136223ms step_avg:60.73ms
step:2244/2315 train_time:136284ms step_avg:60.73ms
step:2245/2315 train_time:136345ms step_avg:60.73ms
step:2246/2315 train_time:136406ms step_avg:60.73ms
step:2247/2315 train_time:136468ms step_avg:60.73ms
step:2248/2315 train_time:136529ms step_avg:60.73ms
step:2249/2315 train_time:136591ms step_avg:60.73ms
step:2250/2315 train_time:136652ms step_avg:60.73ms
step:2250/2315 val_loss:3.2891 train_time:136715ms step_avg:60.76ms
step:2251/2315 train_time:136735ms step_avg:60.74ms
step:2252/2315 train_time:136777ms step_avg:60.74ms
step:2253/2315 train_time:136843ms step_avg:60.74ms
step:2254/2315 train_time:136906ms step_avg:60.74ms
step:2255/2315 train_time:136968ms step_avg:60.74ms
step:2256/2315 train_time:137029ms step_avg:60.74ms
step:2257/2315 train_time:137090ms step_avg:60.74ms
step:2258/2315 train_time:137151ms step_avg:60.74ms
step:2259/2315 train_time:137212ms step_avg:60.74ms
step:2260/2315 train_time:137272ms step_avg:60.74ms
step:2261/2315 train_time:137334ms step_avg:60.74ms
step:2262/2315 train_time:137395ms step_avg:60.74ms
step:2263/2315 train_time:137455ms step_avg:60.74ms
step:2264/2315 train_time:137516ms step_avg:60.74ms
step:2265/2315 train_time:137576ms step_avg:60.74ms
step:2266/2315 train_time:137637ms step_avg:60.74ms
step:2267/2315 train_time:137699ms step_avg:60.74ms
step:2268/2315 train_time:137761ms step_avg:60.74ms
step:2269/2315 train_time:137824ms step_avg:60.74ms
step:2270/2315 train_time:137885ms step_avg:60.74ms
step:2271/2315 train_time:137948ms step_avg:60.74ms
step:2272/2315 train_time:138009ms step_avg:60.74ms
step:2273/2315 train_time:138070ms step_avg:60.74ms
step:2274/2315 train_time:138131ms step_avg:60.74ms
step:2275/2315 train_time:138192ms step_avg:60.74ms
step:2276/2315 train_time:138253ms step_avg:60.74ms
step:2277/2315 train_time:138314ms step_avg:60.74ms
step:2278/2315 train_time:138375ms step_avg:60.74ms
step:2279/2315 train_time:138436ms step_avg:60.74ms
step:2280/2315 train_time:138496ms step_avg:60.74ms
step:2281/2315 train_time:138557ms step_avg:60.74ms
step:2282/2315 train_time:138618ms step_avg:60.74ms
step:2283/2315 train_time:138679ms step_avg:60.74ms
step:2284/2315 train_time:138741ms step_avg:60.74ms
step:2285/2315 train_time:138803ms step_avg:60.75ms
step:2286/2315 train_time:138865ms step_avg:60.75ms
step:2287/2315 train_time:138926ms step_avg:60.75ms
step:2288/2315 train_time:138987ms step_avg:60.75ms
step:2289/2315 train_time:139049ms step_avg:60.75ms
step:2290/2315 train_time:139110ms step_avg:60.75ms
step:2291/2315 train_time:139171ms step_avg:60.75ms
step:2292/2315 train_time:139232ms step_avg:60.75ms
step:2293/2315 train_time:139293ms step_avg:60.75ms
step:2294/2315 train_time:139354ms step_avg:60.75ms
step:2295/2315 train_time:139415ms step_avg:60.75ms
step:2296/2315 train_time:139476ms step_avg:60.75ms
step:2297/2315 train_time:139537ms step_avg:60.75ms
step:2298/2315 train_time:139598ms step_avg:60.75ms
step:2299/2315 train_time:139659ms step_avg:60.75ms
step:2300/2315 train_time:139720ms step_avg:60.75ms
step:2301/2315 train_time:139782ms step_avg:60.75ms
step:2302/2315 train_time:139843ms step_avg:60.75ms
step:2303/2315 train_time:139905ms step_avg:60.75ms
step:2304/2315 train_time:139966ms step_avg:60.75ms
step:2305/2315 train_time:140028ms step_avg:60.75ms
step:2306/2315 train_time:140089ms step_avg:60.75ms
step:2307/2315 train_time:140150ms step_avg:60.75ms
step:2308/2315 train_time:140211ms step_avg:60.75ms
step:2309/2315 train_time:140273ms step_avg:60.75ms
step:2310/2315 train_time:140334ms step_avg:60.75ms
step:2311/2315 train_time:140395ms step_avg:60.75ms
step:2312/2315 train_time:140456ms step_avg:60.75ms
step:2313/2315 train_time:140517ms step_avg:60.75ms
step:2314/2315 train_time:140578ms step_avg:60.75ms
step:2315/2315 train_time:140639ms step_avg:60.75ms
step:2315/2315 val_loss:3.2764 train_time:140701ms step_avg:60.78ms
peak memory allocated: 29512 MiB reserved: 44076 MiB

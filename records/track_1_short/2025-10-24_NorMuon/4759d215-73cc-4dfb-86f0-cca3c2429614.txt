import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]
# polar_express_coeffs = [
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
#     (3.4445, -4.7750,  2.0315),
# ]



@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True, beta2=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    if i==0:
                        second_momentum_buffer = torch.zeros((chunk_size, *p_example.shape[:-1], 1), dtype=torch.float32, device=p_example.device) if p_example.size(-2) >= p_example.size(-1) else torch.zeros((chunk_size, *p_example.shape[:-3], 1, p_example.shape[-1]), device=p_example.device, dtype=torch.float32)
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)
                    # state["second_momentum_buffer"] = torch.zeros_like(grad[...,0:1]) if param.size(-2) >= param.size(-1) else torch.zeros_like(grad[...,0:1,:])
                    # state["second_momentum_buffer"] = torch.zeros((*grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((*grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)
                    if i==0:
                        state["second_momentum_buffer"] = torch.zeros((chunk_size, *grad.shape[:-1], 1), dtype=torch.float32, device=grad.device) if param.size(-2) >= param.size(-1) else torch.zeros((chunk_size, *grad.shape[:-3], 1, grad.shape[-1]), dtype=torch.float32, device=grad.device)

                momentum_buffer = state["momentum_buffer"]
                if i==0:
                    second_momentum_buffer = state["second_momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)
            
            ###################################
            vnorm = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_mean = torch.mean(v_chunk * v_chunk, dim=-1, keepdim=True) if v_chunk.size(-2) >= v_chunk.size(-1) else torch.mean(v_chunk * v_chunk, dim=-2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.float(), 1 - group["beta2"])
            step_size = 1 / second_momentum_buffer.sqrt().clamp_min(1e-10)
            v_chunk.mul_(step_size)
            vnorm_new = v_chunk.norm(dim=(-2,-1), keepdim=True)
            v_chunk.mul_(vnorm / (vnorm_new.clamp_min(1e-10)))
            ####################################


            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2275  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0, beta2=0.95)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Oct 22 2025, 02:20:45) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251022+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Fri Oct 24 06:12:36 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000001:00:00.0 Off |                    0 |
| N/A   35C    P0            118W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000002:00:00.0 Off |                    0 |
| N/A   35C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000003:00:00.0 Off |                    0 |
| N/A   30C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000008:00:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000009:00:00.0 Off |                    0 |
| N/A   28C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   0000000A:00:00.0 Off |                    0 |
| N/A   35C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   0000000C:00:00.0 Off |                    0 |
| N/A   34C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2315 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2315 train_time:92ms step_avg:92.36ms
step:2/2315 train_time:188ms step_avg:93.98ms
step:3/2315 train_time:208ms step_avg:69.47ms
step:4/2315 train_time:246ms step_avg:61.43ms
step:5/2315 train_time:304ms step_avg:60.82ms
step:6/2315 train_time:364ms step_avg:60.61ms
step:7/2315 train_time:423ms step_avg:60.38ms
step:8/2315 train_time:483ms step_avg:60.31ms
step:9/2315 train_time:542ms step_avg:60.26ms
step:10/2315 train_time:603ms step_avg:60.27ms
step:11/2315 train_time:662ms step_avg:60.20ms
step:12/2315 train_time:722ms step_avg:60.16ms
step:13/2315 train_time:782ms step_avg:60.12ms
step:14/2315 train_time:841ms step_avg:60.09ms
step:15/2315 train_time:901ms step_avg:60.09ms
step:16/2315 train_time:961ms step_avg:60.08ms
step:17/2315 train_time:1022ms step_avg:60.14ms
step:18/2315 train_time:1085ms step_avg:60.26ms
step:19/2315 train_time:1149ms step_avg:60.47ms
step:20/2315 train_time:1211ms step_avg:60.55ms
step:21/2315 train_time:1272ms step_avg:60.56ms
step:22/2315 train_time:1332ms step_avg:60.55ms
step:23/2315 train_time:1392ms step_avg:60.52ms
step:24/2315 train_time:1452ms step_avg:60.50ms
step:25/2315 train_time:1512ms step_avg:60.47ms
step:26/2315 train_time:1572ms step_avg:60.45ms
step:27/2315 train_time:1632ms step_avg:60.44ms
step:28/2315 train_time:1692ms step_avg:60.44ms
step:29/2315 train_time:1752ms step_avg:60.43ms
step:30/2315 train_time:1813ms step_avg:60.43ms
step:31/2315 train_time:1873ms step_avg:60.43ms
step:32/2315 train_time:1934ms step_avg:60.42ms
step:33/2315 train_time:1994ms step_avg:60.43ms
step:34/2315 train_time:2056ms step_avg:60.46ms
step:35/2315 train_time:2117ms step_avg:60.49ms
step:36/2315 train_time:2179ms step_avg:60.53ms
step:37/2315 train_time:2240ms step_avg:60.55ms
step:38/2315 train_time:2301ms step_avg:60.55ms
step:39/2315 train_time:2362ms step_avg:60.56ms
step:40/2315 train_time:2422ms step_avg:60.55ms
step:41/2315 train_time:2482ms step_avg:60.54ms
step:42/2315 train_time:2542ms step_avg:60.53ms
step:43/2315 train_time:2603ms step_avg:60.54ms
step:44/2315 train_time:2664ms step_avg:60.53ms
step:45/2315 train_time:2724ms step_avg:60.54ms
step:46/2315 train_time:2785ms step_avg:60.54ms
step:47/2315 train_time:2846ms step_avg:60.55ms
step:48/2315 train_time:2906ms step_avg:60.54ms
step:49/2315 train_time:2966ms step_avg:60.54ms
step:50/2315 train_time:3026ms step_avg:60.53ms
step:51/2315 train_time:3087ms step_avg:60.52ms
step:52/2315 train_time:3146ms step_avg:60.50ms
step:53/2315 train_time:3206ms step_avg:60.50ms
step:54/2315 train_time:3266ms step_avg:60.48ms
step:55/2315 train_time:3326ms step_avg:60.48ms
step:56/2315 train_time:3386ms step_avg:60.47ms
step:57/2315 train_time:3446ms step_avg:60.46ms
step:58/2315 train_time:3506ms step_avg:60.44ms
step:59/2315 train_time:3565ms step_avg:60.43ms
step:60/2315 train_time:3626ms step_avg:60.43ms
step:61/2315 train_time:3685ms step_avg:60.42ms
step:62/2315 train_time:3745ms step_avg:60.41ms
step:63/2315 train_time:3806ms step_avg:60.41ms
step:64/2315 train_time:3866ms step_avg:60.41ms
step:65/2315 train_time:3926ms step_avg:60.41ms
step:66/2315 train_time:3987ms step_avg:60.40ms
step:67/2315 train_time:4047ms step_avg:60.40ms
step:68/2315 train_time:4106ms step_avg:60.39ms
step:69/2315 train_time:4166ms step_avg:60.38ms
step:70/2315 train_time:4226ms step_avg:60.37ms
step:71/2315 train_time:4286ms step_avg:60.37ms
step:72/2315 train_time:4346ms step_avg:60.36ms
step:73/2315 train_time:4406ms step_avg:60.35ms
step:74/2315 train_time:4466ms step_avg:60.35ms
step:75/2315 train_time:4525ms step_avg:60.34ms
step:76/2315 train_time:4585ms step_avg:60.33ms
step:77/2315 train_time:4645ms step_avg:60.33ms
step:78/2315 train_time:4705ms step_avg:60.32ms
step:79/2315 train_time:4765ms step_avg:60.32ms
step:80/2315 train_time:4825ms step_avg:60.31ms
step:81/2315 train_time:4885ms step_avg:60.31ms
step:82/2315 train_time:4945ms step_avg:60.31ms
step:83/2315 train_time:5006ms step_avg:60.31ms
step:84/2315 train_time:5066ms step_avg:60.31ms
step:85/2315 train_time:5126ms step_avg:60.30ms
step:86/2315 train_time:5186ms step_avg:60.30ms
step:87/2315 train_time:5245ms step_avg:60.29ms
step:88/2315 train_time:5305ms step_avg:60.28ms
step:89/2315 train_time:5365ms step_avg:60.28ms
step:90/2315 train_time:5424ms step_avg:60.27ms
step:91/2315 train_time:5484ms step_avg:60.27ms
step:92/2315 train_time:5544ms step_avg:60.26ms
step:93/2315 train_time:5605ms step_avg:60.27ms
step:94/2315 train_time:5664ms step_avg:60.26ms
step:95/2315 train_time:5724ms step_avg:60.26ms
step:96/2315 train_time:5784ms step_avg:60.25ms
step:97/2315 train_time:5845ms step_avg:60.25ms
step:98/2315 train_time:5905ms step_avg:60.25ms
step:99/2315 train_time:5965ms step_avg:60.25ms
step:100/2315 train_time:6025ms step_avg:60.25ms
step:101/2315 train_time:6085ms step_avg:60.25ms
step:102/2315 train_time:6145ms step_avg:60.24ms
step:103/2315 train_time:6205ms step_avg:60.24ms
step:104/2315 train_time:6265ms step_avg:60.24ms
step:105/2315 train_time:6325ms step_avg:60.24ms
step:106/2315 train_time:6385ms step_avg:60.24ms
step:107/2315 train_time:6445ms step_avg:60.23ms
step:108/2315 train_time:6505ms step_avg:60.23ms
step:109/2315 train_time:6565ms step_avg:60.23ms
step:110/2315 train_time:6625ms step_avg:60.22ms
step:111/2315 train_time:6684ms step_avg:60.22ms
step:112/2315 train_time:6744ms step_avg:60.22ms
step:113/2315 train_time:6804ms step_avg:60.21ms
step:114/2315 train_time:6864ms step_avg:60.21ms
step:115/2315 train_time:6925ms step_avg:60.22ms
step:116/2315 train_time:6985ms step_avg:60.22ms
step:117/2315 train_time:7045ms step_avg:60.22ms
step:118/2315 train_time:7105ms step_avg:60.21ms
step:119/2315 train_time:7165ms step_avg:60.21ms
step:120/2315 train_time:7225ms step_avg:60.21ms
step:121/2315 train_time:7285ms step_avg:60.21ms
step:122/2315 train_time:7345ms step_avg:60.21ms
step:123/2315 train_time:7406ms step_avg:60.21ms
step:124/2315 train_time:7465ms step_avg:60.20ms
step:125/2315 train_time:7525ms step_avg:60.20ms
step:126/2315 train_time:7585ms step_avg:60.20ms
step:127/2315 train_time:7645ms step_avg:60.19ms
step:128/2315 train_time:7704ms step_avg:60.19ms
step:129/2315 train_time:7764ms step_avg:60.19ms
step:130/2315 train_time:7825ms step_avg:60.19ms
step:131/2315 train_time:7885ms step_avg:60.19ms
step:132/2315 train_time:7945ms step_avg:60.19ms
step:133/2315 train_time:8005ms step_avg:60.19ms
step:134/2315 train_time:8065ms step_avg:60.19ms
step:135/2315 train_time:8125ms step_avg:60.19ms
step:136/2315 train_time:8185ms step_avg:60.18ms
step:137/2315 train_time:8245ms step_avg:60.18ms
step:138/2315 train_time:8305ms step_avg:60.18ms
step:139/2315 train_time:8365ms step_avg:60.18ms
step:140/2315 train_time:8424ms step_avg:60.17ms
step:141/2315 train_time:8484ms step_avg:60.17ms
step:142/2315 train_time:8544ms step_avg:60.17ms
step:143/2315 train_time:8604ms step_avg:60.17ms
step:144/2315 train_time:8664ms step_avg:60.17ms
step:145/2315 train_time:8724ms step_avg:60.17ms
step:146/2315 train_time:8784ms step_avg:60.16ms
step:147/2315 train_time:8844ms step_avg:60.17ms
step:148/2315 train_time:8904ms step_avg:60.16ms
step:149/2315 train_time:8964ms step_avg:60.16ms
step:150/2315 train_time:9024ms step_avg:60.16ms
step:151/2315 train_time:9084ms step_avg:60.16ms
step:152/2315 train_time:9144ms step_avg:60.16ms
step:153/2315 train_time:9205ms step_avg:60.16ms
step:154/2315 train_time:9264ms step_avg:60.16ms
step:155/2315 train_time:9324ms step_avg:60.16ms
step:156/2315 train_time:9384ms step_avg:60.15ms
step:157/2315 train_time:9444ms step_avg:60.15ms
step:158/2315 train_time:9504ms step_avg:60.15ms
step:159/2315 train_time:9564ms step_avg:60.15ms
step:160/2315 train_time:9624ms step_avg:60.15ms
step:161/2315 train_time:9684ms step_avg:60.15ms
step:162/2315 train_time:9744ms step_avg:60.15ms
step:163/2315 train_time:9804ms step_avg:60.15ms
step:164/2315 train_time:9864ms step_avg:60.15ms
step:165/2315 train_time:9924ms step_avg:60.14ms
step:166/2315 train_time:9984ms step_avg:60.14ms
step:167/2315 train_time:10044ms step_avg:60.14ms
step:168/2315 train_time:10104ms step_avg:60.14ms
step:169/2315 train_time:10164ms step_avg:60.14ms
step:170/2315 train_time:10224ms step_avg:60.14ms
step:171/2315 train_time:10285ms step_avg:60.14ms
step:172/2315 train_time:10344ms step_avg:60.14ms
step:173/2315 train_time:10404ms step_avg:60.14ms
step:174/2315 train_time:10464ms step_avg:60.14ms
step:175/2315 train_time:10524ms step_avg:60.14ms
step:176/2315 train_time:10585ms step_avg:60.14ms
step:177/2315 train_time:10644ms step_avg:60.14ms
step:178/2315 train_time:10704ms step_avg:60.13ms
step:179/2315 train_time:10764ms step_avg:60.13ms
step:180/2315 train_time:10824ms step_avg:60.13ms
step:181/2315 train_time:10883ms step_avg:60.13ms
step:182/2315 train_time:10944ms step_avg:60.13ms
step:183/2315 train_time:11004ms step_avg:60.13ms
step:184/2315 train_time:11063ms step_avg:60.13ms
step:185/2315 train_time:11123ms step_avg:60.13ms
step:186/2315 train_time:11184ms step_avg:60.13ms
step:187/2315 train_time:11244ms step_avg:60.13ms
step:188/2315 train_time:11304ms step_avg:60.13ms
step:189/2315 train_time:11364ms step_avg:60.13ms
step:190/2315 train_time:11424ms step_avg:60.12ms
step:191/2315 train_time:11483ms step_avg:60.12ms
step:192/2315 train_time:11543ms step_avg:60.12ms
step:193/2315 train_time:11603ms step_avg:60.12ms
step:194/2315 train_time:11663ms step_avg:60.12ms
step:195/2315 train_time:11723ms step_avg:60.12ms
step:196/2315 train_time:11783ms step_avg:60.12ms
step:197/2315 train_time:11843ms step_avg:60.12ms
step:198/2315 train_time:11903ms step_avg:60.11ms
step:199/2315 train_time:11963ms step_avg:60.12ms
step:200/2315 train_time:12024ms step_avg:60.12ms
step:201/2315 train_time:12084ms step_avg:60.12ms
step:202/2315 train_time:12144ms step_avg:60.12ms
step:203/2315 train_time:12205ms step_avg:60.12ms
step:204/2315 train_time:12265ms step_avg:60.12ms
step:205/2315 train_time:12325ms step_avg:60.12ms
step:206/2315 train_time:12385ms step_avg:60.12ms
step:207/2315 train_time:12444ms step_avg:60.12ms
step:208/2315 train_time:12504ms step_avg:60.11ms
step:209/2315 train_time:12563ms step_avg:60.11ms
step:210/2315 train_time:12623ms step_avg:60.11ms
step:211/2315 train_time:12684ms step_avg:60.11ms
step:212/2315 train_time:12744ms step_avg:60.11ms
step:213/2315 train_time:12804ms step_avg:60.11ms
step:214/2315 train_time:12864ms step_avg:60.11ms
step:215/2315 train_time:12924ms step_avg:60.11ms
step:216/2315 train_time:12984ms step_avg:60.11ms
step:217/2315 train_time:13043ms step_avg:60.11ms
step:218/2315 train_time:13103ms step_avg:60.11ms
step:219/2315 train_time:13164ms step_avg:60.11ms
step:220/2315 train_time:13224ms step_avg:60.11ms
step:221/2315 train_time:13284ms step_avg:60.11ms
step:222/2315 train_time:13344ms step_avg:60.11ms
step:223/2315 train_time:13405ms step_avg:60.11ms
step:224/2315 train_time:13464ms step_avg:60.11ms
step:225/2315 train_time:13524ms step_avg:60.11ms
step:226/2315 train_time:13584ms step_avg:60.11ms
step:227/2315 train_time:13644ms step_avg:60.11ms
step:228/2315 train_time:13704ms step_avg:60.11ms
step:229/2315 train_time:13764ms step_avg:60.10ms
step:230/2315 train_time:13823ms step_avg:60.10ms
step:231/2315 train_time:13883ms step_avg:60.10ms
step:232/2315 train_time:13944ms step_avg:60.10ms
step:233/2315 train_time:14003ms step_avg:60.10ms
step:234/2315 train_time:14063ms step_avg:60.10ms
step:235/2315 train_time:14123ms step_avg:60.10ms
step:236/2315 train_time:14183ms step_avg:60.10ms
step:237/2315 train_time:14243ms step_avg:60.10ms
step:238/2315 train_time:14304ms step_avg:60.10ms
step:239/2315 train_time:14363ms step_avg:60.10ms
step:240/2315 train_time:14424ms step_avg:60.10ms
step:241/2315 train_time:14484ms step_avg:60.10ms
step:242/2315 train_time:14544ms step_avg:60.10ms
step:243/2315 train_time:14604ms step_avg:60.10ms
step:244/2315 train_time:14663ms step_avg:60.10ms
step:245/2315 train_time:14724ms step_avg:60.10ms
step:246/2315 train_time:14784ms step_avg:60.10ms
step:247/2315 train_time:14844ms step_avg:60.10ms
step:248/2315 train_time:14904ms step_avg:60.09ms
step:249/2315 train_time:14963ms step_avg:60.09ms
step:250/2315 train_time:15023ms step_avg:60.09ms
step:250/2315 val_loss:4.0730 train_time:15085ms step_avg:60.34ms
step:251/2315 train_time:15105ms step_avg:60.18ms
step:252/2315 train_time:15145ms step_avg:60.10ms
step:253/2315 train_time:15209ms step_avg:60.11ms
step:254/2315 train_time:15273ms step_avg:60.13ms
step:255/2315 train_time:15335ms step_avg:60.14ms
step:256/2315 train_time:15395ms step_avg:60.14ms
step:257/2315 train_time:15455ms step_avg:60.14ms
step:258/2315 train_time:15514ms step_avg:60.13ms
step:259/2315 train_time:15575ms step_avg:60.14ms
step:260/2315 train_time:15634ms step_avg:60.13ms
step:261/2315 train_time:15694ms step_avg:60.13ms
step:262/2315 train_time:15753ms step_avg:60.13ms
step:263/2315 train_time:15813ms step_avg:60.13ms
step:264/2315 train_time:15872ms step_avg:60.12ms
step:265/2315 train_time:15931ms step_avg:60.12ms
step:266/2315 train_time:15991ms step_avg:60.12ms
step:267/2315 train_time:16052ms step_avg:60.12ms
step:268/2315 train_time:16113ms step_avg:60.12ms
step:269/2315 train_time:16174ms step_avg:60.13ms
step:270/2315 train_time:16236ms step_avg:60.13ms
step:271/2315 train_time:16299ms step_avg:60.14ms
step:272/2315 train_time:16359ms step_avg:60.14ms
step:273/2315 train_time:16420ms step_avg:60.15ms
step:274/2315 train_time:16480ms step_avg:60.15ms
step:275/2315 train_time:16540ms step_avg:60.14ms
step:276/2315 train_time:16599ms step_avg:60.14ms
step:277/2315 train_time:16659ms step_avg:60.14ms
step:278/2315 train_time:16719ms step_avg:60.14ms
step:279/2315 train_time:16778ms step_avg:60.14ms
step:280/2315 train_time:16838ms step_avg:60.13ms
step:281/2315 train_time:16898ms step_avg:60.14ms
step:282/2315 train_time:16958ms step_avg:60.13ms
step:283/2315 train_time:17017ms step_avg:60.13ms
step:284/2315 train_time:17077ms step_avg:60.13ms
step:285/2315 train_time:17138ms step_avg:60.13ms
step:286/2315 train_time:17198ms step_avg:60.13ms
step:287/2315 train_time:17259ms step_avg:60.14ms
step:288/2315 train_time:17320ms step_avg:60.14ms
step:289/2315 train_time:17381ms step_avg:60.14ms
step:290/2315 train_time:17441ms step_avg:60.14ms
step:291/2315 train_time:17501ms step_avg:60.14ms
step:292/2315 train_time:17560ms step_avg:60.14ms
step:293/2315 train_time:17620ms step_avg:60.14ms
step:294/2315 train_time:17680ms step_avg:60.14ms
step:295/2315 train_time:17739ms step_avg:60.13ms
step:296/2315 train_time:17799ms step_avg:60.13ms
step:297/2315 train_time:17859ms step_avg:60.13ms
step:298/2315 train_time:17918ms step_avg:60.13ms
step:299/2315 train_time:17979ms step_avg:60.13ms
step:300/2315 train_time:18038ms step_avg:60.13ms
step:301/2315 train_time:18099ms step_avg:60.13ms
step:302/2315 train_time:18159ms step_avg:60.13ms
step:303/2315 train_time:18220ms step_avg:60.13ms
step:304/2315 train_time:18279ms step_avg:60.13ms
step:305/2315 train_time:18340ms step_avg:60.13ms
step:306/2315 train_time:18401ms step_avg:60.13ms
step:307/2315 train_time:18461ms step_avg:60.13ms
step:308/2315 train_time:18521ms step_avg:60.13ms
step:309/2315 train_time:18581ms step_avg:60.13ms
step:310/2315 train_time:18640ms step_avg:60.13ms
step:311/2315 train_time:18700ms step_avg:60.13ms
step:312/2315 train_time:18760ms step_avg:60.13ms
step:313/2315 train_time:18819ms step_avg:60.12ms
step:314/2315 train_time:18879ms step_avg:60.12ms
step:315/2315 train_time:18939ms step_avg:60.12ms
step:316/2315 train_time:18999ms step_avg:60.12ms
step:317/2315 train_time:19059ms step_avg:60.12ms
step:318/2315 train_time:19119ms step_avg:60.12ms
step:319/2315 train_time:19179ms step_avg:60.12ms
step:320/2315 train_time:19240ms step_avg:60.13ms
step:321/2315 train_time:19300ms step_avg:60.12ms
step:322/2315 train_time:19360ms step_avg:60.12ms
step:323/2315 train_time:19420ms step_avg:60.12ms
step:324/2315 train_time:19480ms step_avg:60.12ms
step:325/2315 train_time:19540ms step_avg:60.12ms
step:326/2315 train_time:19600ms step_avg:60.12ms
step:327/2315 train_time:19660ms step_avg:60.12ms
step:328/2315 train_time:19720ms step_avg:60.12ms
step:329/2315 train_time:19780ms step_avg:60.12ms
step:330/2315 train_time:19840ms step_avg:60.12ms
step:331/2315 train_time:19899ms step_avg:60.12ms
step:332/2315 train_time:19959ms step_avg:60.12ms
step:333/2315 train_time:20019ms step_avg:60.12ms
step:334/2315 train_time:20079ms step_avg:60.12ms
step:335/2315 train_time:20139ms step_avg:60.12ms
step:336/2315 train_time:20199ms step_avg:60.12ms
step:337/2315 train_time:20259ms step_avg:60.12ms
step:338/2315 train_time:20319ms step_avg:60.12ms
step:339/2315 train_time:20379ms step_avg:60.12ms
step:340/2315 train_time:20440ms step_avg:60.12ms
step:341/2315 train_time:20500ms step_avg:60.12ms
step:342/2315 train_time:20560ms step_avg:60.12ms
step:343/2315 train_time:20620ms step_avg:60.12ms
step:344/2315 train_time:20680ms step_avg:60.12ms
step:345/2315 train_time:20741ms step_avg:60.12ms
step:346/2315 train_time:20800ms step_avg:60.12ms
step:347/2315 train_time:20859ms step_avg:60.11ms
step:348/2315 train_time:20919ms step_avg:60.11ms
step:349/2315 train_time:20978ms step_avg:60.11ms
step:350/2315 train_time:21038ms step_avg:60.11ms
step:351/2315 train_time:21098ms step_avg:60.11ms
step:352/2315 train_time:21158ms step_avg:60.11ms
step:353/2315 train_time:21218ms step_avg:60.11ms
step:354/2315 train_time:21279ms step_avg:60.11ms
step:355/2315 train_time:21339ms step_avg:60.11ms
step:356/2315 train_time:21399ms step_avg:60.11ms
step:357/2315 train_time:21459ms step_avg:60.11ms
step:358/2315 train_time:21520ms step_avg:60.11ms
step:359/2315 train_time:21579ms step_avg:60.11ms
step:360/2315 train_time:21639ms step_avg:60.11ms
step:361/2315 train_time:21699ms step_avg:60.11ms
step:362/2315 train_time:21759ms step_avg:60.11ms
step:363/2315 train_time:21819ms step_avg:60.11ms
step:364/2315 train_time:21879ms step_avg:60.11ms
step:365/2315 train_time:21939ms step_avg:60.11ms
step:366/2315 train_time:21998ms step_avg:60.10ms
step:367/2315 train_time:22058ms step_avg:60.10ms
step:368/2315 train_time:22118ms step_avg:60.10ms
step:369/2315 train_time:22178ms step_avg:60.10ms
step:370/2315 train_time:22239ms step_avg:60.10ms
step:371/2315 train_time:22299ms step_avg:60.10ms
step:372/2315 train_time:22358ms step_avg:60.10ms
step:373/2315 train_time:22418ms step_avg:60.10ms
step:374/2315 train_time:22478ms step_avg:60.10ms
step:375/2315 train_time:22539ms step_avg:60.10ms
step:376/2315 train_time:22599ms step_avg:60.10ms
step:377/2315 train_time:22659ms step_avg:60.10ms
step:378/2315 train_time:22718ms step_avg:60.10ms
step:379/2315 train_time:22779ms step_avg:60.10ms
step:380/2315 train_time:22839ms step_avg:60.10ms
step:381/2315 train_time:22899ms step_avg:60.10ms
step:382/2315 train_time:22958ms step_avg:60.10ms
step:383/2315 train_time:23018ms step_avg:60.10ms
step:384/2315 train_time:23078ms step_avg:60.10ms
step:385/2315 train_time:23138ms step_avg:60.10ms
step:386/2315 train_time:23198ms step_avg:60.10ms
step:387/2315 train_time:23258ms step_avg:60.10ms
step:388/2315 train_time:23318ms step_avg:60.10ms
step:389/2315 train_time:23378ms step_avg:60.10ms
step:390/2315 train_time:23439ms step_avg:60.10ms
step:391/2315 train_time:23499ms step_avg:60.10ms
step:392/2315 train_time:23560ms step_avg:60.10ms
step:393/2315 train_time:23620ms step_avg:60.10ms
step:394/2315 train_time:23679ms step_avg:60.10ms
step:395/2315 train_time:23739ms step_avg:60.10ms
step:396/2315 train_time:23799ms step_avg:60.10ms
step:397/2315 train_time:23859ms step_avg:60.10ms
step:398/2315 train_time:23919ms step_avg:60.10ms
step:399/2315 train_time:23978ms step_avg:60.10ms
step:400/2315 train_time:24039ms step_avg:60.10ms
step:401/2315 train_time:24099ms step_avg:60.10ms
step:402/2315 train_time:24159ms step_avg:60.10ms
step:403/2315 train_time:24219ms step_avg:60.10ms
step:404/2315 train_time:24278ms step_avg:60.09ms
step:405/2315 train_time:24339ms step_avg:60.10ms
step:406/2315 train_time:24399ms step_avg:60.10ms
step:407/2315 train_time:24460ms step_avg:60.10ms
step:408/2315 train_time:24519ms step_avg:60.10ms
step:409/2315 train_time:24579ms step_avg:60.10ms
step:410/2315 train_time:24639ms step_avg:60.10ms
step:411/2315 train_time:24699ms step_avg:60.10ms
step:412/2315 train_time:24759ms step_avg:60.10ms
step:413/2315 train_time:24820ms step_avg:60.10ms
step:414/2315 train_time:24879ms step_avg:60.09ms
step:415/2315 train_time:24939ms step_avg:60.09ms
step:416/2315 train_time:24999ms step_avg:60.09ms
step:417/2315 train_time:25059ms step_avg:60.09ms
step:418/2315 train_time:25119ms step_avg:60.09ms
step:419/2315 train_time:25178ms step_avg:60.09ms
step:420/2315 train_time:25238ms step_avg:60.09ms
step:421/2315 train_time:25298ms step_avg:60.09ms
step:422/2315 train_time:25358ms step_avg:60.09ms
step:423/2315 train_time:25419ms step_avg:60.09ms
step:424/2315 train_time:25479ms step_avg:60.09ms
step:425/2315 train_time:25540ms step_avg:60.09ms
step:426/2315 train_time:25600ms step_avg:60.09ms
step:427/2315 train_time:25660ms step_avg:60.09ms
step:428/2315 train_time:25720ms step_avg:60.09ms
step:429/2315 train_time:25779ms step_avg:60.09ms
step:430/2315 train_time:25839ms step_avg:60.09ms
step:431/2315 train_time:25899ms step_avg:60.09ms
step:432/2315 train_time:25959ms step_avg:60.09ms
step:433/2315 train_time:26019ms step_avg:60.09ms
step:434/2315 train_time:26080ms step_avg:60.09ms
step:435/2315 train_time:26140ms step_avg:60.09ms
step:436/2315 train_time:26199ms step_avg:60.09ms
step:437/2315 train_time:26259ms step_avg:60.09ms
step:438/2315 train_time:26319ms step_avg:60.09ms
step:439/2315 train_time:26379ms step_avg:60.09ms
step:440/2315 train_time:26439ms step_avg:60.09ms
step:441/2315 train_time:26499ms step_avg:60.09ms
step:442/2315 train_time:26559ms step_avg:60.09ms
step:443/2315 train_time:26620ms step_avg:60.09ms
step:444/2315 train_time:26679ms step_avg:60.09ms
step:445/2315 train_time:26739ms step_avg:60.09ms
step:446/2315 train_time:26799ms step_avg:60.09ms
step:447/2315 train_time:26859ms step_avg:60.09ms
step:448/2315 train_time:26919ms step_avg:60.09ms
step:449/2315 train_time:26979ms step_avg:60.09ms
step:450/2315 train_time:27039ms step_avg:60.09ms
step:451/2315 train_time:27099ms step_avg:60.09ms
step:452/2315 train_time:27159ms step_avg:60.09ms
step:453/2315 train_time:27219ms step_avg:60.09ms
step:454/2315 train_time:27278ms step_avg:60.08ms
step:455/2315 train_time:27338ms step_avg:60.08ms
step:456/2315 train_time:27399ms step_avg:60.08ms
step:457/2315 train_time:27459ms step_avg:60.08ms
step:458/2315 train_time:27519ms step_avg:60.08ms
step:459/2315 train_time:27580ms step_avg:60.09ms
step:460/2315 train_time:27640ms step_avg:60.09ms
step:461/2315 train_time:27700ms step_avg:60.09ms
step:462/2315 train_time:27759ms step_avg:60.08ms
step:463/2315 train_time:27819ms step_avg:60.08ms
step:464/2315 train_time:27878ms step_avg:60.08ms
step:465/2315 train_time:27939ms step_avg:60.08ms
step:466/2315 train_time:27999ms step_avg:60.08ms
step:467/2315 train_time:28058ms step_avg:60.08ms
step:468/2315 train_time:28119ms step_avg:60.08ms
step:469/2315 train_time:28180ms step_avg:60.08ms
step:470/2315 train_time:28239ms step_avg:60.08ms
step:471/2315 train_time:28299ms step_avg:60.08ms
step:472/2315 train_time:28359ms step_avg:60.08ms
step:473/2315 train_time:28419ms step_avg:60.08ms
step:474/2315 train_time:28479ms step_avg:60.08ms
step:475/2315 train_time:28539ms step_avg:60.08ms
step:476/2315 train_time:28599ms step_avg:60.08ms
step:477/2315 train_time:28659ms step_avg:60.08ms
step:478/2315 train_time:28720ms step_avg:60.08ms
step:479/2315 train_time:28780ms step_avg:60.08ms
step:480/2315 train_time:28839ms step_avg:60.08ms
step:481/2315 train_time:28899ms step_avg:60.08ms
step:482/2315 train_time:28960ms step_avg:60.08ms
step:483/2315 train_time:29020ms step_avg:60.08ms
step:484/2315 train_time:29080ms step_avg:60.08ms
step:485/2315 train_time:29140ms step_avg:60.08ms
step:486/2315 train_time:29199ms step_avg:60.08ms
step:487/2315 train_time:29259ms step_avg:60.08ms
step:488/2315 train_time:29320ms step_avg:60.08ms
step:489/2315 train_time:29380ms step_avg:60.08ms
step:490/2315 train_time:29440ms step_avg:60.08ms
step:491/2315 train_time:29501ms step_avg:60.08ms
step:492/2315 train_time:29560ms step_avg:60.08ms
step:493/2315 train_time:29620ms step_avg:60.08ms
step:494/2315 train_time:29679ms step_avg:60.08ms
step:495/2315 train_time:29739ms step_avg:60.08ms
step:496/2315 train_time:29799ms step_avg:60.08ms
step:497/2315 train_time:29860ms step_avg:60.08ms
step:498/2315 train_time:29920ms step_avg:60.08ms
step:499/2315 train_time:29980ms step_avg:60.08ms
step:500/2315 train_time:30040ms step_avg:60.08ms
step:500/2315 val_loss:3.8149 train_time:30102ms step_avg:60.20ms
step:501/2315 train_time:30121ms step_avg:60.12ms
step:502/2315 train_time:30161ms step_avg:60.08ms
step:503/2315 train_time:30225ms step_avg:60.09ms
step:504/2315 train_time:30289ms step_avg:60.10ms
step:505/2315 train_time:30349ms step_avg:60.10ms
step:506/2315 train_time:30409ms step_avg:60.10ms
step:507/2315 train_time:30468ms step_avg:60.09ms
step:508/2315 train_time:30528ms step_avg:60.09ms
step:509/2315 train_time:30587ms step_avg:60.09ms
step:510/2315 train_time:30646ms step_avg:60.09ms
step:511/2315 train_time:30705ms step_avg:60.09ms
step:512/2315 train_time:30764ms step_avg:60.09ms
step:513/2315 train_time:30823ms step_avg:60.08ms
step:514/2315 train_time:30882ms step_avg:60.08ms
step:515/2315 train_time:30941ms step_avg:60.08ms
step:516/2315 train_time:31001ms step_avg:60.08ms
step:517/2315 train_time:31061ms step_avg:60.08ms
step:518/2315 train_time:31120ms step_avg:60.08ms
step:519/2315 train_time:31181ms step_avg:60.08ms
step:520/2315 train_time:31242ms step_avg:60.08ms
step:521/2315 train_time:31302ms step_avg:60.08ms
step:522/2315 train_time:31363ms step_avg:60.08ms
step:523/2315 train_time:31423ms step_avg:60.08ms
step:524/2315 train_time:31483ms step_avg:60.08ms
step:525/2315 train_time:31542ms step_avg:60.08ms
step:526/2315 train_time:31602ms step_avg:60.08ms
step:527/2315 train_time:31662ms step_avg:60.08ms
step:528/2315 train_time:31721ms step_avg:60.08ms
step:529/2315 train_time:31781ms step_avg:60.08ms
step:530/2315 train_time:31840ms step_avg:60.08ms
step:531/2315 train_time:31900ms step_avg:60.08ms
step:532/2315 train_time:31960ms step_avg:60.07ms
step:533/2315 train_time:32019ms step_avg:60.07ms
step:534/2315 train_time:32079ms step_avg:60.07ms
step:535/2315 train_time:32139ms step_avg:60.07ms
step:536/2315 train_time:32199ms step_avg:60.07ms
step:537/2315 train_time:32260ms step_avg:60.07ms
step:538/2315 train_time:32320ms step_avg:60.07ms
step:539/2315 train_time:32381ms step_avg:60.08ms
step:540/2315 train_time:32440ms step_avg:60.07ms
step:541/2315 train_time:32501ms step_avg:60.08ms
step:542/2315 train_time:32560ms step_avg:60.07ms
step:543/2315 train_time:32620ms step_avg:60.07ms
step:544/2315 train_time:32681ms step_avg:60.07ms
step:545/2315 train_time:32741ms step_avg:60.07ms
step:546/2315 train_time:32801ms step_avg:60.07ms
step:547/2315 train_time:32860ms step_avg:60.07ms
step:548/2315 train_time:32920ms step_avg:60.07ms
step:549/2315 train_time:32980ms step_avg:60.07ms
step:550/2315 train_time:33039ms step_avg:60.07ms
step:551/2315 train_time:33099ms step_avg:60.07ms
step:552/2315 train_time:33159ms step_avg:60.07ms
step:553/2315 train_time:33219ms step_avg:60.07ms
step:554/2315 train_time:33280ms step_avg:60.07ms
step:555/2315 train_time:33339ms step_avg:60.07ms
step:556/2315 train_time:33400ms step_avg:60.07ms
step:557/2315 train_time:33460ms step_avg:60.07ms
step:558/2315 train_time:33521ms step_avg:60.07ms
step:559/2315 train_time:33580ms step_avg:60.07ms
step:560/2315 train_time:33640ms step_avg:60.07ms
step:561/2315 train_time:33700ms step_avg:60.07ms
step:562/2315 train_time:33760ms step_avg:60.07ms
step:563/2315 train_time:33820ms step_avg:60.07ms
step:564/2315 train_time:33880ms step_avg:60.07ms
step:565/2315 train_time:33939ms step_avg:60.07ms
step:566/2315 train_time:33999ms step_avg:60.07ms
step:567/2315 train_time:34059ms step_avg:60.07ms
step:568/2315 train_time:34119ms step_avg:60.07ms
step:569/2315 train_time:34180ms step_avg:60.07ms
step:570/2315 train_time:34240ms step_avg:60.07ms
step:571/2315 train_time:34301ms step_avg:60.07ms
step:572/2315 train_time:34361ms step_avg:60.07ms
step:573/2315 train_time:34421ms step_avg:60.07ms
step:574/2315 train_time:34481ms step_avg:60.07ms
step:575/2315 train_time:34542ms step_avg:60.07ms
step:576/2315 train_time:34601ms step_avg:60.07ms
step:577/2315 train_time:34661ms step_avg:60.07ms
step:578/2315 train_time:34720ms step_avg:60.07ms
step:579/2315 train_time:34780ms step_avg:60.07ms
step:580/2315 train_time:34840ms step_avg:60.07ms
step:581/2315 train_time:34900ms step_avg:60.07ms
step:582/2315 train_time:34960ms step_avg:60.07ms
step:583/2315 train_time:35021ms step_avg:60.07ms
step:584/2315 train_time:35081ms step_avg:60.07ms
step:585/2315 train_time:35140ms step_avg:60.07ms
step:586/2315 train_time:35200ms step_avg:60.07ms
step:587/2315 train_time:35260ms step_avg:60.07ms
step:588/2315 train_time:35320ms step_avg:60.07ms
step:589/2315 train_time:35381ms step_avg:60.07ms
step:590/2315 train_time:35440ms step_avg:60.07ms
step:591/2315 train_time:35501ms step_avg:60.07ms
step:592/2315 train_time:35560ms step_avg:60.07ms
step:593/2315 train_time:35621ms step_avg:60.07ms
step:594/2315 train_time:35681ms step_avg:60.07ms
step:595/2315 train_time:35741ms step_avg:60.07ms
step:596/2315 train_time:35801ms step_avg:60.07ms
step:597/2315 train_time:35861ms step_avg:60.07ms
step:598/2315 train_time:35921ms step_avg:60.07ms
step:599/2315 train_time:35980ms step_avg:60.07ms
step:600/2315 train_time:36039ms step_avg:60.07ms
step:601/2315 train_time:36100ms step_avg:60.07ms
step:602/2315 train_time:36160ms step_avg:60.07ms
step:603/2315 train_time:36220ms step_avg:60.07ms
step:604/2315 train_time:36280ms step_avg:60.07ms
step:605/2315 train_time:36340ms step_avg:60.07ms
step:606/2315 train_time:36400ms step_avg:60.07ms
step:607/2315 train_time:36460ms step_avg:60.07ms
step:608/2315 train_time:36520ms step_avg:60.07ms
step:609/2315 train_time:36580ms step_avg:60.07ms
step:610/2315 train_time:36640ms step_avg:60.06ms
step:611/2315 train_time:36700ms step_avg:60.07ms
step:612/2315 train_time:36759ms step_avg:60.06ms
step:613/2315 train_time:36820ms step_avg:60.07ms
step:614/2315 train_time:36880ms step_avg:60.07ms
step:615/2315 train_time:36940ms step_avg:60.07ms
step:616/2315 train_time:37000ms step_avg:60.06ms
step:617/2315 train_time:37060ms step_avg:60.06ms
step:618/2315 train_time:37119ms step_avg:60.06ms
step:619/2315 train_time:37179ms step_avg:60.06ms
step:620/2315 train_time:37239ms step_avg:60.06ms
step:621/2315 train_time:37299ms step_avg:60.06ms
step:622/2315 train_time:37359ms step_avg:60.06ms
step:623/2315 train_time:37419ms step_avg:60.06ms
step:624/2315 train_time:37480ms step_avg:60.06ms
step:625/2315 train_time:37541ms step_avg:60.07ms
step:626/2315 train_time:37601ms step_avg:60.07ms
step:627/2315 train_time:37662ms step_avg:60.07ms
step:628/2315 train_time:37721ms step_avg:60.07ms
step:629/2315 train_time:37781ms step_avg:60.06ms
step:630/2315 train_time:37841ms step_avg:60.06ms
step:631/2315 train_time:37901ms step_avg:60.07ms
step:632/2315 train_time:37961ms step_avg:60.06ms
step:633/2315 train_time:38021ms step_avg:60.06ms
step:634/2315 train_time:38081ms step_avg:60.07ms
step:635/2315 train_time:38140ms step_avg:60.06ms
step:636/2315 train_time:38200ms step_avg:60.06ms
step:637/2315 train_time:38260ms step_avg:60.06ms
step:638/2315 train_time:38320ms step_avg:60.06ms
step:639/2315 train_time:38379ms step_avg:60.06ms
step:640/2315 train_time:38439ms step_avg:60.06ms
step:641/2315 train_time:38499ms step_avg:60.06ms
step:642/2315 train_time:38559ms step_avg:60.06ms
step:643/2315 train_time:38620ms step_avg:60.06ms
step:644/2315 train_time:38680ms step_avg:60.06ms
step:645/2315 train_time:38740ms step_avg:60.06ms
step:646/2315 train_time:38800ms step_avg:60.06ms
step:647/2315 train_time:38860ms step_avg:60.06ms
step:648/2315 train_time:38919ms step_avg:60.06ms
step:649/2315 train_time:38979ms step_avg:60.06ms
step:650/2315 train_time:39039ms step_avg:60.06ms
step:651/2315 train_time:39100ms step_avg:60.06ms
step:652/2315 train_time:39160ms step_avg:60.06ms
step:653/2315 train_time:39220ms step_avg:60.06ms
step:654/2315 train_time:39280ms step_avg:60.06ms
step:655/2315 train_time:39339ms step_avg:60.06ms
step:656/2315 train_time:39399ms step_avg:60.06ms
step:657/2315 train_time:39459ms step_avg:60.06ms
step:658/2315 train_time:39518ms step_avg:60.06ms
step:659/2315 train_time:39579ms step_avg:60.06ms
step:660/2315 train_time:39640ms step_avg:60.06ms
step:661/2315 train_time:39700ms step_avg:60.06ms
step:662/2315 train_time:39760ms step_avg:60.06ms
step:663/2315 train_time:39820ms step_avg:60.06ms
step:664/2315 train_time:39879ms step_avg:60.06ms
step:665/2315 train_time:39940ms step_avg:60.06ms
step:666/2315 train_time:40000ms step_avg:60.06ms
step:667/2315 train_time:40060ms step_avg:60.06ms
step:668/2315 train_time:40120ms step_avg:60.06ms
step:669/2315 train_time:40181ms step_avg:60.06ms
step:670/2315 train_time:40241ms step_avg:60.06ms
step:671/2315 train_time:40301ms step_avg:60.06ms
step:672/2315 train_time:40360ms step_avg:60.06ms
step:673/2315 train_time:40421ms step_avg:60.06ms
step:674/2315 train_time:40480ms step_avg:60.06ms
step:675/2315 train_time:40540ms step_avg:60.06ms
step:676/2315 train_time:40600ms step_avg:60.06ms
step:677/2315 train_time:40660ms step_avg:60.06ms
step:678/2315 train_time:40720ms step_avg:60.06ms
step:679/2315 train_time:40780ms step_avg:60.06ms
step:680/2315 train_time:40840ms step_avg:60.06ms
step:681/2315 train_time:40900ms step_avg:60.06ms
step:682/2315 train_time:40959ms step_avg:60.06ms
step:683/2315 train_time:41019ms step_avg:60.06ms
step:684/2315 train_time:41080ms step_avg:60.06ms
step:685/2315 train_time:41140ms step_avg:60.06ms
step:686/2315 train_time:41200ms step_avg:60.06ms
step:687/2315 train_time:41260ms step_avg:60.06ms
step:688/2315 train_time:41321ms step_avg:60.06ms
step:689/2315 train_time:41381ms step_avg:60.06ms
step:690/2315 train_time:41440ms step_avg:60.06ms
step:691/2315 train_time:41500ms step_avg:60.06ms
step:692/2315 train_time:41560ms step_avg:60.06ms
step:693/2315 train_time:41620ms step_avg:60.06ms
step:694/2315 train_time:41680ms step_avg:60.06ms
step:695/2315 train_time:41740ms step_avg:60.06ms
step:696/2315 train_time:41800ms step_avg:60.06ms
step:697/2315 train_time:41859ms step_avg:60.06ms
step:698/2315 train_time:41919ms step_avg:60.06ms
step:699/2315 train_time:41979ms step_avg:60.06ms
step:700/2315 train_time:42039ms step_avg:60.06ms
step:701/2315 train_time:42099ms step_avg:60.06ms
step:702/2315 train_time:42159ms step_avg:60.06ms
step:703/2315 train_time:42219ms step_avg:60.06ms
step:704/2315 train_time:42279ms step_avg:60.06ms
step:705/2315 train_time:42340ms step_avg:60.06ms
step:706/2315 train_time:42399ms step_avg:60.06ms
step:707/2315 train_time:42459ms step_avg:60.06ms
step:708/2315 train_time:42519ms step_avg:60.06ms
step:709/2315 train_time:42579ms step_avg:60.06ms
step:710/2315 train_time:42640ms step_avg:60.06ms
step:711/2315 train_time:42699ms step_avg:60.06ms
step:712/2315 train_time:42759ms step_avg:60.06ms
step:713/2315 train_time:42819ms step_avg:60.05ms
step:714/2315 train_time:42879ms step_avg:60.05ms
step:715/2315 train_time:42939ms step_avg:60.05ms
step:716/2315 train_time:42998ms step_avg:60.05ms
step:717/2315 train_time:43058ms step_avg:60.05ms
step:718/2315 train_time:43118ms step_avg:60.05ms
step:719/2315 train_time:43179ms step_avg:60.05ms
step:720/2315 train_time:43239ms step_avg:60.05ms
step:721/2315 train_time:43299ms step_avg:60.05ms
step:722/2315 train_time:43359ms step_avg:60.05ms
step:723/2315 train_time:43419ms step_avg:60.05ms
step:724/2315 train_time:43480ms step_avg:60.06ms
step:725/2315 train_time:43540ms step_avg:60.06ms
step:726/2315 train_time:43600ms step_avg:60.05ms
step:727/2315 train_time:43660ms step_avg:60.06ms
step:728/2315 train_time:43720ms step_avg:60.06ms
step:729/2315 train_time:43780ms step_avg:60.06ms
step:730/2315 train_time:43840ms step_avg:60.05ms
step:731/2315 train_time:43900ms step_avg:60.06ms
step:732/2315 train_time:43960ms step_avg:60.05ms
step:733/2315 train_time:44020ms step_avg:60.05ms
step:734/2315 train_time:44079ms step_avg:60.05ms
step:735/2315 train_time:44139ms step_avg:60.05ms
step:736/2315 train_time:44200ms step_avg:60.05ms
step:737/2315 train_time:44259ms step_avg:60.05ms
step:738/2315 train_time:44320ms step_avg:60.05ms
step:739/2315 train_time:44380ms step_avg:60.05ms
step:740/2315 train_time:44440ms step_avg:60.05ms
step:741/2315 train_time:44500ms step_avg:60.05ms
step:742/2315 train_time:44560ms step_avg:60.05ms
step:743/2315 train_time:44620ms step_avg:60.05ms
step:744/2315 train_time:44680ms step_avg:60.05ms
step:745/2315 train_time:44740ms step_avg:60.05ms
step:746/2315 train_time:44800ms step_avg:60.05ms
step:747/2315 train_time:44860ms step_avg:60.05ms
step:748/2315 train_time:44920ms step_avg:60.05ms
step:749/2315 train_time:44980ms step_avg:60.05ms
step:750/2315 train_time:45040ms step_avg:60.05ms
step:750/2315 val_loss:3.6857 train_time:45101ms step_avg:60.14ms
step:751/2315 train_time:45120ms step_avg:60.08ms
step:752/2315 train_time:45162ms step_avg:60.06ms
step:753/2315 train_time:45223ms step_avg:60.06ms
step:754/2315 train_time:45287ms step_avg:60.06ms
step:755/2315 train_time:45348ms step_avg:60.06ms
step:756/2315 train_time:45409ms step_avg:60.06ms
step:757/2315 train_time:45469ms step_avg:60.06ms
step:758/2315 train_time:45528ms step_avg:60.06ms
step:759/2315 train_time:45588ms step_avg:60.06ms
step:760/2315 train_time:45647ms step_avg:60.06ms
step:761/2315 train_time:45707ms step_avg:60.06ms
step:762/2315 train_time:45767ms step_avg:60.06ms
step:763/2315 train_time:45826ms step_avg:60.06ms
step:764/2315 train_time:45886ms step_avg:60.06ms
step:765/2315 train_time:45946ms step_avg:60.06ms
step:766/2315 train_time:46006ms step_avg:60.06ms
step:767/2315 train_time:46068ms step_avg:60.06ms
step:768/2315 train_time:46129ms step_avg:60.06ms
step:769/2315 train_time:46192ms step_avg:60.07ms
step:770/2315 train_time:46254ms step_avg:60.07ms
step:771/2315 train_time:46315ms step_avg:60.07ms
step:772/2315 train_time:46376ms step_avg:60.07ms
step:773/2315 train_time:46437ms step_avg:60.07ms
step:774/2315 train_time:46497ms step_avg:60.07ms
step:775/2315 train_time:46558ms step_avg:60.07ms
step:776/2315 train_time:46619ms step_avg:60.08ms
step:777/2315 train_time:46679ms step_avg:60.08ms
step:778/2315 train_time:46739ms step_avg:60.08ms
step:779/2315 train_time:46800ms step_avg:60.08ms
step:780/2315 train_time:46860ms step_avg:60.08ms
step:781/2315 train_time:46920ms step_avg:60.08ms
step:782/2315 train_time:46980ms step_avg:60.08ms
step:783/2315 train_time:47041ms step_avg:60.08ms
step:784/2315 train_time:47102ms step_avg:60.08ms
step:785/2315 train_time:47163ms step_avg:60.08ms
step:786/2315 train_time:47223ms step_avg:60.08ms
step:787/2315 train_time:47283ms step_avg:60.08ms
step:788/2315 train_time:47343ms step_avg:60.08ms
step:789/2315 train_time:47404ms step_avg:60.08ms
step:790/2315 train_time:47465ms step_avg:60.08ms
step:791/2315 train_time:47527ms step_avg:60.08ms
step:792/2315 train_time:47589ms step_avg:60.09ms
step:793/2315 train_time:47650ms step_avg:60.09ms
step:794/2315 train_time:47710ms step_avg:60.09ms
step:795/2315 train_time:47771ms step_avg:60.09ms
step:796/2315 train_time:47831ms step_avg:60.09ms
step:797/2315 train_time:47892ms step_avg:60.09ms
step:798/2315 train_time:47952ms step_avg:60.09ms
step:799/2315 train_time:48013ms step_avg:60.09ms
step:800/2315 train_time:48074ms step_avg:60.09ms
step:801/2315 train_time:48135ms step_avg:60.09ms
step:802/2315 train_time:48196ms step_avg:60.10ms
step:803/2315 train_time:48258ms step_avg:60.10ms
step:804/2315 train_time:48318ms step_avg:60.10ms
step:805/2315 train_time:48379ms step_avg:60.10ms
step:806/2315 train_time:48440ms step_avg:60.10ms
step:807/2315 train_time:48501ms step_avg:60.10ms
step:808/2315 train_time:48561ms step_avg:60.10ms
step:809/2315 train_time:48622ms step_avg:60.10ms
step:810/2315 train_time:48682ms step_avg:60.10ms
step:811/2315 train_time:48742ms step_avg:60.10ms
step:812/2315 train_time:48802ms step_avg:60.10ms
step:813/2315 train_time:48862ms step_avg:60.10ms
step:814/2315 train_time:48921ms step_avg:60.10ms
step:815/2315 train_time:48982ms step_avg:60.10ms
step:816/2315 train_time:49042ms step_avg:60.10ms
step:817/2315 train_time:49102ms step_avg:60.10ms
step:818/2315 train_time:49162ms step_avg:60.10ms
step:819/2315 train_time:49223ms step_avg:60.10ms
step:820/2315 train_time:49283ms step_avg:60.10ms
step:821/2315 train_time:49344ms step_avg:60.10ms
step:822/2315 train_time:49404ms step_avg:60.10ms
step:823/2315 train_time:49465ms step_avg:60.10ms
step:824/2315 train_time:49525ms step_avg:60.10ms
step:825/2315 train_time:49587ms step_avg:60.10ms
step:826/2315 train_time:49647ms step_avg:60.11ms
step:827/2315 train_time:49708ms step_avg:60.11ms
step:828/2315 train_time:49769ms step_avg:60.11ms
step:829/2315 train_time:49829ms step_avg:60.11ms
step:830/2315 train_time:49890ms step_avg:60.11ms
step:831/2315 train_time:49950ms step_avg:60.11ms
step:832/2315 train_time:50011ms step_avg:60.11ms
step:833/2315 train_time:50072ms step_avg:60.11ms
step:834/2315 train_time:50133ms step_avg:60.11ms
step:835/2315 train_time:50194ms step_avg:60.11ms
step:836/2315 train_time:50254ms step_avg:60.11ms
step:837/2315 train_time:50316ms step_avg:60.11ms
step:838/2315 train_time:50377ms step_avg:60.12ms
step:839/2315 train_time:50438ms step_avg:60.12ms
step:840/2315 train_time:50499ms step_avg:60.12ms
step:841/2315 train_time:50560ms step_avg:60.12ms
step:842/2315 train_time:50621ms step_avg:60.12ms
step:843/2315 train_time:50682ms step_avg:60.12ms
step:844/2315 train_time:50742ms step_avg:60.12ms
step:845/2315 train_time:50802ms step_avg:60.12ms
step:846/2315 train_time:50862ms step_avg:60.12ms
step:847/2315 train_time:50922ms step_avg:60.12ms
step:848/2315 train_time:50982ms step_avg:60.12ms
step:849/2315 train_time:51043ms step_avg:60.12ms
step:850/2315 train_time:51103ms step_avg:60.12ms
step:851/2315 train_time:51164ms step_avg:60.12ms
step:852/2315 train_time:51225ms step_avg:60.12ms
step:853/2315 train_time:51286ms step_avg:60.12ms
step:854/2315 train_time:51347ms step_avg:60.12ms
step:855/2315 train_time:51407ms step_avg:60.13ms
step:856/2315 train_time:51469ms step_avg:60.13ms
step:857/2315 train_time:51530ms step_avg:60.13ms
step:858/2315 train_time:51591ms step_avg:60.13ms
step:859/2315 train_time:51652ms step_avg:60.13ms
step:860/2315 train_time:51713ms step_avg:60.13ms
step:861/2315 train_time:51774ms step_avg:60.13ms
step:862/2315 train_time:51835ms step_avg:60.13ms
step:863/2315 train_time:51896ms step_avg:60.13ms
step:864/2315 train_time:51956ms step_avg:60.13ms
step:865/2315 train_time:52018ms step_avg:60.14ms
step:866/2315 train_time:52078ms step_avg:60.14ms
step:867/2315 train_time:52139ms step_avg:60.14ms
step:868/2315 train_time:52199ms step_avg:60.14ms
step:869/2315 train_time:52260ms step_avg:60.14ms
step:870/2315 train_time:52320ms step_avg:60.14ms
step:871/2315 train_time:52381ms step_avg:60.14ms
step:872/2315 train_time:52441ms step_avg:60.14ms
step:873/2315 train_time:52502ms step_avg:60.14ms
step:874/2315 train_time:52562ms step_avg:60.14ms
step:875/2315 train_time:52622ms step_avg:60.14ms
step:876/2315 train_time:52682ms step_avg:60.14ms
step:877/2315 train_time:52742ms step_avg:60.14ms
step:878/2315 train_time:52802ms step_avg:60.14ms
step:879/2315 train_time:52862ms step_avg:60.14ms
step:880/2315 train_time:52922ms step_avg:60.14ms
step:881/2315 train_time:52984ms step_avg:60.14ms
step:882/2315 train_time:53044ms step_avg:60.14ms
step:883/2315 train_time:53105ms step_avg:60.14ms
step:884/2315 train_time:53166ms step_avg:60.14ms
step:885/2315 train_time:53227ms step_avg:60.14ms
step:886/2315 train_time:53287ms step_avg:60.14ms
step:887/2315 train_time:53348ms step_avg:60.14ms
step:888/2315 train_time:53409ms step_avg:60.15ms
step:889/2315 train_time:53470ms step_avg:60.15ms
step:890/2315 train_time:53531ms step_avg:60.15ms
step:891/2315 train_time:53592ms step_avg:60.15ms
step:892/2315 train_time:53653ms step_avg:60.15ms
step:893/2315 train_time:53714ms step_avg:60.15ms
step:894/2315 train_time:53774ms step_avg:60.15ms
step:895/2315 train_time:53835ms step_avg:60.15ms
step:896/2315 train_time:53896ms step_avg:60.15ms
step:897/2315 train_time:53958ms step_avg:60.15ms
step:898/2315 train_time:54018ms step_avg:60.15ms
step:899/2315 train_time:54079ms step_avg:60.15ms
step:900/2315 train_time:54140ms step_avg:60.16ms
step:901/2315 train_time:54200ms step_avg:60.16ms
step:902/2315 train_time:54261ms step_avg:60.16ms
step:903/2315 train_time:54322ms step_avg:60.16ms
step:904/2315 train_time:54382ms step_avg:60.16ms
step:905/2315 train_time:54443ms step_avg:60.16ms
step:906/2315 train_time:54503ms step_avg:60.16ms
step:907/2315 train_time:54563ms step_avg:60.16ms
step:908/2315 train_time:54623ms step_avg:60.16ms
step:909/2315 train_time:54684ms step_avg:60.16ms
step:910/2315 train_time:54744ms step_avg:60.16ms
step:911/2315 train_time:54805ms step_avg:60.16ms
step:912/2315 train_time:54865ms step_avg:60.16ms
step:913/2315 train_time:54927ms step_avg:60.16ms
step:914/2315 train_time:54988ms step_avg:60.16ms
step:915/2315 train_time:55049ms step_avg:60.16ms
step:916/2315 train_time:55110ms step_avg:60.16ms
step:917/2315 train_time:55171ms step_avg:60.16ms
step:918/2315 train_time:55231ms step_avg:60.16ms
step:919/2315 train_time:55292ms step_avg:60.17ms
step:920/2315 train_time:55353ms step_avg:60.17ms
step:921/2315 train_time:55414ms step_avg:60.17ms
step:922/2315 train_time:55475ms step_avg:60.17ms
step:923/2315 train_time:55536ms step_avg:60.17ms
step:924/2315 train_time:55597ms step_avg:60.17ms
step:925/2315 train_time:55658ms step_avg:60.17ms
step:926/2315 train_time:55719ms step_avg:60.17ms
step:927/2315 train_time:55780ms step_avg:60.17ms
step:928/2315 train_time:55840ms step_avg:60.17ms
step:929/2315 train_time:55901ms step_avg:60.17ms
step:930/2315 train_time:55961ms step_avg:60.17ms
step:931/2315 train_time:56022ms step_avg:60.17ms
step:932/2315 train_time:56082ms step_avg:60.17ms
step:933/2315 train_time:56142ms step_avg:60.17ms
step:934/2315 train_time:56203ms step_avg:60.17ms
step:935/2315 train_time:56263ms step_avg:60.17ms
step:936/2315 train_time:56323ms step_avg:60.17ms
step:937/2315 train_time:56383ms step_avg:60.17ms
step:938/2315 train_time:56443ms step_avg:60.17ms
step:939/2315 train_time:56505ms step_avg:60.18ms
step:940/2315 train_time:56566ms step_avg:60.18ms
step:941/2315 train_time:56627ms step_avg:60.18ms
step:942/2315 train_time:56688ms step_avg:60.18ms
step:943/2315 train_time:56749ms step_avg:60.18ms
step:944/2315 train_time:56810ms step_avg:60.18ms
step:945/2315 train_time:56871ms step_avg:60.18ms
step:946/2315 train_time:56931ms step_avg:60.18ms
step:947/2315 train_time:56992ms step_avg:60.18ms
step:948/2315 train_time:57053ms step_avg:60.18ms
step:949/2315 train_time:57114ms step_avg:60.18ms
step:950/2315 train_time:57174ms step_avg:60.18ms
step:951/2315 train_time:57235ms step_avg:60.18ms
step:952/2315 train_time:57296ms step_avg:60.18ms
step:953/2315 train_time:57357ms step_avg:60.19ms
step:954/2315 train_time:57418ms step_avg:60.19ms
step:955/2315 train_time:57478ms step_avg:60.19ms
step:956/2315 train_time:57539ms step_avg:60.19ms
step:957/2315 train_time:57599ms step_avg:60.19ms
step:958/2315 train_time:57660ms step_avg:60.19ms
step:959/2315 train_time:57721ms step_avg:60.19ms
step:960/2315 train_time:57782ms step_avg:60.19ms
step:961/2315 train_time:57842ms step_avg:60.19ms
step:962/2315 train_time:57902ms step_avg:60.19ms
step:963/2315 train_time:57962ms step_avg:60.19ms
step:964/2315 train_time:58022ms step_avg:60.19ms
step:965/2315 train_time:58083ms step_avg:60.19ms
step:966/2315 train_time:58143ms step_avg:60.19ms
step:967/2315 train_time:58203ms step_avg:60.19ms
step:968/2315 train_time:58264ms step_avg:60.19ms
step:969/2315 train_time:58325ms step_avg:60.19ms
step:970/2315 train_time:58386ms step_avg:60.19ms
step:971/2315 train_time:58447ms step_avg:60.19ms
step:972/2315 train_time:58507ms step_avg:60.19ms
step:973/2315 train_time:58569ms step_avg:60.19ms
step:974/2315 train_time:58630ms step_avg:60.20ms
step:975/2315 train_time:58691ms step_avg:60.20ms
step:976/2315 train_time:58751ms step_avg:60.20ms
step:977/2315 train_time:58812ms step_avg:60.20ms
step:978/2315 train_time:58873ms step_avg:60.20ms
step:979/2315 train_time:58934ms step_avg:60.20ms
step:980/2315 train_time:58995ms step_avg:60.20ms
step:981/2315 train_time:59056ms step_avg:60.20ms
step:982/2315 train_time:59116ms step_avg:60.20ms
step:983/2315 train_time:59178ms step_avg:60.20ms
step:984/2315 train_time:59238ms step_avg:60.20ms
step:985/2315 train_time:59299ms step_avg:60.20ms
step:986/2315 train_time:59360ms step_avg:60.20ms
step:987/2315 train_time:59420ms step_avg:60.20ms
step:988/2315 train_time:59481ms step_avg:60.20ms
step:989/2315 train_time:59542ms step_avg:60.20ms
step:990/2315 train_time:59602ms step_avg:60.20ms
step:991/2315 train_time:59662ms step_avg:60.20ms
step:992/2315 train_time:59722ms step_avg:60.20ms
step:993/2315 train_time:59782ms step_avg:60.20ms
step:994/2315 train_time:59842ms step_avg:60.20ms
step:995/2315 train_time:59903ms step_avg:60.20ms
step:996/2315 train_time:59963ms step_avg:60.20ms
step:997/2315 train_time:60023ms step_avg:60.20ms
step:998/2315 train_time:60084ms step_avg:60.20ms
step:999/2315 train_time:60144ms step_avg:60.20ms
step:1000/2315 train_time:60205ms step_avg:60.20ms
step:1000/2315 val_loss:3.5726 train_time:60268ms step_avg:60.27ms
step:1001/2315 train_time:60288ms step_avg:60.23ms
step:1002/2315 train_time:60329ms step_avg:60.21ms
step:1003/2315 train_time:60392ms step_avg:60.21ms
step:1004/2315 train_time:60459ms step_avg:60.22ms
step:1005/2315 train_time:60521ms step_avg:60.22ms
step:1006/2315 train_time:60581ms step_avg:60.22ms
step:1007/2315 train_time:60643ms step_avg:60.22ms
step:1008/2315 train_time:60703ms step_avg:60.22ms
step:1009/2315 train_time:60763ms step_avg:60.22ms
step:1010/2315 train_time:60824ms step_avg:60.22ms
step:1011/2315 train_time:60884ms step_avg:60.22ms
step:1012/2315 train_time:60944ms step_avg:60.22ms
step:1013/2315 train_time:61004ms step_avg:60.22ms
step:1014/2315 train_time:61064ms step_avg:60.22ms
step:1015/2315 train_time:61124ms step_avg:60.22ms
step:1016/2315 train_time:61185ms step_avg:60.22ms
step:1017/2315 train_time:61246ms step_avg:60.22ms
step:1018/2315 train_time:61308ms step_avg:60.22ms
step:1019/2315 train_time:61370ms step_avg:60.23ms
step:1020/2315 train_time:61432ms step_avg:60.23ms
step:1021/2315 train_time:61493ms step_avg:60.23ms
step:1022/2315 train_time:61554ms step_avg:60.23ms
step:1023/2315 train_time:61615ms step_avg:60.23ms
step:1024/2315 train_time:61676ms step_avg:60.23ms
step:1025/2315 train_time:61737ms step_avg:60.23ms
step:1026/2315 train_time:61797ms step_avg:60.23ms
step:1027/2315 train_time:61858ms step_avg:60.23ms
step:1028/2315 train_time:61918ms step_avg:60.23ms
step:1029/2315 train_time:61978ms step_avg:60.23ms
step:1030/2315 train_time:62037ms step_avg:60.23ms
step:1031/2315 train_time:62098ms step_avg:60.23ms
step:1032/2315 train_time:62158ms step_avg:60.23ms
step:1033/2315 train_time:62219ms step_avg:60.23ms
step:1034/2315 train_time:62280ms step_avg:60.23ms
step:1035/2315 train_time:62342ms step_avg:60.23ms
step:1036/2315 train_time:62403ms step_avg:60.23ms
step:1037/2315 train_time:62465ms step_avg:60.24ms
step:1038/2315 train_time:62527ms step_avg:60.24ms
step:1039/2315 train_time:62588ms step_avg:60.24ms
step:1040/2315 train_time:62649ms step_avg:60.24ms
step:1041/2315 train_time:62709ms step_avg:60.24ms
step:1042/2315 train_time:62770ms step_avg:60.24ms
step:1043/2315 train_time:62831ms step_avg:60.24ms
step:1044/2315 train_time:62891ms step_avg:60.24ms
step:1045/2315 train_time:62951ms step_avg:60.24ms
step:1046/2315 train_time:63011ms step_avg:60.24ms
step:1047/2315 train_time:63071ms step_avg:60.24ms
step:1048/2315 train_time:63131ms step_avg:60.24ms
step:1049/2315 train_time:63191ms step_avg:60.24ms
step:1050/2315 train_time:63251ms step_avg:60.24ms
step:1051/2315 train_time:63312ms step_avg:60.24ms
step:1052/2315 train_time:63372ms step_avg:60.24ms
step:1053/2315 train_time:63434ms step_avg:60.24ms
step:1054/2315 train_time:63495ms step_avg:60.24ms
step:1055/2315 train_time:63556ms step_avg:60.24ms
step:1056/2315 train_time:63618ms step_avg:60.24ms
step:1057/2315 train_time:63679ms step_avg:60.25ms
step:1058/2315 train_time:63740ms step_avg:60.25ms
step:1059/2315 train_time:63801ms step_avg:60.25ms
step:1060/2315 train_time:63861ms step_avg:60.25ms
step:1061/2315 train_time:63922ms step_avg:60.25ms
step:1062/2315 train_time:63983ms step_avg:60.25ms
step:1063/2315 train_time:64043ms step_avg:60.25ms
step:1064/2315 train_time:64103ms step_avg:60.25ms
step:1065/2315 train_time:64164ms step_avg:60.25ms
step:1066/2315 train_time:64225ms step_avg:60.25ms
step:1067/2315 train_time:64286ms step_avg:60.25ms
step:1068/2315 train_time:64347ms step_avg:60.25ms
step:1069/2315 train_time:64408ms step_avg:60.25ms
step:1070/2315 train_time:64469ms step_avg:60.25ms
step:1071/2315 train_time:64530ms step_avg:60.25ms
step:1072/2315 train_time:64591ms step_avg:60.25ms
step:1073/2315 train_time:64651ms step_avg:60.25ms
step:1074/2315 train_time:64711ms step_avg:60.25ms
step:1075/2315 train_time:64772ms step_avg:60.25ms
step:1076/2315 train_time:64832ms step_avg:60.25ms
step:1077/2315 train_time:64892ms step_avg:60.25ms
step:1078/2315 train_time:64952ms step_avg:60.25ms
step:1079/2315 train_time:65013ms step_avg:60.25ms
step:1080/2315 train_time:65074ms step_avg:60.25ms
step:1081/2315 train_time:65135ms step_avg:60.25ms
step:1082/2315 train_time:65196ms step_avg:60.25ms
step:1083/2315 train_time:65257ms step_avg:60.26ms
step:1084/2315 train_time:65318ms step_avg:60.26ms
step:1085/2315 train_time:65379ms step_avg:60.26ms
step:1086/2315 train_time:65440ms step_avg:60.26ms
step:1087/2315 train_time:65501ms step_avg:60.26ms
step:1088/2315 train_time:65562ms step_avg:60.26ms
step:1089/2315 train_time:65623ms step_avg:60.26ms
step:1090/2315 train_time:65684ms step_avg:60.26ms
step:1091/2315 train_time:65745ms step_avg:60.26ms
step:1092/2315 train_time:65806ms step_avg:60.26ms
step:1093/2315 train_time:65867ms step_avg:60.26ms
step:1094/2315 train_time:65927ms step_avg:60.26ms
step:1095/2315 train_time:65988ms step_avg:60.26ms
step:1096/2315 train_time:66049ms step_avg:60.26ms
step:1097/2315 train_time:66109ms step_avg:60.26ms
step:1098/2315 train_time:66170ms step_avg:60.26ms
step:1099/2315 train_time:66231ms step_avg:60.26ms
step:1100/2315 train_time:66291ms step_avg:60.26ms
step:1101/2315 train_time:66351ms step_avg:60.26ms
step:1102/2315 train_time:66412ms step_avg:60.26ms
step:1103/2315 train_time:66472ms step_avg:60.26ms
step:1104/2315 train_time:66532ms step_avg:60.26ms
step:1105/2315 train_time:66593ms step_avg:60.26ms
step:1106/2315 train_time:66653ms step_avg:60.27ms
step:1107/2315 train_time:66714ms step_avg:60.27ms
step:1108/2315 train_time:66774ms step_avg:60.27ms
step:1109/2315 train_time:66835ms step_avg:60.27ms
step:1110/2315 train_time:66896ms step_avg:60.27ms
step:1111/2315 train_time:66957ms step_avg:60.27ms
step:1112/2315 train_time:67019ms step_avg:60.27ms
step:1113/2315 train_time:67080ms step_avg:60.27ms
step:1114/2315 train_time:67141ms step_avg:60.27ms
step:1115/2315 train_time:67201ms step_avg:60.27ms
step:1116/2315 train_time:67261ms step_avg:60.27ms
step:1117/2315 train_time:67322ms step_avg:60.27ms
step:1118/2315 train_time:67383ms step_avg:60.27ms
step:1119/2315 train_time:67444ms step_avg:60.27ms
step:1120/2315 train_time:67505ms step_avg:60.27ms
step:1121/2315 train_time:67566ms step_avg:60.27ms
step:1122/2315 train_time:67626ms step_avg:60.27ms
step:1123/2315 train_time:67688ms step_avg:60.27ms
step:1124/2315 train_time:67749ms step_avg:60.27ms
step:1125/2315 train_time:67810ms step_avg:60.28ms
step:1126/2315 train_time:67870ms step_avg:60.28ms
step:1127/2315 train_time:67932ms step_avg:60.28ms
step:1128/2315 train_time:67992ms step_avg:60.28ms
step:1129/2315 train_time:68052ms step_avg:60.28ms
step:1130/2315 train_time:68112ms step_avg:60.28ms
step:1131/2315 train_time:68173ms step_avg:60.28ms
step:1132/2315 train_time:68233ms step_avg:60.28ms
step:1133/2315 train_time:68294ms step_avg:60.28ms
step:1134/2315 train_time:68355ms step_avg:60.28ms
step:1135/2315 train_time:68415ms step_avg:60.28ms
step:1136/2315 train_time:68476ms step_avg:60.28ms
step:1137/2315 train_time:68536ms step_avg:60.28ms
step:1138/2315 train_time:68597ms step_avg:60.28ms
step:1139/2315 train_time:68658ms step_avg:60.28ms
step:1140/2315 train_time:68720ms step_avg:60.28ms
step:1141/2315 train_time:68781ms step_avg:60.28ms
step:1142/2315 train_time:68841ms step_avg:60.28ms
step:1143/2315 train_time:68902ms step_avg:60.28ms
step:1144/2315 train_time:68963ms step_avg:60.28ms
step:1145/2315 train_time:69024ms step_avg:60.28ms
step:1146/2315 train_time:69085ms step_avg:60.28ms
step:1147/2315 train_time:69146ms step_avg:60.28ms
step:1148/2315 train_time:69207ms step_avg:60.28ms
step:1149/2315 train_time:69268ms step_avg:60.29ms
step:1150/2315 train_time:69328ms step_avg:60.29ms
step:1151/2315 train_time:69389ms step_avg:60.29ms
step:1152/2315 train_time:69450ms step_avg:60.29ms
step:1153/2315 train_time:69510ms step_avg:60.29ms
step:1154/2315 train_time:69570ms step_avg:60.29ms
step:1155/2315 train_time:69631ms step_avg:60.29ms
step:1156/2315 train_time:69691ms step_avg:60.29ms
step:1157/2315 train_time:69752ms step_avg:60.29ms
step:1158/2315 train_time:69812ms step_avg:60.29ms
step:1159/2315 train_time:69873ms step_avg:60.29ms
step:1160/2315 train_time:69933ms step_avg:60.29ms
step:1161/2315 train_time:69994ms step_avg:60.29ms
step:1162/2315 train_time:70056ms step_avg:60.29ms
step:1163/2315 train_time:70116ms step_avg:60.29ms
step:1164/2315 train_time:70176ms step_avg:60.29ms
step:1165/2315 train_time:70238ms step_avg:60.29ms
step:1166/2315 train_time:70299ms step_avg:60.29ms
step:1167/2315 train_time:70360ms step_avg:60.29ms
step:1168/2315 train_time:70420ms step_avg:60.29ms
step:1169/2315 train_time:70482ms step_avg:60.29ms
step:1170/2315 train_time:70543ms step_avg:60.29ms
step:1171/2315 train_time:70604ms step_avg:60.29ms
step:1172/2315 train_time:70664ms step_avg:60.29ms
step:1173/2315 train_time:70725ms step_avg:60.29ms
step:1174/2315 train_time:70786ms step_avg:60.29ms
step:1175/2315 train_time:70847ms step_avg:60.30ms
step:1176/2315 train_time:70908ms step_avg:60.30ms
step:1177/2315 train_time:70968ms step_avg:60.30ms
step:1178/2315 train_time:71029ms step_avg:60.30ms
step:1179/2315 train_time:71090ms step_avg:60.30ms
step:1180/2315 train_time:71151ms step_avg:60.30ms
step:1181/2315 train_time:71211ms step_avg:60.30ms
step:1182/2315 train_time:71272ms step_avg:60.30ms
step:1183/2315 train_time:71332ms step_avg:60.30ms
step:1184/2315 train_time:71392ms step_avg:60.30ms
step:1185/2315 train_time:71453ms step_avg:60.30ms
step:1186/2315 train_time:71513ms step_avg:60.30ms
step:1187/2315 train_time:71574ms step_avg:60.30ms
step:1188/2315 train_time:71634ms step_avg:60.30ms
step:1189/2315 train_time:71694ms step_avg:60.30ms
step:1190/2315 train_time:71756ms step_avg:60.30ms
step:1191/2315 train_time:71817ms step_avg:60.30ms
step:1192/2315 train_time:71877ms step_avg:60.30ms
step:1193/2315 train_time:71938ms step_avg:60.30ms
step:1194/2315 train_time:71999ms step_avg:60.30ms
step:1195/2315 train_time:72061ms step_avg:60.30ms
step:1196/2315 train_time:72121ms step_avg:60.30ms
step:1197/2315 train_time:72182ms step_avg:60.30ms
step:1198/2315 train_time:72243ms step_avg:60.30ms
step:1199/2315 train_time:72304ms step_avg:60.30ms
step:1200/2315 train_time:72365ms step_avg:60.30ms
step:1201/2315 train_time:72426ms step_avg:60.30ms
step:1202/2315 train_time:72487ms step_avg:60.31ms
step:1203/2315 train_time:72548ms step_avg:60.31ms
step:1204/2315 train_time:72609ms step_avg:60.31ms
step:1205/2315 train_time:72669ms step_avg:60.31ms
step:1206/2315 train_time:72730ms step_avg:60.31ms
step:1207/2315 train_time:72791ms step_avg:60.31ms
step:1208/2315 train_time:72852ms step_avg:60.31ms
step:1209/2315 train_time:72912ms step_avg:60.31ms
step:1210/2315 train_time:72972ms step_avg:60.31ms
step:1211/2315 train_time:73033ms step_avg:60.31ms
step:1212/2315 train_time:73093ms step_avg:60.31ms
step:1213/2315 train_time:73154ms step_avg:60.31ms
step:1214/2315 train_time:73214ms step_avg:60.31ms
step:1215/2315 train_time:73275ms step_avg:60.31ms
step:1216/2315 train_time:73336ms step_avg:60.31ms
step:1217/2315 train_time:73397ms step_avg:60.31ms
step:1218/2315 train_time:73459ms step_avg:60.31ms
step:1219/2315 train_time:73521ms step_avg:60.31ms
step:1220/2315 train_time:73582ms step_avg:60.31ms
step:1221/2315 train_time:73643ms step_avg:60.31ms
step:1222/2315 train_time:73703ms step_avg:60.31ms
step:1223/2315 train_time:73764ms step_avg:60.31ms
step:1224/2315 train_time:73825ms step_avg:60.31ms
step:1225/2315 train_time:73886ms step_avg:60.32ms
step:1226/2315 train_time:73947ms step_avg:60.32ms
step:1227/2315 train_time:74008ms step_avg:60.32ms
step:1228/2315 train_time:74069ms step_avg:60.32ms
step:1229/2315 train_time:74130ms step_avg:60.32ms
step:1230/2315 train_time:74190ms step_avg:60.32ms
step:1231/2315 train_time:74251ms step_avg:60.32ms
step:1232/2315 train_time:74311ms step_avg:60.32ms
step:1233/2315 train_time:74372ms step_avg:60.32ms
step:1234/2315 train_time:74432ms step_avg:60.32ms
step:1235/2315 train_time:74493ms step_avg:60.32ms
step:1236/2315 train_time:74553ms step_avg:60.32ms
step:1237/2315 train_time:74614ms step_avg:60.32ms
step:1238/2315 train_time:74674ms step_avg:60.32ms
step:1239/2315 train_time:74735ms step_avg:60.32ms
step:1240/2315 train_time:74795ms step_avg:60.32ms
step:1241/2315 train_time:74857ms step_avg:60.32ms
step:1242/2315 train_time:74917ms step_avg:60.32ms
step:1243/2315 train_time:74978ms step_avg:60.32ms
step:1244/2315 train_time:75038ms step_avg:60.32ms
step:1245/2315 train_time:75099ms step_avg:60.32ms
step:1246/2315 train_time:75160ms step_avg:60.32ms
step:1247/2315 train_time:75221ms step_avg:60.32ms
step:1248/2315 train_time:75281ms step_avg:60.32ms
step:1249/2315 train_time:75343ms step_avg:60.32ms
step:1250/2315 train_time:75404ms step_avg:60.32ms
step:1250/2315 val_loss:3.5140 train_time:75467ms step_avg:60.37ms
step:1251/2315 train_time:75486ms step_avg:60.34ms
step:1252/2315 train_time:75528ms step_avg:60.33ms
step:1253/2315 train_time:75593ms step_avg:60.33ms
step:1254/2315 train_time:75656ms step_avg:60.33ms
step:1255/2315 train_time:75718ms step_avg:60.33ms
step:1256/2315 train_time:75779ms step_avg:60.33ms
step:1257/2315 train_time:75840ms step_avg:60.33ms
step:1258/2315 train_time:75899ms step_avg:60.33ms
step:1259/2315 train_time:75959ms step_avg:60.33ms
step:1260/2315 train_time:76019ms step_avg:60.33ms
step:1261/2315 train_time:76078ms step_avg:60.33ms
step:1262/2315 train_time:76138ms step_avg:60.33ms
step:1263/2315 train_time:76198ms step_avg:60.33ms
step:1264/2315 train_time:76258ms step_avg:60.33ms
step:1265/2315 train_time:76317ms step_avg:60.33ms
step:1266/2315 train_time:76377ms step_avg:60.33ms
step:1267/2315 train_time:76438ms step_avg:60.33ms
step:1268/2315 train_time:76499ms step_avg:60.33ms
step:1269/2315 train_time:76560ms step_avg:60.33ms
step:1270/2315 train_time:76622ms step_avg:60.33ms
step:1271/2315 train_time:76683ms step_avg:60.33ms
step:1272/2315 train_time:76744ms step_avg:60.33ms
step:1273/2315 train_time:76805ms step_avg:60.33ms
step:1274/2315 train_time:76866ms step_avg:60.33ms
step:1275/2315 train_time:76927ms step_avg:60.33ms
step:1276/2315 train_time:76987ms step_avg:60.33ms
step:1277/2315 train_time:77048ms step_avg:60.33ms
step:1278/2315 train_time:77108ms step_avg:60.34ms
step:1279/2315 train_time:77169ms step_avg:60.34ms
step:1280/2315 train_time:77229ms step_avg:60.34ms
step:1281/2315 train_time:77290ms step_avg:60.34ms
step:1282/2315 train_time:77350ms step_avg:60.34ms
step:1283/2315 train_time:77411ms step_avg:60.34ms
step:1284/2315 train_time:77471ms step_avg:60.34ms
step:1285/2315 train_time:77532ms step_avg:60.34ms
step:1286/2315 train_time:77593ms step_avg:60.34ms
step:1287/2315 train_time:77655ms step_avg:60.34ms
step:1288/2315 train_time:77717ms step_avg:60.34ms
step:1289/2315 train_time:77779ms step_avg:60.34ms
step:1290/2315 train_time:77839ms step_avg:60.34ms
step:1291/2315 train_time:77900ms step_avg:60.34ms
step:1292/2315 train_time:77960ms step_avg:60.34ms
step:1293/2315 train_time:78020ms step_avg:60.34ms
step:1294/2315 train_time:78080ms step_avg:60.34ms
step:1295/2315 train_time:78140ms step_avg:60.34ms
step:1296/2315 train_time:78200ms step_avg:60.34ms
step:1297/2315 train_time:78260ms step_avg:60.34ms
step:1298/2315 train_time:78320ms step_avg:60.34ms
step:1299/2315 train_time:78381ms step_avg:60.34ms
step:1300/2315 train_time:78441ms step_avg:60.34ms
step:1301/2315 train_time:78502ms step_avg:60.34ms
step:1302/2315 train_time:78563ms step_avg:60.34ms
step:1303/2315 train_time:78625ms step_avg:60.34ms
step:1304/2315 train_time:78686ms step_avg:60.34ms
step:1305/2315 train_time:78748ms step_avg:60.34ms
step:1306/2315 train_time:78809ms step_avg:60.34ms
step:1307/2315 train_time:78870ms step_avg:60.34ms
step:1308/2315 train_time:78931ms step_avg:60.34ms
step:1309/2315 train_time:78992ms step_avg:60.35ms
step:1310/2315 train_time:79053ms step_avg:60.35ms
step:1311/2315 train_time:79113ms step_avg:60.35ms
step:1312/2315 train_time:79174ms step_avg:60.35ms
step:1313/2315 train_time:79235ms step_avg:60.35ms
step:1314/2315 train_time:79295ms step_avg:60.35ms
step:1315/2315 train_time:79356ms step_avg:60.35ms
step:1316/2315 train_time:79417ms step_avg:60.35ms
step:1317/2315 train_time:79477ms step_avg:60.35ms
step:1318/2315 train_time:79538ms step_avg:60.35ms
step:1319/2315 train_time:79599ms step_avg:60.35ms
step:1320/2315 train_time:79659ms step_avg:60.35ms
step:1321/2315 train_time:79720ms step_avg:60.35ms
step:1322/2315 train_time:79781ms step_avg:60.35ms
step:1323/2315 train_time:79842ms step_avg:60.35ms
step:1324/2315 train_time:79902ms step_avg:60.35ms
step:1325/2315 train_time:79962ms step_avg:60.35ms
step:1326/2315 train_time:80023ms step_avg:60.35ms
step:1327/2315 train_time:80084ms step_avg:60.35ms
step:1328/2315 train_time:80145ms step_avg:60.35ms
step:1329/2315 train_time:80206ms step_avg:60.35ms
step:1330/2315 train_time:80266ms step_avg:60.35ms
step:1331/2315 train_time:80328ms step_avg:60.35ms
step:1332/2315 train_time:80389ms step_avg:60.35ms
step:1333/2315 train_time:80450ms step_avg:60.35ms
step:1334/2315 train_time:80511ms step_avg:60.35ms
step:1335/2315 train_time:80572ms step_avg:60.35ms
step:1336/2315 train_time:80633ms step_avg:60.35ms
step:1337/2315 train_time:80694ms step_avg:60.35ms
step:1338/2315 train_time:80755ms step_avg:60.36ms
step:1339/2315 train_time:80816ms step_avg:60.36ms
step:1340/2315 train_time:80877ms step_avg:60.36ms
step:1341/2315 train_time:80937ms step_avg:60.36ms
step:1342/2315 train_time:80998ms step_avg:60.36ms
step:1343/2315 train_time:81058ms step_avg:60.36ms
step:1344/2315 train_time:81119ms step_avg:60.36ms
step:1345/2315 train_time:81179ms step_avg:60.36ms
step:1346/2315 train_time:81240ms step_avg:60.36ms
step:1347/2315 train_time:81300ms step_avg:60.36ms
step:1348/2315 train_time:81360ms step_avg:60.36ms
step:1349/2315 train_time:81421ms step_avg:60.36ms
step:1350/2315 train_time:81480ms step_avg:60.36ms
step:1351/2315 train_time:81541ms step_avg:60.36ms
step:1352/2315 train_time:81603ms step_avg:60.36ms
step:1353/2315 train_time:81663ms step_avg:60.36ms
step:1354/2315 train_time:81724ms step_avg:60.36ms
step:1355/2315 train_time:81785ms step_avg:60.36ms
step:1356/2315 train_time:81846ms step_avg:60.36ms
step:1357/2315 train_time:81907ms step_avg:60.36ms
step:1358/2315 train_time:81968ms step_avg:60.36ms
step:1359/2315 train_time:82030ms step_avg:60.36ms
step:1360/2315 train_time:82090ms step_avg:60.36ms
step:1361/2315 train_time:82151ms step_avg:60.36ms
step:1362/2315 train_time:82212ms step_avg:60.36ms
step:1363/2315 train_time:82273ms step_avg:60.36ms
step:1364/2315 train_time:82333ms step_avg:60.36ms
step:1365/2315 train_time:82394ms step_avg:60.36ms
step:1366/2315 train_time:82455ms step_avg:60.36ms
step:1367/2315 train_time:82516ms step_avg:60.36ms
step:1368/2315 train_time:82576ms step_avg:60.36ms
step:1369/2315 train_time:82638ms step_avg:60.36ms
step:1370/2315 train_time:82698ms step_avg:60.36ms
step:1371/2315 train_time:82759ms step_avg:60.36ms
step:1372/2315 train_time:82819ms step_avg:60.36ms
step:1373/2315 train_time:82880ms step_avg:60.36ms
step:1374/2315 train_time:82940ms step_avg:60.36ms
step:1375/2315 train_time:83001ms step_avg:60.36ms
step:1376/2315 train_time:83061ms step_avg:60.36ms
step:1377/2315 train_time:83121ms step_avg:60.36ms
step:1378/2315 train_time:83181ms step_avg:60.36ms
step:1379/2315 train_time:83242ms step_avg:60.36ms
step:1380/2315 train_time:83302ms step_avg:60.36ms
step:1381/2315 train_time:83363ms step_avg:60.36ms
step:1382/2315 train_time:83424ms step_avg:60.36ms
step:1383/2315 train_time:83485ms step_avg:60.37ms
step:1384/2315 train_time:83545ms step_avg:60.37ms
step:1385/2315 train_time:83606ms step_avg:60.37ms
step:1386/2315 train_time:83667ms step_avg:60.37ms
step:1387/2315 train_time:83729ms step_avg:60.37ms
step:1388/2315 train_time:83789ms step_avg:60.37ms
step:1389/2315 train_time:83851ms step_avg:60.37ms
step:1390/2315 train_time:83911ms step_avg:60.37ms
step:1391/2315 train_time:83972ms step_avg:60.37ms
step:1392/2315 train_time:84033ms step_avg:60.37ms
step:1393/2315 train_time:84094ms step_avg:60.37ms
step:1394/2315 train_time:84154ms step_avg:60.37ms
step:1395/2315 train_time:84216ms step_avg:60.37ms
step:1396/2315 train_time:84276ms step_avg:60.37ms
step:1397/2315 train_time:84337ms step_avg:60.37ms
step:1398/2315 train_time:84397ms step_avg:60.37ms
step:1399/2315 train_time:84458ms step_avg:60.37ms
step:1400/2315 train_time:84518ms step_avg:60.37ms
step:1401/2315 train_time:84579ms step_avg:60.37ms
step:1402/2315 train_time:84639ms step_avg:60.37ms
step:1403/2315 train_time:84699ms step_avg:60.37ms
step:1404/2315 train_time:84759ms step_avg:60.37ms
step:1405/2315 train_time:84820ms step_avg:60.37ms
step:1406/2315 train_time:84880ms step_avg:60.37ms
step:1407/2315 train_time:84940ms step_avg:60.37ms
step:1408/2315 train_time:85000ms step_avg:60.37ms
step:1409/2315 train_time:85060ms step_avg:60.37ms
step:1410/2315 train_time:85121ms step_avg:60.37ms
step:1411/2315 train_time:85183ms step_avg:60.37ms
step:1412/2315 train_time:85244ms step_avg:60.37ms
step:1413/2315 train_time:85305ms step_avg:60.37ms
step:1414/2315 train_time:85365ms step_avg:60.37ms
step:1415/2315 train_time:85426ms step_avg:60.37ms
step:1416/2315 train_time:85488ms step_avg:60.37ms
step:1417/2315 train_time:85549ms step_avg:60.37ms
step:1418/2315 train_time:85609ms step_avg:60.37ms
step:1419/2315 train_time:85671ms step_avg:60.37ms
step:1420/2315 train_time:85732ms step_avg:60.37ms
step:1421/2315 train_time:85793ms step_avg:60.38ms
step:1422/2315 train_time:85854ms step_avg:60.38ms
step:1423/2315 train_time:85915ms step_avg:60.38ms
step:1424/2315 train_time:85975ms step_avg:60.38ms
step:1425/2315 train_time:86036ms step_avg:60.38ms
step:1426/2315 train_time:86096ms step_avg:60.38ms
step:1427/2315 train_time:86158ms step_avg:60.38ms
step:1428/2315 train_time:86218ms step_avg:60.38ms
step:1429/2315 train_time:86279ms step_avg:60.38ms
step:1430/2315 train_time:86340ms step_avg:60.38ms
step:1431/2315 train_time:86400ms step_avg:60.38ms
step:1432/2315 train_time:86461ms step_avg:60.38ms
step:1433/2315 train_time:86521ms step_avg:60.38ms
step:1434/2315 train_time:86581ms step_avg:60.38ms
step:1435/2315 train_time:86641ms step_avg:60.38ms
step:1436/2315 train_time:86701ms step_avg:60.38ms
step:1437/2315 train_time:86762ms step_avg:60.38ms
step:1438/2315 train_time:86822ms step_avg:60.38ms
step:1439/2315 train_time:86883ms step_avg:60.38ms
step:1440/2315 train_time:86944ms step_avg:60.38ms
step:1441/2315 train_time:87005ms step_avg:60.38ms
step:1442/2315 train_time:87066ms step_avg:60.38ms
step:1443/2315 train_time:87128ms step_avg:60.38ms
step:1444/2315 train_time:87189ms step_avg:60.38ms
step:1445/2315 train_time:87250ms step_avg:60.38ms
step:1446/2315 train_time:87310ms step_avg:60.38ms
step:1447/2315 train_time:87372ms step_avg:60.38ms
step:1448/2315 train_time:87432ms step_avg:60.38ms
step:1449/2315 train_time:87494ms step_avg:60.38ms
step:1450/2315 train_time:87554ms step_avg:60.38ms
step:1451/2315 train_time:87615ms step_avg:60.38ms
step:1452/2315 train_time:87676ms step_avg:60.38ms
step:1453/2315 train_time:87736ms step_avg:60.38ms
step:1454/2315 train_time:87797ms step_avg:60.38ms
step:1455/2315 train_time:87858ms step_avg:60.38ms
step:1456/2315 train_time:87918ms step_avg:60.38ms
step:1457/2315 train_time:87979ms step_avg:60.38ms
step:1458/2315 train_time:88039ms step_avg:60.38ms
step:1459/2315 train_time:88099ms step_avg:60.38ms
step:1460/2315 train_time:88160ms step_avg:60.38ms
step:1461/2315 train_time:88220ms step_avg:60.38ms
step:1462/2315 train_time:88281ms step_avg:60.38ms
step:1463/2315 train_time:88342ms step_avg:60.38ms
step:1464/2315 train_time:88404ms step_avg:60.38ms
step:1465/2315 train_time:88465ms step_avg:60.39ms
step:1466/2315 train_time:88526ms step_avg:60.39ms
step:1467/2315 train_time:88588ms step_avg:60.39ms
step:1468/2315 train_time:88649ms step_avg:60.39ms
step:1469/2315 train_time:88710ms step_avg:60.39ms
step:1470/2315 train_time:88770ms step_avg:60.39ms
step:1471/2315 train_time:88831ms step_avg:60.39ms
step:1472/2315 train_time:88891ms step_avg:60.39ms
step:1473/2315 train_time:88952ms step_avg:60.39ms
step:1474/2315 train_time:89013ms step_avg:60.39ms
step:1475/2315 train_time:89074ms step_avg:60.39ms
step:1476/2315 train_time:89135ms step_avg:60.39ms
step:1477/2315 train_time:89196ms step_avg:60.39ms
step:1478/2315 train_time:89257ms step_avg:60.39ms
step:1479/2315 train_time:89318ms step_avg:60.39ms
step:1480/2315 train_time:89379ms step_avg:60.39ms
step:1481/2315 train_time:89439ms step_avg:60.39ms
step:1482/2315 train_time:89500ms step_avg:60.39ms
step:1483/2315 train_time:89560ms step_avg:60.39ms
step:1484/2315 train_time:89621ms step_avg:60.39ms
step:1485/2315 train_time:89681ms step_avg:60.39ms
step:1486/2315 train_time:89741ms step_avg:60.39ms
step:1487/2315 train_time:89803ms step_avg:60.39ms
step:1488/2315 train_time:89864ms step_avg:60.39ms
step:1489/2315 train_time:89925ms step_avg:60.39ms
step:1490/2315 train_time:89987ms step_avg:60.39ms
step:1491/2315 train_time:90048ms step_avg:60.39ms
step:1492/2315 train_time:90109ms step_avg:60.39ms
step:1493/2315 train_time:90170ms step_avg:60.39ms
step:1494/2315 train_time:90230ms step_avg:60.39ms
step:1495/2315 train_time:90291ms step_avg:60.40ms
step:1496/2315 train_time:90352ms step_avg:60.40ms
step:1497/2315 train_time:90413ms step_avg:60.40ms
step:1498/2315 train_time:90474ms step_avg:60.40ms
step:1499/2315 train_time:90536ms step_avg:60.40ms
step:1500/2315 train_time:90596ms step_avg:60.40ms
step:1500/2315 val_loss:3.4498 train_time:90659ms step_avg:60.44ms
step:1501/2315 train_time:90679ms step_avg:60.41ms
step:1502/2315 train_time:90719ms step_avg:60.40ms
step:1503/2315 train_time:90783ms step_avg:60.40ms
step:1504/2315 train_time:90846ms step_avg:60.40ms
step:1505/2315 train_time:90907ms step_avg:60.40ms
step:1506/2315 train_time:90968ms step_avg:60.40ms
step:1507/2315 train_time:91029ms step_avg:60.40ms
step:1508/2315 train_time:91089ms step_avg:60.40ms
step:1509/2315 train_time:91150ms step_avg:60.40ms
step:1510/2315 train_time:91209ms step_avg:60.40ms
step:1511/2315 train_time:91269ms step_avg:60.40ms
step:1512/2315 train_time:91329ms step_avg:60.40ms
step:1513/2315 train_time:91389ms step_avg:60.40ms
step:1514/2315 train_time:91449ms step_avg:60.40ms
step:1515/2315 train_time:91509ms step_avg:60.40ms
step:1516/2315 train_time:91568ms step_avg:60.40ms
step:1517/2315 train_time:91629ms step_avg:60.40ms
step:1518/2315 train_time:91690ms step_avg:60.40ms
step:1519/2315 train_time:91752ms step_avg:60.40ms
step:1520/2315 train_time:91814ms step_avg:60.40ms
step:1521/2315 train_time:91876ms step_avg:60.40ms
step:1522/2315 train_time:91937ms step_avg:60.41ms
step:1523/2315 train_time:91998ms step_avg:60.41ms
step:1524/2315 train_time:92060ms step_avg:60.41ms
step:1525/2315 train_time:92122ms step_avg:60.41ms
step:1526/2315 train_time:92183ms step_avg:60.41ms
step:1527/2315 train_time:92243ms step_avg:60.41ms
step:1528/2315 train_time:92304ms step_avg:60.41ms
step:1529/2315 train_time:92366ms step_avg:60.41ms
step:1530/2315 train_time:92427ms step_avg:60.41ms
step:1531/2315 train_time:92487ms step_avg:60.41ms
step:1532/2315 train_time:92548ms step_avg:60.41ms
step:1533/2315 train_time:92609ms step_avg:60.41ms
step:1534/2315 train_time:92670ms step_avg:60.41ms
step:1535/2315 train_time:92731ms step_avg:60.41ms
step:1536/2315 train_time:92792ms step_avg:60.41ms
step:1537/2315 train_time:92854ms step_avg:60.41ms
step:1538/2315 train_time:92915ms step_avg:60.41ms
step:1539/2315 train_time:92977ms step_avg:60.41ms
step:1540/2315 train_time:93038ms step_avg:60.41ms
step:1541/2315 train_time:93100ms step_avg:60.42ms
step:1542/2315 train_time:93161ms step_avg:60.42ms
step:1543/2315 train_time:93222ms step_avg:60.42ms
step:1544/2315 train_time:93283ms step_avg:60.42ms
step:1545/2315 train_time:93344ms step_avg:60.42ms
step:1546/2315 train_time:93405ms step_avg:60.42ms
step:1547/2315 train_time:93466ms step_avg:60.42ms
step:1548/2315 train_time:93527ms step_avg:60.42ms
step:1549/2315 train_time:93587ms step_avg:60.42ms
step:1550/2315 train_time:93649ms step_avg:60.42ms
step:1551/2315 train_time:93710ms step_avg:60.42ms
step:1552/2315 train_time:93771ms step_avg:60.42ms
step:1553/2315 train_time:93833ms step_avg:60.42ms
step:1554/2315 train_time:93894ms step_avg:60.42ms
step:1555/2315 train_time:93955ms step_avg:60.42ms
step:1556/2315 train_time:94017ms step_avg:60.42ms
step:1557/2315 train_time:94079ms step_avg:60.42ms
step:1558/2315 train_time:94140ms step_avg:60.42ms
step:1559/2315 train_time:94201ms step_avg:60.42ms
step:1560/2315 train_time:94262ms step_avg:60.42ms
step:1561/2315 train_time:94323ms step_avg:60.42ms
step:1562/2315 train_time:94384ms step_avg:60.42ms
step:1563/2315 train_time:94445ms step_avg:60.43ms
step:1564/2315 train_time:94506ms step_avg:60.43ms
step:1565/2315 train_time:94567ms step_avg:60.43ms
step:1566/2315 train_time:94628ms step_avg:60.43ms
step:1567/2315 train_time:94690ms step_avg:60.43ms
step:1568/2315 train_time:94751ms step_avg:60.43ms
step:1569/2315 train_time:94813ms step_avg:60.43ms
step:1570/2315 train_time:94875ms step_avg:60.43ms
step:1571/2315 train_time:94935ms step_avg:60.43ms
step:1572/2315 train_time:94996ms step_avg:60.43ms
step:1573/2315 train_time:95057ms step_avg:60.43ms
step:1574/2315 train_time:95119ms step_avg:60.43ms
step:1575/2315 train_time:95181ms step_avg:60.43ms
step:1576/2315 train_time:95241ms step_avg:60.43ms
step:1577/2315 train_time:95302ms step_avg:60.43ms
step:1578/2315 train_time:95363ms step_avg:60.43ms
step:1579/2315 train_time:95425ms step_avg:60.43ms
step:1580/2315 train_time:95486ms step_avg:60.43ms
step:1581/2315 train_time:95547ms step_avg:60.43ms
step:1582/2315 train_time:95609ms step_avg:60.44ms
step:1583/2315 train_time:95670ms step_avg:60.44ms
step:1584/2315 train_time:95732ms step_avg:60.44ms
step:1585/2315 train_time:95793ms step_avg:60.44ms
step:1586/2315 train_time:95854ms step_avg:60.44ms
step:1587/2315 train_time:95915ms step_avg:60.44ms
step:1588/2315 train_time:95976ms step_avg:60.44ms
step:1589/2315 train_time:96037ms step_avg:60.44ms
step:1590/2315 train_time:96098ms step_avg:60.44ms
step:1591/2315 train_time:96159ms step_avg:60.44ms
step:1592/2315 train_time:96221ms step_avg:60.44ms
step:1593/2315 train_time:96283ms step_avg:60.44ms
step:1594/2315 train_time:96344ms step_avg:60.44ms
step:1595/2315 train_time:96405ms step_avg:60.44ms
step:1596/2315 train_time:96466ms step_avg:60.44ms
step:1597/2315 train_time:96527ms step_avg:60.44ms
step:1598/2315 train_time:96588ms step_avg:60.44ms
step:1599/2315 train_time:96650ms step_avg:60.44ms
step:1600/2315 train_time:96711ms step_avg:60.44ms
step:1601/2315 train_time:96773ms step_avg:60.45ms
step:1602/2315 train_time:96834ms step_avg:60.45ms
step:1603/2315 train_time:96895ms step_avg:60.45ms
step:1604/2315 train_time:96956ms step_avg:60.45ms
step:1605/2315 train_time:97016ms step_avg:60.45ms
step:1606/2315 train_time:97077ms step_avg:60.45ms
step:1607/2315 train_time:97139ms step_avg:60.45ms
step:1608/2315 train_time:97200ms step_avg:60.45ms
step:1609/2315 train_time:97261ms step_avg:60.45ms
step:1610/2315 train_time:97322ms step_avg:60.45ms
step:1611/2315 train_time:97384ms step_avg:60.45ms
step:1612/2315 train_time:97445ms step_avg:60.45ms
step:1613/2315 train_time:97507ms step_avg:60.45ms
step:1614/2315 train_time:97568ms step_avg:60.45ms
step:1615/2315 train_time:97629ms step_avg:60.45ms
step:1616/2315 train_time:97690ms step_avg:60.45ms
step:1617/2315 train_time:97751ms step_avg:60.45ms
step:1618/2315 train_time:97812ms step_avg:60.45ms
step:1619/2315 train_time:97873ms step_avg:60.45ms
step:1620/2315 train_time:97935ms step_avg:60.45ms
step:1621/2315 train_time:97996ms step_avg:60.45ms
step:1622/2315 train_time:98056ms step_avg:60.45ms
step:1623/2315 train_time:98118ms step_avg:60.45ms
step:1624/2315 train_time:98180ms step_avg:60.46ms
step:1625/2315 train_time:98242ms step_avg:60.46ms
step:1626/2315 train_time:98303ms step_avg:60.46ms
step:1627/2315 train_time:98364ms step_avg:60.46ms
step:1628/2315 train_time:98425ms step_avg:60.46ms
step:1629/2315 train_time:98487ms step_avg:60.46ms
step:1630/2315 train_time:98548ms step_avg:60.46ms
step:1631/2315 train_time:98610ms step_avg:60.46ms
step:1632/2315 train_time:98671ms step_avg:60.46ms
step:1633/2315 train_time:98732ms step_avg:60.46ms
step:1634/2315 train_time:98793ms step_avg:60.46ms
step:1635/2315 train_time:98854ms step_avg:60.46ms
step:1636/2315 train_time:98915ms step_avg:60.46ms
step:1637/2315 train_time:98976ms step_avg:60.46ms
step:1638/2315 train_time:99036ms step_avg:60.46ms
step:1639/2315 train_time:99097ms step_avg:60.46ms
step:1640/2315 train_time:99160ms step_avg:60.46ms
step:1641/2315 train_time:99222ms step_avg:60.46ms
step:1642/2315 train_time:99283ms step_avg:60.46ms
step:1643/2315 train_time:99344ms step_avg:60.46ms
step:1644/2315 train_time:99406ms step_avg:60.47ms
step:1645/2315 train_time:99466ms step_avg:60.47ms
step:1646/2315 train_time:99527ms step_avg:60.47ms
step:1647/2315 train_time:99588ms step_avg:60.47ms
step:1648/2315 train_time:99649ms step_avg:60.47ms
step:1649/2315 train_time:99710ms step_avg:60.47ms
step:1650/2315 train_time:99771ms step_avg:60.47ms
step:1651/2315 train_time:99832ms step_avg:60.47ms
step:1652/2315 train_time:99894ms step_avg:60.47ms
step:1653/2315 train_time:99954ms step_avg:60.47ms
step:1654/2315 train_time:100016ms step_avg:60.47ms
step:1655/2315 train_time:100077ms step_avg:60.47ms
step:1656/2315 train_time:100138ms step_avg:60.47ms
step:1657/2315 train_time:100200ms step_avg:60.47ms
step:1658/2315 train_time:100261ms step_avg:60.47ms
step:1659/2315 train_time:100322ms step_avg:60.47ms
step:1660/2315 train_time:100383ms step_avg:60.47ms
step:1661/2315 train_time:100445ms step_avg:60.47ms
step:1662/2315 train_time:100506ms step_avg:60.47ms
step:1663/2315 train_time:100567ms step_avg:60.47ms
step:1664/2315 train_time:100628ms step_avg:60.47ms
step:1665/2315 train_time:100690ms step_avg:60.47ms
step:1666/2315 train_time:100751ms step_avg:60.47ms
step:1667/2315 train_time:100812ms step_avg:60.48ms
step:1668/2315 train_time:100874ms step_avg:60.48ms
step:1669/2315 train_time:100934ms step_avg:60.48ms
step:1670/2315 train_time:100996ms step_avg:60.48ms
step:1671/2315 train_time:101057ms step_avg:60.48ms
step:1672/2315 train_time:101118ms step_avg:60.48ms
step:1673/2315 train_time:101179ms step_avg:60.48ms
step:1674/2315 train_time:101240ms step_avg:60.48ms
step:1675/2315 train_time:101301ms step_avg:60.48ms
step:1676/2315 train_time:101362ms step_avg:60.48ms
step:1677/2315 train_time:101424ms step_avg:60.48ms
step:1678/2315 train_time:101485ms step_avg:60.48ms
step:1679/2315 train_time:101547ms step_avg:60.48ms
step:1680/2315 train_time:101608ms step_avg:60.48ms
step:1681/2315 train_time:101670ms step_avg:60.48ms
step:1682/2315 train_time:101731ms step_avg:60.48ms
step:1683/2315 train_time:101792ms step_avg:60.48ms
step:1684/2315 train_time:101852ms step_avg:60.48ms
step:1685/2315 train_time:101913ms step_avg:60.48ms
step:1686/2315 train_time:101974ms step_avg:60.48ms
step:1687/2315 train_time:102035ms step_avg:60.48ms
step:1688/2315 train_time:102096ms step_avg:60.48ms
step:1689/2315 train_time:102158ms step_avg:60.48ms
step:1690/2315 train_time:102219ms step_avg:60.48ms
step:1691/2315 train_time:102280ms step_avg:60.48ms
step:1692/2315 train_time:102342ms step_avg:60.49ms
step:1693/2315 train_time:102403ms step_avg:60.49ms
step:1694/2315 train_time:102463ms step_avg:60.49ms
step:1695/2315 train_time:102525ms step_avg:60.49ms
step:1696/2315 train_time:102586ms step_avg:60.49ms
step:1697/2315 train_time:102648ms step_avg:60.49ms
step:1698/2315 train_time:102710ms step_avg:60.49ms
step:1699/2315 train_time:102771ms step_avg:60.49ms
step:1700/2315 train_time:102832ms step_avg:60.49ms
step:1701/2315 train_time:102893ms step_avg:60.49ms
step:1702/2315 train_time:102954ms step_avg:60.49ms
step:1703/2315 train_time:103014ms step_avg:60.49ms
step:1704/2315 train_time:103075ms step_avg:60.49ms
step:1705/2315 train_time:103136ms step_avg:60.49ms
step:1706/2315 train_time:103197ms step_avg:60.49ms
step:1707/2315 train_time:103259ms step_avg:60.49ms
step:1708/2315 train_time:103320ms step_avg:60.49ms
step:1709/2315 train_time:103381ms step_avg:60.49ms
step:1710/2315 train_time:103442ms step_avg:60.49ms
step:1711/2315 train_time:103504ms step_avg:60.49ms
step:1712/2315 train_time:103565ms step_avg:60.49ms
step:1713/2315 train_time:103627ms step_avg:60.49ms
step:1714/2315 train_time:103688ms step_avg:60.49ms
step:1715/2315 train_time:103750ms step_avg:60.50ms
step:1716/2315 train_time:103811ms step_avg:60.50ms
step:1717/2315 train_time:103872ms step_avg:60.50ms
step:1718/2315 train_time:103933ms step_avg:60.50ms
step:1719/2315 train_time:103994ms step_avg:60.50ms
step:1720/2315 train_time:104055ms step_avg:60.50ms
step:1721/2315 train_time:104116ms step_avg:60.50ms
step:1722/2315 train_time:104177ms step_avg:60.50ms
step:1723/2315 train_time:104238ms step_avg:60.50ms
step:1724/2315 train_time:104299ms step_avg:60.50ms
step:1725/2315 train_time:104361ms step_avg:60.50ms
step:1726/2315 train_time:104422ms step_avg:60.50ms
step:1727/2315 train_time:104483ms step_avg:60.50ms
step:1728/2315 train_time:104545ms step_avg:60.50ms
step:1729/2315 train_time:104606ms step_avg:60.50ms
step:1730/2315 train_time:104668ms step_avg:60.50ms
step:1731/2315 train_time:104729ms step_avg:60.50ms
step:1732/2315 train_time:104791ms step_avg:60.50ms
step:1733/2315 train_time:104852ms step_avg:60.50ms
step:1734/2315 train_time:104913ms step_avg:60.50ms
step:1735/2315 train_time:104973ms step_avg:60.50ms
step:1736/2315 train_time:105034ms step_avg:60.50ms
step:1737/2315 train_time:105095ms step_avg:60.50ms
step:1738/2315 train_time:105156ms step_avg:60.50ms
step:1739/2315 train_time:105218ms step_avg:60.50ms
step:1740/2315 train_time:105279ms step_avg:60.51ms
step:1741/2315 train_time:105341ms step_avg:60.51ms
step:1742/2315 train_time:105401ms step_avg:60.51ms
step:1743/2315 train_time:105462ms step_avg:60.51ms
step:1744/2315 train_time:105524ms step_avg:60.51ms
step:1745/2315 train_time:105585ms step_avg:60.51ms
step:1746/2315 train_time:105647ms step_avg:60.51ms
step:1747/2315 train_time:105709ms step_avg:60.51ms
step:1748/2315 train_time:105770ms step_avg:60.51ms
step:1749/2315 train_time:105831ms step_avg:60.51ms
step:1750/2315 train_time:105892ms step_avg:60.51ms
step:1750/2315 val_loss:3.3820 train_time:105955ms step_avg:60.55ms
step:1751/2315 train_time:105974ms step_avg:60.52ms
step:1752/2315 train_time:106015ms step_avg:60.51ms
step:1753/2315 train_time:106079ms step_avg:60.51ms
step:1754/2315 train_time:106144ms step_avg:60.52ms
step:1755/2315 train_time:106205ms step_avg:60.52ms
step:1756/2315 train_time:106266ms step_avg:60.52ms
step:1757/2315 train_time:106326ms step_avg:60.52ms
step:1758/2315 train_time:106388ms step_avg:60.52ms
step:1759/2315 train_time:106448ms step_avg:60.52ms
step:1760/2315 train_time:106509ms step_avg:60.52ms
step:1761/2315 train_time:106569ms step_avg:60.52ms
step:1762/2315 train_time:106629ms step_avg:60.52ms
step:1763/2315 train_time:106689ms step_avg:60.52ms
step:1764/2315 train_time:106749ms step_avg:60.52ms
step:1765/2315 train_time:106810ms step_avg:60.52ms
step:1766/2315 train_time:106870ms step_avg:60.52ms
step:1767/2315 train_time:106932ms step_avg:60.52ms
step:1768/2315 train_time:106995ms step_avg:60.52ms
step:1769/2315 train_time:107058ms step_avg:60.52ms
step:1770/2315 train_time:107120ms step_avg:60.52ms
step:1771/2315 train_time:107181ms step_avg:60.52ms
step:1772/2315 train_time:107243ms step_avg:60.52ms
step:1773/2315 train_time:107303ms step_avg:60.52ms
step:1774/2315 train_time:107364ms step_avg:60.52ms
step:1775/2315 train_time:107426ms step_avg:60.52ms
step:1776/2315 train_time:107486ms step_avg:60.52ms
step:1777/2315 train_time:107547ms step_avg:60.52ms
step:1778/2315 train_time:107608ms step_avg:60.52ms
step:1779/2315 train_time:107668ms step_avg:60.52ms
step:1780/2315 train_time:107729ms step_avg:60.52ms
step:1781/2315 train_time:107789ms step_avg:60.52ms
step:1782/2315 train_time:107850ms step_avg:60.52ms
step:1783/2315 train_time:107912ms step_avg:60.52ms
step:1784/2315 train_time:107973ms step_avg:60.52ms
step:1785/2315 train_time:108036ms step_avg:60.52ms
step:1786/2315 train_time:108098ms step_avg:60.53ms
step:1787/2315 train_time:108160ms step_avg:60.53ms
step:1788/2315 train_time:108221ms step_avg:60.53ms
step:1789/2315 train_time:108282ms step_avg:60.53ms
step:1790/2315 train_time:108343ms step_avg:60.53ms
step:1791/2315 train_time:108404ms step_avg:60.53ms
step:1792/2315 train_time:108465ms step_avg:60.53ms
step:1793/2315 train_time:108526ms step_avg:60.53ms
step:1794/2315 train_time:108587ms step_avg:60.53ms
step:1795/2315 train_time:108647ms step_avg:60.53ms
step:1796/2315 train_time:108707ms step_avg:60.53ms
step:1797/2315 train_time:108768ms step_avg:60.53ms
step:1798/2315 train_time:108828ms step_avg:60.53ms
step:1799/2315 train_time:108890ms step_avg:60.53ms
step:1800/2315 train_time:108951ms step_avg:60.53ms
step:1801/2315 train_time:109013ms step_avg:60.53ms
step:1802/2315 train_time:109075ms step_avg:60.53ms
step:1803/2315 train_time:109137ms step_avg:60.53ms
step:1804/2315 train_time:109199ms step_avg:60.53ms
step:1805/2315 train_time:109260ms step_avg:60.53ms
step:1806/2315 train_time:109321ms step_avg:60.53ms
step:1807/2315 train_time:109382ms step_avg:60.53ms
step:1808/2315 train_time:109443ms step_avg:60.53ms
step:1809/2315 train_time:109504ms step_avg:60.53ms
step:1810/2315 train_time:109565ms step_avg:60.53ms
step:1811/2315 train_time:109626ms step_avg:60.53ms
step:1812/2315 train_time:109687ms step_avg:60.53ms
step:1813/2315 train_time:109748ms step_avg:60.53ms
step:1814/2315 train_time:109809ms step_avg:60.53ms
step:1815/2315 train_time:109869ms step_avg:60.53ms
step:1816/2315 train_time:109931ms step_avg:60.53ms
step:1817/2315 train_time:109992ms step_avg:60.53ms
step:1818/2315 train_time:110053ms step_avg:60.54ms
step:1819/2315 train_time:110115ms step_avg:60.54ms
step:1820/2315 train_time:110176ms step_avg:60.54ms
step:1821/2315 train_time:110238ms step_avg:60.54ms
step:1822/2315 train_time:110299ms step_avg:60.54ms
step:1823/2315 train_time:110361ms step_avg:60.54ms
step:1824/2315 train_time:110421ms step_avg:60.54ms
step:1825/2315 train_time:110483ms step_avg:60.54ms
step:1826/2315 train_time:110544ms step_avg:60.54ms
step:1827/2315 train_time:110605ms step_avg:60.54ms
step:1828/2315 train_time:110666ms step_avg:60.54ms
step:1829/2315 train_time:110728ms step_avg:60.54ms
step:1830/2315 train_time:110788ms step_avg:60.54ms
step:1831/2315 train_time:110848ms step_avg:60.54ms
step:1832/2315 train_time:110909ms step_avg:60.54ms
step:1833/2315 train_time:110970ms step_avg:60.54ms
step:1834/2315 train_time:111031ms step_avg:60.54ms
step:1835/2315 train_time:111094ms step_avg:60.54ms
step:1836/2315 train_time:111154ms step_avg:60.54ms
step:1837/2315 train_time:111216ms step_avg:60.54ms
step:1838/2315 train_time:111278ms step_avg:60.54ms
step:1839/2315 train_time:111340ms step_avg:60.54ms
step:1840/2315 train_time:111401ms step_avg:60.54ms
step:1841/2315 train_time:111462ms step_avg:60.54ms
step:1842/2315 train_time:111523ms step_avg:60.54ms
step:1843/2315 train_time:111584ms step_avg:60.54ms
step:1844/2315 train_time:111645ms step_avg:60.54ms
step:1845/2315 train_time:111706ms step_avg:60.55ms
step:1846/2315 train_time:111767ms step_avg:60.55ms
step:1847/2315 train_time:111828ms step_avg:60.55ms
step:1848/2315 train_time:111889ms step_avg:60.55ms
step:1849/2315 train_time:111950ms step_avg:60.55ms
step:1850/2315 train_time:112011ms step_avg:60.55ms
step:1851/2315 train_time:112073ms step_avg:60.55ms
step:1852/2315 train_time:112134ms step_avg:60.55ms
step:1853/2315 train_time:112196ms step_avg:60.55ms
step:1854/2315 train_time:112257ms step_avg:60.55ms
step:1855/2315 train_time:112319ms step_avg:60.55ms
step:1856/2315 train_time:112380ms step_avg:60.55ms
step:1857/2315 train_time:112441ms step_avg:60.55ms
step:1858/2315 train_time:112502ms step_avg:60.55ms
step:1859/2315 train_time:112563ms step_avg:60.55ms
step:1860/2315 train_time:112624ms step_avg:60.55ms
step:1861/2315 train_time:112686ms step_avg:60.55ms
step:1862/2315 train_time:112747ms step_avg:60.55ms
step:1863/2315 train_time:112807ms step_avg:60.55ms
step:1864/2315 train_time:112868ms step_avg:60.55ms
step:1865/2315 train_time:112929ms step_avg:60.55ms
step:1866/2315 train_time:112990ms step_avg:60.55ms
step:1867/2315 train_time:113051ms step_avg:60.55ms
step:1868/2315 train_time:113113ms step_avg:60.55ms
step:1869/2315 train_time:113175ms step_avg:60.55ms
step:1870/2315 train_time:113236ms step_avg:60.55ms
step:1871/2315 train_time:113298ms step_avg:60.55ms
step:1872/2315 train_time:113359ms step_avg:60.56ms
step:1873/2315 train_time:113421ms step_avg:60.56ms
step:1874/2315 train_time:113482ms step_avg:60.56ms
step:1875/2315 train_time:113543ms step_avg:60.56ms
step:1876/2315 train_time:113604ms step_avg:60.56ms
step:1877/2315 train_time:113665ms step_avg:60.56ms
step:1878/2315 train_time:113726ms step_avg:60.56ms
step:1879/2315 train_time:113787ms step_avg:60.56ms
step:1880/2315 train_time:113848ms step_avg:60.56ms
step:1881/2315 train_time:113909ms step_avg:60.56ms
step:1882/2315 train_time:113970ms step_avg:60.56ms
step:1883/2315 train_time:114031ms step_avg:60.56ms
step:1884/2315 train_time:114092ms step_avg:60.56ms
step:1885/2315 train_time:114154ms step_avg:60.56ms
step:1886/2315 train_time:114215ms step_avg:60.56ms
step:1887/2315 train_time:114276ms step_avg:60.56ms
step:1888/2315 train_time:114338ms step_avg:60.56ms
step:1889/2315 train_time:114400ms step_avg:60.56ms
step:1890/2315 train_time:114461ms step_avg:60.56ms
step:1891/2315 train_time:114523ms step_avg:60.56ms
step:1892/2315 train_time:114583ms step_avg:60.56ms
step:1893/2315 train_time:114645ms step_avg:60.56ms
step:1894/2315 train_time:114705ms step_avg:60.56ms
step:1895/2315 train_time:114766ms step_avg:60.56ms
step:1896/2315 train_time:114827ms step_avg:60.56ms
step:1897/2315 train_time:114888ms step_avg:60.56ms
step:1898/2315 train_time:114949ms step_avg:60.56ms
step:1899/2315 train_time:115010ms step_avg:60.56ms
step:1900/2315 train_time:115071ms step_avg:60.56ms
step:1901/2315 train_time:115132ms step_avg:60.56ms
step:1902/2315 train_time:115195ms step_avg:60.56ms
step:1903/2315 train_time:115256ms step_avg:60.57ms
step:1904/2315 train_time:115318ms step_avg:60.57ms
step:1905/2315 train_time:115378ms step_avg:60.57ms
step:1906/2315 train_time:115440ms step_avg:60.57ms
step:1907/2315 train_time:115501ms step_avg:60.57ms
step:1908/2315 train_time:115562ms step_avg:60.57ms
step:1909/2315 train_time:115623ms step_avg:60.57ms
step:1910/2315 train_time:115684ms step_avg:60.57ms
step:1911/2315 train_time:115746ms step_avg:60.57ms
step:1912/2315 train_time:115807ms step_avg:60.57ms
step:1913/2315 train_time:115868ms step_avg:60.57ms
step:1914/2315 train_time:115928ms step_avg:60.57ms
step:1915/2315 train_time:115989ms step_avg:60.57ms
step:1916/2315 train_time:116050ms step_avg:60.57ms
step:1917/2315 train_time:116111ms step_avg:60.57ms
step:1918/2315 train_time:116172ms step_avg:60.57ms
step:1919/2315 train_time:116234ms step_avg:60.57ms
step:1920/2315 train_time:116295ms step_avg:60.57ms
step:1921/2315 train_time:116356ms step_avg:60.57ms
step:1922/2315 train_time:116418ms step_avg:60.57ms
step:1923/2315 train_time:116480ms step_avg:60.57ms
step:1924/2315 train_time:116541ms step_avg:60.57ms
step:1925/2315 train_time:116602ms step_avg:60.57ms
step:1926/2315 train_time:116663ms step_avg:60.57ms
step:1927/2315 train_time:116725ms step_avg:60.57ms
step:1928/2315 train_time:116786ms step_avg:60.57ms
step:1929/2315 train_time:116847ms step_avg:60.57ms
step:1930/2315 train_time:116908ms step_avg:60.57ms
step:1931/2315 train_time:116969ms step_avg:60.57ms
step:1932/2315 train_time:117029ms step_avg:60.57ms
step:1933/2315 train_time:117090ms step_avg:60.57ms
step:1934/2315 train_time:117152ms step_avg:60.57ms
step:1935/2315 train_time:117213ms step_avg:60.58ms
step:1936/2315 train_time:117275ms step_avg:60.58ms
step:1937/2315 train_time:117337ms step_avg:60.58ms
step:1938/2315 train_time:117398ms step_avg:60.58ms
step:1939/2315 train_time:117459ms step_avg:60.58ms
step:1940/2315 train_time:117519ms step_avg:60.58ms
step:1941/2315 train_time:117581ms step_avg:60.58ms
step:1942/2315 train_time:117643ms step_avg:60.58ms
step:1943/2315 train_time:117704ms step_avg:60.58ms
step:1944/2315 train_time:117764ms step_avg:60.58ms
step:1945/2315 train_time:117826ms step_avg:60.58ms
step:1946/2315 train_time:117887ms step_avg:60.58ms
step:1947/2315 train_time:117949ms step_avg:60.58ms
step:1948/2315 train_time:118010ms step_avg:60.58ms
step:1949/2315 train_time:118071ms step_avg:60.58ms
step:1950/2315 train_time:118132ms step_avg:60.58ms
step:1951/2315 train_time:118193ms step_avg:60.58ms
step:1952/2315 train_time:118254ms step_avg:60.58ms
step:1953/2315 train_time:118315ms step_avg:60.58ms
step:1954/2315 train_time:118376ms step_avg:60.58ms
step:1955/2315 train_time:118438ms step_avg:60.58ms
step:1956/2315 train_time:118499ms step_avg:60.58ms
step:1957/2315 train_time:118561ms step_avg:60.58ms
step:1958/2315 train_time:118622ms step_avg:60.58ms
step:1959/2315 train_time:118684ms step_avg:60.58ms
step:1960/2315 train_time:118745ms step_avg:60.58ms
step:1961/2315 train_time:118807ms step_avg:60.58ms
step:1962/2315 train_time:118868ms step_avg:60.58ms
step:1963/2315 train_time:118928ms step_avg:60.58ms
step:1964/2315 train_time:118989ms step_avg:60.59ms
step:1965/2315 train_time:119050ms step_avg:60.59ms
step:1966/2315 train_time:119111ms step_avg:60.59ms
step:1967/2315 train_time:119172ms step_avg:60.59ms
step:1968/2315 train_time:119234ms step_avg:60.59ms
step:1969/2315 train_time:119296ms step_avg:60.59ms
step:1970/2315 train_time:119356ms step_avg:60.59ms
step:1971/2315 train_time:119417ms step_avg:60.59ms
step:1972/2315 train_time:119478ms step_avg:60.59ms
step:1973/2315 train_time:119540ms step_avg:60.59ms
step:1974/2315 train_time:119601ms step_avg:60.59ms
step:1975/2315 train_time:119662ms step_avg:60.59ms
step:1976/2315 train_time:119723ms step_avg:60.59ms
step:1977/2315 train_time:119784ms step_avg:60.59ms
step:1978/2315 train_time:119845ms step_avg:60.59ms
step:1979/2315 train_time:119907ms step_avg:60.59ms
step:1980/2315 train_time:119968ms step_avg:60.59ms
step:1981/2315 train_time:120029ms step_avg:60.59ms
step:1982/2315 train_time:120090ms step_avg:60.59ms
step:1983/2315 train_time:120151ms step_avg:60.59ms
step:1984/2315 train_time:120212ms step_avg:60.59ms
step:1985/2315 train_time:120274ms step_avg:60.59ms
step:1986/2315 train_time:120336ms step_avg:60.59ms
step:1987/2315 train_time:120397ms step_avg:60.59ms
step:1988/2315 train_time:120458ms step_avg:60.59ms
step:1989/2315 train_time:120520ms step_avg:60.59ms
step:1990/2315 train_time:120581ms step_avg:60.59ms
step:1991/2315 train_time:120642ms step_avg:60.59ms
step:1992/2315 train_time:120703ms step_avg:60.59ms
step:1993/2315 train_time:120764ms step_avg:60.59ms
step:1994/2315 train_time:120826ms step_avg:60.59ms
step:1995/2315 train_time:120887ms step_avg:60.59ms
step:1996/2315 train_time:120948ms step_avg:60.60ms
step:1997/2315 train_time:121009ms step_avg:60.60ms
step:1998/2315 train_time:121070ms step_avg:60.60ms
step:1999/2315 train_time:121131ms step_avg:60.60ms
step:2000/2315 train_time:121192ms step_avg:60.60ms
step:2000/2315 val_loss:3.3303 train_time:121254ms step_avg:60.63ms
step:2001/2315 train_time:121273ms step_avg:60.61ms
step:2002/2315 train_time:121317ms step_avg:60.60ms
step:2003/2315 train_time:121381ms step_avg:60.60ms
step:2004/2315 train_time:121445ms step_avg:60.60ms
step:2005/2315 train_time:121507ms step_avg:60.60ms
step:2006/2315 train_time:121568ms step_avg:60.60ms
step:2007/2315 train_time:121629ms step_avg:60.60ms
step:2008/2315 train_time:121689ms step_avg:60.60ms
step:2009/2315 train_time:121749ms step_avg:60.60ms
step:2010/2315 train_time:121809ms step_avg:60.60ms
step:2011/2315 train_time:121870ms step_avg:60.60ms
step:2012/2315 train_time:121930ms step_avg:60.60ms
step:2013/2315 train_time:121991ms step_avg:60.60ms
step:2014/2315 train_time:122053ms step_avg:60.60ms
step:2015/2315 train_time:122114ms step_avg:60.60ms
step:2016/2315 train_time:122175ms step_avg:60.60ms
step:2017/2315 train_time:122237ms step_avg:60.60ms
step:2018/2315 train_time:122299ms step_avg:60.60ms
step:2019/2315 train_time:122362ms step_avg:60.61ms
step:2020/2315 train_time:122424ms step_avg:60.61ms
step:2021/2315 train_time:122485ms step_avg:60.61ms
step:2022/2315 train_time:122547ms step_avg:60.61ms
step:2023/2315 train_time:122608ms step_avg:60.61ms
step:2024/2315 train_time:122668ms step_avg:60.61ms
step:2025/2315 train_time:122729ms step_avg:60.61ms
step:2026/2315 train_time:122790ms step_avg:60.61ms
step:2027/2315 train_time:122850ms step_avg:60.61ms
step:2028/2315 train_time:122911ms step_avg:60.61ms
step:2029/2315 train_time:122972ms step_avg:60.61ms
step:2030/2315 train_time:123032ms step_avg:60.61ms
step:2031/2315 train_time:123093ms step_avg:60.61ms
step:2032/2315 train_time:123154ms step_avg:60.61ms
step:2033/2315 train_time:123215ms step_avg:60.61ms
step:2034/2315 train_time:123277ms step_avg:60.61ms
step:2035/2315 train_time:123340ms step_avg:60.61ms
step:2036/2315 train_time:123402ms step_avg:60.61ms
step:2037/2315 train_time:123465ms step_avg:60.61ms
step:2038/2315 train_time:123526ms step_avg:60.61ms
step:2039/2315 train_time:123587ms step_avg:60.61ms
step:2040/2315 train_time:123648ms step_avg:60.61ms
step:2041/2315 train_time:123709ms step_avg:60.61ms
step:2042/2315 train_time:123771ms step_avg:60.61ms
step:2043/2315 train_time:123831ms step_avg:60.61ms
step:2044/2315 train_time:123892ms step_avg:60.61ms
step:2045/2315 train_time:123952ms step_avg:60.61ms
step:2046/2315 train_time:124014ms step_avg:60.61ms
step:2047/2315 train_time:124076ms step_avg:60.61ms
step:2048/2315 train_time:124137ms step_avg:60.61ms
step:2049/2315 train_time:124198ms step_avg:60.61ms
step:2050/2315 train_time:124260ms step_avg:60.61ms
step:2051/2315 train_time:124321ms step_avg:60.62ms
step:2052/2315 train_time:124383ms step_avg:60.62ms
step:2053/2315 train_time:124444ms step_avg:60.62ms
step:2054/2315 train_time:124506ms step_avg:60.62ms
step:2055/2315 train_time:124567ms step_avg:60.62ms
step:2056/2315 train_time:124628ms step_avg:60.62ms
step:2057/2315 train_time:124689ms step_avg:60.62ms
step:2058/2315 train_time:124751ms step_avg:60.62ms
step:2059/2315 train_time:124812ms step_avg:60.62ms
step:2060/2315 train_time:124872ms step_avg:60.62ms
step:2061/2315 train_time:124933ms step_avg:60.62ms
step:2062/2315 train_time:124994ms step_avg:60.62ms
step:2063/2315 train_time:125055ms step_avg:60.62ms
step:2064/2315 train_time:125117ms step_avg:60.62ms
step:2065/2315 train_time:125179ms step_avg:60.62ms
step:2066/2315 train_time:125240ms step_avg:60.62ms
step:2067/2315 train_time:125302ms step_avg:60.62ms
step:2068/2315 train_time:125363ms step_avg:60.62ms
step:2069/2315 train_time:125424ms step_avg:60.62ms
step:2070/2315 train_time:125486ms step_avg:60.62ms
step:2071/2315 train_time:125547ms step_avg:60.62ms
step:2072/2315 train_time:125608ms step_avg:60.62ms
step:2073/2315 train_time:125669ms step_avg:60.62ms
step:2074/2315 train_time:125730ms step_avg:60.62ms
step:2075/2315 train_time:125791ms step_avg:60.62ms
step:2076/2315 train_time:125852ms step_avg:60.62ms
step:2077/2315 train_time:125913ms step_avg:60.62ms
step:2078/2315 train_time:125974ms step_avg:60.62ms
step:2079/2315 train_time:126036ms step_avg:60.62ms
step:2080/2315 train_time:126097ms step_avg:60.62ms
step:2081/2315 train_time:126159ms step_avg:60.62ms
step:2082/2315 train_time:126221ms step_avg:60.62ms
step:2083/2315 train_time:126282ms step_avg:60.62ms
step:2084/2315 train_time:126343ms step_avg:60.63ms
step:2085/2315 train_time:126404ms step_avg:60.63ms
step:2086/2315 train_time:126465ms step_avg:60.63ms
step:2087/2315 train_time:126526ms step_avg:60.63ms
step:2088/2315 train_time:126587ms step_avg:60.63ms
step:2089/2315 train_time:126649ms step_avg:60.63ms
step:2090/2315 train_time:126710ms step_avg:60.63ms
step:2091/2315 train_time:126772ms step_avg:60.63ms
step:2092/2315 train_time:126832ms step_avg:60.63ms
step:2093/2315 train_time:126893ms step_avg:60.63ms
step:2094/2315 train_time:126955ms step_avg:60.63ms
step:2095/2315 train_time:127016ms step_avg:60.63ms
step:2096/2315 train_time:127077ms step_avg:60.63ms
step:2097/2315 train_time:127138ms step_avg:60.63ms
step:2098/2315 train_time:127199ms step_avg:60.63ms
step:2099/2315 train_time:127261ms step_avg:60.63ms
step:2100/2315 train_time:127322ms step_avg:60.63ms
step:2101/2315 train_time:127384ms step_avg:60.63ms
step:2102/2315 train_time:127445ms step_avg:60.63ms
step:2103/2315 train_time:127507ms step_avg:60.63ms
step:2104/2315 train_time:127568ms step_avg:60.63ms
step:2105/2315 train_time:127629ms step_avg:60.63ms
step:2106/2315 train_time:127690ms step_avg:60.63ms
step:2107/2315 train_time:127752ms step_avg:60.63ms
step:2108/2315 train_time:127813ms step_avg:60.63ms
step:2109/2315 train_time:127874ms step_avg:60.63ms
step:2110/2315 train_time:127935ms step_avg:60.63ms
step:2111/2315 train_time:127996ms step_avg:60.63ms
step:2112/2315 train_time:128057ms step_avg:60.63ms
step:2113/2315 train_time:128118ms step_avg:60.63ms
step:2114/2315 train_time:128179ms step_avg:60.63ms
step:2115/2315 train_time:128240ms step_avg:60.63ms
step:2116/2315 train_time:128301ms step_avg:60.63ms
step:2117/2315 train_time:128363ms step_avg:60.63ms
step:2118/2315 train_time:128424ms step_avg:60.63ms
step:2119/2315 train_time:128486ms step_avg:60.64ms
step:2120/2315 train_time:128547ms step_avg:60.64ms
step:2121/2315 train_time:128608ms step_avg:60.64ms
step:2122/2315 train_time:128669ms step_avg:60.64ms
step:2123/2315 train_time:128731ms step_avg:60.64ms
step:2124/2315 train_time:128792ms step_avg:60.64ms
step:2125/2315 train_time:128853ms step_avg:60.64ms
step:2126/2315 train_time:128914ms step_avg:60.64ms
step:2127/2315 train_time:128975ms step_avg:60.64ms
step:2128/2315 train_time:129036ms step_avg:60.64ms
step:2129/2315 train_time:129098ms step_avg:60.64ms
step:2130/2315 train_time:129159ms step_avg:60.64ms
step:2131/2315 train_time:129220ms step_avg:60.64ms
step:2132/2315 train_time:129282ms step_avg:60.64ms
step:2133/2315 train_time:129344ms step_avg:60.64ms
step:2134/2315 train_time:129405ms step_avg:60.64ms
step:2135/2315 train_time:129466ms step_avg:60.64ms
step:2136/2315 train_time:129528ms step_avg:60.64ms
step:2137/2315 train_time:129589ms step_avg:60.64ms
step:2138/2315 train_time:129650ms step_avg:60.64ms
step:2139/2315 train_time:129711ms step_avg:60.64ms
step:2140/2315 train_time:129772ms step_avg:60.64ms
step:2141/2315 train_time:129834ms step_avg:60.64ms
step:2142/2315 train_time:129894ms step_avg:60.64ms
step:2143/2315 train_time:129955ms step_avg:60.64ms
step:2144/2315 train_time:130016ms step_avg:60.64ms
step:2145/2315 train_time:130078ms step_avg:60.64ms
step:2146/2315 train_time:130139ms step_avg:60.64ms
step:2147/2315 train_time:130201ms step_avg:60.64ms
step:2148/2315 train_time:130262ms step_avg:60.64ms
step:2149/2315 train_time:130324ms step_avg:60.64ms
step:2150/2315 train_time:130385ms step_avg:60.64ms
step:2151/2315 train_time:130447ms step_avg:60.64ms
step:2152/2315 train_time:130507ms step_avg:60.64ms
step:2153/2315 train_time:130569ms step_avg:60.65ms
step:2154/2315 train_time:130630ms step_avg:60.65ms
step:2155/2315 train_time:130692ms step_avg:60.65ms
step:2156/2315 train_time:130753ms step_avg:60.65ms
step:2157/2315 train_time:130814ms step_avg:60.65ms
step:2158/2315 train_time:130875ms step_avg:60.65ms
step:2159/2315 train_time:130936ms step_avg:60.65ms
step:2160/2315 train_time:130997ms step_avg:60.65ms
step:2161/2315 train_time:131058ms step_avg:60.65ms
step:2162/2315 train_time:131119ms step_avg:60.65ms
step:2163/2315 train_time:131181ms step_avg:60.65ms
step:2164/2315 train_time:131241ms step_avg:60.65ms
step:2165/2315 train_time:131303ms step_avg:60.65ms
step:2166/2315 train_time:131365ms step_avg:60.65ms
step:2167/2315 train_time:131426ms step_avg:60.65ms
step:2168/2315 train_time:131487ms step_avg:60.65ms
step:2169/2315 train_time:131549ms step_avg:60.65ms
step:2170/2315 train_time:131610ms step_avg:60.65ms
step:2171/2315 train_time:131671ms step_avg:60.65ms
step:2172/2315 train_time:131732ms step_avg:60.65ms
step:2173/2315 train_time:131793ms step_avg:60.65ms
step:2174/2315 train_time:131854ms step_avg:60.65ms
step:2175/2315 train_time:131916ms step_avg:60.65ms
step:2176/2315 train_time:131977ms step_avg:60.65ms
step:2177/2315 train_time:132039ms step_avg:60.65ms
step:2178/2315 train_time:132100ms step_avg:60.65ms
step:2179/2315 train_time:132161ms step_avg:60.65ms
step:2180/2315 train_time:132223ms step_avg:60.65ms
step:2181/2315 train_time:132284ms step_avg:60.65ms
step:2182/2315 train_time:132346ms step_avg:60.65ms
step:2183/2315 train_time:132407ms step_avg:60.65ms
step:2184/2315 train_time:132468ms step_avg:60.65ms
step:2185/2315 train_time:132530ms step_avg:60.65ms
step:2186/2315 train_time:132591ms step_avg:60.65ms
step:2187/2315 train_time:132652ms step_avg:60.65ms
step:2188/2315 train_time:132714ms step_avg:60.66ms
step:2189/2315 train_time:132775ms step_avg:60.66ms
step:2190/2315 train_time:132836ms step_avg:60.66ms
step:2191/2315 train_time:132898ms step_avg:60.66ms
step:2192/2315 train_time:132959ms step_avg:60.66ms
step:2193/2315 train_time:133020ms step_avg:60.66ms
step:2194/2315 train_time:133081ms step_avg:60.66ms
step:2195/2315 train_time:133142ms step_avg:60.66ms
step:2196/2315 train_time:133203ms step_avg:60.66ms
step:2197/2315 train_time:133264ms step_avg:60.66ms
step:2198/2315 train_time:133325ms step_avg:60.66ms
step:2199/2315 train_time:133386ms step_avg:60.66ms
step:2200/2315 train_time:133447ms step_avg:60.66ms
step:2201/2315 train_time:133509ms step_avg:60.66ms
step:2202/2315 train_time:133570ms step_avg:60.66ms
step:2203/2315 train_time:133631ms step_avg:60.66ms
step:2204/2315 train_time:133691ms step_avg:60.66ms
step:2205/2315 train_time:133753ms step_avg:60.66ms
step:2206/2315 train_time:133813ms step_avg:60.66ms
step:2207/2315 train_time:133875ms step_avg:60.66ms
step:2208/2315 train_time:133936ms step_avg:60.66ms
step:2209/2315 train_time:133998ms step_avg:60.66ms
step:2210/2315 train_time:134059ms step_avg:60.66ms
step:2211/2315 train_time:134120ms step_avg:60.66ms
step:2212/2315 train_time:134181ms step_avg:60.66ms
step:2213/2315 train_time:134242ms step_avg:60.66ms
step:2214/2315 train_time:134303ms step_avg:60.66ms
step:2215/2315 train_time:134366ms step_avg:60.66ms
step:2216/2315 train_time:134427ms step_avg:60.66ms
step:2217/2315 train_time:134488ms step_avg:60.66ms
step:2218/2315 train_time:134550ms step_avg:60.66ms
step:2219/2315 train_time:134611ms step_avg:60.66ms
step:2220/2315 train_time:134672ms step_avg:60.66ms
step:2221/2315 train_time:134733ms step_avg:60.66ms
step:2222/2315 train_time:134793ms step_avg:60.66ms
step:2223/2315 train_time:134855ms step_avg:60.66ms
step:2224/2315 train_time:134917ms step_avg:60.66ms
step:2225/2315 train_time:134977ms step_avg:60.66ms
step:2226/2315 train_time:135038ms step_avg:60.66ms
step:2227/2315 train_time:135100ms step_avg:60.66ms
step:2228/2315 train_time:135161ms step_avg:60.66ms
step:2229/2315 train_time:135222ms step_avg:60.66ms
step:2230/2315 train_time:135283ms step_avg:60.66ms
step:2231/2315 train_time:135344ms step_avg:60.67ms
step:2232/2315 train_time:135405ms step_avg:60.67ms
step:2233/2315 train_time:135466ms step_avg:60.67ms
step:2234/2315 train_time:135527ms step_avg:60.67ms
step:2235/2315 train_time:135589ms step_avg:60.67ms
step:2236/2315 train_time:135650ms step_avg:60.67ms
step:2237/2315 train_time:135712ms step_avg:60.67ms
step:2238/2315 train_time:135772ms step_avg:60.67ms
step:2239/2315 train_time:135834ms step_avg:60.67ms
step:2240/2315 train_time:135894ms step_avg:60.67ms
step:2241/2315 train_time:135955ms step_avg:60.67ms
step:2242/2315 train_time:136018ms step_avg:60.67ms
step:2243/2315 train_time:136079ms step_avg:60.67ms
step:2244/2315 train_time:136141ms step_avg:60.67ms
step:2245/2315 train_time:136202ms step_avg:60.67ms
step:2246/2315 train_time:136263ms step_avg:60.67ms
step:2247/2315 train_time:136324ms step_avg:60.67ms
step:2248/2315 train_time:136385ms step_avg:60.67ms
step:2249/2315 train_time:136446ms step_avg:60.67ms
step:2250/2315 train_time:136507ms step_avg:60.67ms
step:2250/2315 val_loss:3.2905 train_time:136570ms step_avg:60.70ms
step:2251/2315 train_time:136590ms step_avg:60.68ms
step:2252/2315 train_time:136631ms step_avg:60.67ms
step:2253/2315 train_time:136699ms step_avg:60.67ms
step:2254/2315 train_time:136763ms step_avg:60.68ms
step:2255/2315 train_time:136825ms step_avg:60.68ms
step:2256/2315 train_time:136887ms step_avg:60.68ms
step:2257/2315 train_time:136947ms step_avg:60.68ms
step:2258/2315 train_time:137008ms step_avg:60.68ms
step:2259/2315 train_time:137069ms step_avg:60.68ms
step:2260/2315 train_time:137129ms step_avg:60.68ms
step:2261/2315 train_time:137190ms step_avg:60.68ms
step:2262/2315 train_time:137250ms step_avg:60.68ms
step:2263/2315 train_time:137310ms step_avg:60.68ms
step:2264/2315 train_time:137370ms step_avg:60.68ms
step:2265/2315 train_time:137430ms step_avg:60.68ms
step:2266/2315 train_time:137491ms step_avg:60.68ms
step:2267/2315 train_time:137551ms step_avg:60.68ms
step:2268/2315 train_time:137613ms step_avg:60.68ms
step:2269/2315 train_time:137676ms step_avg:60.68ms
step:2270/2315 train_time:137740ms step_avg:60.68ms
step:2271/2315 train_time:137802ms step_avg:60.68ms
step:2272/2315 train_time:137864ms step_avg:60.68ms
step:2273/2315 train_time:137925ms step_avg:60.68ms
step:2274/2315 train_time:137986ms step_avg:60.68ms
step:2275/2315 train_time:138048ms step_avg:60.68ms
step:2276/2315 train_time:138108ms step_avg:60.68ms
step:2277/2315 train_time:138169ms step_avg:60.68ms
step:2278/2315 train_time:138229ms step_avg:60.68ms
step:2279/2315 train_time:138290ms step_avg:60.68ms
step:2280/2315 train_time:138350ms step_avg:60.68ms
step:2281/2315 train_time:138410ms step_avg:60.68ms
step:2282/2315 train_time:138471ms step_avg:60.68ms
step:2283/2315 train_time:138531ms step_avg:60.68ms
step:2284/2315 train_time:138592ms step_avg:60.68ms
step:2285/2315 train_time:138654ms step_avg:60.68ms
step:2286/2315 train_time:138715ms step_avg:60.68ms
step:2287/2315 train_time:138778ms step_avg:60.68ms
step:2288/2315 train_time:138840ms step_avg:60.68ms
step:2289/2315 train_time:138902ms step_avg:60.68ms
step:2290/2315 train_time:138963ms step_avg:60.68ms
step:2291/2315 train_time:139024ms step_avg:60.68ms
step:2292/2315 train_time:139085ms step_avg:60.68ms
step:2293/2315 train_time:139146ms step_avg:60.68ms
step:2294/2315 train_time:139207ms step_avg:60.68ms
step:2295/2315 train_time:139268ms step_avg:60.68ms
step:2296/2315 train_time:139328ms step_avg:60.68ms
step:2297/2315 train_time:139390ms step_avg:60.68ms
step:2298/2315 train_time:139451ms step_avg:60.68ms
step:2299/2315 train_time:139511ms step_avg:60.68ms
step:2300/2315 train_time:139572ms step_avg:60.68ms
step:2301/2315 train_time:139633ms step_avg:60.68ms
step:2302/2315 train_time:139694ms step_avg:60.68ms
step:2303/2315 train_time:139755ms step_avg:60.68ms
step:2304/2315 train_time:139817ms step_avg:60.68ms
step:2305/2315 train_time:139880ms step_avg:60.69ms
step:2306/2315 train_time:139941ms step_avg:60.69ms
step:2307/2315 train_time:140003ms step_avg:60.69ms
step:2308/2315 train_time:140064ms step_avg:60.69ms
step:2309/2315 train_time:140126ms step_avg:60.69ms
step:2310/2315 train_time:140187ms step_avg:60.69ms
step:2311/2315 train_time:140248ms step_avg:60.69ms
step:2312/2315 train_time:140309ms step_avg:60.69ms
step:2313/2315 train_time:140370ms step_avg:60.69ms
step:2314/2315 train_time:140430ms step_avg:60.69ms
step:2315/2315 train_time:140491ms step_avg:60.69ms
step:2315/2315 val_loss:3.2780 train_time:140553ms step_avg:60.71ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

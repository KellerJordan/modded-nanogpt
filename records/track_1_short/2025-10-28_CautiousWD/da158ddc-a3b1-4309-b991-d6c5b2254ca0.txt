import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, eps=1e-8, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp_up', 'mlp_down']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            elif params[module_idx].label == "smear_gate":
                # dividing by magnitude is equivalent of SVN for 1d tensors
                v_chunk = updated_grads / (updated_grads.norm(dim=(-2, -1), keepdim=True).clamp_min(1e-10))
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            same_sign = torch.signbit(v_chunk) == torch.signbit(param_chunk)
            v_chunk.add_(eff_wd * (param_chunk * same_sign.to(ref_param.dtype)))

            param_chunk.add_(-eff_lr * v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp_up'
        self.c_proj.label = 'mlp_down'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 2270
    lr_schedule = (0.5, 0.98)    # breakpoints for 3-part schedule: (flat, linear decay, flat)
    lr_min = 0.1
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 5, 7, 9, 11, 13)
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=0.01)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

def get_lr(step: int):
    assert step < args.num_iterations
    # Three part schedule: flat, linear decrease, flat
    lr_schedule = args.lr_schedule
    x = step / args.num_iterations

    if x < lr_schedule[0]:
        return 1.0
    elif x < lr_schedule[1]:
        progress = (x - lr_schedule[0]) / (lr_schedule[1] - lr_schedule[0])
        lr = 1.0 - (1.0 - args.lr_min) * progress
    else:
        lr = args.lr_min
    return lr

def get_ws(step: int):
    assert step <= args.num_iterations
    x = step / (args.num_iterations + 1)
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset()  #  momentum buffer not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    loss = 0
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        loss += model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps
    loss.backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Oct 28 03:33:35 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   40C    P0            128W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   32C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   31C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2270 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2270 train_time:121ms step_avg:121.43ms
step:2/2270 train_time:143ms step_avg:71.25ms
step:3/2270 train_time:181ms step_avg:60.36ms
step:4/2270 train_time:237ms step_avg:59.33ms
step:5/2270 train_time:297ms step_avg:59.39ms
step:6/2270 train_time:355ms step_avg:59.18ms
step:7/2270 train_time:416ms step_avg:59.49ms
step:8/2270 train_time:475ms step_avg:59.34ms
step:9/2270 train_time:535ms step_avg:59.48ms
step:10/2270 train_time:594ms step_avg:59.39ms
step:11/2270 train_time:655ms step_avg:59.52ms
step:12/2270 train_time:713ms step_avg:59.43ms
step:13/2270 train_time:774ms step_avg:59.54ms
step:14/2270 train_time:833ms step_avg:59.49ms
step:15/2270 train_time:894ms step_avg:59.57ms
step:16/2270 train_time:952ms step_avg:59.52ms
step:17/2270 train_time:1016ms step_avg:59.78ms
step:18/2270 train_time:1079ms step_avg:59.96ms
step:19/2270 train_time:1145ms step_avg:60.27ms
step:20/2270 train_time:1205ms step_avg:60.26ms
step:21/2270 train_time:1267ms step_avg:60.36ms
step:22/2270 train_time:1326ms step_avg:60.28ms
step:23/2270 train_time:1387ms step_avg:60.32ms
step:24/2270 train_time:1446ms step_avg:60.25ms
step:25/2270 train_time:1508ms step_avg:60.30ms
step:26/2270 train_time:1566ms step_avg:60.25ms
step:27/2270 train_time:1627ms step_avg:60.27ms
step:28/2270 train_time:1686ms step_avg:60.21ms
step:29/2270 train_time:1747ms step_avg:60.25ms
step:30/2270 train_time:1806ms step_avg:60.19ms
step:31/2270 train_time:1867ms step_avg:60.24ms
step:32/2270 train_time:1926ms step_avg:60.19ms
step:33/2270 train_time:1988ms step_avg:60.25ms
step:34/2270 train_time:2047ms step_avg:60.22ms
step:35/2270 train_time:2110ms step_avg:60.29ms
step:36/2270 train_time:2170ms step_avg:60.28ms
step:37/2270 train_time:2232ms step_avg:60.32ms
step:38/2270 train_time:2290ms step_avg:60.27ms
step:39/2270 train_time:2351ms step_avg:60.29ms
step:40/2270 train_time:2410ms step_avg:60.26ms
step:41/2270 train_time:2472ms step_avg:60.28ms
step:42/2270 train_time:2531ms step_avg:60.26ms
step:43/2270 train_time:2592ms step_avg:60.28ms
step:44/2270 train_time:2651ms step_avg:60.25ms
step:45/2270 train_time:2713ms step_avg:60.28ms
step:46/2270 train_time:2772ms step_avg:60.26ms
step:47/2270 train_time:2833ms step_avg:60.29ms
step:48/2270 train_time:2892ms step_avg:60.25ms
step:49/2270 train_time:2954ms step_avg:60.29ms
step:50/2270 train_time:3014ms step_avg:60.27ms
step:51/2270 train_time:3076ms step_avg:60.31ms
step:52/2270 train_time:3135ms step_avg:60.29ms
step:53/2270 train_time:3197ms step_avg:60.31ms
step:54/2270 train_time:3256ms step_avg:60.29ms
step:55/2270 train_time:3318ms step_avg:60.32ms
step:56/2270 train_time:3377ms step_avg:60.30ms
step:57/2270 train_time:3439ms step_avg:60.33ms
step:58/2270 train_time:3498ms step_avg:60.31ms
step:59/2270 train_time:3560ms step_avg:60.34ms
step:60/2270 train_time:3620ms step_avg:60.33ms
step:61/2270 train_time:3681ms step_avg:60.35ms
step:62/2270 train_time:3740ms step_avg:60.32ms
step:63/2270 train_time:3802ms step_avg:60.35ms
step:64/2270 train_time:3861ms step_avg:60.33ms
step:65/2270 train_time:3923ms step_avg:60.35ms
step:66/2270 train_time:3983ms step_avg:60.34ms
step:67/2270 train_time:4045ms step_avg:60.37ms
step:68/2270 train_time:4104ms step_avg:60.35ms
step:69/2270 train_time:4166ms step_avg:60.37ms
step:70/2270 train_time:4225ms step_avg:60.35ms
step:71/2270 train_time:4287ms step_avg:60.38ms
step:72/2270 train_time:4346ms step_avg:60.36ms
step:73/2270 train_time:4407ms step_avg:60.37ms
step:74/2270 train_time:4466ms step_avg:60.36ms
step:75/2270 train_time:4528ms step_avg:60.38ms
step:76/2270 train_time:4587ms step_avg:60.35ms
step:77/2270 train_time:4648ms step_avg:60.37ms
step:78/2270 train_time:4707ms step_avg:60.34ms
step:79/2270 train_time:4768ms step_avg:60.35ms
step:80/2270 train_time:4826ms step_avg:60.32ms
step:81/2270 train_time:4887ms step_avg:60.33ms
step:82/2270 train_time:4945ms step_avg:60.30ms
step:83/2270 train_time:5006ms step_avg:60.31ms
step:84/2270 train_time:5065ms step_avg:60.30ms
step:85/2270 train_time:5126ms step_avg:60.31ms
step:86/2270 train_time:5185ms step_avg:60.29ms
step:87/2270 train_time:5246ms step_avg:60.30ms
step:88/2270 train_time:5305ms step_avg:60.29ms
step:89/2270 train_time:5367ms step_avg:60.30ms
step:90/2270 train_time:5425ms step_avg:60.28ms
step:91/2270 train_time:5487ms step_avg:60.30ms
step:92/2270 train_time:5546ms step_avg:60.28ms
step:93/2270 train_time:5607ms step_avg:60.29ms
step:94/2270 train_time:5666ms step_avg:60.28ms
step:95/2270 train_time:5728ms step_avg:60.29ms
step:96/2270 train_time:5786ms step_avg:60.27ms
step:97/2270 train_time:5848ms step_avg:60.28ms
step:98/2270 train_time:5906ms step_avg:60.26ms
step:99/2270 train_time:5968ms step_avg:60.28ms
step:100/2270 train_time:6026ms step_avg:60.26ms
step:101/2270 train_time:6088ms step_avg:60.27ms
step:102/2270 train_time:6146ms step_avg:60.26ms
step:103/2270 train_time:6208ms step_avg:60.27ms
step:104/2270 train_time:6267ms step_avg:60.26ms
step:105/2270 train_time:6328ms step_avg:60.27ms
step:106/2270 train_time:6387ms step_avg:60.25ms
step:107/2270 train_time:6448ms step_avg:60.26ms
step:108/2270 train_time:6506ms step_avg:60.24ms
step:109/2270 train_time:6567ms step_avg:60.25ms
step:110/2270 train_time:6626ms step_avg:60.24ms
step:111/2270 train_time:6688ms step_avg:60.25ms
step:112/2270 train_time:6747ms step_avg:60.24ms
step:113/2270 train_time:6807ms step_avg:60.24ms
step:114/2270 train_time:6866ms step_avg:60.23ms
step:115/2270 train_time:6927ms step_avg:60.23ms
step:116/2270 train_time:6986ms step_avg:60.22ms
step:117/2270 train_time:7047ms step_avg:60.23ms
step:118/2270 train_time:7106ms step_avg:60.22ms
step:119/2270 train_time:7168ms step_avg:60.24ms
step:120/2270 train_time:7227ms step_avg:60.22ms
step:121/2270 train_time:7289ms step_avg:60.24ms
step:122/2270 train_time:7346ms step_avg:60.21ms
step:123/2270 train_time:7408ms step_avg:60.23ms
step:124/2270 train_time:7466ms step_avg:60.21ms
step:125/2270 train_time:7527ms step_avg:60.22ms
step:126/2270 train_time:7586ms step_avg:60.21ms
step:127/2270 train_time:7647ms step_avg:60.22ms
step:128/2270 train_time:7706ms step_avg:60.20ms
step:129/2270 train_time:7767ms step_avg:60.21ms
step:130/2270 train_time:7826ms step_avg:60.20ms
step:131/2270 train_time:7887ms step_avg:60.21ms
step:132/2270 train_time:7945ms step_avg:60.19ms
step:133/2270 train_time:8007ms step_avg:60.20ms
step:134/2270 train_time:8066ms step_avg:60.19ms
step:135/2270 train_time:8128ms step_avg:60.21ms
step:136/2270 train_time:8186ms step_avg:60.19ms
step:137/2270 train_time:8248ms step_avg:60.20ms
step:138/2270 train_time:8306ms step_avg:60.19ms
step:139/2270 train_time:8367ms step_avg:60.19ms
step:140/2270 train_time:8425ms step_avg:60.18ms
step:141/2270 train_time:8486ms step_avg:60.18ms
step:142/2270 train_time:8544ms step_avg:60.17ms
step:143/2270 train_time:8606ms step_avg:60.18ms
step:144/2270 train_time:8665ms step_avg:60.17ms
step:145/2270 train_time:8726ms step_avg:60.18ms
step:146/2270 train_time:8784ms step_avg:60.17ms
step:147/2270 train_time:8846ms step_avg:60.18ms
step:148/2270 train_time:8905ms step_avg:60.17ms
step:149/2270 train_time:8967ms step_avg:60.18ms
step:150/2270 train_time:9025ms step_avg:60.17ms
step:151/2270 train_time:9087ms step_avg:60.18ms
step:152/2270 train_time:9145ms step_avg:60.17ms
step:153/2270 train_time:9207ms step_avg:60.18ms
step:154/2270 train_time:9266ms step_avg:60.17ms
step:155/2270 train_time:9327ms step_avg:60.17ms
step:156/2270 train_time:9385ms step_avg:60.16ms
step:157/2270 train_time:9447ms step_avg:60.17ms
step:158/2270 train_time:9505ms step_avg:60.16ms
step:159/2270 train_time:9568ms step_avg:60.17ms
step:160/2270 train_time:9626ms step_avg:60.16ms
step:161/2270 train_time:9687ms step_avg:60.17ms
step:162/2270 train_time:9745ms step_avg:60.15ms
step:163/2270 train_time:9808ms step_avg:60.17ms
step:164/2270 train_time:9866ms step_avg:60.16ms
step:165/2270 train_time:9926ms step_avg:60.16ms
step:166/2270 train_time:9985ms step_avg:60.15ms
step:167/2270 train_time:10046ms step_avg:60.16ms
step:168/2270 train_time:10105ms step_avg:60.15ms
step:169/2270 train_time:10166ms step_avg:60.16ms
step:170/2270 train_time:10225ms step_avg:60.15ms
step:171/2270 train_time:10286ms step_avg:60.15ms
step:172/2270 train_time:10345ms step_avg:60.14ms
step:173/2270 train_time:10406ms step_avg:60.15ms
step:174/2270 train_time:10465ms step_avg:60.15ms
step:175/2270 train_time:10526ms step_avg:60.15ms
step:176/2270 train_time:10585ms step_avg:60.14ms
step:177/2270 train_time:10646ms step_avg:60.15ms
step:178/2270 train_time:10705ms step_avg:60.14ms
step:179/2270 train_time:10767ms step_avg:60.15ms
step:180/2270 train_time:10825ms step_avg:60.14ms
step:181/2270 train_time:10886ms step_avg:60.14ms
step:182/2270 train_time:10944ms step_avg:60.13ms
step:183/2270 train_time:11006ms step_avg:60.14ms
step:184/2270 train_time:11065ms step_avg:60.14ms
step:185/2270 train_time:11127ms step_avg:60.14ms
step:186/2270 train_time:11185ms step_avg:60.14ms
step:187/2270 train_time:11246ms step_avg:60.14ms
step:188/2270 train_time:11305ms step_avg:60.13ms
step:189/2270 train_time:11367ms step_avg:60.14ms
step:190/2270 train_time:11425ms step_avg:60.13ms
step:191/2270 train_time:11486ms step_avg:60.14ms
step:192/2270 train_time:11545ms step_avg:60.13ms
step:193/2270 train_time:11607ms step_avg:60.14ms
step:194/2270 train_time:11666ms step_avg:60.13ms
step:195/2270 train_time:11727ms step_avg:60.14ms
step:196/2270 train_time:11786ms step_avg:60.13ms
step:197/2270 train_time:11847ms step_avg:60.14ms
step:198/2270 train_time:11906ms step_avg:60.13ms
step:199/2270 train_time:11967ms step_avg:60.13ms
step:200/2270 train_time:12025ms step_avg:60.13ms
step:201/2270 train_time:12087ms step_avg:60.13ms
step:202/2270 train_time:12145ms step_avg:60.13ms
step:203/2270 train_time:12207ms step_avg:60.13ms
step:204/2270 train_time:12266ms step_avg:60.13ms
step:205/2270 train_time:12327ms step_avg:60.13ms
step:206/2270 train_time:12385ms step_avg:60.12ms
step:207/2270 train_time:12446ms step_avg:60.13ms
step:208/2270 train_time:12505ms step_avg:60.12ms
step:209/2270 train_time:12567ms step_avg:60.13ms
step:210/2270 train_time:12625ms step_avg:60.12ms
step:211/2270 train_time:12687ms step_avg:60.13ms
step:212/2270 train_time:12745ms step_avg:60.12ms
step:213/2270 train_time:12806ms step_avg:60.12ms
step:214/2270 train_time:12865ms step_avg:60.12ms
step:215/2270 train_time:12927ms step_avg:60.12ms
step:216/2270 train_time:12985ms step_avg:60.12ms
step:217/2270 train_time:13046ms step_avg:60.12ms
step:218/2270 train_time:13105ms step_avg:60.12ms
step:219/2270 train_time:13167ms step_avg:60.12ms
step:220/2270 train_time:13226ms step_avg:60.12ms
step:221/2270 train_time:13287ms step_avg:60.12ms
step:222/2270 train_time:13345ms step_avg:60.11ms
step:223/2270 train_time:13407ms step_avg:60.12ms
step:224/2270 train_time:13466ms step_avg:60.11ms
step:225/2270 train_time:13528ms step_avg:60.12ms
step:226/2270 train_time:13586ms step_avg:60.12ms
step:227/2270 train_time:13648ms step_avg:60.12ms
step:228/2270 train_time:13706ms step_avg:60.11ms
step:229/2270 train_time:13767ms step_avg:60.12ms
step:230/2270 train_time:13825ms step_avg:60.11ms
step:231/2270 train_time:13886ms step_avg:60.11ms
step:232/2270 train_time:13945ms step_avg:60.11ms
step:233/2270 train_time:14006ms step_avg:60.11ms
step:234/2270 train_time:14065ms step_avg:60.11ms
step:235/2270 train_time:14127ms step_avg:60.12ms
step:236/2270 train_time:14186ms step_avg:60.11ms
step:237/2270 train_time:14247ms step_avg:60.11ms
step:238/2270 train_time:14306ms step_avg:60.11ms
step:239/2270 train_time:14367ms step_avg:60.11ms
step:240/2270 train_time:14426ms step_avg:60.11ms
step:241/2270 train_time:14488ms step_avg:60.11ms
step:242/2270 train_time:14546ms step_avg:60.11ms
step:243/2270 train_time:14607ms step_avg:60.11ms
step:244/2270 train_time:14666ms step_avg:60.11ms
step:245/2270 train_time:14727ms step_avg:60.11ms
step:246/2270 train_time:14786ms step_avg:60.10ms
step:247/2270 train_time:14847ms step_avg:60.11ms
step:248/2270 train_time:14905ms step_avg:60.10ms
step:249/2270 train_time:14966ms step_avg:60.10ms
step:250/2270 train_time:15025ms step_avg:60.10ms
step:250/2270 val_loss:4.0816 train_time:15088ms step_avg:60.35ms
step:251/2270 train_time:15106ms step_avg:60.18ms
step:252/2270 train_time:15149ms step_avg:60.11ms
step:253/2270 train_time:15214ms step_avg:60.13ms
step:254/2270 train_time:15278ms step_avg:60.15ms
step:255/2270 train_time:15339ms step_avg:60.15ms
step:256/2270 train_time:15397ms step_avg:60.14ms
step:257/2270 train_time:15457ms step_avg:60.15ms
step:258/2270 train_time:15516ms step_avg:60.14ms
step:259/2270 train_time:15577ms step_avg:60.14ms
step:260/2270 train_time:15634ms step_avg:60.13ms
step:261/2270 train_time:15694ms step_avg:60.13ms
step:262/2270 train_time:15752ms step_avg:60.12ms
step:263/2270 train_time:15813ms step_avg:60.12ms
step:264/2270 train_time:15870ms step_avg:60.11ms
step:265/2270 train_time:15931ms step_avg:60.12ms
step:266/2270 train_time:15989ms step_avg:60.11ms
step:267/2270 train_time:16050ms step_avg:60.11ms
step:268/2270 train_time:16110ms step_avg:60.11ms
step:269/2270 train_time:16172ms step_avg:60.12ms
step:270/2270 train_time:16232ms step_avg:60.12ms
step:271/2270 train_time:16293ms step_avg:60.12ms
step:272/2270 train_time:16352ms step_avg:60.12ms
step:273/2270 train_time:16415ms step_avg:60.13ms
step:274/2270 train_time:16473ms step_avg:60.12ms
step:275/2270 train_time:16534ms step_avg:60.12ms
step:276/2270 train_time:16592ms step_avg:60.12ms
step:277/2270 train_time:16653ms step_avg:60.12ms
step:278/2270 train_time:16711ms step_avg:60.11ms
step:279/2270 train_time:16771ms step_avg:60.11ms
step:280/2270 train_time:16829ms step_avg:60.10ms
step:281/2270 train_time:16889ms step_avg:60.10ms
step:282/2270 train_time:16947ms step_avg:60.10ms
step:283/2270 train_time:17008ms step_avg:60.10ms
step:284/2270 train_time:17067ms step_avg:60.09ms
step:285/2270 train_time:17129ms step_avg:60.10ms
step:286/2270 train_time:17188ms step_avg:60.10ms
step:287/2270 train_time:17251ms step_avg:60.11ms
step:288/2270 train_time:17311ms step_avg:60.11ms
step:289/2270 train_time:17373ms step_avg:60.11ms
step:290/2270 train_time:17432ms step_avg:60.11ms
step:291/2270 train_time:17494ms step_avg:60.12ms
step:292/2270 train_time:17552ms step_avg:60.11ms
step:293/2270 train_time:17613ms step_avg:60.11ms
step:294/2270 train_time:17671ms step_avg:60.11ms
step:295/2270 train_time:17731ms step_avg:60.11ms
step:296/2270 train_time:17790ms step_avg:60.10ms
step:297/2270 train_time:17850ms step_avg:60.10ms
step:298/2270 train_time:17909ms step_avg:60.10ms
step:299/2270 train_time:17970ms step_avg:60.10ms
step:300/2270 train_time:18028ms step_avg:60.09ms
step:301/2270 train_time:18089ms step_avg:60.10ms
step:302/2270 train_time:18148ms step_avg:60.09ms
step:303/2270 train_time:18211ms step_avg:60.10ms
step:304/2270 train_time:18270ms step_avg:60.10ms
step:305/2270 train_time:18333ms step_avg:60.11ms
step:306/2270 train_time:18391ms step_avg:60.10ms
step:307/2270 train_time:18453ms step_avg:60.11ms
step:308/2270 train_time:18512ms step_avg:60.10ms
step:309/2270 train_time:18572ms step_avg:60.10ms
step:310/2270 train_time:18630ms step_avg:60.10ms
step:311/2270 train_time:18691ms step_avg:60.10ms
step:312/2270 train_time:18749ms step_avg:60.09ms
step:313/2270 train_time:18810ms step_avg:60.10ms
step:314/2270 train_time:18868ms step_avg:60.09ms
step:315/2270 train_time:18929ms step_avg:60.09ms
step:316/2270 train_time:18987ms step_avg:60.09ms
step:317/2270 train_time:19048ms step_avg:60.09ms
step:318/2270 train_time:19107ms step_avg:60.08ms
step:319/2270 train_time:19168ms step_avg:60.09ms
step:320/2270 train_time:19227ms step_avg:60.08ms
step:321/2270 train_time:19288ms step_avg:60.09ms
step:322/2270 train_time:19348ms step_avg:60.09ms
step:323/2270 train_time:19411ms step_avg:60.10ms
step:324/2270 train_time:19470ms step_avg:60.09ms
step:325/2270 train_time:19531ms step_avg:60.10ms
step:326/2270 train_time:19590ms step_avg:60.09ms
step:327/2270 train_time:19651ms step_avg:60.09ms
step:328/2270 train_time:19709ms step_avg:60.09ms
step:329/2270 train_time:19771ms step_avg:60.09ms
step:330/2270 train_time:19829ms step_avg:60.09ms
step:331/2270 train_time:19889ms step_avg:60.09ms
step:332/2270 train_time:19948ms step_avg:60.08ms
step:333/2270 train_time:20009ms step_avg:60.09ms
step:334/2270 train_time:20067ms step_avg:60.08ms
step:335/2270 train_time:20128ms step_avg:60.08ms
step:336/2270 train_time:20187ms step_avg:60.08ms
step:337/2270 train_time:20249ms step_avg:60.09ms
step:338/2270 train_time:20309ms step_avg:60.08ms
step:339/2270 train_time:20370ms step_avg:60.09ms
step:340/2270 train_time:20430ms step_avg:60.09ms
step:341/2270 train_time:20491ms step_avg:60.09ms
step:342/2270 train_time:20550ms step_avg:60.09ms
step:343/2270 train_time:20611ms step_avg:60.09ms
step:344/2270 train_time:20670ms step_avg:60.09ms
step:345/2270 train_time:20731ms step_avg:60.09ms
step:346/2270 train_time:20789ms step_avg:60.08ms
step:347/2270 train_time:20851ms step_avg:60.09ms
step:348/2270 train_time:20909ms step_avg:60.08ms
step:349/2270 train_time:20970ms step_avg:60.09ms
step:350/2270 train_time:21028ms step_avg:60.08ms
step:351/2270 train_time:21089ms step_avg:60.08ms
step:352/2270 train_time:21148ms step_avg:60.08ms
step:353/2270 train_time:21211ms step_avg:60.09ms
step:354/2270 train_time:21269ms step_avg:60.08ms
step:355/2270 train_time:21331ms step_avg:60.09ms
step:356/2270 train_time:21390ms step_avg:60.08ms
step:357/2270 train_time:21451ms step_avg:60.09ms
step:358/2270 train_time:21510ms step_avg:60.08ms
step:359/2270 train_time:21572ms step_avg:60.09ms
step:360/2270 train_time:21631ms step_avg:60.08ms
step:361/2270 train_time:21692ms step_avg:60.09ms
step:362/2270 train_time:21750ms step_avg:60.08ms
step:363/2270 train_time:21812ms step_avg:60.09ms
step:364/2270 train_time:21870ms step_avg:60.08ms
step:365/2270 train_time:21931ms step_avg:60.08ms
step:366/2270 train_time:21988ms step_avg:60.08ms
step:367/2270 train_time:22050ms step_avg:60.08ms
step:368/2270 train_time:22108ms step_avg:60.08ms
step:369/2270 train_time:22170ms step_avg:60.08ms
step:370/2270 train_time:22229ms step_avg:60.08ms
step:371/2270 train_time:22290ms step_avg:60.08ms
step:372/2270 train_time:22349ms step_avg:60.08ms
step:373/2270 train_time:22411ms step_avg:60.08ms
step:374/2270 train_time:22470ms step_avg:60.08ms
step:375/2270 train_time:22531ms step_avg:60.08ms
step:376/2270 train_time:22590ms step_avg:60.08ms
step:377/2270 train_time:22651ms step_avg:60.08ms
step:378/2270 train_time:22710ms step_avg:60.08ms
step:379/2270 train_time:22771ms step_avg:60.08ms
step:380/2270 train_time:22830ms step_avg:60.08ms
step:381/2270 train_time:22891ms step_avg:60.08ms
step:382/2270 train_time:22949ms step_avg:60.08ms
step:383/2270 train_time:23011ms step_avg:60.08ms
step:384/2270 train_time:23070ms step_avg:60.08ms
step:385/2270 train_time:23132ms step_avg:60.08ms
step:386/2270 train_time:23190ms step_avg:60.08ms
step:387/2270 train_time:23251ms step_avg:60.08ms
step:388/2270 train_time:23310ms step_avg:60.08ms
step:389/2270 train_time:23372ms step_avg:60.08ms
step:390/2270 train_time:23430ms step_avg:60.08ms
step:391/2270 train_time:23493ms step_avg:60.08ms
step:392/2270 train_time:23551ms step_avg:60.08ms
step:393/2270 train_time:23613ms step_avg:60.08ms
step:394/2270 train_time:23672ms step_avg:60.08ms
step:395/2270 train_time:23733ms step_avg:60.08ms
step:396/2270 train_time:23792ms step_avg:60.08ms
step:397/2270 train_time:23853ms step_avg:60.08ms
step:398/2270 train_time:23911ms step_avg:60.08ms
step:399/2270 train_time:23972ms step_avg:60.08ms
step:400/2270 train_time:24031ms step_avg:60.08ms
step:401/2270 train_time:24092ms step_avg:60.08ms
step:402/2270 train_time:24150ms step_avg:60.08ms
step:403/2270 train_time:24211ms step_avg:60.08ms
step:404/2270 train_time:24270ms step_avg:60.08ms
step:405/2270 train_time:24332ms step_avg:60.08ms
step:406/2270 train_time:24391ms step_avg:60.08ms
step:407/2270 train_time:24452ms step_avg:60.08ms
step:408/2270 train_time:24512ms step_avg:60.08ms
step:409/2270 train_time:24573ms step_avg:60.08ms
step:410/2270 train_time:24632ms step_avg:60.08ms
step:411/2270 train_time:24693ms step_avg:60.08ms
step:412/2270 train_time:24752ms step_avg:60.08ms
step:413/2270 train_time:24813ms step_avg:60.08ms
step:414/2270 train_time:24872ms step_avg:60.08ms
step:415/2270 train_time:24933ms step_avg:60.08ms
step:416/2270 train_time:24991ms step_avg:60.08ms
step:417/2270 train_time:25053ms step_avg:60.08ms
step:418/2270 train_time:25112ms step_avg:60.08ms
step:419/2270 train_time:25173ms step_avg:60.08ms
step:420/2270 train_time:25232ms step_avg:60.08ms
step:421/2270 train_time:25293ms step_avg:60.08ms
step:422/2270 train_time:25351ms step_avg:60.07ms
step:423/2270 train_time:25413ms step_avg:60.08ms
step:424/2270 train_time:25472ms step_avg:60.07ms
step:425/2270 train_time:25533ms step_avg:60.08ms
step:426/2270 train_time:25591ms step_avg:60.07ms
step:427/2270 train_time:25653ms step_avg:60.08ms
step:428/2270 train_time:25711ms step_avg:60.07ms
step:429/2270 train_time:25773ms step_avg:60.08ms
step:430/2270 train_time:25832ms step_avg:60.07ms
step:431/2270 train_time:25893ms step_avg:60.08ms
step:432/2270 train_time:25952ms step_avg:60.07ms
step:433/2270 train_time:26013ms step_avg:60.08ms
step:434/2270 train_time:26072ms step_avg:60.07ms
step:435/2270 train_time:26133ms step_avg:60.07ms
step:436/2270 train_time:26191ms step_avg:60.07ms
step:437/2270 train_time:26253ms step_avg:60.07ms
step:438/2270 train_time:26311ms step_avg:60.07ms
step:439/2270 train_time:26373ms step_avg:60.08ms
step:440/2270 train_time:26432ms step_avg:60.07ms
step:441/2270 train_time:26493ms step_avg:60.08ms
step:442/2270 train_time:26551ms step_avg:60.07ms
step:443/2270 train_time:26613ms step_avg:60.07ms
step:444/2270 train_time:26672ms step_avg:60.07ms
step:445/2270 train_time:26733ms step_avg:60.07ms
step:446/2270 train_time:26791ms step_avg:60.07ms
step:447/2270 train_time:26853ms step_avg:60.07ms
step:448/2270 train_time:26911ms step_avg:60.07ms
step:449/2270 train_time:26972ms step_avg:60.07ms
step:450/2270 train_time:27031ms step_avg:60.07ms
step:451/2270 train_time:27092ms step_avg:60.07ms
step:452/2270 train_time:27151ms step_avg:60.07ms
step:453/2270 train_time:27212ms step_avg:60.07ms
step:454/2270 train_time:27271ms step_avg:60.07ms
step:455/2270 train_time:27333ms step_avg:60.07ms
step:456/2270 train_time:27391ms step_avg:60.07ms
step:457/2270 train_time:27453ms step_avg:60.07ms
step:458/2270 train_time:27511ms step_avg:60.07ms
step:459/2270 train_time:27572ms step_avg:60.07ms
step:460/2270 train_time:27631ms step_avg:60.07ms
step:461/2270 train_time:27692ms step_avg:60.07ms
step:462/2270 train_time:27751ms step_avg:60.07ms
step:463/2270 train_time:27813ms step_avg:60.07ms
step:464/2270 train_time:27871ms step_avg:60.07ms
step:465/2270 train_time:27933ms step_avg:60.07ms
step:466/2270 train_time:27991ms step_avg:60.07ms
step:467/2270 train_time:28053ms step_avg:60.07ms
step:468/2270 train_time:28111ms step_avg:60.07ms
step:469/2270 train_time:28173ms step_avg:60.07ms
step:470/2270 train_time:28231ms step_avg:60.07ms
step:471/2270 train_time:28292ms step_avg:60.07ms
step:472/2270 train_time:28351ms step_avg:60.06ms
step:473/2270 train_time:28413ms step_avg:60.07ms
step:474/2270 train_time:28471ms step_avg:60.07ms
step:475/2270 train_time:28533ms step_avg:60.07ms
step:476/2270 train_time:28592ms step_avg:60.07ms
step:477/2270 train_time:28653ms step_avg:60.07ms
step:478/2270 train_time:28712ms step_avg:60.07ms
step:479/2270 train_time:28774ms step_avg:60.07ms
step:480/2270 train_time:28832ms step_avg:60.07ms
step:481/2270 train_time:28894ms step_avg:60.07ms
step:482/2270 train_time:28952ms step_avg:60.07ms
step:483/2270 train_time:29014ms step_avg:60.07ms
step:484/2270 train_time:29072ms step_avg:60.07ms
step:485/2270 train_time:29133ms step_avg:60.07ms
step:486/2270 train_time:29191ms step_avg:60.06ms
step:487/2270 train_time:29253ms step_avg:60.07ms
step:488/2270 train_time:29312ms step_avg:60.07ms
step:489/2270 train_time:29373ms step_avg:60.07ms
step:490/2270 train_time:29432ms step_avg:60.06ms
step:491/2270 train_time:29493ms step_avg:60.07ms
step:492/2270 train_time:29551ms step_avg:60.06ms
step:493/2270 train_time:29613ms step_avg:60.07ms
step:494/2270 train_time:29672ms step_avg:60.06ms
step:495/2270 train_time:29733ms step_avg:60.07ms
step:496/2270 train_time:29791ms step_avg:60.06ms
step:497/2270 train_time:29853ms step_avg:60.07ms
step:498/2270 train_time:29912ms step_avg:60.06ms
step:499/2270 train_time:29973ms step_avg:60.07ms
step:500/2270 train_time:30031ms step_avg:60.06ms
step:500/2270 val_loss:3.7890 train_time:30094ms step_avg:60.19ms
step:501/2270 train_time:30113ms step_avg:60.11ms
step:502/2270 train_time:30157ms step_avg:60.07ms
step:503/2270 train_time:30219ms step_avg:60.08ms
step:504/2270 train_time:30279ms step_avg:60.08ms
step:505/2270 train_time:30342ms step_avg:60.08ms
step:506/2270 train_time:30401ms step_avg:60.08ms
step:507/2270 train_time:30463ms step_avg:60.08ms
step:508/2270 train_time:30521ms step_avg:60.08ms
step:509/2270 train_time:30582ms step_avg:60.08ms
step:510/2270 train_time:30641ms step_avg:60.08ms
step:511/2270 train_time:30702ms step_avg:60.08ms
step:512/2270 train_time:30760ms step_avg:60.08ms
step:513/2270 train_time:30821ms step_avg:60.08ms
step:514/2270 train_time:30880ms step_avg:60.08ms
step:515/2270 train_time:30940ms step_avg:60.08ms
step:516/2270 train_time:31001ms step_avg:60.08ms
step:517/2270 train_time:31068ms step_avg:60.09ms
step:518/2270 train_time:31129ms step_avg:60.09ms
step:519/2270 train_time:31191ms step_avg:60.10ms
step:520/2270 train_time:31250ms step_avg:60.10ms
step:521/2270 train_time:31312ms step_avg:60.10ms
step:522/2270 train_time:31370ms step_avg:60.10ms
step:523/2270 train_time:31432ms step_avg:60.10ms
step:524/2270 train_time:31490ms step_avg:60.09ms
step:525/2270 train_time:31551ms step_avg:60.10ms
step:526/2270 train_time:31609ms step_avg:60.09ms
step:527/2270 train_time:31670ms step_avg:60.09ms
step:528/2270 train_time:31728ms step_avg:60.09ms
step:529/2270 train_time:31789ms step_avg:60.09ms
step:530/2270 train_time:31848ms step_avg:60.09ms
step:531/2270 train_time:31909ms step_avg:60.09ms
step:532/2270 train_time:31968ms step_avg:60.09ms
step:533/2270 train_time:32031ms step_avg:60.10ms
step:534/2270 train_time:32091ms step_avg:60.10ms
step:535/2270 train_time:32153ms step_avg:60.10ms
step:536/2270 train_time:32212ms step_avg:60.10ms
step:537/2270 train_time:32273ms step_avg:60.10ms
step:538/2270 train_time:32332ms step_avg:60.10ms
step:539/2270 train_time:32394ms step_avg:60.10ms
step:540/2270 train_time:32452ms step_avg:60.10ms
step:541/2270 train_time:32513ms step_avg:60.10ms
step:542/2270 train_time:32572ms step_avg:60.10ms
step:543/2270 train_time:32633ms step_avg:60.10ms
step:544/2270 train_time:32691ms step_avg:60.09ms
step:545/2270 train_time:32752ms step_avg:60.10ms
step:546/2270 train_time:32810ms step_avg:60.09ms
step:547/2270 train_time:32872ms step_avg:60.09ms
step:548/2270 train_time:32930ms step_avg:60.09ms
step:549/2270 train_time:32992ms step_avg:60.09ms
step:550/2270 train_time:33051ms step_avg:60.09ms
step:551/2270 train_time:33113ms step_avg:60.10ms
step:552/2270 train_time:33171ms step_avg:60.09ms
step:553/2270 train_time:33233ms step_avg:60.10ms
step:554/2270 train_time:33292ms step_avg:60.09ms
step:555/2270 train_time:33353ms step_avg:60.10ms
step:556/2270 train_time:33412ms step_avg:60.09ms
step:557/2270 train_time:33473ms step_avg:60.10ms
step:558/2270 train_time:33532ms step_avg:60.09ms
step:559/2270 train_time:33593ms step_avg:60.09ms
step:560/2270 train_time:33652ms step_avg:60.09ms
step:561/2270 train_time:33713ms step_avg:60.09ms
step:562/2270 train_time:33772ms step_avg:60.09ms
step:563/2270 train_time:33832ms step_avg:60.09ms
step:564/2270 train_time:33891ms step_avg:60.09ms
step:565/2270 train_time:33952ms step_avg:60.09ms
step:566/2270 train_time:34011ms step_avg:60.09ms
step:567/2270 train_time:34072ms step_avg:60.09ms
step:568/2270 train_time:34131ms step_avg:60.09ms
step:569/2270 train_time:34192ms step_avg:60.09ms
step:570/2270 train_time:34252ms step_avg:60.09ms
step:571/2270 train_time:34313ms step_avg:60.09ms
step:572/2270 train_time:34372ms step_avg:60.09ms
step:573/2270 train_time:34433ms step_avg:60.09ms
step:574/2270 train_time:34491ms step_avg:60.09ms
step:575/2270 train_time:34552ms step_avg:60.09ms
step:576/2270 train_time:34611ms step_avg:60.09ms
step:577/2270 train_time:34672ms step_avg:60.09ms
step:578/2270 train_time:34730ms step_avg:60.09ms
step:579/2270 train_time:34792ms step_avg:60.09ms
step:580/2270 train_time:34851ms step_avg:60.09ms
step:581/2270 train_time:34911ms step_avg:60.09ms
step:582/2270 train_time:34970ms step_avg:60.09ms
step:583/2270 train_time:35031ms step_avg:60.09ms
step:584/2270 train_time:35090ms step_avg:60.09ms
step:585/2270 train_time:35152ms step_avg:60.09ms
step:586/2270 train_time:35211ms step_avg:60.09ms
step:587/2270 train_time:35272ms step_avg:60.09ms
step:588/2270 train_time:35331ms step_avg:60.09ms
step:589/2270 train_time:35392ms step_avg:60.09ms
step:590/2270 train_time:35451ms step_avg:60.09ms
step:591/2270 train_time:35512ms step_avg:60.09ms
step:592/2270 train_time:35570ms step_avg:60.08ms
step:593/2270 train_time:35631ms step_avg:60.09ms
step:594/2270 train_time:35690ms step_avg:60.08ms
step:595/2270 train_time:35751ms step_avg:60.09ms
step:596/2270 train_time:35810ms step_avg:60.08ms
step:597/2270 train_time:35871ms step_avg:60.09ms
step:598/2270 train_time:35930ms step_avg:60.08ms
step:599/2270 train_time:35992ms step_avg:60.09ms
step:600/2270 train_time:36051ms step_avg:60.09ms
step:601/2270 train_time:36112ms step_avg:60.09ms
step:602/2270 train_time:36171ms step_avg:60.08ms
step:603/2270 train_time:36232ms step_avg:60.09ms
step:604/2270 train_time:36291ms step_avg:60.08ms
step:605/2270 train_time:36352ms step_avg:60.09ms
step:606/2270 train_time:36411ms step_avg:60.08ms
step:607/2270 train_time:36472ms step_avg:60.09ms
step:608/2270 train_time:36531ms step_avg:60.08ms
step:609/2270 train_time:36592ms step_avg:60.09ms
step:610/2270 train_time:36651ms step_avg:60.08ms
step:611/2270 train_time:36712ms step_avg:60.09ms
step:612/2270 train_time:36771ms step_avg:60.08ms
step:613/2270 train_time:36833ms step_avg:60.09ms
step:614/2270 train_time:36891ms step_avg:60.08ms
step:615/2270 train_time:36953ms step_avg:60.09ms
step:616/2270 train_time:37011ms step_avg:60.08ms
step:617/2270 train_time:37072ms step_avg:60.08ms
step:618/2270 train_time:37131ms step_avg:60.08ms
step:619/2270 train_time:37192ms step_avg:60.08ms
step:620/2270 train_time:37251ms step_avg:60.08ms
step:621/2270 train_time:37312ms step_avg:60.08ms
step:622/2270 train_time:37372ms step_avg:60.08ms
step:623/2270 train_time:37433ms step_avg:60.09ms
step:624/2270 train_time:37492ms step_avg:60.08ms
step:625/2270 train_time:37553ms step_avg:60.08ms
step:626/2270 train_time:37612ms step_avg:60.08ms
step:627/2270 train_time:37674ms step_avg:60.09ms
step:628/2270 train_time:37733ms step_avg:60.08ms
step:629/2270 train_time:37795ms step_avg:60.09ms
step:630/2270 train_time:37853ms step_avg:60.08ms
step:631/2270 train_time:37915ms step_avg:60.09ms
step:632/2270 train_time:37973ms step_avg:60.08ms
step:633/2270 train_time:38035ms step_avg:60.09ms
step:634/2270 train_time:38095ms step_avg:60.09ms
step:635/2270 train_time:38156ms step_avg:60.09ms
step:636/2270 train_time:38215ms step_avg:60.09ms
step:637/2270 train_time:38276ms step_avg:60.09ms
step:638/2270 train_time:38335ms step_avg:60.09ms
step:639/2270 train_time:38397ms step_avg:60.09ms
step:640/2270 train_time:38456ms step_avg:60.09ms
step:641/2270 train_time:38518ms step_avg:60.09ms
step:642/2270 train_time:38577ms step_avg:60.09ms
step:643/2270 train_time:38639ms step_avg:60.09ms
step:644/2270 train_time:38698ms step_avg:60.09ms
step:645/2270 train_time:38759ms step_avg:60.09ms
step:646/2270 train_time:38818ms step_avg:60.09ms
step:647/2270 train_time:38880ms step_avg:60.09ms
step:648/2270 train_time:38939ms step_avg:60.09ms
step:649/2270 train_time:39002ms step_avg:60.10ms
step:650/2270 train_time:39061ms step_avg:60.09ms
step:651/2270 train_time:39122ms step_avg:60.10ms
step:652/2270 train_time:39181ms step_avg:60.09ms
step:653/2270 train_time:39243ms step_avg:60.10ms
step:654/2270 train_time:39303ms step_avg:60.10ms
step:655/2270 train_time:39365ms step_avg:60.10ms
step:656/2270 train_time:39425ms step_avg:60.10ms
step:657/2270 train_time:39487ms step_avg:60.10ms
step:658/2270 train_time:39546ms step_avg:60.10ms
step:659/2270 train_time:39608ms step_avg:60.10ms
step:660/2270 train_time:39667ms step_avg:60.10ms
step:661/2270 train_time:39729ms step_avg:60.10ms
step:662/2270 train_time:39789ms step_avg:60.10ms
step:663/2270 train_time:39850ms step_avg:60.11ms
step:664/2270 train_time:39908ms step_avg:60.10ms
step:665/2270 train_time:39970ms step_avg:60.11ms
step:666/2270 train_time:40029ms step_avg:60.10ms
step:667/2270 train_time:40091ms step_avg:60.11ms
step:668/2270 train_time:40149ms step_avg:60.10ms
step:669/2270 train_time:40211ms step_avg:60.11ms
step:670/2270 train_time:40270ms step_avg:60.10ms
step:671/2270 train_time:40332ms step_avg:60.11ms
step:672/2270 train_time:40390ms step_avg:60.10ms
step:673/2270 train_time:40452ms step_avg:60.11ms
step:674/2270 train_time:40510ms step_avg:60.10ms
step:675/2270 train_time:40571ms step_avg:60.11ms
step:676/2270 train_time:40630ms step_avg:60.10ms
step:677/2270 train_time:40692ms step_avg:60.11ms
step:678/2270 train_time:40751ms step_avg:60.10ms
step:679/2270 train_time:40812ms step_avg:60.11ms
step:680/2270 train_time:40871ms step_avg:60.10ms
step:681/2270 train_time:40932ms step_avg:60.11ms
step:682/2270 train_time:40991ms step_avg:60.10ms
step:683/2270 train_time:41052ms step_avg:60.11ms
step:684/2270 train_time:41111ms step_avg:60.10ms
step:685/2270 train_time:41172ms step_avg:60.10ms
step:686/2270 train_time:41231ms step_avg:60.10ms
step:687/2270 train_time:41292ms step_avg:60.10ms
step:688/2270 train_time:41350ms step_avg:60.10ms
step:689/2270 train_time:41411ms step_avg:60.10ms
step:690/2270 train_time:41470ms step_avg:60.10ms
step:691/2270 train_time:41532ms step_avg:60.10ms
step:692/2270 train_time:41590ms step_avg:60.10ms
step:693/2270 train_time:41652ms step_avg:60.10ms
step:694/2270 train_time:41711ms step_avg:60.10ms
step:695/2270 train_time:41773ms step_avg:60.10ms
step:696/2270 train_time:41831ms step_avg:60.10ms
step:697/2270 train_time:41893ms step_avg:60.10ms
step:698/2270 train_time:41952ms step_avg:60.10ms
step:699/2270 train_time:42013ms step_avg:60.10ms
step:700/2270 train_time:42072ms step_avg:60.10ms
step:701/2270 train_time:42133ms step_avg:60.10ms
step:702/2270 train_time:42192ms step_avg:60.10ms
step:703/2270 train_time:42254ms step_avg:60.11ms
step:704/2270 train_time:42313ms step_avg:60.10ms
step:705/2270 train_time:42374ms step_avg:60.11ms
step:706/2270 train_time:42433ms step_avg:60.10ms
step:707/2270 train_time:42495ms step_avg:60.11ms
step:708/2270 train_time:42554ms step_avg:60.10ms
step:709/2270 train_time:42616ms step_avg:60.11ms
step:710/2270 train_time:42675ms step_avg:60.11ms
step:711/2270 train_time:42738ms step_avg:60.11ms
step:712/2270 train_time:42797ms step_avg:60.11ms
step:713/2270 train_time:42859ms step_avg:60.11ms
step:714/2270 train_time:42918ms step_avg:60.11ms
step:715/2270 train_time:42979ms step_avg:60.11ms
step:716/2270 train_time:43039ms step_avg:60.11ms
step:717/2270 train_time:43101ms step_avg:60.11ms
step:718/2270 train_time:43160ms step_avg:60.11ms
step:719/2270 train_time:43222ms step_avg:60.11ms
step:720/2270 train_time:43281ms step_avg:60.11ms
step:721/2270 train_time:43343ms step_avg:60.12ms
step:722/2270 train_time:43402ms step_avg:60.11ms
step:723/2270 train_time:43465ms step_avg:60.12ms
step:724/2270 train_time:43524ms step_avg:60.12ms
step:725/2270 train_time:43586ms step_avg:60.12ms
step:726/2270 train_time:43645ms step_avg:60.12ms
step:727/2270 train_time:43708ms step_avg:60.12ms
step:728/2270 train_time:43767ms step_avg:60.12ms
step:729/2270 train_time:43829ms step_avg:60.12ms
step:730/2270 train_time:43888ms step_avg:60.12ms
step:731/2270 train_time:43950ms step_avg:60.12ms
step:732/2270 train_time:44009ms step_avg:60.12ms
step:733/2270 train_time:44070ms step_avg:60.12ms
step:734/2270 train_time:44129ms step_avg:60.12ms
step:735/2270 train_time:44191ms step_avg:60.12ms
step:736/2270 train_time:44250ms step_avg:60.12ms
step:737/2270 train_time:44311ms step_avg:60.12ms
step:738/2270 train_time:44370ms step_avg:60.12ms
step:739/2270 train_time:44432ms step_avg:60.12ms
step:740/2270 train_time:44490ms step_avg:60.12ms
step:741/2270 train_time:44552ms step_avg:60.12ms
step:742/2270 train_time:44611ms step_avg:60.12ms
step:743/2270 train_time:44672ms step_avg:60.12ms
step:744/2270 train_time:44731ms step_avg:60.12ms
step:745/2270 train_time:44793ms step_avg:60.12ms
step:746/2270 train_time:44852ms step_avg:60.12ms
step:747/2270 train_time:44912ms step_avg:60.12ms
step:748/2270 train_time:44971ms step_avg:60.12ms
step:749/2270 train_time:45033ms step_avg:60.12ms
step:750/2270 train_time:45091ms step_avg:60.12ms
step:750/2270 val_loss:3.6605 train_time:45153ms step_avg:60.20ms
step:751/2270 train_time:45172ms step_avg:60.15ms
step:752/2270 train_time:45218ms step_avg:60.13ms
step:753/2270 train_time:45280ms step_avg:60.13ms
step:754/2270 train_time:45341ms step_avg:60.13ms
step:755/2270 train_time:45403ms step_avg:60.14ms
step:756/2270 train_time:45462ms step_avg:60.14ms
step:757/2270 train_time:45523ms step_avg:60.14ms
step:758/2270 train_time:45582ms step_avg:60.13ms
step:759/2270 train_time:45644ms step_avg:60.14ms
step:760/2270 train_time:45703ms step_avg:60.14ms
step:761/2270 train_time:45765ms step_avg:60.14ms
step:762/2270 train_time:45824ms step_avg:60.14ms
step:763/2270 train_time:45886ms step_avg:60.14ms
step:764/2270 train_time:45946ms step_avg:60.14ms
step:765/2270 train_time:46008ms step_avg:60.14ms
step:766/2270 train_time:46067ms step_avg:60.14ms
step:767/2270 train_time:46130ms step_avg:60.14ms
step:768/2270 train_time:46189ms step_avg:60.14ms
step:769/2270 train_time:46252ms step_avg:60.15ms
step:770/2270 train_time:46311ms step_avg:60.14ms
step:771/2270 train_time:46373ms step_avg:60.15ms
step:772/2270 train_time:46433ms step_avg:60.15ms
step:773/2270 train_time:46495ms step_avg:60.15ms
step:774/2270 train_time:46554ms step_avg:60.15ms
step:775/2270 train_time:46615ms step_avg:60.15ms
step:776/2270 train_time:46675ms step_avg:60.15ms
step:777/2270 train_time:46737ms step_avg:60.15ms
step:778/2270 train_time:46797ms step_avg:60.15ms
step:779/2270 train_time:46859ms step_avg:60.15ms
step:780/2270 train_time:46918ms step_avg:60.15ms
step:781/2270 train_time:46980ms step_avg:60.15ms
step:782/2270 train_time:47039ms step_avg:60.15ms
step:783/2270 train_time:47103ms step_avg:60.16ms
step:784/2270 train_time:47163ms step_avg:60.16ms
step:785/2270 train_time:47226ms step_avg:60.16ms
step:786/2270 train_time:47286ms step_avg:60.16ms
step:787/2270 train_time:47348ms step_avg:60.16ms
step:788/2270 train_time:47408ms step_avg:60.16ms
step:789/2270 train_time:47470ms step_avg:60.16ms
step:790/2270 train_time:47529ms step_avg:60.16ms
step:791/2270 train_time:47590ms step_avg:60.16ms
step:792/2270 train_time:47649ms step_avg:60.16ms
step:793/2270 train_time:47710ms step_avg:60.16ms
step:794/2270 train_time:47769ms step_avg:60.16ms
step:795/2270 train_time:47831ms step_avg:60.16ms
step:796/2270 train_time:47889ms step_avg:60.16ms
step:797/2270 train_time:47951ms step_avg:60.16ms
step:798/2270 train_time:48010ms step_avg:60.16ms
step:799/2270 train_time:48073ms step_avg:60.17ms
step:800/2270 train_time:48132ms step_avg:60.17ms
step:801/2270 train_time:48195ms step_avg:60.17ms
step:802/2270 train_time:48254ms step_avg:60.17ms
step:803/2270 train_time:48317ms step_avg:60.17ms
step:804/2270 train_time:48376ms step_avg:60.17ms
step:805/2270 train_time:48439ms step_avg:60.17ms
step:806/2270 train_time:48499ms step_avg:60.17ms
step:807/2270 train_time:48561ms step_avg:60.17ms
step:808/2270 train_time:48620ms step_avg:60.17ms
step:809/2270 train_time:48683ms step_avg:60.18ms
step:810/2270 train_time:48743ms step_avg:60.18ms
step:811/2270 train_time:48805ms step_avg:60.18ms
step:812/2270 train_time:48864ms step_avg:60.18ms
step:813/2270 train_time:48926ms step_avg:60.18ms
step:814/2270 train_time:48986ms step_avg:60.18ms
step:815/2270 train_time:49048ms step_avg:60.18ms
step:816/2270 train_time:49107ms step_avg:60.18ms
step:817/2270 train_time:49169ms step_avg:60.18ms
step:818/2270 train_time:49228ms step_avg:60.18ms
step:819/2270 train_time:49289ms step_avg:60.18ms
step:820/2270 train_time:49349ms step_avg:60.18ms
step:821/2270 train_time:49410ms step_avg:60.18ms
step:822/2270 train_time:49469ms step_avg:60.18ms
step:823/2270 train_time:49531ms step_avg:60.18ms
step:824/2270 train_time:49590ms step_avg:60.18ms
step:825/2270 train_time:49652ms step_avg:60.18ms
step:826/2270 train_time:49711ms step_avg:60.18ms
step:827/2270 train_time:49773ms step_avg:60.18ms
step:828/2270 train_time:49832ms step_avg:60.18ms
step:829/2270 train_time:49894ms step_avg:60.19ms
step:830/2270 train_time:49954ms step_avg:60.19ms
step:831/2270 train_time:50016ms step_avg:60.19ms
step:832/2270 train_time:50076ms step_avg:60.19ms
step:833/2270 train_time:50138ms step_avg:60.19ms
step:834/2270 train_time:50198ms step_avg:60.19ms
step:835/2270 train_time:50260ms step_avg:60.19ms
step:836/2270 train_time:50319ms step_avg:60.19ms
step:837/2270 train_time:50382ms step_avg:60.19ms
step:838/2270 train_time:50442ms step_avg:60.19ms
step:839/2270 train_time:50505ms step_avg:60.20ms
step:840/2270 train_time:50565ms step_avg:60.20ms
step:841/2270 train_time:50627ms step_avg:60.20ms
step:842/2270 train_time:50686ms step_avg:60.20ms
step:843/2270 train_time:50749ms step_avg:60.20ms
step:844/2270 train_time:50808ms step_avg:60.20ms
step:845/2270 train_time:50870ms step_avg:60.20ms
step:846/2270 train_time:50929ms step_avg:60.20ms
step:847/2270 train_time:50990ms step_avg:60.20ms
step:848/2270 train_time:51049ms step_avg:60.20ms
step:849/2270 train_time:51112ms step_avg:60.20ms
step:850/2270 train_time:51171ms step_avg:60.20ms
step:851/2270 train_time:51232ms step_avg:60.20ms
step:852/2270 train_time:51293ms step_avg:60.20ms
step:853/2270 train_time:51355ms step_avg:60.20ms
step:854/2270 train_time:51415ms step_avg:60.20ms
step:855/2270 train_time:51476ms step_avg:60.21ms
step:856/2270 train_time:51536ms step_avg:60.21ms
step:857/2270 train_time:51600ms step_avg:60.21ms
step:858/2270 train_time:51660ms step_avg:60.21ms
step:859/2270 train_time:51722ms step_avg:60.21ms
step:860/2270 train_time:51782ms step_avg:60.21ms
step:861/2270 train_time:51845ms step_avg:60.21ms
step:862/2270 train_time:51905ms step_avg:60.21ms
step:863/2270 train_time:51967ms step_avg:60.22ms
step:864/2270 train_time:52026ms step_avg:60.22ms
step:865/2270 train_time:52088ms step_avg:60.22ms
step:866/2270 train_time:52147ms step_avg:60.22ms
step:867/2270 train_time:52208ms step_avg:60.22ms
step:868/2270 train_time:52268ms step_avg:60.22ms
step:869/2270 train_time:52329ms step_avg:60.22ms
step:870/2270 train_time:52388ms step_avg:60.22ms
step:871/2270 train_time:52450ms step_avg:60.22ms
step:872/2270 train_time:52509ms step_avg:60.22ms
step:873/2270 train_time:52570ms step_avg:60.22ms
step:874/2270 train_time:52629ms step_avg:60.22ms
step:875/2270 train_time:52691ms step_avg:60.22ms
step:876/2270 train_time:52751ms step_avg:60.22ms
step:877/2270 train_time:52813ms step_avg:60.22ms
step:878/2270 train_time:52872ms step_avg:60.22ms
step:879/2270 train_time:52934ms step_avg:60.22ms
step:880/2270 train_time:52994ms step_avg:60.22ms
step:881/2270 train_time:53056ms step_avg:60.22ms
step:882/2270 train_time:53116ms step_avg:60.22ms
step:883/2270 train_time:53178ms step_avg:60.22ms
step:884/2270 train_time:53237ms step_avg:60.22ms
step:885/2270 train_time:53299ms step_avg:60.23ms
step:886/2270 train_time:53359ms step_avg:60.22ms
step:887/2270 train_time:53421ms step_avg:60.23ms
step:888/2270 train_time:53481ms step_avg:60.23ms
step:889/2270 train_time:53543ms step_avg:60.23ms
step:890/2270 train_time:53602ms step_avg:60.23ms
step:891/2270 train_time:53665ms step_avg:60.23ms
step:892/2270 train_time:53725ms step_avg:60.23ms
step:893/2270 train_time:53787ms step_avg:60.23ms
step:894/2270 train_time:53847ms step_avg:60.23ms
step:895/2270 train_time:53909ms step_avg:60.23ms
step:896/2270 train_time:53968ms step_avg:60.23ms
step:897/2270 train_time:54029ms step_avg:60.23ms
step:898/2270 train_time:54087ms step_avg:60.23ms
step:899/2270 train_time:54150ms step_avg:60.23ms
step:900/2270 train_time:54209ms step_avg:60.23ms
step:901/2270 train_time:54270ms step_avg:60.23ms
step:902/2270 train_time:54329ms step_avg:60.23ms
step:903/2270 train_time:54390ms step_avg:60.23ms
step:904/2270 train_time:54449ms step_avg:60.23ms
step:905/2270 train_time:54511ms step_avg:60.23ms
step:906/2270 train_time:54571ms step_avg:60.23ms
step:907/2270 train_time:54633ms step_avg:60.23ms
step:908/2270 train_time:54692ms step_avg:60.23ms
step:909/2270 train_time:54754ms step_avg:60.24ms
step:910/2270 train_time:54813ms step_avg:60.23ms
step:911/2270 train_time:54875ms step_avg:60.24ms
step:912/2270 train_time:54935ms step_avg:60.24ms
step:913/2270 train_time:54997ms step_avg:60.24ms
step:914/2270 train_time:55056ms step_avg:60.24ms
step:915/2270 train_time:55119ms step_avg:60.24ms
step:916/2270 train_time:55178ms step_avg:60.24ms
step:917/2270 train_time:55240ms step_avg:60.24ms
step:918/2270 train_time:55300ms step_avg:60.24ms
step:919/2270 train_time:55363ms step_avg:60.24ms
step:920/2270 train_time:55422ms step_avg:60.24ms
step:921/2270 train_time:55485ms step_avg:60.24ms
step:922/2270 train_time:55544ms step_avg:60.24ms
step:923/2270 train_time:55606ms step_avg:60.25ms
step:924/2270 train_time:55666ms step_avg:60.24ms
step:925/2270 train_time:55728ms step_avg:60.25ms
step:926/2270 train_time:55787ms step_avg:60.25ms
step:927/2270 train_time:55849ms step_avg:60.25ms
step:928/2270 train_time:55909ms step_avg:60.25ms
step:929/2270 train_time:55970ms step_avg:60.25ms
step:930/2270 train_time:56029ms step_avg:60.25ms
step:931/2270 train_time:56090ms step_avg:60.25ms
step:932/2270 train_time:56149ms step_avg:60.25ms
step:933/2270 train_time:56210ms step_avg:60.25ms
step:934/2270 train_time:56269ms step_avg:60.24ms
step:935/2270 train_time:56330ms step_avg:60.25ms
step:936/2270 train_time:56389ms step_avg:60.24ms
step:937/2270 train_time:56451ms step_avg:60.25ms
step:938/2270 train_time:56510ms step_avg:60.25ms
step:939/2270 train_time:56572ms step_avg:60.25ms
step:940/2270 train_time:56632ms step_avg:60.25ms
step:941/2270 train_time:56694ms step_avg:60.25ms
step:942/2270 train_time:56754ms step_avg:60.25ms
step:943/2270 train_time:56816ms step_avg:60.25ms
step:944/2270 train_time:56875ms step_avg:60.25ms
step:945/2270 train_time:56937ms step_avg:60.25ms
step:946/2270 train_time:56997ms step_avg:60.25ms
step:947/2270 train_time:57059ms step_avg:60.25ms
step:948/2270 train_time:57119ms step_avg:60.25ms
step:949/2270 train_time:57181ms step_avg:60.25ms
step:950/2270 train_time:57240ms step_avg:60.25ms
step:951/2270 train_time:57303ms step_avg:60.26ms
step:952/2270 train_time:57362ms step_avg:60.25ms
step:953/2270 train_time:57425ms step_avg:60.26ms
step:954/2270 train_time:57485ms step_avg:60.26ms
step:955/2270 train_time:57548ms step_avg:60.26ms
step:956/2270 train_time:57607ms step_avg:60.26ms
step:957/2270 train_time:57669ms step_avg:60.26ms
step:958/2270 train_time:57728ms step_avg:60.26ms
step:959/2270 train_time:57790ms step_avg:60.26ms
step:960/2270 train_time:57850ms step_avg:60.26ms
step:961/2270 train_time:57911ms step_avg:60.26ms
step:962/2270 train_time:57970ms step_avg:60.26ms
step:963/2270 train_time:58032ms step_avg:60.26ms
step:964/2270 train_time:58091ms step_avg:60.26ms
step:965/2270 train_time:58153ms step_avg:60.26ms
step:966/2270 train_time:58213ms step_avg:60.26ms
step:967/2270 train_time:58275ms step_avg:60.26ms
step:968/2270 train_time:58335ms step_avg:60.26ms
step:969/2270 train_time:58398ms step_avg:60.27ms
step:970/2270 train_time:58458ms step_avg:60.27ms
step:971/2270 train_time:58521ms step_avg:60.27ms
step:972/2270 train_time:58580ms step_avg:60.27ms
step:973/2270 train_time:58643ms step_avg:60.27ms
step:974/2270 train_time:58703ms step_avg:60.27ms
step:975/2270 train_time:58765ms step_avg:60.27ms
step:976/2270 train_time:58825ms step_avg:60.27ms
step:977/2270 train_time:58888ms step_avg:60.27ms
step:978/2270 train_time:58947ms step_avg:60.27ms
step:979/2270 train_time:59008ms step_avg:60.27ms
step:980/2270 train_time:59067ms step_avg:60.27ms
step:981/2270 train_time:59130ms step_avg:60.27ms
step:982/2270 train_time:59188ms step_avg:60.27ms
step:983/2270 train_time:59249ms step_avg:60.27ms
step:984/2270 train_time:59309ms step_avg:60.27ms
step:985/2270 train_time:59370ms step_avg:60.27ms
step:986/2270 train_time:59429ms step_avg:60.27ms
step:987/2270 train_time:59491ms step_avg:60.27ms
step:988/2270 train_time:59550ms step_avg:60.27ms
step:989/2270 train_time:59613ms step_avg:60.28ms
step:990/2270 train_time:59673ms step_avg:60.28ms
step:991/2270 train_time:59735ms step_avg:60.28ms
step:992/2270 train_time:59794ms step_avg:60.28ms
step:993/2270 train_time:59856ms step_avg:60.28ms
step:994/2270 train_time:59916ms step_avg:60.28ms
step:995/2270 train_time:59978ms step_avg:60.28ms
step:996/2270 train_time:60038ms step_avg:60.28ms
step:997/2270 train_time:60100ms step_avg:60.28ms
step:998/2270 train_time:60160ms step_avg:60.28ms
step:999/2270 train_time:60222ms step_avg:60.28ms
step:1000/2270 train_time:60282ms step_avg:60.28ms
step:1000/2270 val_loss:3.5766 train_time:60346ms step_avg:60.35ms
step:1001/2270 train_time:60365ms step_avg:60.30ms
step:1002/2270 train_time:60412ms step_avg:60.29ms
step:1003/2270 train_time:60473ms step_avg:60.29ms
step:1004/2270 train_time:60532ms step_avg:60.29ms
step:1005/2270 train_time:60595ms step_avg:60.29ms
step:1006/2270 train_time:60655ms step_avg:60.29ms
step:1007/2270 train_time:60716ms step_avg:60.29ms
step:1008/2270 train_time:60774ms step_avg:60.29ms
step:1009/2270 train_time:60836ms step_avg:60.29ms
step:1010/2270 train_time:60895ms step_avg:60.29ms
step:1011/2270 train_time:60956ms step_avg:60.29ms
step:1012/2270 train_time:61014ms step_avg:60.29ms
step:1013/2270 train_time:61075ms step_avg:60.29ms
step:1014/2270 train_time:61134ms step_avg:60.29ms
step:1015/2270 train_time:61195ms step_avg:60.29ms
step:1016/2270 train_time:61256ms step_avg:60.29ms
step:1017/2270 train_time:61323ms step_avg:60.30ms
step:1018/2270 train_time:61384ms step_avg:60.30ms
step:1019/2270 train_time:61446ms step_avg:60.30ms
step:1020/2270 train_time:61507ms step_avg:60.30ms
step:1021/2270 train_time:61569ms step_avg:60.30ms
step:1022/2270 train_time:61629ms step_avg:60.30ms
step:1023/2270 train_time:61690ms step_avg:60.30ms
step:1024/2270 train_time:61749ms step_avg:60.30ms
step:1025/2270 train_time:61811ms step_avg:60.30ms
step:1026/2270 train_time:61869ms step_avg:60.30ms
step:1027/2270 train_time:61930ms step_avg:60.30ms
step:1028/2270 train_time:61989ms step_avg:60.30ms
step:1029/2270 train_time:62050ms step_avg:60.30ms
step:1030/2270 train_time:62109ms step_avg:60.30ms
step:1031/2270 train_time:62171ms step_avg:60.30ms
step:1032/2270 train_time:62230ms step_avg:60.30ms
step:1033/2270 train_time:62293ms step_avg:60.30ms
step:1034/2270 train_time:62353ms step_avg:60.30ms
step:1035/2270 train_time:62416ms step_avg:60.30ms
step:1036/2270 train_time:62476ms step_avg:60.30ms
step:1037/2270 train_time:62539ms step_avg:60.31ms
step:1038/2270 train_time:62599ms step_avg:60.31ms
step:1039/2270 train_time:62662ms step_avg:60.31ms
step:1040/2270 train_time:62721ms step_avg:60.31ms
step:1041/2270 train_time:62783ms step_avg:60.31ms
step:1042/2270 train_time:62843ms step_avg:60.31ms
step:1043/2270 train_time:62905ms step_avg:60.31ms
step:1044/2270 train_time:62965ms step_avg:60.31ms
step:1045/2270 train_time:63027ms step_avg:60.31ms
step:1046/2270 train_time:63087ms step_avg:60.31ms
step:1047/2270 train_time:63149ms step_avg:60.31ms
step:1048/2270 train_time:63208ms step_avg:60.31ms
step:1049/2270 train_time:63270ms step_avg:60.31ms
step:1050/2270 train_time:63330ms step_avg:60.31ms
step:1051/2270 train_time:63392ms step_avg:60.32ms
step:1052/2270 train_time:63451ms step_avg:60.31ms
step:1053/2270 train_time:63513ms step_avg:60.32ms
step:1054/2270 train_time:63572ms step_avg:60.31ms
step:1055/2270 train_time:63633ms step_avg:60.32ms
step:1056/2270 train_time:63693ms step_avg:60.31ms
step:1057/2270 train_time:63755ms step_avg:60.32ms
step:1058/2270 train_time:63814ms step_avg:60.32ms
step:1059/2270 train_time:63876ms step_avg:60.32ms
step:1060/2270 train_time:63936ms step_avg:60.32ms
step:1061/2270 train_time:63998ms step_avg:60.32ms
step:1062/2270 train_time:64058ms step_avg:60.32ms
step:1063/2270 train_time:64120ms step_avg:60.32ms
step:1064/2270 train_time:64180ms step_avg:60.32ms
step:1065/2270 train_time:64242ms step_avg:60.32ms
step:1066/2270 train_time:64302ms step_avg:60.32ms
step:1067/2270 train_time:64364ms step_avg:60.32ms
step:1068/2270 train_time:64425ms step_avg:60.32ms
step:1069/2270 train_time:64488ms step_avg:60.33ms
step:1070/2270 train_time:64547ms step_avg:60.32ms
step:1071/2270 train_time:64609ms step_avg:60.33ms
step:1072/2270 train_time:64668ms step_avg:60.32ms
step:1073/2270 train_time:64730ms step_avg:60.33ms
step:1074/2270 train_time:64789ms step_avg:60.33ms
step:1075/2270 train_time:64851ms step_avg:60.33ms
step:1076/2270 train_time:64910ms step_avg:60.33ms
step:1077/2270 train_time:64972ms step_avg:60.33ms
step:1078/2270 train_time:65031ms step_avg:60.33ms
step:1079/2270 train_time:65093ms step_avg:60.33ms
step:1080/2270 train_time:65153ms step_avg:60.33ms
step:1081/2270 train_time:65215ms step_avg:60.33ms
step:1082/2270 train_time:65275ms step_avg:60.33ms
step:1083/2270 train_time:65338ms step_avg:60.33ms
step:1084/2270 train_time:65398ms step_avg:60.33ms
step:1085/2270 train_time:65460ms step_avg:60.33ms
step:1086/2270 train_time:65520ms step_avg:60.33ms
step:1087/2270 train_time:65582ms step_avg:60.33ms
step:1088/2270 train_time:65642ms step_avg:60.33ms
step:1089/2270 train_time:65705ms step_avg:60.33ms
step:1090/2270 train_time:65764ms step_avg:60.33ms
step:1091/2270 train_time:65827ms step_avg:60.34ms
step:1092/2270 train_time:65887ms step_avg:60.34ms
step:1093/2270 train_time:65949ms step_avg:60.34ms
step:1094/2270 train_time:66008ms step_avg:60.34ms
step:1095/2270 train_time:66069ms step_avg:60.34ms
step:1096/2270 train_time:66128ms step_avg:60.34ms
step:1097/2270 train_time:66190ms step_avg:60.34ms
step:1098/2270 train_time:66249ms step_avg:60.34ms
step:1099/2270 train_time:66311ms step_avg:60.34ms
step:1100/2270 train_time:66370ms step_avg:60.34ms
step:1101/2270 train_time:66432ms step_avg:60.34ms
step:1102/2270 train_time:66491ms step_avg:60.34ms
step:1103/2270 train_time:66553ms step_avg:60.34ms
step:1104/2270 train_time:66612ms step_avg:60.34ms
step:1105/2270 train_time:66675ms step_avg:60.34ms
step:1106/2270 train_time:66734ms step_avg:60.34ms
step:1107/2270 train_time:66797ms step_avg:60.34ms
step:1108/2270 train_time:66858ms step_avg:60.34ms
step:1109/2270 train_time:66920ms step_avg:60.34ms
step:1110/2270 train_time:66980ms step_avg:60.34ms
step:1111/2270 train_time:67042ms step_avg:60.34ms
step:1112/2270 train_time:67101ms step_avg:60.34ms
step:1113/2270 train_time:67164ms step_avg:60.35ms
step:1114/2270 train_time:67224ms step_avg:60.34ms
step:1115/2270 train_time:67287ms step_avg:60.35ms
step:1116/2270 train_time:67346ms step_avg:60.35ms
step:1117/2270 train_time:67408ms step_avg:60.35ms
step:1118/2270 train_time:67468ms step_avg:60.35ms
step:1119/2270 train_time:67530ms step_avg:60.35ms
step:1120/2270 train_time:67590ms step_avg:60.35ms
step:1121/2270 train_time:67652ms step_avg:60.35ms
step:1122/2270 train_time:67711ms step_avg:60.35ms
step:1123/2270 train_time:67773ms step_avg:60.35ms
step:1124/2270 train_time:67832ms step_avg:60.35ms
step:1125/2270 train_time:67894ms step_avg:60.35ms
step:1126/2270 train_time:67954ms step_avg:60.35ms
step:1127/2270 train_time:68015ms step_avg:60.35ms
step:1128/2270 train_time:68075ms step_avg:60.35ms
step:1129/2270 train_time:68138ms step_avg:60.35ms
step:1130/2270 train_time:68198ms step_avg:60.35ms
step:1131/2270 train_time:68260ms step_avg:60.35ms
step:1132/2270 train_time:68320ms step_avg:60.35ms
step:1133/2270 train_time:68382ms step_avg:60.36ms
step:1134/2270 train_time:68442ms step_avg:60.35ms
step:1135/2270 train_time:68505ms step_avg:60.36ms
step:1136/2270 train_time:68565ms step_avg:60.36ms
step:1137/2270 train_time:68627ms step_avg:60.36ms
step:1138/2270 train_time:68687ms step_avg:60.36ms
step:1139/2270 train_time:68750ms step_avg:60.36ms
step:1140/2270 train_time:68809ms step_avg:60.36ms
step:1141/2270 train_time:68871ms step_avg:60.36ms
step:1142/2270 train_time:68930ms step_avg:60.36ms
step:1143/2270 train_time:68992ms step_avg:60.36ms
step:1144/2270 train_time:69051ms step_avg:60.36ms
step:1145/2270 train_time:69113ms step_avg:60.36ms
step:1146/2270 train_time:69173ms step_avg:60.36ms
step:1147/2270 train_time:69235ms step_avg:60.36ms
step:1148/2270 train_time:69295ms step_avg:60.36ms
step:1149/2270 train_time:69359ms step_avg:60.36ms
step:1150/2270 train_time:69419ms step_avg:60.36ms
step:1151/2270 train_time:69481ms step_avg:60.37ms
step:1152/2270 train_time:69541ms step_avg:60.37ms
step:1153/2270 train_time:69605ms step_avg:60.37ms
step:1154/2270 train_time:69665ms step_avg:60.37ms
step:1155/2270 train_time:69728ms step_avg:60.37ms
step:1156/2270 train_time:69788ms step_avg:60.37ms
step:1157/2270 train_time:69850ms step_avg:60.37ms
step:1158/2270 train_time:69909ms step_avg:60.37ms
step:1159/2270 train_time:69972ms step_avg:60.37ms
step:1160/2270 train_time:70031ms step_avg:60.37ms
step:1161/2270 train_time:70093ms step_avg:60.37ms
step:1162/2270 train_time:70152ms step_avg:60.37ms
step:1163/2270 train_time:70215ms step_avg:60.37ms
step:1164/2270 train_time:70274ms step_avg:60.37ms
step:1165/2270 train_time:70336ms step_avg:60.37ms
step:1166/2270 train_time:70396ms step_avg:60.37ms
step:1167/2270 train_time:70458ms step_avg:60.38ms
step:1168/2270 train_time:70519ms step_avg:60.38ms
step:1169/2270 train_time:70582ms step_avg:60.38ms
step:1170/2270 train_time:70643ms step_avg:60.38ms
step:1171/2270 train_time:70706ms step_avg:60.38ms
step:1172/2270 train_time:70766ms step_avg:60.38ms
step:1173/2270 train_time:70828ms step_avg:60.38ms
step:1174/2270 train_time:70888ms step_avg:60.38ms
step:1175/2270 train_time:70951ms step_avg:60.38ms
step:1176/2270 train_time:71010ms step_avg:60.38ms
step:1177/2270 train_time:71072ms step_avg:60.38ms
step:1178/2270 train_time:71132ms step_avg:60.38ms
step:1179/2270 train_time:71194ms step_avg:60.38ms
step:1180/2270 train_time:71253ms step_avg:60.38ms
step:1181/2270 train_time:71315ms step_avg:60.39ms
step:1182/2270 train_time:71375ms step_avg:60.38ms
step:1183/2270 train_time:71438ms step_avg:60.39ms
step:1184/2270 train_time:71498ms step_avg:60.39ms
step:1185/2270 train_time:71562ms step_avg:60.39ms
step:1186/2270 train_time:71622ms step_avg:60.39ms
step:1187/2270 train_time:71684ms step_avg:60.39ms
step:1188/2270 train_time:71744ms step_avg:60.39ms
step:1189/2270 train_time:71807ms step_avg:60.39ms
step:1190/2270 train_time:71867ms step_avg:60.39ms
step:1191/2270 train_time:71930ms step_avg:60.39ms
step:1192/2270 train_time:71990ms step_avg:60.39ms
step:1193/2270 train_time:72052ms step_avg:60.40ms
step:1194/2270 train_time:72111ms step_avg:60.39ms
step:1195/2270 train_time:72173ms step_avg:60.40ms
step:1196/2270 train_time:72232ms step_avg:60.39ms
step:1197/2270 train_time:72294ms step_avg:60.40ms
step:1198/2270 train_time:72353ms step_avg:60.39ms
step:1199/2270 train_time:72415ms step_avg:60.40ms
step:1200/2270 train_time:72475ms step_avg:60.40ms
step:1201/2270 train_time:72537ms step_avg:60.40ms
step:1202/2270 train_time:72598ms step_avg:60.40ms
step:1203/2270 train_time:72662ms step_avg:60.40ms
step:1204/2270 train_time:72722ms step_avg:60.40ms
step:1205/2270 train_time:72785ms step_avg:60.40ms
step:1206/2270 train_time:72845ms step_avg:60.40ms
step:1207/2270 train_time:72908ms step_avg:60.40ms
step:1208/2270 train_time:72968ms step_avg:60.40ms
step:1209/2270 train_time:73030ms step_avg:60.41ms
step:1210/2270 train_time:73089ms step_avg:60.40ms
step:1211/2270 train_time:73152ms step_avg:60.41ms
step:1212/2270 train_time:73211ms step_avg:60.41ms
step:1213/2270 train_time:73273ms step_avg:60.41ms
step:1214/2270 train_time:73333ms step_avg:60.41ms
step:1215/2270 train_time:73395ms step_avg:60.41ms
step:1216/2270 train_time:73454ms step_avg:60.41ms
step:1217/2270 train_time:73516ms step_avg:60.41ms
step:1218/2270 train_time:73576ms step_avg:60.41ms
step:1219/2270 train_time:73640ms step_avg:60.41ms
step:1220/2270 train_time:73700ms step_avg:60.41ms
step:1221/2270 train_time:73763ms step_avg:60.41ms
step:1222/2270 train_time:73822ms step_avg:60.41ms
step:1223/2270 train_time:73885ms step_avg:60.41ms
step:1224/2270 train_time:73946ms step_avg:60.41ms
step:1225/2270 train_time:74009ms step_avg:60.42ms
step:1226/2270 train_time:74069ms step_avg:60.42ms
step:1227/2270 train_time:74131ms step_avg:60.42ms
step:1228/2270 train_time:74191ms step_avg:60.42ms
step:1229/2270 train_time:74253ms step_avg:60.42ms
step:1230/2270 train_time:74313ms step_avg:60.42ms
step:1231/2270 train_time:74375ms step_avg:60.42ms
step:1232/2270 train_time:74434ms step_avg:60.42ms
step:1233/2270 train_time:74496ms step_avg:60.42ms
step:1234/2270 train_time:74556ms step_avg:60.42ms
step:1235/2270 train_time:74619ms step_avg:60.42ms
step:1236/2270 train_time:74679ms step_avg:60.42ms
step:1237/2270 train_time:74742ms step_avg:60.42ms
step:1238/2270 train_time:74802ms step_avg:60.42ms
step:1239/2270 train_time:74865ms step_avg:60.42ms
step:1240/2270 train_time:74925ms step_avg:60.42ms
step:1241/2270 train_time:74989ms step_avg:60.43ms
step:1242/2270 train_time:75048ms step_avg:60.43ms
step:1243/2270 train_time:75111ms step_avg:60.43ms
step:1244/2270 train_time:75171ms step_avg:60.43ms
step:1245/2270 train_time:75233ms step_avg:60.43ms
step:1246/2270 train_time:75293ms step_avg:60.43ms
step:1247/2270 train_time:75355ms step_avg:60.43ms
step:1248/2270 train_time:75414ms step_avg:60.43ms
step:1249/2270 train_time:75476ms step_avg:60.43ms
step:1250/2270 train_time:75535ms step_avg:60.43ms
step:1250/2270 val_loss:3.5020 train_time:75599ms step_avg:60.48ms
step:1251/2270 train_time:75624ms step_avg:60.45ms
step:1252/2270 train_time:75661ms step_avg:60.43ms
step:1253/2270 train_time:75723ms step_avg:60.43ms
step:1254/2270 train_time:75784ms step_avg:60.43ms
step:1255/2270 train_time:75849ms step_avg:60.44ms
step:1256/2270 train_time:75909ms step_avg:60.44ms
step:1257/2270 train_time:75971ms step_avg:60.44ms
step:1258/2270 train_time:76030ms step_avg:60.44ms
step:1259/2270 train_time:76092ms step_avg:60.44ms
step:1260/2270 train_time:76151ms step_avg:60.44ms
step:1261/2270 train_time:76213ms step_avg:60.44ms
step:1262/2270 train_time:76271ms step_avg:60.44ms
step:1263/2270 train_time:76332ms step_avg:60.44ms
step:1264/2270 train_time:76391ms step_avg:60.44ms
step:1265/2270 train_time:76453ms step_avg:60.44ms
step:1266/2270 train_time:76515ms step_avg:60.44ms
step:1267/2270 train_time:76579ms step_avg:60.44ms
step:1268/2270 train_time:76640ms step_avg:60.44ms
step:1269/2270 train_time:76702ms step_avg:60.44ms
step:1270/2270 train_time:76761ms step_avg:60.44ms
step:1271/2270 train_time:76823ms step_avg:60.44ms
step:1272/2270 train_time:76884ms step_avg:60.44ms
step:1273/2270 train_time:76945ms step_avg:60.44ms
step:1274/2270 train_time:77005ms step_avg:60.44ms
step:1275/2270 train_time:77067ms step_avg:60.44ms
step:1276/2270 train_time:77126ms step_avg:60.44ms
step:1277/2270 train_time:77189ms step_avg:60.45ms
step:1278/2270 train_time:77249ms step_avg:60.45ms
step:1279/2270 train_time:77310ms step_avg:60.45ms
step:1280/2270 train_time:77370ms step_avg:60.45ms
step:1281/2270 train_time:77433ms step_avg:60.45ms
step:1282/2270 train_time:77494ms step_avg:60.45ms
step:1283/2270 train_time:77557ms step_avg:60.45ms
step:1284/2270 train_time:77616ms step_avg:60.45ms
step:1285/2270 train_time:77679ms step_avg:60.45ms
step:1286/2270 train_time:77739ms step_avg:60.45ms
step:1287/2270 train_time:77801ms step_avg:60.45ms
step:1288/2270 train_time:77861ms step_avg:60.45ms
step:1289/2270 train_time:77923ms step_avg:60.45ms
step:1290/2270 train_time:77983ms step_avg:60.45ms
step:1291/2270 train_time:78045ms step_avg:60.45ms
step:1292/2270 train_time:78104ms step_avg:60.45ms
step:1293/2270 train_time:78166ms step_avg:60.45ms
step:1294/2270 train_time:78226ms step_avg:60.45ms
step:1295/2270 train_time:78288ms step_avg:60.45ms
step:1296/2270 train_time:78348ms step_avg:60.45ms
step:1297/2270 train_time:78410ms step_avg:60.46ms
step:1298/2270 train_time:78471ms step_avg:60.46ms
step:1299/2270 train_time:78535ms step_avg:60.46ms
step:1300/2270 train_time:78594ms step_avg:60.46ms
step:1301/2270 train_time:78657ms step_avg:60.46ms
step:1302/2270 train_time:78716ms step_avg:60.46ms
step:1303/2270 train_time:78778ms step_avg:60.46ms
step:1304/2270 train_time:78838ms step_avg:60.46ms
step:1305/2270 train_time:78900ms step_avg:60.46ms
step:1306/2270 train_time:78960ms step_avg:60.46ms
step:1307/2270 train_time:79021ms step_avg:60.46ms
step:1308/2270 train_time:79081ms step_avg:60.46ms
step:1309/2270 train_time:79143ms step_avg:60.46ms
step:1310/2270 train_time:79203ms step_avg:60.46ms
step:1311/2270 train_time:79266ms step_avg:60.46ms
step:1312/2270 train_time:79326ms step_avg:60.46ms
step:1313/2270 train_time:79389ms step_avg:60.46ms
step:1314/2270 train_time:79449ms step_avg:60.46ms
step:1315/2270 train_time:79511ms step_avg:60.46ms
step:1316/2270 train_time:79572ms step_avg:60.46ms
step:1317/2270 train_time:79635ms step_avg:60.47ms
step:1318/2270 train_time:79694ms step_avg:60.47ms
step:1319/2270 train_time:79756ms step_avg:60.47ms
step:1320/2270 train_time:79815ms step_avg:60.47ms
step:1321/2270 train_time:79877ms step_avg:60.47ms
step:1322/2270 train_time:79936ms step_avg:60.47ms
step:1323/2270 train_time:79998ms step_avg:60.47ms
step:1324/2270 train_time:80056ms step_avg:60.47ms
step:1325/2270 train_time:80118ms step_avg:60.47ms
step:1326/2270 train_time:80178ms step_avg:60.47ms
step:1327/2270 train_time:80240ms step_avg:60.47ms
step:1328/2270 train_time:80301ms step_avg:60.47ms
step:1329/2270 train_time:80364ms step_avg:60.47ms
step:1330/2270 train_time:80425ms step_avg:60.47ms
step:1331/2270 train_time:80488ms step_avg:60.47ms
step:1332/2270 train_time:80548ms step_avg:60.47ms
step:1333/2270 train_time:80611ms step_avg:60.47ms
step:1334/2270 train_time:80672ms step_avg:60.47ms
step:1335/2270 train_time:80734ms step_avg:60.48ms
step:1336/2270 train_time:80794ms step_avg:60.47ms
step:1337/2270 train_time:80856ms step_avg:60.48ms
step:1338/2270 train_time:80915ms step_avg:60.47ms
step:1339/2270 train_time:80977ms step_avg:60.48ms
step:1340/2270 train_time:81036ms step_avg:60.47ms
step:1341/2270 train_time:81098ms step_avg:60.48ms
step:1342/2270 train_time:81157ms step_avg:60.47ms
step:1343/2270 train_time:81218ms step_avg:60.48ms
step:1344/2270 train_time:81278ms step_avg:60.47ms
step:1345/2270 train_time:81341ms step_avg:60.48ms
step:1346/2270 train_time:81402ms step_avg:60.48ms
step:1347/2270 train_time:81465ms step_avg:60.48ms
step:1348/2270 train_time:81525ms step_avg:60.48ms
step:1349/2270 train_time:81588ms step_avg:60.48ms
step:1350/2270 train_time:81648ms step_avg:60.48ms
step:1351/2270 train_time:81711ms step_avg:60.48ms
step:1352/2270 train_time:81770ms step_avg:60.48ms
step:1353/2270 train_time:81833ms step_avg:60.48ms
step:1354/2270 train_time:81893ms step_avg:60.48ms
step:1355/2270 train_time:81954ms step_avg:60.48ms
step:1356/2270 train_time:82013ms step_avg:60.48ms
step:1357/2270 train_time:82076ms step_avg:60.48ms
step:1358/2270 train_time:82135ms step_avg:60.48ms
step:1359/2270 train_time:82197ms step_avg:60.48ms
step:1360/2270 train_time:82256ms step_avg:60.48ms
step:1361/2270 train_time:82319ms step_avg:60.48ms
step:1362/2270 train_time:82378ms step_avg:60.48ms
step:1363/2270 train_time:82440ms step_avg:60.48ms
step:1364/2270 train_time:82500ms step_avg:60.48ms
step:1365/2270 train_time:82563ms step_avg:60.49ms
step:1366/2270 train_time:82624ms step_avg:60.49ms
step:1367/2270 train_time:82686ms step_avg:60.49ms
step:1368/2270 train_time:82747ms step_avg:60.49ms
step:1369/2270 train_time:82810ms step_avg:60.49ms
step:1370/2270 train_time:82869ms step_avg:60.49ms
step:1371/2270 train_time:82932ms step_avg:60.49ms
step:1372/2270 train_time:82992ms step_avg:60.49ms
step:1373/2270 train_time:83054ms step_avg:60.49ms
step:1374/2270 train_time:83113ms step_avg:60.49ms
step:1375/2270 train_time:83175ms step_avg:60.49ms
step:1376/2270 train_time:83235ms step_avg:60.49ms
step:1377/2270 train_time:83296ms step_avg:60.49ms
step:1378/2270 train_time:83355ms step_avg:60.49ms
step:1379/2270 train_time:83417ms step_avg:60.49ms
step:1380/2270 train_time:83476ms step_avg:60.49ms
step:1381/2270 train_time:83538ms step_avg:60.49ms
step:1382/2270 train_time:83599ms step_avg:60.49ms
step:1383/2270 train_time:83661ms step_avg:60.49ms
step:1384/2270 train_time:83721ms step_avg:60.49ms
step:1385/2270 train_time:83784ms step_avg:60.49ms
step:1386/2270 train_time:83844ms step_avg:60.49ms
step:1387/2270 train_time:83907ms step_avg:60.50ms
step:1388/2270 train_time:83967ms step_avg:60.49ms
step:1389/2270 train_time:84030ms step_avg:60.50ms
step:1390/2270 train_time:84090ms step_avg:60.50ms
step:1391/2270 train_time:84153ms step_avg:60.50ms
step:1392/2270 train_time:84212ms step_avg:60.50ms
step:1393/2270 train_time:84274ms step_avg:60.50ms
step:1394/2270 train_time:84333ms step_avg:60.50ms
step:1395/2270 train_time:84395ms step_avg:60.50ms
step:1396/2270 train_time:84454ms step_avg:60.50ms
step:1397/2270 train_time:84516ms step_avg:60.50ms
step:1398/2270 train_time:84575ms step_avg:60.50ms
step:1399/2270 train_time:84636ms step_avg:60.50ms
step:1400/2270 train_time:84696ms step_avg:60.50ms
step:1401/2270 train_time:84758ms step_avg:60.50ms
step:1402/2270 train_time:84818ms step_avg:60.50ms
step:1403/2270 train_time:84881ms step_avg:60.50ms
step:1404/2270 train_time:84941ms step_avg:60.50ms
step:1405/2270 train_time:85004ms step_avg:60.50ms
step:1406/2270 train_time:85065ms step_avg:60.50ms
step:1407/2270 train_time:85127ms step_avg:60.50ms
step:1408/2270 train_time:85187ms step_avg:60.50ms
step:1409/2270 train_time:85250ms step_avg:60.50ms
step:1410/2270 train_time:85310ms step_avg:60.50ms
step:1411/2270 train_time:85372ms step_avg:60.50ms
step:1412/2270 train_time:85432ms step_avg:60.50ms
step:1413/2270 train_time:85495ms step_avg:60.51ms
step:1414/2270 train_time:85554ms step_avg:60.50ms
step:1415/2270 train_time:85616ms step_avg:60.51ms
step:1416/2270 train_time:85675ms step_avg:60.50ms
step:1417/2270 train_time:85737ms step_avg:60.51ms
step:1418/2270 train_time:85796ms step_avg:60.51ms
step:1419/2270 train_time:85858ms step_avg:60.51ms
step:1420/2270 train_time:85917ms step_avg:60.51ms
step:1421/2270 train_time:85980ms step_avg:60.51ms
step:1422/2270 train_time:86040ms step_avg:60.51ms
step:1423/2270 train_time:86103ms step_avg:60.51ms
step:1424/2270 train_time:86163ms step_avg:60.51ms
step:1425/2270 train_time:86226ms step_avg:60.51ms
step:1426/2270 train_time:86286ms step_avg:60.51ms
step:1427/2270 train_time:86348ms step_avg:60.51ms
step:1428/2270 train_time:86408ms step_avg:60.51ms
step:1429/2270 train_time:86471ms step_avg:60.51ms
step:1430/2270 train_time:86531ms step_avg:60.51ms
step:1431/2270 train_time:86594ms step_avg:60.51ms
step:1432/2270 train_time:86653ms step_avg:60.51ms
step:1433/2270 train_time:86715ms step_avg:60.51ms
step:1434/2270 train_time:86775ms step_avg:60.51ms
step:1435/2270 train_time:86836ms step_avg:60.51ms
step:1436/2270 train_time:86896ms step_avg:60.51ms
step:1437/2270 train_time:86957ms step_avg:60.51ms
step:1438/2270 train_time:87017ms step_avg:60.51ms
step:1439/2270 train_time:87079ms step_avg:60.51ms
step:1440/2270 train_time:87139ms step_avg:60.51ms
step:1441/2270 train_time:87201ms step_avg:60.51ms
step:1442/2270 train_time:87261ms step_avg:60.51ms
step:1443/2270 train_time:87324ms step_avg:60.52ms
step:1444/2270 train_time:87386ms step_avg:60.52ms
step:1445/2270 train_time:87448ms step_avg:60.52ms
step:1446/2270 train_time:87508ms step_avg:60.52ms
step:1447/2270 train_time:87571ms step_avg:60.52ms
step:1448/2270 train_time:87632ms step_avg:60.52ms
step:1449/2270 train_time:87694ms step_avg:60.52ms
step:1450/2270 train_time:87753ms step_avg:60.52ms
step:1451/2270 train_time:87815ms step_avg:60.52ms
step:1452/2270 train_time:87874ms step_avg:60.52ms
step:1453/2270 train_time:87936ms step_avg:60.52ms
step:1454/2270 train_time:87995ms step_avg:60.52ms
step:1455/2270 train_time:88057ms step_avg:60.52ms
step:1456/2270 train_time:88117ms step_avg:60.52ms
step:1457/2270 train_time:88179ms step_avg:60.52ms
step:1458/2270 train_time:88239ms step_avg:60.52ms
step:1459/2270 train_time:88301ms step_avg:60.52ms
step:1460/2270 train_time:88361ms step_avg:60.52ms
step:1461/2270 train_time:88424ms step_avg:60.52ms
step:1462/2270 train_time:88485ms step_avg:60.52ms
step:1463/2270 train_time:88548ms step_avg:60.53ms
step:1464/2270 train_time:88608ms step_avg:60.52ms
step:1465/2270 train_time:88671ms step_avg:60.53ms
step:1466/2270 train_time:88731ms step_avg:60.53ms
step:1467/2270 train_time:88793ms step_avg:60.53ms
step:1468/2270 train_time:88853ms step_avg:60.53ms
step:1469/2270 train_time:88915ms step_avg:60.53ms
step:1470/2270 train_time:88974ms step_avg:60.53ms
step:1471/2270 train_time:89036ms step_avg:60.53ms
step:1472/2270 train_time:89097ms step_avg:60.53ms
step:1473/2270 train_time:89158ms step_avg:60.53ms
step:1474/2270 train_time:89217ms step_avg:60.53ms
step:1475/2270 train_time:89279ms step_avg:60.53ms
step:1476/2270 train_time:89338ms step_avg:60.53ms
step:1477/2270 train_time:89400ms step_avg:60.53ms
step:1478/2270 train_time:89460ms step_avg:60.53ms
step:1479/2270 train_time:89524ms step_avg:60.53ms
step:1480/2270 train_time:89585ms step_avg:60.53ms
step:1481/2270 train_time:89647ms step_avg:60.53ms
step:1482/2270 train_time:89707ms step_avg:60.53ms
step:1483/2270 train_time:89770ms step_avg:60.53ms
step:1484/2270 train_time:89830ms step_avg:60.53ms
step:1485/2270 train_time:89894ms step_avg:60.53ms
step:1486/2270 train_time:89953ms step_avg:60.53ms
step:1487/2270 train_time:90015ms step_avg:60.53ms
step:1488/2270 train_time:90074ms step_avg:60.53ms
step:1489/2270 train_time:90136ms step_avg:60.53ms
step:1490/2270 train_time:90195ms step_avg:60.53ms
step:1491/2270 train_time:90257ms step_avg:60.53ms
step:1492/2270 train_time:90316ms step_avg:60.53ms
step:1493/2270 train_time:90378ms step_avg:60.53ms
step:1494/2270 train_time:90437ms step_avg:60.53ms
step:1495/2270 train_time:90500ms step_avg:60.54ms
step:1496/2270 train_time:90561ms step_avg:60.54ms
step:1497/2270 train_time:90623ms step_avg:60.54ms
step:1498/2270 train_time:90684ms step_avg:60.54ms
step:1499/2270 train_time:90746ms step_avg:60.54ms
step:1500/2270 train_time:90807ms step_avg:60.54ms
step:1500/2270 val_loss:3.4336 train_time:90871ms step_avg:60.58ms
step:1501/2270 train_time:90889ms step_avg:60.55ms
step:1502/2270 train_time:90934ms step_avg:60.54ms
step:1503/2270 train_time:91001ms step_avg:60.55ms
step:1504/2270 train_time:91062ms step_avg:60.55ms
step:1505/2270 train_time:91124ms step_avg:60.55ms
step:1506/2270 train_time:91184ms step_avg:60.55ms
step:1507/2270 train_time:91245ms step_avg:60.55ms
step:1508/2270 train_time:91304ms step_avg:60.55ms
step:1509/2270 train_time:91366ms step_avg:60.55ms
step:1510/2270 train_time:91425ms step_avg:60.55ms
step:1511/2270 train_time:91486ms step_avg:60.55ms
step:1512/2270 train_time:91545ms step_avg:60.55ms
step:1513/2270 train_time:91607ms step_avg:60.55ms
step:1514/2270 train_time:91667ms step_avg:60.55ms
step:1515/2270 train_time:91728ms step_avg:60.55ms
step:1516/2270 train_time:91788ms step_avg:60.55ms
step:1517/2270 train_time:91852ms step_avg:60.55ms
step:1518/2270 train_time:91913ms step_avg:60.55ms
step:1519/2270 train_time:91978ms step_avg:60.55ms
step:1520/2270 train_time:92038ms step_avg:60.55ms
step:1521/2270 train_time:92102ms step_avg:60.55ms
step:1522/2270 train_time:92163ms step_avg:60.55ms
step:1523/2270 train_time:92225ms step_avg:60.55ms
step:1524/2270 train_time:92284ms step_avg:60.55ms
step:1525/2270 train_time:92347ms step_avg:60.56ms
step:1526/2270 train_time:92406ms step_avg:60.55ms
step:1527/2270 train_time:92468ms step_avg:60.56ms
step:1528/2270 train_time:92528ms step_avg:60.55ms
step:1529/2270 train_time:92589ms step_avg:60.56ms
step:1530/2270 train_time:92649ms step_avg:60.55ms
step:1531/2270 train_time:92711ms step_avg:60.56ms
step:1532/2270 train_time:92770ms step_avg:60.55ms
step:1533/2270 train_time:92833ms step_avg:60.56ms
step:1534/2270 train_time:92893ms step_avg:60.56ms
step:1535/2270 train_time:92956ms step_avg:60.56ms
step:1536/2270 train_time:93017ms step_avg:60.56ms
step:1537/2270 train_time:93080ms step_avg:60.56ms
step:1538/2270 train_time:93141ms step_avg:60.56ms
step:1539/2270 train_time:93204ms step_avg:60.56ms
step:1540/2270 train_time:93264ms step_avg:60.56ms
step:1541/2270 train_time:93327ms step_avg:60.56ms
step:1542/2270 train_time:93386ms step_avg:60.56ms
step:1543/2270 train_time:93449ms step_avg:60.56ms
step:1544/2270 train_time:93508ms step_avg:60.56ms
step:1545/2270 train_time:93571ms step_avg:60.56ms
step:1546/2270 train_time:93630ms step_avg:60.56ms
step:1547/2270 train_time:93692ms step_avg:60.56ms
step:1548/2270 train_time:93751ms step_avg:60.56ms
step:1549/2270 train_time:93813ms step_avg:60.56ms
step:1550/2270 train_time:93874ms step_avg:60.56ms
step:1551/2270 train_time:93937ms step_avg:60.57ms
step:1552/2270 train_time:93997ms step_avg:60.57ms
step:1553/2270 train_time:94060ms step_avg:60.57ms
step:1554/2270 train_time:94121ms step_avg:60.57ms
step:1555/2270 train_time:94184ms step_avg:60.57ms
step:1556/2270 train_time:94244ms step_avg:60.57ms
step:1557/2270 train_time:94307ms step_avg:60.57ms
step:1558/2270 train_time:94367ms step_avg:60.57ms
step:1559/2270 train_time:94429ms step_avg:60.57ms
step:1560/2270 train_time:94489ms step_avg:60.57ms
step:1561/2270 train_time:94551ms step_avg:60.57ms
step:1562/2270 train_time:94610ms step_avg:60.57ms
step:1563/2270 train_time:94673ms step_avg:60.57ms
step:1564/2270 train_time:94732ms step_avg:60.57ms
step:1565/2270 train_time:94795ms step_avg:60.57ms
step:1566/2270 train_time:94854ms step_avg:60.57ms
step:1567/2270 train_time:94917ms step_avg:60.57ms
step:1568/2270 train_time:94978ms step_avg:60.57ms
step:1569/2270 train_time:95041ms step_avg:60.57ms
step:1570/2270 train_time:95101ms step_avg:60.57ms
step:1571/2270 train_time:95164ms step_avg:60.58ms
step:1572/2270 train_time:95225ms step_avg:60.58ms
step:1573/2270 train_time:95288ms step_avg:60.58ms
step:1574/2270 train_time:95348ms step_avg:60.58ms
step:1575/2270 train_time:95410ms step_avg:60.58ms
step:1576/2270 train_time:95470ms step_avg:60.58ms
step:1577/2270 train_time:95532ms step_avg:60.58ms
step:1578/2270 train_time:95592ms step_avg:60.58ms
step:1579/2270 train_time:95654ms step_avg:60.58ms
step:1580/2270 train_time:95714ms step_avg:60.58ms
step:1581/2270 train_time:95776ms step_avg:60.58ms
step:1582/2270 train_time:95836ms step_avg:60.58ms
step:1583/2270 train_time:95899ms step_avg:60.58ms
step:1584/2270 train_time:95960ms step_avg:60.58ms
step:1585/2270 train_time:96023ms step_avg:60.58ms
step:1586/2270 train_time:96082ms step_avg:60.58ms
step:1587/2270 train_time:96145ms step_avg:60.58ms
step:1588/2270 train_time:96205ms step_avg:60.58ms
step:1589/2270 train_time:96268ms step_avg:60.58ms
step:1590/2270 train_time:96328ms step_avg:60.58ms
step:1591/2270 train_time:96391ms step_avg:60.58ms
step:1592/2270 train_time:96450ms step_avg:60.58ms
step:1593/2270 train_time:96512ms step_avg:60.59ms
step:1594/2270 train_time:96572ms step_avg:60.58ms
step:1595/2270 train_time:96635ms step_avg:60.59ms
step:1596/2270 train_time:96695ms step_avg:60.59ms
step:1597/2270 train_time:96757ms step_avg:60.59ms
step:1598/2270 train_time:96817ms step_avg:60.59ms
step:1599/2270 train_time:96880ms step_avg:60.59ms
step:1600/2270 train_time:96940ms step_avg:60.59ms
step:1601/2270 train_time:97003ms step_avg:60.59ms
step:1602/2270 train_time:97063ms step_avg:60.59ms
step:1603/2270 train_time:97126ms step_avg:60.59ms
step:1604/2270 train_time:97186ms step_avg:60.59ms
step:1605/2270 train_time:97249ms step_avg:60.59ms
step:1606/2270 train_time:97308ms step_avg:60.59ms
step:1607/2270 train_time:97370ms step_avg:60.59ms
step:1608/2270 train_time:97430ms step_avg:60.59ms
step:1609/2270 train_time:97492ms step_avg:60.59ms
step:1610/2270 train_time:97552ms step_avg:60.59ms
step:1611/2270 train_time:97614ms step_avg:60.59ms
step:1612/2270 train_time:97674ms step_avg:60.59ms
step:1613/2270 train_time:97736ms step_avg:60.59ms
step:1614/2270 train_time:97795ms step_avg:60.59ms
step:1615/2270 train_time:97858ms step_avg:60.59ms
step:1616/2270 train_time:97918ms step_avg:60.59ms
step:1617/2270 train_time:97981ms step_avg:60.59ms
step:1618/2270 train_time:98041ms step_avg:60.59ms
step:1619/2270 train_time:98104ms step_avg:60.60ms
step:1620/2270 train_time:98165ms step_avg:60.60ms
step:1621/2270 train_time:98228ms step_avg:60.60ms
step:1622/2270 train_time:98287ms step_avg:60.60ms
step:1623/2270 train_time:98350ms step_avg:60.60ms
step:1624/2270 train_time:98410ms step_avg:60.60ms
step:1625/2270 train_time:98472ms step_avg:60.60ms
step:1626/2270 train_time:98532ms step_avg:60.60ms
step:1627/2270 train_time:98595ms step_avg:60.60ms
step:1628/2270 train_time:98655ms step_avg:60.60ms
step:1629/2270 train_time:98718ms step_avg:60.60ms
step:1630/2270 train_time:98778ms step_avg:60.60ms
step:1631/2270 train_time:98840ms step_avg:60.60ms
step:1632/2270 train_time:98901ms step_avg:60.60ms
step:1633/2270 train_time:98964ms step_avg:60.60ms
step:1634/2270 train_time:99024ms step_avg:60.60ms
step:1635/2270 train_time:99087ms step_avg:60.60ms
step:1636/2270 train_time:99147ms step_avg:60.60ms
step:1637/2270 train_time:99209ms step_avg:60.60ms
step:1638/2270 train_time:99269ms step_avg:60.60ms
step:1639/2270 train_time:99332ms step_avg:60.60ms
step:1640/2270 train_time:99392ms step_avg:60.60ms
step:1641/2270 train_time:99454ms step_avg:60.61ms
step:1642/2270 train_time:99514ms step_avg:60.61ms
step:1643/2270 train_time:99576ms step_avg:60.61ms
step:1644/2270 train_time:99636ms step_avg:60.61ms
step:1645/2270 train_time:99699ms step_avg:60.61ms
step:1646/2270 train_time:99760ms step_avg:60.61ms
step:1647/2270 train_time:99822ms step_avg:60.61ms
step:1648/2270 train_time:99883ms step_avg:60.61ms
step:1649/2270 train_time:99945ms step_avg:60.61ms
step:1650/2270 train_time:100005ms step_avg:60.61ms
step:1651/2270 train_time:100068ms step_avg:60.61ms
step:1652/2270 train_time:100128ms step_avg:60.61ms
step:1653/2270 train_time:100190ms step_avg:60.61ms
step:1654/2270 train_time:100250ms step_avg:60.61ms
step:1655/2270 train_time:100312ms step_avg:60.61ms
step:1656/2270 train_time:100373ms step_avg:60.61ms
step:1657/2270 train_time:100435ms step_avg:60.61ms
step:1658/2270 train_time:100496ms step_avg:60.61ms
step:1659/2270 train_time:100558ms step_avg:60.61ms
step:1660/2270 train_time:100618ms step_avg:60.61ms
step:1661/2270 train_time:100681ms step_avg:60.61ms
step:1662/2270 train_time:100741ms step_avg:60.61ms
step:1663/2270 train_time:100804ms step_avg:60.62ms
step:1664/2270 train_time:100865ms step_avg:60.62ms
step:1665/2270 train_time:100927ms step_avg:60.62ms
step:1666/2270 train_time:100987ms step_avg:60.62ms
step:1667/2270 train_time:101049ms step_avg:60.62ms
step:1668/2270 train_time:101109ms step_avg:60.62ms
step:1669/2270 train_time:101172ms step_avg:60.62ms
step:1670/2270 train_time:101232ms step_avg:60.62ms
step:1671/2270 train_time:101294ms step_avg:60.62ms
step:1672/2270 train_time:101354ms step_avg:60.62ms
step:1673/2270 train_time:101417ms step_avg:60.62ms
step:1674/2270 train_time:101477ms step_avg:60.62ms
step:1675/2270 train_time:101540ms step_avg:60.62ms
step:1676/2270 train_time:101600ms step_avg:60.62ms
step:1677/2270 train_time:101664ms step_avg:60.62ms
step:1678/2270 train_time:101724ms step_avg:60.62ms
step:1679/2270 train_time:101787ms step_avg:60.62ms
step:1680/2270 train_time:101846ms step_avg:60.62ms
step:1681/2270 train_time:101908ms step_avg:60.62ms
step:1682/2270 train_time:101968ms step_avg:60.62ms
step:1683/2270 train_time:102031ms step_avg:60.62ms
step:1684/2270 train_time:102091ms step_avg:60.62ms
step:1685/2270 train_time:102153ms step_avg:60.63ms
step:1686/2270 train_time:102213ms step_avg:60.62ms
step:1687/2270 train_time:102276ms step_avg:60.63ms
step:1688/2270 train_time:102336ms step_avg:60.63ms
step:1689/2270 train_time:102398ms step_avg:60.63ms
step:1690/2270 train_time:102458ms step_avg:60.63ms
step:1691/2270 train_time:102521ms step_avg:60.63ms
step:1692/2270 train_time:102581ms step_avg:60.63ms
step:1693/2270 train_time:102644ms step_avg:60.63ms
step:1694/2270 train_time:102703ms step_avg:60.63ms
step:1695/2270 train_time:102766ms step_avg:60.63ms
step:1696/2270 train_time:102826ms step_avg:60.63ms
step:1697/2270 train_time:102889ms step_avg:60.63ms
step:1698/2270 train_time:102948ms step_avg:60.63ms
step:1699/2270 train_time:103011ms step_avg:60.63ms
step:1700/2270 train_time:103072ms step_avg:60.63ms
step:1701/2270 train_time:103134ms step_avg:60.63ms
step:1702/2270 train_time:103194ms step_avg:60.63ms
step:1703/2270 train_time:103257ms step_avg:60.63ms
step:1704/2270 train_time:103317ms step_avg:60.63ms
step:1705/2270 train_time:103379ms step_avg:60.63ms
step:1706/2270 train_time:103439ms step_avg:60.63ms
step:1707/2270 train_time:103502ms step_avg:60.63ms
step:1708/2270 train_time:103562ms step_avg:60.63ms
step:1709/2270 train_time:103625ms step_avg:60.63ms
step:1710/2270 train_time:103685ms step_avg:60.63ms
step:1711/2270 train_time:103747ms step_avg:60.64ms
step:1712/2270 train_time:103807ms step_avg:60.63ms
step:1713/2270 train_time:103869ms step_avg:60.64ms
step:1714/2270 train_time:103929ms step_avg:60.64ms
step:1715/2270 train_time:103992ms step_avg:60.64ms
step:1716/2270 train_time:104052ms step_avg:60.64ms
step:1717/2270 train_time:104114ms step_avg:60.64ms
step:1718/2270 train_time:104174ms step_avg:60.64ms
step:1719/2270 train_time:104237ms step_avg:60.64ms
step:1720/2270 train_time:104297ms step_avg:60.64ms
step:1721/2270 train_time:104359ms step_avg:60.64ms
step:1722/2270 train_time:104420ms step_avg:60.64ms
step:1723/2270 train_time:104482ms step_avg:60.64ms
step:1724/2270 train_time:104543ms step_avg:60.64ms
step:1725/2270 train_time:104606ms step_avg:60.64ms
step:1726/2270 train_time:104666ms step_avg:60.64ms
step:1727/2270 train_time:104729ms step_avg:60.64ms
step:1728/2270 train_time:104788ms step_avg:60.64ms
step:1729/2270 train_time:104851ms step_avg:60.64ms
step:1730/2270 train_time:104911ms step_avg:60.64ms
step:1731/2270 train_time:104973ms step_avg:60.64ms
step:1732/2270 train_time:105033ms step_avg:60.64ms
step:1733/2270 train_time:105096ms step_avg:60.64ms
step:1734/2270 train_time:105156ms step_avg:60.64ms
step:1735/2270 train_time:105218ms step_avg:60.64ms
step:1736/2270 train_time:105278ms step_avg:60.64ms
step:1737/2270 train_time:105341ms step_avg:60.65ms
step:1738/2270 train_time:105400ms step_avg:60.64ms
step:1739/2270 train_time:105463ms step_avg:60.65ms
step:1740/2270 train_time:105522ms step_avg:60.64ms
step:1741/2270 train_time:105585ms step_avg:60.65ms
step:1742/2270 train_time:105645ms step_avg:60.65ms
step:1743/2270 train_time:105708ms step_avg:60.65ms
step:1744/2270 train_time:105768ms step_avg:60.65ms
step:1745/2270 train_time:105830ms step_avg:60.65ms
step:1746/2270 train_time:105890ms step_avg:60.65ms
step:1747/2270 train_time:105953ms step_avg:60.65ms
step:1748/2270 train_time:106013ms step_avg:60.65ms
step:1749/2270 train_time:106075ms step_avg:60.65ms
step:1750/2270 train_time:106135ms step_avg:60.65ms
step:1750/2270 val_loss:3.3709 train_time:106199ms step_avg:60.69ms
step:1751/2270 train_time:106218ms step_avg:60.66ms
step:1752/2270 train_time:106264ms step_avg:60.65ms
step:1753/2270 train_time:106327ms step_avg:60.65ms
step:1754/2270 train_time:106388ms step_avg:60.65ms
step:1755/2270 train_time:106451ms step_avg:60.66ms
step:1756/2270 train_time:106511ms step_avg:60.66ms
step:1757/2270 train_time:106572ms step_avg:60.66ms
step:1758/2270 train_time:106632ms step_avg:60.66ms
step:1759/2270 train_time:106693ms step_avg:60.66ms
step:1760/2270 train_time:106752ms step_avg:60.65ms
step:1761/2270 train_time:106813ms step_avg:60.65ms
step:1762/2270 train_time:106872ms step_avg:60.65ms
step:1763/2270 train_time:106934ms step_avg:60.65ms
step:1764/2270 train_time:106993ms step_avg:60.65ms
step:1765/2270 train_time:107055ms step_avg:60.65ms
step:1766/2270 train_time:107114ms step_avg:60.65ms
step:1767/2270 train_time:107179ms step_avg:60.66ms
step:1768/2270 train_time:107241ms step_avg:60.66ms
step:1769/2270 train_time:107305ms step_avg:60.66ms
step:1770/2270 train_time:107365ms step_avg:60.66ms
step:1771/2270 train_time:107429ms step_avg:60.66ms
step:1772/2270 train_time:107489ms step_avg:60.66ms
step:1773/2270 train_time:107551ms step_avg:60.66ms
step:1774/2270 train_time:107610ms step_avg:60.66ms
step:1775/2270 train_time:107672ms step_avg:60.66ms
step:1776/2270 train_time:107731ms step_avg:60.66ms
step:1777/2270 train_time:107793ms step_avg:60.66ms
step:1778/2270 train_time:107852ms step_avg:60.66ms
step:1779/2270 train_time:107914ms step_avg:60.66ms
step:1780/2270 train_time:107973ms step_avg:60.66ms
step:1781/2270 train_time:108035ms step_avg:60.66ms
step:1782/2270 train_time:108095ms step_avg:60.66ms
step:1783/2270 train_time:108158ms step_avg:60.66ms
step:1784/2270 train_time:108219ms step_avg:60.66ms
step:1785/2270 train_time:108283ms step_avg:60.66ms
step:1786/2270 train_time:108344ms step_avg:60.66ms
step:1787/2270 train_time:108407ms step_avg:60.66ms
step:1788/2270 train_time:108467ms step_avg:60.66ms
step:1789/2270 train_time:108530ms step_avg:60.67ms
step:1790/2270 train_time:108590ms step_avg:60.66ms
step:1791/2270 train_time:108652ms step_avg:60.67ms
step:1792/2270 train_time:108712ms step_avg:60.66ms
step:1793/2270 train_time:108773ms step_avg:60.67ms
step:1794/2270 train_time:108833ms step_avg:60.66ms
step:1795/2270 train_time:108895ms step_avg:60.67ms
step:1796/2270 train_time:108954ms step_avg:60.66ms
step:1797/2270 train_time:109016ms step_avg:60.67ms
step:1798/2270 train_time:109075ms step_avg:60.66ms
step:1799/2270 train_time:109137ms step_avg:60.67ms
step:1800/2270 train_time:109198ms step_avg:60.67ms
step:1801/2270 train_time:109260ms step_avg:60.67ms
step:1802/2270 train_time:109320ms step_avg:60.67ms
step:1803/2270 train_time:109384ms step_avg:60.67ms
step:1804/2270 train_time:109445ms step_avg:60.67ms
step:1805/2270 train_time:109508ms step_avg:60.67ms
step:1806/2270 train_time:109568ms step_avg:60.67ms
step:1807/2270 train_time:109630ms step_avg:60.67ms
step:1808/2270 train_time:109690ms step_avg:60.67ms
step:1809/2270 train_time:109752ms step_avg:60.67ms
step:1810/2270 train_time:109811ms step_avg:60.67ms
step:1811/2270 train_time:109873ms step_avg:60.67ms
step:1812/2270 train_time:109933ms step_avg:60.67ms
step:1813/2270 train_time:109995ms step_avg:60.67ms
step:1814/2270 train_time:110054ms step_avg:60.67ms
step:1815/2270 train_time:110116ms step_avg:60.67ms
step:1816/2270 train_time:110176ms step_avg:60.67ms
step:1817/2270 train_time:110239ms step_avg:60.67ms
step:1818/2270 train_time:110300ms step_avg:60.67ms
step:1819/2270 train_time:110364ms step_avg:60.67ms
step:1820/2270 train_time:110424ms step_avg:60.67ms
step:1821/2270 train_time:110487ms step_avg:60.67ms
step:1822/2270 train_time:110547ms step_avg:60.67ms
step:1823/2270 train_time:110609ms step_avg:60.67ms
step:1824/2270 train_time:110669ms step_avg:60.67ms
step:1825/2270 train_time:110731ms step_avg:60.67ms
step:1826/2270 train_time:110791ms step_avg:60.67ms
step:1827/2270 train_time:110853ms step_avg:60.67ms
step:1828/2270 train_time:110913ms step_avg:60.67ms
step:1829/2270 train_time:110975ms step_avg:60.68ms
step:1830/2270 train_time:111034ms step_avg:60.67ms
step:1831/2270 train_time:111096ms step_avg:60.67ms
step:1832/2270 train_time:111156ms step_avg:60.67ms
step:1833/2270 train_time:111218ms step_avg:60.68ms
step:1834/2270 train_time:111279ms step_avg:60.68ms
step:1835/2270 train_time:111342ms step_avg:60.68ms
step:1836/2270 train_time:111403ms step_avg:60.68ms
step:1837/2270 train_time:111465ms step_avg:60.68ms
step:1838/2270 train_time:111525ms step_avg:60.68ms
step:1839/2270 train_time:111589ms step_avg:60.68ms
step:1840/2270 train_time:111649ms step_avg:60.68ms
step:1841/2270 train_time:111712ms step_avg:60.68ms
step:1842/2270 train_time:111771ms step_avg:60.68ms
step:1843/2270 train_time:111834ms step_avg:60.68ms
step:1844/2270 train_time:111893ms step_avg:60.68ms
step:1845/2270 train_time:111955ms step_avg:60.68ms
step:1846/2270 train_time:112015ms step_avg:60.68ms
step:1847/2270 train_time:112077ms step_avg:60.68ms
step:1848/2270 train_time:112136ms step_avg:60.68ms
step:1849/2270 train_time:112198ms step_avg:60.68ms
step:1850/2270 train_time:112259ms step_avg:60.68ms
step:1851/2270 train_time:112321ms step_avg:60.68ms
step:1852/2270 train_time:112382ms step_avg:60.68ms
step:1853/2270 train_time:112446ms step_avg:60.68ms
step:1854/2270 train_time:112506ms step_avg:60.68ms
step:1855/2270 train_time:112568ms step_avg:60.68ms
step:1856/2270 train_time:112629ms step_avg:60.68ms
step:1857/2270 train_time:112692ms step_avg:60.68ms
step:1858/2270 train_time:112751ms step_avg:60.68ms
step:1859/2270 train_time:112813ms step_avg:60.68ms
step:1860/2270 train_time:112873ms step_avg:60.68ms
step:1861/2270 train_time:112935ms step_avg:60.69ms
step:1862/2270 train_time:112995ms step_avg:60.68ms
step:1863/2270 train_time:113057ms step_avg:60.69ms
step:1864/2270 train_time:113116ms step_avg:60.68ms
step:1865/2270 train_time:113179ms step_avg:60.69ms
step:1866/2270 train_time:113239ms step_avg:60.69ms
step:1867/2270 train_time:113301ms step_avg:60.69ms
step:1868/2270 train_time:113362ms step_avg:60.69ms
step:1869/2270 train_time:113425ms step_avg:60.69ms
step:1870/2270 train_time:113485ms step_avg:60.69ms
step:1871/2270 train_time:113548ms step_avg:60.69ms
step:1872/2270 train_time:113608ms step_avg:60.69ms
step:1873/2270 train_time:113671ms step_avg:60.69ms
step:1874/2270 train_time:113730ms step_avg:60.69ms
step:1875/2270 train_time:113793ms step_avg:60.69ms
step:1876/2270 train_time:113852ms step_avg:60.69ms
step:1877/2270 train_time:113915ms step_avg:60.69ms
step:1878/2270 train_time:113975ms step_avg:60.69ms
step:1879/2270 train_time:114037ms step_avg:60.69ms
step:1880/2270 train_time:114097ms step_avg:60.69ms
step:1881/2270 train_time:114159ms step_avg:60.69ms
step:1882/2270 train_time:114218ms step_avg:60.69ms
step:1883/2270 train_time:114281ms step_avg:60.69ms
step:1884/2270 train_time:114342ms step_avg:60.69ms
step:1885/2270 train_time:114405ms step_avg:60.69ms
step:1886/2270 train_time:114465ms step_avg:60.69ms
step:1887/2270 train_time:114529ms step_avg:60.69ms
step:1888/2270 train_time:114589ms step_avg:60.69ms
step:1889/2270 train_time:114652ms step_avg:60.69ms
step:1890/2270 train_time:114712ms step_avg:60.69ms
step:1891/2270 train_time:114774ms step_avg:60.69ms
step:1892/2270 train_time:114834ms step_avg:60.69ms
step:1893/2270 train_time:114897ms step_avg:60.70ms
step:1894/2270 train_time:114957ms step_avg:60.70ms
step:1895/2270 train_time:115019ms step_avg:60.70ms
step:1896/2270 train_time:115079ms step_avg:60.70ms
step:1897/2270 train_time:115141ms step_avg:60.70ms
step:1898/2270 train_time:115201ms step_avg:60.70ms
step:1899/2270 train_time:115264ms step_avg:60.70ms
step:1900/2270 train_time:115324ms step_avg:60.70ms
step:1901/2270 train_time:115387ms step_avg:60.70ms
step:1902/2270 train_time:115448ms step_avg:60.70ms
step:1903/2270 train_time:115511ms step_avg:60.70ms
step:1904/2270 train_time:115571ms step_avg:60.70ms
step:1905/2270 train_time:115634ms step_avg:60.70ms
step:1906/2270 train_time:115694ms step_avg:60.70ms
step:1907/2270 train_time:115757ms step_avg:60.70ms
step:1908/2270 train_time:115816ms step_avg:60.70ms
step:1909/2270 train_time:115879ms step_avg:60.70ms
step:1910/2270 train_time:115940ms step_avg:60.70ms
step:1911/2270 train_time:116003ms step_avg:60.70ms
step:1912/2270 train_time:116063ms step_avg:60.70ms
step:1913/2270 train_time:116125ms step_avg:60.70ms
step:1914/2270 train_time:116186ms step_avg:60.70ms
step:1915/2270 train_time:116248ms step_avg:60.70ms
step:1916/2270 train_time:116307ms step_avg:60.70ms
step:1917/2270 train_time:116370ms step_avg:60.70ms
step:1918/2270 train_time:116430ms step_avg:60.70ms
step:1919/2270 train_time:116492ms step_avg:60.70ms
step:1920/2270 train_time:116552ms step_avg:60.70ms
step:1921/2270 train_time:116615ms step_avg:60.71ms
step:1922/2270 train_time:116675ms step_avg:60.70ms
step:1923/2270 train_time:116737ms step_avg:60.71ms
step:1924/2270 train_time:116798ms step_avg:60.71ms
step:1925/2270 train_time:116860ms step_avg:60.71ms
step:1926/2270 train_time:116920ms step_avg:60.71ms
step:1927/2270 train_time:116983ms step_avg:60.71ms
step:1928/2270 train_time:117043ms step_avg:60.71ms
step:1929/2270 train_time:117106ms step_avg:60.71ms
step:1930/2270 train_time:117166ms step_avg:60.71ms
step:1931/2270 train_time:117228ms step_avg:60.71ms
step:1932/2270 train_time:117289ms step_avg:60.71ms
step:1933/2270 train_time:117351ms step_avg:60.71ms
step:1934/2270 train_time:117411ms step_avg:60.71ms
step:1935/2270 train_time:117474ms step_avg:60.71ms
step:1936/2270 train_time:117534ms step_avg:60.71ms
step:1937/2270 train_time:117596ms step_avg:60.71ms
step:1938/2270 train_time:117656ms step_avg:60.71ms
step:1939/2270 train_time:117718ms step_avg:60.71ms
step:1940/2270 train_time:117779ms step_avg:60.71ms
step:1941/2270 train_time:117842ms step_avg:60.71ms
step:1942/2270 train_time:117902ms step_avg:60.71ms
step:1943/2270 train_time:117965ms step_avg:60.71ms
step:1944/2270 train_time:118025ms step_avg:60.71ms
step:1945/2270 train_time:118087ms step_avg:60.71ms
step:1946/2270 train_time:118148ms step_avg:60.71ms
step:1947/2270 train_time:118211ms step_avg:60.71ms
step:1948/2270 train_time:118271ms step_avg:60.71ms
step:1949/2270 train_time:118333ms step_avg:60.71ms
step:1950/2270 train_time:118393ms step_avg:60.71ms
step:1951/2270 train_time:118455ms step_avg:60.71ms
step:1952/2270 train_time:118515ms step_avg:60.71ms
step:1953/2270 train_time:118577ms step_avg:60.72ms
step:1954/2270 train_time:118637ms step_avg:60.71ms
step:1955/2270 train_time:118699ms step_avg:60.72ms
step:1956/2270 train_time:118760ms step_avg:60.72ms
step:1957/2270 train_time:118823ms step_avg:60.72ms
step:1958/2270 train_time:118883ms step_avg:60.72ms
step:1959/2270 train_time:118946ms step_avg:60.72ms
step:1960/2270 train_time:119006ms step_avg:60.72ms
step:1961/2270 train_time:119069ms step_avg:60.72ms
step:1962/2270 train_time:119128ms step_avg:60.72ms
step:1963/2270 train_time:119191ms step_avg:60.72ms
step:1964/2270 train_time:119251ms step_avg:60.72ms
step:1965/2270 train_time:119313ms step_avg:60.72ms
step:1966/2270 train_time:119374ms step_avg:60.72ms
step:1967/2270 train_time:119436ms step_avg:60.72ms
step:1968/2270 train_time:119496ms step_avg:60.72ms
step:1969/2270 train_time:119558ms step_avg:60.72ms
step:1970/2270 train_time:119618ms step_avg:60.72ms
step:1971/2270 train_time:119681ms step_avg:60.72ms
step:1972/2270 train_time:119741ms step_avg:60.72ms
step:1973/2270 train_time:119804ms step_avg:60.72ms
step:1974/2270 train_time:119864ms step_avg:60.72ms
step:1975/2270 train_time:119926ms step_avg:60.72ms
step:1976/2270 train_time:119987ms step_avg:60.72ms
step:1977/2270 train_time:120050ms step_avg:60.72ms
step:1978/2270 train_time:120110ms step_avg:60.72ms
step:1979/2270 train_time:120172ms step_avg:60.72ms
step:1980/2270 train_time:120232ms step_avg:60.72ms
step:1981/2270 train_time:120295ms step_avg:60.72ms
step:1982/2270 train_time:120355ms step_avg:60.72ms
step:1983/2270 train_time:120418ms step_avg:60.73ms
step:1984/2270 train_time:120478ms step_avg:60.72ms
step:1985/2270 train_time:120540ms step_avg:60.73ms
step:1986/2270 train_time:120601ms step_avg:60.73ms
step:1987/2270 train_time:120663ms step_avg:60.73ms
step:1988/2270 train_time:120723ms step_avg:60.73ms
step:1989/2270 train_time:120786ms step_avg:60.73ms
step:1990/2270 train_time:120846ms step_avg:60.73ms
step:1991/2270 train_time:120909ms step_avg:60.73ms
step:1992/2270 train_time:120970ms step_avg:60.73ms
step:1993/2270 train_time:121032ms step_avg:60.73ms
step:1994/2270 train_time:121092ms step_avg:60.73ms
step:1995/2270 train_time:121155ms step_avg:60.73ms
step:1996/2270 train_time:121215ms step_avg:60.73ms
step:1997/2270 train_time:121277ms step_avg:60.73ms
step:1998/2270 train_time:121338ms step_avg:60.73ms
step:1999/2270 train_time:121400ms step_avg:60.73ms
step:2000/2270 train_time:121460ms step_avg:60.73ms
step:2000/2270 val_loss:3.3194 train_time:121524ms step_avg:60.76ms
step:2001/2270 train_time:121544ms step_avg:60.74ms
step:2002/2270 train_time:121586ms step_avg:60.73ms
step:2003/2270 train_time:121649ms step_avg:60.73ms
step:2004/2270 train_time:121710ms step_avg:60.73ms
step:2005/2270 train_time:121775ms step_avg:60.74ms
step:2006/2270 train_time:121835ms step_avg:60.74ms
step:2007/2270 train_time:121898ms step_avg:60.74ms
step:2008/2270 train_time:121957ms step_avg:60.74ms
step:2009/2270 train_time:122019ms step_avg:60.74ms
step:2010/2270 train_time:122078ms step_avg:60.74ms
step:2011/2270 train_time:122140ms step_avg:60.74ms
step:2012/2270 train_time:122199ms step_avg:60.74ms
step:2013/2270 train_time:122261ms step_avg:60.74ms
step:2014/2270 train_time:122320ms step_avg:60.74ms
step:2015/2270 train_time:122382ms step_avg:60.74ms
step:2016/2270 train_time:122442ms step_avg:60.74ms
step:2017/2270 train_time:122506ms step_avg:60.74ms
step:2018/2270 train_time:122568ms step_avg:60.74ms
step:2019/2270 train_time:122631ms step_avg:60.74ms
step:2020/2270 train_time:122692ms step_avg:60.74ms
step:2021/2270 train_time:122756ms step_avg:60.74ms
step:2022/2270 train_time:122817ms step_avg:60.74ms
step:2023/2270 train_time:122880ms step_avg:60.74ms
step:2024/2270 train_time:122939ms step_avg:60.74ms
step:2025/2270 train_time:123002ms step_avg:60.74ms
step:2026/2270 train_time:123061ms step_avg:60.74ms
step:2027/2270 train_time:123123ms step_avg:60.74ms
step:2028/2270 train_time:123182ms step_avg:60.74ms
step:2029/2270 train_time:123244ms step_avg:60.74ms
step:2030/2270 train_time:123303ms step_avg:60.74ms
step:2031/2270 train_time:123365ms step_avg:60.74ms
step:2032/2270 train_time:123426ms step_avg:60.74ms
step:2033/2270 train_time:123488ms step_avg:60.74ms
step:2034/2270 train_time:123548ms step_avg:60.74ms
step:2035/2270 train_time:123612ms step_avg:60.74ms
step:2036/2270 train_time:123673ms step_avg:60.74ms
step:2037/2270 train_time:123737ms step_avg:60.74ms
step:2038/2270 train_time:123797ms step_avg:60.74ms
step:2039/2270 train_time:123860ms step_avg:60.75ms
step:2040/2270 train_time:123920ms step_avg:60.75ms
step:2041/2270 train_time:123983ms step_avg:60.75ms
step:2042/2270 train_time:124042ms step_avg:60.75ms
step:2043/2270 train_time:124105ms step_avg:60.75ms
step:2044/2270 train_time:124165ms step_avg:60.75ms
step:2045/2270 train_time:124227ms step_avg:60.75ms
step:2046/2270 train_time:124287ms step_avg:60.75ms
step:2047/2270 train_time:124349ms step_avg:60.75ms
step:2048/2270 train_time:124409ms step_avg:60.75ms
step:2049/2270 train_time:124471ms step_avg:60.75ms
step:2050/2270 train_time:124531ms step_avg:60.75ms
step:2051/2270 train_time:124595ms step_avg:60.75ms
step:2052/2270 train_time:124655ms step_avg:60.75ms
step:2053/2270 train_time:124719ms step_avg:60.75ms
step:2054/2270 train_time:124779ms step_avg:60.75ms
step:2055/2270 train_time:124842ms step_avg:60.75ms
step:2056/2270 train_time:124902ms step_avg:60.75ms
step:2057/2270 train_time:124965ms step_avg:60.75ms
step:2058/2270 train_time:125026ms step_avg:60.75ms
step:2059/2270 train_time:125088ms step_avg:60.75ms
step:2060/2270 train_time:125148ms step_avg:60.75ms
step:2061/2270 train_time:125210ms step_avg:60.75ms
step:2062/2270 train_time:125270ms step_avg:60.75ms
step:2063/2270 train_time:125333ms step_avg:60.75ms
step:2064/2270 train_time:125392ms step_avg:60.75ms
step:2065/2270 train_time:125455ms step_avg:60.75ms
step:2066/2270 train_time:125516ms step_avg:60.75ms
step:2067/2270 train_time:125579ms step_avg:60.75ms
step:2068/2270 train_time:125639ms step_avg:60.75ms
step:2069/2270 train_time:125702ms step_avg:60.75ms
step:2070/2270 train_time:125762ms step_avg:60.75ms
step:2071/2270 train_time:125825ms step_avg:60.76ms
step:2072/2270 train_time:125885ms step_avg:60.76ms
step:2073/2270 train_time:125948ms step_avg:60.76ms
step:2074/2270 train_time:126008ms step_avg:60.76ms
step:2075/2270 train_time:126071ms step_avg:60.76ms
step:2076/2270 train_time:126131ms step_avg:60.76ms
step:2077/2270 train_time:126194ms step_avg:60.76ms
step:2078/2270 train_time:126254ms step_avg:60.76ms
step:2079/2270 train_time:126317ms step_avg:60.76ms
step:2080/2270 train_time:126377ms step_avg:60.76ms
step:2081/2270 train_time:126440ms step_avg:60.76ms
step:2082/2270 train_time:126500ms step_avg:60.76ms
step:2083/2270 train_time:126562ms step_avg:60.76ms
step:2084/2270 train_time:126622ms step_avg:60.76ms
step:2085/2270 train_time:126685ms step_avg:60.76ms
step:2086/2270 train_time:126745ms step_avg:60.76ms
step:2087/2270 train_time:126808ms step_avg:60.76ms
step:2088/2270 train_time:126868ms step_avg:60.76ms
step:2089/2270 train_time:126931ms step_avg:60.76ms
step:2090/2270 train_time:126992ms step_avg:60.76ms
step:2091/2270 train_time:127055ms step_avg:60.76ms
step:2092/2270 train_time:127116ms step_avg:60.76ms
step:2093/2270 train_time:127178ms step_avg:60.76ms
step:2094/2270 train_time:127238ms step_avg:60.76ms
step:2095/2270 train_time:127300ms step_avg:60.76ms
step:2096/2270 train_time:127360ms step_avg:60.76ms
step:2097/2270 train_time:127422ms step_avg:60.76ms
step:2098/2270 train_time:127482ms step_avg:60.76ms
step:2099/2270 train_time:127545ms step_avg:60.76ms
step:2100/2270 train_time:127604ms step_avg:60.76ms
step:2101/2270 train_time:127667ms step_avg:60.76ms
step:2102/2270 train_time:127727ms step_avg:60.76ms
step:2103/2270 train_time:127790ms step_avg:60.77ms
step:2104/2270 train_time:127850ms step_avg:60.77ms
step:2105/2270 train_time:127914ms step_avg:60.77ms
step:2106/2270 train_time:127974ms step_avg:60.77ms
step:2107/2270 train_time:128037ms step_avg:60.77ms
step:2108/2270 train_time:128097ms step_avg:60.77ms
step:2109/2270 train_time:128159ms step_avg:60.77ms
step:2110/2270 train_time:128219ms step_avg:60.77ms
step:2111/2270 train_time:128282ms step_avg:60.77ms
step:2112/2270 train_time:128341ms step_avg:60.77ms
step:2113/2270 train_time:128404ms step_avg:60.77ms
step:2114/2270 train_time:128464ms step_avg:60.77ms
step:2115/2270 train_time:128526ms step_avg:60.77ms
step:2116/2270 train_time:128586ms step_avg:60.77ms
step:2117/2270 train_time:128649ms step_avg:60.77ms
step:2118/2270 train_time:128709ms step_avg:60.77ms
step:2119/2270 train_time:128772ms step_avg:60.77ms
step:2120/2270 train_time:128832ms step_avg:60.77ms
step:2121/2270 train_time:128895ms step_avg:60.77ms
step:2122/2270 train_time:128956ms step_avg:60.77ms
step:2123/2270 train_time:129019ms step_avg:60.77ms
step:2124/2270 train_time:129080ms step_avg:60.77ms
step:2125/2270 train_time:129142ms step_avg:60.77ms
step:2126/2270 train_time:129202ms step_avg:60.77ms
step:2127/2270 train_time:129265ms step_avg:60.77ms
step:2128/2270 train_time:129325ms step_avg:60.77ms
step:2129/2270 train_time:129387ms step_avg:60.77ms
step:2130/2270 train_time:129447ms step_avg:60.77ms
step:2131/2270 train_time:129509ms step_avg:60.77ms
step:2132/2270 train_time:129569ms step_avg:60.77ms
step:2133/2270 train_time:129632ms step_avg:60.77ms
step:2134/2270 train_time:129692ms step_avg:60.77ms
step:2135/2270 train_time:129755ms step_avg:60.78ms
step:2136/2270 train_time:129815ms step_avg:60.77ms
step:2137/2270 train_time:129878ms step_avg:60.78ms
step:2138/2270 train_time:129938ms step_avg:60.78ms
step:2139/2270 train_time:130001ms step_avg:60.78ms
step:2140/2270 train_time:130061ms step_avg:60.78ms
step:2141/2270 train_time:130124ms step_avg:60.78ms
step:2142/2270 train_time:130184ms step_avg:60.78ms
step:2143/2270 train_time:130246ms step_avg:60.78ms
step:2144/2270 train_time:130306ms step_avg:60.78ms
step:2145/2270 train_time:130368ms step_avg:60.78ms
step:2146/2270 train_time:130429ms step_avg:60.78ms
step:2147/2270 train_time:130492ms step_avg:60.78ms
step:2148/2270 train_time:130552ms step_avg:60.78ms
step:2149/2270 train_time:130615ms step_avg:60.78ms
step:2150/2270 train_time:130675ms step_avg:60.78ms
step:2151/2270 train_time:130738ms step_avg:60.78ms
step:2152/2270 train_time:130798ms step_avg:60.78ms
step:2153/2270 train_time:130860ms step_avg:60.78ms
step:2154/2270 train_time:130921ms step_avg:60.78ms
step:2155/2270 train_time:130983ms step_avg:60.78ms
step:2156/2270 train_time:131043ms step_avg:60.78ms
step:2157/2270 train_time:131106ms step_avg:60.78ms
step:2158/2270 train_time:131166ms step_avg:60.78ms
step:2159/2270 train_time:131229ms step_avg:60.78ms
step:2160/2270 train_time:131289ms step_avg:60.78ms
step:2161/2270 train_time:131352ms step_avg:60.78ms
step:2162/2270 train_time:131412ms step_avg:60.78ms
step:2163/2270 train_time:131475ms step_avg:60.78ms
step:2164/2270 train_time:131535ms step_avg:60.78ms
step:2165/2270 train_time:131598ms step_avg:60.78ms
step:2166/2270 train_time:131658ms step_avg:60.78ms
step:2167/2270 train_time:131720ms step_avg:60.78ms
step:2168/2270 train_time:131780ms step_avg:60.78ms
step:2169/2270 train_time:131843ms step_avg:60.78ms
step:2170/2270 train_time:131903ms step_avg:60.78ms
step:2171/2270 train_time:131967ms step_avg:60.79ms
step:2172/2270 train_time:132026ms step_avg:60.79ms
step:2173/2270 train_time:132089ms step_avg:60.79ms
step:2174/2270 train_time:132149ms step_avg:60.79ms
step:2175/2270 train_time:132212ms step_avg:60.79ms
step:2176/2270 train_time:132272ms step_avg:60.79ms
step:2177/2270 train_time:132335ms step_avg:60.79ms
step:2178/2270 train_time:132395ms step_avg:60.79ms
step:2179/2270 train_time:132458ms step_avg:60.79ms
step:2180/2270 train_time:132518ms step_avg:60.79ms
step:2181/2270 train_time:132580ms step_avg:60.79ms
step:2182/2270 train_time:132641ms step_avg:60.79ms
step:2183/2270 train_time:132703ms step_avg:60.79ms
step:2184/2270 train_time:132763ms step_avg:60.79ms
step:2185/2270 train_time:132825ms step_avg:60.79ms
step:2186/2270 train_time:132885ms step_avg:60.79ms
step:2187/2270 train_time:132948ms step_avg:60.79ms
step:2188/2270 train_time:133009ms step_avg:60.79ms
step:2189/2270 train_time:133071ms step_avg:60.79ms
step:2190/2270 train_time:133132ms step_avg:60.79ms
step:2191/2270 train_time:133195ms step_avg:60.79ms
step:2192/2270 train_time:133256ms step_avg:60.79ms
step:2193/2270 train_time:133319ms step_avg:60.79ms
step:2194/2270 train_time:133378ms step_avg:60.79ms
step:2195/2270 train_time:133440ms step_avg:60.79ms
step:2196/2270 train_time:133500ms step_avg:60.79ms
step:2197/2270 train_time:133563ms step_avg:60.79ms
step:2198/2270 train_time:133622ms step_avg:60.79ms
step:2199/2270 train_time:133685ms step_avg:60.79ms
step:2200/2270 train_time:133745ms step_avg:60.79ms
step:2201/2270 train_time:133808ms step_avg:60.79ms
step:2202/2270 train_time:133868ms step_avg:60.79ms
step:2203/2270 train_time:133932ms step_avg:60.80ms
step:2204/2270 train_time:133992ms step_avg:60.79ms
step:2205/2270 train_time:134055ms step_avg:60.80ms
step:2206/2270 train_time:134116ms step_avg:60.80ms
step:2207/2270 train_time:134178ms step_avg:60.80ms
step:2208/2270 train_time:134238ms step_avg:60.80ms
step:2209/2270 train_time:134301ms step_avg:60.80ms
step:2210/2270 train_time:134361ms step_avg:60.80ms
step:2211/2270 train_time:134424ms step_avg:60.80ms
step:2212/2270 train_time:134483ms step_avg:60.80ms
step:2213/2270 train_time:134546ms step_avg:60.80ms
step:2214/2270 train_time:134606ms step_avg:60.80ms
step:2215/2270 train_time:134669ms step_avg:60.80ms
step:2216/2270 train_time:134729ms step_avg:60.80ms
step:2217/2270 train_time:134792ms step_avg:60.80ms
step:2218/2270 train_time:134853ms step_avg:60.80ms
step:2219/2270 train_time:134916ms step_avg:60.80ms
step:2220/2270 train_time:134976ms step_avg:60.80ms
step:2221/2270 train_time:135039ms step_avg:60.80ms
step:2222/2270 train_time:135099ms step_avg:60.80ms
step:2223/2270 train_time:135161ms step_avg:60.80ms
step:2224/2270 train_time:135222ms step_avg:60.80ms
step:2225/2270 train_time:135285ms step_avg:60.80ms
step:2226/2270 train_time:135345ms step_avg:60.80ms
step:2227/2270 train_time:135407ms step_avg:60.80ms
step:2228/2270 train_time:135468ms step_avg:60.80ms
step:2229/2270 train_time:135530ms step_avg:60.80ms
step:2230/2270 train_time:135590ms step_avg:60.80ms
step:2231/2270 train_time:135653ms step_avg:60.80ms
step:2232/2270 train_time:135713ms step_avg:60.80ms
step:2233/2270 train_time:135776ms step_avg:60.80ms
step:2234/2270 train_time:135836ms step_avg:60.80ms
step:2235/2270 train_time:135899ms step_avg:60.80ms
step:2236/2270 train_time:135959ms step_avg:60.80ms
step:2237/2270 train_time:136021ms step_avg:60.81ms
step:2238/2270 train_time:136081ms step_avg:60.80ms
step:2239/2270 train_time:136144ms step_avg:60.81ms
step:2240/2270 train_time:136204ms step_avg:60.81ms
step:2241/2270 train_time:136266ms step_avg:60.81ms
step:2242/2270 train_time:136327ms step_avg:60.81ms
step:2243/2270 train_time:136390ms step_avg:60.81ms
step:2244/2270 train_time:136450ms step_avg:60.81ms
step:2245/2270 train_time:136513ms step_avg:60.81ms
step:2246/2270 train_time:136573ms step_avg:60.81ms
step:2247/2270 train_time:136637ms step_avg:60.81ms
step:2248/2270 train_time:136696ms step_avg:60.81ms
step:2249/2270 train_time:136760ms step_avg:60.81ms
step:2250/2270 train_time:136820ms step_avg:60.81ms
step:2250/2270 val_loss:3.2818 train_time:136883ms step_avg:60.84ms
step:2251/2270 train_time:136902ms step_avg:60.82ms
step:2252/2270 train_time:136947ms step_avg:60.81ms
step:2253/2270 train_time:137013ms step_avg:60.81ms
step:2254/2270 train_time:137074ms step_avg:60.81ms
step:2255/2270 train_time:137136ms step_avg:60.81ms
step:2256/2270 train_time:137197ms step_avg:60.81ms
step:2257/2270 train_time:137259ms step_avg:60.81ms
step:2258/2270 train_time:137319ms step_avg:60.81ms
step:2259/2270 train_time:137381ms step_avg:60.81ms
step:2260/2270 train_time:137441ms step_avg:60.81ms
step:2261/2270 train_time:137504ms step_avg:60.82ms
step:2262/2270 train_time:137564ms step_avg:60.82ms
step:2263/2270 train_time:137626ms step_avg:60.82ms
step:2264/2270 train_time:137686ms step_avg:60.82ms
step:2265/2270 train_time:137749ms step_avg:60.82ms
step:2266/2270 train_time:137810ms step_avg:60.82ms
step:2267/2270 train_time:137875ms step_avg:60.82ms
step:2268/2270 train_time:137937ms step_avg:60.82ms
step:2269/2270 train_time:138001ms step_avg:60.82ms
step:2270/2270 train_time:138063ms step_avg:60.82ms
step:2270/2270 val_loss:3.2774 train_time:138127ms step_avg:60.85ms
peak memory allocated: 29249 MiB reserved: 50528 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, eps=1e-8, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp_up', 'mlp_down']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            elif params[module_idx].label == "smear_gate":
                # dividing by magnitude is equivalent of SVN for 1d tensors
                v_chunk = updated_grads / (updated_grads.norm(dim=(-2, -1), keepdim=True).clamp_min(1e-10))
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            same_sign = torch.signbit(v_chunk) == torch.signbit(param_chunk)
            v_chunk.add_(eff_wd * (param_chunk * same_sign.to(ref_param.dtype)))

            param_chunk.add_(-eff_lr * v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp_up'
        self.c_proj.label = 'mlp_down'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 2270
    lr_schedule = (0.5, 0.98)    # breakpoints for 3-part schedule: (flat, linear decay, flat)
    lr_min = 0.1
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 5, 7, 9, 11, 13)
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=0.01)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

def get_lr(step: int):
    assert step < args.num_iterations
    # Three part schedule: flat, linear decrease, flat
    lr_schedule = args.lr_schedule
    x = step / args.num_iterations

    if x < lr_schedule[0]:
        return 1.0
    elif x < lr_schedule[1]:
        progress = (x - lr_schedule[0]) / (lr_schedule[1] - lr_schedule[0])
        lr = 1.0 - (1.0 - args.lr_min) * progress
    else:
        lr = args.lr_min
    return lr

def get_ws(step: int):
    assert step <= args.num_iterations
    x = step / (args.num_iterations + 1)
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset()  #  momentum buffer not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    loss = 0
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        loss += model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps
    loss.backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Oct 28 03:24:54 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   34C    P0            123W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   31C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   30C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   33C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   31C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2270 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2270 train_time:121ms step_avg:120.72ms
step:2/2270 train_time:143ms step_avg:71.38ms
step:3/2270 train_time:181ms step_avg:60.24ms
step:4/2270 train_time:237ms step_avg:59.29ms
step:5/2270 train_time:297ms step_avg:59.34ms
step:6/2270 train_time:355ms step_avg:59.12ms
step:7/2270 train_time:416ms step_avg:59.43ms
step:8/2270 train_time:474ms step_avg:59.24ms
step:9/2270 train_time:535ms step_avg:59.47ms
step:10/2270 train_time:593ms step_avg:59.35ms
step:11/2270 train_time:655ms step_avg:59.50ms
step:12/2270 train_time:713ms step_avg:59.43ms
step:13/2270 train_time:774ms step_avg:59.55ms
step:14/2270 train_time:833ms step_avg:59.49ms
step:15/2270 train_time:894ms step_avg:59.59ms
step:16/2270 train_time:953ms step_avg:59.56ms
step:17/2270 train_time:1018ms step_avg:59.86ms
step:18/2270 train_time:1081ms step_avg:60.07ms
step:19/2270 train_time:1146ms step_avg:60.31ms
step:20/2270 train_time:1206ms step_avg:60.30ms
step:21/2270 train_time:1268ms step_avg:60.37ms
step:22/2270 train_time:1327ms step_avg:60.30ms
step:23/2270 train_time:1388ms step_avg:60.35ms
step:24/2270 train_time:1447ms step_avg:60.28ms
step:25/2270 train_time:1508ms step_avg:60.32ms
step:26/2270 train_time:1567ms step_avg:60.25ms
step:27/2270 train_time:1628ms step_avg:60.29ms
step:28/2270 train_time:1686ms step_avg:60.23ms
step:29/2270 train_time:1748ms step_avg:60.27ms
step:30/2270 train_time:1808ms step_avg:60.25ms
step:31/2270 train_time:1869ms step_avg:60.30ms
step:32/2270 train_time:1928ms step_avg:60.24ms
step:33/2270 train_time:1990ms step_avg:60.30ms
step:34/2270 train_time:2050ms step_avg:60.30ms
step:35/2270 train_time:2113ms step_avg:60.37ms
step:36/2270 train_time:2173ms step_avg:60.36ms
step:37/2270 train_time:2236ms step_avg:60.43ms
step:38/2270 train_time:2296ms step_avg:60.41ms
step:39/2270 train_time:2358ms step_avg:60.46ms
step:40/2270 train_time:2417ms step_avg:60.43ms
step:41/2270 train_time:2478ms step_avg:60.45ms
step:42/2270 train_time:2538ms step_avg:60.42ms
step:43/2270 train_time:2599ms step_avg:60.44ms
step:44/2270 train_time:2657ms step_avg:60.40ms
step:45/2270 train_time:2719ms step_avg:60.42ms
step:46/2270 train_time:2778ms step_avg:60.39ms
step:47/2270 train_time:2841ms step_avg:60.44ms
step:48/2270 train_time:2899ms step_avg:60.39ms
step:49/2270 train_time:2960ms step_avg:60.41ms
step:50/2270 train_time:3020ms step_avg:60.39ms
step:51/2270 train_time:3081ms step_avg:60.41ms
step:52/2270 train_time:3140ms step_avg:60.39ms
step:53/2270 train_time:3202ms step_avg:60.41ms
step:54/2270 train_time:3261ms step_avg:60.38ms
step:55/2270 train_time:3322ms step_avg:60.40ms
step:56/2270 train_time:3381ms step_avg:60.37ms
step:57/2270 train_time:3442ms step_avg:60.39ms
step:58/2270 train_time:3501ms step_avg:60.36ms
step:59/2270 train_time:3563ms step_avg:60.38ms
step:60/2270 train_time:3621ms step_avg:60.35ms
step:61/2270 train_time:3682ms step_avg:60.36ms
step:62/2270 train_time:3741ms step_avg:60.33ms
step:63/2270 train_time:3802ms step_avg:60.34ms
step:64/2270 train_time:3860ms step_avg:60.31ms
step:65/2270 train_time:3921ms step_avg:60.33ms
step:66/2270 train_time:3980ms step_avg:60.31ms
step:67/2270 train_time:4042ms step_avg:60.33ms
step:68/2270 train_time:4101ms step_avg:60.31ms
step:69/2270 train_time:4163ms step_avg:60.33ms
step:70/2270 train_time:4221ms step_avg:60.31ms
step:71/2270 train_time:4283ms step_avg:60.32ms
step:72/2270 train_time:4341ms step_avg:60.29ms
step:73/2270 train_time:4402ms step_avg:60.30ms
step:74/2270 train_time:4461ms step_avg:60.28ms
step:75/2270 train_time:4522ms step_avg:60.30ms
step:76/2270 train_time:4581ms step_avg:60.27ms
step:77/2270 train_time:4642ms step_avg:60.29ms
step:78/2270 train_time:4701ms step_avg:60.26ms
step:79/2270 train_time:4762ms step_avg:60.28ms
step:80/2270 train_time:4820ms step_avg:60.26ms
step:81/2270 train_time:4881ms step_avg:60.26ms
step:82/2270 train_time:4941ms step_avg:60.25ms
step:83/2270 train_time:5002ms step_avg:60.26ms
step:84/2270 train_time:5061ms step_avg:60.24ms
step:85/2270 train_time:5122ms step_avg:60.26ms
step:86/2270 train_time:5181ms step_avg:60.24ms
step:87/2270 train_time:5243ms step_avg:60.26ms
step:88/2270 train_time:5302ms step_avg:60.25ms
step:89/2270 train_time:5363ms step_avg:60.26ms
step:90/2270 train_time:5421ms step_avg:60.23ms
step:91/2270 train_time:5483ms step_avg:60.25ms
step:92/2270 train_time:5542ms step_avg:60.24ms
step:93/2270 train_time:5603ms step_avg:60.24ms
step:94/2270 train_time:5661ms step_avg:60.22ms
step:95/2270 train_time:5722ms step_avg:60.23ms
step:96/2270 train_time:5781ms step_avg:60.22ms
step:97/2270 train_time:5842ms step_avg:60.23ms
step:98/2270 train_time:5900ms step_avg:60.21ms
step:99/2270 train_time:5962ms step_avg:60.22ms
step:100/2270 train_time:6020ms step_avg:60.20ms
step:101/2270 train_time:6082ms step_avg:60.22ms
step:102/2270 train_time:6141ms step_avg:60.21ms
step:103/2270 train_time:6202ms step_avg:60.21ms
step:104/2270 train_time:6260ms step_avg:60.20ms
step:105/2270 train_time:6322ms step_avg:60.21ms
step:106/2270 train_time:6381ms step_avg:60.20ms
step:107/2270 train_time:6442ms step_avg:60.21ms
step:108/2270 train_time:6500ms step_avg:60.19ms
step:109/2270 train_time:6561ms step_avg:60.19ms
step:110/2270 train_time:6620ms step_avg:60.18ms
step:111/2270 train_time:6681ms step_avg:60.19ms
step:112/2270 train_time:6740ms step_avg:60.18ms
step:113/2270 train_time:6801ms step_avg:60.18ms
step:114/2270 train_time:6859ms step_avg:60.17ms
step:115/2270 train_time:6920ms step_avg:60.17ms
step:116/2270 train_time:6979ms step_avg:60.16ms
step:117/2270 train_time:7040ms step_avg:60.17ms
step:118/2270 train_time:7099ms step_avg:60.16ms
step:119/2270 train_time:7160ms step_avg:60.16ms
step:120/2270 train_time:7218ms step_avg:60.15ms
step:121/2270 train_time:7280ms step_avg:60.16ms
step:122/2270 train_time:7339ms step_avg:60.16ms
step:123/2270 train_time:7401ms step_avg:60.17ms
step:124/2270 train_time:7460ms step_avg:60.16ms
step:125/2270 train_time:7521ms step_avg:60.17ms
step:126/2270 train_time:7579ms step_avg:60.15ms
step:127/2270 train_time:7641ms step_avg:60.17ms
step:128/2270 train_time:7699ms step_avg:60.15ms
step:129/2270 train_time:7760ms step_avg:60.16ms
step:130/2270 train_time:7819ms step_avg:60.14ms
step:131/2270 train_time:7880ms step_avg:60.15ms
step:132/2270 train_time:7938ms step_avg:60.14ms
step:133/2270 train_time:7999ms step_avg:60.15ms
step:134/2270 train_time:8058ms step_avg:60.13ms
step:135/2270 train_time:8120ms step_avg:60.15ms
step:136/2270 train_time:8178ms step_avg:60.13ms
step:137/2270 train_time:8240ms step_avg:60.15ms
step:138/2270 train_time:8299ms step_avg:60.13ms
step:139/2270 train_time:8360ms step_avg:60.14ms
step:140/2270 train_time:8418ms step_avg:60.13ms
step:141/2270 train_time:8480ms step_avg:60.14ms
step:142/2270 train_time:8538ms step_avg:60.13ms
step:143/2270 train_time:8600ms step_avg:60.14ms
step:144/2270 train_time:8659ms step_avg:60.14ms
step:145/2270 train_time:8721ms step_avg:60.14ms
step:146/2270 train_time:8779ms step_avg:60.13ms
step:147/2270 train_time:8840ms step_avg:60.14ms
step:148/2270 train_time:8899ms step_avg:60.13ms
step:149/2270 train_time:8960ms step_avg:60.13ms
step:150/2270 train_time:9018ms step_avg:60.12ms
step:151/2270 train_time:9080ms step_avg:60.13ms
step:152/2270 train_time:9138ms step_avg:60.12ms
step:153/2270 train_time:9200ms step_avg:60.13ms
step:154/2270 train_time:9259ms step_avg:60.12ms
step:155/2270 train_time:9320ms step_avg:60.13ms
step:156/2270 train_time:9378ms step_avg:60.12ms
step:157/2270 train_time:9441ms step_avg:60.13ms
step:158/2270 train_time:9500ms step_avg:60.13ms
step:159/2270 train_time:9561ms step_avg:60.13ms
step:160/2270 train_time:9619ms step_avg:60.12ms
step:161/2270 train_time:9680ms step_avg:60.13ms
step:162/2270 train_time:9739ms step_avg:60.12ms
step:163/2270 train_time:9800ms step_avg:60.12ms
step:164/2270 train_time:9858ms step_avg:60.11ms
step:165/2270 train_time:9919ms step_avg:60.12ms
step:166/2270 train_time:9977ms step_avg:60.10ms
step:167/2270 train_time:10039ms step_avg:60.11ms
step:168/2270 train_time:10097ms step_avg:60.10ms
step:169/2270 train_time:10159ms step_avg:60.11ms
step:170/2270 train_time:10217ms step_avg:60.10ms
step:171/2270 train_time:10279ms step_avg:60.11ms
step:172/2270 train_time:10338ms step_avg:60.11ms
step:173/2270 train_time:10399ms step_avg:60.11ms
step:174/2270 train_time:10457ms step_avg:60.10ms
step:175/2270 train_time:10519ms step_avg:60.11ms
step:176/2270 train_time:10578ms step_avg:60.10ms
step:177/2270 train_time:10640ms step_avg:60.11ms
step:178/2270 train_time:10699ms step_avg:60.10ms
step:179/2270 train_time:10760ms step_avg:60.11ms
step:180/2270 train_time:10819ms step_avg:60.10ms
step:181/2270 train_time:10879ms step_avg:60.11ms
step:182/2270 train_time:10938ms step_avg:60.10ms
step:183/2270 train_time:10999ms step_avg:60.11ms
step:184/2270 train_time:11058ms step_avg:60.10ms
step:185/2270 train_time:11119ms step_avg:60.10ms
step:186/2270 train_time:11177ms step_avg:60.09ms
step:187/2270 train_time:11239ms step_avg:60.10ms
step:188/2270 train_time:11298ms step_avg:60.10ms
step:189/2270 train_time:11359ms step_avg:60.10ms
step:190/2270 train_time:11417ms step_avg:60.09ms
step:191/2270 train_time:11479ms step_avg:60.10ms
step:192/2270 train_time:11538ms step_avg:60.09ms
step:193/2270 train_time:11599ms step_avg:60.10ms
step:194/2270 train_time:11658ms step_avg:60.09ms
step:195/2270 train_time:11719ms step_avg:60.10ms
step:196/2270 train_time:11778ms step_avg:60.09ms
step:197/2270 train_time:11839ms step_avg:60.10ms
step:198/2270 train_time:11898ms step_avg:60.09ms
step:199/2270 train_time:11959ms step_avg:60.10ms
step:200/2270 train_time:12018ms step_avg:60.09ms
step:201/2270 train_time:12079ms step_avg:60.09ms
step:202/2270 train_time:12138ms step_avg:60.09ms
step:203/2270 train_time:12198ms step_avg:60.09ms
step:204/2270 train_time:12257ms step_avg:60.08ms
step:205/2270 train_time:12319ms step_avg:60.09ms
step:206/2270 train_time:12377ms step_avg:60.08ms
step:207/2270 train_time:12439ms step_avg:60.09ms
step:208/2270 train_time:12498ms step_avg:60.08ms
step:209/2270 train_time:12559ms step_avg:60.09ms
step:210/2270 train_time:12617ms step_avg:60.08ms
step:211/2270 train_time:12679ms step_avg:60.09ms
step:212/2270 train_time:12738ms step_avg:60.09ms
step:213/2270 train_time:12800ms step_avg:60.09ms
step:214/2270 train_time:12858ms step_avg:60.08ms
step:215/2270 train_time:12919ms step_avg:60.09ms
step:216/2270 train_time:12978ms step_avg:60.08ms
step:217/2270 train_time:13040ms step_avg:60.09ms
step:218/2270 train_time:13099ms step_avg:60.09ms
step:219/2270 train_time:13159ms step_avg:60.09ms
step:220/2270 train_time:13218ms step_avg:60.08ms
step:221/2270 train_time:13279ms step_avg:60.08ms
step:222/2270 train_time:13338ms step_avg:60.08ms
step:223/2270 train_time:13399ms step_avg:60.09ms
step:224/2270 train_time:13458ms step_avg:60.08ms
step:225/2270 train_time:13520ms step_avg:60.09ms
step:226/2270 train_time:13578ms step_avg:60.08ms
step:227/2270 train_time:13640ms step_avg:60.09ms
step:228/2270 train_time:13699ms step_avg:60.08ms
step:229/2270 train_time:13760ms step_avg:60.09ms
step:230/2270 train_time:13818ms step_avg:60.08ms
step:231/2270 train_time:13879ms step_avg:60.08ms
step:232/2270 train_time:13938ms step_avg:60.08ms
step:233/2270 train_time:14000ms step_avg:60.09ms
step:234/2270 train_time:14058ms step_avg:60.08ms
step:235/2270 train_time:14120ms step_avg:60.08ms
step:236/2270 train_time:14178ms step_avg:60.08ms
step:237/2270 train_time:14239ms step_avg:60.08ms
step:238/2270 train_time:14298ms step_avg:60.07ms
step:239/2270 train_time:14359ms step_avg:60.08ms
step:240/2270 train_time:14417ms step_avg:60.07ms
step:241/2270 train_time:14479ms step_avg:60.08ms
step:242/2270 train_time:14538ms step_avg:60.07ms
step:243/2270 train_time:14599ms step_avg:60.08ms
step:244/2270 train_time:14657ms step_avg:60.07ms
step:245/2270 train_time:14719ms step_avg:60.08ms
step:246/2270 train_time:14778ms step_avg:60.07ms
step:247/2270 train_time:14839ms step_avg:60.08ms
step:248/2270 train_time:14898ms step_avg:60.07ms
step:249/2270 train_time:14960ms step_avg:60.08ms
step:250/2270 train_time:15018ms step_avg:60.07ms
step:250/2270 val_loss:4.0707 train_time:15080ms step_avg:60.32ms
step:251/2270 train_time:15100ms step_avg:60.16ms
step:252/2270 train_time:15142ms step_avg:60.09ms
step:253/2270 train_time:15209ms step_avg:60.12ms
step:254/2270 train_time:15270ms step_avg:60.12ms
step:255/2270 train_time:15332ms step_avg:60.12ms
step:256/2270 train_time:15390ms step_avg:60.12ms
step:257/2270 train_time:15451ms step_avg:60.12ms
step:258/2270 train_time:15509ms step_avg:60.11ms
step:259/2270 train_time:15569ms step_avg:60.11ms
step:260/2270 train_time:15627ms step_avg:60.10ms
step:261/2270 train_time:15687ms step_avg:60.10ms
step:262/2270 train_time:15745ms step_avg:60.10ms
step:263/2270 train_time:15806ms step_avg:60.10ms
step:264/2270 train_time:15863ms step_avg:60.09ms
step:265/2270 train_time:15923ms step_avg:60.09ms
step:266/2270 train_time:15982ms step_avg:60.08ms
step:267/2270 train_time:16042ms step_avg:60.08ms
step:268/2270 train_time:16101ms step_avg:60.08ms
step:269/2270 train_time:16165ms step_avg:60.09ms
step:270/2270 train_time:16225ms step_avg:60.09ms
step:271/2270 train_time:16288ms step_avg:60.10ms
step:272/2270 train_time:16348ms step_avg:60.10ms
step:273/2270 train_time:16409ms step_avg:60.11ms
step:274/2270 train_time:16467ms step_avg:60.10ms
step:275/2270 train_time:16529ms step_avg:60.11ms
step:276/2270 train_time:16588ms step_avg:60.10ms
step:277/2270 train_time:16649ms step_avg:60.10ms
step:278/2270 train_time:16707ms step_avg:60.10ms
step:279/2270 train_time:16767ms step_avg:60.10ms
step:280/2270 train_time:16825ms step_avg:60.09ms
step:281/2270 train_time:16886ms step_avg:60.09ms
step:282/2270 train_time:16944ms step_avg:60.09ms
step:283/2270 train_time:17005ms step_avg:60.09ms
step:284/2270 train_time:17064ms step_avg:60.09ms
step:285/2270 train_time:17127ms step_avg:60.09ms
step:286/2270 train_time:17188ms step_avg:60.10ms
step:287/2270 train_time:17250ms step_avg:60.10ms
step:288/2270 train_time:17309ms step_avg:60.10ms
step:289/2270 train_time:17370ms step_avg:60.10ms
step:290/2270 train_time:17428ms step_avg:60.10ms
step:291/2270 train_time:17490ms step_avg:60.10ms
step:292/2270 train_time:17549ms step_avg:60.10ms
step:293/2270 train_time:17609ms step_avg:60.10ms
step:294/2270 train_time:17667ms step_avg:60.09ms
step:295/2270 train_time:17728ms step_avg:60.09ms
step:296/2270 train_time:17786ms step_avg:60.09ms
step:297/2270 train_time:17847ms step_avg:60.09ms
step:298/2270 train_time:17906ms step_avg:60.09ms
step:299/2270 train_time:17966ms step_avg:60.09ms
step:300/2270 train_time:18025ms step_avg:60.08ms
step:301/2270 train_time:18087ms step_avg:60.09ms
step:302/2270 train_time:18147ms step_avg:60.09ms
step:303/2270 train_time:18209ms step_avg:60.10ms
step:304/2270 train_time:18268ms step_avg:60.09ms
step:305/2270 train_time:18329ms step_avg:60.10ms
step:306/2270 train_time:18389ms step_avg:60.09ms
step:307/2270 train_time:18450ms step_avg:60.10ms
step:308/2270 train_time:18508ms step_avg:60.09ms
step:309/2270 train_time:18569ms step_avg:60.09ms
step:310/2270 train_time:18627ms step_avg:60.09ms
step:311/2270 train_time:18688ms step_avg:60.09ms
step:312/2270 train_time:18747ms step_avg:60.09ms
step:313/2270 train_time:18808ms step_avg:60.09ms
step:314/2270 train_time:18866ms step_avg:60.08ms
step:315/2270 train_time:18926ms step_avg:60.08ms
step:316/2270 train_time:18985ms step_avg:60.08ms
step:317/2270 train_time:19047ms step_avg:60.08ms
step:318/2270 train_time:19105ms step_avg:60.08ms
step:319/2270 train_time:19166ms step_avg:60.08ms
step:320/2270 train_time:19226ms step_avg:60.08ms
step:321/2270 train_time:19287ms step_avg:60.09ms
step:322/2270 train_time:19347ms step_avg:60.08ms
step:323/2270 train_time:19409ms step_avg:60.09ms
step:324/2270 train_time:19467ms step_avg:60.08ms
step:325/2270 train_time:19529ms step_avg:60.09ms
step:326/2270 train_time:19587ms step_avg:60.08ms
step:327/2270 train_time:19648ms step_avg:60.09ms
step:328/2270 train_time:19707ms step_avg:60.08ms
step:329/2270 train_time:19767ms step_avg:60.08ms
step:330/2270 train_time:19826ms step_avg:60.08ms
step:331/2270 train_time:19887ms step_avg:60.08ms
step:332/2270 train_time:19945ms step_avg:60.07ms
step:333/2270 train_time:20006ms step_avg:60.08ms
step:334/2270 train_time:20064ms step_avg:60.07ms
step:335/2270 train_time:20126ms step_avg:60.08ms
step:336/2270 train_time:20186ms step_avg:60.08ms
step:337/2270 train_time:20247ms step_avg:60.08ms
step:338/2270 train_time:20306ms step_avg:60.08ms
step:339/2270 train_time:20368ms step_avg:60.08ms
step:340/2270 train_time:20427ms step_avg:60.08ms
step:341/2270 train_time:20489ms step_avg:60.08ms
step:342/2270 train_time:20547ms step_avg:60.08ms
step:343/2270 train_time:20609ms step_avg:60.08ms
step:344/2270 train_time:20667ms step_avg:60.08ms
step:345/2270 train_time:20728ms step_avg:60.08ms
step:346/2270 train_time:20786ms step_avg:60.08ms
step:347/2270 train_time:20847ms step_avg:60.08ms
step:348/2270 train_time:20906ms step_avg:60.07ms
step:349/2270 train_time:20967ms step_avg:60.08ms
step:350/2270 train_time:21026ms step_avg:60.07ms
step:351/2270 train_time:21088ms step_avg:60.08ms
step:352/2270 train_time:21146ms step_avg:60.07ms
step:353/2270 train_time:21208ms step_avg:60.08ms
step:354/2270 train_time:21266ms step_avg:60.07ms
step:355/2270 train_time:21328ms step_avg:60.08ms
step:356/2270 train_time:21388ms step_avg:60.08ms
step:357/2270 train_time:21449ms step_avg:60.08ms
step:358/2270 train_time:21508ms step_avg:60.08ms
step:359/2270 train_time:21569ms step_avg:60.08ms
step:360/2270 train_time:21628ms step_avg:60.08ms
step:361/2270 train_time:21689ms step_avg:60.08ms
step:362/2270 train_time:21748ms step_avg:60.08ms
step:363/2270 train_time:21808ms step_avg:60.08ms
step:364/2270 train_time:21866ms step_avg:60.07ms
step:365/2270 train_time:21928ms step_avg:60.08ms
step:366/2270 train_time:21986ms step_avg:60.07ms
step:367/2270 train_time:22047ms step_avg:60.07ms
step:368/2270 train_time:22106ms step_avg:60.07ms
step:369/2270 train_time:22167ms step_avg:60.07ms
step:370/2270 train_time:22225ms step_avg:60.07ms
step:371/2270 train_time:22287ms step_avg:60.07ms
step:372/2270 train_time:22346ms step_avg:60.07ms
step:373/2270 train_time:22407ms step_avg:60.07ms
step:374/2270 train_time:22465ms step_avg:60.07ms
step:375/2270 train_time:22527ms step_avg:60.07ms
step:376/2270 train_time:22587ms step_avg:60.07ms
step:377/2270 train_time:22648ms step_avg:60.08ms
step:378/2270 train_time:22707ms step_avg:60.07ms
step:379/2270 train_time:22768ms step_avg:60.07ms
step:380/2270 train_time:22827ms step_avg:60.07ms
step:381/2270 train_time:22889ms step_avg:60.08ms
step:382/2270 train_time:22948ms step_avg:60.07ms
step:383/2270 train_time:23009ms step_avg:60.08ms
step:384/2270 train_time:23069ms step_avg:60.07ms
step:385/2270 train_time:23130ms step_avg:60.08ms
step:386/2270 train_time:23189ms step_avg:60.08ms
step:387/2270 train_time:23250ms step_avg:60.08ms
step:388/2270 train_time:23309ms step_avg:60.07ms
step:389/2270 train_time:23370ms step_avg:60.08ms
step:390/2270 train_time:23430ms step_avg:60.08ms
step:391/2270 train_time:23491ms step_avg:60.08ms
step:392/2270 train_time:23551ms step_avg:60.08ms
step:393/2270 train_time:23612ms step_avg:60.08ms
step:394/2270 train_time:23671ms step_avg:60.08ms
step:395/2270 train_time:23733ms step_avg:60.08ms
step:396/2270 train_time:23792ms step_avg:60.08ms
step:397/2270 train_time:23854ms step_avg:60.09ms
step:398/2270 train_time:23913ms step_avg:60.08ms
step:399/2270 train_time:23975ms step_avg:60.09ms
step:400/2270 train_time:24034ms step_avg:60.08ms
step:401/2270 train_time:24095ms step_avg:60.09ms
step:402/2270 train_time:24154ms step_avg:60.08ms
step:403/2270 train_time:24215ms step_avg:60.09ms
step:404/2270 train_time:24275ms step_avg:60.09ms
step:405/2270 train_time:24336ms step_avg:60.09ms
step:406/2270 train_time:24395ms step_avg:60.09ms
step:407/2270 train_time:24458ms step_avg:60.09ms
step:408/2270 train_time:24516ms step_avg:60.09ms
step:409/2270 train_time:24578ms step_avg:60.09ms
step:410/2270 train_time:24638ms step_avg:60.09ms
step:411/2270 train_time:24700ms step_avg:60.10ms
step:412/2270 train_time:24758ms step_avg:60.09ms
step:413/2270 train_time:24820ms step_avg:60.10ms
step:414/2270 train_time:24879ms step_avg:60.09ms
step:415/2270 train_time:24941ms step_avg:60.10ms
step:416/2270 train_time:25000ms step_avg:60.10ms
step:417/2270 train_time:25062ms step_avg:60.10ms
step:418/2270 train_time:25122ms step_avg:60.10ms
step:419/2270 train_time:25185ms step_avg:60.11ms
step:420/2270 train_time:25244ms step_avg:60.10ms
step:421/2270 train_time:25306ms step_avg:60.11ms
step:422/2270 train_time:25365ms step_avg:60.11ms
step:423/2270 train_time:25427ms step_avg:60.11ms
step:424/2270 train_time:25486ms step_avg:60.11ms
step:425/2270 train_time:25549ms step_avg:60.11ms
step:426/2270 train_time:25607ms step_avg:60.11ms
step:427/2270 train_time:25668ms step_avg:60.11ms
step:428/2270 train_time:25727ms step_avg:60.11ms
step:429/2270 train_time:25789ms step_avg:60.12ms
step:430/2270 train_time:25848ms step_avg:60.11ms
step:431/2270 train_time:25909ms step_avg:60.11ms
step:432/2270 train_time:25968ms step_avg:60.11ms
step:433/2270 train_time:26030ms step_avg:60.11ms
step:434/2270 train_time:26089ms step_avg:60.11ms
step:435/2270 train_time:26151ms step_avg:60.12ms
step:436/2270 train_time:26210ms step_avg:60.11ms
step:437/2270 train_time:26272ms step_avg:60.12ms
step:438/2270 train_time:26331ms step_avg:60.12ms
step:439/2270 train_time:26392ms step_avg:60.12ms
step:440/2270 train_time:26451ms step_avg:60.12ms
step:441/2270 train_time:26513ms step_avg:60.12ms
step:442/2270 train_time:26572ms step_avg:60.12ms
step:443/2270 train_time:26634ms step_avg:60.12ms
step:444/2270 train_time:26692ms step_avg:60.12ms
step:445/2270 train_time:26754ms step_avg:60.12ms
step:446/2270 train_time:26813ms step_avg:60.12ms
step:447/2270 train_time:26874ms step_avg:60.12ms
step:448/2270 train_time:26934ms step_avg:60.12ms
step:449/2270 train_time:26995ms step_avg:60.12ms
step:450/2270 train_time:27054ms step_avg:60.12ms
step:451/2270 train_time:27116ms step_avg:60.12ms
step:452/2270 train_time:27175ms step_avg:60.12ms
step:453/2270 train_time:27237ms step_avg:60.13ms
step:454/2270 train_time:27296ms step_avg:60.12ms
step:455/2270 train_time:27357ms step_avg:60.13ms
step:456/2270 train_time:27416ms step_avg:60.12ms
step:457/2270 train_time:27478ms step_avg:60.13ms
step:458/2270 train_time:27537ms step_avg:60.12ms
step:459/2270 train_time:27599ms step_avg:60.13ms
step:460/2270 train_time:27658ms step_avg:60.13ms
step:461/2270 train_time:27720ms step_avg:60.13ms
step:462/2270 train_time:27778ms step_avg:60.13ms
step:463/2270 train_time:27841ms step_avg:60.13ms
step:464/2270 train_time:27900ms step_avg:60.13ms
step:465/2270 train_time:27962ms step_avg:60.13ms
step:466/2270 train_time:28021ms step_avg:60.13ms
step:467/2270 train_time:28085ms step_avg:60.14ms
step:468/2270 train_time:28144ms step_avg:60.14ms
step:469/2270 train_time:28206ms step_avg:60.14ms
step:470/2270 train_time:28266ms step_avg:60.14ms
step:471/2270 train_time:28329ms step_avg:60.15ms
step:472/2270 train_time:28388ms step_avg:60.14ms
step:473/2270 train_time:28449ms step_avg:60.15ms
step:474/2270 train_time:28507ms step_avg:60.14ms
step:475/2270 train_time:28568ms step_avg:60.14ms
step:476/2270 train_time:28627ms step_avg:60.14ms
step:477/2270 train_time:28688ms step_avg:60.14ms
step:478/2270 train_time:28747ms step_avg:60.14ms
step:479/2270 train_time:28808ms step_avg:60.14ms
step:480/2270 train_time:28867ms step_avg:60.14ms
step:481/2270 train_time:28929ms step_avg:60.14ms
step:482/2270 train_time:28988ms step_avg:60.14ms
step:483/2270 train_time:29050ms step_avg:60.14ms
step:484/2270 train_time:29109ms step_avg:60.14ms
step:485/2270 train_time:29170ms step_avg:60.14ms
step:486/2270 train_time:29229ms step_avg:60.14ms
step:487/2270 train_time:29291ms step_avg:60.15ms
step:488/2270 train_time:29349ms step_avg:60.14ms
step:489/2270 train_time:29411ms step_avg:60.14ms
step:490/2270 train_time:29469ms step_avg:60.14ms
step:491/2270 train_time:29532ms step_avg:60.15ms
step:492/2270 train_time:29590ms step_avg:60.14ms
step:493/2270 train_time:29651ms step_avg:60.14ms
step:494/2270 train_time:29709ms step_avg:60.14ms
step:495/2270 train_time:29771ms step_avg:60.14ms
step:496/2270 train_time:29830ms step_avg:60.14ms
step:497/2270 train_time:29892ms step_avg:60.14ms
step:498/2270 train_time:29951ms step_avg:60.14ms
step:499/2270 train_time:30012ms step_avg:60.14ms
step:500/2270 train_time:30072ms step_avg:60.14ms
step:500/2270 val_loss:3.7858 train_time:30134ms step_avg:60.27ms
step:501/2270 train_time:30158ms step_avg:60.20ms
step:502/2270 train_time:30195ms step_avg:60.15ms
step:503/2270 train_time:30257ms step_avg:60.15ms
step:504/2270 train_time:30317ms step_avg:60.15ms
step:505/2270 train_time:30381ms step_avg:60.16ms
step:506/2270 train_time:30441ms step_avg:60.16ms
step:507/2270 train_time:30501ms step_avg:60.16ms
step:508/2270 train_time:30560ms step_avg:60.16ms
step:509/2270 train_time:30621ms step_avg:60.16ms
step:510/2270 train_time:30679ms step_avg:60.15ms
step:511/2270 train_time:30740ms step_avg:60.16ms
step:512/2270 train_time:30798ms step_avg:60.15ms
step:513/2270 train_time:30859ms step_avg:60.15ms
step:514/2270 train_time:30917ms step_avg:60.15ms
step:515/2270 train_time:30978ms step_avg:60.15ms
step:516/2270 train_time:31041ms step_avg:60.16ms
step:517/2270 train_time:31108ms step_avg:60.17ms
step:518/2270 train_time:31170ms step_avg:60.17ms
step:519/2270 train_time:31233ms step_avg:60.18ms
step:520/2270 train_time:31292ms step_avg:60.18ms
step:521/2270 train_time:31354ms step_avg:60.18ms
step:522/2270 train_time:31413ms step_avg:60.18ms
step:523/2270 train_time:31474ms step_avg:60.18ms
step:524/2270 train_time:31533ms step_avg:60.18ms
step:525/2270 train_time:31594ms step_avg:60.18ms
step:526/2270 train_time:31653ms step_avg:60.18ms
step:527/2270 train_time:31714ms step_avg:60.18ms
step:528/2270 train_time:31772ms step_avg:60.18ms
step:529/2270 train_time:31833ms step_avg:60.18ms
step:530/2270 train_time:31892ms step_avg:60.17ms
step:531/2270 train_time:31953ms step_avg:60.17ms
step:532/2270 train_time:32012ms step_avg:60.17ms
step:533/2270 train_time:32074ms step_avg:60.18ms
step:534/2270 train_time:32134ms step_avg:60.18ms
step:535/2270 train_time:32196ms step_avg:60.18ms
step:536/2270 train_time:32255ms step_avg:60.18ms
step:537/2270 train_time:32317ms step_avg:60.18ms
step:538/2270 train_time:32377ms step_avg:60.18ms
step:539/2270 train_time:32438ms step_avg:60.18ms
step:540/2270 train_time:32497ms step_avg:60.18ms
step:541/2270 train_time:32558ms step_avg:60.18ms
step:542/2270 train_time:32617ms step_avg:60.18ms
step:543/2270 train_time:32678ms step_avg:60.18ms
step:544/2270 train_time:32737ms step_avg:60.18ms
step:545/2270 train_time:32799ms step_avg:60.18ms
step:546/2270 train_time:32857ms step_avg:60.18ms
step:547/2270 train_time:32919ms step_avg:60.18ms
step:548/2270 train_time:32978ms step_avg:60.18ms
step:549/2270 train_time:33040ms step_avg:60.18ms
step:550/2270 train_time:33100ms step_avg:60.18ms
step:551/2270 train_time:33163ms step_avg:60.19ms
step:552/2270 train_time:33221ms step_avg:60.18ms
step:553/2270 train_time:33283ms step_avg:60.19ms
step:554/2270 train_time:33343ms step_avg:60.19ms
step:555/2270 train_time:33405ms step_avg:60.19ms
step:556/2270 train_time:33466ms step_avg:60.19ms
step:557/2270 train_time:33528ms step_avg:60.19ms
step:558/2270 train_time:33588ms step_avg:60.19ms
step:559/2270 train_time:33649ms step_avg:60.20ms
step:560/2270 train_time:33709ms step_avg:60.19ms
step:561/2270 train_time:33771ms step_avg:60.20ms
step:562/2270 train_time:33830ms step_avg:60.20ms
step:563/2270 train_time:33892ms step_avg:60.20ms
step:564/2270 train_time:33951ms step_avg:60.20ms
step:565/2270 train_time:34013ms step_avg:60.20ms
step:566/2270 train_time:34072ms step_avg:60.20ms
step:567/2270 train_time:34134ms step_avg:60.20ms
step:568/2270 train_time:34193ms step_avg:60.20ms
step:569/2270 train_time:34254ms step_avg:60.20ms
step:570/2270 train_time:34314ms step_avg:60.20ms
step:571/2270 train_time:34376ms step_avg:60.20ms
step:572/2270 train_time:34436ms step_avg:60.20ms
step:573/2270 train_time:34497ms step_avg:60.20ms
step:574/2270 train_time:34556ms step_avg:60.20ms
step:575/2270 train_time:34618ms step_avg:60.21ms
step:576/2270 train_time:34678ms step_avg:60.21ms
step:577/2270 train_time:34741ms step_avg:60.21ms
step:578/2270 train_time:34800ms step_avg:60.21ms
step:579/2270 train_time:34861ms step_avg:60.21ms
step:580/2270 train_time:34920ms step_avg:60.21ms
step:581/2270 train_time:34983ms step_avg:60.21ms
step:582/2270 train_time:35042ms step_avg:60.21ms
step:583/2270 train_time:35104ms step_avg:60.21ms
step:584/2270 train_time:35163ms step_avg:60.21ms
step:585/2270 train_time:35226ms step_avg:60.22ms
step:586/2270 train_time:35285ms step_avg:60.21ms
step:587/2270 train_time:35347ms step_avg:60.22ms
step:588/2270 train_time:35407ms step_avg:60.22ms
step:589/2270 train_time:35469ms step_avg:60.22ms
step:590/2270 train_time:35529ms step_avg:60.22ms
step:591/2270 train_time:35591ms step_avg:60.22ms
step:592/2270 train_time:35650ms step_avg:60.22ms
step:593/2270 train_time:35712ms step_avg:60.22ms
step:594/2270 train_time:35771ms step_avg:60.22ms
step:595/2270 train_time:35833ms step_avg:60.22ms
step:596/2270 train_time:35892ms step_avg:60.22ms
step:597/2270 train_time:35953ms step_avg:60.22ms
step:598/2270 train_time:36012ms step_avg:60.22ms
step:599/2270 train_time:36073ms step_avg:60.22ms
step:600/2270 train_time:36133ms step_avg:60.22ms
step:601/2270 train_time:36195ms step_avg:60.22ms
step:602/2270 train_time:36254ms step_avg:60.22ms
step:603/2270 train_time:36316ms step_avg:60.23ms
step:604/2270 train_time:36375ms step_avg:60.22ms
step:605/2270 train_time:36437ms step_avg:60.23ms
step:606/2270 train_time:36496ms step_avg:60.22ms
step:607/2270 train_time:36558ms step_avg:60.23ms
step:608/2270 train_time:36618ms step_avg:60.23ms
step:609/2270 train_time:36680ms step_avg:60.23ms
step:610/2270 train_time:36739ms step_avg:60.23ms
step:611/2270 train_time:36800ms step_avg:60.23ms
step:612/2270 train_time:36859ms step_avg:60.23ms
step:613/2270 train_time:36920ms step_avg:60.23ms
step:614/2270 train_time:36980ms step_avg:60.23ms
step:615/2270 train_time:37041ms step_avg:60.23ms
step:616/2270 train_time:37100ms step_avg:60.23ms
step:617/2270 train_time:37163ms step_avg:60.23ms
step:618/2270 train_time:37221ms step_avg:60.23ms
step:619/2270 train_time:37284ms step_avg:60.23ms
step:620/2270 train_time:37343ms step_avg:60.23ms
step:621/2270 train_time:37406ms step_avg:60.23ms
step:622/2270 train_time:37465ms step_avg:60.23ms
step:623/2270 train_time:37528ms step_avg:60.24ms
step:624/2270 train_time:37587ms step_avg:60.24ms
step:625/2270 train_time:37650ms step_avg:60.24ms
step:626/2270 train_time:37709ms step_avg:60.24ms
step:627/2270 train_time:37772ms step_avg:60.24ms
step:628/2270 train_time:37831ms step_avg:60.24ms
step:629/2270 train_time:37892ms step_avg:60.24ms
step:630/2270 train_time:37951ms step_avg:60.24ms
step:631/2270 train_time:38012ms step_avg:60.24ms
step:632/2270 train_time:38072ms step_avg:60.24ms
step:633/2270 train_time:38133ms step_avg:60.24ms
step:634/2270 train_time:38192ms step_avg:60.24ms
step:635/2270 train_time:38254ms step_avg:60.24ms
step:636/2270 train_time:38314ms step_avg:60.24ms
step:637/2270 train_time:38376ms step_avg:60.24ms
step:638/2270 train_time:38435ms step_avg:60.24ms
step:639/2270 train_time:38497ms step_avg:60.25ms
step:640/2270 train_time:38556ms step_avg:60.24ms
step:641/2270 train_time:38618ms step_avg:60.25ms
step:642/2270 train_time:38678ms step_avg:60.25ms
step:643/2270 train_time:38739ms step_avg:60.25ms
step:644/2270 train_time:38799ms step_avg:60.25ms
step:645/2270 train_time:38860ms step_avg:60.25ms
step:646/2270 train_time:38919ms step_avg:60.25ms
step:647/2270 train_time:38980ms step_avg:60.25ms
step:648/2270 train_time:39039ms step_avg:60.25ms
step:649/2270 train_time:39102ms step_avg:60.25ms
step:650/2270 train_time:39160ms step_avg:60.25ms
step:651/2270 train_time:39223ms step_avg:60.25ms
step:652/2270 train_time:39283ms step_avg:60.25ms
step:653/2270 train_time:39345ms step_avg:60.25ms
step:654/2270 train_time:39404ms step_avg:60.25ms
step:655/2270 train_time:39466ms step_avg:60.25ms
step:656/2270 train_time:39525ms step_avg:60.25ms
step:657/2270 train_time:39588ms step_avg:60.25ms
step:658/2270 train_time:39648ms step_avg:60.25ms
step:659/2270 train_time:39711ms step_avg:60.26ms
step:660/2270 train_time:39770ms step_avg:60.26ms
step:661/2270 train_time:39832ms step_avg:60.26ms
step:662/2270 train_time:39891ms step_avg:60.26ms
step:663/2270 train_time:39952ms step_avg:60.26ms
step:664/2270 train_time:40010ms step_avg:60.26ms
step:665/2270 train_time:40072ms step_avg:60.26ms
step:666/2270 train_time:40130ms step_avg:60.26ms
step:667/2270 train_time:40192ms step_avg:60.26ms
step:668/2270 train_time:40251ms step_avg:60.26ms
step:669/2270 train_time:40314ms step_avg:60.26ms
step:670/2270 train_time:40373ms step_avg:60.26ms
step:671/2270 train_time:40435ms step_avg:60.26ms
step:672/2270 train_time:40495ms step_avg:60.26ms
step:673/2270 train_time:40557ms step_avg:60.26ms
step:674/2270 train_time:40617ms step_avg:60.26ms
step:675/2270 train_time:40680ms step_avg:60.27ms
step:676/2270 train_time:40739ms step_avg:60.27ms
step:677/2270 train_time:40801ms step_avg:60.27ms
step:678/2270 train_time:40860ms step_avg:60.27ms
step:679/2270 train_time:40921ms step_avg:60.27ms
step:680/2270 train_time:40980ms step_avg:60.27ms
step:681/2270 train_time:41042ms step_avg:60.27ms
step:682/2270 train_time:41101ms step_avg:60.27ms
step:683/2270 train_time:41163ms step_avg:60.27ms
step:684/2270 train_time:41222ms step_avg:60.27ms
step:685/2270 train_time:41284ms step_avg:60.27ms
step:686/2270 train_time:41344ms step_avg:60.27ms
step:687/2270 train_time:41406ms step_avg:60.27ms
step:688/2270 train_time:41465ms step_avg:60.27ms
step:689/2270 train_time:41528ms step_avg:60.27ms
step:690/2270 train_time:41588ms step_avg:60.27ms
step:691/2270 train_time:41651ms step_avg:60.28ms
step:692/2270 train_time:41709ms step_avg:60.27ms
step:693/2270 train_time:41772ms step_avg:60.28ms
step:694/2270 train_time:41831ms step_avg:60.28ms
step:695/2270 train_time:41892ms step_avg:60.28ms
step:696/2270 train_time:41951ms step_avg:60.27ms
step:697/2270 train_time:42012ms step_avg:60.28ms
step:698/2270 train_time:42071ms step_avg:60.27ms
step:699/2270 train_time:42133ms step_avg:60.28ms
step:700/2270 train_time:42192ms step_avg:60.27ms
step:701/2270 train_time:42254ms step_avg:60.28ms
step:702/2270 train_time:42313ms step_avg:60.27ms
step:703/2270 train_time:42375ms step_avg:60.28ms
step:704/2270 train_time:42435ms step_avg:60.28ms
step:705/2270 train_time:42498ms step_avg:60.28ms
step:706/2270 train_time:42558ms step_avg:60.28ms
step:707/2270 train_time:42619ms step_avg:60.28ms
step:708/2270 train_time:42679ms step_avg:60.28ms
step:709/2270 train_time:42741ms step_avg:60.28ms
step:710/2270 train_time:42800ms step_avg:60.28ms
step:711/2270 train_time:42862ms step_avg:60.28ms
step:712/2270 train_time:42921ms step_avg:60.28ms
step:713/2270 train_time:42983ms step_avg:60.28ms
step:714/2270 train_time:43042ms step_avg:60.28ms
step:715/2270 train_time:43104ms step_avg:60.29ms
step:716/2270 train_time:43164ms step_avg:60.28ms
step:717/2270 train_time:43226ms step_avg:60.29ms
step:718/2270 train_time:43286ms step_avg:60.29ms
step:719/2270 train_time:43348ms step_avg:60.29ms
step:720/2270 train_time:43407ms step_avg:60.29ms
step:721/2270 train_time:43469ms step_avg:60.29ms
step:722/2270 train_time:43529ms step_avg:60.29ms
step:723/2270 train_time:43591ms step_avg:60.29ms
step:724/2270 train_time:43650ms step_avg:60.29ms
step:725/2270 train_time:43712ms step_avg:60.29ms
step:726/2270 train_time:43771ms step_avg:60.29ms
step:727/2270 train_time:43833ms step_avg:60.29ms
step:728/2270 train_time:43891ms step_avg:60.29ms
step:729/2270 train_time:43953ms step_avg:60.29ms
step:730/2270 train_time:44012ms step_avg:60.29ms
step:731/2270 train_time:44073ms step_avg:60.29ms
step:732/2270 train_time:44132ms step_avg:60.29ms
step:733/2270 train_time:44194ms step_avg:60.29ms
step:734/2270 train_time:44253ms step_avg:60.29ms
step:735/2270 train_time:44316ms step_avg:60.29ms
step:736/2270 train_time:44375ms step_avg:60.29ms
step:737/2270 train_time:44436ms step_avg:60.29ms
step:738/2270 train_time:44496ms step_avg:60.29ms
step:739/2270 train_time:44558ms step_avg:60.29ms
step:740/2270 train_time:44617ms step_avg:60.29ms
step:741/2270 train_time:44679ms step_avg:60.30ms
step:742/2270 train_time:44738ms step_avg:60.29ms
step:743/2270 train_time:44800ms step_avg:60.30ms
step:744/2270 train_time:44859ms step_avg:60.29ms
step:745/2270 train_time:44921ms step_avg:60.30ms
step:746/2270 train_time:44980ms step_avg:60.29ms
step:747/2270 train_time:45041ms step_avg:60.30ms
step:748/2270 train_time:45100ms step_avg:60.29ms
step:749/2270 train_time:45162ms step_avg:60.30ms
step:750/2270 train_time:45221ms step_avg:60.30ms
step:750/2270 val_loss:3.6586 train_time:45285ms step_avg:60.38ms
step:751/2270 train_time:45310ms step_avg:60.33ms
step:752/2270 train_time:45347ms step_avg:60.30ms
step:753/2270 train_time:45412ms step_avg:60.31ms
step:754/2270 train_time:45472ms step_avg:60.31ms
step:755/2270 train_time:45534ms step_avg:60.31ms
step:756/2270 train_time:45593ms step_avg:60.31ms
step:757/2270 train_time:45654ms step_avg:60.31ms
step:758/2270 train_time:45712ms step_avg:60.31ms
step:759/2270 train_time:45774ms step_avg:60.31ms
step:760/2270 train_time:45833ms step_avg:60.31ms
step:761/2270 train_time:45896ms step_avg:60.31ms
step:762/2270 train_time:45954ms step_avg:60.31ms
step:763/2270 train_time:46015ms step_avg:60.31ms
step:764/2270 train_time:46075ms step_avg:60.31ms
step:765/2270 train_time:46137ms step_avg:60.31ms
step:766/2270 train_time:46197ms step_avg:60.31ms
step:767/2270 train_time:46261ms step_avg:60.31ms
step:768/2270 train_time:46322ms step_avg:60.31ms
step:769/2270 train_time:46386ms step_avg:60.32ms
step:770/2270 train_time:46445ms step_avg:60.32ms
step:771/2270 train_time:46508ms step_avg:60.32ms
step:772/2270 train_time:46568ms step_avg:60.32ms
step:773/2270 train_time:46629ms step_avg:60.32ms
step:774/2270 train_time:46688ms step_avg:60.32ms
step:775/2270 train_time:46750ms step_avg:60.32ms
step:776/2270 train_time:46809ms step_avg:60.32ms
step:777/2270 train_time:46871ms step_avg:60.32ms
step:778/2270 train_time:46930ms step_avg:60.32ms
step:779/2270 train_time:46991ms step_avg:60.32ms
step:780/2270 train_time:47050ms step_avg:60.32ms
step:781/2270 train_time:47112ms step_avg:60.32ms
step:782/2270 train_time:47171ms step_avg:60.32ms
step:783/2270 train_time:47234ms step_avg:60.33ms
step:784/2270 train_time:47295ms step_avg:60.33ms
step:785/2270 train_time:47359ms step_avg:60.33ms
step:786/2270 train_time:47420ms step_avg:60.33ms
step:787/2270 train_time:47482ms step_avg:60.33ms
step:788/2270 train_time:47542ms step_avg:60.33ms
step:789/2270 train_time:47604ms step_avg:60.34ms
step:790/2270 train_time:47664ms step_avg:60.33ms
step:791/2270 train_time:47725ms step_avg:60.34ms
step:792/2270 train_time:47784ms step_avg:60.33ms
step:793/2270 train_time:47846ms step_avg:60.34ms
step:794/2270 train_time:47905ms step_avg:60.33ms
step:795/2270 train_time:47967ms step_avg:60.34ms
step:796/2270 train_time:48027ms step_avg:60.34ms
step:797/2270 train_time:48089ms step_avg:60.34ms
step:798/2270 train_time:48150ms step_avg:60.34ms
step:799/2270 train_time:48211ms step_avg:60.34ms
step:800/2270 train_time:48271ms step_avg:60.34ms
step:801/2270 train_time:48335ms step_avg:60.34ms
step:802/2270 train_time:48396ms step_avg:60.34ms
step:803/2270 train_time:48458ms step_avg:60.35ms
step:804/2270 train_time:48518ms step_avg:60.35ms
step:805/2270 train_time:48580ms step_avg:60.35ms
step:806/2270 train_time:48640ms step_avg:60.35ms
step:807/2270 train_time:48703ms step_avg:60.35ms
step:808/2270 train_time:48762ms step_avg:60.35ms
step:809/2270 train_time:48824ms step_avg:60.35ms
step:810/2270 train_time:48883ms step_avg:60.35ms
step:811/2270 train_time:48945ms step_avg:60.35ms
step:812/2270 train_time:49004ms step_avg:60.35ms
step:813/2270 train_time:49066ms step_avg:60.35ms
step:814/2270 train_time:49126ms step_avg:60.35ms
step:815/2270 train_time:49188ms step_avg:60.35ms
step:816/2270 train_time:49248ms step_avg:60.35ms
step:817/2270 train_time:49310ms step_avg:60.36ms
step:818/2270 train_time:49370ms step_avg:60.35ms
step:819/2270 train_time:49433ms step_avg:60.36ms
step:820/2270 train_time:49492ms step_avg:60.36ms
step:821/2270 train_time:49556ms step_avg:60.36ms
step:822/2270 train_time:49615ms step_avg:60.36ms
step:823/2270 train_time:49677ms step_avg:60.36ms
step:824/2270 train_time:49737ms step_avg:60.36ms
step:825/2270 train_time:49799ms step_avg:60.36ms
step:826/2270 train_time:49859ms step_avg:60.36ms
step:827/2270 train_time:49922ms step_avg:60.36ms
step:828/2270 train_time:49981ms step_avg:60.36ms
step:829/2270 train_time:50043ms step_avg:60.37ms
step:830/2270 train_time:50103ms step_avg:60.36ms
step:831/2270 train_time:50164ms step_avg:60.37ms
step:832/2270 train_time:50223ms step_avg:60.36ms
step:833/2270 train_time:50285ms step_avg:60.37ms
step:834/2270 train_time:50345ms step_avg:60.37ms
step:835/2270 train_time:50407ms step_avg:60.37ms
step:836/2270 train_time:50467ms step_avg:60.37ms
step:837/2270 train_time:50529ms step_avg:60.37ms
step:838/2270 train_time:50589ms step_avg:60.37ms
step:839/2270 train_time:50651ms step_avg:60.37ms
step:840/2270 train_time:50711ms step_avg:60.37ms
step:841/2270 train_time:50773ms step_avg:60.37ms
step:842/2270 train_time:50833ms step_avg:60.37ms
step:843/2270 train_time:50895ms step_avg:60.37ms
step:844/2270 train_time:50955ms step_avg:60.37ms
step:845/2270 train_time:51017ms step_avg:60.38ms
step:846/2270 train_time:51077ms step_avg:60.37ms
step:847/2270 train_time:51139ms step_avg:60.38ms
step:848/2270 train_time:51199ms step_avg:60.38ms
step:849/2270 train_time:51262ms step_avg:60.38ms
step:850/2270 train_time:51322ms step_avg:60.38ms
step:851/2270 train_time:51385ms step_avg:60.38ms
step:852/2270 train_time:51443ms step_avg:60.38ms
step:853/2270 train_time:51505ms step_avg:60.38ms
step:854/2270 train_time:51565ms step_avg:60.38ms
step:855/2270 train_time:51627ms step_avg:60.38ms
step:856/2270 train_time:51687ms step_avg:60.38ms
step:857/2270 train_time:51748ms step_avg:60.38ms
step:858/2270 train_time:51808ms step_avg:60.38ms
step:859/2270 train_time:51870ms step_avg:60.38ms
step:860/2270 train_time:51930ms step_avg:60.38ms
step:861/2270 train_time:51992ms step_avg:60.39ms
step:862/2270 train_time:52051ms step_avg:60.38ms
step:863/2270 train_time:52114ms step_avg:60.39ms
step:864/2270 train_time:52173ms step_avg:60.39ms
step:865/2270 train_time:52235ms step_avg:60.39ms
step:866/2270 train_time:52295ms step_avg:60.39ms
step:867/2270 train_time:52357ms step_avg:60.39ms
step:868/2270 train_time:52417ms step_avg:60.39ms
step:869/2270 train_time:52479ms step_avg:60.39ms
step:870/2270 train_time:52539ms step_avg:60.39ms
step:871/2270 train_time:52601ms step_avg:60.39ms
step:872/2270 train_time:52661ms step_avg:60.39ms
step:873/2270 train_time:52723ms step_avg:60.39ms
step:874/2270 train_time:52782ms step_avg:60.39ms
step:875/2270 train_time:52844ms step_avg:60.39ms
step:876/2270 train_time:52903ms step_avg:60.39ms
step:877/2270 train_time:52965ms step_avg:60.39ms
step:878/2270 train_time:53024ms step_avg:60.39ms
step:879/2270 train_time:53086ms step_avg:60.39ms
step:880/2270 train_time:53146ms step_avg:60.39ms
step:881/2270 train_time:53208ms step_avg:60.39ms
step:882/2270 train_time:53267ms step_avg:60.39ms
step:883/2270 train_time:53329ms step_avg:60.40ms
step:884/2270 train_time:53389ms step_avg:60.39ms
step:885/2270 train_time:53451ms step_avg:60.40ms
step:886/2270 train_time:53511ms step_avg:60.40ms
step:887/2270 train_time:53573ms step_avg:60.40ms
step:888/2270 train_time:53632ms step_avg:60.40ms
step:889/2270 train_time:53695ms step_avg:60.40ms
step:890/2270 train_time:53754ms step_avg:60.40ms
step:891/2270 train_time:53817ms step_avg:60.40ms
step:892/2270 train_time:53876ms step_avg:60.40ms
step:893/2270 train_time:53939ms step_avg:60.40ms
step:894/2270 train_time:53998ms step_avg:60.40ms
step:895/2270 train_time:54060ms step_avg:60.40ms
step:896/2270 train_time:54120ms step_avg:60.40ms
step:897/2270 train_time:54183ms step_avg:60.40ms
step:898/2270 train_time:54242ms step_avg:60.40ms
step:899/2270 train_time:54304ms step_avg:60.41ms
step:900/2270 train_time:54364ms step_avg:60.40ms
step:901/2270 train_time:54426ms step_avg:60.41ms
step:902/2270 train_time:54485ms step_avg:60.40ms
step:903/2270 train_time:54547ms step_avg:60.41ms
step:904/2270 train_time:54606ms step_avg:60.41ms
step:905/2270 train_time:54667ms step_avg:60.41ms
step:906/2270 train_time:54727ms step_avg:60.41ms
step:907/2270 train_time:54789ms step_avg:60.41ms
step:908/2270 train_time:54849ms step_avg:60.41ms
step:909/2270 train_time:54911ms step_avg:60.41ms
step:910/2270 train_time:54971ms step_avg:60.41ms
step:911/2270 train_time:55033ms step_avg:60.41ms
step:912/2270 train_time:55093ms step_avg:60.41ms
step:913/2270 train_time:55156ms step_avg:60.41ms
step:914/2270 train_time:55215ms step_avg:60.41ms
step:915/2270 train_time:55277ms step_avg:60.41ms
step:916/2270 train_time:55337ms step_avg:60.41ms
step:917/2270 train_time:55400ms step_avg:60.41ms
step:918/2270 train_time:55460ms step_avg:60.41ms
step:919/2270 train_time:55522ms step_avg:60.42ms
step:920/2270 train_time:55581ms step_avg:60.41ms
step:921/2270 train_time:55644ms step_avg:60.42ms
step:922/2270 train_time:55703ms step_avg:60.42ms
step:923/2270 train_time:55766ms step_avg:60.42ms
step:924/2270 train_time:55825ms step_avg:60.42ms
step:925/2270 train_time:55887ms step_avg:60.42ms
step:926/2270 train_time:55946ms step_avg:60.42ms
step:927/2270 train_time:56009ms step_avg:60.42ms
step:928/2270 train_time:56068ms step_avg:60.42ms
step:929/2270 train_time:56130ms step_avg:60.42ms
step:930/2270 train_time:56190ms step_avg:60.42ms
step:931/2270 train_time:56253ms step_avg:60.42ms
step:932/2270 train_time:56312ms step_avg:60.42ms
step:933/2270 train_time:56374ms step_avg:60.42ms
step:934/2270 train_time:56434ms step_avg:60.42ms
step:935/2270 train_time:56497ms step_avg:60.42ms
step:936/2270 train_time:56556ms step_avg:60.42ms
step:937/2270 train_time:56618ms step_avg:60.42ms
step:938/2270 train_time:56677ms step_avg:60.42ms
step:939/2270 train_time:56740ms step_avg:60.43ms
step:940/2270 train_time:56800ms step_avg:60.43ms
step:941/2270 train_time:56862ms step_avg:60.43ms
step:942/2270 train_time:56922ms step_avg:60.43ms
step:943/2270 train_time:56985ms step_avg:60.43ms
step:944/2270 train_time:57044ms step_avg:60.43ms
step:945/2270 train_time:57106ms step_avg:60.43ms
step:946/2270 train_time:57165ms step_avg:60.43ms
step:947/2270 train_time:57227ms step_avg:60.43ms
step:948/2270 train_time:57288ms step_avg:60.43ms
step:949/2270 train_time:57349ms step_avg:60.43ms
step:950/2270 train_time:57409ms step_avg:60.43ms
step:951/2270 train_time:57471ms step_avg:60.43ms
step:952/2270 train_time:57531ms step_avg:60.43ms
step:953/2270 train_time:57593ms step_avg:60.43ms
step:954/2270 train_time:57652ms step_avg:60.43ms
step:955/2270 train_time:57714ms step_avg:60.43ms
step:956/2270 train_time:57774ms step_avg:60.43ms
step:957/2270 train_time:57836ms step_avg:60.44ms
step:958/2270 train_time:57896ms step_avg:60.43ms
step:959/2270 train_time:57959ms step_avg:60.44ms
step:960/2270 train_time:58019ms step_avg:60.44ms
step:961/2270 train_time:58081ms step_avg:60.44ms
step:962/2270 train_time:58141ms step_avg:60.44ms
step:963/2270 train_time:58203ms step_avg:60.44ms
step:964/2270 train_time:58263ms step_avg:60.44ms
step:965/2270 train_time:58325ms step_avg:60.44ms
step:966/2270 train_time:58385ms step_avg:60.44ms
step:967/2270 train_time:58446ms step_avg:60.44ms
step:968/2270 train_time:58506ms step_avg:60.44ms
step:969/2270 train_time:58568ms step_avg:60.44ms
step:970/2270 train_time:58628ms step_avg:60.44ms
step:971/2270 train_time:58690ms step_avg:60.44ms
step:972/2270 train_time:58750ms step_avg:60.44ms
step:973/2270 train_time:58812ms step_avg:60.44ms
step:974/2270 train_time:58871ms step_avg:60.44ms
step:975/2270 train_time:58933ms step_avg:60.44ms
step:976/2270 train_time:58993ms step_avg:60.44ms
step:977/2270 train_time:59055ms step_avg:60.45ms
step:978/2270 train_time:59115ms step_avg:60.44ms
step:979/2270 train_time:59177ms step_avg:60.45ms
step:980/2270 train_time:59237ms step_avg:60.45ms
step:981/2270 train_time:59299ms step_avg:60.45ms
step:982/2270 train_time:59359ms step_avg:60.45ms
step:983/2270 train_time:59421ms step_avg:60.45ms
step:984/2270 train_time:59482ms step_avg:60.45ms
step:985/2270 train_time:59544ms step_avg:60.45ms
step:986/2270 train_time:59604ms step_avg:60.45ms
step:987/2270 train_time:59666ms step_avg:60.45ms
step:988/2270 train_time:59725ms step_avg:60.45ms
step:989/2270 train_time:59786ms step_avg:60.45ms
step:990/2270 train_time:59846ms step_avg:60.45ms
step:991/2270 train_time:59908ms step_avg:60.45ms
step:992/2270 train_time:59968ms step_avg:60.45ms
step:993/2270 train_time:60031ms step_avg:60.45ms
step:994/2270 train_time:60090ms step_avg:60.45ms
step:995/2270 train_time:60153ms step_avg:60.45ms
step:996/2270 train_time:60212ms step_avg:60.45ms
step:997/2270 train_time:60274ms step_avg:60.46ms
step:998/2270 train_time:60333ms step_avg:60.45ms
step:999/2270 train_time:60396ms step_avg:60.46ms
step:1000/2270 train_time:60456ms step_avg:60.46ms
step:1000/2270 val_loss:3.5731 train_time:60519ms step_avg:60.52ms
step:1001/2270 train_time:60544ms step_avg:60.48ms
step:1002/2270 train_time:60580ms step_avg:60.46ms
step:1003/2270 train_time:60642ms step_avg:60.46ms
step:1004/2270 train_time:60703ms step_avg:60.46ms
step:1005/2270 train_time:60766ms step_avg:60.46ms
step:1006/2270 train_time:60826ms step_avg:60.46ms
step:1007/2270 train_time:60888ms step_avg:60.46ms
step:1008/2270 train_time:60946ms step_avg:60.46ms
step:1009/2270 train_time:61007ms step_avg:60.46ms
step:1010/2270 train_time:61066ms step_avg:60.46ms
step:1011/2270 train_time:61127ms step_avg:60.46ms
step:1012/2270 train_time:61186ms step_avg:60.46ms
step:1013/2270 train_time:61248ms step_avg:60.46ms
step:1014/2270 train_time:61307ms step_avg:60.46ms
step:1015/2270 train_time:61369ms step_avg:60.46ms
step:1016/2270 train_time:61432ms step_avg:60.46ms
step:1017/2270 train_time:61498ms step_avg:60.47ms
step:1018/2270 train_time:61557ms step_avg:60.47ms
step:1019/2270 train_time:61620ms step_avg:60.47ms
step:1020/2270 train_time:61680ms step_avg:60.47ms
step:1021/2270 train_time:61742ms step_avg:60.47ms
step:1022/2270 train_time:61802ms step_avg:60.47ms
step:1023/2270 train_time:61863ms step_avg:60.47ms
step:1024/2270 train_time:61922ms step_avg:60.47ms
step:1025/2270 train_time:61984ms step_avg:60.47ms
step:1026/2270 train_time:62044ms step_avg:60.47ms
step:1027/2270 train_time:62105ms step_avg:60.47ms
step:1028/2270 train_time:62163ms step_avg:60.47ms
step:1029/2270 train_time:62225ms step_avg:60.47ms
step:1030/2270 train_time:62284ms step_avg:60.47ms
step:1031/2270 train_time:62346ms step_avg:60.47ms
step:1032/2270 train_time:62407ms step_avg:60.47ms
step:1033/2270 train_time:62471ms step_avg:60.48ms
step:1034/2270 train_time:62531ms step_avg:60.47ms
step:1035/2270 train_time:62593ms step_avg:60.48ms
step:1036/2270 train_time:62653ms step_avg:60.48ms
step:1037/2270 train_time:62716ms step_avg:60.48ms
step:1038/2270 train_time:62777ms step_avg:60.48ms
step:1039/2270 train_time:62838ms step_avg:60.48ms
step:1040/2270 train_time:62897ms step_avg:60.48ms
step:1041/2270 train_time:62959ms step_avg:60.48ms
step:1042/2270 train_time:63018ms step_avg:60.48ms
step:1043/2270 train_time:63080ms step_avg:60.48ms
step:1044/2270 train_time:63138ms step_avg:60.48ms
step:1045/2270 train_time:63200ms step_avg:60.48ms
step:1046/2270 train_time:63259ms step_avg:60.48ms
step:1047/2270 train_time:63321ms step_avg:60.48ms
step:1048/2270 train_time:63381ms step_avg:60.48ms
step:1049/2270 train_time:63444ms step_avg:60.48ms
step:1050/2270 train_time:63504ms step_avg:60.48ms
step:1051/2270 train_time:63567ms step_avg:60.48ms
step:1052/2270 train_time:63626ms step_avg:60.48ms
step:1053/2270 train_time:63689ms step_avg:60.48ms
step:1054/2270 train_time:63749ms step_avg:60.48ms
step:1055/2270 train_time:63811ms step_avg:60.48ms
step:1056/2270 train_time:63871ms step_avg:60.48ms
step:1057/2270 train_time:63933ms step_avg:60.48ms
step:1058/2270 train_time:63992ms step_avg:60.48ms
step:1059/2270 train_time:64054ms step_avg:60.49ms
step:1060/2270 train_time:64114ms step_avg:60.48ms
step:1061/2270 train_time:64176ms step_avg:60.49ms
step:1062/2270 train_time:64235ms step_avg:60.49ms
step:1063/2270 train_time:64297ms step_avg:60.49ms
step:1064/2270 train_time:64357ms step_avg:60.49ms
step:1065/2270 train_time:64419ms step_avg:60.49ms
step:1066/2270 train_time:64478ms step_avg:60.49ms
step:1067/2270 train_time:64540ms step_avg:60.49ms
step:1068/2270 train_time:64600ms step_avg:60.49ms
step:1069/2270 train_time:64663ms step_avg:60.49ms
step:1070/2270 train_time:64723ms step_avg:60.49ms
step:1071/2270 train_time:64785ms step_avg:60.49ms
step:1072/2270 train_time:64845ms step_avg:60.49ms
step:1073/2270 train_time:64907ms step_avg:60.49ms
step:1074/2270 train_time:64966ms step_avg:60.49ms
step:1075/2270 train_time:65028ms step_avg:60.49ms
step:1076/2270 train_time:65087ms step_avg:60.49ms
step:1077/2270 train_time:65149ms step_avg:60.49ms
step:1078/2270 train_time:65209ms step_avg:60.49ms
step:1079/2270 train_time:65272ms step_avg:60.49ms
step:1080/2270 train_time:65331ms step_avg:60.49ms
step:1081/2270 train_time:65394ms step_avg:60.49ms
step:1082/2270 train_time:65454ms step_avg:60.49ms
step:1083/2270 train_time:65516ms step_avg:60.50ms
step:1084/2270 train_time:65576ms step_avg:60.49ms
step:1085/2270 train_time:65639ms step_avg:60.50ms
step:1086/2270 train_time:65698ms step_avg:60.50ms
step:1087/2270 train_time:65760ms step_avg:60.50ms
step:1088/2270 train_time:65819ms step_avg:60.50ms
step:1089/2270 train_time:65881ms step_avg:60.50ms
step:1090/2270 train_time:65940ms step_avg:60.50ms
step:1091/2270 train_time:66002ms step_avg:60.50ms
step:1092/2270 train_time:66062ms step_avg:60.50ms
step:1093/2270 train_time:66124ms step_avg:60.50ms
step:1094/2270 train_time:66184ms step_avg:60.50ms
step:1095/2270 train_time:66246ms step_avg:60.50ms
step:1096/2270 train_time:66306ms step_avg:60.50ms
step:1097/2270 train_time:66368ms step_avg:60.50ms
step:1098/2270 train_time:66428ms step_avg:60.50ms
step:1099/2270 train_time:66490ms step_avg:60.50ms
step:1100/2270 train_time:66550ms step_avg:60.50ms
step:1101/2270 train_time:66613ms step_avg:60.50ms
step:1102/2270 train_time:66672ms step_avg:60.50ms
step:1103/2270 train_time:66735ms step_avg:60.50ms
step:1104/2270 train_time:66795ms step_avg:60.50ms
step:1105/2270 train_time:66858ms step_avg:60.50ms
step:1106/2270 train_time:66917ms step_avg:60.50ms
step:1107/2270 train_time:66979ms step_avg:60.51ms
step:1108/2270 train_time:67039ms step_avg:60.50ms
step:1109/2270 train_time:67101ms step_avg:60.51ms
step:1110/2270 train_time:67161ms step_avg:60.51ms
step:1111/2270 train_time:67224ms step_avg:60.51ms
step:1112/2270 train_time:67284ms step_avg:60.51ms
step:1113/2270 train_time:67346ms step_avg:60.51ms
step:1114/2270 train_time:67405ms step_avg:60.51ms
step:1115/2270 train_time:67468ms step_avg:60.51ms
step:1116/2270 train_time:67527ms step_avg:60.51ms
step:1117/2270 train_time:67590ms step_avg:60.51ms
step:1118/2270 train_time:67649ms step_avg:60.51ms
step:1119/2270 train_time:67712ms step_avg:60.51ms
step:1120/2270 train_time:67771ms step_avg:60.51ms
step:1121/2270 train_time:67834ms step_avg:60.51ms
step:1122/2270 train_time:67895ms step_avg:60.51ms
step:1123/2270 train_time:67957ms step_avg:60.51ms
step:1124/2270 train_time:68017ms step_avg:60.51ms
step:1125/2270 train_time:68079ms step_avg:60.51ms
step:1126/2270 train_time:68138ms step_avg:60.51ms
step:1127/2270 train_time:68201ms step_avg:60.52ms
step:1128/2270 train_time:68260ms step_avg:60.51ms
step:1129/2270 train_time:68322ms step_avg:60.52ms
step:1130/2270 train_time:68381ms step_avg:60.51ms
step:1131/2270 train_time:68443ms step_avg:60.52ms
step:1132/2270 train_time:68503ms step_avg:60.52ms
step:1133/2270 train_time:68566ms step_avg:60.52ms
step:1134/2270 train_time:68626ms step_avg:60.52ms
step:1135/2270 train_time:68688ms step_avg:60.52ms
step:1136/2270 train_time:68748ms step_avg:60.52ms
step:1137/2270 train_time:68811ms step_avg:60.52ms
step:1138/2270 train_time:68870ms step_avg:60.52ms
step:1139/2270 train_time:68934ms step_avg:60.52ms
step:1140/2270 train_time:68993ms step_avg:60.52ms
step:1141/2270 train_time:69056ms step_avg:60.52ms
step:1142/2270 train_time:69115ms step_avg:60.52ms
step:1143/2270 train_time:69178ms step_avg:60.52ms
step:1144/2270 train_time:69238ms step_avg:60.52ms
step:1145/2270 train_time:69300ms step_avg:60.52ms
step:1146/2270 train_time:69360ms step_avg:60.52ms
step:1147/2270 train_time:69422ms step_avg:60.52ms
step:1148/2270 train_time:69481ms step_avg:60.52ms
step:1149/2270 train_time:69544ms step_avg:60.53ms
step:1150/2270 train_time:69604ms step_avg:60.53ms
step:1151/2270 train_time:69667ms step_avg:60.53ms
step:1152/2270 train_time:69727ms step_avg:60.53ms
step:1153/2270 train_time:69789ms step_avg:60.53ms
step:1154/2270 train_time:69849ms step_avg:60.53ms
step:1155/2270 train_time:69912ms step_avg:60.53ms
step:1156/2270 train_time:69972ms step_avg:60.53ms
step:1157/2270 train_time:70034ms step_avg:60.53ms
step:1158/2270 train_time:70094ms step_avg:60.53ms
step:1159/2270 train_time:70157ms step_avg:60.53ms
step:1160/2270 train_time:70217ms step_avg:60.53ms
step:1161/2270 train_time:70280ms step_avg:60.53ms
step:1162/2270 train_time:70340ms step_avg:60.53ms
step:1163/2270 train_time:70402ms step_avg:60.53ms
step:1164/2270 train_time:70462ms step_avg:60.53ms
step:1165/2270 train_time:70525ms step_avg:60.54ms
step:1166/2270 train_time:70585ms step_avg:60.54ms
step:1167/2270 train_time:70647ms step_avg:60.54ms
step:1168/2270 train_time:70707ms step_avg:60.54ms
step:1169/2270 train_time:70769ms step_avg:60.54ms
step:1170/2270 train_time:70829ms step_avg:60.54ms
step:1171/2270 train_time:70891ms step_avg:60.54ms
step:1172/2270 train_time:70952ms step_avg:60.54ms
step:1173/2270 train_time:71014ms step_avg:60.54ms
step:1174/2270 train_time:71074ms step_avg:60.54ms
step:1175/2270 train_time:71137ms step_avg:60.54ms
step:1176/2270 train_time:71197ms step_avg:60.54ms
step:1177/2270 train_time:71259ms step_avg:60.54ms
step:1178/2270 train_time:71318ms step_avg:60.54ms
step:1179/2270 train_time:71381ms step_avg:60.54ms
step:1180/2270 train_time:71441ms step_avg:60.54ms
step:1181/2270 train_time:71503ms step_avg:60.54ms
step:1182/2270 train_time:71563ms step_avg:60.54ms
step:1183/2270 train_time:71625ms step_avg:60.55ms
step:1184/2270 train_time:71685ms step_avg:60.55ms
step:1185/2270 train_time:71748ms step_avg:60.55ms
step:1186/2270 train_time:71807ms step_avg:60.55ms
step:1187/2270 train_time:71871ms step_avg:60.55ms
step:1188/2270 train_time:71930ms step_avg:60.55ms
step:1189/2270 train_time:71993ms step_avg:60.55ms
step:1190/2270 train_time:72052ms step_avg:60.55ms
step:1191/2270 train_time:72115ms step_avg:60.55ms
step:1192/2270 train_time:72175ms step_avg:60.55ms
step:1193/2270 train_time:72238ms step_avg:60.55ms
step:1194/2270 train_time:72298ms step_avg:60.55ms
step:1195/2270 train_time:72360ms step_avg:60.55ms
step:1196/2270 train_time:72419ms step_avg:60.55ms
step:1197/2270 train_time:72481ms step_avg:60.55ms
step:1198/2270 train_time:72541ms step_avg:60.55ms
step:1199/2270 train_time:72603ms step_avg:60.55ms
step:1200/2270 train_time:72663ms step_avg:60.55ms
step:1201/2270 train_time:72726ms step_avg:60.55ms
step:1202/2270 train_time:72786ms step_avg:60.55ms
step:1203/2270 train_time:72849ms step_avg:60.56ms
step:1204/2270 train_time:72908ms step_avg:60.56ms
step:1205/2270 train_time:72971ms step_avg:60.56ms
step:1206/2270 train_time:73031ms step_avg:60.56ms
step:1207/2270 train_time:73093ms step_avg:60.56ms
step:1208/2270 train_time:73153ms step_avg:60.56ms
step:1209/2270 train_time:73216ms step_avg:60.56ms
step:1210/2270 train_time:73275ms step_avg:60.56ms
step:1211/2270 train_time:73339ms step_avg:60.56ms
step:1212/2270 train_time:73398ms step_avg:60.56ms
step:1213/2270 train_time:73461ms step_avg:60.56ms
step:1214/2270 train_time:73521ms step_avg:60.56ms
step:1215/2270 train_time:73582ms step_avg:60.56ms
step:1216/2270 train_time:73642ms step_avg:60.56ms
step:1217/2270 train_time:73705ms step_avg:60.56ms
step:1218/2270 train_time:73764ms step_avg:60.56ms
step:1219/2270 train_time:73827ms step_avg:60.56ms
step:1220/2270 train_time:73887ms step_avg:60.56ms
step:1221/2270 train_time:73949ms step_avg:60.56ms
step:1222/2270 train_time:74009ms step_avg:60.56ms
step:1223/2270 train_time:74073ms step_avg:60.57ms
step:1224/2270 train_time:74133ms step_avg:60.57ms
step:1225/2270 train_time:74195ms step_avg:60.57ms
step:1226/2270 train_time:74255ms step_avg:60.57ms
step:1227/2270 train_time:74318ms step_avg:60.57ms
step:1228/2270 train_time:74377ms step_avg:60.57ms
step:1229/2270 train_time:74440ms step_avg:60.57ms
step:1230/2270 train_time:74500ms step_avg:60.57ms
step:1231/2270 train_time:74562ms step_avg:60.57ms
step:1232/2270 train_time:74621ms step_avg:60.57ms
step:1233/2270 train_time:74683ms step_avg:60.57ms
step:1234/2270 train_time:74743ms step_avg:60.57ms
step:1235/2270 train_time:74805ms step_avg:60.57ms
step:1236/2270 train_time:74865ms step_avg:60.57ms
step:1237/2270 train_time:74927ms step_avg:60.57ms
step:1238/2270 train_time:74987ms step_avg:60.57ms
step:1239/2270 train_time:75051ms step_avg:60.57ms
step:1240/2270 train_time:75110ms step_avg:60.57ms
step:1241/2270 train_time:75173ms step_avg:60.57ms
step:1242/2270 train_time:75233ms step_avg:60.57ms
step:1243/2270 train_time:75296ms step_avg:60.58ms
step:1244/2270 train_time:75357ms step_avg:60.58ms
step:1245/2270 train_time:75420ms step_avg:60.58ms
step:1246/2270 train_time:75479ms step_avg:60.58ms
step:1247/2270 train_time:75542ms step_avg:60.58ms
step:1248/2270 train_time:75601ms step_avg:60.58ms
step:1249/2270 train_time:75664ms step_avg:60.58ms
step:1250/2270 train_time:75723ms step_avg:60.58ms
step:1250/2270 val_loss:3.5012 train_time:75787ms step_avg:60.63ms
step:1251/2270 train_time:75812ms step_avg:60.60ms
step:1252/2270 train_time:75848ms step_avg:60.58ms
step:1253/2270 train_time:75909ms step_avg:60.58ms
step:1254/2270 train_time:75968ms step_avg:60.58ms
step:1255/2270 train_time:76030ms step_avg:60.58ms
step:1256/2270 train_time:76089ms step_avg:60.58ms
step:1257/2270 train_time:76151ms step_avg:60.58ms
step:1258/2270 train_time:76210ms step_avg:60.58ms
step:1259/2270 train_time:76271ms step_avg:60.58ms
step:1260/2270 train_time:76330ms step_avg:60.58ms
step:1261/2270 train_time:76391ms step_avg:60.58ms
step:1262/2270 train_time:76449ms step_avg:60.58ms
step:1263/2270 train_time:76511ms step_avg:60.58ms
step:1264/2270 train_time:76570ms step_avg:60.58ms
step:1265/2270 train_time:76631ms step_avg:60.58ms
step:1266/2270 train_time:76695ms step_avg:60.58ms
step:1267/2270 train_time:76763ms step_avg:60.59ms
step:1268/2270 train_time:76824ms step_avg:60.59ms
step:1269/2270 train_time:76887ms step_avg:60.59ms
step:1270/2270 train_time:76947ms step_avg:60.59ms
step:1271/2270 train_time:77009ms step_avg:60.59ms
step:1272/2270 train_time:77068ms step_avg:60.59ms
step:1273/2270 train_time:77129ms step_avg:60.59ms
step:1274/2270 train_time:77188ms step_avg:60.59ms
step:1275/2270 train_time:77250ms step_avg:60.59ms
step:1276/2270 train_time:77309ms step_avg:60.59ms
step:1277/2270 train_time:77370ms step_avg:60.59ms
step:1278/2270 train_time:77429ms step_avg:60.59ms
step:1279/2270 train_time:77490ms step_avg:60.59ms
step:1280/2270 train_time:77549ms step_avg:60.59ms
step:1281/2270 train_time:77612ms step_avg:60.59ms
step:1282/2270 train_time:77674ms step_avg:60.59ms
step:1283/2270 train_time:77738ms step_avg:60.59ms
step:1284/2270 train_time:77799ms step_avg:60.59ms
step:1285/2270 train_time:77861ms step_avg:60.59ms
step:1286/2270 train_time:77921ms step_avg:60.59ms
step:1287/2270 train_time:77984ms step_avg:60.59ms
step:1288/2270 train_time:78043ms step_avg:60.59ms
step:1289/2270 train_time:78106ms step_avg:60.59ms
step:1290/2270 train_time:78165ms step_avg:60.59ms
step:1291/2270 train_time:78227ms step_avg:60.59ms
step:1292/2270 train_time:78286ms step_avg:60.59ms
step:1293/2270 train_time:78348ms step_avg:60.59ms
step:1294/2270 train_time:78407ms step_avg:60.59ms
step:1295/2270 train_time:78468ms step_avg:60.59ms
step:1296/2270 train_time:78528ms step_avg:60.59ms
step:1297/2270 train_time:78590ms step_avg:60.59ms
step:1298/2270 train_time:78650ms step_avg:60.59ms
step:1299/2270 train_time:78713ms step_avg:60.60ms
step:1300/2270 train_time:78773ms step_avg:60.59ms
step:1301/2270 train_time:78837ms step_avg:60.60ms
step:1302/2270 train_time:78897ms step_avg:60.60ms
step:1303/2270 train_time:78959ms step_avg:60.60ms
step:1304/2270 train_time:79019ms step_avg:60.60ms
step:1305/2270 train_time:79082ms step_avg:60.60ms
step:1306/2270 train_time:79141ms step_avg:60.60ms
step:1307/2270 train_time:79203ms step_avg:60.60ms
step:1308/2270 train_time:79262ms step_avg:60.60ms
step:1309/2270 train_time:79325ms step_avg:60.60ms
step:1310/2270 train_time:79384ms step_avg:60.60ms
step:1311/2270 train_time:79446ms step_avg:60.60ms
step:1312/2270 train_time:79506ms step_avg:60.60ms
step:1313/2270 train_time:79569ms step_avg:60.60ms
step:1314/2270 train_time:79629ms step_avg:60.60ms
step:1315/2270 train_time:79692ms step_avg:60.60ms
step:1316/2270 train_time:79751ms step_avg:60.60ms
step:1317/2270 train_time:79814ms step_avg:60.60ms
step:1318/2270 train_time:79874ms step_avg:60.60ms
step:1319/2270 train_time:79936ms step_avg:60.60ms
step:1320/2270 train_time:79996ms step_avg:60.60ms
step:1321/2270 train_time:80058ms step_avg:60.60ms
step:1322/2270 train_time:80118ms step_avg:60.60ms
step:1323/2270 train_time:80180ms step_avg:60.60ms
step:1324/2270 train_time:80240ms step_avg:60.60ms
step:1325/2270 train_time:80302ms step_avg:60.61ms
step:1326/2270 train_time:80361ms step_avg:60.60ms
step:1327/2270 train_time:80424ms step_avg:60.61ms
step:1328/2270 train_time:80484ms step_avg:60.61ms
step:1329/2270 train_time:80546ms step_avg:60.61ms
step:1330/2270 train_time:80606ms step_avg:60.61ms
step:1331/2270 train_time:80669ms step_avg:60.61ms
step:1332/2270 train_time:80729ms step_avg:60.61ms
step:1333/2270 train_time:80792ms step_avg:60.61ms
step:1334/2270 train_time:80852ms step_avg:60.61ms
step:1335/2270 train_time:80914ms step_avg:60.61ms
step:1336/2270 train_time:80974ms step_avg:60.61ms
step:1337/2270 train_time:81036ms step_avg:60.61ms
step:1338/2270 train_time:81096ms step_avg:60.61ms
step:1339/2270 train_time:81158ms step_avg:60.61ms
step:1340/2270 train_time:81218ms step_avg:60.61ms
step:1341/2270 train_time:81280ms step_avg:60.61ms
step:1342/2270 train_time:81339ms step_avg:60.61ms
step:1343/2270 train_time:81402ms step_avg:60.61ms
step:1344/2270 train_time:81461ms step_avg:60.61ms
step:1345/2270 train_time:81524ms step_avg:60.61ms
step:1346/2270 train_time:81584ms step_avg:60.61ms
step:1347/2270 train_time:81648ms step_avg:60.61ms
step:1348/2270 train_time:81707ms step_avg:60.61ms
step:1349/2270 train_time:81771ms step_avg:60.62ms
step:1350/2270 train_time:81830ms step_avg:60.62ms
step:1351/2270 train_time:81893ms step_avg:60.62ms
step:1352/2270 train_time:81952ms step_avg:60.62ms
step:1353/2270 train_time:82014ms step_avg:60.62ms
step:1354/2270 train_time:82074ms step_avg:60.62ms
step:1355/2270 train_time:82137ms step_avg:60.62ms
step:1356/2270 train_time:82197ms step_avg:60.62ms
step:1357/2270 train_time:82259ms step_avg:60.62ms
step:1358/2270 train_time:82319ms step_avg:60.62ms
step:1359/2270 train_time:82382ms step_avg:60.62ms
step:1360/2270 train_time:82441ms step_avg:60.62ms
step:1361/2270 train_time:82504ms step_avg:60.62ms
step:1362/2270 train_time:82564ms step_avg:60.62ms
step:1363/2270 train_time:82626ms step_avg:60.62ms
step:1364/2270 train_time:82686ms step_avg:60.62ms
step:1365/2270 train_time:82749ms step_avg:60.62ms
step:1366/2270 train_time:82809ms step_avg:60.62ms
step:1367/2270 train_time:82872ms step_avg:60.62ms
step:1368/2270 train_time:82931ms step_avg:60.62ms
step:1369/2270 train_time:82993ms step_avg:60.62ms
step:1370/2270 train_time:83053ms step_avg:60.62ms
step:1371/2270 train_time:83115ms step_avg:60.62ms
step:1372/2270 train_time:83175ms step_avg:60.62ms
step:1373/2270 train_time:83238ms step_avg:60.62ms
step:1374/2270 train_time:83298ms step_avg:60.62ms
step:1375/2270 train_time:83360ms step_avg:60.63ms
step:1376/2270 train_time:83420ms step_avg:60.63ms
step:1377/2270 train_time:83483ms step_avg:60.63ms
step:1378/2270 train_time:83542ms step_avg:60.63ms
step:1379/2270 train_time:83605ms step_avg:60.63ms
step:1380/2270 train_time:83665ms step_avg:60.63ms
step:1381/2270 train_time:83728ms step_avg:60.63ms
step:1382/2270 train_time:83788ms step_avg:60.63ms
step:1383/2270 train_time:83851ms step_avg:60.63ms
step:1384/2270 train_time:83910ms step_avg:60.63ms
step:1385/2270 train_time:83972ms step_avg:60.63ms
step:1386/2270 train_time:84032ms step_avg:60.63ms
step:1387/2270 train_time:84094ms step_avg:60.63ms
step:1388/2270 train_time:84154ms step_avg:60.63ms
step:1389/2270 train_time:84216ms step_avg:60.63ms
step:1390/2270 train_time:84276ms step_avg:60.63ms
step:1391/2270 train_time:84338ms step_avg:60.63ms
step:1392/2270 train_time:84399ms step_avg:60.63ms
step:1393/2270 train_time:84461ms step_avg:60.63ms
step:1394/2270 train_time:84521ms step_avg:60.63ms
step:1395/2270 train_time:84584ms step_avg:60.63ms
step:1396/2270 train_time:84643ms step_avg:60.63ms
step:1397/2270 train_time:84706ms step_avg:60.63ms
step:1398/2270 train_time:84765ms step_avg:60.63ms
step:1399/2270 train_time:84828ms step_avg:60.63ms
step:1400/2270 train_time:84888ms step_avg:60.63ms
step:1401/2270 train_time:84951ms step_avg:60.64ms
step:1402/2270 train_time:85011ms step_avg:60.64ms
step:1403/2270 train_time:85072ms step_avg:60.64ms
step:1404/2270 train_time:85132ms step_avg:60.64ms
step:1405/2270 train_time:85194ms step_avg:60.64ms
step:1406/2270 train_time:85254ms step_avg:60.64ms
step:1407/2270 train_time:85317ms step_avg:60.64ms
step:1408/2270 train_time:85377ms step_avg:60.64ms
step:1409/2270 train_time:85440ms step_avg:60.64ms
step:1410/2270 train_time:85500ms step_avg:60.64ms
step:1411/2270 train_time:85563ms step_avg:60.64ms
step:1412/2270 train_time:85622ms step_avg:60.64ms
step:1413/2270 train_time:85684ms step_avg:60.64ms
step:1414/2270 train_time:85744ms step_avg:60.64ms
step:1415/2270 train_time:85807ms step_avg:60.64ms
step:1416/2270 train_time:85867ms step_avg:60.64ms
step:1417/2270 train_time:85929ms step_avg:60.64ms
step:1418/2270 train_time:85989ms step_avg:60.64ms
step:1419/2270 train_time:86051ms step_avg:60.64ms
step:1420/2270 train_time:86111ms step_avg:60.64ms
step:1421/2270 train_time:86173ms step_avg:60.64ms
step:1422/2270 train_time:86232ms step_avg:60.64ms
step:1423/2270 train_time:86295ms step_avg:60.64ms
step:1424/2270 train_time:86354ms step_avg:60.64ms
step:1425/2270 train_time:86418ms step_avg:60.64ms
step:1426/2270 train_time:86478ms step_avg:60.64ms
step:1427/2270 train_time:86540ms step_avg:60.64ms
step:1428/2270 train_time:86599ms step_avg:60.64ms
step:1429/2270 train_time:86662ms step_avg:60.65ms
step:1430/2270 train_time:86722ms step_avg:60.65ms
step:1431/2270 train_time:86784ms step_avg:60.65ms
step:1432/2270 train_time:86844ms step_avg:60.65ms
step:1433/2270 train_time:86907ms step_avg:60.65ms
step:1434/2270 train_time:86967ms step_avg:60.65ms
step:1435/2270 train_time:87031ms step_avg:60.65ms
step:1436/2270 train_time:87090ms step_avg:60.65ms
step:1437/2270 train_time:87153ms step_avg:60.65ms
step:1438/2270 train_time:87211ms step_avg:60.65ms
step:1439/2270 train_time:87274ms step_avg:60.65ms
step:1440/2270 train_time:87333ms step_avg:60.65ms
step:1441/2270 train_time:87395ms step_avg:60.65ms
step:1442/2270 train_time:87455ms step_avg:60.65ms
step:1443/2270 train_time:87518ms step_avg:60.65ms
step:1444/2270 train_time:87578ms step_avg:60.65ms
step:1445/2270 train_time:87640ms step_avg:60.65ms
step:1446/2270 train_time:87700ms step_avg:60.65ms
step:1447/2270 train_time:87763ms step_avg:60.65ms
step:1448/2270 train_time:87823ms step_avg:60.65ms
step:1449/2270 train_time:87886ms step_avg:60.65ms
step:1450/2270 train_time:87946ms step_avg:60.65ms
step:1451/2270 train_time:88008ms step_avg:60.65ms
step:1452/2270 train_time:88068ms step_avg:60.65ms
step:1453/2270 train_time:88131ms step_avg:60.65ms
step:1454/2270 train_time:88191ms step_avg:60.65ms
step:1455/2270 train_time:88254ms step_avg:60.66ms
step:1456/2270 train_time:88313ms step_avg:60.65ms
step:1457/2270 train_time:88376ms step_avg:60.66ms
step:1458/2270 train_time:88436ms step_avg:60.66ms
step:1459/2270 train_time:88498ms step_avg:60.66ms
step:1460/2270 train_time:88558ms step_avg:60.66ms
step:1461/2270 train_time:88621ms step_avg:60.66ms
step:1462/2270 train_time:88680ms step_avg:60.66ms
step:1463/2270 train_time:88743ms step_avg:60.66ms
step:1464/2270 train_time:88802ms step_avg:60.66ms
step:1465/2270 train_time:88865ms step_avg:60.66ms
step:1466/2270 train_time:88925ms step_avg:60.66ms
step:1467/2270 train_time:88988ms step_avg:60.66ms
step:1468/2270 train_time:89048ms step_avg:60.66ms
step:1469/2270 train_time:89111ms step_avg:60.66ms
step:1470/2270 train_time:89171ms step_avg:60.66ms
step:1471/2270 train_time:89233ms step_avg:60.66ms
step:1472/2270 train_time:89293ms step_avg:60.66ms
step:1473/2270 train_time:89355ms step_avg:60.66ms
step:1474/2270 train_time:89415ms step_avg:60.66ms
step:1475/2270 train_time:89477ms step_avg:60.66ms
step:1476/2270 train_time:89538ms step_avg:60.66ms
step:1477/2270 train_time:89600ms step_avg:60.66ms
step:1478/2270 train_time:89660ms step_avg:60.66ms
step:1479/2270 train_time:89723ms step_avg:60.66ms
step:1480/2270 train_time:89782ms step_avg:60.66ms
step:1481/2270 train_time:89845ms step_avg:60.66ms
step:1482/2270 train_time:89904ms step_avg:60.66ms
step:1483/2270 train_time:89967ms step_avg:60.67ms
step:1484/2270 train_time:90027ms step_avg:60.67ms
step:1485/2270 train_time:90090ms step_avg:60.67ms
step:1486/2270 train_time:90150ms step_avg:60.67ms
step:1487/2270 train_time:90212ms step_avg:60.67ms
step:1488/2270 train_time:90271ms step_avg:60.67ms
step:1489/2270 train_time:90335ms step_avg:60.67ms
step:1490/2270 train_time:90393ms step_avg:60.67ms
step:1491/2270 train_time:90456ms step_avg:60.67ms
step:1492/2270 train_time:90515ms step_avg:60.67ms
step:1493/2270 train_time:90578ms step_avg:60.67ms
step:1494/2270 train_time:90638ms step_avg:60.67ms
step:1495/2270 train_time:90701ms step_avg:60.67ms
step:1496/2270 train_time:90761ms step_avg:60.67ms
step:1497/2270 train_time:90823ms step_avg:60.67ms
step:1498/2270 train_time:90883ms step_avg:60.67ms
step:1499/2270 train_time:90946ms step_avg:60.67ms
step:1500/2270 train_time:91006ms step_avg:60.67ms
step:1500/2270 val_loss:3.4328 train_time:91070ms step_avg:60.71ms
step:1501/2270 train_time:91093ms step_avg:60.69ms
step:1502/2270 train_time:91131ms step_avg:60.67ms
step:1503/2270 train_time:91196ms step_avg:60.68ms
step:1504/2270 train_time:91257ms step_avg:60.68ms
step:1505/2270 train_time:91320ms step_avg:60.68ms
step:1506/2270 train_time:91381ms step_avg:60.68ms
step:1507/2270 train_time:91443ms step_avg:60.68ms
step:1508/2270 train_time:91502ms step_avg:60.68ms
step:1509/2270 train_time:91564ms step_avg:60.68ms
step:1510/2270 train_time:91624ms step_avg:60.68ms
step:1511/2270 train_time:91687ms step_avg:60.68ms
step:1512/2270 train_time:91746ms step_avg:60.68ms
step:1513/2270 train_time:91807ms step_avg:60.68ms
step:1514/2270 train_time:91866ms step_avg:60.68ms
step:1515/2270 train_time:91928ms step_avg:60.68ms
step:1516/2270 train_time:91988ms step_avg:60.68ms
step:1517/2270 train_time:92051ms step_avg:60.68ms
step:1518/2270 train_time:92113ms step_avg:60.68ms
step:1519/2270 train_time:92176ms step_avg:60.68ms
step:1520/2270 train_time:92237ms step_avg:60.68ms
step:1521/2270 train_time:92300ms step_avg:60.68ms
step:1522/2270 train_time:92360ms step_avg:60.68ms
step:1523/2270 train_time:92422ms step_avg:60.68ms
step:1524/2270 train_time:92482ms step_avg:60.68ms
step:1525/2270 train_time:92545ms step_avg:60.68ms
step:1526/2270 train_time:92605ms step_avg:60.68ms
step:1527/2270 train_time:92668ms step_avg:60.69ms
step:1528/2270 train_time:92726ms step_avg:60.68ms
step:1529/2270 train_time:92788ms step_avg:60.69ms
step:1530/2270 train_time:92847ms step_avg:60.68ms
step:1531/2270 train_time:92909ms step_avg:60.69ms
step:1532/2270 train_time:92969ms step_avg:60.68ms
step:1533/2270 train_time:93032ms step_avg:60.69ms
step:1534/2270 train_time:93093ms step_avg:60.69ms
step:1535/2270 train_time:93155ms step_avg:60.69ms
step:1536/2270 train_time:93216ms step_avg:60.69ms
step:1537/2270 train_time:93279ms step_avg:60.69ms
step:1538/2270 train_time:93339ms step_avg:60.69ms
step:1539/2270 train_time:93401ms step_avg:60.69ms
step:1540/2270 train_time:93462ms step_avg:60.69ms
step:1541/2270 train_time:93525ms step_avg:60.69ms
step:1542/2270 train_time:93585ms step_avg:60.69ms
step:1543/2270 train_time:93648ms step_avg:60.69ms
step:1544/2270 train_time:93707ms step_avg:60.69ms
step:1545/2270 train_time:93769ms step_avg:60.69ms
step:1546/2270 train_time:93828ms step_avg:60.69ms
step:1547/2270 train_time:93891ms step_avg:60.69ms
step:1548/2270 train_time:93950ms step_avg:60.69ms
step:1549/2270 train_time:94013ms step_avg:60.69ms
step:1550/2270 train_time:94074ms step_avg:60.69ms
step:1551/2270 train_time:94136ms step_avg:60.69ms
step:1552/2270 train_time:94196ms step_avg:60.69ms
step:1553/2270 train_time:94259ms step_avg:60.69ms
step:1554/2270 train_time:94320ms step_avg:60.69ms
step:1555/2270 train_time:94382ms step_avg:60.70ms
step:1556/2270 train_time:94443ms step_avg:60.70ms
step:1557/2270 train_time:94506ms step_avg:60.70ms
step:1558/2270 train_time:94566ms step_avg:60.70ms
step:1559/2270 train_time:94629ms step_avg:60.70ms
step:1560/2270 train_time:94689ms step_avg:60.70ms
step:1561/2270 train_time:94751ms step_avg:60.70ms
step:1562/2270 train_time:94814ms step_avg:60.70ms
step:1563/2270 train_time:94873ms step_avg:60.70ms
step:1564/2270 train_time:94932ms step_avg:60.70ms
step:1565/2270 train_time:94995ms step_avg:60.70ms
step:1566/2270 train_time:95055ms step_avg:60.70ms
step:1567/2270 train_time:95118ms step_avg:60.70ms
step:1568/2270 train_time:95177ms step_avg:60.70ms
step:1569/2270 train_time:95240ms step_avg:60.70ms
step:1570/2270 train_time:95301ms step_avg:60.70ms
step:1571/2270 train_time:95363ms step_avg:60.70ms
step:1572/2270 train_time:95424ms step_avg:60.70ms
step:1573/2270 train_time:95487ms step_avg:60.70ms
step:1574/2270 train_time:95547ms step_avg:60.70ms
step:1575/2270 train_time:95610ms step_avg:60.70ms
step:1576/2270 train_time:95670ms step_avg:60.70ms
step:1577/2270 train_time:95732ms step_avg:60.71ms
step:1578/2270 train_time:95792ms step_avg:60.70ms
step:1579/2270 train_time:95854ms step_avg:60.71ms
step:1580/2270 train_time:95914ms step_avg:60.70ms
step:1581/2270 train_time:95976ms step_avg:60.71ms
step:1582/2270 train_time:96036ms step_avg:60.71ms
step:1583/2270 train_time:96099ms step_avg:60.71ms
step:1584/2270 train_time:96160ms step_avg:60.71ms
step:1585/2270 train_time:96222ms step_avg:60.71ms
step:1586/2270 train_time:96281ms step_avg:60.71ms
step:1587/2270 train_time:96344ms step_avg:60.71ms
step:1588/2270 train_time:96405ms step_avg:60.71ms
step:1589/2270 train_time:96468ms step_avg:60.71ms
step:1590/2270 train_time:96527ms step_avg:60.71ms
step:1591/2270 train_time:96590ms step_avg:60.71ms
step:1592/2270 train_time:96649ms step_avg:60.71ms
step:1593/2270 train_time:96711ms step_avg:60.71ms
step:1594/2270 train_time:96772ms step_avg:60.71ms
step:1595/2270 train_time:96835ms step_avg:60.71ms
step:1596/2270 train_time:96894ms step_avg:60.71ms
step:1597/2270 train_time:96956ms step_avg:60.71ms
step:1598/2270 train_time:97016ms step_avg:60.71ms
step:1599/2270 train_time:97079ms step_avg:60.71ms
step:1600/2270 train_time:97139ms step_avg:60.71ms
step:1601/2270 train_time:97201ms step_avg:60.71ms
step:1602/2270 train_time:97261ms step_avg:60.71ms
step:1603/2270 train_time:97325ms step_avg:60.71ms
step:1604/2270 train_time:97385ms step_avg:60.71ms
step:1605/2270 train_time:97448ms step_avg:60.72ms
step:1606/2270 train_time:97507ms step_avg:60.71ms
step:1607/2270 train_time:97570ms step_avg:60.72ms
step:1608/2270 train_time:97630ms step_avg:60.72ms
step:1609/2270 train_time:97692ms step_avg:60.72ms
step:1610/2270 train_time:97752ms step_avg:60.72ms
step:1611/2270 train_time:97815ms step_avg:60.72ms
step:1612/2270 train_time:97874ms step_avg:60.72ms
step:1613/2270 train_time:97937ms step_avg:60.72ms
step:1614/2270 train_time:97997ms step_avg:60.72ms
step:1615/2270 train_time:98060ms step_avg:60.72ms
step:1616/2270 train_time:98119ms step_avg:60.72ms
step:1617/2270 train_time:98182ms step_avg:60.72ms
step:1618/2270 train_time:98242ms step_avg:60.72ms
step:1619/2270 train_time:98305ms step_avg:60.72ms
step:1620/2270 train_time:98365ms step_avg:60.72ms
step:1621/2270 train_time:98428ms step_avg:60.72ms
step:1622/2270 train_time:98487ms step_avg:60.72ms
step:1623/2270 train_time:98550ms step_avg:60.72ms
step:1624/2270 train_time:98609ms step_avg:60.72ms
step:1625/2270 train_time:98672ms step_avg:60.72ms
step:1626/2270 train_time:98731ms step_avg:60.72ms
step:1627/2270 train_time:98794ms step_avg:60.72ms
step:1628/2270 train_time:98854ms step_avg:60.72ms
step:1629/2270 train_time:98917ms step_avg:60.72ms
step:1630/2270 train_time:98976ms step_avg:60.72ms
step:1631/2270 train_time:99039ms step_avg:60.72ms
step:1632/2270 train_time:99099ms step_avg:60.72ms
step:1633/2270 train_time:99161ms step_avg:60.72ms
step:1634/2270 train_time:99221ms step_avg:60.72ms
step:1635/2270 train_time:99283ms step_avg:60.72ms
step:1636/2270 train_time:99344ms step_avg:60.72ms
step:1637/2270 train_time:99407ms step_avg:60.73ms
step:1638/2270 train_time:99467ms step_avg:60.72ms
step:1639/2270 train_time:99530ms step_avg:60.73ms
step:1640/2270 train_time:99589ms step_avg:60.73ms
step:1641/2270 train_time:99652ms step_avg:60.73ms
step:1642/2270 train_time:99712ms step_avg:60.73ms
step:1643/2270 train_time:99774ms step_avg:60.73ms
step:1644/2270 train_time:99835ms step_avg:60.73ms
step:1645/2270 train_time:99897ms step_avg:60.73ms
step:1646/2270 train_time:99957ms step_avg:60.73ms
step:1647/2270 train_time:100020ms step_avg:60.73ms
step:1648/2270 train_time:100080ms step_avg:60.73ms
step:1649/2270 train_time:100143ms step_avg:60.73ms
step:1650/2270 train_time:100204ms step_avg:60.73ms
step:1651/2270 train_time:100266ms step_avg:60.73ms
step:1652/2270 train_time:100326ms step_avg:60.73ms
step:1653/2270 train_time:100388ms step_avg:60.73ms
step:1654/2270 train_time:100448ms step_avg:60.73ms
step:1655/2270 train_time:100511ms step_avg:60.73ms
step:1656/2270 train_time:100571ms step_avg:60.73ms
step:1657/2270 train_time:100634ms step_avg:60.73ms
step:1658/2270 train_time:100694ms step_avg:60.73ms
step:1659/2270 train_time:100757ms step_avg:60.73ms
step:1660/2270 train_time:100817ms step_avg:60.73ms
step:1661/2270 train_time:100880ms step_avg:60.73ms
step:1662/2270 train_time:100940ms step_avg:60.73ms
step:1663/2270 train_time:101003ms step_avg:60.74ms
step:1664/2270 train_time:101063ms step_avg:60.74ms
step:1665/2270 train_time:101126ms step_avg:60.74ms
step:1666/2270 train_time:101185ms step_avg:60.74ms
step:1667/2270 train_time:101248ms step_avg:60.74ms
step:1668/2270 train_time:101309ms step_avg:60.74ms
step:1669/2270 train_time:101371ms step_avg:60.74ms
step:1670/2270 train_time:101431ms step_avg:60.74ms
step:1671/2270 train_time:101494ms step_avg:60.74ms
step:1672/2270 train_time:101554ms step_avg:60.74ms
step:1673/2270 train_time:101617ms step_avg:60.74ms
step:1674/2270 train_time:101677ms step_avg:60.74ms
step:1675/2270 train_time:101740ms step_avg:60.74ms
step:1676/2270 train_time:101800ms step_avg:60.74ms
step:1677/2270 train_time:101863ms step_avg:60.74ms
step:1678/2270 train_time:101923ms step_avg:60.74ms
step:1679/2270 train_time:101986ms step_avg:60.74ms
step:1680/2270 train_time:102046ms step_avg:60.74ms
step:1681/2270 train_time:102108ms step_avg:60.74ms
step:1682/2270 train_time:102169ms step_avg:60.74ms
step:1683/2270 train_time:102231ms step_avg:60.74ms
step:1684/2270 train_time:102291ms step_avg:60.74ms
step:1685/2270 train_time:102354ms step_avg:60.74ms
step:1686/2270 train_time:102414ms step_avg:60.74ms
step:1687/2270 train_time:102476ms step_avg:60.74ms
step:1688/2270 train_time:102536ms step_avg:60.74ms
step:1689/2270 train_time:102599ms step_avg:60.75ms
step:1690/2270 train_time:102659ms step_avg:60.75ms
step:1691/2270 train_time:102722ms step_avg:60.75ms
step:1692/2270 train_time:102782ms step_avg:60.75ms
step:1693/2270 train_time:102845ms step_avg:60.75ms
step:1694/2270 train_time:102906ms step_avg:60.75ms
step:1695/2270 train_time:102968ms step_avg:60.75ms
step:1696/2270 train_time:103028ms step_avg:60.75ms
step:1697/2270 train_time:103090ms step_avg:60.75ms
step:1698/2270 train_time:103150ms step_avg:60.75ms
step:1699/2270 train_time:103213ms step_avg:60.75ms
step:1700/2270 train_time:103273ms step_avg:60.75ms
step:1701/2270 train_time:103335ms step_avg:60.75ms
step:1702/2270 train_time:103395ms step_avg:60.75ms
step:1703/2270 train_time:103457ms step_avg:60.75ms
step:1704/2270 train_time:103517ms step_avg:60.75ms
step:1705/2270 train_time:103580ms step_avg:60.75ms
step:1706/2270 train_time:103640ms step_avg:60.75ms
step:1707/2270 train_time:103703ms step_avg:60.75ms
step:1708/2270 train_time:103762ms step_avg:60.75ms
step:1709/2270 train_time:103826ms step_avg:60.75ms
step:1710/2270 train_time:103886ms step_avg:60.75ms
step:1711/2270 train_time:103949ms step_avg:60.75ms
step:1712/2270 train_time:104009ms step_avg:60.75ms
step:1713/2270 train_time:104072ms step_avg:60.75ms
step:1714/2270 train_time:104132ms step_avg:60.75ms
step:1715/2270 train_time:104195ms step_avg:60.75ms
step:1716/2270 train_time:104255ms step_avg:60.75ms
step:1717/2270 train_time:104317ms step_avg:60.76ms
step:1718/2270 train_time:104376ms step_avg:60.75ms
step:1719/2270 train_time:104439ms step_avg:60.76ms
step:1720/2270 train_time:104498ms step_avg:60.75ms
step:1721/2270 train_time:104561ms step_avg:60.76ms
step:1722/2270 train_time:104621ms step_avg:60.76ms
step:1723/2270 train_time:104683ms step_avg:60.76ms
step:1724/2270 train_time:104743ms step_avg:60.76ms
step:1725/2270 train_time:104806ms step_avg:60.76ms
step:1726/2270 train_time:104866ms step_avg:60.76ms
step:1727/2270 train_time:104929ms step_avg:60.76ms
step:1728/2270 train_time:104988ms step_avg:60.76ms
step:1729/2270 train_time:105050ms step_avg:60.76ms
step:1730/2270 train_time:105111ms step_avg:60.76ms
step:1731/2270 train_time:105174ms step_avg:60.76ms
step:1732/2270 train_time:105233ms step_avg:60.76ms
step:1733/2270 train_time:105296ms step_avg:60.76ms
step:1734/2270 train_time:105356ms step_avg:60.76ms
step:1735/2270 train_time:105419ms step_avg:60.76ms
step:1736/2270 train_time:105479ms step_avg:60.76ms
step:1737/2270 train_time:105542ms step_avg:60.76ms
step:1738/2270 train_time:105602ms step_avg:60.76ms
step:1739/2270 train_time:105665ms step_avg:60.76ms
step:1740/2270 train_time:105724ms step_avg:60.76ms
step:1741/2270 train_time:105787ms step_avg:60.76ms
step:1742/2270 train_time:105847ms step_avg:60.76ms
step:1743/2270 train_time:105910ms step_avg:60.76ms
step:1744/2270 train_time:105970ms step_avg:60.76ms
step:1745/2270 train_time:106033ms step_avg:60.76ms
step:1746/2270 train_time:106093ms step_avg:60.76ms
step:1747/2270 train_time:106155ms step_avg:60.76ms
step:1748/2270 train_time:106215ms step_avg:60.76ms
step:1749/2270 train_time:106278ms step_avg:60.77ms
step:1750/2270 train_time:106338ms step_avg:60.76ms
step:1750/2270 val_loss:3.3705 train_time:106401ms step_avg:60.80ms
step:1751/2270 train_time:106426ms step_avg:60.78ms
step:1752/2270 train_time:106462ms step_avg:60.77ms
step:1753/2270 train_time:106524ms step_avg:60.77ms
step:1754/2270 train_time:106585ms step_avg:60.77ms
step:1755/2270 train_time:106649ms step_avg:60.77ms
step:1756/2270 train_time:106708ms step_avg:60.77ms
step:1757/2270 train_time:106769ms step_avg:60.77ms
step:1758/2270 train_time:106829ms step_avg:60.77ms
step:1759/2270 train_time:106890ms step_avg:60.77ms
step:1760/2270 train_time:106949ms step_avg:60.77ms
step:1761/2270 train_time:107010ms step_avg:60.77ms
step:1762/2270 train_time:107069ms step_avg:60.77ms
step:1763/2270 train_time:107132ms step_avg:60.77ms
step:1764/2270 train_time:107192ms step_avg:60.77ms
step:1765/2270 train_time:107253ms step_avg:60.77ms
step:1766/2270 train_time:107316ms step_avg:60.77ms
step:1767/2270 train_time:107381ms step_avg:60.77ms
step:1768/2270 train_time:107441ms step_avg:60.77ms
step:1769/2270 train_time:107504ms step_avg:60.77ms
step:1770/2270 train_time:107564ms step_avg:60.77ms
step:1771/2270 train_time:107627ms step_avg:60.77ms
step:1772/2270 train_time:107687ms step_avg:60.77ms
step:1773/2270 train_time:107749ms step_avg:60.77ms
step:1774/2270 train_time:107809ms step_avg:60.77ms
step:1775/2270 train_time:107871ms step_avg:60.77ms
step:1776/2270 train_time:107930ms step_avg:60.77ms
step:1777/2270 train_time:107992ms step_avg:60.77ms
step:1778/2270 train_time:108051ms step_avg:60.77ms
step:1779/2270 train_time:108113ms step_avg:60.77ms
step:1780/2270 train_time:108174ms step_avg:60.77ms
step:1781/2270 train_time:108237ms step_avg:60.77ms
step:1782/2270 train_time:108297ms step_avg:60.77ms
step:1783/2270 train_time:108361ms step_avg:60.77ms
step:1784/2270 train_time:108421ms step_avg:60.77ms
step:1785/2270 train_time:108484ms step_avg:60.78ms
step:1786/2270 train_time:108544ms step_avg:60.78ms
step:1787/2270 train_time:108607ms step_avg:60.78ms
step:1788/2270 train_time:108667ms step_avg:60.78ms
step:1789/2270 train_time:108729ms step_avg:60.78ms
step:1790/2270 train_time:108789ms step_avg:60.78ms
step:1791/2270 train_time:108851ms step_avg:60.78ms
step:1792/2270 train_time:108910ms step_avg:60.78ms
step:1793/2270 train_time:108972ms step_avg:60.78ms
step:1794/2270 train_time:109031ms step_avg:60.78ms
step:1795/2270 train_time:109093ms step_avg:60.78ms
step:1796/2270 train_time:109152ms step_avg:60.78ms
step:1797/2270 train_time:109215ms step_avg:60.78ms
step:1798/2270 train_time:109276ms step_avg:60.78ms
step:1799/2270 train_time:109338ms step_avg:60.78ms
step:1800/2270 train_time:109399ms step_avg:60.78ms
step:1801/2270 train_time:109462ms step_avg:60.78ms
step:1802/2270 train_time:109522ms step_avg:60.78ms
step:1803/2270 train_time:109585ms step_avg:60.78ms
step:1804/2270 train_time:109645ms step_avg:60.78ms
step:1805/2270 train_time:109707ms step_avg:60.78ms
step:1806/2270 train_time:109767ms step_avg:60.78ms
step:1807/2270 train_time:109830ms step_avg:60.78ms
step:1808/2270 train_time:109890ms step_avg:60.78ms
step:1809/2270 train_time:109952ms step_avg:60.78ms
step:1810/2270 train_time:110011ms step_avg:60.78ms
step:1811/2270 train_time:110073ms step_avg:60.78ms
step:1812/2270 train_time:110133ms step_avg:60.78ms
step:1813/2270 train_time:110195ms step_avg:60.78ms
step:1814/2270 train_time:110255ms step_avg:60.78ms
step:1815/2270 train_time:110318ms step_avg:60.78ms
step:1816/2270 train_time:110379ms step_avg:60.78ms
step:1817/2270 train_time:110442ms step_avg:60.78ms
step:1818/2270 train_time:110502ms step_avg:60.78ms
step:1819/2270 train_time:110565ms step_avg:60.78ms
step:1820/2270 train_time:110625ms step_avg:60.78ms
step:1821/2270 train_time:110688ms step_avg:60.78ms
step:1822/2270 train_time:110748ms step_avg:60.78ms
step:1823/2270 train_time:110811ms step_avg:60.78ms
step:1824/2270 train_time:110870ms step_avg:60.78ms
step:1825/2270 train_time:110932ms step_avg:60.78ms
step:1826/2270 train_time:110992ms step_avg:60.78ms
step:1827/2270 train_time:111054ms step_avg:60.79ms
step:1828/2270 train_time:111114ms step_avg:60.78ms
step:1829/2270 train_time:111176ms step_avg:60.79ms
step:1830/2270 train_time:111236ms step_avg:60.78ms
step:1831/2270 train_time:111299ms step_avg:60.79ms
step:1832/2270 train_time:111359ms step_avg:60.79ms
step:1833/2270 train_time:111422ms step_avg:60.79ms
step:1834/2270 train_time:111482ms step_avg:60.79ms
step:1835/2270 train_time:111545ms step_avg:60.79ms
step:1836/2270 train_time:111605ms step_avg:60.79ms
step:1837/2270 train_time:111667ms step_avg:60.79ms
step:1838/2270 train_time:111728ms step_avg:60.79ms
step:1839/2270 train_time:111790ms step_avg:60.79ms
step:1840/2270 train_time:111850ms step_avg:60.79ms
step:1841/2270 train_time:111912ms step_avg:60.79ms
step:1842/2270 train_time:111972ms step_avg:60.79ms
step:1843/2270 train_time:112034ms step_avg:60.79ms
step:1844/2270 train_time:112094ms step_avg:60.79ms
step:1845/2270 train_time:112157ms step_avg:60.79ms
step:1846/2270 train_time:112216ms step_avg:60.79ms
step:1847/2270 train_time:112280ms step_avg:60.79ms
step:1848/2270 train_time:112339ms step_avg:60.79ms
step:1849/2270 train_time:112402ms step_avg:60.79ms
step:1850/2270 train_time:112462ms step_avg:60.79ms
step:1851/2270 train_time:112525ms step_avg:60.79ms
step:1852/2270 train_time:112585ms step_avg:60.79ms
step:1853/2270 train_time:112648ms step_avg:60.79ms
step:1854/2270 train_time:112708ms step_avg:60.79ms
step:1855/2270 train_time:112771ms step_avg:60.79ms
step:1856/2270 train_time:112831ms step_avg:60.79ms
step:1857/2270 train_time:112894ms step_avg:60.79ms
step:1858/2270 train_time:112953ms step_avg:60.79ms
step:1859/2270 train_time:113015ms step_avg:60.79ms
step:1860/2270 train_time:113076ms step_avg:60.79ms
step:1861/2270 train_time:113138ms step_avg:60.79ms
step:1862/2270 train_time:113197ms step_avg:60.79ms
step:1863/2270 train_time:113260ms step_avg:60.79ms
step:1864/2270 train_time:113319ms step_avg:60.79ms
step:1865/2270 train_time:113381ms step_avg:60.79ms
step:1866/2270 train_time:113442ms step_avg:60.79ms
step:1867/2270 train_time:113505ms step_avg:60.80ms
step:1868/2270 train_time:113565ms step_avg:60.79ms
step:1869/2270 train_time:113628ms step_avg:60.80ms
step:1870/2270 train_time:113688ms step_avg:60.80ms
step:1871/2270 train_time:113750ms step_avg:60.80ms
step:1872/2270 train_time:113810ms step_avg:60.80ms
step:1873/2270 train_time:113872ms step_avg:60.80ms
step:1874/2270 train_time:113931ms step_avg:60.80ms
step:1875/2270 train_time:113993ms step_avg:60.80ms
step:1876/2270 train_time:114053ms step_avg:60.80ms
step:1877/2270 train_time:114116ms step_avg:60.80ms
step:1878/2270 train_time:114176ms step_avg:60.80ms
step:1879/2270 train_time:114239ms step_avg:60.80ms
step:1880/2270 train_time:114299ms step_avg:60.80ms
step:1881/2270 train_time:114361ms step_avg:60.80ms
step:1882/2270 train_time:114422ms step_avg:60.80ms
step:1883/2270 train_time:114484ms step_avg:60.80ms
step:1884/2270 train_time:114544ms step_avg:60.80ms
step:1885/2270 train_time:114607ms step_avg:60.80ms
step:1886/2270 train_time:114668ms step_avg:60.80ms
step:1887/2270 train_time:114731ms step_avg:60.80ms
step:1888/2270 train_time:114791ms step_avg:60.80ms
step:1889/2270 train_time:114854ms step_avg:60.80ms
step:1890/2270 train_time:114914ms step_avg:60.80ms
step:1891/2270 train_time:114976ms step_avg:60.80ms
step:1892/2270 train_time:115036ms step_avg:60.80ms
step:1893/2270 train_time:115099ms step_avg:60.80ms
step:1894/2270 train_time:115159ms step_avg:60.80ms
step:1895/2270 train_time:115221ms step_avg:60.80ms
step:1896/2270 train_time:115282ms step_avg:60.80ms
step:1897/2270 train_time:115344ms step_avg:60.80ms
step:1898/2270 train_time:115404ms step_avg:60.80ms
step:1899/2270 train_time:115467ms step_avg:60.80ms
step:1900/2270 train_time:115527ms step_avg:60.80ms
step:1901/2270 train_time:115590ms step_avg:60.80ms
step:1902/2270 train_time:115650ms step_avg:60.80ms
step:1903/2270 train_time:115713ms step_avg:60.81ms
step:1904/2270 train_time:115773ms step_avg:60.81ms
step:1905/2270 train_time:115836ms step_avg:60.81ms
step:1906/2270 train_time:115895ms step_avg:60.81ms
step:1907/2270 train_time:115958ms step_avg:60.81ms
step:1908/2270 train_time:116018ms step_avg:60.81ms
step:1909/2270 train_time:116080ms step_avg:60.81ms
step:1910/2270 train_time:116140ms step_avg:60.81ms
step:1911/2270 train_time:116203ms step_avg:60.81ms
step:1912/2270 train_time:116263ms step_avg:60.81ms
step:1913/2270 train_time:116326ms step_avg:60.81ms
step:1914/2270 train_time:116386ms step_avg:60.81ms
step:1915/2270 train_time:116449ms step_avg:60.81ms
step:1916/2270 train_time:116508ms step_avg:60.81ms
step:1917/2270 train_time:116571ms step_avg:60.81ms
step:1918/2270 train_time:116631ms step_avg:60.81ms
step:1919/2270 train_time:116693ms step_avg:60.81ms
step:1920/2270 train_time:116754ms step_avg:60.81ms
step:1921/2270 train_time:116817ms step_avg:60.81ms
step:1922/2270 train_time:116877ms step_avg:60.81ms
step:1923/2270 train_time:116939ms step_avg:60.81ms
step:1924/2270 train_time:116999ms step_avg:60.81ms
step:1925/2270 train_time:117062ms step_avg:60.81ms
step:1926/2270 train_time:117123ms step_avg:60.81ms
step:1927/2270 train_time:117186ms step_avg:60.81ms
step:1928/2270 train_time:117246ms step_avg:60.81ms
step:1929/2270 train_time:117309ms step_avg:60.81ms
step:1930/2270 train_time:117368ms step_avg:60.81ms
step:1931/2270 train_time:117431ms step_avg:60.81ms
step:1932/2270 train_time:117491ms step_avg:60.81ms
step:1933/2270 train_time:117553ms step_avg:60.81ms
step:1934/2270 train_time:117614ms step_avg:60.81ms
step:1935/2270 train_time:117677ms step_avg:60.81ms
step:1936/2270 train_time:117737ms step_avg:60.81ms
step:1937/2270 train_time:117799ms step_avg:60.82ms
step:1938/2270 train_time:117859ms step_avg:60.81ms
step:1939/2270 train_time:117922ms step_avg:60.82ms
step:1940/2270 train_time:117982ms step_avg:60.82ms
step:1941/2270 train_time:118045ms step_avg:60.82ms
step:1942/2270 train_time:118105ms step_avg:60.82ms
step:1943/2270 train_time:118168ms step_avg:60.82ms
step:1944/2270 train_time:118228ms step_avg:60.82ms
step:1945/2270 train_time:118291ms step_avg:60.82ms
step:1946/2270 train_time:118350ms step_avg:60.82ms
step:1947/2270 train_time:118413ms step_avg:60.82ms
step:1948/2270 train_time:118474ms step_avg:60.82ms
step:1949/2270 train_time:118536ms step_avg:60.82ms
step:1950/2270 train_time:118596ms step_avg:60.82ms
step:1951/2270 train_time:118658ms step_avg:60.82ms
step:1952/2270 train_time:118718ms step_avg:60.82ms
step:1953/2270 train_time:118781ms step_avg:60.82ms
step:1954/2270 train_time:118841ms step_avg:60.82ms
step:1955/2270 train_time:118903ms step_avg:60.82ms
step:1956/2270 train_time:118963ms step_avg:60.82ms
step:1957/2270 train_time:119027ms step_avg:60.82ms
step:1958/2270 train_time:119087ms step_avg:60.82ms
step:1959/2270 train_time:119151ms step_avg:60.82ms
step:1960/2270 train_time:119211ms step_avg:60.82ms
step:1961/2270 train_time:119273ms step_avg:60.82ms
step:1962/2270 train_time:119334ms step_avg:60.82ms
step:1963/2270 train_time:119396ms step_avg:60.82ms
step:1964/2270 train_time:119456ms step_avg:60.82ms
step:1965/2270 train_time:119518ms step_avg:60.82ms
step:1966/2270 train_time:119579ms step_avg:60.82ms
step:1967/2270 train_time:119641ms step_avg:60.82ms
step:1968/2270 train_time:119701ms step_avg:60.82ms
step:1969/2270 train_time:119764ms step_avg:60.82ms
step:1970/2270 train_time:119824ms step_avg:60.82ms
step:1971/2270 train_time:119887ms step_avg:60.83ms
step:1972/2270 train_time:119948ms step_avg:60.83ms
step:1973/2270 train_time:120011ms step_avg:60.83ms
step:1974/2270 train_time:120070ms step_avg:60.83ms
step:1975/2270 train_time:120132ms step_avg:60.83ms
step:1976/2270 train_time:120192ms step_avg:60.83ms
step:1977/2270 train_time:120255ms step_avg:60.83ms
step:1978/2270 train_time:120315ms step_avg:60.83ms
step:1979/2270 train_time:120377ms step_avg:60.83ms
step:1980/2270 train_time:120437ms step_avg:60.83ms
step:1981/2270 train_time:120500ms step_avg:60.83ms
step:1982/2270 train_time:120560ms step_avg:60.83ms
step:1983/2270 train_time:120623ms step_avg:60.83ms
step:1984/2270 train_time:120684ms step_avg:60.83ms
step:1985/2270 train_time:120746ms step_avg:60.83ms
step:1986/2270 train_time:120806ms step_avg:60.83ms
step:1987/2270 train_time:120869ms step_avg:60.83ms
step:1988/2270 train_time:120929ms step_avg:60.83ms
step:1989/2270 train_time:120991ms step_avg:60.83ms
step:1990/2270 train_time:121051ms step_avg:60.83ms
step:1991/2270 train_time:121114ms step_avg:60.83ms
step:1992/2270 train_time:121175ms step_avg:60.83ms
step:1993/2270 train_time:121237ms step_avg:60.83ms
step:1994/2270 train_time:121297ms step_avg:60.83ms
step:1995/2270 train_time:121360ms step_avg:60.83ms
step:1996/2270 train_time:121420ms step_avg:60.83ms
step:1997/2270 train_time:121482ms step_avg:60.83ms
step:1998/2270 train_time:121542ms step_avg:60.83ms
step:1999/2270 train_time:121605ms step_avg:60.83ms
step:2000/2270 train_time:121665ms step_avg:60.83ms
step:2000/2270 val_loss:3.3191 train_time:121729ms step_avg:60.86ms
step:2001/2270 train_time:121749ms step_avg:60.84ms
step:2002/2270 train_time:121793ms step_avg:60.84ms
step:2003/2270 train_time:121855ms step_avg:60.84ms
step:2004/2270 train_time:121915ms step_avg:60.84ms
step:2005/2270 train_time:121978ms step_avg:60.84ms
step:2006/2270 train_time:122038ms step_avg:60.84ms
step:2007/2270 train_time:122100ms step_avg:60.84ms
step:2008/2270 train_time:122159ms step_avg:60.84ms
step:2009/2270 train_time:122221ms step_avg:60.84ms
step:2010/2270 train_time:122280ms step_avg:60.84ms
step:2011/2270 train_time:122343ms step_avg:60.84ms
step:2012/2270 train_time:122402ms step_avg:60.84ms
step:2013/2270 train_time:122464ms step_avg:60.84ms
step:2014/2270 train_time:122524ms step_avg:60.84ms
step:2015/2270 train_time:122586ms step_avg:60.84ms
step:2016/2270 train_time:122646ms step_avg:60.84ms
step:2017/2270 train_time:122712ms step_avg:60.84ms
step:2018/2270 train_time:122774ms step_avg:60.84ms
step:2019/2270 train_time:122837ms step_avg:60.84ms
step:2020/2270 train_time:122898ms step_avg:60.84ms
step:2021/2270 train_time:122961ms step_avg:60.84ms
step:2022/2270 train_time:123021ms step_avg:60.84ms
step:2023/2270 train_time:123083ms step_avg:60.84ms
step:2024/2270 train_time:123142ms step_avg:60.84ms
step:2025/2270 train_time:123205ms step_avg:60.84ms
step:2026/2270 train_time:123264ms step_avg:60.84ms
step:2027/2270 train_time:123327ms step_avg:60.84ms
step:2028/2270 train_time:123386ms step_avg:60.84ms
step:2029/2270 train_time:123448ms step_avg:60.84ms
step:2030/2270 train_time:123508ms step_avg:60.84ms
step:2031/2270 train_time:123570ms step_avg:60.84ms
step:2032/2270 train_time:123631ms step_avg:60.84ms
step:2033/2270 train_time:123695ms step_avg:60.84ms
step:2034/2270 train_time:123756ms step_avg:60.84ms
step:2035/2270 train_time:123819ms step_avg:60.84ms
step:2036/2270 train_time:123880ms step_avg:60.84ms
step:2037/2270 train_time:123943ms step_avg:60.85ms
step:2038/2270 train_time:124003ms step_avg:60.85ms
step:2039/2270 train_time:124066ms step_avg:60.85ms
step:2040/2270 train_time:124126ms step_avg:60.85ms
step:2041/2270 train_time:124188ms step_avg:60.85ms
step:2042/2270 train_time:124248ms step_avg:60.85ms
step:2043/2270 train_time:124311ms step_avg:60.85ms
step:2044/2270 train_time:124371ms step_avg:60.85ms
step:2045/2270 train_time:124434ms step_avg:60.85ms
step:2046/2270 train_time:124494ms step_avg:60.85ms
step:2047/2270 train_time:124557ms step_avg:60.85ms
step:2048/2270 train_time:124617ms step_avg:60.85ms
step:2049/2270 train_time:124680ms step_avg:60.85ms
step:2050/2270 train_time:124740ms step_avg:60.85ms
step:2051/2270 train_time:124805ms step_avg:60.85ms
step:2052/2270 train_time:124865ms step_avg:60.85ms
step:2053/2270 train_time:124928ms step_avg:60.85ms
step:2054/2270 train_time:124988ms step_avg:60.85ms
step:2055/2270 train_time:125051ms step_avg:60.85ms
step:2056/2270 train_time:125111ms step_avg:60.85ms
step:2057/2270 train_time:125174ms step_avg:60.85ms
step:2058/2270 train_time:125234ms step_avg:60.85ms
step:2059/2270 train_time:125296ms step_avg:60.85ms
step:2060/2270 train_time:125355ms step_avg:60.85ms
step:2061/2270 train_time:125418ms step_avg:60.85ms
step:2062/2270 train_time:125478ms step_avg:60.85ms
step:2063/2270 train_time:125540ms step_avg:60.85ms
step:2064/2270 train_time:125600ms step_avg:60.85ms
step:2065/2270 train_time:125664ms step_avg:60.85ms
step:2066/2270 train_time:125724ms step_avg:60.85ms
step:2067/2270 train_time:125787ms step_avg:60.86ms
step:2068/2270 train_time:125848ms step_avg:60.85ms
step:2069/2270 train_time:125911ms step_avg:60.86ms
step:2070/2270 train_time:125971ms step_avg:60.86ms
step:2071/2270 train_time:126034ms step_avg:60.86ms
step:2072/2270 train_time:126094ms step_avg:60.86ms
step:2073/2270 train_time:126156ms step_avg:60.86ms
step:2074/2270 train_time:126216ms step_avg:60.86ms
step:2075/2270 train_time:126279ms step_avg:60.86ms
step:2076/2270 train_time:126339ms step_avg:60.86ms
step:2077/2270 train_time:126401ms step_avg:60.86ms
step:2078/2270 train_time:126461ms step_avg:60.86ms
step:2079/2270 train_time:126524ms step_avg:60.86ms
step:2080/2270 train_time:126584ms step_avg:60.86ms
step:2081/2270 train_time:126646ms step_avg:60.86ms
step:2082/2270 train_time:126707ms step_avg:60.86ms
step:2083/2270 train_time:126769ms step_avg:60.86ms
step:2084/2270 train_time:126829ms step_avg:60.86ms
step:2085/2270 train_time:126892ms step_avg:60.86ms
step:2086/2270 train_time:126953ms step_avg:60.86ms
step:2087/2270 train_time:127017ms step_avg:60.86ms
step:2088/2270 train_time:127076ms step_avg:60.86ms
step:2089/2270 train_time:127139ms step_avg:60.86ms
step:2090/2270 train_time:127198ms step_avg:60.86ms
step:2091/2270 train_time:127261ms step_avg:60.86ms
step:2092/2270 train_time:127321ms step_avg:60.86ms
step:2093/2270 train_time:127384ms step_avg:60.86ms
step:2094/2270 train_time:127444ms step_avg:60.86ms
step:2095/2270 train_time:127507ms step_avg:60.86ms
step:2096/2270 train_time:127566ms step_avg:60.86ms
step:2097/2270 train_time:127629ms step_avg:60.86ms
step:2098/2270 train_time:127689ms step_avg:60.86ms
step:2099/2270 train_time:127752ms step_avg:60.86ms
step:2100/2270 train_time:127813ms step_avg:60.86ms
step:2101/2270 train_time:127875ms step_avg:60.86ms
step:2102/2270 train_time:127935ms step_avg:60.86ms
step:2103/2270 train_time:127998ms step_avg:60.86ms
step:2104/2270 train_time:128059ms step_avg:60.86ms
step:2105/2270 train_time:128121ms step_avg:60.86ms
step:2106/2270 train_time:128181ms step_avg:60.86ms
step:2107/2270 train_time:128243ms step_avg:60.87ms
step:2108/2270 train_time:128304ms step_avg:60.87ms
step:2109/2270 train_time:128366ms step_avg:60.87ms
step:2110/2270 train_time:128426ms step_avg:60.87ms
step:2111/2270 train_time:128488ms step_avg:60.87ms
step:2112/2270 train_time:128548ms step_avg:60.87ms
step:2113/2270 train_time:128611ms step_avg:60.87ms
step:2114/2270 train_time:128671ms step_avg:60.87ms
step:2115/2270 train_time:128733ms step_avg:60.87ms
step:2116/2270 train_time:128793ms step_avg:60.87ms
step:2117/2270 train_time:128856ms step_avg:60.87ms
step:2118/2270 train_time:128916ms step_avg:60.87ms
step:2119/2270 train_time:128979ms step_avg:60.87ms
step:2120/2270 train_time:129038ms step_avg:60.87ms
step:2121/2270 train_time:129101ms step_avg:60.87ms
step:2122/2270 train_time:129161ms step_avg:60.87ms
step:2123/2270 train_time:129224ms step_avg:60.87ms
step:2124/2270 train_time:129284ms step_avg:60.87ms
step:2125/2270 train_time:129347ms step_avg:60.87ms
step:2126/2270 train_time:129406ms step_avg:60.87ms
step:2127/2270 train_time:129469ms step_avg:60.87ms
step:2128/2270 train_time:129529ms step_avg:60.87ms
step:2129/2270 train_time:129591ms step_avg:60.87ms
step:2130/2270 train_time:129652ms step_avg:60.87ms
step:2131/2270 train_time:129714ms step_avg:60.87ms
step:2132/2270 train_time:129774ms step_avg:60.87ms
step:2133/2270 train_time:129837ms step_avg:60.87ms
step:2134/2270 train_time:129897ms step_avg:60.87ms
step:2135/2270 train_time:129959ms step_avg:60.87ms
step:2136/2270 train_time:130020ms step_avg:60.87ms
step:2137/2270 train_time:130083ms step_avg:60.87ms
step:2138/2270 train_time:130143ms step_avg:60.87ms
step:2139/2270 train_time:130206ms step_avg:60.87ms
step:2140/2270 train_time:130265ms step_avg:60.87ms
step:2141/2270 train_time:130328ms step_avg:60.87ms
step:2142/2270 train_time:130389ms step_avg:60.87ms
step:2143/2270 train_time:130451ms step_avg:60.87ms
step:2144/2270 train_time:130512ms step_avg:60.87ms
step:2145/2270 train_time:130574ms step_avg:60.87ms
step:2146/2270 train_time:130633ms step_avg:60.87ms
step:2147/2270 train_time:130697ms step_avg:60.87ms
step:2148/2270 train_time:130757ms step_avg:60.87ms
step:2149/2270 train_time:130819ms step_avg:60.87ms
step:2150/2270 train_time:130880ms step_avg:60.87ms
step:2151/2270 train_time:130943ms step_avg:60.88ms
step:2152/2270 train_time:131003ms step_avg:60.87ms
step:2153/2270 train_time:131065ms step_avg:60.88ms
step:2154/2270 train_time:131125ms step_avg:60.88ms
step:2155/2270 train_time:131188ms step_avg:60.88ms
step:2156/2270 train_time:131248ms step_avg:60.88ms
step:2157/2270 train_time:131311ms step_avg:60.88ms
step:2158/2270 train_time:131372ms step_avg:60.88ms
step:2159/2270 train_time:131435ms step_avg:60.88ms
step:2160/2270 train_time:131495ms step_avg:60.88ms
step:2161/2270 train_time:131557ms step_avg:60.88ms
step:2162/2270 train_time:131618ms step_avg:60.88ms
step:2163/2270 train_time:131680ms step_avg:60.88ms
step:2164/2270 train_time:131740ms step_avg:60.88ms
step:2165/2270 train_time:131802ms step_avg:60.88ms
step:2166/2270 train_time:131863ms step_avg:60.88ms
step:2167/2270 train_time:131926ms step_avg:60.88ms
step:2168/2270 train_time:131986ms step_avg:60.88ms
step:2169/2270 train_time:132049ms step_avg:60.88ms
step:2170/2270 train_time:132109ms step_avg:60.88ms
step:2171/2270 train_time:132173ms step_avg:60.88ms
step:2172/2270 train_time:132233ms step_avg:60.88ms
step:2173/2270 train_time:132296ms step_avg:60.88ms
step:2174/2270 train_time:132355ms step_avg:60.88ms
step:2175/2270 train_time:132418ms step_avg:60.88ms
step:2176/2270 train_time:132478ms step_avg:60.88ms
step:2177/2270 train_time:132540ms step_avg:60.88ms
step:2178/2270 train_time:132601ms step_avg:60.88ms
step:2179/2270 train_time:132663ms step_avg:60.88ms
step:2180/2270 train_time:132723ms step_avg:60.88ms
step:2181/2270 train_time:132786ms step_avg:60.88ms
step:2182/2270 train_time:132846ms step_avg:60.88ms
step:2183/2270 train_time:132909ms step_avg:60.88ms
step:2184/2270 train_time:132969ms step_avg:60.88ms
step:2185/2270 train_time:133032ms step_avg:60.88ms
step:2186/2270 train_time:133093ms step_avg:60.88ms
step:2187/2270 train_time:133156ms step_avg:60.89ms
step:2188/2270 train_time:133215ms step_avg:60.88ms
step:2189/2270 train_time:133277ms step_avg:60.89ms
step:2190/2270 train_time:133337ms step_avg:60.88ms
step:2191/2270 train_time:133400ms step_avg:60.89ms
step:2192/2270 train_time:133460ms step_avg:60.89ms
step:2193/2270 train_time:133523ms step_avg:60.89ms
step:2194/2270 train_time:133583ms step_avg:60.89ms
step:2195/2270 train_time:133645ms step_avg:60.89ms
step:2196/2270 train_time:133706ms step_avg:60.89ms
step:2197/2270 train_time:133768ms step_avg:60.89ms
step:2198/2270 train_time:133828ms step_avg:60.89ms
step:2199/2270 train_time:133892ms step_avg:60.89ms
step:2200/2270 train_time:133953ms step_avg:60.89ms
step:2201/2270 train_time:134016ms step_avg:60.89ms
step:2202/2270 train_time:134076ms step_avg:60.89ms
step:2203/2270 train_time:134139ms step_avg:60.89ms
step:2204/2270 train_time:134199ms step_avg:60.89ms
step:2205/2270 train_time:134261ms step_avg:60.89ms
step:2206/2270 train_time:134321ms step_avg:60.89ms
step:2207/2270 train_time:134384ms step_avg:60.89ms
step:2208/2270 train_time:134444ms step_avg:60.89ms
step:2209/2270 train_time:134507ms step_avg:60.89ms
step:2210/2270 train_time:134568ms step_avg:60.89ms
step:2211/2270 train_time:134630ms step_avg:60.89ms
step:2212/2270 train_time:134690ms step_avg:60.89ms
step:2213/2270 train_time:134754ms step_avg:60.89ms
step:2214/2270 train_time:134814ms step_avg:60.89ms
step:2215/2270 train_time:134877ms step_avg:60.89ms
step:2216/2270 train_time:134937ms step_avg:60.89ms
step:2217/2270 train_time:135000ms step_avg:60.89ms
step:2218/2270 train_time:135060ms step_avg:60.89ms
step:2219/2270 train_time:135123ms step_avg:60.89ms
step:2220/2270 train_time:135183ms step_avg:60.89ms
step:2221/2270 train_time:135245ms step_avg:60.89ms
step:2222/2270 train_time:135305ms step_avg:60.89ms
step:2223/2270 train_time:135368ms step_avg:60.89ms
step:2224/2270 train_time:135429ms step_avg:60.89ms
step:2225/2270 train_time:135491ms step_avg:60.90ms
step:2226/2270 train_time:135552ms step_avg:60.89ms
step:2227/2270 train_time:135615ms step_avg:60.90ms
step:2228/2270 train_time:135674ms step_avg:60.90ms
step:2229/2270 train_time:135737ms step_avg:60.90ms
step:2230/2270 train_time:135798ms step_avg:60.90ms
step:2231/2270 train_time:135861ms step_avg:60.90ms
step:2232/2270 train_time:135921ms step_avg:60.90ms
step:2233/2270 train_time:135984ms step_avg:60.90ms
step:2234/2270 train_time:136044ms step_avg:60.90ms
step:2235/2270 train_time:136107ms step_avg:60.90ms
step:2236/2270 train_time:136167ms step_avg:60.90ms
step:2237/2270 train_time:136229ms step_avg:60.90ms
step:2238/2270 train_time:136289ms step_avg:60.90ms
step:2239/2270 train_time:136352ms step_avg:60.90ms
step:2240/2270 train_time:136412ms step_avg:60.90ms
step:2241/2270 train_time:136475ms step_avg:60.90ms
step:2242/2270 train_time:136535ms step_avg:60.90ms
step:2243/2270 train_time:136597ms step_avg:60.90ms
step:2244/2270 train_time:136658ms step_avg:60.90ms
step:2245/2270 train_time:136720ms step_avg:60.90ms
step:2246/2270 train_time:136780ms step_avg:60.90ms
step:2247/2270 train_time:136843ms step_avg:60.90ms
step:2248/2270 train_time:136903ms step_avg:60.90ms
step:2249/2270 train_time:136966ms step_avg:60.90ms
step:2250/2270 train_time:137026ms step_avg:60.90ms
step:2250/2270 val_loss:3.2818 train_time:137090ms step_avg:60.93ms
step:2251/2270 train_time:137112ms step_avg:60.91ms
step:2252/2270 train_time:137154ms step_avg:60.90ms
step:2253/2270 train_time:137219ms step_avg:60.90ms
step:2254/2270 train_time:137281ms step_avg:60.91ms
step:2255/2270 train_time:137343ms step_avg:60.91ms
step:2256/2270 train_time:137402ms step_avg:60.91ms
step:2257/2270 train_time:137464ms step_avg:60.91ms
step:2258/2270 train_time:137524ms step_avg:60.91ms
step:2259/2270 train_time:137585ms step_avg:60.91ms
step:2260/2270 train_time:137645ms step_avg:60.91ms
step:2261/2270 train_time:137708ms step_avg:60.91ms
step:2262/2270 train_time:137767ms step_avg:60.90ms
step:2263/2270 train_time:137829ms step_avg:60.91ms
step:2264/2270 train_time:137888ms step_avg:60.90ms
step:2265/2270 train_time:137950ms step_avg:60.91ms
step:2266/2270 train_time:138011ms step_avg:60.90ms
step:2267/2270 train_time:138075ms step_avg:60.91ms
step:2268/2270 train_time:138137ms step_avg:60.91ms
step:2269/2270 train_time:138201ms step_avg:60.91ms
step:2270/2270 train_time:138262ms step_avg:60.91ms
step:2270/2270 val_loss:3.2769 train_time:138326ms step_avg:60.94ms
peak memory allocated: 29250 MiB reserved: 50528 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, eps=1e-8, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp_up', 'mlp_down']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            elif params[module_idx].label == "smear_gate":
                # dividing by magnitude is equivalent of SVN for 1d tensors
                v_chunk = updated_grads / (updated_grads.norm(dim=(-2, -1), keepdim=True).clamp_min(1e-10))
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            same_sign = torch.signbit(v_chunk) == torch.signbit(param_chunk)
            v_chunk.add_(eff_wd * (param_chunk * same_sign.to(ref_param.dtype)))

            param_chunk.add_(-eff_lr * v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp_up'
        self.c_proj.label = 'mlp_down'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 2270
    lr_schedule = (0.5, 0.98)    # breakpoints for 3-part schedule: (flat, linear decay, flat)
    lr_min = 0.1
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 5, 7, 9, 11, 13)
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=0.01)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

def get_lr(step: int):
    assert step < args.num_iterations
    # Three part schedule: flat, linear decrease, flat
    lr_schedule = args.lr_schedule
    x = step / args.num_iterations

    if x < lr_schedule[0]:
        return 1.0
    elif x < lr_schedule[1]:
        progress = (x - lr_schedule[0]) / (lr_schedule[1] - lr_schedule[0])
        lr = 1.0 - (1.0 - args.lr_min) * progress
    else:
        lr = args.lr_min
    return lr

def get_ws(step: int):
    assert step <= args.num_iterations
    x = step / (args.num_iterations + 1)
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset()  #  momentum buffer not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    loss = 0
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        loss += model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps
    loss.backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Oct 28 03:42:17 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   41C    P0            128W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   32C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   32C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   31C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2270 val_loss:10.8258 train_time:0ms step_avg:0.07ms
step:1/2270 train_time:116ms step_avg:115.51ms
step:2/2270 train_time:136ms step_avg:67.92ms
step:3/2270 train_time:175ms step_avg:58.30ms
step:4/2270 train_time:231ms step_avg:57.78ms
step:5/2270 train_time:291ms step_avg:58.12ms
step:6/2270 train_time:348ms step_avg:58.06ms
step:7/2270 train_time:409ms step_avg:58.44ms
step:8/2270 train_time:467ms step_avg:58.41ms
step:9/2270 train_time:528ms step_avg:58.65ms
step:10/2270 train_time:586ms step_avg:58.60ms
step:11/2270 train_time:646ms step_avg:58.77ms
step:12/2270 train_time:705ms step_avg:58.71ms
step:13/2270 train_time:765ms step_avg:58.86ms
step:14/2270 train_time:824ms step_avg:58.85ms
step:15/2270 train_time:884ms step_avg:58.96ms
step:16/2270 train_time:943ms step_avg:58.93ms
step:17/2270 train_time:1004ms step_avg:59.04ms
step:18/2270 train_time:1064ms step_avg:59.09ms
step:19/2270 train_time:1127ms step_avg:59.29ms
step:20/2270 train_time:1186ms step_avg:59.29ms
step:21/2270 train_time:1247ms step_avg:59.40ms
step:22/2270 train_time:1306ms step_avg:59.38ms
step:23/2270 train_time:1368ms step_avg:59.48ms
step:24/2270 train_time:1427ms step_avg:59.45ms
step:25/2270 train_time:1488ms step_avg:59.52ms
step:26/2270 train_time:1546ms step_avg:59.47ms
step:27/2270 train_time:1607ms step_avg:59.52ms
step:28/2270 train_time:1666ms step_avg:59.50ms
step:29/2270 train_time:1727ms step_avg:59.54ms
step:30/2270 train_time:1785ms step_avg:59.51ms
step:31/2270 train_time:1846ms step_avg:59.54ms
step:32/2270 train_time:1904ms step_avg:59.50ms
step:33/2270 train_time:1965ms step_avg:59.55ms
step:34/2270 train_time:2024ms step_avg:59.53ms
step:35/2270 train_time:2086ms step_avg:59.60ms
step:36/2270 train_time:2145ms step_avg:59.59ms
step:37/2270 train_time:2207ms step_avg:59.64ms
step:38/2270 train_time:2266ms step_avg:59.63ms
step:39/2270 train_time:2327ms step_avg:59.66ms
step:40/2270 train_time:2386ms step_avg:59.64ms
step:41/2270 train_time:2447ms step_avg:59.68ms
step:42/2270 train_time:2505ms step_avg:59.65ms
step:43/2270 train_time:2566ms step_avg:59.68ms
step:44/2270 train_time:2624ms step_avg:59.65ms
step:45/2270 train_time:2685ms step_avg:59.67ms
step:46/2270 train_time:2744ms step_avg:59.65ms
step:47/2270 train_time:2804ms step_avg:59.66ms
step:48/2270 train_time:2862ms step_avg:59.63ms
step:49/2270 train_time:2923ms step_avg:59.65ms
step:50/2270 train_time:2982ms step_avg:59.63ms
step:51/2270 train_time:3043ms step_avg:59.66ms
step:52/2270 train_time:3102ms step_avg:59.65ms
step:53/2270 train_time:3164ms step_avg:59.69ms
step:54/2270 train_time:3223ms step_avg:59.68ms
step:55/2270 train_time:3285ms step_avg:59.73ms
step:56/2270 train_time:3343ms step_avg:59.70ms
step:57/2270 train_time:3405ms step_avg:59.73ms
step:58/2270 train_time:3463ms step_avg:59.71ms
step:59/2270 train_time:3525ms step_avg:59.74ms
step:60/2270 train_time:3583ms step_avg:59.72ms
step:61/2270 train_time:3644ms step_avg:59.73ms
step:62/2270 train_time:3702ms step_avg:59.71ms
step:63/2270 train_time:3764ms step_avg:59.74ms
step:64/2270 train_time:3822ms step_avg:59.73ms
step:65/2270 train_time:3884ms step_avg:59.75ms
step:66/2270 train_time:3942ms step_avg:59.73ms
step:67/2270 train_time:4003ms step_avg:59.75ms
step:68/2270 train_time:4062ms step_avg:59.73ms
step:69/2270 train_time:4123ms step_avg:59.75ms
step:70/2270 train_time:4182ms step_avg:59.74ms
step:71/2270 train_time:4244ms step_avg:59.77ms
step:72/2270 train_time:4303ms step_avg:59.76ms
step:73/2270 train_time:4364ms step_avg:59.78ms
step:74/2270 train_time:4423ms step_avg:59.77ms
step:75/2270 train_time:4485ms step_avg:59.79ms
step:76/2270 train_time:4543ms step_avg:59.77ms
step:77/2270 train_time:4604ms step_avg:59.79ms
step:78/2270 train_time:4663ms step_avg:59.78ms
step:79/2270 train_time:4724ms step_avg:59.80ms
step:80/2270 train_time:4782ms step_avg:59.78ms
step:81/2270 train_time:4843ms step_avg:59.79ms
step:82/2270 train_time:4902ms step_avg:59.78ms
step:83/2270 train_time:4963ms step_avg:59.79ms
step:84/2270 train_time:5021ms step_avg:59.77ms
step:85/2270 train_time:5082ms step_avg:59.78ms
step:86/2270 train_time:5140ms step_avg:59.77ms
step:87/2270 train_time:5202ms step_avg:59.79ms
step:88/2270 train_time:5260ms step_avg:59.78ms
step:89/2270 train_time:5323ms step_avg:59.81ms
step:90/2270 train_time:5382ms step_avg:59.79ms
step:91/2270 train_time:5444ms step_avg:59.82ms
step:92/2270 train_time:5502ms step_avg:59.80ms
step:93/2270 train_time:5564ms step_avg:59.83ms
step:94/2270 train_time:5623ms step_avg:59.82ms
step:95/2270 train_time:5684ms step_avg:59.83ms
step:96/2270 train_time:5743ms step_avg:59.82ms
step:97/2270 train_time:5803ms step_avg:59.83ms
step:98/2270 train_time:5862ms step_avg:59.82ms
step:99/2270 train_time:5923ms step_avg:59.83ms
step:100/2270 train_time:5981ms step_avg:59.81ms
step:101/2270 train_time:6042ms step_avg:59.83ms
step:102/2270 train_time:6100ms step_avg:59.81ms
step:103/2270 train_time:6163ms step_avg:59.83ms
step:104/2270 train_time:6221ms step_avg:59.82ms
step:105/2270 train_time:6283ms step_avg:59.84ms
step:106/2270 train_time:6342ms step_avg:59.83ms
step:107/2270 train_time:6403ms step_avg:59.84ms
step:108/2270 train_time:6462ms step_avg:59.83ms
step:109/2270 train_time:6523ms step_avg:59.84ms
step:110/2270 train_time:6582ms step_avg:59.83ms
step:111/2270 train_time:6643ms step_avg:59.85ms
step:112/2270 train_time:6701ms step_avg:59.83ms
step:113/2270 train_time:6763ms step_avg:59.85ms
step:114/2270 train_time:6821ms step_avg:59.84ms
step:115/2270 train_time:6883ms step_avg:59.85ms
step:116/2270 train_time:6941ms step_avg:59.84ms
step:117/2270 train_time:7002ms step_avg:59.85ms
step:118/2270 train_time:7061ms step_avg:59.84ms
step:119/2270 train_time:7122ms step_avg:59.85ms
step:120/2270 train_time:7182ms step_avg:59.85ms
step:121/2270 train_time:7242ms step_avg:59.85ms
step:122/2270 train_time:7301ms step_avg:59.84ms
step:123/2270 train_time:7363ms step_avg:59.86ms
step:124/2270 train_time:7422ms step_avg:59.85ms
step:125/2270 train_time:7483ms step_avg:59.87ms
step:126/2270 train_time:7542ms step_avg:59.85ms
step:127/2270 train_time:7603ms step_avg:59.87ms
step:128/2270 train_time:7662ms step_avg:59.86ms
step:129/2270 train_time:7723ms step_avg:59.87ms
step:130/2270 train_time:7781ms step_avg:59.85ms
step:131/2270 train_time:7842ms step_avg:59.87ms
step:132/2270 train_time:7901ms step_avg:59.85ms
step:133/2270 train_time:7963ms step_avg:59.87ms
step:134/2270 train_time:8021ms step_avg:59.86ms
step:135/2270 train_time:8083ms step_avg:59.88ms
step:136/2270 train_time:8142ms step_avg:59.86ms
step:137/2270 train_time:8203ms step_avg:59.87ms
step:138/2270 train_time:8261ms step_avg:59.87ms
step:139/2270 train_time:8323ms step_avg:59.88ms
step:140/2270 train_time:8381ms step_avg:59.86ms
step:141/2270 train_time:8442ms step_avg:59.88ms
step:142/2270 train_time:8501ms step_avg:59.87ms
step:143/2270 train_time:8563ms step_avg:59.88ms
step:144/2270 train_time:8621ms step_avg:59.87ms
step:145/2270 train_time:8683ms step_avg:59.88ms
step:146/2270 train_time:8741ms step_avg:59.87ms
step:147/2270 train_time:8802ms step_avg:59.88ms
step:148/2270 train_time:8860ms step_avg:59.87ms
step:149/2270 train_time:8922ms step_avg:59.88ms
step:150/2270 train_time:8981ms step_avg:59.87ms
step:151/2270 train_time:9042ms step_avg:59.88ms
step:152/2270 train_time:9100ms step_avg:59.87ms
step:153/2270 train_time:9161ms step_avg:59.87ms
step:154/2270 train_time:9220ms step_avg:59.87ms
step:155/2270 train_time:9282ms step_avg:59.89ms
step:156/2270 train_time:9341ms step_avg:59.88ms
step:157/2270 train_time:9403ms step_avg:59.89ms
step:158/2270 train_time:9461ms step_avg:59.88ms
step:159/2270 train_time:9523ms step_avg:59.89ms
step:160/2270 train_time:9581ms step_avg:59.88ms
step:161/2270 train_time:9642ms step_avg:59.89ms
step:162/2270 train_time:9701ms step_avg:59.88ms
step:163/2270 train_time:9762ms step_avg:59.89ms
step:164/2270 train_time:9820ms step_avg:59.88ms
step:165/2270 train_time:9882ms step_avg:59.89ms
step:166/2270 train_time:9940ms step_avg:59.88ms
step:167/2270 train_time:10002ms step_avg:59.89ms
step:168/2270 train_time:10060ms step_avg:59.88ms
step:169/2270 train_time:10121ms step_avg:59.89ms
step:170/2270 train_time:10179ms step_avg:59.88ms
step:171/2270 train_time:10241ms step_avg:59.89ms
step:172/2270 train_time:10299ms step_avg:59.88ms
step:173/2270 train_time:10361ms step_avg:59.89ms
step:174/2270 train_time:10420ms step_avg:59.88ms
step:175/2270 train_time:10481ms step_avg:59.89ms
step:176/2270 train_time:10540ms step_avg:59.89ms
step:177/2270 train_time:10601ms step_avg:59.89ms
step:178/2270 train_time:10660ms step_avg:59.89ms
step:179/2270 train_time:10721ms step_avg:59.90ms
step:180/2270 train_time:10780ms step_avg:59.89ms
step:181/2270 train_time:10841ms step_avg:59.89ms
step:182/2270 train_time:10900ms step_avg:59.89ms
step:183/2270 train_time:10961ms step_avg:59.90ms
step:184/2270 train_time:11020ms step_avg:59.89ms
step:185/2270 train_time:11081ms step_avg:59.90ms
step:186/2270 train_time:11139ms step_avg:59.89ms
step:187/2270 train_time:11200ms step_avg:59.89ms
step:188/2270 train_time:11259ms step_avg:59.89ms
step:189/2270 train_time:11321ms step_avg:59.90ms
step:190/2270 train_time:11381ms step_avg:59.90ms
step:191/2270 train_time:11441ms step_avg:59.90ms
step:192/2270 train_time:11499ms step_avg:59.89ms
step:193/2270 train_time:11561ms step_avg:59.90ms
step:194/2270 train_time:11620ms step_avg:59.89ms
step:195/2270 train_time:11681ms step_avg:59.90ms
step:196/2270 train_time:11740ms step_avg:59.90ms
step:197/2270 train_time:11801ms step_avg:59.90ms
step:198/2270 train_time:11860ms step_avg:59.90ms
step:199/2270 train_time:11921ms step_avg:59.90ms
step:200/2270 train_time:11980ms step_avg:59.90ms
step:201/2270 train_time:12041ms step_avg:59.90ms
step:202/2270 train_time:12100ms step_avg:59.90ms
step:203/2270 train_time:12161ms step_avg:59.91ms
step:204/2270 train_time:12220ms step_avg:59.90ms
step:205/2270 train_time:12282ms step_avg:59.91ms
step:206/2270 train_time:12340ms step_avg:59.90ms
step:207/2270 train_time:12401ms step_avg:59.91ms
step:208/2270 train_time:12460ms step_avg:59.90ms
step:209/2270 train_time:12521ms step_avg:59.91ms
step:210/2270 train_time:12580ms step_avg:59.90ms
step:211/2270 train_time:12641ms step_avg:59.91ms
step:212/2270 train_time:12699ms step_avg:59.90ms
step:213/2270 train_time:12761ms step_avg:59.91ms
step:214/2270 train_time:12820ms step_avg:59.90ms
step:215/2270 train_time:12881ms step_avg:59.91ms
step:216/2270 train_time:12939ms step_avg:59.90ms
step:217/2270 train_time:13000ms step_avg:59.91ms
step:218/2270 train_time:13060ms step_avg:59.91ms
step:219/2270 train_time:13121ms step_avg:59.91ms
step:220/2270 train_time:13179ms step_avg:59.91ms
step:221/2270 train_time:13240ms step_avg:59.91ms
step:222/2270 train_time:13299ms step_avg:59.91ms
step:223/2270 train_time:13361ms step_avg:59.92ms
step:224/2270 train_time:13420ms step_avg:59.91ms
step:225/2270 train_time:13482ms step_avg:59.92ms
step:226/2270 train_time:13541ms step_avg:59.91ms
step:227/2270 train_time:13602ms step_avg:59.92ms
step:228/2270 train_time:13661ms step_avg:59.92ms
step:229/2270 train_time:13722ms step_avg:59.92ms
step:230/2270 train_time:13780ms step_avg:59.91ms
step:231/2270 train_time:13841ms step_avg:59.92ms
step:232/2270 train_time:13900ms step_avg:59.91ms
step:233/2270 train_time:13961ms step_avg:59.92ms
step:234/2270 train_time:14020ms step_avg:59.91ms
step:235/2270 train_time:14081ms step_avg:59.92ms
step:236/2270 train_time:14139ms step_avg:59.91ms
step:237/2270 train_time:14201ms step_avg:59.92ms
step:238/2270 train_time:14260ms step_avg:59.92ms
step:239/2270 train_time:14322ms step_avg:59.92ms
step:240/2270 train_time:14381ms step_avg:59.92ms
step:241/2270 train_time:14442ms step_avg:59.92ms
step:242/2270 train_time:14500ms step_avg:59.92ms
step:243/2270 train_time:14562ms step_avg:59.92ms
step:244/2270 train_time:14620ms step_avg:59.92ms
step:245/2270 train_time:14682ms step_avg:59.92ms
step:246/2270 train_time:14740ms step_avg:59.92ms
step:247/2270 train_time:14801ms step_avg:59.92ms
step:248/2270 train_time:14860ms step_avg:59.92ms
step:249/2270 train_time:14922ms step_avg:59.93ms
step:250/2270 train_time:14980ms step_avg:59.92ms
step:250/2270 val_loss:4.0752 train_time:15042ms step_avg:60.17ms
step:251/2270 train_time:15060ms step_avg:60.00ms
step:252/2270 train_time:15103ms step_avg:59.93ms
step:253/2270 train_time:15170ms step_avg:59.96ms
step:254/2270 train_time:15234ms step_avg:59.98ms
step:255/2270 train_time:15297ms step_avg:59.99ms
step:256/2270 train_time:15355ms step_avg:59.98ms
step:257/2270 train_time:15415ms step_avg:59.98ms
step:258/2270 train_time:15473ms step_avg:59.97ms
step:259/2270 train_time:15533ms step_avg:59.97ms
step:260/2270 train_time:15591ms step_avg:59.97ms
step:261/2270 train_time:15651ms step_avg:59.96ms
step:262/2270 train_time:15709ms step_avg:59.96ms
step:263/2270 train_time:15769ms step_avg:59.96ms
step:264/2270 train_time:15827ms step_avg:59.95ms
step:265/2270 train_time:15887ms step_avg:59.95ms
step:266/2270 train_time:15945ms step_avg:59.94ms
step:267/2270 train_time:16006ms step_avg:59.95ms
step:268/2270 train_time:16065ms step_avg:59.95ms
step:269/2270 train_time:16129ms step_avg:59.96ms
step:270/2270 train_time:16189ms step_avg:59.96ms
step:271/2270 train_time:16252ms step_avg:59.97ms
step:272/2270 train_time:16311ms step_avg:59.97ms
step:273/2270 train_time:16373ms step_avg:59.97ms
step:274/2270 train_time:16431ms step_avg:59.97ms
step:275/2270 train_time:16492ms step_avg:59.97ms
step:276/2270 train_time:16550ms step_avg:59.96ms
step:277/2270 train_time:16611ms step_avg:59.97ms
step:278/2270 train_time:16669ms step_avg:59.96ms
step:279/2270 train_time:16729ms step_avg:59.96ms
step:280/2270 train_time:16787ms step_avg:59.95ms
step:281/2270 train_time:16847ms step_avg:59.95ms
step:282/2270 train_time:16905ms step_avg:59.95ms
step:283/2270 train_time:16966ms step_avg:59.95ms
step:284/2270 train_time:17025ms step_avg:59.95ms
step:285/2270 train_time:17086ms step_avg:59.95ms
step:286/2270 train_time:17146ms step_avg:59.95ms
step:287/2270 train_time:17209ms step_avg:59.96ms
step:288/2270 train_time:17269ms step_avg:59.96ms
step:289/2270 train_time:17331ms step_avg:59.97ms
step:290/2270 train_time:17389ms step_avg:59.96ms
step:291/2270 train_time:17451ms step_avg:59.97ms
step:292/2270 train_time:17510ms step_avg:59.96ms
step:293/2270 train_time:17570ms step_avg:59.97ms
step:294/2270 train_time:17629ms step_avg:59.96ms
step:295/2270 train_time:17689ms step_avg:59.96ms
step:296/2270 train_time:17747ms step_avg:59.96ms
step:297/2270 train_time:17808ms step_avg:59.96ms
step:298/2270 train_time:17866ms step_avg:59.95ms
step:299/2270 train_time:17927ms step_avg:59.96ms
step:300/2270 train_time:17986ms step_avg:59.95ms
step:301/2270 train_time:18047ms step_avg:59.96ms
step:302/2270 train_time:18106ms step_avg:59.95ms
step:303/2270 train_time:18168ms step_avg:59.96ms
step:304/2270 train_time:18228ms step_avg:59.96ms
step:305/2270 train_time:18290ms step_avg:59.97ms
step:306/2270 train_time:18349ms step_avg:59.97ms
step:307/2270 train_time:18411ms step_avg:59.97ms
step:308/2270 train_time:18470ms step_avg:59.97ms
step:309/2270 train_time:18530ms step_avg:59.97ms
step:310/2270 train_time:18589ms step_avg:59.96ms
step:311/2270 train_time:18649ms step_avg:59.96ms
step:312/2270 train_time:18707ms step_avg:59.96ms
step:313/2270 train_time:18768ms step_avg:59.96ms
step:314/2270 train_time:18826ms step_avg:59.96ms
step:315/2270 train_time:18887ms step_avg:59.96ms
step:316/2270 train_time:18945ms step_avg:59.95ms
step:317/2270 train_time:19006ms step_avg:59.96ms
step:318/2270 train_time:19065ms step_avg:59.95ms
step:319/2270 train_time:19127ms step_avg:59.96ms
step:320/2270 train_time:19186ms step_avg:59.96ms
step:321/2270 train_time:19248ms step_avg:59.96ms
step:322/2270 train_time:19308ms step_avg:59.96ms
step:323/2270 train_time:19370ms step_avg:59.97ms
step:324/2270 train_time:19429ms step_avg:59.97ms
step:325/2270 train_time:19490ms step_avg:59.97ms
step:326/2270 train_time:19549ms step_avg:59.96ms
step:327/2270 train_time:19610ms step_avg:59.97ms
step:328/2270 train_time:19668ms step_avg:59.96ms
step:329/2270 train_time:19729ms step_avg:59.97ms
step:330/2270 train_time:19787ms step_avg:59.96ms
step:331/2270 train_time:19848ms step_avg:59.96ms
step:332/2270 train_time:19907ms step_avg:59.96ms
step:333/2270 train_time:19968ms step_avg:59.96ms
step:334/2270 train_time:20026ms step_avg:59.96ms
step:335/2270 train_time:20087ms step_avg:59.96ms
step:336/2270 train_time:20146ms step_avg:59.96ms
step:337/2270 train_time:20207ms step_avg:59.96ms
step:338/2270 train_time:20266ms step_avg:59.96ms
step:339/2270 train_time:20328ms step_avg:59.96ms
step:340/2270 train_time:20387ms step_avg:59.96ms
step:341/2270 train_time:20449ms step_avg:59.97ms
step:342/2270 train_time:20509ms step_avg:59.97ms
step:343/2270 train_time:20570ms step_avg:59.97ms
step:344/2270 train_time:20628ms step_avg:59.96ms
step:345/2270 train_time:20689ms step_avg:59.97ms
step:346/2270 train_time:20747ms step_avg:59.96ms
step:347/2270 train_time:20808ms step_avg:59.96ms
step:348/2270 train_time:20867ms step_avg:59.96ms
step:349/2270 train_time:20928ms step_avg:59.96ms
step:350/2270 train_time:20986ms step_avg:59.96ms
step:351/2270 train_time:21048ms step_avg:59.97ms
step:352/2270 train_time:21107ms step_avg:59.96ms
step:353/2270 train_time:21168ms step_avg:59.97ms
step:354/2270 train_time:21227ms step_avg:59.96ms
step:355/2270 train_time:21289ms step_avg:59.97ms
step:356/2270 train_time:21347ms step_avg:59.96ms
step:357/2270 train_time:21409ms step_avg:59.97ms
step:358/2270 train_time:21469ms step_avg:59.97ms
step:359/2270 train_time:21530ms step_avg:59.97ms
step:360/2270 train_time:21589ms step_avg:59.97ms
step:361/2270 train_time:21649ms step_avg:59.97ms
step:362/2270 train_time:21708ms step_avg:59.97ms
step:363/2270 train_time:21769ms step_avg:59.97ms
step:364/2270 train_time:21827ms step_avg:59.96ms
step:365/2270 train_time:21888ms step_avg:59.97ms
step:366/2270 train_time:21946ms step_avg:59.96ms
step:367/2270 train_time:22007ms step_avg:59.96ms
step:368/2270 train_time:22066ms step_avg:59.96ms
step:369/2270 train_time:22127ms step_avg:59.96ms
step:370/2270 train_time:22186ms step_avg:59.96ms
step:371/2270 train_time:22247ms step_avg:59.97ms
step:372/2270 train_time:22308ms step_avg:59.97ms
step:373/2270 train_time:22369ms step_avg:59.97ms
step:374/2270 train_time:22428ms step_avg:59.97ms
step:375/2270 train_time:22489ms step_avg:59.97ms
step:376/2270 train_time:22548ms step_avg:59.97ms
step:377/2270 train_time:22610ms step_avg:59.97ms
step:378/2270 train_time:22669ms step_avg:59.97ms
step:379/2270 train_time:22730ms step_avg:59.97ms
step:380/2270 train_time:22788ms step_avg:59.97ms
step:381/2270 train_time:22850ms step_avg:59.97ms
step:382/2270 train_time:22908ms step_avg:59.97ms
step:383/2270 train_time:22970ms step_avg:59.97ms
step:384/2270 train_time:23029ms step_avg:59.97ms
step:385/2270 train_time:23090ms step_avg:59.97ms
step:386/2270 train_time:23149ms step_avg:59.97ms
step:387/2270 train_time:23210ms step_avg:59.97ms
step:388/2270 train_time:23270ms step_avg:59.97ms
step:389/2270 train_time:23333ms step_avg:59.98ms
step:390/2270 train_time:23391ms step_avg:59.98ms
step:391/2270 train_time:23452ms step_avg:59.98ms
step:392/2270 train_time:23511ms step_avg:59.98ms
step:393/2270 train_time:23572ms step_avg:59.98ms
step:394/2270 train_time:23631ms step_avg:59.98ms
step:395/2270 train_time:23693ms step_avg:59.98ms
step:396/2270 train_time:23751ms step_avg:59.98ms
step:397/2270 train_time:23812ms step_avg:59.98ms
step:398/2270 train_time:23870ms step_avg:59.98ms
step:399/2270 train_time:23932ms step_avg:59.98ms
step:400/2270 train_time:23990ms step_avg:59.97ms
step:401/2270 train_time:24051ms step_avg:59.98ms
step:402/2270 train_time:24110ms step_avg:59.97ms
step:403/2270 train_time:24171ms step_avg:59.98ms
step:404/2270 train_time:24230ms step_avg:59.98ms
step:405/2270 train_time:24292ms step_avg:59.98ms
step:406/2270 train_time:24351ms step_avg:59.98ms
step:407/2270 train_time:24412ms step_avg:59.98ms
step:408/2270 train_time:24471ms step_avg:59.98ms
step:409/2270 train_time:24533ms step_avg:59.98ms
step:410/2270 train_time:24591ms step_avg:59.98ms
step:411/2270 train_time:24652ms step_avg:59.98ms
step:412/2270 train_time:24711ms step_avg:59.98ms
step:413/2270 train_time:24773ms step_avg:59.98ms
step:414/2270 train_time:24831ms step_avg:59.98ms
step:415/2270 train_time:24892ms step_avg:59.98ms
step:416/2270 train_time:24950ms step_avg:59.98ms
step:417/2270 train_time:25011ms step_avg:59.98ms
step:418/2270 train_time:25070ms step_avg:59.98ms
step:419/2270 train_time:25131ms step_avg:59.98ms
step:420/2270 train_time:25190ms step_avg:59.98ms
step:421/2270 train_time:25251ms step_avg:59.98ms
step:422/2270 train_time:25310ms step_avg:59.98ms
step:423/2270 train_time:25372ms step_avg:59.98ms
step:424/2270 train_time:25431ms step_avg:59.98ms
step:425/2270 train_time:25492ms step_avg:59.98ms
step:426/2270 train_time:25551ms step_avg:59.98ms
step:427/2270 train_time:25612ms step_avg:59.98ms
step:428/2270 train_time:25671ms step_avg:59.98ms
step:429/2270 train_time:25733ms step_avg:59.98ms
step:430/2270 train_time:25791ms step_avg:59.98ms
step:431/2270 train_time:25852ms step_avg:59.98ms
step:432/2270 train_time:25911ms step_avg:59.98ms
step:433/2270 train_time:25972ms step_avg:59.98ms
step:434/2270 train_time:26031ms step_avg:59.98ms
step:435/2270 train_time:26093ms step_avg:59.98ms
step:436/2270 train_time:26151ms step_avg:59.98ms
step:437/2270 train_time:26212ms step_avg:59.98ms
step:438/2270 train_time:26271ms step_avg:59.98ms
step:439/2270 train_time:26333ms step_avg:59.98ms
step:440/2270 train_time:26391ms step_avg:59.98ms
step:441/2270 train_time:26452ms step_avg:59.98ms
step:442/2270 train_time:26511ms step_avg:59.98ms
step:443/2270 train_time:26573ms step_avg:59.98ms
step:444/2270 train_time:26631ms step_avg:59.98ms
step:445/2270 train_time:26693ms step_avg:59.98ms
step:446/2270 train_time:26751ms step_avg:59.98ms
step:447/2270 train_time:26813ms step_avg:59.98ms
step:448/2270 train_time:26871ms step_avg:59.98ms
step:449/2270 train_time:26932ms step_avg:59.98ms
step:450/2270 train_time:26991ms step_avg:59.98ms
step:451/2270 train_time:27052ms step_avg:59.98ms
step:452/2270 train_time:27110ms step_avg:59.98ms
step:453/2270 train_time:27172ms step_avg:59.98ms
step:454/2270 train_time:27231ms step_avg:59.98ms
step:455/2270 train_time:27292ms step_avg:59.98ms
step:456/2270 train_time:27350ms step_avg:59.98ms
step:457/2270 train_time:27412ms step_avg:59.98ms
step:458/2270 train_time:27471ms step_avg:59.98ms
step:459/2270 train_time:27532ms step_avg:59.98ms
step:460/2270 train_time:27591ms step_avg:59.98ms
step:461/2270 train_time:27652ms step_avg:59.98ms
step:462/2270 train_time:27711ms step_avg:59.98ms
step:463/2270 train_time:27772ms step_avg:59.98ms
step:464/2270 train_time:27831ms step_avg:59.98ms
step:465/2270 train_time:27892ms step_avg:59.98ms
step:466/2270 train_time:27951ms step_avg:59.98ms
step:467/2270 train_time:28012ms step_avg:59.98ms
step:468/2270 train_time:28071ms step_avg:59.98ms
step:469/2270 train_time:28132ms step_avg:59.98ms
step:470/2270 train_time:28190ms step_avg:59.98ms
step:471/2270 train_time:28251ms step_avg:59.98ms
step:472/2270 train_time:28310ms step_avg:59.98ms
step:473/2270 train_time:28371ms step_avg:59.98ms
step:474/2270 train_time:28431ms step_avg:59.98ms
step:475/2270 train_time:28492ms step_avg:59.98ms
step:476/2270 train_time:28551ms step_avg:59.98ms
step:477/2270 train_time:28613ms step_avg:59.98ms
step:478/2270 train_time:28671ms step_avg:59.98ms
step:479/2270 train_time:28733ms step_avg:59.98ms
step:480/2270 train_time:28791ms step_avg:59.98ms
step:481/2270 train_time:28853ms step_avg:59.98ms
step:482/2270 train_time:28911ms step_avg:59.98ms
step:483/2270 train_time:28972ms step_avg:59.98ms
step:484/2270 train_time:29031ms step_avg:59.98ms
step:485/2270 train_time:29091ms step_avg:59.98ms
step:486/2270 train_time:29150ms step_avg:59.98ms
step:487/2270 train_time:29212ms step_avg:59.98ms
step:488/2270 train_time:29270ms step_avg:59.98ms
step:489/2270 train_time:29331ms step_avg:59.98ms
step:490/2270 train_time:29390ms step_avg:59.98ms
step:491/2270 train_time:29452ms step_avg:59.98ms
step:492/2270 train_time:29511ms step_avg:59.98ms
step:493/2270 train_time:29572ms step_avg:59.98ms
step:494/2270 train_time:29631ms step_avg:59.98ms
step:495/2270 train_time:29692ms step_avg:59.98ms
step:496/2270 train_time:29751ms step_avg:59.98ms
step:497/2270 train_time:29812ms step_avg:59.98ms
step:498/2270 train_time:29871ms step_avg:59.98ms
step:499/2270 train_time:29933ms step_avg:59.99ms
step:500/2270 train_time:29991ms step_avg:59.98ms
step:500/2270 val_loss:3.7957 train_time:30053ms step_avg:60.11ms
step:501/2270 train_time:30072ms step_avg:60.02ms
step:502/2270 train_time:30113ms step_avg:59.99ms
step:503/2270 train_time:30174ms step_avg:59.99ms
step:504/2270 train_time:30233ms step_avg:59.99ms
step:505/2270 train_time:30295ms step_avg:59.99ms
step:506/2270 train_time:30354ms step_avg:59.99ms
step:507/2270 train_time:30415ms step_avg:59.99ms
step:508/2270 train_time:30473ms step_avg:59.99ms
step:509/2270 train_time:30534ms step_avg:59.99ms
step:510/2270 train_time:30592ms step_avg:59.98ms
step:511/2270 train_time:30652ms step_avg:59.98ms
step:512/2270 train_time:30710ms step_avg:59.98ms
step:513/2270 train_time:30770ms step_avg:59.98ms
step:514/2270 train_time:30828ms step_avg:59.98ms
step:515/2270 train_time:30889ms step_avg:59.98ms
step:516/2270 train_time:30948ms step_avg:59.98ms
step:517/2270 train_time:31011ms step_avg:59.98ms
step:518/2270 train_time:31070ms step_avg:59.98ms
step:519/2270 train_time:31132ms step_avg:59.98ms
step:520/2270 train_time:31191ms step_avg:59.98ms
step:521/2270 train_time:31252ms step_avg:59.99ms
step:522/2270 train_time:31312ms step_avg:59.98ms
step:523/2270 train_time:31373ms step_avg:59.99ms
step:524/2270 train_time:31431ms step_avg:59.98ms
step:525/2270 train_time:31492ms step_avg:59.99ms
step:526/2270 train_time:31550ms step_avg:59.98ms
step:527/2270 train_time:31613ms step_avg:59.99ms
step:528/2270 train_time:31669ms step_avg:59.98ms
step:529/2270 train_time:31730ms step_avg:59.98ms
step:530/2270 train_time:31788ms step_avg:59.98ms
step:531/2270 train_time:31849ms step_avg:59.98ms
step:532/2270 train_time:31908ms step_avg:59.98ms
step:533/2270 train_time:31970ms step_avg:59.98ms
step:534/2270 train_time:32029ms step_avg:59.98ms
step:535/2270 train_time:32090ms step_avg:59.98ms
step:536/2270 train_time:32149ms step_avg:59.98ms
step:537/2270 train_time:32211ms step_avg:59.98ms
step:538/2270 train_time:32270ms step_avg:59.98ms
step:539/2270 train_time:32332ms step_avg:59.99ms
step:540/2270 train_time:32391ms step_avg:59.98ms
step:541/2270 train_time:32452ms step_avg:59.99ms
step:542/2270 train_time:32510ms step_avg:59.98ms
step:543/2270 train_time:32571ms step_avg:59.98ms
step:544/2270 train_time:32630ms step_avg:59.98ms
step:545/2270 train_time:32691ms step_avg:59.98ms
step:546/2270 train_time:32749ms step_avg:59.98ms
step:547/2270 train_time:32810ms step_avg:59.98ms
step:548/2270 train_time:32868ms step_avg:59.98ms
step:549/2270 train_time:32929ms step_avg:59.98ms
step:550/2270 train_time:32988ms step_avg:59.98ms
step:551/2270 train_time:33050ms step_avg:59.98ms
step:552/2270 train_time:33109ms step_avg:59.98ms
step:553/2270 train_time:33171ms step_avg:59.98ms
step:554/2270 train_time:33230ms step_avg:59.98ms
step:555/2270 train_time:33292ms step_avg:59.99ms
step:556/2270 train_time:33351ms step_avg:59.98ms
step:557/2270 train_time:33412ms step_avg:59.99ms
step:558/2270 train_time:33470ms step_avg:59.98ms
step:559/2270 train_time:33532ms step_avg:59.99ms
step:560/2270 train_time:33590ms step_avg:59.98ms
step:561/2270 train_time:33651ms step_avg:59.98ms
step:562/2270 train_time:33709ms step_avg:59.98ms
step:563/2270 train_time:33771ms step_avg:59.98ms
step:564/2270 train_time:33829ms step_avg:59.98ms
step:565/2270 train_time:33890ms step_avg:59.98ms
step:566/2270 train_time:33949ms step_avg:59.98ms
step:567/2270 train_time:34010ms step_avg:59.98ms
step:568/2270 train_time:34068ms step_avg:59.98ms
step:569/2270 train_time:34131ms step_avg:59.98ms
step:570/2270 train_time:34189ms step_avg:59.98ms
step:571/2270 train_time:34251ms step_avg:59.98ms
step:572/2270 train_time:34310ms step_avg:59.98ms
step:573/2270 train_time:34371ms step_avg:59.99ms
step:574/2270 train_time:34430ms step_avg:59.98ms
step:575/2270 train_time:34491ms step_avg:59.98ms
step:576/2270 train_time:34550ms step_avg:59.98ms
step:577/2270 train_time:34611ms step_avg:59.98ms
step:578/2270 train_time:34669ms step_avg:59.98ms
step:579/2270 train_time:34730ms step_avg:59.98ms
step:580/2270 train_time:34789ms step_avg:59.98ms
step:581/2270 train_time:34850ms step_avg:59.98ms
step:582/2270 train_time:34909ms step_avg:59.98ms
step:583/2270 train_time:34970ms step_avg:59.98ms
step:584/2270 train_time:35029ms step_avg:59.98ms
step:585/2270 train_time:35090ms step_avg:59.98ms
step:586/2270 train_time:35149ms step_avg:59.98ms
step:587/2270 train_time:35211ms step_avg:59.98ms
step:588/2270 train_time:35270ms step_avg:59.98ms
step:589/2270 train_time:35332ms step_avg:59.99ms
step:590/2270 train_time:35391ms step_avg:59.99ms
step:591/2270 train_time:35452ms step_avg:59.99ms
step:592/2270 train_time:35511ms step_avg:59.98ms
step:593/2270 train_time:35572ms step_avg:59.99ms
step:594/2270 train_time:35630ms step_avg:59.98ms
step:595/2270 train_time:35691ms step_avg:59.98ms
step:596/2270 train_time:35750ms step_avg:59.98ms
step:597/2270 train_time:35810ms step_avg:59.98ms
step:598/2270 train_time:35869ms step_avg:59.98ms
step:599/2270 train_time:35930ms step_avg:59.98ms
step:600/2270 train_time:35989ms step_avg:59.98ms
step:601/2270 train_time:36050ms step_avg:59.98ms
step:602/2270 train_time:36109ms step_avg:59.98ms
step:603/2270 train_time:36171ms step_avg:59.99ms
step:604/2270 train_time:36230ms step_avg:59.98ms
step:605/2270 train_time:36291ms step_avg:59.99ms
step:606/2270 train_time:36350ms step_avg:59.98ms
step:607/2270 train_time:36412ms step_avg:59.99ms
step:608/2270 train_time:36470ms step_avg:59.98ms
step:609/2270 train_time:36531ms step_avg:59.99ms
step:610/2270 train_time:36590ms step_avg:59.98ms
step:611/2270 train_time:36651ms step_avg:59.99ms
step:612/2270 train_time:36710ms step_avg:59.98ms
step:613/2270 train_time:36771ms step_avg:59.98ms
step:614/2270 train_time:36829ms step_avg:59.98ms
step:615/2270 train_time:36890ms step_avg:59.98ms
step:616/2270 train_time:36949ms step_avg:59.98ms
step:617/2270 train_time:37010ms step_avg:59.98ms
step:618/2270 train_time:37069ms step_avg:59.98ms
step:619/2270 train_time:37131ms step_avg:59.98ms
step:620/2270 train_time:37190ms step_avg:59.98ms
step:621/2270 train_time:37251ms step_avg:59.99ms
step:622/2270 train_time:37310ms step_avg:59.98ms
step:623/2270 train_time:37372ms step_avg:59.99ms
step:624/2270 train_time:37431ms step_avg:59.98ms
step:625/2270 train_time:37492ms step_avg:59.99ms
step:626/2270 train_time:37551ms step_avg:59.99ms
step:627/2270 train_time:37612ms step_avg:59.99ms
step:628/2270 train_time:37670ms step_avg:59.98ms
step:629/2270 train_time:37732ms step_avg:59.99ms
step:630/2270 train_time:37790ms step_avg:59.98ms
step:631/2270 train_time:37851ms step_avg:59.99ms
step:632/2270 train_time:37909ms step_avg:59.98ms
step:633/2270 train_time:37970ms step_avg:59.98ms
step:634/2270 train_time:38029ms step_avg:59.98ms
step:635/2270 train_time:38091ms step_avg:59.99ms
step:636/2270 train_time:38150ms step_avg:59.98ms
step:637/2270 train_time:38211ms step_avg:59.99ms
step:638/2270 train_time:38270ms step_avg:59.98ms
step:639/2270 train_time:38332ms step_avg:59.99ms
step:640/2270 train_time:38390ms step_avg:59.99ms
step:641/2270 train_time:38452ms step_avg:59.99ms
step:642/2270 train_time:38513ms step_avg:59.99ms
step:643/2270 train_time:38572ms step_avg:59.99ms
step:644/2270 train_time:38630ms step_avg:59.99ms
step:645/2270 train_time:38692ms step_avg:59.99ms
step:646/2270 train_time:38750ms step_avg:59.98ms
step:647/2270 train_time:38811ms step_avg:59.99ms
step:648/2270 train_time:38869ms step_avg:59.98ms
step:649/2270 train_time:38930ms step_avg:59.99ms
step:650/2270 train_time:38989ms step_avg:59.98ms
step:651/2270 train_time:39051ms step_avg:59.99ms
step:652/2270 train_time:39110ms step_avg:59.98ms
step:653/2270 train_time:39171ms step_avg:59.99ms
step:654/2270 train_time:39229ms step_avg:59.98ms
step:655/2270 train_time:39292ms step_avg:59.99ms
step:656/2270 train_time:39351ms step_avg:59.99ms
step:657/2270 train_time:39412ms step_avg:59.99ms
step:658/2270 train_time:39470ms step_avg:59.99ms
step:659/2270 train_time:39531ms step_avg:59.99ms
step:660/2270 train_time:39590ms step_avg:59.99ms
step:661/2270 train_time:39652ms step_avg:59.99ms
step:662/2270 train_time:39710ms step_avg:59.99ms
step:663/2270 train_time:39772ms step_avg:59.99ms
step:664/2270 train_time:39830ms step_avg:59.98ms
step:665/2270 train_time:39891ms step_avg:59.99ms
step:666/2270 train_time:39950ms step_avg:59.98ms
step:667/2270 train_time:40012ms step_avg:59.99ms
step:668/2270 train_time:40070ms step_avg:59.99ms
step:669/2270 train_time:40131ms step_avg:59.99ms
step:670/2270 train_time:40190ms step_avg:59.98ms
step:671/2270 train_time:40251ms step_avg:59.99ms
step:672/2270 train_time:40310ms step_avg:59.98ms
step:673/2270 train_time:40371ms step_avg:59.99ms
step:674/2270 train_time:40430ms step_avg:59.98ms
step:675/2270 train_time:40491ms step_avg:59.99ms
step:676/2270 train_time:40550ms step_avg:59.98ms
step:677/2270 train_time:40611ms step_avg:59.99ms
step:678/2270 train_time:40670ms step_avg:59.98ms
step:679/2270 train_time:40731ms step_avg:59.99ms
step:680/2270 train_time:40790ms step_avg:59.98ms
step:681/2270 train_time:40851ms step_avg:59.99ms
step:682/2270 train_time:40910ms step_avg:59.98ms
step:683/2270 train_time:40971ms step_avg:59.99ms
step:684/2270 train_time:41030ms step_avg:59.98ms
step:685/2270 train_time:41091ms step_avg:59.99ms
step:686/2270 train_time:41149ms step_avg:59.98ms
step:687/2270 train_time:41213ms step_avg:59.99ms
step:688/2270 train_time:41270ms step_avg:59.99ms
step:689/2270 train_time:41332ms step_avg:59.99ms
step:690/2270 train_time:41390ms step_avg:59.99ms
step:691/2270 train_time:41451ms step_avg:59.99ms
step:692/2270 train_time:41510ms step_avg:59.99ms
step:693/2270 train_time:41571ms step_avg:59.99ms
step:694/2270 train_time:41629ms step_avg:59.98ms
step:695/2270 train_time:41691ms step_avg:59.99ms
step:696/2270 train_time:41750ms step_avg:59.99ms
step:697/2270 train_time:41811ms step_avg:59.99ms
step:698/2270 train_time:41870ms step_avg:59.99ms
step:699/2270 train_time:41931ms step_avg:59.99ms
step:700/2270 train_time:41989ms step_avg:59.98ms
step:701/2270 train_time:42051ms step_avg:59.99ms
step:702/2270 train_time:42109ms step_avg:59.98ms
step:703/2270 train_time:42171ms step_avg:59.99ms
step:704/2270 train_time:42229ms step_avg:59.98ms
step:705/2270 train_time:42291ms step_avg:59.99ms
step:706/2270 train_time:42350ms step_avg:59.99ms
step:707/2270 train_time:42413ms step_avg:59.99ms
step:708/2270 train_time:42470ms step_avg:59.99ms
step:709/2270 train_time:42531ms step_avg:59.99ms
step:710/2270 train_time:42590ms step_avg:59.99ms
step:711/2270 train_time:42651ms step_avg:59.99ms
step:712/2270 train_time:42709ms step_avg:59.99ms
step:713/2270 train_time:42771ms step_avg:59.99ms
step:714/2270 train_time:42830ms step_avg:59.99ms
step:715/2270 train_time:42891ms step_avg:59.99ms
step:716/2270 train_time:42950ms step_avg:59.99ms
step:717/2270 train_time:43011ms step_avg:59.99ms
step:718/2270 train_time:43070ms step_avg:59.99ms
step:719/2270 train_time:43131ms step_avg:59.99ms
step:720/2270 train_time:43190ms step_avg:59.99ms
step:721/2270 train_time:43251ms step_avg:59.99ms
step:722/2270 train_time:43310ms step_avg:59.99ms
step:723/2270 train_time:43371ms step_avg:59.99ms
step:724/2270 train_time:43429ms step_avg:59.99ms
step:725/2270 train_time:43491ms step_avg:59.99ms
step:726/2270 train_time:43550ms step_avg:59.99ms
step:727/2270 train_time:43612ms step_avg:59.99ms
step:728/2270 train_time:43670ms step_avg:59.99ms
step:729/2270 train_time:43731ms step_avg:59.99ms
step:730/2270 train_time:43790ms step_avg:59.99ms
step:731/2270 train_time:43851ms step_avg:59.99ms
step:732/2270 train_time:43910ms step_avg:59.99ms
step:733/2270 train_time:43971ms step_avg:59.99ms
step:734/2270 train_time:44030ms step_avg:59.99ms
step:735/2270 train_time:44091ms step_avg:59.99ms
step:736/2270 train_time:44149ms step_avg:59.99ms
step:737/2270 train_time:44210ms step_avg:59.99ms
step:738/2270 train_time:44269ms step_avg:59.99ms
step:739/2270 train_time:44331ms step_avg:59.99ms
step:740/2270 train_time:44390ms step_avg:59.99ms
step:741/2270 train_time:44451ms step_avg:59.99ms
step:742/2270 train_time:44510ms step_avg:59.99ms
step:743/2270 train_time:44571ms step_avg:59.99ms
step:744/2270 train_time:44630ms step_avg:59.99ms
step:745/2270 train_time:44692ms step_avg:59.99ms
step:746/2270 train_time:44750ms step_avg:59.99ms
step:747/2270 train_time:44811ms step_avg:59.99ms
step:748/2270 train_time:44870ms step_avg:59.99ms
step:749/2270 train_time:44931ms step_avg:59.99ms
step:750/2270 train_time:44990ms step_avg:59.99ms
step:750/2270 val_loss:3.6623 train_time:45053ms step_avg:60.07ms
step:751/2270 train_time:45077ms step_avg:60.02ms
step:752/2270 train_time:45115ms step_avg:59.99ms
step:753/2270 train_time:45180ms step_avg:60.00ms
step:754/2270 train_time:45242ms step_avg:60.00ms
step:755/2270 train_time:45304ms step_avg:60.00ms
step:756/2270 train_time:45364ms step_avg:60.01ms
step:757/2270 train_time:45423ms step_avg:60.00ms
step:758/2270 train_time:45481ms step_avg:60.00ms
step:759/2270 train_time:45543ms step_avg:60.00ms
step:760/2270 train_time:45601ms step_avg:60.00ms
step:761/2270 train_time:45662ms step_avg:60.00ms
step:762/2270 train_time:45720ms step_avg:60.00ms
step:763/2270 train_time:45781ms step_avg:60.00ms
step:764/2270 train_time:45839ms step_avg:60.00ms
step:765/2270 train_time:45900ms step_avg:60.00ms
step:766/2270 train_time:45959ms step_avg:60.00ms
step:767/2270 train_time:46020ms step_avg:60.00ms
step:768/2270 train_time:46080ms step_avg:60.00ms
step:769/2270 train_time:46143ms step_avg:60.00ms
step:770/2270 train_time:46204ms step_avg:60.01ms
step:771/2270 train_time:46266ms step_avg:60.01ms
step:772/2270 train_time:46325ms step_avg:60.01ms
step:773/2270 train_time:46386ms step_avg:60.01ms
step:774/2270 train_time:46446ms step_avg:60.01ms
step:775/2270 train_time:46507ms step_avg:60.01ms
step:776/2270 train_time:46566ms step_avg:60.01ms
step:777/2270 train_time:46628ms step_avg:60.01ms
step:778/2270 train_time:46688ms step_avg:60.01ms
step:779/2270 train_time:46750ms step_avg:60.01ms
step:780/2270 train_time:46809ms step_avg:60.01ms
step:781/2270 train_time:46870ms step_avg:60.01ms
step:782/2270 train_time:46930ms step_avg:60.01ms
step:783/2270 train_time:46992ms step_avg:60.02ms
step:784/2270 train_time:47052ms step_avg:60.02ms
step:785/2270 train_time:47116ms step_avg:60.02ms
step:786/2270 train_time:47176ms step_avg:60.02ms
step:787/2270 train_time:47238ms step_avg:60.02ms
step:788/2270 train_time:47297ms step_avg:60.02ms
step:789/2270 train_time:47359ms step_avg:60.02ms
step:790/2270 train_time:47418ms step_avg:60.02ms
step:791/2270 train_time:47479ms step_avg:60.02ms
step:792/2270 train_time:47538ms step_avg:60.02ms
step:793/2270 train_time:47599ms step_avg:60.02ms
step:794/2270 train_time:47658ms step_avg:60.02ms
step:795/2270 train_time:47719ms step_avg:60.02ms
step:796/2270 train_time:47778ms step_avg:60.02ms
step:797/2270 train_time:47839ms step_avg:60.02ms
step:798/2270 train_time:47898ms step_avg:60.02ms
step:799/2270 train_time:47961ms step_avg:60.03ms
step:800/2270 train_time:48018ms step_avg:60.02ms
step:801/2270 train_time:48079ms step_avg:60.02ms
step:802/2270 train_time:48139ms step_avg:60.02ms
step:803/2270 train_time:48200ms step_avg:60.03ms
step:804/2270 train_time:48260ms step_avg:60.02ms
step:805/2270 train_time:48321ms step_avg:60.03ms
step:806/2270 train_time:48380ms step_avg:60.03ms
step:807/2270 train_time:48442ms step_avg:60.03ms
step:808/2270 train_time:48501ms step_avg:60.03ms
step:809/2270 train_time:48562ms step_avg:60.03ms
step:810/2270 train_time:48621ms step_avg:60.03ms
step:811/2270 train_time:48683ms step_avg:60.03ms
step:812/2270 train_time:48742ms step_avg:60.03ms
step:813/2270 train_time:48803ms step_avg:60.03ms
step:814/2270 train_time:48862ms step_avg:60.03ms
step:815/2270 train_time:48924ms step_avg:60.03ms
step:816/2270 train_time:48983ms step_avg:60.03ms
step:817/2270 train_time:49045ms step_avg:60.03ms
step:818/2270 train_time:49104ms step_avg:60.03ms
step:819/2270 train_time:49167ms step_avg:60.03ms
step:820/2270 train_time:49226ms step_avg:60.03ms
step:821/2270 train_time:49289ms step_avg:60.03ms
step:822/2270 train_time:49349ms step_avg:60.03ms
step:823/2270 train_time:49411ms step_avg:60.04ms
step:824/2270 train_time:49471ms step_avg:60.04ms
step:825/2270 train_time:49533ms step_avg:60.04ms
step:826/2270 train_time:49593ms step_avg:60.04ms
step:827/2270 train_time:49657ms step_avg:60.04ms
step:828/2270 train_time:49716ms step_avg:60.04ms
step:829/2270 train_time:49778ms step_avg:60.05ms
step:830/2270 train_time:49837ms step_avg:60.04ms
step:831/2270 train_time:49898ms step_avg:60.05ms
step:832/2270 train_time:49957ms step_avg:60.04ms
step:833/2270 train_time:50019ms step_avg:60.05ms
step:834/2270 train_time:50077ms step_avg:60.04ms
step:835/2270 train_time:50139ms step_avg:60.05ms
step:836/2270 train_time:50198ms step_avg:60.05ms
step:837/2270 train_time:50260ms step_avg:60.05ms
step:838/2270 train_time:50319ms step_avg:60.05ms
step:839/2270 train_time:50380ms step_avg:60.05ms
step:840/2270 train_time:50438ms step_avg:60.05ms
step:841/2270 train_time:50500ms step_avg:60.05ms
step:842/2270 train_time:50558ms step_avg:60.05ms
step:843/2270 train_time:50620ms step_avg:60.05ms
step:844/2270 train_time:50678ms step_avg:60.05ms
step:845/2270 train_time:50740ms step_avg:60.05ms
step:846/2270 train_time:50799ms step_avg:60.05ms
step:847/2270 train_time:50860ms step_avg:60.05ms
step:848/2270 train_time:50919ms step_avg:60.05ms
step:849/2270 train_time:50980ms step_avg:60.05ms
step:850/2270 train_time:51039ms step_avg:60.05ms
step:851/2270 train_time:51100ms step_avg:60.05ms
step:852/2270 train_time:51159ms step_avg:60.05ms
step:853/2270 train_time:51220ms step_avg:60.05ms
step:854/2270 train_time:51279ms step_avg:60.05ms
step:855/2270 train_time:51341ms step_avg:60.05ms
step:856/2270 train_time:51400ms step_avg:60.05ms
step:857/2270 train_time:51461ms step_avg:60.05ms
step:858/2270 train_time:51520ms step_avg:60.05ms
step:859/2270 train_time:51581ms step_avg:60.05ms
step:860/2270 train_time:51640ms step_avg:60.05ms
step:861/2270 train_time:51702ms step_avg:60.05ms
step:862/2270 train_time:51761ms step_avg:60.05ms
step:863/2270 train_time:51822ms step_avg:60.05ms
step:864/2270 train_time:51881ms step_avg:60.05ms
step:865/2270 train_time:51942ms step_avg:60.05ms
step:866/2270 train_time:52001ms step_avg:60.05ms
step:867/2270 train_time:52064ms step_avg:60.05ms
step:868/2270 train_time:52121ms step_avg:60.05ms
step:869/2270 train_time:52183ms step_avg:60.05ms
step:870/2270 train_time:52242ms step_avg:60.05ms
step:871/2270 train_time:52304ms step_avg:60.05ms
step:872/2270 train_time:52363ms step_avg:60.05ms
step:873/2270 train_time:52425ms step_avg:60.05ms
step:874/2270 train_time:52484ms step_avg:60.05ms
step:875/2270 train_time:52546ms step_avg:60.05ms
step:876/2270 train_time:52605ms step_avg:60.05ms
step:877/2270 train_time:52667ms step_avg:60.05ms
step:878/2270 train_time:52727ms step_avg:60.05ms
step:879/2270 train_time:52789ms step_avg:60.06ms
step:880/2270 train_time:52849ms step_avg:60.06ms
step:881/2270 train_time:52911ms step_avg:60.06ms
step:882/2270 train_time:52970ms step_avg:60.06ms
step:883/2270 train_time:53032ms step_avg:60.06ms
step:884/2270 train_time:53092ms step_avg:60.06ms
step:885/2270 train_time:53154ms step_avg:60.06ms
step:886/2270 train_time:53214ms step_avg:60.06ms
step:887/2270 train_time:53276ms step_avg:60.06ms
step:888/2270 train_time:53336ms step_avg:60.06ms
step:889/2270 train_time:53397ms step_avg:60.06ms
step:890/2270 train_time:53457ms step_avg:60.06ms
step:891/2270 train_time:53518ms step_avg:60.07ms
step:892/2270 train_time:53578ms step_avg:60.06ms
step:893/2270 train_time:53639ms step_avg:60.07ms
step:894/2270 train_time:53697ms step_avg:60.06ms
step:895/2270 train_time:53759ms step_avg:60.07ms
step:896/2270 train_time:53818ms step_avg:60.06ms
step:897/2270 train_time:53880ms step_avg:60.07ms
step:898/2270 train_time:53939ms step_avg:60.07ms
step:899/2270 train_time:54000ms step_avg:60.07ms
step:900/2270 train_time:54058ms step_avg:60.06ms
step:901/2270 train_time:54120ms step_avg:60.07ms
step:902/2270 train_time:54179ms step_avg:60.06ms
step:903/2270 train_time:54240ms step_avg:60.07ms
step:904/2270 train_time:54300ms step_avg:60.07ms
step:905/2270 train_time:54361ms step_avg:60.07ms
step:906/2270 train_time:54420ms step_avg:60.07ms
step:907/2270 train_time:54481ms step_avg:60.07ms
step:908/2270 train_time:54540ms step_avg:60.07ms
step:909/2270 train_time:54602ms step_avg:60.07ms
step:910/2270 train_time:54661ms step_avg:60.07ms
step:911/2270 train_time:54722ms step_avg:60.07ms
step:912/2270 train_time:54781ms step_avg:60.07ms
step:913/2270 train_time:54842ms step_avg:60.07ms
step:914/2270 train_time:54901ms step_avg:60.07ms
step:915/2270 train_time:54963ms step_avg:60.07ms
step:916/2270 train_time:55022ms step_avg:60.07ms
step:917/2270 train_time:55084ms step_avg:60.07ms
step:918/2270 train_time:55144ms step_avg:60.07ms
step:919/2270 train_time:55205ms step_avg:60.07ms
step:920/2270 train_time:55267ms step_avg:60.07ms
step:921/2270 train_time:55326ms step_avg:60.07ms
step:922/2270 train_time:55385ms step_avg:60.07ms
step:923/2270 train_time:55448ms step_avg:60.07ms
step:924/2270 train_time:55507ms step_avg:60.07ms
step:925/2270 train_time:55570ms step_avg:60.08ms
step:926/2270 train_time:55630ms step_avg:60.08ms
step:927/2270 train_time:55692ms step_avg:60.08ms
step:928/2270 train_time:55751ms step_avg:60.08ms
step:929/2270 train_time:55813ms step_avg:60.08ms
step:930/2270 train_time:55873ms step_avg:60.08ms
step:931/2270 train_time:55935ms step_avg:60.08ms
step:932/2270 train_time:55994ms step_avg:60.08ms
step:933/2270 train_time:56057ms step_avg:60.08ms
step:934/2270 train_time:56116ms step_avg:60.08ms
step:935/2270 train_time:56178ms step_avg:60.08ms
step:936/2270 train_time:56237ms step_avg:60.08ms
step:937/2270 train_time:56298ms step_avg:60.08ms
step:938/2270 train_time:56357ms step_avg:60.08ms
step:939/2270 train_time:56419ms step_avg:60.08ms
step:940/2270 train_time:56478ms step_avg:60.08ms
step:941/2270 train_time:56540ms step_avg:60.08ms
step:942/2270 train_time:56599ms step_avg:60.08ms
step:943/2270 train_time:56660ms step_avg:60.08ms
step:944/2270 train_time:56719ms step_avg:60.08ms
step:945/2270 train_time:56781ms step_avg:60.09ms
step:946/2270 train_time:56840ms step_avg:60.08ms
step:947/2270 train_time:56901ms step_avg:60.09ms
step:948/2270 train_time:56960ms step_avg:60.08ms
step:949/2270 train_time:57022ms step_avg:60.09ms
step:950/2270 train_time:57081ms step_avg:60.08ms
step:951/2270 train_time:57142ms step_avg:60.09ms
step:952/2270 train_time:57201ms step_avg:60.09ms
step:953/2270 train_time:57263ms step_avg:60.09ms
step:954/2270 train_time:57322ms step_avg:60.09ms
step:955/2270 train_time:57384ms step_avg:60.09ms
step:956/2270 train_time:57443ms step_avg:60.09ms
step:957/2270 train_time:57505ms step_avg:60.09ms
step:958/2270 train_time:57564ms step_avg:60.09ms
step:959/2270 train_time:57625ms step_avg:60.09ms
step:960/2270 train_time:57684ms step_avg:60.09ms
step:961/2270 train_time:57746ms step_avg:60.09ms
step:962/2270 train_time:57806ms step_avg:60.09ms
step:963/2270 train_time:57869ms step_avg:60.09ms
step:964/2270 train_time:57928ms step_avg:60.09ms
step:965/2270 train_time:57991ms step_avg:60.09ms
step:966/2270 train_time:58050ms step_avg:60.09ms
step:967/2270 train_time:58113ms step_avg:60.10ms
step:968/2270 train_time:58173ms step_avg:60.10ms
step:969/2270 train_time:58235ms step_avg:60.10ms
step:970/2270 train_time:58295ms step_avg:60.10ms
step:971/2270 train_time:58358ms step_avg:60.10ms
step:972/2270 train_time:58417ms step_avg:60.10ms
step:973/2270 train_time:58478ms step_avg:60.10ms
step:974/2270 train_time:58537ms step_avg:60.10ms
step:975/2270 train_time:58599ms step_avg:60.10ms
step:976/2270 train_time:58658ms step_avg:60.10ms
step:977/2270 train_time:58719ms step_avg:60.10ms
step:978/2270 train_time:58778ms step_avg:60.10ms
step:979/2270 train_time:58839ms step_avg:60.10ms
step:980/2270 train_time:58898ms step_avg:60.10ms
step:981/2270 train_time:58959ms step_avg:60.10ms
step:982/2270 train_time:59019ms step_avg:60.10ms
step:983/2270 train_time:59080ms step_avg:60.10ms
step:984/2270 train_time:59139ms step_avg:60.10ms
step:985/2270 train_time:59201ms step_avg:60.10ms
step:986/2270 train_time:59260ms step_avg:60.10ms
step:987/2270 train_time:59321ms step_avg:60.10ms
step:988/2270 train_time:59380ms step_avg:60.10ms
step:989/2270 train_time:59442ms step_avg:60.10ms
step:990/2270 train_time:59501ms step_avg:60.10ms
step:991/2270 train_time:59562ms step_avg:60.10ms
step:992/2270 train_time:59621ms step_avg:60.10ms
step:993/2270 train_time:59682ms step_avg:60.10ms
step:994/2270 train_time:59742ms step_avg:60.10ms
step:995/2270 train_time:59803ms step_avg:60.10ms
step:996/2270 train_time:59862ms step_avg:60.10ms
step:997/2270 train_time:59923ms step_avg:60.10ms
step:998/2270 train_time:59982ms step_avg:60.10ms
step:999/2270 train_time:60044ms step_avg:60.10ms
step:1000/2270 train_time:60104ms step_avg:60.10ms
step:1000/2270 val_loss:3.5765 train_time:60166ms step_avg:60.17ms
step:1001/2270 train_time:60184ms step_avg:60.12ms
step:1002/2270 train_time:60229ms step_avg:60.11ms
step:1003/2270 train_time:60292ms step_avg:60.11ms
step:1004/2270 train_time:60355ms step_avg:60.11ms
step:1005/2270 train_time:60419ms step_avg:60.12ms
step:1006/2270 train_time:60478ms step_avg:60.12ms
step:1007/2270 train_time:60539ms step_avg:60.12ms
step:1008/2270 train_time:60597ms step_avg:60.12ms
step:1009/2270 train_time:60658ms step_avg:60.12ms
step:1010/2270 train_time:60717ms step_avg:60.12ms
step:1011/2270 train_time:60777ms step_avg:60.12ms
step:1012/2270 train_time:60836ms step_avg:60.11ms
step:1013/2270 train_time:60897ms step_avg:60.12ms
step:1014/2270 train_time:60955ms step_avg:60.11ms
step:1015/2270 train_time:61016ms step_avg:60.11ms
step:1016/2270 train_time:61075ms step_avg:60.11ms
step:1017/2270 train_time:61139ms step_avg:60.12ms
step:1018/2270 train_time:61198ms step_avg:60.12ms
step:1019/2270 train_time:61260ms step_avg:60.12ms
step:1020/2270 train_time:61320ms step_avg:60.12ms
step:1021/2270 train_time:61383ms step_avg:60.12ms
step:1022/2270 train_time:61442ms step_avg:60.12ms
step:1023/2270 train_time:61504ms step_avg:60.12ms
step:1024/2270 train_time:61563ms step_avg:60.12ms
step:1025/2270 train_time:61624ms step_avg:60.12ms
step:1026/2270 train_time:61683ms step_avg:60.12ms
step:1027/2270 train_time:61745ms step_avg:60.12ms
step:1028/2270 train_time:61804ms step_avg:60.12ms
step:1029/2270 train_time:61866ms step_avg:60.12ms
step:1030/2270 train_time:61925ms step_avg:60.12ms
step:1031/2270 train_time:61986ms step_avg:60.12ms
step:1032/2270 train_time:62047ms step_avg:60.12ms
step:1033/2270 train_time:62109ms step_avg:60.12ms
step:1034/2270 train_time:62169ms step_avg:60.12ms
step:1035/2270 train_time:62232ms step_avg:60.13ms
step:1036/2270 train_time:62292ms step_avg:60.13ms
step:1037/2270 train_time:62356ms step_avg:60.13ms
step:1038/2270 train_time:62416ms step_avg:60.13ms
step:1039/2270 train_time:62478ms step_avg:60.13ms
step:1040/2270 train_time:62537ms step_avg:60.13ms
step:1041/2270 train_time:62599ms step_avg:60.13ms
step:1042/2270 train_time:62657ms step_avg:60.13ms
step:1043/2270 train_time:62718ms step_avg:60.13ms
step:1044/2270 train_time:62777ms step_avg:60.13ms
step:1045/2270 train_time:62838ms step_avg:60.13ms
step:1046/2270 train_time:62897ms step_avg:60.13ms
step:1047/2270 train_time:62958ms step_avg:60.13ms
step:1048/2270 train_time:63017ms step_avg:60.13ms
step:1049/2270 train_time:63078ms step_avg:60.13ms
step:1050/2270 train_time:63137ms step_avg:60.13ms
step:1051/2270 train_time:63199ms step_avg:60.13ms
step:1052/2270 train_time:63258ms step_avg:60.13ms
step:1053/2270 train_time:63320ms step_avg:60.13ms
step:1054/2270 train_time:63379ms step_avg:60.13ms
step:1055/2270 train_time:63441ms step_avg:60.13ms
step:1056/2270 train_time:63503ms step_avg:60.13ms
step:1057/2270 train_time:63562ms step_avg:60.13ms
step:1058/2270 train_time:63621ms step_avg:60.13ms
step:1059/2270 train_time:63682ms step_avg:60.13ms
step:1060/2270 train_time:63741ms step_avg:60.13ms
step:1061/2270 train_time:63802ms step_avg:60.13ms
step:1062/2270 train_time:63861ms step_avg:60.13ms
step:1063/2270 train_time:63923ms step_avg:60.13ms
step:1064/2270 train_time:63982ms step_avg:60.13ms
step:1065/2270 train_time:64043ms step_avg:60.13ms
step:1066/2270 train_time:64103ms step_avg:60.13ms
step:1067/2270 train_time:64165ms step_avg:60.14ms
step:1068/2270 train_time:64225ms step_avg:60.14ms
step:1069/2270 train_time:64287ms step_avg:60.14ms
step:1070/2270 train_time:64346ms step_avg:60.14ms
step:1071/2270 train_time:64409ms step_avg:60.14ms
step:1072/2270 train_time:64469ms step_avg:60.14ms
step:1073/2270 train_time:64532ms step_avg:60.14ms
step:1074/2270 train_time:64592ms step_avg:60.14ms
step:1075/2270 train_time:64655ms step_avg:60.14ms
step:1076/2270 train_time:64715ms step_avg:60.14ms
step:1077/2270 train_time:64776ms step_avg:60.15ms
step:1078/2270 train_time:64836ms step_avg:60.14ms
step:1079/2270 train_time:64897ms step_avg:60.15ms
step:1080/2270 train_time:64956ms step_avg:60.14ms
step:1081/2270 train_time:65018ms step_avg:60.15ms
step:1082/2270 train_time:65076ms step_avg:60.14ms
step:1083/2270 train_time:65138ms step_avg:60.15ms
step:1084/2270 train_time:65197ms step_avg:60.14ms
step:1085/2270 train_time:65258ms step_avg:60.15ms
step:1086/2270 train_time:65317ms step_avg:60.14ms
step:1087/2270 train_time:65378ms step_avg:60.15ms
step:1088/2270 train_time:65438ms step_avg:60.15ms
step:1089/2270 train_time:65500ms step_avg:60.15ms
step:1090/2270 train_time:65559ms step_avg:60.15ms
step:1091/2270 train_time:65620ms step_avg:60.15ms
step:1092/2270 train_time:65679ms step_avg:60.15ms
step:1093/2270 train_time:65741ms step_avg:60.15ms
step:1094/2270 train_time:65800ms step_avg:60.15ms
step:1095/2270 train_time:65861ms step_avg:60.15ms
step:1096/2270 train_time:65920ms step_avg:60.15ms
step:1097/2270 train_time:65982ms step_avg:60.15ms
step:1098/2270 train_time:66041ms step_avg:60.15ms
step:1099/2270 train_time:66102ms step_avg:60.15ms
step:1100/2270 train_time:66161ms step_avg:60.15ms
step:1101/2270 train_time:66223ms step_avg:60.15ms
step:1102/2270 train_time:66282ms step_avg:60.15ms
step:1103/2270 train_time:66344ms step_avg:60.15ms
step:1104/2270 train_time:66404ms step_avg:60.15ms
step:1105/2270 train_time:66466ms step_avg:60.15ms
step:1106/2270 train_time:66526ms step_avg:60.15ms
step:1107/2270 train_time:66587ms step_avg:60.15ms
step:1108/2270 train_time:66647ms step_avg:60.15ms
step:1109/2270 train_time:66709ms step_avg:60.15ms
step:1110/2270 train_time:66769ms step_avg:60.15ms
step:1111/2270 train_time:66832ms step_avg:60.15ms
step:1112/2270 train_time:66891ms step_avg:60.15ms
step:1113/2270 train_time:66954ms step_avg:60.16ms
step:1114/2270 train_time:67014ms step_avg:60.16ms
step:1115/2270 train_time:67077ms step_avg:60.16ms
step:1116/2270 train_time:67136ms step_avg:60.16ms
step:1117/2270 train_time:67197ms step_avg:60.16ms
step:1118/2270 train_time:67256ms step_avg:60.16ms
step:1119/2270 train_time:67318ms step_avg:60.16ms
step:1120/2270 train_time:67377ms step_avg:60.16ms
step:1121/2270 train_time:67438ms step_avg:60.16ms
step:1122/2270 train_time:67498ms step_avg:60.16ms
step:1123/2270 train_time:67559ms step_avg:60.16ms
step:1124/2270 train_time:67618ms step_avg:60.16ms
step:1125/2270 train_time:67679ms step_avg:60.16ms
step:1126/2270 train_time:67738ms step_avg:60.16ms
step:1127/2270 train_time:67799ms step_avg:60.16ms
step:1128/2270 train_time:67858ms step_avg:60.16ms
step:1129/2270 train_time:67920ms step_avg:60.16ms
step:1130/2270 train_time:67979ms step_avg:60.16ms
step:1131/2270 train_time:68041ms step_avg:60.16ms
step:1132/2270 train_time:68099ms step_avg:60.16ms
step:1133/2270 train_time:68161ms step_avg:60.16ms
step:1134/2270 train_time:68220ms step_avg:60.16ms
step:1135/2270 train_time:68282ms step_avg:60.16ms
step:1136/2270 train_time:68341ms step_avg:60.16ms
step:1137/2270 train_time:68402ms step_avg:60.16ms
step:1138/2270 train_time:68461ms step_avg:60.16ms
step:1139/2270 train_time:68524ms step_avg:60.16ms
step:1140/2270 train_time:68583ms step_avg:60.16ms
step:1141/2270 train_time:68645ms step_avg:60.16ms
step:1142/2270 train_time:68705ms step_avg:60.16ms
step:1143/2270 train_time:68767ms step_avg:60.16ms
step:1144/2270 train_time:68827ms step_avg:60.16ms
step:1145/2270 train_time:68889ms step_avg:60.17ms
step:1146/2270 train_time:68949ms step_avg:60.17ms
step:1147/2270 train_time:69012ms step_avg:60.17ms
step:1148/2270 train_time:69072ms step_avg:60.17ms
step:1149/2270 train_time:69135ms step_avg:60.17ms
step:1150/2270 train_time:69195ms step_avg:60.17ms
step:1151/2270 train_time:69257ms step_avg:60.17ms
step:1152/2270 train_time:69317ms step_avg:60.17ms
step:1153/2270 train_time:69379ms step_avg:60.17ms
step:1154/2270 train_time:69439ms step_avg:60.17ms
step:1155/2270 train_time:69500ms step_avg:60.17ms
step:1156/2270 train_time:69560ms step_avg:60.17ms
step:1157/2270 train_time:69621ms step_avg:60.17ms
step:1158/2270 train_time:69680ms step_avg:60.17ms
step:1159/2270 train_time:69742ms step_avg:60.17ms
step:1160/2270 train_time:69801ms step_avg:60.17ms
step:1161/2270 train_time:69862ms step_avg:60.17ms
step:1162/2270 train_time:69922ms step_avg:60.17ms
step:1163/2270 train_time:69984ms step_avg:60.18ms
step:1164/2270 train_time:70043ms step_avg:60.17ms
step:1165/2270 train_time:70106ms step_avg:60.18ms
step:1166/2270 train_time:70165ms step_avg:60.18ms
step:1167/2270 train_time:70228ms step_avg:60.18ms
step:1168/2270 train_time:70288ms step_avg:60.18ms
step:1169/2270 train_time:70350ms step_avg:60.18ms
step:1170/2270 train_time:70410ms step_avg:60.18ms
step:1171/2270 train_time:70473ms step_avg:60.18ms
step:1172/2270 train_time:70533ms step_avg:60.18ms
step:1173/2270 train_time:70596ms step_avg:60.18ms
step:1174/2270 train_time:70655ms step_avg:60.18ms
step:1175/2270 train_time:70717ms step_avg:60.18ms
step:1176/2270 train_time:70776ms step_avg:60.18ms
step:1177/2270 train_time:70839ms step_avg:60.19ms
step:1178/2270 train_time:70898ms step_avg:60.18ms
step:1179/2270 train_time:70959ms step_avg:60.19ms
step:1180/2270 train_time:71019ms step_avg:60.19ms
step:1181/2270 train_time:71080ms step_avg:60.19ms
step:1182/2270 train_time:71139ms step_avg:60.19ms
step:1183/2270 train_time:71201ms step_avg:60.19ms
step:1184/2270 train_time:71260ms step_avg:60.19ms
step:1185/2270 train_time:71322ms step_avg:60.19ms
step:1186/2270 train_time:71381ms step_avg:60.19ms
step:1187/2270 train_time:71443ms step_avg:60.19ms
step:1188/2270 train_time:71502ms step_avg:60.19ms
step:1189/2270 train_time:71565ms step_avg:60.19ms
step:1190/2270 train_time:71625ms step_avg:60.19ms
step:1191/2270 train_time:71687ms step_avg:60.19ms
step:1192/2270 train_time:71746ms step_avg:60.19ms
step:1193/2270 train_time:71808ms step_avg:60.19ms
step:1194/2270 train_time:71868ms step_avg:60.19ms
step:1195/2270 train_time:71931ms step_avg:60.19ms
step:1196/2270 train_time:71991ms step_avg:60.19ms
step:1197/2270 train_time:72054ms step_avg:60.20ms
step:1198/2270 train_time:72114ms step_avg:60.19ms
step:1199/2270 train_time:72176ms step_avg:60.20ms
step:1200/2270 train_time:72236ms step_avg:60.20ms
step:1201/2270 train_time:72298ms step_avg:60.20ms
step:1202/2270 train_time:72357ms step_avg:60.20ms
step:1203/2270 train_time:72419ms step_avg:60.20ms
step:1204/2270 train_time:72478ms step_avg:60.20ms
step:1205/2270 train_time:72540ms step_avg:60.20ms
step:1206/2270 train_time:72599ms step_avg:60.20ms
step:1207/2270 train_time:72661ms step_avg:60.20ms
step:1208/2270 train_time:72720ms step_avg:60.20ms
step:1209/2270 train_time:72781ms step_avg:60.20ms
step:1210/2270 train_time:72841ms step_avg:60.20ms
step:1211/2270 train_time:72902ms step_avg:60.20ms
step:1212/2270 train_time:72961ms step_avg:60.20ms
step:1213/2270 train_time:73024ms step_avg:60.20ms
step:1214/2270 train_time:73082ms step_avg:60.20ms
step:1215/2270 train_time:73145ms step_avg:60.20ms
step:1216/2270 train_time:73205ms step_avg:60.20ms
step:1217/2270 train_time:73267ms step_avg:60.20ms
step:1218/2270 train_time:73327ms step_avg:60.20ms
step:1219/2270 train_time:73390ms step_avg:60.20ms
step:1220/2270 train_time:73450ms step_avg:60.21ms
step:1221/2270 train_time:73512ms step_avg:60.21ms
step:1222/2270 train_time:73573ms step_avg:60.21ms
step:1223/2270 train_time:73636ms step_avg:60.21ms
step:1224/2270 train_time:73695ms step_avg:60.21ms
step:1225/2270 train_time:73757ms step_avg:60.21ms
step:1226/2270 train_time:73816ms step_avg:60.21ms
step:1227/2270 train_time:73879ms step_avg:60.21ms
step:1228/2270 train_time:73938ms step_avg:60.21ms
step:1229/2270 train_time:74000ms step_avg:60.21ms
step:1230/2270 train_time:74059ms step_avg:60.21ms
step:1231/2270 train_time:74121ms step_avg:60.21ms
step:1232/2270 train_time:74180ms step_avg:60.21ms
step:1233/2270 train_time:74242ms step_avg:60.21ms
step:1234/2270 train_time:74301ms step_avg:60.21ms
step:1235/2270 train_time:74362ms step_avg:60.21ms
step:1236/2270 train_time:74422ms step_avg:60.21ms
step:1237/2270 train_time:74483ms step_avg:60.21ms
step:1238/2270 train_time:74543ms step_avg:60.21ms
step:1239/2270 train_time:74605ms step_avg:60.21ms
step:1240/2270 train_time:74665ms step_avg:60.21ms
step:1241/2270 train_time:74727ms step_avg:60.21ms
step:1242/2270 train_time:74787ms step_avg:60.21ms
step:1243/2270 train_time:74849ms step_avg:60.22ms
step:1244/2270 train_time:74909ms step_avg:60.22ms
step:1245/2270 train_time:74972ms step_avg:60.22ms
step:1246/2270 train_time:75032ms step_avg:60.22ms
step:1247/2270 train_time:75095ms step_avg:60.22ms
step:1248/2270 train_time:75155ms step_avg:60.22ms
step:1249/2270 train_time:75217ms step_avg:60.22ms
step:1250/2270 train_time:75276ms step_avg:60.22ms
step:1250/2270 val_loss:3.5038 train_time:75339ms step_avg:60.27ms
step:1251/2270 train_time:75357ms step_avg:60.24ms
step:1252/2270 train_time:75399ms step_avg:60.22ms
step:1253/2270 train_time:75462ms step_avg:60.23ms
step:1254/2270 train_time:75524ms step_avg:60.23ms
step:1255/2270 train_time:75588ms step_avg:60.23ms
step:1256/2270 train_time:75648ms step_avg:60.23ms
step:1257/2270 train_time:75709ms step_avg:60.23ms
step:1258/2270 train_time:75768ms step_avg:60.23ms
step:1259/2270 train_time:75828ms step_avg:60.23ms
step:1260/2270 train_time:75887ms step_avg:60.23ms
step:1261/2270 train_time:75948ms step_avg:60.23ms
step:1262/2270 train_time:76007ms step_avg:60.23ms
step:1263/2270 train_time:76068ms step_avg:60.23ms
step:1264/2270 train_time:76126ms step_avg:60.23ms
step:1265/2270 train_time:76188ms step_avg:60.23ms
step:1266/2270 train_time:76253ms step_avg:60.23ms
step:1267/2270 train_time:76318ms step_avg:60.23ms
step:1268/2270 train_time:76378ms step_avg:60.24ms
step:1269/2270 train_time:76441ms step_avg:60.24ms
step:1270/2270 train_time:76501ms step_avg:60.24ms
step:1271/2270 train_time:76564ms step_avg:60.24ms
step:1272/2270 train_time:76624ms step_avg:60.24ms
step:1273/2270 train_time:76686ms step_avg:60.24ms
step:1274/2270 train_time:76746ms step_avg:60.24ms
step:1275/2270 train_time:76807ms step_avg:60.24ms
step:1276/2270 train_time:76866ms step_avg:60.24ms
step:1277/2270 train_time:76927ms step_avg:60.24ms
step:1278/2270 train_time:76986ms step_avg:60.24ms
step:1279/2270 train_time:77047ms step_avg:60.24ms
step:1280/2270 train_time:77107ms step_avg:60.24ms
step:1281/2270 train_time:77169ms step_avg:60.24ms
step:1282/2270 train_time:77231ms step_avg:60.24ms
step:1283/2270 train_time:77294ms step_avg:60.24ms
step:1284/2270 train_time:77354ms step_avg:60.24ms
step:1285/2270 train_time:77417ms step_avg:60.25ms
step:1286/2270 train_time:77477ms step_avg:60.25ms
step:1287/2270 train_time:77539ms step_avg:60.25ms
step:1288/2270 train_time:77599ms step_avg:60.25ms
step:1289/2270 train_time:77661ms step_avg:60.25ms
step:1290/2270 train_time:77721ms step_avg:60.25ms
step:1291/2270 train_time:77783ms step_avg:60.25ms
step:1292/2270 train_time:77842ms step_avg:60.25ms
step:1293/2270 train_time:77904ms step_avg:60.25ms
step:1294/2270 train_time:77963ms step_avg:60.25ms
step:1295/2270 train_time:78025ms step_avg:60.25ms
step:1296/2270 train_time:78084ms step_avg:60.25ms
step:1297/2270 train_time:78147ms step_avg:60.25ms
step:1298/2270 train_time:78208ms step_avg:60.25ms
step:1299/2270 train_time:78270ms step_avg:60.25ms
step:1300/2270 train_time:78329ms step_avg:60.25ms
step:1301/2270 train_time:78391ms step_avg:60.25ms
step:1302/2270 train_time:78451ms step_avg:60.25ms
step:1303/2270 train_time:78513ms step_avg:60.26ms
step:1304/2270 train_time:78572ms step_avg:60.25ms
step:1305/2270 train_time:78633ms step_avg:60.26ms
step:1306/2270 train_time:78692ms step_avg:60.25ms
step:1307/2270 train_time:78754ms step_avg:60.26ms
step:1308/2270 train_time:78813ms step_avg:60.25ms
step:1309/2270 train_time:78874ms step_avg:60.26ms
step:1310/2270 train_time:78934ms step_avg:60.25ms
step:1311/2270 train_time:78996ms step_avg:60.26ms
step:1312/2270 train_time:79056ms step_avg:60.26ms
step:1313/2270 train_time:79118ms step_avg:60.26ms
step:1314/2270 train_time:79178ms step_avg:60.26ms
step:1315/2270 train_time:79240ms step_avg:60.26ms
step:1316/2270 train_time:79301ms step_avg:60.26ms
step:1317/2270 train_time:79363ms step_avg:60.26ms
step:1318/2270 train_time:79423ms step_avg:60.26ms
step:1319/2270 train_time:79487ms step_avg:60.26ms
step:1320/2270 train_time:79547ms step_avg:60.26ms
step:1321/2270 train_time:79608ms step_avg:60.26ms
step:1322/2270 train_time:79667ms step_avg:60.26ms
step:1323/2270 train_time:79729ms step_avg:60.26ms
step:1324/2270 train_time:79788ms step_avg:60.26ms
step:1325/2270 train_time:79850ms step_avg:60.26ms
step:1326/2270 train_time:79909ms step_avg:60.26ms
step:1327/2270 train_time:79970ms step_avg:60.26ms
step:1328/2270 train_time:80029ms step_avg:60.26ms
step:1329/2270 train_time:80090ms step_avg:60.26ms
step:1330/2270 train_time:80150ms step_avg:60.26ms
step:1331/2270 train_time:80211ms step_avg:60.26ms
step:1332/2270 train_time:80270ms step_avg:60.26ms
step:1333/2270 train_time:80333ms step_avg:60.26ms
step:1334/2270 train_time:80392ms step_avg:60.26ms
step:1335/2270 train_time:80455ms step_avg:60.27ms
step:1336/2270 train_time:80515ms step_avg:60.27ms
step:1337/2270 train_time:80577ms step_avg:60.27ms
step:1338/2270 train_time:80636ms step_avg:60.27ms
step:1339/2270 train_time:80699ms step_avg:60.27ms
step:1340/2270 train_time:80758ms step_avg:60.27ms
step:1341/2270 train_time:80821ms step_avg:60.27ms
step:1342/2270 train_time:80880ms step_avg:60.27ms
step:1343/2270 train_time:80943ms step_avg:60.27ms
step:1344/2270 train_time:81003ms step_avg:60.27ms
step:1345/2270 train_time:81065ms step_avg:60.27ms
step:1346/2270 train_time:81125ms step_avg:60.27ms
step:1347/2270 train_time:81187ms step_avg:60.27ms
step:1348/2270 train_time:81247ms step_avg:60.27ms
step:1349/2270 train_time:81309ms step_avg:60.27ms
step:1350/2270 train_time:81368ms step_avg:60.27ms
step:1351/2270 train_time:81430ms step_avg:60.27ms
step:1352/2270 train_time:81489ms step_avg:60.27ms
step:1353/2270 train_time:81551ms step_avg:60.27ms
step:1354/2270 train_time:81610ms step_avg:60.27ms
step:1355/2270 train_time:81672ms step_avg:60.27ms
step:1356/2270 train_time:81731ms step_avg:60.27ms
step:1357/2270 train_time:81792ms step_avg:60.27ms
step:1358/2270 train_time:81852ms step_avg:60.27ms
step:1359/2270 train_time:81914ms step_avg:60.27ms
step:1360/2270 train_time:81972ms step_avg:60.27ms
step:1361/2270 train_time:82034ms step_avg:60.27ms
step:1362/2270 train_time:82093ms step_avg:60.27ms
step:1363/2270 train_time:82156ms step_avg:60.28ms
step:1364/2270 train_time:82216ms step_avg:60.28ms
step:1365/2270 train_time:82279ms step_avg:60.28ms
step:1366/2270 train_time:82339ms step_avg:60.28ms
step:1367/2270 train_time:82401ms step_avg:60.28ms
step:1368/2270 train_time:82461ms step_avg:60.28ms
step:1369/2270 train_time:82523ms step_avg:60.28ms
step:1370/2270 train_time:82583ms step_avg:60.28ms
step:1371/2270 train_time:82645ms step_avg:60.28ms
step:1372/2270 train_time:82705ms step_avg:60.28ms
step:1373/2270 train_time:82767ms step_avg:60.28ms
step:1374/2270 train_time:82827ms step_avg:60.28ms
step:1375/2270 train_time:82888ms step_avg:60.28ms
step:1376/2270 train_time:82948ms step_avg:60.28ms
step:1377/2270 train_time:83010ms step_avg:60.28ms
step:1378/2270 train_time:83069ms step_avg:60.28ms
step:1379/2270 train_time:83130ms step_avg:60.28ms
step:1380/2270 train_time:83189ms step_avg:60.28ms
step:1381/2270 train_time:83250ms step_avg:60.28ms
step:1382/2270 train_time:83310ms step_avg:60.28ms
step:1383/2270 train_time:83372ms step_avg:60.28ms
step:1384/2270 train_time:83431ms step_avg:60.28ms
step:1385/2270 train_time:83492ms step_avg:60.28ms
step:1386/2270 train_time:83552ms step_avg:60.28ms
step:1387/2270 train_time:83614ms step_avg:60.28ms
step:1388/2270 train_time:83673ms step_avg:60.28ms
step:1389/2270 train_time:83735ms step_avg:60.28ms
step:1390/2270 train_time:83795ms step_avg:60.28ms
step:1391/2270 train_time:83857ms step_avg:60.29ms
step:1392/2270 train_time:83917ms step_avg:60.28ms
step:1393/2270 train_time:83979ms step_avg:60.29ms
step:1394/2270 train_time:84038ms step_avg:60.29ms
step:1395/2270 train_time:84101ms step_avg:60.29ms
step:1396/2270 train_time:84161ms step_avg:60.29ms
step:1397/2270 train_time:84224ms step_avg:60.29ms
step:1398/2270 train_time:84284ms step_avg:60.29ms
step:1399/2270 train_time:84347ms step_avg:60.29ms
step:1400/2270 train_time:84407ms step_avg:60.29ms
step:1401/2270 train_time:84468ms step_avg:60.29ms
step:1402/2270 train_time:84528ms step_avg:60.29ms
step:1403/2270 train_time:84589ms step_avg:60.29ms
step:1404/2270 train_time:84648ms step_avg:60.29ms
step:1405/2270 train_time:84710ms step_avg:60.29ms
step:1406/2270 train_time:84769ms step_avg:60.29ms
step:1407/2270 train_time:84831ms step_avg:60.29ms
step:1408/2270 train_time:84890ms step_avg:60.29ms
step:1409/2270 train_time:84951ms step_avg:60.29ms
step:1410/2270 train_time:85010ms step_avg:60.29ms
step:1411/2270 train_time:85072ms step_avg:60.29ms
step:1412/2270 train_time:85131ms step_avg:60.29ms
step:1413/2270 train_time:85193ms step_avg:60.29ms
step:1414/2270 train_time:85252ms step_avg:60.29ms
step:1415/2270 train_time:85315ms step_avg:60.29ms
step:1416/2270 train_time:85374ms step_avg:60.29ms
step:1417/2270 train_time:85437ms step_avg:60.29ms
step:1418/2270 train_time:85496ms step_avg:60.29ms
step:1419/2270 train_time:85559ms step_avg:60.30ms
step:1420/2270 train_time:85619ms step_avg:60.30ms
step:1421/2270 train_time:85681ms step_avg:60.30ms
step:1422/2270 train_time:85741ms step_avg:60.30ms
step:1423/2270 train_time:85804ms step_avg:60.30ms
step:1424/2270 train_time:85864ms step_avg:60.30ms
step:1425/2270 train_time:85926ms step_avg:60.30ms
step:1426/2270 train_time:85986ms step_avg:60.30ms
step:1427/2270 train_time:86048ms step_avg:60.30ms
step:1428/2270 train_time:86107ms step_avg:60.30ms
step:1429/2270 train_time:86170ms step_avg:60.30ms
step:1430/2270 train_time:86229ms step_avg:60.30ms
step:1431/2270 train_time:86291ms step_avg:60.30ms
step:1432/2270 train_time:86350ms step_avg:60.30ms
step:1433/2270 train_time:86412ms step_avg:60.30ms
step:1434/2270 train_time:86471ms step_avg:60.30ms
step:1435/2270 train_time:86533ms step_avg:60.30ms
step:1436/2270 train_time:86592ms step_avg:60.30ms
step:1437/2270 train_time:86654ms step_avg:60.30ms
step:1438/2270 train_time:86714ms step_avg:60.30ms
step:1439/2270 train_time:86775ms step_avg:60.30ms
step:1440/2270 train_time:86836ms step_avg:60.30ms
step:1441/2270 train_time:86898ms step_avg:60.30ms
step:1442/2270 train_time:86958ms step_avg:60.30ms
step:1443/2270 train_time:87020ms step_avg:60.31ms
step:1444/2270 train_time:87080ms step_avg:60.30ms
step:1445/2270 train_time:87143ms step_avg:60.31ms
step:1446/2270 train_time:87203ms step_avg:60.31ms
step:1447/2270 train_time:87266ms step_avg:60.31ms
step:1448/2270 train_time:87326ms step_avg:60.31ms
step:1449/2270 train_time:87388ms step_avg:60.31ms
step:1450/2270 train_time:87448ms step_avg:60.31ms
step:1451/2270 train_time:87510ms step_avg:60.31ms
step:1452/2270 train_time:87569ms step_avg:60.31ms
step:1453/2270 train_time:87630ms step_avg:60.31ms
step:1454/2270 train_time:87689ms step_avg:60.31ms
step:1455/2270 train_time:87751ms step_avg:60.31ms
step:1456/2270 train_time:87810ms step_avg:60.31ms
step:1457/2270 train_time:87872ms step_avg:60.31ms
step:1458/2270 train_time:87931ms step_avg:60.31ms
step:1459/2270 train_time:87992ms step_avg:60.31ms
step:1460/2270 train_time:88052ms step_avg:60.31ms
step:1461/2270 train_time:88114ms step_avg:60.31ms
step:1462/2270 train_time:88173ms step_avg:60.31ms
step:1463/2270 train_time:88236ms step_avg:60.31ms
step:1464/2270 train_time:88296ms step_avg:60.31ms
step:1465/2270 train_time:88358ms step_avg:60.31ms
step:1466/2270 train_time:88418ms step_avg:60.31ms
step:1467/2270 train_time:88480ms step_avg:60.31ms
step:1468/2270 train_time:88540ms step_avg:60.31ms
step:1469/2270 train_time:88603ms step_avg:60.31ms
step:1470/2270 train_time:88663ms step_avg:60.32ms
step:1471/2270 train_time:88725ms step_avg:60.32ms
step:1472/2270 train_time:88785ms step_avg:60.32ms
step:1473/2270 train_time:88847ms step_avg:60.32ms
step:1474/2270 train_time:88907ms step_avg:60.32ms
step:1475/2270 train_time:88968ms step_avg:60.32ms
step:1476/2270 train_time:89027ms step_avg:60.32ms
step:1477/2270 train_time:89089ms step_avg:60.32ms
step:1478/2270 train_time:89148ms step_avg:60.32ms
step:1479/2270 train_time:89210ms step_avg:60.32ms
step:1480/2270 train_time:89270ms step_avg:60.32ms
step:1481/2270 train_time:89331ms step_avg:60.32ms
step:1482/2270 train_time:89390ms step_avg:60.32ms
step:1483/2270 train_time:89452ms step_avg:60.32ms
step:1484/2270 train_time:89511ms step_avg:60.32ms
step:1485/2270 train_time:89573ms step_avg:60.32ms
step:1486/2270 train_time:89632ms step_avg:60.32ms
step:1487/2270 train_time:89695ms step_avg:60.32ms
step:1488/2270 train_time:89754ms step_avg:60.32ms
step:1489/2270 train_time:89817ms step_avg:60.32ms
step:1490/2270 train_time:89877ms step_avg:60.32ms
step:1491/2270 train_time:89939ms step_avg:60.32ms
step:1492/2270 train_time:89998ms step_avg:60.32ms
step:1493/2270 train_time:90061ms step_avg:60.32ms
step:1494/2270 train_time:90121ms step_avg:60.32ms
step:1495/2270 train_time:90184ms step_avg:60.32ms
step:1496/2270 train_time:90244ms step_avg:60.32ms
step:1497/2270 train_time:90306ms step_avg:60.32ms
step:1498/2270 train_time:90366ms step_avg:60.32ms
step:1499/2270 train_time:90428ms step_avg:60.33ms
step:1500/2270 train_time:90487ms step_avg:60.32ms
step:1500/2270 val_loss:3.4355 train_time:90550ms step_avg:60.37ms
step:1501/2270 train_time:90568ms step_avg:60.34ms
step:1502/2270 train_time:90614ms step_avg:60.33ms
step:1503/2270 train_time:90678ms step_avg:60.33ms
step:1504/2270 train_time:90740ms step_avg:60.33ms
step:1505/2270 train_time:90803ms step_avg:60.33ms
step:1506/2270 train_time:90863ms step_avg:60.33ms
step:1507/2270 train_time:90926ms step_avg:60.34ms
step:1508/2270 train_time:90983ms step_avg:60.33ms
step:1509/2270 train_time:91045ms step_avg:60.33ms
step:1510/2270 train_time:91104ms step_avg:60.33ms
step:1511/2270 train_time:91165ms step_avg:60.33ms
step:1512/2270 train_time:91224ms step_avg:60.33ms
step:1513/2270 train_time:91285ms step_avg:60.33ms
step:1514/2270 train_time:91344ms step_avg:60.33ms
step:1515/2270 train_time:91405ms step_avg:60.33ms
step:1516/2270 train_time:91465ms step_avg:60.33ms
step:1517/2270 train_time:91529ms step_avg:60.34ms
step:1518/2270 train_time:91590ms step_avg:60.34ms
step:1519/2270 train_time:91653ms step_avg:60.34ms
step:1520/2270 train_time:91713ms step_avg:60.34ms
step:1521/2270 train_time:91777ms step_avg:60.34ms
step:1522/2270 train_time:91837ms step_avg:60.34ms
step:1523/2270 train_time:91899ms step_avg:60.34ms
step:1524/2270 train_time:91959ms step_avg:60.34ms
step:1525/2270 train_time:92022ms step_avg:60.34ms
step:1526/2270 train_time:92082ms step_avg:60.34ms
step:1527/2270 train_time:92145ms step_avg:60.34ms
step:1528/2270 train_time:92204ms step_avg:60.34ms
step:1529/2270 train_time:92265ms step_avg:60.34ms
step:1530/2270 train_time:92324ms step_avg:60.34ms
step:1531/2270 train_time:92387ms step_avg:60.34ms
step:1532/2270 train_time:92446ms step_avg:60.34ms
step:1533/2270 train_time:92509ms step_avg:60.34ms
step:1534/2270 train_time:92568ms step_avg:60.34ms
step:1535/2270 train_time:92631ms step_avg:60.35ms
step:1536/2270 train_time:92690ms step_avg:60.35ms
step:1537/2270 train_time:92753ms step_avg:60.35ms
step:1538/2270 train_time:92812ms step_avg:60.35ms
step:1539/2270 train_time:92875ms step_avg:60.35ms
step:1540/2270 train_time:92935ms step_avg:60.35ms
step:1541/2270 train_time:92997ms step_avg:60.35ms
step:1542/2270 train_time:93058ms step_avg:60.35ms
step:1543/2270 train_time:93120ms step_avg:60.35ms
step:1544/2270 train_time:93181ms step_avg:60.35ms
step:1545/2270 train_time:93244ms step_avg:60.35ms
step:1546/2270 train_time:93303ms step_avg:60.35ms
step:1547/2270 train_time:93365ms step_avg:60.35ms
step:1548/2270 train_time:93425ms step_avg:60.35ms
step:1549/2270 train_time:93487ms step_avg:60.35ms
step:1550/2270 train_time:93548ms step_avg:60.35ms
step:1551/2270 train_time:93610ms step_avg:60.35ms
step:1552/2270 train_time:93670ms step_avg:60.35ms
step:1553/2270 train_time:93733ms step_avg:60.36ms
step:1554/2270 train_time:93793ms step_avg:60.36ms
step:1555/2270 train_time:93855ms step_avg:60.36ms
step:1556/2270 train_time:93915ms step_avg:60.36ms
step:1557/2270 train_time:93978ms step_avg:60.36ms
step:1558/2270 train_time:94038ms step_avg:60.36ms
step:1559/2270 train_time:94100ms step_avg:60.36ms
step:1560/2270 train_time:94160ms step_avg:60.36ms
step:1561/2270 train_time:94223ms step_avg:60.36ms
step:1562/2270 train_time:94284ms step_avg:60.36ms
step:1563/2270 train_time:94346ms step_avg:60.36ms
step:1564/2270 train_time:94405ms step_avg:60.36ms
step:1565/2270 train_time:94467ms step_avg:60.36ms
step:1566/2270 train_time:94527ms step_avg:60.36ms
step:1567/2270 train_time:94590ms step_avg:60.36ms
step:1568/2270 train_time:94650ms step_avg:60.36ms
step:1569/2270 train_time:94712ms step_avg:60.36ms
step:1570/2270 train_time:94772ms step_avg:60.36ms
step:1571/2270 train_time:94834ms step_avg:60.37ms
step:1572/2270 train_time:94893ms step_avg:60.36ms
step:1573/2270 train_time:94956ms step_avg:60.37ms
step:1574/2270 train_time:95016ms step_avg:60.37ms
step:1575/2270 train_time:95078ms step_avg:60.37ms
step:1576/2270 train_time:95140ms step_avg:60.37ms
step:1577/2270 train_time:95202ms step_avg:60.37ms
step:1578/2270 train_time:95262ms step_avg:60.37ms
step:1579/2270 train_time:95325ms step_avg:60.37ms
step:1580/2270 train_time:95385ms step_avg:60.37ms
step:1581/2270 train_time:95447ms step_avg:60.37ms
step:1582/2270 train_time:95507ms step_avg:60.37ms
step:1583/2270 train_time:95569ms step_avg:60.37ms
step:1584/2270 train_time:95629ms step_avg:60.37ms
step:1585/2270 train_time:95691ms step_avg:60.37ms
step:1586/2270 train_time:95751ms step_avg:60.37ms
step:1587/2270 train_time:95812ms step_avg:60.37ms
step:1588/2270 train_time:95872ms step_avg:60.37ms
step:1589/2270 train_time:95934ms step_avg:60.37ms
step:1590/2270 train_time:95993ms step_avg:60.37ms
step:1591/2270 train_time:96056ms step_avg:60.37ms
step:1592/2270 train_time:96116ms step_avg:60.37ms
step:1593/2270 train_time:96179ms step_avg:60.38ms
step:1594/2270 train_time:96240ms step_avg:60.38ms
step:1595/2270 train_time:96303ms step_avg:60.38ms
step:1596/2270 train_time:96363ms step_avg:60.38ms
step:1597/2270 train_time:96426ms step_avg:60.38ms
step:1598/2270 train_time:96486ms step_avg:60.38ms
step:1599/2270 train_time:96549ms step_avg:60.38ms
step:1600/2270 train_time:96609ms step_avg:60.38ms
step:1601/2270 train_time:96670ms step_avg:60.38ms
step:1602/2270 train_time:96730ms step_avg:60.38ms
step:1603/2270 train_time:96792ms step_avg:60.38ms
step:1604/2270 train_time:96851ms step_avg:60.38ms
step:1605/2270 train_time:96913ms step_avg:60.38ms
step:1606/2270 train_time:96973ms step_avg:60.38ms
step:1607/2270 train_time:97036ms step_avg:60.38ms
step:1608/2270 train_time:97096ms step_avg:60.38ms
step:1609/2270 train_time:97158ms step_avg:60.38ms
step:1610/2270 train_time:97218ms step_avg:60.38ms
step:1611/2270 train_time:97281ms step_avg:60.39ms
step:1612/2270 train_time:97342ms step_avg:60.39ms
step:1613/2270 train_time:97405ms step_avg:60.39ms
step:1614/2270 train_time:97465ms step_avg:60.39ms
step:1615/2270 train_time:97528ms step_avg:60.39ms
step:1616/2270 train_time:97588ms step_avg:60.39ms
step:1617/2270 train_time:97650ms step_avg:60.39ms
step:1618/2270 train_time:97710ms step_avg:60.39ms
step:1619/2270 train_time:97772ms step_avg:60.39ms
step:1620/2270 train_time:97832ms step_avg:60.39ms
step:1621/2270 train_time:97894ms step_avg:60.39ms
step:1622/2270 train_time:97953ms step_avg:60.39ms
step:1623/2270 train_time:98016ms step_avg:60.39ms
step:1624/2270 train_time:98076ms step_avg:60.39ms
step:1625/2270 train_time:98139ms step_avg:60.39ms
step:1626/2270 train_time:98199ms step_avg:60.39ms
step:1627/2270 train_time:98262ms step_avg:60.39ms
step:1628/2270 train_time:98322ms step_avg:60.39ms
step:1629/2270 train_time:98385ms step_avg:60.40ms
step:1630/2270 train_time:98446ms step_avg:60.40ms
step:1631/2270 train_time:98508ms step_avg:60.40ms
step:1632/2270 train_time:98568ms step_avg:60.40ms
step:1633/2270 train_time:98631ms step_avg:60.40ms
step:1634/2270 train_time:98691ms step_avg:60.40ms
step:1635/2270 train_time:98753ms step_avg:60.40ms
step:1636/2270 train_time:98813ms step_avg:60.40ms
step:1637/2270 train_time:98875ms step_avg:60.40ms
step:1638/2270 train_time:98935ms step_avg:60.40ms
step:1639/2270 train_time:98998ms step_avg:60.40ms
step:1640/2270 train_time:99058ms step_avg:60.40ms
step:1641/2270 train_time:99121ms step_avg:60.40ms
step:1642/2270 train_time:99181ms step_avg:60.40ms
step:1643/2270 train_time:99244ms step_avg:60.40ms
step:1644/2270 train_time:99304ms step_avg:60.40ms
step:1645/2270 train_time:99366ms step_avg:60.40ms
step:1646/2270 train_time:99426ms step_avg:60.40ms
step:1647/2270 train_time:99489ms step_avg:60.41ms
step:1648/2270 train_time:99549ms step_avg:60.41ms
step:1649/2270 train_time:99611ms step_avg:60.41ms
step:1650/2270 train_time:99672ms step_avg:60.41ms
step:1651/2270 train_time:99734ms step_avg:60.41ms
step:1652/2270 train_time:99794ms step_avg:60.41ms
step:1653/2270 train_time:99856ms step_avg:60.41ms
step:1654/2270 train_time:99916ms step_avg:60.41ms
step:1655/2270 train_time:99978ms step_avg:60.41ms
step:1656/2270 train_time:100038ms step_avg:60.41ms
step:1657/2270 train_time:100100ms step_avg:60.41ms
step:1658/2270 train_time:100160ms step_avg:60.41ms
step:1659/2270 train_time:100222ms step_avg:60.41ms
step:1660/2270 train_time:100283ms step_avg:60.41ms
step:1661/2270 train_time:100346ms step_avg:60.41ms
step:1662/2270 train_time:100406ms step_avg:60.41ms
step:1663/2270 train_time:100468ms step_avg:60.41ms
step:1664/2270 train_time:100528ms step_avg:60.41ms
step:1665/2270 train_time:100590ms step_avg:60.41ms
step:1666/2270 train_time:100650ms step_avg:60.41ms
step:1667/2270 train_time:100712ms step_avg:60.42ms
step:1668/2270 train_time:100771ms step_avg:60.41ms
step:1669/2270 train_time:100833ms step_avg:60.42ms
step:1670/2270 train_time:100893ms step_avg:60.41ms
step:1671/2270 train_time:100956ms step_avg:60.42ms
step:1672/2270 train_time:101016ms step_avg:60.42ms
step:1673/2270 train_time:101078ms step_avg:60.42ms
step:1674/2270 train_time:101139ms step_avg:60.42ms
step:1675/2270 train_time:101201ms step_avg:60.42ms
step:1676/2270 train_time:101261ms step_avg:60.42ms
step:1677/2270 train_time:101324ms step_avg:60.42ms
step:1678/2270 train_time:101385ms step_avg:60.42ms
step:1679/2270 train_time:101447ms step_avg:60.42ms
step:1680/2270 train_time:101507ms step_avg:60.42ms
step:1681/2270 train_time:101569ms step_avg:60.42ms
step:1682/2270 train_time:101629ms step_avg:60.42ms
step:1683/2270 train_time:101691ms step_avg:60.42ms
step:1684/2270 train_time:101751ms step_avg:60.42ms
step:1685/2270 train_time:101813ms step_avg:60.42ms
step:1686/2270 train_time:101872ms step_avg:60.42ms
step:1687/2270 train_time:101934ms step_avg:60.42ms
step:1688/2270 train_time:101994ms step_avg:60.42ms
step:1689/2270 train_time:102057ms step_avg:60.42ms
step:1690/2270 train_time:102117ms step_avg:60.42ms
step:1691/2270 train_time:102180ms step_avg:60.43ms
step:1692/2270 train_time:102241ms step_avg:60.43ms
step:1693/2270 train_time:102303ms step_avg:60.43ms
step:1694/2270 train_time:102364ms step_avg:60.43ms
step:1695/2270 train_time:102426ms step_avg:60.43ms
step:1696/2270 train_time:102486ms step_avg:60.43ms
step:1697/2270 train_time:102549ms step_avg:60.43ms
step:1698/2270 train_time:102609ms step_avg:60.43ms
step:1699/2270 train_time:102671ms step_avg:60.43ms
step:1700/2270 train_time:102731ms step_avg:60.43ms
step:1701/2270 train_time:102793ms step_avg:60.43ms
step:1702/2270 train_time:102852ms step_avg:60.43ms
step:1703/2270 train_time:102914ms step_avg:60.43ms
step:1704/2270 train_time:102973ms step_avg:60.43ms
step:1705/2270 train_time:103036ms step_avg:60.43ms
step:1706/2270 train_time:103095ms step_avg:60.43ms
step:1707/2270 train_time:103159ms step_avg:60.43ms
step:1708/2270 train_time:103219ms step_avg:60.43ms
step:1709/2270 train_time:103282ms step_avg:60.43ms
step:1710/2270 train_time:103343ms step_avg:60.43ms
step:1711/2270 train_time:103406ms step_avg:60.44ms
step:1712/2270 train_time:103466ms step_avg:60.44ms
step:1713/2270 train_time:103528ms step_avg:60.44ms
step:1714/2270 train_time:103588ms step_avg:60.44ms
step:1715/2270 train_time:103651ms step_avg:60.44ms
step:1716/2270 train_time:103710ms step_avg:60.44ms
step:1717/2270 train_time:103772ms step_avg:60.44ms
step:1718/2270 train_time:103833ms step_avg:60.44ms
step:1719/2270 train_time:103893ms step_avg:60.44ms
step:1720/2270 train_time:103953ms step_avg:60.44ms
step:1721/2270 train_time:104015ms step_avg:60.44ms
step:1722/2270 train_time:104074ms step_avg:60.44ms
step:1723/2270 train_time:104137ms step_avg:60.44ms
step:1724/2270 train_time:104198ms step_avg:60.44ms
step:1725/2270 train_time:104262ms step_avg:60.44ms
step:1726/2270 train_time:104322ms step_avg:60.44ms
step:1727/2270 train_time:104385ms step_avg:60.44ms
step:1728/2270 train_time:104445ms step_avg:60.44ms
step:1729/2270 train_time:104508ms step_avg:60.44ms
step:1730/2270 train_time:104567ms step_avg:60.44ms
step:1731/2270 train_time:104631ms step_avg:60.45ms
step:1732/2270 train_time:104691ms step_avg:60.45ms
step:1733/2270 train_time:104753ms step_avg:60.45ms
step:1734/2270 train_time:104812ms step_avg:60.45ms
step:1735/2270 train_time:104874ms step_avg:60.45ms
step:1736/2270 train_time:104933ms step_avg:60.45ms
step:1737/2270 train_time:104996ms step_avg:60.45ms
step:1738/2270 train_time:105055ms step_avg:60.45ms
step:1739/2270 train_time:105118ms step_avg:60.45ms
step:1740/2270 train_time:105178ms step_avg:60.45ms
step:1741/2270 train_time:105242ms step_avg:60.45ms
step:1742/2270 train_time:105302ms step_avg:60.45ms
step:1743/2270 train_time:105365ms step_avg:60.45ms
step:1744/2270 train_time:105425ms step_avg:60.45ms
step:1745/2270 train_time:105488ms step_avg:60.45ms
step:1746/2270 train_time:105548ms step_avg:60.45ms
step:1747/2270 train_time:105610ms step_avg:60.45ms
step:1748/2270 train_time:105670ms step_avg:60.45ms
step:1749/2270 train_time:105732ms step_avg:60.45ms
step:1750/2270 train_time:105791ms step_avg:60.45ms
step:1750/2270 val_loss:3.3727 train_time:105854ms step_avg:60.49ms
step:1751/2270 train_time:105872ms step_avg:60.46ms
step:1752/2270 train_time:105917ms step_avg:60.46ms
step:1753/2270 train_time:105981ms step_avg:60.46ms
step:1754/2270 train_time:106044ms step_avg:60.46ms
step:1755/2270 train_time:106106ms step_avg:60.46ms
step:1756/2270 train_time:106166ms step_avg:60.46ms
step:1757/2270 train_time:106227ms step_avg:60.46ms
step:1758/2270 train_time:106286ms step_avg:60.46ms
step:1759/2270 train_time:106348ms step_avg:60.46ms
step:1760/2270 train_time:106408ms step_avg:60.46ms
step:1761/2270 train_time:106469ms step_avg:60.46ms
step:1762/2270 train_time:106528ms step_avg:60.46ms
step:1763/2270 train_time:106590ms step_avg:60.46ms
step:1764/2270 train_time:106649ms step_avg:60.46ms
step:1765/2270 train_time:106712ms step_avg:60.46ms
step:1766/2270 train_time:106774ms step_avg:60.46ms
step:1767/2270 train_time:106839ms step_avg:60.46ms
step:1768/2270 train_time:106902ms step_avg:60.46ms
step:1769/2270 train_time:106965ms step_avg:60.47ms
step:1770/2270 train_time:107026ms step_avg:60.47ms
step:1771/2270 train_time:107089ms step_avg:60.47ms
step:1772/2270 train_time:107149ms step_avg:60.47ms
step:1773/2270 train_time:107211ms step_avg:60.47ms
step:1774/2270 train_time:107271ms step_avg:60.47ms
step:1775/2270 train_time:107333ms step_avg:60.47ms
step:1776/2270 train_time:107393ms step_avg:60.47ms
step:1777/2270 train_time:107455ms step_avg:60.47ms
step:1778/2270 train_time:107513ms step_avg:60.47ms
step:1779/2270 train_time:107575ms step_avg:60.47ms
step:1780/2270 train_time:107634ms step_avg:60.47ms
step:1781/2270 train_time:107696ms step_avg:60.47ms
step:1782/2270 train_time:107756ms step_avg:60.47ms
step:1783/2270 train_time:107820ms step_avg:60.47ms
step:1784/2270 train_time:107880ms step_avg:60.47ms
step:1785/2270 train_time:107944ms step_avg:60.47ms
step:1786/2270 train_time:108004ms step_avg:60.47ms
step:1787/2270 train_time:108067ms step_avg:60.47ms
step:1788/2270 train_time:108127ms step_avg:60.47ms
step:1789/2270 train_time:108190ms step_avg:60.47ms
step:1790/2270 train_time:108250ms step_avg:60.47ms
step:1791/2270 train_time:108312ms step_avg:60.48ms
step:1792/2270 train_time:108372ms step_avg:60.48ms
step:1793/2270 train_time:108434ms step_avg:60.48ms
step:1794/2270 train_time:108493ms step_avg:60.48ms
step:1795/2270 train_time:108555ms step_avg:60.48ms
step:1796/2270 train_time:108614ms step_avg:60.48ms
step:1797/2270 train_time:108676ms step_avg:60.48ms
step:1798/2270 train_time:108736ms step_avg:60.48ms
step:1799/2270 train_time:108799ms step_avg:60.48ms
step:1800/2270 train_time:108859ms step_avg:60.48ms
step:1801/2270 train_time:108922ms step_avg:60.48ms
step:1802/2270 train_time:108981ms step_avg:60.48ms
step:1803/2270 train_time:109043ms step_avg:60.48ms
step:1804/2270 train_time:109103ms step_avg:60.48ms
step:1805/2270 train_time:109165ms step_avg:60.48ms
step:1806/2270 train_time:109225ms step_avg:60.48ms
step:1807/2270 train_time:109288ms step_avg:60.48ms
step:1808/2270 train_time:109348ms step_avg:60.48ms
step:1809/2270 train_time:109410ms step_avg:60.48ms
step:1810/2270 train_time:109470ms step_avg:60.48ms
step:1811/2270 train_time:109533ms step_avg:60.48ms
step:1812/2270 train_time:109593ms step_avg:60.48ms
step:1813/2270 train_time:109656ms step_avg:60.48ms
step:1814/2270 train_time:109715ms step_avg:60.48ms
step:1815/2270 train_time:109778ms step_avg:60.48ms
step:1816/2270 train_time:109838ms step_avg:60.48ms
step:1817/2270 train_time:109900ms step_avg:60.48ms
step:1818/2270 train_time:109961ms step_avg:60.48ms
step:1819/2270 train_time:110023ms step_avg:60.49ms
step:1820/2270 train_time:110083ms step_avg:60.49ms
step:1821/2270 train_time:110146ms step_avg:60.49ms
step:1822/2270 train_time:110205ms step_avg:60.49ms
step:1823/2270 train_time:110268ms step_avg:60.49ms
step:1824/2270 train_time:110327ms step_avg:60.49ms
step:1825/2270 train_time:110390ms step_avg:60.49ms
step:1826/2270 train_time:110450ms step_avg:60.49ms
step:1827/2270 train_time:110513ms step_avg:60.49ms
step:1828/2270 train_time:110573ms step_avg:60.49ms
step:1829/2270 train_time:110636ms step_avg:60.49ms
step:1830/2270 train_time:110695ms step_avg:60.49ms
step:1831/2270 train_time:110758ms step_avg:60.49ms
step:1832/2270 train_time:110817ms step_avg:60.49ms
step:1833/2270 train_time:110880ms step_avg:60.49ms
step:1834/2270 train_time:110939ms step_avg:60.49ms
step:1835/2270 train_time:111002ms step_avg:60.49ms
step:1836/2270 train_time:111062ms step_avg:60.49ms
step:1837/2270 train_time:111123ms step_avg:60.49ms
step:1838/2270 train_time:111183ms step_avg:60.49ms
step:1839/2270 train_time:111245ms step_avg:60.49ms
step:1840/2270 train_time:111305ms step_avg:60.49ms
step:1841/2270 train_time:111368ms step_avg:60.49ms
step:1842/2270 train_time:111428ms step_avg:60.49ms
step:1843/2270 train_time:111490ms step_avg:60.49ms
step:1844/2270 train_time:111551ms step_avg:60.49ms
step:1845/2270 train_time:111614ms step_avg:60.50ms
step:1846/2270 train_time:111674ms step_avg:60.49ms
step:1847/2270 train_time:111736ms step_avg:60.50ms
step:1848/2270 train_time:111796ms step_avg:60.50ms
step:1849/2270 train_time:111859ms step_avg:60.50ms
step:1850/2270 train_time:111919ms step_avg:60.50ms
step:1851/2270 train_time:111981ms step_avg:60.50ms
step:1852/2270 train_time:112041ms step_avg:60.50ms
step:1853/2270 train_time:112104ms step_avg:60.50ms
step:1854/2270 train_time:112163ms step_avg:60.50ms
step:1855/2270 train_time:112225ms step_avg:60.50ms
step:1856/2270 train_time:112284ms step_avg:60.50ms
step:1857/2270 train_time:112347ms step_avg:60.50ms
step:1858/2270 train_time:112407ms step_avg:60.50ms
step:1859/2270 train_time:112469ms step_avg:60.50ms
step:1860/2270 train_time:112529ms step_avg:60.50ms
step:1861/2270 train_time:112592ms step_avg:60.50ms
step:1862/2270 train_time:112653ms step_avg:60.50ms
step:1863/2270 train_time:112715ms step_avg:60.50ms
step:1864/2270 train_time:112776ms step_avg:60.50ms
step:1865/2270 train_time:112838ms step_avg:60.50ms
step:1866/2270 train_time:112898ms step_avg:60.50ms
step:1867/2270 train_time:112961ms step_avg:60.50ms
step:1868/2270 train_time:113021ms step_avg:60.50ms
step:1869/2270 train_time:113083ms step_avg:60.50ms
step:1870/2270 train_time:113142ms step_avg:60.50ms
step:1871/2270 train_time:113204ms step_avg:60.50ms
step:1872/2270 train_time:113264ms step_avg:60.50ms
step:1873/2270 train_time:113325ms step_avg:60.50ms
step:1874/2270 train_time:113385ms step_avg:60.50ms
step:1875/2270 train_time:113447ms step_avg:60.51ms
step:1876/2270 train_time:113508ms step_avg:60.51ms
step:1877/2270 train_time:113571ms step_avg:60.51ms
step:1878/2270 train_time:113631ms step_avg:60.51ms
step:1879/2270 train_time:113694ms step_avg:60.51ms
step:1880/2270 train_time:113755ms step_avg:60.51ms
step:1881/2270 train_time:113817ms step_avg:60.51ms
step:1882/2270 train_time:113877ms step_avg:60.51ms
step:1883/2270 train_time:113940ms step_avg:60.51ms
step:1884/2270 train_time:114000ms step_avg:60.51ms
step:1885/2270 train_time:114062ms step_avg:60.51ms
step:1886/2270 train_time:114121ms step_avg:60.51ms
step:1887/2270 train_time:114183ms step_avg:60.51ms
step:1888/2270 train_time:114242ms step_avg:60.51ms
step:1889/2270 train_time:114304ms step_avg:60.51ms
step:1890/2270 train_time:114363ms step_avg:60.51ms
step:1891/2270 train_time:114425ms step_avg:60.51ms
step:1892/2270 train_time:114485ms step_avg:60.51ms
step:1893/2270 train_time:114547ms step_avg:60.51ms
step:1894/2270 train_time:114608ms step_avg:60.51ms
step:1895/2270 train_time:114671ms step_avg:60.51ms
step:1896/2270 train_time:114732ms step_avg:60.51ms
step:1897/2270 train_time:114795ms step_avg:60.51ms
step:1898/2270 train_time:114856ms step_avg:60.51ms
step:1899/2270 train_time:114919ms step_avg:60.52ms
step:1900/2270 train_time:114978ms step_avg:60.51ms
step:1901/2270 train_time:115041ms step_avg:60.52ms
step:1902/2270 train_time:115101ms step_avg:60.52ms
step:1903/2270 train_time:115163ms step_avg:60.52ms
step:1904/2270 train_time:115222ms step_avg:60.52ms
step:1905/2270 train_time:115284ms step_avg:60.52ms
step:1906/2270 train_time:115343ms step_avg:60.52ms
step:1907/2270 train_time:115406ms step_avg:60.52ms
step:1908/2270 train_time:115466ms step_avg:60.52ms
step:1909/2270 train_time:115529ms step_avg:60.52ms
step:1910/2270 train_time:115590ms step_avg:60.52ms
step:1911/2270 train_time:115653ms step_avg:60.52ms
step:1912/2270 train_time:115714ms step_avg:60.52ms
step:1913/2270 train_time:115777ms step_avg:60.52ms
step:1914/2270 train_time:115837ms step_avg:60.52ms
step:1915/2270 train_time:115900ms step_avg:60.52ms
step:1916/2270 train_time:115960ms step_avg:60.52ms
step:1917/2270 train_time:116023ms step_avg:60.52ms
step:1918/2270 train_time:116082ms step_avg:60.52ms
step:1919/2270 train_time:116144ms step_avg:60.52ms
step:1920/2270 train_time:116205ms step_avg:60.52ms
step:1921/2270 train_time:116267ms step_avg:60.52ms
step:1922/2270 train_time:116327ms step_avg:60.52ms
step:1923/2270 train_time:116389ms step_avg:60.52ms
step:1924/2270 train_time:116451ms step_avg:60.53ms
step:1925/2270 train_time:116512ms step_avg:60.53ms
step:1926/2270 train_time:116572ms step_avg:60.53ms
step:1927/2270 train_time:116635ms step_avg:60.53ms
step:1928/2270 train_time:116695ms step_avg:60.53ms
step:1929/2270 train_time:116758ms step_avg:60.53ms
step:1930/2270 train_time:116818ms step_avg:60.53ms
step:1931/2270 train_time:116881ms step_avg:60.53ms
step:1932/2270 train_time:116941ms step_avg:60.53ms
step:1933/2270 train_time:117003ms step_avg:60.53ms
step:1934/2270 train_time:117063ms step_avg:60.53ms
step:1935/2270 train_time:117126ms step_avg:60.53ms
step:1936/2270 train_time:117186ms step_avg:60.53ms
step:1937/2270 train_time:117248ms step_avg:60.53ms
step:1938/2270 train_time:117308ms step_avg:60.53ms
step:1939/2270 train_time:117370ms step_avg:60.53ms
step:1940/2270 train_time:117430ms step_avg:60.53ms
step:1941/2270 train_time:117494ms step_avg:60.53ms
step:1942/2270 train_time:117553ms step_avg:60.53ms
step:1943/2270 train_time:117616ms step_avg:60.53ms
step:1944/2270 train_time:117677ms step_avg:60.53ms
step:1945/2270 train_time:117739ms step_avg:60.53ms
step:1946/2270 train_time:117799ms step_avg:60.53ms
step:1947/2270 train_time:117861ms step_avg:60.53ms
step:1948/2270 train_time:117922ms step_avg:60.54ms
step:1949/2270 train_time:117985ms step_avg:60.54ms
step:1950/2270 train_time:118044ms step_avg:60.54ms
step:1951/2270 train_time:118107ms step_avg:60.54ms
step:1952/2270 train_time:118167ms step_avg:60.54ms
step:1953/2270 train_time:118229ms step_avg:60.54ms
step:1954/2270 train_time:118290ms step_avg:60.54ms
step:1955/2270 train_time:118352ms step_avg:60.54ms
step:1956/2270 train_time:118412ms step_avg:60.54ms
step:1957/2270 train_time:118474ms step_avg:60.54ms
step:1958/2270 train_time:118534ms step_avg:60.54ms
step:1959/2270 train_time:118597ms step_avg:60.54ms
step:1960/2270 train_time:118657ms step_avg:60.54ms
step:1961/2270 train_time:118720ms step_avg:60.54ms
step:1962/2270 train_time:118780ms step_avg:60.54ms
step:1963/2270 train_time:118844ms step_avg:60.54ms
step:1964/2270 train_time:118902ms step_avg:60.54ms
step:1965/2270 train_time:118964ms step_avg:60.54ms
step:1966/2270 train_time:119025ms step_avg:60.54ms
step:1967/2270 train_time:119087ms step_avg:60.54ms
step:1968/2270 train_time:119147ms step_avg:60.54ms
step:1969/2270 train_time:119210ms step_avg:60.54ms
step:1970/2270 train_time:119270ms step_avg:60.54ms
step:1971/2270 train_time:119333ms step_avg:60.54ms
step:1972/2270 train_time:119394ms step_avg:60.54ms
step:1973/2270 train_time:119456ms step_avg:60.55ms
step:1974/2270 train_time:119516ms step_avg:60.55ms
step:1975/2270 train_time:119579ms step_avg:60.55ms
step:1976/2270 train_time:119639ms step_avg:60.55ms
step:1977/2270 train_time:119702ms step_avg:60.55ms
step:1978/2270 train_time:119762ms step_avg:60.55ms
step:1979/2270 train_time:119824ms step_avg:60.55ms
step:1980/2270 train_time:119884ms step_avg:60.55ms
step:1981/2270 train_time:119947ms step_avg:60.55ms
step:1982/2270 train_time:120008ms step_avg:60.55ms
step:1983/2270 train_time:120071ms step_avg:60.55ms
step:1984/2270 train_time:120131ms step_avg:60.55ms
step:1985/2270 train_time:120194ms step_avg:60.55ms
step:1986/2270 train_time:120254ms step_avg:60.55ms
step:1987/2270 train_time:120316ms step_avg:60.55ms
step:1988/2270 train_time:120376ms step_avg:60.55ms
step:1989/2270 train_time:120438ms step_avg:60.55ms
step:1990/2270 train_time:120498ms step_avg:60.55ms
step:1991/2270 train_time:120561ms step_avg:60.55ms
step:1992/2270 train_time:120621ms step_avg:60.55ms
step:1993/2270 train_time:120684ms step_avg:60.55ms
step:1994/2270 train_time:120744ms step_avg:60.55ms
step:1995/2270 train_time:120807ms step_avg:60.55ms
step:1996/2270 train_time:120867ms step_avg:60.55ms
step:1997/2270 train_time:120930ms step_avg:60.56ms
step:1998/2270 train_time:120990ms step_avg:60.56ms
step:1999/2270 train_time:121054ms step_avg:60.56ms
step:2000/2270 train_time:121113ms step_avg:60.56ms
step:2000/2270 val_loss:3.3213 train_time:121177ms step_avg:60.59ms
step:2001/2270 train_time:121195ms step_avg:60.57ms
step:2002/2270 train_time:121239ms step_avg:60.56ms
step:2003/2270 train_time:121302ms step_avg:60.56ms
step:2004/2270 train_time:121363ms step_avg:60.56ms
step:2005/2270 train_time:121426ms step_avg:60.56ms
step:2006/2270 train_time:121488ms step_avg:60.56ms
step:2007/2270 train_time:121550ms step_avg:60.56ms
step:2008/2270 train_time:121609ms step_avg:60.56ms
step:2009/2270 train_time:121673ms step_avg:60.56ms
step:2010/2270 train_time:121731ms step_avg:60.56ms
step:2011/2270 train_time:121793ms step_avg:60.56ms
step:2012/2270 train_time:121852ms step_avg:60.56ms
step:2013/2270 train_time:121915ms step_avg:60.56ms
step:2014/2270 train_time:121976ms step_avg:60.56ms
step:2015/2270 train_time:122039ms step_avg:60.57ms
step:2016/2270 train_time:122099ms step_avg:60.57ms
step:2017/2270 train_time:122164ms step_avg:60.57ms
step:2018/2270 train_time:122225ms step_avg:60.57ms
step:2019/2270 train_time:122288ms step_avg:60.57ms
step:2020/2270 train_time:122349ms step_avg:60.57ms
step:2021/2270 train_time:122413ms step_avg:60.57ms
step:2022/2270 train_time:122474ms step_avg:60.57ms
step:2023/2270 train_time:122537ms step_avg:60.57ms
step:2024/2270 train_time:122596ms step_avg:60.57ms
step:2025/2270 train_time:122658ms step_avg:60.57ms
step:2026/2270 train_time:122717ms step_avg:60.57ms
step:2027/2270 train_time:122778ms step_avg:60.57ms
step:2028/2270 train_time:122837ms step_avg:60.57ms
step:2029/2270 train_time:122899ms step_avg:60.57ms
step:2030/2270 train_time:122958ms step_avg:60.57ms
step:2031/2270 train_time:123020ms step_avg:60.57ms
step:2032/2270 train_time:123080ms step_avg:60.57ms
step:2033/2270 train_time:123143ms step_avg:60.57ms
step:2034/2270 train_time:123203ms step_avg:60.57ms
step:2035/2270 train_time:123267ms step_avg:60.57ms
step:2036/2270 train_time:123327ms step_avg:60.57ms
step:2037/2270 train_time:123390ms step_avg:60.57ms
step:2038/2270 train_time:123451ms step_avg:60.57ms
step:2039/2270 train_time:123514ms step_avg:60.58ms
step:2040/2270 train_time:123574ms step_avg:60.58ms
step:2041/2270 train_time:123637ms step_avg:60.58ms
step:2042/2270 train_time:123696ms step_avg:60.58ms
step:2043/2270 train_time:123758ms step_avg:60.58ms
step:2044/2270 train_time:123818ms step_avg:60.58ms
step:2045/2270 train_time:123880ms step_avg:60.58ms
step:2046/2270 train_time:123939ms step_avg:60.58ms
step:2047/2270 train_time:124001ms step_avg:60.58ms
step:2048/2270 train_time:124062ms step_avg:60.58ms
step:2049/2270 train_time:124125ms step_avg:60.58ms
step:2050/2270 train_time:124185ms step_avg:60.58ms
step:2051/2270 train_time:124248ms step_avg:60.58ms
step:2052/2270 train_time:124308ms step_avg:60.58ms
step:2053/2270 train_time:124372ms step_avg:60.58ms
step:2054/2270 train_time:124433ms step_avg:60.58ms
step:2055/2270 train_time:124495ms step_avg:60.58ms
step:2056/2270 train_time:124555ms step_avg:60.58ms
step:2057/2270 train_time:124618ms step_avg:60.58ms
step:2058/2270 train_time:124678ms step_avg:60.58ms
step:2059/2270 train_time:124740ms step_avg:60.58ms
step:2060/2270 train_time:124800ms step_avg:60.58ms
step:2061/2270 train_time:124861ms step_avg:60.58ms
step:2062/2270 train_time:124921ms step_avg:60.58ms
step:2063/2270 train_time:124984ms step_avg:60.58ms
step:2064/2270 train_time:125044ms step_avg:60.58ms
step:2065/2270 train_time:125106ms step_avg:60.58ms
step:2066/2270 train_time:125166ms step_avg:60.58ms
step:2067/2270 train_time:125229ms step_avg:60.58ms
step:2068/2270 train_time:125289ms step_avg:60.58ms
step:2069/2270 train_time:125352ms step_avg:60.59ms
step:2070/2270 train_time:125413ms step_avg:60.59ms
step:2071/2270 train_time:125477ms step_avg:60.59ms
step:2072/2270 train_time:125536ms step_avg:60.59ms
step:2073/2270 train_time:125598ms step_avg:60.59ms
step:2074/2270 train_time:125658ms step_avg:60.59ms
step:2075/2270 train_time:125721ms step_avg:60.59ms
step:2076/2270 train_time:125781ms step_avg:60.59ms
step:2077/2270 train_time:125843ms step_avg:60.59ms
step:2078/2270 train_time:125903ms step_avg:60.59ms
step:2079/2270 train_time:125966ms step_avg:60.59ms
step:2080/2270 train_time:126026ms step_avg:60.59ms
step:2081/2270 train_time:126088ms step_avg:60.59ms
step:2082/2270 train_time:126148ms step_avg:60.59ms
step:2083/2270 train_time:126211ms step_avg:60.59ms
step:2084/2270 train_time:126272ms step_avg:60.59ms
step:2085/2270 train_time:126334ms step_avg:60.59ms
step:2086/2270 train_time:126395ms step_avg:60.59ms
step:2087/2270 train_time:126459ms step_avg:60.59ms
step:2088/2270 train_time:126518ms step_avg:60.59ms
step:2089/2270 train_time:126581ms step_avg:60.59ms
step:2090/2270 train_time:126641ms step_avg:60.59ms
step:2091/2270 train_time:126703ms step_avg:60.59ms
step:2092/2270 train_time:126763ms step_avg:60.59ms
step:2093/2270 train_time:126826ms step_avg:60.60ms
step:2094/2270 train_time:126886ms step_avg:60.60ms
step:2095/2270 train_time:126948ms step_avg:60.60ms
step:2096/2270 train_time:127008ms step_avg:60.60ms
step:2097/2270 train_time:127071ms step_avg:60.60ms
step:2098/2270 train_time:127131ms step_avg:60.60ms
step:2099/2270 train_time:127194ms step_avg:60.60ms
step:2100/2270 train_time:127254ms step_avg:60.60ms
step:2101/2270 train_time:127317ms step_avg:60.60ms
step:2102/2270 train_time:127377ms step_avg:60.60ms
step:2103/2270 train_time:127439ms step_avg:60.60ms
step:2104/2270 train_time:127499ms step_avg:60.60ms
step:2105/2270 train_time:127562ms step_avg:60.60ms
step:2106/2270 train_time:127622ms step_avg:60.60ms
step:2107/2270 train_time:127684ms step_avg:60.60ms
step:2108/2270 train_time:127744ms step_avg:60.60ms
step:2109/2270 train_time:127806ms step_avg:60.60ms
step:2110/2270 train_time:127867ms step_avg:60.60ms
step:2111/2270 train_time:127929ms step_avg:60.60ms
step:2112/2270 train_time:127989ms step_avg:60.60ms
step:2113/2270 train_time:128052ms step_avg:60.60ms
step:2114/2270 train_time:128112ms step_avg:60.60ms
step:2115/2270 train_time:128175ms step_avg:60.60ms
step:2116/2270 train_time:128235ms step_avg:60.60ms
step:2117/2270 train_time:128297ms step_avg:60.60ms
step:2118/2270 train_time:128357ms step_avg:60.60ms
step:2119/2270 train_time:128420ms step_avg:60.60ms
step:2120/2270 train_time:128480ms step_avg:60.60ms
step:2121/2270 train_time:128542ms step_avg:60.60ms
step:2122/2270 train_time:128602ms step_avg:60.60ms
step:2123/2270 train_time:128665ms step_avg:60.61ms
step:2124/2270 train_time:128725ms step_avg:60.61ms
step:2125/2270 train_time:128788ms step_avg:60.61ms
step:2126/2270 train_time:128848ms step_avg:60.61ms
step:2127/2270 train_time:128910ms step_avg:60.61ms
step:2128/2270 train_time:128970ms step_avg:60.61ms
step:2129/2270 train_time:129032ms step_avg:60.61ms
step:2130/2270 train_time:129093ms step_avg:60.61ms
step:2131/2270 train_time:129155ms step_avg:60.61ms
step:2132/2270 train_time:129216ms step_avg:60.61ms
step:2133/2270 train_time:129279ms step_avg:60.61ms
step:2134/2270 train_time:129339ms step_avg:60.61ms
step:2135/2270 train_time:129401ms step_avg:60.61ms
step:2136/2270 train_time:129461ms step_avg:60.61ms
step:2137/2270 train_time:129524ms step_avg:60.61ms
step:2138/2270 train_time:129585ms step_avg:60.61ms
step:2139/2270 train_time:129647ms step_avg:60.61ms
step:2140/2270 train_time:129707ms step_avg:60.61ms
step:2141/2270 train_time:129769ms step_avg:60.61ms
step:2142/2270 train_time:129830ms step_avg:60.61ms
step:2143/2270 train_time:129892ms step_avg:60.61ms
step:2144/2270 train_time:129952ms step_avg:60.61ms
step:2145/2270 train_time:130015ms step_avg:60.61ms
step:2146/2270 train_time:130075ms step_avg:60.61ms
step:2147/2270 train_time:130138ms step_avg:60.61ms
step:2148/2270 train_time:130198ms step_avg:60.61ms
step:2149/2270 train_time:130260ms step_avg:60.61ms
step:2150/2270 train_time:130320ms step_avg:60.61ms
step:2151/2270 train_time:130382ms step_avg:60.61ms
step:2152/2270 train_time:130443ms step_avg:60.61ms
step:2153/2270 train_time:130505ms step_avg:60.62ms
step:2154/2270 train_time:130565ms step_avg:60.62ms
step:2155/2270 train_time:130627ms step_avg:60.62ms
step:2156/2270 train_time:130687ms step_avg:60.62ms
step:2157/2270 train_time:130750ms step_avg:60.62ms
step:2158/2270 train_time:130811ms step_avg:60.62ms
step:2159/2270 train_time:130873ms step_avg:60.62ms
step:2160/2270 train_time:130934ms step_avg:60.62ms
step:2161/2270 train_time:130997ms step_avg:60.62ms
step:2162/2270 train_time:131056ms step_avg:60.62ms
step:2163/2270 train_time:131119ms step_avg:60.62ms
step:2164/2270 train_time:131179ms step_avg:60.62ms
step:2165/2270 train_time:131241ms step_avg:60.62ms
step:2166/2270 train_time:131302ms step_avg:60.62ms
step:2167/2270 train_time:131364ms step_avg:60.62ms
step:2168/2270 train_time:131424ms step_avg:60.62ms
step:2169/2270 train_time:131486ms step_avg:60.62ms
step:2170/2270 train_time:131546ms step_avg:60.62ms
step:2171/2270 train_time:131609ms step_avg:60.62ms
step:2172/2270 train_time:131669ms step_avg:60.62ms
step:2173/2270 train_time:131732ms step_avg:60.62ms
step:2174/2270 train_time:131792ms step_avg:60.62ms
step:2175/2270 train_time:131855ms step_avg:60.62ms
step:2176/2270 train_time:131915ms step_avg:60.62ms
step:2177/2270 train_time:131978ms step_avg:60.62ms
step:2178/2270 train_time:132038ms step_avg:60.62ms
step:2179/2270 train_time:132100ms step_avg:60.62ms
step:2180/2270 train_time:132160ms step_avg:60.62ms
step:2181/2270 train_time:132223ms step_avg:60.62ms
step:2182/2270 train_time:132283ms step_avg:60.62ms
step:2183/2270 train_time:132345ms step_avg:60.63ms
step:2184/2270 train_time:132405ms step_avg:60.62ms
step:2185/2270 train_time:132467ms step_avg:60.63ms
step:2186/2270 train_time:132528ms step_avg:60.63ms
step:2187/2270 train_time:132591ms step_avg:60.63ms
step:2188/2270 train_time:132651ms step_avg:60.63ms
step:2189/2270 train_time:132714ms step_avg:60.63ms
step:2190/2270 train_time:132775ms step_avg:60.63ms
step:2191/2270 train_time:132838ms step_avg:60.63ms
step:2192/2270 train_time:132897ms step_avg:60.63ms
step:2193/2270 train_time:132959ms step_avg:60.63ms
step:2194/2270 train_time:133019ms step_avg:60.63ms
step:2195/2270 train_time:133081ms step_avg:60.63ms
step:2196/2270 train_time:133141ms step_avg:60.63ms
step:2197/2270 train_time:133203ms step_avg:60.63ms
step:2198/2270 train_time:133264ms step_avg:60.63ms
step:2199/2270 train_time:133326ms step_avg:60.63ms
step:2200/2270 train_time:133386ms step_avg:60.63ms
step:2201/2270 train_time:133449ms step_avg:60.63ms
step:2202/2270 train_time:133509ms step_avg:60.63ms
step:2203/2270 train_time:133572ms step_avg:60.63ms
step:2204/2270 train_time:133633ms step_avg:60.63ms
step:2205/2270 train_time:133695ms step_avg:60.63ms
step:2206/2270 train_time:133756ms step_avg:60.63ms
step:2207/2270 train_time:133819ms step_avg:60.63ms
step:2208/2270 train_time:133879ms step_avg:60.63ms
step:2209/2270 train_time:133940ms step_avg:60.63ms
step:2210/2270 train_time:134000ms step_avg:60.63ms
step:2211/2270 train_time:134063ms step_avg:60.63ms
step:2212/2270 train_time:134124ms step_avg:60.63ms
step:2213/2270 train_time:134186ms step_avg:60.64ms
step:2214/2270 train_time:134246ms step_avg:60.64ms
step:2215/2270 train_time:134309ms step_avg:60.64ms
step:2216/2270 train_time:134372ms step_avg:60.64ms
step:2217/2270 train_time:134432ms step_avg:60.64ms
step:2218/2270 train_time:134492ms step_avg:60.64ms
step:2219/2270 train_time:134554ms step_avg:60.64ms
step:2220/2270 train_time:134614ms step_avg:60.64ms
step:2221/2270 train_time:134677ms step_avg:60.64ms
step:2222/2270 train_time:134737ms step_avg:60.64ms
step:2223/2270 train_time:134800ms step_avg:60.64ms
step:2224/2270 train_time:134860ms step_avg:60.64ms
step:2225/2270 train_time:134922ms step_avg:60.64ms
step:2226/2270 train_time:134982ms step_avg:60.64ms
step:2227/2270 train_time:135044ms step_avg:60.64ms
step:2228/2270 train_time:135104ms step_avg:60.64ms
step:2229/2270 train_time:135166ms step_avg:60.64ms
step:2230/2270 train_time:135226ms step_avg:60.64ms
step:2231/2270 train_time:135288ms step_avg:60.64ms
step:2232/2270 train_time:135348ms step_avg:60.64ms
step:2233/2270 train_time:135411ms step_avg:60.64ms
step:2234/2270 train_time:135471ms step_avg:60.64ms
step:2235/2270 train_time:135535ms step_avg:60.64ms
step:2236/2270 train_time:135595ms step_avg:60.64ms
step:2237/2270 train_time:135657ms step_avg:60.64ms
step:2238/2270 train_time:135717ms step_avg:60.64ms
step:2239/2270 train_time:135780ms step_avg:60.64ms
step:2240/2270 train_time:135840ms step_avg:60.64ms
step:2241/2270 train_time:135902ms step_avg:60.64ms
step:2242/2270 train_time:135963ms step_avg:60.64ms
step:2243/2270 train_time:136025ms step_avg:60.64ms
step:2244/2270 train_time:136085ms step_avg:60.64ms
step:2245/2270 train_time:136148ms step_avg:60.64ms
step:2246/2270 train_time:136208ms step_avg:60.64ms
step:2247/2270 train_time:136271ms step_avg:60.65ms
step:2248/2270 train_time:136331ms step_avg:60.65ms
step:2249/2270 train_time:136394ms step_avg:60.65ms
step:2250/2270 train_time:136454ms step_avg:60.65ms
step:2250/2270 val_loss:3.2838 train_time:136518ms step_avg:60.67ms
step:2251/2270 train_time:136536ms step_avg:60.66ms
step:2252/2270 train_time:136582ms step_avg:60.65ms
step:2253/2270 train_time:136650ms step_avg:60.65ms
step:2254/2270 train_time:136714ms step_avg:60.65ms
step:2255/2270 train_time:136776ms step_avg:60.65ms
step:2256/2270 train_time:136836ms step_avg:60.65ms
step:2257/2270 train_time:136898ms step_avg:60.65ms
step:2258/2270 train_time:136957ms step_avg:60.65ms
step:2259/2270 train_time:137019ms step_avg:60.65ms
step:2260/2270 train_time:137078ms step_avg:60.65ms
step:2261/2270 train_time:137141ms step_avg:60.66ms
step:2262/2270 train_time:137201ms step_avg:60.65ms
step:2263/2270 train_time:137263ms step_avg:60.66ms
step:2264/2270 train_time:137323ms step_avg:60.65ms
step:2265/2270 train_time:137385ms step_avg:60.66ms
step:2266/2270 train_time:137446ms step_avg:60.66ms
step:2267/2270 train_time:137510ms step_avg:60.66ms
step:2268/2270 train_time:137573ms step_avg:60.66ms
step:2269/2270 train_time:137637ms step_avg:60.66ms
step:2270/2270 train_time:137698ms step_avg:60.66ms
step:2270/2270 val_loss:3.2793 train_time:137762ms step_avg:60.69ms
peak memory allocated: 29249 MiB reserved: 50528 MiB

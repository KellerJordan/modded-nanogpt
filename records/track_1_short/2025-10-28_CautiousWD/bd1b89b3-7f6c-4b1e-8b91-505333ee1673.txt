import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, eps=1e-8, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp_up', 'mlp_down']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            elif params[module_idx].label == "smear_gate":
                # dividing by magnitude is equivalent of SVN for 1d tensors
                v_chunk = updated_grads / (updated_grads.norm(dim=(-2, -1), keepdim=True).clamp_min(1e-10))
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            same_sign = torch.signbit(v_chunk) == torch.signbit(param_chunk)
            v_chunk.add_(eff_wd * (param_chunk * same_sign.to(ref_param.dtype)))

            param_chunk.add_(-eff_lr * v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp_up'
        self.c_proj.label = 'mlp_down'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 2270
    lr_schedule = (0.5, 0.98)    # breakpoints for 3-part schedule: (flat, linear decay, flat)
    lr_min = 0.1
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 5, 7, 9, 11, 13)
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=0.01)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

def get_lr(step: int):
    assert step < args.num_iterations
    # Three part schedule: flat, linear decrease, flat
    lr_schedule = args.lr_schedule
    x = step / args.num_iterations

    if x < lr_schedule[0]:
        return 1.0
    elif x < lr_schedule[1]:
        progress = (x - lr_schedule[0]) / (lr_schedule[1] - lr_schedule[0])
        lr = 1.0 - (1.0 - args.lr_min) * progress
    else:
        lr = args.lr_min
    return lr

def get_ws(step: int):
    assert step <= args.num_iterations
    x = step / (args.num_iterations + 1)
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset()  #  momentum buffer not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    loss = 0
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        loss += model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps
    loss.backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Oct 28 03:46:37 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   40C    P0            127W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   32C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   31C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2270 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2270 train_time:114ms step_avg:114.18ms
step:2/2270 train_time:136ms step_avg:67.94ms
step:3/2270 train_time:174ms step_avg:58.02ms
step:4/2270 train_time:230ms step_avg:57.58ms
step:5/2270 train_time:290ms step_avg:57.96ms
step:6/2270 train_time:348ms step_avg:57.94ms
step:7/2270 train_time:408ms step_avg:58.33ms
step:8/2270 train_time:467ms step_avg:58.35ms
step:9/2270 train_time:528ms step_avg:58.62ms
step:10/2270 train_time:586ms step_avg:58.61ms
step:11/2270 train_time:647ms step_avg:58.81ms
step:12/2270 train_time:705ms step_avg:58.76ms
step:13/2270 train_time:767ms step_avg:58.96ms
step:14/2270 train_time:825ms step_avg:58.95ms
step:15/2270 train_time:886ms step_avg:59.10ms
step:16/2270 train_time:945ms step_avg:59.04ms
step:17/2270 train_time:1006ms step_avg:59.18ms
step:18/2270 train_time:1066ms step_avg:59.21ms
step:19/2270 train_time:1130ms step_avg:59.50ms
step:20/2270 train_time:1191ms step_avg:59.54ms
step:21/2270 train_time:1253ms step_avg:59.66ms
step:22/2270 train_time:1312ms step_avg:59.62ms
step:23/2270 train_time:1373ms step_avg:59.71ms
step:24/2270 train_time:1432ms step_avg:59.66ms
step:25/2270 train_time:1493ms step_avg:59.72ms
step:26/2270 train_time:1552ms step_avg:59.69ms
step:27/2270 train_time:1613ms step_avg:59.74ms
step:28/2270 train_time:1672ms step_avg:59.72ms
step:29/2270 train_time:1733ms step_avg:59.76ms
step:30/2270 train_time:1791ms step_avg:59.71ms
step:31/2270 train_time:1852ms step_avg:59.75ms
step:32/2270 train_time:1911ms step_avg:59.71ms
step:33/2270 train_time:1972ms step_avg:59.76ms
step:34/2270 train_time:2031ms step_avg:59.75ms
step:35/2270 train_time:2093ms step_avg:59.81ms
step:36/2270 train_time:2152ms step_avg:59.78ms
step:37/2270 train_time:2214ms step_avg:59.84ms
step:38/2270 train_time:2273ms step_avg:59.82ms
step:39/2270 train_time:2335ms step_avg:59.86ms
step:40/2270 train_time:2393ms step_avg:59.83ms
step:41/2270 train_time:2455ms step_avg:59.88ms
step:42/2270 train_time:2514ms step_avg:59.85ms
step:43/2270 train_time:2575ms step_avg:59.89ms
step:44/2270 train_time:2634ms step_avg:59.87ms
step:45/2270 train_time:2696ms step_avg:59.91ms
step:46/2270 train_time:2754ms step_avg:59.88ms
step:47/2270 train_time:2816ms step_avg:59.91ms
step:48/2270 train_time:2874ms step_avg:59.89ms
step:49/2270 train_time:2937ms step_avg:59.94ms
step:50/2270 train_time:2996ms step_avg:59.91ms
step:51/2270 train_time:3057ms step_avg:59.95ms
step:52/2270 train_time:3117ms step_avg:59.94ms
step:53/2270 train_time:3180ms step_avg:60.00ms
step:54/2270 train_time:3239ms step_avg:59.99ms
step:55/2270 train_time:3301ms step_avg:60.02ms
step:56/2270 train_time:3360ms step_avg:60.01ms
step:57/2270 train_time:3422ms step_avg:60.04ms
step:58/2270 train_time:3482ms step_avg:60.04ms
step:59/2270 train_time:3544ms step_avg:60.07ms
step:60/2270 train_time:3603ms step_avg:60.05ms
step:61/2270 train_time:3665ms step_avg:60.08ms
step:62/2270 train_time:3723ms step_avg:60.06ms
step:63/2270 train_time:3785ms step_avg:60.08ms
step:64/2270 train_time:3844ms step_avg:60.06ms
step:65/2270 train_time:3905ms step_avg:60.08ms
step:66/2270 train_time:3964ms step_avg:60.06ms
step:67/2270 train_time:4026ms step_avg:60.08ms
step:68/2270 train_time:4085ms step_avg:60.07ms
step:69/2270 train_time:4147ms step_avg:60.10ms
step:70/2270 train_time:4205ms step_avg:60.07ms
step:71/2270 train_time:4266ms step_avg:60.09ms
step:72/2270 train_time:4325ms step_avg:60.06ms
step:73/2270 train_time:4386ms step_avg:60.09ms
step:74/2270 train_time:4445ms step_avg:60.07ms
step:75/2270 train_time:4507ms step_avg:60.09ms
step:76/2270 train_time:4565ms step_avg:60.07ms
step:77/2270 train_time:4627ms step_avg:60.09ms
step:78/2270 train_time:4685ms step_avg:60.07ms
step:79/2270 train_time:4746ms step_avg:60.08ms
step:80/2270 train_time:4804ms step_avg:60.06ms
step:81/2270 train_time:4866ms step_avg:60.07ms
step:82/2270 train_time:4924ms step_avg:60.05ms
step:83/2270 train_time:4986ms step_avg:60.07ms
step:84/2270 train_time:5044ms step_avg:60.05ms
step:85/2270 train_time:5106ms step_avg:60.07ms
step:86/2270 train_time:5165ms step_avg:60.06ms
step:87/2270 train_time:5226ms step_avg:60.07ms
step:88/2270 train_time:5284ms step_avg:60.05ms
step:89/2270 train_time:5346ms step_avg:60.06ms
step:90/2270 train_time:5404ms step_avg:60.05ms
step:91/2270 train_time:5466ms step_avg:60.07ms
step:92/2270 train_time:5525ms step_avg:60.05ms
step:93/2270 train_time:5586ms step_avg:60.07ms
step:94/2270 train_time:5645ms step_avg:60.05ms
step:95/2270 train_time:5706ms step_avg:60.07ms
step:96/2270 train_time:5764ms step_avg:60.05ms
step:97/2270 train_time:5826ms step_avg:60.06ms
step:98/2270 train_time:5885ms step_avg:60.05ms
step:99/2270 train_time:5945ms step_avg:60.05ms
step:100/2270 train_time:6004ms step_avg:60.04ms
step:101/2270 train_time:6066ms step_avg:60.06ms
step:102/2270 train_time:6124ms step_avg:60.04ms
step:103/2270 train_time:6186ms step_avg:60.05ms
step:104/2270 train_time:6244ms step_avg:60.04ms
step:105/2270 train_time:6306ms step_avg:60.05ms
step:106/2270 train_time:6364ms step_avg:60.04ms
step:107/2270 train_time:6426ms step_avg:60.06ms
step:108/2270 train_time:6485ms step_avg:60.05ms
step:109/2270 train_time:6546ms step_avg:60.06ms
step:110/2270 train_time:6605ms step_avg:60.04ms
step:111/2270 train_time:6667ms step_avg:60.06ms
step:112/2270 train_time:6725ms step_avg:60.04ms
step:113/2270 train_time:6787ms step_avg:60.06ms
step:114/2270 train_time:6845ms step_avg:60.04ms
step:115/2270 train_time:6905ms step_avg:60.05ms
step:116/2270 train_time:6964ms step_avg:60.03ms
step:117/2270 train_time:7026ms step_avg:60.05ms
step:118/2270 train_time:7084ms step_avg:60.04ms
step:119/2270 train_time:7146ms step_avg:60.05ms
step:120/2270 train_time:7204ms step_avg:60.03ms
step:121/2270 train_time:7266ms step_avg:60.05ms
step:122/2270 train_time:7324ms step_avg:60.03ms
step:123/2270 train_time:7385ms step_avg:60.04ms
step:124/2270 train_time:7444ms step_avg:60.03ms
step:125/2270 train_time:7505ms step_avg:60.04ms
step:126/2270 train_time:7564ms step_avg:60.03ms
step:127/2270 train_time:7626ms step_avg:60.04ms
step:128/2270 train_time:7684ms step_avg:60.03ms
step:129/2270 train_time:7746ms step_avg:60.04ms
step:130/2270 train_time:7804ms step_avg:60.03ms
step:131/2270 train_time:7865ms step_avg:60.04ms
step:132/2270 train_time:7923ms step_avg:60.02ms
step:133/2270 train_time:7985ms step_avg:60.04ms
step:134/2270 train_time:8043ms step_avg:60.03ms
step:135/2270 train_time:8105ms step_avg:60.04ms
step:136/2270 train_time:8164ms step_avg:60.03ms
step:137/2270 train_time:8225ms step_avg:60.04ms
step:138/2270 train_time:8284ms step_avg:60.03ms
step:139/2270 train_time:8345ms step_avg:60.04ms
step:140/2270 train_time:8403ms step_avg:60.02ms
step:141/2270 train_time:8465ms step_avg:60.03ms
step:142/2270 train_time:8523ms step_avg:60.02ms
step:143/2270 train_time:8585ms step_avg:60.03ms
step:144/2270 train_time:8643ms step_avg:60.02ms
step:145/2270 train_time:8705ms step_avg:60.03ms
step:146/2270 train_time:8764ms step_avg:60.03ms
step:147/2270 train_time:8825ms step_avg:60.04ms
step:148/2270 train_time:8884ms step_avg:60.03ms
step:149/2270 train_time:8945ms step_avg:60.03ms
step:150/2270 train_time:9004ms step_avg:60.02ms
step:151/2270 train_time:9066ms step_avg:60.04ms
step:152/2270 train_time:9124ms step_avg:60.03ms
step:153/2270 train_time:9186ms step_avg:60.04ms
step:154/2270 train_time:9244ms step_avg:60.02ms
step:155/2270 train_time:9305ms step_avg:60.03ms
step:156/2270 train_time:9364ms step_avg:60.03ms
step:157/2270 train_time:9425ms step_avg:60.03ms
step:158/2270 train_time:9484ms step_avg:60.03ms
step:159/2270 train_time:9545ms step_avg:60.03ms
step:160/2270 train_time:9604ms step_avg:60.02ms
step:161/2270 train_time:9665ms step_avg:60.03ms
step:162/2270 train_time:9724ms step_avg:60.02ms
step:163/2270 train_time:9785ms step_avg:60.03ms
step:164/2270 train_time:9844ms step_avg:60.02ms
step:165/2270 train_time:9905ms step_avg:60.03ms
step:166/2270 train_time:9964ms step_avg:60.02ms
step:167/2270 train_time:10026ms step_avg:60.03ms
step:168/2270 train_time:10084ms step_avg:60.03ms
step:169/2270 train_time:10145ms step_avg:60.03ms
step:170/2270 train_time:10203ms step_avg:60.02ms
step:171/2270 train_time:10265ms step_avg:60.03ms
step:172/2270 train_time:10323ms step_avg:60.02ms
step:173/2270 train_time:10385ms step_avg:60.03ms
step:174/2270 train_time:10443ms step_avg:60.02ms
step:175/2270 train_time:10505ms step_avg:60.03ms
step:176/2270 train_time:10563ms step_avg:60.02ms
step:177/2270 train_time:10625ms step_avg:60.03ms
step:178/2270 train_time:10683ms step_avg:60.02ms
step:179/2270 train_time:10744ms step_avg:60.02ms
step:180/2270 train_time:10803ms step_avg:60.02ms
step:181/2270 train_time:10866ms step_avg:60.03ms
step:182/2270 train_time:10924ms step_avg:60.02ms
step:183/2270 train_time:10985ms step_avg:60.03ms
step:184/2270 train_time:11044ms step_avg:60.02ms
step:185/2270 train_time:11105ms step_avg:60.03ms
step:186/2270 train_time:11163ms step_avg:60.02ms
step:187/2270 train_time:11225ms step_avg:60.03ms
step:188/2270 train_time:11283ms step_avg:60.02ms
step:189/2270 train_time:11344ms step_avg:60.02ms
step:190/2270 train_time:11403ms step_avg:60.02ms
step:191/2270 train_time:11464ms step_avg:60.02ms
step:192/2270 train_time:11523ms step_avg:60.02ms
step:193/2270 train_time:11584ms step_avg:60.02ms
step:194/2270 train_time:11643ms step_avg:60.01ms
step:195/2270 train_time:11704ms step_avg:60.02ms
step:196/2270 train_time:11762ms step_avg:60.01ms
step:197/2270 train_time:11824ms step_avg:60.02ms
step:198/2270 train_time:11883ms step_avg:60.02ms
step:199/2270 train_time:11945ms step_avg:60.02ms
step:200/2270 train_time:12003ms step_avg:60.02ms
step:201/2270 train_time:12065ms step_avg:60.03ms
step:202/2270 train_time:12124ms step_avg:60.02ms
step:203/2270 train_time:12186ms step_avg:60.03ms
step:204/2270 train_time:12244ms step_avg:60.02ms
step:205/2270 train_time:12305ms step_avg:60.02ms
step:206/2270 train_time:12364ms step_avg:60.02ms
step:207/2270 train_time:12425ms step_avg:60.02ms
step:208/2270 train_time:12483ms step_avg:60.02ms
step:209/2270 train_time:12544ms step_avg:60.02ms
step:210/2270 train_time:12602ms step_avg:60.01ms
step:211/2270 train_time:12665ms step_avg:60.02ms
step:212/2270 train_time:12723ms step_avg:60.02ms
step:213/2270 train_time:12785ms step_avg:60.02ms
step:214/2270 train_time:12844ms step_avg:60.02ms
step:215/2270 train_time:12904ms step_avg:60.02ms
step:216/2270 train_time:12963ms step_avg:60.01ms
step:217/2270 train_time:13026ms step_avg:60.03ms
step:218/2270 train_time:13083ms step_avg:60.02ms
step:219/2270 train_time:13145ms step_avg:60.02ms
step:220/2270 train_time:13203ms step_avg:60.02ms
step:221/2270 train_time:13265ms step_avg:60.02ms
step:222/2270 train_time:13324ms step_avg:60.02ms
step:223/2270 train_time:13385ms step_avg:60.02ms
step:224/2270 train_time:13443ms step_avg:60.01ms
step:225/2270 train_time:13504ms step_avg:60.02ms
step:226/2270 train_time:13563ms step_avg:60.01ms
step:227/2270 train_time:13624ms step_avg:60.02ms
step:228/2270 train_time:13683ms step_avg:60.01ms
step:229/2270 train_time:13744ms step_avg:60.02ms
step:230/2270 train_time:13802ms step_avg:60.01ms
step:231/2270 train_time:13865ms step_avg:60.02ms
step:232/2270 train_time:13924ms step_avg:60.02ms
step:233/2270 train_time:13986ms step_avg:60.02ms
step:234/2270 train_time:14044ms step_avg:60.02ms
step:235/2270 train_time:14105ms step_avg:60.02ms
step:236/2270 train_time:14164ms step_avg:60.02ms
step:237/2270 train_time:14226ms step_avg:60.02ms
step:238/2270 train_time:14284ms step_avg:60.02ms
step:239/2270 train_time:14346ms step_avg:60.02ms
step:240/2270 train_time:14404ms step_avg:60.02ms
step:241/2270 train_time:14466ms step_avg:60.02ms
step:242/2270 train_time:14524ms step_avg:60.02ms
step:243/2270 train_time:14585ms step_avg:60.02ms
step:244/2270 train_time:14644ms step_avg:60.02ms
step:245/2270 train_time:14705ms step_avg:60.02ms
step:246/2270 train_time:14764ms step_avg:60.02ms
step:247/2270 train_time:14826ms step_avg:60.02ms
step:248/2270 train_time:14884ms step_avg:60.02ms
step:249/2270 train_time:14946ms step_avg:60.02ms
step:250/2270 train_time:15004ms step_avg:60.01ms
step:250/2270 val_loss:4.0724 train_time:15066ms step_avg:60.27ms
step:251/2270 train_time:15085ms step_avg:60.10ms
step:252/2270 train_time:15128ms step_avg:60.03ms
step:253/2270 train_time:15198ms step_avg:60.07ms
step:254/2270 train_time:15261ms step_avg:60.08ms
step:255/2270 train_time:15323ms step_avg:60.09ms
step:256/2270 train_time:15382ms step_avg:60.09ms
step:257/2270 train_time:15443ms step_avg:60.09ms
step:258/2270 train_time:15501ms step_avg:60.08ms
step:259/2270 train_time:15561ms step_avg:60.08ms
step:260/2270 train_time:15619ms step_avg:60.07ms
step:261/2270 train_time:15679ms step_avg:60.07ms
step:262/2270 train_time:15737ms step_avg:60.07ms
step:263/2270 train_time:15797ms step_avg:60.07ms
step:264/2270 train_time:15855ms step_avg:60.06ms
step:265/2270 train_time:15915ms step_avg:60.05ms
step:266/2270 train_time:15972ms step_avg:60.05ms
step:267/2270 train_time:16032ms step_avg:60.05ms
step:268/2270 train_time:16091ms step_avg:60.04ms
step:269/2270 train_time:16155ms step_avg:60.06ms
step:270/2270 train_time:16215ms step_avg:60.05ms
step:271/2270 train_time:16276ms step_avg:60.06ms
step:272/2270 train_time:16335ms step_avg:60.05ms
step:273/2270 train_time:16396ms step_avg:60.06ms
step:274/2270 train_time:16454ms step_avg:60.05ms
step:275/2270 train_time:16514ms step_avg:60.05ms
step:276/2270 train_time:16572ms step_avg:60.04ms
step:277/2270 train_time:16633ms step_avg:60.05ms
step:278/2270 train_time:16690ms step_avg:60.04ms
step:279/2270 train_time:16751ms step_avg:60.04ms
step:280/2270 train_time:16809ms step_avg:60.03ms
step:281/2270 train_time:16870ms step_avg:60.03ms
step:282/2270 train_time:16928ms step_avg:60.03ms
step:283/2270 train_time:16989ms step_avg:60.03ms
step:284/2270 train_time:17049ms step_avg:60.03ms
step:285/2270 train_time:17111ms step_avg:60.04ms
step:286/2270 train_time:17171ms step_avg:60.04ms
step:287/2270 train_time:17233ms step_avg:60.04ms
step:288/2270 train_time:17291ms step_avg:60.04ms
step:289/2270 train_time:17353ms step_avg:60.04ms
step:290/2270 train_time:17411ms step_avg:60.04ms
step:291/2270 train_time:17472ms step_avg:60.04ms
step:292/2270 train_time:17530ms step_avg:60.04ms
step:293/2270 train_time:17591ms step_avg:60.04ms
step:294/2270 train_time:17649ms step_avg:60.03ms
step:295/2270 train_time:17710ms step_avg:60.04ms
step:296/2270 train_time:17769ms step_avg:60.03ms
step:297/2270 train_time:17830ms step_avg:60.03ms
step:298/2270 train_time:17888ms step_avg:60.03ms
step:299/2270 train_time:17949ms step_avg:60.03ms
step:300/2270 train_time:18008ms step_avg:60.03ms
step:301/2270 train_time:18069ms step_avg:60.03ms
step:302/2270 train_time:18129ms step_avg:60.03ms
step:303/2270 train_time:18191ms step_avg:60.04ms
step:304/2270 train_time:18250ms step_avg:60.03ms
step:305/2270 train_time:18313ms step_avg:60.04ms
step:306/2270 train_time:18371ms step_avg:60.04ms
step:307/2270 train_time:18432ms step_avg:60.04ms
step:308/2270 train_time:18491ms step_avg:60.03ms
step:309/2270 train_time:18551ms step_avg:60.04ms
step:310/2270 train_time:18610ms step_avg:60.03ms
step:311/2270 train_time:18672ms step_avg:60.04ms
step:312/2270 train_time:18730ms step_avg:60.03ms
step:313/2270 train_time:18791ms step_avg:60.03ms
step:314/2270 train_time:18849ms step_avg:60.03ms
step:315/2270 train_time:18910ms step_avg:60.03ms
step:316/2270 train_time:18969ms step_avg:60.03ms
step:317/2270 train_time:19031ms step_avg:60.04ms
step:318/2270 train_time:19090ms step_avg:60.03ms
step:319/2270 train_time:19151ms step_avg:60.03ms
step:320/2270 train_time:19210ms step_avg:60.03ms
step:321/2270 train_time:19273ms step_avg:60.04ms
step:322/2270 train_time:19331ms step_avg:60.03ms
step:323/2270 train_time:19393ms step_avg:60.04ms
step:324/2270 train_time:19451ms step_avg:60.03ms
step:325/2270 train_time:19512ms step_avg:60.04ms
step:326/2270 train_time:19570ms step_avg:60.03ms
step:327/2270 train_time:19631ms step_avg:60.04ms
step:328/2270 train_time:19690ms step_avg:60.03ms
step:329/2270 train_time:19751ms step_avg:60.03ms
step:330/2270 train_time:19809ms step_avg:60.03ms
step:331/2270 train_time:19870ms step_avg:60.03ms
step:332/2270 train_time:19928ms step_avg:60.02ms
step:333/2270 train_time:19989ms step_avg:60.03ms
step:334/2270 train_time:20049ms step_avg:60.03ms
step:335/2270 train_time:20111ms step_avg:60.03ms
step:336/2270 train_time:20170ms step_avg:60.03ms
step:337/2270 train_time:20231ms step_avg:60.03ms
step:338/2270 train_time:20290ms step_avg:60.03ms
step:339/2270 train_time:20352ms step_avg:60.03ms
step:340/2270 train_time:20410ms step_avg:60.03ms
step:341/2270 train_time:20472ms step_avg:60.04ms
step:342/2270 train_time:20530ms step_avg:60.03ms
step:343/2270 train_time:20592ms step_avg:60.03ms
step:344/2270 train_time:20650ms step_avg:60.03ms
step:345/2270 train_time:20711ms step_avg:60.03ms
step:346/2270 train_time:20770ms step_avg:60.03ms
step:347/2270 train_time:20831ms step_avg:60.03ms
step:348/2270 train_time:20889ms step_avg:60.03ms
step:349/2270 train_time:20950ms step_avg:60.03ms
step:350/2270 train_time:21009ms step_avg:60.03ms
step:351/2270 train_time:21071ms step_avg:60.03ms
step:352/2270 train_time:21129ms step_avg:60.03ms
step:353/2270 train_time:21191ms step_avg:60.03ms
step:354/2270 train_time:21249ms step_avg:60.03ms
step:355/2270 train_time:21311ms step_avg:60.03ms
step:356/2270 train_time:21370ms step_avg:60.03ms
step:357/2270 train_time:21432ms step_avg:60.03ms
step:358/2270 train_time:21490ms step_avg:60.03ms
step:359/2270 train_time:21551ms step_avg:60.03ms
step:360/2270 train_time:21610ms step_avg:60.03ms
step:361/2270 train_time:21671ms step_avg:60.03ms
step:362/2270 train_time:21729ms step_avg:60.02ms
step:363/2270 train_time:21790ms step_avg:60.03ms
step:364/2270 train_time:21848ms step_avg:60.02ms
step:365/2270 train_time:21909ms step_avg:60.03ms
step:366/2270 train_time:21968ms step_avg:60.02ms
step:367/2270 train_time:22030ms step_avg:60.03ms
step:368/2270 train_time:22088ms step_avg:60.02ms
step:369/2270 train_time:22151ms step_avg:60.03ms
step:370/2270 train_time:22210ms step_avg:60.03ms
step:371/2270 train_time:22272ms step_avg:60.03ms
step:372/2270 train_time:22331ms step_avg:60.03ms
step:373/2270 train_time:22392ms step_avg:60.03ms
step:374/2270 train_time:22451ms step_avg:60.03ms
step:375/2270 train_time:22512ms step_avg:60.03ms
step:376/2270 train_time:22571ms step_avg:60.03ms
step:377/2270 train_time:22632ms step_avg:60.03ms
step:378/2270 train_time:22690ms step_avg:60.03ms
step:379/2270 train_time:22752ms step_avg:60.03ms
step:380/2270 train_time:22811ms step_avg:60.03ms
step:381/2270 train_time:22872ms step_avg:60.03ms
step:382/2270 train_time:22931ms step_avg:60.03ms
step:383/2270 train_time:22992ms step_avg:60.03ms
step:384/2270 train_time:23050ms step_avg:60.03ms
step:385/2270 train_time:23112ms step_avg:60.03ms
step:386/2270 train_time:23171ms step_avg:60.03ms
step:387/2270 train_time:23233ms step_avg:60.03ms
step:388/2270 train_time:23291ms step_avg:60.03ms
step:389/2270 train_time:23353ms step_avg:60.03ms
step:390/2270 train_time:23412ms step_avg:60.03ms
step:391/2270 train_time:23473ms step_avg:60.03ms
step:392/2270 train_time:23532ms step_avg:60.03ms
step:393/2270 train_time:23593ms step_avg:60.03ms
step:394/2270 train_time:23651ms step_avg:60.03ms
step:395/2270 train_time:23713ms step_avg:60.03ms
step:396/2270 train_time:23771ms step_avg:60.03ms
step:397/2270 train_time:23832ms step_avg:60.03ms
step:398/2270 train_time:23891ms step_avg:60.03ms
step:399/2270 train_time:23952ms step_avg:60.03ms
step:400/2270 train_time:24011ms step_avg:60.03ms
step:401/2270 train_time:24072ms step_avg:60.03ms
step:402/2270 train_time:24131ms step_avg:60.03ms
step:403/2270 train_time:24192ms step_avg:60.03ms
step:404/2270 train_time:24251ms step_avg:60.03ms
step:405/2270 train_time:24313ms step_avg:60.03ms
step:406/2270 train_time:24372ms step_avg:60.03ms
step:407/2270 train_time:24433ms step_avg:60.03ms
step:408/2270 train_time:24492ms step_avg:60.03ms
step:409/2270 train_time:24553ms step_avg:60.03ms
step:410/2270 train_time:24611ms step_avg:60.03ms
step:411/2270 train_time:24673ms step_avg:60.03ms
step:412/2270 train_time:24731ms step_avg:60.03ms
step:413/2270 train_time:24793ms step_avg:60.03ms
step:414/2270 train_time:24851ms step_avg:60.03ms
step:415/2270 train_time:24912ms step_avg:60.03ms
step:416/2270 train_time:24970ms step_avg:60.03ms
step:417/2270 train_time:25032ms step_avg:60.03ms
step:418/2270 train_time:25091ms step_avg:60.03ms
step:419/2270 train_time:25153ms step_avg:60.03ms
step:420/2270 train_time:25211ms step_avg:60.03ms
step:421/2270 train_time:25273ms step_avg:60.03ms
step:422/2270 train_time:25331ms step_avg:60.03ms
step:423/2270 train_time:25393ms step_avg:60.03ms
step:424/2270 train_time:25452ms step_avg:60.03ms
step:425/2270 train_time:25514ms step_avg:60.03ms
step:426/2270 train_time:25572ms step_avg:60.03ms
step:427/2270 train_time:25633ms step_avg:60.03ms
step:428/2270 train_time:25692ms step_avg:60.03ms
step:429/2270 train_time:25753ms step_avg:60.03ms
step:430/2270 train_time:25811ms step_avg:60.03ms
step:431/2270 train_time:25873ms step_avg:60.03ms
step:432/2270 train_time:25931ms step_avg:60.03ms
step:433/2270 train_time:25993ms step_avg:60.03ms
step:434/2270 train_time:26051ms step_avg:60.03ms
step:435/2270 train_time:26113ms step_avg:60.03ms
step:436/2270 train_time:26172ms step_avg:60.03ms
step:437/2270 train_time:26233ms step_avg:60.03ms
step:438/2270 train_time:26292ms step_avg:60.03ms
step:439/2270 train_time:26353ms step_avg:60.03ms
step:440/2270 train_time:26412ms step_avg:60.03ms
step:441/2270 train_time:26474ms step_avg:60.03ms
step:442/2270 train_time:26532ms step_avg:60.03ms
step:443/2270 train_time:26593ms step_avg:60.03ms
step:444/2270 train_time:26652ms step_avg:60.03ms
step:445/2270 train_time:26713ms step_avg:60.03ms
step:446/2270 train_time:26771ms step_avg:60.03ms
step:447/2270 train_time:26833ms step_avg:60.03ms
step:448/2270 train_time:26891ms step_avg:60.02ms
step:449/2270 train_time:26952ms step_avg:60.03ms
step:450/2270 train_time:27011ms step_avg:60.02ms
step:451/2270 train_time:27073ms step_avg:60.03ms
step:452/2270 train_time:27132ms step_avg:60.03ms
step:453/2270 train_time:27194ms step_avg:60.03ms
step:454/2270 train_time:27252ms step_avg:60.03ms
step:455/2270 train_time:27313ms step_avg:60.03ms
step:456/2270 train_time:27372ms step_avg:60.03ms
step:457/2270 train_time:27433ms step_avg:60.03ms
step:458/2270 train_time:27492ms step_avg:60.03ms
step:459/2270 train_time:27554ms step_avg:60.03ms
step:460/2270 train_time:27612ms step_avg:60.03ms
step:461/2270 train_time:27673ms step_avg:60.03ms
step:462/2270 train_time:27732ms step_avg:60.03ms
step:463/2270 train_time:27793ms step_avg:60.03ms
step:464/2270 train_time:27851ms step_avg:60.02ms
step:465/2270 train_time:27912ms step_avg:60.03ms
step:466/2270 train_time:27971ms step_avg:60.02ms
step:467/2270 train_time:28032ms step_avg:60.03ms
step:468/2270 train_time:28091ms step_avg:60.02ms
step:469/2270 train_time:28153ms step_avg:60.03ms
step:470/2270 train_time:28212ms step_avg:60.02ms
step:471/2270 train_time:28273ms step_avg:60.03ms
step:472/2270 train_time:28331ms step_avg:60.02ms
step:473/2270 train_time:28393ms step_avg:60.03ms
step:474/2270 train_time:28451ms step_avg:60.02ms
step:475/2270 train_time:28514ms step_avg:60.03ms
step:476/2270 train_time:28572ms step_avg:60.03ms
step:477/2270 train_time:28633ms step_avg:60.03ms
step:478/2270 train_time:28691ms step_avg:60.02ms
step:479/2270 train_time:28753ms step_avg:60.03ms
step:480/2270 train_time:28811ms step_avg:60.02ms
step:481/2270 train_time:28873ms step_avg:60.03ms
step:482/2270 train_time:28931ms step_avg:60.02ms
step:483/2270 train_time:28993ms step_avg:60.03ms
step:484/2270 train_time:29051ms step_avg:60.02ms
step:485/2270 train_time:29113ms step_avg:60.03ms
step:486/2270 train_time:29171ms step_avg:60.02ms
step:487/2270 train_time:29233ms step_avg:60.03ms
step:488/2270 train_time:29291ms step_avg:60.02ms
step:489/2270 train_time:29353ms step_avg:60.03ms
step:490/2270 train_time:29411ms step_avg:60.02ms
step:491/2270 train_time:29473ms step_avg:60.03ms
step:492/2270 train_time:29532ms step_avg:60.02ms
step:493/2270 train_time:29593ms step_avg:60.03ms
step:494/2270 train_time:29651ms step_avg:60.02ms
step:495/2270 train_time:29713ms step_avg:60.03ms
step:496/2270 train_time:29772ms step_avg:60.02ms
step:497/2270 train_time:29833ms step_avg:60.03ms
step:498/2270 train_time:29892ms step_avg:60.02ms
step:499/2270 train_time:29952ms step_avg:60.02ms
step:500/2270 train_time:30011ms step_avg:60.02ms
step:500/2270 val_loss:3.7886 train_time:30073ms step_avg:60.15ms
step:501/2270 train_time:30093ms step_avg:60.06ms
step:502/2270 train_time:30135ms step_avg:60.03ms
step:503/2270 train_time:30196ms step_avg:60.03ms
step:504/2270 train_time:30255ms step_avg:60.03ms
step:505/2270 train_time:30318ms step_avg:60.04ms
step:506/2270 train_time:30378ms step_avg:60.04ms
step:507/2270 train_time:30439ms step_avg:60.04ms
step:508/2270 train_time:30498ms step_avg:60.03ms
step:509/2270 train_time:30559ms step_avg:60.04ms
step:510/2270 train_time:30618ms step_avg:60.04ms
step:511/2270 train_time:30679ms step_avg:60.04ms
step:512/2270 train_time:30738ms step_avg:60.03ms
step:513/2270 train_time:30799ms step_avg:60.04ms
step:514/2270 train_time:30857ms step_avg:60.03ms
step:515/2270 train_time:30918ms step_avg:60.03ms
step:516/2270 train_time:30982ms step_avg:60.04ms
step:517/2270 train_time:31049ms step_avg:60.06ms
step:518/2270 train_time:31110ms step_avg:60.06ms
step:519/2270 train_time:31171ms step_avg:60.06ms
step:520/2270 train_time:31230ms step_avg:60.06ms
step:521/2270 train_time:31291ms step_avg:60.06ms
step:522/2270 train_time:31349ms step_avg:60.06ms
step:523/2270 train_time:31410ms step_avg:60.06ms
step:524/2270 train_time:31468ms step_avg:60.05ms
step:525/2270 train_time:31530ms step_avg:60.06ms
step:526/2270 train_time:31588ms step_avg:60.05ms
step:527/2270 train_time:31648ms step_avg:60.05ms
step:528/2270 train_time:31706ms step_avg:60.05ms
step:529/2270 train_time:31767ms step_avg:60.05ms
step:530/2270 train_time:31825ms step_avg:60.05ms
step:531/2270 train_time:31885ms step_avg:60.05ms
step:532/2270 train_time:31944ms step_avg:60.05ms
step:533/2270 train_time:32006ms step_avg:60.05ms
step:534/2270 train_time:32065ms step_avg:60.05ms
step:535/2270 train_time:32127ms step_avg:60.05ms
step:536/2270 train_time:32186ms step_avg:60.05ms
step:537/2270 train_time:32247ms step_avg:60.05ms
step:538/2270 train_time:32306ms step_avg:60.05ms
step:539/2270 train_time:32368ms step_avg:60.05ms
step:540/2270 train_time:32426ms step_avg:60.05ms
step:541/2270 train_time:32487ms step_avg:60.05ms
step:542/2270 train_time:32546ms step_avg:60.05ms
step:543/2270 train_time:32606ms step_avg:60.05ms
step:544/2270 train_time:32664ms step_avg:60.04ms
step:545/2270 train_time:32725ms step_avg:60.05ms
step:546/2270 train_time:32783ms step_avg:60.04ms
step:547/2270 train_time:32844ms step_avg:60.04ms
step:548/2270 train_time:32902ms step_avg:60.04ms
step:549/2270 train_time:32964ms step_avg:60.04ms
step:550/2270 train_time:33023ms step_avg:60.04ms
step:551/2270 train_time:33085ms step_avg:60.04ms
step:552/2270 train_time:33144ms step_avg:60.04ms
step:553/2270 train_time:33206ms step_avg:60.05ms
step:554/2270 train_time:33265ms step_avg:60.04ms
step:555/2270 train_time:33326ms step_avg:60.05ms
step:556/2270 train_time:33385ms step_avg:60.04ms
step:557/2270 train_time:33446ms step_avg:60.05ms
step:558/2270 train_time:33504ms step_avg:60.04ms
step:559/2270 train_time:33565ms step_avg:60.05ms
step:560/2270 train_time:33624ms step_avg:60.04ms
step:561/2270 train_time:33685ms step_avg:60.04ms
step:562/2270 train_time:33743ms step_avg:60.04ms
step:563/2270 train_time:33805ms step_avg:60.04ms
step:564/2270 train_time:33863ms step_avg:60.04ms
step:565/2270 train_time:33925ms step_avg:60.04ms
step:566/2270 train_time:33983ms step_avg:60.04ms
step:567/2270 train_time:34045ms step_avg:60.04ms
step:568/2270 train_time:34104ms step_avg:60.04ms
step:569/2270 train_time:34165ms step_avg:60.04ms
step:570/2270 train_time:34224ms step_avg:60.04ms
step:571/2270 train_time:34286ms step_avg:60.04ms
step:572/2270 train_time:34345ms step_avg:60.04ms
step:573/2270 train_time:34407ms step_avg:60.05ms
step:574/2270 train_time:34465ms step_avg:60.04ms
step:575/2270 train_time:34526ms step_avg:60.05ms
step:576/2270 train_time:34585ms step_avg:60.04ms
step:577/2270 train_time:34646ms step_avg:60.04ms
step:578/2270 train_time:34704ms step_avg:60.04ms
step:579/2270 train_time:34765ms step_avg:60.04ms
step:580/2270 train_time:34823ms step_avg:60.04ms
step:581/2270 train_time:34885ms step_avg:60.04ms
step:582/2270 train_time:34944ms step_avg:60.04ms
step:583/2270 train_time:35006ms step_avg:60.04ms
step:584/2270 train_time:35064ms step_avg:60.04ms
step:585/2270 train_time:35125ms step_avg:60.04ms
step:586/2270 train_time:35184ms step_avg:60.04ms
step:587/2270 train_time:35246ms step_avg:60.04ms
step:588/2270 train_time:35305ms step_avg:60.04ms
step:589/2270 train_time:35367ms step_avg:60.05ms
step:590/2270 train_time:35426ms step_avg:60.04ms
step:591/2270 train_time:35487ms step_avg:60.05ms
step:592/2270 train_time:35546ms step_avg:60.04ms
step:593/2270 train_time:35607ms step_avg:60.05ms
step:594/2270 train_time:35665ms step_avg:60.04ms
step:595/2270 train_time:35726ms step_avg:60.04ms
step:596/2270 train_time:35784ms step_avg:60.04ms
step:597/2270 train_time:35845ms step_avg:60.04ms
step:598/2270 train_time:35903ms step_avg:60.04ms
step:599/2270 train_time:35965ms step_avg:60.04ms
step:600/2270 train_time:36024ms step_avg:60.04ms
step:601/2270 train_time:36085ms step_avg:60.04ms
step:602/2270 train_time:36144ms step_avg:60.04ms
step:603/2270 train_time:36205ms step_avg:60.04ms
step:604/2270 train_time:36264ms step_avg:60.04ms
step:605/2270 train_time:36325ms step_avg:60.04ms
step:606/2270 train_time:36384ms step_avg:60.04ms
step:607/2270 train_time:36446ms step_avg:60.04ms
step:608/2270 train_time:36505ms step_avg:60.04ms
step:609/2270 train_time:36566ms step_avg:60.04ms
step:610/2270 train_time:36625ms step_avg:60.04ms
step:611/2270 train_time:36685ms step_avg:60.04ms
step:612/2270 train_time:36744ms step_avg:60.04ms
step:613/2270 train_time:36805ms step_avg:60.04ms
step:614/2270 train_time:36864ms step_avg:60.04ms
step:615/2270 train_time:36925ms step_avg:60.04ms
step:616/2270 train_time:36983ms step_avg:60.04ms
step:617/2270 train_time:37045ms step_avg:60.04ms
step:618/2270 train_time:37104ms step_avg:60.04ms
step:619/2270 train_time:37166ms step_avg:60.04ms
step:620/2270 train_time:37224ms step_avg:60.04ms
step:621/2270 train_time:37286ms step_avg:60.04ms
step:622/2270 train_time:37345ms step_avg:60.04ms
step:623/2270 train_time:37406ms step_avg:60.04ms
step:624/2270 train_time:37464ms step_avg:60.04ms
step:625/2270 train_time:37526ms step_avg:60.04ms
step:626/2270 train_time:37584ms step_avg:60.04ms
step:627/2270 train_time:37646ms step_avg:60.04ms
step:628/2270 train_time:37704ms step_avg:60.04ms
step:629/2270 train_time:37766ms step_avg:60.04ms
step:630/2270 train_time:37824ms step_avg:60.04ms
step:631/2270 train_time:37885ms step_avg:60.04ms
step:632/2270 train_time:37944ms step_avg:60.04ms
step:633/2270 train_time:38005ms step_avg:60.04ms
step:634/2270 train_time:38064ms step_avg:60.04ms
step:635/2270 train_time:38125ms step_avg:60.04ms
step:636/2270 train_time:38183ms step_avg:60.04ms
step:637/2270 train_time:38246ms step_avg:60.04ms
step:638/2270 train_time:38305ms step_avg:60.04ms
step:639/2270 train_time:38366ms step_avg:60.04ms
step:640/2270 train_time:38424ms step_avg:60.04ms
step:641/2270 train_time:38486ms step_avg:60.04ms
step:642/2270 train_time:38545ms step_avg:60.04ms
step:643/2270 train_time:38606ms step_avg:60.04ms
step:644/2270 train_time:38665ms step_avg:60.04ms
step:645/2270 train_time:38726ms step_avg:60.04ms
step:646/2270 train_time:38785ms step_avg:60.04ms
step:647/2270 train_time:38846ms step_avg:60.04ms
step:648/2270 train_time:38904ms step_avg:60.04ms
step:649/2270 train_time:38965ms step_avg:60.04ms
step:650/2270 train_time:39023ms step_avg:60.04ms
step:651/2270 train_time:39085ms step_avg:60.04ms
step:652/2270 train_time:39144ms step_avg:60.04ms
step:653/2270 train_time:39205ms step_avg:60.04ms
step:654/2270 train_time:39264ms step_avg:60.04ms
step:655/2270 train_time:39326ms step_avg:60.04ms
step:656/2270 train_time:39385ms step_avg:60.04ms
step:657/2270 train_time:39446ms step_avg:60.04ms
step:658/2270 train_time:39505ms step_avg:60.04ms
step:659/2270 train_time:39566ms step_avg:60.04ms
step:660/2270 train_time:39624ms step_avg:60.04ms
step:661/2270 train_time:39685ms step_avg:60.04ms
step:662/2270 train_time:39744ms step_avg:60.04ms
step:663/2270 train_time:39806ms step_avg:60.04ms
step:664/2270 train_time:39864ms step_avg:60.04ms
step:665/2270 train_time:39925ms step_avg:60.04ms
step:666/2270 train_time:39984ms step_avg:60.04ms
step:667/2270 train_time:40045ms step_avg:60.04ms
step:668/2270 train_time:40103ms step_avg:60.04ms
step:669/2270 train_time:40165ms step_avg:60.04ms
step:670/2270 train_time:40224ms step_avg:60.04ms
step:671/2270 train_time:40285ms step_avg:60.04ms
step:672/2270 train_time:40344ms step_avg:60.04ms
step:673/2270 train_time:40406ms step_avg:60.04ms
step:674/2270 train_time:40464ms step_avg:60.04ms
step:675/2270 train_time:40526ms step_avg:60.04ms
step:676/2270 train_time:40584ms step_avg:60.04ms
step:677/2270 train_time:40645ms step_avg:60.04ms
step:678/2270 train_time:40704ms step_avg:60.04ms
step:679/2270 train_time:40766ms step_avg:60.04ms
step:680/2270 train_time:40824ms step_avg:60.04ms
step:681/2270 train_time:40885ms step_avg:60.04ms
step:682/2270 train_time:40944ms step_avg:60.03ms
step:683/2270 train_time:41005ms step_avg:60.04ms
step:684/2270 train_time:41063ms step_avg:60.03ms
step:685/2270 train_time:41124ms step_avg:60.04ms
step:686/2270 train_time:41183ms step_avg:60.03ms
step:687/2270 train_time:41245ms step_avg:60.04ms
step:688/2270 train_time:41304ms step_avg:60.03ms
step:689/2270 train_time:41365ms step_avg:60.04ms
step:690/2270 train_time:41424ms step_avg:60.03ms
step:691/2270 train_time:41485ms step_avg:60.04ms
step:692/2270 train_time:41544ms step_avg:60.03ms
step:693/2270 train_time:41606ms step_avg:60.04ms
step:694/2270 train_time:41664ms step_avg:60.03ms
step:695/2270 train_time:41726ms step_avg:60.04ms
step:696/2270 train_time:41784ms step_avg:60.03ms
step:697/2270 train_time:41846ms step_avg:60.04ms
step:698/2270 train_time:41905ms step_avg:60.04ms
step:699/2270 train_time:41966ms step_avg:60.04ms
step:700/2270 train_time:42024ms step_avg:60.03ms
step:701/2270 train_time:42085ms step_avg:60.04ms
step:702/2270 train_time:42144ms step_avg:60.03ms
step:703/2270 train_time:42206ms step_avg:60.04ms
step:704/2270 train_time:42265ms step_avg:60.04ms
step:705/2270 train_time:42326ms step_avg:60.04ms
step:706/2270 train_time:42384ms step_avg:60.03ms
step:707/2270 train_time:42446ms step_avg:60.04ms
step:708/2270 train_time:42504ms step_avg:60.03ms
step:709/2270 train_time:42566ms step_avg:60.04ms
step:710/2270 train_time:42625ms step_avg:60.03ms
step:711/2270 train_time:42686ms step_avg:60.04ms
step:712/2270 train_time:42744ms step_avg:60.03ms
step:713/2270 train_time:42806ms step_avg:60.04ms
step:714/2270 train_time:42865ms step_avg:60.03ms
step:715/2270 train_time:42926ms step_avg:60.04ms
step:716/2270 train_time:42984ms step_avg:60.03ms
step:717/2270 train_time:43046ms step_avg:60.04ms
step:718/2270 train_time:43104ms step_avg:60.03ms
step:719/2270 train_time:43166ms step_avg:60.04ms
step:720/2270 train_time:43224ms step_avg:60.03ms
step:721/2270 train_time:43286ms step_avg:60.04ms
step:722/2270 train_time:43344ms step_avg:60.03ms
step:723/2270 train_time:43406ms step_avg:60.04ms
step:724/2270 train_time:43464ms step_avg:60.03ms
step:725/2270 train_time:43526ms step_avg:60.04ms
step:726/2270 train_time:43585ms step_avg:60.03ms
step:727/2270 train_time:43646ms step_avg:60.04ms
step:728/2270 train_time:43705ms step_avg:60.03ms
step:729/2270 train_time:43766ms step_avg:60.04ms
step:730/2270 train_time:43825ms step_avg:60.03ms
step:731/2270 train_time:43886ms step_avg:60.04ms
step:732/2270 train_time:43945ms step_avg:60.03ms
step:733/2270 train_time:44006ms step_avg:60.04ms
step:734/2270 train_time:44064ms step_avg:60.03ms
step:735/2270 train_time:44126ms step_avg:60.03ms
step:736/2270 train_time:44184ms step_avg:60.03ms
step:737/2270 train_time:44246ms step_avg:60.03ms
step:738/2270 train_time:44304ms step_avg:60.03ms
step:739/2270 train_time:44365ms step_avg:60.03ms
step:740/2270 train_time:44424ms step_avg:60.03ms
step:741/2270 train_time:44486ms step_avg:60.03ms
step:742/2270 train_time:44545ms step_avg:60.03ms
step:743/2270 train_time:44606ms step_avg:60.04ms
step:744/2270 train_time:44665ms step_avg:60.03ms
step:745/2270 train_time:44727ms step_avg:60.04ms
step:746/2270 train_time:44785ms step_avg:60.03ms
step:747/2270 train_time:44846ms step_avg:60.03ms
step:748/2270 train_time:44904ms step_avg:60.03ms
step:749/2270 train_time:44966ms step_avg:60.03ms
step:750/2270 train_time:45024ms step_avg:60.03ms
step:750/2270 val_loss:3.6606 train_time:45087ms step_avg:60.12ms
step:751/2270 train_time:45106ms step_avg:60.06ms
step:752/2270 train_time:45147ms step_avg:60.04ms
step:753/2270 train_time:45211ms step_avg:60.04ms
step:754/2270 train_time:45273ms step_avg:60.04ms
step:755/2270 train_time:45335ms step_avg:60.05ms
step:756/2270 train_time:45394ms step_avg:60.04ms
step:757/2270 train_time:45454ms step_avg:60.05ms
step:758/2270 train_time:45512ms step_avg:60.04ms
step:759/2270 train_time:45574ms step_avg:60.04ms
step:760/2270 train_time:45632ms step_avg:60.04ms
step:761/2270 train_time:45693ms step_avg:60.04ms
step:762/2270 train_time:45751ms step_avg:60.04ms
step:763/2270 train_time:45812ms step_avg:60.04ms
step:764/2270 train_time:45870ms step_avg:60.04ms
step:765/2270 train_time:45931ms step_avg:60.04ms
step:766/2270 train_time:45991ms step_avg:60.04ms
step:767/2270 train_time:46054ms step_avg:60.04ms
step:768/2270 train_time:46113ms step_avg:60.04ms
step:769/2270 train_time:46176ms step_avg:60.05ms
step:770/2270 train_time:46236ms step_avg:60.05ms
step:771/2270 train_time:46298ms step_avg:60.05ms
step:772/2270 train_time:46358ms step_avg:60.05ms
step:773/2270 train_time:46419ms step_avg:60.05ms
step:774/2270 train_time:46478ms step_avg:60.05ms
step:775/2270 train_time:46540ms step_avg:60.05ms
step:776/2270 train_time:46599ms step_avg:60.05ms
step:777/2270 train_time:46660ms step_avg:60.05ms
step:778/2270 train_time:46719ms step_avg:60.05ms
step:779/2270 train_time:46782ms step_avg:60.05ms
step:780/2270 train_time:46841ms step_avg:60.05ms
step:781/2270 train_time:46903ms step_avg:60.05ms
step:782/2270 train_time:46962ms step_avg:60.05ms
step:783/2270 train_time:47024ms step_avg:60.06ms
step:784/2270 train_time:47083ms step_avg:60.05ms
step:785/2270 train_time:47147ms step_avg:60.06ms
step:786/2270 train_time:47207ms step_avg:60.06ms
step:787/2270 train_time:47270ms step_avg:60.06ms
step:788/2270 train_time:47330ms step_avg:60.06ms
step:789/2270 train_time:47393ms step_avg:60.07ms
step:790/2270 train_time:47452ms step_avg:60.07ms
step:791/2270 train_time:47515ms step_avg:60.07ms
step:792/2270 train_time:47573ms step_avg:60.07ms
step:793/2270 train_time:47634ms step_avg:60.07ms
step:794/2270 train_time:47693ms step_avg:60.07ms
step:795/2270 train_time:47754ms step_avg:60.07ms
step:796/2270 train_time:47813ms step_avg:60.07ms
step:797/2270 train_time:47874ms step_avg:60.07ms
step:798/2270 train_time:47932ms step_avg:60.07ms
step:799/2270 train_time:47994ms step_avg:60.07ms
step:800/2270 train_time:48052ms step_avg:60.07ms
step:801/2270 train_time:48114ms step_avg:60.07ms
step:802/2270 train_time:48173ms step_avg:60.07ms
step:803/2270 train_time:48235ms step_avg:60.07ms
step:804/2270 train_time:48294ms step_avg:60.07ms
step:805/2270 train_time:48356ms step_avg:60.07ms
step:806/2270 train_time:48416ms step_avg:60.07ms
step:807/2270 train_time:48478ms step_avg:60.07ms
step:808/2270 train_time:48537ms step_avg:60.07ms
step:809/2270 train_time:48598ms step_avg:60.07ms
step:810/2270 train_time:48658ms step_avg:60.07ms
step:811/2270 train_time:48719ms step_avg:60.07ms
step:812/2270 train_time:48778ms step_avg:60.07ms
step:813/2270 train_time:48839ms step_avg:60.07ms
step:814/2270 train_time:48898ms step_avg:60.07ms
step:815/2270 train_time:48960ms step_avg:60.07ms
step:816/2270 train_time:49019ms step_avg:60.07ms
step:817/2270 train_time:49080ms step_avg:60.07ms
step:818/2270 train_time:49140ms step_avg:60.07ms
step:819/2270 train_time:49202ms step_avg:60.08ms
step:820/2270 train_time:49262ms step_avg:60.08ms
step:821/2270 train_time:49324ms step_avg:60.08ms
step:822/2270 train_time:49385ms step_avg:60.08ms
step:823/2270 train_time:49449ms step_avg:60.08ms
step:824/2270 train_time:49508ms step_avg:60.08ms
step:825/2270 train_time:49571ms step_avg:60.09ms
step:826/2270 train_time:49631ms step_avg:60.09ms
step:827/2270 train_time:49693ms step_avg:60.09ms
step:828/2270 train_time:49752ms step_avg:60.09ms
step:829/2270 train_time:49814ms step_avg:60.09ms
step:830/2270 train_time:49872ms step_avg:60.09ms
step:831/2270 train_time:49934ms step_avg:60.09ms
step:832/2270 train_time:49992ms step_avg:60.09ms
step:833/2270 train_time:50054ms step_avg:60.09ms
step:834/2270 train_time:50112ms step_avg:60.09ms
step:835/2270 train_time:50174ms step_avg:60.09ms
step:836/2270 train_time:50233ms step_avg:60.09ms
step:837/2270 train_time:50294ms step_avg:60.09ms
step:838/2270 train_time:50353ms step_avg:60.09ms
step:839/2270 train_time:50414ms step_avg:60.09ms
step:840/2270 train_time:50473ms step_avg:60.09ms
step:841/2270 train_time:50536ms step_avg:60.09ms
step:842/2270 train_time:50595ms step_avg:60.09ms
step:843/2270 train_time:50656ms step_avg:60.09ms
step:844/2270 train_time:50715ms step_avg:60.09ms
step:845/2270 train_time:50777ms step_avg:60.09ms
step:846/2270 train_time:50836ms step_avg:60.09ms
step:847/2270 train_time:50898ms step_avg:60.09ms
step:848/2270 train_time:50956ms step_avg:60.09ms
step:849/2270 train_time:51018ms step_avg:60.09ms
step:850/2270 train_time:51077ms step_avg:60.09ms
step:851/2270 train_time:51138ms step_avg:60.09ms
step:852/2270 train_time:51197ms step_avg:60.09ms
step:853/2270 train_time:51259ms step_avg:60.09ms
step:854/2270 train_time:51318ms step_avg:60.09ms
step:855/2270 train_time:51379ms step_avg:60.09ms
step:856/2270 train_time:51438ms step_avg:60.09ms
step:857/2270 train_time:51501ms step_avg:60.09ms
step:858/2270 train_time:51560ms step_avg:60.09ms
step:859/2270 train_time:51622ms step_avg:60.10ms
step:860/2270 train_time:51682ms step_avg:60.10ms
step:861/2270 train_time:51744ms step_avg:60.10ms
step:862/2270 train_time:51804ms step_avg:60.10ms
step:863/2270 train_time:51866ms step_avg:60.10ms
step:864/2270 train_time:51926ms step_avg:60.10ms
step:865/2270 train_time:51988ms step_avg:60.10ms
step:866/2270 train_time:52048ms step_avg:60.10ms
step:867/2270 train_time:52110ms step_avg:60.10ms
step:868/2270 train_time:52170ms step_avg:60.10ms
step:869/2270 train_time:52232ms step_avg:60.11ms
step:870/2270 train_time:52291ms step_avg:60.10ms
step:871/2270 train_time:52353ms step_avg:60.11ms
step:872/2270 train_time:52411ms step_avg:60.10ms
step:873/2270 train_time:52474ms step_avg:60.11ms
step:874/2270 train_time:52533ms step_avg:60.11ms
step:875/2270 train_time:52595ms step_avg:60.11ms
step:876/2270 train_time:52654ms step_avg:60.11ms
step:877/2270 train_time:52715ms step_avg:60.11ms
step:878/2270 train_time:52773ms step_avg:60.11ms
step:879/2270 train_time:52835ms step_avg:60.11ms
step:880/2270 train_time:52894ms step_avg:60.11ms
step:881/2270 train_time:52955ms step_avg:60.11ms
step:882/2270 train_time:53014ms step_avg:60.11ms
step:883/2270 train_time:53075ms step_avg:60.11ms
step:884/2270 train_time:53134ms step_avg:60.11ms
step:885/2270 train_time:53196ms step_avg:60.11ms
step:886/2270 train_time:53255ms step_avg:60.11ms
step:887/2270 train_time:53317ms step_avg:60.11ms
step:888/2270 train_time:53376ms step_avg:60.11ms
step:889/2270 train_time:53437ms step_avg:60.11ms
step:890/2270 train_time:53496ms step_avg:60.11ms
step:891/2270 train_time:53558ms step_avg:60.11ms
step:892/2270 train_time:53617ms step_avg:60.11ms
step:893/2270 train_time:53679ms step_avg:60.11ms
step:894/2270 train_time:53737ms step_avg:60.11ms
step:895/2270 train_time:53799ms step_avg:60.11ms
step:896/2270 train_time:53858ms step_avg:60.11ms
step:897/2270 train_time:53920ms step_avg:60.11ms
step:898/2270 train_time:53979ms step_avg:60.11ms
step:899/2270 train_time:54041ms step_avg:60.11ms
step:900/2270 train_time:54100ms step_avg:60.11ms
step:901/2270 train_time:54162ms step_avg:60.11ms
step:902/2270 train_time:54222ms step_avg:60.11ms
step:903/2270 train_time:54284ms step_avg:60.11ms
step:904/2270 train_time:54343ms step_avg:60.11ms
step:905/2270 train_time:54405ms step_avg:60.12ms
step:906/2270 train_time:54465ms step_avg:60.12ms
step:907/2270 train_time:54527ms step_avg:60.12ms
step:908/2270 train_time:54588ms step_avg:60.12ms
step:909/2270 train_time:54650ms step_avg:60.12ms
step:910/2270 train_time:54710ms step_avg:60.12ms
step:911/2270 train_time:54772ms step_avg:60.12ms
step:912/2270 train_time:54831ms step_avg:60.12ms
step:913/2270 train_time:54894ms step_avg:60.12ms
step:914/2270 train_time:54953ms step_avg:60.12ms
step:915/2270 train_time:55014ms step_avg:60.12ms
step:916/2270 train_time:55073ms step_avg:60.12ms
step:917/2270 train_time:55134ms step_avg:60.12ms
step:918/2270 train_time:55193ms step_avg:60.12ms
step:919/2270 train_time:55254ms step_avg:60.12ms
step:920/2270 train_time:55313ms step_avg:60.12ms
step:921/2270 train_time:55375ms step_avg:60.12ms
step:922/2270 train_time:55433ms step_avg:60.12ms
step:923/2270 train_time:55495ms step_avg:60.12ms
step:924/2270 train_time:55554ms step_avg:60.12ms
step:925/2270 train_time:55616ms step_avg:60.13ms
step:926/2270 train_time:55675ms step_avg:60.12ms
step:927/2270 train_time:55736ms step_avg:60.13ms
step:928/2270 train_time:55796ms step_avg:60.12ms
step:929/2270 train_time:55857ms step_avg:60.13ms
step:930/2270 train_time:55916ms step_avg:60.12ms
step:931/2270 train_time:55978ms step_avg:60.13ms
step:932/2270 train_time:56037ms step_avg:60.13ms
step:933/2270 train_time:56098ms step_avg:60.13ms
step:934/2270 train_time:56157ms step_avg:60.13ms
step:935/2270 train_time:56218ms step_avg:60.13ms
step:936/2270 train_time:56277ms step_avg:60.13ms
step:937/2270 train_time:56340ms step_avg:60.13ms
step:938/2270 train_time:56399ms step_avg:60.13ms
step:939/2270 train_time:56460ms step_avg:60.13ms
step:940/2270 train_time:56519ms step_avg:60.13ms
step:941/2270 train_time:56581ms step_avg:60.13ms
step:942/2270 train_time:56641ms step_avg:60.13ms
step:943/2270 train_time:56704ms step_avg:60.13ms
step:944/2270 train_time:56764ms step_avg:60.13ms
step:945/2270 train_time:56826ms step_avg:60.13ms
step:946/2270 train_time:56886ms step_avg:60.13ms
step:947/2270 train_time:56948ms step_avg:60.14ms
step:948/2270 train_time:57008ms step_avg:60.14ms
step:949/2270 train_time:57071ms step_avg:60.14ms
step:950/2270 train_time:57130ms step_avg:60.14ms
step:951/2270 train_time:57191ms step_avg:60.14ms
step:952/2270 train_time:57251ms step_avg:60.14ms
step:953/2270 train_time:57313ms step_avg:60.14ms
step:954/2270 train_time:57372ms step_avg:60.14ms
step:955/2270 train_time:57434ms step_avg:60.14ms
step:956/2270 train_time:57492ms step_avg:60.14ms
step:957/2270 train_time:57554ms step_avg:60.14ms
step:958/2270 train_time:57612ms step_avg:60.14ms
step:959/2270 train_time:57674ms step_avg:60.14ms
step:960/2270 train_time:57733ms step_avg:60.14ms
step:961/2270 train_time:57794ms step_avg:60.14ms
step:962/2270 train_time:57853ms step_avg:60.14ms
step:963/2270 train_time:57914ms step_avg:60.14ms
step:964/2270 train_time:57973ms step_avg:60.14ms
step:965/2270 train_time:58035ms step_avg:60.14ms
step:966/2270 train_time:58094ms step_avg:60.14ms
step:967/2270 train_time:58155ms step_avg:60.14ms
step:968/2270 train_time:58215ms step_avg:60.14ms
step:969/2270 train_time:58276ms step_avg:60.14ms
step:970/2270 train_time:58336ms step_avg:60.14ms
step:971/2270 train_time:58397ms step_avg:60.14ms
step:972/2270 train_time:58456ms step_avg:60.14ms
step:973/2270 train_time:58518ms step_avg:60.14ms
step:974/2270 train_time:58577ms step_avg:60.14ms
step:975/2270 train_time:58639ms step_avg:60.14ms
step:976/2270 train_time:58698ms step_avg:60.14ms
step:977/2270 train_time:58760ms step_avg:60.14ms
step:978/2270 train_time:58819ms step_avg:60.14ms
step:979/2270 train_time:58881ms step_avg:60.14ms
step:980/2270 train_time:58939ms step_avg:60.14ms
step:981/2270 train_time:59002ms step_avg:60.14ms
step:982/2270 train_time:59061ms step_avg:60.14ms
step:983/2270 train_time:59123ms step_avg:60.15ms
step:984/2270 train_time:59184ms step_avg:60.15ms
step:985/2270 train_time:59247ms step_avg:60.15ms
step:986/2270 train_time:59306ms step_avg:60.15ms
step:987/2270 train_time:59368ms step_avg:60.15ms
step:988/2270 train_time:59428ms step_avg:60.15ms
step:989/2270 train_time:59491ms step_avg:60.15ms
step:990/2270 train_time:59551ms step_avg:60.15ms
step:991/2270 train_time:59613ms step_avg:60.15ms
step:992/2270 train_time:59672ms step_avg:60.15ms
step:993/2270 train_time:59734ms step_avg:60.15ms
step:994/2270 train_time:59793ms step_avg:60.15ms
step:995/2270 train_time:59855ms step_avg:60.16ms
step:996/2270 train_time:59914ms step_avg:60.15ms
step:997/2270 train_time:59975ms step_avg:60.16ms
step:998/2270 train_time:60034ms step_avg:60.15ms
step:999/2270 train_time:60095ms step_avg:60.16ms
step:1000/2270 train_time:60155ms step_avg:60.15ms
step:1000/2270 val_loss:3.5724 train_time:60217ms step_avg:60.22ms
step:1001/2270 train_time:60235ms step_avg:60.18ms
step:1002/2270 train_time:60277ms step_avg:60.16ms
step:1003/2270 train_time:60340ms step_avg:60.16ms
step:1004/2270 train_time:60401ms step_avg:60.16ms
step:1005/2270 train_time:60465ms step_avg:60.16ms
step:1006/2270 train_time:60525ms step_avg:60.16ms
step:1007/2270 train_time:60586ms step_avg:60.17ms
step:1008/2270 train_time:60645ms step_avg:60.16ms
step:1009/2270 train_time:60706ms step_avg:60.16ms
step:1010/2270 train_time:60765ms step_avg:60.16ms
step:1011/2270 train_time:60826ms step_avg:60.16ms
step:1012/2270 train_time:60884ms step_avg:60.16ms
step:1013/2270 train_time:60945ms step_avg:60.16ms
step:1014/2270 train_time:61004ms step_avg:60.16ms
step:1015/2270 train_time:61066ms step_avg:60.16ms
step:1016/2270 train_time:61125ms step_avg:60.16ms
step:1017/2270 train_time:61188ms step_avg:60.17ms
step:1018/2270 train_time:61248ms step_avg:60.17ms
step:1019/2270 train_time:61311ms step_avg:60.17ms
step:1020/2270 train_time:61371ms step_avg:60.17ms
step:1021/2270 train_time:61434ms step_avg:60.17ms
step:1022/2270 train_time:61493ms step_avg:60.17ms
step:1023/2270 train_time:61555ms step_avg:60.17ms
step:1024/2270 train_time:61614ms step_avg:60.17ms
step:1025/2270 train_time:61675ms step_avg:60.17ms
step:1026/2270 train_time:61735ms step_avg:60.17ms
step:1027/2270 train_time:61796ms step_avg:60.17ms
step:1028/2270 train_time:61855ms step_avg:60.17ms
step:1029/2270 train_time:61917ms step_avg:60.17ms
step:1030/2270 train_time:61976ms step_avg:60.17ms
step:1031/2270 train_time:62038ms step_avg:60.17ms
step:1032/2270 train_time:62097ms step_avg:60.17ms
step:1033/2270 train_time:62160ms step_avg:60.17ms
step:1034/2270 train_time:62220ms step_avg:60.17ms
step:1035/2270 train_time:62284ms step_avg:60.18ms
step:1036/2270 train_time:62345ms step_avg:60.18ms
step:1037/2270 train_time:62408ms step_avg:60.18ms
step:1038/2270 train_time:62467ms step_avg:60.18ms
step:1039/2270 train_time:62529ms step_avg:60.18ms
step:1040/2270 train_time:62588ms step_avg:60.18ms
step:1041/2270 train_time:62650ms step_avg:60.18ms
step:1042/2270 train_time:62708ms step_avg:60.18ms
step:1043/2270 train_time:62769ms step_avg:60.18ms
step:1044/2270 train_time:62828ms step_avg:60.18ms
step:1045/2270 train_time:62889ms step_avg:60.18ms
step:1046/2270 train_time:62947ms step_avg:60.18ms
step:1047/2270 train_time:63008ms step_avg:60.18ms
step:1048/2270 train_time:63067ms step_avg:60.18ms
step:1049/2270 train_time:63128ms step_avg:60.18ms
step:1050/2270 train_time:63187ms step_avg:60.18ms
step:1051/2270 train_time:63249ms step_avg:60.18ms
step:1052/2270 train_time:63309ms step_avg:60.18ms
step:1053/2270 train_time:63371ms step_avg:60.18ms
step:1054/2270 train_time:63430ms step_avg:60.18ms
step:1055/2270 train_time:63492ms step_avg:60.18ms
step:1056/2270 train_time:63552ms step_avg:60.18ms
step:1057/2270 train_time:63614ms step_avg:60.18ms
step:1058/2270 train_time:63673ms step_avg:60.18ms
step:1059/2270 train_time:63734ms step_avg:60.18ms
step:1060/2270 train_time:63793ms step_avg:60.18ms
step:1061/2270 train_time:63855ms step_avg:60.18ms
step:1062/2270 train_time:63914ms step_avg:60.18ms
step:1063/2270 train_time:63976ms step_avg:60.18ms
step:1064/2270 train_time:64035ms step_avg:60.18ms
step:1065/2270 train_time:64097ms step_avg:60.18ms
step:1066/2270 train_time:64156ms step_avg:60.18ms
step:1067/2270 train_time:64218ms step_avg:60.19ms
step:1068/2270 train_time:64279ms step_avg:60.19ms
step:1069/2270 train_time:64342ms step_avg:60.19ms
step:1070/2270 train_time:64403ms step_avg:60.19ms
step:1071/2270 train_time:64465ms step_avg:60.19ms
step:1072/2270 train_time:64525ms step_avg:60.19ms
step:1073/2270 train_time:64587ms step_avg:60.19ms
step:1074/2270 train_time:64646ms step_avg:60.19ms
step:1075/2270 train_time:64708ms step_avg:60.19ms
step:1076/2270 train_time:64767ms step_avg:60.19ms
step:1077/2270 train_time:64828ms step_avg:60.19ms
step:1078/2270 train_time:64887ms step_avg:60.19ms
step:1079/2270 train_time:64949ms step_avg:60.19ms
step:1080/2270 train_time:65007ms step_avg:60.19ms
step:1081/2270 train_time:65069ms step_avg:60.19ms
step:1082/2270 train_time:65127ms step_avg:60.19ms
step:1083/2270 train_time:65188ms step_avg:60.19ms
step:1084/2270 train_time:65248ms step_avg:60.19ms
step:1085/2270 train_time:65310ms step_avg:60.19ms
step:1086/2270 train_time:65369ms step_avg:60.19ms
step:1087/2270 train_time:65431ms step_avg:60.19ms
step:1088/2270 train_time:65491ms step_avg:60.19ms
step:1089/2270 train_time:65553ms step_avg:60.20ms
step:1090/2270 train_time:65612ms step_avg:60.19ms
step:1091/2270 train_time:65674ms step_avg:60.20ms
step:1092/2270 train_time:65733ms step_avg:60.20ms
step:1093/2270 train_time:65795ms step_avg:60.20ms
step:1094/2270 train_time:65854ms step_avg:60.20ms
step:1095/2270 train_time:65915ms step_avg:60.20ms
step:1096/2270 train_time:65974ms step_avg:60.20ms
step:1097/2270 train_time:66036ms step_avg:60.20ms
step:1098/2270 train_time:66095ms step_avg:60.20ms
step:1099/2270 train_time:66158ms step_avg:60.20ms
step:1100/2270 train_time:66218ms step_avg:60.20ms
step:1101/2270 train_time:66280ms step_avg:60.20ms
step:1102/2270 train_time:66341ms step_avg:60.20ms
step:1103/2270 train_time:66404ms step_avg:60.20ms
step:1104/2270 train_time:66464ms step_avg:60.20ms
step:1105/2270 train_time:66526ms step_avg:60.20ms
step:1106/2270 train_time:66585ms step_avg:60.20ms
step:1107/2270 train_time:66648ms step_avg:60.21ms
step:1108/2270 train_time:66707ms step_avg:60.20ms
step:1109/2270 train_time:66769ms step_avg:60.21ms
step:1110/2270 train_time:66828ms step_avg:60.21ms
step:1111/2270 train_time:66890ms step_avg:60.21ms
step:1112/2270 train_time:66948ms step_avg:60.21ms
step:1113/2270 train_time:67009ms step_avg:60.21ms
step:1114/2270 train_time:67068ms step_avg:60.20ms
step:1115/2270 train_time:67129ms step_avg:60.21ms
step:1116/2270 train_time:67188ms step_avg:60.20ms
step:1117/2270 train_time:67250ms step_avg:60.21ms
step:1118/2270 train_time:67309ms step_avg:60.20ms
step:1119/2270 train_time:67370ms step_avg:60.21ms
step:1120/2270 train_time:67430ms step_avg:60.20ms
step:1121/2270 train_time:67491ms step_avg:60.21ms
step:1122/2270 train_time:67551ms step_avg:60.21ms
step:1123/2270 train_time:67612ms step_avg:60.21ms
step:1124/2270 train_time:67671ms step_avg:60.21ms
step:1125/2270 train_time:67732ms step_avg:60.21ms
step:1126/2270 train_time:67791ms step_avg:60.21ms
step:1127/2270 train_time:67853ms step_avg:60.21ms
step:1128/2270 train_time:67912ms step_avg:60.21ms
step:1129/2270 train_time:67974ms step_avg:60.21ms
step:1130/2270 train_time:68033ms step_avg:60.21ms
step:1131/2270 train_time:68095ms step_avg:60.21ms
step:1132/2270 train_time:68154ms step_avg:60.21ms
step:1133/2270 train_time:68216ms step_avg:60.21ms
step:1134/2270 train_time:68275ms step_avg:60.21ms
step:1135/2270 train_time:68336ms step_avg:60.21ms
step:1136/2270 train_time:68396ms step_avg:60.21ms
step:1137/2270 train_time:68459ms step_avg:60.21ms
step:1138/2270 train_time:68519ms step_avg:60.21ms
step:1139/2270 train_time:68583ms step_avg:60.21ms
step:1140/2270 train_time:68643ms step_avg:60.21ms
step:1141/2270 train_time:68707ms step_avg:60.22ms
step:1142/2270 train_time:68766ms step_avg:60.22ms
step:1143/2270 train_time:68829ms step_avg:60.22ms
step:1144/2270 train_time:68888ms step_avg:60.22ms
step:1145/2270 train_time:68950ms step_avg:60.22ms
step:1146/2270 train_time:69009ms step_avg:60.22ms
step:1147/2270 train_time:69070ms step_avg:60.22ms
step:1148/2270 train_time:69130ms step_avg:60.22ms
step:1149/2270 train_time:69191ms step_avg:60.22ms
step:1150/2270 train_time:69251ms step_avg:60.22ms
step:1151/2270 train_time:69312ms step_avg:60.22ms
step:1152/2270 train_time:69371ms step_avg:60.22ms
step:1153/2270 train_time:69433ms step_avg:60.22ms
step:1154/2270 train_time:69492ms step_avg:60.22ms
step:1155/2270 train_time:69554ms step_avg:60.22ms
step:1156/2270 train_time:69614ms step_avg:60.22ms
step:1157/2270 train_time:69676ms step_avg:60.22ms
step:1158/2270 train_time:69736ms step_avg:60.22ms
step:1159/2270 train_time:69799ms step_avg:60.22ms
step:1160/2270 train_time:69859ms step_avg:60.22ms
step:1161/2270 train_time:69922ms step_avg:60.23ms
step:1162/2270 train_time:69981ms step_avg:60.23ms
step:1163/2270 train_time:70044ms step_avg:60.23ms
step:1164/2270 train_time:70104ms step_avg:60.23ms
step:1165/2270 train_time:70166ms step_avg:60.23ms
step:1166/2270 train_time:70226ms step_avg:60.23ms
step:1167/2270 train_time:70288ms step_avg:60.23ms
step:1168/2270 train_time:70347ms step_avg:60.23ms
step:1169/2270 train_time:70410ms step_avg:60.23ms
step:1170/2270 train_time:70469ms step_avg:60.23ms
step:1171/2270 train_time:70531ms step_avg:60.23ms
step:1172/2270 train_time:70590ms step_avg:60.23ms
step:1173/2270 train_time:70652ms step_avg:60.23ms
step:1174/2270 train_time:70712ms step_avg:60.23ms
step:1175/2270 train_time:70773ms step_avg:60.23ms
step:1176/2270 train_time:70832ms step_avg:60.23ms
step:1177/2270 train_time:70895ms step_avg:60.23ms
step:1178/2270 train_time:70954ms step_avg:60.23ms
step:1179/2270 train_time:71016ms step_avg:60.23ms
step:1180/2270 train_time:71076ms step_avg:60.23ms
step:1181/2270 train_time:71139ms step_avg:60.24ms
step:1182/2270 train_time:71198ms step_avg:60.24ms
step:1183/2270 train_time:71261ms step_avg:60.24ms
step:1184/2270 train_time:71321ms step_avg:60.24ms
step:1185/2270 train_time:71384ms step_avg:60.24ms
step:1186/2270 train_time:71444ms step_avg:60.24ms
step:1187/2270 train_time:71506ms step_avg:60.24ms
step:1188/2270 train_time:71566ms step_avg:60.24ms
step:1189/2270 train_time:71628ms step_avg:60.24ms
step:1190/2270 train_time:71688ms step_avg:60.24ms
step:1191/2270 train_time:71751ms step_avg:60.24ms
step:1192/2270 train_time:71810ms step_avg:60.24ms
step:1193/2270 train_time:71871ms step_avg:60.24ms
step:1194/2270 train_time:71930ms step_avg:60.24ms
step:1195/2270 train_time:71992ms step_avg:60.24ms
step:1196/2270 train_time:72051ms step_avg:60.24ms
step:1197/2270 train_time:72113ms step_avg:60.24ms
step:1198/2270 train_time:72172ms step_avg:60.24ms
step:1199/2270 train_time:72233ms step_avg:60.24ms
step:1200/2270 train_time:72292ms step_avg:60.24ms
step:1201/2270 train_time:72355ms step_avg:60.25ms
step:1202/2270 train_time:72415ms step_avg:60.25ms
step:1203/2270 train_time:72477ms step_avg:60.25ms
step:1204/2270 train_time:72537ms step_avg:60.25ms
step:1205/2270 train_time:72600ms step_avg:60.25ms
step:1206/2270 train_time:72660ms step_avg:60.25ms
step:1207/2270 train_time:72723ms step_avg:60.25ms
step:1208/2270 train_time:72783ms step_avg:60.25ms
step:1209/2270 train_time:72845ms step_avg:60.25ms
step:1210/2270 train_time:72906ms step_avg:60.25ms
step:1211/2270 train_time:72968ms step_avg:60.25ms
step:1212/2270 train_time:73027ms step_avg:60.25ms
step:1213/2270 train_time:73089ms step_avg:60.25ms
step:1214/2270 train_time:73148ms step_avg:60.25ms
step:1215/2270 train_time:73210ms step_avg:60.26ms
step:1216/2270 train_time:73269ms step_avg:60.25ms
step:1217/2270 train_time:73330ms step_avg:60.25ms
step:1218/2270 train_time:73390ms step_avg:60.25ms
step:1219/2270 train_time:73452ms step_avg:60.26ms
step:1220/2270 train_time:73511ms step_avg:60.25ms
step:1221/2270 train_time:73572ms step_avg:60.26ms
step:1222/2270 train_time:73632ms step_avg:60.26ms
step:1223/2270 train_time:73694ms step_avg:60.26ms
step:1224/2270 train_time:73753ms step_avg:60.26ms
step:1225/2270 train_time:73816ms step_avg:60.26ms
step:1226/2270 train_time:73876ms step_avg:60.26ms
step:1227/2270 train_time:73938ms step_avg:60.26ms
step:1228/2270 train_time:73998ms step_avg:60.26ms
step:1229/2270 train_time:74061ms step_avg:60.26ms
step:1230/2270 train_time:74120ms step_avg:60.26ms
step:1231/2270 train_time:74183ms step_avg:60.26ms
step:1232/2270 train_time:74243ms step_avg:60.26ms
step:1233/2270 train_time:74306ms step_avg:60.26ms
step:1234/2270 train_time:74365ms step_avg:60.26ms
step:1235/2270 train_time:74427ms step_avg:60.27ms
step:1236/2270 train_time:74487ms step_avg:60.26ms
step:1237/2270 train_time:74548ms step_avg:60.27ms
step:1238/2270 train_time:74608ms step_avg:60.26ms
step:1239/2270 train_time:74670ms step_avg:60.27ms
step:1240/2270 train_time:74729ms step_avg:60.27ms
step:1241/2270 train_time:74791ms step_avg:60.27ms
step:1242/2270 train_time:74851ms step_avg:60.27ms
step:1243/2270 train_time:74912ms step_avg:60.27ms
step:1244/2270 train_time:74972ms step_avg:60.27ms
step:1245/2270 train_time:75034ms step_avg:60.27ms
step:1246/2270 train_time:75093ms step_avg:60.27ms
step:1247/2270 train_time:75155ms step_avg:60.27ms
step:1248/2270 train_time:75214ms step_avg:60.27ms
step:1249/2270 train_time:75277ms step_avg:60.27ms
step:1250/2270 train_time:75337ms step_avg:60.27ms
step:1250/2270 val_loss:3.5010 train_time:75400ms step_avg:60.32ms
step:1251/2270 train_time:75420ms step_avg:60.29ms
step:1252/2270 train_time:75461ms step_avg:60.27ms
step:1253/2270 train_time:75523ms step_avg:60.27ms
step:1254/2270 train_time:75582ms step_avg:60.27ms
step:1255/2270 train_time:75645ms step_avg:60.27ms
step:1256/2270 train_time:75704ms step_avg:60.27ms
step:1257/2270 train_time:75765ms step_avg:60.27ms
step:1258/2270 train_time:75823ms step_avg:60.27ms
step:1259/2270 train_time:75884ms step_avg:60.27ms
step:1260/2270 train_time:75943ms step_avg:60.27ms
step:1261/2270 train_time:76004ms step_avg:60.27ms
step:1262/2270 train_time:76063ms step_avg:60.27ms
step:1263/2270 train_time:76124ms step_avg:60.27ms
step:1264/2270 train_time:76182ms step_avg:60.27ms
step:1265/2270 train_time:76243ms step_avg:60.27ms
step:1266/2270 train_time:76302ms step_avg:60.27ms
step:1267/2270 train_time:76367ms step_avg:60.27ms
step:1268/2270 train_time:76428ms step_avg:60.27ms
step:1269/2270 train_time:76490ms step_avg:60.28ms
step:1270/2270 train_time:76550ms step_avg:60.28ms
step:1271/2270 train_time:76613ms step_avg:60.28ms
step:1272/2270 train_time:76673ms step_avg:60.28ms
step:1273/2270 train_time:76734ms step_avg:60.28ms
step:1274/2270 train_time:76794ms step_avg:60.28ms
step:1275/2270 train_time:76857ms step_avg:60.28ms
step:1276/2270 train_time:76916ms step_avg:60.28ms
step:1277/2270 train_time:76978ms step_avg:60.28ms
step:1278/2270 train_time:77037ms step_avg:60.28ms
step:1279/2270 train_time:77100ms step_avg:60.28ms
step:1280/2270 train_time:77159ms step_avg:60.28ms
step:1281/2270 train_time:77221ms step_avg:60.28ms
step:1282/2270 train_time:77280ms step_avg:60.28ms
step:1283/2270 train_time:77343ms step_avg:60.28ms
step:1284/2270 train_time:77403ms step_avg:60.28ms
step:1285/2270 train_time:77465ms step_avg:60.28ms
step:1286/2270 train_time:77524ms step_avg:60.28ms
step:1287/2270 train_time:77586ms step_avg:60.28ms
step:1288/2270 train_time:77645ms step_avg:60.28ms
step:1289/2270 train_time:77707ms step_avg:60.28ms
step:1290/2270 train_time:77766ms step_avg:60.28ms
step:1291/2270 train_time:77828ms step_avg:60.29ms
step:1292/2270 train_time:77888ms step_avg:60.28ms
step:1293/2270 train_time:77950ms step_avg:60.29ms
step:1294/2270 train_time:78010ms step_avg:60.29ms
step:1295/2270 train_time:78073ms step_avg:60.29ms
step:1296/2270 train_time:78132ms step_avg:60.29ms
step:1297/2270 train_time:78195ms step_avg:60.29ms
step:1298/2270 train_time:78254ms step_avg:60.29ms
step:1299/2270 train_time:78317ms step_avg:60.29ms
step:1300/2270 train_time:78378ms step_avg:60.29ms
step:1301/2270 train_time:78441ms step_avg:60.29ms
step:1302/2270 train_time:78500ms step_avg:60.29ms
step:1303/2270 train_time:78562ms step_avg:60.29ms
step:1304/2270 train_time:78621ms step_avg:60.29ms
step:1305/2270 train_time:78683ms step_avg:60.29ms
step:1306/2270 train_time:78743ms step_avg:60.29ms
step:1307/2270 train_time:78804ms step_avg:60.29ms
step:1308/2270 train_time:78863ms step_avg:60.29ms
step:1309/2270 train_time:78925ms step_avg:60.29ms
step:1310/2270 train_time:78984ms step_avg:60.29ms
step:1311/2270 train_time:79045ms step_avg:60.29ms
step:1312/2270 train_time:79104ms step_avg:60.29ms
step:1313/2270 train_time:79165ms step_avg:60.29ms
step:1314/2270 train_time:79223ms step_avg:60.29ms
step:1315/2270 train_time:79286ms step_avg:60.29ms
step:1316/2270 train_time:79345ms step_avg:60.29ms
step:1317/2270 train_time:79407ms step_avg:60.29ms
step:1318/2270 train_time:79466ms step_avg:60.29ms
step:1319/2270 train_time:79528ms step_avg:60.29ms
step:1320/2270 train_time:79587ms step_avg:60.29ms
step:1321/2270 train_time:79650ms step_avg:60.30ms
step:1322/2270 train_time:79710ms step_avg:60.29ms
step:1323/2270 train_time:79772ms step_avg:60.30ms
step:1324/2270 train_time:79832ms step_avg:60.30ms
step:1325/2270 train_time:79895ms step_avg:60.30ms
step:1326/2270 train_time:79954ms step_avg:60.30ms
step:1327/2270 train_time:80016ms step_avg:60.30ms
step:1328/2270 train_time:80077ms step_avg:60.30ms
step:1329/2270 train_time:80139ms step_avg:60.30ms
step:1330/2270 train_time:80199ms step_avg:60.30ms
step:1331/2270 train_time:80261ms step_avg:60.30ms
step:1332/2270 train_time:80320ms step_avg:60.30ms
step:1333/2270 train_time:80382ms step_avg:60.30ms
step:1334/2270 train_time:80441ms step_avg:60.30ms
step:1335/2270 train_time:80504ms step_avg:60.30ms
step:1336/2270 train_time:80563ms step_avg:60.30ms
step:1337/2270 train_time:80625ms step_avg:60.30ms
step:1338/2270 train_time:80684ms step_avg:60.30ms
step:1339/2270 train_time:80745ms step_avg:60.30ms
step:1340/2270 train_time:80804ms step_avg:60.30ms
step:1341/2270 train_time:80866ms step_avg:60.30ms
step:1342/2270 train_time:80925ms step_avg:60.30ms
step:1343/2270 train_time:80987ms step_avg:60.30ms
step:1344/2270 train_time:81046ms step_avg:60.30ms
step:1345/2270 train_time:81108ms step_avg:60.30ms
step:1346/2270 train_time:81168ms step_avg:60.30ms
step:1347/2270 train_time:81230ms step_avg:60.30ms
step:1348/2270 train_time:81289ms step_avg:60.30ms
step:1349/2270 train_time:81352ms step_avg:60.31ms
step:1350/2270 train_time:81412ms step_avg:60.31ms
step:1351/2270 train_time:81475ms step_avg:60.31ms
step:1352/2270 train_time:81534ms step_avg:60.31ms
step:1353/2270 train_time:81597ms step_avg:60.31ms
step:1354/2270 train_time:81658ms step_avg:60.31ms
step:1355/2270 train_time:81720ms step_avg:60.31ms
step:1356/2270 train_time:81780ms step_avg:60.31ms
step:1357/2270 train_time:81842ms step_avg:60.31ms
step:1358/2270 train_time:81901ms step_avg:60.31ms
step:1359/2270 train_time:81962ms step_avg:60.31ms
step:1360/2270 train_time:82021ms step_avg:60.31ms
step:1361/2270 train_time:82083ms step_avg:60.31ms
step:1362/2270 train_time:82142ms step_avg:60.31ms
step:1363/2270 train_time:82204ms step_avg:60.31ms
step:1364/2270 train_time:82263ms step_avg:60.31ms
step:1365/2270 train_time:82325ms step_avg:60.31ms
step:1366/2270 train_time:82384ms step_avg:60.31ms
step:1367/2270 train_time:82446ms step_avg:60.31ms
step:1368/2270 train_time:82505ms step_avg:60.31ms
step:1369/2270 train_time:82567ms step_avg:60.31ms
step:1370/2270 train_time:82627ms step_avg:60.31ms
step:1371/2270 train_time:82690ms step_avg:60.31ms
step:1372/2270 train_time:82749ms step_avg:60.31ms
step:1373/2270 train_time:82812ms step_avg:60.31ms
step:1374/2270 train_time:82872ms step_avg:60.31ms
step:1375/2270 train_time:82934ms step_avg:60.32ms
step:1376/2270 train_time:82994ms step_avg:60.32ms
step:1377/2270 train_time:83056ms step_avg:60.32ms
step:1378/2270 train_time:83115ms step_avg:60.32ms
step:1379/2270 train_time:83178ms step_avg:60.32ms
step:1380/2270 train_time:83238ms step_avg:60.32ms
step:1381/2270 train_time:83300ms step_avg:60.32ms
step:1382/2270 train_time:83360ms step_avg:60.32ms
step:1383/2270 train_time:83422ms step_avg:60.32ms
step:1384/2270 train_time:83481ms step_avg:60.32ms
step:1385/2270 train_time:83544ms step_avg:60.32ms
step:1386/2270 train_time:83603ms step_avg:60.32ms
step:1387/2270 train_time:83664ms step_avg:60.32ms
step:1388/2270 train_time:83724ms step_avg:60.32ms
step:1389/2270 train_time:83786ms step_avg:60.32ms
step:1390/2270 train_time:83845ms step_avg:60.32ms
step:1391/2270 train_time:83907ms step_avg:60.32ms
step:1392/2270 train_time:83966ms step_avg:60.32ms
step:1393/2270 train_time:84028ms step_avg:60.32ms
step:1394/2270 train_time:84087ms step_avg:60.32ms
step:1395/2270 train_time:84149ms step_avg:60.32ms
step:1396/2270 train_time:84208ms step_avg:60.32ms
step:1397/2270 train_time:84270ms step_avg:60.32ms
step:1398/2270 train_time:84330ms step_avg:60.32ms
step:1399/2270 train_time:84392ms step_avg:60.32ms
step:1400/2270 train_time:84452ms step_avg:60.32ms
step:1401/2270 train_time:84514ms step_avg:60.32ms
step:1402/2270 train_time:84574ms step_avg:60.32ms
step:1403/2270 train_time:84637ms step_avg:60.33ms
step:1404/2270 train_time:84697ms step_avg:60.33ms
step:1405/2270 train_time:84760ms step_avg:60.33ms
step:1406/2270 train_time:84819ms step_avg:60.33ms
step:1407/2270 train_time:84882ms step_avg:60.33ms
step:1408/2270 train_time:84940ms step_avg:60.33ms
step:1409/2270 train_time:85002ms step_avg:60.33ms
step:1410/2270 train_time:85062ms step_avg:60.33ms
step:1411/2270 train_time:85123ms step_avg:60.33ms
step:1412/2270 train_time:85183ms step_avg:60.33ms
step:1413/2270 train_time:85244ms step_avg:60.33ms
step:1414/2270 train_time:85304ms step_avg:60.33ms
step:1415/2270 train_time:85365ms step_avg:60.33ms
step:1416/2270 train_time:85424ms step_avg:60.33ms
step:1417/2270 train_time:85486ms step_avg:60.33ms
step:1418/2270 train_time:85545ms step_avg:60.33ms
step:1419/2270 train_time:85607ms step_avg:60.33ms
step:1420/2270 train_time:85667ms step_avg:60.33ms
step:1421/2270 train_time:85729ms step_avg:60.33ms
step:1422/2270 train_time:85789ms step_avg:60.33ms
step:1423/2270 train_time:85851ms step_avg:60.33ms
step:1424/2270 train_time:85911ms step_avg:60.33ms
step:1425/2270 train_time:85973ms step_avg:60.33ms
step:1426/2270 train_time:86033ms step_avg:60.33ms
step:1427/2270 train_time:86096ms step_avg:60.33ms
step:1428/2270 train_time:86156ms step_avg:60.33ms
step:1429/2270 train_time:86219ms step_avg:60.34ms
step:1430/2270 train_time:86279ms step_avg:60.33ms
step:1431/2270 train_time:86341ms step_avg:60.34ms
step:1432/2270 train_time:86400ms step_avg:60.34ms
step:1433/2270 train_time:86463ms step_avg:60.34ms
step:1434/2270 train_time:86522ms step_avg:60.34ms
step:1435/2270 train_time:86584ms step_avg:60.34ms
step:1436/2270 train_time:86643ms step_avg:60.34ms
step:1437/2270 train_time:86705ms step_avg:60.34ms
step:1438/2270 train_time:86764ms step_avg:60.34ms
step:1439/2270 train_time:86826ms step_avg:60.34ms
step:1440/2270 train_time:86885ms step_avg:60.34ms
step:1441/2270 train_time:86946ms step_avg:60.34ms
step:1442/2270 train_time:87006ms step_avg:60.34ms
step:1443/2270 train_time:87068ms step_avg:60.34ms
step:1444/2270 train_time:87127ms step_avg:60.34ms
step:1445/2270 train_time:87189ms step_avg:60.34ms
step:1446/2270 train_time:87248ms step_avg:60.34ms
step:1447/2270 train_time:87311ms step_avg:60.34ms
step:1448/2270 train_time:87371ms step_avg:60.34ms
step:1449/2270 train_time:87434ms step_avg:60.34ms
step:1450/2270 train_time:87493ms step_avg:60.34ms
step:1451/2270 train_time:87556ms step_avg:60.34ms
step:1452/2270 train_time:87616ms step_avg:60.34ms
step:1453/2270 train_time:87679ms step_avg:60.34ms
step:1454/2270 train_time:87738ms step_avg:60.34ms
step:1455/2270 train_time:87801ms step_avg:60.34ms
step:1456/2270 train_time:87860ms step_avg:60.34ms
step:1457/2270 train_time:87922ms step_avg:60.34ms
step:1458/2270 train_time:87982ms step_avg:60.34ms
step:1459/2270 train_time:88043ms step_avg:60.35ms
step:1460/2270 train_time:88103ms step_avg:60.34ms
step:1461/2270 train_time:88165ms step_avg:60.35ms
step:1462/2270 train_time:88224ms step_avg:60.34ms
step:1463/2270 train_time:88286ms step_avg:60.35ms
step:1464/2270 train_time:88344ms step_avg:60.34ms
step:1465/2270 train_time:88406ms step_avg:60.35ms
step:1466/2270 train_time:88466ms step_avg:60.34ms
step:1467/2270 train_time:88528ms step_avg:60.35ms
step:1468/2270 train_time:88587ms step_avg:60.35ms
step:1469/2270 train_time:88650ms step_avg:60.35ms
step:1470/2270 train_time:88710ms step_avg:60.35ms
step:1471/2270 train_time:88772ms step_avg:60.35ms
step:1472/2270 train_time:88831ms step_avg:60.35ms
step:1473/2270 train_time:88894ms step_avg:60.35ms
step:1474/2270 train_time:88954ms step_avg:60.35ms
step:1475/2270 train_time:89017ms step_avg:60.35ms
step:1476/2270 train_time:89077ms step_avg:60.35ms
step:1477/2270 train_time:89140ms step_avg:60.35ms
step:1478/2270 train_time:89199ms step_avg:60.35ms
step:1479/2270 train_time:89262ms step_avg:60.35ms
step:1480/2270 train_time:89321ms step_avg:60.35ms
step:1481/2270 train_time:89383ms step_avg:60.35ms
step:1482/2270 train_time:89442ms step_avg:60.35ms
step:1483/2270 train_time:89505ms step_avg:60.35ms
step:1484/2270 train_time:89564ms step_avg:60.35ms
step:1485/2270 train_time:89626ms step_avg:60.35ms
step:1486/2270 train_time:89685ms step_avg:60.35ms
step:1487/2270 train_time:89746ms step_avg:60.35ms
step:1488/2270 train_time:89806ms step_avg:60.35ms
step:1489/2270 train_time:89868ms step_avg:60.35ms
step:1490/2270 train_time:89927ms step_avg:60.35ms
step:1491/2270 train_time:89989ms step_avg:60.36ms
step:1492/2270 train_time:90049ms step_avg:60.35ms
step:1493/2270 train_time:90111ms step_avg:60.36ms
step:1494/2270 train_time:90171ms step_avg:60.36ms
step:1495/2270 train_time:90234ms step_avg:60.36ms
step:1496/2270 train_time:90294ms step_avg:60.36ms
step:1497/2270 train_time:90356ms step_avg:60.36ms
step:1498/2270 train_time:90417ms step_avg:60.36ms
step:1499/2270 train_time:90479ms step_avg:60.36ms
step:1500/2270 train_time:90539ms step_avg:60.36ms
step:1500/2270 val_loss:3.4331 train_time:90602ms step_avg:60.40ms
step:1501/2270 train_time:90622ms step_avg:60.37ms
step:1502/2270 train_time:90665ms step_avg:60.36ms
step:1503/2270 train_time:90731ms step_avg:60.37ms
step:1504/2270 train_time:90792ms step_avg:60.37ms
step:1505/2270 train_time:90854ms step_avg:60.37ms
step:1506/2270 train_time:90914ms step_avg:60.37ms
step:1507/2270 train_time:90975ms step_avg:60.37ms
step:1508/2270 train_time:91034ms step_avg:60.37ms
step:1509/2270 train_time:91096ms step_avg:60.37ms
step:1510/2270 train_time:91155ms step_avg:60.37ms
step:1511/2270 train_time:91218ms step_avg:60.37ms
step:1512/2270 train_time:91278ms step_avg:60.37ms
step:1513/2270 train_time:91341ms step_avg:60.37ms
step:1514/2270 train_time:91401ms step_avg:60.37ms
step:1515/2270 train_time:91462ms step_avg:60.37ms
step:1516/2270 train_time:91523ms step_avg:60.37ms
step:1517/2270 train_time:91586ms step_avg:60.37ms
step:1518/2270 train_time:91648ms step_avg:60.37ms
step:1519/2270 train_time:91711ms step_avg:60.38ms
step:1520/2270 train_time:91772ms step_avg:60.38ms
step:1521/2270 train_time:91835ms step_avg:60.38ms
step:1522/2270 train_time:91894ms step_avg:60.38ms
step:1523/2270 train_time:91957ms step_avg:60.38ms
step:1524/2270 train_time:92017ms step_avg:60.38ms
step:1525/2270 train_time:92078ms step_avg:60.38ms
step:1526/2270 train_time:92138ms step_avg:60.38ms
step:1527/2270 train_time:92201ms step_avg:60.38ms
step:1528/2270 train_time:92261ms step_avg:60.38ms
step:1529/2270 train_time:92323ms step_avg:60.38ms
step:1530/2270 train_time:92382ms step_avg:60.38ms
step:1531/2270 train_time:92444ms step_avg:60.38ms
step:1532/2270 train_time:92504ms step_avg:60.38ms
step:1533/2270 train_time:92567ms step_avg:60.38ms
step:1534/2270 train_time:92627ms step_avg:60.38ms
step:1535/2270 train_time:92690ms step_avg:60.38ms
step:1536/2270 train_time:92750ms step_avg:60.38ms
step:1537/2270 train_time:92813ms step_avg:60.39ms
step:1538/2270 train_time:92872ms step_avg:60.38ms
step:1539/2270 train_time:92934ms step_avg:60.39ms
step:1540/2270 train_time:92994ms step_avg:60.39ms
step:1541/2270 train_time:93057ms step_avg:60.39ms
step:1542/2270 train_time:93117ms step_avg:60.39ms
step:1543/2270 train_time:93181ms step_avg:60.39ms
step:1544/2270 train_time:93241ms step_avg:60.39ms
step:1545/2270 train_time:93303ms step_avg:60.39ms
step:1546/2270 train_time:93362ms step_avg:60.39ms
step:1547/2270 train_time:93424ms step_avg:60.39ms
step:1548/2270 train_time:93484ms step_avg:60.39ms
step:1549/2270 train_time:93547ms step_avg:60.39ms
step:1550/2270 train_time:93607ms step_avg:60.39ms
step:1551/2270 train_time:93669ms step_avg:60.39ms
step:1552/2270 train_time:93729ms step_avg:60.39ms
step:1553/2270 train_time:93791ms step_avg:60.39ms
step:1554/2270 train_time:93851ms step_avg:60.39ms
step:1555/2270 train_time:93913ms step_avg:60.39ms
step:1556/2270 train_time:93973ms step_avg:60.39ms
step:1557/2270 train_time:94035ms step_avg:60.40ms
step:1558/2270 train_time:94095ms step_avg:60.39ms
step:1559/2270 train_time:94158ms step_avg:60.40ms
step:1560/2270 train_time:94218ms step_avg:60.40ms
step:1561/2270 train_time:94281ms step_avg:60.40ms
step:1562/2270 train_time:94341ms step_avg:60.40ms
step:1563/2270 train_time:94404ms step_avg:60.40ms
step:1564/2270 train_time:94463ms step_avg:60.40ms
step:1565/2270 train_time:94526ms step_avg:60.40ms
step:1566/2270 train_time:94586ms step_avg:60.40ms
step:1567/2270 train_time:94648ms step_avg:60.40ms
step:1568/2270 train_time:94708ms step_avg:60.40ms
step:1569/2270 train_time:94770ms step_avg:60.40ms
step:1570/2270 train_time:94830ms step_avg:60.40ms
step:1571/2270 train_time:94892ms step_avg:60.40ms
step:1572/2270 train_time:94951ms step_avg:60.40ms
step:1573/2270 train_time:95014ms step_avg:60.40ms
step:1574/2270 train_time:95074ms step_avg:60.40ms
step:1575/2270 train_time:95137ms step_avg:60.40ms
step:1576/2270 train_time:95197ms step_avg:60.40ms
step:1577/2270 train_time:95260ms step_avg:60.41ms
step:1578/2270 train_time:95320ms step_avg:60.41ms
step:1579/2270 train_time:95383ms step_avg:60.41ms
step:1580/2270 train_time:95443ms step_avg:60.41ms
step:1581/2270 train_time:95506ms step_avg:60.41ms
step:1582/2270 train_time:95566ms step_avg:60.41ms
step:1583/2270 train_time:95628ms step_avg:60.41ms
step:1584/2270 train_time:95688ms step_avg:60.41ms
step:1585/2270 train_time:95750ms step_avg:60.41ms
step:1586/2270 train_time:95809ms step_avg:60.41ms
step:1587/2270 train_time:95871ms step_avg:60.41ms
step:1588/2270 train_time:95930ms step_avg:60.41ms
step:1589/2270 train_time:95992ms step_avg:60.41ms
step:1590/2270 train_time:96052ms step_avg:60.41ms
step:1591/2270 train_time:96114ms step_avg:60.41ms
step:1592/2270 train_time:96175ms step_avg:60.41ms
step:1593/2270 train_time:96238ms step_avg:60.41ms
step:1594/2270 train_time:96298ms step_avg:60.41ms
step:1595/2270 train_time:96361ms step_avg:60.41ms
step:1596/2270 train_time:96421ms step_avg:60.41ms
step:1597/2270 train_time:96484ms step_avg:60.42ms
step:1598/2270 train_time:96545ms step_avg:60.42ms
step:1599/2270 train_time:96608ms step_avg:60.42ms
step:1600/2270 train_time:96668ms step_avg:60.42ms
step:1601/2270 train_time:96730ms step_avg:60.42ms
step:1602/2270 train_time:96789ms step_avg:60.42ms
step:1603/2270 train_time:96851ms step_avg:60.42ms
step:1604/2270 train_time:96911ms step_avg:60.42ms
step:1605/2270 train_time:96973ms step_avg:60.42ms
step:1606/2270 train_time:97033ms step_avg:60.42ms
step:1607/2270 train_time:97095ms step_avg:60.42ms
step:1608/2270 train_time:97155ms step_avg:60.42ms
step:1609/2270 train_time:97218ms step_avg:60.42ms
step:1610/2270 train_time:97278ms step_avg:60.42ms
step:1611/2270 train_time:97342ms step_avg:60.42ms
step:1612/2270 train_time:97402ms step_avg:60.42ms
step:1613/2270 train_time:97464ms step_avg:60.42ms
step:1614/2270 train_time:97524ms step_avg:60.42ms
step:1615/2270 train_time:97587ms step_avg:60.43ms
step:1616/2270 train_time:97647ms step_avg:60.42ms
step:1617/2270 train_time:97709ms step_avg:60.43ms
step:1618/2270 train_time:97769ms step_avg:60.43ms
step:1619/2270 train_time:97831ms step_avg:60.43ms
step:1620/2270 train_time:97890ms step_avg:60.43ms
step:1621/2270 train_time:97952ms step_avg:60.43ms
step:1622/2270 train_time:98012ms step_avg:60.43ms
step:1623/2270 train_time:98074ms step_avg:60.43ms
step:1624/2270 train_time:98134ms step_avg:60.43ms
step:1625/2270 train_time:98197ms step_avg:60.43ms
step:1626/2270 train_time:98257ms step_avg:60.43ms
step:1627/2270 train_time:98320ms step_avg:60.43ms
step:1628/2270 train_time:98380ms step_avg:60.43ms
step:1629/2270 train_time:98443ms step_avg:60.43ms
step:1630/2270 train_time:98503ms step_avg:60.43ms
step:1631/2270 train_time:98566ms step_avg:60.43ms
step:1632/2270 train_time:98626ms step_avg:60.43ms
step:1633/2270 train_time:98688ms step_avg:60.43ms
step:1634/2270 train_time:98748ms step_avg:60.43ms
step:1635/2270 train_time:98810ms step_avg:60.43ms
step:1636/2270 train_time:98870ms step_avg:60.43ms
step:1637/2270 train_time:98931ms step_avg:60.43ms
step:1638/2270 train_time:98990ms step_avg:60.43ms
step:1639/2270 train_time:99052ms step_avg:60.43ms
step:1640/2270 train_time:99112ms step_avg:60.43ms
step:1641/2270 train_time:99174ms step_avg:60.44ms
step:1642/2270 train_time:99234ms step_avg:60.44ms
step:1643/2270 train_time:99298ms step_avg:60.44ms
step:1644/2270 train_time:99358ms step_avg:60.44ms
step:1645/2270 train_time:99421ms step_avg:60.44ms
step:1646/2270 train_time:99482ms step_avg:60.44ms
step:1647/2270 train_time:99545ms step_avg:60.44ms
step:1648/2270 train_time:99605ms step_avg:60.44ms
step:1649/2270 train_time:99668ms step_avg:60.44ms
step:1650/2270 train_time:99728ms step_avg:60.44ms
step:1651/2270 train_time:99790ms step_avg:60.44ms
step:1652/2270 train_time:99849ms step_avg:60.44ms
step:1653/2270 train_time:99911ms step_avg:60.44ms
step:1654/2270 train_time:99970ms step_avg:60.44ms
step:1655/2270 train_time:100032ms step_avg:60.44ms
step:1656/2270 train_time:100092ms step_avg:60.44ms
step:1657/2270 train_time:100154ms step_avg:60.44ms
step:1658/2270 train_time:100214ms step_avg:60.44ms
step:1659/2270 train_time:100277ms step_avg:60.44ms
step:1660/2270 train_time:100337ms step_avg:60.44ms
step:1661/2270 train_time:100402ms step_avg:60.45ms
step:1662/2270 train_time:100462ms step_avg:60.45ms
step:1663/2270 train_time:100525ms step_avg:60.45ms
step:1664/2270 train_time:100585ms step_avg:60.45ms
step:1665/2270 train_time:100648ms step_avg:60.45ms
step:1666/2270 train_time:100708ms step_avg:60.45ms
step:1667/2270 train_time:100770ms step_avg:60.45ms
step:1668/2270 train_time:100829ms step_avg:60.45ms
step:1669/2270 train_time:100891ms step_avg:60.45ms
step:1670/2270 train_time:100951ms step_avg:60.45ms
step:1671/2270 train_time:101013ms step_avg:60.45ms
step:1672/2270 train_time:101072ms step_avg:60.45ms
step:1673/2270 train_time:101134ms step_avg:60.45ms
step:1674/2270 train_time:101193ms step_avg:60.45ms
step:1675/2270 train_time:101256ms step_avg:60.45ms
step:1676/2270 train_time:101317ms step_avg:60.45ms
step:1677/2270 train_time:101380ms step_avg:60.45ms
step:1678/2270 train_time:101441ms step_avg:60.45ms
step:1679/2270 train_time:101504ms step_avg:60.45ms
step:1680/2270 train_time:101564ms step_avg:60.45ms
step:1681/2270 train_time:101627ms step_avg:60.46ms
step:1682/2270 train_time:101687ms step_avg:60.46ms
step:1683/2270 train_time:101749ms step_avg:60.46ms
step:1684/2270 train_time:101809ms step_avg:60.46ms
step:1685/2270 train_time:101871ms step_avg:60.46ms
step:1686/2270 train_time:101930ms step_avg:60.46ms
step:1687/2270 train_time:101992ms step_avg:60.46ms
step:1688/2270 train_time:102052ms step_avg:60.46ms
step:1689/2270 train_time:102114ms step_avg:60.46ms
step:1690/2270 train_time:102173ms step_avg:60.46ms
step:1691/2270 train_time:102235ms step_avg:60.46ms
step:1692/2270 train_time:102295ms step_avg:60.46ms
step:1693/2270 train_time:102358ms step_avg:60.46ms
step:1694/2270 train_time:102418ms step_avg:60.46ms
step:1695/2270 train_time:102482ms step_avg:60.46ms
step:1696/2270 train_time:102542ms step_avg:60.46ms
step:1697/2270 train_time:102605ms step_avg:60.46ms
step:1698/2270 train_time:102664ms step_avg:60.46ms
step:1699/2270 train_time:102727ms step_avg:60.46ms
step:1700/2270 train_time:102787ms step_avg:60.46ms
step:1701/2270 train_time:102849ms step_avg:60.46ms
step:1702/2270 train_time:102909ms step_avg:60.46ms
step:1703/2270 train_time:102971ms step_avg:60.46ms
step:1704/2270 train_time:103030ms step_avg:60.46ms
step:1705/2270 train_time:103091ms step_avg:60.46ms
step:1706/2270 train_time:103151ms step_avg:60.46ms
step:1707/2270 train_time:103213ms step_avg:60.46ms
step:1708/2270 train_time:103273ms step_avg:60.46ms
step:1709/2270 train_time:103337ms step_avg:60.47ms
step:1710/2270 train_time:103397ms step_avg:60.47ms
step:1711/2270 train_time:103461ms step_avg:60.47ms
step:1712/2270 train_time:103521ms step_avg:60.47ms
step:1713/2270 train_time:103585ms step_avg:60.47ms
step:1714/2270 train_time:103645ms step_avg:60.47ms
step:1715/2270 train_time:103708ms step_avg:60.47ms
step:1716/2270 train_time:103768ms step_avg:60.47ms
step:1717/2270 train_time:103830ms step_avg:60.47ms
step:1718/2270 train_time:103889ms step_avg:60.47ms
step:1719/2270 train_time:103952ms step_avg:60.47ms
step:1720/2270 train_time:104011ms step_avg:60.47ms
step:1721/2270 train_time:104073ms step_avg:60.47ms
step:1722/2270 train_time:104132ms step_avg:60.47ms
step:1723/2270 train_time:104195ms step_avg:60.47ms
step:1724/2270 train_time:104255ms step_avg:60.47ms
step:1725/2270 train_time:104318ms step_avg:60.47ms
step:1726/2270 train_time:104379ms step_avg:60.47ms
step:1727/2270 train_time:104443ms step_avg:60.48ms
step:1728/2270 train_time:104503ms step_avg:60.48ms
step:1729/2270 train_time:104566ms step_avg:60.48ms
step:1730/2270 train_time:104626ms step_avg:60.48ms
step:1731/2270 train_time:104689ms step_avg:60.48ms
step:1732/2270 train_time:104749ms step_avg:60.48ms
step:1733/2270 train_time:104811ms step_avg:60.48ms
step:1734/2270 train_time:104871ms step_avg:60.48ms
step:1735/2270 train_time:104933ms step_avg:60.48ms
step:1736/2270 train_time:104993ms step_avg:60.48ms
step:1737/2270 train_time:105055ms step_avg:60.48ms
step:1738/2270 train_time:105115ms step_avg:60.48ms
step:1739/2270 train_time:105177ms step_avg:60.48ms
step:1740/2270 train_time:105238ms step_avg:60.48ms
step:1741/2270 train_time:105300ms step_avg:60.48ms
step:1742/2270 train_time:105361ms step_avg:60.48ms
step:1743/2270 train_time:105424ms step_avg:60.48ms
step:1744/2270 train_time:105483ms step_avg:60.48ms
step:1745/2270 train_time:105547ms step_avg:60.49ms
step:1746/2270 train_time:105607ms step_avg:60.49ms
step:1747/2270 train_time:105669ms step_avg:60.49ms
step:1748/2270 train_time:105729ms step_avg:60.49ms
step:1749/2270 train_time:105792ms step_avg:60.49ms
step:1750/2270 train_time:105852ms step_avg:60.49ms
step:1750/2270 val_loss:3.3703 train_time:105915ms step_avg:60.52ms
step:1751/2270 train_time:105933ms step_avg:60.50ms
step:1752/2270 train_time:105977ms step_avg:60.49ms
step:1753/2270 train_time:106039ms step_avg:60.49ms
step:1754/2270 train_time:106099ms step_avg:60.49ms
step:1755/2270 train_time:106163ms step_avg:60.49ms
step:1756/2270 train_time:106223ms step_avg:60.49ms
step:1757/2270 train_time:106285ms step_avg:60.49ms
step:1758/2270 train_time:106345ms step_avg:60.49ms
step:1759/2270 train_time:106407ms step_avg:60.49ms
step:1760/2270 train_time:106466ms step_avg:60.49ms
step:1761/2270 train_time:106528ms step_avg:60.49ms
step:1762/2270 train_time:106587ms step_avg:60.49ms
step:1763/2270 train_time:106649ms step_avg:60.49ms
step:1764/2270 train_time:106709ms step_avg:60.49ms
step:1765/2270 train_time:106770ms step_avg:60.49ms
step:1766/2270 train_time:106833ms step_avg:60.49ms
step:1767/2270 train_time:106899ms step_avg:60.50ms
step:1768/2270 train_time:106959ms step_avg:60.50ms
step:1769/2270 train_time:107023ms step_avg:60.50ms
step:1770/2270 train_time:107083ms step_avg:60.50ms
step:1771/2270 train_time:107145ms step_avg:60.50ms
step:1772/2270 train_time:107205ms step_avg:60.50ms
step:1773/2270 train_time:107267ms step_avg:60.50ms
step:1774/2270 train_time:107327ms step_avg:60.50ms
step:1775/2270 train_time:107390ms step_avg:60.50ms
step:1776/2270 train_time:107449ms step_avg:60.50ms
step:1777/2270 train_time:107511ms step_avg:60.50ms
step:1778/2270 train_time:107570ms step_avg:60.50ms
step:1779/2270 train_time:107632ms step_avg:60.50ms
step:1780/2270 train_time:107691ms step_avg:60.50ms
step:1781/2270 train_time:107753ms step_avg:60.50ms
step:1782/2270 train_time:107813ms step_avg:60.50ms
step:1783/2270 train_time:107877ms step_avg:60.50ms
step:1784/2270 train_time:107937ms step_avg:60.50ms
step:1785/2270 train_time:108000ms step_avg:60.50ms
step:1786/2270 train_time:108060ms step_avg:60.50ms
step:1787/2270 train_time:108123ms step_avg:60.51ms
step:1788/2270 train_time:108183ms step_avg:60.51ms
step:1789/2270 train_time:108246ms step_avg:60.51ms
step:1790/2270 train_time:108305ms step_avg:60.51ms
step:1791/2270 train_time:108368ms step_avg:60.51ms
step:1792/2270 train_time:108428ms step_avg:60.51ms
step:1793/2270 train_time:108490ms step_avg:60.51ms
step:1794/2270 train_time:108549ms step_avg:60.51ms
step:1795/2270 train_time:108611ms step_avg:60.51ms
step:1796/2270 train_time:108670ms step_avg:60.51ms
step:1797/2270 train_time:108733ms step_avg:60.51ms
step:1798/2270 train_time:108792ms step_avg:60.51ms
step:1799/2270 train_time:108855ms step_avg:60.51ms
step:1800/2270 train_time:108915ms step_avg:60.51ms
step:1801/2270 train_time:108978ms step_avg:60.51ms
step:1802/2270 train_time:109038ms step_avg:60.51ms
step:1803/2270 train_time:109100ms step_avg:60.51ms
step:1804/2270 train_time:109160ms step_avg:60.51ms
step:1805/2270 train_time:109222ms step_avg:60.51ms
step:1806/2270 train_time:109282ms step_avg:60.51ms
step:1807/2270 train_time:109344ms step_avg:60.51ms
step:1808/2270 train_time:109404ms step_avg:60.51ms
step:1809/2270 train_time:109466ms step_avg:60.51ms
step:1810/2270 train_time:109527ms step_avg:60.51ms
step:1811/2270 train_time:109590ms step_avg:60.51ms
step:1812/2270 train_time:109650ms step_avg:60.51ms
step:1813/2270 train_time:109713ms step_avg:60.51ms
step:1814/2270 train_time:109773ms step_avg:60.51ms
step:1815/2270 train_time:109836ms step_avg:60.52ms
step:1816/2270 train_time:109896ms step_avg:60.52ms
step:1817/2270 train_time:109959ms step_avg:60.52ms
step:1818/2270 train_time:110018ms step_avg:60.52ms
step:1819/2270 train_time:110081ms step_avg:60.52ms
step:1820/2270 train_time:110140ms step_avg:60.52ms
step:1821/2270 train_time:110203ms step_avg:60.52ms
step:1822/2270 train_time:110262ms step_avg:60.52ms
step:1823/2270 train_time:110325ms step_avg:60.52ms
step:1824/2270 train_time:110384ms step_avg:60.52ms
step:1825/2270 train_time:110447ms step_avg:60.52ms
step:1826/2270 train_time:110507ms step_avg:60.52ms
step:1827/2270 train_time:110570ms step_avg:60.52ms
step:1828/2270 train_time:110630ms step_avg:60.52ms
step:1829/2270 train_time:110693ms step_avg:60.52ms
step:1830/2270 train_time:110752ms step_avg:60.52ms
step:1831/2270 train_time:110815ms step_avg:60.52ms
step:1832/2270 train_time:110875ms step_avg:60.52ms
step:1833/2270 train_time:110937ms step_avg:60.52ms
step:1834/2270 train_time:110997ms step_avg:60.52ms
step:1835/2270 train_time:111059ms step_avg:60.52ms
step:1836/2270 train_time:111118ms step_avg:60.52ms
step:1837/2270 train_time:111180ms step_avg:60.52ms
step:1838/2270 train_time:111240ms step_avg:60.52ms
step:1839/2270 train_time:111302ms step_avg:60.52ms
step:1840/2270 train_time:111361ms step_avg:60.52ms
step:1841/2270 train_time:111423ms step_avg:60.52ms
step:1842/2270 train_time:111484ms step_avg:60.52ms
step:1843/2270 train_time:111546ms step_avg:60.52ms
step:1844/2270 train_time:111607ms step_avg:60.52ms
step:1845/2270 train_time:111670ms step_avg:60.53ms
step:1846/2270 train_time:111730ms step_avg:60.53ms
step:1847/2270 train_time:111793ms step_avg:60.53ms
step:1848/2270 train_time:111853ms step_avg:60.53ms
step:1849/2270 train_time:111916ms step_avg:60.53ms
step:1850/2270 train_time:111975ms step_avg:60.53ms
step:1851/2270 train_time:112038ms step_avg:60.53ms
step:1852/2270 train_time:112097ms step_avg:60.53ms
step:1853/2270 train_time:112158ms step_avg:60.53ms
step:1854/2270 train_time:112218ms step_avg:60.53ms
step:1855/2270 train_time:112279ms step_avg:60.53ms
step:1856/2270 train_time:112339ms step_avg:60.53ms
step:1857/2270 train_time:112401ms step_avg:60.53ms
step:1858/2270 train_time:112461ms step_avg:60.53ms
step:1859/2270 train_time:112524ms step_avg:60.53ms
step:1860/2270 train_time:112584ms step_avg:60.53ms
step:1861/2270 train_time:112648ms step_avg:60.53ms
step:1862/2270 train_time:112708ms step_avg:60.53ms
step:1863/2270 train_time:112771ms step_avg:60.53ms
step:1864/2270 train_time:112832ms step_avg:60.53ms
step:1865/2270 train_time:112895ms step_avg:60.53ms
step:1866/2270 train_time:112955ms step_avg:60.53ms
step:1867/2270 train_time:113017ms step_avg:60.53ms
step:1868/2270 train_time:113077ms step_avg:60.53ms
step:1869/2270 train_time:113139ms step_avg:60.53ms
step:1870/2270 train_time:113198ms step_avg:60.53ms
step:1871/2270 train_time:113260ms step_avg:60.53ms
step:1872/2270 train_time:113319ms step_avg:60.53ms
step:1873/2270 train_time:113381ms step_avg:60.53ms
step:1874/2270 train_time:113440ms step_avg:60.53ms
step:1875/2270 train_time:113503ms step_avg:60.53ms
step:1876/2270 train_time:113563ms step_avg:60.53ms
step:1877/2270 train_time:113626ms step_avg:60.54ms
step:1878/2270 train_time:113686ms step_avg:60.54ms
step:1879/2270 train_time:113749ms step_avg:60.54ms
step:1880/2270 train_time:113810ms step_avg:60.54ms
step:1881/2270 train_time:113872ms step_avg:60.54ms
step:1882/2270 train_time:113932ms step_avg:60.54ms
step:1883/2270 train_time:113995ms step_avg:60.54ms
step:1884/2270 train_time:114054ms step_avg:60.54ms
step:1885/2270 train_time:114116ms step_avg:60.54ms
step:1886/2270 train_time:114176ms step_avg:60.54ms
step:1887/2270 train_time:114238ms step_avg:60.54ms
step:1888/2270 train_time:114297ms step_avg:60.54ms
step:1889/2270 train_time:114359ms step_avg:60.54ms
step:1890/2270 train_time:114419ms step_avg:60.54ms
step:1891/2270 train_time:114481ms step_avg:60.54ms
step:1892/2270 train_time:114541ms step_avg:60.54ms
step:1893/2270 train_time:114604ms step_avg:60.54ms
step:1894/2270 train_time:114665ms step_avg:60.54ms
step:1895/2270 train_time:114728ms step_avg:60.54ms
step:1896/2270 train_time:114789ms step_avg:60.54ms
step:1897/2270 train_time:114852ms step_avg:60.54ms
step:1898/2270 train_time:114912ms step_avg:60.54ms
step:1899/2270 train_time:114976ms step_avg:60.55ms
step:1900/2270 train_time:115036ms step_avg:60.55ms
step:1901/2270 train_time:115099ms step_avg:60.55ms
step:1902/2270 train_time:115158ms step_avg:60.55ms
step:1903/2270 train_time:115220ms step_avg:60.55ms
step:1904/2270 train_time:115279ms step_avg:60.55ms
step:1905/2270 train_time:115341ms step_avg:60.55ms
step:1906/2270 train_time:115401ms step_avg:60.55ms
step:1907/2270 train_time:115463ms step_avg:60.55ms
step:1908/2270 train_time:115523ms step_avg:60.55ms
step:1909/2270 train_time:115586ms step_avg:60.55ms
step:1910/2270 train_time:115646ms step_avg:60.55ms
step:1911/2270 train_time:115709ms step_avg:60.55ms
step:1912/2270 train_time:115770ms step_avg:60.55ms
step:1913/2270 train_time:115833ms step_avg:60.55ms
step:1914/2270 train_time:115893ms step_avg:60.55ms
step:1915/2270 train_time:115957ms step_avg:60.55ms
step:1916/2270 train_time:116017ms step_avg:60.55ms
step:1917/2270 train_time:116079ms step_avg:60.55ms
step:1918/2270 train_time:116139ms step_avg:60.55ms
step:1919/2270 train_time:116201ms step_avg:60.55ms
step:1920/2270 train_time:116260ms step_avg:60.55ms
step:1921/2270 train_time:116322ms step_avg:60.55ms
step:1922/2270 train_time:116382ms step_avg:60.55ms
step:1923/2270 train_time:116444ms step_avg:60.55ms
step:1924/2270 train_time:116505ms step_avg:60.55ms
step:1925/2270 train_time:116568ms step_avg:60.55ms
step:1926/2270 train_time:116629ms step_avg:60.55ms
step:1927/2270 train_time:116691ms step_avg:60.56ms
step:1928/2270 train_time:116752ms step_avg:60.56ms
step:1929/2270 train_time:116814ms step_avg:60.56ms
step:1930/2270 train_time:116874ms step_avg:60.56ms
step:1931/2270 train_time:116938ms step_avg:60.56ms
step:1932/2270 train_time:116998ms step_avg:60.56ms
step:1933/2270 train_time:117060ms step_avg:60.56ms
step:1934/2270 train_time:117120ms step_avg:60.56ms
step:1935/2270 train_time:117183ms step_avg:60.56ms
step:1936/2270 train_time:117243ms step_avg:60.56ms
step:1937/2270 train_time:117305ms step_avg:60.56ms
step:1938/2270 train_time:117365ms step_avg:60.56ms
step:1939/2270 train_time:117428ms step_avg:60.56ms
step:1940/2270 train_time:117488ms step_avg:60.56ms
step:1941/2270 train_time:117551ms step_avg:60.56ms
step:1942/2270 train_time:117610ms step_avg:60.56ms
step:1943/2270 train_time:117673ms step_avg:60.56ms
step:1944/2270 train_time:117732ms step_avg:60.56ms
step:1945/2270 train_time:117795ms step_avg:60.56ms
step:1946/2270 train_time:117855ms step_avg:60.56ms
step:1947/2270 train_time:117917ms step_avg:60.56ms
step:1948/2270 train_time:117978ms step_avg:60.56ms
step:1949/2270 train_time:118040ms step_avg:60.56ms
step:1950/2270 train_time:118100ms step_avg:60.56ms
step:1951/2270 train_time:118163ms step_avg:60.57ms
step:1952/2270 train_time:118223ms step_avg:60.56ms
step:1953/2270 train_time:118285ms step_avg:60.57ms
step:1954/2270 train_time:118345ms step_avg:60.57ms
step:1955/2270 train_time:118407ms step_avg:60.57ms
step:1956/2270 train_time:118467ms step_avg:60.57ms
step:1957/2270 train_time:118530ms step_avg:60.57ms
step:1958/2270 train_time:118590ms step_avg:60.57ms
step:1959/2270 train_time:118653ms step_avg:60.57ms
step:1960/2270 train_time:118713ms step_avg:60.57ms
step:1961/2270 train_time:118776ms step_avg:60.57ms
step:1962/2270 train_time:118836ms step_avg:60.57ms
step:1963/2270 train_time:118899ms step_avg:60.57ms
step:1964/2270 train_time:118959ms step_avg:60.57ms
step:1965/2270 train_time:119021ms step_avg:60.57ms
step:1966/2270 train_time:119082ms step_avg:60.57ms
step:1967/2270 train_time:119144ms step_avg:60.57ms
step:1968/2270 train_time:119204ms step_avg:60.57ms
step:1969/2270 train_time:119266ms step_avg:60.57ms
step:1970/2270 train_time:119327ms step_avg:60.57ms
step:1971/2270 train_time:119390ms step_avg:60.57ms
step:1972/2270 train_time:119450ms step_avg:60.57ms
step:1973/2270 train_time:119512ms step_avg:60.57ms
step:1974/2270 train_time:119572ms step_avg:60.57ms
step:1975/2270 train_time:119635ms step_avg:60.57ms
step:1976/2270 train_time:119695ms step_avg:60.57ms
step:1977/2270 train_time:119757ms step_avg:60.58ms
step:1978/2270 train_time:119817ms step_avg:60.57ms
step:1979/2270 train_time:119880ms step_avg:60.58ms
step:1980/2270 train_time:119940ms step_avg:60.58ms
step:1981/2270 train_time:120003ms step_avg:60.58ms
step:1982/2270 train_time:120063ms step_avg:60.58ms
step:1983/2270 train_time:120125ms step_avg:60.58ms
step:1984/2270 train_time:120185ms step_avg:60.58ms
step:1985/2270 train_time:120248ms step_avg:60.58ms
step:1986/2270 train_time:120308ms step_avg:60.58ms
step:1987/2270 train_time:120370ms step_avg:60.58ms
step:1988/2270 train_time:120430ms step_avg:60.58ms
step:1989/2270 train_time:120493ms step_avg:60.58ms
step:1990/2270 train_time:120552ms step_avg:60.58ms
step:1991/2270 train_time:120615ms step_avg:60.58ms
step:1992/2270 train_time:120676ms step_avg:60.58ms
step:1993/2270 train_time:120738ms step_avg:60.58ms
step:1994/2270 train_time:120798ms step_avg:60.58ms
step:1995/2270 train_time:120861ms step_avg:60.58ms
step:1996/2270 train_time:120921ms step_avg:60.58ms
step:1997/2270 train_time:120984ms step_avg:60.58ms
step:1998/2270 train_time:121044ms step_avg:60.58ms
step:1999/2270 train_time:121106ms step_avg:60.58ms
step:2000/2270 train_time:121166ms step_avg:60.58ms
step:2000/2270 val_loss:3.3186 train_time:121229ms step_avg:60.61ms
step:2001/2270 train_time:121250ms step_avg:60.59ms
step:2002/2270 train_time:121292ms step_avg:60.59ms
step:2003/2270 train_time:121355ms step_avg:60.59ms
step:2004/2270 train_time:121415ms step_avg:60.59ms
step:2005/2270 train_time:121479ms step_avg:60.59ms
step:2006/2270 train_time:121540ms step_avg:60.59ms
step:2007/2270 train_time:121603ms step_avg:60.59ms
step:2008/2270 train_time:121662ms step_avg:60.59ms
step:2009/2270 train_time:121725ms step_avg:60.59ms
step:2010/2270 train_time:121784ms step_avg:60.59ms
step:2011/2270 train_time:121846ms step_avg:60.59ms
step:2012/2270 train_time:121905ms step_avg:60.59ms
step:2013/2270 train_time:121967ms step_avg:60.59ms
step:2014/2270 train_time:122027ms step_avg:60.59ms
step:2015/2270 train_time:122088ms step_avg:60.59ms
step:2016/2270 train_time:122150ms step_avg:60.59ms
step:2017/2270 train_time:122216ms step_avg:60.59ms
step:2018/2270 train_time:122278ms step_avg:60.59ms
step:2019/2270 train_time:122341ms step_avg:60.59ms
step:2020/2270 train_time:122401ms step_avg:60.59ms
step:2021/2270 train_time:122465ms step_avg:60.60ms
step:2022/2270 train_time:122524ms step_avg:60.60ms
step:2023/2270 train_time:122587ms step_avg:60.60ms
step:2024/2270 train_time:122646ms step_avg:60.60ms
step:2025/2270 train_time:122708ms step_avg:60.60ms
step:2026/2270 train_time:122768ms step_avg:60.60ms
step:2027/2270 train_time:122829ms step_avg:60.60ms
step:2028/2270 train_time:122888ms step_avg:60.60ms
step:2029/2270 train_time:122950ms step_avg:60.60ms
step:2030/2270 train_time:123010ms step_avg:60.60ms
step:2031/2270 train_time:123072ms step_avg:60.60ms
step:2032/2270 train_time:123132ms step_avg:60.60ms
step:2033/2270 train_time:123195ms step_avg:60.60ms
step:2034/2270 train_time:123256ms step_avg:60.60ms
step:2035/2270 train_time:123320ms step_avg:60.60ms
step:2036/2270 train_time:123381ms step_avg:60.60ms
step:2037/2270 train_time:123445ms step_avg:60.60ms
step:2038/2270 train_time:123505ms step_avg:60.60ms
step:2039/2270 train_time:123568ms step_avg:60.60ms
step:2040/2270 train_time:123627ms step_avg:60.60ms
step:2041/2270 train_time:123689ms step_avg:60.60ms
step:2042/2270 train_time:123749ms step_avg:60.60ms
step:2043/2270 train_time:123811ms step_avg:60.60ms
step:2044/2270 train_time:123870ms step_avg:60.60ms
step:2045/2270 train_time:123933ms step_avg:60.60ms
step:2046/2270 train_time:123993ms step_avg:60.60ms
step:2047/2270 train_time:124055ms step_avg:60.60ms
step:2048/2270 train_time:124116ms step_avg:60.60ms
step:2049/2270 train_time:124179ms step_avg:60.60ms
step:2050/2270 train_time:124239ms step_avg:60.60ms
step:2051/2270 train_time:124302ms step_avg:60.61ms
step:2052/2270 train_time:124364ms step_avg:60.61ms
step:2053/2270 train_time:124426ms step_avg:60.61ms
step:2054/2270 train_time:124486ms step_avg:60.61ms
step:2055/2270 train_time:124549ms step_avg:60.61ms
step:2056/2270 train_time:124608ms step_avg:60.61ms
step:2057/2270 train_time:124671ms step_avg:60.61ms
step:2058/2270 train_time:124731ms step_avg:60.61ms
step:2059/2270 train_time:124793ms step_avg:60.61ms
step:2060/2270 train_time:124852ms step_avg:60.61ms
step:2061/2270 train_time:124914ms step_avg:60.61ms
step:2062/2270 train_time:124974ms step_avg:60.61ms
step:2063/2270 train_time:125037ms step_avg:60.61ms
step:2064/2270 train_time:125097ms step_avg:60.61ms
step:2065/2270 train_time:125159ms step_avg:60.61ms
step:2066/2270 train_time:125220ms step_avg:60.61ms
step:2067/2270 train_time:125283ms step_avg:60.61ms
step:2068/2270 train_time:125344ms step_avg:60.61ms
step:2069/2270 train_time:125407ms step_avg:60.61ms
step:2070/2270 train_time:125467ms step_avg:60.61ms
step:2071/2270 train_time:125529ms step_avg:60.61ms
step:2072/2270 train_time:125589ms step_avg:60.61ms
step:2073/2270 train_time:125652ms step_avg:60.61ms
step:2074/2270 train_time:125711ms step_avg:60.61ms
step:2075/2270 train_time:125774ms step_avg:60.61ms
step:2076/2270 train_time:125834ms step_avg:60.61ms
step:2077/2270 train_time:125896ms step_avg:60.61ms
step:2078/2270 train_time:125956ms step_avg:60.61ms
step:2079/2270 train_time:126019ms step_avg:60.62ms
step:2080/2270 train_time:126079ms step_avg:60.62ms
step:2081/2270 train_time:126142ms step_avg:60.62ms
step:2082/2270 train_time:126203ms step_avg:60.62ms
step:2083/2270 train_time:126266ms step_avg:60.62ms
step:2084/2270 train_time:126326ms step_avg:60.62ms
step:2085/2270 train_time:126389ms step_avg:60.62ms
step:2086/2270 train_time:126449ms step_avg:60.62ms
step:2087/2270 train_time:126511ms step_avg:60.62ms
step:2088/2270 train_time:126571ms step_avg:60.62ms
step:2089/2270 train_time:126634ms step_avg:60.62ms
step:2090/2270 train_time:126693ms step_avg:60.62ms
step:2091/2270 train_time:126756ms step_avg:60.62ms
step:2092/2270 train_time:126816ms step_avg:60.62ms
step:2093/2270 train_time:126879ms step_avg:60.62ms
step:2094/2270 train_time:126938ms step_avg:60.62ms
step:2095/2270 train_time:127001ms step_avg:60.62ms
step:2096/2270 train_time:127061ms step_avg:60.62ms
step:2097/2270 train_time:127124ms step_avg:60.62ms
step:2098/2270 train_time:127184ms step_avg:60.62ms
step:2099/2270 train_time:127247ms step_avg:60.62ms
step:2100/2270 train_time:127307ms step_avg:60.62ms
step:2101/2270 train_time:127370ms step_avg:60.62ms
step:2102/2270 train_time:127430ms step_avg:60.62ms
step:2103/2270 train_time:127493ms step_avg:60.62ms
step:2104/2270 train_time:127553ms step_avg:60.62ms
step:2105/2270 train_time:127616ms step_avg:60.63ms
step:2106/2270 train_time:127677ms step_avg:60.63ms
step:2107/2270 train_time:127740ms step_avg:60.63ms
step:2108/2270 train_time:127800ms step_avg:60.63ms
step:2109/2270 train_time:127863ms step_avg:60.63ms
step:2110/2270 train_time:127923ms step_avg:60.63ms
step:2111/2270 train_time:127986ms step_avg:60.63ms
step:2112/2270 train_time:128045ms step_avg:60.63ms
step:2113/2270 train_time:128108ms step_avg:60.63ms
step:2114/2270 train_time:128168ms step_avg:60.63ms
step:2115/2270 train_time:128230ms step_avg:60.63ms
step:2116/2270 train_time:128290ms step_avg:60.63ms
step:2117/2270 train_time:128353ms step_avg:60.63ms
step:2118/2270 train_time:128413ms step_avg:60.63ms
step:2119/2270 train_time:128476ms step_avg:60.63ms
step:2120/2270 train_time:128537ms step_avg:60.63ms
step:2121/2270 train_time:128599ms step_avg:60.63ms
step:2122/2270 train_time:128660ms step_avg:60.63ms
step:2123/2270 train_time:128723ms step_avg:60.63ms
step:2124/2270 train_time:128783ms step_avg:60.63ms
step:2125/2270 train_time:128846ms step_avg:60.63ms
step:2126/2270 train_time:128905ms step_avg:60.63ms
step:2127/2270 train_time:128968ms step_avg:60.63ms
step:2128/2270 train_time:129027ms step_avg:60.63ms
step:2129/2270 train_time:129089ms step_avg:60.63ms
step:2130/2270 train_time:129149ms step_avg:60.63ms
step:2131/2270 train_time:129211ms step_avg:60.63ms
step:2132/2270 train_time:129272ms step_avg:60.63ms
step:2133/2270 train_time:129334ms step_avg:60.63ms
step:2134/2270 train_time:129393ms step_avg:60.63ms
step:2135/2270 train_time:129456ms step_avg:60.63ms
step:2136/2270 train_time:129515ms step_avg:60.63ms
step:2137/2270 train_time:129578ms step_avg:60.64ms
step:2138/2270 train_time:129638ms step_avg:60.64ms
step:2139/2270 train_time:129702ms step_avg:60.64ms
step:2140/2270 train_time:129762ms step_avg:60.64ms
step:2141/2270 train_time:129825ms step_avg:60.64ms
step:2142/2270 train_time:129885ms step_avg:60.64ms
step:2143/2270 train_time:129948ms step_avg:60.64ms
step:2144/2270 train_time:130007ms step_avg:60.64ms
step:2145/2270 train_time:130070ms step_avg:60.64ms
step:2146/2270 train_time:130130ms step_avg:60.64ms
step:2147/2270 train_time:130193ms step_avg:60.64ms
step:2148/2270 train_time:130253ms step_avg:60.64ms
step:2149/2270 train_time:130315ms step_avg:60.64ms
step:2150/2270 train_time:130376ms step_avg:60.64ms
step:2151/2270 train_time:130439ms step_avg:60.64ms
step:2152/2270 train_time:130499ms step_avg:60.64ms
step:2153/2270 train_time:130563ms step_avg:60.64ms
step:2154/2270 train_time:130623ms step_avg:60.64ms
step:2155/2270 train_time:130686ms step_avg:60.64ms
step:2156/2270 train_time:130745ms step_avg:60.64ms
step:2157/2270 train_time:130808ms step_avg:60.64ms
step:2158/2270 train_time:130868ms step_avg:60.64ms
step:2159/2270 train_time:130931ms step_avg:60.64ms
step:2160/2270 train_time:130991ms step_avg:60.64ms
step:2161/2270 train_time:131053ms step_avg:60.64ms
step:2162/2270 train_time:131113ms step_avg:60.64ms
step:2163/2270 train_time:131175ms step_avg:60.65ms
step:2164/2270 train_time:131235ms step_avg:60.64ms
step:2165/2270 train_time:131298ms step_avg:60.65ms
step:2166/2270 train_time:131358ms step_avg:60.65ms
step:2167/2270 train_time:131421ms step_avg:60.65ms
step:2168/2270 train_time:131481ms step_avg:60.65ms
step:2169/2270 train_time:131545ms step_avg:60.65ms
step:2170/2270 train_time:131605ms step_avg:60.65ms
step:2171/2270 train_time:131668ms step_avg:60.65ms
step:2172/2270 train_time:131728ms step_avg:60.65ms
step:2173/2270 train_time:131790ms step_avg:60.65ms
step:2174/2270 train_time:131850ms step_avg:60.65ms
step:2175/2270 train_time:131913ms step_avg:60.65ms
step:2176/2270 train_time:131972ms step_avg:60.65ms
step:2177/2270 train_time:132035ms step_avg:60.65ms
step:2178/2270 train_time:132095ms step_avg:60.65ms
step:2179/2270 train_time:132158ms step_avg:60.65ms
step:2180/2270 train_time:132218ms step_avg:60.65ms
step:2181/2270 train_time:132281ms step_avg:60.65ms
step:2182/2270 train_time:132341ms step_avg:60.65ms
step:2183/2270 train_time:132404ms step_avg:60.65ms
step:2184/2270 train_time:132464ms step_avg:60.65ms
step:2185/2270 train_time:132527ms step_avg:60.65ms
step:2186/2270 train_time:132587ms step_avg:60.65ms
step:2187/2270 train_time:132650ms step_avg:60.65ms
step:2188/2270 train_time:132710ms step_avg:60.65ms
step:2189/2270 train_time:132772ms step_avg:60.65ms
step:2190/2270 train_time:132832ms step_avg:60.65ms
step:2191/2270 train_time:132895ms step_avg:60.65ms
step:2192/2270 train_time:132955ms step_avg:60.65ms
step:2193/2270 train_time:133018ms step_avg:60.66ms
step:2194/2270 train_time:133078ms step_avg:60.66ms
step:2195/2270 train_time:133140ms step_avg:60.66ms
step:2196/2270 train_time:133201ms step_avg:60.66ms
step:2197/2270 train_time:133263ms step_avg:60.66ms
step:2198/2270 train_time:133323ms step_avg:60.66ms
step:2199/2270 train_time:133386ms step_avg:60.66ms
step:2200/2270 train_time:133446ms step_avg:60.66ms
step:2201/2270 train_time:133508ms step_avg:60.66ms
step:2202/2270 train_time:133568ms step_avg:60.66ms
step:2203/2270 train_time:133631ms step_avg:60.66ms
step:2204/2270 train_time:133691ms step_avg:60.66ms
step:2205/2270 train_time:133754ms step_avg:60.66ms
step:2206/2270 train_time:133813ms step_avg:60.66ms
step:2207/2270 train_time:133876ms step_avg:60.66ms
step:2208/2270 train_time:133936ms step_avg:60.66ms
step:2209/2270 train_time:133999ms step_avg:60.66ms
step:2210/2270 train_time:134060ms step_avg:60.66ms
step:2211/2270 train_time:134123ms step_avg:60.66ms
step:2212/2270 train_time:134183ms step_avg:60.66ms
step:2213/2270 train_time:134246ms step_avg:60.66ms
step:2214/2270 train_time:134306ms step_avg:60.66ms
step:2215/2270 train_time:134368ms step_avg:60.66ms
step:2216/2270 train_time:134428ms step_avg:60.66ms
step:2217/2270 train_time:134490ms step_avg:60.66ms
step:2218/2270 train_time:134549ms step_avg:60.66ms
step:2219/2270 train_time:134612ms step_avg:60.66ms
step:2220/2270 train_time:134672ms step_avg:60.66ms
step:2221/2270 train_time:134735ms step_avg:60.66ms
step:2222/2270 train_time:134794ms step_avg:60.66ms
step:2223/2270 train_time:134857ms step_avg:60.66ms
step:2224/2270 train_time:134918ms step_avg:60.66ms
step:2225/2270 train_time:134981ms step_avg:60.67ms
step:2226/2270 train_time:135041ms step_avg:60.67ms
step:2227/2270 train_time:135105ms step_avg:60.67ms
step:2228/2270 train_time:135165ms step_avg:60.67ms
step:2229/2270 train_time:135227ms step_avg:60.67ms
step:2230/2270 train_time:135287ms step_avg:60.67ms
step:2231/2270 train_time:135350ms step_avg:60.67ms
step:2232/2270 train_time:135410ms step_avg:60.67ms
step:2233/2270 train_time:135473ms step_avg:60.67ms
step:2234/2270 train_time:135532ms step_avg:60.67ms
step:2235/2270 train_time:135595ms step_avg:60.67ms
step:2236/2270 train_time:135655ms step_avg:60.67ms
step:2237/2270 train_time:135717ms step_avg:60.67ms
step:2238/2270 train_time:135777ms step_avg:60.67ms
step:2239/2270 train_time:135840ms step_avg:60.67ms
step:2240/2270 train_time:135900ms step_avg:60.67ms
step:2241/2270 train_time:135963ms step_avg:60.67ms
step:2242/2270 train_time:136023ms step_avg:60.67ms
step:2243/2270 train_time:136086ms step_avg:60.67ms
step:2244/2270 train_time:136146ms step_avg:60.67ms
step:2245/2270 train_time:136209ms step_avg:60.67ms
step:2246/2270 train_time:136269ms step_avg:60.67ms
step:2247/2270 train_time:136331ms step_avg:60.67ms
step:2248/2270 train_time:136391ms step_avg:60.67ms
step:2249/2270 train_time:136454ms step_avg:60.67ms
step:2250/2270 train_time:136514ms step_avg:60.67ms
step:2250/2270 val_loss:3.2811 train_time:136578ms step_avg:60.70ms
step:2251/2270 train_time:136597ms step_avg:60.68ms
step:2252/2270 train_time:136642ms step_avg:60.68ms
step:2253/2270 train_time:136707ms step_avg:60.68ms
step:2254/2270 train_time:136768ms step_avg:60.68ms
step:2255/2270 train_time:136831ms step_avg:60.68ms
step:2256/2270 train_time:136892ms step_avg:60.68ms
step:2257/2270 train_time:136954ms step_avg:60.68ms
step:2258/2270 train_time:137014ms step_avg:60.68ms
step:2259/2270 train_time:137076ms step_avg:60.68ms
step:2260/2270 train_time:137135ms step_avg:60.68ms
step:2261/2270 train_time:137198ms step_avg:60.68ms
step:2262/2270 train_time:137258ms step_avg:60.68ms
step:2263/2270 train_time:137320ms step_avg:60.68ms
step:2264/2270 train_time:137379ms step_avg:60.68ms
step:2265/2270 train_time:137441ms step_avg:60.68ms
step:2266/2270 train_time:137501ms step_avg:60.68ms
step:2267/2270 train_time:137565ms step_avg:60.68ms
step:2268/2270 train_time:137626ms step_avg:60.68ms
step:2269/2270 train_time:137689ms step_avg:60.68ms
step:2270/2270 train_time:137751ms step_avg:60.68ms
step:2270/2270 val_loss:3.2764 train_time:137816ms step_avg:60.71ms
peak memory allocated: 29626 MiB reserved: 50528 MiB

import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn, autocast
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
#import wandb

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)

        params = list(params)
        sizes = {p.shape for p in params}

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params,))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * self.world_size
            for base_i in range(0, len(params), self.world_size):
                if base_i + self.rank < len(params):
                    grad = params[base_i + self.rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + self.world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * self.world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), self.world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                futures.append(dist.all_gather(params_pad[base_i:base_i + self.world_size], params_pad[base_i + self.rank], async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01, rank: int = 0, world_size: int = 1):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        self.rank = rank
        self.world_size = world_size

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(
                params=group_params,
            ))
        super().__init__(param_groups, defaults)

    @torch.compile
    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // self.world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)

                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']

                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)

                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t

                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        for param in self.embed.parameters():
            param.lr_mul = 75.
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embeds in self.value_embeds:
            for param in self.value_embeds.parameters():
                param.lr_mul = 75.
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.lr_mul = 27.5
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % world_size
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            #return causal_mask
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, world_size: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos:pos+max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()

    starts = []
    batch_end=None
    for i in range(len(boundary_positions) - 1):
        end = boundary_positions[i + 1].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == world_size:
                batch_end = end
                break
            start = end
    assert batch_end is not None # increase max_batch_span if necessary
    batch_span = batch_end-pos
    return starts, batch_span

def distributed_data_generator(filename_pattern: str, batch_size: int, rank: int, world_size: int, align_to_bos: bool):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    batch_span = batch_size
    max_batch_span = 2*batch_size if align_to_bos else batch_size #provide buffer to handle samples up to length local_batch_size

    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, world_size, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    train_align_to_bos = True # align local batch start indicies with next bos_token
    val_align_to_bos = False # False to maintain same eval as prior records
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

#if master_process:
#    wandb.init(project="modded-nanogpt-tiny", name=f"run-{os.path.basename(__file__)}", save_code=True)

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=True):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
if master_process:
    print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=next_multiple_of_n(50257, n=128), num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094

optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0, rank=rank, world_size=world_size)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]

for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

for n, p in model.named_parameters():
    wd_mul = getattr(p, "wd_mul", 1.0)
    lr_mul = getattr(p, "lr_mul", 1.0)

    print0(f"{n}: {p.shape} {p.dtype} {wd_mul} {lr_mul}")

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
embedding_params = sum(p.numel() for n, p in model.named_parameters() if "embed" in n)
non_embedding_params = total_params - embedding_params

print0(f"")
print0(f"Model parameters:")
print0(f"  Total parameters: {total_params:,}")
print0(f"  Embedding parameters: {embedding_params:,}")
print0(f"  Non-embedding parameters: {non_embedding_params:,}")

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    w = min((1 - x) / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.05
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)

torch.cuda.synchronize()
dist.barrier()
with torch.profiler.profile() as prof:
    for _ in range(warmup_steps):
        inputs, targets = next(train_loader)
        model(inputs, targets, get_window_size_blocks(1)).backward()
        for opt in optimizers:
            opt.step()
    model.zero_grad(set_to_none=True)
    torch.cuda.synchronize()
    dist.barrier()
os.makedirs("traces", exist_ok=True)
prof.export_chrome_trace(f"traces/trace_{rank}.json")

model.load_state_dict(initial_state['model'])
for opt, opt_state in zip(optimizers, initial_state['optimizers']):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size, args.val_align_to_bos)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        #if master_process:
        #    wandb.log({"val/loss": val_loss}, step=step)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.8.0.dev20250524+cu126 compiled for CUDA 12.6
Sun Jul 13 01:22:15 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    5856MiB /  81559MiB |      3%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   36C    P0            121W /  700W |    1517MiB /  81559MiB |      2%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   38C    P0            125W /  700W |    1517MiB /  81559MiB |      4%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   31C    P0            118W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   32C    P0            121W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   38C    P0            120W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   38C    P0            122W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   31C    P0            115W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           79661      C   /usr/bin/python3                       1508MiB |
|    0   N/A  N/A           79662      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           79663      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           79664      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           79665      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           79666      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           79667      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           79668      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A           79662      C   /usr/bin/python3                       1508MiB |
|    2   N/A  N/A           79663      C   /usr/bin/python3                       1508MiB |
|    3   N/A  N/A           79664      C   /usr/bin/python3                       1508MiB |
|    4   N/A  N/A           79665      C   /usr/bin/python3                       1508MiB |
|    5   N/A  N/A           79666      C   /usr/bin/python3                       1508MiB |
|    6   N/A  N/A           79667      C   /usr/bin/python3                       1508MiB |
|    7   N/A  N/A           79668      C   /usr/bin/python3                       1508MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
scalars: torch.Size([64]) torch.float32 1.0 5.0
embed.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.0.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.1.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.2.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
blocks.0.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.0.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.1.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.1.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.2.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.2.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.3.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.3.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.4.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.4.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.5.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.5.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.6.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.6.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.7.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.7.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.8.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.8.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.9.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.9.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.10.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.10.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.11.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.11.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
lm_head.weight: torch.Size([50304, 768]) torch.float32 1.0 27.5

Model parameters:
  Total parameters: 275,742,784
  Embedding parameters: 154,533,888
  Non-embedding parameters: 121,208,896
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.01ms
step:1/1750 train_time:149ms step_avg:149.39ms
step:2/1750 train_time:175ms step_avg:87.29ms
step:3/1750 train_time:248ms step_avg:82.66ms
step:4/1750 train_time:338ms step_avg:84.57ms
step:5/1750 train_time:430ms step_avg:86.08ms
step:6/1750 train_time:523ms step_avg:87.20ms
step:7/1750 train_time:616ms step_avg:87.95ms
step:8/1750 train_time:708ms step_avg:88.54ms
step:9/1750 train_time:800ms step_avg:88.91ms
step:10/1750 train_time:892ms step_avg:89.24ms
step:11/1750 train_time:985ms step_avg:89.56ms
step:12/1750 train_time:1079ms step_avg:89.91ms
step:13/1750 train_time:1174ms step_avg:90.27ms
step:14/1750 train_time:1268ms step_avg:90.60ms
step:15/1750 train_time:1361ms step_avg:90.73ms
step:16/1750 train_time:1455ms step_avg:90.91ms
step:17/1750 train_time:1548ms step_avg:91.07ms
step:18/1750 train_time:1642ms step_avg:91.20ms
step:19/1750 train_time:1734ms step_avg:91.27ms
step:20/1750 train_time:1827ms step_avg:91.36ms
step:21/1750 train_time:1920ms step_avg:91.43ms
step:22/1750 train_time:2013ms step_avg:91.50ms
step:23/1750 train_time:2107ms step_avg:91.59ms
step:24/1750 train_time:2200ms step_avg:91.66ms
step:25/1750 train_time:2294ms step_avg:91.75ms
step:26/1750 train_time:2387ms step_avg:91.82ms
step:27/1750 train_time:2480ms step_avg:91.85ms
step:28/1750 train_time:2574ms step_avg:91.92ms
step:29/1750 train_time:2667ms step_avg:91.95ms
step:30/1750 train_time:2759ms step_avg:91.98ms
step:31/1750 train_time:2853ms step_avg:92.02ms
step:32/1750 train_time:2946ms step_avg:92.07ms
step:33/1750 train_time:3039ms step_avg:92.09ms
step:34/1750 train_time:3133ms step_avg:92.14ms
step:35/1750 train_time:3227ms step_avg:92.19ms
step:36/1750 train_time:3320ms step_avg:92.21ms
step:37/1750 train_time:3413ms step_avg:92.25ms
step:38/1750 train_time:3507ms step_avg:92.28ms
step:39/1750 train_time:3600ms step_avg:92.30ms
step:40/1750 train_time:3693ms step_avg:92.32ms
step:41/1750 train_time:3786ms step_avg:92.34ms
step:42/1750 train_time:3878ms step_avg:92.34ms
step:43/1750 train_time:3971ms step_avg:92.35ms
step:44/1750 train_time:4064ms step_avg:92.36ms
step:45/1750 train_time:4158ms step_avg:92.39ms
step:46/1750 train_time:4252ms step_avg:92.43ms
step:47/1750 train_time:4346ms step_avg:92.46ms
step:48/1750 train_time:4439ms step_avg:92.47ms
step:49/1750 train_time:4532ms step_avg:92.49ms
step:50/1750 train_time:4626ms step_avg:92.52ms
step:51/1750 train_time:4719ms step_avg:92.53ms
step:52/1750 train_time:4812ms step_avg:92.54ms
step:53/1750 train_time:4905ms step_avg:92.56ms
step:54/1750 train_time:4999ms step_avg:92.57ms
step:55/1750 train_time:5092ms step_avg:92.58ms
step:56/1750 train_time:5187ms step_avg:92.63ms
step:57/1750 train_time:5278ms step_avg:92.60ms
step:58/1750 train_time:5372ms step_avg:92.63ms
step:59/1750 train_time:5466ms step_avg:92.65ms
step:60/1750 train_time:5559ms step_avg:92.65ms
step:61/1750 train_time:5653ms step_avg:92.67ms
step:62/1750 train_time:5747ms step_avg:92.69ms
step:63/1750 train_time:5840ms step_avg:92.71ms
step:64/1750 train_time:5933ms step_avg:92.71ms
step:65/1750 train_time:6026ms step_avg:92.71ms
step:66/1750 train_time:6119ms step_avg:92.72ms
step:67/1750 train_time:6212ms step_avg:92.72ms
step:68/1750 train_time:6306ms step_avg:92.73ms
step:69/1750 train_time:6399ms step_avg:92.74ms
step:70/1750 train_time:6493ms step_avg:92.75ms
step:71/1750 train_time:6586ms step_avg:92.77ms
step:72/1750 train_time:6679ms step_avg:92.77ms
step:73/1750 train_time:6773ms step_avg:92.78ms
step:74/1750 train_time:6866ms step_avg:92.79ms
step:75/1750 train_time:6959ms step_avg:92.79ms
step:76/1750 train_time:7052ms step_avg:92.79ms
step:77/1750 train_time:7145ms step_avg:92.79ms
step:78/1750 train_time:7238ms step_avg:92.80ms
step:79/1750 train_time:7332ms step_avg:92.81ms
step:80/1750 train_time:7425ms step_avg:92.82ms
step:81/1750 train_time:7518ms step_avg:92.82ms
step:82/1750 train_time:7611ms step_avg:92.82ms
step:83/1750 train_time:7705ms step_avg:92.84ms
step:84/1750 train_time:7798ms step_avg:92.84ms
step:85/1750 train_time:7892ms step_avg:92.84ms
step:86/1750 train_time:7985ms step_avg:92.85ms
step:87/1750 train_time:8078ms step_avg:92.85ms
step:88/1750 train_time:8171ms step_avg:92.86ms
step:89/1750 train_time:8265ms step_avg:92.87ms
step:90/1750 train_time:8358ms step_avg:92.87ms
step:91/1750 train_time:8452ms step_avg:92.87ms
step:92/1750 train_time:8545ms step_avg:92.88ms
step:93/1750 train_time:8639ms step_avg:92.89ms
step:94/1750 train_time:8733ms step_avg:92.90ms
step:95/1750 train_time:8826ms step_avg:92.91ms
step:96/1750 train_time:8919ms step_avg:92.91ms
step:97/1750 train_time:9013ms step_avg:92.92ms
step:98/1750 train_time:9106ms step_avg:92.92ms
step:99/1750 train_time:9199ms step_avg:92.92ms
step:100/1750 train_time:9292ms step_avg:92.92ms
step:101/1750 train_time:9386ms step_avg:92.93ms
step:102/1750 train_time:9479ms step_avg:92.93ms
step:103/1750 train_time:9572ms step_avg:92.93ms
step:104/1750 train_time:9665ms step_avg:92.94ms
step:105/1750 train_time:9758ms step_avg:92.93ms
step:106/1750 train_time:9852ms step_avg:92.94ms
step:107/1750 train_time:9945ms step_avg:92.95ms
step:108/1750 train_time:10039ms step_avg:92.95ms
step:109/1750 train_time:10132ms step_avg:92.95ms
step:110/1750 train_time:10226ms step_avg:92.96ms
step:111/1750 train_time:10319ms step_avg:92.97ms
step:112/1750 train_time:10412ms step_avg:92.97ms
step:113/1750 train_time:10506ms step_avg:92.98ms
step:114/1750 train_time:10599ms step_avg:92.97ms
step:115/1750 train_time:10692ms step_avg:92.98ms
step:116/1750 train_time:10786ms step_avg:92.98ms
step:117/1750 train_time:10878ms step_avg:92.98ms
step:118/1750 train_time:10972ms step_avg:92.98ms
step:119/1750 train_time:11065ms step_avg:92.98ms
step:120/1750 train_time:11158ms step_avg:92.99ms
step:121/1750 train_time:11252ms step_avg:92.99ms
step:122/1750 train_time:11346ms step_avg:93.00ms
step:123/1750 train_time:11439ms step_avg:93.00ms
step:124/1750 train_time:11532ms step_avg:93.00ms
step:125/1750 train_time:11626ms step_avg:93.01ms
step:125/1750 val_loss:4.6370 train_time:11714ms step_avg:93.71ms
step:126/1750 train_time:11741ms step_avg:93.18ms
step:127/1750 train_time:11819ms step_avg:93.06ms
step:128/1750 train_time:11919ms step_avg:93.12ms
step:129/1750 train_time:12013ms step_avg:93.13ms
step:130/1750 train_time:12107ms step_avg:93.13ms
step:131/1750 train_time:12198ms step_avg:93.12ms
step:132/1750 train_time:12291ms step_avg:93.12ms
step:133/1750 train_time:12384ms step_avg:93.11ms
step:134/1750 train_time:12477ms step_avg:93.11ms
step:135/1750 train_time:12571ms step_avg:93.12ms
step:136/1750 train_time:12664ms step_avg:93.12ms
step:137/1750 train_time:12758ms step_avg:93.13ms
step:138/1750 train_time:12853ms step_avg:93.14ms
step:139/1750 train_time:12948ms step_avg:93.15ms
step:140/1750 train_time:13042ms step_avg:93.16ms
step:141/1750 train_time:13136ms step_avg:93.17ms
step:142/1750 train_time:13230ms step_avg:93.17ms
step:143/1750 train_time:13324ms step_avg:93.17ms
step:144/1750 train_time:13417ms step_avg:93.17ms
step:145/1750 train_time:13510ms step_avg:93.17ms
step:146/1750 train_time:13603ms step_avg:93.17ms
step:147/1750 train_time:13696ms step_avg:93.17ms
step:148/1750 train_time:13792ms step_avg:93.19ms
step:149/1750 train_time:13886ms step_avg:93.20ms
step:150/1750 train_time:13980ms step_avg:93.20ms
step:151/1750 train_time:14074ms step_avg:93.21ms
step:152/1750 train_time:14169ms step_avg:93.22ms
step:153/1750 train_time:14262ms step_avg:93.22ms
step:154/1750 train_time:14356ms step_avg:93.22ms
step:155/1750 train_time:14449ms step_avg:93.22ms
step:156/1750 train_time:14542ms step_avg:93.22ms
step:157/1750 train_time:14635ms step_avg:93.22ms
step:158/1750 train_time:14728ms step_avg:93.22ms
step:159/1750 train_time:14822ms step_avg:93.22ms
step:160/1750 train_time:14915ms step_avg:93.22ms
step:161/1750 train_time:15010ms step_avg:93.23ms
step:162/1750 train_time:15103ms step_avg:93.23ms
step:163/1750 train_time:15198ms step_avg:93.24ms
step:164/1750 train_time:15291ms step_avg:93.24ms
step:165/1750 train_time:15385ms step_avg:93.24ms
step:166/1750 train_time:15477ms step_avg:93.24ms
step:167/1750 train_time:15571ms step_avg:93.24ms
step:168/1750 train_time:15665ms step_avg:93.24ms
step:169/1750 train_time:15759ms step_avg:93.25ms
step:170/1750 train_time:15853ms step_avg:93.25ms
step:171/1750 train_time:15947ms step_avg:93.26ms
step:172/1750 train_time:16041ms step_avg:93.26ms
step:173/1750 train_time:16135ms step_avg:93.26ms
step:174/1750 train_time:16229ms step_avg:93.27ms
step:175/1750 train_time:16322ms step_avg:93.27ms
step:176/1750 train_time:16415ms step_avg:93.27ms
step:177/1750 train_time:16508ms step_avg:93.27ms
step:178/1750 train_time:16602ms step_avg:93.27ms
step:179/1750 train_time:16695ms step_avg:93.27ms
step:180/1750 train_time:16789ms step_avg:93.27ms
step:181/1750 train_time:16882ms step_avg:93.27ms
step:182/1750 train_time:16975ms step_avg:93.27ms
step:183/1750 train_time:17069ms step_avg:93.27ms
step:184/1750 train_time:17163ms step_avg:93.28ms
step:185/1750 train_time:17257ms step_avg:93.28ms
step:186/1750 train_time:17351ms step_avg:93.29ms
step:187/1750 train_time:17445ms step_avg:93.29ms
step:188/1750 train_time:17538ms step_avg:93.29ms
step:189/1750 train_time:17632ms step_avg:93.29ms
step:190/1750 train_time:17726ms step_avg:93.29ms
step:191/1750 train_time:17820ms step_avg:93.30ms
step:192/1750 train_time:17913ms step_avg:93.30ms
step:193/1750 train_time:18006ms step_avg:93.30ms
step:194/1750 train_time:18100ms step_avg:93.30ms
step:195/1750 train_time:18193ms step_avg:93.30ms
step:196/1750 train_time:18287ms step_avg:93.30ms
step:197/1750 train_time:18380ms step_avg:93.30ms
step:198/1750 train_time:18474ms step_avg:93.30ms
step:199/1750 train_time:18568ms step_avg:93.31ms
step:200/1750 train_time:18661ms step_avg:93.30ms
step:201/1750 train_time:18755ms step_avg:93.31ms
step:202/1750 train_time:18848ms step_avg:93.31ms
step:203/1750 train_time:18941ms step_avg:93.31ms
step:204/1750 train_time:19035ms step_avg:93.31ms
step:205/1750 train_time:19129ms step_avg:93.31ms
step:206/1750 train_time:19222ms step_avg:93.31ms
step:207/1750 train_time:19316ms step_avg:93.31ms
step:208/1750 train_time:19409ms step_avg:93.31ms
step:209/1750 train_time:19503ms step_avg:93.32ms
step:210/1750 train_time:19597ms step_avg:93.32ms
step:211/1750 train_time:19691ms step_avg:93.32ms
step:212/1750 train_time:19785ms step_avg:93.32ms
step:213/1750 train_time:19878ms step_avg:93.32ms
step:214/1750 train_time:19972ms step_avg:93.33ms
step:215/1750 train_time:20066ms step_avg:93.33ms
step:216/1750 train_time:20160ms step_avg:93.34ms
step:217/1750 train_time:20254ms step_avg:93.34ms
step:218/1750 train_time:20348ms step_avg:93.34ms
step:219/1750 train_time:20441ms step_avg:93.34ms
step:220/1750 train_time:20535ms step_avg:93.34ms
step:221/1750 train_time:20628ms step_avg:93.34ms
step:222/1750 train_time:20721ms step_avg:93.34ms
step:223/1750 train_time:20815ms step_avg:93.34ms
step:224/1750 train_time:20908ms step_avg:93.34ms
step:225/1750 train_time:21002ms step_avg:93.34ms
step:226/1750 train_time:21096ms step_avg:93.34ms
step:227/1750 train_time:21189ms step_avg:93.34ms
step:228/1750 train_time:21283ms step_avg:93.35ms
step:229/1750 train_time:21377ms step_avg:93.35ms
step:230/1750 train_time:21472ms step_avg:93.36ms
step:231/1750 train_time:21565ms step_avg:93.36ms
step:232/1750 train_time:21659ms step_avg:93.36ms
step:233/1750 train_time:21753ms step_avg:93.36ms
step:234/1750 train_time:21846ms step_avg:93.36ms
step:235/1750 train_time:21940ms step_avg:93.36ms
step:236/1750 train_time:22034ms step_avg:93.36ms
step:237/1750 train_time:22128ms step_avg:93.37ms
step:238/1750 train_time:22221ms step_avg:93.36ms
step:239/1750 train_time:22314ms step_avg:93.37ms
step:240/1750 train_time:22408ms step_avg:93.37ms
step:241/1750 train_time:22501ms step_avg:93.37ms
step:242/1750 train_time:22595ms step_avg:93.37ms
step:243/1750 train_time:22688ms step_avg:93.37ms
step:244/1750 train_time:22781ms step_avg:93.37ms
step:245/1750 train_time:22875ms step_avg:93.37ms
step:246/1750 train_time:22969ms step_avg:93.37ms
step:247/1750 train_time:23063ms step_avg:93.37ms
step:248/1750 train_time:23156ms step_avg:93.37ms
step:249/1750 train_time:23250ms step_avg:93.37ms
step:250/1750 train_time:23344ms step_avg:93.38ms
step:250/1750 val_loss:4.0940 train_time:23433ms step_avg:93.73ms
step:251/1750 train_time:23459ms step_avg:93.46ms
step:252/1750 train_time:23539ms step_avg:93.41ms
step:253/1750 train_time:23639ms step_avg:93.43ms
step:254/1750 train_time:23733ms step_avg:93.44ms
step:255/1750 train_time:23826ms step_avg:93.44ms
step:256/1750 train_time:23920ms step_avg:93.44ms
step:257/1750 train_time:24013ms step_avg:93.44ms
step:258/1750 train_time:24106ms step_avg:93.43ms
step:259/1750 train_time:24199ms step_avg:93.43ms
step:260/1750 train_time:24292ms step_avg:93.43ms
step:261/1750 train_time:24386ms step_avg:93.43ms
step:262/1750 train_time:24483ms step_avg:93.45ms
step:263/1750 train_time:24579ms step_avg:93.46ms
step:264/1750 train_time:24675ms step_avg:93.46ms
step:265/1750 train_time:24769ms step_avg:93.47ms
step:266/1750 train_time:24863ms step_avg:93.47ms
step:267/1750 train_time:24958ms step_avg:93.47ms
step:268/1750 train_time:25051ms step_avg:93.47ms
step:269/1750 train_time:25145ms step_avg:93.47ms
step:270/1750 train_time:25239ms step_avg:93.48ms
step:271/1750 train_time:25332ms step_avg:93.48ms
step:272/1750 train_time:25426ms step_avg:93.48ms
step:273/1750 train_time:25521ms step_avg:93.48ms
step:274/1750 train_time:25616ms step_avg:93.49ms
step:275/1750 train_time:25711ms step_avg:93.49ms
step:276/1750 train_time:25806ms step_avg:93.50ms
step:277/1750 train_time:25901ms step_avg:93.50ms
step:278/1750 train_time:25995ms step_avg:93.51ms
step:279/1750 train_time:26089ms step_avg:93.51ms
step:280/1750 train_time:26183ms step_avg:93.51ms
step:281/1750 train_time:26276ms step_avg:93.51ms
step:282/1750 train_time:26370ms step_avg:93.51ms
step:283/1750 train_time:26464ms step_avg:93.51ms
step:284/1750 train_time:26559ms step_avg:93.52ms
step:285/1750 train_time:26654ms step_avg:93.52ms
step:286/1750 train_time:26749ms step_avg:93.53ms
step:287/1750 train_time:26844ms step_avg:93.53ms
step:288/1750 train_time:26938ms step_avg:93.54ms
step:289/1750 train_time:27032ms step_avg:93.54ms
step:290/1750 train_time:27126ms step_avg:93.54ms
step:291/1750 train_time:27221ms step_avg:93.54ms
step:292/1750 train_time:27315ms step_avg:93.55ms
step:293/1750 train_time:27409ms step_avg:93.55ms
step:294/1750 train_time:27503ms step_avg:93.55ms
step:295/1750 train_time:27599ms step_avg:93.56ms
step:296/1750 train_time:27693ms step_avg:93.56ms
step:297/1750 train_time:27788ms step_avg:93.56ms
step:298/1750 train_time:27882ms step_avg:93.56ms
step:299/1750 train_time:27976ms step_avg:93.57ms
step:300/1750 train_time:28070ms step_avg:93.57ms
step:301/1750 train_time:28164ms step_avg:93.57ms
step:302/1750 train_time:28258ms step_avg:93.57ms
step:303/1750 train_time:28352ms step_avg:93.57ms
step:304/1750 train_time:28446ms step_avg:93.57ms
step:305/1750 train_time:28540ms step_avg:93.58ms
step:306/1750 train_time:28635ms step_avg:93.58ms
step:307/1750 train_time:28729ms step_avg:93.58ms
step:308/1750 train_time:28823ms step_avg:93.58ms
step:309/1750 train_time:28917ms step_avg:93.58ms
step:310/1750 train_time:29011ms step_avg:93.59ms
step:311/1750 train_time:29106ms step_avg:93.59ms
step:312/1750 train_time:29200ms step_avg:93.59ms
step:313/1750 train_time:29295ms step_avg:93.59ms
step:314/1750 train_time:29389ms step_avg:93.60ms
step:315/1750 train_time:29483ms step_avg:93.60ms
step:316/1750 train_time:29577ms step_avg:93.60ms
step:317/1750 train_time:29670ms step_avg:93.60ms
step:318/1750 train_time:29766ms step_avg:93.60ms
step:319/1750 train_time:29860ms step_avg:93.60ms
step:320/1750 train_time:29954ms step_avg:93.61ms
step:321/1750 train_time:30048ms step_avg:93.61ms
step:322/1750 train_time:30142ms step_avg:93.61ms
step:323/1750 train_time:30235ms step_avg:93.61ms
step:324/1750 train_time:30329ms step_avg:93.61ms
step:325/1750 train_time:30423ms step_avg:93.61ms
step:326/1750 train_time:30517ms step_avg:93.61ms
step:327/1750 train_time:30611ms step_avg:93.61ms
step:328/1750 train_time:30705ms step_avg:93.61ms
step:329/1750 train_time:30799ms step_avg:93.61ms
step:330/1750 train_time:30893ms step_avg:93.62ms
step:331/1750 train_time:30988ms step_avg:93.62ms
step:332/1750 train_time:31082ms step_avg:93.62ms
step:333/1750 train_time:31175ms step_avg:93.62ms
step:334/1750 train_time:31269ms step_avg:93.62ms
step:335/1750 train_time:31363ms step_avg:93.62ms
step:336/1750 train_time:31457ms step_avg:93.62ms
step:337/1750 train_time:31550ms step_avg:93.62ms
step:338/1750 train_time:31645ms step_avg:93.62ms
step:339/1750 train_time:31739ms step_avg:93.62ms
step:340/1750 train_time:31832ms step_avg:93.62ms
step:341/1750 train_time:31926ms step_avg:93.62ms
step:342/1750 train_time:32020ms step_avg:93.63ms
step:343/1750 train_time:32114ms step_avg:93.63ms
step:344/1750 train_time:32208ms step_avg:93.63ms
step:345/1750 train_time:32303ms step_avg:93.63ms
step:346/1750 train_time:32397ms step_avg:93.63ms
step:347/1750 train_time:32490ms step_avg:93.63ms
step:348/1750 train_time:32584ms step_avg:93.63ms
step:349/1750 train_time:32678ms step_avg:93.63ms
step:350/1750 train_time:32773ms step_avg:93.64ms
step:351/1750 train_time:32867ms step_avg:93.64ms
step:352/1750 train_time:32962ms step_avg:93.64ms
step:353/1750 train_time:33057ms step_avg:93.65ms
step:354/1750 train_time:33150ms step_avg:93.65ms
step:355/1750 train_time:33244ms step_avg:93.65ms
step:356/1750 train_time:33340ms step_avg:93.65ms
step:357/1750 train_time:33434ms step_avg:93.65ms
step:358/1750 train_time:33528ms step_avg:93.65ms
step:359/1750 train_time:33623ms step_avg:93.66ms
step:360/1750 train_time:33717ms step_avg:93.66ms
step:361/1750 train_time:33812ms step_avg:93.66ms
step:362/1750 train_time:33905ms step_avg:93.66ms
step:363/1750 train_time:34000ms step_avg:93.66ms
step:364/1750 train_time:34094ms step_avg:93.67ms
step:365/1750 train_time:34189ms step_avg:93.67ms
step:366/1750 train_time:34282ms step_avg:93.67ms
step:367/1750 train_time:34376ms step_avg:93.67ms
step:368/1750 train_time:34470ms step_avg:93.67ms
step:369/1750 train_time:34564ms step_avg:93.67ms
step:370/1750 train_time:34657ms step_avg:93.67ms
step:371/1750 train_time:34751ms step_avg:93.67ms
step:372/1750 train_time:34845ms step_avg:93.67ms
step:373/1750 train_time:34939ms step_avg:93.67ms
step:374/1750 train_time:35034ms step_avg:93.67ms
step:375/1750 train_time:35127ms step_avg:93.67ms
step:375/1750 val_loss:3.8920 train_time:35217ms step_avg:93.91ms
step:376/1750 train_time:35243ms step_avg:93.73ms
step:377/1750 train_time:35324ms step_avg:93.70ms
step:378/1750 train_time:35421ms step_avg:93.71ms
step:379/1750 train_time:35515ms step_avg:93.71ms
step:380/1750 train_time:35609ms step_avg:93.71ms
step:381/1750 train_time:35703ms step_avg:93.71ms
step:382/1750 train_time:35797ms step_avg:93.71ms
step:383/1750 train_time:35890ms step_avg:93.71ms
step:384/1750 train_time:35984ms step_avg:93.71ms
step:385/1750 train_time:36078ms step_avg:93.71ms
step:386/1750 train_time:36171ms step_avg:93.71ms
step:387/1750 train_time:36266ms step_avg:93.71ms
step:388/1750 train_time:36362ms step_avg:93.72ms
step:389/1750 train_time:36456ms step_avg:93.72ms
step:390/1750 train_time:36551ms step_avg:93.72ms
step:391/1750 train_time:36647ms step_avg:93.73ms
step:392/1750 train_time:36743ms step_avg:93.73ms
step:393/1750 train_time:36839ms step_avg:93.74ms
step:394/1750 train_time:36935ms step_avg:93.74ms
step:395/1750 train_time:37031ms step_avg:93.75ms
step:396/1750 train_time:37127ms step_avg:93.75ms
step:397/1750 train_time:37223ms step_avg:93.76ms
step:398/1750 train_time:37319ms step_avg:93.77ms
step:399/1750 train_time:37416ms step_avg:93.77ms
step:400/1750 train_time:37513ms step_avg:93.78ms
step:401/1750 train_time:37609ms step_avg:93.79ms
step:402/1750 train_time:37706ms step_avg:93.80ms
step:403/1750 train_time:37802ms step_avg:93.80ms
step:404/1750 train_time:37898ms step_avg:93.81ms
step:405/1750 train_time:37994ms step_avg:93.81ms
step:406/1750 train_time:38090ms step_avg:93.82ms
step:407/1750 train_time:38187ms step_avg:93.83ms
step:408/1750 train_time:38283ms step_avg:93.83ms
step:409/1750 train_time:38379ms step_avg:93.84ms
step:410/1750 train_time:38476ms step_avg:93.84ms
step:411/1750 train_time:38571ms step_avg:93.85ms
step:412/1750 train_time:38668ms step_avg:93.85ms
step:413/1750 train_time:38764ms step_avg:93.86ms
step:414/1750 train_time:38860ms step_avg:93.87ms
step:415/1750 train_time:38957ms step_avg:93.87ms
step:416/1750 train_time:39053ms step_avg:93.88ms
step:417/1750 train_time:39149ms step_avg:93.88ms
step:418/1750 train_time:39245ms step_avg:93.89ms
step:419/1750 train_time:39342ms step_avg:93.89ms
step:420/1750 train_time:39438ms step_avg:93.90ms
step:421/1750 train_time:39534ms step_avg:93.90ms
step:422/1750 train_time:39631ms step_avg:93.91ms
step:423/1750 train_time:39728ms step_avg:93.92ms
step:424/1750 train_time:39824ms step_avg:93.93ms
step:425/1750 train_time:39921ms step_avg:93.93ms
step:426/1750 train_time:40018ms step_avg:93.94ms
step:427/1750 train_time:40114ms step_avg:93.94ms
step:428/1750 train_time:40211ms step_avg:93.95ms
step:429/1750 train_time:40307ms step_avg:93.96ms
step:430/1750 train_time:40404ms step_avg:93.96ms
step:431/1750 train_time:40500ms step_avg:93.97ms
step:432/1750 train_time:40596ms step_avg:93.97ms
step:433/1750 train_time:40692ms step_avg:93.98ms
step:434/1750 train_time:40789ms step_avg:93.98ms
step:435/1750 train_time:40886ms step_avg:93.99ms
step:436/1750 train_time:40983ms step_avg:94.00ms
step:437/1750 train_time:41080ms step_avg:94.00ms
step:438/1750 train_time:41176ms step_avg:94.01ms
step:439/1750 train_time:41272ms step_avg:94.01ms
step:440/1750 train_time:41369ms step_avg:94.02ms
step:441/1750 train_time:41465ms step_avg:94.03ms
step:442/1750 train_time:41562ms step_avg:94.03ms
step:443/1750 train_time:41658ms step_avg:94.04ms
step:444/1750 train_time:41755ms step_avg:94.04ms
step:445/1750 train_time:41851ms step_avg:94.05ms
step:446/1750 train_time:41948ms step_avg:94.05ms
step:447/1750 train_time:42045ms step_avg:94.06ms
step:448/1750 train_time:42141ms step_avg:94.06ms
step:449/1750 train_time:42237ms step_avg:94.07ms
step:450/1750 train_time:42333ms step_avg:94.07ms
step:451/1750 train_time:42429ms step_avg:94.08ms
step:452/1750 train_time:42526ms step_avg:94.08ms
step:453/1750 train_time:42622ms step_avg:94.09ms
step:454/1750 train_time:42719ms step_avg:94.09ms
step:455/1750 train_time:42815ms step_avg:94.10ms
step:456/1750 train_time:42912ms step_avg:94.10ms
step:457/1750 train_time:43008ms step_avg:94.11ms
step:458/1750 train_time:43105ms step_avg:94.11ms
step:459/1750 train_time:43202ms step_avg:94.12ms
step:460/1750 train_time:43298ms step_avg:94.13ms
step:461/1750 train_time:43394ms step_avg:94.13ms
step:462/1750 train_time:43490ms step_avg:94.13ms
step:463/1750 train_time:43586ms step_avg:94.14ms
step:464/1750 train_time:43683ms step_avg:94.14ms
step:465/1750 train_time:43779ms step_avg:94.15ms
step:466/1750 train_time:43875ms step_avg:94.15ms
step:467/1750 train_time:43972ms step_avg:94.16ms
step:468/1750 train_time:44068ms step_avg:94.16ms
step:469/1750 train_time:44164ms step_avg:94.17ms
step:470/1750 train_time:44261ms step_avg:94.17ms
step:471/1750 train_time:44358ms step_avg:94.18ms
step:472/1750 train_time:44454ms step_avg:94.18ms
step:473/1750 train_time:44550ms step_avg:94.19ms
step:474/1750 train_time:44647ms step_avg:94.19ms
step:475/1750 train_time:44743ms step_avg:94.20ms
step:476/1750 train_time:44840ms step_avg:94.20ms
step:477/1750 train_time:44936ms step_avg:94.20ms
step:478/1750 train_time:45032ms step_avg:94.21ms
step:479/1750 train_time:45129ms step_avg:94.21ms
step:480/1750 train_time:45225ms step_avg:94.22ms
step:481/1750 train_time:45322ms step_avg:94.22ms
step:482/1750 train_time:45419ms step_avg:94.23ms
step:483/1750 train_time:45515ms step_avg:94.23ms
step:484/1750 train_time:45611ms step_avg:94.24ms
step:485/1750 train_time:45708ms step_avg:94.24ms
step:486/1750 train_time:45805ms step_avg:94.25ms
step:487/1750 train_time:45901ms step_avg:94.25ms
step:488/1750 train_time:45997ms step_avg:94.26ms
step:489/1750 train_time:46093ms step_avg:94.26ms
step:490/1750 train_time:46189ms step_avg:94.26ms
step:491/1750 train_time:46286ms step_avg:94.27ms
step:492/1750 train_time:46382ms step_avg:94.27ms
step:493/1750 train_time:46478ms step_avg:94.28ms
step:494/1750 train_time:46575ms step_avg:94.28ms
step:495/1750 train_time:46672ms step_avg:94.29ms
step:496/1750 train_time:46768ms step_avg:94.29ms
step:497/1750 train_time:46864ms step_avg:94.29ms
step:498/1750 train_time:46961ms step_avg:94.30ms
step:499/1750 train_time:47056ms step_avg:94.30ms
step:500/1750 train_time:47152ms step_avg:94.30ms
step:500/1750 val_loss:3.7451 train_time:47243ms step_avg:94.49ms
step:501/1750 train_time:47269ms step_avg:94.35ms
step:502/1750 train_time:47357ms step_avg:94.34ms
step:503/1750 train_time:47457ms step_avg:94.35ms
step:504/1750 train_time:47554ms step_avg:94.35ms
step:505/1750 train_time:47651ms step_avg:94.36ms
step:506/1750 train_time:47746ms step_avg:94.36ms
step:507/1750 train_time:47842ms step_avg:94.36ms
step:508/1750 train_time:47938ms step_avg:94.37ms
step:509/1750 train_time:48033ms step_avg:94.37ms
step:510/1750 train_time:48128ms step_avg:94.37ms
step:511/1750 train_time:48224ms step_avg:94.37ms
step:512/1750 train_time:48321ms step_avg:94.38ms
step:513/1750 train_time:48419ms step_avg:94.38ms
step:514/1750 train_time:48516ms step_avg:94.39ms
step:515/1750 train_time:48613ms step_avg:94.39ms
step:516/1750 train_time:48709ms step_avg:94.40ms
step:517/1750 train_time:48806ms step_avg:94.40ms
step:518/1750 train_time:48901ms step_avg:94.40ms
step:519/1750 train_time:48997ms step_avg:94.41ms
step:520/1750 train_time:49093ms step_avg:94.41ms
step:521/1750 train_time:49190ms step_avg:94.41ms
step:522/1750 train_time:49287ms step_avg:94.42ms
step:523/1750 train_time:49384ms step_avg:94.42ms
step:524/1750 train_time:49481ms step_avg:94.43ms
step:525/1750 train_time:49578ms step_avg:94.44ms
step:526/1750 train_time:49675ms step_avg:94.44ms
step:527/1750 train_time:49772ms step_avg:94.44ms
step:528/1750 train_time:49870ms step_avg:94.45ms
step:529/1750 train_time:49966ms step_avg:94.45ms
step:530/1750 train_time:50062ms step_avg:94.46ms
step:531/1750 train_time:50159ms step_avg:94.46ms
step:532/1750 train_time:50255ms step_avg:94.46ms
step:533/1750 train_time:50353ms step_avg:94.47ms
step:534/1750 train_time:50450ms step_avg:94.48ms
step:535/1750 train_time:50547ms step_avg:94.48ms
step:536/1750 train_time:50644ms step_avg:94.49ms
step:537/1750 train_time:50740ms step_avg:94.49ms
step:538/1750 train_time:50836ms step_avg:94.49ms
step:539/1750 train_time:50934ms step_avg:94.50ms
step:540/1750 train_time:51031ms step_avg:94.50ms
step:541/1750 train_time:51128ms step_avg:94.51ms
step:542/1750 train_time:51224ms step_avg:94.51ms
step:543/1750 train_time:51321ms step_avg:94.51ms
step:544/1750 train_time:51418ms step_avg:94.52ms
step:545/1750 train_time:51515ms step_avg:94.52ms
step:546/1750 train_time:51612ms step_avg:94.53ms
step:547/1750 train_time:51709ms step_avg:94.53ms
step:548/1750 train_time:51805ms step_avg:94.54ms
step:549/1750 train_time:51902ms step_avg:94.54ms
step:550/1750 train_time:51999ms step_avg:94.54ms
step:551/1750 train_time:52096ms step_avg:94.55ms
step:552/1750 train_time:52193ms step_avg:94.55ms
step:553/1750 train_time:52290ms step_avg:94.56ms
step:554/1750 train_time:52387ms step_avg:94.56ms
step:555/1750 train_time:52483ms step_avg:94.56ms
step:556/1750 train_time:52580ms step_avg:94.57ms
step:557/1750 train_time:52677ms step_avg:94.57ms
step:558/1750 train_time:52774ms step_avg:94.58ms
step:559/1750 train_time:52871ms step_avg:94.58ms
step:560/1750 train_time:52968ms step_avg:94.59ms
step:561/1750 train_time:53064ms step_avg:94.59ms
step:562/1750 train_time:53160ms step_avg:94.59ms
step:563/1750 train_time:53257ms step_avg:94.59ms
step:564/1750 train_time:53353ms step_avg:94.60ms
step:565/1750 train_time:53450ms step_avg:94.60ms
step:566/1750 train_time:53548ms step_avg:94.61ms
step:567/1750 train_time:53645ms step_avg:94.61ms
step:568/1750 train_time:53741ms step_avg:94.62ms
step:569/1750 train_time:53838ms step_avg:94.62ms
step:570/1750 train_time:53935ms step_avg:94.62ms
step:571/1750 train_time:54032ms step_avg:94.63ms
step:572/1750 train_time:54128ms step_avg:94.63ms
step:573/1750 train_time:54225ms step_avg:94.63ms
step:574/1750 train_time:54322ms step_avg:94.64ms
step:575/1750 train_time:54419ms step_avg:94.64ms
step:576/1750 train_time:54515ms step_avg:94.64ms
step:577/1750 train_time:54613ms step_avg:94.65ms
step:578/1750 train_time:54710ms step_avg:94.65ms
step:579/1750 train_time:54806ms step_avg:94.66ms
step:580/1750 train_time:54903ms step_avg:94.66ms
step:581/1750 train_time:54999ms step_avg:94.66ms
step:582/1750 train_time:55096ms step_avg:94.67ms
step:583/1750 train_time:55193ms step_avg:94.67ms
step:584/1750 train_time:55290ms step_avg:94.67ms
step:585/1750 train_time:55387ms step_avg:94.68ms
step:586/1750 train_time:55484ms step_avg:94.68ms
step:587/1750 train_time:55580ms step_avg:94.69ms
step:588/1750 train_time:55677ms step_avg:94.69ms
step:589/1750 train_time:55773ms step_avg:94.69ms
step:590/1750 train_time:55870ms step_avg:94.70ms
step:591/1750 train_time:55967ms step_avg:94.70ms
step:592/1750 train_time:56064ms step_avg:94.70ms
step:593/1750 train_time:56162ms step_avg:94.71ms
step:594/1750 train_time:56258ms step_avg:94.71ms
step:595/1750 train_time:56355ms step_avg:94.71ms
step:596/1750 train_time:56452ms step_avg:94.72ms
step:597/1750 train_time:56549ms step_avg:94.72ms
step:598/1750 train_time:56646ms step_avg:94.73ms
step:599/1750 train_time:56742ms step_avg:94.73ms
step:600/1750 train_time:56839ms step_avg:94.73ms
step:601/1750 train_time:56936ms step_avg:94.74ms
step:602/1750 train_time:57033ms step_avg:94.74ms
step:603/1750 train_time:57131ms step_avg:94.75ms
step:604/1750 train_time:57228ms step_avg:94.75ms
step:605/1750 train_time:57326ms step_avg:94.75ms
step:606/1750 train_time:57422ms step_avg:94.76ms
step:607/1750 train_time:57519ms step_avg:94.76ms
step:608/1750 train_time:57616ms step_avg:94.76ms
step:609/1750 train_time:57713ms step_avg:94.77ms
step:610/1750 train_time:57810ms step_avg:94.77ms
step:611/1750 train_time:57907ms step_avg:94.77ms
step:612/1750 train_time:58003ms step_avg:94.78ms
step:613/1750 train_time:58099ms step_avg:94.78ms
step:614/1750 train_time:58195ms step_avg:94.78ms
step:615/1750 train_time:58292ms step_avg:94.78ms
step:616/1750 train_time:58390ms step_avg:94.79ms
step:617/1750 train_time:58486ms step_avg:94.79ms
step:618/1750 train_time:58583ms step_avg:94.79ms
step:619/1750 train_time:58680ms step_avg:94.80ms
step:620/1750 train_time:58776ms step_avg:94.80ms
step:621/1750 train_time:58873ms step_avg:94.80ms
step:622/1750 train_time:58970ms step_avg:94.81ms
step:623/1750 train_time:59067ms step_avg:94.81ms
step:624/1750 train_time:59164ms step_avg:94.81ms
step:625/1750 train_time:59261ms step_avg:94.82ms
step:625/1750 val_loss:3.6606 train_time:59352ms step_avg:94.96ms
step:626/1750 train_time:59378ms step_avg:94.85ms
step:627/1750 train_time:59462ms step_avg:94.84ms
step:628/1750 train_time:59561ms step_avg:94.84ms
step:629/1750 train_time:59658ms step_avg:94.85ms
step:630/1750 train_time:59755ms step_avg:94.85ms
step:631/1750 train_time:59851ms step_avg:94.85ms
step:632/1750 train_time:59947ms step_avg:94.85ms
step:633/1750 train_time:60042ms step_avg:94.85ms
step:634/1750 train_time:60138ms step_avg:94.86ms
step:635/1750 train_time:60235ms step_avg:94.86ms
step:636/1750 train_time:60331ms step_avg:94.86ms
step:637/1750 train_time:60428ms step_avg:94.86ms
step:638/1750 train_time:60526ms step_avg:94.87ms
step:639/1750 train_time:60625ms step_avg:94.87ms
step:640/1750 train_time:60723ms step_avg:94.88ms
step:641/1750 train_time:60820ms step_avg:94.88ms
step:642/1750 train_time:60916ms step_avg:94.89ms
step:643/1750 train_time:61013ms step_avg:94.89ms
step:644/1750 train_time:61109ms step_avg:94.89ms
step:645/1750 train_time:61206ms step_avg:94.89ms
step:646/1750 train_time:61303ms step_avg:94.90ms
step:647/1750 train_time:61400ms step_avg:94.90ms
step:648/1750 train_time:61497ms step_avg:94.90ms
step:649/1750 train_time:61595ms step_avg:94.91ms
step:650/1750 train_time:61693ms step_avg:94.91ms
step:651/1750 train_time:61792ms step_avg:94.92ms
step:652/1750 train_time:61892ms step_avg:94.93ms
step:653/1750 train_time:61991ms step_avg:94.93ms
step:654/1750 train_time:62089ms step_avg:94.94ms
step:655/1750 train_time:62189ms step_avg:94.94ms
step:656/1750 train_time:62289ms step_avg:94.95ms
step:657/1750 train_time:62388ms step_avg:94.96ms
step:658/1750 train_time:62487ms step_avg:94.97ms
step:659/1750 train_time:62586ms step_avg:94.97ms
step:660/1750 train_time:62685ms step_avg:94.98ms
step:661/1750 train_time:62784ms step_avg:94.98ms
step:662/1750 train_time:62883ms step_avg:94.99ms
step:663/1750 train_time:62981ms step_avg:94.99ms
step:664/1750 train_time:63080ms step_avg:95.00ms
step:665/1750 train_time:63179ms step_avg:95.01ms
step:666/1750 train_time:63277ms step_avg:95.01ms
step:667/1750 train_time:63375ms step_avg:95.02ms
step:668/1750 train_time:63474ms step_avg:95.02ms
step:669/1750 train_time:63572ms step_avg:95.03ms
step:670/1750 train_time:63671ms step_avg:95.03ms
step:671/1750 train_time:63771ms step_avg:95.04ms
step:672/1750 train_time:63870ms step_avg:95.04ms
step:673/1750 train_time:63969ms step_avg:95.05ms
step:674/1750 train_time:64069ms step_avg:95.06ms
step:675/1750 train_time:64169ms step_avg:95.06ms
step:676/1750 train_time:64268ms step_avg:95.07ms
step:677/1750 train_time:64369ms step_avg:95.08ms
step:678/1750 train_time:64467ms step_avg:95.08ms
step:679/1750 train_time:64565ms step_avg:95.09ms
step:680/1750 train_time:64664ms step_avg:95.09ms
step:681/1750 train_time:64762ms step_avg:95.10ms
step:682/1750 train_time:64861ms step_avg:95.10ms
step:683/1750 train_time:64959ms step_avg:95.11ms
step:684/1750 train_time:65058ms step_avg:95.11ms
step:685/1750 train_time:65157ms step_avg:95.12ms
step:686/1750 train_time:65256ms step_avg:95.13ms
step:687/1750 train_time:65355ms step_avg:95.13ms
step:688/1750 train_time:65454ms step_avg:95.14ms
step:689/1750 train_time:65553ms step_avg:95.14ms
step:690/1750 train_time:65651ms step_avg:95.15ms
step:691/1750 train_time:65750ms step_avg:95.15ms
step:692/1750 train_time:65850ms step_avg:95.16ms
step:693/1750 train_time:65949ms step_avg:95.16ms
step:694/1750 train_time:66049ms step_avg:95.17ms
step:695/1750 train_time:66149ms step_avg:95.18ms
step:696/1750 train_time:66247ms step_avg:95.18ms
step:697/1750 train_time:66346ms step_avg:95.19ms
step:698/1750 train_time:66444ms step_avg:95.19ms
step:699/1750 train_time:66543ms step_avg:95.20ms
step:700/1750 train_time:66643ms step_avg:95.20ms
step:701/1750 train_time:66742ms step_avg:95.21ms
step:702/1750 train_time:66841ms step_avg:95.21ms
step:703/1750 train_time:66939ms step_avg:95.22ms
step:704/1750 train_time:67038ms step_avg:95.22ms
step:705/1750 train_time:67136ms step_avg:95.23ms
step:706/1750 train_time:67236ms step_avg:95.23ms
step:707/1750 train_time:67335ms step_avg:95.24ms
step:708/1750 train_time:67435ms step_avg:95.25ms
step:709/1750 train_time:67533ms step_avg:95.25ms
step:710/1750 train_time:67632ms step_avg:95.26ms
step:711/1750 train_time:67731ms step_avg:95.26ms
step:712/1750 train_time:67830ms step_avg:95.27ms
step:713/1750 train_time:67930ms step_avg:95.27ms
step:714/1750 train_time:68031ms step_avg:95.28ms
step:715/1750 train_time:68130ms step_avg:95.29ms
step:716/1750 train_time:68229ms step_avg:95.29ms
step:717/1750 train_time:68329ms step_avg:95.30ms
step:718/1750 train_time:68429ms step_avg:95.30ms
step:719/1750 train_time:68528ms step_avg:95.31ms
step:720/1750 train_time:68626ms step_avg:95.31ms
step:721/1750 train_time:68725ms step_avg:95.32ms
step:722/1750 train_time:68824ms step_avg:95.32ms
step:723/1750 train_time:68922ms step_avg:95.33ms
step:724/1750 train_time:69020ms step_avg:95.33ms
step:725/1750 train_time:69118ms step_avg:95.34ms
step:726/1750 train_time:69217ms step_avg:95.34ms
step:727/1750 train_time:69316ms step_avg:95.34ms
step:728/1750 train_time:69414ms step_avg:95.35ms
step:729/1750 train_time:69513ms step_avg:95.35ms
step:730/1750 train_time:69611ms step_avg:95.36ms
step:731/1750 train_time:69710ms step_avg:95.36ms
step:732/1750 train_time:69810ms step_avg:95.37ms
step:733/1750 train_time:69910ms step_avg:95.37ms
step:734/1750 train_time:70010ms step_avg:95.38ms
step:735/1750 train_time:70109ms step_avg:95.39ms
step:736/1750 train_time:70209ms step_avg:95.39ms
step:737/1750 train_time:70309ms step_avg:95.40ms
step:738/1750 train_time:70407ms step_avg:95.40ms
step:739/1750 train_time:70505ms step_avg:95.41ms
step:740/1750 train_time:70603ms step_avg:95.41ms
step:741/1750 train_time:70702ms step_avg:95.41ms
step:742/1750 train_time:70800ms step_avg:95.42ms
step:743/1750 train_time:70898ms step_avg:95.42ms
step:744/1750 train_time:70997ms step_avg:95.43ms
step:745/1750 train_time:71096ms step_avg:95.43ms
step:746/1750 train_time:71196ms step_avg:95.44ms
step:747/1750 train_time:71295ms step_avg:95.44ms
step:748/1750 train_time:71394ms step_avg:95.45ms
step:749/1750 train_time:71493ms step_avg:95.45ms
step:750/1750 train_time:71593ms step_avg:95.46ms
step:750/1750 val_loss:3.5968 train_time:71686ms step_avg:95.58ms
step:751/1750 train_time:71712ms step_avg:95.49ms
step:752/1750 train_time:71797ms step_avg:95.47ms
step:753/1750 train_time:71897ms step_avg:95.48ms
step:754/1750 train_time:71996ms step_avg:95.49ms
step:755/1750 train_time:72095ms step_avg:95.49ms
step:756/1750 train_time:72195ms step_avg:95.50ms
step:757/1750 train_time:72294ms step_avg:95.50ms
step:758/1750 train_time:72393ms step_avg:95.51ms
step:759/1750 train_time:72492ms step_avg:95.51ms
step:760/1750 train_time:72590ms step_avg:95.51ms
step:761/1750 train_time:72690ms step_avg:95.52ms
step:762/1750 train_time:72789ms step_avg:95.52ms
step:763/1750 train_time:72889ms step_avg:95.53ms
step:764/1750 train_time:72988ms step_avg:95.53ms
step:765/1750 train_time:73087ms step_avg:95.54ms
step:766/1750 train_time:73186ms step_avg:95.54ms
step:767/1750 train_time:73285ms step_avg:95.55ms
step:768/1750 train_time:73384ms step_avg:95.55ms
step:769/1750 train_time:73482ms step_avg:95.56ms
step:770/1750 train_time:73581ms step_avg:95.56ms
step:771/1750 train_time:73680ms step_avg:95.56ms
step:772/1750 train_time:73780ms step_avg:95.57ms
step:773/1750 train_time:73879ms step_avg:95.57ms
step:774/1750 train_time:73978ms step_avg:95.58ms
step:775/1750 train_time:74078ms step_avg:95.58ms
step:776/1750 train_time:74177ms step_avg:95.59ms
step:777/1750 train_time:74275ms step_avg:95.59ms
step:778/1750 train_time:74374ms step_avg:95.60ms
step:779/1750 train_time:74474ms step_avg:95.60ms
step:780/1750 train_time:74574ms step_avg:95.61ms
step:781/1750 train_time:74674ms step_avg:95.61ms
step:782/1750 train_time:74773ms step_avg:95.62ms
step:783/1750 train_time:74872ms step_avg:95.62ms
step:784/1750 train_time:74970ms step_avg:95.63ms
step:785/1750 train_time:75069ms step_avg:95.63ms
step:786/1750 train_time:75167ms step_avg:95.63ms
step:787/1750 train_time:75266ms step_avg:95.64ms
step:788/1750 train_time:75365ms step_avg:95.64ms
step:789/1750 train_time:75464ms step_avg:95.65ms
step:790/1750 train_time:75563ms step_avg:95.65ms
step:791/1750 train_time:75662ms step_avg:95.65ms
step:792/1750 train_time:75761ms step_avg:95.66ms
step:793/1750 train_time:75860ms step_avg:95.66ms
step:794/1750 train_time:75959ms step_avg:95.67ms
step:795/1750 train_time:76059ms step_avg:95.67ms
step:796/1750 train_time:76158ms step_avg:95.68ms
step:797/1750 train_time:76258ms step_avg:95.68ms
step:798/1750 train_time:76357ms step_avg:95.69ms
step:799/1750 train_time:76456ms step_avg:95.69ms
step:800/1750 train_time:76555ms step_avg:95.69ms
step:801/1750 train_time:76654ms step_avg:95.70ms
step:802/1750 train_time:76753ms step_avg:95.70ms
step:803/1750 train_time:76852ms step_avg:95.71ms
step:804/1750 train_time:76951ms step_avg:95.71ms
step:805/1750 train_time:77051ms step_avg:95.72ms
step:806/1750 train_time:77151ms step_avg:95.72ms
step:807/1750 train_time:77250ms step_avg:95.72ms
step:808/1750 train_time:77349ms step_avg:95.73ms
step:809/1750 train_time:77448ms step_avg:95.73ms
step:810/1750 train_time:77547ms step_avg:95.74ms
step:811/1750 train_time:77646ms step_avg:95.74ms
step:812/1750 train_time:77745ms step_avg:95.74ms
step:813/1750 train_time:77843ms step_avg:95.75ms
step:814/1750 train_time:77942ms step_avg:95.75ms
step:815/1750 train_time:78041ms step_avg:95.76ms
step:816/1750 train_time:78140ms step_avg:95.76ms
step:817/1750 train_time:78240ms step_avg:95.76ms
step:818/1750 train_time:78339ms step_avg:95.77ms
step:819/1750 train_time:78439ms step_avg:95.77ms
step:820/1750 train_time:78539ms step_avg:95.78ms
step:821/1750 train_time:78639ms step_avg:95.78ms
step:822/1750 train_time:78738ms step_avg:95.79ms
step:823/1750 train_time:78837ms step_avg:95.79ms
step:824/1750 train_time:78936ms step_avg:95.80ms
step:825/1750 train_time:79036ms step_avg:95.80ms
step:826/1750 train_time:79135ms step_avg:95.81ms
step:827/1750 train_time:79235ms step_avg:95.81ms
step:828/1750 train_time:79335ms step_avg:95.81ms
step:829/1750 train_time:79435ms step_avg:95.82ms
step:830/1750 train_time:79535ms step_avg:95.82ms
step:831/1750 train_time:79634ms step_avg:95.83ms
step:832/1750 train_time:79733ms step_avg:95.83ms
step:833/1750 train_time:79831ms step_avg:95.84ms
step:834/1750 train_time:79930ms step_avg:95.84ms
step:835/1750 train_time:80028ms step_avg:95.84ms
step:836/1750 train_time:80127ms step_avg:95.85ms
step:837/1750 train_time:80225ms step_avg:95.85ms
step:838/1750 train_time:80324ms step_avg:95.85ms
step:839/1750 train_time:80424ms step_avg:95.86ms
step:840/1750 train_time:80523ms step_avg:95.86ms
step:841/1750 train_time:80623ms step_avg:95.87ms
step:842/1750 train_time:80722ms step_avg:95.87ms
step:843/1750 train_time:80820ms step_avg:95.87ms
step:844/1750 train_time:80920ms step_avg:95.88ms
step:845/1750 train_time:81019ms step_avg:95.88ms
step:846/1750 train_time:81118ms step_avg:95.88ms
step:847/1750 train_time:81217ms step_avg:95.89ms
step:848/1750 train_time:81316ms step_avg:95.89ms
step:849/1750 train_time:81416ms step_avg:95.90ms
step:850/1750 train_time:81516ms step_avg:95.90ms
step:851/1750 train_time:81615ms step_avg:95.91ms
step:852/1750 train_time:81714ms step_avg:95.91ms
step:853/1750 train_time:81814ms step_avg:95.91ms
step:854/1750 train_time:81912ms step_avg:95.92ms
step:855/1750 train_time:82012ms step_avg:95.92ms
step:856/1750 train_time:82111ms step_avg:95.92ms
step:857/1750 train_time:82210ms step_avg:95.93ms
step:858/1750 train_time:82310ms step_avg:95.93ms
step:859/1750 train_time:82409ms step_avg:95.94ms
step:860/1750 train_time:82509ms step_avg:95.94ms
step:861/1750 train_time:82608ms step_avg:95.94ms
step:862/1750 train_time:82707ms step_avg:95.95ms
step:863/1750 train_time:82806ms step_avg:95.95ms
step:864/1750 train_time:82905ms step_avg:95.96ms
step:865/1750 train_time:83003ms step_avg:95.96ms
step:866/1750 train_time:83102ms step_avg:95.96ms
step:867/1750 train_time:83201ms step_avg:95.96ms
step:868/1750 train_time:83301ms step_avg:95.97ms
step:869/1750 train_time:83401ms step_avg:95.97ms
step:870/1750 train_time:83501ms step_avg:95.98ms
step:871/1750 train_time:83600ms step_avg:95.98ms
step:872/1750 train_time:83699ms step_avg:95.99ms
step:873/1750 train_time:83798ms step_avg:95.99ms
step:874/1750 train_time:83897ms step_avg:95.99ms
step:875/1750 train_time:83995ms step_avg:95.99ms
step:875/1750 val_loss:3.5479 train_time:84088ms step_avg:96.10ms
step:876/1750 train_time:84114ms step_avg:96.02ms
step:877/1750 train_time:84202ms step_avg:96.01ms
step:878/1750 train_time:84303ms step_avg:96.02ms
step:879/1750 train_time:84401ms step_avg:96.02ms
step:880/1750 train_time:84500ms step_avg:96.02ms
step:881/1750 train_time:84598ms step_avg:96.02ms
step:882/1750 train_time:84696ms step_avg:96.03ms
step:883/1750 train_time:84794ms step_avg:96.03ms
step:884/1750 train_time:84892ms step_avg:96.03ms
step:885/1750 train_time:84990ms step_avg:96.03ms
step:886/1750 train_time:85089ms step_avg:96.04ms
step:887/1750 train_time:85188ms step_avg:96.04ms
step:888/1750 train_time:85288ms step_avg:96.04ms
step:889/1750 train_time:85388ms step_avg:96.05ms
step:890/1750 train_time:85487ms step_avg:96.05ms
step:891/1750 train_time:85587ms step_avg:96.06ms
step:892/1750 train_time:85685ms step_avg:96.06ms
step:893/1750 train_time:85784ms step_avg:96.06ms
step:894/1750 train_time:85883ms step_avg:96.07ms
step:895/1750 train_time:85981ms step_avg:96.07ms
step:896/1750 train_time:86080ms step_avg:96.07ms
step:897/1750 train_time:86178ms step_avg:96.07ms
step:898/1750 train_time:86277ms step_avg:96.08ms
step:899/1750 train_time:86377ms step_avg:96.08ms
step:900/1750 train_time:86477ms step_avg:96.09ms
step:901/1750 train_time:86576ms step_avg:96.09ms
step:902/1750 train_time:86675ms step_avg:96.09ms
step:903/1750 train_time:86774ms step_avg:96.10ms
step:904/1750 train_time:86873ms step_avg:96.10ms
step:905/1750 train_time:86972ms step_avg:96.10ms
step:906/1750 train_time:87071ms step_avg:96.10ms
step:907/1750 train_time:87170ms step_avg:96.11ms
step:908/1750 train_time:87270ms step_avg:96.11ms
step:909/1750 train_time:87369ms step_avg:96.12ms
step:910/1750 train_time:87471ms step_avg:96.12ms
step:911/1750 train_time:87572ms step_avg:96.13ms
step:912/1750 train_time:87673ms step_avg:96.13ms
step:913/1750 train_time:87773ms step_avg:96.14ms
step:914/1750 train_time:87873ms step_avg:96.14ms
step:915/1750 train_time:87974ms step_avg:96.15ms
step:916/1750 train_time:88074ms step_avg:96.15ms
step:917/1750 train_time:88175ms step_avg:96.16ms
step:918/1750 train_time:88276ms step_avg:96.16ms
step:919/1750 train_time:88377ms step_avg:96.17ms
step:920/1750 train_time:88478ms step_avg:96.17ms
step:921/1750 train_time:88578ms step_avg:96.18ms
step:922/1750 train_time:88678ms step_avg:96.18ms
step:923/1750 train_time:88778ms step_avg:96.18ms
step:924/1750 train_time:88878ms step_avg:96.19ms
step:925/1750 train_time:88978ms step_avg:96.19ms
step:926/1750 train_time:89079ms step_avg:96.20ms
step:927/1750 train_time:89179ms step_avg:96.20ms
step:928/1750 train_time:89279ms step_avg:96.21ms
step:929/1750 train_time:89379ms step_avg:96.21ms
step:930/1750 train_time:89479ms step_avg:96.21ms
step:931/1750 train_time:89580ms step_avg:96.22ms
step:932/1750 train_time:89680ms step_avg:96.22ms
step:933/1750 train_time:89780ms step_avg:96.23ms
step:934/1750 train_time:89880ms step_avg:96.23ms
step:935/1750 train_time:89980ms step_avg:96.24ms
step:936/1750 train_time:90080ms step_avg:96.24ms
step:937/1750 train_time:90180ms step_avg:96.24ms
step:938/1750 train_time:90280ms step_avg:96.25ms
step:939/1750 train_time:90380ms step_avg:96.25ms
step:940/1750 train_time:90481ms step_avg:96.26ms
step:941/1750 train_time:90581ms step_avg:96.26ms
step:942/1750 train_time:90681ms step_avg:96.26ms
step:943/1750 train_time:90782ms step_avg:96.27ms
step:944/1750 train_time:90882ms step_avg:96.27ms
step:945/1750 train_time:90983ms step_avg:96.28ms
step:946/1750 train_time:91083ms step_avg:96.28ms
step:947/1750 train_time:91184ms step_avg:96.29ms
step:948/1750 train_time:91284ms step_avg:96.29ms
step:949/1750 train_time:91385ms step_avg:96.30ms
step:950/1750 train_time:91486ms step_avg:96.30ms
step:951/1750 train_time:91587ms step_avg:96.31ms
step:952/1750 train_time:91687ms step_avg:96.31ms
step:953/1750 train_time:91788ms step_avg:96.31ms
step:954/1750 train_time:91888ms step_avg:96.32ms
step:955/1750 train_time:91988ms step_avg:96.32ms
step:956/1750 train_time:92089ms step_avg:96.33ms
step:957/1750 train_time:92190ms step_avg:96.33ms
step:958/1750 train_time:92290ms step_avg:96.34ms
step:959/1750 train_time:92391ms step_avg:96.34ms
step:960/1750 train_time:92492ms step_avg:96.35ms
step:961/1750 train_time:92593ms step_avg:96.35ms
step:962/1750 train_time:92694ms step_avg:96.36ms
step:963/1750 train_time:92795ms step_avg:96.36ms
step:964/1750 train_time:92895ms step_avg:96.36ms
step:965/1750 train_time:92995ms step_avg:96.37ms
step:966/1750 train_time:93095ms step_avg:96.37ms
step:967/1750 train_time:93196ms step_avg:96.38ms
step:968/1750 train_time:93297ms step_avg:96.38ms
step:969/1750 train_time:93397ms step_avg:96.38ms
step:970/1750 train_time:93497ms step_avg:96.39ms
step:971/1750 train_time:93597ms step_avg:96.39ms
step:972/1750 train_time:93698ms step_avg:96.40ms
step:973/1750 train_time:93798ms step_avg:96.40ms
step:974/1750 train_time:93897ms step_avg:96.40ms
step:975/1750 train_time:93997ms step_avg:96.41ms
step:976/1750 train_time:94098ms step_avg:96.41ms
step:977/1750 train_time:94199ms step_avg:96.42ms
step:978/1750 train_time:94299ms step_avg:96.42ms
step:979/1750 train_time:94400ms step_avg:96.42ms
step:980/1750 train_time:94500ms step_avg:96.43ms
step:981/1750 train_time:94600ms step_avg:96.43ms
step:982/1750 train_time:94701ms step_avg:96.44ms
step:983/1750 train_time:94802ms step_avg:96.44ms
step:984/1750 train_time:94902ms step_avg:96.45ms
step:985/1750 train_time:95004ms step_avg:96.45ms
step:986/1750 train_time:95105ms step_avg:96.46ms
step:987/1750 train_time:95205ms step_avg:96.46ms
step:988/1750 train_time:95306ms step_avg:96.46ms
step:989/1750 train_time:95406ms step_avg:96.47ms
step:990/1750 train_time:95506ms step_avg:96.47ms
step:991/1750 train_time:95607ms step_avg:96.48ms
step:992/1750 train_time:95707ms step_avg:96.48ms
step:993/1750 train_time:95808ms step_avg:96.48ms
step:994/1750 train_time:95909ms step_avg:96.49ms
step:995/1750 train_time:96010ms step_avg:96.49ms
step:996/1750 train_time:96110ms step_avg:96.50ms
step:997/1750 train_time:96211ms step_avg:96.50ms
step:998/1750 train_time:96311ms step_avg:96.50ms
step:999/1750 train_time:96411ms step_avg:96.51ms
step:1000/1750 train_time:96511ms step_avg:96.51ms
step:1000/1750 val_loss:3.5060 train_time:96607ms step_avg:96.61ms
step:1001/1750 train_time:96634ms step_avg:96.54ms
step:1002/1750 train_time:96722ms step_avg:96.53ms
step:1003/1750 train_time:96822ms step_avg:96.53ms
step:1004/1750 train_time:96923ms step_avg:96.54ms
step:1005/1750 train_time:97023ms step_avg:96.54ms
step:1006/1750 train_time:97122ms step_avg:96.54ms
step:1007/1750 train_time:97222ms step_avg:96.55ms
step:1008/1750 train_time:97321ms step_avg:96.55ms
step:1009/1750 train_time:97421ms step_avg:96.55ms
step:1010/1750 train_time:97521ms step_avg:96.56ms
step:1011/1750 train_time:97624ms step_avg:96.56ms
step:1012/1750 train_time:97726ms step_avg:96.57ms
step:1013/1750 train_time:97828ms step_avg:96.57ms
step:1014/1750 train_time:97928ms step_avg:96.58ms
step:1015/1750 train_time:98028ms step_avg:96.58ms
step:1016/1750 train_time:98129ms step_avg:96.58ms
step:1017/1750 train_time:98229ms step_avg:96.59ms
step:1018/1750 train_time:98330ms step_avg:96.59ms
step:1019/1750 train_time:98430ms step_avg:96.59ms
step:1020/1750 train_time:98531ms step_avg:96.60ms
step:1021/1750 train_time:98632ms step_avg:96.60ms
step:1022/1750 train_time:98732ms step_avg:96.61ms
step:1023/1750 train_time:98833ms step_avg:96.61ms
step:1024/1750 train_time:98935ms step_avg:96.62ms
step:1025/1750 train_time:99035ms step_avg:96.62ms
step:1026/1750 train_time:99135ms step_avg:96.62ms
step:1027/1750 train_time:99236ms step_avg:96.63ms
step:1028/1750 train_time:99335ms step_avg:96.63ms
step:1029/1750 train_time:99437ms step_avg:96.63ms
step:1030/1750 train_time:99537ms step_avg:96.64ms
step:1031/1750 train_time:99638ms step_avg:96.64ms
step:1032/1750 train_time:99738ms step_avg:96.65ms
step:1033/1750 train_time:99839ms step_avg:96.65ms
step:1034/1750 train_time:99939ms step_avg:96.65ms
step:1035/1750 train_time:100039ms step_avg:96.66ms
step:1036/1750 train_time:100138ms step_avg:96.66ms
step:1037/1750 train_time:100239ms step_avg:96.66ms
step:1038/1750 train_time:100339ms step_avg:96.67ms
step:1039/1750 train_time:100440ms step_avg:96.67ms
step:1040/1750 train_time:100541ms step_avg:96.67ms
step:1041/1750 train_time:100643ms step_avg:96.68ms
step:1042/1750 train_time:100743ms step_avg:96.68ms
step:1043/1750 train_time:100844ms step_avg:96.69ms
step:1044/1750 train_time:100945ms step_avg:96.69ms
step:1045/1750 train_time:101046ms step_avg:96.70ms
step:1046/1750 train_time:101148ms step_avg:96.70ms
step:1047/1750 train_time:101250ms step_avg:96.70ms
step:1048/1750 train_time:101350ms step_avg:96.71ms
step:1049/1750 train_time:101450ms step_avg:96.71ms
step:1050/1750 train_time:101551ms step_avg:96.72ms
step:1051/1750 train_time:101652ms step_avg:96.72ms
step:1052/1750 train_time:101752ms step_avg:96.72ms
step:1053/1750 train_time:101853ms step_avg:96.73ms
step:1054/1750 train_time:101954ms step_avg:96.73ms
step:1055/1750 train_time:102056ms step_avg:96.74ms
step:1056/1750 train_time:102156ms step_avg:96.74ms
step:1057/1750 train_time:102256ms step_avg:96.74ms
step:1058/1750 train_time:102356ms step_avg:96.74ms
step:1059/1750 train_time:102456ms step_avg:96.75ms
step:1060/1750 train_time:102557ms step_avg:96.75ms
step:1061/1750 train_time:102658ms step_avg:96.76ms
step:1062/1750 train_time:102759ms step_avg:96.76ms
step:1063/1750 train_time:102861ms step_avg:96.76ms
step:1064/1750 train_time:102963ms step_avg:96.77ms
step:1065/1750 train_time:103063ms step_avg:96.77ms
step:1066/1750 train_time:103164ms step_avg:96.78ms
step:1067/1750 train_time:103264ms step_avg:96.78ms
step:1068/1750 train_time:103364ms step_avg:96.78ms
step:1069/1750 train_time:103465ms step_avg:96.79ms
step:1070/1750 train_time:103565ms step_avg:96.79ms
step:1071/1750 train_time:103667ms step_avg:96.79ms
step:1072/1750 train_time:103769ms step_avg:96.80ms
step:1073/1750 train_time:103870ms step_avg:96.80ms
step:1074/1750 train_time:103970ms step_avg:96.81ms
step:1075/1750 train_time:104071ms step_avg:96.81ms
step:1076/1750 train_time:104172ms step_avg:96.81ms
step:1077/1750 train_time:104273ms step_avg:96.82ms
step:1078/1750 train_time:104374ms step_avg:96.82ms
step:1079/1750 train_time:104475ms step_avg:96.83ms
step:1080/1750 train_time:104576ms step_avg:96.83ms
step:1081/1750 train_time:104677ms step_avg:96.83ms
step:1082/1750 train_time:104777ms step_avg:96.84ms
step:1083/1750 train_time:104877ms step_avg:96.84ms
step:1084/1750 train_time:104977ms step_avg:96.84ms
step:1085/1750 train_time:105078ms step_avg:96.85ms
step:1086/1750 train_time:105178ms step_avg:96.85ms
step:1087/1750 train_time:105279ms step_avg:96.85ms
step:1088/1750 train_time:105380ms step_avg:96.86ms
step:1089/1750 train_time:105480ms step_avg:96.86ms
step:1090/1750 train_time:105582ms step_avg:96.86ms
step:1091/1750 train_time:105684ms step_avg:96.87ms
step:1092/1750 train_time:105784ms step_avg:96.87ms
step:1093/1750 train_time:105884ms step_avg:96.88ms
step:1094/1750 train_time:105985ms step_avg:96.88ms
step:1095/1750 train_time:106086ms step_avg:96.88ms
step:1096/1750 train_time:106187ms step_avg:96.89ms
step:1097/1750 train_time:106288ms step_avg:96.89ms
step:1098/1750 train_time:106389ms step_avg:96.89ms
step:1099/1750 train_time:106489ms step_avg:96.90ms
step:1100/1750 train_time:106589ms step_avg:96.90ms
step:1101/1750 train_time:106691ms step_avg:96.90ms
step:1102/1750 train_time:106792ms step_avg:96.91ms
step:1103/1750 train_time:106893ms step_avg:96.91ms
step:1104/1750 train_time:106994ms step_avg:96.91ms
step:1105/1750 train_time:107094ms step_avg:96.92ms
step:1106/1750 train_time:107196ms step_avg:96.92ms
step:1107/1750 train_time:107296ms step_avg:96.92ms
step:1108/1750 train_time:107396ms step_avg:96.93ms
step:1109/1750 train_time:107496ms step_avg:96.93ms
step:1110/1750 train_time:107597ms step_avg:96.93ms
step:1111/1750 train_time:107697ms step_avg:96.94ms
step:1112/1750 train_time:107798ms step_avg:96.94ms
step:1113/1750 train_time:107899ms step_avg:96.94ms
step:1114/1750 train_time:107999ms step_avg:96.95ms
step:1115/1750 train_time:108099ms step_avg:96.95ms
step:1116/1750 train_time:108199ms step_avg:96.95ms
step:1117/1750 train_time:108300ms step_avg:96.96ms
step:1118/1750 train_time:108400ms step_avg:96.96ms
step:1119/1750 train_time:108501ms step_avg:96.96ms
step:1120/1750 train_time:108603ms step_avg:96.97ms
step:1121/1750 train_time:108703ms step_avg:96.97ms
step:1122/1750 train_time:108805ms step_avg:96.97ms
step:1123/1750 train_time:108906ms step_avg:96.98ms
step:1124/1750 train_time:109007ms step_avg:96.98ms
step:1125/1750 train_time:109107ms step_avg:96.98ms
step:1125/1750 val_loss:3.4544 train_time:109202ms step_avg:97.07ms
step:1126/1750 train_time:109229ms step_avg:97.01ms
step:1127/1750 train_time:109316ms step_avg:97.00ms
step:1128/1750 train_time:109419ms step_avg:97.00ms
step:1129/1750 train_time:109519ms step_avg:97.01ms
step:1130/1750 train_time:109620ms step_avg:97.01ms
step:1131/1750 train_time:109720ms step_avg:97.01ms
step:1132/1750 train_time:109821ms step_avg:97.02ms
step:1133/1750 train_time:109921ms step_avg:97.02ms
step:1134/1750 train_time:110021ms step_avg:97.02ms
step:1135/1750 train_time:110121ms step_avg:97.02ms
step:1136/1750 train_time:110224ms step_avg:97.03ms
step:1137/1750 train_time:110326ms step_avg:97.03ms
step:1138/1750 train_time:110427ms step_avg:97.04ms
step:1139/1750 train_time:110527ms step_avg:97.04ms
step:1140/1750 train_time:110627ms step_avg:97.04ms
step:1141/1750 train_time:110727ms step_avg:97.04ms
step:1142/1750 train_time:110827ms step_avg:97.05ms
step:1143/1750 train_time:110927ms step_avg:97.05ms
step:1144/1750 train_time:111028ms step_avg:97.05ms
step:1145/1750 train_time:111129ms step_avg:97.06ms
step:1146/1750 train_time:111230ms step_avg:97.06ms
step:1147/1750 train_time:111331ms step_avg:97.06ms
step:1148/1750 train_time:111433ms step_avg:97.07ms
step:1149/1750 train_time:111533ms step_avg:97.07ms
step:1150/1750 train_time:111633ms step_avg:97.07ms
step:1151/1750 train_time:111733ms step_avg:97.07ms
step:1152/1750 train_time:111833ms step_avg:97.08ms
step:1153/1750 train_time:111934ms step_avg:97.08ms
step:1154/1750 train_time:112034ms step_avg:97.08ms
step:1155/1750 train_time:112135ms step_avg:97.09ms
step:1156/1750 train_time:112236ms step_avg:97.09ms
step:1157/1750 train_time:112337ms step_avg:97.09ms
step:1158/1750 train_time:112438ms step_avg:97.10ms
step:1159/1750 train_time:112540ms step_avg:97.10ms
step:1160/1750 train_time:112640ms step_avg:97.10ms
step:1161/1750 train_time:112741ms step_avg:97.11ms
step:1162/1750 train_time:112841ms step_avg:97.11ms
step:1163/1750 train_time:112941ms step_avg:97.11ms
step:1164/1750 train_time:113042ms step_avg:97.11ms
step:1165/1750 train_time:113142ms step_avg:97.12ms
step:1166/1750 train_time:113243ms step_avg:97.12ms
step:1167/1750 train_time:113345ms step_avg:97.12ms
step:1168/1750 train_time:113446ms step_avg:97.13ms
step:1169/1750 train_time:113548ms step_avg:97.13ms
step:1170/1750 train_time:113650ms step_avg:97.14ms
step:1171/1750 train_time:113751ms step_avg:97.14ms
step:1172/1750 train_time:113853ms step_avg:97.14ms
step:1173/1750 train_time:113956ms step_avg:97.15ms
step:1174/1750 train_time:114058ms step_avg:97.15ms
step:1175/1750 train_time:114159ms step_avg:97.16ms
step:1176/1750 train_time:114264ms step_avg:97.16ms
step:1177/1750 train_time:114365ms step_avg:97.17ms
step:1178/1750 train_time:114467ms step_avg:97.17ms
step:1179/1750 train_time:114570ms step_avg:97.18ms
step:1180/1750 train_time:114671ms step_avg:97.18ms
step:1181/1750 train_time:114772ms step_avg:97.18ms
step:1182/1750 train_time:114877ms step_avg:97.19ms
step:1183/1750 train_time:114977ms step_avg:97.19ms
step:1184/1750 train_time:115079ms step_avg:97.20ms
step:1185/1750 train_time:115182ms step_avg:97.20ms
step:1186/1750 train_time:115284ms step_avg:97.20ms
step:1187/1750 train_time:115385ms step_avg:97.21ms
step:1188/1750 train_time:115487ms step_avg:97.21ms
step:1189/1750 train_time:115588ms step_avg:97.21ms
step:1190/1750 train_time:115688ms step_avg:97.22ms
step:1191/1750 train_time:115790ms step_avg:97.22ms
step:1192/1750 train_time:115892ms step_avg:97.22ms
step:1193/1750 train_time:115994ms step_avg:97.23ms
step:1194/1750 train_time:116096ms step_avg:97.23ms
step:1195/1750 train_time:116198ms step_avg:97.24ms
step:1196/1750 train_time:116300ms step_avg:97.24ms
step:1197/1750 train_time:116402ms step_avg:97.24ms
step:1198/1750 train_time:116503ms step_avg:97.25ms
step:1199/1750 train_time:116606ms step_avg:97.25ms
step:1200/1750 train_time:116707ms step_avg:97.26ms
step:1201/1750 train_time:116809ms step_avg:97.26ms
step:1202/1750 train_time:116911ms step_avg:97.26ms
step:1203/1750 train_time:117013ms step_avg:97.27ms
step:1204/1750 train_time:117115ms step_avg:97.27ms
step:1205/1750 train_time:117216ms step_avg:97.28ms
step:1206/1750 train_time:117318ms step_avg:97.28ms
step:1207/1750 train_time:117420ms step_avg:97.28ms
step:1208/1750 train_time:117523ms step_avg:97.29ms
step:1209/1750 train_time:117624ms step_avg:97.29ms
step:1210/1750 train_time:117726ms step_avg:97.29ms
step:1211/1750 train_time:117827ms step_avg:97.30ms
step:1212/1750 train_time:117930ms step_avg:97.30ms
step:1213/1750 train_time:118032ms step_avg:97.31ms
step:1214/1750 train_time:118132ms step_avg:97.31ms
step:1215/1750 train_time:118236ms step_avg:97.31ms
step:1216/1750 train_time:118337ms step_avg:97.32ms
step:1217/1750 train_time:118439ms step_avg:97.32ms
step:1218/1750 train_time:118543ms step_avg:97.33ms
step:1219/1750 train_time:118646ms step_avg:97.33ms
step:1220/1750 train_time:118747ms step_avg:97.33ms
step:1221/1750 train_time:118849ms step_avg:97.34ms
step:1222/1750 train_time:118951ms step_avg:97.34ms
step:1223/1750 train_time:119053ms step_avg:97.35ms
step:1224/1750 train_time:119155ms step_avg:97.35ms
step:1225/1750 train_time:119257ms step_avg:97.35ms
step:1226/1750 train_time:119359ms step_avg:97.36ms
step:1227/1750 train_time:119461ms step_avg:97.36ms
step:1228/1750 train_time:119563ms step_avg:97.36ms
step:1229/1750 train_time:119665ms step_avg:97.37ms
step:1230/1750 train_time:119767ms step_avg:97.37ms
step:1231/1750 train_time:119868ms step_avg:97.37ms
step:1232/1750 train_time:119970ms step_avg:97.38ms
step:1233/1750 train_time:120072ms step_avg:97.38ms
step:1234/1750 train_time:120174ms step_avg:97.39ms
step:1235/1750 train_time:120276ms step_avg:97.39ms
step:1236/1750 train_time:120380ms step_avg:97.40ms
step:1237/1750 train_time:120482ms step_avg:97.40ms
step:1238/1750 train_time:120584ms step_avg:97.40ms
step:1239/1750 train_time:120686ms step_avg:97.41ms
step:1240/1750 train_time:120787ms step_avg:97.41ms
step:1241/1750 train_time:120889ms step_avg:97.41ms
step:1242/1750 train_time:120991ms step_avg:97.42ms
step:1243/1750 train_time:121093ms step_avg:97.42ms
step:1244/1750 train_time:121194ms step_avg:97.42ms
step:1245/1750 train_time:121296ms step_avg:97.43ms
step:1246/1750 train_time:121399ms step_avg:97.43ms
step:1247/1750 train_time:121500ms step_avg:97.43ms
step:1248/1750 train_time:121602ms step_avg:97.44ms
step:1249/1750 train_time:121704ms step_avg:97.44ms
step:1250/1750 train_time:121806ms step_avg:97.45ms
step:1250/1750 val_loss:3.4076 train_time:121902ms step_avg:97.52ms
step:1251/1750 train_time:121929ms step_avg:97.47ms
step:1252/1750 train_time:122019ms step_avg:97.46ms
step:1253/1750 train_time:122120ms step_avg:97.46ms
step:1254/1750 train_time:122223ms step_avg:97.47ms
step:1255/1750 train_time:122325ms step_avg:97.47ms
step:1256/1750 train_time:122427ms step_avg:97.47ms
step:1257/1750 train_time:122529ms step_avg:97.48ms
step:1258/1750 train_time:122629ms step_avg:97.48ms
step:1259/1750 train_time:122730ms step_avg:97.48ms
step:1260/1750 train_time:122831ms step_avg:97.48ms
step:1261/1750 train_time:122933ms step_avg:97.49ms
step:1262/1750 train_time:123036ms step_avg:97.49ms
step:1263/1750 train_time:123138ms step_avg:97.50ms
step:1264/1750 train_time:123240ms step_avg:97.50ms
step:1265/1750 train_time:123342ms step_avg:97.50ms
step:1266/1750 train_time:123443ms step_avg:97.51ms
step:1267/1750 train_time:123544ms step_avg:97.51ms
step:1268/1750 train_time:123646ms step_avg:97.51ms
step:1269/1750 train_time:123748ms step_avg:97.52ms
step:1270/1750 train_time:123850ms step_avg:97.52ms
step:1271/1750 train_time:123953ms step_avg:97.52ms
step:1272/1750 train_time:124054ms step_avg:97.53ms
step:1273/1750 train_time:124155ms step_avg:97.53ms
step:1274/1750 train_time:124257ms step_avg:97.53ms
step:1275/1750 train_time:124358ms step_avg:97.54ms
step:1276/1750 train_time:124460ms step_avg:97.54ms
step:1277/1750 train_time:124562ms step_avg:97.54ms
step:1278/1750 train_time:124664ms step_avg:97.55ms
step:1279/1750 train_time:124767ms step_avg:97.55ms
step:1280/1750 train_time:124868ms step_avg:97.55ms
step:1281/1750 train_time:124971ms step_avg:97.56ms
step:1282/1750 train_time:125072ms step_avg:97.56ms
step:1283/1750 train_time:125175ms step_avg:97.56ms
step:1284/1750 train_time:125276ms step_avg:97.57ms
step:1285/1750 train_time:125377ms step_avg:97.57ms
step:1286/1750 train_time:125480ms step_avg:97.57ms
step:1287/1750 train_time:125582ms step_avg:97.58ms
step:1288/1750 train_time:125683ms step_avg:97.58ms
step:1289/1750 train_time:125785ms step_avg:97.58ms
step:1290/1750 train_time:125886ms step_avg:97.59ms
step:1291/1750 train_time:125989ms step_avg:97.59ms
step:1292/1750 train_time:126091ms step_avg:97.59ms
step:1293/1750 train_time:126192ms step_avg:97.60ms
step:1294/1750 train_time:126294ms step_avg:97.60ms
step:1295/1750 train_time:126396ms step_avg:97.60ms
step:1296/1750 train_time:126497ms step_avg:97.61ms
step:1297/1750 train_time:126600ms step_avg:97.61ms
step:1298/1750 train_time:126701ms step_avg:97.61ms
step:1299/1750 train_time:126802ms step_avg:97.62ms
step:1300/1750 train_time:126905ms step_avg:97.62ms
step:1301/1750 train_time:127008ms step_avg:97.62ms
step:1302/1750 train_time:127110ms step_avg:97.63ms
step:1303/1750 train_time:127212ms step_avg:97.63ms
step:1304/1750 train_time:127313ms step_avg:97.63ms
step:1305/1750 train_time:127415ms step_avg:97.64ms
step:1306/1750 train_time:127516ms step_avg:97.64ms
step:1307/1750 train_time:127618ms step_avg:97.64ms
step:1308/1750 train_time:127721ms step_avg:97.65ms
step:1309/1750 train_time:127823ms step_avg:97.65ms
step:1310/1750 train_time:127925ms step_avg:97.65ms
step:1311/1750 train_time:128027ms step_avg:97.66ms
step:1312/1750 train_time:128129ms step_avg:97.66ms
step:1313/1750 train_time:128232ms step_avg:97.66ms
step:1314/1750 train_time:128334ms step_avg:97.67ms
step:1315/1750 train_time:128436ms step_avg:97.67ms
step:1316/1750 train_time:128537ms step_avg:97.67ms
step:1317/1750 train_time:128638ms step_avg:97.67ms
step:1318/1750 train_time:128740ms step_avg:97.68ms
step:1319/1750 train_time:128843ms step_avg:97.68ms
step:1320/1750 train_time:128946ms step_avg:97.69ms
step:1321/1750 train_time:129047ms step_avg:97.69ms
step:1322/1750 train_time:129149ms step_avg:97.69ms
step:1323/1750 train_time:129250ms step_avg:97.69ms
step:1324/1750 train_time:129352ms step_avg:97.70ms
step:1325/1750 train_time:129453ms step_avg:97.70ms
step:1326/1750 train_time:129556ms step_avg:97.70ms
step:1327/1750 train_time:129659ms step_avg:97.71ms
step:1328/1750 train_time:129761ms step_avg:97.71ms
step:1329/1750 train_time:129863ms step_avg:97.71ms
step:1330/1750 train_time:129965ms step_avg:97.72ms
step:1331/1750 train_time:130068ms step_avg:97.72ms
step:1332/1750 train_time:130170ms step_avg:97.73ms
step:1333/1750 train_time:130272ms step_avg:97.73ms
step:1334/1750 train_time:130373ms step_avg:97.73ms
step:1335/1750 train_time:130475ms step_avg:97.73ms
step:1336/1750 train_time:130578ms step_avg:97.74ms
step:1337/1750 train_time:130680ms step_avg:97.74ms
step:1338/1750 train_time:130781ms step_avg:97.74ms
step:1339/1750 train_time:130884ms step_avg:97.75ms
step:1340/1750 train_time:130986ms step_avg:97.75ms
step:1341/1750 train_time:131088ms step_avg:97.75ms
step:1342/1750 train_time:131191ms step_avg:97.76ms
step:1343/1750 train_time:131292ms step_avg:97.76ms
step:1344/1750 train_time:131394ms step_avg:97.76ms
step:1345/1750 train_time:131496ms step_avg:97.77ms
step:1346/1750 train_time:131598ms step_avg:97.77ms
step:1347/1750 train_time:131699ms step_avg:97.77ms
step:1348/1750 train_time:131802ms step_avg:97.78ms
step:1349/1750 train_time:131904ms step_avg:97.78ms
step:1350/1750 train_time:132007ms step_avg:97.78ms
step:1351/1750 train_time:132108ms step_avg:97.79ms
step:1352/1750 train_time:132211ms step_avg:97.79ms
step:1353/1750 train_time:132313ms step_avg:97.79ms
step:1354/1750 train_time:132414ms step_avg:97.79ms
step:1355/1750 train_time:132515ms step_avg:97.80ms
step:1356/1750 train_time:132618ms step_avg:97.80ms
step:1357/1750 train_time:132718ms step_avg:97.80ms
step:1358/1750 train_time:132819ms step_avg:97.81ms
step:1359/1750 train_time:132921ms step_avg:97.81ms
step:1360/1750 train_time:133023ms step_avg:97.81ms
step:1361/1750 train_time:133125ms step_avg:97.81ms
step:1362/1750 train_time:133228ms step_avg:97.82ms
step:1363/1750 train_time:133331ms step_avg:97.82ms
step:1364/1750 train_time:133432ms step_avg:97.82ms
step:1365/1750 train_time:133533ms step_avg:97.83ms
step:1366/1750 train_time:133635ms step_avg:97.83ms
step:1367/1750 train_time:133736ms step_avg:97.83ms
step:1368/1750 train_time:133840ms step_avg:97.84ms
step:1369/1750 train_time:133941ms step_avg:97.84ms
step:1370/1750 train_time:134043ms step_avg:97.84ms
step:1371/1750 train_time:134146ms step_avg:97.85ms
step:1372/1750 train_time:134249ms step_avg:97.85ms
step:1373/1750 train_time:134351ms step_avg:97.85ms
step:1374/1750 train_time:134452ms step_avg:97.85ms
step:1375/1750 train_time:134554ms step_avg:97.86ms
step:1375/1750 val_loss:3.3663 train_time:134650ms step_avg:97.93ms
step:1376/1750 train_time:134677ms step_avg:97.88ms
step:1377/1750 train_time:134771ms step_avg:97.87ms
step:1378/1750 train_time:134874ms step_avg:97.88ms
step:1379/1750 train_time:134976ms step_avg:97.88ms
step:1380/1750 train_time:135078ms step_avg:97.88ms
step:1381/1750 train_time:135179ms step_avg:97.88ms
step:1382/1750 train_time:135280ms step_avg:97.89ms
step:1383/1750 train_time:135380ms step_avg:97.89ms
step:1384/1750 train_time:135482ms step_avg:97.89ms
step:1385/1750 train_time:135583ms step_avg:97.89ms
step:1386/1750 train_time:135687ms step_avg:97.90ms
step:1387/1750 train_time:135790ms step_avg:97.90ms
step:1388/1750 train_time:135892ms step_avg:97.90ms
step:1389/1750 train_time:135994ms step_avg:97.91ms
step:1390/1750 train_time:136094ms step_avg:97.91ms
step:1391/1750 train_time:136197ms step_avg:97.91ms
step:1392/1750 train_time:136299ms step_avg:97.92ms
step:1393/1750 train_time:136400ms step_avg:97.92ms
step:1394/1750 train_time:136502ms step_avg:97.92ms
step:1395/1750 train_time:136605ms step_avg:97.92ms
step:1396/1750 train_time:136707ms step_avg:97.93ms
step:1397/1750 train_time:136809ms step_avg:97.93ms
step:1398/1750 train_time:136911ms step_avg:97.93ms
step:1399/1750 train_time:137014ms step_avg:97.94ms
step:1400/1750 train_time:137115ms step_avg:97.94ms
step:1401/1750 train_time:137217ms step_avg:97.94ms
step:1402/1750 train_time:137319ms step_avg:97.94ms
step:1403/1750 train_time:137421ms step_avg:97.95ms
step:1404/1750 train_time:137523ms step_avg:97.95ms
step:1405/1750 train_time:137623ms step_avg:97.95ms
step:1406/1750 train_time:137725ms step_avg:97.96ms
step:1407/1750 train_time:137828ms step_avg:97.96ms
step:1408/1750 train_time:137930ms step_avg:97.96ms
step:1409/1750 train_time:138033ms step_avg:97.97ms
step:1410/1750 train_time:138135ms step_avg:97.97ms
step:1411/1750 train_time:138236ms step_avg:97.97ms
step:1412/1750 train_time:138339ms step_avg:97.97ms
step:1413/1750 train_time:138440ms step_avg:97.98ms
step:1414/1750 train_time:138542ms step_avg:97.98ms
step:1415/1750 train_time:138645ms step_avg:97.98ms
step:1416/1750 train_time:138746ms step_avg:97.98ms
step:1417/1750 train_time:138848ms step_avg:97.99ms
step:1418/1750 train_time:138951ms step_avg:97.99ms
step:1419/1750 train_time:139053ms step_avg:97.99ms
step:1420/1750 train_time:139155ms step_avg:98.00ms
step:1421/1750 train_time:139257ms step_avg:98.00ms
step:1422/1750 train_time:139358ms step_avg:98.00ms
step:1423/1750 train_time:139459ms step_avg:98.00ms
step:1424/1750 train_time:139562ms step_avg:98.01ms
step:1425/1750 train_time:139664ms step_avg:98.01ms
step:1426/1750 train_time:139765ms step_avg:98.01ms
step:1427/1750 train_time:139867ms step_avg:98.01ms
step:1428/1750 train_time:139972ms step_avg:98.02ms
step:1429/1750 train_time:140075ms step_avg:98.02ms
step:1430/1750 train_time:140178ms step_avg:98.03ms
step:1431/1750 train_time:140281ms step_avg:98.03ms
step:1432/1750 train_time:140384ms step_avg:98.03ms
step:1433/1750 train_time:140487ms step_avg:98.04ms
step:1434/1750 train_time:140589ms step_avg:98.04ms
step:1435/1750 train_time:140692ms step_avg:98.04ms
step:1436/1750 train_time:140795ms step_avg:98.05ms
step:1437/1750 train_time:140899ms step_avg:98.05ms
step:1438/1750 train_time:141001ms step_avg:98.05ms
step:1439/1750 train_time:141104ms step_avg:98.06ms
step:1440/1750 train_time:141210ms step_avg:98.06ms
step:1441/1750 train_time:141316ms step_avg:98.07ms
step:1442/1750 train_time:141417ms step_avg:98.07ms
step:1443/1750 train_time:141520ms step_avg:98.07ms
step:1444/1750 train_time:141622ms step_avg:98.08ms
step:1445/1750 train_time:141724ms step_avg:98.08ms
step:1446/1750 train_time:141826ms step_avg:98.08ms
step:1447/1750 train_time:141929ms step_avg:98.09ms
step:1448/1750 train_time:142032ms step_avg:98.09ms
step:1449/1750 train_time:142134ms step_avg:98.09ms
step:1450/1750 train_time:142238ms step_avg:98.09ms
step:1451/1750 train_time:142340ms step_avg:98.10ms
step:1452/1750 train_time:142443ms step_avg:98.10ms
step:1453/1750 train_time:142548ms step_avg:98.11ms
step:1454/1750 train_time:142653ms step_avg:98.11ms
step:1455/1750 train_time:142755ms step_avg:98.11ms
step:1456/1750 train_time:142857ms step_avg:98.12ms
step:1457/1750 train_time:142960ms step_avg:98.12ms
step:1458/1750 train_time:143063ms step_avg:98.12ms
step:1459/1750 train_time:143167ms step_avg:98.13ms
step:1460/1750 train_time:143270ms step_avg:98.13ms
step:1461/1750 train_time:143374ms step_avg:98.13ms
step:1462/1750 train_time:143477ms step_avg:98.14ms
step:1463/1750 train_time:143580ms step_avg:98.14ms
step:1464/1750 train_time:143684ms step_avg:98.14ms
step:1465/1750 train_time:143787ms step_avg:98.15ms
step:1466/1750 train_time:143890ms step_avg:98.15ms
step:1467/1750 train_time:143992ms step_avg:98.15ms
step:1468/1750 train_time:144096ms step_avg:98.16ms
step:1469/1750 train_time:144200ms step_avg:98.16ms
step:1470/1750 train_time:144303ms step_avg:98.16ms
step:1471/1750 train_time:144405ms step_avg:98.17ms
step:1472/1750 train_time:144507ms step_avg:98.17ms
step:1473/1750 train_time:144612ms step_avg:98.18ms
step:1474/1750 train_time:144716ms step_avg:98.18ms
step:1475/1750 train_time:144818ms step_avg:98.18ms
step:1476/1750 train_time:144921ms step_avg:98.18ms
step:1477/1750 train_time:145024ms step_avg:98.19ms
step:1478/1750 train_time:145127ms step_avg:98.19ms
step:1479/1750 train_time:145230ms step_avg:98.19ms
step:1480/1750 train_time:145332ms step_avg:98.20ms
step:1481/1750 train_time:145435ms step_avg:98.20ms
step:1482/1750 train_time:145539ms step_avg:98.20ms
step:1483/1750 train_time:145641ms step_avg:98.21ms
step:1484/1750 train_time:145746ms step_avg:98.21ms
step:1485/1750 train_time:145850ms step_avg:98.22ms
step:1486/1750 train_time:145954ms step_avg:98.22ms
step:1487/1750 train_time:146057ms step_avg:98.22ms
step:1488/1750 train_time:146160ms step_avg:98.23ms
step:1489/1750 train_time:146263ms step_avg:98.23ms
step:1490/1750 train_time:146365ms step_avg:98.23ms
step:1491/1750 train_time:146468ms step_avg:98.23ms
step:1492/1750 train_time:146572ms step_avg:98.24ms
step:1493/1750 train_time:146675ms step_avg:98.24ms
step:1494/1750 train_time:146778ms step_avg:98.24ms
step:1495/1750 train_time:146880ms step_avg:98.25ms
step:1496/1750 train_time:146984ms step_avg:98.25ms
step:1497/1750 train_time:147086ms step_avg:98.25ms
step:1498/1750 train_time:147189ms step_avg:98.26ms
step:1499/1750 train_time:147292ms step_avg:98.26ms
step:1500/1750 train_time:147394ms step_avg:98.26ms
step:1500/1750 val_loss:3.3303 train_time:147491ms step_avg:98.33ms
step:1501/1750 train_time:147518ms step_avg:98.28ms
step:1502/1750 train_time:147610ms step_avg:98.28ms
step:1503/1750 train_time:147713ms step_avg:98.28ms
step:1504/1750 train_time:147815ms step_avg:98.28ms
step:1505/1750 train_time:147918ms step_avg:98.28ms
step:1506/1750 train_time:148020ms step_avg:98.29ms
step:1507/1750 train_time:148123ms step_avg:98.29ms
step:1508/1750 train_time:148224ms step_avg:98.29ms
step:1509/1750 train_time:148327ms step_avg:98.30ms
step:1510/1750 train_time:148430ms step_avg:98.30ms
step:1511/1750 train_time:148536ms step_avg:98.30ms
step:1512/1750 train_time:148640ms step_avg:98.31ms
step:1513/1750 train_time:148744ms step_avg:98.31ms
step:1514/1750 train_time:148848ms step_avg:98.31ms
step:1515/1750 train_time:148954ms step_avg:98.32ms
step:1516/1750 train_time:149057ms step_avg:98.32ms
step:1517/1750 train_time:149159ms step_avg:98.32ms
step:1518/1750 train_time:149262ms step_avg:98.33ms
step:1519/1750 train_time:149368ms step_avg:98.33ms
step:1520/1750 train_time:149470ms step_avg:98.34ms
step:1521/1750 train_time:149572ms step_avg:98.34ms
step:1522/1750 train_time:149674ms step_avg:98.34ms
step:1523/1750 train_time:149777ms step_avg:98.34ms
step:1524/1750 train_time:149881ms step_avg:98.35ms
step:1525/1750 train_time:149985ms step_avg:98.35ms
step:1526/1750 train_time:150089ms step_avg:98.35ms
step:1527/1750 train_time:150192ms step_avg:98.36ms
step:1528/1750 train_time:150298ms step_avg:98.36ms
step:1529/1750 train_time:150400ms step_avg:98.37ms
step:1530/1750 train_time:150504ms step_avg:98.37ms
step:1531/1750 train_time:150605ms step_avg:98.37ms
step:1532/1750 train_time:150708ms step_avg:98.37ms
step:1533/1750 train_time:150811ms step_avg:98.38ms
step:1534/1750 train_time:150913ms step_avg:98.38ms
step:1535/1750 train_time:151017ms step_avg:98.38ms
step:1536/1750 train_time:151120ms step_avg:98.39ms
step:1537/1750 train_time:151223ms step_avg:98.39ms
step:1538/1750 train_time:151327ms step_avg:98.39ms
step:1539/1750 train_time:151430ms step_avg:98.39ms
step:1540/1750 train_time:151532ms step_avg:98.40ms
step:1541/1750 train_time:151636ms step_avg:98.40ms
step:1542/1750 train_time:151740ms step_avg:98.40ms
step:1543/1750 train_time:151844ms step_avg:98.41ms
step:1544/1750 train_time:151948ms step_avg:98.41ms
step:1545/1750 train_time:152050ms step_avg:98.41ms
step:1546/1750 train_time:152152ms step_avg:98.42ms
step:1547/1750 train_time:152256ms step_avg:98.42ms
step:1548/1750 train_time:152360ms step_avg:98.42ms
step:1549/1750 train_time:152464ms step_avg:98.43ms
step:1550/1750 train_time:152568ms step_avg:98.43ms
step:1551/1750 train_time:152673ms step_avg:98.44ms
step:1552/1750 train_time:152775ms step_avg:98.44ms
step:1553/1750 train_time:152877ms step_avg:98.44ms
step:1554/1750 train_time:152980ms step_avg:98.44ms
step:1555/1750 train_time:153084ms step_avg:98.45ms
step:1556/1750 train_time:153187ms step_avg:98.45ms
step:1557/1750 train_time:153292ms step_avg:98.45ms
step:1558/1750 train_time:153396ms step_avg:98.46ms
step:1559/1750 train_time:153499ms step_avg:98.46ms
step:1560/1750 train_time:153603ms step_avg:98.46ms
step:1561/1750 train_time:153707ms step_avg:98.47ms
step:1562/1750 train_time:153810ms step_avg:98.47ms
step:1563/1750 train_time:153916ms step_avg:98.47ms
step:1564/1750 train_time:154019ms step_avg:98.48ms
step:1565/1750 train_time:154122ms step_avg:98.48ms
step:1566/1750 train_time:154225ms step_avg:98.48ms
step:1567/1750 train_time:154327ms step_avg:98.49ms
step:1568/1750 train_time:154430ms step_avg:98.49ms
step:1569/1750 train_time:154533ms step_avg:98.49ms
step:1570/1750 train_time:154639ms step_avg:98.50ms
step:1571/1750 train_time:154742ms step_avg:98.50ms
step:1572/1750 train_time:154845ms step_avg:98.50ms
step:1573/1750 train_time:154948ms step_avg:98.50ms
step:1574/1750 train_time:155051ms step_avg:98.51ms
step:1575/1750 train_time:155154ms step_avg:98.51ms
step:1576/1750 train_time:155256ms step_avg:98.51ms
step:1577/1750 train_time:155360ms step_avg:98.52ms
step:1578/1750 train_time:155463ms step_avg:98.52ms
step:1579/1750 train_time:155566ms step_avg:98.52ms
step:1580/1750 train_time:155670ms step_avg:98.53ms
step:1581/1750 train_time:155774ms step_avg:98.53ms
step:1582/1750 train_time:155877ms step_avg:98.53ms
step:1583/1750 train_time:155981ms step_avg:98.54ms
step:1584/1750 train_time:156088ms step_avg:98.54ms
step:1585/1750 train_time:156191ms step_avg:98.54ms
step:1586/1750 train_time:156295ms step_avg:98.55ms
step:1587/1750 train_time:156398ms step_avg:98.55ms
step:1588/1750 train_time:156501ms step_avg:98.55ms
step:1589/1750 train_time:156605ms step_avg:98.56ms
step:1590/1750 train_time:156709ms step_avg:98.56ms
step:1591/1750 train_time:156812ms step_avg:98.56ms
step:1592/1750 train_time:156915ms step_avg:98.56ms
step:1593/1750 train_time:157018ms step_avg:98.57ms
step:1594/1750 train_time:157123ms step_avg:98.57ms
step:1595/1750 train_time:157226ms step_avg:98.57ms
step:1596/1750 train_time:157329ms step_avg:98.58ms
step:1597/1750 train_time:157432ms step_avg:98.58ms
step:1598/1750 train_time:157537ms step_avg:98.58ms
step:1599/1750 train_time:157640ms step_avg:98.59ms
step:1600/1750 train_time:157745ms step_avg:98.59ms
step:1601/1750 train_time:157848ms step_avg:98.59ms
step:1602/1750 train_time:157951ms step_avg:98.60ms
step:1603/1750 train_time:158054ms step_avg:98.60ms
step:1604/1750 train_time:158157ms step_avg:98.60ms
step:1605/1750 train_time:158261ms step_avg:98.60ms
step:1606/1750 train_time:158366ms step_avg:98.61ms
step:1607/1750 train_time:158469ms step_avg:98.61ms
step:1608/1750 train_time:158571ms step_avg:98.61ms
step:1609/1750 train_time:158674ms step_avg:98.62ms
step:1610/1750 train_time:158778ms step_avg:98.62ms
step:1611/1750 train_time:158882ms step_avg:98.62ms
step:1612/1750 train_time:158987ms step_avg:98.63ms
step:1613/1750 train_time:159090ms step_avg:98.63ms
step:1614/1750 train_time:159192ms step_avg:98.63ms
step:1615/1750 train_time:159294ms step_avg:98.63ms
step:1616/1750 train_time:159398ms step_avg:98.64ms
step:1617/1750 train_time:159501ms step_avg:98.64ms
step:1618/1750 train_time:159607ms step_avg:98.64ms
step:1619/1750 train_time:159709ms step_avg:98.65ms
step:1620/1750 train_time:159813ms step_avg:98.65ms
step:1621/1750 train_time:159916ms step_avg:98.65ms
step:1622/1750 train_time:160018ms step_avg:98.65ms
step:1623/1750 train_time:160121ms step_avg:98.66ms
step:1624/1750 train_time:160225ms step_avg:98.66ms
step:1625/1750 train_time:160329ms step_avg:98.66ms
step:1625/1750 val_loss:3.3001 train_time:160426ms step_avg:98.72ms
step:1626/1750 train_time:160453ms step_avg:98.68ms
step:1627/1750 train_time:160544ms step_avg:98.67ms
step:1628/1750 train_time:160648ms step_avg:98.68ms
step:1629/1750 train_time:160750ms step_avg:98.68ms
step:1630/1750 train_time:160853ms step_avg:98.68ms
step:1631/1750 train_time:160956ms step_avg:98.69ms
step:1632/1750 train_time:161058ms step_avg:98.69ms
step:1633/1750 train_time:161161ms step_avg:98.69ms
step:1634/1750 train_time:161265ms step_avg:98.69ms
step:1635/1750 train_time:161367ms step_avg:98.70ms
step:1636/1750 train_time:161472ms step_avg:98.70ms
step:1637/1750 train_time:161575ms step_avg:98.70ms
step:1638/1750 train_time:161678ms step_avg:98.70ms
step:1639/1750 train_time:161780ms step_avg:98.71ms
step:1640/1750 train_time:161883ms step_avg:98.71ms
step:1641/1750 train_time:161986ms step_avg:98.71ms
step:1642/1750 train_time:162090ms step_avg:98.72ms
step:1643/1750 train_time:162193ms step_avg:98.72ms
step:1644/1750 train_time:162296ms step_avg:98.72ms
step:1645/1750 train_time:162399ms step_avg:98.72ms
step:1646/1750 train_time:162502ms step_avg:98.73ms
step:1647/1750 train_time:162608ms step_avg:98.73ms
step:1648/1750 train_time:162711ms step_avg:98.73ms
step:1649/1750 train_time:162813ms step_avg:98.73ms
step:1650/1750 train_time:162916ms step_avg:98.74ms
step:1651/1750 train_time:163019ms step_avg:98.74ms
step:1652/1750 train_time:163122ms step_avg:98.74ms
step:1653/1750 train_time:163226ms step_avg:98.75ms
step:1654/1750 train_time:163329ms step_avg:98.75ms
step:1655/1750 train_time:163433ms step_avg:98.75ms
step:1656/1750 train_time:163538ms step_avg:98.75ms
step:1657/1750 train_time:163640ms step_avg:98.76ms
step:1658/1750 train_time:163743ms step_avg:98.76ms
step:1659/1750 train_time:163849ms step_avg:98.76ms
step:1660/1750 train_time:163952ms step_avg:98.77ms
step:1661/1750 train_time:164058ms step_avg:98.77ms
step:1662/1750 train_time:164161ms step_avg:98.77ms
step:1663/1750 train_time:164265ms step_avg:98.78ms
step:1664/1750 train_time:164368ms step_avg:98.78ms
step:1665/1750 train_time:164473ms step_avg:98.78ms
step:1666/1750 train_time:164576ms step_avg:98.79ms
step:1667/1750 train_time:164678ms step_avg:98.79ms
step:1668/1750 train_time:164784ms step_avg:98.79ms
step:1669/1750 train_time:164888ms step_avg:98.79ms
step:1670/1750 train_time:164990ms step_avg:98.80ms
step:1671/1750 train_time:165093ms step_avg:98.80ms
step:1672/1750 train_time:165198ms step_avg:98.80ms
step:1673/1750 train_time:165300ms step_avg:98.80ms
step:1674/1750 train_time:165405ms step_avg:98.81ms
step:1675/1750 train_time:165508ms step_avg:98.81ms
step:1676/1750 train_time:165611ms step_avg:98.81ms
step:1677/1750 train_time:165714ms step_avg:98.82ms
step:1678/1750 train_time:165818ms step_avg:98.82ms
step:1679/1750 train_time:165922ms step_avg:98.82ms
step:1680/1750 train_time:166024ms step_avg:98.82ms
step:1681/1750 train_time:166128ms step_avg:98.83ms
step:1682/1750 train_time:166233ms step_avg:98.83ms
step:1683/1750 train_time:166336ms step_avg:98.83ms
step:1684/1750 train_time:166440ms step_avg:98.84ms
step:1685/1750 train_time:166544ms step_avg:98.84ms
step:1686/1750 train_time:166646ms step_avg:98.84ms
step:1687/1750 train_time:166750ms step_avg:98.84ms
step:1688/1750 train_time:166854ms step_avg:98.85ms
step:1689/1750 train_time:166959ms step_avg:98.85ms
step:1690/1750 train_time:167064ms step_avg:98.85ms
step:1691/1750 train_time:167168ms step_avg:98.86ms
step:1692/1750 train_time:167271ms step_avg:98.86ms
step:1693/1750 train_time:167375ms step_avg:98.86ms
step:1694/1750 train_time:167479ms step_avg:98.87ms
step:1695/1750 train_time:167583ms step_avg:98.87ms
step:1696/1750 train_time:167686ms step_avg:98.87ms
step:1697/1750 train_time:167793ms step_avg:98.88ms
step:1698/1750 train_time:167896ms step_avg:98.88ms
step:1699/1750 train_time:167999ms step_avg:98.88ms
step:1700/1750 train_time:168104ms step_avg:98.88ms
step:1701/1750 train_time:168207ms step_avg:98.89ms
step:1702/1750 train_time:168312ms step_avg:98.89ms
step:1703/1750 train_time:168417ms step_avg:98.89ms
step:1704/1750 train_time:168521ms step_avg:98.90ms
step:1705/1750 train_time:168624ms step_avg:98.90ms
step:1706/1750 train_time:168728ms step_avg:98.90ms
step:1707/1750 train_time:168833ms step_avg:98.91ms
step:1708/1750 train_time:168939ms step_avg:98.91ms
step:1709/1750 train_time:169043ms step_avg:98.91ms
step:1710/1750 train_time:169147ms step_avg:98.92ms
step:1711/1750 train_time:169252ms step_avg:98.92ms
step:1712/1750 train_time:169356ms step_avg:98.92ms
step:1713/1750 train_time:169460ms step_avg:98.93ms
step:1714/1750 train_time:169562ms step_avg:98.93ms
step:1715/1750 train_time:169669ms step_avg:98.93ms
step:1716/1750 train_time:169773ms step_avg:98.94ms
step:1717/1750 train_time:169877ms step_avg:98.94ms
step:1718/1750 train_time:169981ms step_avg:98.94ms
step:1719/1750 train_time:170089ms step_avg:98.95ms
step:1720/1750 train_time:170192ms step_avg:98.95ms
step:1721/1750 train_time:170297ms step_avg:98.95ms
step:1722/1750 train_time:170401ms step_avg:98.96ms
step:1723/1750 train_time:170505ms step_avg:98.96ms
step:1724/1750 train_time:170609ms step_avg:98.96ms
step:1725/1750 train_time:170714ms step_avg:98.96ms
step:1726/1750 train_time:170818ms step_avg:98.97ms
step:1727/1750 train_time:170922ms step_avg:98.97ms
step:1728/1750 train_time:171028ms step_avg:98.97ms
step:1729/1750 train_time:171132ms step_avg:98.98ms
step:1730/1750 train_time:171235ms step_avg:98.98ms
step:1731/1750 train_time:171340ms step_avg:98.98ms
step:1732/1750 train_time:171443ms step_avg:98.99ms
step:1733/1750 train_time:171547ms step_avg:98.99ms
step:1734/1750 train_time:171652ms step_avg:98.99ms
step:1735/1750 train_time:171756ms step_avg:98.99ms
step:1736/1750 train_time:171861ms step_avg:99.00ms
step:1737/1750 train_time:171966ms step_avg:99.00ms
step:1738/1750 train_time:172069ms step_avg:99.00ms
step:1739/1750 train_time:172174ms step_avg:99.01ms
step:1740/1750 train_time:172279ms step_avg:99.01ms
step:1741/1750 train_time:172388ms step_avg:99.02ms
step:1742/1750 train_time:172492ms step_avg:99.02ms
step:1743/1750 train_time:172597ms step_avg:99.02ms
step:1744/1750 train_time:172701ms step_avg:99.03ms
step:1745/1750 train_time:172805ms step_avg:99.03ms
step:1746/1750 train_time:172908ms step_avg:99.03ms
step:1747/1750 train_time:173012ms step_avg:99.03ms
step:1748/1750 train_time:173117ms step_avg:99.04ms
step:1749/1750 train_time:173221ms step_avg:99.04ms
step:1750/1750 train_time:173326ms step_avg:99.04ms
step:1750/1750 val_loss:3.2791 train_time:173424ms step_avg:99.10ms
peak memory allocated: 33277 MiB reserved: 48972 MiB

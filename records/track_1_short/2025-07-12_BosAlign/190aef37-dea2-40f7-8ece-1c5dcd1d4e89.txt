import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn, autocast
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
#import wandb

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)

        params = list(params)
        sizes = {p.shape for p in params}

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params,))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * self.world_size
            for base_i in range(0, len(params), self.world_size):
                if base_i + self.rank < len(params):
                    grad = params[base_i + self.rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + self.world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * self.world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), self.world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                futures.append(dist.all_gather(params_pad[base_i:base_i + self.world_size], params_pad[base_i + self.rank], async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01, rank: int = 0, world_size: int = 1):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        self.rank = rank
        self.world_size = world_size

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(
                params=group_params,
            ))
        super().__init__(param_groups, defaults)

    @torch.compile
    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // self.world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)

                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']

                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)

                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t

                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        for param in self.embed.parameters():
            param.lr_mul = 75.
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embeds in self.value_embeds:
            for param in self.value_embeds.parameters():
                param.lr_mul = 75.
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.lr_mul = 27.5
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % world_size
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            #return causal_mask
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, world_size: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos:pos+max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()

    starts = []
    batch_end=None
    for i in range(len(boundary_positions) - 1):
        end = boundary_positions[i + 1].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == world_size:
                batch_end = end
                break
            start = end
    assert batch_end is not None # increase max_batch_span if necessary
    batch_span = batch_end-pos
    return starts, batch_span

def distributed_data_generator(filename_pattern: str, batch_size: int, rank: int, world_size: int, align_to_bos: bool):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    batch_span = batch_size
    max_batch_span = 2*batch_size if align_to_bos else batch_size #provide buffer to handle samples up to length local_batch_size

    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, world_size, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    train_align_to_bos = True # align local batch start indicies with next bos_token
    val_align_to_bos = False # False to maintain same eval as prior records
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

#if master_process:
#    wandb.init(project="modded-nanogpt-tiny", name=f"run-{os.path.basename(__file__)}", save_code=True)

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=True):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
if master_process:
    print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=next_multiple_of_n(50257, n=128), num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094

optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0, rank=rank, world_size=world_size)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]

for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

for n, p in model.named_parameters():
    wd_mul = getattr(p, "wd_mul", 1.0)
    lr_mul = getattr(p, "lr_mul", 1.0)

    print0(f"{n}: {p.shape} {p.dtype} {wd_mul} {lr_mul}")

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
embedding_params = sum(p.numel() for n, p in model.named_parameters() if "embed" in n)
non_embedding_params = total_params - embedding_params

print0(f"")
print0(f"Model parameters:")
print0(f"  Total parameters: {total_params:,}")
print0(f"  Embedding parameters: {embedding_params:,}")
print0(f"  Non-embedding parameters: {non_embedding_params:,}")

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    w = min((1 - x) / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.05
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)

torch.cuda.synchronize()
dist.barrier()
with torch.profiler.profile() as prof:
    for _ in range(warmup_steps):
        inputs, targets = next(train_loader)
        model(inputs, targets, get_window_size_blocks(1)).backward()
        for opt in optimizers:
            opt.step()
    model.zero_grad(set_to_none=True)
    torch.cuda.synchronize()
    dist.barrier()
os.makedirs("traces", exist_ok=True)
prof.export_chrome_trace(f"traces/trace_{rank}.json")

model.load_state_dict(initial_state['model'])
for opt, opt_state in zip(optimizers, initial_state['optimizers']):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size, args.val_align_to_bos)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        #if master_process:
        #    wandb.log({"val/loss": val_loss}, step=step)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.8.0.dev20250524+cu126 compiled for CUDA 12.6
Sun Jul 13 01:49:17 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    5856MiB /  81559MiB |      2%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   37C    P0            121W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   38C    P0            123W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    1517MiB /  81559MiB |      3%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1517MiB /  81559MiB |      3%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   39C    P0            121W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   38C    P0            122W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   31C    P0            115W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           93701      C   /usr/bin/python3                       1508MiB |
|    0   N/A  N/A           93702      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           93703      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           93704      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           93705      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           93706      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           93707      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           93708      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A           93702      C   /usr/bin/python3                       1508MiB |
|    2   N/A  N/A           93703      C   /usr/bin/python3                       1508MiB |
|    3   N/A  N/A           93704      C   /usr/bin/python3                       1508MiB |
|    4   N/A  N/A           93705      C   /usr/bin/python3                       1508MiB |
|    5   N/A  N/A           93706      C   /usr/bin/python3                       1508MiB |
|    6   N/A  N/A           93707      C   /usr/bin/python3                       1508MiB |
|    7   N/A  N/A           93708      C   /usr/bin/python3                       1508MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
scalars: torch.Size([64]) torch.float32 1.0 5.0
embed.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.0.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.1.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.2.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
blocks.0.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.0.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.1.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.1.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.2.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.2.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.3.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.3.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.4.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.4.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.5.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.5.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.6.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.6.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.7.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.7.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.8.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.8.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.9.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.9.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.10.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.10.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.11.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.11.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
lm_head.weight: torch.Size([50304, 768]) torch.float32 1.0 27.5

Model parameters:
  Total parameters: 275,742,784
  Embedding parameters: 154,533,888
  Non-embedding parameters: 121,208,896
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.01ms
step:1/1750 train_time:152ms step_avg:152.42ms
step:2/1750 train_time:178ms step_avg:88.83ms
step:3/1750 train_time:250ms step_avg:83.50ms
step:4/1750 train_time:342ms step_avg:85.57ms
step:5/1750 train_time:435ms step_avg:86.95ms
step:6/1750 train_time:527ms step_avg:87.89ms
step:7/1750 train_time:619ms step_avg:88.48ms
step:8/1750 train_time:712ms step_avg:88.96ms
step:9/1750 train_time:804ms step_avg:89.33ms
step:10/1750 train_time:897ms step_avg:89.66ms
step:11/1750 train_time:989ms step_avg:89.93ms
step:12/1750 train_time:1084ms step_avg:90.31ms
step:13/1750 train_time:1179ms step_avg:90.67ms
step:14/1750 train_time:1274ms step_avg:91.00ms
step:15/1750 train_time:1368ms step_avg:91.18ms
step:16/1750 train_time:1460ms step_avg:91.26ms
step:17/1750 train_time:1553ms step_avg:91.38ms
step:18/1750 train_time:1647ms step_avg:91.47ms
step:19/1750 train_time:1739ms step_avg:91.52ms
step:20/1750 train_time:1832ms step_avg:91.59ms
step:21/1750 train_time:1925ms step_avg:91.65ms
step:22/1750 train_time:2017ms step_avg:91.69ms
step:23/1750 train_time:2111ms step_avg:91.77ms
step:24/1750 train_time:2205ms step_avg:91.89ms
step:25/1750 train_time:2298ms step_avg:91.93ms
step:26/1750 train_time:2392ms step_avg:92.01ms
step:27/1750 train_time:2485ms step_avg:92.05ms
step:28/1750 train_time:2578ms step_avg:92.08ms
step:29/1750 train_time:2672ms step_avg:92.13ms
step:30/1750 train_time:2765ms step_avg:92.17ms
step:31/1750 train_time:2858ms step_avg:92.20ms
step:32/1750 train_time:2952ms step_avg:92.25ms
step:33/1750 train_time:3045ms step_avg:92.28ms
step:34/1750 train_time:3138ms step_avg:92.30ms
step:35/1750 train_time:3232ms step_avg:92.33ms
step:36/1750 train_time:3325ms step_avg:92.37ms
step:37/1750 train_time:3418ms step_avg:92.38ms
step:38/1750 train_time:3512ms step_avg:92.42ms
step:39/1750 train_time:3606ms step_avg:92.46ms
step:40/1750 train_time:3699ms step_avg:92.47ms
step:41/1750 train_time:3792ms step_avg:92.50ms
step:42/1750 train_time:3885ms step_avg:92.51ms
step:43/1750 train_time:3978ms step_avg:92.51ms
step:44/1750 train_time:4072ms step_avg:92.55ms
step:45/1750 train_time:4166ms step_avg:92.57ms
step:46/1750 train_time:4259ms step_avg:92.58ms
step:47/1750 train_time:4352ms step_avg:92.60ms
step:48/1750 train_time:4446ms step_avg:92.62ms
step:49/1750 train_time:4538ms step_avg:92.62ms
step:50/1750 train_time:4632ms step_avg:92.64ms
step:51/1750 train_time:4725ms step_avg:92.64ms
step:52/1750 train_time:4817ms step_avg:92.64ms
step:53/1750 train_time:4911ms step_avg:92.65ms
step:54/1750 train_time:5004ms step_avg:92.67ms
step:55/1750 train_time:5097ms step_avg:92.67ms
step:56/1750 train_time:5191ms step_avg:92.70ms
step:57/1750 train_time:5285ms step_avg:92.72ms
step:58/1750 train_time:5379ms step_avg:92.73ms
step:59/1750 train_time:5472ms step_avg:92.74ms
step:60/1750 train_time:5566ms step_avg:92.76ms
step:61/1750 train_time:5658ms step_avg:92.76ms
step:62/1750 train_time:5752ms step_avg:92.77ms
step:63/1750 train_time:5846ms step_avg:92.79ms
step:64/1750 train_time:5939ms step_avg:92.80ms
step:65/1750 train_time:6033ms step_avg:92.81ms
step:66/1750 train_time:6126ms step_avg:92.82ms
step:67/1750 train_time:6219ms step_avg:92.82ms
step:68/1750 train_time:6312ms step_avg:92.82ms
step:69/1750 train_time:6405ms step_avg:92.83ms
step:70/1750 train_time:6498ms step_avg:92.83ms
step:71/1750 train_time:6592ms step_avg:92.84ms
step:72/1750 train_time:6686ms step_avg:92.86ms
step:73/1750 train_time:6778ms step_avg:92.85ms
step:74/1750 train_time:6873ms step_avg:92.88ms
step:75/1750 train_time:6966ms step_avg:92.88ms
step:76/1750 train_time:7059ms step_avg:92.88ms
step:77/1750 train_time:7153ms step_avg:92.90ms
step:78/1750 train_time:7248ms step_avg:92.92ms
step:79/1750 train_time:7341ms step_avg:92.92ms
step:80/1750 train_time:7433ms step_avg:92.92ms
step:81/1750 train_time:7527ms step_avg:92.92ms
step:82/1750 train_time:7620ms step_avg:92.92ms
step:83/1750 train_time:7713ms step_avg:92.92ms
step:84/1750 train_time:7807ms step_avg:92.94ms
step:85/1750 train_time:7900ms step_avg:92.94ms
step:86/1750 train_time:7993ms step_avg:92.94ms
step:87/1750 train_time:8085ms step_avg:92.93ms
step:88/1750 train_time:8178ms step_avg:92.93ms
step:89/1750 train_time:8273ms step_avg:92.96ms
step:90/1750 train_time:8367ms step_avg:92.97ms
step:91/1750 train_time:8459ms step_avg:92.96ms
step:92/1750 train_time:8553ms step_avg:92.97ms
step:93/1750 train_time:8646ms step_avg:92.97ms
step:94/1750 train_time:8739ms step_avg:92.97ms
step:95/1750 train_time:8834ms step_avg:92.98ms
step:96/1750 train_time:8927ms step_avg:92.99ms
step:97/1750 train_time:9019ms step_avg:92.98ms
step:98/1750 train_time:9113ms step_avg:92.99ms
step:99/1750 train_time:9207ms step_avg:93.00ms
step:100/1750 train_time:9300ms step_avg:93.00ms
step:101/1750 train_time:9393ms step_avg:93.00ms
step:102/1750 train_time:9487ms step_avg:93.01ms
step:103/1750 train_time:9580ms step_avg:93.01ms
step:104/1750 train_time:9674ms step_avg:93.02ms
step:105/1750 train_time:9767ms step_avg:93.02ms
step:106/1750 train_time:9861ms step_avg:93.02ms
step:107/1750 train_time:9954ms step_avg:93.03ms
step:108/1750 train_time:10047ms step_avg:93.02ms
step:109/1750 train_time:10139ms step_avg:93.02ms
step:110/1750 train_time:10234ms step_avg:93.03ms
step:111/1750 train_time:10327ms step_avg:93.03ms
step:112/1750 train_time:10419ms step_avg:93.03ms
step:113/1750 train_time:10513ms step_avg:93.03ms
step:114/1750 train_time:10606ms step_avg:93.03ms
step:115/1750 train_time:10699ms step_avg:93.03ms
step:116/1750 train_time:10793ms step_avg:93.04ms
step:117/1750 train_time:10886ms step_avg:93.04ms
step:118/1750 train_time:10979ms step_avg:93.04ms
step:119/1750 train_time:11073ms step_avg:93.05ms
step:120/1750 train_time:11167ms step_avg:93.06ms
step:121/1750 train_time:11260ms step_avg:93.06ms
step:122/1750 train_time:11354ms step_avg:93.07ms
step:123/1750 train_time:11447ms step_avg:93.07ms
step:124/1750 train_time:11540ms step_avg:93.06ms
step:125/1750 train_time:11634ms step_avg:93.07ms
step:125/1750 val_loss:4.6215 train_time:11722ms step_avg:93.78ms
step:126/1750 train_time:11751ms step_avg:93.27ms
step:127/1750 train_time:11825ms step_avg:93.11ms
step:128/1750 train_time:11927ms step_avg:93.18ms
step:129/1750 train_time:12022ms step_avg:93.19ms
step:130/1750 train_time:12115ms step_avg:93.19ms
step:131/1750 train_time:12208ms step_avg:93.19ms
step:132/1750 train_time:12301ms step_avg:93.19ms
step:133/1750 train_time:12394ms step_avg:93.19ms
step:134/1750 train_time:12486ms step_avg:93.18ms
step:135/1750 train_time:12580ms step_avg:93.18ms
step:136/1750 train_time:12673ms step_avg:93.18ms
step:137/1750 train_time:12766ms step_avg:93.18ms
step:138/1750 train_time:12862ms step_avg:93.21ms
step:139/1750 train_time:12958ms step_avg:93.22ms
step:140/1750 train_time:13052ms step_avg:93.23ms
step:141/1750 train_time:13146ms step_avg:93.23ms
step:142/1750 train_time:13239ms step_avg:93.23ms
step:143/1750 train_time:13333ms step_avg:93.24ms
step:144/1750 train_time:13426ms step_avg:93.24ms
step:145/1750 train_time:13520ms step_avg:93.24ms
step:146/1750 train_time:13614ms step_avg:93.25ms
step:147/1750 train_time:13708ms step_avg:93.25ms
step:148/1750 train_time:13802ms step_avg:93.26ms
step:149/1750 train_time:13897ms step_avg:93.27ms
step:150/1750 train_time:13991ms step_avg:93.27ms
step:151/1750 train_time:14085ms step_avg:93.28ms
step:152/1750 train_time:14179ms step_avg:93.28ms
step:153/1750 train_time:14273ms step_avg:93.28ms
step:154/1750 train_time:14366ms step_avg:93.28ms
step:155/1750 train_time:14459ms step_avg:93.28ms
step:156/1750 train_time:14552ms step_avg:93.29ms
step:157/1750 train_time:14646ms step_avg:93.29ms
step:158/1750 train_time:14740ms step_avg:93.29ms
step:159/1750 train_time:14834ms step_avg:93.29ms
step:160/1750 train_time:14928ms step_avg:93.30ms
step:161/1750 train_time:15023ms step_avg:93.31ms
step:162/1750 train_time:15117ms step_avg:93.32ms
step:163/1750 train_time:15211ms step_avg:93.32ms
step:164/1750 train_time:15304ms step_avg:93.32ms
step:165/1750 train_time:15399ms step_avg:93.32ms
step:166/1750 train_time:15492ms step_avg:93.32ms
step:167/1750 train_time:15585ms step_avg:93.32ms
step:168/1750 train_time:15678ms step_avg:93.32ms
step:169/1750 train_time:15772ms step_avg:93.33ms
step:170/1750 train_time:15865ms step_avg:93.33ms
step:171/1750 train_time:15960ms step_avg:93.33ms
step:172/1750 train_time:16054ms step_avg:93.34ms
step:173/1750 train_time:16148ms step_avg:93.34ms
step:174/1750 train_time:16242ms step_avg:93.34ms
step:175/1750 train_time:16336ms step_avg:93.35ms
step:176/1750 train_time:16429ms step_avg:93.35ms
step:177/1750 train_time:16523ms step_avg:93.35ms
step:178/1750 train_time:16617ms step_avg:93.36ms
step:179/1750 train_time:16711ms step_avg:93.36ms
step:180/1750 train_time:16804ms step_avg:93.36ms
step:181/1750 train_time:16898ms step_avg:93.36ms
step:182/1750 train_time:16992ms step_avg:93.36ms
step:183/1750 train_time:17086ms step_avg:93.36ms
step:184/1750 train_time:17180ms step_avg:93.37ms
step:185/1750 train_time:17274ms step_avg:93.37ms
step:186/1750 train_time:17367ms step_avg:93.37ms
step:187/1750 train_time:17462ms step_avg:93.38ms
step:188/1750 train_time:17556ms step_avg:93.38ms
step:189/1750 train_time:17649ms step_avg:93.38ms
step:190/1750 train_time:17742ms step_avg:93.38ms
step:191/1750 train_time:17836ms step_avg:93.38ms
step:192/1750 train_time:17929ms step_avg:93.38ms
step:193/1750 train_time:18024ms step_avg:93.39ms
step:194/1750 train_time:18118ms step_avg:93.39ms
step:195/1750 train_time:18212ms step_avg:93.40ms
step:196/1750 train_time:18306ms step_avg:93.40ms
step:197/1750 train_time:18400ms step_avg:93.40ms
step:198/1750 train_time:18493ms step_avg:93.40ms
step:199/1750 train_time:18587ms step_avg:93.40ms
step:200/1750 train_time:18681ms step_avg:93.41ms
step:201/1750 train_time:18775ms step_avg:93.41ms
step:202/1750 train_time:18868ms step_avg:93.41ms
step:203/1750 train_time:18962ms step_avg:93.41ms
step:204/1750 train_time:19057ms step_avg:93.41ms
step:205/1750 train_time:19150ms step_avg:93.42ms
step:206/1750 train_time:19244ms step_avg:93.42ms
step:207/1750 train_time:19338ms step_avg:93.42ms
step:208/1750 train_time:19431ms step_avg:93.42ms
step:209/1750 train_time:19525ms step_avg:93.42ms
step:210/1750 train_time:19620ms step_avg:93.43ms
step:211/1750 train_time:19713ms step_avg:93.43ms
step:212/1750 train_time:19807ms step_avg:93.43ms
step:213/1750 train_time:19901ms step_avg:93.43ms
step:214/1750 train_time:19996ms step_avg:93.44ms
step:215/1750 train_time:20090ms step_avg:93.44ms
step:216/1750 train_time:20184ms step_avg:93.44ms
step:217/1750 train_time:20278ms step_avg:93.45ms
step:218/1750 train_time:20372ms step_avg:93.45ms
step:219/1750 train_time:20465ms step_avg:93.45ms
step:220/1750 train_time:20559ms step_avg:93.45ms
step:221/1750 train_time:20652ms step_avg:93.45ms
step:222/1750 train_time:20746ms step_avg:93.45ms
step:223/1750 train_time:20840ms step_avg:93.45ms
step:224/1750 train_time:20933ms step_avg:93.45ms
step:225/1750 train_time:21027ms step_avg:93.45ms
step:226/1750 train_time:21122ms step_avg:93.46ms
step:227/1750 train_time:21216ms step_avg:93.46ms
step:228/1750 train_time:21310ms step_avg:93.46ms
step:229/1750 train_time:21404ms step_avg:93.47ms
step:230/1750 train_time:21497ms step_avg:93.47ms
step:231/1750 train_time:21591ms step_avg:93.47ms
step:232/1750 train_time:21684ms step_avg:93.47ms
step:233/1750 train_time:21779ms step_avg:93.47ms
step:234/1750 train_time:21872ms step_avg:93.47ms
step:235/1750 train_time:21966ms step_avg:93.47ms
step:236/1750 train_time:22060ms step_avg:93.47ms
step:237/1750 train_time:22154ms step_avg:93.48ms
step:238/1750 train_time:22247ms step_avg:93.48ms
step:239/1750 train_time:22341ms step_avg:93.48ms
step:240/1750 train_time:22434ms step_avg:93.48ms
step:241/1750 train_time:22528ms step_avg:93.48ms
step:242/1750 train_time:22622ms step_avg:93.48ms
step:243/1750 train_time:22716ms step_avg:93.48ms
step:244/1750 train_time:22809ms step_avg:93.48ms
step:245/1750 train_time:22903ms step_avg:93.48ms
step:246/1750 train_time:22997ms step_avg:93.48ms
step:247/1750 train_time:23090ms step_avg:93.48ms
step:248/1750 train_time:23184ms step_avg:93.48ms
step:249/1750 train_time:23277ms step_avg:93.48ms
step:250/1750 train_time:23370ms step_avg:93.48ms
step:250/1750 val_loss:4.0926 train_time:23459ms step_avg:93.84ms
step:251/1750 train_time:23486ms step_avg:93.57ms
step:252/1750 train_time:23565ms step_avg:93.51ms
step:253/1750 train_time:23664ms step_avg:93.53ms
step:254/1750 train_time:23759ms step_avg:93.54ms
step:255/1750 train_time:23852ms step_avg:93.54ms
step:256/1750 train_time:23945ms step_avg:93.53ms
step:257/1750 train_time:24038ms step_avg:93.53ms
step:258/1750 train_time:24131ms step_avg:93.53ms
step:259/1750 train_time:24224ms step_avg:93.53ms
step:260/1750 train_time:24317ms step_avg:93.53ms
step:261/1750 train_time:24412ms step_avg:93.53ms
step:262/1750 train_time:24508ms step_avg:93.54ms
step:263/1750 train_time:24603ms step_avg:93.55ms
step:264/1750 train_time:24699ms step_avg:93.56ms
step:265/1750 train_time:24794ms step_avg:93.56ms
step:266/1750 train_time:24888ms step_avg:93.57ms
step:267/1750 train_time:24982ms step_avg:93.57ms
step:268/1750 train_time:25077ms step_avg:93.57ms
step:269/1750 train_time:25171ms step_avg:93.57ms
step:270/1750 train_time:25264ms step_avg:93.57ms
step:271/1750 train_time:25358ms step_avg:93.57ms
step:272/1750 train_time:25452ms step_avg:93.57ms
step:273/1750 train_time:25547ms step_avg:93.58ms
step:274/1750 train_time:25641ms step_avg:93.58ms
step:275/1750 train_time:25736ms step_avg:93.58ms
step:276/1750 train_time:25830ms step_avg:93.59ms
step:277/1750 train_time:25924ms step_avg:93.59ms
step:278/1750 train_time:26018ms step_avg:93.59ms
step:279/1750 train_time:26112ms step_avg:93.59ms
step:280/1750 train_time:26206ms step_avg:93.59ms
step:281/1750 train_time:26300ms step_avg:93.60ms
step:282/1750 train_time:26394ms step_avg:93.60ms
step:283/1750 train_time:26488ms step_avg:93.60ms
step:284/1750 train_time:26583ms step_avg:93.60ms
step:285/1750 train_time:26678ms step_avg:93.61ms
step:286/1750 train_time:26772ms step_avg:93.61ms
step:287/1750 train_time:26866ms step_avg:93.61ms
step:288/1750 train_time:26961ms step_avg:93.61ms
step:289/1750 train_time:27055ms step_avg:93.62ms
step:290/1750 train_time:27149ms step_avg:93.62ms
step:291/1750 train_time:27242ms step_avg:93.62ms
step:292/1750 train_time:27337ms step_avg:93.62ms
step:293/1750 train_time:27431ms step_avg:93.62ms
step:294/1750 train_time:27525ms step_avg:93.62ms
step:295/1750 train_time:27620ms step_avg:93.63ms
step:296/1750 train_time:27715ms step_avg:93.63ms
step:297/1750 train_time:27809ms step_avg:93.63ms
step:298/1750 train_time:27903ms step_avg:93.64ms
step:299/1750 train_time:27998ms step_avg:93.64ms
step:300/1750 train_time:28093ms step_avg:93.64ms
step:301/1750 train_time:28188ms step_avg:93.65ms
step:302/1750 train_time:28282ms step_avg:93.65ms
step:303/1750 train_time:28376ms step_avg:93.65ms
step:304/1750 train_time:28470ms step_avg:93.65ms
step:305/1750 train_time:28564ms step_avg:93.65ms
step:306/1750 train_time:28658ms step_avg:93.65ms
step:307/1750 train_time:28753ms step_avg:93.66ms
step:308/1750 train_time:28846ms step_avg:93.66ms
step:309/1750 train_time:28941ms step_avg:93.66ms
step:310/1750 train_time:29035ms step_avg:93.66ms
step:311/1750 train_time:29130ms step_avg:93.66ms
step:312/1750 train_time:29224ms step_avg:93.67ms
step:313/1750 train_time:29320ms step_avg:93.67ms
step:314/1750 train_time:29414ms step_avg:93.68ms
step:315/1750 train_time:29508ms step_avg:93.68ms
step:316/1750 train_time:29602ms step_avg:93.68ms
step:317/1750 train_time:29696ms step_avg:93.68ms
step:318/1750 train_time:29791ms step_avg:93.68ms
step:319/1750 train_time:29885ms step_avg:93.68ms
step:320/1750 train_time:29980ms step_avg:93.69ms
step:321/1750 train_time:30074ms step_avg:93.69ms
step:322/1750 train_time:30168ms step_avg:93.69ms
step:323/1750 train_time:30263ms step_avg:93.69ms
step:324/1750 train_time:30358ms step_avg:93.70ms
step:325/1750 train_time:30453ms step_avg:93.70ms
step:326/1750 train_time:30547ms step_avg:93.70ms
step:327/1750 train_time:30642ms step_avg:93.70ms
step:328/1750 train_time:30737ms step_avg:93.71ms
step:329/1750 train_time:30831ms step_avg:93.71ms
step:330/1750 train_time:30924ms step_avg:93.71ms
step:331/1750 train_time:31020ms step_avg:93.72ms
step:332/1750 train_time:31114ms step_avg:93.72ms
step:333/1750 train_time:31209ms step_avg:93.72ms
step:334/1750 train_time:31303ms step_avg:93.72ms
step:335/1750 train_time:31397ms step_avg:93.72ms
step:336/1750 train_time:31492ms step_avg:93.73ms
step:337/1750 train_time:31586ms step_avg:93.73ms
step:338/1750 train_time:31680ms step_avg:93.73ms
step:339/1750 train_time:31775ms step_avg:93.73ms
step:340/1750 train_time:31869ms step_avg:93.73ms
step:341/1750 train_time:31963ms step_avg:93.73ms
step:342/1750 train_time:32058ms step_avg:93.74ms
step:343/1750 train_time:32152ms step_avg:93.74ms
step:344/1750 train_time:32246ms step_avg:93.74ms
step:345/1750 train_time:32341ms step_avg:93.74ms
step:346/1750 train_time:32436ms step_avg:93.74ms
step:347/1750 train_time:32530ms step_avg:93.75ms
step:348/1750 train_time:32624ms step_avg:93.75ms
step:349/1750 train_time:32719ms step_avg:93.75ms
step:350/1750 train_time:32814ms step_avg:93.75ms
step:351/1750 train_time:32908ms step_avg:93.75ms
step:352/1750 train_time:33002ms step_avg:93.76ms
step:353/1750 train_time:33097ms step_avg:93.76ms
step:354/1750 train_time:33191ms step_avg:93.76ms
step:355/1750 train_time:33285ms step_avg:93.76ms
step:356/1750 train_time:33380ms step_avg:93.76ms
step:357/1750 train_time:33475ms step_avg:93.77ms
step:358/1750 train_time:33568ms step_avg:93.77ms
step:359/1750 train_time:33663ms step_avg:93.77ms
step:360/1750 train_time:33757ms step_avg:93.77ms
step:361/1750 train_time:33851ms step_avg:93.77ms
step:362/1750 train_time:33945ms step_avg:93.77ms
step:363/1750 train_time:34039ms step_avg:93.77ms
step:364/1750 train_time:34134ms step_avg:93.77ms
step:365/1750 train_time:34228ms step_avg:93.77ms
step:366/1750 train_time:34322ms step_avg:93.78ms
step:367/1750 train_time:34417ms step_avg:93.78ms
step:368/1750 train_time:34512ms step_avg:93.78ms
step:369/1750 train_time:34607ms step_avg:93.78ms
step:370/1750 train_time:34701ms step_avg:93.79ms
step:371/1750 train_time:34795ms step_avg:93.79ms
step:372/1750 train_time:34890ms step_avg:93.79ms
step:373/1750 train_time:34984ms step_avg:93.79ms
step:374/1750 train_time:35079ms step_avg:93.79ms
step:375/1750 train_time:35174ms step_avg:93.80ms
step:375/1750 val_loss:3.8881 train_time:35263ms step_avg:94.03ms
step:376/1750 train_time:35290ms step_avg:93.86ms
step:377/1750 train_time:35371ms step_avg:93.82ms
step:378/1750 train_time:35467ms step_avg:93.83ms
step:379/1750 train_time:35563ms step_avg:93.83ms
step:380/1750 train_time:35657ms step_avg:93.83ms
step:381/1750 train_time:35751ms step_avg:93.83ms
step:382/1750 train_time:35844ms step_avg:93.83ms
step:383/1750 train_time:35938ms step_avg:93.83ms
step:384/1750 train_time:36032ms step_avg:93.83ms
step:385/1750 train_time:36125ms step_avg:93.83ms
step:386/1750 train_time:36218ms step_avg:93.83ms
step:387/1750 train_time:36314ms step_avg:93.83ms
step:388/1750 train_time:36410ms step_avg:93.84ms
step:389/1750 train_time:36505ms step_avg:93.84ms
step:390/1750 train_time:36600ms step_avg:93.85ms
step:391/1750 train_time:36696ms step_avg:93.85ms
step:392/1750 train_time:36792ms step_avg:93.86ms
step:393/1750 train_time:36888ms step_avg:93.86ms
step:394/1750 train_time:36983ms step_avg:93.87ms
step:395/1750 train_time:37080ms step_avg:93.87ms
step:396/1750 train_time:37176ms step_avg:93.88ms
step:397/1750 train_time:37273ms step_avg:93.89ms
step:398/1750 train_time:37370ms step_avg:93.90ms
step:399/1750 train_time:37467ms step_avg:93.90ms
step:400/1750 train_time:37563ms step_avg:93.91ms
step:401/1750 train_time:37660ms step_avg:93.92ms
step:402/1750 train_time:37757ms step_avg:93.92ms
step:403/1750 train_time:37853ms step_avg:93.93ms
step:404/1750 train_time:37949ms step_avg:93.93ms
step:405/1750 train_time:38044ms step_avg:93.94ms
step:406/1750 train_time:38141ms step_avg:93.94ms
step:407/1750 train_time:38237ms step_avg:93.95ms
step:408/1750 train_time:38335ms step_avg:93.96ms
step:409/1750 train_time:38432ms step_avg:93.97ms
step:410/1750 train_time:38530ms step_avg:93.97ms
step:411/1750 train_time:38626ms step_avg:93.98ms
step:412/1750 train_time:38722ms step_avg:93.99ms
step:413/1750 train_time:38819ms step_avg:93.99ms
step:414/1750 train_time:38916ms step_avg:94.00ms
step:415/1750 train_time:39013ms step_avg:94.01ms
step:416/1750 train_time:39108ms step_avg:94.01ms
step:417/1750 train_time:39204ms step_avg:94.01ms
step:418/1750 train_time:39302ms step_avg:94.02ms
step:419/1750 train_time:39400ms step_avg:94.03ms
step:420/1750 train_time:39497ms step_avg:94.04ms
step:421/1750 train_time:39593ms step_avg:94.05ms
step:422/1750 train_time:39690ms step_avg:94.05ms
step:423/1750 train_time:39785ms step_avg:94.06ms
step:424/1750 train_time:39882ms step_avg:94.06ms
step:425/1750 train_time:39979ms step_avg:94.07ms
step:426/1750 train_time:40076ms step_avg:94.07ms
step:427/1750 train_time:40172ms step_avg:94.08ms
step:428/1750 train_time:40268ms step_avg:94.08ms
step:429/1750 train_time:40364ms step_avg:94.09ms
step:430/1750 train_time:40461ms step_avg:94.09ms
step:431/1750 train_time:40557ms step_avg:94.10ms
step:432/1750 train_time:40654ms step_avg:94.11ms
step:433/1750 train_time:40750ms step_avg:94.11ms
step:434/1750 train_time:40846ms step_avg:94.11ms
step:435/1750 train_time:40943ms step_avg:94.12ms
step:436/1750 train_time:41040ms step_avg:94.13ms
step:437/1750 train_time:41136ms step_avg:94.13ms
step:438/1750 train_time:41232ms step_avg:94.14ms
step:439/1750 train_time:41328ms step_avg:94.14ms
step:440/1750 train_time:41424ms step_avg:94.15ms
step:441/1750 train_time:41521ms step_avg:94.15ms
step:442/1750 train_time:41618ms step_avg:94.16ms
step:443/1750 train_time:41714ms step_avg:94.16ms
step:444/1750 train_time:41811ms step_avg:94.17ms
step:445/1750 train_time:41907ms step_avg:94.17ms
step:446/1750 train_time:42003ms step_avg:94.18ms
step:447/1750 train_time:42100ms step_avg:94.18ms
step:448/1750 train_time:42196ms step_avg:94.19ms
step:449/1750 train_time:42293ms step_avg:94.19ms
step:450/1750 train_time:42389ms step_avg:94.20ms
step:451/1750 train_time:42486ms step_avg:94.20ms
step:452/1750 train_time:42582ms step_avg:94.21ms
step:453/1750 train_time:42680ms step_avg:94.22ms
step:454/1750 train_time:42777ms step_avg:94.22ms
step:455/1750 train_time:42874ms step_avg:94.23ms
step:456/1750 train_time:42971ms step_avg:94.23ms
step:457/1750 train_time:43067ms step_avg:94.24ms
step:458/1750 train_time:43164ms step_avg:94.24ms
step:459/1750 train_time:43260ms step_avg:94.25ms
step:460/1750 train_time:43357ms step_avg:94.25ms
step:461/1750 train_time:43454ms step_avg:94.26ms
step:462/1750 train_time:43551ms step_avg:94.27ms
step:463/1750 train_time:43647ms step_avg:94.27ms
step:464/1750 train_time:43743ms step_avg:94.27ms
step:465/1750 train_time:43840ms step_avg:94.28ms
step:466/1750 train_time:43936ms step_avg:94.28ms
step:467/1750 train_time:44034ms step_avg:94.29ms
step:468/1750 train_time:44130ms step_avg:94.29ms
step:469/1750 train_time:44226ms step_avg:94.30ms
step:470/1750 train_time:44322ms step_avg:94.30ms
step:471/1750 train_time:44419ms step_avg:94.31ms
step:472/1750 train_time:44516ms step_avg:94.31ms
step:473/1750 train_time:44612ms step_avg:94.32ms
step:474/1750 train_time:44708ms step_avg:94.32ms
step:475/1750 train_time:44805ms step_avg:94.33ms
step:476/1750 train_time:44902ms step_avg:94.33ms
step:477/1750 train_time:44999ms step_avg:94.34ms
step:478/1750 train_time:45095ms step_avg:94.34ms
step:479/1750 train_time:45192ms step_avg:94.35ms
step:480/1750 train_time:45289ms step_avg:94.35ms
step:481/1750 train_time:45385ms step_avg:94.35ms
step:482/1750 train_time:45481ms step_avg:94.36ms
step:483/1750 train_time:45579ms step_avg:94.37ms
step:484/1750 train_time:45675ms step_avg:94.37ms
step:485/1750 train_time:45771ms step_avg:94.37ms
step:486/1750 train_time:45868ms step_avg:94.38ms
step:487/1750 train_time:45964ms step_avg:94.38ms
step:488/1750 train_time:46061ms step_avg:94.39ms
step:489/1750 train_time:46158ms step_avg:94.39ms
step:490/1750 train_time:46255ms step_avg:94.40ms
step:491/1750 train_time:46351ms step_avg:94.40ms
step:492/1750 train_time:46448ms step_avg:94.41ms
step:493/1750 train_time:46545ms step_avg:94.41ms
step:494/1750 train_time:46641ms step_avg:94.41ms
step:495/1750 train_time:46738ms step_avg:94.42ms
step:496/1750 train_time:46835ms step_avg:94.43ms
step:497/1750 train_time:46932ms step_avg:94.43ms
step:498/1750 train_time:47027ms step_avg:94.43ms
step:499/1750 train_time:47124ms step_avg:94.44ms
step:500/1750 train_time:47220ms step_avg:94.44ms
step:500/1750 val_loss:3.7425 train_time:47312ms step_avg:94.62ms
step:501/1750 train_time:47339ms step_avg:94.49ms
step:502/1750 train_time:47426ms step_avg:94.47ms
step:503/1750 train_time:47525ms step_avg:94.48ms
step:504/1750 train_time:47623ms step_avg:94.49ms
step:505/1750 train_time:47719ms step_avg:94.49ms
step:506/1750 train_time:47815ms step_avg:94.50ms
step:507/1750 train_time:47910ms step_avg:94.50ms
step:508/1750 train_time:48006ms step_avg:94.50ms
step:509/1750 train_time:48102ms step_avg:94.50ms
step:510/1750 train_time:48198ms step_avg:94.51ms
step:511/1750 train_time:48293ms step_avg:94.51ms
step:512/1750 train_time:48390ms step_avg:94.51ms
step:513/1750 train_time:48488ms step_avg:94.52ms
step:514/1750 train_time:48585ms step_avg:94.52ms
step:515/1750 train_time:48682ms step_avg:94.53ms
step:516/1750 train_time:48779ms step_avg:94.53ms
step:517/1750 train_time:48875ms step_avg:94.54ms
step:518/1750 train_time:48971ms step_avg:94.54ms
step:519/1750 train_time:49067ms step_avg:94.54ms
step:520/1750 train_time:49164ms step_avg:94.55ms
step:521/1750 train_time:49260ms step_avg:94.55ms
step:522/1750 train_time:49357ms step_avg:94.55ms
step:523/1750 train_time:49453ms step_avg:94.56ms
step:524/1750 train_time:49550ms step_avg:94.56ms
step:525/1750 train_time:49647ms step_avg:94.57ms
step:526/1750 train_time:49745ms step_avg:94.57ms
step:527/1750 train_time:49843ms step_avg:94.58ms
step:528/1750 train_time:49940ms step_avg:94.58ms
step:529/1750 train_time:50037ms step_avg:94.59ms
step:530/1750 train_time:50134ms step_avg:94.59ms
step:531/1750 train_time:50230ms step_avg:94.60ms
step:532/1750 train_time:50327ms step_avg:94.60ms
step:533/1750 train_time:50425ms step_avg:94.61ms
step:534/1750 train_time:50522ms step_avg:94.61ms
step:535/1750 train_time:50620ms step_avg:94.62ms
step:536/1750 train_time:50717ms step_avg:94.62ms
step:537/1750 train_time:50814ms step_avg:94.63ms
step:538/1750 train_time:50910ms step_avg:94.63ms
step:539/1750 train_time:51007ms step_avg:94.63ms
step:540/1750 train_time:51105ms step_avg:94.64ms
step:541/1750 train_time:51202ms step_avg:94.64ms
step:542/1750 train_time:51299ms step_avg:94.65ms
step:543/1750 train_time:51396ms step_avg:94.65ms
step:544/1750 train_time:51492ms step_avg:94.65ms
step:545/1750 train_time:51589ms step_avg:94.66ms
step:546/1750 train_time:51686ms step_avg:94.66ms
step:547/1750 train_time:51784ms step_avg:94.67ms
step:548/1750 train_time:51882ms step_avg:94.68ms
step:549/1750 train_time:51980ms step_avg:94.68ms
step:550/1750 train_time:52077ms step_avg:94.69ms
step:551/1750 train_time:52174ms step_avg:94.69ms
step:552/1750 train_time:52270ms step_avg:94.69ms
step:553/1750 train_time:52367ms step_avg:94.70ms
step:554/1750 train_time:52464ms step_avg:94.70ms
step:555/1750 train_time:52561ms step_avg:94.70ms
step:556/1750 train_time:52659ms step_avg:94.71ms
step:557/1750 train_time:52757ms step_avg:94.72ms
step:558/1750 train_time:52853ms step_avg:94.72ms
step:559/1750 train_time:52950ms step_avg:94.72ms
step:560/1750 train_time:53047ms step_avg:94.73ms
step:561/1750 train_time:53144ms step_avg:94.73ms
step:562/1750 train_time:53241ms step_avg:94.73ms
step:563/1750 train_time:53339ms step_avg:94.74ms
step:564/1750 train_time:53435ms step_avg:94.74ms
step:565/1750 train_time:53532ms step_avg:94.75ms
step:566/1750 train_time:53628ms step_avg:94.75ms
step:567/1750 train_time:53726ms step_avg:94.75ms
step:568/1750 train_time:53823ms step_avg:94.76ms
step:569/1750 train_time:53921ms step_avg:94.76ms
step:570/1750 train_time:54019ms step_avg:94.77ms
step:571/1750 train_time:54115ms step_avg:94.77ms
step:572/1750 train_time:54211ms step_avg:94.78ms
step:573/1750 train_time:54308ms step_avg:94.78ms
step:574/1750 train_time:54406ms step_avg:94.78ms
step:575/1750 train_time:54503ms step_avg:94.79ms
step:576/1750 train_time:54600ms step_avg:94.79ms
step:577/1750 train_time:54697ms step_avg:94.80ms
step:578/1750 train_time:54794ms step_avg:94.80ms
step:579/1750 train_time:54891ms step_avg:94.80ms
step:580/1750 train_time:54988ms step_avg:94.81ms
step:581/1750 train_time:55085ms step_avg:94.81ms
step:582/1750 train_time:55182ms step_avg:94.82ms
step:583/1750 train_time:55280ms step_avg:94.82ms
step:584/1750 train_time:55377ms step_avg:94.82ms
step:585/1750 train_time:55475ms step_avg:94.83ms
step:586/1750 train_time:55570ms step_avg:94.83ms
step:587/1750 train_time:55667ms step_avg:94.83ms
step:588/1750 train_time:55764ms step_avg:94.84ms
step:589/1750 train_time:55862ms step_avg:94.84ms
step:590/1750 train_time:55959ms step_avg:94.85ms
step:591/1750 train_time:56057ms step_avg:94.85ms
step:592/1750 train_time:56153ms step_avg:94.85ms
step:593/1750 train_time:56249ms step_avg:94.85ms
step:594/1750 train_time:56346ms step_avg:94.86ms
step:595/1750 train_time:56444ms step_avg:94.86ms
step:596/1750 train_time:56541ms step_avg:94.87ms
step:597/1750 train_time:56639ms step_avg:94.87ms
step:598/1750 train_time:56735ms step_avg:94.88ms
step:599/1750 train_time:56831ms step_avg:94.88ms
step:600/1750 train_time:56928ms step_avg:94.88ms
step:601/1750 train_time:57026ms step_avg:94.89ms
step:602/1750 train_time:57124ms step_avg:94.89ms
step:603/1750 train_time:57221ms step_avg:94.89ms
step:604/1750 train_time:57317ms step_avg:94.90ms
step:605/1750 train_time:57414ms step_avg:94.90ms
step:606/1750 train_time:57510ms step_avg:94.90ms
step:607/1750 train_time:57607ms step_avg:94.90ms
step:608/1750 train_time:57705ms step_avg:94.91ms
step:609/1750 train_time:57802ms step_avg:94.91ms
step:610/1750 train_time:57899ms step_avg:94.92ms
step:611/1750 train_time:57997ms step_avg:94.92ms
step:612/1750 train_time:58094ms step_avg:94.92ms
step:613/1750 train_time:58189ms step_avg:94.93ms
step:614/1750 train_time:58286ms step_avg:94.93ms
step:615/1750 train_time:58384ms step_avg:94.93ms
step:616/1750 train_time:58482ms step_avg:94.94ms
step:617/1750 train_time:58579ms step_avg:94.94ms
step:618/1750 train_time:58676ms step_avg:94.94ms
step:619/1750 train_time:58773ms step_avg:94.95ms
step:620/1750 train_time:58869ms step_avg:94.95ms
step:621/1750 train_time:58966ms step_avg:94.95ms
step:622/1750 train_time:59064ms step_avg:94.96ms
step:623/1750 train_time:59161ms step_avg:94.96ms
step:624/1750 train_time:59259ms step_avg:94.97ms
step:625/1750 train_time:59356ms step_avg:94.97ms
step:625/1750 val_loss:3.6574 train_time:59447ms step_avg:95.11ms
step:626/1750 train_time:59473ms step_avg:95.00ms
step:627/1750 train_time:59557ms step_avg:94.99ms
step:628/1750 train_time:59657ms step_avg:95.00ms
step:629/1750 train_time:59755ms step_avg:95.00ms
step:630/1750 train_time:59851ms step_avg:95.00ms
step:631/1750 train_time:59947ms step_avg:95.00ms
step:632/1750 train_time:60044ms step_avg:95.01ms
step:633/1750 train_time:60140ms step_avg:95.01ms
step:634/1750 train_time:60235ms step_avg:95.01ms
step:635/1750 train_time:60331ms step_avg:95.01ms
step:636/1750 train_time:60427ms step_avg:95.01ms
step:637/1750 train_time:60526ms step_avg:95.02ms
step:638/1750 train_time:60625ms step_avg:95.02ms
step:639/1750 train_time:60724ms step_avg:95.03ms
step:640/1750 train_time:60821ms step_avg:95.03ms
step:641/1750 train_time:60918ms step_avg:95.04ms
step:642/1750 train_time:61015ms step_avg:95.04ms
step:643/1750 train_time:61111ms step_avg:95.04ms
step:644/1750 train_time:61208ms step_avg:95.04ms
step:645/1750 train_time:61304ms step_avg:95.04ms
step:646/1750 train_time:61400ms step_avg:95.05ms
step:647/1750 train_time:61497ms step_avg:95.05ms
step:648/1750 train_time:61595ms step_avg:95.05ms
step:649/1750 train_time:61692ms step_avg:95.06ms
step:650/1750 train_time:61789ms step_avg:95.06ms
step:651/1750 train_time:61888ms step_avg:95.07ms
step:652/1750 train_time:61987ms step_avg:95.07ms
step:653/1750 train_time:62086ms step_avg:95.08ms
step:654/1750 train_time:62185ms step_avg:95.08ms
step:655/1750 train_time:62285ms step_avg:95.09ms
step:656/1750 train_time:62385ms step_avg:95.10ms
step:657/1750 train_time:62484ms step_avg:95.11ms
step:658/1750 train_time:62584ms step_avg:95.11ms
step:659/1750 train_time:62684ms step_avg:95.12ms
step:660/1750 train_time:62783ms step_avg:95.13ms
step:661/1750 train_time:62883ms step_avg:95.13ms
step:662/1750 train_time:62981ms step_avg:95.14ms
step:663/1750 train_time:63080ms step_avg:95.14ms
step:664/1750 train_time:63178ms step_avg:95.15ms
step:665/1750 train_time:63277ms step_avg:95.15ms
step:666/1750 train_time:63375ms step_avg:95.16ms
step:667/1750 train_time:63473ms step_avg:95.16ms
step:668/1750 train_time:63572ms step_avg:95.17ms
step:669/1750 train_time:63670ms step_avg:95.17ms
step:670/1750 train_time:63769ms step_avg:95.18ms
step:671/1750 train_time:63868ms step_avg:95.18ms
step:672/1750 train_time:63967ms step_avg:95.19ms
step:673/1750 train_time:64067ms step_avg:95.20ms
step:674/1750 train_time:64166ms step_avg:95.20ms
step:675/1750 train_time:64265ms step_avg:95.21ms
step:676/1750 train_time:64365ms step_avg:95.21ms
step:677/1750 train_time:64465ms step_avg:95.22ms
step:678/1750 train_time:64564ms step_avg:95.23ms
step:679/1750 train_time:64664ms step_avg:95.23ms
step:680/1750 train_time:64763ms step_avg:95.24ms
step:681/1750 train_time:64862ms step_avg:95.24ms
step:682/1750 train_time:64960ms step_avg:95.25ms
step:683/1750 train_time:65059ms step_avg:95.25ms
step:684/1750 train_time:65157ms step_avg:95.26ms
step:685/1750 train_time:65256ms step_avg:95.26ms
step:686/1750 train_time:65354ms step_avg:95.27ms
step:687/1750 train_time:65451ms step_avg:95.27ms
step:688/1750 train_time:65550ms step_avg:95.28ms
step:689/1750 train_time:65648ms step_avg:95.28ms
step:690/1750 train_time:65746ms step_avg:95.28ms
step:691/1750 train_time:65846ms step_avg:95.29ms
step:692/1750 train_time:65945ms step_avg:95.30ms
step:693/1750 train_time:66045ms step_avg:95.30ms
step:694/1750 train_time:66144ms step_avg:95.31ms
step:695/1750 train_time:66244ms step_avg:95.32ms
step:696/1750 train_time:66344ms step_avg:95.32ms
step:697/1750 train_time:66443ms step_avg:95.33ms
step:698/1750 train_time:66542ms step_avg:95.33ms
step:699/1750 train_time:66640ms step_avg:95.34ms
step:700/1750 train_time:66738ms step_avg:95.34ms
step:701/1750 train_time:66836ms step_avg:95.34ms
step:702/1750 train_time:66935ms step_avg:95.35ms
step:703/1750 train_time:67033ms step_avg:95.35ms
step:704/1750 train_time:67131ms step_avg:95.36ms
step:705/1750 train_time:67230ms step_avg:95.36ms
step:706/1750 train_time:67328ms step_avg:95.37ms
step:707/1750 train_time:67427ms step_avg:95.37ms
step:708/1750 train_time:67526ms step_avg:95.38ms
step:709/1750 train_time:67625ms step_avg:95.38ms
step:710/1750 train_time:67723ms step_avg:95.38ms
step:711/1750 train_time:67822ms step_avg:95.39ms
step:712/1750 train_time:67921ms step_avg:95.39ms
step:713/1750 train_time:68020ms step_avg:95.40ms
step:714/1750 train_time:68120ms step_avg:95.41ms
step:715/1750 train_time:68218ms step_avg:95.41ms
step:716/1750 train_time:68317ms step_avg:95.42ms
step:717/1750 train_time:68415ms step_avg:95.42ms
step:718/1750 train_time:68514ms step_avg:95.42ms
step:719/1750 train_time:68611ms step_avg:95.43ms
step:720/1750 train_time:68709ms step_avg:95.43ms
step:721/1750 train_time:68807ms step_avg:95.43ms
step:722/1750 train_time:68906ms step_avg:95.44ms
step:723/1750 train_time:69006ms step_avg:95.44ms
step:724/1750 train_time:69106ms step_avg:95.45ms
step:725/1750 train_time:69205ms step_avg:95.46ms
step:726/1750 train_time:69305ms step_avg:95.46ms
step:727/1750 train_time:69405ms step_avg:95.47ms
step:728/1750 train_time:69505ms step_avg:95.47ms
step:729/1750 train_time:69605ms step_avg:95.48ms
step:730/1750 train_time:69704ms step_avg:95.48ms
step:731/1750 train_time:69803ms step_avg:95.49ms
step:732/1750 train_time:69901ms step_avg:95.49ms
step:733/1750 train_time:70000ms step_avg:95.50ms
step:734/1750 train_time:70098ms step_avg:95.50ms
step:735/1750 train_time:70196ms step_avg:95.50ms
step:736/1750 train_time:70295ms step_avg:95.51ms
step:737/1750 train_time:70393ms step_avg:95.51ms
step:738/1750 train_time:70491ms step_avg:95.52ms
step:739/1750 train_time:70590ms step_avg:95.52ms
step:740/1750 train_time:70688ms step_avg:95.52ms
step:741/1750 train_time:70787ms step_avg:95.53ms
step:742/1750 train_time:70885ms step_avg:95.53ms
step:743/1750 train_time:70984ms step_avg:95.54ms
step:744/1750 train_time:71083ms step_avg:95.54ms
step:745/1750 train_time:71183ms step_avg:95.55ms
step:746/1750 train_time:71282ms step_avg:95.55ms
step:747/1750 train_time:71382ms step_avg:95.56ms
step:748/1750 train_time:71482ms step_avg:95.56ms
step:749/1750 train_time:71581ms step_avg:95.57ms
step:750/1750 train_time:71680ms step_avg:95.57ms
step:750/1750 val_loss:3.5943 train_time:71773ms step_avg:95.70ms
step:751/1750 train_time:71799ms step_avg:95.60ms
step:752/1750 train_time:71883ms step_avg:95.59ms
step:753/1750 train_time:71983ms step_avg:95.60ms
step:754/1750 train_time:72082ms step_avg:95.60ms
step:755/1750 train_time:72180ms step_avg:95.60ms
step:756/1750 train_time:72278ms step_avg:95.61ms
step:757/1750 train_time:72377ms step_avg:95.61ms
step:758/1750 train_time:72475ms step_avg:95.61ms
step:759/1750 train_time:72572ms step_avg:95.62ms
step:760/1750 train_time:72670ms step_avg:95.62ms
step:761/1750 train_time:72769ms step_avg:95.62ms
step:762/1750 train_time:72869ms step_avg:95.63ms
step:763/1750 train_time:72969ms step_avg:95.63ms
step:764/1750 train_time:73068ms step_avg:95.64ms
step:765/1750 train_time:73167ms step_avg:95.64ms
step:766/1750 train_time:73265ms step_avg:95.65ms
step:767/1750 train_time:73363ms step_avg:95.65ms
step:768/1750 train_time:73461ms step_avg:95.65ms
step:769/1750 train_time:73559ms step_avg:95.66ms
step:770/1750 train_time:73657ms step_avg:95.66ms
step:771/1750 train_time:73755ms step_avg:95.66ms
step:772/1750 train_time:73854ms step_avg:95.67ms
step:773/1750 train_time:73953ms step_avg:95.67ms
step:774/1750 train_time:74052ms step_avg:95.67ms
step:775/1750 train_time:74152ms step_avg:95.68ms
step:776/1750 train_time:74251ms step_avg:95.68ms
step:777/1750 train_time:74350ms step_avg:95.69ms
step:778/1750 train_time:74448ms step_avg:95.69ms
step:779/1750 train_time:74546ms step_avg:95.69ms
step:780/1750 train_time:74645ms step_avg:95.70ms
step:781/1750 train_time:74743ms step_avg:95.70ms
step:782/1750 train_time:74841ms step_avg:95.70ms
step:783/1750 train_time:74939ms step_avg:95.71ms
step:784/1750 train_time:75038ms step_avg:95.71ms
step:785/1750 train_time:75137ms step_avg:95.72ms
step:786/1750 train_time:75237ms step_avg:95.72ms
step:787/1750 train_time:75336ms step_avg:95.73ms
step:788/1750 train_time:75436ms step_avg:95.73ms
step:789/1750 train_time:75536ms step_avg:95.74ms
step:790/1750 train_time:75636ms step_avg:95.74ms
step:791/1750 train_time:75735ms step_avg:95.75ms
step:792/1750 train_time:75834ms step_avg:95.75ms
step:793/1750 train_time:75933ms step_avg:95.75ms
step:794/1750 train_time:76032ms step_avg:95.76ms
step:795/1750 train_time:76131ms step_avg:95.76ms
step:796/1750 train_time:76229ms step_avg:95.77ms
step:797/1750 train_time:76328ms step_avg:95.77ms
step:798/1750 train_time:76427ms step_avg:95.77ms
step:799/1750 train_time:76526ms step_avg:95.78ms
step:800/1750 train_time:76624ms step_avg:95.78ms
step:801/1750 train_time:76723ms step_avg:95.78ms
step:802/1750 train_time:76821ms step_avg:95.79ms
step:803/1750 train_time:76920ms step_avg:95.79ms
step:804/1750 train_time:77018ms step_avg:95.79ms
step:805/1750 train_time:77116ms step_avg:95.80ms
step:806/1750 train_time:77216ms step_avg:95.80ms
step:807/1750 train_time:77315ms step_avg:95.81ms
step:808/1750 train_time:77414ms step_avg:95.81ms
step:809/1750 train_time:77514ms step_avg:95.81ms
step:810/1750 train_time:77613ms step_avg:95.82ms
step:811/1750 train_time:77713ms step_avg:95.82ms
step:812/1750 train_time:77812ms step_avg:95.83ms
step:813/1750 train_time:77912ms step_avg:95.83ms
step:814/1750 train_time:78011ms step_avg:95.84ms
step:815/1750 train_time:78109ms step_avg:95.84ms
step:816/1750 train_time:78208ms step_avg:95.84ms
step:817/1750 train_time:78306ms step_avg:95.85ms
step:818/1750 train_time:78404ms step_avg:95.85ms
step:819/1750 train_time:78502ms step_avg:95.85ms
step:820/1750 train_time:78601ms step_avg:95.85ms
step:821/1750 train_time:78700ms step_avg:95.86ms
step:822/1750 train_time:78799ms step_avg:95.86ms
step:823/1750 train_time:78898ms step_avg:95.87ms
step:824/1750 train_time:78997ms step_avg:95.87ms
step:825/1750 train_time:79097ms step_avg:95.87ms
step:826/1750 train_time:79196ms step_avg:95.88ms
step:827/1750 train_time:79297ms step_avg:95.88ms
step:828/1750 train_time:79396ms step_avg:95.89ms
step:829/1750 train_time:79496ms step_avg:95.89ms
step:830/1750 train_time:79596ms step_avg:95.90ms
step:831/1750 train_time:79695ms step_avg:95.90ms
step:832/1750 train_time:79794ms step_avg:95.91ms
step:833/1750 train_time:79893ms step_avg:95.91ms
step:834/1750 train_time:79993ms step_avg:95.92ms
step:835/1750 train_time:80093ms step_avg:95.92ms
step:836/1750 train_time:80192ms step_avg:95.92ms
step:837/1750 train_time:80291ms step_avg:95.93ms
step:838/1750 train_time:80390ms step_avg:95.93ms
step:839/1750 train_time:80489ms step_avg:95.93ms
step:840/1750 train_time:80588ms step_avg:95.94ms
step:841/1750 train_time:80687ms step_avg:95.94ms
step:842/1750 train_time:80786ms step_avg:95.95ms
step:843/1750 train_time:80885ms step_avg:95.95ms
step:844/1750 train_time:80983ms step_avg:95.95ms
step:845/1750 train_time:81081ms step_avg:95.95ms
step:846/1750 train_time:81180ms step_avg:95.96ms
step:847/1750 train_time:81278ms step_avg:95.96ms
step:848/1750 train_time:81377ms step_avg:95.96ms
step:849/1750 train_time:81476ms step_avg:95.97ms
step:850/1750 train_time:81575ms step_avg:95.97ms
step:851/1750 train_time:81676ms step_avg:95.98ms
step:852/1750 train_time:81776ms step_avg:95.98ms
step:853/1750 train_time:81876ms step_avg:95.99ms
step:854/1750 train_time:81975ms step_avg:95.99ms
step:855/1750 train_time:82075ms step_avg:95.99ms
step:856/1750 train_time:82174ms step_avg:96.00ms
step:857/1750 train_time:82274ms step_avg:96.00ms
step:858/1750 train_time:82373ms step_avg:96.01ms
step:859/1750 train_time:82472ms step_avg:96.01ms
step:860/1750 train_time:82571ms step_avg:96.01ms
step:861/1750 train_time:82671ms step_avg:96.02ms
step:862/1750 train_time:82770ms step_avg:96.02ms
step:863/1750 train_time:82869ms step_avg:96.02ms
step:864/1750 train_time:82968ms step_avg:96.03ms
step:865/1750 train_time:83066ms step_avg:96.03ms
step:866/1750 train_time:83164ms step_avg:96.03ms
step:867/1750 train_time:83263ms step_avg:96.04ms
step:868/1750 train_time:83360ms step_avg:96.04ms
step:869/1750 train_time:83458ms step_avg:96.04ms
step:870/1750 train_time:83557ms step_avg:96.04ms
step:871/1750 train_time:83656ms step_avg:96.05ms
step:872/1750 train_time:83755ms step_avg:96.05ms
step:873/1750 train_time:83854ms step_avg:96.05ms
step:874/1750 train_time:83954ms step_avg:96.06ms
step:875/1750 train_time:84054ms step_avg:96.06ms
step:875/1750 val_loss:3.5468 train_time:84148ms step_avg:96.17ms
step:876/1750 train_time:84175ms step_avg:96.09ms
step:877/1750 train_time:84263ms step_avg:96.08ms
step:878/1750 train_time:84363ms step_avg:96.09ms
step:879/1750 train_time:84462ms step_avg:96.09ms
step:880/1750 train_time:84561ms step_avg:96.09ms
step:881/1750 train_time:84659ms step_avg:96.09ms
step:882/1750 train_time:84758ms step_avg:96.10ms
step:883/1750 train_time:84856ms step_avg:96.10ms
step:884/1750 train_time:84954ms step_avg:96.10ms
step:885/1750 train_time:85052ms step_avg:96.10ms
step:886/1750 train_time:85152ms step_avg:96.11ms
step:887/1750 train_time:85251ms step_avg:96.11ms
step:888/1750 train_time:85350ms step_avg:96.12ms
step:889/1750 train_time:85449ms step_avg:96.12ms
step:890/1750 train_time:85548ms step_avg:96.12ms
step:891/1750 train_time:85647ms step_avg:96.12ms
step:892/1750 train_time:85745ms step_avg:96.13ms
step:893/1750 train_time:85844ms step_avg:96.13ms
step:894/1750 train_time:85942ms step_avg:96.13ms
step:895/1750 train_time:86041ms step_avg:96.14ms
step:896/1750 train_time:86140ms step_avg:96.14ms
step:897/1750 train_time:86241ms step_avg:96.14ms
step:898/1750 train_time:86341ms step_avg:96.15ms
step:899/1750 train_time:86441ms step_avg:96.15ms
step:900/1750 train_time:86541ms step_avg:96.16ms
step:901/1750 train_time:86640ms step_avg:96.16ms
step:902/1750 train_time:86740ms step_avg:96.16ms
step:903/1750 train_time:86839ms step_avg:96.17ms
step:904/1750 train_time:86938ms step_avg:96.17ms
step:905/1750 train_time:87037ms step_avg:96.17ms
step:906/1750 train_time:87135ms step_avg:96.18ms
step:907/1750 train_time:87234ms step_avg:96.18ms
step:908/1750 train_time:87333ms step_avg:96.18ms
step:909/1750 train_time:87432ms step_avg:96.18ms
step:910/1750 train_time:87532ms step_avg:96.19ms
step:911/1750 train_time:87632ms step_avg:96.19ms
step:912/1750 train_time:87732ms step_avg:96.20ms
step:913/1750 train_time:87832ms step_avg:96.20ms
step:914/1750 train_time:87932ms step_avg:96.21ms
step:915/1750 train_time:88033ms step_avg:96.21ms
step:916/1750 train_time:88133ms step_avg:96.22ms
step:917/1750 train_time:88233ms step_avg:96.22ms
step:918/1750 train_time:88333ms step_avg:96.22ms
step:919/1750 train_time:88433ms step_avg:96.23ms
step:920/1750 train_time:88533ms step_avg:96.23ms
step:921/1750 train_time:88633ms step_avg:96.24ms
step:922/1750 train_time:88733ms step_avg:96.24ms
step:923/1750 train_time:88833ms step_avg:96.24ms
step:924/1750 train_time:88933ms step_avg:96.25ms
step:925/1750 train_time:89034ms step_avg:96.25ms
step:926/1750 train_time:89134ms step_avg:96.26ms
step:927/1750 train_time:89234ms step_avg:96.26ms
step:928/1750 train_time:89334ms step_avg:96.27ms
step:929/1750 train_time:89434ms step_avg:96.27ms
step:930/1750 train_time:89534ms step_avg:96.27ms
step:931/1750 train_time:89634ms step_avg:96.28ms
step:932/1750 train_time:89734ms step_avg:96.28ms
step:933/1750 train_time:89834ms step_avg:96.29ms
step:934/1750 train_time:89935ms step_avg:96.29ms
step:935/1750 train_time:90035ms step_avg:96.29ms
step:936/1750 train_time:90134ms step_avg:96.30ms
step:937/1750 train_time:90234ms step_avg:96.30ms
step:938/1750 train_time:90335ms step_avg:96.31ms
step:939/1750 train_time:90435ms step_avg:96.31ms
step:940/1750 train_time:90536ms step_avg:96.32ms
step:941/1750 train_time:90636ms step_avg:96.32ms
step:942/1750 train_time:90736ms step_avg:96.32ms
step:943/1750 train_time:90837ms step_avg:96.33ms
step:944/1750 train_time:90939ms step_avg:96.33ms
step:945/1750 train_time:91039ms step_avg:96.34ms
step:946/1750 train_time:91139ms step_avg:96.34ms
step:947/1750 train_time:91240ms step_avg:96.35ms
step:948/1750 train_time:91341ms step_avg:96.35ms
step:949/1750 train_time:91442ms step_avg:96.36ms
step:950/1750 train_time:91543ms step_avg:96.36ms
step:951/1750 train_time:91643ms step_avg:96.37ms
step:952/1750 train_time:91744ms step_avg:96.37ms
step:953/1750 train_time:91844ms step_avg:96.37ms
step:954/1750 train_time:91945ms step_avg:96.38ms
step:955/1750 train_time:92046ms step_avg:96.38ms
step:956/1750 train_time:92145ms step_avg:96.39ms
step:957/1750 train_time:92246ms step_avg:96.39ms
step:958/1750 train_time:92347ms step_avg:96.40ms
step:959/1750 train_time:92447ms step_avg:96.40ms
step:960/1750 train_time:92547ms step_avg:96.40ms
step:961/1750 train_time:92647ms step_avg:96.41ms
step:962/1750 train_time:92747ms step_avg:96.41ms
step:963/1750 train_time:92848ms step_avg:96.41ms
step:964/1750 train_time:92948ms step_avg:96.42ms
step:965/1750 train_time:93047ms step_avg:96.42ms
step:966/1750 train_time:93147ms step_avg:96.43ms
step:967/1750 train_time:93247ms step_avg:96.43ms
step:968/1750 train_time:93348ms step_avg:96.43ms
step:969/1750 train_time:93448ms step_avg:96.44ms
step:970/1750 train_time:93548ms step_avg:96.44ms
step:971/1750 train_time:93647ms step_avg:96.44ms
step:972/1750 train_time:93747ms step_avg:96.45ms
step:973/1750 train_time:93847ms step_avg:96.45ms
step:974/1750 train_time:93947ms step_avg:96.45ms
step:975/1750 train_time:94047ms step_avg:96.46ms
step:976/1750 train_time:94147ms step_avg:96.46ms
step:977/1750 train_time:94246ms step_avg:96.47ms
step:978/1750 train_time:94347ms step_avg:96.47ms
step:979/1750 train_time:94447ms step_avg:96.47ms
step:980/1750 train_time:94547ms step_avg:96.48ms
step:981/1750 train_time:94648ms step_avg:96.48ms
step:982/1750 train_time:94748ms step_avg:96.48ms
step:983/1750 train_time:94848ms step_avg:96.49ms
step:984/1750 train_time:94947ms step_avg:96.49ms
step:985/1750 train_time:95047ms step_avg:96.49ms
step:986/1750 train_time:95147ms step_avg:96.50ms
step:987/1750 train_time:95247ms step_avg:96.50ms
step:988/1750 train_time:95347ms step_avg:96.51ms
step:989/1750 train_time:95448ms step_avg:96.51ms
step:990/1750 train_time:95548ms step_avg:96.51ms
step:991/1750 train_time:95648ms step_avg:96.52ms
step:992/1750 train_time:95748ms step_avg:96.52ms
step:993/1750 train_time:95848ms step_avg:96.52ms
step:994/1750 train_time:95948ms step_avg:96.53ms
step:995/1750 train_time:96048ms step_avg:96.53ms
step:996/1750 train_time:96147ms step_avg:96.53ms
step:997/1750 train_time:96248ms step_avg:96.54ms
step:998/1750 train_time:96347ms step_avg:96.54ms
step:999/1750 train_time:96447ms step_avg:96.54ms
step:1000/1750 train_time:96547ms step_avg:96.55ms
step:1000/1750 val_loss:3.5052 train_time:96642ms step_avg:96.64ms
step:1001/1750 train_time:96671ms step_avg:96.57ms
step:1002/1750 train_time:96757ms step_avg:96.56ms
step:1003/1750 train_time:96858ms step_avg:96.57ms
step:1004/1750 train_time:96959ms step_avg:96.57ms
step:1005/1750 train_time:97058ms step_avg:96.58ms
step:1006/1750 train_time:97157ms step_avg:96.58ms
step:1007/1750 train_time:97257ms step_avg:96.58ms
step:1008/1750 train_time:97356ms step_avg:96.58ms
step:1009/1750 train_time:97456ms step_avg:96.59ms
step:1010/1750 train_time:97556ms step_avg:96.59ms
step:1011/1750 train_time:97657ms step_avg:96.59ms
step:1012/1750 train_time:97758ms step_avg:96.60ms
step:1013/1750 train_time:97859ms step_avg:96.60ms
step:1014/1750 train_time:97959ms step_avg:96.61ms
step:1015/1750 train_time:98059ms step_avg:96.61ms
step:1016/1750 train_time:98158ms step_avg:96.61ms
step:1017/1750 train_time:98258ms step_avg:96.62ms
step:1018/1750 train_time:98357ms step_avg:96.62ms
step:1019/1750 train_time:98457ms step_avg:96.62ms
step:1020/1750 train_time:98557ms step_avg:96.62ms
step:1021/1750 train_time:98657ms step_avg:96.63ms
step:1022/1750 train_time:98757ms step_avg:96.63ms
step:1023/1750 train_time:98858ms step_avg:96.64ms
step:1024/1750 train_time:98959ms step_avg:96.64ms
step:1025/1750 train_time:99059ms step_avg:96.64ms
step:1026/1750 train_time:99160ms step_avg:96.65ms
step:1027/1750 train_time:99259ms step_avg:96.65ms
step:1028/1750 train_time:99360ms step_avg:96.65ms
step:1029/1750 train_time:99460ms step_avg:96.66ms
step:1030/1750 train_time:99559ms step_avg:96.66ms
step:1031/1750 train_time:99659ms step_avg:96.66ms
step:1032/1750 train_time:99759ms step_avg:96.67ms
step:1033/1750 train_time:99859ms step_avg:96.67ms
step:1034/1750 train_time:99959ms step_avg:96.67ms
step:1035/1750 train_time:100059ms step_avg:96.68ms
step:1036/1750 train_time:100159ms step_avg:96.68ms
step:1037/1750 train_time:100259ms step_avg:96.68ms
step:1038/1750 train_time:100359ms step_avg:96.69ms
step:1039/1750 train_time:100458ms step_avg:96.69ms
step:1040/1750 train_time:100558ms step_avg:96.69ms
step:1041/1750 train_time:100658ms step_avg:96.69ms
step:1042/1750 train_time:100758ms step_avg:96.70ms
step:1043/1750 train_time:100858ms step_avg:96.70ms
step:1044/1750 train_time:100957ms step_avg:96.70ms
step:1045/1750 train_time:101058ms step_avg:96.71ms
step:1046/1750 train_time:101159ms step_avg:96.71ms
step:1047/1750 train_time:101259ms step_avg:96.71ms
step:1048/1750 train_time:101359ms step_avg:96.72ms
step:1049/1750 train_time:101459ms step_avg:96.72ms
step:1050/1750 train_time:101559ms step_avg:96.72ms
step:1051/1750 train_time:101659ms step_avg:96.73ms
step:1052/1750 train_time:101758ms step_avg:96.73ms
step:1053/1750 train_time:101858ms step_avg:96.73ms
step:1054/1750 train_time:101958ms step_avg:96.73ms
step:1055/1750 train_time:102059ms step_avg:96.74ms
step:1056/1750 train_time:102159ms step_avg:96.74ms
step:1057/1750 train_time:102259ms step_avg:96.74ms
step:1058/1750 train_time:102359ms step_avg:96.75ms
step:1059/1750 train_time:102460ms step_avg:96.75ms
step:1060/1750 train_time:102560ms step_avg:96.75ms
step:1061/1750 train_time:102660ms step_avg:96.76ms
step:1062/1750 train_time:102760ms step_avg:96.76ms
step:1063/1750 train_time:102860ms step_avg:96.76ms
step:1064/1750 train_time:102960ms step_avg:96.77ms
step:1065/1750 train_time:103060ms step_avg:96.77ms
step:1066/1750 train_time:103160ms step_avg:96.77ms
step:1067/1750 train_time:103260ms step_avg:96.78ms
step:1068/1750 train_time:103359ms step_avg:96.78ms
step:1069/1750 train_time:103460ms step_avg:96.78ms
step:1070/1750 train_time:103561ms step_avg:96.79ms
step:1071/1750 train_time:103661ms step_avg:96.79ms
step:1072/1750 train_time:103761ms step_avg:96.79ms
step:1073/1750 train_time:103860ms step_avg:96.79ms
step:1074/1750 train_time:103960ms step_avg:96.80ms
step:1075/1750 train_time:104061ms step_avg:96.80ms
step:1076/1750 train_time:104160ms step_avg:96.80ms
step:1077/1750 train_time:104261ms step_avg:96.81ms
step:1078/1750 train_time:104361ms step_avg:96.81ms
step:1079/1750 train_time:104462ms step_avg:96.81ms
step:1080/1750 train_time:104562ms step_avg:96.82ms
step:1081/1750 train_time:104661ms step_avg:96.82ms
step:1082/1750 train_time:104761ms step_avg:96.82ms
step:1083/1750 train_time:104860ms step_avg:96.82ms
step:1084/1750 train_time:104960ms step_avg:96.83ms
step:1085/1750 train_time:105060ms step_avg:96.83ms
step:1086/1750 train_time:105160ms step_avg:96.83ms
step:1087/1750 train_time:105260ms step_avg:96.84ms
step:1088/1750 train_time:105361ms step_avg:96.84ms
step:1089/1750 train_time:105461ms step_avg:96.84ms
step:1090/1750 train_time:105562ms step_avg:96.85ms
step:1091/1750 train_time:105662ms step_avg:96.85ms
step:1092/1750 train_time:105762ms step_avg:96.85ms
step:1093/1750 train_time:105861ms step_avg:96.85ms
step:1094/1750 train_time:105963ms step_avg:96.86ms
step:1095/1750 train_time:106063ms step_avg:96.86ms
step:1096/1750 train_time:106163ms step_avg:96.86ms
step:1097/1750 train_time:106265ms step_avg:96.87ms
step:1098/1750 train_time:106366ms step_avg:96.87ms
step:1099/1750 train_time:106467ms step_avg:96.88ms
step:1100/1750 train_time:106567ms step_avg:96.88ms
step:1101/1750 train_time:106668ms step_avg:96.88ms
step:1102/1750 train_time:106768ms step_avg:96.89ms
step:1103/1750 train_time:106868ms step_avg:96.89ms
step:1104/1750 train_time:106969ms step_avg:96.89ms
step:1105/1750 train_time:107070ms step_avg:96.90ms
step:1106/1750 train_time:107172ms step_avg:96.90ms
step:1107/1750 train_time:107274ms step_avg:96.91ms
step:1108/1750 train_time:107375ms step_avg:96.91ms
step:1109/1750 train_time:107476ms step_avg:96.91ms
step:1110/1750 train_time:107577ms step_avg:96.92ms
step:1111/1750 train_time:107677ms step_avg:96.92ms
step:1112/1750 train_time:107777ms step_avg:96.92ms
step:1113/1750 train_time:107877ms step_avg:96.92ms
step:1114/1750 train_time:107977ms step_avg:96.93ms
step:1115/1750 train_time:108078ms step_avg:96.93ms
step:1116/1750 train_time:108179ms step_avg:96.93ms
step:1117/1750 train_time:108278ms step_avg:96.94ms
step:1118/1750 train_time:108379ms step_avg:96.94ms
step:1119/1750 train_time:108478ms step_avg:96.94ms
step:1120/1750 train_time:108578ms step_avg:96.94ms
step:1121/1750 train_time:108678ms step_avg:96.95ms
step:1122/1750 train_time:108779ms step_avg:96.95ms
step:1123/1750 train_time:108879ms step_avg:96.95ms
step:1124/1750 train_time:108978ms step_avg:96.96ms
step:1125/1750 train_time:109078ms step_avg:96.96ms
step:1125/1750 val_loss:3.4528 train_time:109172ms step_avg:97.04ms
step:1126/1750 train_time:109199ms step_avg:96.98ms
step:1127/1750 train_time:109286ms step_avg:96.97ms
step:1128/1750 train_time:109387ms step_avg:96.97ms
step:1129/1750 train_time:109488ms step_avg:96.98ms
step:1130/1750 train_time:109589ms step_avg:96.98ms
step:1131/1750 train_time:109689ms step_avg:96.98ms
step:1132/1750 train_time:109790ms step_avg:96.99ms
step:1133/1750 train_time:109891ms step_avg:96.99ms
step:1134/1750 train_time:109991ms step_avg:96.99ms
step:1135/1750 train_time:110091ms step_avg:97.00ms
step:1136/1750 train_time:110193ms step_avg:97.00ms
step:1137/1750 train_time:110295ms step_avg:97.01ms
step:1138/1750 train_time:110396ms step_avg:97.01ms
step:1139/1750 train_time:110496ms step_avg:97.01ms
step:1140/1750 train_time:110596ms step_avg:97.01ms
step:1141/1750 train_time:110696ms step_avg:97.02ms
step:1142/1750 train_time:110796ms step_avg:97.02ms
step:1143/1750 train_time:110895ms step_avg:97.02ms
step:1144/1750 train_time:110995ms step_avg:97.02ms
step:1145/1750 train_time:111096ms step_avg:97.03ms
step:1146/1750 train_time:111196ms step_avg:97.03ms
step:1147/1750 train_time:111296ms step_avg:97.03ms
step:1148/1750 train_time:111398ms step_avg:97.04ms
step:1149/1750 train_time:111498ms step_avg:97.04ms
step:1150/1750 train_time:111599ms step_avg:97.04ms
step:1151/1750 train_time:111698ms step_avg:97.04ms
step:1152/1750 train_time:111798ms step_avg:97.05ms
step:1153/1750 train_time:111898ms step_avg:97.05ms
step:1154/1750 train_time:111999ms step_avg:97.05ms
step:1155/1750 train_time:112098ms step_avg:97.05ms
step:1156/1750 train_time:112198ms step_avg:97.06ms
step:1157/1750 train_time:112298ms step_avg:97.06ms
step:1158/1750 train_time:112398ms step_avg:97.06ms
step:1159/1750 train_time:112498ms step_avg:97.06ms
step:1160/1750 train_time:112598ms step_avg:97.07ms
step:1161/1750 train_time:112698ms step_avg:97.07ms
step:1162/1750 train_time:112798ms step_avg:97.07ms
step:1163/1750 train_time:112900ms step_avg:97.08ms
step:1164/1750 train_time:113000ms step_avg:97.08ms
step:1165/1750 train_time:113101ms step_avg:97.08ms
step:1166/1750 train_time:113201ms step_avg:97.08ms
step:1167/1750 train_time:113300ms step_avg:97.09ms
step:1168/1750 train_time:113401ms step_avg:97.09ms
step:1169/1750 train_time:113502ms step_avg:97.09ms
step:1170/1750 train_time:113603ms step_avg:97.10ms
step:1171/1750 train_time:113706ms step_avg:97.10ms
step:1172/1750 train_time:113807ms step_avg:97.11ms
step:1173/1750 train_time:113909ms step_avg:97.11ms
step:1174/1750 train_time:114011ms step_avg:97.11ms
step:1175/1750 train_time:114116ms step_avg:97.12ms
step:1176/1750 train_time:114218ms step_avg:97.12ms
step:1177/1750 train_time:114319ms step_avg:97.13ms
step:1178/1750 train_time:114420ms step_avg:97.13ms
step:1179/1750 train_time:114522ms step_avg:97.13ms
step:1180/1750 train_time:114623ms step_avg:97.14ms
step:1181/1750 train_time:114725ms step_avg:97.14ms
step:1182/1750 train_time:114827ms step_avg:97.15ms
step:1183/1750 train_time:114929ms step_avg:97.15ms
step:1184/1750 train_time:115032ms step_avg:97.16ms
step:1185/1750 train_time:115134ms step_avg:97.16ms
step:1186/1750 train_time:115237ms step_avg:97.16ms
step:1187/1750 train_time:115338ms step_avg:97.17ms
step:1188/1750 train_time:115439ms step_avg:97.17ms
step:1189/1750 train_time:115540ms step_avg:97.17ms
step:1190/1750 train_time:115640ms step_avg:97.18ms
step:1191/1750 train_time:115742ms step_avg:97.18ms
step:1192/1750 train_time:115843ms step_avg:97.18ms
step:1193/1750 train_time:115946ms step_avg:97.19ms
step:1194/1750 train_time:116050ms step_avg:97.19ms
step:1195/1750 train_time:116152ms step_avg:97.20ms
step:1196/1750 train_time:116255ms step_avg:97.20ms
step:1197/1750 train_time:116357ms step_avg:97.21ms
step:1198/1750 train_time:116458ms step_avg:97.21ms
step:1199/1750 train_time:116559ms step_avg:97.21ms
step:1200/1750 train_time:116660ms step_avg:97.22ms
step:1201/1750 train_time:116761ms step_avg:97.22ms
step:1202/1750 train_time:116864ms step_avg:97.22ms
step:1203/1750 train_time:116966ms step_avg:97.23ms
step:1204/1750 train_time:117067ms step_avg:97.23ms
step:1205/1750 train_time:117170ms step_avg:97.24ms
step:1206/1750 train_time:117272ms step_avg:97.24ms
step:1207/1750 train_time:117375ms step_avg:97.25ms
step:1208/1750 train_time:117477ms step_avg:97.25ms
step:1209/1750 train_time:117577ms step_avg:97.25ms
step:1210/1750 train_time:117679ms step_avg:97.25ms
step:1211/1750 train_time:117780ms step_avg:97.26ms
step:1212/1750 train_time:117881ms step_avg:97.26ms
step:1213/1750 train_time:117982ms step_avg:97.26ms
step:1214/1750 train_time:118083ms step_avg:97.27ms
step:1215/1750 train_time:118185ms step_avg:97.27ms
step:1216/1750 train_time:118288ms step_avg:97.28ms
step:1217/1750 train_time:118391ms step_avg:97.28ms
step:1218/1750 train_time:118495ms step_avg:97.29ms
step:1219/1750 train_time:118597ms step_avg:97.29ms
step:1220/1750 train_time:118699ms step_avg:97.29ms
step:1221/1750 train_time:118800ms step_avg:97.30ms
step:1222/1750 train_time:118902ms step_avg:97.30ms
step:1223/1750 train_time:119003ms step_avg:97.30ms
step:1224/1750 train_time:119104ms step_avg:97.31ms
step:1225/1750 train_time:119206ms step_avg:97.31ms
step:1226/1750 train_time:119308ms step_avg:97.31ms
step:1227/1750 train_time:119410ms step_avg:97.32ms
step:1228/1750 train_time:119512ms step_avg:97.32ms
step:1229/1750 train_time:119615ms step_avg:97.33ms
step:1230/1750 train_time:119718ms step_avg:97.33ms
step:1231/1750 train_time:119819ms step_avg:97.33ms
step:1232/1750 train_time:119921ms step_avg:97.34ms
step:1233/1750 train_time:120022ms step_avg:97.34ms
step:1234/1750 train_time:120123ms step_avg:97.34ms
step:1235/1750 train_time:120224ms step_avg:97.35ms
step:1236/1750 train_time:120326ms step_avg:97.35ms
step:1237/1750 train_time:120429ms step_avg:97.36ms
step:1238/1750 train_time:120532ms step_avg:97.36ms
step:1239/1750 train_time:120635ms step_avg:97.36ms
step:1240/1750 train_time:120737ms step_avg:97.37ms
step:1241/1750 train_time:120839ms step_avg:97.37ms
step:1242/1750 train_time:120940ms step_avg:97.37ms
step:1243/1750 train_time:121040ms step_avg:97.38ms
step:1244/1750 train_time:121141ms step_avg:97.38ms
step:1245/1750 train_time:121241ms step_avg:97.38ms
step:1246/1750 train_time:121343ms step_avg:97.39ms
step:1247/1750 train_time:121445ms step_avg:97.39ms
step:1248/1750 train_time:121549ms step_avg:97.39ms
step:1249/1750 train_time:121652ms step_avg:97.40ms
step:1250/1750 train_time:121754ms step_avg:97.40ms
step:1250/1750 val_loss:3.4072 train_time:121851ms step_avg:97.48ms
step:1251/1750 train_time:121878ms step_avg:97.42ms
step:1252/1750 train_time:121967ms step_avg:97.42ms
step:1253/1750 train_time:122069ms step_avg:97.42ms
step:1254/1750 train_time:122171ms step_avg:97.42ms
step:1255/1750 train_time:122273ms step_avg:97.43ms
step:1256/1750 train_time:122374ms step_avg:97.43ms
step:1257/1750 train_time:122476ms step_avg:97.43ms
step:1258/1750 train_time:122577ms step_avg:97.44ms
step:1259/1750 train_time:122677ms step_avg:97.44ms
step:1260/1750 train_time:122778ms step_avg:97.44ms
step:1261/1750 train_time:122880ms step_avg:97.45ms
step:1262/1750 train_time:122983ms step_avg:97.45ms
step:1263/1750 train_time:123083ms step_avg:97.45ms
step:1264/1750 train_time:123185ms step_avg:97.46ms
step:1265/1750 train_time:123285ms step_avg:97.46ms
step:1266/1750 train_time:123386ms step_avg:97.46ms
step:1267/1750 train_time:123489ms step_avg:97.47ms
step:1268/1750 train_time:123590ms step_avg:97.47ms
step:1269/1750 train_time:123692ms step_avg:97.47ms
step:1270/1750 train_time:123795ms step_avg:97.48ms
step:1271/1750 train_time:123900ms step_avg:97.48ms
step:1272/1750 train_time:124001ms step_avg:97.49ms
step:1273/1750 train_time:124102ms step_avg:97.49ms
step:1274/1750 train_time:124203ms step_avg:97.49ms
step:1275/1750 train_time:124303ms step_avg:97.49ms
step:1276/1750 train_time:124405ms step_avg:97.50ms
step:1277/1750 train_time:124506ms step_avg:97.50ms
step:1278/1750 train_time:124608ms step_avg:97.50ms
step:1279/1750 train_time:124711ms step_avg:97.51ms
step:1280/1750 train_time:124814ms step_avg:97.51ms
step:1281/1750 train_time:124917ms step_avg:97.52ms
step:1282/1750 train_time:125019ms step_avg:97.52ms
step:1283/1750 train_time:125120ms step_avg:97.52ms
step:1284/1750 train_time:125221ms step_avg:97.52ms
step:1285/1750 train_time:125322ms step_avg:97.53ms
step:1286/1750 train_time:125422ms step_avg:97.53ms
step:1287/1750 train_time:125523ms step_avg:97.53ms
step:1288/1750 train_time:125624ms step_avg:97.53ms
step:1289/1750 train_time:125726ms step_avg:97.54ms
step:1290/1750 train_time:125828ms step_avg:97.54ms
step:1291/1750 train_time:125931ms step_avg:97.55ms
step:1292/1750 train_time:126033ms step_avg:97.55ms
step:1293/1750 train_time:126136ms step_avg:97.55ms
step:1294/1750 train_time:126239ms step_avg:97.56ms
step:1295/1750 train_time:126340ms step_avg:97.56ms
step:1296/1750 train_time:126440ms step_avg:97.56ms
step:1297/1750 train_time:126541ms step_avg:97.56ms
step:1298/1750 train_time:126642ms step_avg:97.57ms
step:1299/1750 train_time:126744ms step_avg:97.57ms
step:1300/1750 train_time:126845ms step_avg:97.57ms
step:1301/1750 train_time:126948ms step_avg:97.58ms
step:1302/1750 train_time:127051ms step_avg:97.58ms
step:1303/1750 train_time:127155ms step_avg:97.59ms
step:1304/1750 train_time:127257ms step_avg:97.59ms
step:1305/1750 train_time:127359ms step_avg:97.59ms
step:1306/1750 train_time:127460ms step_avg:97.60ms
step:1307/1750 train_time:127562ms step_avg:97.60ms
step:1308/1750 train_time:127664ms step_avg:97.60ms
step:1309/1750 train_time:127765ms step_avg:97.60ms
step:1310/1750 train_time:127868ms step_avg:97.61ms
step:1311/1750 train_time:127970ms step_avg:97.61ms
step:1312/1750 train_time:128072ms step_avg:97.62ms
step:1313/1750 train_time:128176ms step_avg:97.62ms
step:1314/1750 train_time:128279ms step_avg:97.62ms
step:1315/1750 train_time:128380ms step_avg:97.63ms
step:1316/1750 train_time:128482ms step_avg:97.63ms
step:1317/1750 train_time:128583ms step_avg:97.63ms
step:1318/1750 train_time:128684ms step_avg:97.64ms
step:1319/1750 train_time:128786ms step_avg:97.64ms
step:1320/1750 train_time:128888ms step_avg:97.64ms
step:1321/1750 train_time:128990ms step_avg:97.65ms
step:1322/1750 train_time:129091ms step_avg:97.65ms
step:1323/1750 train_time:129194ms step_avg:97.65ms
step:1324/1750 train_time:129298ms step_avg:97.66ms
step:1325/1750 train_time:129400ms step_avg:97.66ms
step:1326/1750 train_time:129502ms step_avg:97.66ms
step:1327/1750 train_time:129604ms step_avg:97.67ms
step:1328/1750 train_time:129705ms step_avg:97.67ms
step:1329/1750 train_time:129806ms step_avg:97.67ms
step:1330/1750 train_time:129908ms step_avg:97.67ms
step:1331/1750 train_time:130009ms step_avg:97.68ms
step:1332/1750 train_time:130111ms step_avg:97.68ms
step:1333/1750 train_time:130214ms step_avg:97.69ms
step:1334/1750 train_time:130317ms step_avg:97.69ms
step:1335/1750 train_time:130420ms step_avg:97.69ms
step:1336/1750 train_time:130522ms step_avg:97.70ms
step:1337/1750 train_time:130623ms step_avg:97.70ms
step:1338/1750 train_time:130724ms step_avg:97.70ms
step:1339/1750 train_time:130826ms step_avg:97.70ms
step:1340/1750 train_time:130928ms step_avg:97.71ms
step:1341/1750 train_time:131028ms step_avg:97.71ms
step:1342/1750 train_time:131131ms step_avg:97.71ms
step:1343/1750 train_time:131233ms step_avg:97.72ms
step:1344/1750 train_time:131336ms step_avg:97.72ms
step:1345/1750 train_time:131438ms step_avg:97.72ms
step:1346/1750 train_time:131540ms step_avg:97.73ms
step:1347/1750 train_time:131643ms step_avg:97.73ms
step:1348/1750 train_time:131743ms step_avg:97.73ms
step:1349/1750 train_time:131844ms step_avg:97.73ms
step:1350/1750 train_time:131946ms step_avg:97.74ms
step:1351/1750 train_time:132046ms step_avg:97.74ms
step:1352/1750 train_time:132147ms step_avg:97.74ms
step:1353/1750 train_time:132250ms step_avg:97.75ms
step:1354/1750 train_time:132354ms step_avg:97.75ms
step:1355/1750 train_time:132456ms step_avg:97.75ms
step:1356/1750 train_time:132559ms step_avg:97.76ms
step:1357/1750 train_time:132660ms step_avg:97.76ms
step:1358/1750 train_time:132761ms step_avg:97.76ms
step:1359/1750 train_time:132862ms step_avg:97.76ms
step:1360/1750 train_time:132964ms step_avg:97.77ms
step:1361/1750 train_time:133065ms step_avg:97.77ms
step:1362/1750 train_time:133166ms step_avg:97.77ms
step:1363/1750 train_time:133269ms step_avg:97.78ms
step:1364/1750 train_time:133371ms step_avg:97.78ms
step:1365/1750 train_time:133473ms step_avg:97.78ms
step:1366/1750 train_time:133576ms step_avg:97.79ms
step:1367/1750 train_time:133678ms step_avg:97.79ms
step:1368/1750 train_time:133780ms step_avg:97.79ms
step:1369/1750 train_time:133881ms step_avg:97.79ms
step:1370/1750 train_time:133983ms step_avg:97.80ms
step:1371/1750 train_time:134084ms step_avg:97.80ms
step:1372/1750 train_time:134185ms step_avg:97.80ms
step:1373/1750 train_time:134286ms step_avg:97.80ms
step:1374/1750 train_time:134387ms step_avg:97.81ms
step:1375/1750 train_time:134490ms step_avg:97.81ms
step:1375/1750 val_loss:3.3665 train_time:134587ms step_avg:97.88ms
step:1376/1750 train_time:134615ms step_avg:97.83ms
step:1377/1750 train_time:134708ms step_avg:97.83ms
step:1378/1750 train_time:134810ms step_avg:97.83ms
step:1379/1750 train_time:134911ms step_avg:97.83ms
step:1380/1750 train_time:135013ms step_avg:97.84ms
step:1381/1750 train_time:135114ms step_avg:97.84ms
step:1382/1750 train_time:135216ms step_avg:97.84ms
step:1383/1750 train_time:135318ms step_avg:97.84ms
step:1384/1750 train_time:135419ms step_avg:97.85ms
step:1385/1750 train_time:135520ms step_avg:97.85ms
step:1386/1750 train_time:135624ms step_avg:97.85ms
step:1387/1750 train_time:135726ms step_avg:97.86ms
step:1388/1750 train_time:135827ms step_avg:97.86ms
step:1389/1750 train_time:135929ms step_avg:97.86ms
step:1390/1750 train_time:136031ms step_avg:97.86ms
step:1391/1750 train_time:136132ms step_avg:97.87ms
step:1392/1750 train_time:136235ms step_avg:97.87ms
step:1393/1750 train_time:136336ms step_avg:97.87ms
step:1394/1750 train_time:136438ms step_avg:97.88ms
step:1395/1750 train_time:136542ms step_avg:97.88ms
step:1396/1750 train_time:136644ms step_avg:97.88ms
step:1397/1750 train_time:136746ms step_avg:97.89ms
step:1398/1750 train_time:136848ms step_avg:97.89ms
step:1399/1750 train_time:136950ms step_avg:97.89ms
step:1400/1750 train_time:137051ms step_avg:97.89ms
step:1401/1750 train_time:137152ms step_avg:97.90ms
step:1402/1750 train_time:137254ms step_avg:97.90ms
step:1403/1750 train_time:137357ms step_avg:97.90ms
step:1404/1750 train_time:137460ms step_avg:97.91ms
step:1405/1750 train_time:137562ms step_avg:97.91ms
step:1406/1750 train_time:137665ms step_avg:97.91ms
step:1407/1750 train_time:137767ms step_avg:97.92ms
step:1408/1750 train_time:137868ms step_avg:97.92ms
step:1409/1750 train_time:137972ms step_avg:97.92ms
step:1410/1750 train_time:138073ms step_avg:97.92ms
step:1411/1750 train_time:138174ms step_avg:97.93ms
step:1412/1750 train_time:138276ms step_avg:97.93ms
step:1413/1750 train_time:138378ms step_avg:97.93ms
step:1414/1750 train_time:138481ms step_avg:97.94ms
step:1415/1750 train_time:138584ms step_avg:97.94ms
step:1416/1750 train_time:138685ms step_avg:97.94ms
step:1417/1750 train_time:138787ms step_avg:97.94ms
step:1418/1750 train_time:138890ms step_avg:97.95ms
step:1419/1750 train_time:138992ms step_avg:97.95ms
step:1420/1750 train_time:139093ms step_avg:97.95ms
step:1421/1750 train_time:139194ms step_avg:97.96ms
step:1422/1750 train_time:139296ms step_avg:97.96ms
step:1423/1750 train_time:139399ms step_avg:97.96ms
step:1424/1750 train_time:139502ms step_avg:97.96ms
step:1425/1750 train_time:139604ms step_avg:97.97ms
step:1426/1750 train_time:139706ms step_avg:97.97ms
step:1427/1750 train_time:139808ms step_avg:97.97ms
step:1428/1750 train_time:139913ms step_avg:97.98ms
step:1429/1750 train_time:140014ms step_avg:97.98ms
step:1430/1750 train_time:140117ms step_avg:97.98ms
step:1431/1750 train_time:140220ms step_avg:97.99ms
step:1432/1750 train_time:140322ms step_avg:97.99ms
step:1433/1750 train_time:140425ms step_avg:97.99ms
step:1434/1750 train_time:140527ms step_avg:98.00ms
step:1435/1750 train_time:140631ms step_avg:98.00ms
step:1436/1750 train_time:140736ms step_avg:98.01ms
step:1437/1750 train_time:140840ms step_avg:98.01ms
step:1438/1750 train_time:140942ms step_avg:98.01ms
step:1439/1750 train_time:141046ms step_avg:98.02ms
step:1440/1750 train_time:141150ms step_avg:98.02ms
step:1441/1750 train_time:141254ms step_avg:98.03ms
step:1442/1750 train_time:141356ms step_avg:98.03ms
step:1443/1750 train_time:141460ms step_avg:98.03ms
step:1444/1750 train_time:141564ms step_avg:98.04ms
step:1445/1750 train_time:141666ms step_avg:98.04ms
step:1446/1750 train_time:141770ms step_avg:98.04ms
step:1447/1750 train_time:141872ms step_avg:98.05ms
step:1448/1750 train_time:141976ms step_avg:98.05ms
step:1449/1750 train_time:142079ms step_avg:98.05ms
step:1450/1750 train_time:142183ms step_avg:98.06ms
step:1451/1750 train_time:142285ms step_avg:98.06ms
step:1452/1750 train_time:142387ms step_avg:98.06ms
step:1453/1750 train_time:142490ms step_avg:98.07ms
step:1454/1750 train_time:142596ms step_avg:98.07ms
step:1455/1750 train_time:142699ms step_avg:98.08ms
step:1456/1750 train_time:142802ms step_avg:98.08ms
step:1457/1750 train_time:142906ms step_avg:98.08ms
step:1458/1750 train_time:143008ms step_avg:98.09ms
step:1459/1750 train_time:143111ms step_avg:98.09ms
step:1460/1750 train_time:143213ms step_avg:98.09ms
step:1461/1750 train_time:143315ms step_avg:98.09ms
step:1462/1750 train_time:143419ms step_avg:98.10ms
step:1463/1750 train_time:143523ms step_avg:98.10ms
step:1464/1750 train_time:143625ms step_avg:98.10ms
step:1465/1750 train_time:143728ms step_avg:98.11ms
step:1466/1750 train_time:143830ms step_avg:98.11ms
step:1467/1750 train_time:143934ms step_avg:98.11ms
step:1468/1750 train_time:144038ms step_avg:98.12ms
step:1469/1750 train_time:144141ms step_avg:98.12ms
step:1470/1750 train_time:144244ms step_avg:98.13ms
step:1471/1750 train_time:144348ms step_avg:98.13ms
step:1472/1750 train_time:144451ms step_avg:98.13ms
step:1473/1750 train_time:144553ms step_avg:98.14ms
step:1474/1750 train_time:144658ms step_avg:98.14ms
step:1475/1750 train_time:144761ms step_avg:98.14ms
step:1476/1750 train_time:144864ms step_avg:98.15ms
step:1477/1750 train_time:144967ms step_avg:98.15ms
step:1478/1750 train_time:145070ms step_avg:98.15ms
step:1479/1750 train_time:145173ms step_avg:98.16ms
step:1480/1750 train_time:145275ms step_avg:98.16ms
step:1481/1750 train_time:145380ms step_avg:98.16ms
step:1482/1750 train_time:145484ms step_avg:98.17ms
step:1483/1750 train_time:145586ms step_avg:98.17ms
step:1484/1750 train_time:145689ms step_avg:98.17ms
step:1485/1750 train_time:145792ms step_avg:98.18ms
step:1486/1750 train_time:145895ms step_avg:98.18ms
step:1487/1750 train_time:145999ms step_avg:98.18ms
step:1488/1750 train_time:146103ms step_avg:98.19ms
step:1489/1750 train_time:146206ms step_avg:98.19ms
step:1490/1750 train_time:146309ms step_avg:98.19ms
step:1491/1750 train_time:146412ms step_avg:98.20ms
step:1492/1750 train_time:146515ms step_avg:98.20ms
step:1493/1750 train_time:146619ms step_avg:98.20ms
step:1494/1750 train_time:146721ms step_avg:98.21ms
step:1495/1750 train_time:146825ms step_avg:98.21ms
step:1496/1750 train_time:146928ms step_avg:98.21ms
step:1497/1750 train_time:147030ms step_avg:98.22ms
step:1498/1750 train_time:147133ms step_avg:98.22ms
step:1499/1750 train_time:147234ms step_avg:98.22ms
step:1500/1750 train_time:147338ms step_avg:98.23ms
step:1500/1750 val_loss:3.3302 train_time:147436ms step_avg:98.29ms
step:1501/1750 train_time:147462ms step_avg:98.24ms
step:1502/1750 train_time:147552ms step_avg:98.24ms
step:1503/1750 train_time:147655ms step_avg:98.24ms
step:1504/1750 train_time:147757ms step_avg:98.24ms
step:1505/1750 train_time:147858ms step_avg:98.24ms
step:1506/1750 train_time:147960ms step_avg:98.25ms
step:1507/1750 train_time:148063ms step_avg:98.25ms
step:1508/1750 train_time:148165ms step_avg:98.25ms
step:1509/1750 train_time:148267ms step_avg:98.26ms
step:1510/1750 train_time:148370ms step_avg:98.26ms
step:1511/1750 train_time:148476ms step_avg:98.26ms
step:1512/1750 train_time:148579ms step_avg:98.27ms
step:1513/1750 train_time:148682ms step_avg:98.27ms
step:1514/1750 train_time:148786ms step_avg:98.27ms
step:1515/1750 train_time:148892ms step_avg:98.28ms
step:1516/1750 train_time:148995ms step_avg:98.28ms
step:1517/1750 train_time:149097ms step_avg:98.28ms
step:1518/1750 train_time:149199ms step_avg:98.29ms
step:1519/1750 train_time:149302ms step_avg:98.29ms
step:1520/1750 train_time:149405ms step_avg:98.29ms
step:1521/1750 train_time:149508ms step_avg:98.30ms
step:1522/1750 train_time:149612ms step_avg:98.30ms
step:1523/1750 train_time:149714ms step_avg:98.30ms
step:1524/1750 train_time:149819ms step_avg:98.31ms
step:1525/1750 train_time:149922ms step_avg:98.31ms
step:1526/1750 train_time:150026ms step_avg:98.31ms
step:1527/1750 train_time:150130ms step_avg:98.32ms
step:1528/1750 train_time:150235ms step_avg:98.32ms
step:1529/1750 train_time:150338ms step_avg:98.32ms
step:1530/1750 train_time:150441ms step_avg:98.33ms
step:1531/1750 train_time:150543ms step_avg:98.33ms
step:1532/1750 train_time:150646ms step_avg:98.33ms
step:1533/1750 train_time:150749ms step_avg:98.34ms
step:1534/1750 train_time:150853ms step_avg:98.34ms
step:1535/1750 train_time:150956ms step_avg:98.34ms
step:1536/1750 train_time:151059ms step_avg:98.35ms
step:1537/1750 train_time:151161ms step_avg:98.35ms
step:1538/1750 train_time:151264ms step_avg:98.35ms
step:1539/1750 train_time:151366ms step_avg:98.35ms
step:1540/1750 train_time:151470ms step_avg:98.36ms
step:1541/1750 train_time:151573ms step_avg:98.36ms
step:1542/1750 train_time:151677ms step_avg:98.36ms
step:1543/1750 train_time:151781ms step_avg:98.37ms
step:1544/1750 train_time:151884ms step_avg:98.37ms
step:1545/1750 train_time:151987ms step_avg:98.37ms
step:1546/1750 train_time:152090ms step_avg:98.38ms
step:1547/1750 train_time:152195ms step_avg:98.38ms
step:1548/1750 train_time:152299ms step_avg:98.38ms
step:1549/1750 train_time:152401ms step_avg:98.39ms
step:1550/1750 train_time:152505ms step_avg:98.39ms
step:1551/1750 train_time:152607ms step_avg:98.39ms
step:1552/1750 train_time:152711ms step_avg:98.40ms
step:1553/1750 train_time:152815ms step_avg:98.40ms
step:1554/1750 train_time:152917ms step_avg:98.40ms
step:1555/1750 train_time:153019ms step_avg:98.40ms
step:1556/1750 train_time:153122ms step_avg:98.41ms
step:1557/1750 train_time:153228ms step_avg:98.41ms
step:1558/1750 train_time:153332ms step_avg:98.42ms
step:1559/1750 train_time:153436ms step_avg:98.42ms
step:1560/1750 train_time:153539ms step_avg:98.42ms
step:1561/1750 train_time:153641ms step_avg:98.43ms
step:1562/1750 train_time:153744ms step_avg:98.43ms
step:1563/1750 train_time:153850ms step_avg:98.43ms
step:1564/1750 train_time:153954ms step_avg:98.44ms
step:1565/1750 train_time:154056ms step_avg:98.44ms
step:1566/1750 train_time:154159ms step_avg:98.44ms
step:1567/1750 train_time:154261ms step_avg:98.44ms
step:1568/1750 train_time:154363ms step_avg:98.45ms
step:1569/1750 train_time:154465ms step_avg:98.45ms
step:1570/1750 train_time:154571ms step_avg:98.45ms
step:1571/1750 train_time:154674ms step_avg:98.46ms
step:1572/1750 train_time:154776ms step_avg:98.46ms
step:1573/1750 train_time:154879ms step_avg:98.46ms
step:1574/1750 train_time:154983ms step_avg:98.46ms
step:1575/1750 train_time:155085ms step_avg:98.47ms
step:1576/1750 train_time:155189ms step_avg:98.47ms
step:1577/1750 train_time:155294ms step_avg:98.47ms
step:1578/1750 train_time:155397ms step_avg:98.48ms
step:1579/1750 train_time:155500ms step_avg:98.48ms
step:1580/1750 train_time:155603ms step_avg:98.48ms
step:1581/1750 train_time:155706ms step_avg:98.49ms
step:1582/1750 train_time:155809ms step_avg:98.49ms
step:1583/1750 train_time:155915ms step_avg:98.49ms
step:1584/1750 train_time:156020ms step_avg:98.50ms
step:1585/1750 train_time:156122ms step_avg:98.50ms
step:1586/1750 train_time:156226ms step_avg:98.50ms
step:1587/1750 train_time:156329ms step_avg:98.51ms
step:1588/1750 train_time:156433ms step_avg:98.51ms
step:1589/1750 train_time:156536ms step_avg:98.51ms
step:1590/1750 train_time:156639ms step_avg:98.51ms
step:1591/1750 train_time:156741ms step_avg:98.52ms
step:1592/1750 train_time:156844ms step_avg:98.52ms
step:1593/1750 train_time:156946ms step_avg:98.52ms
step:1594/1750 train_time:157052ms step_avg:98.53ms
step:1595/1750 train_time:157154ms step_avg:98.53ms
step:1596/1750 train_time:157257ms step_avg:98.53ms
step:1597/1750 train_time:157359ms step_avg:98.53ms
step:1598/1750 train_time:157463ms step_avg:98.54ms
step:1599/1750 train_time:157566ms step_avg:98.54ms
step:1600/1750 train_time:157671ms step_avg:98.54ms
step:1601/1750 train_time:157774ms step_avg:98.55ms
step:1602/1750 train_time:157877ms step_avg:98.55ms
step:1603/1750 train_time:157980ms step_avg:98.55ms
step:1604/1750 train_time:158083ms step_avg:98.56ms
step:1605/1750 train_time:158188ms step_avg:98.56ms
step:1606/1750 train_time:158291ms step_avg:98.56ms
step:1607/1750 train_time:158394ms step_avg:98.57ms
step:1608/1750 train_time:158497ms step_avg:98.57ms
step:1609/1750 train_time:158599ms step_avg:98.57ms
step:1610/1750 train_time:158704ms step_avg:98.57ms
step:1611/1750 train_time:158807ms step_avg:98.58ms
step:1612/1750 train_time:158912ms step_avg:98.58ms
step:1613/1750 train_time:159014ms step_avg:98.58ms
step:1614/1750 train_time:159116ms step_avg:98.58ms
step:1615/1750 train_time:159219ms step_avg:98.59ms
step:1616/1750 train_time:159321ms step_avg:98.59ms
step:1617/1750 train_time:159425ms step_avg:98.59ms
step:1618/1750 train_time:159529ms step_avg:98.60ms
step:1619/1750 train_time:159633ms step_avg:98.60ms
step:1620/1750 train_time:159736ms step_avg:98.60ms
step:1621/1750 train_time:159838ms step_avg:98.60ms
step:1622/1750 train_time:159940ms step_avg:98.61ms
step:1623/1750 train_time:160043ms step_avg:98.61ms
step:1624/1750 train_time:160148ms step_avg:98.61ms
step:1625/1750 train_time:160252ms step_avg:98.62ms
step:1625/1750 val_loss:3.3001 train_time:160351ms step_avg:98.68ms
step:1626/1750 train_time:160377ms step_avg:98.63ms
step:1627/1750 train_time:160469ms step_avg:98.63ms
step:1628/1750 train_time:160571ms step_avg:98.63ms
step:1629/1750 train_time:160674ms step_avg:98.63ms
step:1630/1750 train_time:160778ms step_avg:98.64ms
step:1631/1750 train_time:160880ms step_avg:98.64ms
step:1632/1750 train_time:160983ms step_avg:98.64ms
step:1633/1750 train_time:161085ms step_avg:98.64ms
step:1634/1750 train_time:161190ms step_avg:98.65ms
step:1635/1750 train_time:161291ms step_avg:98.65ms
step:1636/1750 train_time:161396ms step_avg:98.65ms
step:1637/1750 train_time:161500ms step_avg:98.66ms
step:1638/1750 train_time:161603ms step_avg:98.66ms
step:1639/1750 train_time:161706ms step_avg:98.66ms
step:1640/1750 train_time:161808ms step_avg:98.66ms
step:1641/1750 train_time:161912ms step_avg:98.67ms
step:1642/1750 train_time:162013ms step_avg:98.67ms
step:1643/1750 train_time:162116ms step_avg:98.67ms
step:1644/1750 train_time:162219ms step_avg:98.67ms
step:1645/1750 train_time:162321ms step_avg:98.68ms
step:1646/1750 train_time:162425ms step_avg:98.68ms
step:1647/1750 train_time:162529ms step_avg:98.68ms
step:1648/1750 train_time:162633ms step_avg:98.69ms
step:1649/1750 train_time:162737ms step_avg:98.69ms
step:1650/1750 train_time:162841ms step_avg:98.69ms
step:1651/1750 train_time:162944ms step_avg:98.69ms
step:1652/1750 train_time:163046ms step_avg:98.70ms
step:1653/1750 train_time:163149ms step_avg:98.70ms
step:1654/1750 train_time:163251ms step_avg:98.70ms
step:1655/1750 train_time:163355ms step_avg:98.70ms
step:1656/1750 train_time:163458ms step_avg:98.71ms
step:1657/1750 train_time:163562ms step_avg:98.71ms
step:1658/1750 train_time:163665ms step_avg:98.71ms
step:1659/1750 train_time:163772ms step_avg:98.72ms
step:1660/1750 train_time:163875ms step_avg:98.72ms
step:1661/1750 train_time:163980ms step_avg:98.72ms
step:1662/1750 train_time:164084ms step_avg:98.73ms
step:1663/1750 train_time:164188ms step_avg:98.73ms
step:1664/1750 train_time:164291ms step_avg:98.73ms
step:1665/1750 train_time:164396ms step_avg:98.74ms
step:1666/1750 train_time:164500ms step_avg:98.74ms
step:1667/1750 train_time:164603ms step_avg:98.74ms
step:1668/1750 train_time:164708ms step_avg:98.75ms
step:1669/1750 train_time:164812ms step_avg:98.75ms
step:1670/1750 train_time:164914ms step_avg:98.75ms
step:1671/1750 train_time:165017ms step_avg:98.75ms
step:1672/1750 train_time:165122ms step_avg:98.76ms
step:1673/1750 train_time:165226ms step_avg:98.76ms
step:1674/1750 train_time:165328ms step_avg:98.76ms
step:1675/1750 train_time:165430ms step_avg:98.76ms
step:1676/1750 train_time:165533ms step_avg:98.77ms
step:1677/1750 train_time:165636ms step_avg:98.77ms
step:1678/1750 train_time:165740ms step_avg:98.77ms
step:1679/1750 train_time:165844ms step_avg:98.78ms
step:1680/1750 train_time:165947ms step_avg:98.78ms
step:1681/1750 train_time:166051ms step_avg:98.78ms
step:1682/1750 train_time:166156ms step_avg:98.78ms
step:1683/1750 train_time:166258ms step_avg:98.79ms
step:1684/1750 train_time:166362ms step_avg:98.79ms
step:1685/1750 train_time:166466ms step_avg:98.79ms
step:1686/1750 train_time:166568ms step_avg:98.79ms
step:1687/1750 train_time:166671ms step_avg:98.80ms
step:1688/1750 train_time:166773ms step_avg:98.80ms
step:1689/1750 train_time:166878ms step_avg:98.80ms
step:1690/1750 train_time:166982ms step_avg:98.81ms
step:1691/1750 train_time:167087ms step_avg:98.81ms
step:1692/1750 train_time:167190ms step_avg:98.81ms
step:1693/1750 train_time:167293ms step_avg:98.81ms
step:1694/1750 train_time:167398ms step_avg:98.82ms
step:1695/1750 train_time:167505ms step_avg:98.82ms
step:1696/1750 train_time:167607ms step_avg:98.83ms
step:1697/1750 train_time:167714ms step_avg:98.83ms
step:1698/1750 train_time:167817ms step_avg:98.83ms
step:1699/1750 train_time:167921ms step_avg:98.84ms
step:1700/1750 train_time:168026ms step_avg:98.84ms
step:1701/1750 train_time:168130ms step_avg:98.84ms
step:1702/1750 train_time:168236ms step_avg:98.85ms
step:1703/1750 train_time:168340ms step_avg:98.85ms
step:1704/1750 train_time:168444ms step_avg:98.85ms
step:1705/1750 train_time:168548ms step_avg:98.85ms
step:1706/1750 train_time:168652ms step_avg:98.86ms
step:1707/1750 train_time:168756ms step_avg:98.86ms
step:1708/1750 train_time:168862ms step_avg:98.87ms
step:1709/1750 train_time:168965ms step_avg:98.87ms
step:1710/1750 train_time:169070ms step_avg:98.87ms
step:1711/1750 train_time:169174ms step_avg:98.87ms
step:1712/1750 train_time:169278ms step_avg:98.88ms
step:1713/1750 train_time:169383ms step_avg:98.88ms
step:1714/1750 train_time:169487ms step_avg:98.88ms
step:1715/1750 train_time:169593ms step_avg:98.89ms
step:1716/1750 train_time:169696ms step_avg:98.89ms
step:1717/1750 train_time:169800ms step_avg:98.89ms
step:1718/1750 train_time:169904ms step_avg:98.90ms
step:1719/1750 train_time:170011ms step_avg:98.90ms
step:1720/1750 train_time:170115ms step_avg:98.90ms
step:1721/1750 train_time:170219ms step_avg:98.91ms
step:1722/1750 train_time:170324ms step_avg:98.91ms
step:1723/1750 train_time:170427ms step_avg:98.91ms
step:1724/1750 train_time:170532ms step_avg:98.92ms
step:1725/1750 train_time:170636ms step_avg:98.92ms
step:1726/1750 train_time:170740ms step_avg:98.92ms
step:1727/1750 train_time:170845ms step_avg:98.93ms
step:1728/1750 train_time:170950ms step_avg:98.93ms
step:1729/1750 train_time:171054ms step_avg:98.93ms
step:1730/1750 train_time:171157ms step_avg:98.93ms
step:1731/1750 train_time:171262ms step_avg:98.94ms
step:1732/1750 train_time:171366ms step_avg:98.94ms
step:1733/1750 train_time:171470ms step_avg:98.94ms
step:1734/1750 train_time:171575ms step_avg:98.95ms
step:1735/1750 train_time:171678ms step_avg:98.95ms
step:1736/1750 train_time:171783ms step_avg:98.95ms
step:1737/1750 train_time:171888ms step_avg:98.96ms
step:1738/1750 train_time:171992ms step_avg:98.96ms
step:1739/1750 train_time:172096ms step_avg:98.96ms
step:1740/1750 train_time:172201ms step_avg:98.97ms
step:1741/1750 train_time:172310ms step_avg:98.97ms
step:1742/1750 train_time:172414ms step_avg:98.97ms
step:1743/1750 train_time:172519ms step_avg:98.98ms
step:1744/1750 train_time:172623ms step_avg:98.98ms
step:1745/1750 train_time:172727ms step_avg:98.98ms
step:1746/1750 train_time:172830ms step_avg:98.99ms
step:1747/1750 train_time:172934ms step_avg:98.99ms
step:1748/1750 train_time:173039ms step_avg:98.99ms
step:1749/1750 train_time:173143ms step_avg:99.00ms
step:1750/1750 train_time:173250ms step_avg:99.00ms
step:1750/1750 val_loss:3.2794 train_time:173348ms step_avg:99.06ms
peak memory allocated: 33277 MiB reserved: 48012 MiB

import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn, autocast
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
#import wandb

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)

        params = list(params)
        sizes = {p.shape for p in params}

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params,))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * self.world_size
            for base_i in range(0, len(params), self.world_size):
                if base_i + self.rank < len(params):
                    grad = params[base_i + self.rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + self.world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * self.world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), self.world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                futures.append(dist.all_gather(params_pad[base_i:base_i + self.world_size], params_pad[base_i + self.rank], async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01, rank: int = 0, world_size: int = 1):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        self.rank = rank
        self.world_size = world_size

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(
                params=group_params,
            ))
        super().__init__(param_groups, defaults)

    @torch.compile
    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // self.world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)

                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']

                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)

                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t

                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        for param in self.embed.parameters():
            param.lr_mul = 75.
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embeds in self.value_embeds:
            for param in self.value_embeds.parameters():
                param.lr_mul = 75.
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.lr_mul = 27.5
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % world_size
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            #return causal_mask
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, world_size: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos:pos+max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()

    starts = []
    batch_end=None
    for i in range(len(boundary_positions) - 1):
        end = boundary_positions[i + 1].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == world_size:
                batch_end = end
                break
            start = end
    assert batch_end is not None # increase max_batch_span if necessary
    batch_span = batch_end-pos
    return starts, batch_span

def distributed_data_generator(filename_pattern: str, batch_size: int, rank: int, world_size: int, align_to_bos: bool):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    batch_span = batch_size
    max_batch_span = 2*batch_size if align_to_bos else batch_size #provide buffer to handle samples up to length local_batch_size

    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, world_size, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    train_align_to_bos = True # align local batch start indicies with next bos_token
    val_align_to_bos = False # False to maintain same eval as prior records
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

#if master_process:
#    wandb.init(project="modded-nanogpt-tiny", name=f"run-{os.path.basename(__file__)}", save_code=True)

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=True):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
if master_process:
    print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=next_multiple_of_n(50257, n=128), num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094

optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0, rank=rank, world_size=world_size)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]

for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

for n, p in model.named_parameters():
    wd_mul = getattr(p, "wd_mul", 1.0)
    lr_mul = getattr(p, "lr_mul", 1.0)

    print0(f"{n}: {p.shape} {p.dtype} {wd_mul} {lr_mul}")

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
embedding_params = sum(p.numel() for n, p in model.named_parameters() if "embed" in n)
non_embedding_params = total_params - embedding_params

print0(f"")
print0(f"Model parameters:")
print0(f"  Total parameters: {total_params:,}")
print0(f"  Embedding parameters: {embedding_params:,}")
print0(f"  Non-embedding parameters: {non_embedding_params:,}")

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    w = min((1 - x) / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.05
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)

torch.cuda.synchronize()
dist.barrier()
with torch.profiler.profile() as prof:
    for _ in range(warmup_steps):
        inputs, targets = next(train_loader)
        model(inputs, targets, get_window_size_blocks(1)).backward()
        for opt in optimizers:
            opt.step()
    model.zero_grad(set_to_none=True)
    torch.cuda.synchronize()
    dist.barrier()
os.makedirs("traces", exist_ok=True)
prof.export_chrome_trace(f"traces/trace_{rank}.json")

model.load_state_dict(initial_state['model'])
for opt, opt_state in zip(optimizers, initial_state['optimizers']):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size, args.val_align_to_bos)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        #if master_process:
        #    wandb.log({"val/loss": val_loss}, step=step)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.8.0.dev20250524+cu126 compiled for CUDA 12.6
Sun Jul 13 00:39:45 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    5856MiB /  81559MiB |      3%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   36C    P0            121W /  700W |    1517MiB /  81559MiB |      2%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   38C    P0            125W /  700W |    1517MiB /  81559MiB |      2%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   31C    P0            117W /  700W |    1517MiB /  81559MiB |      2%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1517MiB /  81559MiB |      2%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   38C    P0            121W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   38C    P0            123W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   31C    P0            114W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           57748      C   /usr/bin/python3                       1508MiB |
|    0   N/A  N/A           57749      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           57750      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           57751      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           57752      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           57753      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           57754      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           57755      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A           57749      C   /usr/bin/python3                       1508MiB |
|    2   N/A  N/A           57750      C   /usr/bin/python3                       1508MiB |
|    3   N/A  N/A           57751      C   /usr/bin/python3                       1508MiB |
|    4   N/A  N/A           57752      C   /usr/bin/python3                       1508MiB |
|    5   N/A  N/A           57753      C   /usr/bin/python3                       1508MiB |
|    6   N/A  N/A           57754      C   /usr/bin/python3                       1508MiB |
|    7   N/A  N/A           57755      C   /usr/bin/python3                       1508MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
scalars: torch.Size([64]) torch.float32 1.0 5.0
embed.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.0.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.1.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.2.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
blocks.0.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.0.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.1.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.1.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.2.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.2.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.3.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.3.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.4.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.4.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.5.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.5.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.6.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.6.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.7.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.7.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.8.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.8.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.9.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.9.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.10.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.10.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.11.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.11.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
lm_head.weight: torch.Size([50304, 768]) torch.float32 1.0 27.5

Model parameters:
  Total parameters: 275,742,784
  Embedding parameters: 154,533,888
  Non-embedding parameters: 121,208,896
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.01ms
step:1/1750 train_time:151ms step_avg:150.70ms
step:2/1750 train_time:176ms step_avg:88.24ms
step:3/1750 train_time:248ms step_avg:82.81ms
step:4/1750 train_time:340ms step_avg:84.99ms
step:5/1750 train_time:432ms step_avg:86.40ms
step:6/1750 train_time:525ms step_avg:87.50ms
step:7/1750 train_time:617ms step_avg:88.11ms
step:8/1750 train_time:709ms step_avg:88.65ms
step:9/1750 train_time:802ms step_avg:89.07ms
step:10/1750 train_time:895ms step_avg:89.47ms
step:11/1750 train_time:988ms step_avg:89.80ms
step:12/1750 train_time:1082ms step_avg:90.15ms
step:13/1750 train_time:1177ms step_avg:90.57ms
step:14/1750 train_time:1271ms step_avg:90.80ms
step:15/1750 train_time:1365ms step_avg:91.02ms
step:16/1750 train_time:1459ms step_avg:91.22ms
step:17/1750 train_time:1552ms step_avg:91.31ms
step:18/1750 train_time:1645ms step_avg:91.38ms
step:19/1750 train_time:1738ms step_avg:91.45ms
step:20/1750 train_time:1830ms step_avg:91.50ms
step:21/1750 train_time:1922ms step_avg:91.54ms
step:22/1750 train_time:2016ms step_avg:91.62ms
step:23/1750 train_time:2110ms step_avg:91.73ms
step:24/1750 train_time:2203ms step_avg:91.81ms
step:25/1750 train_time:2297ms step_avg:91.88ms
step:26/1750 train_time:2390ms step_avg:91.92ms
step:27/1750 train_time:2484ms step_avg:91.99ms
step:28/1750 train_time:2577ms step_avg:92.03ms
step:29/1750 train_time:2670ms step_avg:92.07ms
step:30/1750 train_time:2763ms step_avg:92.10ms
step:31/1750 train_time:2855ms step_avg:92.11ms
step:32/1750 train_time:2948ms step_avg:92.13ms
step:33/1750 train_time:3042ms step_avg:92.18ms
step:34/1750 train_time:3135ms step_avg:92.21ms
step:35/1750 train_time:3229ms step_avg:92.27ms
step:36/1750 train_time:3323ms step_avg:92.32ms
step:37/1750 train_time:3416ms step_avg:92.34ms
step:38/1750 train_time:3510ms step_avg:92.38ms
step:39/1750 train_time:3604ms step_avg:92.41ms
step:40/1750 train_time:3696ms step_avg:92.41ms
step:41/1750 train_time:3789ms step_avg:92.41ms
step:42/1750 train_time:3883ms step_avg:92.45ms
step:43/1750 train_time:3975ms step_avg:92.45ms
step:44/1750 train_time:4068ms step_avg:92.46ms
step:45/1750 train_time:4162ms step_avg:92.49ms
step:46/1750 train_time:4257ms step_avg:92.54ms
step:47/1750 train_time:4349ms step_avg:92.54ms
step:48/1750 train_time:4444ms step_avg:92.58ms
step:49/1750 train_time:4538ms step_avg:92.61ms
step:50/1750 train_time:4629ms step_avg:92.59ms
step:51/1750 train_time:4723ms step_avg:92.62ms
step:52/1750 train_time:4816ms step_avg:92.62ms
step:53/1750 train_time:4909ms step_avg:92.63ms
step:54/1750 train_time:5003ms step_avg:92.64ms
step:55/1750 train_time:5097ms step_avg:92.67ms
step:56/1750 train_time:5189ms step_avg:92.66ms
step:57/1750 train_time:5282ms step_avg:92.67ms
step:58/1750 train_time:5375ms step_avg:92.66ms
step:59/1750 train_time:5468ms step_avg:92.68ms
step:60/1750 train_time:5561ms step_avg:92.69ms
step:61/1750 train_time:5655ms step_avg:92.71ms
step:62/1750 train_time:5749ms step_avg:92.73ms
step:63/1750 train_time:5843ms step_avg:92.74ms
step:64/1750 train_time:5936ms step_avg:92.74ms
step:65/1750 train_time:6029ms step_avg:92.75ms
step:66/1750 train_time:6122ms step_avg:92.76ms
step:67/1750 train_time:6216ms step_avg:92.77ms
step:68/1750 train_time:6309ms step_avg:92.78ms
step:69/1750 train_time:6403ms step_avg:92.80ms
step:70/1750 train_time:6496ms step_avg:92.80ms
step:71/1750 train_time:6588ms step_avg:92.80ms
step:72/1750 train_time:6682ms step_avg:92.81ms
step:73/1750 train_time:6776ms step_avg:92.83ms
step:74/1750 train_time:6869ms step_avg:92.82ms
step:75/1750 train_time:6963ms step_avg:92.84ms
step:76/1750 train_time:7056ms step_avg:92.84ms
step:77/1750 train_time:7150ms step_avg:92.86ms
step:78/1750 train_time:7243ms step_avg:92.86ms
step:79/1750 train_time:7337ms step_avg:92.87ms
step:80/1750 train_time:7430ms step_avg:92.87ms
step:81/1750 train_time:7523ms step_avg:92.88ms
step:82/1750 train_time:7616ms step_avg:92.88ms
step:83/1750 train_time:7710ms step_avg:92.89ms
step:84/1750 train_time:7803ms step_avg:92.89ms
step:85/1750 train_time:7896ms step_avg:92.89ms
step:86/1750 train_time:7989ms step_avg:92.89ms
step:87/1750 train_time:8083ms step_avg:92.91ms
step:88/1750 train_time:8176ms step_avg:92.91ms
step:89/1750 train_time:8269ms step_avg:92.90ms
step:90/1750 train_time:8362ms step_avg:92.91ms
step:91/1750 train_time:8455ms step_avg:92.91ms
step:92/1750 train_time:8548ms step_avg:92.92ms
step:93/1750 train_time:8642ms step_avg:92.93ms
step:94/1750 train_time:8736ms step_avg:92.94ms
step:95/1750 train_time:8829ms step_avg:92.94ms
step:96/1750 train_time:8923ms step_avg:92.94ms
step:97/1750 train_time:9015ms step_avg:92.94ms
step:98/1750 train_time:9108ms step_avg:92.94ms
step:99/1750 train_time:9201ms step_avg:92.94ms
step:100/1750 train_time:9294ms step_avg:92.94ms
step:101/1750 train_time:9387ms step_avg:92.94ms
step:102/1750 train_time:9480ms step_avg:92.94ms
step:103/1750 train_time:9573ms step_avg:92.94ms
step:104/1750 train_time:9667ms step_avg:92.95ms
step:105/1750 train_time:9761ms step_avg:92.96ms
step:106/1750 train_time:9854ms step_avg:92.96ms
step:107/1750 train_time:9947ms step_avg:92.97ms
step:108/1750 train_time:10042ms step_avg:92.98ms
step:109/1750 train_time:10134ms step_avg:92.98ms
step:110/1750 train_time:10228ms step_avg:92.98ms
step:111/1750 train_time:10321ms step_avg:92.98ms
step:112/1750 train_time:10414ms step_avg:92.98ms
step:113/1750 train_time:10507ms step_avg:92.98ms
step:114/1750 train_time:10600ms step_avg:92.99ms
step:115/1750 train_time:10693ms step_avg:92.99ms
step:116/1750 train_time:10786ms step_avg:92.98ms
step:117/1750 train_time:10879ms step_avg:92.99ms
step:118/1750 train_time:10972ms step_avg:92.98ms
step:119/1750 train_time:11066ms step_avg:92.99ms
step:120/1750 train_time:11159ms step_avg:92.99ms
step:121/1750 train_time:11251ms step_avg:92.99ms
step:122/1750 train_time:11344ms step_avg:92.99ms
step:123/1750 train_time:11437ms step_avg:92.98ms
step:124/1750 train_time:11530ms step_avg:92.98ms
step:125/1750 train_time:11623ms step_avg:92.99ms
step:125/1750 val_loss:4.6333 train_time:11711ms step_avg:93.69ms
step:126/1750 train_time:11738ms step_avg:93.16ms
step:127/1750 train_time:11817ms step_avg:93.05ms
step:128/1750 train_time:11920ms step_avg:93.12ms
step:129/1750 train_time:12014ms step_avg:93.13ms
step:130/1750 train_time:12107ms step_avg:93.13ms
step:131/1750 train_time:12199ms step_avg:93.12ms
step:132/1750 train_time:12292ms step_avg:93.12ms
step:133/1750 train_time:12384ms step_avg:93.12ms
step:134/1750 train_time:12477ms step_avg:93.12ms
step:135/1750 train_time:12571ms step_avg:93.12ms
step:136/1750 train_time:12664ms step_avg:93.11ms
step:137/1750 train_time:12758ms step_avg:93.12ms
step:138/1750 train_time:12853ms step_avg:93.14ms
step:139/1750 train_time:12948ms step_avg:93.15ms
step:140/1750 train_time:13042ms step_avg:93.16ms
step:141/1750 train_time:13136ms step_avg:93.17ms
step:142/1750 train_time:13231ms step_avg:93.18ms
step:143/1750 train_time:13324ms step_avg:93.18ms
step:144/1750 train_time:13417ms step_avg:93.17ms
step:145/1750 train_time:13510ms step_avg:93.17ms
step:146/1750 train_time:13603ms step_avg:93.17ms
step:147/1750 train_time:13697ms step_avg:93.17ms
step:148/1750 train_time:13792ms step_avg:93.19ms
step:149/1750 train_time:13886ms step_avg:93.19ms
step:150/1750 train_time:13980ms step_avg:93.20ms
step:151/1750 train_time:14075ms step_avg:93.21ms
step:152/1750 train_time:14169ms step_avg:93.22ms
step:153/1750 train_time:14264ms step_avg:93.23ms
step:154/1750 train_time:14357ms step_avg:93.23ms
step:155/1750 train_time:14451ms step_avg:93.23ms
step:156/1750 train_time:14544ms step_avg:93.23ms
step:157/1750 train_time:14636ms step_avg:93.23ms
step:158/1750 train_time:14730ms step_avg:93.23ms
step:159/1750 train_time:14824ms step_avg:93.23ms
step:160/1750 train_time:14918ms step_avg:93.24ms
step:161/1750 train_time:15012ms step_avg:93.24ms
step:162/1750 train_time:15106ms step_avg:93.25ms
step:163/1750 train_time:15200ms step_avg:93.25ms
step:164/1750 train_time:15294ms step_avg:93.26ms
step:165/1750 train_time:15388ms step_avg:93.26ms
step:166/1750 train_time:15482ms step_avg:93.26ms
step:167/1750 train_time:15576ms step_avg:93.27ms
step:168/1750 train_time:15670ms step_avg:93.27ms
step:169/1750 train_time:15766ms step_avg:93.29ms
step:170/1750 train_time:15858ms step_avg:93.28ms
step:171/1750 train_time:15951ms step_avg:93.28ms
step:172/1750 train_time:16045ms step_avg:93.29ms
step:173/1750 train_time:16139ms step_avg:93.29ms
step:174/1750 train_time:16233ms step_avg:93.29ms
step:175/1750 train_time:16326ms step_avg:93.29ms
step:176/1750 train_time:16420ms step_avg:93.30ms
step:177/1750 train_time:16514ms step_avg:93.30ms
step:178/1750 train_time:16607ms step_avg:93.30ms
step:179/1750 train_time:16701ms step_avg:93.30ms
step:180/1750 train_time:16795ms step_avg:93.31ms
step:181/1750 train_time:16889ms step_avg:93.31ms
step:182/1750 train_time:16983ms step_avg:93.31ms
step:183/1750 train_time:17078ms step_avg:93.32ms
step:184/1750 train_time:17172ms step_avg:93.33ms
step:185/1750 train_time:17266ms step_avg:93.33ms
step:186/1750 train_time:17359ms step_avg:93.33ms
step:187/1750 train_time:17453ms step_avg:93.33ms
step:188/1750 train_time:17547ms step_avg:93.33ms
step:189/1750 train_time:17641ms step_avg:93.34ms
step:190/1750 train_time:17735ms step_avg:93.34ms
step:191/1750 train_time:17829ms step_avg:93.34ms
step:192/1750 train_time:17922ms step_avg:93.34ms
step:193/1750 train_time:18016ms step_avg:93.35ms
step:194/1750 train_time:18110ms step_avg:93.35ms
step:195/1750 train_time:18204ms step_avg:93.35ms
step:196/1750 train_time:18298ms step_avg:93.36ms
step:197/1750 train_time:18392ms step_avg:93.36ms
step:198/1750 train_time:18485ms step_avg:93.36ms
step:199/1750 train_time:18579ms step_avg:93.36ms
step:200/1750 train_time:18673ms step_avg:93.37ms
step:201/1750 train_time:18767ms step_avg:93.37ms
step:202/1750 train_time:18861ms step_avg:93.37ms
step:203/1750 train_time:18955ms step_avg:93.38ms
step:204/1750 train_time:19049ms step_avg:93.38ms
step:205/1750 train_time:19143ms step_avg:93.38ms
step:206/1750 train_time:19237ms step_avg:93.38ms
step:207/1750 train_time:19331ms step_avg:93.39ms
step:208/1750 train_time:19424ms step_avg:93.39ms
step:209/1750 train_time:19518ms step_avg:93.39ms
step:210/1750 train_time:19611ms step_avg:93.39ms
step:211/1750 train_time:19704ms step_avg:93.39ms
step:212/1750 train_time:19798ms step_avg:93.39ms
step:213/1750 train_time:19892ms step_avg:93.39ms
step:214/1750 train_time:19985ms step_avg:93.39ms
step:215/1750 train_time:20079ms step_avg:93.39ms
step:216/1750 train_time:20173ms step_avg:93.39ms
step:217/1750 train_time:20267ms step_avg:93.40ms
step:218/1750 train_time:20361ms step_avg:93.40ms
step:219/1750 train_time:20455ms step_avg:93.40ms
step:220/1750 train_time:20549ms step_avg:93.40ms
step:221/1750 train_time:20643ms step_avg:93.41ms
step:222/1750 train_time:20737ms step_avg:93.41ms
step:223/1750 train_time:20831ms step_avg:93.41ms
step:224/1750 train_time:20924ms step_avg:93.41ms
step:225/1750 train_time:21018ms step_avg:93.41ms
step:226/1750 train_time:21112ms step_avg:93.42ms
step:227/1750 train_time:21205ms step_avg:93.42ms
step:228/1750 train_time:21299ms step_avg:93.42ms
step:229/1750 train_time:21393ms step_avg:93.42ms
step:230/1750 train_time:21486ms step_avg:93.42ms
step:231/1750 train_time:21579ms step_avg:93.42ms
step:232/1750 train_time:21673ms step_avg:93.42ms
step:233/1750 train_time:21766ms step_avg:93.42ms
step:234/1750 train_time:21860ms step_avg:93.42ms
step:235/1750 train_time:21954ms step_avg:93.42ms
step:236/1750 train_time:22047ms step_avg:93.42ms
step:237/1750 train_time:22141ms step_avg:93.42ms
step:238/1750 train_time:22235ms step_avg:93.43ms
step:239/1750 train_time:22329ms step_avg:93.43ms
step:240/1750 train_time:22422ms step_avg:93.42ms
step:241/1750 train_time:22516ms step_avg:93.43ms
step:242/1750 train_time:22611ms step_avg:93.43ms
step:243/1750 train_time:22704ms step_avg:93.43ms
step:244/1750 train_time:22798ms step_avg:93.43ms
step:245/1750 train_time:22891ms step_avg:93.43ms
step:246/1750 train_time:22985ms step_avg:93.43ms
step:247/1750 train_time:23078ms step_avg:93.44ms
step:248/1750 train_time:23172ms step_avg:93.44ms
step:249/1750 train_time:23266ms step_avg:93.44ms
step:250/1750 train_time:23359ms step_avg:93.44ms
step:250/1750 val_loss:4.0996 train_time:23448ms step_avg:93.79ms
step:251/1750 train_time:23475ms step_avg:93.53ms
step:252/1750 train_time:23555ms step_avg:93.47ms
step:253/1750 train_time:23653ms step_avg:93.49ms
step:254/1750 train_time:23747ms step_avg:93.49ms
step:255/1750 train_time:23840ms step_avg:93.49ms
step:256/1750 train_time:23933ms step_avg:93.49ms
step:257/1750 train_time:24025ms step_avg:93.48ms
step:258/1750 train_time:24119ms step_avg:93.48ms
step:259/1750 train_time:24211ms step_avg:93.48ms
step:260/1750 train_time:24304ms step_avg:93.48ms
step:261/1750 train_time:24397ms step_avg:93.48ms
step:262/1750 train_time:24492ms step_avg:93.48ms
step:263/1750 train_time:24587ms step_avg:93.49ms
step:264/1750 train_time:24683ms step_avg:93.49ms
step:265/1750 train_time:24777ms step_avg:93.50ms
step:266/1750 train_time:24873ms step_avg:93.51ms
step:267/1750 train_time:24966ms step_avg:93.51ms
step:268/1750 train_time:25060ms step_avg:93.51ms
step:269/1750 train_time:25154ms step_avg:93.51ms
step:270/1750 train_time:25248ms step_avg:93.51ms
step:271/1750 train_time:25342ms step_avg:93.51ms
step:272/1750 train_time:25436ms step_avg:93.51ms
step:273/1750 train_time:25530ms step_avg:93.52ms
step:274/1750 train_time:25625ms step_avg:93.52ms
step:275/1750 train_time:25720ms step_avg:93.53ms
step:276/1750 train_time:25815ms step_avg:93.53ms
step:277/1750 train_time:25909ms step_avg:93.53ms
step:278/1750 train_time:26003ms step_avg:93.53ms
step:279/1750 train_time:26096ms step_avg:93.53ms
step:280/1750 train_time:26190ms step_avg:93.54ms
step:281/1750 train_time:26284ms step_avg:93.54ms
step:282/1750 train_time:26378ms step_avg:93.54ms
step:283/1750 train_time:26472ms step_avg:93.54ms
step:284/1750 train_time:26565ms step_avg:93.54ms
step:285/1750 train_time:26660ms step_avg:93.54ms
step:286/1750 train_time:26754ms step_avg:93.55ms
step:287/1750 train_time:26848ms step_avg:93.55ms
step:288/1750 train_time:26942ms step_avg:93.55ms
step:289/1750 train_time:27036ms step_avg:93.55ms
step:290/1750 train_time:27131ms step_avg:93.55ms
step:291/1750 train_time:27224ms step_avg:93.55ms
step:292/1750 train_time:27318ms step_avg:93.56ms
step:293/1750 train_time:27413ms step_avg:93.56ms
step:294/1750 train_time:27507ms step_avg:93.56ms
step:295/1750 train_time:27601ms step_avg:93.56ms
step:296/1750 train_time:27695ms step_avg:93.57ms
step:297/1750 train_time:27789ms step_avg:93.57ms
step:298/1750 train_time:27883ms step_avg:93.57ms
step:299/1750 train_time:27977ms step_avg:93.57ms
step:300/1750 train_time:28070ms step_avg:93.57ms
step:301/1750 train_time:28165ms step_avg:93.57ms
step:302/1750 train_time:28260ms step_avg:93.58ms
step:303/1750 train_time:28354ms step_avg:93.58ms
step:304/1750 train_time:28449ms step_avg:93.58ms
step:305/1750 train_time:28542ms step_avg:93.58ms
step:306/1750 train_time:28636ms step_avg:93.58ms
step:307/1750 train_time:28730ms step_avg:93.58ms
step:308/1750 train_time:28824ms step_avg:93.59ms
step:309/1750 train_time:28918ms step_avg:93.59ms
step:310/1750 train_time:29013ms step_avg:93.59ms
step:311/1750 train_time:29106ms step_avg:93.59ms
step:312/1750 train_time:29201ms step_avg:93.59ms
step:313/1750 train_time:29295ms step_avg:93.59ms
step:314/1750 train_time:29389ms step_avg:93.60ms
step:315/1750 train_time:29483ms step_avg:93.60ms
step:316/1750 train_time:29577ms step_avg:93.60ms
step:317/1750 train_time:29671ms step_avg:93.60ms
step:318/1750 train_time:29764ms step_avg:93.60ms
step:319/1750 train_time:29858ms step_avg:93.60ms
step:320/1750 train_time:29953ms step_avg:93.60ms
step:321/1750 train_time:30046ms step_avg:93.60ms
step:322/1750 train_time:30140ms step_avg:93.60ms
step:323/1750 train_time:30235ms step_avg:93.61ms
step:324/1750 train_time:30329ms step_avg:93.61ms
step:325/1750 train_time:30423ms step_avg:93.61ms
step:326/1750 train_time:30517ms step_avg:93.61ms
step:327/1750 train_time:30612ms step_avg:93.61ms
step:328/1750 train_time:30706ms step_avg:93.61ms
step:329/1750 train_time:30800ms step_avg:93.62ms
step:330/1750 train_time:30894ms step_avg:93.62ms
step:331/1750 train_time:30988ms step_avg:93.62ms
step:332/1750 train_time:31082ms step_avg:93.62ms
step:333/1750 train_time:31176ms step_avg:93.62ms
step:334/1750 train_time:31271ms step_avg:93.63ms
step:335/1750 train_time:31365ms step_avg:93.63ms
step:336/1750 train_time:31459ms step_avg:93.63ms
step:337/1750 train_time:31554ms step_avg:93.63ms
step:338/1750 train_time:31648ms step_avg:93.63ms
step:339/1750 train_time:31741ms step_avg:93.63ms
step:340/1750 train_time:31836ms step_avg:93.63ms
step:341/1750 train_time:31930ms step_avg:93.64ms
step:342/1750 train_time:32024ms step_avg:93.64ms
step:343/1750 train_time:32118ms step_avg:93.64ms
step:344/1750 train_time:32212ms step_avg:93.64ms
step:345/1750 train_time:32306ms step_avg:93.64ms
step:346/1750 train_time:32400ms step_avg:93.64ms
step:347/1750 train_time:32495ms step_avg:93.64ms
step:348/1750 train_time:32590ms step_avg:93.65ms
step:349/1750 train_time:32683ms step_avg:93.65ms
step:350/1750 train_time:32777ms step_avg:93.65ms
step:351/1750 train_time:32871ms step_avg:93.65ms
step:352/1750 train_time:32965ms step_avg:93.65ms
step:353/1750 train_time:33059ms step_avg:93.65ms
step:354/1750 train_time:33153ms step_avg:93.65ms
step:355/1750 train_time:33247ms step_avg:93.65ms
step:356/1750 train_time:33341ms step_avg:93.65ms
step:357/1750 train_time:33435ms step_avg:93.66ms
step:358/1750 train_time:33529ms step_avg:93.66ms
step:359/1750 train_time:33623ms step_avg:93.66ms
step:360/1750 train_time:33717ms step_avg:93.66ms
step:361/1750 train_time:33812ms step_avg:93.66ms
step:362/1750 train_time:33905ms step_avg:93.66ms
step:363/1750 train_time:34000ms step_avg:93.66ms
step:364/1750 train_time:34094ms step_avg:93.66ms
step:365/1750 train_time:34188ms step_avg:93.67ms
step:366/1750 train_time:34283ms step_avg:93.67ms
step:367/1750 train_time:34377ms step_avg:93.67ms
step:368/1750 train_time:34471ms step_avg:93.67ms
step:369/1750 train_time:34565ms step_avg:93.67ms
step:370/1750 train_time:34659ms step_avg:93.67ms
step:371/1750 train_time:34753ms step_avg:93.67ms
step:372/1750 train_time:34846ms step_avg:93.67ms
step:373/1750 train_time:34941ms step_avg:93.68ms
step:374/1750 train_time:35035ms step_avg:93.68ms
step:375/1750 train_time:35129ms step_avg:93.68ms
step:375/1750 val_loss:3.9031 train_time:35217ms step_avg:93.91ms
step:376/1750 train_time:35244ms step_avg:93.74ms
step:377/1750 train_time:35324ms step_avg:93.70ms
step:378/1750 train_time:35423ms step_avg:93.71ms
step:379/1750 train_time:35519ms step_avg:93.72ms
step:380/1750 train_time:35613ms step_avg:93.72ms
step:381/1750 train_time:35707ms step_avg:93.72ms
step:382/1750 train_time:35800ms step_avg:93.72ms
step:383/1750 train_time:35894ms step_avg:93.72ms
step:384/1750 train_time:35987ms step_avg:93.72ms
step:385/1750 train_time:36082ms step_avg:93.72ms
step:386/1750 train_time:36175ms step_avg:93.72ms
step:387/1750 train_time:36269ms step_avg:93.72ms
step:388/1750 train_time:36364ms step_avg:93.72ms
step:389/1750 train_time:36460ms step_avg:93.73ms
step:390/1750 train_time:36555ms step_avg:93.73ms
step:391/1750 train_time:36651ms step_avg:93.74ms
step:392/1750 train_time:36747ms step_avg:93.74ms
step:393/1750 train_time:36843ms step_avg:93.75ms
step:394/1750 train_time:36939ms step_avg:93.75ms
step:395/1750 train_time:37034ms step_avg:93.76ms
step:396/1750 train_time:37130ms step_avg:93.76ms
step:397/1750 train_time:37227ms step_avg:93.77ms
step:398/1750 train_time:37324ms step_avg:93.78ms
step:399/1750 train_time:37420ms step_avg:93.79ms
step:400/1750 train_time:37518ms step_avg:93.80ms
step:401/1750 train_time:37614ms step_avg:93.80ms
step:402/1750 train_time:37710ms step_avg:93.81ms
step:403/1750 train_time:37807ms step_avg:93.81ms
step:404/1750 train_time:37903ms step_avg:93.82ms
step:405/1750 train_time:37999ms step_avg:93.82ms
step:406/1750 train_time:38095ms step_avg:93.83ms
step:407/1750 train_time:38190ms step_avg:93.83ms
step:408/1750 train_time:38286ms step_avg:93.84ms
step:409/1750 train_time:38383ms step_avg:93.85ms
step:410/1750 train_time:38480ms step_avg:93.85ms
step:411/1750 train_time:38577ms step_avg:93.86ms
step:412/1750 train_time:38673ms step_avg:93.87ms
step:413/1750 train_time:38769ms step_avg:93.87ms
step:414/1750 train_time:38865ms step_avg:93.88ms
step:415/1750 train_time:38961ms step_avg:93.88ms
step:416/1750 train_time:39058ms step_avg:93.89ms
step:417/1750 train_time:39154ms step_avg:93.89ms
step:418/1750 train_time:39250ms step_avg:93.90ms
step:419/1750 train_time:39346ms step_avg:93.90ms
step:420/1750 train_time:39443ms step_avg:93.91ms
step:421/1750 train_time:39540ms step_avg:93.92ms
step:422/1750 train_time:39637ms step_avg:93.93ms
step:423/1750 train_time:39734ms step_avg:93.93ms
step:424/1750 train_time:39830ms step_avg:93.94ms
step:425/1750 train_time:39926ms step_avg:93.94ms
step:426/1750 train_time:40023ms step_avg:93.95ms
step:427/1750 train_time:40119ms step_avg:93.96ms
step:428/1750 train_time:40215ms step_avg:93.96ms
step:429/1750 train_time:40310ms step_avg:93.96ms
step:430/1750 train_time:40407ms step_avg:93.97ms
step:431/1750 train_time:40504ms step_avg:93.98ms
step:432/1750 train_time:40601ms step_avg:93.98ms
step:433/1750 train_time:40697ms step_avg:93.99ms
step:434/1750 train_time:40793ms step_avg:93.99ms
step:435/1750 train_time:40890ms step_avg:94.00ms
step:436/1750 train_time:40986ms step_avg:94.01ms
step:437/1750 train_time:41083ms step_avg:94.01ms
step:438/1750 train_time:41179ms step_avg:94.02ms
step:439/1750 train_time:41275ms step_avg:94.02ms
step:440/1750 train_time:41371ms step_avg:94.02ms
step:441/1750 train_time:41467ms step_avg:94.03ms
step:442/1750 train_time:41564ms step_avg:94.04ms
step:443/1750 train_time:41661ms step_avg:94.04ms
step:444/1750 train_time:41758ms step_avg:94.05ms
step:445/1750 train_time:41854ms step_avg:94.05ms
step:446/1750 train_time:41951ms step_avg:94.06ms
step:447/1750 train_time:42047ms step_avg:94.06ms
step:448/1750 train_time:42144ms step_avg:94.07ms
step:449/1750 train_time:42240ms step_avg:94.08ms
step:450/1750 train_time:42336ms step_avg:94.08ms
step:451/1750 train_time:42432ms step_avg:94.08ms
step:452/1750 train_time:42528ms step_avg:94.09ms
step:453/1750 train_time:42625ms step_avg:94.09ms
step:454/1750 train_time:42721ms step_avg:94.10ms
step:455/1750 train_time:42818ms step_avg:94.11ms
step:456/1750 train_time:42915ms step_avg:94.11ms
step:457/1750 train_time:43011ms step_avg:94.12ms
step:458/1750 train_time:43107ms step_avg:94.12ms
step:459/1750 train_time:43204ms step_avg:94.13ms
step:460/1750 train_time:43300ms step_avg:94.13ms
step:461/1750 train_time:43396ms step_avg:94.13ms
step:462/1750 train_time:43492ms step_avg:94.14ms
step:463/1750 train_time:43588ms step_avg:94.14ms
step:464/1750 train_time:43685ms step_avg:94.15ms
step:465/1750 train_time:43781ms step_avg:94.15ms
step:466/1750 train_time:43878ms step_avg:94.16ms
step:467/1750 train_time:43975ms step_avg:94.17ms
step:468/1750 train_time:44072ms step_avg:94.17ms
step:469/1750 train_time:44167ms step_avg:94.17ms
step:470/1750 train_time:44263ms step_avg:94.18ms
step:471/1750 train_time:44359ms step_avg:94.18ms
step:472/1750 train_time:44456ms step_avg:94.19ms
step:473/1750 train_time:44552ms step_avg:94.19ms
step:474/1750 train_time:44648ms step_avg:94.19ms
step:475/1750 train_time:44744ms step_avg:94.20ms
step:476/1750 train_time:44840ms step_avg:94.20ms
step:477/1750 train_time:44937ms step_avg:94.21ms
step:478/1750 train_time:45033ms step_avg:94.21ms
step:479/1750 train_time:45129ms step_avg:94.21ms
step:480/1750 train_time:45225ms step_avg:94.22ms
step:481/1750 train_time:45322ms step_avg:94.22ms
step:482/1750 train_time:45418ms step_avg:94.23ms
step:483/1750 train_time:45515ms step_avg:94.23ms
step:484/1750 train_time:45611ms step_avg:94.24ms
step:485/1750 train_time:45707ms step_avg:94.24ms
step:486/1750 train_time:45803ms step_avg:94.24ms
step:487/1750 train_time:45899ms step_avg:94.25ms
step:488/1750 train_time:45995ms step_avg:94.25ms
step:489/1750 train_time:46091ms step_avg:94.25ms
step:490/1750 train_time:46187ms step_avg:94.26ms
step:491/1750 train_time:46284ms step_avg:94.26ms
step:492/1750 train_time:46380ms step_avg:94.27ms
step:493/1750 train_time:46477ms step_avg:94.27ms
step:494/1750 train_time:46573ms step_avg:94.28ms
step:495/1750 train_time:46669ms step_avg:94.28ms
step:496/1750 train_time:46764ms step_avg:94.28ms
step:497/1750 train_time:46861ms step_avg:94.29ms
step:498/1750 train_time:46957ms step_avg:94.29ms
step:499/1750 train_time:47053ms step_avg:94.29ms
step:500/1750 train_time:47149ms step_avg:94.30ms
step:500/1750 val_loss:3.7530 train_time:47240ms step_avg:94.48ms
step:501/1750 train_time:47267ms step_avg:94.34ms
step:502/1750 train_time:47352ms step_avg:94.33ms
step:503/1750 train_time:47451ms step_avg:94.34ms
step:504/1750 train_time:47548ms step_avg:94.34ms
step:505/1750 train_time:47645ms step_avg:94.35ms
step:506/1750 train_time:47740ms step_avg:94.35ms
step:507/1750 train_time:47835ms step_avg:94.35ms
step:508/1750 train_time:47931ms step_avg:94.35ms
step:509/1750 train_time:48027ms step_avg:94.35ms
step:510/1750 train_time:48122ms step_avg:94.36ms
step:511/1750 train_time:48218ms step_avg:94.36ms
step:512/1750 train_time:48315ms step_avg:94.37ms
step:513/1750 train_time:48413ms step_avg:94.37ms
step:514/1750 train_time:48512ms step_avg:94.38ms
step:515/1750 train_time:48609ms step_avg:94.39ms
step:516/1750 train_time:48706ms step_avg:94.39ms
step:517/1750 train_time:48801ms step_avg:94.39ms
step:518/1750 train_time:48897ms step_avg:94.40ms
step:519/1750 train_time:48992ms step_avg:94.40ms
step:520/1750 train_time:49088ms step_avg:94.40ms
step:521/1750 train_time:49185ms step_avg:94.41ms
step:522/1750 train_time:49283ms step_avg:94.41ms
step:523/1750 train_time:49380ms step_avg:94.42ms
step:524/1750 train_time:49476ms step_avg:94.42ms
step:525/1750 train_time:49573ms step_avg:94.43ms
step:526/1750 train_time:49671ms step_avg:94.43ms
step:527/1750 train_time:49768ms step_avg:94.44ms
step:528/1750 train_time:49864ms step_avg:94.44ms
step:529/1750 train_time:49961ms step_avg:94.44ms
step:530/1750 train_time:50057ms step_avg:94.45ms
step:531/1750 train_time:50154ms step_avg:94.45ms
step:532/1750 train_time:50251ms step_avg:94.46ms
step:533/1750 train_time:50349ms step_avg:94.46ms
step:534/1750 train_time:50446ms step_avg:94.47ms
step:535/1750 train_time:50544ms step_avg:94.47ms
step:536/1750 train_time:50640ms step_avg:94.48ms
step:537/1750 train_time:50736ms step_avg:94.48ms
step:538/1750 train_time:50834ms step_avg:94.49ms
step:539/1750 train_time:50930ms step_avg:94.49ms
step:540/1750 train_time:51027ms step_avg:94.49ms
step:541/1750 train_time:51123ms step_avg:94.50ms
step:542/1750 train_time:51220ms step_avg:94.50ms
step:543/1750 train_time:51316ms step_avg:94.50ms
step:544/1750 train_time:51412ms step_avg:94.51ms
step:545/1750 train_time:51510ms step_avg:94.51ms
step:546/1750 train_time:51607ms step_avg:94.52ms
step:547/1750 train_time:51705ms step_avg:94.52ms
step:548/1750 train_time:51801ms step_avg:94.53ms
step:549/1750 train_time:51898ms step_avg:94.53ms
step:550/1750 train_time:51995ms step_avg:94.54ms
step:551/1750 train_time:52092ms step_avg:94.54ms
step:552/1750 train_time:52189ms step_avg:94.54ms
step:553/1750 train_time:52286ms step_avg:94.55ms
step:554/1750 train_time:52383ms step_avg:94.55ms
step:555/1750 train_time:52479ms step_avg:94.56ms
step:556/1750 train_time:52576ms step_avg:94.56ms
step:557/1750 train_time:52672ms step_avg:94.56ms
step:558/1750 train_time:52769ms step_avg:94.57ms
step:559/1750 train_time:52866ms step_avg:94.57ms
step:560/1750 train_time:52963ms step_avg:94.58ms
step:561/1750 train_time:53059ms step_avg:94.58ms
step:562/1750 train_time:53155ms step_avg:94.58ms
step:563/1750 train_time:53252ms step_avg:94.59ms
step:564/1750 train_time:53349ms step_avg:94.59ms
step:565/1750 train_time:53446ms step_avg:94.59ms
step:566/1750 train_time:53543ms step_avg:94.60ms
step:567/1750 train_time:53639ms step_avg:94.60ms
step:568/1750 train_time:53736ms step_avg:94.61ms
step:569/1750 train_time:53833ms step_avg:94.61ms
step:570/1750 train_time:53930ms step_avg:94.61ms
step:571/1750 train_time:54027ms step_avg:94.62ms
step:572/1750 train_time:54123ms step_avg:94.62ms
step:573/1750 train_time:54219ms step_avg:94.62ms
step:574/1750 train_time:54316ms step_avg:94.63ms
step:575/1750 train_time:54414ms step_avg:94.63ms
step:576/1750 train_time:54511ms step_avg:94.64ms
step:577/1750 train_time:54608ms step_avg:94.64ms
step:578/1750 train_time:54706ms step_avg:94.65ms
step:579/1750 train_time:54803ms step_avg:94.65ms
step:580/1750 train_time:54899ms step_avg:94.65ms
step:581/1750 train_time:54996ms step_avg:94.66ms
step:582/1750 train_time:55092ms step_avg:94.66ms
step:583/1750 train_time:55189ms step_avg:94.66ms
step:584/1750 train_time:55286ms step_avg:94.67ms
step:585/1750 train_time:55383ms step_avg:94.67ms
step:586/1750 train_time:55480ms step_avg:94.68ms
step:587/1750 train_time:55576ms step_avg:94.68ms
step:588/1750 train_time:55674ms step_avg:94.68ms
step:589/1750 train_time:55770ms step_avg:94.69ms
step:590/1750 train_time:55868ms step_avg:94.69ms
step:591/1750 train_time:55965ms step_avg:94.70ms
step:592/1750 train_time:56061ms step_avg:94.70ms
step:593/1750 train_time:56158ms step_avg:94.70ms
step:594/1750 train_time:56255ms step_avg:94.71ms
step:595/1750 train_time:56352ms step_avg:94.71ms
step:596/1750 train_time:56449ms step_avg:94.71ms
step:597/1750 train_time:56547ms step_avg:94.72ms
step:598/1750 train_time:56644ms step_avg:94.72ms
step:599/1750 train_time:56741ms step_avg:94.73ms
step:600/1750 train_time:56838ms step_avg:94.73ms
step:601/1750 train_time:56935ms step_avg:94.73ms
step:602/1750 train_time:57032ms step_avg:94.74ms
step:603/1750 train_time:57129ms step_avg:94.74ms
step:604/1750 train_time:57226ms step_avg:94.74ms
step:605/1750 train_time:57322ms step_avg:94.75ms
step:606/1750 train_time:57418ms step_avg:94.75ms
step:607/1750 train_time:57515ms step_avg:94.75ms
step:608/1750 train_time:57612ms step_avg:94.76ms
step:609/1750 train_time:57709ms step_avg:94.76ms
step:610/1750 train_time:57807ms step_avg:94.76ms
step:611/1750 train_time:57903ms step_avg:94.77ms
step:612/1750 train_time:58000ms step_avg:94.77ms
step:613/1750 train_time:58096ms step_avg:94.77ms
step:614/1750 train_time:58193ms step_avg:94.78ms
step:615/1750 train_time:58289ms step_avg:94.78ms
step:616/1750 train_time:58386ms step_avg:94.78ms
step:617/1750 train_time:58483ms step_avg:94.79ms
step:618/1750 train_time:58579ms step_avg:94.79ms
step:619/1750 train_time:58675ms step_avg:94.79ms
step:620/1750 train_time:58772ms step_avg:94.79ms
step:621/1750 train_time:58869ms step_avg:94.80ms
step:622/1750 train_time:58966ms step_avg:94.80ms
step:623/1750 train_time:59063ms step_avg:94.80ms
step:624/1750 train_time:59159ms step_avg:94.81ms
step:625/1750 train_time:59255ms step_avg:94.81ms
step:625/1750 val_loss:3.6643 train_time:59348ms step_avg:94.96ms
step:626/1750 train_time:59375ms step_avg:94.85ms
step:627/1750 train_time:59458ms step_avg:94.83ms
step:628/1750 train_time:59556ms step_avg:94.83ms
step:629/1750 train_time:59653ms step_avg:94.84ms
step:630/1750 train_time:59749ms step_avg:94.84ms
step:631/1750 train_time:59845ms step_avg:94.84ms
step:632/1750 train_time:59941ms step_avg:94.84ms
step:633/1750 train_time:60036ms step_avg:94.84ms
step:634/1750 train_time:60133ms step_avg:94.85ms
step:635/1750 train_time:60229ms step_avg:94.85ms
step:636/1750 train_time:60327ms step_avg:94.85ms
step:637/1750 train_time:60426ms step_avg:94.86ms
step:638/1750 train_time:60524ms step_avg:94.86ms
step:639/1750 train_time:60621ms step_avg:94.87ms
step:640/1750 train_time:60717ms step_avg:94.87ms
step:641/1750 train_time:60813ms step_avg:94.87ms
step:642/1750 train_time:60909ms step_avg:94.87ms
step:643/1750 train_time:61006ms step_avg:94.88ms
step:644/1750 train_time:61103ms step_avg:94.88ms
step:645/1750 train_time:61199ms step_avg:94.88ms
step:646/1750 train_time:61295ms step_avg:94.88ms
step:647/1750 train_time:61393ms step_avg:94.89ms
step:648/1750 train_time:61491ms step_avg:94.89ms
step:649/1750 train_time:61587ms step_avg:94.90ms
step:650/1750 train_time:61685ms step_avg:94.90ms
step:651/1750 train_time:61783ms step_avg:94.90ms
step:652/1750 train_time:61881ms step_avg:94.91ms
step:653/1750 train_time:61979ms step_avg:94.91ms
step:654/1750 train_time:62076ms step_avg:94.92ms
step:655/1750 train_time:62174ms step_avg:94.92ms
step:656/1750 train_time:62273ms step_avg:94.93ms
step:657/1750 train_time:62372ms step_avg:94.93ms
step:658/1750 train_time:62471ms step_avg:94.94ms
step:659/1750 train_time:62569ms step_avg:94.95ms
step:660/1750 train_time:62668ms step_avg:94.95ms
step:661/1750 train_time:62766ms step_avg:94.96ms
step:662/1750 train_time:62865ms step_avg:94.96ms
step:663/1750 train_time:62963ms step_avg:94.97ms
step:664/1750 train_time:63061ms step_avg:94.97ms
step:665/1750 train_time:63158ms step_avg:94.97ms
step:666/1750 train_time:63256ms step_avg:94.98ms
step:667/1750 train_time:63354ms step_avg:94.98ms
step:668/1750 train_time:63453ms step_avg:94.99ms
step:669/1750 train_time:63552ms step_avg:94.99ms
step:670/1750 train_time:63651ms step_avg:95.00ms
step:671/1750 train_time:63749ms step_avg:95.01ms
step:672/1750 train_time:63848ms step_avg:95.01ms
step:673/1750 train_time:63947ms step_avg:95.02ms
step:674/1750 train_time:64047ms step_avg:95.02ms
step:675/1750 train_time:64146ms step_avg:95.03ms
step:676/1750 train_time:64245ms step_avg:95.04ms
step:677/1750 train_time:64344ms step_avg:95.04ms
step:678/1750 train_time:64442ms step_avg:95.05ms
step:679/1750 train_time:64540ms step_avg:95.05ms
step:680/1750 train_time:64638ms step_avg:95.06ms
step:681/1750 train_time:64737ms step_avg:95.06ms
step:682/1750 train_time:64836ms step_avg:95.07ms
step:683/1750 train_time:64934ms step_avg:95.07ms
step:684/1750 train_time:65033ms step_avg:95.08ms
step:685/1750 train_time:65132ms step_avg:95.08ms
step:686/1750 train_time:65231ms step_avg:95.09ms
step:687/1750 train_time:65330ms step_avg:95.09ms
step:688/1750 train_time:65428ms step_avg:95.10ms
step:689/1750 train_time:65527ms step_avg:95.11ms
step:690/1750 train_time:65627ms step_avg:95.11ms
step:691/1750 train_time:65726ms step_avg:95.12ms
step:692/1750 train_time:65825ms step_avg:95.12ms
step:693/1750 train_time:65924ms step_avg:95.13ms
step:694/1750 train_time:66023ms step_avg:95.13ms
step:695/1750 train_time:66121ms step_avg:95.14ms
step:696/1750 train_time:66219ms step_avg:95.14ms
step:697/1750 train_time:66317ms step_avg:95.15ms
step:698/1750 train_time:66415ms step_avg:95.15ms
step:699/1750 train_time:66513ms step_avg:95.15ms
step:700/1750 train_time:66612ms step_avg:95.16ms
step:701/1750 train_time:66710ms step_avg:95.16ms
step:702/1750 train_time:66809ms step_avg:95.17ms
step:703/1750 train_time:66908ms step_avg:95.18ms
step:704/1750 train_time:67008ms step_avg:95.18ms
step:705/1750 train_time:67107ms step_avg:95.19ms
step:706/1750 train_time:67206ms step_avg:95.19ms
step:707/1750 train_time:67306ms step_avg:95.20ms
step:708/1750 train_time:67404ms step_avg:95.20ms
step:709/1750 train_time:67503ms step_avg:95.21ms
step:710/1750 train_time:67601ms step_avg:95.21ms
step:711/1750 train_time:67699ms step_avg:95.22ms
step:712/1750 train_time:67799ms step_avg:95.22ms
step:713/1750 train_time:67897ms step_avg:95.23ms
step:714/1750 train_time:67996ms step_avg:95.23ms
step:715/1750 train_time:68094ms step_avg:95.24ms
step:716/1750 train_time:68192ms step_avg:95.24ms
step:717/1750 train_time:68291ms step_avg:95.25ms
step:718/1750 train_time:68389ms step_avg:95.25ms
step:719/1750 train_time:68487ms step_avg:95.25ms
step:720/1750 train_time:68586ms step_avg:95.26ms
step:721/1750 train_time:68684ms step_avg:95.26ms
step:722/1750 train_time:68783ms step_avg:95.27ms
step:723/1750 train_time:68882ms step_avg:95.27ms
step:724/1750 train_time:68980ms step_avg:95.28ms
step:725/1750 train_time:69078ms step_avg:95.28ms
step:726/1750 train_time:69176ms step_avg:95.28ms
step:727/1750 train_time:69275ms step_avg:95.29ms
step:728/1750 train_time:69373ms step_avg:95.29ms
step:729/1750 train_time:69471ms step_avg:95.30ms
step:730/1750 train_time:69570ms step_avg:95.30ms
step:731/1750 train_time:69668ms step_avg:95.31ms
step:732/1750 train_time:69767ms step_avg:95.31ms
step:733/1750 train_time:69866ms step_avg:95.31ms
step:734/1750 train_time:69964ms step_avg:95.32ms
step:735/1750 train_time:70064ms step_avg:95.32ms
step:736/1750 train_time:70162ms step_avg:95.33ms
step:737/1750 train_time:70261ms step_avg:95.33ms
step:738/1750 train_time:70359ms step_avg:95.34ms
step:739/1750 train_time:70457ms step_avg:95.34ms
step:740/1750 train_time:70555ms step_avg:95.34ms
step:741/1750 train_time:70654ms step_avg:95.35ms
step:742/1750 train_time:70752ms step_avg:95.35ms
step:743/1750 train_time:70851ms step_avg:95.36ms
step:744/1750 train_time:70950ms step_avg:95.36ms
step:745/1750 train_time:71049ms step_avg:95.37ms
step:746/1750 train_time:71147ms step_avg:95.37ms
step:747/1750 train_time:71247ms step_avg:95.38ms
step:748/1750 train_time:71347ms step_avg:95.38ms
step:749/1750 train_time:71446ms step_avg:95.39ms
step:750/1750 train_time:71545ms step_avg:95.39ms
step:750/1750 val_loss:3.5982 train_time:71638ms step_avg:95.52ms
step:751/1750 train_time:71665ms step_avg:95.43ms
step:752/1750 train_time:71749ms step_avg:95.41ms
step:753/1750 train_time:71849ms step_avg:95.42ms
step:754/1750 train_time:71948ms step_avg:95.42ms
step:755/1750 train_time:72045ms step_avg:95.42ms
step:756/1750 train_time:72143ms step_avg:95.43ms
step:757/1750 train_time:72241ms step_avg:95.43ms
step:758/1750 train_time:72339ms step_avg:95.43ms
step:759/1750 train_time:72437ms step_avg:95.44ms
step:760/1750 train_time:72535ms step_avg:95.44ms
step:761/1750 train_time:72633ms step_avg:95.44ms
step:762/1750 train_time:72732ms step_avg:95.45ms
step:763/1750 train_time:72832ms step_avg:95.45ms
step:764/1750 train_time:72931ms step_avg:95.46ms
step:765/1750 train_time:73029ms step_avg:95.46ms
step:766/1750 train_time:73127ms step_avg:95.47ms
step:767/1750 train_time:73225ms step_avg:95.47ms
step:768/1750 train_time:73323ms step_avg:95.47ms
step:769/1750 train_time:73421ms step_avg:95.48ms
step:770/1750 train_time:73519ms step_avg:95.48ms
step:771/1750 train_time:73617ms step_avg:95.48ms
step:772/1750 train_time:73717ms step_avg:95.49ms
step:773/1750 train_time:73816ms step_avg:95.49ms
step:774/1750 train_time:73915ms step_avg:95.50ms
step:775/1750 train_time:74013ms step_avg:95.50ms
step:776/1750 train_time:74112ms step_avg:95.51ms
step:777/1750 train_time:74210ms step_avg:95.51ms
step:778/1750 train_time:74308ms step_avg:95.51ms
step:779/1750 train_time:74406ms step_avg:95.52ms
step:780/1750 train_time:74505ms step_avg:95.52ms
step:781/1750 train_time:74603ms step_avg:95.52ms
step:782/1750 train_time:74701ms step_avg:95.53ms
step:783/1750 train_time:74800ms step_avg:95.53ms
step:784/1750 train_time:74899ms step_avg:95.53ms
step:785/1750 train_time:74998ms step_avg:95.54ms
step:786/1750 train_time:75098ms step_avg:95.54ms
step:787/1750 train_time:75197ms step_avg:95.55ms
step:788/1750 train_time:75297ms step_avg:95.55ms
step:789/1750 train_time:75396ms step_avg:95.56ms
step:790/1750 train_time:75496ms step_avg:95.56ms
step:791/1750 train_time:75595ms step_avg:95.57ms
step:792/1750 train_time:75693ms step_avg:95.57ms
step:793/1750 train_time:75793ms step_avg:95.58ms
step:794/1750 train_time:75891ms step_avg:95.58ms
step:795/1750 train_time:75990ms step_avg:95.58ms
step:796/1750 train_time:76088ms step_avg:95.59ms
step:797/1750 train_time:76187ms step_avg:95.59ms
step:798/1750 train_time:76286ms step_avg:95.60ms
step:799/1750 train_time:76385ms step_avg:95.60ms
step:800/1750 train_time:76484ms step_avg:95.60ms
step:801/1750 train_time:76583ms step_avg:95.61ms
step:802/1750 train_time:76682ms step_avg:95.61ms
step:803/1750 train_time:76781ms step_avg:95.62ms
step:804/1750 train_time:76881ms step_avg:95.62ms
step:805/1750 train_time:76979ms step_avg:95.63ms
step:806/1750 train_time:77078ms step_avg:95.63ms
step:807/1750 train_time:77176ms step_avg:95.63ms
step:808/1750 train_time:77276ms step_avg:95.64ms
step:809/1750 train_time:77376ms step_avg:95.64ms
step:810/1750 train_time:77475ms step_avg:95.65ms
step:811/1750 train_time:77573ms step_avg:95.65ms
step:812/1750 train_time:77673ms step_avg:95.66ms
step:813/1750 train_time:77771ms step_avg:95.66ms
step:814/1750 train_time:77869ms step_avg:95.66ms
step:815/1750 train_time:77968ms step_avg:95.67ms
step:816/1750 train_time:78066ms step_avg:95.67ms
step:817/1750 train_time:78164ms step_avg:95.67ms
step:818/1750 train_time:78262ms step_avg:95.68ms
step:819/1750 train_time:78361ms step_avg:95.68ms
step:820/1750 train_time:78460ms step_avg:95.68ms
step:821/1750 train_time:78559ms step_avg:95.69ms
step:822/1750 train_time:78658ms step_avg:95.69ms
step:823/1750 train_time:78761ms step_avg:95.70ms
step:824/1750 train_time:78857ms step_avg:95.70ms
step:825/1750 train_time:78956ms step_avg:95.70ms
step:826/1750 train_time:79056ms step_avg:95.71ms
step:827/1750 train_time:79155ms step_avg:95.71ms
step:828/1750 train_time:79255ms step_avg:95.72ms
step:829/1750 train_time:79353ms step_avg:95.72ms
step:830/1750 train_time:79452ms step_avg:95.72ms
step:831/1750 train_time:79549ms step_avg:95.73ms
step:832/1750 train_time:79648ms step_avg:95.73ms
step:833/1750 train_time:79747ms step_avg:95.73ms
step:834/1750 train_time:79845ms step_avg:95.74ms
step:835/1750 train_time:79943ms step_avg:95.74ms
step:836/1750 train_time:80042ms step_avg:95.74ms
step:837/1750 train_time:80141ms step_avg:95.75ms
step:838/1750 train_time:80240ms step_avg:95.75ms
step:839/1750 train_time:80339ms step_avg:95.76ms
step:840/1750 train_time:80438ms step_avg:95.76ms
step:841/1750 train_time:80537ms step_avg:95.76ms
step:842/1750 train_time:80636ms step_avg:95.77ms
step:843/1750 train_time:80736ms step_avg:95.77ms
step:844/1750 train_time:80836ms step_avg:95.78ms
step:845/1750 train_time:80935ms step_avg:95.78ms
step:846/1750 train_time:81033ms step_avg:95.78ms
step:847/1750 train_time:81132ms step_avg:95.79ms
step:848/1750 train_time:81231ms step_avg:95.79ms
step:849/1750 train_time:81330ms step_avg:95.80ms
step:850/1750 train_time:81429ms step_avg:95.80ms
step:851/1750 train_time:81529ms step_avg:95.80ms
step:852/1750 train_time:81627ms step_avg:95.81ms
step:853/1750 train_time:81726ms step_avg:95.81ms
step:854/1750 train_time:81825ms step_avg:95.81ms
step:855/1750 train_time:81925ms step_avg:95.82ms
step:856/1750 train_time:82025ms step_avg:95.82ms
step:857/1750 train_time:82124ms step_avg:95.83ms
step:858/1750 train_time:82223ms step_avg:95.83ms
step:859/1750 train_time:82323ms step_avg:95.84ms
step:860/1750 train_time:82423ms step_avg:95.84ms
step:861/1750 train_time:82522ms step_avg:95.84ms
step:862/1750 train_time:82621ms step_avg:95.85ms
step:863/1750 train_time:82719ms step_avg:95.85ms
step:864/1750 train_time:82818ms step_avg:95.85ms
step:865/1750 train_time:82917ms step_avg:95.86ms
step:866/1750 train_time:83017ms step_avg:95.86ms
step:867/1750 train_time:83116ms step_avg:95.87ms
step:868/1750 train_time:83216ms step_avg:95.87ms
step:869/1750 train_time:83315ms step_avg:95.87ms
step:870/1750 train_time:83414ms step_avg:95.88ms
step:871/1750 train_time:83513ms step_avg:95.88ms
step:872/1750 train_time:83612ms step_avg:95.89ms
step:873/1750 train_time:83711ms step_avg:95.89ms
step:874/1750 train_time:83809ms step_avg:95.89ms
step:875/1750 train_time:83908ms step_avg:95.89ms
step:875/1750 val_loss:3.5501 train_time:84001ms step_avg:96.00ms
step:876/1750 train_time:84027ms step_avg:95.92ms
step:877/1750 train_time:84113ms step_avg:95.91ms
step:878/1750 train_time:84213ms step_avg:95.91ms
step:879/1750 train_time:84311ms step_avg:95.92ms
step:880/1750 train_time:84409ms step_avg:95.92ms
step:881/1750 train_time:84508ms step_avg:95.92ms
step:882/1750 train_time:84606ms step_avg:95.93ms
step:883/1750 train_time:84704ms step_avg:95.93ms
step:884/1750 train_time:84802ms step_avg:95.93ms
step:885/1750 train_time:84900ms step_avg:95.93ms
step:886/1750 train_time:84999ms step_avg:95.94ms
step:887/1750 train_time:85099ms step_avg:95.94ms
step:888/1750 train_time:85198ms step_avg:95.94ms
step:889/1750 train_time:85297ms step_avg:95.95ms
step:890/1750 train_time:85396ms step_avg:95.95ms
step:891/1750 train_time:85494ms step_avg:95.95ms
step:892/1750 train_time:85592ms step_avg:95.96ms
step:893/1750 train_time:85690ms step_avg:95.96ms
step:894/1750 train_time:85790ms step_avg:95.96ms
step:895/1750 train_time:85888ms step_avg:95.96ms
step:896/1750 train_time:85988ms step_avg:95.97ms
step:897/1750 train_time:86087ms step_avg:95.97ms
step:898/1750 train_time:86186ms step_avg:95.98ms
step:899/1750 train_time:86285ms step_avg:95.98ms
step:900/1750 train_time:86385ms step_avg:95.98ms
step:901/1750 train_time:86485ms step_avg:95.99ms
step:902/1750 train_time:86584ms step_avg:95.99ms
step:903/1750 train_time:86683ms step_avg:95.99ms
step:904/1750 train_time:86782ms step_avg:96.00ms
step:905/1750 train_time:86881ms step_avg:96.00ms
step:906/1750 train_time:86980ms step_avg:96.00ms
step:907/1750 train_time:87078ms step_avg:96.01ms
step:908/1750 train_time:87177ms step_avg:96.01ms
step:909/1750 train_time:87276ms step_avg:96.01ms
step:910/1750 train_time:87376ms step_avg:96.02ms
step:911/1750 train_time:87477ms step_avg:96.02ms
step:912/1750 train_time:87577ms step_avg:96.03ms
step:913/1750 train_time:87677ms step_avg:96.03ms
step:914/1750 train_time:87777ms step_avg:96.04ms
step:915/1750 train_time:87877ms step_avg:96.04ms
step:916/1750 train_time:87977ms step_avg:96.05ms
step:917/1750 train_time:88077ms step_avg:96.05ms
step:918/1750 train_time:88178ms step_avg:96.05ms
step:919/1750 train_time:88278ms step_avg:96.06ms
step:920/1750 train_time:88379ms step_avg:96.06ms
step:921/1750 train_time:88478ms step_avg:96.07ms
step:922/1750 train_time:88579ms step_avg:96.07ms
step:923/1750 train_time:88679ms step_avg:96.08ms
step:924/1750 train_time:88778ms step_avg:96.08ms
step:925/1750 train_time:88878ms step_avg:96.08ms
step:926/1750 train_time:88979ms step_avg:96.09ms
step:927/1750 train_time:89079ms step_avg:96.09ms
step:928/1750 train_time:89179ms step_avg:96.10ms
step:929/1750 train_time:89279ms step_avg:96.10ms
step:930/1750 train_time:89379ms step_avg:96.11ms
step:931/1750 train_time:89479ms step_avg:96.11ms
step:932/1750 train_time:89579ms step_avg:96.11ms
step:933/1750 train_time:89679ms step_avg:96.12ms
step:934/1750 train_time:89778ms step_avg:96.12ms
step:935/1750 train_time:89878ms step_avg:96.13ms
step:936/1750 train_time:89978ms step_avg:96.13ms
step:937/1750 train_time:90078ms step_avg:96.13ms
step:938/1750 train_time:90179ms step_avg:96.14ms
step:939/1750 train_time:90279ms step_avg:96.14ms
step:940/1750 train_time:90379ms step_avg:96.15ms
step:941/1750 train_time:90479ms step_avg:96.15ms
step:942/1750 train_time:90579ms step_avg:96.16ms
step:943/1750 train_time:90679ms step_avg:96.16ms
step:944/1750 train_time:90779ms step_avg:96.16ms
step:945/1750 train_time:90878ms step_avg:96.17ms
step:946/1750 train_time:90979ms step_avg:96.17ms
step:947/1750 train_time:91078ms step_avg:96.18ms
step:948/1750 train_time:91179ms step_avg:96.18ms
step:949/1750 train_time:91279ms step_avg:96.18ms
step:950/1750 train_time:91379ms step_avg:96.19ms
step:951/1750 train_time:91478ms step_avg:96.19ms
step:952/1750 train_time:91578ms step_avg:96.20ms
step:953/1750 train_time:91679ms step_avg:96.20ms
step:954/1750 train_time:91779ms step_avg:96.20ms
step:955/1750 train_time:91879ms step_avg:96.21ms
step:956/1750 train_time:91979ms step_avg:96.21ms
step:957/1750 train_time:92079ms step_avg:96.22ms
step:958/1750 train_time:92178ms step_avg:96.22ms
step:959/1750 train_time:92278ms step_avg:96.22ms
step:960/1750 train_time:92379ms step_avg:96.23ms
step:961/1750 train_time:92478ms step_avg:96.23ms
step:962/1750 train_time:92578ms step_avg:96.23ms
step:963/1750 train_time:92678ms step_avg:96.24ms
step:964/1750 train_time:92778ms step_avg:96.24ms
step:965/1750 train_time:92877ms step_avg:96.25ms
step:966/1750 train_time:92977ms step_avg:96.25ms
step:967/1750 train_time:93078ms step_avg:96.25ms
step:968/1750 train_time:93178ms step_avg:96.26ms
step:969/1750 train_time:93278ms step_avg:96.26ms
step:970/1750 train_time:93378ms step_avg:96.27ms
step:971/1750 train_time:93478ms step_avg:96.27ms
step:972/1750 train_time:93578ms step_avg:96.27ms
step:973/1750 train_time:93679ms step_avg:96.28ms
step:974/1750 train_time:93779ms step_avg:96.28ms
step:975/1750 train_time:93878ms step_avg:96.29ms
step:976/1750 train_time:93978ms step_avg:96.29ms
step:977/1750 train_time:94078ms step_avg:96.29ms
step:978/1750 train_time:94179ms step_avg:96.30ms
step:979/1750 train_time:94279ms step_avg:96.30ms
step:980/1750 train_time:94380ms step_avg:96.31ms
step:981/1750 train_time:94480ms step_avg:96.31ms
step:982/1750 train_time:94580ms step_avg:96.31ms
step:983/1750 train_time:94680ms step_avg:96.32ms
step:984/1750 train_time:94780ms step_avg:96.32ms
step:985/1750 train_time:94880ms step_avg:96.32ms
step:986/1750 train_time:94980ms step_avg:96.33ms
step:987/1750 train_time:95079ms step_avg:96.33ms
step:988/1750 train_time:95179ms step_avg:96.34ms
step:989/1750 train_time:95280ms step_avg:96.34ms
step:990/1750 train_time:95381ms step_avg:96.34ms
step:991/1750 train_time:95481ms step_avg:96.35ms
step:992/1750 train_time:95581ms step_avg:96.35ms
step:993/1750 train_time:95682ms step_avg:96.36ms
step:994/1750 train_time:95782ms step_avg:96.36ms
step:995/1750 train_time:95883ms step_avg:96.36ms
step:996/1750 train_time:95984ms step_avg:96.37ms
step:997/1750 train_time:96086ms step_avg:96.37ms
step:998/1750 train_time:96187ms step_avg:96.38ms
step:999/1750 train_time:96287ms step_avg:96.38ms
step:1000/1750 train_time:96388ms step_avg:96.39ms
step:1000/1750 val_loss:3.5067 train_time:96483ms step_avg:96.48ms
step:1001/1750 train_time:96510ms step_avg:96.41ms
step:1002/1750 train_time:96600ms step_avg:96.41ms
step:1003/1750 train_time:96700ms step_avg:96.41ms
step:1004/1750 train_time:96801ms step_avg:96.42ms
step:1005/1750 train_time:96900ms step_avg:96.42ms
step:1006/1750 train_time:97000ms step_avg:96.42ms
step:1007/1750 train_time:97099ms step_avg:96.42ms
step:1008/1750 train_time:97198ms step_avg:96.43ms
step:1009/1750 train_time:97298ms step_avg:96.43ms
step:1010/1750 train_time:97397ms step_avg:96.43ms
step:1011/1750 train_time:97498ms step_avg:96.44ms
step:1012/1750 train_time:97598ms step_avg:96.44ms
step:1013/1750 train_time:97699ms step_avg:96.45ms
step:1014/1750 train_time:97799ms step_avg:96.45ms
step:1015/1750 train_time:97899ms step_avg:96.45ms
step:1016/1750 train_time:97999ms step_avg:96.46ms
step:1017/1750 train_time:98099ms step_avg:96.46ms
step:1018/1750 train_time:98198ms step_avg:96.46ms
step:1019/1750 train_time:98297ms step_avg:96.46ms
step:1020/1750 train_time:98398ms step_avg:96.47ms
step:1021/1750 train_time:98499ms step_avg:96.47ms
step:1022/1750 train_time:98599ms step_avg:96.48ms
step:1023/1750 train_time:98698ms step_avg:96.48ms
step:1024/1750 train_time:98800ms step_avg:96.48ms
step:1025/1750 train_time:98900ms step_avg:96.49ms
step:1026/1750 train_time:99000ms step_avg:96.49ms
step:1027/1750 train_time:99099ms step_avg:96.49ms
step:1028/1750 train_time:99199ms step_avg:96.50ms
step:1029/1750 train_time:99299ms step_avg:96.50ms
step:1030/1750 train_time:99399ms step_avg:96.50ms
step:1031/1750 train_time:99499ms step_avg:96.51ms
step:1032/1750 train_time:99599ms step_avg:96.51ms
step:1033/1750 train_time:99698ms step_avg:96.51ms
step:1034/1750 train_time:99798ms step_avg:96.52ms
step:1035/1750 train_time:99897ms step_avg:96.52ms
step:1036/1750 train_time:99997ms step_avg:96.52ms
step:1037/1750 train_time:100098ms step_avg:96.53ms
step:1038/1750 train_time:100198ms step_avg:96.53ms
step:1039/1750 train_time:100298ms step_avg:96.53ms
step:1040/1750 train_time:100398ms step_avg:96.54ms
step:1041/1750 train_time:100498ms step_avg:96.54ms
step:1042/1750 train_time:100599ms step_avg:96.54ms
step:1043/1750 train_time:100699ms step_avg:96.55ms
step:1044/1750 train_time:100800ms step_avg:96.55ms
step:1045/1750 train_time:100900ms step_avg:96.55ms
step:1046/1750 train_time:101000ms step_avg:96.56ms
step:1047/1750 train_time:101100ms step_avg:96.56ms
step:1048/1750 train_time:101199ms step_avg:96.56ms
step:1049/1750 train_time:101299ms step_avg:96.57ms
step:1050/1750 train_time:101399ms step_avg:96.57ms
step:1051/1750 train_time:101499ms step_avg:96.57ms
step:1052/1750 train_time:101599ms step_avg:96.58ms
step:1053/1750 train_time:101699ms step_avg:96.58ms
step:1054/1750 train_time:101800ms step_avg:96.58ms
step:1055/1750 train_time:101902ms step_avg:96.59ms
step:1056/1750 train_time:102003ms step_avg:96.59ms
step:1057/1750 train_time:102104ms step_avg:96.60ms
step:1058/1750 train_time:102205ms step_avg:96.60ms
step:1059/1750 train_time:102306ms step_avg:96.61ms
step:1060/1750 train_time:102407ms step_avg:96.61ms
step:1061/1750 train_time:102507ms step_avg:96.61ms
step:1062/1750 train_time:102607ms step_avg:96.62ms
step:1063/1750 train_time:102708ms step_avg:96.62ms
step:1064/1750 train_time:102809ms step_avg:96.62ms
step:1065/1750 train_time:102909ms step_avg:96.63ms
step:1066/1750 train_time:103010ms step_avg:96.63ms
step:1067/1750 train_time:103111ms step_avg:96.64ms
step:1068/1750 train_time:103212ms step_avg:96.64ms
step:1069/1750 train_time:103313ms step_avg:96.64ms
step:1070/1750 train_time:103413ms step_avg:96.65ms
step:1071/1750 train_time:103514ms step_avg:96.65ms
step:1072/1750 train_time:103614ms step_avg:96.65ms
step:1073/1750 train_time:103714ms step_avg:96.66ms
step:1074/1750 train_time:103814ms step_avg:96.66ms
step:1075/1750 train_time:103915ms step_avg:96.67ms
step:1076/1750 train_time:104015ms step_avg:96.67ms
step:1077/1750 train_time:104116ms step_avg:96.67ms
step:1078/1750 train_time:104217ms step_avg:96.68ms
step:1079/1750 train_time:104318ms step_avg:96.68ms
step:1080/1750 train_time:104418ms step_avg:96.68ms
step:1081/1750 train_time:104518ms step_avg:96.69ms
step:1082/1750 train_time:104618ms step_avg:96.69ms
step:1083/1750 train_time:104718ms step_avg:96.69ms
step:1084/1750 train_time:104818ms step_avg:96.70ms
step:1085/1750 train_time:104919ms step_avg:96.70ms
step:1086/1750 train_time:105019ms step_avg:96.70ms
step:1087/1750 train_time:105119ms step_avg:96.71ms
step:1088/1750 train_time:105219ms step_avg:96.71ms
step:1089/1750 train_time:105319ms step_avg:96.71ms
step:1090/1750 train_time:105419ms step_avg:96.72ms
step:1091/1750 train_time:105520ms step_avg:96.72ms
step:1092/1750 train_time:105619ms step_avg:96.72ms
step:1093/1750 train_time:105720ms step_avg:96.72ms
step:1094/1750 train_time:105821ms step_avg:96.73ms
step:1095/1750 train_time:105921ms step_avg:96.73ms
step:1096/1750 train_time:106021ms step_avg:96.73ms
step:1097/1750 train_time:106123ms step_avg:96.74ms
step:1098/1750 train_time:106224ms step_avg:96.74ms
step:1099/1750 train_time:106325ms step_avg:96.75ms
step:1100/1750 train_time:106425ms step_avg:96.75ms
step:1101/1750 train_time:106526ms step_avg:96.75ms
step:1102/1750 train_time:106626ms step_avg:96.76ms
step:1103/1750 train_time:106727ms step_avg:96.76ms
step:1104/1750 train_time:106828ms step_avg:96.76ms
step:1105/1750 train_time:106929ms step_avg:96.77ms
step:1106/1750 train_time:107030ms step_avg:96.77ms
step:1107/1750 train_time:107131ms step_avg:96.78ms
step:1108/1750 train_time:107231ms step_avg:96.78ms
step:1109/1750 train_time:107332ms step_avg:96.78ms
step:1110/1750 train_time:107432ms step_avg:96.79ms
step:1111/1750 train_time:107532ms step_avg:96.79ms
step:1112/1750 train_time:107633ms step_avg:96.79ms
step:1113/1750 train_time:107734ms step_avg:96.80ms
step:1114/1750 train_time:107834ms step_avg:96.80ms
step:1115/1750 train_time:107935ms step_avg:96.80ms
step:1116/1750 train_time:108036ms step_avg:96.81ms
step:1117/1750 train_time:108136ms step_avg:96.81ms
step:1118/1750 train_time:108237ms step_avg:96.81ms
step:1119/1750 train_time:108338ms step_avg:96.82ms
step:1120/1750 train_time:108438ms step_avg:96.82ms
step:1121/1750 train_time:108539ms step_avg:96.82ms
step:1122/1750 train_time:108639ms step_avg:96.83ms
step:1123/1750 train_time:108739ms step_avg:96.83ms
step:1124/1750 train_time:108839ms step_avg:96.83ms
step:1125/1750 train_time:108939ms step_avg:96.83ms
step:1125/1750 val_loss:3.4555 train_time:109034ms step_avg:96.92ms
step:1126/1750 train_time:109060ms step_avg:96.86ms
step:1127/1750 train_time:109152ms step_avg:96.85ms
step:1128/1750 train_time:109253ms step_avg:96.86ms
step:1129/1750 train_time:109354ms step_avg:96.86ms
step:1130/1750 train_time:109454ms step_avg:96.86ms
step:1131/1750 train_time:109554ms step_avg:96.86ms
step:1132/1750 train_time:109654ms step_avg:96.87ms
step:1133/1750 train_time:109754ms step_avg:96.87ms
step:1134/1750 train_time:109854ms step_avg:96.87ms
step:1135/1750 train_time:109953ms step_avg:96.87ms
step:1136/1750 train_time:110055ms step_avg:96.88ms
step:1137/1750 train_time:110158ms step_avg:96.88ms
step:1138/1750 train_time:110258ms step_avg:96.89ms
step:1139/1750 train_time:110359ms step_avg:96.89ms
step:1140/1750 train_time:110460ms step_avg:96.90ms
step:1141/1750 train_time:110561ms step_avg:96.90ms
step:1142/1750 train_time:110661ms step_avg:96.90ms
step:1143/1750 train_time:110762ms step_avg:96.90ms
step:1144/1750 train_time:110862ms step_avg:96.91ms
step:1145/1750 train_time:110963ms step_avg:96.91ms
step:1146/1750 train_time:111065ms step_avg:96.92ms
step:1147/1750 train_time:111166ms step_avg:96.92ms
step:1148/1750 train_time:111267ms step_avg:96.92ms
step:1149/1750 train_time:111370ms step_avg:96.93ms
step:1150/1750 train_time:111470ms step_avg:96.93ms
step:1151/1750 train_time:111572ms step_avg:96.93ms
step:1152/1750 train_time:111672ms step_avg:96.94ms
step:1153/1750 train_time:111772ms step_avg:96.94ms
step:1154/1750 train_time:111873ms step_avg:96.94ms
step:1155/1750 train_time:111973ms step_avg:96.95ms
step:1156/1750 train_time:112074ms step_avg:96.95ms
step:1157/1750 train_time:112175ms step_avg:96.95ms
step:1158/1750 train_time:112275ms step_avg:96.96ms
step:1159/1750 train_time:112376ms step_avg:96.96ms
step:1160/1750 train_time:112476ms step_avg:96.96ms
step:1161/1750 train_time:112576ms step_avg:96.96ms
step:1162/1750 train_time:112677ms step_avg:96.97ms
step:1163/1750 train_time:112778ms step_avg:96.97ms
step:1164/1750 train_time:112878ms step_avg:96.97ms
step:1165/1750 train_time:112979ms step_avg:96.98ms
step:1166/1750 train_time:113079ms step_avg:96.98ms
step:1167/1750 train_time:113179ms step_avg:96.98ms
step:1168/1750 train_time:113280ms step_avg:96.99ms
step:1169/1750 train_time:113383ms step_avg:96.99ms
step:1170/1750 train_time:113485ms step_avg:97.00ms
step:1171/1750 train_time:113587ms step_avg:97.00ms
step:1172/1750 train_time:113690ms step_avg:97.01ms
step:1173/1750 train_time:113792ms step_avg:97.01ms
step:1174/1750 train_time:113893ms step_avg:97.01ms
step:1175/1750 train_time:113995ms step_avg:97.02ms
step:1176/1750 train_time:114096ms step_avg:97.02ms
step:1177/1750 train_time:114198ms step_avg:97.02ms
step:1178/1750 train_time:114300ms step_avg:97.03ms
step:1179/1750 train_time:114403ms step_avg:97.03ms
step:1180/1750 train_time:114505ms step_avg:97.04ms
step:1181/1750 train_time:114607ms step_avg:97.04ms
step:1182/1750 train_time:114711ms step_avg:97.05ms
step:1183/1750 train_time:114813ms step_avg:97.05ms
step:1184/1750 train_time:114915ms step_avg:97.06ms
step:1185/1750 train_time:115018ms step_avg:97.06ms
step:1186/1750 train_time:115119ms step_avg:97.06ms
step:1187/1750 train_time:115221ms step_avg:97.07ms
step:1188/1750 train_time:115323ms step_avg:97.07ms
step:1189/1750 train_time:115425ms step_avg:97.08ms
step:1190/1750 train_time:115526ms step_avg:97.08ms
step:1191/1750 train_time:115627ms step_avg:97.08ms
step:1192/1750 train_time:115730ms step_avg:97.09ms
step:1193/1750 train_time:115833ms step_avg:97.09ms
step:1194/1750 train_time:115934ms step_avg:97.10ms
step:1195/1750 train_time:116035ms step_avg:97.10ms
step:1196/1750 train_time:116137ms step_avg:97.10ms
step:1197/1750 train_time:116239ms step_avg:97.11ms
step:1198/1750 train_time:116340ms step_avg:97.11ms
step:1199/1750 train_time:116442ms step_avg:97.12ms
step:1200/1750 train_time:116544ms step_avg:97.12ms
step:1201/1750 train_time:116646ms step_avg:97.12ms
step:1202/1750 train_time:116749ms step_avg:97.13ms
step:1203/1750 train_time:116852ms step_avg:97.13ms
step:1204/1750 train_time:116955ms step_avg:97.14ms
step:1205/1750 train_time:117055ms step_avg:97.14ms
step:1206/1750 train_time:117156ms step_avg:97.14ms
step:1207/1750 train_time:117257ms step_avg:97.15ms
step:1208/1750 train_time:117359ms step_avg:97.15ms
step:1209/1750 train_time:117460ms step_avg:97.15ms
step:1210/1750 train_time:117563ms step_avg:97.16ms
step:1211/1750 train_time:117665ms step_avg:97.16ms
step:1212/1750 train_time:117767ms step_avg:97.17ms
step:1213/1750 train_time:117871ms step_avg:97.17ms
step:1214/1750 train_time:117973ms step_avg:97.18ms
step:1215/1750 train_time:118075ms step_avg:97.18ms
step:1216/1750 train_time:118178ms step_avg:97.19ms
step:1217/1750 train_time:118279ms step_avg:97.19ms
step:1218/1750 train_time:118381ms step_avg:97.19ms
step:1219/1750 train_time:118483ms step_avg:97.20ms
step:1220/1750 train_time:118585ms step_avg:97.20ms
step:1221/1750 train_time:118687ms step_avg:97.20ms
step:1222/1750 train_time:118789ms step_avg:97.21ms
step:1223/1750 train_time:118893ms step_avg:97.21ms
step:1224/1750 train_time:118994ms step_avg:97.22ms
step:1225/1750 train_time:119096ms step_avg:97.22ms
step:1226/1750 train_time:119197ms step_avg:97.22ms
step:1227/1750 train_time:119299ms step_avg:97.23ms
step:1228/1750 train_time:119400ms step_avg:97.23ms
step:1229/1750 train_time:119501ms step_avg:97.23ms
step:1230/1750 train_time:119603ms step_avg:97.24ms
step:1231/1750 train_time:119705ms step_avg:97.24ms
step:1232/1750 train_time:119808ms step_avg:97.25ms
step:1233/1750 train_time:119911ms step_avg:97.25ms
step:1234/1750 train_time:120014ms step_avg:97.26ms
step:1235/1750 train_time:120115ms step_avg:97.26ms
step:1236/1750 train_time:120218ms step_avg:97.26ms
step:1237/1750 train_time:120319ms step_avg:97.27ms
step:1238/1750 train_time:120421ms step_avg:97.27ms
step:1239/1750 train_time:120523ms step_avg:97.27ms
step:1240/1750 train_time:120625ms step_avg:97.28ms
step:1241/1750 train_time:120727ms step_avg:97.28ms
step:1242/1750 train_time:120830ms step_avg:97.29ms
step:1243/1750 train_time:120933ms step_avg:97.29ms
step:1244/1750 train_time:121035ms step_avg:97.29ms
step:1245/1750 train_time:121137ms step_avg:97.30ms
step:1246/1750 train_time:121240ms step_avg:97.30ms
step:1247/1750 train_time:121341ms step_avg:97.31ms
step:1248/1750 train_time:121443ms step_avg:97.31ms
step:1249/1750 train_time:121545ms step_avg:97.31ms
step:1250/1750 train_time:121647ms step_avg:97.32ms
step:1250/1750 val_loss:3.4078 train_time:121744ms step_avg:97.40ms
step:1251/1750 train_time:121772ms step_avg:97.34ms
step:1252/1750 train_time:121862ms step_avg:97.33ms
step:1253/1750 train_time:121964ms step_avg:97.34ms
step:1254/1750 train_time:122065ms step_avg:97.34ms
step:1255/1750 train_time:122167ms step_avg:97.34ms
step:1256/1750 train_time:122269ms step_avg:97.35ms
step:1257/1750 train_time:122371ms step_avg:97.35ms
step:1258/1750 train_time:122473ms step_avg:97.36ms
step:1259/1750 train_time:122573ms step_avg:97.36ms
step:1260/1750 train_time:122675ms step_avg:97.36ms
step:1261/1750 train_time:122778ms step_avg:97.37ms
step:1262/1750 train_time:122880ms step_avg:97.37ms
step:1263/1750 train_time:122982ms step_avg:97.37ms
step:1264/1750 train_time:123085ms step_avg:97.38ms
step:1265/1750 train_time:123187ms step_avg:97.38ms
step:1266/1750 train_time:123288ms step_avg:97.38ms
step:1267/1750 train_time:123391ms step_avg:97.39ms
step:1268/1750 train_time:123493ms step_avg:97.39ms
step:1269/1750 train_time:123594ms step_avg:97.40ms
step:1270/1750 train_time:123696ms step_avg:97.40ms
step:1271/1750 train_time:123799ms step_avg:97.40ms
step:1272/1750 train_time:123900ms step_avg:97.41ms
step:1273/1750 train_time:124002ms step_avg:97.41ms
step:1274/1750 train_time:124104ms step_avg:97.41ms
step:1275/1750 train_time:124206ms step_avg:97.42ms
step:1276/1750 train_time:124308ms step_avg:97.42ms
step:1277/1750 train_time:124411ms step_avg:97.42ms
step:1278/1750 train_time:124513ms step_avg:97.43ms
step:1279/1750 train_time:124615ms step_avg:97.43ms
step:1280/1750 train_time:124716ms step_avg:97.43ms
step:1281/1750 train_time:124819ms step_avg:97.44ms
step:1282/1750 train_time:124921ms step_avg:97.44ms
step:1283/1750 train_time:125023ms step_avg:97.45ms
step:1284/1750 train_time:125125ms step_avg:97.45ms
step:1285/1750 train_time:125226ms step_avg:97.45ms
step:1286/1750 train_time:125328ms step_avg:97.46ms
step:1287/1750 train_time:125430ms step_avg:97.46ms
step:1288/1750 train_time:125533ms step_avg:97.46ms
step:1289/1750 train_time:125635ms step_avg:97.47ms
step:1290/1750 train_time:125736ms step_avg:97.47ms
step:1291/1750 train_time:125838ms step_avg:97.47ms
step:1292/1750 train_time:125941ms step_avg:97.48ms
step:1293/1750 train_time:126043ms step_avg:97.48ms
step:1294/1750 train_time:126146ms step_avg:97.49ms
step:1295/1750 train_time:126248ms step_avg:97.49ms
step:1296/1750 train_time:126349ms step_avg:97.49ms
step:1297/1750 train_time:126451ms step_avg:97.49ms
step:1298/1750 train_time:126553ms step_avg:97.50ms
step:1299/1750 train_time:126654ms step_avg:97.50ms
step:1300/1750 train_time:126757ms step_avg:97.51ms
step:1301/1750 train_time:126859ms step_avg:97.51ms
step:1302/1750 train_time:126960ms step_avg:97.51ms
step:1303/1750 train_time:127062ms step_avg:97.52ms
step:1304/1750 train_time:127164ms step_avg:97.52ms
step:1305/1750 train_time:127267ms step_avg:97.52ms
step:1306/1750 train_time:127369ms step_avg:97.53ms
step:1307/1750 train_time:127472ms step_avg:97.53ms
step:1308/1750 train_time:127574ms step_avg:97.53ms
step:1309/1750 train_time:127675ms step_avg:97.54ms
step:1310/1750 train_time:127777ms step_avg:97.54ms
step:1311/1750 train_time:127879ms step_avg:97.54ms
step:1312/1750 train_time:127981ms step_avg:97.55ms
step:1313/1750 train_time:128085ms step_avg:97.55ms
step:1314/1750 train_time:128187ms step_avg:97.56ms
step:1315/1750 train_time:128290ms step_avg:97.56ms
step:1316/1750 train_time:128392ms step_avg:97.56ms
step:1317/1750 train_time:128494ms step_avg:97.57ms
step:1318/1750 train_time:128595ms step_avg:97.57ms
step:1319/1750 train_time:128697ms step_avg:97.57ms
step:1320/1750 train_time:128799ms step_avg:97.58ms
step:1321/1750 train_time:128902ms step_avg:97.58ms
step:1322/1750 train_time:129003ms step_avg:97.58ms
step:1323/1750 train_time:129104ms step_avg:97.58ms
step:1324/1750 train_time:129208ms step_avg:97.59ms
step:1325/1750 train_time:129311ms step_avg:97.59ms
step:1326/1750 train_time:129413ms step_avg:97.60ms
step:1327/1750 train_time:129515ms step_avg:97.60ms
step:1328/1750 train_time:129616ms step_avg:97.60ms
step:1329/1750 train_time:129717ms step_avg:97.61ms
step:1330/1750 train_time:129820ms step_avg:97.61ms
step:1331/1750 train_time:129922ms step_avg:97.61ms
step:1332/1750 train_time:130024ms step_avg:97.62ms
step:1333/1750 train_time:130126ms step_avg:97.62ms
step:1334/1750 train_time:130229ms step_avg:97.62ms
step:1335/1750 train_time:130331ms step_avg:97.63ms
step:1336/1750 train_time:130435ms step_avg:97.63ms
step:1337/1750 train_time:130537ms step_avg:97.63ms
step:1338/1750 train_time:130638ms step_avg:97.64ms
step:1339/1750 train_time:130740ms step_avg:97.64ms
step:1340/1750 train_time:130841ms step_avg:97.64ms
step:1341/1750 train_time:130943ms step_avg:97.65ms
step:1342/1750 train_time:131046ms step_avg:97.65ms
step:1343/1750 train_time:131148ms step_avg:97.65ms
step:1344/1750 train_time:131250ms step_avg:97.66ms
step:1345/1750 train_time:131352ms step_avg:97.66ms
step:1346/1750 train_time:131455ms step_avg:97.66ms
step:1347/1750 train_time:131557ms step_avg:97.67ms
step:1348/1750 train_time:131659ms step_avg:97.67ms
step:1349/1750 train_time:131761ms step_avg:97.67ms
step:1350/1750 train_time:131863ms step_avg:97.68ms
step:1351/1750 train_time:131965ms step_avg:97.68ms
step:1352/1750 train_time:132067ms step_avg:97.68ms
step:1353/1750 train_time:132168ms step_avg:97.69ms
step:1354/1750 train_time:132271ms step_avg:97.69ms
step:1355/1750 train_time:132373ms step_avg:97.69ms
step:1356/1750 train_time:132476ms step_avg:97.70ms
step:1357/1750 train_time:132578ms step_avg:97.70ms
step:1358/1750 train_time:132680ms step_avg:97.70ms
step:1359/1750 train_time:132782ms step_avg:97.71ms
step:1360/1750 train_time:132883ms step_avg:97.71ms
step:1361/1750 train_time:132985ms step_avg:97.71ms
step:1362/1750 train_time:133087ms step_avg:97.71ms
step:1363/1750 train_time:133191ms step_avg:97.72ms
step:1364/1750 train_time:133294ms step_avg:97.72ms
step:1365/1750 train_time:133397ms step_avg:97.73ms
step:1366/1750 train_time:133499ms step_avg:97.73ms
step:1367/1750 train_time:133601ms step_avg:97.73ms
step:1368/1750 train_time:133704ms step_avg:97.74ms
step:1369/1750 train_time:133806ms step_avg:97.74ms
step:1370/1750 train_time:133907ms step_avg:97.74ms
step:1371/1750 train_time:134008ms step_avg:97.75ms
step:1372/1750 train_time:134109ms step_avg:97.75ms
step:1373/1750 train_time:134212ms step_avg:97.75ms
step:1374/1750 train_time:134315ms step_avg:97.75ms
step:1375/1750 train_time:134417ms step_avg:97.76ms
step:1375/1750 val_loss:3.3672 train_time:134513ms step_avg:97.83ms
step:1376/1750 train_time:134540ms step_avg:97.78ms
step:1377/1750 train_time:134628ms step_avg:97.77ms
step:1378/1750 train_time:134731ms step_avg:97.77ms
step:1379/1750 train_time:134833ms step_avg:97.78ms
step:1380/1750 train_time:134935ms step_avg:97.78ms
step:1381/1750 train_time:135037ms step_avg:97.78ms
step:1382/1750 train_time:135138ms step_avg:97.78ms
step:1383/1750 train_time:135238ms step_avg:97.79ms
step:1384/1750 train_time:135339ms step_avg:97.79ms
step:1385/1750 train_time:135442ms step_avg:97.79ms
step:1386/1750 train_time:135545ms step_avg:97.80ms
step:1387/1750 train_time:135648ms step_avg:97.80ms
step:1388/1750 train_time:135752ms step_avg:97.80ms
step:1389/1750 train_time:135855ms step_avg:97.81ms
step:1390/1750 train_time:135956ms step_avg:97.81ms
step:1391/1750 train_time:136058ms step_avg:97.81ms
step:1392/1750 train_time:136160ms step_avg:97.82ms
step:1393/1750 train_time:136261ms step_avg:97.82ms
step:1394/1750 train_time:136362ms step_avg:97.82ms
step:1395/1750 train_time:136466ms step_avg:97.82ms
step:1396/1750 train_time:136569ms step_avg:97.83ms
step:1397/1750 train_time:136671ms step_avg:97.83ms
step:1398/1750 train_time:136773ms step_avg:97.83ms
step:1399/1750 train_time:136874ms step_avg:97.84ms
step:1400/1750 train_time:136976ms step_avg:97.84ms
step:1401/1750 train_time:137079ms step_avg:97.84ms
step:1402/1750 train_time:137180ms step_avg:97.85ms
step:1403/1750 train_time:137282ms step_avg:97.85ms
step:1404/1750 train_time:137384ms step_avg:97.85ms
step:1405/1750 train_time:137486ms step_avg:97.86ms
step:1406/1750 train_time:137590ms step_avg:97.86ms
step:1407/1750 train_time:137693ms step_avg:97.86ms
step:1408/1750 train_time:137794ms step_avg:97.87ms
step:1409/1750 train_time:137898ms step_avg:97.87ms
step:1410/1750 train_time:138000ms step_avg:97.87ms
step:1411/1750 train_time:138102ms step_avg:97.88ms
step:1412/1750 train_time:138204ms step_avg:97.88ms
step:1413/1750 train_time:138307ms step_avg:97.88ms
step:1414/1750 train_time:138409ms step_avg:97.88ms
step:1415/1750 train_time:138512ms step_avg:97.89ms
step:1416/1750 train_time:138613ms step_avg:97.89ms
step:1417/1750 train_time:138715ms step_avg:97.89ms
step:1418/1750 train_time:138818ms step_avg:97.90ms
step:1419/1750 train_time:138921ms step_avg:97.90ms
step:1420/1750 train_time:139023ms step_avg:97.90ms
step:1421/1750 train_time:139125ms step_avg:97.91ms
step:1422/1750 train_time:139226ms step_avg:97.91ms
step:1423/1750 train_time:139329ms step_avg:97.91ms
step:1424/1750 train_time:139431ms step_avg:97.92ms
step:1425/1750 train_time:139533ms step_avg:97.92ms
step:1426/1750 train_time:139636ms step_avg:97.92ms
step:1427/1750 train_time:139738ms step_avg:97.92ms
step:1428/1750 train_time:139842ms step_avg:97.93ms
step:1429/1750 train_time:139946ms step_avg:97.93ms
step:1430/1750 train_time:140049ms step_avg:97.94ms
step:1431/1750 train_time:140152ms step_avg:97.94ms
step:1432/1750 train_time:140254ms step_avg:97.94ms
step:1433/1750 train_time:140357ms step_avg:97.95ms
step:1434/1750 train_time:140460ms step_avg:97.95ms
step:1435/1750 train_time:140565ms step_avg:97.95ms
step:1436/1750 train_time:140672ms step_avg:97.96ms
step:1437/1750 train_time:140775ms step_avg:97.96ms
step:1438/1750 train_time:140877ms step_avg:97.97ms
step:1439/1750 train_time:140980ms step_avg:97.97ms
step:1440/1750 train_time:141085ms step_avg:97.98ms
step:1441/1750 train_time:141190ms step_avg:97.98ms
step:1442/1750 train_time:141292ms step_avg:97.98ms
step:1443/1750 train_time:141394ms step_avg:97.99ms
step:1444/1750 train_time:141497ms step_avg:97.99ms
step:1445/1750 train_time:141602ms step_avg:97.99ms
step:1446/1750 train_time:141703ms step_avg:98.00ms
step:1447/1750 train_time:141806ms step_avg:98.00ms
step:1448/1750 train_time:141910ms step_avg:98.00ms
step:1449/1750 train_time:142012ms step_avg:98.01ms
step:1450/1750 train_time:142115ms step_avg:98.01ms
step:1451/1750 train_time:142218ms step_avg:98.01ms
step:1452/1750 train_time:142321ms step_avg:98.02ms
step:1453/1750 train_time:142424ms step_avg:98.02ms
step:1454/1750 train_time:142529ms step_avg:98.03ms
step:1455/1750 train_time:142632ms step_avg:98.03ms
step:1456/1750 train_time:142734ms step_avg:98.03ms
step:1457/1750 train_time:142839ms step_avg:98.04ms
step:1458/1750 train_time:142942ms step_avg:98.04ms
step:1459/1750 train_time:143046ms step_avg:98.04ms
step:1460/1750 train_time:143149ms step_avg:98.05ms
step:1461/1750 train_time:143253ms step_avg:98.05ms
step:1462/1750 train_time:143355ms step_avg:98.05ms
step:1463/1750 train_time:143459ms step_avg:98.06ms
step:1464/1750 train_time:143563ms step_avg:98.06ms
step:1465/1750 train_time:143666ms step_avg:98.07ms
step:1466/1750 train_time:143771ms step_avg:98.07ms
step:1467/1750 train_time:143873ms step_avg:98.07ms
step:1468/1750 train_time:143977ms step_avg:98.08ms
step:1469/1750 train_time:144081ms step_avg:98.08ms
step:1470/1750 train_time:144186ms step_avg:98.09ms
step:1471/1750 train_time:144290ms step_avg:98.09ms
step:1472/1750 train_time:144392ms step_avg:98.09ms
step:1473/1750 train_time:144494ms step_avg:98.10ms
step:1474/1750 train_time:144600ms step_avg:98.10ms
step:1475/1750 train_time:144702ms step_avg:98.10ms
step:1476/1750 train_time:144806ms step_avg:98.11ms
step:1477/1750 train_time:144908ms step_avg:98.11ms
step:1478/1750 train_time:145012ms step_avg:98.11ms
step:1479/1750 train_time:145114ms step_avg:98.12ms
step:1480/1750 train_time:145217ms step_avg:98.12ms
step:1481/1750 train_time:145321ms step_avg:98.12ms
step:1482/1750 train_time:145425ms step_avg:98.13ms
step:1483/1750 train_time:145528ms step_avg:98.13ms
step:1484/1750 train_time:145632ms step_avg:98.14ms
step:1485/1750 train_time:145736ms step_avg:98.14ms
step:1486/1750 train_time:145839ms step_avg:98.14ms
step:1487/1750 train_time:145942ms step_avg:98.14ms
step:1488/1750 train_time:146047ms step_avg:98.15ms
step:1489/1750 train_time:146151ms step_avg:98.15ms
step:1490/1750 train_time:146255ms step_avg:98.16ms
step:1491/1750 train_time:146358ms step_avg:98.16ms
step:1492/1750 train_time:146462ms step_avg:98.16ms
step:1493/1750 train_time:146565ms step_avg:98.17ms
step:1494/1750 train_time:146669ms step_avg:98.17ms
step:1495/1750 train_time:146771ms step_avg:98.17ms
step:1496/1750 train_time:146874ms step_avg:98.18ms
step:1497/1750 train_time:146975ms step_avg:98.18ms
step:1498/1750 train_time:147078ms step_avg:98.18ms
step:1499/1750 train_time:147182ms step_avg:98.19ms
step:1500/1750 train_time:147287ms step_avg:98.19ms
step:1500/1750 val_loss:3.3303 train_time:147384ms step_avg:98.26ms
step:1501/1750 train_time:147412ms step_avg:98.21ms
step:1502/1750 train_time:147503ms step_avg:98.20ms
step:1503/1750 train_time:147606ms step_avg:98.21ms
step:1504/1750 train_time:147709ms step_avg:98.21ms
step:1505/1750 train_time:147811ms step_avg:98.21ms
step:1506/1750 train_time:147913ms step_avg:98.22ms
step:1507/1750 train_time:148016ms step_avg:98.22ms
step:1508/1750 train_time:148118ms step_avg:98.22ms
step:1509/1750 train_time:148220ms step_avg:98.22ms
step:1510/1750 train_time:148322ms step_avg:98.23ms
step:1511/1750 train_time:148427ms step_avg:98.23ms
step:1512/1750 train_time:148530ms step_avg:98.23ms
step:1513/1750 train_time:148634ms step_avg:98.24ms
step:1514/1750 train_time:148739ms step_avg:98.24ms
step:1515/1750 train_time:148847ms step_avg:98.25ms
step:1516/1750 train_time:148949ms step_avg:98.25ms
step:1517/1750 train_time:149051ms step_avg:98.25ms
step:1518/1750 train_time:149154ms step_avg:98.26ms
step:1519/1750 train_time:149260ms step_avg:98.26ms
step:1520/1750 train_time:149363ms step_avg:98.27ms
step:1521/1750 train_time:149466ms step_avg:98.27ms
step:1522/1750 train_time:149568ms step_avg:98.27ms
step:1523/1750 train_time:149671ms step_avg:98.27ms
step:1524/1750 train_time:149775ms step_avg:98.28ms
step:1525/1750 train_time:149878ms step_avg:98.28ms
step:1526/1750 train_time:149981ms step_avg:98.28ms
step:1527/1750 train_time:150085ms step_avg:98.29ms
step:1528/1750 train_time:150192ms step_avg:98.29ms
step:1529/1750 train_time:150294ms step_avg:98.30ms
step:1530/1750 train_time:150397ms step_avg:98.30ms
step:1531/1750 train_time:150499ms step_avg:98.30ms
step:1532/1750 train_time:150603ms step_avg:98.30ms
step:1533/1750 train_time:150706ms step_avg:98.31ms
step:1534/1750 train_time:150809ms step_avg:98.31ms
step:1535/1750 train_time:150911ms step_avg:98.31ms
step:1536/1750 train_time:151014ms step_avg:98.32ms
step:1537/1750 train_time:151118ms step_avg:98.32ms
step:1538/1750 train_time:151221ms step_avg:98.32ms
step:1539/1750 train_time:151324ms step_avg:98.33ms
step:1540/1750 train_time:151427ms step_avg:98.33ms
step:1541/1750 train_time:151531ms step_avg:98.33ms
step:1542/1750 train_time:151634ms step_avg:98.34ms
step:1543/1750 train_time:151738ms step_avg:98.34ms
step:1544/1750 train_time:151843ms step_avg:98.34ms
step:1545/1750 train_time:151946ms step_avg:98.35ms
step:1546/1750 train_time:152048ms step_avg:98.35ms
step:1547/1750 train_time:152151ms step_avg:98.35ms
step:1548/1750 train_time:152256ms step_avg:98.36ms
step:1549/1750 train_time:152360ms step_avg:98.36ms
step:1550/1750 train_time:152462ms step_avg:98.36ms
step:1551/1750 train_time:152567ms step_avg:98.37ms
step:1552/1750 train_time:152669ms step_avg:98.37ms
step:1553/1750 train_time:152772ms step_avg:98.37ms
step:1554/1750 train_time:152875ms step_avg:98.37ms
step:1555/1750 train_time:152977ms step_avg:98.38ms
step:1556/1750 train_time:153082ms step_avg:98.38ms
step:1557/1750 train_time:153186ms step_avg:98.39ms
step:1558/1750 train_time:153289ms step_avg:98.39ms
step:1559/1750 train_time:153392ms step_avg:98.39ms
step:1560/1750 train_time:153496ms step_avg:98.39ms
step:1561/1750 train_time:153599ms step_avg:98.40ms
step:1562/1750 train_time:153703ms step_avg:98.40ms
step:1563/1750 train_time:153809ms step_avg:98.41ms
step:1564/1750 train_time:153911ms step_avg:98.41ms
step:1565/1750 train_time:154014ms step_avg:98.41ms
step:1566/1750 train_time:154117ms step_avg:98.41ms
step:1567/1750 train_time:154219ms step_avg:98.42ms
step:1568/1750 train_time:154323ms step_avg:98.42ms
step:1569/1750 train_time:154426ms step_avg:98.42ms
step:1570/1750 train_time:154531ms step_avg:98.43ms
step:1571/1750 train_time:154633ms step_avg:98.43ms
step:1572/1750 train_time:154737ms step_avg:98.43ms
step:1573/1750 train_time:154842ms step_avg:98.44ms
step:1574/1750 train_time:154944ms step_avg:98.44ms
step:1575/1750 train_time:155046ms step_avg:98.44ms
step:1576/1750 train_time:155150ms step_avg:98.45ms
step:1577/1750 train_time:155254ms step_avg:98.45ms
step:1578/1750 train_time:155357ms step_avg:98.45ms
step:1579/1750 train_time:155460ms step_avg:98.45ms
step:1580/1750 train_time:155565ms step_avg:98.46ms
step:1581/1750 train_time:155668ms step_avg:98.46ms
step:1582/1750 train_time:155770ms step_avg:98.46ms
step:1583/1750 train_time:155875ms step_avg:98.47ms
step:1584/1750 train_time:155980ms step_avg:98.47ms
step:1585/1750 train_time:156083ms step_avg:98.48ms
step:1586/1750 train_time:156187ms step_avg:98.48ms
step:1587/1750 train_time:156290ms step_avg:98.48ms
step:1588/1750 train_time:156392ms step_avg:98.48ms
step:1589/1750 train_time:156495ms step_avg:98.49ms
step:1590/1750 train_time:156599ms step_avg:98.49ms
step:1591/1750 train_time:156703ms step_avg:98.49ms
step:1592/1750 train_time:156806ms step_avg:98.50ms
step:1593/1750 train_time:156909ms step_avg:98.50ms
step:1594/1750 train_time:157014ms step_avg:98.50ms
step:1595/1750 train_time:157116ms step_avg:98.51ms
step:1596/1750 train_time:157219ms step_avg:98.51ms
step:1597/1750 train_time:157323ms step_avg:98.51ms
step:1598/1750 train_time:157428ms step_avg:98.52ms
step:1599/1750 train_time:157530ms step_avg:98.52ms
step:1600/1750 train_time:157635ms step_avg:98.52ms
step:1601/1750 train_time:157739ms step_avg:98.53ms
step:1602/1750 train_time:157842ms step_avg:98.53ms
step:1603/1750 train_time:157946ms step_avg:98.53ms
step:1604/1750 train_time:158048ms step_avg:98.53ms
step:1605/1750 train_time:158151ms step_avg:98.54ms
step:1606/1750 train_time:158255ms step_avg:98.54ms
step:1607/1750 train_time:158358ms step_avg:98.54ms
step:1608/1750 train_time:158462ms step_avg:98.55ms
step:1609/1750 train_time:158566ms step_avg:98.55ms
step:1610/1750 train_time:158669ms step_avg:98.55ms
step:1611/1750 train_time:158773ms step_avg:98.56ms
step:1612/1750 train_time:158878ms step_avg:98.56ms
step:1613/1750 train_time:158981ms step_avg:98.56ms
step:1614/1750 train_time:159083ms step_avg:98.56ms
step:1615/1750 train_time:159186ms step_avg:98.57ms
step:1616/1750 train_time:159288ms step_avg:98.57ms
step:1617/1750 train_time:159391ms step_avg:98.57ms
step:1618/1750 train_time:159495ms step_avg:98.58ms
step:1619/1750 train_time:159599ms step_avg:98.58ms
step:1620/1750 train_time:159703ms step_avg:98.58ms
step:1621/1750 train_time:159806ms step_avg:98.58ms
step:1622/1750 train_time:159909ms step_avg:98.59ms
step:1623/1750 train_time:160011ms step_avg:98.59ms
step:1624/1750 train_time:160115ms step_avg:98.59ms
step:1625/1750 train_time:160219ms step_avg:98.60ms
step:1625/1750 val_loss:3.3004 train_time:160316ms step_avg:98.66ms
step:1626/1750 train_time:160343ms step_avg:98.61ms
step:1627/1750 train_time:160435ms step_avg:98.61ms
step:1628/1750 train_time:160539ms step_avg:98.61ms
step:1629/1750 train_time:160642ms step_avg:98.61ms
step:1630/1750 train_time:160745ms step_avg:98.62ms
step:1631/1750 train_time:160847ms step_avg:98.62ms
step:1632/1750 train_time:160950ms step_avg:98.62ms
step:1633/1750 train_time:161053ms step_avg:98.62ms
step:1634/1750 train_time:161158ms step_avg:98.63ms
step:1635/1750 train_time:161261ms step_avg:98.63ms
step:1636/1750 train_time:161365ms step_avg:98.63ms
step:1637/1750 train_time:161469ms step_avg:98.64ms
step:1638/1750 train_time:161572ms step_avg:98.64ms
step:1639/1750 train_time:161677ms step_avg:98.64ms
step:1640/1750 train_time:161780ms step_avg:98.65ms
step:1641/1750 train_time:161883ms step_avg:98.65ms
step:1642/1750 train_time:161985ms step_avg:98.65ms
step:1643/1750 train_time:162087ms step_avg:98.65ms
step:1644/1750 train_time:162191ms step_avg:98.66ms
step:1645/1750 train_time:162293ms step_avg:98.66ms
step:1646/1750 train_time:162397ms step_avg:98.66ms
step:1647/1750 train_time:162501ms step_avg:98.66ms
step:1648/1750 train_time:162605ms step_avg:98.67ms
step:1649/1750 train_time:162708ms step_avg:98.67ms
step:1650/1750 train_time:162811ms step_avg:98.67ms
step:1651/1750 train_time:162914ms step_avg:98.68ms
step:1652/1750 train_time:163018ms step_avg:98.68ms
step:1653/1750 train_time:163121ms step_avg:98.68ms
step:1654/1750 train_time:163223ms step_avg:98.68ms
step:1655/1750 train_time:163328ms step_avg:98.69ms
step:1656/1750 train_time:163432ms step_avg:98.69ms
step:1657/1750 train_time:163534ms step_avg:98.69ms
step:1658/1750 train_time:163639ms step_avg:98.70ms
step:1659/1750 train_time:163746ms step_avg:98.70ms
step:1660/1750 train_time:163850ms step_avg:98.70ms
step:1661/1750 train_time:163955ms step_avg:98.71ms
step:1662/1750 train_time:164059ms step_avg:98.71ms
step:1663/1750 train_time:164163ms step_avg:98.72ms
step:1664/1750 train_time:164265ms step_avg:98.72ms
step:1665/1750 train_time:164369ms step_avg:98.72ms
step:1666/1750 train_time:164473ms step_avg:98.72ms
step:1667/1750 train_time:164576ms step_avg:98.73ms
step:1668/1750 train_time:164680ms step_avg:98.73ms
step:1669/1750 train_time:164785ms step_avg:98.73ms
step:1670/1750 train_time:164888ms step_avg:98.74ms
step:1671/1750 train_time:164991ms step_avg:98.74ms
step:1672/1750 train_time:165095ms step_avg:98.74ms
step:1673/1750 train_time:165198ms step_avg:98.74ms
step:1674/1750 train_time:165301ms step_avg:98.75ms
step:1675/1750 train_time:165404ms step_avg:98.75ms
step:1676/1750 train_time:165508ms step_avg:98.75ms
step:1677/1750 train_time:165611ms step_avg:98.75ms
step:1678/1750 train_time:165716ms step_avg:98.76ms
step:1679/1750 train_time:165820ms step_avg:98.76ms
step:1680/1750 train_time:165922ms step_avg:98.76ms
step:1681/1750 train_time:166025ms step_avg:98.77ms
step:1682/1750 train_time:166130ms step_avg:98.77ms
step:1683/1750 train_time:166233ms step_avg:98.77ms
step:1684/1750 train_time:166337ms step_avg:98.78ms
step:1685/1750 train_time:166440ms step_avg:98.78ms
step:1686/1750 train_time:166543ms step_avg:98.78ms
step:1687/1750 train_time:166645ms step_avg:98.78ms
step:1688/1750 train_time:166750ms step_avg:98.79ms
step:1689/1750 train_time:166855ms step_avg:98.79ms
step:1690/1750 train_time:166959ms step_avg:98.79ms
step:1691/1750 train_time:167063ms step_avg:98.80ms
step:1692/1750 train_time:167166ms step_avg:98.80ms
step:1693/1750 train_time:167270ms step_avg:98.80ms
step:1694/1750 train_time:167374ms step_avg:98.80ms
step:1695/1750 train_time:167481ms step_avg:98.81ms
step:1696/1750 train_time:167584ms step_avg:98.81ms
step:1697/1750 train_time:167691ms step_avg:98.82ms
step:1698/1750 train_time:167794ms step_avg:98.82ms
step:1699/1750 train_time:167898ms step_avg:98.82ms
step:1700/1750 train_time:168002ms step_avg:98.82ms
step:1701/1750 train_time:168105ms step_avg:98.83ms
step:1702/1750 train_time:168211ms step_avg:98.83ms
step:1703/1750 train_time:168315ms step_avg:98.83ms
step:1704/1750 train_time:168418ms step_avg:98.84ms
step:1705/1750 train_time:168522ms step_avg:98.84ms
step:1706/1750 train_time:168627ms step_avg:98.84ms
step:1707/1750 train_time:168732ms step_avg:98.85ms
step:1708/1750 train_time:168837ms step_avg:98.85ms
step:1709/1750 train_time:168940ms step_avg:98.85ms
step:1710/1750 train_time:169045ms step_avg:98.86ms
step:1711/1750 train_time:169150ms step_avg:98.86ms
step:1712/1750 train_time:169253ms step_avg:98.86ms
step:1713/1750 train_time:169358ms step_avg:98.87ms
step:1714/1750 train_time:169461ms step_avg:98.87ms
step:1715/1750 train_time:169567ms step_avg:98.87ms
step:1716/1750 train_time:169670ms step_avg:98.88ms
step:1717/1750 train_time:169775ms step_avg:98.88ms
step:1718/1750 train_time:169878ms step_avg:98.88ms
step:1719/1750 train_time:169985ms step_avg:98.89ms
step:1720/1750 train_time:170088ms step_avg:98.89ms
step:1721/1750 train_time:170192ms step_avg:98.89ms
step:1722/1750 train_time:170297ms step_avg:98.89ms
step:1723/1750 train_time:170400ms step_avg:98.90ms
step:1724/1750 train_time:170506ms step_avg:98.90ms
step:1725/1750 train_time:170611ms step_avg:98.90ms
step:1726/1750 train_time:170714ms step_avg:98.91ms
step:1727/1750 train_time:170819ms step_avg:98.91ms
step:1728/1750 train_time:170925ms step_avg:98.91ms
step:1729/1750 train_time:171029ms step_avg:98.92ms
step:1730/1750 train_time:171131ms step_avg:98.92ms
step:1731/1750 train_time:171236ms step_avg:98.92ms
step:1732/1750 train_time:171341ms step_avg:98.93ms
step:1733/1750 train_time:171445ms step_avg:98.93ms
step:1734/1750 train_time:171550ms step_avg:98.93ms
step:1735/1750 train_time:171654ms step_avg:98.94ms
step:1736/1750 train_time:171757ms step_avg:98.94ms
step:1737/1750 train_time:171863ms step_avg:98.94ms
step:1738/1750 train_time:171966ms step_avg:98.94ms
step:1739/1750 train_time:172071ms step_avg:98.95ms
step:1740/1750 train_time:172176ms step_avg:98.95ms
step:1741/1750 train_time:172286ms step_avg:98.96ms
step:1742/1750 train_time:172390ms step_avg:98.96ms
step:1743/1750 train_time:172495ms step_avg:98.96ms
step:1744/1750 train_time:172599ms step_avg:98.97ms
step:1745/1750 train_time:172703ms step_avg:98.97ms
step:1746/1750 train_time:172806ms step_avg:98.97ms
step:1747/1750 train_time:172909ms step_avg:98.98ms
step:1748/1750 train_time:173014ms step_avg:98.98ms
step:1749/1750 train_time:173118ms step_avg:98.98ms
step:1750/1750 train_time:173223ms step_avg:98.98ms
step:1750/1750 val_loss:3.2792 train_time:173322ms step_avg:99.04ms
peak memory allocated: 33277 MiB reserved: 48852 MiB

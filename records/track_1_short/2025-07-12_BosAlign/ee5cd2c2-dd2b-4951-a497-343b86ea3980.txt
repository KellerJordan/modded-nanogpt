import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn, autocast
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
#import wandb

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)

        params = list(params)
        sizes = {p.shape for p in params}

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params,))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * self.world_size
            for base_i in range(0, len(params), self.world_size):
                if base_i + self.rank < len(params):
                    grad = params[base_i + self.rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + self.world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * self.world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), self.world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                futures.append(dist.all_gather(params_pad[base_i:base_i + self.world_size], params_pad[base_i + self.rank], async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01, rank: int = 0, world_size: int = 1):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        self.rank = rank
        self.world_size = world_size

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(
                params=group_params,
            ))
        super().__init__(param_groups, defaults)

    @torch.compile
    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // self.world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)

                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']

                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)

                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t

                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        for param in self.embed.parameters():
            param.lr_mul = 75.
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embeds in self.value_embeds:
            for param in self.value_embeds.parameters():
                param.lr_mul = 75.
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.lr_mul = 27.5
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % world_size
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            #return causal_mask
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, world_size: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos:pos+max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()

    starts = []
    batch_end=None
    for i in range(len(boundary_positions) - 1):
        end = boundary_positions[i + 1].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == world_size:
                batch_end = end
                break
            start = end
    assert batch_end is not None # increase max_batch_span if necessary
    batch_span = batch_end-pos
    return starts, batch_span

def distributed_data_generator(filename_pattern: str, batch_size: int, rank: int, world_size: int, align_to_bos: bool):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    batch_span = batch_size
    max_batch_span = 2*batch_size if align_to_bos else batch_size #provide buffer to handle samples up to length local_batch_size

    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, world_size, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    train_align_to_bos = True # align local batch start indicies with next bos_token
    val_align_to_bos = False # False to maintain same eval as prior records
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

#if master_process:
#    wandb.init(project="modded-nanogpt-tiny", name=f"run-{os.path.basename(__file__)}", save_code=True)

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=True):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
if master_process:
    print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=next_multiple_of_n(50257, n=128), num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094

optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0, rank=rank, world_size=world_size)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]

for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

for n, p in model.named_parameters():
    wd_mul = getattr(p, "wd_mul", 1.0)
    lr_mul = getattr(p, "lr_mul", 1.0)

    print0(f"{n}: {p.shape} {p.dtype} {wd_mul} {lr_mul}")

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
embedding_params = sum(p.numel() for n, p in model.named_parameters() if "embed" in n)
non_embedding_params = total_params - embedding_params

print0(f"")
print0(f"Model parameters:")
print0(f"  Total parameters: {total_params:,}")
print0(f"  Embedding parameters: {embedding_params:,}")
print0(f"  Non-embedding parameters: {non_embedding_params:,}")

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    w = min((1 - x) / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.05
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)

torch.cuda.synchronize()
dist.barrier()
with torch.profiler.profile() as prof:
    for _ in range(warmup_steps):
        inputs, targets = next(train_loader)
        model(inputs, targets, get_window_size_blocks(1)).backward()
        for opt in optimizers:
            opt.step()
    model.zero_grad(set_to_none=True)
    torch.cuda.synchronize()
    dist.barrier()
os.makedirs("traces", exist_ok=True)
prof.export_chrome_trace(f"traces/trace_{rank}.json")

model.load_state_dict(initial_state['model'])
for opt, opt_state in zip(optimizers, initial_state['optimizers']):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size, args.val_align_to_bos)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        #if master_process:
        #    wandb.log({"val/loss": val_loss}, step=step)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.8.0.dev20250524+cu126 compiled for CUDA 12.6
Sun Jul 13 01:41:34 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    5856MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   37C    P0            122W /  700W |    1517MiB /  81559MiB |      3%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   38C    P0            123W /  700W |    1517MiB /  81559MiB |      2%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   31C    P0            117W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   39C    P0            120W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   38C    P0            122W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   31C    P0            115W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           89679      C   /usr/bin/python3                       1508MiB |
|    0   N/A  N/A           89680      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           89681      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           89682      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           89683      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           89684      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           89685      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           89686      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A           89680      C   /usr/bin/python3                       1508MiB |
|    2   N/A  N/A           89681      C   /usr/bin/python3                       1508MiB |
|    3   N/A  N/A           89682      C   /usr/bin/python3                       1508MiB |
|    4   N/A  N/A           89683      C   /usr/bin/python3                       1508MiB |
|    5   N/A  N/A           89684      C   /usr/bin/python3                       1508MiB |
|    6   N/A  N/A           89685      C   /usr/bin/python3                       1508MiB |
|    7   N/A  N/A           89686      C   /usr/bin/python3                       1508MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
scalars: torch.Size([64]) torch.float32 1.0 5.0
embed.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.0.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.1.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.2.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
blocks.0.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.0.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.1.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.1.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.2.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.2.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.3.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.3.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.4.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.4.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.5.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.5.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.6.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.6.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.7.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.7.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.8.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.8.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.9.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.9.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.10.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.10.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.11.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.11.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
lm_head.weight: torch.Size([50304, 768]) torch.float32 1.0 27.5

Model parameters:
  Total parameters: 275,742,784
  Embedding parameters: 154,533,888
  Non-embedding parameters: 121,208,896
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.01ms
step:1/1750 train_time:150ms step_avg:149.82ms
step:2/1750 train_time:174ms step_avg:87.08ms
step:3/1750 train_time:248ms step_avg:82.82ms
step:4/1750 train_time:340ms step_avg:85.05ms
step:5/1750 train_time:433ms step_avg:86.56ms
step:6/1750 train_time:525ms step_avg:87.57ms
step:7/1750 train_time:618ms step_avg:88.23ms
step:8/1750 train_time:710ms step_avg:88.81ms
step:9/1750 train_time:803ms step_avg:89.21ms
step:10/1750 train_time:895ms step_avg:89.53ms
step:11/1750 train_time:988ms step_avg:89.80ms
step:12/1750 train_time:1082ms step_avg:90.16ms
step:13/1750 train_time:1177ms step_avg:90.58ms
step:14/1750 train_time:1272ms step_avg:90.84ms
step:15/1750 train_time:1365ms step_avg:91.03ms
step:16/1750 train_time:1458ms step_avg:91.15ms
step:17/1750 train_time:1551ms step_avg:91.25ms
step:18/1750 train_time:1645ms step_avg:91.39ms
step:19/1750 train_time:1737ms step_avg:91.45ms
step:20/1750 train_time:1829ms step_avg:91.47ms
step:21/1750 train_time:1922ms step_avg:91.53ms
step:22/1750 train_time:2015ms step_avg:91.59ms
step:23/1750 train_time:2109ms step_avg:91.69ms
step:24/1750 train_time:2203ms step_avg:91.79ms
step:25/1750 train_time:2297ms step_avg:91.87ms
step:26/1750 train_time:2390ms step_avg:91.92ms
step:27/1750 train_time:2483ms step_avg:91.95ms
step:28/1750 train_time:2576ms step_avg:92.00ms
step:29/1750 train_time:2669ms step_avg:92.04ms
step:30/1750 train_time:2761ms step_avg:92.05ms
step:31/1750 train_time:2854ms step_avg:92.05ms
step:32/1750 train_time:2947ms step_avg:92.08ms
step:33/1750 train_time:3040ms step_avg:92.11ms
step:34/1750 train_time:3133ms step_avg:92.15ms
step:35/1750 train_time:3227ms step_avg:92.19ms
step:36/1750 train_time:3322ms step_avg:92.28ms
step:37/1750 train_time:3414ms step_avg:92.27ms
step:38/1750 train_time:3508ms step_avg:92.32ms
step:39/1750 train_time:3601ms step_avg:92.34ms
step:40/1750 train_time:3694ms step_avg:92.35ms
step:41/1750 train_time:3787ms step_avg:92.38ms
step:42/1750 train_time:3881ms step_avg:92.40ms
step:43/1750 train_time:3974ms step_avg:92.42ms
step:44/1750 train_time:4067ms step_avg:92.44ms
step:45/1750 train_time:4160ms step_avg:92.45ms
step:46/1750 train_time:4253ms step_avg:92.46ms
step:47/1750 train_time:4347ms step_avg:92.49ms
step:48/1750 train_time:4441ms step_avg:92.53ms
step:49/1750 train_time:4534ms step_avg:92.54ms
step:50/1750 train_time:4628ms step_avg:92.55ms
step:51/1750 train_time:4721ms step_avg:92.58ms
step:52/1750 train_time:4813ms step_avg:92.56ms
step:53/1750 train_time:4907ms step_avg:92.58ms
step:54/1750 train_time:5000ms step_avg:92.59ms
step:55/1750 train_time:5093ms step_avg:92.60ms
step:56/1750 train_time:5186ms step_avg:92.61ms
step:57/1750 train_time:5280ms step_avg:92.63ms
step:58/1750 train_time:5373ms step_avg:92.64ms
step:59/1750 train_time:5468ms step_avg:92.67ms
step:60/1750 train_time:5561ms step_avg:92.69ms
step:61/1750 train_time:5654ms step_avg:92.69ms
step:62/1750 train_time:5746ms step_avg:92.69ms
step:63/1750 train_time:5840ms step_avg:92.69ms
step:64/1750 train_time:5932ms step_avg:92.69ms
step:65/1750 train_time:6026ms step_avg:92.70ms
step:66/1750 train_time:6119ms step_avg:92.71ms
step:67/1750 train_time:6213ms step_avg:92.72ms
step:68/1750 train_time:6306ms step_avg:92.74ms
step:69/1750 train_time:6400ms step_avg:92.75ms
step:70/1750 train_time:6493ms step_avg:92.76ms
step:71/1750 train_time:6587ms step_avg:92.77ms
step:72/1750 train_time:6681ms step_avg:92.79ms
step:73/1750 train_time:6774ms step_avg:92.80ms
step:74/1750 train_time:6867ms step_avg:92.80ms
step:75/1750 train_time:6960ms step_avg:92.80ms
step:76/1750 train_time:7053ms step_avg:92.80ms
step:77/1750 train_time:7146ms step_avg:92.80ms
step:78/1750 train_time:7239ms step_avg:92.81ms
step:79/1750 train_time:7332ms step_avg:92.81ms
step:80/1750 train_time:7425ms step_avg:92.82ms
step:81/1750 train_time:7519ms step_avg:92.83ms
step:82/1750 train_time:7613ms step_avg:92.84ms
step:83/1750 train_time:7706ms step_avg:92.85ms
step:84/1750 train_time:7800ms step_avg:92.86ms
step:85/1750 train_time:7893ms step_avg:92.86ms
step:86/1750 train_time:7986ms step_avg:92.86ms
step:87/1750 train_time:8079ms step_avg:92.86ms
step:88/1750 train_time:8172ms step_avg:92.87ms
step:89/1750 train_time:8265ms step_avg:92.87ms
step:90/1750 train_time:8358ms step_avg:92.87ms
step:91/1750 train_time:8452ms step_avg:92.88ms
step:92/1750 train_time:8546ms step_avg:92.89ms
step:93/1750 train_time:8638ms step_avg:92.89ms
step:94/1750 train_time:8732ms step_avg:92.89ms
step:95/1750 train_time:8825ms step_avg:92.90ms
step:96/1750 train_time:8918ms step_avg:92.90ms
step:97/1750 train_time:9012ms step_avg:92.90ms
step:98/1750 train_time:9105ms step_avg:92.91ms
step:99/1750 train_time:9198ms step_avg:92.91ms
step:100/1750 train_time:9292ms step_avg:92.92ms
step:101/1750 train_time:9385ms step_avg:92.92ms
step:102/1750 train_time:9479ms step_avg:92.93ms
step:103/1750 train_time:9572ms step_avg:92.93ms
step:104/1750 train_time:9665ms step_avg:92.93ms
step:105/1750 train_time:9758ms step_avg:92.93ms
step:106/1750 train_time:9851ms step_avg:92.93ms
step:107/1750 train_time:9944ms step_avg:92.93ms
step:108/1750 train_time:10036ms step_avg:92.93ms
step:109/1750 train_time:10129ms step_avg:92.93ms
step:110/1750 train_time:10223ms step_avg:92.93ms
step:111/1750 train_time:10316ms step_avg:92.94ms
step:112/1750 train_time:10410ms step_avg:92.95ms
step:113/1750 train_time:10503ms step_avg:92.95ms
step:114/1750 train_time:10596ms step_avg:92.95ms
step:115/1750 train_time:10689ms step_avg:92.95ms
step:116/1750 train_time:10782ms step_avg:92.95ms
step:117/1750 train_time:10875ms step_avg:92.95ms
step:118/1750 train_time:10968ms step_avg:92.95ms
step:119/1750 train_time:11061ms step_avg:92.95ms
step:120/1750 train_time:11154ms step_avg:92.95ms
step:121/1750 train_time:11247ms step_avg:92.95ms
step:122/1750 train_time:11341ms step_avg:92.96ms
step:123/1750 train_time:11434ms step_avg:92.96ms
step:124/1750 train_time:11528ms step_avg:92.97ms
step:125/1750 train_time:11622ms step_avg:92.97ms
step:125/1750 val_loss:4.6439 train_time:11709ms step_avg:93.67ms
step:126/1750 train_time:11736ms step_avg:93.14ms
step:127/1750 train_time:11816ms step_avg:93.04ms
step:128/1750 train_time:11917ms step_avg:93.10ms
step:129/1750 train_time:12011ms step_avg:93.11ms
step:130/1750 train_time:12105ms step_avg:93.11ms
step:131/1750 train_time:12197ms step_avg:93.11ms
step:132/1750 train_time:12290ms step_avg:93.11ms
step:133/1750 train_time:12383ms step_avg:93.11ms
step:134/1750 train_time:12476ms step_avg:93.11ms
step:135/1750 train_time:12570ms step_avg:93.11ms
step:136/1750 train_time:12663ms step_avg:93.11ms
step:137/1750 train_time:12757ms step_avg:93.12ms
step:138/1750 train_time:12853ms step_avg:93.13ms
step:139/1750 train_time:12949ms step_avg:93.16ms
step:140/1750 train_time:13045ms step_avg:93.18ms
step:141/1750 train_time:13139ms step_avg:93.18ms
step:142/1750 train_time:13233ms step_avg:93.19ms
step:143/1750 train_time:13326ms step_avg:93.19ms
step:144/1750 train_time:13420ms step_avg:93.20ms
step:145/1750 train_time:13514ms step_avg:93.20ms
step:146/1750 train_time:13607ms step_avg:93.20ms
step:147/1750 train_time:13700ms step_avg:93.20ms
step:148/1750 train_time:13793ms step_avg:93.20ms
step:149/1750 train_time:13888ms step_avg:93.20ms
step:150/1750 train_time:13982ms step_avg:93.21ms
step:151/1750 train_time:14076ms step_avg:93.22ms
step:152/1750 train_time:14170ms step_avg:93.22ms
step:153/1750 train_time:14264ms step_avg:93.23ms
step:154/1750 train_time:14358ms step_avg:93.23ms
step:155/1750 train_time:14452ms step_avg:93.24ms
step:156/1750 train_time:14546ms step_avg:93.24ms
step:157/1750 train_time:14638ms step_avg:93.24ms
step:158/1750 train_time:14732ms step_avg:93.24ms
step:159/1750 train_time:14826ms step_avg:93.25ms
step:160/1750 train_time:14920ms step_avg:93.25ms
step:161/1750 train_time:15014ms step_avg:93.25ms
step:162/1750 train_time:15108ms step_avg:93.26ms
step:163/1750 train_time:15202ms step_avg:93.26ms
step:164/1750 train_time:15296ms step_avg:93.27ms
step:165/1750 train_time:15390ms step_avg:93.27ms
step:166/1750 train_time:15484ms step_avg:93.28ms
step:167/1750 train_time:15579ms step_avg:93.28ms
step:168/1750 train_time:15672ms step_avg:93.29ms
step:169/1750 train_time:15767ms step_avg:93.30ms
step:170/1750 train_time:15860ms step_avg:93.29ms
step:171/1750 train_time:15954ms step_avg:93.30ms
step:172/1750 train_time:16049ms step_avg:93.31ms
step:173/1750 train_time:16142ms step_avg:93.31ms
step:174/1750 train_time:16235ms step_avg:93.31ms
step:175/1750 train_time:16329ms step_avg:93.31ms
step:176/1750 train_time:16423ms step_avg:93.31ms
step:177/1750 train_time:16517ms step_avg:93.32ms
step:178/1750 train_time:16611ms step_avg:93.32ms
step:179/1750 train_time:16705ms step_avg:93.32ms
step:180/1750 train_time:16798ms step_avg:93.32ms
step:181/1750 train_time:16893ms step_avg:93.33ms
step:182/1750 train_time:16987ms step_avg:93.33ms
step:183/1750 train_time:17080ms step_avg:93.33ms
step:184/1750 train_time:17174ms step_avg:93.34ms
step:185/1750 train_time:17268ms step_avg:93.34ms
step:186/1750 train_time:17361ms step_avg:93.34ms
step:187/1750 train_time:17455ms step_avg:93.34ms
step:188/1750 train_time:17549ms step_avg:93.35ms
step:189/1750 train_time:17642ms step_avg:93.35ms
step:190/1750 train_time:17737ms step_avg:93.35ms
step:191/1750 train_time:17830ms step_avg:93.35ms
step:192/1750 train_time:17924ms step_avg:93.36ms
step:193/1750 train_time:18018ms step_avg:93.36ms
step:194/1750 train_time:18112ms step_avg:93.36ms
step:195/1750 train_time:18206ms step_avg:93.36ms
step:196/1750 train_time:18299ms step_avg:93.36ms
step:197/1750 train_time:18393ms step_avg:93.36ms
step:198/1750 train_time:18487ms step_avg:93.37ms
step:199/1750 train_time:18580ms step_avg:93.37ms
step:200/1750 train_time:18674ms step_avg:93.37ms
step:201/1750 train_time:18767ms step_avg:93.37ms
step:202/1750 train_time:18861ms step_avg:93.37ms
step:203/1750 train_time:18954ms step_avg:93.37ms
step:204/1750 train_time:19048ms step_avg:93.37ms
step:205/1750 train_time:19142ms step_avg:93.38ms
step:206/1750 train_time:19236ms step_avg:93.38ms
step:207/1750 train_time:19330ms step_avg:93.38ms
step:208/1750 train_time:19424ms step_avg:93.38ms
step:209/1750 train_time:19518ms step_avg:93.39ms
step:210/1750 train_time:19612ms step_avg:93.39ms
step:211/1750 train_time:19706ms step_avg:93.39ms
step:212/1750 train_time:19799ms step_avg:93.39ms
step:213/1750 train_time:19894ms step_avg:93.40ms
step:214/1750 train_time:19987ms step_avg:93.40ms
step:215/1750 train_time:20080ms step_avg:93.40ms
step:216/1750 train_time:20174ms step_avg:93.40ms
step:217/1750 train_time:20268ms step_avg:93.40ms
step:218/1750 train_time:20361ms step_avg:93.40ms
step:219/1750 train_time:20455ms step_avg:93.40ms
step:220/1750 train_time:20548ms step_avg:93.40ms
step:221/1750 train_time:20642ms step_avg:93.40ms
step:222/1750 train_time:20737ms step_avg:93.41ms
step:223/1750 train_time:20831ms step_avg:93.41ms
step:224/1750 train_time:20925ms step_avg:93.42ms
step:225/1750 train_time:21019ms step_avg:93.42ms
step:226/1750 train_time:21112ms step_avg:93.42ms
step:227/1750 train_time:21206ms step_avg:93.42ms
step:228/1750 train_time:21300ms step_avg:93.42ms
step:229/1750 train_time:21394ms step_avg:93.42ms
step:230/1750 train_time:21487ms step_avg:93.42ms
step:231/1750 train_time:21582ms step_avg:93.43ms
step:232/1750 train_time:21675ms step_avg:93.43ms
step:233/1750 train_time:21769ms step_avg:93.43ms
step:234/1750 train_time:21864ms step_avg:93.43ms
step:235/1750 train_time:21957ms step_avg:93.44ms
step:236/1750 train_time:22052ms step_avg:93.44ms
step:237/1750 train_time:22146ms step_avg:93.44ms
step:238/1750 train_time:22239ms step_avg:93.44ms
step:239/1750 train_time:22333ms step_avg:93.44ms
step:240/1750 train_time:22427ms step_avg:93.44ms
step:241/1750 train_time:22520ms step_avg:93.45ms
step:242/1750 train_time:22614ms step_avg:93.44ms
step:243/1750 train_time:22707ms step_avg:93.45ms
step:244/1750 train_time:22802ms step_avg:93.45ms
step:245/1750 train_time:22896ms step_avg:93.45ms
step:246/1750 train_time:22989ms step_avg:93.45ms
step:247/1750 train_time:23083ms step_avg:93.45ms
step:248/1750 train_time:23177ms step_avg:93.45ms
step:249/1750 train_time:23271ms step_avg:93.46ms
step:250/1750 train_time:23364ms step_avg:93.46ms
step:250/1750 val_loss:4.0986 train_time:23453ms step_avg:93.81ms
step:251/1750 train_time:23479ms step_avg:93.54ms
step:252/1750 train_time:23561ms step_avg:93.49ms
step:253/1750 train_time:23656ms step_avg:93.50ms
step:254/1750 train_time:23751ms step_avg:93.51ms
step:255/1750 train_time:23844ms step_avg:93.51ms
step:256/1750 train_time:23937ms step_avg:93.50ms
step:257/1750 train_time:24030ms step_avg:93.50ms
step:258/1750 train_time:24123ms step_avg:93.50ms
step:259/1750 train_time:24216ms step_avg:93.50ms
step:260/1750 train_time:24309ms step_avg:93.49ms
step:261/1750 train_time:24401ms step_avg:93.49ms
step:262/1750 train_time:24496ms step_avg:93.50ms
step:263/1750 train_time:24592ms step_avg:93.51ms
step:264/1750 train_time:24687ms step_avg:93.51ms
step:265/1750 train_time:24781ms step_avg:93.51ms
step:266/1750 train_time:24876ms step_avg:93.52ms
step:267/1750 train_time:24969ms step_avg:93.52ms
step:268/1750 train_time:25063ms step_avg:93.52ms
step:269/1750 train_time:25157ms step_avg:93.52ms
step:270/1750 train_time:25251ms step_avg:93.52ms
step:271/1750 train_time:25345ms step_avg:93.52ms
step:272/1750 train_time:25439ms step_avg:93.53ms
step:273/1750 train_time:25534ms step_avg:93.53ms
step:274/1750 train_time:25628ms step_avg:93.53ms
step:275/1750 train_time:25723ms step_avg:93.54ms
step:276/1750 train_time:25818ms step_avg:93.54ms
step:277/1750 train_time:25911ms step_avg:93.54ms
step:278/1750 train_time:26005ms step_avg:93.54ms
step:279/1750 train_time:26099ms step_avg:93.55ms
step:280/1750 train_time:26193ms step_avg:93.55ms
step:281/1750 train_time:26287ms step_avg:93.55ms
step:282/1750 train_time:26380ms step_avg:93.55ms
step:283/1750 train_time:26475ms step_avg:93.55ms
step:284/1750 train_time:26570ms step_avg:93.55ms
step:285/1750 train_time:26664ms step_avg:93.56ms
step:286/1750 train_time:26758ms step_avg:93.56ms
step:287/1750 train_time:26853ms step_avg:93.56ms
step:288/1750 train_time:26946ms step_avg:93.56ms
step:289/1750 train_time:27041ms step_avg:93.57ms
step:290/1750 train_time:27134ms step_avg:93.57ms
step:291/1750 train_time:27229ms step_avg:93.57ms
step:292/1750 train_time:27324ms step_avg:93.57ms
step:293/1750 train_time:27418ms step_avg:93.58ms
step:294/1750 train_time:27512ms step_avg:93.58ms
step:295/1750 train_time:27606ms step_avg:93.58ms
step:296/1750 train_time:27701ms step_avg:93.58ms
step:297/1750 train_time:27795ms step_avg:93.59ms
step:298/1750 train_time:27890ms step_avg:93.59ms
step:299/1750 train_time:27984ms step_avg:93.59ms
step:300/1750 train_time:28078ms step_avg:93.59ms
step:301/1750 train_time:28172ms step_avg:93.59ms
step:302/1750 train_time:28266ms step_avg:93.60ms
step:303/1750 train_time:28360ms step_avg:93.60ms
step:304/1750 train_time:28455ms step_avg:93.60ms
step:305/1750 train_time:28548ms step_avg:93.60ms
step:306/1750 train_time:28642ms step_avg:93.60ms
step:307/1750 train_time:28737ms step_avg:93.61ms
step:308/1750 train_time:28831ms step_avg:93.61ms
step:309/1750 train_time:28925ms step_avg:93.61ms
step:310/1750 train_time:29019ms step_avg:93.61ms
step:311/1750 train_time:29114ms step_avg:93.61ms
step:312/1750 train_time:29208ms step_avg:93.61ms
step:313/1750 train_time:29302ms step_avg:93.62ms
step:314/1750 train_time:29396ms step_avg:93.62ms
step:315/1750 train_time:29490ms step_avg:93.62ms
step:316/1750 train_time:29585ms step_avg:93.62ms
step:317/1750 train_time:29679ms step_avg:93.62ms
step:318/1750 train_time:29773ms step_avg:93.63ms
step:319/1750 train_time:29867ms step_avg:93.63ms
step:320/1750 train_time:29961ms step_avg:93.63ms
step:321/1750 train_time:30055ms step_avg:93.63ms
step:322/1750 train_time:30149ms step_avg:93.63ms
step:323/1750 train_time:30243ms step_avg:93.63ms
step:324/1750 train_time:30337ms step_avg:93.63ms
step:325/1750 train_time:30432ms step_avg:93.64ms
step:326/1750 train_time:30526ms step_avg:93.64ms
step:327/1750 train_time:30621ms step_avg:93.64ms
step:328/1750 train_time:30715ms step_avg:93.64ms
step:329/1750 train_time:30809ms step_avg:93.64ms
step:330/1750 train_time:30903ms step_avg:93.65ms
step:331/1750 train_time:30997ms step_avg:93.65ms
step:332/1750 train_time:31092ms step_avg:93.65ms
step:333/1750 train_time:31186ms step_avg:93.65ms
step:334/1750 train_time:31280ms step_avg:93.65ms
step:335/1750 train_time:31375ms step_avg:93.66ms
step:336/1750 train_time:31469ms step_avg:93.66ms
step:337/1750 train_time:31564ms step_avg:93.66ms
step:338/1750 train_time:31658ms step_avg:93.66ms
step:339/1750 train_time:31752ms step_avg:93.66ms
step:340/1750 train_time:31846ms step_avg:93.66ms
step:341/1750 train_time:31941ms step_avg:93.67ms
step:342/1750 train_time:32035ms step_avg:93.67ms
step:343/1750 train_time:32129ms step_avg:93.67ms
step:344/1750 train_time:32223ms step_avg:93.67ms
step:345/1750 train_time:32317ms step_avg:93.67ms
step:346/1750 train_time:32412ms step_avg:93.68ms
step:347/1750 train_time:32505ms step_avg:93.68ms
step:348/1750 train_time:32599ms step_avg:93.68ms
step:349/1750 train_time:32693ms step_avg:93.68ms
step:350/1750 train_time:32787ms step_avg:93.68ms
step:351/1750 train_time:32882ms step_avg:93.68ms
step:352/1750 train_time:32976ms step_avg:93.68ms
step:353/1750 train_time:33071ms step_avg:93.68ms
step:354/1750 train_time:33165ms step_avg:93.69ms
step:355/1750 train_time:33260ms step_avg:93.69ms
step:356/1750 train_time:33354ms step_avg:93.69ms
step:357/1750 train_time:33448ms step_avg:93.69ms
step:358/1750 train_time:33542ms step_avg:93.69ms
step:359/1750 train_time:33636ms step_avg:93.69ms
step:360/1750 train_time:33731ms step_avg:93.70ms
step:361/1750 train_time:33825ms step_avg:93.70ms
step:362/1750 train_time:33919ms step_avg:93.70ms
step:363/1750 train_time:34013ms step_avg:93.70ms
step:364/1750 train_time:34108ms step_avg:93.70ms
step:365/1750 train_time:34203ms step_avg:93.71ms
step:366/1750 train_time:34297ms step_avg:93.71ms
step:367/1750 train_time:34391ms step_avg:93.71ms
step:368/1750 train_time:34486ms step_avg:93.71ms
step:369/1750 train_time:34580ms step_avg:93.71ms
step:370/1750 train_time:34674ms step_avg:93.71ms
step:371/1750 train_time:34768ms step_avg:93.71ms
step:372/1750 train_time:34862ms step_avg:93.71ms
step:373/1750 train_time:34957ms step_avg:93.72ms
step:374/1750 train_time:35051ms step_avg:93.72ms
step:375/1750 train_time:35145ms step_avg:93.72ms
step:375/1750 val_loss:3.8998 train_time:35234ms step_avg:93.96ms
step:376/1750 train_time:35261ms step_avg:93.78ms
step:377/1750 train_time:35344ms step_avg:93.75ms
step:378/1750 train_time:35441ms step_avg:93.76ms
step:379/1750 train_time:35536ms step_avg:93.76ms
step:380/1750 train_time:35630ms step_avg:93.76ms
step:381/1750 train_time:35724ms step_avg:93.76ms
step:382/1750 train_time:35818ms step_avg:93.77ms
step:383/1750 train_time:35912ms step_avg:93.76ms
step:384/1750 train_time:36005ms step_avg:93.76ms
step:385/1750 train_time:36098ms step_avg:93.76ms
step:386/1750 train_time:36191ms step_avg:93.76ms
step:387/1750 train_time:36286ms step_avg:93.76ms
step:388/1750 train_time:36382ms step_avg:93.77ms
step:389/1750 train_time:36477ms step_avg:93.77ms
step:390/1750 train_time:36571ms step_avg:93.77ms
step:391/1750 train_time:36668ms step_avg:93.78ms
step:392/1750 train_time:36764ms step_avg:93.79ms
step:393/1750 train_time:36861ms step_avg:93.79ms
step:394/1750 train_time:36957ms step_avg:93.80ms
step:395/1750 train_time:37052ms step_avg:93.80ms
step:396/1750 train_time:37148ms step_avg:93.81ms
step:397/1750 train_time:37244ms step_avg:93.81ms
step:398/1750 train_time:37340ms step_avg:93.82ms
step:399/1750 train_time:37438ms step_avg:93.83ms
step:400/1750 train_time:37535ms step_avg:93.84ms
step:401/1750 train_time:37632ms step_avg:93.85ms
step:402/1750 train_time:37728ms step_avg:93.85ms
step:403/1750 train_time:37824ms step_avg:93.86ms
step:404/1750 train_time:37921ms step_avg:93.86ms
step:405/1750 train_time:38017ms step_avg:93.87ms
step:406/1750 train_time:38113ms step_avg:93.87ms
step:407/1750 train_time:38209ms step_avg:93.88ms
step:408/1750 train_time:38305ms step_avg:93.88ms
step:409/1750 train_time:38401ms step_avg:93.89ms
step:410/1750 train_time:38497ms step_avg:93.90ms
step:411/1750 train_time:38594ms step_avg:93.90ms
step:412/1750 train_time:38689ms step_avg:93.91ms
step:413/1750 train_time:38786ms step_avg:93.91ms
step:414/1750 train_time:38882ms step_avg:93.92ms
step:415/1750 train_time:38978ms step_avg:93.92ms
step:416/1750 train_time:39075ms step_avg:93.93ms
step:417/1750 train_time:39171ms step_avg:93.93ms
step:418/1750 train_time:39267ms step_avg:93.94ms
step:419/1750 train_time:39364ms step_avg:93.95ms
step:420/1750 train_time:39460ms step_avg:93.95ms
step:421/1750 train_time:39557ms step_avg:93.96ms
step:422/1750 train_time:39653ms step_avg:93.96ms
step:423/1750 train_time:39749ms step_avg:93.97ms
step:424/1750 train_time:39846ms step_avg:93.98ms
step:425/1750 train_time:39942ms step_avg:93.98ms
step:426/1750 train_time:40039ms step_avg:93.99ms
step:427/1750 train_time:40136ms step_avg:93.99ms
step:428/1750 train_time:40232ms step_avg:94.00ms
step:429/1750 train_time:40327ms step_avg:94.00ms
step:430/1750 train_time:40424ms step_avg:94.01ms
step:431/1750 train_time:40521ms step_avg:94.02ms
step:432/1750 train_time:40617ms step_avg:94.02ms
step:433/1750 train_time:40713ms step_avg:94.03ms
step:434/1750 train_time:40809ms step_avg:94.03ms
step:435/1750 train_time:40905ms step_avg:94.04ms
step:436/1750 train_time:41002ms step_avg:94.04ms
step:437/1750 train_time:41099ms step_avg:94.05ms
step:438/1750 train_time:41195ms step_avg:94.05ms
step:439/1750 train_time:41292ms step_avg:94.06ms
step:440/1750 train_time:41387ms step_avg:94.06ms
step:441/1750 train_time:41484ms step_avg:94.07ms
step:442/1750 train_time:41581ms step_avg:94.08ms
step:443/1750 train_time:41677ms step_avg:94.08ms
step:444/1750 train_time:41773ms step_avg:94.08ms
step:445/1750 train_time:41869ms step_avg:94.09ms
step:446/1750 train_time:41965ms step_avg:94.09ms
step:447/1750 train_time:42061ms step_avg:94.10ms
step:448/1750 train_time:42157ms step_avg:94.10ms
step:449/1750 train_time:42253ms step_avg:94.10ms
step:450/1750 train_time:42349ms step_avg:94.11ms
step:451/1750 train_time:42445ms step_avg:94.11ms
step:452/1750 train_time:42542ms step_avg:94.12ms
step:453/1750 train_time:42638ms step_avg:94.12ms
step:454/1750 train_time:42735ms step_avg:94.13ms
step:455/1750 train_time:42831ms step_avg:94.13ms
step:456/1750 train_time:42927ms step_avg:94.14ms
step:457/1750 train_time:43024ms step_avg:94.14ms
step:458/1750 train_time:43119ms step_avg:94.15ms
step:459/1750 train_time:43215ms step_avg:94.15ms
step:460/1750 train_time:43312ms step_avg:94.16ms
step:461/1750 train_time:43407ms step_avg:94.16ms
step:462/1750 train_time:43504ms step_avg:94.17ms
step:463/1750 train_time:43601ms step_avg:94.17ms
step:464/1750 train_time:43698ms step_avg:94.18ms
step:465/1750 train_time:43794ms step_avg:94.18ms
step:466/1750 train_time:43890ms step_avg:94.19ms
step:467/1750 train_time:43988ms step_avg:94.19ms
step:468/1750 train_time:44083ms step_avg:94.19ms
step:469/1750 train_time:44180ms step_avg:94.20ms
step:470/1750 train_time:44276ms step_avg:94.20ms
step:471/1750 train_time:44373ms step_avg:94.21ms
step:472/1750 train_time:44469ms step_avg:94.21ms
step:473/1750 train_time:44566ms step_avg:94.22ms
step:474/1750 train_time:44662ms step_avg:94.22ms
step:475/1750 train_time:44758ms step_avg:94.23ms
step:476/1750 train_time:44854ms step_avg:94.23ms
step:477/1750 train_time:44951ms step_avg:94.24ms
step:478/1750 train_time:45048ms step_avg:94.24ms
step:479/1750 train_time:45144ms step_avg:94.25ms
step:480/1750 train_time:45241ms step_avg:94.25ms
step:481/1750 train_time:45337ms step_avg:94.26ms
step:482/1750 train_time:45433ms step_avg:94.26ms
step:483/1750 train_time:45529ms step_avg:94.26ms
step:484/1750 train_time:45625ms step_avg:94.27ms
step:485/1750 train_time:45721ms step_avg:94.27ms
step:486/1750 train_time:45818ms step_avg:94.27ms
step:487/1750 train_time:45913ms step_avg:94.28ms
step:488/1750 train_time:46009ms step_avg:94.28ms
step:489/1750 train_time:46106ms step_avg:94.29ms
step:490/1750 train_time:46203ms step_avg:94.29ms
step:491/1750 train_time:46300ms step_avg:94.30ms
step:492/1750 train_time:46396ms step_avg:94.30ms
step:493/1750 train_time:46492ms step_avg:94.30ms
step:494/1750 train_time:46588ms step_avg:94.31ms
step:495/1750 train_time:46684ms step_avg:94.31ms
step:496/1750 train_time:46781ms step_avg:94.32ms
step:497/1750 train_time:46877ms step_avg:94.32ms
step:498/1750 train_time:46973ms step_avg:94.32ms
step:499/1750 train_time:47069ms step_avg:94.33ms
step:500/1750 train_time:47166ms step_avg:94.33ms
step:500/1750 val_loss:3.7504 train_time:47258ms step_avg:94.52ms
step:501/1750 train_time:47284ms step_avg:94.38ms
step:502/1750 train_time:47369ms step_avg:94.36ms
step:503/1750 train_time:47469ms step_avg:94.37ms
step:504/1750 train_time:47566ms step_avg:94.38ms
step:505/1750 train_time:47663ms step_avg:94.38ms
step:506/1750 train_time:47759ms step_avg:94.38ms
step:507/1750 train_time:47854ms step_avg:94.39ms
step:508/1750 train_time:47950ms step_avg:94.39ms
step:509/1750 train_time:48046ms step_avg:94.39ms
step:510/1750 train_time:48142ms step_avg:94.40ms
step:511/1750 train_time:48237ms step_avg:94.40ms
step:512/1750 train_time:48334ms step_avg:94.40ms
step:513/1750 train_time:48431ms step_avg:94.41ms
step:514/1750 train_time:48529ms step_avg:94.41ms
step:515/1750 train_time:48626ms step_avg:94.42ms
step:516/1750 train_time:48722ms step_avg:94.42ms
step:517/1750 train_time:48818ms step_avg:94.42ms
step:518/1750 train_time:48914ms step_avg:94.43ms
step:519/1750 train_time:49010ms step_avg:94.43ms
step:520/1750 train_time:49107ms step_avg:94.44ms
step:521/1750 train_time:49204ms step_avg:94.44ms
step:522/1750 train_time:49301ms step_avg:94.45ms
step:523/1750 train_time:49399ms step_avg:94.45ms
step:524/1750 train_time:49496ms step_avg:94.46ms
step:525/1750 train_time:49593ms step_avg:94.46ms
step:526/1750 train_time:49691ms step_avg:94.47ms
step:527/1750 train_time:49788ms step_avg:94.47ms
step:528/1750 train_time:49885ms step_avg:94.48ms
step:529/1750 train_time:49982ms step_avg:94.48ms
step:530/1750 train_time:50078ms step_avg:94.49ms
step:531/1750 train_time:50175ms step_avg:94.49ms
step:532/1750 train_time:50272ms step_avg:94.50ms
step:533/1750 train_time:50369ms step_avg:94.50ms
step:534/1750 train_time:50466ms step_avg:94.51ms
step:535/1750 train_time:50563ms step_avg:94.51ms
step:536/1750 train_time:50661ms step_avg:94.52ms
step:537/1750 train_time:50758ms step_avg:94.52ms
step:538/1750 train_time:50854ms step_avg:94.52ms
step:539/1750 train_time:50951ms step_avg:94.53ms
step:540/1750 train_time:51047ms step_avg:94.53ms
step:541/1750 train_time:51145ms step_avg:94.54ms
step:542/1750 train_time:51241ms step_avg:94.54ms
step:543/1750 train_time:51338ms step_avg:94.55ms
step:544/1750 train_time:51435ms step_avg:94.55ms
step:545/1750 train_time:51531ms step_avg:94.55ms
step:546/1750 train_time:51628ms step_avg:94.56ms
step:547/1750 train_time:51725ms step_avg:94.56ms
step:548/1750 train_time:51822ms step_avg:94.57ms
step:549/1750 train_time:51919ms step_avg:94.57ms
step:550/1750 train_time:52015ms step_avg:94.57ms
step:551/1750 train_time:52112ms step_avg:94.58ms
step:552/1750 train_time:52210ms step_avg:94.58ms
step:553/1750 train_time:52307ms step_avg:94.59ms
step:554/1750 train_time:52404ms step_avg:94.59ms
step:555/1750 train_time:52501ms step_avg:94.60ms
step:556/1750 train_time:52598ms step_avg:94.60ms
step:557/1750 train_time:52694ms step_avg:94.60ms
step:558/1750 train_time:52791ms step_avg:94.61ms
step:559/1750 train_time:52889ms step_avg:94.61ms
step:560/1750 train_time:52986ms step_avg:94.62ms
step:561/1750 train_time:53083ms step_avg:94.62ms
step:562/1750 train_time:53179ms step_avg:94.63ms
step:563/1750 train_time:53276ms step_avg:94.63ms
step:564/1750 train_time:53372ms step_avg:94.63ms
step:565/1750 train_time:53469ms step_avg:94.64ms
step:566/1750 train_time:53566ms step_avg:94.64ms
step:567/1750 train_time:53663ms step_avg:94.64ms
step:568/1750 train_time:53759ms step_avg:94.65ms
step:569/1750 train_time:53856ms step_avg:94.65ms
step:570/1750 train_time:53953ms step_avg:94.65ms
step:571/1750 train_time:54049ms step_avg:94.66ms
step:572/1750 train_time:54147ms step_avg:94.66ms
step:573/1750 train_time:54244ms step_avg:94.67ms
step:574/1750 train_time:54340ms step_avg:94.67ms
step:575/1750 train_time:54437ms step_avg:94.67ms
step:576/1750 train_time:54534ms step_avg:94.68ms
step:577/1750 train_time:54631ms step_avg:94.68ms
step:578/1750 train_time:54728ms step_avg:94.68ms
step:579/1750 train_time:54825ms step_avg:94.69ms
step:580/1750 train_time:54922ms step_avg:94.69ms
step:581/1750 train_time:55019ms step_avg:94.70ms
step:582/1750 train_time:55116ms step_avg:94.70ms
step:583/1750 train_time:55212ms step_avg:94.70ms
step:584/1750 train_time:55309ms step_avg:94.71ms
step:585/1750 train_time:55406ms step_avg:94.71ms
step:586/1750 train_time:55504ms step_avg:94.72ms
step:587/1750 train_time:55601ms step_avg:94.72ms
step:588/1750 train_time:55698ms step_avg:94.72ms
step:589/1750 train_time:55794ms step_avg:94.73ms
step:590/1750 train_time:55892ms step_avg:94.73ms
step:591/1750 train_time:55990ms step_avg:94.74ms
step:592/1750 train_time:56087ms step_avg:94.74ms
step:593/1750 train_time:56183ms step_avg:94.74ms
step:594/1750 train_time:56280ms step_avg:94.75ms
step:595/1750 train_time:56376ms step_avg:94.75ms
step:596/1750 train_time:56473ms step_avg:94.75ms
step:597/1750 train_time:56570ms step_avg:94.76ms
step:598/1750 train_time:56667ms step_avg:94.76ms
step:599/1750 train_time:56764ms step_avg:94.76ms
step:600/1750 train_time:56861ms step_avg:94.77ms
step:601/1750 train_time:56958ms step_avg:94.77ms
step:602/1750 train_time:57054ms step_avg:94.77ms
step:603/1750 train_time:57151ms step_avg:94.78ms
step:604/1750 train_time:57248ms step_avg:94.78ms
step:605/1750 train_time:57345ms step_avg:94.78ms
step:606/1750 train_time:57441ms step_avg:94.79ms
step:607/1750 train_time:57538ms step_avg:94.79ms
step:608/1750 train_time:57635ms step_avg:94.79ms
step:609/1750 train_time:57732ms step_avg:94.80ms
step:610/1750 train_time:57829ms step_avg:94.80ms
step:611/1750 train_time:57926ms step_avg:94.80ms
step:612/1750 train_time:58023ms step_avg:94.81ms
step:613/1750 train_time:58119ms step_avg:94.81ms
step:614/1750 train_time:58215ms step_avg:94.81ms
step:615/1750 train_time:58312ms step_avg:94.82ms
step:616/1750 train_time:58408ms step_avg:94.82ms
step:617/1750 train_time:58506ms step_avg:94.82ms
step:618/1750 train_time:58603ms step_avg:94.83ms
step:619/1750 train_time:58699ms step_avg:94.83ms
step:620/1750 train_time:58796ms step_avg:94.83ms
step:621/1750 train_time:58893ms step_avg:94.84ms
step:622/1750 train_time:58990ms step_avg:94.84ms
step:623/1750 train_time:59086ms step_avg:94.84ms
step:624/1750 train_time:59183ms step_avg:94.84ms
step:625/1750 train_time:59279ms step_avg:94.85ms
step:625/1750 val_loss:3.6605 train_time:59370ms step_avg:94.99ms
step:626/1750 train_time:59396ms step_avg:94.88ms
step:627/1750 train_time:59481ms step_avg:94.87ms
step:628/1750 train_time:59581ms step_avg:94.87ms
step:629/1750 train_time:59678ms step_avg:94.88ms
step:630/1750 train_time:59774ms step_avg:94.88ms
step:631/1750 train_time:59870ms step_avg:94.88ms
step:632/1750 train_time:59966ms step_avg:94.88ms
step:633/1750 train_time:60062ms step_avg:94.89ms
step:634/1750 train_time:60158ms step_avg:94.89ms
step:635/1750 train_time:60254ms step_avg:94.89ms
step:636/1750 train_time:60350ms step_avg:94.89ms
step:637/1750 train_time:60448ms step_avg:94.90ms
step:638/1750 train_time:60547ms step_avg:94.90ms
step:639/1750 train_time:60646ms step_avg:94.91ms
step:640/1750 train_time:60742ms step_avg:94.91ms
step:641/1750 train_time:60839ms step_avg:94.91ms
step:642/1750 train_time:60935ms step_avg:94.91ms
step:643/1750 train_time:61032ms step_avg:94.92ms
step:644/1750 train_time:61128ms step_avg:94.92ms
step:645/1750 train_time:61225ms step_avg:94.92ms
step:646/1750 train_time:61320ms step_avg:94.92ms
step:647/1750 train_time:61417ms step_avg:94.93ms
step:648/1750 train_time:61514ms step_avg:94.93ms
step:649/1750 train_time:61613ms step_avg:94.93ms
step:650/1750 train_time:61710ms step_avg:94.94ms
step:651/1750 train_time:61809ms step_avg:94.95ms
step:652/1750 train_time:61909ms step_avg:94.95ms
step:653/1750 train_time:62006ms step_avg:94.96ms
step:654/1750 train_time:62104ms step_avg:94.96ms
step:655/1750 train_time:62203ms step_avg:94.97ms
step:656/1750 train_time:62300ms step_avg:94.97ms
step:657/1750 train_time:62398ms step_avg:94.97ms
step:658/1750 train_time:62497ms step_avg:94.98ms
step:659/1750 train_time:62595ms step_avg:94.99ms
step:660/1750 train_time:62695ms step_avg:94.99ms
step:661/1750 train_time:62793ms step_avg:95.00ms
step:662/1750 train_time:62892ms step_avg:95.00ms
step:663/1750 train_time:62991ms step_avg:95.01ms
step:664/1750 train_time:63090ms step_avg:95.01ms
step:665/1750 train_time:63188ms step_avg:95.02ms
step:666/1750 train_time:63287ms step_avg:95.03ms
step:667/1750 train_time:63385ms step_avg:95.03ms
step:668/1750 train_time:63485ms step_avg:95.04ms
step:669/1750 train_time:63585ms step_avg:95.04ms
step:670/1750 train_time:63684ms step_avg:95.05ms
step:671/1750 train_time:63783ms step_avg:95.06ms
step:672/1750 train_time:63882ms step_avg:95.06ms
step:673/1750 train_time:63981ms step_avg:95.07ms
step:674/1750 train_time:64080ms step_avg:95.07ms
step:675/1750 train_time:64177ms step_avg:95.08ms
step:676/1750 train_time:64277ms step_avg:95.08ms
step:677/1750 train_time:64374ms step_avg:95.09ms
step:678/1750 train_time:64473ms step_avg:95.09ms
step:679/1750 train_time:64572ms step_avg:95.10ms
step:680/1750 train_time:64671ms step_avg:95.10ms
step:681/1750 train_time:64770ms step_avg:95.11ms
step:682/1750 train_time:64870ms step_avg:95.12ms
step:683/1750 train_time:64969ms step_avg:95.12ms
step:684/1750 train_time:65069ms step_avg:95.13ms
step:685/1750 train_time:65167ms step_avg:95.14ms
step:686/1750 train_time:65266ms step_avg:95.14ms
step:687/1750 train_time:65365ms step_avg:95.14ms
step:688/1750 train_time:65462ms step_avg:95.15ms
step:689/1750 train_time:65561ms step_avg:95.15ms
step:690/1750 train_time:65659ms step_avg:95.16ms
step:691/1750 train_time:65757ms step_avg:95.16ms
step:692/1750 train_time:65855ms step_avg:95.17ms
step:693/1750 train_time:65953ms step_avg:95.17ms
step:694/1750 train_time:66052ms step_avg:95.18ms
step:695/1750 train_time:66151ms step_avg:95.18ms
step:696/1750 train_time:66251ms step_avg:95.19ms
step:697/1750 train_time:66349ms step_avg:95.19ms
step:698/1750 train_time:66448ms step_avg:95.20ms
step:699/1750 train_time:66546ms step_avg:95.20ms
step:700/1750 train_time:66644ms step_avg:95.21ms
step:701/1750 train_time:66742ms step_avg:95.21ms
step:702/1750 train_time:66841ms step_avg:95.21ms
step:703/1750 train_time:66939ms step_avg:95.22ms
step:704/1750 train_time:67037ms step_avg:95.22ms
step:705/1750 train_time:67135ms step_avg:95.23ms
step:706/1750 train_time:67233ms step_avg:95.23ms
step:707/1750 train_time:67332ms step_avg:95.24ms
step:708/1750 train_time:67430ms step_avg:95.24ms
step:709/1750 train_time:67529ms step_avg:95.25ms
step:710/1750 train_time:67627ms step_avg:95.25ms
step:711/1750 train_time:67726ms step_avg:95.25ms
step:712/1750 train_time:67825ms step_avg:95.26ms
step:713/1750 train_time:67923ms step_avg:95.26ms
step:714/1750 train_time:68022ms step_avg:95.27ms
step:715/1750 train_time:68119ms step_avg:95.27ms
step:716/1750 train_time:68217ms step_avg:95.28ms
step:717/1750 train_time:68315ms step_avg:95.28ms
step:718/1750 train_time:68414ms step_avg:95.28ms
step:719/1750 train_time:68512ms step_avg:95.29ms
step:720/1750 train_time:68611ms step_avg:95.29ms
step:721/1750 train_time:68710ms step_avg:95.30ms
step:722/1750 train_time:68810ms step_avg:95.30ms
step:723/1750 train_time:68909ms step_avg:95.31ms
step:724/1750 train_time:69008ms step_avg:95.31ms
step:725/1750 train_time:69107ms step_avg:95.32ms
step:726/1750 train_time:69205ms step_avg:95.32ms
step:727/1750 train_time:69304ms step_avg:95.33ms
step:728/1750 train_time:69402ms step_avg:95.33ms
step:729/1750 train_time:69500ms step_avg:95.34ms
step:730/1750 train_time:69597ms step_avg:95.34ms
step:731/1750 train_time:69695ms step_avg:95.34ms
step:732/1750 train_time:69795ms step_avg:95.35ms
step:733/1750 train_time:69893ms step_avg:95.35ms
step:734/1750 train_time:69993ms step_avg:95.36ms
step:735/1750 train_time:70092ms step_avg:95.36ms
step:736/1750 train_time:70191ms step_avg:95.37ms
step:737/1750 train_time:70290ms step_avg:95.37ms
step:738/1750 train_time:70388ms step_avg:95.38ms
step:739/1750 train_time:70487ms step_avg:95.38ms
step:740/1750 train_time:70584ms step_avg:95.38ms
step:741/1750 train_time:70682ms step_avg:95.39ms
step:742/1750 train_time:70780ms step_avg:95.39ms
step:743/1750 train_time:70879ms step_avg:95.40ms
step:744/1750 train_time:70977ms step_avg:95.40ms
step:745/1750 train_time:71076ms step_avg:95.40ms
step:746/1750 train_time:71175ms step_avg:95.41ms
step:747/1750 train_time:71274ms step_avg:95.41ms
step:748/1750 train_time:71373ms step_avg:95.42ms
step:749/1750 train_time:71472ms step_avg:95.42ms
step:750/1750 train_time:71571ms step_avg:95.43ms
step:750/1750 val_loss:3.5982 train_time:71664ms step_avg:95.55ms
step:751/1750 train_time:71690ms step_avg:95.46ms
step:752/1750 train_time:71778ms step_avg:95.45ms
step:753/1750 train_time:71878ms step_avg:95.46ms
step:754/1750 train_time:71977ms step_avg:95.46ms
step:755/1750 train_time:72075ms step_avg:95.46ms
step:756/1750 train_time:72174ms step_avg:95.47ms
step:757/1750 train_time:72271ms step_avg:95.47ms
step:758/1750 train_time:72369ms step_avg:95.47ms
step:759/1750 train_time:72467ms step_avg:95.48ms
step:760/1750 train_time:72565ms step_avg:95.48ms
step:761/1750 train_time:72664ms step_avg:95.48ms
step:762/1750 train_time:72764ms step_avg:95.49ms
step:763/1750 train_time:72863ms step_avg:95.50ms
step:764/1750 train_time:72962ms step_avg:95.50ms
step:765/1750 train_time:73061ms step_avg:95.50ms
step:766/1750 train_time:73159ms step_avg:95.51ms
step:767/1750 train_time:73257ms step_avg:95.51ms
step:768/1750 train_time:73355ms step_avg:95.51ms
step:769/1750 train_time:73453ms step_avg:95.52ms
step:770/1750 train_time:73550ms step_avg:95.52ms
step:771/1750 train_time:73648ms step_avg:95.52ms
step:772/1750 train_time:73747ms step_avg:95.53ms
step:773/1750 train_time:73845ms step_avg:95.53ms
step:774/1750 train_time:73944ms step_avg:95.53ms
step:775/1750 train_time:74043ms step_avg:95.54ms
step:776/1750 train_time:74142ms step_avg:95.54ms
step:777/1750 train_time:74240ms step_avg:95.55ms
step:778/1750 train_time:74339ms step_avg:95.55ms
step:779/1750 train_time:74438ms step_avg:95.56ms
step:780/1750 train_time:74536ms step_avg:95.56ms
step:781/1750 train_time:74635ms step_avg:95.56ms
step:782/1750 train_time:74736ms step_avg:95.57ms
step:783/1750 train_time:74836ms step_avg:95.58ms
step:784/1750 train_time:74936ms step_avg:95.58ms
step:785/1750 train_time:75036ms step_avg:95.59ms
step:786/1750 train_time:75136ms step_avg:95.59ms
step:787/1750 train_time:75236ms step_avg:95.60ms
step:788/1750 train_time:75336ms step_avg:95.60ms
step:789/1750 train_time:75434ms step_avg:95.61ms
step:790/1750 train_time:75532ms step_avg:95.61ms
step:791/1750 train_time:75631ms step_avg:95.61ms
step:792/1750 train_time:75729ms step_avg:95.62ms
step:793/1750 train_time:75828ms step_avg:95.62ms
step:794/1750 train_time:75926ms step_avg:95.62ms
step:795/1750 train_time:76026ms step_avg:95.63ms
step:796/1750 train_time:76125ms step_avg:95.63ms
step:797/1750 train_time:76224ms step_avg:95.64ms
step:798/1750 train_time:76324ms step_avg:95.64ms
step:799/1750 train_time:76423ms step_avg:95.65ms
step:800/1750 train_time:76522ms step_avg:95.65ms
step:801/1750 train_time:76622ms step_avg:95.66ms
step:802/1750 train_time:76721ms step_avg:95.66ms
step:803/1750 train_time:76820ms step_avg:95.67ms
step:804/1750 train_time:76918ms step_avg:95.67ms
step:805/1750 train_time:77017ms step_avg:95.67ms
step:806/1750 train_time:77117ms step_avg:95.68ms
step:807/1750 train_time:77217ms step_avg:95.68ms
step:808/1750 train_time:77316ms step_avg:95.69ms
step:809/1750 train_time:77415ms step_avg:95.69ms
step:810/1750 train_time:77514ms step_avg:95.70ms
step:811/1750 train_time:77612ms step_avg:95.70ms
step:812/1750 train_time:77712ms step_avg:95.70ms
step:813/1750 train_time:77812ms step_avg:95.71ms
step:814/1750 train_time:77911ms step_avg:95.71ms
step:815/1750 train_time:78010ms step_avg:95.72ms
step:816/1750 train_time:78110ms step_avg:95.72ms
step:817/1750 train_time:78208ms step_avg:95.73ms
step:818/1750 train_time:78307ms step_avg:95.73ms
step:819/1750 train_time:78406ms step_avg:95.73ms
step:820/1750 train_time:78505ms step_avg:95.74ms
step:821/1750 train_time:78604ms step_avg:95.74ms
step:822/1750 train_time:78704ms step_avg:95.75ms
step:823/1750 train_time:78804ms step_avg:95.75ms
step:824/1750 train_time:78903ms step_avg:95.76ms
step:825/1750 train_time:79002ms step_avg:95.76ms
step:826/1750 train_time:79101ms step_avg:95.76ms
step:827/1750 train_time:79200ms step_avg:95.77ms
step:828/1750 train_time:79298ms step_avg:95.77ms
step:829/1750 train_time:79397ms step_avg:95.77ms
step:830/1750 train_time:79495ms step_avg:95.78ms
step:831/1750 train_time:79595ms step_avg:95.78ms
step:832/1750 train_time:79694ms step_avg:95.79ms
step:833/1750 train_time:79793ms step_avg:95.79ms
step:834/1750 train_time:79892ms step_avg:95.79ms
step:835/1750 train_time:79990ms step_avg:95.80ms
step:836/1750 train_time:80089ms step_avg:95.80ms
step:837/1750 train_time:80187ms step_avg:95.80ms
step:838/1750 train_time:80286ms step_avg:95.81ms
step:839/1750 train_time:80385ms step_avg:95.81ms
step:840/1750 train_time:80483ms step_avg:95.81ms
step:841/1750 train_time:80583ms step_avg:95.82ms
step:842/1750 train_time:80683ms step_avg:95.82ms
step:843/1750 train_time:80783ms step_avg:95.83ms
step:844/1750 train_time:80883ms step_avg:95.83ms
step:845/1750 train_time:80983ms step_avg:95.84ms
step:846/1750 train_time:81083ms step_avg:95.84ms
step:847/1750 train_time:81182ms step_avg:95.85ms
step:848/1750 train_time:81281ms step_avg:95.85ms
step:849/1750 train_time:81379ms step_avg:95.85ms
step:850/1750 train_time:81478ms step_avg:95.86ms
step:851/1750 train_time:81577ms step_avg:95.86ms
step:852/1750 train_time:81675ms step_avg:95.86ms
step:853/1750 train_time:81775ms step_avg:95.87ms
step:854/1750 train_time:81873ms step_avg:95.87ms
step:855/1750 train_time:81972ms step_avg:95.87ms
step:856/1750 train_time:82071ms step_avg:95.88ms
step:857/1750 train_time:82171ms step_avg:95.88ms
step:858/1750 train_time:82270ms step_avg:95.89ms
step:859/1750 train_time:82368ms step_avg:95.89ms
step:860/1750 train_time:82466ms step_avg:95.89ms
step:861/1750 train_time:82565ms step_avg:95.89ms
step:862/1750 train_time:82663ms step_avg:95.90ms
step:863/1750 train_time:82763ms step_avg:95.90ms
step:864/1750 train_time:82862ms step_avg:95.91ms
step:865/1750 train_time:82962ms step_avg:95.91ms
step:866/1750 train_time:83061ms step_avg:95.91ms
step:867/1750 train_time:83161ms step_avg:95.92ms
step:868/1750 train_time:83259ms step_avg:95.92ms
step:869/1750 train_time:83358ms step_avg:95.92ms
step:870/1750 train_time:83457ms step_avg:95.93ms
step:871/1750 train_time:83556ms step_avg:95.93ms
step:872/1750 train_time:83655ms step_avg:95.94ms
step:873/1750 train_time:83754ms step_avg:95.94ms
step:874/1750 train_time:83853ms step_avg:95.94ms
step:875/1750 train_time:83952ms step_avg:95.95ms
step:875/1750 val_loss:3.5463 train_time:84045ms step_avg:96.05ms
step:876/1750 train_time:84072ms step_avg:95.97ms
step:877/1750 train_time:84157ms step_avg:95.96ms
step:878/1750 train_time:84258ms step_avg:95.97ms
step:879/1750 train_time:84357ms step_avg:95.97ms
step:880/1750 train_time:84455ms step_avg:95.97ms
step:881/1750 train_time:84554ms step_avg:95.97ms
step:882/1750 train_time:84652ms step_avg:95.98ms
step:883/1750 train_time:84751ms step_avg:95.98ms
step:884/1750 train_time:84849ms step_avg:95.98ms
step:885/1750 train_time:84947ms step_avg:95.99ms
step:886/1750 train_time:85046ms step_avg:95.99ms
step:887/1750 train_time:85146ms step_avg:95.99ms
step:888/1750 train_time:85246ms step_avg:96.00ms
step:889/1750 train_time:85345ms step_avg:96.00ms
step:890/1750 train_time:85444ms step_avg:96.00ms
step:891/1750 train_time:85543ms step_avg:96.01ms
step:892/1750 train_time:85641ms step_avg:96.01ms
step:893/1750 train_time:85740ms step_avg:96.01ms
step:894/1750 train_time:85839ms step_avg:96.02ms
step:895/1750 train_time:85938ms step_avg:96.02ms
step:896/1750 train_time:86037ms step_avg:96.02ms
step:897/1750 train_time:86136ms step_avg:96.03ms
step:898/1750 train_time:86235ms step_avg:96.03ms
step:899/1750 train_time:86335ms step_avg:96.03ms
step:900/1750 train_time:86434ms step_avg:96.04ms
step:901/1750 train_time:86535ms step_avg:96.04ms
step:902/1750 train_time:86634ms step_avg:96.05ms
step:903/1750 train_time:86734ms step_avg:96.05ms
step:904/1750 train_time:86833ms step_avg:96.05ms
step:905/1750 train_time:86931ms step_avg:96.06ms
step:906/1750 train_time:87031ms step_avg:96.06ms
step:907/1750 train_time:87129ms step_avg:96.06ms
step:908/1750 train_time:87228ms step_avg:96.07ms
step:909/1750 train_time:87327ms step_avg:96.07ms
step:910/1750 train_time:87428ms step_avg:96.07ms
step:911/1750 train_time:87528ms step_avg:96.08ms
step:912/1750 train_time:87628ms step_avg:96.08ms
step:913/1750 train_time:87729ms step_avg:96.09ms
step:914/1750 train_time:87829ms step_avg:96.09ms
step:915/1750 train_time:87929ms step_avg:96.10ms
step:916/1750 train_time:88029ms step_avg:96.10ms
step:917/1750 train_time:88129ms step_avg:96.11ms
step:918/1750 train_time:88229ms step_avg:96.11ms
step:919/1750 train_time:88329ms step_avg:96.11ms
step:920/1750 train_time:88430ms step_avg:96.12ms
step:921/1750 train_time:88531ms step_avg:96.13ms
step:922/1750 train_time:88631ms step_avg:96.13ms
step:923/1750 train_time:88731ms step_avg:96.13ms
step:924/1750 train_time:88831ms step_avg:96.14ms
step:925/1750 train_time:88931ms step_avg:96.14ms
step:926/1750 train_time:89032ms step_avg:96.15ms
step:927/1750 train_time:89132ms step_avg:96.15ms
step:928/1750 train_time:89231ms step_avg:96.15ms
step:929/1750 train_time:89332ms step_avg:96.16ms
step:930/1750 train_time:89433ms step_avg:96.16ms
step:931/1750 train_time:89534ms step_avg:96.17ms
step:932/1750 train_time:89634ms step_avg:96.17ms
step:933/1750 train_time:89735ms step_avg:96.18ms
step:934/1750 train_time:89835ms step_avg:96.18ms
step:935/1750 train_time:89937ms step_avg:96.19ms
step:936/1750 train_time:90038ms step_avg:96.19ms
step:937/1750 train_time:90138ms step_avg:96.20ms
step:938/1750 train_time:90240ms step_avg:96.20ms
step:939/1750 train_time:90340ms step_avg:96.21ms
step:940/1750 train_time:90441ms step_avg:96.21ms
step:941/1750 train_time:90541ms step_avg:96.22ms
step:942/1750 train_time:90642ms step_avg:96.22ms
step:943/1750 train_time:90742ms step_avg:96.23ms
step:944/1750 train_time:90842ms step_avg:96.23ms
step:945/1750 train_time:90943ms step_avg:96.24ms
step:946/1750 train_time:91043ms step_avg:96.24ms
step:947/1750 train_time:91143ms step_avg:96.24ms
step:948/1750 train_time:91243ms step_avg:96.25ms
step:949/1750 train_time:91342ms step_avg:96.25ms
step:950/1750 train_time:91443ms step_avg:96.26ms
step:951/1750 train_time:91542ms step_avg:96.26ms
step:952/1750 train_time:91642ms step_avg:96.26ms
step:953/1750 train_time:91742ms step_avg:96.27ms
step:954/1750 train_time:91843ms step_avg:96.27ms
step:955/1750 train_time:91944ms step_avg:96.28ms
step:956/1750 train_time:92044ms step_avg:96.28ms
step:957/1750 train_time:92144ms step_avg:96.28ms
step:958/1750 train_time:92244ms step_avg:96.29ms
step:959/1750 train_time:92344ms step_avg:96.29ms
step:960/1750 train_time:92444ms step_avg:96.30ms
step:961/1750 train_time:92545ms step_avg:96.30ms
step:962/1750 train_time:92646ms step_avg:96.31ms
step:963/1750 train_time:92747ms step_avg:96.31ms
step:964/1750 train_time:92847ms step_avg:96.31ms
step:965/1750 train_time:92947ms step_avg:96.32ms
step:966/1750 train_time:93047ms step_avg:96.32ms
step:967/1750 train_time:93147ms step_avg:96.33ms
step:968/1750 train_time:93246ms step_avg:96.33ms
step:969/1750 train_time:93347ms step_avg:96.33ms
step:970/1750 train_time:93447ms step_avg:96.34ms
step:971/1750 train_time:93547ms step_avg:96.34ms
step:972/1750 train_time:93648ms step_avg:96.35ms
step:973/1750 train_time:93749ms step_avg:96.35ms
step:974/1750 train_time:93850ms step_avg:96.35ms
step:975/1750 train_time:93950ms step_avg:96.36ms
step:976/1750 train_time:94050ms step_avg:96.36ms
step:977/1750 train_time:94151ms step_avg:96.37ms
step:978/1750 train_time:94251ms step_avg:96.37ms
step:979/1750 train_time:94351ms step_avg:96.38ms
step:980/1750 train_time:94452ms step_avg:96.38ms
step:981/1750 train_time:94552ms step_avg:96.38ms
step:982/1750 train_time:94653ms step_avg:96.39ms
step:983/1750 train_time:94755ms step_avg:96.39ms
step:984/1750 train_time:94856ms step_avg:96.40ms
step:985/1750 train_time:94958ms step_avg:96.40ms
step:986/1750 train_time:95060ms step_avg:96.41ms
step:987/1750 train_time:95161ms step_avg:96.41ms
step:988/1750 train_time:95261ms step_avg:96.42ms
step:989/1750 train_time:95361ms step_avg:96.42ms
step:990/1750 train_time:95462ms step_avg:96.43ms
step:991/1750 train_time:95563ms step_avg:96.43ms
step:992/1750 train_time:95664ms step_avg:96.44ms
step:993/1750 train_time:95763ms step_avg:96.44ms
step:994/1750 train_time:95865ms step_avg:96.44ms
step:995/1750 train_time:95965ms step_avg:96.45ms
step:996/1750 train_time:96065ms step_avg:96.45ms
step:997/1750 train_time:96164ms step_avg:96.45ms
step:998/1750 train_time:96264ms step_avg:96.46ms
step:999/1750 train_time:96364ms step_avg:96.46ms
step:1000/1750 train_time:96465ms step_avg:96.46ms
step:1000/1750 val_loss:3.5054 train_time:96560ms step_avg:96.56ms
step:1001/1750 train_time:96588ms step_avg:96.49ms
step:1002/1750 train_time:96672ms step_avg:96.48ms
step:1003/1750 train_time:96773ms step_avg:96.48ms
step:1004/1750 train_time:96873ms step_avg:96.49ms
step:1005/1750 train_time:96974ms step_avg:96.49ms
step:1006/1750 train_time:97074ms step_avg:96.49ms
step:1007/1750 train_time:97175ms step_avg:96.50ms
step:1008/1750 train_time:97275ms step_avg:96.50ms
step:1009/1750 train_time:97376ms step_avg:96.51ms
step:1010/1750 train_time:97476ms step_avg:96.51ms
step:1011/1750 train_time:97578ms step_avg:96.52ms
step:1012/1750 train_time:97678ms step_avg:96.52ms
step:1013/1750 train_time:97779ms step_avg:96.52ms
step:1014/1750 train_time:97879ms step_avg:96.53ms
step:1015/1750 train_time:97979ms step_avg:96.53ms
step:1016/1750 train_time:98079ms step_avg:96.53ms
step:1017/1750 train_time:98179ms step_avg:96.54ms
step:1018/1750 train_time:98278ms step_avg:96.54ms
step:1019/1750 train_time:98378ms step_avg:96.54ms
step:1020/1750 train_time:98479ms step_avg:96.55ms
step:1021/1750 train_time:98580ms step_avg:96.55ms
step:1022/1750 train_time:98680ms step_avg:96.56ms
step:1023/1750 train_time:98781ms step_avg:96.56ms
step:1024/1750 train_time:98883ms step_avg:96.57ms
step:1025/1750 train_time:98983ms step_avg:96.57ms
step:1026/1750 train_time:99084ms step_avg:96.57ms
step:1027/1750 train_time:99184ms step_avg:96.58ms
step:1028/1750 train_time:99284ms step_avg:96.58ms
step:1029/1750 train_time:99384ms step_avg:96.58ms
step:1030/1750 train_time:99485ms step_avg:96.59ms
step:1031/1750 train_time:99584ms step_avg:96.59ms
step:1032/1750 train_time:99684ms step_avg:96.59ms
step:1033/1750 train_time:99785ms step_avg:96.60ms
step:1034/1750 train_time:99885ms step_avg:96.60ms
step:1035/1750 train_time:99985ms step_avg:96.60ms
step:1036/1750 train_time:100085ms step_avg:96.61ms
step:1037/1750 train_time:100185ms step_avg:96.61ms
step:1038/1750 train_time:100285ms step_avg:96.61ms
step:1039/1750 train_time:100384ms step_avg:96.62ms
step:1040/1750 train_time:100484ms step_avg:96.62ms
step:1041/1750 train_time:100585ms step_avg:96.62ms
step:1042/1750 train_time:100685ms step_avg:96.63ms
step:1043/1750 train_time:100786ms step_avg:96.63ms
step:1044/1750 train_time:100888ms step_avg:96.64ms
step:1045/1750 train_time:100989ms step_avg:96.64ms
step:1046/1750 train_time:101089ms step_avg:96.64ms
step:1047/1750 train_time:101189ms step_avg:96.65ms
step:1048/1750 train_time:101291ms step_avg:96.65ms
step:1049/1750 train_time:101391ms step_avg:96.66ms
step:1050/1750 train_time:101494ms step_avg:96.66ms
step:1051/1750 train_time:101596ms step_avg:96.67ms
step:1052/1750 train_time:101696ms step_avg:96.67ms
step:1053/1750 train_time:101798ms step_avg:96.67ms
step:1054/1750 train_time:101900ms step_avg:96.68ms
step:1055/1750 train_time:102002ms step_avg:96.68ms
step:1056/1750 train_time:102102ms step_avg:96.69ms
step:1057/1750 train_time:102202ms step_avg:96.69ms
step:1058/1750 train_time:102302ms step_avg:96.69ms
step:1059/1750 train_time:102402ms step_avg:96.70ms
step:1060/1750 train_time:102502ms step_avg:96.70ms
step:1061/1750 train_time:102602ms step_avg:96.70ms
step:1062/1750 train_time:102703ms step_avg:96.71ms
step:1063/1750 train_time:102803ms step_avg:96.71ms
step:1064/1750 train_time:102903ms step_avg:96.71ms
step:1065/1750 train_time:103003ms step_avg:96.72ms
step:1066/1750 train_time:103103ms step_avg:96.72ms
step:1067/1750 train_time:103203ms step_avg:96.72ms
step:1068/1750 train_time:103303ms step_avg:96.73ms
step:1069/1750 train_time:103403ms step_avg:96.73ms
step:1070/1750 train_time:103504ms step_avg:96.73ms
step:1071/1750 train_time:103604ms step_avg:96.74ms
step:1072/1750 train_time:103704ms step_avg:96.74ms
step:1073/1750 train_time:103804ms step_avg:96.74ms
step:1074/1750 train_time:103905ms step_avg:96.75ms
step:1075/1750 train_time:104005ms step_avg:96.75ms
step:1076/1750 train_time:104106ms step_avg:96.75ms
step:1077/1750 train_time:104206ms step_avg:96.76ms
step:1078/1750 train_time:104306ms step_avg:96.76ms
step:1079/1750 train_time:104406ms step_avg:96.76ms
step:1080/1750 train_time:104507ms step_avg:96.77ms
step:1081/1750 train_time:104607ms step_avg:96.77ms
step:1082/1750 train_time:104708ms step_avg:96.77ms
step:1083/1750 train_time:104808ms step_avg:96.78ms
step:1084/1750 train_time:104908ms step_avg:96.78ms
step:1085/1750 train_time:105008ms step_avg:96.78ms
step:1086/1750 train_time:105108ms step_avg:96.78ms
step:1087/1750 train_time:105209ms step_avg:96.79ms
step:1088/1750 train_time:105309ms step_avg:96.79ms
step:1089/1750 train_time:105409ms step_avg:96.79ms
step:1090/1750 train_time:105511ms step_avg:96.80ms
step:1091/1750 train_time:105612ms step_avg:96.80ms
step:1092/1750 train_time:105712ms step_avg:96.81ms
step:1093/1750 train_time:105812ms step_avg:96.81ms
step:1094/1750 train_time:105913ms step_avg:96.81ms
step:1095/1750 train_time:106014ms step_avg:96.82ms
step:1096/1750 train_time:106115ms step_avg:96.82ms
step:1097/1750 train_time:106216ms step_avg:96.82ms
step:1098/1750 train_time:106317ms step_avg:96.83ms
step:1099/1750 train_time:106418ms step_avg:96.83ms
step:1100/1750 train_time:106519ms step_avg:96.84ms
step:1101/1750 train_time:106619ms step_avg:96.84ms
step:1102/1750 train_time:106719ms step_avg:96.84ms
step:1103/1750 train_time:106819ms step_avg:96.84ms
step:1104/1750 train_time:106919ms step_avg:96.85ms
step:1105/1750 train_time:107020ms step_avg:96.85ms
step:1106/1750 train_time:107121ms step_avg:96.85ms
step:1107/1750 train_time:107222ms step_avg:96.86ms
step:1108/1750 train_time:107324ms step_avg:96.86ms
step:1109/1750 train_time:107424ms step_avg:96.87ms
step:1110/1750 train_time:107525ms step_avg:96.87ms
step:1111/1750 train_time:107625ms step_avg:96.87ms
step:1112/1750 train_time:107726ms step_avg:96.88ms
step:1113/1750 train_time:107827ms step_avg:96.88ms
step:1114/1750 train_time:107927ms step_avg:96.88ms
step:1115/1750 train_time:108028ms step_avg:96.89ms
step:1116/1750 train_time:108129ms step_avg:96.89ms
step:1117/1750 train_time:108229ms step_avg:96.89ms
step:1118/1750 train_time:108329ms step_avg:96.90ms
step:1119/1750 train_time:108429ms step_avg:96.90ms
step:1120/1750 train_time:108530ms step_avg:96.90ms
step:1121/1750 train_time:108631ms step_avg:96.91ms
step:1122/1750 train_time:108731ms step_avg:96.91ms
step:1123/1750 train_time:108832ms step_avg:96.91ms
step:1124/1750 train_time:108932ms step_avg:96.91ms
step:1125/1750 train_time:109034ms step_avg:96.92ms
step:1125/1750 val_loss:3.4528 train_time:109130ms step_avg:97.00ms
step:1126/1750 train_time:109156ms step_avg:96.94ms
step:1127/1750 train_time:109244ms step_avg:96.93ms
step:1128/1750 train_time:109345ms step_avg:96.94ms
step:1129/1750 train_time:109445ms step_avg:96.94ms
step:1130/1750 train_time:109545ms step_avg:96.94ms
step:1131/1750 train_time:109644ms step_avg:96.94ms
step:1132/1750 train_time:109744ms step_avg:96.95ms
step:1133/1750 train_time:109844ms step_avg:96.95ms
step:1134/1750 train_time:109943ms step_avg:96.95ms
step:1135/1750 train_time:110043ms step_avg:96.95ms
step:1136/1750 train_time:110144ms step_avg:96.96ms
step:1137/1750 train_time:110247ms step_avg:96.96ms
step:1138/1750 train_time:110347ms step_avg:96.97ms
step:1139/1750 train_time:110448ms step_avg:96.97ms
step:1140/1750 train_time:110548ms step_avg:96.97ms
step:1141/1750 train_time:110647ms step_avg:96.97ms
step:1142/1750 train_time:110748ms step_avg:96.98ms
step:1143/1750 train_time:110848ms step_avg:96.98ms
step:1144/1750 train_time:110948ms step_avg:96.98ms
step:1145/1750 train_time:111048ms step_avg:96.99ms
step:1146/1750 train_time:111149ms step_avg:96.99ms
step:1147/1750 train_time:111251ms step_avg:96.99ms
step:1148/1750 train_time:111351ms step_avg:97.00ms
step:1149/1750 train_time:111452ms step_avg:97.00ms
step:1150/1750 train_time:111553ms step_avg:97.00ms
step:1151/1750 train_time:111654ms step_avg:97.01ms
step:1152/1750 train_time:111755ms step_avg:97.01ms
step:1153/1750 train_time:111856ms step_avg:97.01ms
step:1154/1750 train_time:111956ms step_avg:97.02ms
step:1155/1750 train_time:112057ms step_avg:97.02ms
step:1156/1750 train_time:112158ms step_avg:97.02ms
step:1157/1750 train_time:112259ms step_avg:97.03ms
step:1158/1750 train_time:112359ms step_avg:97.03ms
step:1159/1750 train_time:112460ms step_avg:97.03ms
step:1160/1750 train_time:112560ms step_avg:97.03ms
step:1161/1750 train_time:112661ms step_avg:97.04ms
step:1162/1750 train_time:112761ms step_avg:97.04ms
step:1163/1750 train_time:112862ms step_avg:97.04ms
step:1164/1750 train_time:112962ms step_avg:97.05ms
step:1165/1750 train_time:113063ms step_avg:97.05ms
step:1166/1750 train_time:113163ms step_avg:97.05ms
step:1167/1750 train_time:113263ms step_avg:97.06ms
step:1168/1750 train_time:113363ms step_avg:97.06ms
step:1169/1750 train_time:113465ms step_avg:97.06ms
step:1170/1750 train_time:113567ms step_avg:97.07ms
step:1171/1750 train_time:113669ms step_avg:97.07ms
step:1172/1750 train_time:113771ms step_avg:97.07ms
step:1173/1750 train_time:113873ms step_avg:97.08ms
step:1174/1750 train_time:113975ms step_avg:97.08ms
step:1175/1750 train_time:114077ms step_avg:97.09ms
step:1176/1750 train_time:114180ms step_avg:97.09ms
step:1177/1750 train_time:114281ms step_avg:97.10ms
step:1178/1750 train_time:114383ms step_avg:97.10ms
step:1179/1750 train_time:114486ms step_avg:97.10ms
step:1180/1750 train_time:114588ms step_avg:97.11ms
step:1181/1750 train_time:114689ms step_avg:97.11ms
step:1182/1750 train_time:114791ms step_avg:97.12ms
step:1183/1750 train_time:114892ms step_avg:97.12ms
step:1184/1750 train_time:114994ms step_avg:97.12ms
step:1185/1750 train_time:115098ms step_avg:97.13ms
step:1186/1750 train_time:115201ms step_avg:97.13ms
step:1187/1750 train_time:115302ms step_avg:97.14ms
step:1188/1750 train_time:115404ms step_avg:97.14ms
step:1189/1750 train_time:115504ms step_avg:97.14ms
step:1190/1750 train_time:115605ms step_avg:97.15ms
step:1191/1750 train_time:115707ms step_avg:97.15ms
step:1192/1750 train_time:115810ms step_avg:97.16ms
step:1193/1750 train_time:115912ms step_avg:97.16ms
step:1194/1750 train_time:116015ms step_avg:97.16ms
step:1195/1750 train_time:116117ms step_avg:97.17ms
step:1196/1750 train_time:116219ms step_avg:97.17ms
step:1197/1750 train_time:116321ms step_avg:97.18ms
step:1198/1750 train_time:116422ms step_avg:97.18ms
step:1199/1750 train_time:116523ms step_avg:97.18ms
step:1200/1750 train_time:116624ms step_avg:97.19ms
step:1201/1750 train_time:116726ms step_avg:97.19ms
step:1202/1750 train_time:116829ms step_avg:97.20ms
step:1203/1750 train_time:116931ms step_avg:97.20ms
step:1204/1750 train_time:117034ms step_avg:97.20ms
step:1205/1750 train_time:117136ms step_avg:97.21ms
step:1206/1750 train_time:117238ms step_avg:97.21ms
step:1207/1750 train_time:117340ms step_avg:97.22ms
step:1208/1750 train_time:117442ms step_avg:97.22ms
step:1209/1750 train_time:117543ms step_avg:97.22ms
step:1210/1750 train_time:117644ms step_avg:97.23ms
step:1211/1750 train_time:117745ms step_avg:97.23ms
step:1212/1750 train_time:117847ms step_avg:97.23ms
step:1213/1750 train_time:117949ms step_avg:97.24ms
step:1214/1750 train_time:118051ms step_avg:97.24ms
step:1215/1750 train_time:118153ms step_avg:97.25ms
step:1216/1750 train_time:118256ms step_avg:97.25ms
step:1217/1750 train_time:118359ms step_avg:97.25ms
step:1218/1750 train_time:118462ms step_avg:97.26ms
step:1219/1750 train_time:118564ms step_avg:97.26ms
step:1220/1750 train_time:118666ms step_avg:97.27ms
step:1221/1750 train_time:118767ms step_avg:97.27ms
step:1222/1750 train_time:118869ms step_avg:97.27ms
step:1223/1750 train_time:118970ms step_avg:97.28ms
step:1224/1750 train_time:119072ms step_avg:97.28ms
step:1225/1750 train_time:119174ms step_avg:97.28ms
step:1226/1750 train_time:119276ms step_avg:97.29ms
step:1227/1750 train_time:119379ms step_avg:97.29ms
step:1228/1750 train_time:119481ms step_avg:97.30ms
step:1229/1750 train_time:119583ms step_avg:97.30ms
step:1230/1750 train_time:119685ms step_avg:97.30ms
step:1231/1750 train_time:119786ms step_avg:97.31ms
step:1232/1750 train_time:119888ms step_avg:97.31ms
step:1233/1750 train_time:119989ms step_avg:97.31ms
step:1234/1750 train_time:120091ms step_avg:97.32ms
step:1235/1750 train_time:120193ms step_avg:97.32ms
step:1236/1750 train_time:120297ms step_avg:97.33ms
step:1237/1750 train_time:120400ms step_avg:97.33ms
step:1238/1750 train_time:120502ms step_avg:97.34ms
step:1239/1750 train_time:120604ms step_avg:97.34ms
step:1240/1750 train_time:120705ms step_avg:97.34ms
step:1241/1750 train_time:120807ms step_avg:97.35ms
step:1242/1750 train_time:120908ms step_avg:97.35ms
step:1243/1750 train_time:121009ms step_avg:97.35ms
step:1244/1750 train_time:121109ms step_avg:97.35ms
step:1245/1750 train_time:121211ms step_avg:97.36ms
step:1246/1750 train_time:121314ms step_avg:97.36ms
step:1247/1750 train_time:121416ms step_avg:97.37ms
step:1248/1750 train_time:121519ms step_avg:97.37ms
step:1249/1750 train_time:121621ms step_avg:97.37ms
step:1250/1750 train_time:121722ms step_avg:97.38ms
step:1250/1750 val_loss:3.4067 train_time:121818ms step_avg:97.45ms
step:1251/1750 train_time:121844ms step_avg:97.40ms
step:1252/1750 train_time:121933ms step_avg:97.39ms
step:1253/1750 train_time:122036ms step_avg:97.39ms
step:1254/1750 train_time:122137ms step_avg:97.40ms
step:1255/1750 train_time:122239ms step_avg:97.40ms
step:1256/1750 train_time:122339ms step_avg:97.40ms
step:1257/1750 train_time:122440ms step_avg:97.41ms
step:1258/1750 train_time:122542ms step_avg:97.41ms
step:1259/1750 train_time:122643ms step_avg:97.41ms
step:1260/1750 train_time:122745ms step_avg:97.42ms
step:1261/1750 train_time:122849ms step_avg:97.42ms
step:1262/1750 train_time:122952ms step_avg:97.43ms
step:1263/1750 train_time:123053ms step_avg:97.43ms
step:1264/1750 train_time:123154ms step_avg:97.43ms
step:1265/1750 train_time:123255ms step_avg:97.43ms
step:1266/1750 train_time:123356ms step_avg:97.44ms
step:1267/1750 train_time:123457ms step_avg:97.44ms
step:1268/1750 train_time:123559ms step_avg:97.44ms
step:1269/1750 train_time:123661ms step_avg:97.45ms
step:1270/1750 train_time:123763ms step_avg:97.45ms
step:1271/1750 train_time:123868ms step_avg:97.46ms
step:1272/1750 train_time:123969ms step_avg:97.46ms
step:1273/1750 train_time:124070ms step_avg:97.46ms
step:1274/1750 train_time:124171ms step_avg:97.47ms
step:1275/1750 train_time:124273ms step_avg:97.47ms
step:1276/1750 train_time:124375ms step_avg:97.47ms
step:1277/1750 train_time:124476ms step_avg:97.47ms
step:1278/1750 train_time:124577ms step_avg:97.48ms
step:1279/1750 train_time:124679ms step_avg:97.48ms
step:1280/1750 train_time:124782ms step_avg:97.49ms
step:1281/1750 train_time:124885ms step_avg:97.49ms
step:1282/1750 train_time:124988ms step_avg:97.49ms
step:1283/1750 train_time:125090ms step_avg:97.50ms
step:1284/1750 train_time:125191ms step_avg:97.50ms
step:1285/1750 train_time:125292ms step_avg:97.50ms
step:1286/1750 train_time:125392ms step_avg:97.51ms
step:1287/1750 train_time:125494ms step_avg:97.51ms
step:1288/1750 train_time:125595ms step_avg:97.51ms
step:1289/1750 train_time:125697ms step_avg:97.52ms
step:1290/1750 train_time:125800ms step_avg:97.52ms
step:1291/1750 train_time:125902ms step_avg:97.52ms
step:1292/1750 train_time:126005ms step_avg:97.53ms
step:1293/1750 train_time:126106ms step_avg:97.53ms
step:1294/1750 train_time:126209ms step_avg:97.53ms
step:1295/1750 train_time:126310ms step_avg:97.54ms
step:1296/1750 train_time:126411ms step_avg:97.54ms
step:1297/1750 train_time:126512ms step_avg:97.54ms
step:1298/1750 train_time:126613ms step_avg:97.54ms
step:1299/1750 train_time:126716ms step_avg:97.55ms
step:1300/1750 train_time:126818ms step_avg:97.55ms
step:1301/1750 train_time:126921ms step_avg:97.56ms
step:1302/1750 train_time:127023ms step_avg:97.56ms
step:1303/1750 train_time:127126ms step_avg:97.56ms
step:1304/1750 train_time:127229ms step_avg:97.57ms
step:1305/1750 train_time:127331ms step_avg:97.57ms
step:1306/1750 train_time:127432ms step_avg:97.57ms
step:1307/1750 train_time:127534ms step_avg:97.58ms
step:1308/1750 train_time:127635ms step_avg:97.58ms
step:1309/1750 train_time:127736ms step_avg:97.58ms
step:1310/1750 train_time:127838ms step_avg:97.59ms
step:1311/1750 train_time:127940ms step_avg:97.59ms
step:1312/1750 train_time:128043ms step_avg:97.59ms
step:1313/1750 train_time:128146ms step_avg:97.60ms
step:1314/1750 train_time:128249ms step_avg:97.60ms
step:1315/1750 train_time:128351ms step_avg:97.61ms
step:1316/1750 train_time:128452ms step_avg:97.61ms
step:1317/1750 train_time:128553ms step_avg:97.61ms
step:1318/1750 train_time:128654ms step_avg:97.61ms
step:1319/1750 train_time:128755ms step_avg:97.62ms
step:1320/1750 train_time:128858ms step_avg:97.62ms
step:1321/1750 train_time:128960ms step_avg:97.62ms
step:1322/1750 train_time:129062ms step_avg:97.63ms
step:1323/1750 train_time:129166ms step_avg:97.63ms
step:1324/1750 train_time:129269ms step_avg:97.64ms
step:1325/1750 train_time:129371ms step_avg:97.64ms
step:1326/1750 train_time:129473ms step_avg:97.64ms
step:1327/1750 train_time:129574ms step_avg:97.64ms
step:1328/1750 train_time:129676ms step_avg:97.65ms
step:1329/1750 train_time:129777ms step_avg:97.65ms
step:1330/1750 train_time:129879ms step_avg:97.65ms
step:1331/1750 train_time:129980ms step_avg:97.66ms
step:1332/1750 train_time:130083ms step_avg:97.66ms
step:1333/1750 train_time:130186ms step_avg:97.66ms
step:1334/1750 train_time:130288ms step_avg:97.67ms
step:1335/1750 train_time:130390ms step_avg:97.67ms
step:1336/1750 train_time:130492ms step_avg:97.67ms
step:1337/1750 train_time:130594ms step_avg:97.68ms
step:1338/1750 train_time:130695ms step_avg:97.68ms
step:1339/1750 train_time:130796ms step_avg:97.68ms
step:1340/1750 train_time:130898ms step_avg:97.69ms
step:1341/1750 train_time:131000ms step_avg:97.69ms
step:1342/1750 train_time:131102ms step_avg:97.69ms
step:1343/1750 train_time:131205ms step_avg:97.70ms
step:1344/1750 train_time:131308ms step_avg:97.70ms
step:1345/1750 train_time:131410ms step_avg:97.70ms
step:1346/1750 train_time:131512ms step_avg:97.71ms
step:1347/1750 train_time:131614ms step_avg:97.71ms
step:1348/1750 train_time:131715ms step_avg:97.71ms
step:1349/1750 train_time:131817ms step_avg:97.71ms
step:1350/1750 train_time:131919ms step_avg:97.72ms
step:1351/1750 train_time:132021ms step_avg:97.72ms
step:1352/1750 train_time:132123ms step_avg:97.72ms
step:1353/1750 train_time:132225ms step_avg:97.73ms
step:1354/1750 train_time:132328ms step_avg:97.73ms
step:1355/1750 train_time:132430ms step_avg:97.73ms
step:1356/1750 train_time:132533ms step_avg:97.74ms
step:1357/1750 train_time:132634ms step_avg:97.74ms
step:1358/1750 train_time:132736ms step_avg:97.74ms
step:1359/1750 train_time:132837ms step_avg:97.75ms
step:1360/1750 train_time:132938ms step_avg:97.75ms
step:1361/1750 train_time:133040ms step_avg:97.75ms
step:1362/1750 train_time:133142ms step_avg:97.75ms
step:1363/1750 train_time:133245ms step_avg:97.76ms
step:1364/1750 train_time:133348ms step_avg:97.76ms
step:1365/1750 train_time:133450ms step_avg:97.77ms
step:1366/1750 train_time:133551ms step_avg:97.77ms
step:1367/1750 train_time:133652ms step_avg:97.77ms
step:1368/1750 train_time:133754ms step_avg:97.77ms
step:1369/1750 train_time:133855ms step_avg:97.78ms
step:1370/1750 train_time:133957ms step_avg:97.78ms
step:1371/1750 train_time:134061ms step_avg:97.78ms
step:1372/1750 train_time:134162ms step_avg:97.79ms
step:1373/1750 train_time:134266ms step_avg:97.79ms
step:1374/1750 train_time:134367ms step_avg:97.79ms
step:1375/1750 train_time:134471ms step_avg:97.80ms
step:1375/1750 val_loss:3.3653 train_time:134566ms step_avg:97.87ms
step:1376/1750 train_time:134593ms step_avg:97.81ms
step:1377/1750 train_time:134682ms step_avg:97.81ms
step:1378/1750 train_time:134784ms step_avg:97.81ms
step:1379/1750 train_time:134886ms step_avg:97.81ms
step:1380/1750 train_time:134988ms step_avg:97.82ms
step:1381/1750 train_time:135089ms step_avg:97.82ms
step:1382/1750 train_time:135191ms step_avg:97.82ms
step:1383/1750 train_time:135292ms step_avg:97.83ms
step:1384/1750 train_time:135394ms step_avg:97.83ms
step:1385/1750 train_time:135496ms step_avg:97.83ms
step:1386/1750 train_time:135600ms step_avg:97.84ms
step:1387/1750 train_time:135703ms step_avg:97.84ms
step:1388/1750 train_time:135805ms step_avg:97.84ms
step:1389/1750 train_time:135907ms step_avg:97.85ms
step:1390/1750 train_time:136009ms step_avg:97.85ms
step:1391/1750 train_time:136112ms step_avg:97.85ms
step:1392/1750 train_time:136213ms step_avg:97.85ms
step:1393/1750 train_time:136315ms step_avg:97.86ms
step:1394/1750 train_time:136417ms step_avg:97.86ms
step:1395/1750 train_time:136519ms step_avg:97.86ms
step:1396/1750 train_time:136621ms step_avg:97.87ms
step:1397/1750 train_time:136724ms step_avg:97.87ms
step:1398/1750 train_time:136825ms step_avg:97.87ms
step:1399/1750 train_time:136928ms step_avg:97.88ms
step:1400/1750 train_time:137029ms step_avg:97.88ms
step:1401/1750 train_time:137131ms step_avg:97.88ms
step:1402/1750 train_time:137232ms step_avg:97.88ms
step:1403/1750 train_time:137334ms step_avg:97.89ms
step:1404/1750 train_time:137437ms step_avg:97.89ms
step:1405/1750 train_time:137539ms step_avg:97.89ms
step:1406/1750 train_time:137641ms step_avg:97.90ms
step:1407/1750 train_time:137744ms step_avg:97.90ms
step:1408/1750 train_time:137846ms step_avg:97.90ms
step:1409/1750 train_time:137949ms step_avg:97.91ms
step:1410/1750 train_time:138052ms step_avg:97.91ms
step:1411/1750 train_time:138153ms step_avg:97.91ms
step:1412/1750 train_time:138255ms step_avg:97.91ms
step:1413/1750 train_time:138356ms step_avg:97.92ms
step:1414/1750 train_time:138458ms step_avg:97.92ms
step:1415/1750 train_time:138561ms step_avg:97.92ms
step:1416/1750 train_time:138662ms step_avg:97.93ms
step:1417/1750 train_time:138764ms step_avg:97.93ms
step:1418/1750 train_time:138866ms step_avg:97.93ms
step:1419/1750 train_time:138968ms step_avg:97.93ms
step:1420/1750 train_time:139070ms step_avg:97.94ms
step:1421/1750 train_time:139173ms step_avg:97.94ms
step:1422/1750 train_time:139275ms step_avg:97.94ms
step:1423/1750 train_time:139377ms step_avg:97.95ms
step:1424/1750 train_time:139479ms step_avg:97.95ms
step:1425/1750 train_time:139582ms step_avg:97.95ms
step:1426/1750 train_time:139684ms step_avg:97.96ms
step:1427/1750 train_time:139786ms step_avg:97.96ms
step:1428/1750 train_time:139890ms step_avg:97.96ms
step:1429/1750 train_time:139993ms step_avg:97.97ms
step:1430/1750 train_time:140095ms step_avg:97.97ms
step:1431/1750 train_time:140198ms step_avg:97.97ms
step:1432/1750 train_time:140301ms step_avg:97.98ms
step:1433/1750 train_time:140403ms step_avg:97.98ms
step:1434/1750 train_time:140506ms step_avg:97.98ms
step:1435/1750 train_time:140610ms step_avg:97.99ms
step:1436/1750 train_time:140714ms step_avg:97.99ms
step:1437/1750 train_time:140817ms step_avg:97.99ms
step:1438/1750 train_time:140919ms step_avg:98.00ms
step:1439/1750 train_time:141024ms step_avg:98.00ms
step:1440/1750 train_time:141128ms step_avg:98.01ms
step:1441/1750 train_time:141232ms step_avg:98.01ms
step:1442/1750 train_time:141335ms step_avg:98.01ms
step:1443/1750 train_time:141438ms step_avg:98.02ms
step:1444/1750 train_time:141541ms step_avg:98.02ms
step:1445/1750 train_time:141643ms step_avg:98.02ms
step:1446/1750 train_time:141747ms step_avg:98.03ms
step:1447/1750 train_time:141849ms step_avg:98.03ms
step:1448/1750 train_time:141952ms step_avg:98.03ms
step:1449/1750 train_time:142055ms step_avg:98.04ms
step:1450/1750 train_time:142159ms step_avg:98.04ms
step:1451/1750 train_time:142261ms step_avg:98.04ms
step:1452/1750 train_time:142364ms step_avg:98.05ms
step:1453/1750 train_time:142467ms step_avg:98.05ms
step:1454/1750 train_time:142572ms step_avg:98.05ms
step:1455/1750 train_time:142675ms step_avg:98.06ms
step:1456/1750 train_time:142778ms step_avg:98.06ms
step:1457/1750 train_time:142882ms step_avg:98.07ms
step:1458/1750 train_time:142985ms step_avg:98.07ms
step:1459/1750 train_time:143088ms step_avg:98.07ms
step:1460/1750 train_time:143190ms step_avg:98.08ms
step:1461/1750 train_time:143294ms step_avg:98.08ms
step:1462/1750 train_time:143397ms step_avg:98.08ms
step:1463/1750 train_time:143500ms step_avg:98.09ms
step:1464/1750 train_time:143604ms step_avg:98.09ms
step:1465/1750 train_time:143707ms step_avg:98.09ms
step:1466/1750 train_time:143809ms step_avg:98.10ms
step:1467/1750 train_time:143912ms step_avg:98.10ms
step:1468/1750 train_time:144017ms step_avg:98.10ms
step:1469/1750 train_time:144121ms step_avg:98.11ms
step:1470/1750 train_time:144224ms step_avg:98.11ms
step:1471/1750 train_time:144327ms step_avg:98.11ms
step:1472/1750 train_time:144429ms step_avg:98.12ms
step:1473/1750 train_time:144533ms step_avg:98.12ms
step:1474/1750 train_time:144638ms step_avg:98.13ms
step:1475/1750 train_time:144740ms step_avg:98.13ms
step:1476/1750 train_time:144842ms step_avg:98.13ms
step:1477/1750 train_time:144947ms step_avg:98.14ms
step:1478/1750 train_time:145050ms step_avg:98.14ms
step:1479/1750 train_time:145152ms step_avg:98.14ms
step:1480/1750 train_time:145255ms step_avg:98.15ms
step:1481/1750 train_time:145359ms step_avg:98.15ms
step:1482/1750 train_time:145463ms step_avg:98.15ms
step:1483/1750 train_time:145565ms step_avg:98.16ms
step:1484/1750 train_time:145669ms step_avg:98.16ms
step:1485/1750 train_time:145773ms step_avg:98.16ms
step:1486/1750 train_time:145876ms step_avg:98.17ms
step:1487/1750 train_time:145979ms step_avg:98.17ms
step:1488/1750 train_time:146082ms step_avg:98.17ms
step:1489/1750 train_time:146184ms step_avg:98.18ms
step:1490/1750 train_time:146287ms step_avg:98.18ms
step:1491/1750 train_time:146389ms step_avg:98.18ms
step:1492/1750 train_time:146492ms step_avg:98.19ms
step:1493/1750 train_time:146596ms step_avg:98.19ms
step:1494/1750 train_time:146699ms step_avg:98.19ms
step:1495/1750 train_time:146802ms step_avg:98.20ms
step:1496/1750 train_time:146905ms step_avg:98.20ms
step:1497/1750 train_time:147007ms step_avg:98.20ms
step:1498/1750 train_time:147110ms step_avg:98.20ms
step:1499/1750 train_time:147212ms step_avg:98.21ms
step:1500/1750 train_time:147317ms step_avg:98.21ms
step:1500/1750 val_loss:3.3298 train_time:147414ms step_avg:98.28ms
step:1501/1750 train_time:147440ms step_avg:98.23ms
step:1502/1750 train_time:147533ms step_avg:98.22ms
step:1503/1750 train_time:147635ms step_avg:98.23ms
step:1504/1750 train_time:147736ms step_avg:98.23ms
step:1505/1750 train_time:147838ms step_avg:98.23ms
step:1506/1750 train_time:147940ms step_avg:98.23ms
step:1507/1750 train_time:148042ms step_avg:98.24ms
step:1508/1750 train_time:148144ms step_avg:98.24ms
step:1509/1750 train_time:148247ms step_avg:98.24ms
step:1510/1750 train_time:148350ms step_avg:98.25ms
step:1511/1750 train_time:148455ms step_avg:98.25ms
step:1512/1750 train_time:148559ms step_avg:98.25ms
step:1513/1750 train_time:148662ms step_avg:98.26ms
step:1514/1750 train_time:148765ms step_avg:98.26ms
step:1515/1750 train_time:148872ms step_avg:98.27ms
step:1516/1750 train_time:148974ms step_avg:98.27ms
step:1517/1750 train_time:149076ms step_avg:98.27ms
step:1518/1750 train_time:149177ms step_avg:98.27ms
step:1519/1750 train_time:149281ms step_avg:98.28ms
step:1520/1750 train_time:149384ms step_avg:98.28ms
step:1521/1750 train_time:149488ms step_avg:98.28ms
step:1522/1750 train_time:149591ms step_avg:98.29ms
step:1523/1750 train_time:149693ms step_avg:98.29ms
step:1524/1750 train_time:149798ms step_avg:98.29ms
step:1525/1750 train_time:149901ms step_avg:98.30ms
step:1526/1750 train_time:150004ms step_avg:98.30ms
step:1527/1750 train_time:150108ms step_avg:98.30ms
step:1528/1750 train_time:150214ms step_avg:98.31ms
step:1529/1750 train_time:150316ms step_avg:98.31ms
step:1530/1750 train_time:150419ms step_avg:98.31ms
step:1531/1750 train_time:150521ms step_avg:98.32ms
step:1532/1750 train_time:150623ms step_avg:98.32ms
step:1533/1750 train_time:150727ms step_avg:98.32ms
step:1534/1750 train_time:150831ms step_avg:98.33ms
step:1535/1750 train_time:150934ms step_avg:98.33ms
step:1536/1750 train_time:151036ms step_avg:98.33ms
step:1537/1750 train_time:151138ms step_avg:98.33ms
step:1538/1750 train_time:151241ms step_avg:98.34ms
step:1539/1750 train_time:151344ms step_avg:98.34ms
step:1540/1750 train_time:151448ms step_avg:98.34ms
step:1541/1750 train_time:151551ms step_avg:98.35ms
step:1542/1750 train_time:151655ms step_avg:98.35ms
step:1543/1750 train_time:151758ms step_avg:98.35ms
step:1544/1750 train_time:151861ms step_avg:98.36ms
step:1545/1750 train_time:151964ms step_avg:98.36ms
step:1546/1750 train_time:152067ms step_avg:98.36ms
step:1547/1750 train_time:152171ms step_avg:98.37ms
step:1548/1750 train_time:152275ms step_avg:98.37ms
step:1549/1750 train_time:152378ms step_avg:98.37ms
step:1550/1750 train_time:152482ms step_avg:98.38ms
step:1551/1750 train_time:152586ms step_avg:98.38ms
step:1552/1750 train_time:152688ms step_avg:98.38ms
step:1553/1750 train_time:152790ms step_avg:98.38ms
step:1554/1750 train_time:152893ms step_avg:98.39ms
step:1555/1750 train_time:152995ms step_avg:98.39ms
step:1556/1750 train_time:153098ms step_avg:98.39ms
step:1557/1750 train_time:153202ms step_avg:98.40ms
step:1558/1750 train_time:153306ms step_avg:98.40ms
step:1559/1750 train_time:153409ms step_avg:98.40ms
step:1560/1750 train_time:153512ms step_avg:98.40ms
step:1561/1750 train_time:153615ms step_avg:98.41ms
step:1562/1750 train_time:153718ms step_avg:98.41ms
step:1563/1750 train_time:153823ms step_avg:98.42ms
step:1564/1750 train_time:153928ms step_avg:98.42ms
step:1565/1750 train_time:154030ms step_avg:98.42ms
step:1566/1750 train_time:154133ms step_avg:98.42ms
step:1567/1750 train_time:154235ms step_avg:98.43ms
step:1568/1750 train_time:154337ms step_avg:98.43ms
step:1569/1750 train_time:154439ms step_avg:98.43ms
step:1570/1750 train_time:154544ms step_avg:98.44ms
step:1571/1750 train_time:154647ms step_avg:98.44ms
step:1572/1750 train_time:154749ms step_avg:98.44ms
step:1573/1750 train_time:154853ms step_avg:98.44ms
step:1574/1750 train_time:154956ms step_avg:98.45ms
step:1575/1750 train_time:155058ms step_avg:98.45ms
step:1576/1750 train_time:155163ms step_avg:98.45ms
step:1577/1750 train_time:155267ms step_avg:98.46ms
step:1578/1750 train_time:155370ms step_avg:98.46ms
step:1579/1750 train_time:155473ms step_avg:98.46ms
step:1580/1750 train_time:155577ms step_avg:98.47ms
step:1581/1750 train_time:155680ms step_avg:98.47ms
step:1582/1750 train_time:155782ms step_avg:98.47ms
step:1583/1750 train_time:155888ms step_avg:98.48ms
step:1584/1750 train_time:155994ms step_avg:98.48ms
step:1585/1750 train_time:156096ms step_avg:98.48ms
step:1586/1750 train_time:156200ms step_avg:98.49ms
step:1587/1750 train_time:156303ms step_avg:98.49ms
step:1588/1750 train_time:156406ms step_avg:98.49ms
step:1589/1750 train_time:156510ms step_avg:98.50ms
step:1590/1750 train_time:156614ms step_avg:98.50ms
step:1591/1750 train_time:156716ms step_avg:98.50ms
step:1592/1750 train_time:156820ms step_avg:98.51ms
step:1593/1750 train_time:156924ms step_avg:98.51ms
step:1594/1750 train_time:157030ms step_avg:98.51ms
step:1595/1750 train_time:157133ms step_avg:98.52ms
step:1596/1750 train_time:157235ms step_avg:98.52ms
step:1597/1750 train_time:157338ms step_avg:98.52ms
step:1598/1750 train_time:157442ms step_avg:98.52ms
step:1599/1750 train_time:157545ms step_avg:98.53ms
step:1600/1750 train_time:157650ms step_avg:98.53ms
step:1601/1750 train_time:157753ms step_avg:98.53ms
step:1602/1750 train_time:157856ms step_avg:98.54ms
step:1603/1750 train_time:157959ms step_avg:98.54ms
step:1604/1750 train_time:158061ms step_avg:98.54ms
step:1605/1750 train_time:158165ms step_avg:98.55ms
step:1606/1750 train_time:158267ms step_avg:98.55ms
step:1607/1750 train_time:158370ms step_avg:98.55ms
step:1608/1750 train_time:158473ms step_avg:98.55ms
step:1609/1750 train_time:158576ms step_avg:98.56ms
step:1610/1750 train_time:158681ms step_avg:98.56ms
step:1611/1750 train_time:158784ms step_avg:98.56ms
step:1612/1750 train_time:158888ms step_avg:98.57ms
step:1613/1750 train_time:158991ms step_avg:98.57ms
step:1614/1750 train_time:159093ms step_avg:98.57ms
step:1615/1750 train_time:159195ms step_avg:98.57ms
step:1616/1750 train_time:159299ms step_avg:98.58ms
step:1617/1750 train_time:159402ms step_avg:98.58ms
step:1618/1750 train_time:159505ms step_avg:98.58ms
step:1619/1750 train_time:159609ms step_avg:98.58ms
step:1620/1750 train_time:159712ms step_avg:98.59ms
step:1621/1750 train_time:159814ms step_avg:98.59ms
step:1622/1750 train_time:159916ms step_avg:98.59ms
step:1623/1750 train_time:160019ms step_avg:98.59ms
step:1624/1750 train_time:160123ms step_avg:98.60ms
step:1625/1750 train_time:160228ms step_avg:98.60ms
step:1625/1750 val_loss:3.2993 train_time:160325ms step_avg:98.66ms
step:1626/1750 train_time:160351ms step_avg:98.62ms
step:1627/1750 train_time:160442ms step_avg:98.61ms
step:1628/1750 train_time:160545ms step_avg:98.61ms
step:1629/1750 train_time:160648ms step_avg:98.62ms
step:1630/1750 train_time:160751ms step_avg:98.62ms
step:1631/1750 train_time:160854ms step_avg:98.62ms
step:1632/1750 train_time:160956ms step_avg:98.63ms
step:1633/1750 train_time:161059ms step_avg:98.63ms
step:1634/1750 train_time:161163ms step_avg:98.63ms
step:1635/1750 train_time:161267ms step_avg:98.63ms
step:1636/1750 train_time:161372ms step_avg:98.64ms
step:1637/1750 train_time:161476ms step_avg:98.64ms
step:1638/1750 train_time:161579ms step_avg:98.64ms
step:1639/1750 train_time:161682ms step_avg:98.65ms
step:1640/1750 train_time:161785ms step_avg:98.65ms
step:1641/1750 train_time:161889ms step_avg:98.65ms
step:1642/1750 train_time:161992ms step_avg:98.66ms
step:1643/1750 train_time:162094ms step_avg:98.66ms
step:1644/1750 train_time:162197ms step_avg:98.66ms
step:1645/1750 train_time:162299ms step_avg:98.66ms
step:1646/1750 train_time:162403ms step_avg:98.67ms
step:1647/1750 train_time:162509ms step_avg:98.67ms
step:1648/1750 train_time:162612ms step_avg:98.67ms
step:1649/1750 train_time:162716ms step_avg:98.68ms
step:1650/1750 train_time:162819ms step_avg:98.68ms
step:1651/1750 train_time:162924ms step_avg:98.68ms
step:1652/1750 train_time:163027ms step_avg:98.68ms
step:1653/1750 train_time:163131ms step_avg:98.69ms
step:1654/1750 train_time:163233ms step_avg:98.69ms
step:1655/1750 train_time:163338ms step_avg:98.69ms
step:1656/1750 train_time:163441ms step_avg:98.70ms
step:1657/1750 train_time:163544ms step_avg:98.70ms
step:1658/1750 train_time:163648ms step_avg:98.70ms
step:1659/1750 train_time:163755ms step_avg:98.71ms
step:1660/1750 train_time:163859ms step_avg:98.71ms
step:1661/1750 train_time:163963ms step_avg:98.71ms
step:1662/1750 train_time:164067ms step_avg:98.72ms
step:1663/1750 train_time:164170ms step_avg:98.72ms
step:1664/1750 train_time:164273ms step_avg:98.72ms
step:1665/1750 train_time:164377ms step_avg:98.72ms
step:1666/1750 train_time:164480ms step_avg:98.73ms
step:1667/1750 train_time:164584ms step_avg:98.73ms
step:1668/1750 train_time:164689ms step_avg:98.73ms
step:1669/1750 train_time:164793ms step_avg:98.74ms
step:1670/1750 train_time:164895ms step_avg:98.74ms
step:1671/1750 train_time:164998ms step_avg:98.74ms
step:1672/1750 train_time:165101ms step_avg:98.74ms
step:1673/1750 train_time:165204ms step_avg:98.75ms
step:1674/1750 train_time:165308ms step_avg:98.75ms
step:1675/1750 train_time:165411ms step_avg:98.75ms
step:1676/1750 train_time:165515ms step_avg:98.76ms
step:1677/1750 train_time:165618ms step_avg:98.76ms
step:1678/1750 train_time:165721ms step_avg:98.76ms
step:1679/1750 train_time:165826ms step_avg:98.76ms
step:1680/1750 train_time:165929ms step_avg:98.77ms
step:1681/1750 train_time:166032ms step_avg:98.77ms
step:1682/1750 train_time:166138ms step_avg:98.77ms
step:1683/1750 train_time:166240ms step_avg:98.78ms
step:1684/1750 train_time:166344ms step_avg:98.78ms
step:1685/1750 train_time:166448ms step_avg:98.78ms
step:1686/1750 train_time:166550ms step_avg:98.78ms
step:1687/1750 train_time:166653ms step_avg:98.79ms
step:1688/1750 train_time:166756ms step_avg:98.79ms
step:1689/1750 train_time:166860ms step_avg:98.79ms
step:1690/1750 train_time:166963ms step_avg:98.79ms
step:1691/1750 train_time:167068ms step_avg:98.80ms
step:1692/1750 train_time:167171ms step_avg:98.80ms
step:1693/1750 train_time:167275ms step_avg:98.80ms
step:1694/1750 train_time:167380ms step_avg:98.81ms
step:1695/1750 train_time:167487ms step_avg:98.81ms
step:1696/1750 train_time:167590ms step_avg:98.81ms
step:1697/1750 train_time:167697ms step_avg:98.82ms
step:1698/1750 train_time:167801ms step_avg:98.82ms
step:1699/1750 train_time:167904ms step_avg:98.83ms
step:1700/1750 train_time:168009ms step_avg:98.83ms
step:1701/1750 train_time:168112ms step_avg:98.83ms
step:1702/1750 train_time:168219ms step_avg:98.84ms
step:1703/1750 train_time:168322ms step_avg:98.84ms
step:1704/1750 train_time:168426ms step_avg:98.84ms
step:1705/1750 train_time:168530ms step_avg:98.84ms
step:1706/1750 train_time:168634ms step_avg:98.85ms
step:1707/1750 train_time:168739ms step_avg:98.85ms
step:1708/1750 train_time:168845ms step_avg:98.86ms
step:1709/1750 train_time:168949ms step_avg:98.86ms
step:1710/1750 train_time:169053ms step_avg:98.86ms
step:1711/1750 train_time:169158ms step_avg:98.87ms
step:1712/1750 train_time:169261ms step_avg:98.87ms
step:1713/1750 train_time:169366ms step_avg:98.87ms
step:1714/1750 train_time:169469ms step_avg:98.87ms
step:1715/1750 train_time:169575ms step_avg:98.88ms
step:1716/1750 train_time:169678ms step_avg:98.88ms
step:1717/1750 train_time:169782ms step_avg:98.88ms
step:1718/1750 train_time:169886ms step_avg:98.89ms
step:1719/1750 train_time:169993ms step_avg:98.89ms
step:1720/1750 train_time:170097ms step_avg:98.89ms
step:1721/1750 train_time:170201ms step_avg:98.90ms
step:1722/1750 train_time:170307ms step_avg:98.90ms
step:1723/1750 train_time:170410ms step_avg:98.90ms
step:1724/1750 train_time:170515ms step_avg:98.91ms
step:1725/1750 train_time:170620ms step_avg:98.91ms
step:1726/1750 train_time:170723ms step_avg:98.91ms
step:1727/1750 train_time:170827ms step_avg:98.92ms
step:1728/1750 train_time:170933ms step_avg:98.92ms
step:1729/1750 train_time:171037ms step_avg:98.92ms
step:1730/1750 train_time:171140ms step_avg:98.92ms
step:1731/1750 train_time:171245ms step_avg:98.93ms
step:1732/1750 train_time:171349ms step_avg:98.93ms
step:1733/1750 train_time:171453ms step_avg:98.93ms
step:1734/1750 train_time:171559ms step_avg:98.94ms
step:1735/1750 train_time:171662ms step_avg:98.94ms
step:1736/1750 train_time:171767ms step_avg:98.94ms
step:1737/1750 train_time:171872ms step_avg:98.95ms
step:1738/1750 train_time:171976ms step_avg:98.95ms
step:1739/1750 train_time:172079ms step_avg:98.95ms
step:1740/1750 train_time:172183ms step_avg:98.96ms
step:1741/1750 train_time:172292ms step_avg:98.96ms
step:1742/1750 train_time:172397ms step_avg:98.97ms
step:1743/1750 train_time:172503ms step_avg:98.97ms
step:1744/1750 train_time:172607ms step_avg:98.97ms
step:1745/1750 train_time:172711ms step_avg:98.97ms
step:1746/1750 train_time:172814ms step_avg:98.98ms
step:1747/1750 train_time:172918ms step_avg:98.98ms
step:1748/1750 train_time:173023ms step_avg:98.98ms
step:1749/1750 train_time:173126ms step_avg:98.99ms
step:1750/1750 train_time:173232ms step_avg:98.99ms
step:1750/1750 val_loss:3.2783 train_time:173331ms step_avg:99.05ms
peak memory allocated: 33277 MiB reserved: 48872 MiB

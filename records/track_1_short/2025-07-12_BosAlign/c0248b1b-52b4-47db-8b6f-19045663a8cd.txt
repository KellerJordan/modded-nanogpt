import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn, autocast
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
#import wandb

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)

        params = list(params)
        sizes = {p.shape for p in params}

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params,))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * self.world_size
            for base_i in range(0, len(params), self.world_size):
                if base_i + self.rank < len(params):
                    grad = params[base_i + self.rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + self.world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * self.world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), self.world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                futures.append(dist.all_gather(params_pad[base_i:base_i + self.world_size], params_pad[base_i + self.rank], async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01, rank: int = 0, world_size: int = 1):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        self.rank = rank
        self.world_size = world_size

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(
                params=group_params,
            ))
        super().__init__(param_groups, defaults)

    @torch.compile
    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // self.world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)

                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']

                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)

                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t

                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        for param in self.embed.parameters():
            param.lr_mul = 75.
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embeds in self.value_embeds:
            for param in self.value_embeds.parameters():
                param.lr_mul = 75.
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.lr_mul = 27.5
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % world_size
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            #return causal_mask
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, world_size: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos:pos+max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()

    starts = []
    batch_end=None
    for i in range(len(boundary_positions) - 1):
        end = boundary_positions[i + 1].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == world_size:
                batch_end = end
                break
            start = end
    assert batch_end is not None # increase max_batch_span if necessary
    batch_span = batch_end-pos
    return starts, batch_span

def distributed_data_generator(filename_pattern: str, batch_size: int, rank: int, world_size: int, align_to_bos: bool):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    batch_span = batch_size
    max_batch_span = 2*batch_size if align_to_bos else batch_size #provide buffer to handle samples up to length local_batch_size

    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, world_size, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    train_align_to_bos = True # align local batch start indicies with next bos_token
    val_align_to_bos = False # False to maintain same eval as prior records
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

#if master_process:
#    wandb.init(project="modded-nanogpt-tiny", name=f"run-{os.path.basename(__file__)}", save_code=True)

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=True):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
if master_process:
    print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=next_multiple_of_n(50257, n=128), num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094

optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0, rank=rank, world_size=world_size)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]

for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

for n, p in model.named_parameters():
    wd_mul = getattr(p, "wd_mul", 1.0)
    lr_mul = getattr(p, "lr_mul", 1.0)

    print0(f"{n}: {p.shape} {p.dtype} {wd_mul} {lr_mul}")

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
embedding_params = sum(p.numel() for n, p in model.named_parameters() if "embed" in n)
non_embedding_params = total_params - embedding_params

print0(f"")
print0(f"Model parameters:")
print0(f"  Total parameters: {total_params:,}")
print0(f"  Embedding parameters: {embedding_params:,}")
print0(f"  Non-embedding parameters: {non_embedding_params:,}")

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    w = min((1 - x) / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.05
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)

torch.cuda.synchronize()
dist.barrier()
with torch.profiler.profile() as prof:
    for _ in range(warmup_steps):
        inputs, targets = next(train_loader)
        model(inputs, targets, get_window_size_blocks(1)).backward()
        for opt in optimizers:
            opt.step()
    model.zero_grad(set_to_none=True)
    torch.cuda.synchronize()
    dist.barrier()
os.makedirs("traces", exist_ok=True)
prof.export_chrome_trace(f"traces/trace_{rank}.json")

model.load_state_dict(initial_state['model'])
for opt, opt_state in zip(optimizers, initial_state['optimizers']):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size, args.val_align_to_bos)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        #if master_process:
        #    wandb.log({"val/loss": val_loss}, step=step)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.8.0.dev20250524+cu126 compiled for CUDA 12.6
Sun Jul 13 00:55:12 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    5856MiB /  81559MiB |      3%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   37C    P0            121W /  700W |    1517MiB /  81559MiB |      2%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   38C    P0            124W /  700W |    1517MiB /  81559MiB |      3%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    1517MiB /  81559MiB |      4%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   39C    P0            118W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   38C    P0            123W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   31C    P0            115W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           65678      C   /usr/bin/python3                       1508MiB |
|    0   N/A  N/A           65679      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           65680      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           65681      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           65682      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           65683      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           65684      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           65685      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A           65679      C   /usr/bin/python3                       1508MiB |
|    2   N/A  N/A           65680      C   /usr/bin/python3                       1508MiB |
|    3   N/A  N/A           65681      C   /usr/bin/python3                       1508MiB |
|    4   N/A  N/A           65682      C   /usr/bin/python3                       1508MiB |
|    5   N/A  N/A           65683      C   /usr/bin/python3                       1508MiB |
|    6   N/A  N/A           65684      C   /usr/bin/python3                       1508MiB |
|    7   N/A  N/A           65685      C   /usr/bin/python3                       1508MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
scalars: torch.Size([64]) torch.float32 1.0 5.0
embed.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.0.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.1.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.2.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
blocks.0.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.0.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.1.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.1.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.2.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.2.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.3.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.3.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.4.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.4.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.5.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.5.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.6.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.6.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.7.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.7.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.8.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.8.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.9.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.9.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.10.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.10.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.11.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.11.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
lm_head.weight: torch.Size([50304, 768]) torch.float32 1.0 27.5

Model parameters:
  Total parameters: 275,742,784
  Embedding parameters: 154,533,888
  Non-embedding parameters: 121,208,896
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.01ms
step:1/1750 train_time:153ms step_avg:152.61ms
step:2/1750 train_time:177ms step_avg:88.53ms
step:3/1750 train_time:250ms step_avg:83.36ms
step:4/1750 train_time:341ms step_avg:85.32ms
step:5/1750 train_time:434ms step_avg:86.77ms
step:6/1750 train_time:526ms step_avg:87.73ms
step:7/1750 train_time:619ms step_avg:88.39ms
step:8/1750 train_time:711ms step_avg:88.93ms
step:9/1750 train_time:804ms step_avg:89.35ms
step:10/1750 train_time:896ms step_avg:89.64ms
step:11/1750 train_time:989ms step_avg:89.92ms
step:12/1750 train_time:1083ms step_avg:90.23ms
step:13/1750 train_time:1178ms step_avg:90.64ms
step:14/1750 train_time:1274ms step_avg:91.01ms
step:15/1750 train_time:1367ms step_avg:91.11ms
step:16/1750 train_time:1460ms step_avg:91.24ms
step:17/1750 train_time:1553ms step_avg:91.34ms
step:18/1750 train_time:1645ms step_avg:91.40ms
step:19/1750 train_time:1738ms step_avg:91.49ms
step:20/1750 train_time:1831ms step_avg:91.57ms
step:21/1750 train_time:1924ms step_avg:91.64ms
step:22/1750 train_time:2018ms step_avg:91.70ms
step:23/1750 train_time:2112ms step_avg:91.82ms
step:24/1750 train_time:2207ms step_avg:91.95ms
step:25/1750 train_time:2300ms step_avg:92.02ms
step:26/1750 train_time:2394ms step_avg:92.06ms
step:27/1750 train_time:2486ms step_avg:92.08ms
step:28/1750 train_time:2579ms step_avg:92.10ms
step:29/1750 train_time:2672ms step_avg:92.14ms
step:30/1750 train_time:2765ms step_avg:92.18ms
step:31/1750 train_time:2858ms step_avg:92.18ms
step:32/1750 train_time:2950ms step_avg:92.20ms
step:33/1750 train_time:3043ms step_avg:92.22ms
step:34/1750 train_time:3138ms step_avg:92.28ms
step:35/1750 train_time:3231ms step_avg:92.33ms
step:36/1750 train_time:3326ms step_avg:92.39ms
step:37/1750 train_time:3419ms step_avg:92.41ms
step:38/1750 train_time:3513ms step_avg:92.44ms
step:39/1750 train_time:3606ms step_avg:92.46ms
step:40/1750 train_time:3699ms step_avg:92.48ms
step:41/1750 train_time:3793ms step_avg:92.50ms
step:42/1750 train_time:3886ms step_avg:92.51ms
step:43/1750 train_time:3978ms step_avg:92.52ms
step:44/1750 train_time:4074ms step_avg:92.58ms
step:45/1750 train_time:4165ms step_avg:92.55ms
step:46/1750 train_time:4258ms step_avg:92.57ms
step:47/1750 train_time:4351ms step_avg:92.57ms
step:48/1750 train_time:4443ms step_avg:92.57ms
step:49/1750 train_time:4537ms step_avg:92.59ms
step:50/1750 train_time:4630ms step_avg:92.60ms
step:51/1750 train_time:4723ms step_avg:92.60ms
step:52/1750 train_time:4816ms step_avg:92.62ms
step:53/1750 train_time:4909ms step_avg:92.62ms
step:54/1750 train_time:5002ms step_avg:92.63ms
step:55/1750 train_time:5096ms step_avg:92.65ms
step:56/1750 train_time:5189ms step_avg:92.66ms
step:57/1750 train_time:5282ms step_avg:92.66ms
step:58/1750 train_time:5375ms step_avg:92.67ms
step:59/1750 train_time:5468ms step_avg:92.68ms
step:60/1750 train_time:5561ms step_avg:92.69ms
step:61/1750 train_time:5655ms step_avg:92.71ms
step:62/1750 train_time:5748ms step_avg:92.71ms
step:63/1750 train_time:5841ms step_avg:92.72ms
step:64/1750 train_time:5934ms step_avg:92.72ms
step:65/1750 train_time:6028ms step_avg:92.74ms
step:66/1750 train_time:6121ms step_avg:92.74ms
step:67/1750 train_time:6214ms step_avg:92.75ms
step:68/1750 train_time:6307ms step_avg:92.75ms
step:69/1750 train_time:6401ms step_avg:92.77ms
step:70/1750 train_time:6494ms step_avg:92.77ms
step:71/1750 train_time:6588ms step_avg:92.79ms
step:72/1750 train_time:6681ms step_avg:92.80ms
step:73/1750 train_time:6774ms step_avg:92.80ms
step:74/1750 train_time:6867ms step_avg:92.80ms
step:75/1750 train_time:6961ms step_avg:92.81ms
step:76/1750 train_time:7054ms step_avg:92.82ms
step:77/1750 train_time:7148ms step_avg:92.83ms
step:78/1750 train_time:7241ms step_avg:92.83ms
step:79/1750 train_time:7334ms step_avg:92.84ms
step:80/1750 train_time:7428ms step_avg:92.85ms
step:81/1750 train_time:7521ms step_avg:92.85ms
step:82/1750 train_time:7614ms step_avg:92.86ms
step:83/1750 train_time:7707ms step_avg:92.86ms
step:84/1750 train_time:7800ms step_avg:92.86ms
step:85/1750 train_time:7893ms step_avg:92.86ms
step:86/1750 train_time:7986ms step_avg:92.86ms
step:87/1750 train_time:8079ms step_avg:92.86ms
step:88/1750 train_time:8172ms step_avg:92.86ms
step:89/1750 train_time:8266ms step_avg:92.87ms
step:90/1750 train_time:8359ms step_avg:92.88ms
step:91/1750 train_time:8452ms step_avg:92.88ms
step:92/1750 train_time:8546ms step_avg:92.89ms
step:93/1750 train_time:8638ms step_avg:92.89ms
step:94/1750 train_time:8732ms step_avg:92.89ms
step:95/1750 train_time:8825ms step_avg:92.90ms
step:96/1750 train_time:8919ms step_avg:92.90ms
step:97/1750 train_time:9012ms step_avg:92.91ms
step:98/1750 train_time:9105ms step_avg:92.91ms
step:99/1750 train_time:9199ms step_avg:92.91ms
step:100/1750 train_time:9292ms step_avg:92.92ms
step:101/1750 train_time:9385ms step_avg:92.92ms
step:102/1750 train_time:9478ms step_avg:92.92ms
step:103/1750 train_time:9571ms step_avg:92.92ms
step:104/1750 train_time:9665ms step_avg:92.93ms
step:105/1750 train_time:9758ms step_avg:92.93ms
step:106/1750 train_time:9850ms step_avg:92.93ms
step:107/1750 train_time:9943ms step_avg:92.93ms
step:108/1750 train_time:10037ms step_avg:92.94ms
step:109/1750 train_time:10131ms step_avg:92.94ms
step:110/1750 train_time:10224ms step_avg:92.95ms
step:111/1750 train_time:10318ms step_avg:92.95ms
step:112/1750 train_time:10411ms step_avg:92.96ms
step:113/1750 train_time:10504ms step_avg:92.96ms
step:114/1750 train_time:10597ms step_avg:92.96ms
step:115/1750 train_time:10690ms step_avg:92.96ms
step:116/1750 train_time:10783ms step_avg:92.96ms
step:117/1750 train_time:10877ms step_avg:92.96ms
step:118/1750 train_time:10970ms step_avg:92.97ms
step:119/1750 train_time:11063ms step_avg:92.97ms
step:120/1750 train_time:11157ms step_avg:92.98ms
step:121/1750 train_time:11251ms step_avg:92.98ms
step:122/1750 train_time:11343ms step_avg:92.98ms
step:123/1750 train_time:11437ms step_avg:92.99ms
step:124/1750 train_time:11531ms step_avg:92.99ms
step:125/1750 train_time:11624ms step_avg:92.99ms
step:125/1750 val_loss:4.6450 train_time:11712ms step_avg:93.70ms
step:126/1750 train_time:11737ms step_avg:93.15ms
step:127/1750 train_time:11816ms step_avg:93.04ms
step:128/1750 train_time:11915ms step_avg:93.08ms
step:129/1750 train_time:12009ms step_avg:93.09ms
step:130/1750 train_time:12101ms step_avg:93.09ms
step:131/1750 train_time:12194ms step_avg:93.08ms
step:132/1750 train_time:12287ms step_avg:93.08ms
step:133/1750 train_time:12379ms step_avg:93.08ms
step:134/1750 train_time:12472ms step_avg:93.07ms
step:135/1750 train_time:12565ms step_avg:93.07ms
step:136/1750 train_time:12658ms step_avg:93.07ms
step:137/1750 train_time:12751ms step_avg:93.08ms
step:138/1750 train_time:12847ms step_avg:93.09ms
step:139/1750 train_time:12942ms step_avg:93.11ms
step:140/1750 train_time:13037ms step_avg:93.12ms
step:141/1750 train_time:13130ms step_avg:93.12ms
step:142/1750 train_time:13224ms step_avg:93.12ms
step:143/1750 train_time:13317ms step_avg:93.13ms
step:144/1750 train_time:13411ms step_avg:93.13ms
step:145/1750 train_time:13503ms step_avg:93.13ms
step:146/1750 train_time:13596ms step_avg:93.13ms
step:147/1750 train_time:13690ms step_avg:93.13ms
step:148/1750 train_time:13784ms step_avg:93.13ms
step:149/1750 train_time:13878ms step_avg:93.14ms
step:150/1750 train_time:13972ms step_avg:93.15ms
step:151/1750 train_time:14067ms step_avg:93.16ms
step:152/1750 train_time:14160ms step_avg:93.16ms
step:153/1750 train_time:14254ms step_avg:93.17ms
step:154/1750 train_time:14348ms step_avg:93.17ms
step:155/1750 train_time:14442ms step_avg:93.18ms
step:156/1750 train_time:14536ms step_avg:93.18ms
step:157/1750 train_time:14629ms step_avg:93.18ms
step:158/1750 train_time:14723ms step_avg:93.18ms
step:159/1750 train_time:14817ms step_avg:93.19ms
step:160/1750 train_time:14911ms step_avg:93.19ms
step:161/1750 train_time:15004ms step_avg:93.19ms
step:162/1750 train_time:15098ms step_avg:93.20ms
step:163/1750 train_time:15191ms step_avg:93.20ms
step:164/1750 train_time:15285ms step_avg:93.20ms
step:165/1750 train_time:15379ms step_avg:93.21ms
step:166/1750 train_time:15472ms step_avg:93.21ms
step:167/1750 train_time:15565ms step_avg:93.21ms
step:168/1750 train_time:15659ms step_avg:93.21ms
step:169/1750 train_time:15753ms step_avg:93.21ms
step:170/1750 train_time:15846ms step_avg:93.21ms
step:171/1750 train_time:15940ms step_avg:93.22ms
step:172/1750 train_time:16034ms step_avg:93.22ms
step:173/1750 train_time:16128ms step_avg:93.22ms
step:174/1750 train_time:16221ms step_avg:93.22ms
step:175/1750 train_time:16315ms step_avg:93.23ms
step:176/1750 train_time:16409ms step_avg:93.23ms
step:177/1750 train_time:16502ms step_avg:93.23ms
step:178/1750 train_time:16596ms step_avg:93.23ms
step:179/1750 train_time:16689ms step_avg:93.23ms
step:180/1750 train_time:16782ms step_avg:93.23ms
step:181/1750 train_time:16876ms step_avg:93.24ms
step:182/1750 train_time:16969ms step_avg:93.24ms
step:183/1750 train_time:17063ms step_avg:93.24ms
step:184/1750 train_time:17158ms step_avg:93.25ms
step:185/1750 train_time:17251ms step_avg:93.25ms
step:186/1750 train_time:17345ms step_avg:93.25ms
step:187/1750 train_time:17438ms step_avg:93.25ms
step:188/1750 train_time:17532ms step_avg:93.25ms
step:189/1750 train_time:17625ms step_avg:93.26ms
step:190/1750 train_time:17719ms step_avg:93.26ms
step:191/1750 train_time:17813ms step_avg:93.26ms
step:192/1750 train_time:17907ms step_avg:93.26ms
step:193/1750 train_time:18000ms step_avg:93.26ms
step:194/1750 train_time:18094ms step_avg:93.27ms
step:195/1750 train_time:18188ms step_avg:93.27ms
step:196/1750 train_time:18281ms step_avg:93.27ms
step:197/1750 train_time:18375ms step_avg:93.27ms
step:198/1750 train_time:18469ms step_avg:93.28ms
step:199/1750 train_time:18563ms step_avg:93.28ms
step:200/1750 train_time:18657ms step_avg:93.28ms
step:201/1750 train_time:18751ms step_avg:93.29ms
step:202/1750 train_time:18845ms step_avg:93.29ms
step:203/1750 train_time:18938ms step_avg:93.29ms
step:204/1750 train_time:19032ms step_avg:93.29ms
step:205/1750 train_time:19126ms step_avg:93.30ms
step:206/1750 train_time:19219ms step_avg:93.30ms
step:207/1750 train_time:19313ms step_avg:93.30ms
step:208/1750 train_time:19407ms step_avg:93.30ms
step:209/1750 train_time:19500ms step_avg:93.30ms
step:210/1750 train_time:19593ms step_avg:93.30ms
step:211/1750 train_time:19687ms step_avg:93.30ms
step:212/1750 train_time:19781ms step_avg:93.31ms
step:213/1750 train_time:19874ms step_avg:93.31ms
step:214/1750 train_time:19969ms step_avg:93.31ms
step:215/1750 train_time:20063ms step_avg:93.31ms
step:216/1750 train_time:20156ms step_avg:93.32ms
step:217/1750 train_time:20250ms step_avg:93.32ms
step:218/1750 train_time:20344ms step_avg:93.32ms
step:219/1750 train_time:20437ms step_avg:93.32ms
step:220/1750 train_time:20530ms step_avg:93.32ms
step:221/1750 train_time:20625ms step_avg:93.32ms
step:222/1750 train_time:20719ms step_avg:93.33ms
step:223/1750 train_time:20812ms step_avg:93.33ms
step:224/1750 train_time:20905ms step_avg:93.33ms
step:225/1750 train_time:20999ms step_avg:93.33ms
step:226/1750 train_time:21093ms step_avg:93.33ms
step:227/1750 train_time:21187ms step_avg:93.33ms
step:228/1750 train_time:21280ms step_avg:93.33ms
step:229/1750 train_time:21374ms step_avg:93.34ms
step:230/1750 train_time:21467ms step_avg:93.34ms
step:231/1750 train_time:21561ms step_avg:93.34ms
step:232/1750 train_time:21655ms step_avg:93.34ms
step:233/1750 train_time:21749ms step_avg:93.34ms
step:234/1750 train_time:21842ms step_avg:93.34ms
step:235/1750 train_time:21936ms step_avg:93.34ms
step:236/1750 train_time:22030ms step_avg:93.35ms
step:237/1750 train_time:22123ms step_avg:93.35ms
step:238/1750 train_time:22217ms step_avg:93.35ms
step:239/1750 train_time:22310ms step_avg:93.35ms
step:240/1750 train_time:22403ms step_avg:93.35ms
step:241/1750 train_time:22497ms step_avg:93.35ms
step:242/1750 train_time:22591ms step_avg:93.35ms
step:243/1750 train_time:22687ms step_avg:93.36ms
step:244/1750 train_time:22779ms step_avg:93.36ms
step:245/1750 train_time:22872ms step_avg:93.35ms
step:246/1750 train_time:22966ms step_avg:93.36ms
step:247/1750 train_time:23060ms step_avg:93.36ms
step:248/1750 train_time:23153ms step_avg:93.36ms
step:249/1750 train_time:23247ms step_avg:93.36ms
step:250/1750 train_time:23341ms step_avg:93.36ms
step:250/1750 val_loss:4.1034 train_time:23430ms step_avg:93.72ms
step:251/1750 train_time:23455ms step_avg:93.44ms
step:252/1750 train_time:23535ms step_avg:93.39ms
step:253/1750 train_time:23632ms step_avg:93.41ms
step:254/1750 train_time:23727ms step_avg:93.41ms
step:255/1750 train_time:23820ms step_avg:93.41ms
step:256/1750 train_time:23913ms step_avg:93.41ms
step:257/1750 train_time:24006ms step_avg:93.41ms
step:258/1750 train_time:24099ms step_avg:93.41ms
step:259/1750 train_time:24191ms step_avg:93.40ms
step:260/1750 train_time:24285ms step_avg:93.40ms
step:261/1750 train_time:24378ms step_avg:93.40ms
step:262/1750 train_time:24472ms step_avg:93.41ms
step:263/1750 train_time:24568ms step_avg:93.42ms
step:264/1750 train_time:24663ms step_avg:93.42ms
step:265/1750 train_time:24758ms step_avg:93.43ms
step:266/1750 train_time:24852ms step_avg:93.43ms
step:267/1750 train_time:24946ms step_avg:93.43ms
step:268/1750 train_time:25039ms step_avg:93.43ms
step:269/1750 train_time:25133ms step_avg:93.43ms
step:270/1750 train_time:25226ms step_avg:93.43ms
step:271/1750 train_time:25320ms step_avg:93.43ms
step:272/1750 train_time:25414ms step_avg:93.43ms
step:273/1750 train_time:25508ms step_avg:93.44ms
step:274/1750 train_time:25603ms step_avg:93.44ms
step:275/1750 train_time:25697ms step_avg:93.44ms
step:276/1750 train_time:25791ms step_avg:93.45ms
step:277/1750 train_time:25885ms step_avg:93.45ms
step:278/1750 train_time:25979ms step_avg:93.45ms
step:279/1750 train_time:26074ms step_avg:93.45ms
step:280/1750 train_time:26168ms step_avg:93.46ms
step:281/1750 train_time:26262ms step_avg:93.46ms
step:282/1750 train_time:26356ms step_avg:93.46ms
step:283/1750 train_time:26450ms step_avg:93.46ms
step:284/1750 train_time:26544ms step_avg:93.47ms
step:285/1750 train_time:26638ms step_avg:93.47ms
step:286/1750 train_time:26732ms step_avg:93.47ms
step:287/1750 train_time:26827ms step_avg:93.47ms
step:288/1750 train_time:26921ms step_avg:93.48ms
step:289/1750 train_time:27015ms step_avg:93.48ms
step:290/1750 train_time:27109ms step_avg:93.48ms
step:291/1750 train_time:27203ms step_avg:93.48ms
step:292/1750 train_time:27298ms step_avg:93.48ms
step:293/1750 train_time:27392ms step_avg:93.49ms
step:294/1750 train_time:27486ms step_avg:93.49ms
step:295/1750 train_time:27580ms step_avg:93.49ms
step:296/1750 train_time:27674ms step_avg:93.49ms
step:297/1750 train_time:27769ms step_avg:93.50ms
step:298/1750 train_time:27863ms step_avg:93.50ms
step:299/1750 train_time:27958ms step_avg:93.50ms
step:300/1750 train_time:28052ms step_avg:93.51ms
step:301/1750 train_time:28147ms step_avg:93.51ms
step:302/1750 train_time:28241ms step_avg:93.51ms
step:303/1750 train_time:28335ms step_avg:93.52ms
step:304/1750 train_time:28430ms step_avg:93.52ms
step:305/1750 train_time:28524ms step_avg:93.52ms
step:306/1750 train_time:28618ms step_avg:93.52ms
step:307/1750 train_time:28712ms step_avg:93.52ms
step:308/1750 train_time:28806ms step_avg:93.53ms
step:309/1750 train_time:28900ms step_avg:93.53ms
step:310/1750 train_time:28994ms step_avg:93.53ms
step:311/1750 train_time:29089ms step_avg:93.53ms
step:312/1750 train_time:29184ms step_avg:93.54ms
step:313/1750 train_time:29279ms step_avg:93.54ms
step:314/1750 train_time:29373ms step_avg:93.54ms
step:315/1750 train_time:29467ms step_avg:93.54ms
step:316/1750 train_time:29561ms step_avg:93.55ms
step:317/1750 train_time:29655ms step_avg:93.55ms
step:318/1750 train_time:29749ms step_avg:93.55ms
step:319/1750 train_time:29843ms step_avg:93.55ms
step:320/1750 train_time:29937ms step_avg:93.55ms
step:321/1750 train_time:30031ms step_avg:93.55ms
step:322/1750 train_time:30124ms step_avg:93.55ms
step:323/1750 train_time:30219ms step_avg:93.56ms
step:324/1750 train_time:30312ms step_avg:93.56ms
step:325/1750 train_time:30407ms step_avg:93.56ms
step:326/1750 train_time:30501ms step_avg:93.56ms
step:327/1750 train_time:30595ms step_avg:93.56ms
step:328/1750 train_time:30690ms step_avg:93.57ms
step:329/1750 train_time:30784ms step_avg:93.57ms
step:330/1750 train_time:30878ms step_avg:93.57ms
step:331/1750 train_time:30972ms step_avg:93.57ms
step:332/1750 train_time:31066ms step_avg:93.57ms
step:333/1750 train_time:31160ms step_avg:93.57ms
step:334/1750 train_time:31254ms step_avg:93.58ms
step:335/1750 train_time:31349ms step_avg:93.58ms
step:336/1750 train_time:31443ms step_avg:93.58ms
step:337/1750 train_time:31537ms step_avg:93.58ms
step:338/1750 train_time:31632ms step_avg:93.59ms
step:339/1750 train_time:31727ms step_avg:93.59ms
step:340/1750 train_time:31821ms step_avg:93.59ms
step:341/1750 train_time:31915ms step_avg:93.59ms
step:342/1750 train_time:32009ms step_avg:93.59ms
step:343/1750 train_time:32103ms step_avg:93.59ms
step:344/1750 train_time:32197ms step_avg:93.60ms
step:345/1750 train_time:32290ms step_avg:93.60ms
step:346/1750 train_time:32385ms step_avg:93.60ms
step:347/1750 train_time:32479ms step_avg:93.60ms
step:348/1750 train_time:32573ms step_avg:93.60ms
step:349/1750 train_time:32667ms step_avg:93.60ms
step:350/1750 train_time:32761ms step_avg:93.60ms
step:351/1750 train_time:32855ms step_avg:93.61ms
step:352/1750 train_time:32950ms step_avg:93.61ms
step:353/1750 train_time:33043ms step_avg:93.61ms
step:354/1750 train_time:33138ms step_avg:93.61ms
step:355/1750 train_time:33232ms step_avg:93.61ms
step:356/1750 train_time:33325ms step_avg:93.61ms
step:357/1750 train_time:33419ms step_avg:93.61ms
step:358/1750 train_time:33513ms step_avg:93.61ms
step:359/1750 train_time:33608ms step_avg:93.62ms
step:360/1750 train_time:33702ms step_avg:93.62ms
step:361/1750 train_time:33796ms step_avg:93.62ms
step:362/1750 train_time:33890ms step_avg:93.62ms
step:363/1750 train_time:33984ms step_avg:93.62ms
step:364/1750 train_time:34079ms step_avg:93.62ms
step:365/1750 train_time:34173ms step_avg:93.62ms
step:366/1750 train_time:34267ms step_avg:93.62ms
step:367/1750 train_time:34360ms step_avg:93.62ms
step:368/1750 train_time:34454ms step_avg:93.62ms
step:369/1750 train_time:34549ms step_avg:93.63ms
step:370/1750 train_time:34643ms step_avg:93.63ms
step:371/1750 train_time:34738ms step_avg:93.63ms
step:372/1750 train_time:34832ms step_avg:93.63ms
step:373/1750 train_time:34926ms step_avg:93.64ms
step:374/1750 train_time:35021ms step_avg:93.64ms
step:375/1750 train_time:35115ms step_avg:93.64ms
step:375/1750 val_loss:3.9064 train_time:35204ms step_avg:93.88ms
step:376/1750 train_time:35229ms step_avg:93.69ms
step:377/1750 train_time:35310ms step_avg:93.66ms
step:378/1750 train_time:35407ms step_avg:93.67ms
step:379/1750 train_time:35503ms step_avg:93.68ms
step:380/1750 train_time:35597ms step_avg:93.68ms
step:381/1750 train_time:35690ms step_avg:93.67ms
step:382/1750 train_time:35784ms step_avg:93.67ms
step:383/1750 train_time:35877ms step_avg:93.67ms
step:384/1750 train_time:35971ms step_avg:93.67ms
step:385/1750 train_time:36065ms step_avg:93.68ms
step:386/1750 train_time:36159ms step_avg:93.68ms
step:387/1750 train_time:36254ms step_avg:93.68ms
step:388/1750 train_time:36350ms step_avg:93.69ms
step:389/1750 train_time:36445ms step_avg:93.69ms
step:390/1750 train_time:36539ms step_avg:93.69ms
step:391/1750 train_time:36635ms step_avg:93.70ms
step:392/1750 train_time:36731ms step_avg:93.70ms
step:393/1750 train_time:36827ms step_avg:93.71ms
step:394/1750 train_time:36923ms step_avg:93.71ms
step:395/1750 train_time:37019ms step_avg:93.72ms
step:396/1750 train_time:37114ms step_avg:93.72ms
step:397/1750 train_time:37211ms step_avg:93.73ms
step:398/1750 train_time:37308ms step_avg:93.74ms
step:399/1750 train_time:37406ms step_avg:93.75ms
step:400/1750 train_time:37503ms step_avg:93.76ms
step:401/1750 train_time:37599ms step_avg:93.76ms
step:402/1750 train_time:37695ms step_avg:93.77ms
step:403/1750 train_time:37791ms step_avg:93.77ms
step:404/1750 train_time:37887ms step_avg:93.78ms
step:405/1750 train_time:37983ms step_avg:93.79ms
step:406/1750 train_time:38080ms step_avg:93.79ms
step:407/1750 train_time:38176ms step_avg:93.80ms
step:408/1750 train_time:38272ms step_avg:93.80ms
step:409/1750 train_time:38368ms step_avg:93.81ms
step:410/1750 train_time:38465ms step_avg:93.82ms
step:411/1750 train_time:38562ms step_avg:93.82ms
step:412/1750 train_time:38658ms step_avg:93.83ms
step:413/1750 train_time:38755ms step_avg:93.84ms
step:414/1750 train_time:38851ms step_avg:93.84ms
step:415/1750 train_time:38947ms step_avg:93.85ms
step:416/1750 train_time:39043ms step_avg:93.85ms
step:417/1750 train_time:39139ms step_avg:93.86ms
step:418/1750 train_time:39235ms step_avg:93.86ms
step:419/1750 train_time:39332ms step_avg:93.87ms
step:420/1750 train_time:39429ms step_avg:93.88ms
step:421/1750 train_time:39526ms step_avg:93.89ms
step:422/1750 train_time:39623ms step_avg:93.89ms
step:423/1750 train_time:39719ms step_avg:93.90ms
step:424/1750 train_time:39815ms step_avg:93.90ms
step:425/1750 train_time:39910ms step_avg:93.91ms
step:426/1750 train_time:40006ms step_avg:93.91ms
step:427/1750 train_time:40103ms step_avg:93.92ms
step:428/1750 train_time:40200ms step_avg:93.92ms
step:429/1750 train_time:40296ms step_avg:93.93ms
step:430/1750 train_time:40393ms step_avg:93.94ms
step:431/1750 train_time:40489ms step_avg:93.94ms
step:432/1750 train_time:40586ms step_avg:93.95ms
step:433/1750 train_time:40683ms step_avg:93.96ms
step:434/1750 train_time:40779ms step_avg:93.96ms
step:435/1750 train_time:40876ms step_avg:93.97ms
step:436/1750 train_time:40973ms step_avg:93.97ms
step:437/1750 train_time:41068ms step_avg:93.98ms
step:438/1750 train_time:41165ms step_avg:93.98ms
step:439/1750 train_time:41263ms step_avg:93.99ms
step:440/1750 train_time:41359ms step_avg:94.00ms
step:441/1750 train_time:41455ms step_avg:94.00ms
step:442/1750 train_time:41552ms step_avg:94.01ms
step:443/1750 train_time:41649ms step_avg:94.01ms
step:444/1750 train_time:41745ms step_avg:94.02ms
step:445/1750 train_time:41842ms step_avg:94.03ms
step:446/1750 train_time:41939ms step_avg:94.03ms
step:447/1750 train_time:42035ms step_avg:94.04ms
step:448/1750 train_time:42132ms step_avg:94.04ms
step:449/1750 train_time:42228ms step_avg:94.05ms
step:450/1750 train_time:42325ms step_avg:94.06ms
step:451/1750 train_time:42422ms step_avg:94.06ms
step:452/1750 train_time:42518ms step_avg:94.07ms
step:453/1750 train_time:42614ms step_avg:94.07ms
step:454/1750 train_time:42711ms step_avg:94.08ms
step:455/1750 train_time:42807ms step_avg:94.08ms
step:456/1750 train_time:42905ms step_avg:94.09ms
step:457/1750 train_time:43001ms step_avg:94.09ms
step:458/1750 train_time:43097ms step_avg:94.10ms
step:459/1750 train_time:43194ms step_avg:94.10ms
step:460/1750 train_time:43290ms step_avg:94.11ms
step:461/1750 train_time:43387ms step_avg:94.11ms
step:462/1750 train_time:43483ms step_avg:94.12ms
step:463/1750 train_time:43580ms step_avg:94.12ms
step:464/1750 train_time:43676ms step_avg:94.13ms
step:465/1750 train_time:43772ms step_avg:94.13ms
step:466/1750 train_time:43869ms step_avg:94.14ms
step:467/1750 train_time:43966ms step_avg:94.15ms
step:468/1750 train_time:44064ms step_avg:94.15ms
step:469/1750 train_time:44160ms step_avg:94.16ms
step:470/1750 train_time:44256ms step_avg:94.16ms
step:471/1750 train_time:44353ms step_avg:94.17ms
step:472/1750 train_time:44449ms step_avg:94.17ms
step:473/1750 train_time:44545ms step_avg:94.18ms
step:474/1750 train_time:44642ms step_avg:94.18ms
step:475/1750 train_time:44738ms step_avg:94.19ms
step:476/1750 train_time:44834ms step_avg:94.19ms
step:477/1750 train_time:44932ms step_avg:94.20ms
step:478/1750 train_time:45028ms step_avg:94.20ms
step:479/1750 train_time:45125ms step_avg:94.21ms
step:480/1750 train_time:45221ms step_avg:94.21ms
step:481/1750 train_time:45316ms step_avg:94.21ms
step:482/1750 train_time:45413ms step_avg:94.22ms
step:483/1750 train_time:45509ms step_avg:94.22ms
step:484/1750 train_time:45606ms step_avg:94.23ms
step:485/1750 train_time:45704ms step_avg:94.23ms
step:486/1750 train_time:45800ms step_avg:94.24ms
step:487/1750 train_time:45896ms step_avg:94.24ms
step:488/1750 train_time:45992ms step_avg:94.25ms
step:489/1750 train_time:46089ms step_avg:94.25ms
step:490/1750 train_time:46185ms step_avg:94.26ms
step:491/1750 train_time:46281ms step_avg:94.26ms
step:492/1750 train_time:46378ms step_avg:94.26ms
step:493/1750 train_time:46474ms step_avg:94.27ms
step:494/1750 train_time:46570ms step_avg:94.27ms
step:495/1750 train_time:46666ms step_avg:94.28ms
step:496/1750 train_time:46763ms step_avg:94.28ms
step:497/1750 train_time:46859ms step_avg:94.28ms
step:498/1750 train_time:46955ms step_avg:94.29ms
step:499/1750 train_time:47053ms step_avg:94.29ms
step:500/1750 train_time:47150ms step_avg:94.30ms
step:500/1750 val_loss:3.7541 train_time:47241ms step_avg:94.48ms
step:501/1750 train_time:47266ms step_avg:94.34ms
step:502/1750 train_time:47352ms step_avg:94.33ms
step:503/1750 train_time:47450ms step_avg:94.33ms
step:504/1750 train_time:47547ms step_avg:94.34ms
step:505/1750 train_time:47644ms step_avg:94.34ms
step:506/1750 train_time:47739ms step_avg:94.35ms
step:507/1750 train_time:47835ms step_avg:94.35ms
step:508/1750 train_time:47930ms step_avg:94.35ms
step:509/1750 train_time:48026ms step_avg:94.35ms
step:510/1750 train_time:48121ms step_avg:94.36ms
step:511/1750 train_time:48217ms step_avg:94.36ms
step:512/1750 train_time:48314ms step_avg:94.36ms
step:513/1750 train_time:48412ms step_avg:94.37ms
step:514/1750 train_time:48509ms step_avg:94.38ms
step:515/1750 train_time:48606ms step_avg:94.38ms
step:516/1750 train_time:48702ms step_avg:94.38ms
step:517/1750 train_time:48798ms step_avg:94.39ms
step:518/1750 train_time:48893ms step_avg:94.39ms
step:519/1750 train_time:48989ms step_avg:94.39ms
step:520/1750 train_time:49086ms step_avg:94.40ms
step:521/1750 train_time:49182ms step_avg:94.40ms
step:522/1750 train_time:49278ms step_avg:94.40ms
step:523/1750 train_time:49375ms step_avg:94.41ms
step:524/1750 train_time:49473ms step_avg:94.41ms
step:525/1750 train_time:49570ms step_avg:94.42ms
step:526/1750 train_time:49667ms step_avg:94.42ms
step:527/1750 train_time:49765ms step_avg:94.43ms
step:528/1750 train_time:49861ms step_avg:94.43ms
step:529/1750 train_time:49957ms step_avg:94.44ms
step:530/1750 train_time:50053ms step_avg:94.44ms
step:531/1750 train_time:50150ms step_avg:94.44ms
step:532/1750 train_time:50246ms step_avg:94.45ms
step:533/1750 train_time:50343ms step_avg:94.45ms
step:534/1750 train_time:50439ms step_avg:94.46ms
step:535/1750 train_time:50536ms step_avg:94.46ms
step:536/1750 train_time:50634ms step_avg:94.47ms
step:537/1750 train_time:50731ms step_avg:94.47ms
step:538/1750 train_time:50827ms step_avg:94.47ms
step:539/1750 train_time:50924ms step_avg:94.48ms
step:540/1750 train_time:51021ms step_avg:94.48ms
step:541/1750 train_time:51117ms step_avg:94.49ms
step:542/1750 train_time:51214ms step_avg:94.49ms
step:543/1750 train_time:51310ms step_avg:94.49ms
step:544/1750 train_time:51407ms step_avg:94.50ms
step:545/1750 train_time:51505ms step_avg:94.50ms
step:546/1750 train_time:51601ms step_avg:94.51ms
step:547/1750 train_time:51698ms step_avg:94.51ms
step:548/1750 train_time:51796ms step_avg:94.52ms
step:549/1750 train_time:51892ms step_avg:94.52ms
step:550/1750 train_time:51989ms step_avg:94.53ms
step:551/1750 train_time:52086ms step_avg:94.53ms
step:552/1750 train_time:52183ms step_avg:94.53ms
step:553/1750 train_time:52280ms step_avg:94.54ms
step:554/1750 train_time:52376ms step_avg:94.54ms
step:555/1750 train_time:52472ms step_avg:94.54ms
step:556/1750 train_time:52569ms step_avg:94.55ms
step:557/1750 train_time:52666ms step_avg:94.55ms
step:558/1750 train_time:52764ms step_avg:94.56ms
step:559/1750 train_time:52861ms step_avg:94.56ms
step:560/1750 train_time:52958ms step_avg:94.57ms
step:561/1750 train_time:53055ms step_avg:94.57ms
step:562/1750 train_time:53152ms step_avg:94.58ms
step:563/1750 train_time:53248ms step_avg:94.58ms
step:564/1750 train_time:53345ms step_avg:94.58ms
step:565/1750 train_time:53442ms step_avg:94.59ms
step:566/1750 train_time:53539ms step_avg:94.59ms
step:567/1750 train_time:53636ms step_avg:94.60ms
step:568/1750 train_time:53733ms step_avg:94.60ms
step:569/1750 train_time:53829ms step_avg:94.60ms
step:570/1750 train_time:53926ms step_avg:94.61ms
step:571/1750 train_time:54023ms step_avg:94.61ms
step:572/1750 train_time:54119ms step_avg:94.61ms
step:573/1750 train_time:54216ms step_avg:94.62ms
step:574/1750 train_time:54312ms step_avg:94.62ms
step:575/1750 train_time:54409ms step_avg:94.62ms
step:576/1750 train_time:54506ms step_avg:94.63ms
step:577/1750 train_time:54603ms step_avg:94.63ms
step:578/1750 train_time:54700ms step_avg:94.64ms
step:579/1750 train_time:54797ms step_avg:94.64ms
step:580/1750 train_time:54894ms step_avg:94.64ms
step:581/1750 train_time:54990ms step_avg:94.65ms
step:582/1750 train_time:55088ms step_avg:94.65ms
step:583/1750 train_time:55184ms step_avg:94.66ms
step:584/1750 train_time:55281ms step_avg:94.66ms
step:585/1750 train_time:55378ms step_avg:94.66ms
step:586/1750 train_time:55475ms step_avg:94.67ms
step:587/1750 train_time:55572ms step_avg:94.67ms
step:588/1750 train_time:55669ms step_avg:94.68ms
step:589/1750 train_time:55766ms step_avg:94.68ms
step:590/1750 train_time:55864ms step_avg:94.68ms
step:591/1750 train_time:55960ms step_avg:94.69ms
step:592/1750 train_time:56057ms step_avg:94.69ms
step:593/1750 train_time:56153ms step_avg:94.69ms
step:594/1750 train_time:56250ms step_avg:94.70ms
step:595/1750 train_time:56346ms step_avg:94.70ms
step:596/1750 train_time:56443ms step_avg:94.70ms
step:597/1750 train_time:56540ms step_avg:94.71ms
step:598/1750 train_time:56636ms step_avg:94.71ms
step:599/1750 train_time:56733ms step_avg:94.71ms
step:600/1750 train_time:56830ms step_avg:94.72ms
step:601/1750 train_time:56927ms step_avg:94.72ms
step:602/1750 train_time:57023ms step_avg:94.72ms
step:603/1750 train_time:57120ms step_avg:94.73ms
step:604/1750 train_time:57216ms step_avg:94.73ms
step:605/1750 train_time:57313ms step_avg:94.73ms
step:606/1750 train_time:57410ms step_avg:94.74ms
step:607/1750 train_time:57506ms step_avg:94.74ms
step:608/1750 train_time:57603ms step_avg:94.74ms
step:609/1750 train_time:57699ms step_avg:94.74ms
step:610/1750 train_time:57797ms step_avg:94.75ms
step:611/1750 train_time:57894ms step_avg:94.75ms
step:612/1750 train_time:57991ms step_avg:94.76ms
step:613/1750 train_time:58088ms step_avg:94.76ms
step:614/1750 train_time:58185ms step_avg:94.76ms
step:615/1750 train_time:58283ms step_avg:94.77ms
step:616/1750 train_time:58379ms step_avg:94.77ms
step:617/1750 train_time:58476ms step_avg:94.77ms
step:618/1750 train_time:58573ms step_avg:94.78ms
step:619/1750 train_time:58669ms step_avg:94.78ms
step:620/1750 train_time:58766ms step_avg:94.78ms
step:621/1750 train_time:58863ms step_avg:94.79ms
step:622/1750 train_time:58960ms step_avg:94.79ms
step:623/1750 train_time:59057ms step_avg:94.80ms
step:624/1750 train_time:59154ms step_avg:94.80ms
step:625/1750 train_time:59250ms step_avg:94.80ms
step:625/1750 val_loss:3.6645 train_time:59342ms step_avg:94.95ms
step:626/1750 train_time:59367ms step_avg:94.84ms
step:627/1750 train_time:59451ms step_avg:94.82ms
step:628/1750 train_time:59549ms step_avg:94.82ms
step:629/1750 train_time:59646ms step_avg:94.83ms
step:630/1750 train_time:59742ms step_avg:94.83ms
step:631/1750 train_time:59838ms step_avg:94.83ms
step:632/1750 train_time:59934ms step_avg:94.83ms
step:633/1750 train_time:60029ms step_avg:94.83ms
step:634/1750 train_time:60125ms step_avg:94.83ms
step:635/1750 train_time:60221ms step_avg:94.84ms
step:636/1750 train_time:60317ms step_avg:94.84ms
step:637/1750 train_time:60415ms step_avg:94.84ms
step:638/1750 train_time:60512ms step_avg:94.85ms
step:639/1750 train_time:60609ms step_avg:94.85ms
step:640/1750 train_time:60706ms step_avg:94.85ms
step:641/1750 train_time:60803ms step_avg:94.86ms
step:642/1750 train_time:60899ms step_avg:94.86ms
step:643/1750 train_time:60996ms step_avg:94.86ms
step:644/1750 train_time:61092ms step_avg:94.86ms
step:645/1750 train_time:61189ms step_avg:94.87ms
step:646/1750 train_time:61286ms step_avg:94.87ms
step:647/1750 train_time:61383ms step_avg:94.87ms
step:648/1750 train_time:61480ms step_avg:94.88ms
step:649/1750 train_time:61578ms step_avg:94.88ms
step:650/1750 train_time:61675ms step_avg:94.89ms
step:651/1750 train_time:61774ms step_avg:94.89ms
step:652/1750 train_time:61872ms step_avg:94.90ms
step:653/1750 train_time:61970ms step_avg:94.90ms
step:654/1750 train_time:62067ms step_avg:94.90ms
step:655/1750 train_time:62165ms step_avg:94.91ms
step:656/1750 train_time:62263ms step_avg:94.91ms
step:657/1750 train_time:62361ms step_avg:94.92ms
step:658/1750 train_time:62459ms step_avg:94.92ms
step:659/1750 train_time:62558ms step_avg:94.93ms
step:660/1750 train_time:62656ms step_avg:94.93ms
step:661/1750 train_time:62754ms step_avg:94.94ms
step:662/1750 train_time:62853ms step_avg:94.94ms
step:663/1750 train_time:62952ms step_avg:94.95ms
step:664/1750 train_time:63050ms step_avg:94.95ms
step:665/1750 train_time:63148ms step_avg:94.96ms
step:666/1750 train_time:63246ms step_avg:94.96ms
step:667/1750 train_time:63344ms step_avg:94.97ms
step:668/1750 train_time:63442ms step_avg:94.97ms
step:669/1750 train_time:63540ms step_avg:94.98ms
step:670/1750 train_time:63639ms step_avg:94.98ms
step:671/1750 train_time:63737ms step_avg:94.99ms
step:672/1750 train_time:63835ms step_avg:94.99ms
step:673/1750 train_time:63934ms step_avg:95.00ms
step:674/1750 train_time:64033ms step_avg:95.00ms
step:675/1750 train_time:64131ms step_avg:95.01ms
step:676/1750 train_time:64230ms step_avg:95.01ms
step:677/1750 train_time:64328ms step_avg:95.02ms
step:678/1750 train_time:64427ms step_avg:95.02ms
step:679/1750 train_time:64525ms step_avg:95.03ms
step:680/1750 train_time:64623ms step_avg:95.03ms
step:681/1750 train_time:64722ms step_avg:95.04ms
step:682/1750 train_time:64821ms step_avg:95.04ms
step:683/1750 train_time:64918ms step_avg:95.05ms
step:684/1750 train_time:65016ms step_avg:95.05ms
step:685/1750 train_time:65115ms step_avg:95.06ms
step:686/1750 train_time:65213ms step_avg:95.06ms
step:687/1750 train_time:65311ms step_avg:95.07ms
step:688/1750 train_time:65409ms step_avg:95.07ms
step:689/1750 train_time:65507ms step_avg:95.08ms
step:690/1750 train_time:65606ms step_avg:95.08ms
step:691/1750 train_time:65704ms step_avg:95.09ms
step:692/1750 train_time:65803ms step_avg:95.09ms
step:693/1750 train_time:65901ms step_avg:95.10ms
step:694/1750 train_time:66000ms step_avg:95.10ms
step:695/1750 train_time:66098ms step_avg:95.11ms
step:696/1750 train_time:66198ms step_avg:95.11ms
step:697/1750 train_time:66297ms step_avg:95.12ms
step:698/1750 train_time:66395ms step_avg:95.12ms
step:699/1750 train_time:66493ms step_avg:95.13ms
step:700/1750 train_time:66591ms step_avg:95.13ms
step:701/1750 train_time:66689ms step_avg:95.13ms
step:702/1750 train_time:66788ms step_avg:95.14ms
step:703/1750 train_time:66886ms step_avg:95.14ms
step:704/1750 train_time:66984ms step_avg:95.15ms
step:705/1750 train_time:67082ms step_avg:95.15ms
step:706/1750 train_time:67181ms step_avg:95.16ms
step:707/1750 train_time:67279ms step_avg:95.16ms
step:708/1750 train_time:67378ms step_avg:95.17ms
step:709/1750 train_time:67476ms step_avg:95.17ms
step:710/1750 train_time:67574ms step_avg:95.18ms
step:711/1750 train_time:67672ms step_avg:95.18ms
step:712/1750 train_time:67771ms step_avg:95.18ms
step:713/1750 train_time:67869ms step_avg:95.19ms
step:714/1750 train_time:67968ms step_avg:95.19ms
step:715/1750 train_time:68066ms step_avg:95.20ms
step:716/1750 train_time:68165ms step_avg:95.20ms
step:717/1750 train_time:68263ms step_avg:95.21ms
step:718/1750 train_time:68363ms step_avg:95.21ms
step:719/1750 train_time:68462ms step_avg:95.22ms
step:720/1750 train_time:68560ms step_avg:95.22ms
step:721/1750 train_time:68659ms step_avg:95.23ms
step:722/1750 train_time:68757ms step_avg:95.23ms
step:723/1750 train_time:68856ms step_avg:95.24ms
step:724/1750 train_time:68955ms step_avg:95.24ms
step:725/1750 train_time:69055ms step_avg:95.25ms
step:726/1750 train_time:69154ms step_avg:95.25ms
step:727/1750 train_time:69254ms step_avg:95.26ms
step:728/1750 train_time:69353ms step_avg:95.27ms
step:729/1750 train_time:69455ms step_avg:95.27ms
step:730/1750 train_time:69550ms step_avg:95.27ms
step:731/1750 train_time:69648ms step_avg:95.28ms
step:732/1750 train_time:69746ms step_avg:95.28ms
step:733/1750 train_time:69844ms step_avg:95.29ms
step:734/1750 train_time:69943ms step_avg:95.29ms
step:735/1750 train_time:70041ms step_avg:95.29ms
step:736/1750 train_time:70139ms step_avg:95.30ms
step:737/1750 train_time:70238ms step_avg:95.30ms
step:738/1750 train_time:70336ms step_avg:95.31ms
step:739/1750 train_time:70435ms step_avg:95.31ms
step:740/1750 train_time:70534ms step_avg:95.32ms
step:741/1750 train_time:70633ms step_avg:95.32ms
step:742/1750 train_time:70732ms step_avg:95.33ms
step:743/1750 train_time:70830ms step_avg:95.33ms
step:744/1750 train_time:70928ms step_avg:95.33ms
step:745/1750 train_time:71027ms step_avg:95.34ms
step:746/1750 train_time:71126ms step_avg:95.34ms
step:747/1750 train_time:71225ms step_avg:95.35ms
step:748/1750 train_time:71324ms step_avg:95.35ms
step:749/1750 train_time:71422ms step_avg:95.36ms
step:750/1750 train_time:71520ms step_avg:95.36ms
step:750/1750 val_loss:3.5995 train_time:71613ms step_avg:95.48ms
step:751/1750 train_time:71637ms step_avg:95.39ms
step:752/1750 train_time:71723ms step_avg:95.38ms
step:753/1750 train_time:71823ms step_avg:95.38ms
step:754/1750 train_time:71923ms step_avg:95.39ms
step:755/1750 train_time:72021ms step_avg:95.39ms
step:756/1750 train_time:72118ms step_avg:95.39ms
step:757/1750 train_time:72217ms step_avg:95.40ms
step:758/1750 train_time:72314ms step_avg:95.40ms
step:759/1750 train_time:72412ms step_avg:95.40ms
step:760/1750 train_time:72509ms step_avg:95.41ms
step:761/1750 train_time:72608ms step_avg:95.41ms
step:762/1750 train_time:72708ms step_avg:95.42ms
step:763/1750 train_time:72808ms step_avg:95.42ms
step:764/1750 train_time:72907ms step_avg:95.43ms
step:765/1750 train_time:73005ms step_avg:95.43ms
step:766/1750 train_time:73103ms step_avg:95.44ms
step:767/1750 train_time:73201ms step_avg:95.44ms
step:768/1750 train_time:73299ms step_avg:95.44ms
step:769/1750 train_time:73398ms step_avg:95.45ms
step:770/1750 train_time:73497ms step_avg:95.45ms
step:771/1750 train_time:73595ms step_avg:95.45ms
step:772/1750 train_time:73694ms step_avg:95.46ms
step:773/1750 train_time:73794ms step_avg:95.46ms
step:774/1750 train_time:73893ms step_avg:95.47ms
step:775/1750 train_time:73992ms step_avg:95.47ms
step:776/1750 train_time:74090ms step_avg:95.48ms
step:777/1750 train_time:74189ms step_avg:95.48ms
step:778/1750 train_time:74287ms step_avg:95.48ms
step:779/1750 train_time:74385ms step_avg:95.49ms
step:780/1750 train_time:74485ms step_avg:95.49ms
step:781/1750 train_time:74583ms step_avg:95.50ms
step:782/1750 train_time:74683ms step_avg:95.50ms
step:783/1750 train_time:74783ms step_avg:95.51ms
step:784/1750 train_time:74882ms step_avg:95.51ms
step:785/1750 train_time:74982ms step_avg:95.52ms
step:786/1750 train_time:75081ms step_avg:95.52ms
step:787/1750 train_time:75180ms step_avg:95.53ms
step:788/1750 train_time:75278ms step_avg:95.53ms
step:789/1750 train_time:75377ms step_avg:95.53ms
step:790/1750 train_time:75475ms step_avg:95.54ms
step:791/1750 train_time:75573ms step_avg:95.54ms
step:792/1750 train_time:75672ms step_avg:95.55ms
step:793/1750 train_time:75772ms step_avg:95.55ms
step:794/1750 train_time:75871ms step_avg:95.56ms
step:795/1750 train_time:75970ms step_avg:95.56ms
step:796/1750 train_time:76068ms step_avg:95.56ms
step:797/1750 train_time:76167ms step_avg:95.57ms
step:798/1750 train_time:76265ms step_avg:95.57ms
step:799/1750 train_time:76365ms step_avg:95.58ms
step:800/1750 train_time:76464ms step_avg:95.58ms
step:801/1750 train_time:76563ms step_avg:95.58ms
step:802/1750 train_time:76664ms step_avg:95.59ms
step:803/1750 train_time:76763ms step_avg:95.60ms
step:804/1750 train_time:76864ms step_avg:95.60ms
step:805/1750 train_time:76964ms step_avg:95.61ms
step:806/1750 train_time:77063ms step_avg:95.61ms
step:807/1750 train_time:77162ms step_avg:95.62ms
step:808/1750 train_time:77261ms step_avg:95.62ms
step:809/1750 train_time:77360ms step_avg:95.62ms
step:810/1750 train_time:77459ms step_avg:95.63ms
step:811/1750 train_time:77557ms step_avg:95.63ms
step:812/1750 train_time:77656ms step_avg:95.64ms
step:813/1750 train_time:77755ms step_avg:95.64ms
step:814/1750 train_time:77854ms step_avg:95.64ms
step:815/1750 train_time:77953ms step_avg:95.65ms
step:816/1750 train_time:78052ms step_avg:95.65ms
step:817/1750 train_time:78152ms step_avg:95.66ms
step:818/1750 train_time:78251ms step_avg:95.66ms
step:819/1750 train_time:78350ms step_avg:95.67ms
step:820/1750 train_time:78449ms step_avg:95.67ms
step:821/1750 train_time:78547ms step_avg:95.67ms
step:822/1750 train_time:78646ms step_avg:95.68ms
step:823/1750 train_time:78744ms step_avg:95.68ms
step:824/1750 train_time:78843ms step_avg:95.68ms
step:825/1750 train_time:78943ms step_avg:95.69ms
step:826/1750 train_time:79042ms step_avg:95.69ms
step:827/1750 train_time:79142ms step_avg:95.70ms
step:828/1750 train_time:79241ms step_avg:95.70ms
step:829/1750 train_time:79340ms step_avg:95.71ms
step:830/1750 train_time:79438ms step_avg:95.71ms
step:831/1750 train_time:79537ms step_avg:95.71ms
step:832/1750 train_time:79636ms step_avg:95.72ms
step:833/1750 train_time:79734ms step_avg:95.72ms
step:834/1750 train_time:79832ms step_avg:95.72ms
step:835/1750 train_time:79931ms step_avg:95.73ms
step:836/1750 train_time:80032ms step_avg:95.73ms
step:837/1750 train_time:80132ms step_avg:95.74ms
step:838/1750 train_time:80232ms step_avg:95.74ms
step:839/1750 train_time:80330ms step_avg:95.75ms
step:840/1750 train_time:80429ms step_avg:95.75ms
step:841/1750 train_time:80528ms step_avg:95.75ms
step:842/1750 train_time:80627ms step_avg:95.76ms
step:843/1750 train_time:80726ms step_avg:95.76ms
step:844/1750 train_time:80825ms step_avg:95.76ms
step:845/1750 train_time:80924ms step_avg:95.77ms
step:846/1750 train_time:81024ms step_avg:95.77ms
step:847/1750 train_time:81123ms step_avg:95.78ms
step:848/1750 train_time:81223ms step_avg:95.78ms
step:849/1750 train_time:81323ms step_avg:95.79ms
step:850/1750 train_time:81423ms step_avg:95.79ms
step:851/1750 train_time:81521ms step_avg:95.79ms
step:852/1750 train_time:81620ms step_avg:95.80ms
step:853/1750 train_time:81718ms step_avg:95.80ms
step:854/1750 train_time:81816ms step_avg:95.80ms
step:855/1750 train_time:81916ms step_avg:95.81ms
step:856/1750 train_time:82014ms step_avg:95.81ms
step:857/1750 train_time:82114ms step_avg:95.82ms
step:858/1750 train_time:82214ms step_avg:95.82ms
step:859/1750 train_time:82313ms step_avg:95.82ms
step:860/1750 train_time:82413ms step_avg:95.83ms
step:861/1750 train_time:82513ms step_avg:95.83ms
step:862/1750 train_time:82612ms step_avg:95.84ms
step:863/1750 train_time:82710ms step_avg:95.84ms
step:864/1750 train_time:82809ms step_avg:95.84ms
step:865/1750 train_time:82908ms step_avg:95.85ms
step:866/1750 train_time:83007ms step_avg:95.85ms
step:867/1750 train_time:83105ms step_avg:95.85ms
step:868/1750 train_time:83204ms step_avg:95.86ms
step:869/1750 train_time:83304ms step_avg:95.86ms
step:870/1750 train_time:83403ms step_avg:95.87ms
step:871/1750 train_time:83503ms step_avg:95.87ms
step:872/1750 train_time:83602ms step_avg:95.87ms
step:873/1750 train_time:83701ms step_avg:95.88ms
step:874/1750 train_time:83799ms step_avg:95.88ms
step:875/1750 train_time:83897ms step_avg:95.88ms
step:875/1750 val_loss:3.5507 train_time:83990ms step_avg:95.99ms
step:876/1750 train_time:84015ms step_avg:95.91ms
step:877/1750 train_time:84104ms step_avg:95.90ms
step:878/1750 train_time:84205ms step_avg:95.91ms
step:879/1750 train_time:84306ms step_avg:95.91ms
step:880/1750 train_time:84405ms step_avg:95.91ms
step:881/1750 train_time:84503ms step_avg:95.92ms
step:882/1750 train_time:84602ms step_avg:95.92ms
step:883/1750 train_time:84700ms step_avg:95.92ms
step:884/1750 train_time:84798ms step_avg:95.93ms
step:885/1750 train_time:84896ms step_avg:95.93ms
step:886/1750 train_time:84994ms step_avg:95.93ms
step:887/1750 train_time:85093ms step_avg:95.93ms
step:888/1750 train_time:85192ms step_avg:95.94ms
step:889/1750 train_time:85292ms step_avg:95.94ms
step:890/1750 train_time:85391ms step_avg:95.94ms
step:891/1750 train_time:85489ms step_avg:95.95ms
step:892/1750 train_time:85588ms step_avg:95.95ms
step:893/1750 train_time:85687ms step_avg:95.95ms
step:894/1750 train_time:85785ms step_avg:95.96ms
step:895/1750 train_time:85884ms step_avg:95.96ms
step:896/1750 train_time:85982ms step_avg:95.96ms
step:897/1750 train_time:86081ms step_avg:95.97ms
step:898/1750 train_time:86181ms step_avg:95.97ms
step:899/1750 train_time:86280ms step_avg:95.97ms
step:900/1750 train_time:86380ms step_avg:95.98ms
step:901/1750 train_time:86478ms step_avg:95.98ms
step:902/1750 train_time:86577ms step_avg:95.98ms
step:903/1750 train_time:86677ms step_avg:95.99ms
step:904/1750 train_time:86776ms step_avg:95.99ms
step:905/1750 train_time:86875ms step_avg:95.99ms
step:906/1750 train_time:86975ms step_avg:96.00ms
step:907/1750 train_time:87075ms step_avg:96.00ms
step:908/1750 train_time:87175ms step_avg:96.01ms
step:909/1750 train_time:87274ms step_avg:96.01ms
step:910/1750 train_time:87375ms step_avg:96.02ms
step:911/1750 train_time:87475ms step_avg:96.02ms
step:912/1750 train_time:87575ms step_avg:96.03ms
step:913/1750 train_time:87675ms step_avg:96.03ms
step:914/1750 train_time:87775ms step_avg:96.03ms
step:915/1750 train_time:87876ms step_avg:96.04ms
step:916/1750 train_time:87976ms step_avg:96.04ms
step:917/1750 train_time:88076ms step_avg:96.05ms
step:918/1750 train_time:88176ms step_avg:96.05ms
step:919/1750 train_time:88277ms step_avg:96.06ms
step:920/1750 train_time:88377ms step_avg:96.06ms
step:921/1750 train_time:88477ms step_avg:96.07ms
step:922/1750 train_time:88577ms step_avg:96.07ms
step:923/1750 train_time:88678ms step_avg:96.08ms
step:924/1750 train_time:88778ms step_avg:96.08ms
step:925/1750 train_time:88878ms step_avg:96.08ms
step:926/1750 train_time:88978ms step_avg:96.09ms
step:927/1750 train_time:89077ms step_avg:96.09ms
step:928/1750 train_time:89178ms step_avg:96.10ms
step:929/1750 train_time:89278ms step_avg:96.10ms
step:930/1750 train_time:89378ms step_avg:96.11ms
step:931/1750 train_time:89479ms step_avg:96.11ms
step:932/1750 train_time:89579ms step_avg:96.11ms
step:933/1750 train_time:89680ms step_avg:96.12ms
step:934/1750 train_time:89779ms step_avg:96.12ms
step:935/1750 train_time:89880ms step_avg:96.13ms
step:936/1750 train_time:89980ms step_avg:96.13ms
step:937/1750 train_time:90080ms step_avg:96.14ms
step:938/1750 train_time:90179ms step_avg:96.14ms
step:939/1750 train_time:90280ms step_avg:96.14ms
step:940/1750 train_time:90380ms step_avg:96.15ms
step:941/1750 train_time:90481ms step_avg:96.15ms
step:942/1750 train_time:90582ms step_avg:96.16ms
step:943/1750 train_time:90683ms step_avg:96.16ms
step:944/1750 train_time:90783ms step_avg:96.17ms
step:945/1750 train_time:90884ms step_avg:96.17ms
step:946/1750 train_time:90984ms step_avg:96.18ms
step:947/1750 train_time:91085ms step_avg:96.18ms
step:948/1750 train_time:91185ms step_avg:96.19ms
step:949/1750 train_time:91285ms step_avg:96.19ms
step:950/1750 train_time:91385ms step_avg:96.20ms
step:951/1750 train_time:91486ms step_avg:96.20ms
step:952/1750 train_time:91587ms step_avg:96.20ms
step:953/1750 train_time:91686ms step_avg:96.21ms
step:954/1750 train_time:91786ms step_avg:96.21ms
step:955/1750 train_time:91888ms step_avg:96.22ms
step:956/1750 train_time:91988ms step_avg:96.22ms
step:957/1750 train_time:92088ms step_avg:96.23ms
step:958/1750 train_time:92188ms step_avg:96.23ms
step:959/1750 train_time:92287ms step_avg:96.23ms
step:960/1750 train_time:92387ms step_avg:96.24ms
step:961/1750 train_time:92486ms step_avg:96.24ms
step:962/1750 train_time:92587ms step_avg:96.24ms
step:963/1750 train_time:92687ms step_avg:96.25ms
step:964/1750 train_time:92787ms step_avg:96.25ms
step:965/1750 train_time:92887ms step_avg:96.26ms
step:966/1750 train_time:92987ms step_avg:96.26ms
step:967/1750 train_time:93088ms step_avg:96.27ms
step:968/1750 train_time:93188ms step_avg:96.27ms
step:969/1750 train_time:93288ms step_avg:96.27ms
step:970/1750 train_time:93388ms step_avg:96.28ms
step:971/1750 train_time:93487ms step_avg:96.28ms
step:972/1750 train_time:93588ms step_avg:96.28ms
step:973/1750 train_time:93687ms step_avg:96.29ms
step:974/1750 train_time:93787ms step_avg:96.29ms
step:975/1750 train_time:93887ms step_avg:96.29ms
step:976/1750 train_time:93987ms step_avg:96.30ms
step:977/1750 train_time:94088ms step_avg:96.30ms
step:978/1750 train_time:94188ms step_avg:96.31ms
step:979/1750 train_time:94288ms step_avg:96.31ms
step:980/1750 train_time:94388ms step_avg:96.31ms
step:981/1750 train_time:94488ms step_avg:96.32ms
step:982/1750 train_time:94588ms step_avg:96.32ms
step:983/1750 train_time:94687ms step_avg:96.32ms
step:984/1750 train_time:94787ms step_avg:96.33ms
step:985/1750 train_time:94887ms step_avg:96.33ms
step:986/1750 train_time:94988ms step_avg:96.34ms
step:987/1750 train_time:95089ms step_avg:96.34ms
step:988/1750 train_time:95188ms step_avg:96.34ms
step:989/1750 train_time:95289ms step_avg:96.35ms
step:990/1750 train_time:95389ms step_avg:96.35ms
step:991/1750 train_time:95488ms step_avg:96.36ms
step:992/1750 train_time:95588ms step_avg:96.36ms
step:993/1750 train_time:95689ms step_avg:96.36ms
step:994/1750 train_time:95789ms step_avg:96.37ms
step:995/1750 train_time:95888ms step_avg:96.37ms
step:996/1750 train_time:95989ms step_avg:96.37ms
step:997/1750 train_time:96089ms step_avg:96.38ms
step:998/1750 train_time:96189ms step_avg:96.38ms
step:999/1750 train_time:96288ms step_avg:96.38ms
step:1000/1750 train_time:96389ms step_avg:96.39ms
step:1000/1750 val_loss:3.5085 train_time:96483ms step_avg:96.48ms
step:1001/1750 train_time:96508ms step_avg:96.41ms
step:1002/1750 train_time:96595ms step_avg:96.40ms
step:1003/1750 train_time:96696ms step_avg:96.41ms
step:1004/1750 train_time:96796ms step_avg:96.41ms
step:1005/1750 train_time:96896ms step_avg:96.41ms
step:1006/1750 train_time:96996ms step_avg:96.42ms
step:1007/1750 train_time:97095ms step_avg:96.42ms
step:1008/1750 train_time:97195ms step_avg:96.42ms
step:1009/1750 train_time:97294ms step_avg:96.43ms
step:1010/1750 train_time:97394ms step_avg:96.43ms
step:1011/1750 train_time:97495ms step_avg:96.43ms
step:1012/1750 train_time:97596ms step_avg:96.44ms
step:1013/1750 train_time:97697ms step_avg:96.44ms
step:1014/1750 train_time:97798ms step_avg:96.45ms
step:1015/1750 train_time:97899ms step_avg:96.45ms
step:1016/1750 train_time:98000ms step_avg:96.46ms
step:1017/1750 train_time:98100ms step_avg:96.46ms
step:1018/1750 train_time:98199ms step_avg:96.46ms
step:1019/1750 train_time:98300ms step_avg:96.47ms
step:1020/1750 train_time:98401ms step_avg:96.47ms
step:1021/1750 train_time:98501ms step_avg:96.47ms
step:1022/1750 train_time:98601ms step_avg:96.48ms
step:1023/1750 train_time:98702ms step_avg:96.48ms
step:1024/1750 train_time:98803ms step_avg:96.49ms
step:1025/1750 train_time:98903ms step_avg:96.49ms
step:1026/1750 train_time:99003ms step_avg:96.49ms
step:1027/1750 train_time:99102ms step_avg:96.50ms
step:1028/1750 train_time:99202ms step_avg:96.50ms
step:1029/1750 train_time:99303ms step_avg:96.50ms
step:1030/1750 train_time:99403ms step_avg:96.51ms
step:1031/1750 train_time:99503ms step_avg:96.51ms
step:1032/1750 train_time:99603ms step_avg:96.51ms
step:1033/1750 train_time:99704ms step_avg:96.52ms
step:1034/1750 train_time:99805ms step_avg:96.52ms
step:1035/1750 train_time:99905ms step_avg:96.53ms
step:1036/1750 train_time:100005ms step_avg:96.53ms
step:1037/1750 train_time:100106ms step_avg:96.53ms
step:1038/1750 train_time:100206ms step_avg:96.54ms
step:1039/1750 train_time:100306ms step_avg:96.54ms
step:1040/1750 train_time:100407ms step_avg:96.55ms
step:1041/1750 train_time:100509ms step_avg:96.55ms
step:1042/1750 train_time:100610ms step_avg:96.55ms
step:1043/1750 train_time:100711ms step_avg:96.56ms
step:1044/1750 train_time:100811ms step_avg:96.56ms
step:1045/1750 train_time:100912ms step_avg:96.57ms
step:1046/1750 train_time:101012ms step_avg:96.57ms
step:1047/1750 train_time:101113ms step_avg:96.57ms
step:1048/1750 train_time:101215ms step_avg:96.58ms
step:1049/1750 train_time:101315ms step_avg:96.58ms
step:1050/1750 train_time:101415ms step_avg:96.59ms
step:1051/1750 train_time:101516ms step_avg:96.59ms
step:1052/1750 train_time:101617ms step_avg:96.59ms
step:1053/1750 train_time:101718ms step_avg:96.60ms
step:1054/1750 train_time:101819ms step_avg:96.60ms
step:1055/1750 train_time:101921ms step_avg:96.61ms
step:1056/1750 train_time:102022ms step_avg:96.61ms
step:1057/1750 train_time:102122ms step_avg:96.62ms
step:1058/1750 train_time:102222ms step_avg:96.62ms
step:1059/1750 train_time:102322ms step_avg:96.62ms
step:1060/1750 train_time:102422ms step_avg:96.62ms
step:1061/1750 train_time:102522ms step_avg:96.63ms
step:1062/1750 train_time:102624ms step_avg:96.63ms
step:1063/1750 train_time:102724ms step_avg:96.64ms
step:1064/1750 train_time:102826ms step_avg:96.64ms
step:1065/1750 train_time:102927ms step_avg:96.65ms
step:1066/1750 train_time:103028ms step_avg:96.65ms
step:1067/1750 train_time:103129ms step_avg:96.65ms
step:1068/1750 train_time:103230ms step_avg:96.66ms
step:1069/1750 train_time:103330ms step_avg:96.66ms
step:1070/1750 train_time:103431ms step_avg:96.66ms
step:1071/1750 train_time:103531ms step_avg:96.67ms
step:1072/1750 train_time:103632ms step_avg:96.67ms
step:1073/1750 train_time:103732ms step_avg:96.68ms
step:1074/1750 train_time:103833ms step_avg:96.68ms
step:1075/1750 train_time:103934ms step_avg:96.68ms
step:1076/1750 train_time:104034ms step_avg:96.69ms
step:1077/1750 train_time:104135ms step_avg:96.69ms
step:1078/1750 train_time:104236ms step_avg:96.69ms
step:1079/1750 train_time:104337ms step_avg:96.70ms
step:1080/1750 train_time:104437ms step_avg:96.70ms
step:1081/1750 train_time:104538ms step_avg:96.70ms
step:1082/1750 train_time:104639ms step_avg:96.71ms
step:1083/1750 train_time:104739ms step_avg:96.71ms
step:1084/1750 train_time:104840ms step_avg:96.72ms
step:1085/1750 train_time:104941ms step_avg:96.72ms
step:1086/1750 train_time:105042ms step_avg:96.72ms
step:1087/1750 train_time:105142ms step_avg:96.73ms
step:1088/1750 train_time:105242ms step_avg:96.73ms
step:1089/1750 train_time:105342ms step_avg:96.73ms
step:1090/1750 train_time:105443ms step_avg:96.74ms
step:1091/1750 train_time:105543ms step_avg:96.74ms
step:1092/1750 train_time:105643ms step_avg:96.74ms
step:1093/1750 train_time:105744ms step_avg:96.75ms
step:1094/1750 train_time:105844ms step_avg:96.75ms
step:1095/1750 train_time:105945ms step_avg:96.75ms
step:1096/1750 train_time:106046ms step_avg:96.76ms
step:1097/1750 train_time:106147ms step_avg:96.76ms
step:1098/1750 train_time:106249ms step_avg:96.77ms
step:1099/1750 train_time:106349ms step_avg:96.77ms
step:1100/1750 train_time:106449ms step_avg:96.77ms
step:1101/1750 train_time:106550ms step_avg:96.78ms
step:1102/1750 train_time:106650ms step_avg:96.78ms
step:1103/1750 train_time:106751ms step_avg:96.78ms
step:1104/1750 train_time:106852ms step_avg:96.79ms
step:1105/1750 train_time:106954ms step_avg:96.79ms
step:1106/1750 train_time:107054ms step_avg:96.79ms
step:1107/1750 train_time:107154ms step_avg:96.80ms
step:1108/1750 train_time:107254ms step_avg:96.80ms
step:1109/1750 train_time:107355ms step_avg:96.80ms
step:1110/1750 train_time:107456ms step_avg:96.81ms
step:1111/1750 train_time:107557ms step_avg:96.81ms
step:1112/1750 train_time:107658ms step_avg:96.82ms
step:1113/1750 train_time:107759ms step_avg:96.82ms
step:1114/1750 train_time:107860ms step_avg:96.82ms
step:1115/1750 train_time:107961ms step_avg:96.83ms
step:1116/1750 train_time:108062ms step_avg:96.83ms
step:1117/1750 train_time:108162ms step_avg:96.83ms
step:1118/1750 train_time:108263ms step_avg:96.84ms
step:1119/1750 train_time:108362ms step_avg:96.84ms
step:1120/1750 train_time:108462ms step_avg:96.84ms
step:1121/1750 train_time:108562ms step_avg:96.84ms
step:1122/1750 train_time:108663ms step_avg:96.85ms
step:1123/1750 train_time:108763ms step_avg:96.85ms
step:1124/1750 train_time:108864ms step_avg:96.85ms
step:1125/1750 train_time:108964ms step_avg:96.86ms
step:1125/1750 val_loss:3.4573 train_time:109059ms step_avg:96.94ms
step:1126/1750 train_time:109084ms step_avg:96.88ms
step:1127/1750 train_time:109176ms step_avg:96.87ms
step:1128/1750 train_time:109276ms step_avg:96.88ms
step:1129/1750 train_time:109377ms step_avg:96.88ms
step:1130/1750 train_time:109477ms step_avg:96.88ms
step:1131/1750 train_time:109577ms step_avg:96.89ms
step:1132/1750 train_time:109678ms step_avg:96.89ms
step:1133/1750 train_time:109779ms step_avg:96.89ms
step:1134/1750 train_time:109879ms step_avg:96.90ms
step:1135/1750 train_time:109980ms step_avg:96.90ms
step:1136/1750 train_time:110083ms step_avg:96.90ms
step:1137/1750 train_time:110184ms step_avg:96.91ms
step:1138/1750 train_time:110285ms step_avg:96.91ms
step:1139/1750 train_time:110386ms step_avg:96.91ms
step:1140/1750 train_time:110487ms step_avg:96.92ms
step:1141/1750 train_time:110588ms step_avg:96.92ms
step:1142/1750 train_time:110690ms step_avg:96.93ms
step:1143/1750 train_time:110790ms step_avg:96.93ms
step:1144/1750 train_time:110890ms step_avg:96.93ms
step:1145/1750 train_time:110992ms step_avg:96.94ms
step:1146/1750 train_time:111092ms step_avg:96.94ms
step:1147/1750 train_time:111193ms step_avg:96.94ms
step:1148/1750 train_time:111294ms step_avg:96.95ms
step:1149/1750 train_time:111395ms step_avg:96.95ms
step:1150/1750 train_time:111496ms step_avg:96.95ms
step:1151/1750 train_time:111598ms step_avg:96.96ms
step:1152/1750 train_time:111698ms step_avg:96.96ms
step:1153/1750 train_time:111800ms step_avg:96.96ms
step:1154/1750 train_time:111900ms step_avg:96.97ms
step:1155/1750 train_time:111999ms step_avg:96.97ms
step:1156/1750 train_time:112100ms step_avg:96.97ms
step:1157/1750 train_time:112201ms step_avg:96.98ms
step:1158/1750 train_time:112302ms step_avg:96.98ms
step:1159/1750 train_time:112402ms step_avg:96.98ms
step:1160/1750 train_time:112503ms step_avg:96.99ms
step:1161/1750 train_time:112604ms step_avg:96.99ms
step:1162/1750 train_time:112704ms step_avg:96.99ms
step:1163/1750 train_time:112805ms step_avg:96.99ms
step:1164/1750 train_time:112906ms step_avg:97.00ms
step:1165/1750 train_time:113006ms step_avg:97.00ms
step:1166/1750 train_time:113107ms step_avg:97.00ms
step:1167/1750 train_time:113208ms step_avg:97.01ms
step:1168/1750 train_time:113308ms step_avg:97.01ms
step:1169/1750 train_time:113410ms step_avg:97.01ms
step:1170/1750 train_time:113512ms step_avg:97.02ms
step:1171/1750 train_time:113613ms step_avg:97.02ms
step:1172/1750 train_time:113716ms step_avg:97.03ms
step:1173/1750 train_time:113818ms step_avg:97.03ms
step:1174/1750 train_time:113920ms step_avg:97.04ms
step:1175/1750 train_time:114022ms step_avg:97.04ms
step:1176/1750 train_time:114124ms step_avg:97.04ms
step:1177/1750 train_time:114225ms step_avg:97.05ms
step:1178/1750 train_time:114326ms step_avg:97.05ms
step:1179/1750 train_time:114430ms step_avg:97.06ms
step:1180/1750 train_time:114533ms step_avg:97.06ms
step:1181/1750 train_time:114634ms step_avg:97.07ms
step:1182/1750 train_time:114737ms step_avg:97.07ms
step:1183/1750 train_time:114838ms step_avg:97.07ms
step:1184/1750 train_time:114941ms step_avg:97.08ms
step:1185/1750 train_time:115042ms step_avg:97.08ms
step:1186/1750 train_time:115144ms step_avg:97.09ms
step:1187/1750 train_time:115246ms step_avg:97.09ms
step:1188/1750 train_time:115348ms step_avg:97.09ms
step:1189/1750 train_time:115449ms step_avg:97.10ms
step:1190/1750 train_time:115550ms step_avg:97.10ms
step:1191/1750 train_time:115653ms step_avg:97.11ms
step:1192/1750 train_time:115755ms step_avg:97.11ms
step:1193/1750 train_time:115856ms step_avg:97.11ms
step:1194/1750 train_time:115958ms step_avg:97.12ms
step:1195/1750 train_time:116060ms step_avg:97.12ms
step:1196/1750 train_time:116162ms step_avg:97.13ms
step:1197/1750 train_time:116263ms step_avg:97.13ms
step:1198/1750 train_time:116365ms step_avg:97.13ms
step:1199/1750 train_time:116467ms step_avg:97.14ms
step:1200/1750 train_time:116569ms step_avg:97.14ms
step:1201/1750 train_time:116672ms step_avg:97.15ms
step:1202/1750 train_time:116775ms step_avg:97.15ms
step:1203/1750 train_time:116877ms step_avg:97.15ms
step:1204/1750 train_time:116979ms step_avg:97.16ms
step:1205/1750 train_time:117080ms step_avg:97.16ms
step:1206/1750 train_time:117181ms step_avg:97.17ms
step:1207/1750 train_time:117283ms step_avg:97.17ms
step:1208/1750 train_time:117385ms step_avg:97.17ms
step:1209/1750 train_time:117487ms step_avg:97.18ms
step:1210/1750 train_time:117590ms step_avg:97.18ms
step:1211/1750 train_time:117692ms step_avg:97.19ms
step:1212/1750 train_time:117794ms step_avg:97.19ms
step:1213/1750 train_time:117896ms step_avg:97.19ms
step:1214/1750 train_time:117997ms step_avg:97.20ms
step:1215/1750 train_time:118100ms step_avg:97.20ms
step:1216/1750 train_time:118202ms step_avg:97.21ms
step:1217/1750 train_time:118303ms step_avg:97.21ms
step:1218/1750 train_time:118405ms step_avg:97.21ms
step:1219/1750 train_time:118507ms step_avg:97.22ms
step:1220/1750 train_time:118608ms step_avg:97.22ms
step:1221/1750 train_time:118712ms step_avg:97.23ms
step:1222/1750 train_time:118814ms step_avg:97.23ms
step:1223/1750 train_time:118916ms step_avg:97.23ms
step:1224/1750 train_time:119017ms step_avg:97.24ms
step:1225/1750 train_time:119119ms step_avg:97.24ms
step:1226/1750 train_time:119221ms step_avg:97.24ms
step:1227/1750 train_time:119323ms step_avg:97.25ms
step:1228/1750 train_time:119424ms step_avg:97.25ms
step:1229/1750 train_time:119525ms step_avg:97.25ms
step:1230/1750 train_time:119627ms step_avg:97.26ms
step:1231/1750 train_time:119729ms step_avg:97.26ms
step:1232/1750 train_time:119832ms step_avg:97.27ms
step:1233/1750 train_time:119933ms step_avg:97.27ms
step:1234/1750 train_time:120036ms step_avg:97.27ms
step:1235/1750 train_time:120137ms step_avg:97.28ms
step:1236/1750 train_time:120240ms step_avg:97.28ms
step:1237/1750 train_time:120343ms step_avg:97.29ms
step:1238/1750 train_time:120444ms step_avg:97.29ms
step:1239/1750 train_time:120546ms step_avg:97.29ms
step:1240/1750 train_time:120646ms step_avg:97.30ms
step:1241/1750 train_time:120749ms step_avg:97.30ms
step:1242/1750 train_time:120850ms step_avg:97.30ms
step:1243/1750 train_time:120953ms step_avg:97.31ms
step:1244/1750 train_time:121055ms step_avg:97.31ms
step:1245/1750 train_time:121157ms step_avg:97.32ms
step:1246/1750 train_time:121259ms step_avg:97.32ms
step:1247/1750 train_time:121361ms step_avg:97.32ms
step:1248/1750 train_time:121463ms step_avg:97.33ms
step:1249/1750 train_time:121564ms step_avg:97.33ms
step:1250/1750 train_time:121665ms step_avg:97.33ms
step:1250/1750 val_loss:3.4099 train_time:121761ms step_avg:97.41ms
step:1251/1750 train_time:121786ms step_avg:97.35ms
step:1252/1750 train_time:121880ms step_avg:97.35ms
step:1253/1750 train_time:121982ms step_avg:97.35ms
step:1254/1750 train_time:122084ms step_avg:97.36ms
step:1255/1750 train_time:122184ms step_avg:97.36ms
step:1256/1750 train_time:122285ms step_avg:97.36ms
step:1257/1750 train_time:122386ms step_avg:97.36ms
step:1258/1750 train_time:122487ms step_avg:97.37ms
step:1259/1750 train_time:122588ms step_avg:97.37ms
step:1260/1750 train_time:122691ms step_avg:97.37ms
step:1261/1750 train_time:122794ms step_avg:97.38ms
step:1262/1750 train_time:122897ms step_avg:97.38ms
step:1263/1750 train_time:122999ms step_avg:97.39ms
step:1264/1750 train_time:123099ms step_avg:97.39ms
step:1265/1750 train_time:123201ms step_avg:97.39ms
step:1266/1750 train_time:123302ms step_avg:97.39ms
step:1267/1750 train_time:123403ms step_avg:97.40ms
step:1268/1750 train_time:123504ms step_avg:97.40ms
step:1269/1750 train_time:123605ms step_avg:97.40ms
step:1270/1750 train_time:123708ms step_avg:97.41ms
step:1271/1750 train_time:123812ms step_avg:97.41ms
step:1272/1750 train_time:123914ms step_avg:97.42ms
step:1273/1750 train_time:124015ms step_avg:97.42ms
step:1274/1750 train_time:124117ms step_avg:97.42ms
step:1275/1750 train_time:124218ms step_avg:97.43ms
step:1276/1750 train_time:124320ms step_avg:97.43ms
step:1277/1750 train_time:124421ms step_avg:97.43ms
step:1278/1750 train_time:124523ms step_avg:97.44ms
step:1279/1750 train_time:124624ms step_avg:97.44ms
step:1280/1750 train_time:124725ms step_avg:97.44ms
step:1281/1750 train_time:124827ms step_avg:97.45ms
step:1282/1750 train_time:124929ms step_avg:97.45ms
step:1283/1750 train_time:125032ms step_avg:97.45ms
step:1284/1750 train_time:125133ms step_avg:97.46ms
step:1285/1750 train_time:125235ms step_avg:97.46ms
step:1286/1750 train_time:125336ms step_avg:97.46ms
step:1287/1750 train_time:125437ms step_avg:97.46ms
step:1288/1750 train_time:125538ms step_avg:97.47ms
step:1289/1750 train_time:125641ms step_avg:97.47ms
step:1290/1750 train_time:125743ms step_avg:97.48ms
step:1291/1750 train_time:125845ms step_avg:97.48ms
step:1292/1750 train_time:125947ms step_avg:97.48ms
step:1293/1750 train_time:126048ms step_avg:97.49ms
step:1294/1750 train_time:126152ms step_avg:97.49ms
step:1295/1750 train_time:126254ms step_avg:97.49ms
step:1296/1750 train_time:126355ms step_avg:97.50ms
step:1297/1750 train_time:126457ms step_avg:97.50ms
step:1298/1750 train_time:126559ms step_avg:97.50ms
step:1299/1750 train_time:126661ms step_avg:97.51ms
step:1300/1750 train_time:126763ms step_avg:97.51ms
step:1301/1750 train_time:126864ms step_avg:97.51ms
step:1302/1750 train_time:126966ms step_avg:97.52ms
step:1303/1750 train_time:127068ms step_avg:97.52ms
step:1304/1750 train_time:127170ms step_avg:97.52ms
step:1305/1750 train_time:127274ms step_avg:97.53ms
step:1306/1750 train_time:127376ms step_avg:97.53ms
step:1307/1750 train_time:127477ms step_avg:97.53ms
step:1308/1750 train_time:127578ms step_avg:97.54ms
step:1309/1750 train_time:127680ms step_avg:97.54ms
step:1310/1750 train_time:127783ms step_avg:97.54ms
step:1311/1750 train_time:127885ms step_avg:97.55ms
step:1312/1750 train_time:127986ms step_avg:97.55ms
step:1313/1750 train_time:128088ms step_avg:97.55ms
step:1314/1750 train_time:128190ms step_avg:97.56ms
step:1315/1750 train_time:128292ms step_avg:97.56ms
step:1316/1750 train_time:128394ms step_avg:97.56ms
step:1317/1750 train_time:128496ms step_avg:97.57ms
step:1318/1750 train_time:128598ms step_avg:97.57ms
step:1319/1750 train_time:128700ms step_avg:97.57ms
step:1320/1750 train_time:128802ms step_avg:97.58ms
step:1321/1750 train_time:128904ms step_avg:97.58ms
step:1322/1750 train_time:129005ms step_avg:97.58ms
step:1323/1750 train_time:129106ms step_avg:97.59ms
step:1324/1750 train_time:129208ms step_avg:97.59ms
step:1325/1750 train_time:129311ms step_avg:97.59ms
step:1326/1750 train_time:129413ms step_avg:97.60ms
step:1327/1750 train_time:129516ms step_avg:97.60ms
step:1328/1750 train_time:129617ms step_avg:97.60ms
step:1329/1750 train_time:129718ms step_avg:97.61ms
step:1330/1750 train_time:129820ms step_avg:97.61ms
step:1331/1750 train_time:129922ms step_avg:97.61ms
step:1332/1750 train_time:130024ms step_avg:97.62ms
step:1333/1750 train_time:130126ms step_avg:97.62ms
step:1334/1750 train_time:130227ms step_avg:97.62ms
step:1335/1750 train_time:130329ms step_avg:97.62ms
step:1336/1750 train_time:130431ms step_avg:97.63ms
step:1337/1750 train_time:130535ms step_avg:97.63ms
step:1338/1750 train_time:130636ms step_avg:97.64ms
step:1339/1750 train_time:130738ms step_avg:97.64ms
step:1340/1750 train_time:130839ms step_avg:97.64ms
step:1341/1750 train_time:130941ms step_avg:97.64ms
step:1342/1750 train_time:131043ms step_avg:97.65ms
step:1343/1750 train_time:131144ms step_avg:97.65ms
step:1344/1750 train_time:131246ms step_avg:97.65ms
step:1345/1750 train_time:131348ms step_avg:97.66ms
step:1346/1750 train_time:131452ms step_avg:97.66ms
step:1347/1750 train_time:131554ms step_avg:97.66ms
step:1348/1750 train_time:131656ms step_avg:97.67ms
step:1349/1750 train_time:131757ms step_avg:97.67ms
step:1350/1750 train_time:131860ms step_avg:97.67ms
step:1351/1750 train_time:131962ms step_avg:97.68ms
step:1352/1750 train_time:132063ms step_avg:97.68ms
step:1353/1750 train_time:132165ms step_avg:97.68ms
step:1354/1750 train_time:132266ms step_avg:97.69ms
step:1355/1750 train_time:132368ms step_avg:97.69ms
step:1356/1750 train_time:132471ms step_avg:97.69ms
step:1357/1750 train_time:132573ms step_avg:97.70ms
step:1358/1750 train_time:132676ms step_avg:97.70ms
step:1359/1750 train_time:132777ms step_avg:97.70ms
step:1360/1750 train_time:132879ms step_avg:97.71ms
step:1361/1750 train_time:132980ms step_avg:97.71ms
step:1362/1750 train_time:133081ms step_avg:97.71ms
step:1363/1750 train_time:133183ms step_avg:97.71ms
step:1364/1750 train_time:133285ms step_avg:97.72ms
step:1365/1750 train_time:133387ms step_avg:97.72ms
step:1366/1750 train_time:133488ms step_avg:97.72ms
step:1367/1750 train_time:133589ms step_avg:97.72ms
step:1368/1750 train_time:133694ms step_avg:97.73ms
step:1369/1750 train_time:133795ms step_avg:97.73ms
step:1370/1750 train_time:133896ms step_avg:97.73ms
step:1371/1750 train_time:133998ms step_avg:97.74ms
step:1372/1750 train_time:134099ms step_avg:97.74ms
step:1373/1750 train_time:134201ms step_avg:97.74ms
step:1374/1750 train_time:134303ms step_avg:97.75ms
step:1375/1750 train_time:134404ms step_avg:97.75ms
step:1375/1750 val_loss:3.3691 train_time:134500ms step_avg:97.82ms
step:1376/1750 train_time:134524ms step_avg:97.76ms
step:1377/1750 train_time:134620ms step_avg:97.76ms
step:1378/1750 train_time:134721ms step_avg:97.77ms
step:1379/1750 train_time:134823ms step_avg:97.77ms
step:1380/1750 train_time:134925ms step_avg:97.77ms
step:1381/1750 train_time:135026ms step_avg:97.77ms
step:1382/1750 train_time:135127ms step_avg:97.78ms
step:1383/1750 train_time:135228ms step_avg:97.78ms
step:1384/1750 train_time:135329ms step_avg:97.78ms
step:1385/1750 train_time:135430ms step_avg:97.78ms
step:1386/1750 train_time:135534ms step_avg:97.79ms
step:1387/1750 train_time:135637ms step_avg:97.79ms
step:1388/1750 train_time:135738ms step_avg:97.79ms
step:1389/1750 train_time:135840ms step_avg:97.80ms
step:1390/1750 train_time:135942ms step_avg:97.80ms
step:1391/1750 train_time:136043ms step_avg:97.80ms
step:1392/1750 train_time:136144ms step_avg:97.80ms
step:1393/1750 train_time:136246ms step_avg:97.81ms
step:1394/1750 train_time:136347ms step_avg:97.81ms
step:1395/1750 train_time:136450ms step_avg:97.81ms
step:1396/1750 train_time:136553ms step_avg:97.82ms
step:1397/1750 train_time:136656ms step_avg:97.82ms
step:1398/1750 train_time:136758ms step_avg:97.82ms
step:1399/1750 train_time:136859ms step_avg:97.83ms
step:1400/1750 train_time:136961ms step_avg:97.83ms
step:1401/1750 train_time:137063ms step_avg:97.83ms
step:1402/1750 train_time:137164ms step_avg:97.83ms
step:1403/1750 train_time:137265ms step_avg:97.84ms
step:1404/1750 train_time:137367ms step_avg:97.84ms
step:1405/1750 train_time:137469ms step_avg:97.84ms
step:1406/1750 train_time:137573ms step_avg:97.85ms
step:1407/1750 train_time:137676ms step_avg:97.85ms
step:1408/1750 train_time:137778ms step_avg:97.85ms
step:1409/1750 train_time:137882ms step_avg:97.86ms
step:1410/1750 train_time:137984ms step_avg:97.86ms
step:1411/1750 train_time:138084ms step_avg:97.86ms
step:1412/1750 train_time:138185ms step_avg:97.87ms
step:1413/1750 train_time:138286ms step_avg:97.87ms
step:1414/1750 train_time:138388ms step_avg:97.87ms
step:1415/1750 train_time:138489ms step_avg:97.87ms
step:1416/1750 train_time:138591ms step_avg:97.88ms
step:1417/1750 train_time:138694ms step_avg:97.88ms
step:1418/1750 train_time:138796ms step_avg:97.88ms
step:1419/1750 train_time:138899ms step_avg:97.89ms
step:1420/1750 train_time:139001ms step_avg:97.89ms
step:1421/1750 train_time:139103ms step_avg:97.89ms
step:1422/1750 train_time:139205ms step_avg:97.89ms
step:1423/1750 train_time:139306ms step_avg:97.90ms
step:1424/1750 train_time:139408ms step_avg:97.90ms
step:1425/1750 train_time:139509ms step_avg:97.90ms
step:1426/1750 train_time:139611ms step_avg:97.90ms
step:1427/1750 train_time:139714ms step_avg:97.91ms
step:1428/1750 train_time:139818ms step_avg:97.91ms
step:1429/1750 train_time:139920ms step_avg:97.91ms
step:1430/1750 train_time:140024ms step_avg:97.92ms
step:1431/1750 train_time:140128ms step_avg:97.92ms
step:1432/1750 train_time:140231ms step_avg:97.93ms
step:1433/1750 train_time:140334ms step_avg:97.93ms
step:1434/1750 train_time:140436ms step_avg:97.93ms
step:1435/1750 train_time:140540ms step_avg:97.94ms
step:1436/1750 train_time:140644ms step_avg:97.94ms
step:1437/1750 train_time:140747ms step_avg:97.95ms
step:1438/1750 train_time:140851ms step_avg:97.95ms
step:1439/1750 train_time:140955ms step_avg:97.95ms
step:1440/1750 train_time:141059ms step_avg:97.96ms
step:1441/1750 train_time:141164ms step_avg:97.96ms
step:1442/1750 train_time:141266ms step_avg:97.97ms
step:1443/1750 train_time:141367ms step_avg:97.97ms
step:1444/1750 train_time:141470ms step_avg:97.97ms
step:1445/1750 train_time:141573ms step_avg:97.97ms
step:1446/1750 train_time:141676ms step_avg:97.98ms
step:1447/1750 train_time:141778ms step_avg:97.98ms
step:1448/1750 train_time:141882ms step_avg:97.98ms
step:1449/1750 train_time:141984ms step_avg:97.99ms
step:1450/1750 train_time:142087ms step_avg:97.99ms
step:1451/1750 train_time:142190ms step_avg:97.99ms
step:1452/1750 train_time:142294ms step_avg:98.00ms
step:1453/1750 train_time:142397ms step_avg:98.00ms
step:1454/1750 train_time:142502ms step_avg:98.01ms
step:1455/1750 train_time:142605ms step_avg:98.01ms
step:1456/1750 train_time:142707ms step_avg:98.01ms
step:1457/1750 train_time:142812ms step_avg:98.02ms
step:1458/1750 train_time:142915ms step_avg:98.02ms
step:1459/1750 train_time:143018ms step_avg:98.02ms
step:1460/1750 train_time:143120ms step_avg:98.03ms
step:1461/1750 train_time:143223ms step_avg:98.03ms
step:1462/1750 train_time:143325ms step_avg:98.03ms
step:1463/1750 train_time:143428ms step_avg:98.04ms
step:1464/1750 train_time:143531ms step_avg:98.04ms
step:1465/1750 train_time:143634ms step_avg:98.04ms
step:1466/1750 train_time:143738ms step_avg:98.05ms
step:1467/1750 train_time:143840ms step_avg:98.05ms
step:1468/1750 train_time:143944ms step_avg:98.05ms
step:1469/1750 train_time:144048ms step_avg:98.06ms
step:1470/1750 train_time:144151ms step_avg:98.06ms
step:1471/1750 train_time:144254ms step_avg:98.07ms
step:1472/1750 train_time:144356ms step_avg:98.07ms
step:1473/1750 train_time:144458ms step_avg:98.07ms
step:1474/1750 train_time:144562ms step_avg:98.07ms
step:1475/1750 train_time:144664ms step_avg:98.08ms
step:1476/1750 train_time:144767ms step_avg:98.08ms
step:1477/1750 train_time:144872ms step_avg:98.09ms
step:1478/1750 train_time:144975ms step_avg:98.09ms
step:1479/1750 train_time:145077ms step_avg:98.09ms
step:1480/1750 train_time:145180ms step_avg:98.09ms
step:1481/1750 train_time:145284ms step_avg:98.10ms
step:1482/1750 train_time:145386ms step_avg:98.10ms
step:1483/1750 train_time:145489ms step_avg:98.10ms
step:1484/1750 train_time:145594ms step_avg:98.11ms
step:1485/1750 train_time:145697ms step_avg:98.11ms
step:1486/1750 train_time:145801ms step_avg:98.12ms
step:1487/1750 train_time:145904ms step_avg:98.12ms
step:1488/1750 train_time:146007ms step_avg:98.12ms
step:1489/1750 train_time:146110ms step_avg:98.13ms
step:1490/1750 train_time:146213ms step_avg:98.13ms
step:1491/1750 train_time:146315ms step_avg:98.13ms
step:1492/1750 train_time:146418ms step_avg:98.14ms
step:1493/1750 train_time:146521ms step_avg:98.14ms
step:1494/1750 train_time:146624ms step_avg:98.14ms
step:1495/1750 train_time:146726ms step_avg:98.14ms
step:1496/1750 train_time:146829ms step_avg:98.15ms
step:1497/1750 train_time:146932ms step_avg:98.15ms
step:1498/1750 train_time:147035ms step_avg:98.15ms
step:1499/1750 train_time:147137ms step_avg:98.16ms
step:1500/1750 train_time:147240ms step_avg:98.16ms
step:1500/1750 val_loss:3.3327 train_time:147336ms step_avg:98.22ms
step:1501/1750 train_time:147361ms step_avg:98.18ms
step:1502/1750 train_time:147453ms step_avg:98.17ms
step:1503/1750 train_time:147556ms step_avg:98.17ms
step:1504/1750 train_time:147659ms step_avg:98.18ms
step:1505/1750 train_time:147761ms step_avg:98.18ms
step:1506/1750 train_time:147863ms step_avg:98.18ms
step:1507/1750 train_time:147966ms step_avg:98.19ms
step:1508/1750 train_time:148068ms step_avg:98.19ms
step:1509/1750 train_time:148170ms step_avg:98.19ms
step:1510/1750 train_time:148273ms step_avg:98.19ms
step:1511/1750 train_time:148379ms step_avg:98.20ms
step:1512/1750 train_time:148483ms step_avg:98.20ms
step:1513/1750 train_time:148586ms step_avg:98.21ms
step:1514/1750 train_time:148689ms step_avg:98.21ms
step:1515/1750 train_time:148795ms step_avg:98.21ms
step:1516/1750 train_time:148897ms step_avg:98.22ms
step:1517/1750 train_time:149000ms step_avg:98.22ms
step:1518/1750 train_time:149103ms step_avg:98.22ms
step:1519/1750 train_time:149207ms step_avg:98.23ms
step:1520/1750 train_time:149311ms step_avg:98.23ms
step:1521/1750 train_time:149414ms step_avg:98.23ms
step:1522/1750 train_time:149516ms step_avg:98.24ms
step:1523/1750 train_time:149619ms step_avg:98.24ms
step:1524/1750 train_time:149723ms step_avg:98.24ms
step:1525/1750 train_time:149826ms step_avg:98.25ms
step:1526/1750 train_time:149930ms step_avg:98.25ms
step:1527/1750 train_time:150033ms step_avg:98.25ms
step:1528/1750 train_time:150138ms step_avg:98.26ms
step:1529/1750 train_time:150241ms step_avg:98.26ms
step:1530/1750 train_time:150344ms step_avg:98.26ms
step:1531/1750 train_time:150446ms step_avg:98.27ms
step:1532/1750 train_time:150549ms step_avg:98.27ms
step:1533/1750 train_time:150652ms step_avg:98.27ms
step:1534/1750 train_time:150754ms step_avg:98.28ms
step:1535/1750 train_time:150857ms step_avg:98.28ms
step:1536/1750 train_time:150960ms step_avg:98.28ms
step:1537/1750 train_time:151063ms step_avg:98.28ms
step:1538/1750 train_time:151166ms step_avg:98.29ms
step:1539/1750 train_time:151269ms step_avg:98.29ms
step:1540/1750 train_time:151372ms step_avg:98.29ms
step:1541/1750 train_time:151476ms step_avg:98.30ms
step:1542/1750 train_time:151580ms step_avg:98.30ms
step:1543/1750 train_time:151684ms step_avg:98.30ms
step:1544/1750 train_time:151787ms step_avg:98.31ms
step:1545/1750 train_time:151890ms step_avg:98.31ms
step:1546/1750 train_time:151993ms step_avg:98.31ms
step:1547/1750 train_time:152097ms step_avg:98.32ms
step:1548/1750 train_time:152201ms step_avg:98.32ms
step:1549/1750 train_time:152304ms step_avg:98.32ms
step:1550/1750 train_time:152406ms step_avg:98.33ms
step:1551/1750 train_time:152509ms step_avg:98.33ms
step:1552/1750 train_time:152613ms step_avg:98.33ms
step:1553/1750 train_time:152717ms step_avg:98.34ms
step:1554/1750 train_time:152819ms step_avg:98.34ms
step:1555/1750 train_time:152922ms step_avg:98.34ms
step:1556/1750 train_time:153025ms step_avg:98.35ms
step:1557/1750 train_time:153130ms step_avg:98.35ms
step:1558/1750 train_time:153233ms step_avg:98.35ms
step:1559/1750 train_time:153336ms step_avg:98.36ms
step:1560/1750 train_time:153440ms step_avg:98.36ms
step:1561/1750 train_time:153543ms step_avg:98.36ms
step:1562/1750 train_time:153646ms step_avg:98.36ms
step:1563/1750 train_time:153752ms step_avg:98.37ms
step:1564/1750 train_time:153855ms step_avg:98.37ms
step:1565/1750 train_time:153957ms step_avg:98.38ms
step:1566/1750 train_time:154060ms step_avg:98.38ms
step:1567/1750 train_time:154162ms step_avg:98.38ms
step:1568/1750 train_time:154265ms step_avg:98.38ms
step:1569/1750 train_time:154368ms step_avg:98.39ms
step:1570/1750 train_time:154473ms step_avg:98.39ms
step:1571/1750 train_time:154575ms step_avg:98.39ms
step:1572/1750 train_time:154678ms step_avg:98.40ms
step:1573/1750 train_time:154782ms step_avg:98.40ms
step:1574/1750 train_time:154885ms step_avg:98.40ms
step:1575/1750 train_time:154988ms step_avg:98.40ms
step:1576/1750 train_time:155092ms step_avg:98.41ms
step:1577/1750 train_time:155196ms step_avg:98.41ms
step:1578/1750 train_time:155299ms step_avg:98.41ms
step:1579/1750 train_time:155403ms step_avg:98.42ms
step:1580/1750 train_time:155506ms step_avg:98.42ms
step:1581/1750 train_time:155610ms step_avg:98.42ms
step:1582/1750 train_time:155713ms step_avg:98.43ms
step:1583/1750 train_time:155819ms step_avg:98.43ms
step:1584/1750 train_time:155923ms step_avg:98.44ms
step:1585/1750 train_time:156026ms step_avg:98.44ms
step:1586/1750 train_time:156131ms step_avg:98.44ms
step:1587/1750 train_time:156234ms step_avg:98.45ms
step:1588/1750 train_time:156337ms step_avg:98.45ms
step:1589/1750 train_time:156440ms step_avg:98.45ms
step:1590/1750 train_time:156544ms step_avg:98.46ms
step:1591/1750 train_time:156646ms step_avg:98.46ms
step:1592/1750 train_time:156750ms step_avg:98.46ms
step:1593/1750 train_time:156853ms step_avg:98.46ms
step:1594/1750 train_time:156959ms step_avg:98.47ms
step:1595/1750 train_time:157061ms step_avg:98.47ms
step:1596/1750 train_time:157164ms step_avg:98.47ms
step:1597/1750 train_time:157267ms step_avg:98.48ms
step:1598/1750 train_time:157373ms step_avg:98.48ms
step:1599/1750 train_time:157475ms step_avg:98.48ms
step:1600/1750 train_time:157581ms step_avg:98.49ms
step:1601/1750 train_time:157684ms step_avg:98.49ms
step:1602/1750 train_time:157786ms step_avg:98.49ms
step:1603/1750 train_time:157889ms step_avg:98.50ms
step:1604/1750 train_time:157992ms step_avg:98.50ms
step:1605/1750 train_time:158095ms step_avg:98.50ms
step:1606/1750 train_time:158198ms step_avg:98.50ms
step:1607/1750 train_time:158302ms step_avg:98.51ms
step:1608/1750 train_time:158404ms step_avg:98.51ms
step:1609/1750 train_time:158508ms step_avg:98.51ms
step:1610/1750 train_time:158613ms step_avg:98.52ms
step:1611/1750 train_time:158716ms step_avg:98.52ms
step:1612/1750 train_time:158819ms step_avg:98.52ms
step:1613/1750 train_time:158922ms step_avg:98.53ms
step:1614/1750 train_time:159024ms step_avg:98.53ms
step:1615/1750 train_time:159126ms step_avg:98.53ms
step:1616/1750 train_time:159230ms step_avg:98.53ms
step:1617/1750 train_time:159334ms step_avg:98.54ms
step:1618/1750 train_time:159437ms step_avg:98.54ms
step:1619/1750 train_time:159540ms step_avg:98.54ms
step:1620/1750 train_time:159643ms step_avg:98.54ms
step:1621/1750 train_time:159745ms step_avg:98.55ms
step:1622/1750 train_time:159848ms step_avg:98.55ms
step:1623/1750 train_time:159952ms step_avg:98.55ms
step:1624/1750 train_time:160056ms step_avg:98.56ms
step:1625/1750 train_time:160160ms step_avg:98.56ms
step:1625/1750 val_loss:3.3026 train_time:160258ms step_avg:98.62ms
step:1626/1750 train_time:160283ms step_avg:98.58ms
step:1627/1750 train_time:160376ms step_avg:98.57ms
step:1628/1750 train_time:160480ms step_avg:98.57ms
step:1629/1750 train_time:160582ms step_avg:98.58ms
step:1630/1750 train_time:160685ms step_avg:98.58ms
step:1631/1750 train_time:160788ms step_avg:98.58ms
step:1632/1750 train_time:160891ms step_avg:98.59ms
step:1633/1750 train_time:160993ms step_avg:98.59ms
step:1634/1750 train_time:161097ms step_avg:98.59ms
step:1635/1750 train_time:161199ms step_avg:98.59ms
step:1636/1750 train_time:161305ms step_avg:98.60ms
step:1637/1750 train_time:161409ms step_avg:98.60ms
step:1638/1750 train_time:161512ms step_avg:98.60ms
step:1639/1750 train_time:161615ms step_avg:98.61ms
step:1640/1750 train_time:161718ms step_avg:98.61ms
step:1641/1750 train_time:161821ms step_avg:98.61ms
step:1642/1750 train_time:161924ms step_avg:98.61ms
step:1643/1750 train_time:162028ms step_avg:98.62ms
step:1644/1750 train_time:162131ms step_avg:98.62ms
step:1645/1750 train_time:162235ms step_avg:98.62ms
step:1646/1750 train_time:162337ms step_avg:98.63ms
step:1647/1750 train_time:162441ms step_avg:98.63ms
step:1648/1750 train_time:162546ms step_avg:98.63ms
step:1649/1750 train_time:162648ms step_avg:98.63ms
step:1650/1750 train_time:162752ms step_avg:98.64ms
step:1651/1750 train_time:162855ms step_avg:98.64ms
step:1652/1750 train_time:162958ms step_avg:98.64ms
step:1653/1750 train_time:163061ms step_avg:98.65ms
step:1654/1750 train_time:163163ms step_avg:98.65ms
step:1655/1750 train_time:163268ms step_avg:98.65ms
step:1656/1750 train_time:163372ms step_avg:98.65ms
step:1657/1750 train_time:163473ms step_avg:98.66ms
step:1658/1750 train_time:163576ms step_avg:98.66ms
step:1659/1750 train_time:163684ms step_avg:98.66ms
step:1660/1750 train_time:163788ms step_avg:98.67ms
step:1661/1750 train_time:163892ms step_avg:98.67ms
step:1662/1750 train_time:163996ms step_avg:98.67ms
step:1663/1750 train_time:164100ms step_avg:98.68ms
step:1664/1750 train_time:164203ms step_avg:98.68ms
step:1665/1750 train_time:164308ms step_avg:98.68ms
step:1666/1750 train_time:164412ms step_avg:98.69ms
step:1667/1750 train_time:164515ms step_avg:98.69ms
step:1668/1750 train_time:164618ms step_avg:98.69ms
step:1669/1750 train_time:164722ms step_avg:98.70ms
step:1670/1750 train_time:164825ms step_avg:98.70ms
step:1671/1750 train_time:164930ms step_avg:98.70ms
step:1672/1750 train_time:165034ms step_avg:98.70ms
step:1673/1750 train_time:165138ms step_avg:98.71ms
step:1674/1750 train_time:165242ms step_avg:98.71ms
step:1675/1750 train_time:165345ms step_avg:98.71ms
step:1676/1750 train_time:165450ms step_avg:98.72ms
step:1677/1750 train_time:165552ms step_avg:98.72ms
step:1678/1750 train_time:165656ms step_avg:98.72ms
step:1679/1750 train_time:165759ms step_avg:98.72ms
step:1680/1750 train_time:165862ms step_avg:98.73ms
step:1681/1750 train_time:165968ms step_avg:98.73ms
step:1682/1750 train_time:166073ms step_avg:98.74ms
step:1683/1750 train_time:166175ms step_avg:98.74ms
step:1684/1750 train_time:166278ms step_avg:98.74ms
step:1685/1750 train_time:166381ms step_avg:98.74ms
step:1686/1750 train_time:166485ms step_avg:98.75ms
step:1687/1750 train_time:166590ms step_avg:98.75ms
step:1688/1750 train_time:166694ms step_avg:98.75ms
step:1689/1750 train_time:166798ms step_avg:98.76ms
step:1690/1750 train_time:166902ms step_avg:98.76ms
step:1691/1750 train_time:167006ms step_avg:98.76ms
step:1692/1750 train_time:167110ms step_avg:98.76ms
step:1693/1750 train_time:167214ms step_avg:98.77ms
step:1694/1750 train_time:167317ms step_avg:98.77ms
step:1695/1750 train_time:167422ms step_avg:98.77ms
step:1696/1750 train_time:167526ms step_avg:98.78ms
step:1697/1750 train_time:167633ms step_avg:98.78ms
step:1698/1750 train_time:167737ms step_avg:98.79ms
step:1699/1750 train_time:167841ms step_avg:98.79ms
step:1700/1750 train_time:167945ms step_avg:98.79ms
step:1701/1750 train_time:168049ms step_avg:98.79ms
step:1702/1750 train_time:168155ms step_avg:98.80ms
step:1703/1750 train_time:168259ms step_avg:98.80ms
step:1704/1750 train_time:168362ms step_avg:98.80ms
step:1705/1750 train_time:168465ms step_avg:98.81ms
step:1706/1750 train_time:168571ms step_avg:98.81ms
step:1707/1750 train_time:168675ms step_avg:98.81ms
step:1708/1750 train_time:168781ms step_avg:98.82ms
step:1709/1750 train_time:168884ms step_avg:98.82ms
step:1710/1750 train_time:168988ms step_avg:98.82ms
step:1711/1750 train_time:169093ms step_avg:98.83ms
step:1712/1750 train_time:169197ms step_avg:98.83ms
step:1713/1750 train_time:169302ms step_avg:98.83ms
step:1714/1750 train_time:169405ms step_avg:98.84ms
step:1715/1750 train_time:169512ms step_avg:98.84ms
step:1716/1750 train_time:169615ms step_avg:98.84ms
step:1717/1750 train_time:169719ms step_avg:98.85ms
step:1718/1750 train_time:169823ms step_avg:98.85ms
step:1719/1750 train_time:169930ms step_avg:98.85ms
step:1720/1750 train_time:170032ms step_avg:98.86ms
step:1721/1750 train_time:170136ms step_avg:98.86ms
step:1722/1750 train_time:170242ms step_avg:98.86ms
step:1723/1750 train_time:170345ms step_avg:98.87ms
step:1724/1750 train_time:170449ms step_avg:98.87ms
step:1725/1750 train_time:170553ms step_avg:98.87ms
step:1726/1750 train_time:170657ms step_avg:98.87ms
step:1727/1750 train_time:170761ms step_avg:98.88ms
step:1728/1750 train_time:170867ms step_avg:98.88ms
step:1729/1750 train_time:170971ms step_avg:98.88ms
step:1730/1750 train_time:171074ms step_avg:98.89ms
step:1731/1750 train_time:171180ms step_avg:98.89ms
step:1732/1750 train_time:171283ms step_avg:98.89ms
step:1733/1750 train_time:171387ms step_avg:98.90ms
step:1734/1750 train_time:171493ms step_avg:98.90ms
step:1735/1750 train_time:171596ms step_avg:98.90ms
step:1736/1750 train_time:171701ms step_avg:98.91ms
step:1737/1750 train_time:171805ms step_avg:98.91ms
step:1738/1750 train_time:171909ms step_avg:98.91ms
step:1739/1750 train_time:172012ms step_avg:98.91ms
step:1740/1750 train_time:172117ms step_avg:98.92ms
step:1741/1750 train_time:172227ms step_avg:98.92ms
step:1742/1750 train_time:172331ms step_avg:98.93ms
step:1743/1750 train_time:172436ms step_avg:98.93ms
step:1744/1750 train_time:172540ms step_avg:98.93ms
step:1745/1750 train_time:172645ms step_avg:98.94ms
step:1746/1750 train_time:172748ms step_avg:98.94ms
step:1747/1750 train_time:172852ms step_avg:98.94ms
step:1748/1750 train_time:172957ms step_avg:98.95ms
step:1749/1750 train_time:173060ms step_avg:98.95ms
step:1750/1750 train_time:173167ms step_avg:98.95ms
step:1750/1750 val_loss:3.2817 train_time:173265ms step_avg:99.01ms
peak memory allocated: 33277 MiB reserved: 48572 MiB

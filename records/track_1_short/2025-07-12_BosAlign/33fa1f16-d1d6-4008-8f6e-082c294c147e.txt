import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn, autocast
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
#import wandb

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)

        params = list(params)
        sizes = {p.shape for p in params}

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params,))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * self.world_size
            for base_i in range(0, len(params), self.world_size):
                if base_i + self.rank < len(params):
                    grad = params[base_i + self.rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + self.world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * self.world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), self.world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                futures.append(dist.all_gather(params_pad[base_i:base_i + self.world_size], params_pad[base_i + self.rank], async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01, rank: int = 0, world_size: int = 1):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        self.rank = rank
        self.world_size = world_size

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(
                params=group_params,
            ))
        super().__init__(param_groups, defaults)

    @torch.compile
    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // self.world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)

                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']

                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)

                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t

                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        for param in self.embed.parameters():
            param.lr_mul = 75.
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embeds in self.value_embeds:
            for param in self.value_embeds.parameters():
                param.lr_mul = 75.
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.lr_mul = 27.5
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % world_size
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            #return causal_mask
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, world_size: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos:pos+max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()

    starts = []
    batch_end=None
    for i in range(len(boundary_positions) - 1):
        end = boundary_positions[i + 1].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == world_size:
                batch_end = end
                break
            start = end
    assert batch_end is not None # increase max_batch_span if necessary
    batch_span = batch_end-pos
    return starts, batch_span

def distributed_data_generator(filename_pattern: str, batch_size: int, rank: int, world_size: int, align_to_bos: bool):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    batch_span = batch_size
    max_batch_span = 2*batch_size if align_to_bos else batch_size #provide buffer to handle samples up to length local_batch_size

    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, world_size, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    train_align_to_bos = True # align local batch start indicies with next bos_token
    val_align_to_bos = False # False to maintain same eval as prior records
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

#if master_process:
#    wandb.init(project="modded-nanogpt-tiny", name=f"run-{os.path.basename(__file__)}", save_code=True)

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=True):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
if master_process:
    print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=next_multiple_of_n(50257, n=128), num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094

optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0, rank=rank, world_size=world_size)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]

for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

for n, p in model.named_parameters():
    wd_mul = getattr(p, "wd_mul", 1.0)
    lr_mul = getattr(p, "lr_mul", 1.0)

    print0(f"{n}: {p.shape} {p.dtype} {wd_mul} {lr_mul}")

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
embedding_params = sum(p.numel() for n, p in model.named_parameters() if "embed" in n)
non_embedding_params = total_params - embedding_params

print0(f"")
print0(f"Model parameters:")
print0(f"  Total parameters: {total_params:,}")
print0(f"  Embedding parameters: {embedding_params:,}")
print0(f"  Non-embedding parameters: {non_embedding_params:,}")

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    w = min((1 - x) / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.05
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)

torch.cuda.synchronize()
dist.barrier()
with torch.profiler.profile() as prof:
    for _ in range(warmup_steps):
        inputs, targets = next(train_loader)
        model(inputs, targets, get_window_size_blocks(1)).backward()
        for opt in optimizers:
            opt.step()
    model.zero_grad(set_to_none=True)
    torch.cuda.synchronize()
    dist.barrier()
os.makedirs("traces", exist_ok=True)
prof.export_chrome_trace(f"traces/trace_{rank}.json")

model.load_state_dict(initial_state['model'])
for opt, opt_state in zip(optimizers, initial_state['optimizers']):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size, args.val_align_to_bos)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        #if master_process:
        #    wandb.log({"val/loss": val_loss}, step=step)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.8.0.dev20250524+cu126 compiled for CUDA 12.6
Sun Jul 13 00:51:21 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    5856MiB /  81559MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   36C    P0            120W /  700W |    1517MiB /  81559MiB |      2%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   38C    P0            123W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   31C    P0            118W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   38C    P0            120W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   38C    P0            123W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   31C    P0            115W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           63702      C   /usr/bin/python3                       1508MiB |
|    0   N/A  N/A           63703      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           63704      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           63705      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           63706      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           63707      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           63708      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           63709      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A           63703      C   /usr/bin/python3                       1508MiB |
|    2   N/A  N/A           63704      C   /usr/bin/python3                       1508MiB |
|    3   N/A  N/A           63705      C   /usr/bin/python3                       1508MiB |
|    4   N/A  N/A           63706      C   /usr/bin/python3                       1508MiB |
|    5   N/A  N/A           63707      C   /usr/bin/python3                       1508MiB |
|    6   N/A  N/A           63708      C   /usr/bin/python3                       1508MiB |
|    7   N/A  N/A           63709      C   /usr/bin/python3                       1508MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
scalars: torch.Size([64]) torch.float32 1.0 5.0
embed.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.0.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.1.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.2.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
blocks.0.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.0.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.1.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.1.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.2.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.2.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.3.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.3.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.4.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.4.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.5.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.5.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.6.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.6.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.7.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.7.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.8.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.8.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.9.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.9.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.10.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.10.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.11.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.11.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
lm_head.weight: torch.Size([50304, 768]) torch.float32 1.0 27.5

Model parameters:
  Total parameters: 275,742,784
  Embedding parameters: 154,533,888
  Non-embedding parameters: 121,208,896
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1750 train_time:153ms step_avg:153.40ms
step:2/1750 train_time:179ms step_avg:89.45ms
step:3/1750 train_time:250ms step_avg:83.40ms
step:4/1750 train_time:342ms step_avg:85.39ms
step:5/1750 train_time:434ms step_avg:86.73ms
step:6/1750 train_time:526ms step_avg:87.63ms
step:7/1750 train_time:618ms step_avg:88.31ms
step:8/1750 train_time:711ms step_avg:88.83ms
step:9/1750 train_time:802ms step_avg:89.17ms
step:10/1750 train_time:897ms step_avg:89.68ms
step:11/1750 train_time:988ms step_avg:89.78ms
step:12/1750 train_time:1082ms step_avg:90.13ms
step:13/1750 train_time:1177ms step_avg:90.56ms
step:14/1750 train_time:1272ms step_avg:90.88ms
step:15/1750 train_time:1367ms step_avg:91.11ms
step:16/1750 train_time:1459ms step_avg:91.19ms
step:17/1750 train_time:1551ms step_avg:91.26ms
step:18/1750 train_time:1644ms step_avg:91.35ms
step:19/1750 train_time:1737ms step_avg:91.42ms
step:20/1750 train_time:1830ms step_avg:91.49ms
step:21/1750 train_time:1922ms step_avg:91.54ms
step:22/1750 train_time:2015ms step_avg:91.59ms
step:23/1750 train_time:2108ms step_avg:91.66ms
step:24/1750 train_time:2202ms step_avg:91.76ms
step:25/1750 train_time:2297ms step_avg:91.87ms
step:26/1750 train_time:2391ms step_avg:91.95ms
step:27/1750 train_time:2483ms step_avg:91.98ms
step:28/1750 train_time:2577ms step_avg:92.03ms
step:29/1750 train_time:2670ms step_avg:92.06ms
step:30/1750 train_time:2763ms step_avg:92.09ms
step:31/1750 train_time:2855ms step_avg:92.11ms
step:32/1750 train_time:2948ms step_avg:92.13ms
step:33/1750 train_time:3041ms step_avg:92.14ms
step:34/1750 train_time:3134ms step_avg:92.17ms
step:35/1750 train_time:3227ms step_avg:92.20ms
step:36/1750 train_time:3321ms step_avg:92.26ms
step:37/1750 train_time:3415ms step_avg:92.30ms
step:38/1750 train_time:3509ms step_avg:92.33ms
step:39/1750 train_time:3602ms step_avg:92.35ms
step:40/1750 train_time:3696ms step_avg:92.39ms
step:41/1750 train_time:3789ms step_avg:92.41ms
step:42/1750 train_time:3882ms step_avg:92.43ms
step:43/1750 train_time:3975ms step_avg:92.45ms
step:44/1750 train_time:4069ms step_avg:92.47ms
step:45/1750 train_time:4161ms step_avg:92.48ms
step:46/1750 train_time:4255ms step_avg:92.50ms
step:47/1750 train_time:4348ms step_avg:92.51ms
step:48/1750 train_time:4441ms step_avg:92.53ms
step:49/1750 train_time:4535ms step_avg:92.55ms
step:50/1750 train_time:4628ms step_avg:92.56ms
step:51/1750 train_time:4721ms step_avg:92.57ms
step:52/1750 train_time:4814ms step_avg:92.57ms
step:53/1750 train_time:4908ms step_avg:92.61ms
step:54/1750 train_time:5000ms step_avg:92.58ms
step:55/1750 train_time:5093ms step_avg:92.60ms
step:56/1750 train_time:5187ms step_avg:92.63ms
step:57/1750 train_time:5279ms step_avg:92.61ms
step:58/1750 train_time:5372ms step_avg:92.62ms
step:59/1750 train_time:5465ms step_avg:92.63ms
step:60/1750 train_time:5559ms step_avg:92.64ms
step:61/1750 train_time:5652ms step_avg:92.65ms
step:62/1750 train_time:5744ms step_avg:92.65ms
step:63/1750 train_time:5838ms step_avg:92.66ms
step:64/1750 train_time:5930ms step_avg:92.66ms
step:65/1750 train_time:6025ms step_avg:92.69ms
step:66/1750 train_time:6119ms step_avg:92.71ms
step:67/1750 train_time:6210ms step_avg:92.69ms
step:68/1750 train_time:6303ms step_avg:92.70ms
step:69/1750 train_time:6397ms step_avg:92.71ms
step:70/1750 train_time:6490ms step_avg:92.72ms
step:71/1750 train_time:6583ms step_avg:92.72ms
step:72/1750 train_time:6676ms step_avg:92.72ms
step:73/1750 train_time:6769ms step_avg:92.72ms
step:74/1750 train_time:6862ms step_avg:92.73ms
step:75/1750 train_time:6955ms step_avg:92.73ms
step:76/1750 train_time:7048ms step_avg:92.73ms
step:77/1750 train_time:7141ms step_avg:92.74ms
step:78/1750 train_time:7233ms step_avg:92.73ms
step:79/1750 train_time:7326ms step_avg:92.73ms
step:80/1750 train_time:7420ms step_avg:92.74ms
step:81/1750 train_time:7513ms step_avg:92.75ms
step:82/1750 train_time:7606ms step_avg:92.76ms
step:83/1750 train_time:7700ms step_avg:92.77ms
step:84/1750 train_time:7793ms step_avg:92.77ms
step:85/1750 train_time:7886ms step_avg:92.77ms
step:86/1750 train_time:7979ms step_avg:92.78ms
step:87/1750 train_time:8072ms step_avg:92.78ms
step:88/1750 train_time:8165ms step_avg:92.79ms
step:89/1750 train_time:8258ms step_avg:92.79ms
step:90/1750 train_time:8352ms step_avg:92.80ms
step:91/1750 train_time:8445ms step_avg:92.80ms
step:92/1750 train_time:8538ms step_avg:92.81ms
step:93/1750 train_time:8632ms step_avg:92.81ms
step:94/1750 train_time:8725ms step_avg:92.81ms
step:95/1750 train_time:8818ms step_avg:92.82ms
step:96/1750 train_time:8913ms step_avg:92.84ms
step:97/1750 train_time:9004ms step_avg:92.82ms
step:98/1750 train_time:9097ms step_avg:92.83ms
step:99/1750 train_time:9191ms step_avg:92.84ms
step:100/1750 train_time:9284ms step_avg:92.84ms
step:101/1750 train_time:9377ms step_avg:92.84ms
step:102/1750 train_time:9470ms step_avg:92.85ms
step:103/1750 train_time:9564ms step_avg:92.85ms
step:104/1750 train_time:9657ms step_avg:92.85ms
step:105/1750 train_time:9750ms step_avg:92.86ms
step:106/1750 train_time:9843ms step_avg:92.86ms
step:107/1750 train_time:9936ms step_avg:92.86ms
step:108/1750 train_time:10029ms step_avg:92.87ms
step:109/1750 train_time:10123ms step_avg:92.87ms
step:110/1750 train_time:10216ms step_avg:92.88ms
step:111/1750 train_time:10309ms step_avg:92.88ms
step:112/1750 train_time:10402ms step_avg:92.88ms
step:113/1750 train_time:10496ms step_avg:92.88ms
step:114/1750 train_time:10589ms step_avg:92.89ms
step:115/1750 train_time:10683ms step_avg:92.89ms
step:116/1750 train_time:10776ms step_avg:92.90ms
step:117/1750 train_time:10870ms step_avg:92.90ms
step:118/1750 train_time:10962ms step_avg:92.90ms
step:119/1750 train_time:11055ms step_avg:92.90ms
step:120/1750 train_time:11150ms step_avg:92.92ms
step:121/1750 train_time:11241ms step_avg:92.90ms
step:122/1750 train_time:11334ms step_avg:92.90ms
step:123/1750 train_time:11427ms step_avg:92.91ms
step:124/1750 train_time:11521ms step_avg:92.91ms
step:125/1750 train_time:11614ms step_avg:92.91ms
step:125/1750 val_loss:4.6446 train_time:11702ms step_avg:93.62ms
step:126/1750 train_time:11730ms step_avg:93.09ms
step:127/1750 train_time:11809ms step_avg:92.98ms
step:128/1750 train_time:11909ms step_avg:93.04ms
step:129/1750 train_time:12005ms step_avg:93.06ms
step:130/1750 train_time:12098ms step_avg:93.06ms
step:131/1750 train_time:12190ms step_avg:93.06ms
step:132/1750 train_time:12284ms step_avg:93.06ms
step:133/1750 train_time:12377ms step_avg:93.06ms
step:134/1750 train_time:12470ms step_avg:93.06ms
step:135/1750 train_time:12563ms step_avg:93.06ms
step:136/1750 train_time:12656ms step_avg:93.06ms
step:137/1750 train_time:12749ms step_avg:93.06ms
step:138/1750 train_time:12844ms step_avg:93.07ms
step:139/1750 train_time:12940ms step_avg:93.09ms
step:140/1750 train_time:13034ms step_avg:93.10ms
step:141/1750 train_time:13128ms step_avg:93.10ms
step:142/1750 train_time:13221ms step_avg:93.11ms
step:143/1750 train_time:13315ms step_avg:93.11ms
step:144/1750 train_time:13408ms step_avg:93.11ms
step:145/1750 train_time:13501ms step_avg:93.11ms
step:146/1750 train_time:13594ms step_avg:93.11ms
step:147/1750 train_time:13688ms step_avg:93.11ms
step:148/1750 train_time:13781ms step_avg:93.12ms
step:149/1750 train_time:13875ms step_avg:93.12ms
step:150/1750 train_time:13970ms step_avg:93.13ms
step:151/1750 train_time:14065ms step_avg:93.14ms
step:152/1750 train_time:14159ms step_avg:93.15ms
step:153/1750 train_time:14252ms step_avg:93.15ms
step:154/1750 train_time:14346ms step_avg:93.16ms
step:155/1750 train_time:14439ms step_avg:93.16ms
step:156/1750 train_time:14532ms step_avg:93.16ms
step:157/1750 train_time:14626ms step_avg:93.16ms
step:158/1750 train_time:14719ms step_avg:93.16ms
step:159/1750 train_time:14813ms step_avg:93.16ms
step:160/1750 train_time:14907ms step_avg:93.17ms
step:161/1750 train_time:15001ms step_avg:93.17ms
step:162/1750 train_time:15095ms step_avg:93.18ms
step:163/1750 train_time:15189ms step_avg:93.18ms
step:164/1750 train_time:15284ms step_avg:93.20ms
step:165/1750 train_time:15377ms step_avg:93.20ms
step:166/1750 train_time:15469ms step_avg:93.19ms
step:167/1750 train_time:15563ms step_avg:93.19ms
step:168/1750 train_time:15657ms step_avg:93.20ms
step:169/1750 train_time:15751ms step_avg:93.20ms
step:170/1750 train_time:15845ms step_avg:93.20ms
step:171/1750 train_time:15939ms step_avg:93.21ms
step:172/1750 train_time:16033ms step_avg:93.22ms
step:173/1750 train_time:16127ms step_avg:93.22ms
step:174/1750 train_time:16221ms step_avg:93.22ms
step:175/1750 train_time:16315ms step_avg:93.23ms
step:176/1750 train_time:16409ms step_avg:93.23ms
step:177/1750 train_time:16502ms step_avg:93.23ms
step:178/1750 train_time:16597ms step_avg:93.24ms
step:179/1750 train_time:16690ms step_avg:93.24ms
step:180/1750 train_time:16784ms step_avg:93.24ms
step:181/1750 train_time:16878ms step_avg:93.25ms
step:182/1750 train_time:16971ms step_avg:93.25ms
step:183/1750 train_time:17065ms step_avg:93.25ms
step:184/1750 train_time:17159ms step_avg:93.25ms
step:185/1750 train_time:17252ms step_avg:93.25ms
step:186/1750 train_time:17345ms step_avg:93.26ms
step:187/1750 train_time:17439ms step_avg:93.26ms
step:188/1750 train_time:17533ms step_avg:93.26ms
step:189/1750 train_time:17626ms step_avg:93.26ms
step:190/1750 train_time:17719ms step_avg:93.26ms
step:191/1750 train_time:17813ms step_avg:93.26ms
step:192/1750 train_time:17907ms step_avg:93.26ms
step:193/1750 train_time:18001ms step_avg:93.27ms
step:194/1750 train_time:18095ms step_avg:93.27ms
step:195/1750 train_time:18189ms step_avg:93.28ms
step:196/1750 train_time:18283ms step_avg:93.28ms
step:197/1750 train_time:18376ms step_avg:93.28ms
step:198/1750 train_time:18470ms step_avg:93.28ms
step:199/1750 train_time:18563ms step_avg:93.28ms
step:200/1750 train_time:18657ms step_avg:93.28ms
step:201/1750 train_time:18750ms step_avg:93.28ms
step:202/1750 train_time:18844ms step_avg:93.29ms
step:203/1750 train_time:18937ms step_avg:93.29ms
step:204/1750 train_time:19030ms step_avg:93.29ms
step:205/1750 train_time:19125ms step_avg:93.29ms
step:206/1750 train_time:19219ms step_avg:93.30ms
step:207/1750 train_time:19313ms step_avg:93.30ms
step:208/1750 train_time:19407ms step_avg:93.30ms
step:209/1750 train_time:19500ms step_avg:93.30ms
step:210/1750 train_time:19594ms step_avg:93.30ms
step:211/1750 train_time:19688ms step_avg:93.31ms
step:212/1750 train_time:19781ms step_avg:93.31ms
step:213/1750 train_time:19875ms step_avg:93.31ms
step:214/1750 train_time:19968ms step_avg:93.31ms
step:215/1750 train_time:20061ms step_avg:93.31ms
step:216/1750 train_time:20155ms step_avg:93.31ms
step:217/1750 train_time:20249ms step_avg:93.31ms
step:218/1750 train_time:20343ms step_avg:93.32ms
step:219/1750 train_time:20438ms step_avg:93.32ms
step:220/1750 train_time:20531ms step_avg:93.32ms
step:221/1750 train_time:20625ms step_avg:93.33ms
step:222/1750 train_time:20720ms step_avg:93.33ms
step:223/1750 train_time:20814ms step_avg:93.34ms
step:224/1750 train_time:20907ms step_avg:93.33ms
step:225/1750 train_time:21000ms step_avg:93.33ms
step:226/1750 train_time:21094ms step_avg:93.34ms
step:227/1750 train_time:21188ms step_avg:93.34ms
step:228/1750 train_time:21281ms step_avg:93.34ms
step:229/1750 train_time:21375ms step_avg:93.34ms
step:230/1750 train_time:21468ms step_avg:93.34ms
step:231/1750 train_time:21561ms step_avg:93.34ms
step:232/1750 train_time:21656ms step_avg:93.34ms
step:233/1750 train_time:21750ms step_avg:93.35ms
step:234/1750 train_time:21844ms step_avg:93.35ms
step:235/1750 train_time:21938ms step_avg:93.35ms
step:236/1750 train_time:22032ms step_avg:93.36ms
step:237/1750 train_time:22126ms step_avg:93.36ms
step:238/1750 train_time:22220ms step_avg:93.36ms
step:239/1750 train_time:22313ms step_avg:93.36ms
step:240/1750 train_time:22407ms step_avg:93.36ms
step:241/1750 train_time:22501ms step_avg:93.36ms
step:242/1750 train_time:22594ms step_avg:93.36ms
step:243/1750 train_time:22688ms step_avg:93.37ms
step:244/1750 train_time:22782ms step_avg:93.37ms
step:245/1750 train_time:22876ms step_avg:93.37ms
step:246/1750 train_time:22970ms step_avg:93.38ms
step:247/1750 train_time:23063ms step_avg:93.37ms
step:248/1750 train_time:23156ms step_avg:93.37ms
step:249/1750 train_time:23250ms step_avg:93.37ms
step:250/1750 train_time:23344ms step_avg:93.37ms
step:250/1750 val_loss:4.1013 train_time:23433ms step_avg:93.73ms
step:251/1750 train_time:23461ms step_avg:93.47ms
step:252/1750 train_time:23540ms step_avg:93.41ms
step:253/1750 train_time:23637ms step_avg:93.43ms
step:254/1750 train_time:23731ms step_avg:93.43ms
step:255/1750 train_time:23824ms step_avg:93.43ms
step:256/1750 train_time:23918ms step_avg:93.43ms
step:257/1750 train_time:24011ms step_avg:93.43ms
step:258/1750 train_time:24104ms step_avg:93.43ms
step:259/1750 train_time:24197ms step_avg:93.43ms
step:260/1750 train_time:24290ms step_avg:93.42ms
step:261/1750 train_time:24384ms step_avg:93.42ms
step:262/1750 train_time:24479ms step_avg:93.43ms
step:263/1750 train_time:24575ms step_avg:93.44ms
step:264/1750 train_time:24671ms step_avg:93.45ms
step:265/1750 train_time:24764ms step_avg:93.45ms
step:266/1750 train_time:24857ms step_avg:93.45ms
step:267/1750 train_time:24951ms step_avg:93.45ms
step:268/1750 train_time:25045ms step_avg:93.45ms
step:269/1750 train_time:25139ms step_avg:93.45ms
step:270/1750 train_time:25233ms step_avg:93.45ms
step:271/1750 train_time:25327ms step_avg:93.46ms
step:272/1750 train_time:25422ms step_avg:93.46ms
step:273/1750 train_time:25516ms step_avg:93.46ms
step:274/1750 train_time:25611ms step_avg:93.47ms
step:275/1750 train_time:25707ms step_avg:93.48ms
step:276/1750 train_time:25801ms step_avg:93.48ms
step:277/1750 train_time:25894ms step_avg:93.48ms
step:278/1750 train_time:25988ms step_avg:93.48ms
step:279/1750 train_time:26084ms step_avg:93.49ms
step:280/1750 train_time:26177ms step_avg:93.49ms
step:281/1750 train_time:26271ms step_avg:93.49ms
step:282/1750 train_time:26365ms step_avg:93.49ms
step:283/1750 train_time:26460ms step_avg:93.50ms
step:284/1750 train_time:26554ms step_avg:93.50ms
step:285/1750 train_time:26649ms step_avg:93.50ms
step:286/1750 train_time:26743ms step_avg:93.51ms
step:287/1750 train_time:26838ms step_avg:93.51ms
step:288/1750 train_time:26933ms step_avg:93.52ms
step:289/1750 train_time:27027ms step_avg:93.52ms
step:290/1750 train_time:27121ms step_avg:93.52ms
step:291/1750 train_time:27215ms step_avg:93.52ms
step:292/1750 train_time:27309ms step_avg:93.52ms
step:293/1750 train_time:27402ms step_avg:93.52ms
step:294/1750 train_time:27497ms step_avg:93.53ms
step:295/1750 train_time:27591ms step_avg:93.53ms
step:296/1750 train_time:27685ms step_avg:93.53ms
step:297/1750 train_time:27779ms step_avg:93.53ms
step:298/1750 train_time:27873ms step_avg:93.53ms
step:299/1750 train_time:27968ms step_avg:93.54ms
step:300/1750 train_time:28062ms step_avg:93.54ms
step:301/1750 train_time:28157ms step_avg:93.55ms
step:302/1750 train_time:28251ms step_avg:93.54ms
step:303/1750 train_time:28345ms step_avg:93.55ms
step:304/1750 train_time:28440ms step_avg:93.55ms
step:305/1750 train_time:28533ms step_avg:93.55ms
step:306/1750 train_time:28628ms step_avg:93.55ms
step:307/1750 train_time:28722ms step_avg:93.56ms
step:308/1750 train_time:28816ms step_avg:93.56ms
step:309/1750 train_time:28910ms step_avg:93.56ms
step:310/1750 train_time:29005ms step_avg:93.56ms
step:311/1750 train_time:29099ms step_avg:93.56ms
step:312/1750 train_time:29193ms step_avg:93.57ms
step:313/1750 train_time:29287ms step_avg:93.57ms
step:314/1750 train_time:29381ms step_avg:93.57ms
step:315/1750 train_time:29475ms step_avg:93.57ms
step:316/1750 train_time:29569ms step_avg:93.57ms
step:317/1750 train_time:29664ms step_avg:93.58ms
step:318/1750 train_time:29759ms step_avg:93.58ms
step:319/1750 train_time:29853ms step_avg:93.58ms
step:320/1750 train_time:29947ms step_avg:93.59ms
step:321/1750 train_time:30042ms step_avg:93.59ms
step:322/1750 train_time:30136ms step_avg:93.59ms
step:323/1750 train_time:30230ms step_avg:93.59ms
step:324/1750 train_time:30325ms step_avg:93.59ms
step:325/1750 train_time:30419ms step_avg:93.60ms
step:326/1750 train_time:30514ms step_avg:93.60ms
step:327/1750 train_time:30607ms step_avg:93.60ms
step:328/1750 train_time:30702ms step_avg:93.60ms
step:329/1750 train_time:30796ms step_avg:93.60ms
step:330/1750 train_time:30890ms step_avg:93.61ms
step:331/1750 train_time:30985ms step_avg:93.61ms
step:332/1750 train_time:31082ms step_avg:93.62ms
step:333/1750 train_time:31174ms step_avg:93.62ms
step:334/1750 train_time:31269ms step_avg:93.62ms
step:335/1750 train_time:31363ms step_avg:93.62ms
step:336/1750 train_time:31458ms step_avg:93.62ms
step:337/1750 train_time:31552ms step_avg:93.63ms
step:338/1750 train_time:31646ms step_avg:93.63ms
step:339/1750 train_time:31741ms step_avg:93.63ms
step:340/1750 train_time:31835ms step_avg:93.63ms
step:341/1750 train_time:31930ms step_avg:93.64ms
step:342/1750 train_time:32024ms step_avg:93.64ms
step:343/1750 train_time:32119ms step_avg:93.64ms
step:344/1750 train_time:32213ms step_avg:93.64ms
step:345/1750 train_time:32307ms step_avg:93.64ms
step:346/1750 train_time:32401ms step_avg:93.64ms
step:347/1750 train_time:32495ms step_avg:93.65ms
step:348/1750 train_time:32590ms step_avg:93.65ms
step:349/1750 train_time:32684ms step_avg:93.65ms
step:350/1750 train_time:32778ms step_avg:93.65ms
step:351/1750 train_time:32872ms step_avg:93.65ms
step:352/1750 train_time:32967ms step_avg:93.66ms
step:353/1750 train_time:33062ms step_avg:93.66ms
step:354/1750 train_time:33156ms step_avg:93.66ms
step:355/1750 train_time:33250ms step_avg:93.66ms
step:356/1750 train_time:33345ms step_avg:93.66ms
step:357/1750 train_time:33439ms step_avg:93.67ms
step:358/1750 train_time:33533ms step_avg:93.67ms
step:359/1750 train_time:33628ms step_avg:93.67ms
step:360/1750 train_time:33723ms step_avg:93.67ms
step:361/1750 train_time:33818ms step_avg:93.68ms
step:362/1750 train_time:33912ms step_avg:93.68ms
step:363/1750 train_time:34006ms step_avg:93.68ms
step:364/1750 train_time:34100ms step_avg:93.68ms
step:365/1750 train_time:34195ms step_avg:93.68ms
step:366/1750 train_time:34289ms step_avg:93.69ms
step:367/1750 train_time:34384ms step_avg:93.69ms
step:368/1750 train_time:34479ms step_avg:93.69ms
step:369/1750 train_time:34573ms step_avg:93.69ms
step:370/1750 train_time:34668ms step_avg:93.70ms
step:371/1750 train_time:34762ms step_avg:93.70ms
step:372/1750 train_time:34857ms step_avg:93.70ms
step:373/1750 train_time:34953ms step_avg:93.71ms
step:374/1750 train_time:35046ms step_avg:93.71ms
step:375/1750 train_time:35140ms step_avg:93.71ms
step:375/1750 val_loss:3.9005 train_time:35229ms step_avg:93.94ms
step:376/1750 train_time:35257ms step_avg:93.77ms
step:377/1750 train_time:35336ms step_avg:93.73ms
step:378/1750 train_time:35433ms step_avg:93.74ms
step:379/1750 train_time:35527ms step_avg:93.74ms
step:380/1750 train_time:35622ms step_avg:93.74ms
step:381/1750 train_time:35716ms step_avg:93.74ms
step:382/1750 train_time:35809ms step_avg:93.74ms
step:383/1750 train_time:35902ms step_avg:93.74ms
step:384/1750 train_time:35996ms step_avg:93.74ms
step:385/1750 train_time:36089ms step_avg:93.74ms
step:386/1750 train_time:36184ms step_avg:93.74ms
step:387/1750 train_time:36279ms step_avg:93.74ms
step:388/1750 train_time:36375ms step_avg:93.75ms
step:389/1750 train_time:36470ms step_avg:93.75ms
step:390/1750 train_time:36564ms step_avg:93.75ms
step:391/1750 train_time:36661ms step_avg:93.76ms
step:392/1750 train_time:36758ms step_avg:93.77ms
step:393/1750 train_time:36854ms step_avg:93.77ms
step:394/1750 train_time:36949ms step_avg:93.78ms
step:395/1750 train_time:37045ms step_avg:93.78ms
step:396/1750 train_time:37141ms step_avg:93.79ms
step:397/1750 train_time:37237ms step_avg:93.80ms
step:398/1750 train_time:37334ms step_avg:93.80ms
step:399/1750 train_time:37432ms step_avg:93.81ms
step:400/1750 train_time:37529ms step_avg:93.82ms
step:401/1750 train_time:37625ms step_avg:93.83ms
step:402/1750 train_time:37723ms step_avg:93.84ms
step:403/1750 train_time:37819ms step_avg:93.84ms
step:404/1750 train_time:37916ms step_avg:93.85ms
step:405/1750 train_time:38012ms step_avg:93.86ms
step:406/1750 train_time:38108ms step_avg:93.86ms
step:407/1750 train_time:38204ms step_avg:93.87ms
step:408/1750 train_time:38300ms step_avg:93.87ms
step:409/1750 train_time:38397ms step_avg:93.88ms
step:410/1750 train_time:38493ms step_avg:93.89ms
step:411/1750 train_time:38589ms step_avg:93.89ms
step:412/1750 train_time:38685ms step_avg:93.90ms
step:413/1750 train_time:38782ms step_avg:93.90ms
step:414/1750 train_time:38879ms step_avg:93.91ms
step:415/1750 train_time:38975ms step_avg:93.92ms
step:416/1750 train_time:39072ms step_avg:93.92ms
step:417/1750 train_time:39169ms step_avg:93.93ms
step:418/1750 train_time:39264ms step_avg:93.93ms
step:419/1750 train_time:39361ms step_avg:93.94ms
step:420/1750 train_time:39458ms step_avg:93.95ms
step:421/1750 train_time:39554ms step_avg:93.95ms
step:422/1750 train_time:39651ms step_avg:93.96ms
step:423/1750 train_time:39747ms step_avg:93.96ms
step:424/1750 train_time:39843ms step_avg:93.97ms
step:425/1750 train_time:39939ms step_avg:93.97ms
step:426/1750 train_time:40036ms step_avg:93.98ms
step:427/1750 train_time:40132ms step_avg:93.99ms
step:428/1750 train_time:40228ms step_avg:93.99ms
step:429/1750 train_time:40325ms step_avg:94.00ms
step:430/1750 train_time:40422ms step_avg:94.00ms
step:431/1750 train_time:40518ms step_avg:94.01ms
step:432/1750 train_time:40614ms step_avg:94.01ms
step:433/1750 train_time:40710ms step_avg:94.02ms
step:434/1750 train_time:40805ms step_avg:94.02ms
step:435/1750 train_time:40901ms step_avg:94.03ms
step:436/1750 train_time:40997ms step_avg:94.03ms
step:437/1750 train_time:41094ms step_avg:94.04ms
step:438/1750 train_time:41190ms step_avg:94.04ms
step:439/1750 train_time:41287ms step_avg:94.05ms
step:440/1750 train_time:41383ms step_avg:94.05ms
step:441/1750 train_time:41479ms step_avg:94.06ms
step:442/1750 train_time:41576ms step_avg:94.06ms
step:443/1750 train_time:41672ms step_avg:94.07ms
step:444/1750 train_time:41768ms step_avg:94.07ms
step:445/1750 train_time:41864ms step_avg:94.08ms
step:446/1750 train_time:41961ms step_avg:94.08ms
step:447/1750 train_time:42057ms step_avg:94.09ms
step:448/1750 train_time:42153ms step_avg:94.09ms
step:449/1750 train_time:42250ms step_avg:94.10ms
step:450/1750 train_time:42346ms step_avg:94.10ms
step:451/1750 train_time:42442ms step_avg:94.11ms
step:452/1750 train_time:42539ms step_avg:94.11ms
step:453/1750 train_time:42636ms step_avg:94.12ms
step:454/1750 train_time:42733ms step_avg:94.12ms
step:455/1750 train_time:42829ms step_avg:94.13ms
step:456/1750 train_time:42925ms step_avg:94.13ms
step:457/1750 train_time:43022ms step_avg:94.14ms
step:458/1750 train_time:43118ms step_avg:94.14ms
step:459/1750 train_time:43215ms step_avg:94.15ms
step:460/1750 train_time:43311ms step_avg:94.15ms
step:461/1750 train_time:43408ms step_avg:94.16ms
step:462/1750 train_time:43504ms step_avg:94.17ms
step:463/1750 train_time:43601ms step_avg:94.17ms
step:464/1750 train_time:43698ms step_avg:94.18ms
step:465/1750 train_time:43796ms step_avg:94.18ms
step:466/1750 train_time:43892ms step_avg:94.19ms
step:467/1750 train_time:43988ms step_avg:94.19ms
step:468/1750 train_time:44085ms step_avg:94.20ms
step:469/1750 train_time:44182ms step_avg:94.20ms
step:470/1750 train_time:44280ms step_avg:94.21ms
step:471/1750 train_time:44375ms step_avg:94.21ms
step:472/1750 train_time:44472ms step_avg:94.22ms
step:473/1750 train_time:44568ms step_avg:94.23ms
step:474/1750 train_time:44664ms step_avg:94.23ms
step:475/1750 train_time:44761ms step_avg:94.23ms
step:476/1750 train_time:44857ms step_avg:94.24ms
step:477/1750 train_time:44953ms step_avg:94.24ms
step:478/1750 train_time:45050ms step_avg:94.25ms
step:479/1750 train_time:45146ms step_avg:94.25ms
step:480/1750 train_time:45242ms step_avg:94.25ms
step:481/1750 train_time:45339ms step_avg:94.26ms
step:482/1750 train_time:45436ms step_avg:94.27ms
step:483/1750 train_time:45533ms step_avg:94.27ms
step:484/1750 train_time:45629ms step_avg:94.28ms
step:485/1750 train_time:45726ms step_avg:94.28ms
step:486/1750 train_time:45823ms step_avg:94.29ms
step:487/1750 train_time:45920ms step_avg:94.29ms
step:488/1750 train_time:46017ms step_avg:94.30ms
step:489/1750 train_time:46114ms step_avg:94.30ms
step:490/1750 train_time:46210ms step_avg:94.31ms
step:491/1750 train_time:46306ms step_avg:94.31ms
step:492/1750 train_time:46403ms step_avg:94.32ms
step:493/1750 train_time:46500ms step_avg:94.32ms
step:494/1750 train_time:46597ms step_avg:94.33ms
step:495/1750 train_time:46693ms step_avg:94.33ms
step:496/1750 train_time:46790ms step_avg:94.33ms
step:497/1750 train_time:46885ms step_avg:94.34ms
step:498/1750 train_time:46982ms step_avg:94.34ms
step:499/1750 train_time:47079ms step_avg:94.35ms
step:500/1750 train_time:47175ms step_avg:94.35ms
step:500/1750 val_loss:3.7521 train_time:47266ms step_avg:94.53ms
step:501/1750 train_time:47294ms step_avg:94.40ms
step:502/1750 train_time:47375ms step_avg:94.37ms
step:503/1750 train_time:47475ms step_avg:94.38ms
step:504/1750 train_time:47571ms step_avg:94.39ms
step:505/1750 train_time:47667ms step_avg:94.39ms
step:506/1750 train_time:47762ms step_avg:94.39ms
step:507/1750 train_time:47858ms step_avg:94.39ms
step:508/1750 train_time:47954ms step_avg:94.40ms
step:509/1750 train_time:48049ms step_avg:94.40ms
step:510/1750 train_time:48144ms step_avg:94.40ms
step:511/1750 train_time:48242ms step_avg:94.41ms
step:512/1750 train_time:48339ms step_avg:94.41ms
step:513/1750 train_time:48436ms step_avg:94.42ms
step:514/1750 train_time:48534ms step_avg:94.42ms
step:515/1750 train_time:48629ms step_avg:94.43ms
step:516/1750 train_time:48725ms step_avg:94.43ms
step:517/1750 train_time:48821ms step_avg:94.43ms
step:518/1750 train_time:48917ms step_avg:94.44ms
step:519/1750 train_time:49013ms step_avg:94.44ms
step:520/1750 train_time:49109ms step_avg:94.44ms
step:521/1750 train_time:49205ms step_avg:94.44ms
step:522/1750 train_time:49302ms step_avg:94.45ms
step:523/1750 train_time:49400ms step_avg:94.45ms
step:524/1750 train_time:49497ms step_avg:94.46ms
step:525/1750 train_time:49594ms step_avg:94.47ms
step:526/1750 train_time:49691ms step_avg:94.47ms
step:527/1750 train_time:49787ms step_avg:94.47ms
step:528/1750 train_time:49884ms step_avg:94.48ms
step:529/1750 train_time:49980ms step_avg:94.48ms
step:530/1750 train_time:50076ms step_avg:94.48ms
step:531/1750 train_time:50171ms step_avg:94.48ms
step:532/1750 train_time:50268ms step_avg:94.49ms
step:533/1750 train_time:50365ms step_avg:94.49ms
step:534/1750 train_time:50463ms step_avg:94.50ms
step:535/1750 train_time:50561ms step_avg:94.51ms
step:536/1750 train_time:50658ms step_avg:94.51ms
step:537/1750 train_time:50755ms step_avg:94.52ms
step:538/1750 train_time:50851ms step_avg:94.52ms
step:539/1750 train_time:50949ms step_avg:94.52ms
step:540/1750 train_time:51045ms step_avg:94.53ms
step:541/1750 train_time:51141ms step_avg:94.53ms
step:542/1750 train_time:51237ms step_avg:94.53ms
step:543/1750 train_time:51335ms step_avg:94.54ms
step:544/1750 train_time:51431ms step_avg:94.54ms
step:545/1750 train_time:51529ms step_avg:94.55ms
step:546/1750 train_time:51626ms step_avg:94.55ms
step:547/1750 train_time:51722ms step_avg:94.56ms
step:548/1750 train_time:51819ms step_avg:94.56ms
step:549/1750 train_time:51916ms step_avg:94.56ms
step:550/1750 train_time:52012ms step_avg:94.57ms
step:551/1750 train_time:52109ms step_avg:94.57ms
step:552/1750 train_time:52206ms step_avg:94.58ms
step:553/1750 train_time:52302ms step_avg:94.58ms
step:554/1750 train_time:52398ms step_avg:94.58ms
step:555/1750 train_time:52495ms step_avg:94.59ms
step:556/1750 train_time:52592ms step_avg:94.59ms
step:557/1750 train_time:52689ms step_avg:94.59ms
step:558/1750 train_time:52786ms step_avg:94.60ms
step:559/1750 train_time:52882ms step_avg:94.60ms
step:560/1750 train_time:52979ms step_avg:94.61ms
step:561/1750 train_time:53075ms step_avg:94.61ms
step:562/1750 train_time:53171ms step_avg:94.61ms
step:563/1750 train_time:53268ms step_avg:94.61ms
step:564/1750 train_time:53364ms step_avg:94.62ms
step:565/1750 train_time:53461ms step_avg:94.62ms
step:566/1750 train_time:53558ms step_avg:94.62ms
step:567/1750 train_time:53655ms step_avg:94.63ms
step:568/1750 train_time:53751ms step_avg:94.63ms
step:569/1750 train_time:53849ms step_avg:94.64ms
step:570/1750 train_time:53946ms step_avg:94.64ms
step:571/1750 train_time:54042ms step_avg:94.64ms
step:572/1750 train_time:54139ms step_avg:94.65ms
step:573/1750 train_time:54235ms step_avg:94.65ms
step:574/1750 train_time:54332ms step_avg:94.66ms
step:575/1750 train_time:54429ms step_avg:94.66ms
step:576/1750 train_time:54526ms step_avg:94.66ms
step:577/1750 train_time:54623ms step_avg:94.67ms
step:578/1750 train_time:54719ms step_avg:94.67ms
step:579/1750 train_time:54816ms step_avg:94.67ms
step:580/1750 train_time:54912ms step_avg:94.68ms
step:581/1750 train_time:55010ms step_avg:94.68ms
step:582/1750 train_time:55107ms step_avg:94.68ms
step:583/1750 train_time:55203ms step_avg:94.69ms
step:584/1750 train_time:55300ms step_avg:94.69ms
step:585/1750 train_time:55397ms step_avg:94.70ms
step:586/1750 train_time:55494ms step_avg:94.70ms
step:587/1750 train_time:55591ms step_avg:94.70ms
step:588/1750 train_time:55688ms step_avg:94.71ms
step:589/1750 train_time:55784ms step_avg:94.71ms
step:590/1750 train_time:55882ms step_avg:94.72ms
step:591/1750 train_time:55980ms step_avg:94.72ms
step:592/1750 train_time:56075ms step_avg:94.72ms
step:593/1750 train_time:56172ms step_avg:94.72ms
step:594/1750 train_time:56269ms step_avg:94.73ms
step:595/1750 train_time:56366ms step_avg:94.73ms
step:596/1750 train_time:56463ms step_avg:94.74ms
step:597/1750 train_time:56560ms step_avg:94.74ms
step:598/1750 train_time:56656ms step_avg:94.74ms
step:599/1750 train_time:56752ms step_avg:94.75ms
step:600/1750 train_time:56849ms step_avg:94.75ms
step:601/1750 train_time:56946ms step_avg:94.75ms
step:602/1750 train_time:57043ms step_avg:94.76ms
step:603/1750 train_time:57140ms step_avg:94.76ms
step:604/1750 train_time:57236ms step_avg:94.76ms
step:605/1750 train_time:57334ms step_avg:94.77ms
step:606/1750 train_time:57431ms step_avg:94.77ms
step:607/1750 train_time:57528ms step_avg:94.78ms
step:608/1750 train_time:57626ms step_avg:94.78ms
step:609/1750 train_time:57722ms step_avg:94.78ms
step:610/1750 train_time:57820ms step_avg:94.79ms
step:611/1750 train_time:57916ms step_avg:94.79ms
step:612/1750 train_time:58013ms step_avg:94.79ms
step:613/1750 train_time:58109ms step_avg:94.80ms
step:614/1750 train_time:58206ms step_avg:94.80ms
step:615/1750 train_time:58303ms step_avg:94.80ms
step:616/1750 train_time:58400ms step_avg:94.80ms
step:617/1750 train_time:58496ms step_avg:94.81ms
step:618/1750 train_time:58592ms step_avg:94.81ms
step:619/1750 train_time:58689ms step_avg:94.81ms
step:620/1750 train_time:58788ms step_avg:94.82ms
step:621/1750 train_time:58883ms step_avg:94.82ms
step:622/1750 train_time:58980ms step_avg:94.82ms
step:623/1750 train_time:59077ms step_avg:94.83ms
step:624/1750 train_time:59174ms step_avg:94.83ms
step:625/1750 train_time:59271ms step_avg:94.83ms
step:625/1750 val_loss:3.6620 train_time:59363ms step_avg:94.98ms
step:626/1750 train_time:59390ms step_avg:94.87ms
step:627/1750 train_time:59475ms step_avg:94.86ms
step:628/1750 train_time:59573ms step_avg:94.86ms
step:629/1750 train_time:59670ms step_avg:94.86ms
step:630/1750 train_time:59767ms step_avg:94.87ms
step:631/1750 train_time:59863ms step_avg:94.87ms
step:632/1750 train_time:59959ms step_avg:94.87ms
step:633/1750 train_time:60055ms step_avg:94.87ms
step:634/1750 train_time:60151ms step_avg:94.87ms
step:635/1750 train_time:60247ms step_avg:94.88ms
step:636/1750 train_time:60343ms step_avg:94.88ms
step:637/1750 train_time:60442ms step_avg:94.88ms
step:638/1750 train_time:60540ms step_avg:94.89ms
step:639/1750 train_time:60637ms step_avg:94.89ms
step:640/1750 train_time:60736ms step_avg:94.90ms
step:641/1750 train_time:60833ms step_avg:94.90ms
step:642/1750 train_time:60929ms step_avg:94.91ms
step:643/1750 train_time:61025ms step_avg:94.91ms
step:644/1750 train_time:61121ms step_avg:94.91ms
step:645/1750 train_time:61217ms step_avg:94.91ms
step:646/1750 train_time:61314ms step_avg:94.91ms
step:647/1750 train_time:61411ms step_avg:94.92ms
step:648/1750 train_time:61508ms step_avg:94.92ms
step:649/1750 train_time:61605ms step_avg:94.92ms
step:650/1750 train_time:61703ms step_avg:94.93ms
step:651/1750 train_time:61801ms step_avg:94.93ms
step:652/1750 train_time:61899ms step_avg:94.94ms
step:653/1750 train_time:61997ms step_avg:94.94ms
step:654/1750 train_time:62095ms step_avg:94.95ms
step:655/1750 train_time:62193ms step_avg:94.95ms
step:656/1750 train_time:62291ms step_avg:94.96ms
step:657/1750 train_time:62389ms step_avg:94.96ms
step:658/1750 train_time:62487ms step_avg:94.97ms
step:659/1750 train_time:62586ms step_avg:94.97ms
step:660/1750 train_time:62685ms step_avg:94.98ms
step:661/1750 train_time:62783ms step_avg:94.98ms
step:662/1750 train_time:62881ms step_avg:94.99ms
step:663/1750 train_time:62981ms step_avg:94.99ms
step:664/1750 train_time:63078ms step_avg:95.00ms
step:665/1750 train_time:63175ms step_avg:95.00ms
step:666/1750 train_time:63273ms step_avg:95.01ms
step:667/1750 train_time:63372ms step_avg:95.01ms
step:668/1750 train_time:63470ms step_avg:95.01ms
step:669/1750 train_time:63568ms step_avg:95.02ms
step:670/1750 train_time:63667ms step_avg:95.03ms
step:671/1750 train_time:63766ms step_avg:95.03ms
step:672/1750 train_time:63865ms step_avg:95.04ms
step:673/1750 train_time:63964ms step_avg:95.04ms
step:674/1750 train_time:64063ms step_avg:95.05ms
step:675/1750 train_time:64162ms step_avg:95.06ms
step:676/1750 train_time:64261ms step_avg:95.06ms
step:677/1750 train_time:64359ms step_avg:95.06ms
step:678/1750 train_time:64458ms step_avg:95.07ms
step:679/1750 train_time:64555ms step_avg:95.07ms
step:680/1750 train_time:64654ms step_avg:95.08ms
step:681/1750 train_time:64752ms step_avg:95.08ms
step:682/1750 train_time:64851ms step_avg:95.09ms
step:683/1750 train_time:64950ms step_avg:95.10ms
step:684/1750 train_time:65049ms step_avg:95.10ms
step:685/1750 train_time:65148ms step_avg:95.11ms
step:686/1750 train_time:65247ms step_avg:95.11ms
step:687/1750 train_time:65345ms step_avg:95.12ms
step:688/1750 train_time:65445ms step_avg:95.12ms
step:689/1750 train_time:65543ms step_avg:95.13ms
step:690/1750 train_time:65641ms step_avg:95.13ms
step:691/1750 train_time:65739ms step_avg:95.14ms
step:692/1750 train_time:65837ms step_avg:95.14ms
step:693/1750 train_time:65936ms step_avg:95.15ms
step:694/1750 train_time:66034ms step_avg:95.15ms
step:695/1750 train_time:66133ms step_avg:95.16ms
step:696/1750 train_time:66232ms step_avg:95.16ms
step:697/1750 train_time:66332ms step_avg:95.17ms
step:698/1750 train_time:66432ms step_avg:95.17ms
step:699/1750 train_time:66530ms step_avg:95.18ms
step:700/1750 train_time:66628ms step_avg:95.18ms
step:701/1750 train_time:66727ms step_avg:95.19ms
step:702/1750 train_time:66825ms step_avg:95.19ms
step:703/1750 train_time:66924ms step_avg:95.20ms
step:704/1750 train_time:67022ms step_avg:95.20ms
step:705/1750 train_time:67120ms step_avg:95.21ms
step:706/1750 train_time:67218ms step_avg:95.21ms
step:707/1750 train_time:67317ms step_avg:95.22ms
step:708/1750 train_time:67417ms step_avg:95.22ms
step:709/1750 train_time:67516ms step_avg:95.23ms
step:710/1750 train_time:67615ms step_avg:95.23ms
step:711/1750 train_time:67714ms step_avg:95.24ms
step:712/1750 train_time:67814ms step_avg:95.24ms
step:713/1750 train_time:67913ms step_avg:95.25ms
step:714/1750 train_time:68012ms step_avg:95.25ms
step:715/1750 train_time:68112ms step_avg:95.26ms
step:716/1750 train_time:68210ms step_avg:95.27ms
step:717/1750 train_time:68308ms step_avg:95.27ms
step:718/1750 train_time:68407ms step_avg:95.27ms
step:719/1750 train_time:68507ms step_avg:95.28ms
step:720/1750 train_time:68606ms step_avg:95.29ms
step:721/1750 train_time:68705ms step_avg:95.29ms
step:722/1750 train_time:68804ms step_avg:95.30ms
step:723/1750 train_time:68902ms step_avg:95.30ms
step:724/1750 train_time:69001ms step_avg:95.31ms
step:725/1750 train_time:69099ms step_avg:95.31ms
step:726/1750 train_time:69199ms step_avg:95.32ms
step:727/1750 train_time:69297ms step_avg:95.32ms
step:728/1750 train_time:69396ms step_avg:95.32ms
step:729/1750 train_time:69495ms step_avg:95.33ms
step:730/1750 train_time:69594ms step_avg:95.33ms
step:731/1750 train_time:69693ms step_avg:95.34ms
step:732/1750 train_time:69792ms step_avg:95.34ms
step:733/1750 train_time:69891ms step_avg:95.35ms
step:734/1750 train_time:69989ms step_avg:95.35ms
step:735/1750 train_time:70087ms step_avg:95.36ms
step:736/1750 train_time:70185ms step_avg:95.36ms
step:737/1750 train_time:70284ms step_avg:95.36ms
step:738/1750 train_time:70383ms step_avg:95.37ms
step:739/1750 train_time:70481ms step_avg:95.37ms
step:740/1750 train_time:70580ms step_avg:95.38ms
step:741/1750 train_time:70679ms step_avg:95.38ms
step:742/1750 train_time:70778ms step_avg:95.39ms
step:743/1750 train_time:70876ms step_avg:95.39ms
step:744/1750 train_time:70975ms step_avg:95.40ms
step:745/1750 train_time:71074ms step_avg:95.40ms
step:746/1750 train_time:71172ms step_avg:95.41ms
step:747/1750 train_time:71270ms step_avg:95.41ms
step:748/1750 train_time:71368ms step_avg:95.41ms
step:749/1750 train_time:71467ms step_avg:95.42ms
step:750/1750 train_time:71565ms step_avg:95.42ms
step:750/1750 val_loss:3.5994 train_time:71658ms step_avg:95.54ms
step:751/1750 train_time:71685ms step_avg:95.45ms
step:752/1750 train_time:71771ms step_avg:95.44ms
step:753/1750 train_time:71872ms step_avg:95.45ms
step:754/1750 train_time:71970ms step_avg:95.45ms
step:755/1750 train_time:72067ms step_avg:95.45ms
step:756/1750 train_time:72165ms step_avg:95.46ms
step:757/1750 train_time:72265ms step_avg:95.46ms
step:758/1750 train_time:72363ms step_avg:95.47ms
step:759/1750 train_time:72460ms step_avg:95.47ms
step:760/1750 train_time:72559ms step_avg:95.47ms
step:761/1750 train_time:72657ms step_avg:95.48ms
step:762/1750 train_time:72756ms step_avg:95.48ms
step:763/1750 train_time:72856ms step_avg:95.49ms
step:764/1750 train_time:72955ms step_avg:95.49ms
step:765/1750 train_time:73055ms step_avg:95.50ms
step:766/1750 train_time:73153ms step_avg:95.50ms
step:767/1750 train_time:73252ms step_avg:95.50ms
step:768/1750 train_time:73351ms step_avg:95.51ms
step:769/1750 train_time:73450ms step_avg:95.51ms
step:770/1750 train_time:73548ms step_avg:95.52ms
step:771/1750 train_time:73646ms step_avg:95.52ms
step:772/1750 train_time:73744ms step_avg:95.52ms
step:773/1750 train_time:73844ms step_avg:95.53ms
step:774/1750 train_time:73944ms step_avg:95.53ms
step:775/1750 train_time:74043ms step_avg:95.54ms
step:776/1750 train_time:74142ms step_avg:95.54ms
step:777/1750 train_time:74241ms step_avg:95.55ms
step:778/1750 train_time:74340ms step_avg:95.55ms
step:779/1750 train_time:74438ms step_avg:95.56ms
step:780/1750 train_time:74538ms step_avg:95.56ms
step:781/1750 train_time:74636ms step_avg:95.57ms
step:782/1750 train_time:74736ms step_avg:95.57ms
step:783/1750 train_time:74835ms step_avg:95.57ms
step:784/1750 train_time:74934ms step_avg:95.58ms
step:785/1750 train_time:75034ms step_avg:95.58ms
step:786/1750 train_time:75133ms step_avg:95.59ms
step:787/1750 train_time:75231ms step_avg:95.59ms
step:788/1750 train_time:75330ms step_avg:95.60ms
step:789/1750 train_time:75428ms step_avg:95.60ms
step:790/1750 train_time:75527ms step_avg:95.60ms
step:791/1750 train_time:75625ms step_avg:95.61ms
step:792/1750 train_time:75724ms step_avg:95.61ms
step:793/1750 train_time:75822ms step_avg:95.61ms
step:794/1750 train_time:75922ms step_avg:95.62ms
step:795/1750 train_time:76021ms step_avg:95.62ms
step:796/1750 train_time:76120ms step_avg:95.63ms
step:797/1750 train_time:76218ms step_avg:95.63ms
step:798/1750 train_time:76317ms step_avg:95.64ms
step:799/1750 train_time:76415ms step_avg:95.64ms
step:800/1750 train_time:76514ms step_avg:95.64ms
step:801/1750 train_time:76613ms step_avg:95.65ms
step:802/1750 train_time:76713ms step_avg:95.65ms
step:803/1750 train_time:76813ms step_avg:95.66ms
step:804/1750 train_time:76912ms step_avg:95.66ms
step:805/1750 train_time:77012ms step_avg:95.67ms
step:806/1750 train_time:77111ms step_avg:95.67ms
step:807/1750 train_time:77209ms step_avg:95.67ms
step:808/1750 train_time:77308ms step_avg:95.68ms
step:809/1750 train_time:77407ms step_avg:95.68ms
step:810/1750 train_time:77505ms step_avg:95.68ms
step:811/1750 train_time:77603ms step_avg:95.69ms
step:812/1750 train_time:77703ms step_avg:95.69ms
step:813/1750 train_time:77801ms step_avg:95.70ms
step:814/1750 train_time:77900ms step_avg:95.70ms
step:815/1750 train_time:77998ms step_avg:95.70ms
step:816/1750 train_time:78097ms step_avg:95.71ms
step:817/1750 train_time:78196ms step_avg:95.71ms
step:818/1750 train_time:78294ms step_avg:95.71ms
step:819/1750 train_time:78393ms step_avg:95.72ms
step:820/1750 train_time:78492ms step_avg:95.72ms
step:821/1750 train_time:78590ms step_avg:95.72ms
step:822/1750 train_time:78689ms step_avg:95.73ms
step:823/1750 train_time:78788ms step_avg:95.73ms
step:824/1750 train_time:78886ms step_avg:95.74ms
step:825/1750 train_time:78985ms step_avg:95.74ms
step:826/1750 train_time:79084ms step_avg:95.74ms
step:827/1750 train_time:79184ms step_avg:95.75ms
step:828/1750 train_time:79282ms step_avg:95.75ms
step:829/1750 train_time:79382ms step_avg:95.76ms
step:830/1750 train_time:79481ms step_avg:95.76ms
step:831/1750 train_time:79580ms step_avg:95.76ms
step:832/1750 train_time:79678ms step_avg:95.77ms
step:833/1750 train_time:79776ms step_avg:95.77ms
step:834/1750 train_time:79875ms step_avg:95.77ms
step:835/1750 train_time:79973ms step_avg:95.78ms
step:836/1750 train_time:80074ms step_avg:95.78ms
step:837/1750 train_time:80173ms step_avg:95.79ms
step:838/1750 train_time:80272ms step_avg:95.79ms
step:839/1750 train_time:80372ms step_avg:95.79ms
step:840/1750 train_time:80470ms step_avg:95.80ms
step:841/1750 train_time:80570ms step_avg:95.80ms
step:842/1750 train_time:80668ms step_avg:95.81ms
step:843/1750 train_time:80767ms step_avg:95.81ms
step:844/1750 train_time:80867ms step_avg:95.81ms
step:845/1750 train_time:80967ms step_avg:95.82ms
step:846/1750 train_time:81067ms step_avg:95.82ms
step:847/1750 train_time:81166ms step_avg:95.83ms
step:848/1750 train_time:81265ms step_avg:95.83ms
step:849/1750 train_time:81364ms step_avg:95.84ms
step:850/1750 train_time:81464ms step_avg:95.84ms
step:851/1750 train_time:81563ms step_avg:95.84ms
step:852/1750 train_time:81661ms step_avg:95.85ms
step:853/1750 train_time:81760ms step_avg:95.85ms
step:854/1750 train_time:81858ms step_avg:95.85ms
step:855/1750 train_time:81956ms step_avg:95.86ms
step:856/1750 train_time:82055ms step_avg:95.86ms
step:857/1750 train_time:82154ms step_avg:95.86ms
step:858/1750 train_time:82253ms step_avg:95.87ms
step:859/1750 train_time:82353ms step_avg:95.87ms
step:860/1750 train_time:82453ms step_avg:95.88ms
step:861/1750 train_time:82554ms step_avg:95.88ms
step:862/1750 train_time:82652ms step_avg:95.88ms
step:863/1750 train_time:82751ms step_avg:95.89ms
step:864/1750 train_time:82851ms step_avg:95.89ms
step:865/1750 train_time:82950ms step_avg:95.90ms
step:866/1750 train_time:83048ms step_avg:95.90ms
step:867/1750 train_time:83147ms step_avg:95.90ms
step:868/1750 train_time:83246ms step_avg:95.90ms
step:869/1750 train_time:83345ms step_avg:95.91ms
step:870/1750 train_time:83444ms step_avg:95.91ms
step:871/1750 train_time:83543ms step_avg:95.92ms
step:872/1750 train_time:83643ms step_avg:95.92ms
step:873/1750 train_time:83743ms step_avg:95.93ms
step:874/1750 train_time:83843ms step_avg:95.93ms
step:875/1750 train_time:83943ms step_avg:95.94ms
step:875/1750 val_loss:3.5480 train_time:84036ms step_avg:96.04ms
step:876/1750 train_time:84064ms step_avg:95.96ms
step:877/1750 train_time:84149ms step_avg:95.95ms
step:878/1750 train_time:84249ms step_avg:95.96ms
step:879/1750 train_time:84348ms step_avg:95.96ms
step:880/1750 train_time:84446ms step_avg:95.96ms
step:881/1750 train_time:84544ms step_avg:95.96ms
step:882/1750 train_time:84642ms step_avg:95.97ms
step:883/1750 train_time:84741ms step_avg:95.97ms
step:884/1750 train_time:84838ms step_avg:95.97ms
step:885/1750 train_time:84937ms step_avg:95.97ms
step:886/1750 train_time:85036ms step_avg:95.98ms
step:887/1750 train_time:85137ms step_avg:95.98ms
step:888/1750 train_time:85236ms step_avg:95.99ms
step:889/1750 train_time:85335ms step_avg:95.99ms
step:890/1750 train_time:85434ms step_avg:95.99ms
step:891/1750 train_time:85533ms step_avg:96.00ms
step:892/1750 train_time:85631ms step_avg:96.00ms
step:893/1750 train_time:85730ms step_avg:96.00ms
step:894/1750 train_time:85830ms step_avg:96.01ms
step:895/1750 train_time:85928ms step_avg:96.01ms
step:896/1750 train_time:86027ms step_avg:96.01ms
step:897/1750 train_time:86125ms step_avg:96.01ms
step:898/1750 train_time:86225ms step_avg:96.02ms
step:899/1750 train_time:86323ms step_avg:96.02ms
step:900/1750 train_time:86422ms step_avg:96.02ms
step:901/1750 train_time:86521ms step_avg:96.03ms
step:902/1750 train_time:86619ms step_avg:96.03ms
step:903/1750 train_time:86719ms step_avg:96.03ms
step:904/1750 train_time:86818ms step_avg:96.04ms
step:905/1750 train_time:86916ms step_avg:96.04ms
step:906/1750 train_time:87014ms step_avg:96.04ms
step:907/1750 train_time:87112ms step_avg:96.04ms
step:908/1750 train_time:87211ms step_avg:96.05ms
step:909/1750 train_time:87310ms step_avg:96.05ms
step:910/1750 train_time:87411ms step_avg:96.06ms
step:911/1750 train_time:87512ms step_avg:96.06ms
step:912/1750 train_time:87613ms step_avg:96.07ms
step:913/1750 train_time:87713ms step_avg:96.07ms
step:914/1750 train_time:87814ms step_avg:96.08ms
step:915/1750 train_time:87914ms step_avg:96.08ms
step:916/1750 train_time:88014ms step_avg:96.08ms
step:917/1750 train_time:88114ms step_avg:96.09ms
step:918/1750 train_time:88214ms step_avg:96.09ms
step:919/1750 train_time:88315ms step_avg:96.10ms
step:920/1750 train_time:88415ms step_avg:96.10ms
step:921/1750 train_time:88516ms step_avg:96.11ms
step:922/1750 train_time:88616ms step_avg:96.11ms
step:923/1750 train_time:88716ms step_avg:96.12ms
step:924/1750 train_time:88816ms step_avg:96.12ms
step:925/1750 train_time:88915ms step_avg:96.12ms
step:926/1750 train_time:89015ms step_avg:96.13ms
step:927/1750 train_time:89116ms step_avg:96.13ms
step:928/1750 train_time:89215ms step_avg:96.14ms
step:929/1750 train_time:89316ms step_avg:96.14ms
step:930/1750 train_time:89416ms step_avg:96.15ms
step:931/1750 train_time:89516ms step_avg:96.15ms
step:932/1750 train_time:89616ms step_avg:96.15ms
step:933/1750 train_time:89717ms step_avg:96.16ms
step:934/1750 train_time:89817ms step_avg:96.16ms
step:935/1750 train_time:89917ms step_avg:96.17ms
step:936/1750 train_time:90017ms step_avg:96.17ms
step:937/1750 train_time:90116ms step_avg:96.18ms
step:938/1750 train_time:90216ms step_avg:96.18ms
step:939/1750 train_time:90316ms step_avg:96.18ms
step:940/1750 train_time:90416ms step_avg:96.19ms
step:941/1750 train_time:90517ms step_avg:96.19ms
step:942/1750 train_time:90617ms step_avg:96.20ms
step:943/1750 train_time:90718ms step_avg:96.20ms
step:944/1750 train_time:90818ms step_avg:96.21ms
step:945/1750 train_time:90919ms step_avg:96.21ms
step:946/1750 train_time:91019ms step_avg:96.21ms
step:947/1750 train_time:91119ms step_avg:96.22ms
step:948/1750 train_time:91220ms step_avg:96.22ms
step:949/1750 train_time:91320ms step_avg:96.23ms
step:950/1750 train_time:91421ms step_avg:96.23ms
step:951/1750 train_time:91521ms step_avg:96.24ms
step:952/1750 train_time:91621ms step_avg:96.24ms
step:953/1750 train_time:91722ms step_avg:96.25ms
step:954/1750 train_time:91824ms step_avg:96.25ms
step:955/1750 train_time:91924ms step_avg:96.26ms
step:956/1750 train_time:92025ms step_avg:96.26ms
step:957/1750 train_time:92126ms step_avg:96.27ms
step:958/1750 train_time:92226ms step_avg:96.27ms
step:959/1750 train_time:92326ms step_avg:96.27ms
step:960/1750 train_time:92427ms step_avg:96.28ms
step:961/1750 train_time:92526ms step_avg:96.28ms
step:962/1750 train_time:92626ms step_avg:96.29ms
step:963/1750 train_time:92727ms step_avg:96.29ms
step:964/1750 train_time:92827ms step_avg:96.29ms
step:965/1750 train_time:92927ms step_avg:96.30ms
step:966/1750 train_time:93028ms step_avg:96.30ms
step:967/1750 train_time:93128ms step_avg:96.31ms
step:968/1750 train_time:93229ms step_avg:96.31ms
step:969/1750 train_time:93331ms step_avg:96.32ms
step:970/1750 train_time:93431ms step_avg:96.32ms
step:971/1750 train_time:93531ms step_avg:96.32ms
step:972/1750 train_time:93633ms step_avg:96.33ms
step:973/1750 train_time:93733ms step_avg:96.33ms
step:974/1750 train_time:93834ms step_avg:96.34ms
step:975/1750 train_time:93934ms step_avg:96.34ms
step:976/1750 train_time:94035ms step_avg:96.35ms
step:977/1750 train_time:94136ms step_avg:96.35ms
step:978/1750 train_time:94236ms step_avg:96.36ms
step:979/1750 train_time:94336ms step_avg:96.36ms
step:980/1750 train_time:94436ms step_avg:96.36ms
step:981/1750 train_time:94536ms step_avg:96.37ms
step:982/1750 train_time:94636ms step_avg:96.37ms
step:983/1750 train_time:94736ms step_avg:96.37ms
step:984/1750 train_time:94837ms step_avg:96.38ms
step:985/1750 train_time:94937ms step_avg:96.38ms
step:986/1750 train_time:95038ms step_avg:96.39ms
step:987/1750 train_time:95138ms step_avg:96.39ms
step:988/1750 train_time:95238ms step_avg:96.39ms
step:989/1750 train_time:95338ms step_avg:96.40ms
step:990/1750 train_time:95437ms step_avg:96.40ms
step:991/1750 train_time:95538ms step_avg:96.41ms
step:992/1750 train_time:95638ms step_avg:96.41ms
step:993/1750 train_time:95738ms step_avg:96.41ms
step:994/1750 train_time:95839ms step_avg:96.42ms
step:995/1750 train_time:95940ms step_avg:96.42ms
step:996/1750 train_time:96041ms step_avg:96.43ms
step:997/1750 train_time:96142ms step_avg:96.43ms
step:998/1750 train_time:96243ms step_avg:96.44ms
step:999/1750 train_time:96342ms step_avg:96.44ms
step:1000/1750 train_time:96443ms step_avg:96.44ms
step:1000/1750 val_loss:3.5086 train_time:96538ms step_avg:96.54ms
step:1001/1750 train_time:96566ms step_avg:96.47ms
step:1002/1750 train_time:96653ms step_avg:96.46ms
step:1003/1750 train_time:96755ms step_avg:96.47ms
step:1004/1750 train_time:96855ms step_avg:96.47ms
step:1005/1750 train_time:96955ms step_avg:96.47ms
step:1006/1750 train_time:97054ms step_avg:96.48ms
step:1007/1750 train_time:97154ms step_avg:96.48ms
step:1008/1750 train_time:97254ms step_avg:96.48ms
step:1009/1750 train_time:97353ms step_avg:96.48ms
step:1010/1750 train_time:97454ms step_avg:96.49ms
step:1011/1750 train_time:97556ms step_avg:96.49ms
step:1012/1750 train_time:97657ms step_avg:96.50ms
step:1013/1750 train_time:97759ms step_avg:96.50ms
step:1014/1750 train_time:97859ms step_avg:96.51ms
step:1015/1750 train_time:97959ms step_avg:96.51ms
step:1016/1750 train_time:98059ms step_avg:96.51ms
step:1017/1750 train_time:98158ms step_avg:96.52ms
step:1018/1750 train_time:98258ms step_avg:96.52ms
step:1019/1750 train_time:98357ms step_avg:96.52ms
step:1020/1750 train_time:98457ms step_avg:96.53ms
step:1021/1750 train_time:98557ms step_avg:96.53ms
step:1022/1750 train_time:98657ms step_avg:96.53ms
step:1023/1750 train_time:98758ms step_avg:96.54ms
step:1024/1750 train_time:98859ms step_avg:96.54ms
step:1025/1750 train_time:98960ms step_avg:96.55ms
step:1026/1750 train_time:99060ms step_avg:96.55ms
step:1027/1750 train_time:99160ms step_avg:96.55ms
step:1028/1750 train_time:99261ms step_avg:96.56ms
step:1029/1750 train_time:99361ms step_avg:96.56ms
step:1030/1750 train_time:99462ms step_avg:96.56ms
step:1031/1750 train_time:99563ms step_avg:96.57ms
step:1032/1750 train_time:99664ms step_avg:96.57ms
step:1033/1750 train_time:99764ms step_avg:96.58ms
step:1034/1750 train_time:99864ms step_avg:96.58ms
step:1035/1750 train_time:99963ms step_avg:96.58ms
step:1036/1750 train_time:100064ms step_avg:96.59ms
step:1037/1750 train_time:100165ms step_avg:96.59ms
step:1038/1750 train_time:100265ms step_avg:96.59ms
step:1039/1750 train_time:100365ms step_avg:96.60ms
step:1040/1750 train_time:100465ms step_avg:96.60ms
step:1041/1750 train_time:100566ms step_avg:96.60ms
step:1042/1750 train_time:100667ms step_avg:96.61ms
step:1043/1750 train_time:100767ms step_avg:96.61ms
step:1044/1750 train_time:100867ms step_avg:96.62ms
step:1045/1750 train_time:100967ms step_avg:96.62ms
step:1046/1750 train_time:101068ms step_avg:96.62ms
step:1047/1750 train_time:101171ms step_avg:96.63ms
step:1048/1750 train_time:101271ms step_avg:96.63ms
step:1049/1750 train_time:101373ms step_avg:96.64ms
step:1050/1750 train_time:101474ms step_avg:96.64ms
step:1051/1750 train_time:101575ms step_avg:96.65ms
step:1052/1750 train_time:101676ms step_avg:96.65ms
step:1053/1750 train_time:101776ms step_avg:96.65ms
step:1054/1750 train_time:101876ms step_avg:96.66ms
step:1055/1750 train_time:101977ms step_avg:96.66ms
step:1056/1750 train_time:102077ms step_avg:96.66ms
step:1057/1750 train_time:102178ms step_avg:96.67ms
step:1058/1750 train_time:102278ms step_avg:96.67ms
step:1059/1750 train_time:102378ms step_avg:96.67ms
step:1060/1750 train_time:102478ms step_avg:96.68ms
step:1061/1750 train_time:102579ms step_avg:96.68ms
step:1062/1750 train_time:102679ms step_avg:96.68ms
step:1063/1750 train_time:102780ms step_avg:96.69ms
step:1064/1750 train_time:102880ms step_avg:96.69ms
step:1065/1750 train_time:102981ms step_avg:96.70ms
step:1066/1750 train_time:103081ms step_avg:96.70ms
step:1067/1750 train_time:103182ms step_avg:96.70ms
step:1068/1750 train_time:103282ms step_avg:96.71ms
step:1069/1750 train_time:103383ms step_avg:96.71ms
step:1070/1750 train_time:103483ms step_avg:96.71ms
step:1071/1750 train_time:103584ms step_avg:96.72ms
step:1072/1750 train_time:103684ms step_avg:96.72ms
step:1073/1750 train_time:103785ms step_avg:96.72ms
step:1074/1750 train_time:103884ms step_avg:96.73ms
step:1075/1750 train_time:103985ms step_avg:96.73ms
step:1076/1750 train_time:104085ms step_avg:96.73ms
step:1077/1750 train_time:104186ms step_avg:96.74ms
step:1078/1750 train_time:104286ms step_avg:96.74ms
step:1079/1750 train_time:104386ms step_avg:96.74ms
step:1080/1750 train_time:104486ms step_avg:96.75ms
step:1081/1750 train_time:104587ms step_avg:96.75ms
step:1082/1750 train_time:104688ms step_avg:96.75ms
step:1083/1750 train_time:104788ms step_avg:96.76ms
step:1084/1750 train_time:104889ms step_avg:96.76ms
step:1085/1750 train_time:104990ms step_avg:96.77ms
step:1086/1750 train_time:105091ms step_avg:96.77ms
step:1087/1750 train_time:105192ms step_avg:96.77ms
step:1088/1750 train_time:105293ms step_avg:96.78ms
step:1089/1750 train_time:105394ms step_avg:96.78ms
step:1090/1750 train_time:105495ms step_avg:96.78ms
step:1091/1750 train_time:105596ms step_avg:96.79ms
step:1092/1750 train_time:105696ms step_avg:96.79ms
step:1093/1750 train_time:105796ms step_avg:96.79ms
step:1094/1750 train_time:105897ms step_avg:96.80ms
step:1095/1750 train_time:105996ms step_avg:96.80ms
step:1096/1750 train_time:106096ms step_avg:96.80ms
step:1097/1750 train_time:106196ms step_avg:96.81ms
step:1098/1750 train_time:106298ms step_avg:96.81ms
step:1099/1750 train_time:106398ms step_avg:96.81ms
step:1100/1750 train_time:106498ms step_avg:96.82ms
step:1101/1750 train_time:106598ms step_avg:96.82ms
step:1102/1750 train_time:106699ms step_avg:96.82ms
step:1103/1750 train_time:106799ms step_avg:96.83ms
step:1104/1750 train_time:106898ms step_avg:96.83ms
step:1105/1750 train_time:106999ms step_avg:96.83ms
step:1106/1750 train_time:107100ms step_avg:96.84ms
step:1107/1750 train_time:107202ms step_avg:96.84ms
step:1108/1750 train_time:107303ms step_avg:96.84ms
step:1109/1750 train_time:107404ms step_avg:96.85ms
step:1110/1750 train_time:107504ms step_avg:96.85ms
step:1111/1750 train_time:107604ms step_avg:96.85ms
step:1112/1750 train_time:107704ms step_avg:96.86ms
step:1113/1750 train_time:107805ms step_avg:96.86ms
step:1114/1750 train_time:107905ms step_avg:96.86ms
step:1115/1750 train_time:108006ms step_avg:96.87ms
step:1116/1750 train_time:108106ms step_avg:96.87ms
step:1117/1750 train_time:108207ms step_avg:96.87ms
step:1118/1750 train_time:108308ms step_avg:96.88ms
step:1119/1750 train_time:108408ms step_avg:96.88ms
step:1120/1750 train_time:108507ms step_avg:96.88ms
step:1121/1750 train_time:108607ms step_avg:96.88ms
step:1122/1750 train_time:108708ms step_avg:96.89ms
step:1123/1750 train_time:108808ms step_avg:96.89ms
step:1124/1750 train_time:108908ms step_avg:96.89ms
step:1125/1750 train_time:109010ms step_avg:96.90ms
step:1125/1750 val_loss:3.4547 train_time:109106ms step_avg:96.98ms
step:1126/1750 train_time:109133ms step_avg:96.92ms
step:1127/1750 train_time:109220ms step_avg:96.91ms
step:1128/1750 train_time:109320ms step_avg:96.92ms
step:1129/1750 train_time:109420ms step_avg:96.92ms
step:1130/1750 train_time:109520ms step_avg:96.92ms
step:1131/1750 train_time:109621ms step_avg:96.92ms
step:1132/1750 train_time:109721ms step_avg:96.93ms
step:1133/1750 train_time:109821ms step_avg:96.93ms
step:1134/1750 train_time:109921ms step_avg:96.93ms
step:1135/1750 train_time:110021ms step_avg:96.94ms
step:1136/1750 train_time:110124ms step_avg:96.94ms
step:1137/1750 train_time:110227ms step_avg:96.95ms
step:1138/1750 train_time:110328ms step_avg:96.95ms
step:1139/1750 train_time:110429ms step_avg:96.95ms
step:1140/1750 train_time:110530ms step_avg:96.96ms
step:1141/1750 train_time:110631ms step_avg:96.96ms
step:1142/1750 train_time:110731ms step_avg:96.96ms
step:1143/1750 train_time:110832ms step_avg:96.97ms
step:1144/1750 train_time:110932ms step_avg:96.97ms
step:1145/1750 train_time:111032ms step_avg:96.97ms
step:1146/1750 train_time:111132ms step_avg:96.97ms
step:1147/1750 train_time:111232ms step_avg:96.98ms
step:1148/1750 train_time:111333ms step_avg:96.98ms
step:1149/1750 train_time:111433ms step_avg:96.98ms
step:1150/1750 train_time:111534ms step_avg:96.99ms
step:1151/1750 train_time:111635ms step_avg:96.99ms
step:1152/1750 train_time:111736ms step_avg:96.99ms
step:1153/1750 train_time:111837ms step_avg:97.00ms
step:1154/1750 train_time:111937ms step_avg:97.00ms
step:1155/1750 train_time:112037ms step_avg:97.00ms
step:1156/1750 train_time:112138ms step_avg:97.00ms
step:1157/1750 train_time:112239ms step_avg:97.01ms
step:1158/1750 train_time:112340ms step_avg:97.01ms
step:1159/1750 train_time:112441ms step_avg:97.02ms
step:1160/1750 train_time:112542ms step_avg:97.02ms
step:1161/1750 train_time:112642ms step_avg:97.02ms
step:1162/1750 train_time:112743ms step_avg:97.02ms
step:1163/1750 train_time:112844ms step_avg:97.03ms
step:1164/1750 train_time:112944ms step_avg:97.03ms
step:1165/1750 train_time:113046ms step_avg:97.03ms
step:1166/1750 train_time:113147ms step_avg:97.04ms
step:1167/1750 train_time:113247ms step_avg:97.04ms
step:1168/1750 train_time:113348ms step_avg:97.04ms
step:1169/1750 train_time:113449ms step_avg:97.05ms
step:1170/1750 train_time:113551ms step_avg:97.05ms
step:1171/1750 train_time:113653ms step_avg:97.06ms
step:1172/1750 train_time:113755ms step_avg:97.06ms
step:1173/1750 train_time:113857ms step_avg:97.06ms
step:1174/1750 train_time:113960ms step_avg:97.07ms
step:1175/1750 train_time:114061ms step_avg:97.07ms
step:1176/1750 train_time:114164ms step_avg:97.08ms
step:1177/1750 train_time:114265ms step_avg:97.08ms
step:1178/1750 train_time:114367ms step_avg:97.09ms
step:1179/1750 train_time:114470ms step_avg:97.09ms
step:1180/1750 train_time:114572ms step_avg:97.10ms
step:1181/1750 train_time:114674ms step_avg:97.10ms
step:1182/1750 train_time:114776ms step_avg:97.10ms
step:1183/1750 train_time:114878ms step_avg:97.11ms
step:1184/1750 train_time:114981ms step_avg:97.11ms
step:1185/1750 train_time:115082ms step_avg:97.12ms
step:1186/1750 train_time:115184ms step_avg:97.12ms
step:1187/1750 train_time:115286ms step_avg:97.12ms
step:1188/1750 train_time:115388ms step_avg:97.13ms
step:1189/1750 train_time:115490ms step_avg:97.13ms
step:1190/1750 train_time:115593ms step_avg:97.14ms
step:1191/1750 train_time:115694ms step_avg:97.14ms
step:1192/1750 train_time:115795ms step_avg:97.14ms
step:1193/1750 train_time:115896ms step_avg:97.15ms
step:1194/1750 train_time:115997ms step_avg:97.15ms
step:1195/1750 train_time:116099ms step_avg:97.15ms
step:1196/1750 train_time:116202ms step_avg:97.16ms
step:1197/1750 train_time:116303ms step_avg:97.16ms
step:1198/1750 train_time:116404ms step_avg:97.17ms
step:1199/1750 train_time:116507ms step_avg:97.17ms
step:1200/1750 train_time:116609ms step_avg:97.17ms
step:1201/1750 train_time:116711ms step_avg:97.18ms
step:1202/1750 train_time:116813ms step_avg:97.18ms
step:1203/1750 train_time:116915ms step_avg:97.19ms
step:1204/1750 train_time:117016ms step_avg:97.19ms
step:1205/1750 train_time:117117ms step_avg:97.19ms
step:1206/1750 train_time:117219ms step_avg:97.20ms
step:1207/1750 train_time:117320ms step_avg:97.20ms
step:1208/1750 train_time:117424ms step_avg:97.21ms
step:1209/1750 train_time:117525ms step_avg:97.21ms
step:1210/1750 train_time:117627ms step_avg:97.21ms
step:1211/1750 train_time:117730ms step_avg:97.22ms
step:1212/1750 train_time:117831ms step_avg:97.22ms
step:1213/1750 train_time:117933ms step_avg:97.22ms
step:1214/1750 train_time:118034ms step_avg:97.23ms
step:1215/1750 train_time:118136ms step_avg:97.23ms
step:1216/1750 train_time:118238ms step_avg:97.24ms
step:1217/1750 train_time:118340ms step_avg:97.24ms
step:1218/1750 train_time:118444ms step_avg:97.24ms
step:1219/1750 train_time:118546ms step_avg:97.25ms
step:1220/1750 train_time:118648ms step_avg:97.25ms
step:1221/1750 train_time:118749ms step_avg:97.26ms
step:1222/1750 train_time:118852ms step_avg:97.26ms
step:1223/1750 train_time:118954ms step_avg:97.26ms
step:1224/1750 train_time:119055ms step_avg:97.27ms
step:1225/1750 train_time:119156ms step_avg:97.27ms
step:1226/1750 train_time:119257ms step_avg:97.27ms
step:1227/1750 train_time:119360ms step_avg:97.28ms
step:1228/1750 train_time:119463ms step_avg:97.28ms
step:1229/1750 train_time:119565ms step_avg:97.29ms
step:1230/1750 train_time:119667ms step_avg:97.29ms
step:1231/1750 train_time:119769ms step_avg:97.29ms
step:1232/1750 train_time:119871ms step_avg:97.30ms
step:1233/1750 train_time:119973ms step_avg:97.30ms
step:1234/1750 train_time:120075ms step_avg:97.31ms
step:1235/1750 train_time:120177ms step_avg:97.31ms
step:1236/1750 train_time:120279ms step_avg:97.31ms
step:1237/1750 train_time:120381ms step_avg:97.32ms
step:1238/1750 train_time:120483ms step_avg:97.32ms
step:1239/1750 train_time:120585ms step_avg:97.32ms
step:1240/1750 train_time:120686ms step_avg:97.33ms
step:1241/1750 train_time:120789ms step_avg:97.33ms
step:1242/1750 train_time:120890ms step_avg:97.33ms
step:1243/1750 train_time:120993ms step_avg:97.34ms
step:1244/1750 train_time:121094ms step_avg:97.34ms
step:1245/1750 train_time:121195ms step_avg:97.35ms
step:1246/1750 train_time:121297ms step_avg:97.35ms
step:1247/1750 train_time:121398ms step_avg:97.35ms
step:1248/1750 train_time:121502ms step_avg:97.36ms
step:1249/1750 train_time:121603ms step_avg:97.36ms
step:1250/1750 train_time:121704ms step_avg:97.36ms
step:1250/1750 val_loss:3.4081 train_time:121801ms step_avg:97.44ms
step:1251/1750 train_time:121828ms step_avg:97.38ms
step:1252/1750 train_time:121916ms step_avg:97.38ms
step:1253/1750 train_time:122018ms step_avg:97.38ms
step:1254/1750 train_time:122119ms step_avg:97.38ms
step:1255/1750 train_time:122220ms step_avg:97.39ms
step:1256/1750 train_time:122321ms step_avg:97.39ms
step:1257/1750 train_time:122422ms step_avg:97.39ms
step:1258/1750 train_time:122522ms step_avg:97.39ms
step:1259/1750 train_time:122623ms step_avg:97.40ms
step:1260/1750 train_time:122724ms step_avg:97.40ms
step:1261/1750 train_time:122827ms step_avg:97.40ms
step:1262/1750 train_time:122929ms step_avg:97.41ms
step:1263/1750 train_time:123031ms step_avg:97.41ms
step:1264/1750 train_time:123133ms step_avg:97.42ms
step:1265/1750 train_time:123234ms step_avg:97.42ms
step:1266/1750 train_time:123335ms step_avg:97.42ms
step:1267/1750 train_time:123437ms step_avg:97.42ms
step:1268/1750 train_time:123539ms step_avg:97.43ms
step:1269/1750 train_time:123640ms step_avg:97.43ms
step:1270/1750 train_time:123742ms step_avg:97.43ms
step:1271/1750 train_time:123847ms step_avg:97.44ms
step:1272/1750 train_time:123948ms step_avg:97.44ms
step:1273/1750 train_time:124049ms step_avg:97.45ms
step:1274/1750 train_time:124151ms step_avg:97.45ms
step:1275/1750 train_time:124253ms step_avg:97.45ms
step:1276/1750 train_time:124355ms step_avg:97.46ms
step:1277/1750 train_time:124457ms step_avg:97.46ms
step:1278/1750 train_time:124558ms step_avg:97.46ms
step:1279/1750 train_time:124660ms step_avg:97.47ms
step:1280/1750 train_time:124761ms step_avg:97.47ms
step:1281/1750 train_time:124862ms step_avg:97.47ms
step:1282/1750 train_time:124966ms step_avg:97.48ms
step:1283/1750 train_time:125067ms step_avg:97.48ms
step:1284/1750 train_time:125167ms step_avg:97.48ms
step:1285/1750 train_time:125268ms step_avg:97.49ms
step:1286/1750 train_time:125370ms step_avg:97.49ms
step:1287/1750 train_time:125473ms step_avg:97.49ms
step:1288/1750 train_time:125576ms step_avg:97.50ms
step:1289/1750 train_time:125678ms step_avg:97.50ms
step:1290/1750 train_time:125779ms step_avg:97.50ms
step:1291/1750 train_time:125880ms step_avg:97.51ms
step:1292/1750 train_time:125982ms step_avg:97.51ms
step:1293/1750 train_time:126084ms step_avg:97.51ms
step:1294/1750 train_time:126186ms step_avg:97.52ms
step:1295/1750 train_time:126288ms step_avg:97.52ms
step:1296/1750 train_time:126389ms step_avg:97.52ms
step:1297/1750 train_time:126491ms step_avg:97.53ms
step:1298/1750 train_time:126594ms step_avg:97.53ms
step:1299/1750 train_time:126696ms step_avg:97.53ms
step:1300/1750 train_time:126797ms step_avg:97.54ms
step:1301/1750 train_time:126900ms step_avg:97.54ms
step:1302/1750 train_time:127002ms step_avg:97.54ms
step:1303/1750 train_time:127104ms step_avg:97.55ms
step:1304/1750 train_time:127204ms step_avg:97.55ms
step:1305/1750 train_time:127307ms step_avg:97.55ms
step:1306/1750 train_time:127408ms step_avg:97.56ms
step:1307/1750 train_time:127511ms step_avg:97.56ms
step:1308/1750 train_time:127614ms step_avg:97.56ms
step:1309/1750 train_time:127717ms step_avg:97.57ms
step:1310/1750 train_time:127819ms step_avg:97.57ms
step:1311/1750 train_time:127921ms step_avg:97.57ms
step:1312/1750 train_time:128022ms step_avg:97.58ms
step:1313/1750 train_time:128125ms step_avg:97.58ms
step:1314/1750 train_time:128228ms step_avg:97.59ms
step:1315/1750 train_time:128329ms step_avg:97.59ms
step:1316/1750 train_time:128431ms step_avg:97.59ms
step:1317/1750 train_time:128532ms step_avg:97.59ms
step:1318/1750 train_time:128634ms step_avg:97.60ms
step:1319/1750 train_time:128736ms step_avg:97.60ms
step:1320/1750 train_time:128839ms step_avg:97.61ms
step:1321/1750 train_time:128941ms step_avg:97.61ms
step:1322/1750 train_time:129043ms step_avg:97.61ms
step:1323/1750 train_time:129143ms step_avg:97.61ms
step:1324/1750 train_time:129245ms step_avg:97.62ms
step:1325/1750 train_time:129347ms step_avg:97.62ms
step:1326/1750 train_time:129450ms step_avg:97.62ms
step:1327/1750 train_time:129551ms step_avg:97.63ms
step:1328/1750 train_time:129654ms step_avg:97.63ms
step:1329/1750 train_time:129756ms step_avg:97.63ms
step:1330/1750 train_time:129859ms step_avg:97.64ms
step:1331/1750 train_time:129961ms step_avg:97.64ms
step:1332/1750 train_time:130063ms step_avg:97.65ms
step:1333/1750 train_time:130165ms step_avg:97.65ms
step:1334/1750 train_time:130266ms step_avg:97.65ms
step:1335/1750 train_time:130368ms step_avg:97.65ms
step:1336/1750 train_time:130469ms step_avg:97.66ms
step:1337/1750 train_time:130572ms step_avg:97.66ms
step:1338/1750 train_time:130674ms step_avg:97.66ms
step:1339/1750 train_time:130776ms step_avg:97.67ms
step:1340/1750 train_time:130877ms step_avg:97.67ms
step:1341/1750 train_time:130979ms step_avg:97.67ms
step:1342/1750 train_time:131081ms step_avg:97.68ms
step:1343/1750 train_time:131184ms step_avg:97.68ms
step:1344/1750 train_time:131285ms step_avg:97.68ms
step:1345/1750 train_time:131387ms step_avg:97.69ms
step:1346/1750 train_time:131489ms step_avg:97.69ms
step:1347/1750 train_time:131591ms step_avg:97.69ms
step:1348/1750 train_time:131694ms step_avg:97.70ms
step:1349/1750 train_time:131795ms step_avg:97.70ms
step:1350/1750 train_time:131898ms step_avg:97.70ms
step:1351/1750 train_time:131999ms step_avg:97.70ms
step:1352/1750 train_time:132101ms step_avg:97.71ms
step:1353/1750 train_time:132203ms step_avg:97.71ms
step:1354/1750 train_time:132305ms step_avg:97.71ms
step:1355/1750 train_time:132407ms step_avg:97.72ms
step:1356/1750 train_time:132508ms step_avg:97.72ms
step:1357/1750 train_time:132610ms step_avg:97.72ms
step:1358/1750 train_time:132712ms step_avg:97.73ms
step:1359/1750 train_time:132815ms step_avg:97.73ms
step:1360/1750 train_time:132917ms step_avg:97.73ms
step:1361/1750 train_time:133020ms step_avg:97.74ms
step:1362/1750 train_time:133121ms step_avg:97.74ms
step:1363/1750 train_time:133223ms step_avg:97.74ms
step:1364/1750 train_time:133325ms step_avg:97.75ms
step:1365/1750 train_time:133427ms step_avg:97.75ms
step:1366/1750 train_time:133528ms step_avg:97.75ms
step:1367/1750 train_time:133629ms step_avg:97.75ms
step:1368/1750 train_time:133732ms step_avg:97.76ms
step:1369/1750 train_time:133834ms step_avg:97.76ms
step:1370/1750 train_time:133936ms step_avg:97.76ms
step:1371/1750 train_time:134038ms step_avg:97.77ms
step:1372/1750 train_time:134139ms step_avg:97.77ms
step:1373/1750 train_time:134241ms step_avg:97.77ms
step:1374/1750 train_time:134343ms step_avg:97.78ms
step:1375/1750 train_time:134445ms step_avg:97.78ms
step:1375/1750 val_loss:3.3671 train_time:134541ms step_avg:97.85ms
step:1376/1750 train_time:134568ms step_avg:97.80ms
step:1377/1750 train_time:134659ms step_avg:97.79ms
step:1378/1750 train_time:134762ms step_avg:97.80ms
step:1379/1750 train_time:134862ms step_avg:97.80ms
step:1380/1750 train_time:134964ms step_avg:97.80ms
step:1381/1750 train_time:135067ms step_avg:97.80ms
step:1382/1750 train_time:135168ms step_avg:97.81ms
step:1383/1750 train_time:135268ms step_avg:97.81ms
step:1384/1750 train_time:135369ms step_avg:97.81ms
step:1385/1750 train_time:135470ms step_avg:97.81ms
step:1386/1750 train_time:135572ms step_avg:97.82ms
step:1387/1750 train_time:135676ms step_avg:97.82ms
step:1388/1750 train_time:135779ms step_avg:97.82ms
step:1389/1750 train_time:135882ms step_avg:97.83ms
step:1390/1750 train_time:135983ms step_avg:97.83ms
step:1391/1750 train_time:136085ms step_avg:97.83ms
step:1392/1750 train_time:136186ms step_avg:97.83ms
step:1393/1750 train_time:136287ms step_avg:97.84ms
step:1394/1750 train_time:136388ms step_avg:97.84ms
step:1395/1750 train_time:136490ms step_avg:97.84ms
step:1396/1750 train_time:136592ms step_avg:97.84ms
step:1397/1750 train_time:136694ms step_avg:97.85ms
step:1398/1750 train_time:136796ms step_avg:97.85ms
step:1399/1750 train_time:136899ms step_avg:97.85ms
step:1400/1750 train_time:137002ms step_avg:97.86ms
step:1401/1750 train_time:137104ms step_avg:97.86ms
step:1402/1750 train_time:137206ms step_avg:97.86ms
step:1403/1750 train_time:137307ms step_avg:97.87ms
step:1404/1750 train_time:137408ms step_avg:97.87ms
step:1405/1750 train_time:137510ms step_avg:97.87ms
step:1406/1750 train_time:137611ms step_avg:97.87ms
step:1407/1750 train_time:137714ms step_avg:97.88ms
step:1408/1750 train_time:137818ms step_avg:97.88ms
step:1409/1750 train_time:137922ms step_avg:97.89ms
step:1410/1750 train_time:138024ms step_avg:97.89ms
step:1411/1750 train_time:138125ms step_avg:97.89ms
step:1412/1750 train_time:138226ms step_avg:97.89ms
step:1413/1750 train_time:138328ms step_avg:97.90ms
step:1414/1750 train_time:138429ms step_avg:97.90ms
step:1415/1750 train_time:138531ms step_avg:97.90ms
step:1416/1750 train_time:138632ms step_avg:97.90ms
step:1417/1750 train_time:138734ms step_avg:97.91ms
step:1418/1750 train_time:138836ms step_avg:97.91ms
step:1419/1750 train_time:138940ms step_avg:97.91ms
step:1420/1750 train_time:139042ms step_avg:97.92ms
step:1421/1750 train_time:139144ms step_avg:97.92ms
step:1422/1750 train_time:139246ms step_avg:97.92ms
step:1423/1750 train_time:139347ms step_avg:97.93ms
step:1424/1750 train_time:139448ms step_avg:97.93ms
step:1425/1750 train_time:139550ms step_avg:97.93ms
step:1426/1750 train_time:139651ms step_avg:97.93ms
step:1427/1750 train_time:139754ms step_avg:97.94ms
step:1428/1750 train_time:139858ms step_avg:97.94ms
step:1429/1750 train_time:139961ms step_avg:97.94ms
step:1430/1750 train_time:140065ms step_avg:97.95ms
step:1431/1750 train_time:140168ms step_avg:97.95ms
step:1432/1750 train_time:140270ms step_avg:97.95ms
step:1433/1750 train_time:140374ms step_avg:97.96ms
step:1434/1750 train_time:140476ms step_avg:97.96ms
step:1435/1750 train_time:140580ms step_avg:97.97ms
step:1436/1750 train_time:140684ms step_avg:97.97ms
step:1437/1750 train_time:140787ms step_avg:97.97ms
step:1438/1750 train_time:140888ms step_avg:97.98ms
step:1439/1750 train_time:140992ms step_avg:97.98ms
step:1440/1750 train_time:141095ms step_avg:97.98ms
step:1441/1750 train_time:141199ms step_avg:97.99ms
step:1442/1750 train_time:141301ms step_avg:97.99ms
step:1443/1750 train_time:141404ms step_avg:97.99ms
step:1444/1750 train_time:141509ms step_avg:98.00ms
step:1445/1750 train_time:141610ms step_avg:98.00ms
step:1446/1750 train_time:141712ms step_avg:98.00ms
step:1447/1750 train_time:141815ms step_avg:98.01ms
step:1448/1750 train_time:141919ms step_avg:98.01ms
step:1449/1750 train_time:142021ms step_avg:98.01ms
step:1450/1750 train_time:142123ms step_avg:98.02ms
step:1451/1750 train_time:142226ms step_avg:98.02ms
step:1452/1750 train_time:142330ms step_avg:98.02ms
step:1453/1750 train_time:142433ms step_avg:98.03ms
step:1454/1750 train_time:142538ms step_avg:98.03ms
step:1455/1750 train_time:142641ms step_avg:98.03ms
step:1456/1750 train_time:142743ms step_avg:98.04ms
step:1457/1750 train_time:142847ms step_avg:98.04ms
step:1458/1750 train_time:142950ms step_avg:98.04ms
step:1459/1750 train_time:143052ms step_avg:98.05ms
step:1460/1750 train_time:143155ms step_avg:98.05ms
step:1461/1750 train_time:143260ms step_avg:98.06ms
step:1462/1750 train_time:143363ms step_avg:98.06ms
step:1463/1750 train_time:143465ms step_avg:98.06ms
step:1464/1750 train_time:143570ms step_avg:98.07ms
step:1465/1750 train_time:143672ms step_avg:98.07ms
step:1466/1750 train_time:143775ms step_avg:98.07ms
step:1467/1750 train_time:143878ms step_avg:98.08ms
step:1468/1750 train_time:143982ms step_avg:98.08ms
step:1469/1750 train_time:144085ms step_avg:98.08ms
step:1470/1750 train_time:144187ms step_avg:98.09ms
step:1471/1750 train_time:144291ms step_avg:98.09ms
step:1472/1750 train_time:144393ms step_avg:98.09ms
step:1473/1750 train_time:144498ms step_avg:98.10ms
step:1474/1750 train_time:144600ms step_avg:98.10ms
step:1475/1750 train_time:144701ms step_avg:98.10ms
step:1476/1750 train_time:144804ms step_avg:98.11ms
step:1477/1750 train_time:144907ms step_avg:98.11ms
step:1478/1750 train_time:145011ms step_avg:98.11ms
step:1479/1750 train_time:145113ms step_avg:98.12ms
step:1480/1750 train_time:145217ms step_avg:98.12ms
step:1481/1750 train_time:145320ms step_avg:98.12ms
step:1482/1750 train_time:145423ms step_avg:98.13ms
step:1483/1750 train_time:145527ms step_avg:98.13ms
step:1484/1750 train_time:145630ms step_avg:98.13ms
step:1485/1750 train_time:145733ms step_avg:98.14ms
step:1486/1750 train_time:145837ms step_avg:98.14ms
step:1487/1750 train_time:145940ms step_avg:98.14ms
step:1488/1750 train_time:146043ms step_avg:98.15ms
step:1489/1750 train_time:146147ms step_avg:98.15ms
step:1490/1750 train_time:146250ms step_avg:98.15ms
step:1491/1750 train_time:146352ms step_avg:98.16ms
step:1492/1750 train_time:146456ms step_avg:98.16ms
step:1493/1750 train_time:146559ms step_avg:98.16ms
step:1494/1750 train_time:146662ms step_avg:98.17ms
step:1495/1750 train_time:146763ms step_avg:98.17ms
step:1496/1750 train_time:146867ms step_avg:98.17ms
step:1497/1750 train_time:146969ms step_avg:98.18ms
step:1498/1750 train_time:147071ms step_avg:98.18ms
step:1499/1750 train_time:147174ms step_avg:98.18ms
step:1500/1750 train_time:147277ms step_avg:98.18ms
step:1500/1750 val_loss:3.3307 train_time:147375ms step_avg:98.25ms
step:1501/1750 train_time:147403ms step_avg:98.20ms
step:1502/1750 train_time:147494ms step_avg:98.20ms
step:1503/1750 train_time:147596ms step_avg:98.20ms
step:1504/1750 train_time:147699ms step_avg:98.20ms
step:1505/1750 train_time:147802ms step_avg:98.21ms
step:1506/1750 train_time:147904ms step_avg:98.21ms
step:1507/1750 train_time:148007ms step_avg:98.21ms
step:1508/1750 train_time:148108ms step_avg:98.21ms
step:1509/1750 train_time:148210ms step_avg:98.22ms
step:1510/1750 train_time:148314ms step_avg:98.22ms
step:1511/1750 train_time:148419ms step_avg:98.23ms
step:1512/1750 train_time:148522ms step_avg:98.23ms
step:1513/1750 train_time:148626ms step_avg:98.23ms
step:1514/1750 train_time:148730ms step_avg:98.24ms
step:1515/1750 train_time:148836ms step_avg:98.24ms
step:1516/1750 train_time:148939ms step_avg:98.25ms
step:1517/1750 train_time:149041ms step_avg:98.25ms
step:1518/1750 train_time:149144ms step_avg:98.25ms
step:1519/1750 train_time:149250ms step_avg:98.26ms
step:1520/1750 train_time:149352ms step_avg:98.26ms
step:1521/1750 train_time:149454ms step_avg:98.26ms
step:1522/1750 train_time:149557ms step_avg:98.26ms
step:1523/1750 train_time:149660ms step_avg:98.27ms
step:1524/1750 train_time:149763ms step_avg:98.27ms
step:1525/1750 train_time:149866ms step_avg:98.27ms
step:1526/1750 train_time:149968ms step_avg:98.28ms
step:1527/1750 train_time:150072ms step_avg:98.28ms
step:1528/1750 train_time:150179ms step_avg:98.28ms
step:1529/1750 train_time:150281ms step_avg:98.29ms
step:1530/1750 train_time:150384ms step_avg:98.29ms
step:1531/1750 train_time:150486ms step_avg:98.29ms
step:1532/1750 train_time:150589ms step_avg:98.30ms
step:1533/1750 train_time:150692ms step_avg:98.30ms
step:1534/1750 train_time:150795ms step_avg:98.30ms
step:1535/1750 train_time:150897ms step_avg:98.30ms
step:1536/1750 train_time:150999ms step_avg:98.31ms
step:1537/1750 train_time:151103ms step_avg:98.31ms
step:1538/1750 train_time:151207ms step_avg:98.31ms
step:1539/1750 train_time:151309ms step_avg:98.32ms
step:1540/1750 train_time:151412ms step_avg:98.32ms
step:1541/1750 train_time:151517ms step_avg:98.32ms
step:1542/1750 train_time:151620ms step_avg:98.33ms
step:1543/1750 train_time:151725ms step_avg:98.33ms
step:1544/1750 train_time:151827ms step_avg:98.33ms
step:1545/1750 train_time:151930ms step_avg:98.34ms
step:1546/1750 train_time:152033ms step_avg:98.34ms
step:1547/1750 train_time:152138ms step_avg:98.34ms
step:1548/1750 train_time:152241ms step_avg:98.35ms
step:1549/1750 train_time:152343ms step_avg:98.35ms
step:1550/1750 train_time:152446ms step_avg:98.35ms
step:1551/1750 train_time:152550ms step_avg:98.36ms
step:1552/1750 train_time:152652ms step_avg:98.36ms
step:1553/1750 train_time:152756ms step_avg:98.36ms
step:1554/1750 train_time:152859ms step_avg:98.36ms
step:1555/1750 train_time:152961ms step_avg:98.37ms
step:1556/1750 train_time:153065ms step_avg:98.37ms
step:1557/1750 train_time:153169ms step_avg:98.37ms
step:1558/1750 train_time:153273ms step_avg:98.38ms
step:1559/1750 train_time:153376ms step_avg:98.38ms
step:1560/1750 train_time:153478ms step_avg:98.38ms
step:1561/1750 train_time:153582ms step_avg:98.39ms
step:1562/1750 train_time:153685ms step_avg:98.39ms
step:1563/1750 train_time:153791ms step_avg:98.39ms
step:1564/1750 train_time:153892ms step_avg:98.40ms
step:1565/1750 train_time:153997ms step_avg:98.40ms
step:1566/1750 train_time:154101ms step_avg:98.40ms
step:1567/1750 train_time:154203ms step_avg:98.41ms
step:1568/1750 train_time:154305ms step_avg:98.41ms
step:1569/1750 train_time:154407ms step_avg:98.41ms
step:1570/1750 train_time:154512ms step_avg:98.42ms
step:1571/1750 train_time:154616ms step_avg:98.42ms
step:1572/1750 train_time:154719ms step_avg:98.42ms
step:1573/1750 train_time:154821ms step_avg:98.42ms
step:1574/1750 train_time:154925ms step_avg:98.43ms
step:1575/1750 train_time:155028ms step_avg:98.43ms
step:1576/1750 train_time:155132ms step_avg:98.43ms
step:1577/1750 train_time:155236ms step_avg:98.44ms
step:1578/1750 train_time:155339ms step_avg:98.44ms
step:1579/1750 train_time:155441ms step_avg:98.44ms
step:1580/1750 train_time:155545ms step_avg:98.45ms
step:1581/1750 train_time:155648ms step_avg:98.45ms
step:1582/1750 train_time:155750ms step_avg:98.45ms
step:1583/1750 train_time:155855ms step_avg:98.46ms
step:1584/1750 train_time:155960ms step_avg:98.46ms
step:1585/1750 train_time:156063ms step_avg:98.46ms
step:1586/1750 train_time:156168ms step_avg:98.47ms
step:1587/1750 train_time:156272ms step_avg:98.47ms
step:1588/1750 train_time:156375ms step_avg:98.47ms
step:1589/1750 train_time:156478ms step_avg:98.48ms
step:1590/1750 train_time:156580ms step_avg:98.48ms
step:1591/1750 train_time:156683ms step_avg:98.48ms
step:1592/1750 train_time:156787ms step_avg:98.48ms
step:1593/1750 train_time:156889ms step_avg:98.49ms
step:1594/1750 train_time:156994ms step_avg:98.49ms
step:1595/1750 train_time:157097ms step_avg:98.49ms
step:1596/1750 train_time:157200ms step_avg:98.50ms
step:1597/1750 train_time:157305ms step_avg:98.50ms
step:1598/1750 train_time:157409ms step_avg:98.50ms
step:1599/1750 train_time:157511ms step_avg:98.51ms
step:1600/1750 train_time:157614ms step_avg:98.51ms
step:1601/1750 train_time:157717ms step_avg:98.51ms
step:1602/1750 train_time:157820ms step_avg:98.51ms
step:1603/1750 train_time:157922ms step_avg:98.52ms
step:1604/1750 train_time:158025ms step_avg:98.52ms
step:1605/1750 train_time:158129ms step_avg:98.52ms
step:1606/1750 train_time:158232ms step_avg:98.53ms
step:1607/1750 train_time:158335ms step_avg:98.53ms
step:1608/1750 train_time:158437ms step_avg:98.53ms
step:1609/1750 train_time:158541ms step_avg:98.53ms
step:1610/1750 train_time:158644ms step_avg:98.54ms
step:1611/1750 train_time:158749ms step_avg:98.54ms
step:1612/1750 train_time:158855ms step_avg:98.55ms
step:1613/1750 train_time:158956ms step_avg:98.55ms
step:1614/1750 train_time:159058ms step_avg:98.55ms
step:1615/1750 train_time:159161ms step_avg:98.55ms
step:1616/1750 train_time:159265ms step_avg:98.55ms
step:1617/1750 train_time:159368ms step_avg:98.56ms
step:1618/1750 train_time:159472ms step_avg:98.56ms
step:1619/1750 train_time:159575ms step_avg:98.56ms
step:1620/1750 train_time:159679ms step_avg:98.57ms
step:1621/1750 train_time:159781ms step_avg:98.57ms
step:1622/1750 train_time:159884ms step_avg:98.57ms
step:1623/1750 train_time:159988ms step_avg:98.58ms
step:1624/1750 train_time:160092ms step_avg:98.58ms
step:1625/1750 train_time:160198ms step_avg:98.58ms
step:1625/1750 val_loss:3.3009 train_time:160295ms step_avg:98.64ms
step:1626/1750 train_time:160322ms step_avg:98.60ms
step:1627/1750 train_time:160411ms step_avg:98.59ms
step:1628/1750 train_time:160515ms step_avg:98.60ms
step:1629/1750 train_time:160617ms step_avg:98.60ms
step:1630/1750 train_time:160722ms step_avg:98.60ms
step:1631/1750 train_time:160825ms step_avg:98.60ms
step:1632/1750 train_time:160927ms step_avg:98.61ms
step:1633/1750 train_time:161030ms step_avg:98.61ms
step:1634/1750 train_time:161135ms step_avg:98.61ms
step:1635/1750 train_time:161236ms step_avg:98.62ms
step:1636/1750 train_time:161342ms step_avg:98.62ms
step:1637/1750 train_time:161444ms step_avg:98.62ms
step:1638/1750 train_time:161547ms step_avg:98.62ms
step:1639/1750 train_time:161651ms step_avg:98.63ms
step:1640/1750 train_time:161754ms step_avg:98.63ms
step:1641/1750 train_time:161857ms step_avg:98.63ms
step:1642/1750 train_time:161960ms step_avg:98.64ms
step:1643/1750 train_time:162062ms step_avg:98.64ms
step:1644/1750 train_time:162165ms step_avg:98.64ms
step:1645/1750 train_time:162268ms step_avg:98.64ms
step:1646/1750 train_time:162372ms step_avg:98.65ms
step:1647/1750 train_time:162476ms step_avg:98.65ms
step:1648/1750 train_time:162579ms step_avg:98.65ms
step:1649/1750 train_time:162683ms step_avg:98.66ms
step:1650/1750 train_time:162785ms step_avg:98.66ms
step:1651/1750 train_time:162889ms step_avg:98.66ms
step:1652/1750 train_time:162992ms step_avg:98.66ms
step:1653/1750 train_time:163096ms step_avg:98.67ms
step:1654/1750 train_time:163197ms step_avg:98.67ms
step:1655/1750 train_time:163302ms step_avg:98.67ms
step:1656/1750 train_time:163405ms step_avg:98.67ms
step:1657/1750 train_time:163507ms step_avg:98.68ms
step:1658/1750 train_time:163612ms step_avg:98.68ms
step:1659/1750 train_time:163718ms step_avg:98.68ms
step:1660/1750 train_time:163820ms step_avg:98.69ms
step:1661/1750 train_time:163925ms step_avg:98.69ms
step:1662/1750 train_time:164029ms step_avg:98.69ms
step:1663/1750 train_time:164134ms step_avg:98.70ms
step:1664/1750 train_time:164237ms step_avg:98.70ms
step:1665/1750 train_time:164341ms step_avg:98.70ms
step:1666/1750 train_time:164444ms step_avg:98.71ms
step:1667/1750 train_time:164546ms step_avg:98.71ms
step:1668/1750 train_time:164651ms step_avg:98.71ms
step:1669/1750 train_time:164756ms step_avg:98.72ms
step:1670/1750 train_time:164859ms step_avg:98.72ms
step:1671/1750 train_time:164962ms step_avg:98.72ms
step:1672/1750 train_time:165067ms step_avg:98.72ms
step:1673/1750 train_time:165171ms step_avg:98.73ms
step:1674/1750 train_time:165275ms step_avg:98.73ms
step:1675/1750 train_time:165378ms step_avg:98.73ms
step:1676/1750 train_time:165483ms step_avg:98.74ms
step:1677/1750 train_time:165585ms step_avg:98.74ms
step:1678/1750 train_time:165688ms step_avg:98.74ms
step:1679/1750 train_time:165793ms step_avg:98.74ms
step:1680/1750 train_time:165895ms step_avg:98.75ms
step:1681/1750 train_time:165998ms step_avg:98.75ms
step:1682/1750 train_time:166102ms step_avg:98.75ms
step:1683/1750 train_time:166204ms step_avg:98.75ms
step:1684/1750 train_time:166309ms step_avg:98.76ms
step:1685/1750 train_time:166413ms step_avg:98.76ms
step:1686/1750 train_time:166515ms step_avg:98.76ms
step:1687/1750 train_time:166618ms step_avg:98.77ms
step:1688/1750 train_time:166723ms step_avg:98.77ms
step:1689/1750 train_time:166827ms step_avg:98.77ms
step:1690/1750 train_time:166931ms step_avg:98.78ms
step:1691/1750 train_time:167035ms step_avg:98.78ms
step:1692/1750 train_time:167137ms step_avg:98.78ms
step:1693/1750 train_time:167241ms step_avg:98.78ms
step:1694/1750 train_time:167345ms step_avg:98.79ms
step:1695/1750 train_time:167451ms step_avg:98.79ms
step:1696/1750 train_time:167555ms step_avg:98.79ms
step:1697/1750 train_time:167663ms step_avg:98.80ms
step:1698/1750 train_time:167766ms step_avg:98.80ms
step:1699/1750 train_time:167870ms step_avg:98.81ms
step:1700/1750 train_time:167974ms step_avg:98.81ms
step:1701/1750 train_time:168077ms step_avg:98.81ms
step:1702/1750 train_time:168183ms step_avg:98.82ms
step:1703/1750 train_time:168287ms step_avg:98.82ms
step:1704/1750 train_time:168391ms step_avg:98.82ms
step:1705/1750 train_time:168495ms step_avg:98.82ms
step:1706/1750 train_time:168599ms step_avg:98.83ms
step:1707/1750 train_time:168704ms step_avg:98.83ms
step:1708/1750 train_time:168808ms step_avg:98.83ms
step:1709/1750 train_time:168912ms step_avg:98.84ms
step:1710/1750 train_time:169016ms step_avg:98.84ms
step:1711/1750 train_time:169121ms step_avg:98.84ms
step:1712/1750 train_time:169225ms step_avg:98.85ms
step:1713/1750 train_time:169330ms step_avg:98.85ms
step:1714/1750 train_time:169432ms step_avg:98.85ms
step:1715/1750 train_time:169538ms step_avg:98.86ms
step:1716/1750 train_time:169641ms step_avg:98.86ms
step:1717/1750 train_time:169745ms step_avg:98.86ms
step:1718/1750 train_time:169849ms step_avg:98.86ms
step:1719/1750 train_time:169957ms step_avg:98.87ms
step:1720/1750 train_time:170060ms step_avg:98.87ms
step:1721/1750 train_time:170164ms step_avg:98.87ms
step:1722/1750 train_time:170268ms step_avg:98.88ms
step:1723/1750 train_time:170372ms step_avg:98.88ms
step:1724/1750 train_time:170476ms step_avg:98.88ms
step:1725/1750 train_time:170581ms step_avg:98.89ms
step:1726/1750 train_time:170685ms step_avg:98.89ms
step:1727/1750 train_time:170790ms step_avg:98.89ms
step:1728/1750 train_time:170896ms step_avg:98.90ms
step:1729/1750 train_time:171000ms step_avg:98.90ms
step:1730/1750 train_time:171103ms step_avg:98.90ms
step:1731/1750 train_time:171208ms step_avg:98.91ms
step:1732/1750 train_time:171311ms step_avg:98.91ms
step:1733/1750 train_time:171415ms step_avg:98.91ms
step:1734/1750 train_time:171520ms step_avg:98.92ms
step:1735/1750 train_time:171623ms step_avg:98.92ms
step:1736/1750 train_time:171728ms step_avg:98.92ms
step:1737/1750 train_time:171834ms step_avg:98.93ms
step:1738/1750 train_time:171937ms step_avg:98.93ms
step:1739/1750 train_time:172042ms step_avg:98.93ms
step:1740/1750 train_time:172146ms step_avg:98.93ms
step:1741/1750 train_time:172255ms step_avg:98.94ms
step:1742/1750 train_time:172359ms step_avg:98.94ms
step:1743/1750 train_time:172464ms step_avg:98.95ms
step:1744/1750 train_time:172568ms step_avg:98.95ms
step:1745/1750 train_time:172673ms step_avg:98.95ms
step:1746/1750 train_time:172776ms step_avg:98.96ms
step:1747/1750 train_time:172880ms step_avg:98.96ms
step:1748/1750 train_time:172985ms step_avg:98.96ms
step:1749/1750 train_time:173088ms step_avg:98.96ms
step:1750/1750 train_time:173193ms step_avg:98.97ms
step:1750/1750 val_loss:3.2803 train_time:173291ms step_avg:99.02ms
peak memory allocated: 33277 MiB reserved: 48972 MiB

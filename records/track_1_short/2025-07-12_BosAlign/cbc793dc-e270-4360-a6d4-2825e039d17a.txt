import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn, autocast
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
#import wandb

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)

        params = list(params)
        sizes = {p.shape for p in params}

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params,))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * self.world_size
            for base_i in range(0, len(params), self.world_size):
                if base_i + self.rank < len(params):
                    grad = params[base_i + self.rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + self.world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * self.world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), self.world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                futures.append(dist.all_gather(params_pad[base_i:base_i + self.world_size], params_pad[base_i + self.rank], async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01, rank: int = 0, world_size: int = 1):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        self.rank = rank
        self.world_size = world_size

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(
                params=group_params,
            ))
        super().__init__(param_groups, defaults)

    @torch.compile
    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // self.world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)

                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']

                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)

                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t

                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        for param in self.embed.parameters():
            param.lr_mul = 75.
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embeds in self.value_embeds:
            for param in self.value_embeds.parameters():
                param.lr_mul = 75.
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.lr_mul = 27.5
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % world_size
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            #return causal_mask
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, world_size: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos:pos+max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()

    starts = []
    batch_end=None
    for i in range(len(boundary_positions) - 1):
        end = boundary_positions[i + 1].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == world_size:
                batch_end = end
                break
            start = end
    assert batch_end is not None # increase max_batch_span if necessary
    batch_span = batch_end-pos
    return starts, batch_span

def distributed_data_generator(filename_pattern: str, batch_size: int, rank: int, world_size: int, align_to_bos: bool):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    batch_span = batch_size
    max_batch_span = 2*batch_size if align_to_bos else batch_size #provide buffer to handle samples up to length local_batch_size

    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, world_size, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    train_align_to_bos = True # align local batch start indicies with next bos_token
    val_align_to_bos = False # False to maintain same eval as prior records
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

#if master_process:
#    wandb.init(project="modded-nanogpt-tiny", name=f"run-{os.path.basename(__file__)}", save_code=True)

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=True):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
if master_process:
    print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=next_multiple_of_n(50257, n=128), num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094

optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0, rank=rank, world_size=world_size)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]

for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

for n, p in model.named_parameters():
    wd_mul = getattr(p, "wd_mul", 1.0)
    lr_mul = getattr(p, "lr_mul", 1.0)

    print0(f"{n}: {p.shape} {p.dtype} {wd_mul} {lr_mul}")

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
embedding_params = sum(p.numel() for n, p in model.named_parameters() if "embed" in n)
non_embedding_params = total_params - embedding_params

print0(f"")
print0(f"Model parameters:")
print0(f"  Total parameters: {total_params:,}")
print0(f"  Embedding parameters: {embedding_params:,}")
print0(f"  Non-embedding parameters: {non_embedding_params:,}")

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    w = min((1 - x) / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.05
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)

torch.cuda.synchronize()
dist.barrier()
with torch.profiler.profile() as prof:
    for _ in range(warmup_steps):
        inputs, targets = next(train_loader)
        model(inputs, targets, get_window_size_blocks(1)).backward()
        for opt in optimizers:
            opt.step()
    model.zero_grad(set_to_none=True)
    torch.cuda.synchronize()
    dist.barrier()
os.makedirs("traces", exist_ok=True)
prof.export_chrome_trace(f"traces/trace_{rank}.json")

model.load_state_dict(initial_state['model'])
for opt, opt_state in zip(optimizers, initial_state['optimizers']):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size, args.val_align_to_bos)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        #if master_process:
        #    wandb.log({"val/loss": val_loss}, step=step)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.8.0.dev20250524+cu126 compiled for CUDA 12.6
Sun Jul 13 01:14:31 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   33C    P0            116W /  700W |    5856MiB /  81559MiB |      4%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   37C    P0            122W /  700W |    1517MiB /  81559MiB |      2%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   39C    P0            124W /  700W |    1517MiB /  81559MiB |      2%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   33C    P0            119W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   39C    P0            120W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   39C    P0            123W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   32C    P0            115W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           75679      C   /usr/bin/python3                       1508MiB |
|    0   N/A  N/A           75680      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           75681      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           75682      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           75683      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           75684      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           75685      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           75686      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A           75680      C   /usr/bin/python3                       1508MiB |
|    2   N/A  N/A           75681      C   /usr/bin/python3                       1508MiB |
|    3   N/A  N/A           75682      C   /usr/bin/python3                       1508MiB |
|    4   N/A  N/A           75683      C   /usr/bin/python3                       1508MiB |
|    5   N/A  N/A           75684      C   /usr/bin/python3                       1508MiB |
|    6   N/A  N/A           75685      C   /usr/bin/python3                       1508MiB |
|    7   N/A  N/A           75686      C   /usr/bin/python3                       1508MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
scalars: torch.Size([64]) torch.float32 1.0 5.0
embed.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.0.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.1.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.2.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
blocks.0.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.0.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.1.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.1.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.2.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.2.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.3.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.3.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.4.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.4.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.5.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.5.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.6.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.6.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.7.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.7.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.8.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.8.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.9.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.9.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.10.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.10.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.11.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.11.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
lm_head.weight: torch.Size([50304, 768]) torch.float32 1.0 27.5

Model parameters:
  Total parameters: 275,742,784
  Embedding parameters: 154,533,888
  Non-embedding parameters: 121,208,896
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1750 train_time:149ms step_avg:148.87ms
step:2/1750 train_time:173ms step_avg:86.64ms
step:3/1750 train_time:249ms step_avg:82.84ms
step:4/1750 train_time:340ms step_avg:85.03ms
step:5/1750 train_time:433ms step_avg:86.55ms
step:6/1750 train_time:525ms step_avg:87.53ms
step:7/1750 train_time:618ms step_avg:88.25ms
step:8/1750 train_time:710ms step_avg:88.79ms
step:9/1750 train_time:803ms step_avg:89.20ms
step:10/1750 train_time:896ms step_avg:89.55ms
step:11/1750 train_time:988ms step_avg:89.80ms
step:12/1750 train_time:1081ms step_avg:90.12ms
step:13/1750 train_time:1176ms step_avg:90.50ms
step:14/1750 train_time:1271ms step_avg:90.80ms
step:15/1750 train_time:1365ms step_avg:90.99ms
step:16/1750 train_time:1458ms step_avg:91.15ms
step:17/1750 train_time:1552ms step_avg:91.30ms
step:18/1750 train_time:1645ms step_avg:91.40ms
step:19/1750 train_time:1738ms step_avg:91.48ms
step:20/1750 train_time:1831ms step_avg:91.57ms
step:21/1750 train_time:1925ms step_avg:91.65ms
step:22/1750 train_time:2018ms step_avg:91.71ms
step:23/1750 train_time:2111ms step_avg:91.78ms
step:24/1750 train_time:2205ms step_avg:91.87ms
step:25/1750 train_time:2299ms step_avg:91.95ms
step:26/1750 train_time:2393ms step_avg:92.02ms
step:27/1750 train_time:2486ms step_avg:92.08ms
step:28/1750 train_time:2580ms step_avg:92.13ms
step:29/1750 train_time:2673ms step_avg:92.18ms
step:30/1750 train_time:2766ms step_avg:92.20ms
step:31/1750 train_time:2860ms step_avg:92.25ms
step:32/1750 train_time:2953ms step_avg:92.27ms
step:33/1750 train_time:3046ms step_avg:92.29ms
step:34/1750 train_time:3139ms step_avg:92.32ms
step:35/1750 train_time:3233ms step_avg:92.36ms
step:36/1750 train_time:3326ms step_avg:92.38ms
step:37/1750 train_time:3420ms step_avg:92.42ms
step:38/1750 train_time:3513ms step_avg:92.45ms
step:39/1750 train_time:3606ms step_avg:92.46ms
step:40/1750 train_time:3699ms step_avg:92.47ms
step:41/1750 train_time:3792ms step_avg:92.49ms
step:42/1750 train_time:3885ms step_avg:92.50ms
step:43/1750 train_time:3978ms step_avg:92.52ms
step:44/1750 train_time:4074ms step_avg:92.59ms
step:45/1750 train_time:4167ms step_avg:92.60ms
step:46/1750 train_time:4261ms step_avg:92.63ms
step:47/1750 train_time:4354ms step_avg:92.64ms
step:48/1750 train_time:4447ms step_avg:92.65ms
step:49/1750 train_time:4540ms step_avg:92.66ms
step:50/1750 train_time:4634ms step_avg:92.67ms
step:51/1750 train_time:4726ms step_avg:92.67ms
step:52/1750 train_time:4820ms step_avg:92.69ms
step:53/1750 train_time:4913ms step_avg:92.70ms
step:54/1750 train_time:5006ms step_avg:92.71ms
step:55/1750 train_time:5100ms step_avg:92.72ms
step:56/1750 train_time:5193ms step_avg:92.73ms
step:57/1750 train_time:5286ms step_avg:92.73ms
step:58/1750 train_time:5379ms step_avg:92.74ms
step:59/1750 train_time:5473ms step_avg:92.76ms
step:60/1750 train_time:5566ms step_avg:92.76ms
step:61/1750 train_time:5659ms step_avg:92.77ms
step:62/1750 train_time:5753ms step_avg:92.79ms
step:63/1750 train_time:5846ms step_avg:92.80ms
step:64/1750 train_time:5939ms step_avg:92.80ms
step:65/1750 train_time:6033ms step_avg:92.81ms
step:66/1750 train_time:6126ms step_avg:92.82ms
step:67/1750 train_time:6219ms step_avg:92.83ms
step:68/1750 train_time:6313ms step_avg:92.83ms
step:69/1750 train_time:6406ms step_avg:92.83ms
step:70/1750 train_time:6499ms step_avg:92.84ms
step:71/1750 train_time:6592ms step_avg:92.85ms
step:72/1750 train_time:6685ms step_avg:92.85ms
step:73/1750 train_time:6779ms step_avg:92.86ms
step:74/1750 train_time:6873ms step_avg:92.87ms
step:75/1750 train_time:6966ms step_avg:92.87ms
step:76/1750 train_time:7060ms step_avg:92.89ms
step:77/1750 train_time:7154ms step_avg:92.91ms
step:78/1750 train_time:7247ms step_avg:92.92ms
step:79/1750 train_time:7340ms step_avg:92.92ms
step:80/1750 train_time:7434ms step_avg:92.92ms
step:81/1750 train_time:7526ms step_avg:92.92ms
step:82/1750 train_time:7620ms step_avg:92.93ms
step:83/1750 train_time:7713ms step_avg:92.93ms
step:84/1750 train_time:7807ms step_avg:92.94ms
step:85/1750 train_time:7900ms step_avg:92.94ms
step:86/1750 train_time:7993ms step_avg:92.95ms
step:87/1750 train_time:8087ms step_avg:92.95ms
step:88/1750 train_time:8181ms step_avg:92.96ms
step:89/1750 train_time:8274ms step_avg:92.97ms
step:90/1750 train_time:8368ms step_avg:92.97ms
step:91/1750 train_time:8461ms step_avg:92.98ms
step:92/1750 train_time:8554ms step_avg:92.98ms
step:93/1750 train_time:8647ms step_avg:92.98ms
step:94/1750 train_time:8741ms step_avg:92.99ms
step:95/1750 train_time:8835ms step_avg:93.00ms
step:96/1750 train_time:8928ms step_avg:93.00ms
step:97/1750 train_time:9021ms step_avg:93.00ms
step:98/1750 train_time:9114ms step_avg:93.00ms
step:99/1750 train_time:9207ms step_avg:93.00ms
step:100/1750 train_time:9301ms step_avg:93.01ms
step:101/1750 train_time:9394ms step_avg:93.01ms
step:102/1750 train_time:9487ms step_avg:93.01ms
step:103/1750 train_time:9580ms step_avg:93.01ms
step:104/1750 train_time:9674ms step_avg:93.02ms
step:105/1750 train_time:9767ms step_avg:93.02ms
step:106/1750 train_time:9861ms step_avg:93.03ms
step:107/1750 train_time:9954ms step_avg:93.03ms
step:108/1750 train_time:10047ms step_avg:93.03ms
step:109/1750 train_time:10141ms step_avg:93.04ms
step:110/1750 train_time:10234ms step_avg:93.04ms
step:111/1750 train_time:10327ms step_avg:93.04ms
step:112/1750 train_time:10420ms step_avg:93.03ms
step:113/1750 train_time:10513ms step_avg:93.04ms
step:114/1750 train_time:10606ms step_avg:93.04ms
step:115/1750 train_time:10700ms step_avg:93.04ms
step:116/1750 train_time:10793ms step_avg:93.04ms
step:117/1750 train_time:10886ms step_avg:93.05ms
step:118/1750 train_time:10980ms step_avg:93.05ms
step:119/1750 train_time:11075ms step_avg:93.07ms
step:120/1750 train_time:11169ms step_avg:93.07ms
step:121/1750 train_time:11262ms step_avg:93.07ms
step:122/1750 train_time:11354ms step_avg:93.07ms
step:123/1750 train_time:11448ms step_avg:93.07ms
step:124/1750 train_time:11541ms step_avg:93.07ms
step:125/1750 train_time:11634ms step_avg:93.08ms
step:125/1750 val_loss:4.6465 train_time:11722ms step_avg:93.78ms
step:126/1750 train_time:11748ms step_avg:93.24ms
step:127/1750 train_time:11828ms step_avg:93.14ms
step:128/1750 train_time:11930ms step_avg:93.20ms
step:129/1750 train_time:12023ms step_avg:93.20ms
step:130/1750 train_time:12116ms step_avg:93.20ms
step:131/1750 train_time:12208ms step_avg:93.19ms
step:132/1750 train_time:12301ms step_avg:93.19ms
step:133/1750 train_time:12394ms step_avg:93.19ms
step:134/1750 train_time:12486ms step_avg:93.18ms
step:135/1750 train_time:12579ms step_avg:93.18ms
step:136/1750 train_time:12672ms step_avg:93.18ms
step:137/1750 train_time:12767ms step_avg:93.19ms
step:138/1750 train_time:12863ms step_avg:93.21ms
step:139/1750 train_time:12957ms step_avg:93.22ms
step:140/1750 train_time:13052ms step_avg:93.23ms
step:141/1750 train_time:13146ms step_avg:93.23ms
step:142/1750 train_time:13239ms step_avg:93.23ms
step:143/1750 train_time:13332ms step_avg:93.23ms
step:144/1750 train_time:13425ms step_avg:93.23ms
step:145/1750 train_time:13518ms step_avg:93.23ms
step:146/1750 train_time:13611ms step_avg:93.23ms
step:147/1750 train_time:13705ms step_avg:93.23ms
step:148/1750 train_time:13800ms step_avg:93.24ms
step:149/1750 train_time:13893ms step_avg:93.24ms
step:150/1750 train_time:13987ms step_avg:93.25ms
step:151/1750 train_time:14082ms step_avg:93.26ms
step:152/1750 train_time:14175ms step_avg:93.26ms
step:153/1750 train_time:14268ms step_avg:93.26ms
step:154/1750 train_time:14362ms step_avg:93.26ms
step:155/1750 train_time:14455ms step_avg:93.26ms
step:156/1750 train_time:14548ms step_avg:93.26ms
step:157/1750 train_time:14642ms step_avg:93.26ms
step:158/1750 train_time:14734ms step_avg:93.26ms
step:159/1750 train_time:14828ms step_avg:93.26ms
step:160/1750 train_time:14923ms step_avg:93.27ms
step:161/1750 train_time:15017ms step_avg:93.27ms
step:162/1750 train_time:15110ms step_avg:93.27ms
step:163/1750 train_time:15205ms step_avg:93.28ms
step:164/1750 train_time:15300ms step_avg:93.29ms
step:165/1750 train_time:15393ms step_avg:93.29ms
step:166/1750 train_time:15487ms step_avg:93.29ms
step:167/1750 train_time:15580ms step_avg:93.29ms
step:168/1750 train_time:15673ms step_avg:93.29ms
step:169/1750 train_time:15767ms step_avg:93.29ms
step:170/1750 train_time:15861ms step_avg:93.30ms
step:171/1750 train_time:15954ms step_avg:93.30ms
step:172/1750 train_time:16048ms step_avg:93.30ms
step:173/1750 train_time:16141ms step_avg:93.30ms
step:174/1750 train_time:16235ms step_avg:93.30ms
step:175/1750 train_time:16328ms step_avg:93.30ms
step:176/1750 train_time:16422ms step_avg:93.31ms
step:177/1750 train_time:16515ms step_avg:93.31ms
step:178/1750 train_time:16609ms step_avg:93.31ms
step:179/1750 train_time:16703ms step_avg:93.31ms
step:180/1750 train_time:16797ms step_avg:93.32ms
step:181/1750 train_time:16894ms step_avg:93.34ms
step:182/1750 train_time:16985ms step_avg:93.32ms
step:183/1750 train_time:17079ms step_avg:93.33ms
step:184/1750 train_time:17172ms step_avg:93.33ms
step:185/1750 train_time:17267ms step_avg:93.33ms
step:186/1750 train_time:17360ms step_avg:93.33ms
step:187/1750 train_time:17454ms step_avg:93.34ms
step:188/1750 train_time:17547ms step_avg:93.33ms
step:189/1750 train_time:17641ms step_avg:93.34ms
step:190/1750 train_time:17734ms step_avg:93.34ms
step:191/1750 train_time:17828ms step_avg:93.34ms
step:192/1750 train_time:17922ms step_avg:93.34ms
step:193/1750 train_time:18016ms step_avg:93.35ms
step:194/1750 train_time:18109ms step_avg:93.35ms
step:195/1750 train_time:18204ms step_avg:93.35ms
step:196/1750 train_time:18298ms step_avg:93.36ms
step:197/1750 train_time:18391ms step_avg:93.35ms
step:198/1750 train_time:18485ms step_avg:93.36ms
step:199/1750 train_time:18578ms step_avg:93.36ms
step:200/1750 train_time:18671ms step_avg:93.36ms
step:201/1750 train_time:18765ms step_avg:93.36ms
step:202/1750 train_time:18860ms step_avg:93.37ms
step:203/1750 train_time:18954ms step_avg:93.37ms
step:204/1750 train_time:19047ms step_avg:93.37ms
step:205/1750 train_time:19140ms step_avg:93.37ms
step:206/1750 train_time:19235ms step_avg:93.37ms
step:207/1750 train_time:19328ms step_avg:93.37ms
step:208/1750 train_time:19423ms step_avg:93.38ms
step:209/1750 train_time:19516ms step_avg:93.38ms
step:210/1750 train_time:19610ms step_avg:93.38ms
step:211/1750 train_time:19704ms step_avg:93.39ms
step:212/1750 train_time:19799ms step_avg:93.39ms
step:213/1750 train_time:19892ms step_avg:93.39ms
step:214/1750 train_time:19986ms step_avg:93.39ms
step:215/1750 train_time:20080ms step_avg:93.40ms
step:216/1750 train_time:20173ms step_avg:93.39ms
step:217/1750 train_time:20266ms step_avg:93.39ms
step:218/1750 train_time:20360ms step_avg:93.39ms
step:219/1750 train_time:20454ms step_avg:93.40ms
step:220/1750 train_time:20547ms step_avg:93.40ms
step:221/1750 train_time:20640ms step_avg:93.40ms
step:222/1750 train_time:20735ms step_avg:93.40ms
step:223/1750 train_time:20828ms step_avg:93.40ms
step:224/1750 train_time:20922ms step_avg:93.40ms
step:225/1750 train_time:21016ms step_avg:93.40ms
step:226/1750 train_time:21109ms step_avg:93.40ms
step:227/1750 train_time:21203ms step_avg:93.41ms
step:228/1750 train_time:21298ms step_avg:93.41ms
step:229/1750 train_time:21391ms step_avg:93.41ms
step:230/1750 train_time:21484ms step_avg:93.41ms
step:231/1750 train_time:21578ms step_avg:93.41ms
step:232/1750 train_time:21671ms step_avg:93.41ms
step:233/1750 train_time:21765ms step_avg:93.41ms
step:234/1750 train_time:21859ms step_avg:93.41ms
step:235/1750 train_time:21952ms step_avg:93.41ms
step:236/1750 train_time:22046ms step_avg:93.42ms
step:237/1750 train_time:22140ms step_avg:93.42ms
step:238/1750 train_time:22233ms step_avg:93.42ms
step:239/1750 train_time:22327ms step_avg:93.42ms
step:240/1750 train_time:22421ms step_avg:93.42ms
step:241/1750 train_time:22514ms step_avg:93.42ms
step:242/1750 train_time:22607ms step_avg:93.42ms
step:243/1750 train_time:22701ms step_avg:93.42ms
step:244/1750 train_time:22795ms step_avg:93.42ms
step:245/1750 train_time:22889ms step_avg:93.42ms
step:246/1750 train_time:22983ms step_avg:93.42ms
step:247/1750 train_time:23076ms step_avg:93.43ms
step:248/1750 train_time:23170ms step_avg:93.43ms
step:249/1750 train_time:23263ms step_avg:93.43ms
step:250/1750 train_time:23357ms step_avg:93.43ms
step:250/1750 val_loss:4.0992 train_time:23446ms step_avg:93.78ms
step:251/1750 train_time:23473ms step_avg:93.52ms
step:252/1750 train_time:23551ms step_avg:93.46ms
step:253/1750 train_time:23649ms step_avg:93.47ms
step:254/1750 train_time:23743ms step_avg:93.47ms
step:255/1750 train_time:23836ms step_avg:93.47ms
step:256/1750 train_time:23929ms step_avg:93.47ms
step:257/1750 train_time:24022ms step_avg:93.47ms
step:258/1750 train_time:24115ms step_avg:93.47ms
step:259/1750 train_time:24208ms step_avg:93.47ms
step:260/1750 train_time:24301ms step_avg:93.46ms
step:261/1750 train_time:24394ms step_avg:93.46ms
step:262/1750 train_time:24490ms step_avg:93.47ms
step:263/1750 train_time:24586ms step_avg:93.48ms
step:264/1750 train_time:24681ms step_avg:93.49ms
step:265/1750 train_time:24775ms step_avg:93.49ms
step:266/1750 train_time:24869ms step_avg:93.49ms
step:267/1750 train_time:24962ms step_avg:93.49ms
step:268/1750 train_time:25056ms step_avg:93.49ms
step:269/1750 train_time:25149ms step_avg:93.49ms
step:270/1750 train_time:25242ms step_avg:93.49ms
step:271/1750 train_time:25336ms step_avg:93.49ms
step:272/1750 train_time:25430ms step_avg:93.49ms
step:273/1750 train_time:25526ms step_avg:93.50ms
step:274/1750 train_time:25622ms step_avg:93.51ms
step:275/1750 train_time:25716ms step_avg:93.51ms
step:276/1750 train_time:25810ms step_avg:93.52ms
step:277/1750 train_time:25904ms step_avg:93.52ms
step:278/1750 train_time:25998ms step_avg:93.52ms
step:279/1750 train_time:26092ms step_avg:93.52ms
step:280/1750 train_time:26185ms step_avg:93.52ms
step:281/1750 train_time:26279ms step_avg:93.52ms
step:282/1750 train_time:26373ms step_avg:93.52ms
step:283/1750 train_time:26468ms step_avg:93.53ms
step:284/1750 train_time:26563ms step_avg:93.53ms
step:285/1750 train_time:26657ms step_avg:93.53ms
step:286/1750 train_time:26751ms step_avg:93.54ms
step:287/1750 train_time:26845ms step_avg:93.54ms
step:288/1750 train_time:26940ms step_avg:93.54ms
step:289/1750 train_time:27034ms step_avg:93.54ms
step:290/1750 train_time:27128ms step_avg:93.54ms
step:291/1750 train_time:27222ms step_avg:93.55ms
step:292/1750 train_time:27316ms step_avg:93.55ms
step:293/1750 train_time:27410ms step_avg:93.55ms
step:294/1750 train_time:27503ms step_avg:93.55ms
step:295/1750 train_time:27598ms step_avg:93.55ms
step:296/1750 train_time:27692ms step_avg:93.56ms
step:297/1750 train_time:27787ms step_avg:93.56ms
step:298/1750 train_time:27882ms step_avg:93.56ms
step:299/1750 train_time:27976ms step_avg:93.56ms
step:300/1750 train_time:28069ms step_avg:93.56ms
step:301/1750 train_time:28163ms step_avg:93.57ms
step:302/1750 train_time:28257ms step_avg:93.57ms
step:303/1750 train_time:28351ms step_avg:93.57ms
step:304/1750 train_time:28445ms step_avg:93.57ms
step:305/1750 train_time:28540ms step_avg:93.57ms
step:306/1750 train_time:28634ms step_avg:93.57ms
step:307/1750 train_time:28728ms step_avg:93.58ms
step:308/1750 train_time:28824ms step_avg:93.58ms
step:309/1750 train_time:28917ms step_avg:93.58ms
step:310/1750 train_time:29011ms step_avg:93.59ms
step:311/1750 train_time:29105ms step_avg:93.59ms
step:312/1750 train_time:29199ms step_avg:93.59ms
step:313/1750 train_time:29293ms step_avg:93.59ms
step:314/1750 train_time:29388ms step_avg:93.59ms
step:315/1750 train_time:29483ms step_avg:93.60ms
step:316/1750 train_time:29577ms step_avg:93.60ms
step:317/1750 train_time:29670ms step_avg:93.60ms
step:318/1750 train_time:29764ms step_avg:93.60ms
step:319/1750 train_time:29859ms step_avg:93.60ms
step:320/1750 train_time:29953ms step_avg:93.60ms
step:321/1750 train_time:30048ms step_avg:93.61ms
step:322/1750 train_time:30142ms step_avg:93.61ms
step:323/1750 train_time:30235ms step_avg:93.61ms
step:324/1750 train_time:30330ms step_avg:93.61ms
step:325/1750 train_time:30425ms step_avg:93.61ms
step:326/1750 train_time:30519ms step_avg:93.62ms
step:327/1750 train_time:30613ms step_avg:93.62ms
step:328/1750 train_time:30707ms step_avg:93.62ms
step:329/1750 train_time:30802ms step_avg:93.62ms
step:330/1750 train_time:30896ms step_avg:93.62ms
step:331/1750 train_time:30990ms step_avg:93.63ms
step:332/1750 train_time:31085ms step_avg:93.63ms
step:333/1750 train_time:31179ms step_avg:93.63ms
step:334/1750 train_time:31273ms step_avg:93.63ms
step:335/1750 train_time:31367ms step_avg:93.63ms
step:336/1750 train_time:31461ms step_avg:93.63ms
step:337/1750 train_time:31555ms step_avg:93.63ms
step:338/1750 train_time:31649ms step_avg:93.64ms
step:339/1750 train_time:31743ms step_avg:93.64ms
step:340/1750 train_time:31837ms step_avg:93.64ms
step:341/1750 train_time:31931ms step_avg:93.64ms
step:342/1750 train_time:32025ms step_avg:93.64ms
step:343/1750 train_time:32120ms step_avg:93.64ms
step:344/1750 train_time:32214ms step_avg:93.64ms
step:345/1750 train_time:32308ms step_avg:93.65ms
step:346/1750 train_time:32402ms step_avg:93.65ms
step:347/1750 train_time:32496ms step_avg:93.65ms
step:348/1750 train_time:32591ms step_avg:93.65ms
step:349/1750 train_time:32685ms step_avg:93.65ms
step:350/1750 train_time:32779ms step_avg:93.65ms
step:351/1750 train_time:32873ms step_avg:93.65ms
step:352/1750 train_time:32967ms step_avg:93.66ms
step:353/1750 train_time:33061ms step_avg:93.66ms
step:354/1750 train_time:33155ms step_avg:93.66ms
step:355/1750 train_time:33249ms step_avg:93.66ms
step:356/1750 train_time:33342ms step_avg:93.66ms
step:357/1750 train_time:33436ms step_avg:93.66ms
step:358/1750 train_time:33530ms step_avg:93.66ms
step:359/1750 train_time:33624ms step_avg:93.66ms
step:360/1750 train_time:33719ms step_avg:93.66ms
step:361/1750 train_time:33813ms step_avg:93.67ms
step:362/1750 train_time:33907ms step_avg:93.67ms
step:363/1750 train_time:34001ms step_avg:93.67ms
step:364/1750 train_time:34095ms step_avg:93.67ms
step:365/1750 train_time:34189ms step_avg:93.67ms
step:366/1750 train_time:34283ms step_avg:93.67ms
step:367/1750 train_time:34377ms step_avg:93.67ms
step:368/1750 train_time:34471ms step_avg:93.67ms
step:369/1750 train_time:34565ms step_avg:93.67ms
step:370/1750 train_time:34660ms step_avg:93.68ms
step:371/1750 train_time:34754ms step_avg:93.68ms
step:372/1750 train_time:34848ms step_avg:93.68ms
step:373/1750 train_time:34942ms step_avg:93.68ms
step:374/1750 train_time:35036ms step_avg:93.68ms
step:375/1750 train_time:35130ms step_avg:93.68ms
step:375/1750 val_loss:3.8940 train_time:35220ms step_avg:93.92ms
step:376/1750 train_time:35247ms step_avg:93.74ms
step:377/1750 train_time:35327ms step_avg:93.71ms
step:378/1750 train_time:35424ms step_avg:93.72ms
step:379/1750 train_time:35521ms step_avg:93.72ms
step:380/1750 train_time:35615ms step_avg:93.72ms
step:381/1750 train_time:35709ms step_avg:93.72ms
step:382/1750 train_time:35802ms step_avg:93.72ms
step:383/1750 train_time:35896ms step_avg:93.72ms
step:384/1750 train_time:35989ms step_avg:93.72ms
step:385/1750 train_time:36083ms step_avg:93.72ms
step:386/1750 train_time:36176ms step_avg:93.72ms
step:387/1750 train_time:36271ms step_avg:93.72ms
step:388/1750 train_time:36368ms step_avg:93.73ms
step:389/1750 train_time:36463ms step_avg:93.74ms
step:390/1750 train_time:36558ms step_avg:93.74ms
step:391/1750 train_time:36654ms step_avg:93.74ms
step:392/1750 train_time:36752ms step_avg:93.75ms
step:393/1750 train_time:36847ms step_avg:93.76ms
step:394/1750 train_time:36943ms step_avg:93.76ms
step:395/1750 train_time:37038ms step_avg:93.77ms
step:396/1750 train_time:37134ms step_avg:93.77ms
step:397/1750 train_time:37230ms step_avg:93.78ms
step:398/1750 train_time:37326ms step_avg:93.78ms
step:399/1750 train_time:37424ms step_avg:93.79ms
step:400/1750 train_time:37520ms step_avg:93.80ms
step:401/1750 train_time:37616ms step_avg:93.81ms
step:402/1750 train_time:37714ms step_avg:93.81ms
step:403/1750 train_time:37810ms step_avg:93.82ms
step:404/1750 train_time:37906ms step_avg:93.83ms
step:405/1750 train_time:38002ms step_avg:93.83ms
step:406/1750 train_time:38098ms step_avg:93.84ms
step:407/1750 train_time:38193ms step_avg:93.84ms
step:408/1750 train_time:38291ms step_avg:93.85ms
step:409/1750 train_time:38388ms step_avg:93.86ms
step:410/1750 train_time:38485ms step_avg:93.87ms
step:411/1750 train_time:38582ms step_avg:93.87ms
step:412/1750 train_time:38678ms step_avg:93.88ms
step:413/1750 train_time:38774ms step_avg:93.88ms
step:414/1750 train_time:38870ms step_avg:93.89ms
step:415/1750 train_time:38967ms step_avg:93.90ms
step:416/1750 train_time:39063ms step_avg:93.90ms
step:417/1750 train_time:39159ms step_avg:93.91ms
step:418/1750 train_time:39255ms step_avg:93.91ms
step:419/1750 train_time:39352ms step_avg:93.92ms
step:420/1750 train_time:39450ms step_avg:93.93ms
step:421/1750 train_time:39547ms step_avg:93.94ms
step:422/1750 train_time:39645ms step_avg:93.94ms
step:423/1750 train_time:39741ms step_avg:93.95ms
step:424/1750 train_time:39837ms step_avg:93.96ms
step:425/1750 train_time:39933ms step_avg:93.96ms
step:426/1750 train_time:40030ms step_avg:93.97ms
step:427/1750 train_time:40127ms step_avg:93.97ms
step:428/1750 train_time:40224ms step_avg:93.98ms
step:429/1750 train_time:40320ms step_avg:93.99ms
step:430/1750 train_time:40416ms step_avg:93.99ms
step:431/1750 train_time:40513ms step_avg:94.00ms
step:432/1750 train_time:40609ms step_avg:94.00ms
step:433/1750 train_time:40706ms step_avg:94.01ms
step:434/1750 train_time:40803ms step_avg:94.02ms
step:435/1750 train_time:40899ms step_avg:94.02ms
step:436/1750 train_time:40995ms step_avg:94.02ms
step:437/1750 train_time:41091ms step_avg:94.03ms
step:438/1750 train_time:41187ms step_avg:94.04ms
step:439/1750 train_time:41284ms step_avg:94.04ms
step:440/1750 train_time:41380ms step_avg:94.04ms
step:441/1750 train_time:41476ms step_avg:94.05ms
step:442/1750 train_time:41573ms step_avg:94.06ms
step:443/1750 train_time:41670ms step_avg:94.06ms
step:444/1750 train_time:41767ms step_avg:94.07ms
step:445/1750 train_time:41863ms step_avg:94.07ms
step:446/1750 train_time:41959ms step_avg:94.08ms
step:447/1750 train_time:42055ms step_avg:94.08ms
step:448/1750 train_time:42152ms step_avg:94.09ms
step:449/1750 train_time:42248ms step_avg:94.09ms
step:450/1750 train_time:42345ms step_avg:94.10ms
step:451/1750 train_time:42441ms step_avg:94.10ms
step:452/1750 train_time:42538ms step_avg:94.11ms
step:453/1750 train_time:42634ms step_avg:94.12ms
step:454/1750 train_time:42731ms step_avg:94.12ms
step:455/1750 train_time:42827ms step_avg:94.12ms
step:456/1750 train_time:42922ms step_avg:94.13ms
step:457/1750 train_time:43018ms step_avg:94.13ms
step:458/1750 train_time:43115ms step_avg:94.14ms
step:459/1750 train_time:43212ms step_avg:94.14ms
step:460/1750 train_time:43308ms step_avg:94.15ms
step:461/1750 train_time:43405ms step_avg:94.15ms
step:462/1750 train_time:43502ms step_avg:94.16ms
step:463/1750 train_time:43598ms step_avg:94.16ms
step:464/1750 train_time:43694ms step_avg:94.17ms
step:465/1750 train_time:43790ms step_avg:94.17ms
step:466/1750 train_time:43886ms step_avg:94.18ms
step:467/1750 train_time:43982ms step_avg:94.18ms
step:468/1750 train_time:44079ms step_avg:94.19ms
step:469/1750 train_time:44175ms step_avg:94.19ms
step:470/1750 train_time:44272ms step_avg:94.20ms
step:471/1750 train_time:44368ms step_avg:94.20ms
step:472/1750 train_time:44465ms step_avg:94.21ms
step:473/1750 train_time:44562ms step_avg:94.21ms
step:474/1750 train_time:44658ms step_avg:94.22ms
step:475/1750 train_time:44754ms step_avg:94.22ms
step:476/1750 train_time:44850ms step_avg:94.22ms
step:477/1750 train_time:44946ms step_avg:94.23ms
step:478/1750 train_time:45043ms step_avg:94.23ms
step:479/1750 train_time:45139ms step_avg:94.24ms
step:480/1750 train_time:45235ms step_avg:94.24ms
step:481/1750 train_time:45331ms step_avg:94.24ms
step:482/1750 train_time:45428ms step_avg:94.25ms
step:483/1750 train_time:45525ms step_avg:94.25ms
step:484/1750 train_time:45621ms step_avg:94.26ms
step:485/1750 train_time:45717ms step_avg:94.26ms
step:486/1750 train_time:45813ms step_avg:94.27ms
step:487/1750 train_time:45910ms step_avg:94.27ms
step:488/1750 train_time:46006ms step_avg:94.28ms
step:489/1750 train_time:46103ms step_avg:94.28ms
step:490/1750 train_time:46199ms step_avg:94.28ms
step:491/1750 train_time:46295ms step_avg:94.29ms
step:492/1750 train_time:46391ms step_avg:94.29ms
step:493/1750 train_time:46488ms step_avg:94.30ms
step:494/1750 train_time:46584ms step_avg:94.30ms
step:495/1750 train_time:46681ms step_avg:94.30ms
step:496/1750 train_time:46778ms step_avg:94.31ms
step:497/1750 train_time:46875ms step_avg:94.32ms
step:498/1750 train_time:46971ms step_avg:94.32ms
step:499/1750 train_time:47067ms step_avg:94.32ms
step:500/1750 train_time:47164ms step_avg:94.33ms
step:500/1750 val_loss:3.7446 train_time:47255ms step_avg:94.51ms
step:501/1750 train_time:47283ms step_avg:94.38ms
step:502/1750 train_time:47363ms step_avg:94.35ms
step:503/1750 train_time:47466ms step_avg:94.37ms
step:504/1750 train_time:47562ms step_avg:94.37ms
step:505/1750 train_time:47659ms step_avg:94.37ms
step:506/1750 train_time:47755ms step_avg:94.38ms
step:507/1750 train_time:47850ms step_avg:94.38ms
step:508/1750 train_time:47946ms step_avg:94.38ms
step:509/1750 train_time:48042ms step_avg:94.38ms
step:510/1750 train_time:48138ms step_avg:94.39ms
step:511/1750 train_time:48234ms step_avg:94.39ms
step:512/1750 train_time:48332ms step_avg:94.40ms
step:513/1750 train_time:48430ms step_avg:94.40ms
step:514/1750 train_time:48527ms step_avg:94.41ms
step:515/1750 train_time:48625ms step_avg:94.42ms
step:516/1750 train_time:48721ms step_avg:94.42ms
step:517/1750 train_time:48818ms step_avg:94.42ms
step:518/1750 train_time:48913ms step_avg:94.43ms
step:519/1750 train_time:49009ms step_avg:94.43ms
step:520/1750 train_time:49105ms step_avg:94.43ms
step:521/1750 train_time:49201ms step_avg:94.44ms
step:522/1750 train_time:49298ms step_avg:94.44ms
step:523/1750 train_time:49396ms step_avg:94.45ms
step:524/1750 train_time:49494ms step_avg:94.45ms
step:525/1750 train_time:49591ms step_avg:94.46ms
step:526/1750 train_time:49689ms step_avg:94.47ms
step:527/1750 train_time:49785ms step_avg:94.47ms
step:528/1750 train_time:49882ms step_avg:94.47ms
step:529/1750 train_time:49978ms step_avg:94.48ms
step:530/1750 train_time:50075ms step_avg:94.48ms
step:531/1750 train_time:50172ms step_avg:94.49ms
step:532/1750 train_time:50269ms step_avg:94.49ms
step:533/1750 train_time:50365ms step_avg:94.49ms
step:534/1750 train_time:50462ms step_avg:94.50ms
step:535/1750 train_time:50560ms step_avg:94.51ms
step:536/1750 train_time:50658ms step_avg:94.51ms
step:537/1750 train_time:50756ms step_avg:94.52ms
step:538/1750 train_time:50853ms step_avg:94.52ms
step:539/1750 train_time:50950ms step_avg:94.53ms
step:540/1750 train_time:51047ms step_avg:94.53ms
step:541/1750 train_time:51143ms step_avg:94.53ms
step:542/1750 train_time:51239ms step_avg:94.54ms
step:543/1750 train_time:51336ms step_avg:94.54ms
step:544/1750 train_time:51432ms step_avg:94.54ms
step:545/1750 train_time:51529ms step_avg:94.55ms
step:546/1750 train_time:51625ms step_avg:94.55ms
step:547/1750 train_time:51722ms step_avg:94.56ms
step:548/1750 train_time:51819ms step_avg:94.56ms
step:549/1750 train_time:51916ms step_avg:94.56ms
step:550/1750 train_time:52013ms step_avg:94.57ms
step:551/1750 train_time:52110ms step_avg:94.57ms
step:552/1750 train_time:52207ms step_avg:94.58ms
step:553/1750 train_time:52303ms step_avg:94.58ms
step:554/1750 train_time:52399ms step_avg:94.58ms
step:555/1750 train_time:52496ms step_avg:94.59ms
step:556/1750 train_time:52593ms step_avg:94.59ms
step:557/1750 train_time:52690ms step_avg:94.60ms
step:558/1750 train_time:52787ms step_avg:94.60ms
step:559/1750 train_time:52884ms step_avg:94.61ms
step:560/1750 train_time:52981ms step_avg:94.61ms
step:561/1750 train_time:53078ms step_avg:94.61ms
step:562/1750 train_time:53175ms step_avg:94.62ms
step:563/1750 train_time:53271ms step_avg:94.62ms
step:564/1750 train_time:53367ms step_avg:94.62ms
step:565/1750 train_time:53464ms step_avg:94.63ms
step:566/1750 train_time:53561ms step_avg:94.63ms
step:567/1750 train_time:53658ms step_avg:94.64ms
step:568/1750 train_time:53756ms step_avg:94.64ms
step:569/1750 train_time:53853ms step_avg:94.64ms
step:570/1750 train_time:53950ms step_avg:94.65ms
step:571/1750 train_time:54048ms step_avg:94.65ms
step:572/1750 train_time:54144ms step_avg:94.66ms
step:573/1750 train_time:54241ms step_avg:94.66ms
step:574/1750 train_time:54339ms step_avg:94.67ms
step:575/1750 train_time:54435ms step_avg:94.67ms
step:576/1750 train_time:54532ms step_avg:94.67ms
step:577/1750 train_time:54629ms step_avg:94.68ms
step:578/1750 train_time:54725ms step_avg:94.68ms
step:579/1750 train_time:54822ms step_avg:94.68ms
step:580/1750 train_time:54919ms step_avg:94.69ms
step:581/1750 train_time:55017ms step_avg:94.69ms
step:582/1750 train_time:55114ms step_avg:94.70ms
step:583/1750 train_time:55211ms step_avg:94.70ms
step:584/1750 train_time:55308ms step_avg:94.71ms
step:585/1750 train_time:55404ms step_avg:94.71ms
step:586/1750 train_time:55500ms step_avg:94.71ms
step:587/1750 train_time:55598ms step_avg:94.72ms
step:588/1750 train_time:55695ms step_avg:94.72ms
step:589/1750 train_time:55791ms step_avg:94.72ms
step:590/1750 train_time:55889ms step_avg:94.73ms
step:591/1750 train_time:55985ms step_avg:94.73ms
step:592/1750 train_time:56082ms step_avg:94.73ms
step:593/1750 train_time:56179ms step_avg:94.74ms
step:594/1750 train_time:56276ms step_avg:94.74ms
step:595/1750 train_time:56373ms step_avg:94.74ms
step:596/1750 train_time:56470ms step_avg:94.75ms
step:597/1750 train_time:56567ms step_avg:94.75ms
step:598/1750 train_time:56663ms step_avg:94.75ms
step:599/1750 train_time:56760ms step_avg:94.76ms
step:600/1750 train_time:56857ms step_avg:94.76ms
step:601/1750 train_time:56953ms step_avg:94.76ms
step:602/1750 train_time:57050ms step_avg:94.77ms
step:603/1750 train_time:57148ms step_avg:94.77ms
step:604/1750 train_time:57244ms step_avg:94.78ms
step:605/1750 train_time:57341ms step_avg:94.78ms
step:606/1750 train_time:57439ms step_avg:94.78ms
step:607/1750 train_time:57536ms step_avg:94.79ms
step:608/1750 train_time:57633ms step_avg:94.79ms
step:609/1750 train_time:57730ms step_avg:94.80ms
step:610/1750 train_time:57826ms step_avg:94.80ms
step:611/1750 train_time:57922ms step_avg:94.80ms
step:612/1750 train_time:58020ms step_avg:94.80ms
step:613/1750 train_time:58117ms step_avg:94.81ms
step:614/1750 train_time:58214ms step_avg:94.81ms
step:615/1750 train_time:58311ms step_avg:94.82ms
step:616/1750 train_time:58409ms step_avg:94.82ms
step:617/1750 train_time:58506ms step_avg:94.82ms
step:618/1750 train_time:58602ms step_avg:94.83ms
step:619/1750 train_time:58699ms step_avg:94.83ms
step:620/1750 train_time:58796ms step_avg:94.83ms
step:621/1750 train_time:58893ms step_avg:94.84ms
step:622/1750 train_time:58990ms step_avg:94.84ms
step:623/1750 train_time:59086ms step_avg:94.84ms
step:624/1750 train_time:59183ms step_avg:94.84ms
step:625/1750 train_time:59280ms step_avg:94.85ms
step:625/1750 val_loss:3.6592 train_time:59372ms step_avg:94.99ms
step:626/1750 train_time:59398ms step_avg:94.89ms
step:627/1750 train_time:59482ms step_avg:94.87ms
step:628/1750 train_time:59583ms step_avg:94.88ms
step:629/1750 train_time:59680ms step_avg:94.88ms
step:630/1750 train_time:59777ms step_avg:94.88ms
step:631/1750 train_time:59873ms step_avg:94.89ms
step:632/1750 train_time:59969ms step_avg:94.89ms
step:633/1750 train_time:60065ms step_avg:94.89ms
step:634/1750 train_time:60161ms step_avg:94.89ms
step:635/1750 train_time:60257ms step_avg:94.89ms
step:636/1750 train_time:60354ms step_avg:94.90ms
step:637/1750 train_time:60452ms step_avg:94.90ms
step:638/1750 train_time:60551ms step_avg:94.91ms
step:639/1750 train_time:60649ms step_avg:94.91ms
step:640/1750 train_time:60746ms step_avg:94.92ms
step:641/1750 train_time:60843ms step_avg:94.92ms
step:642/1750 train_time:60939ms step_avg:94.92ms
step:643/1750 train_time:61036ms step_avg:94.92ms
step:644/1750 train_time:61132ms step_avg:94.93ms
step:645/1750 train_time:61228ms step_avg:94.93ms
step:646/1750 train_time:61325ms step_avg:94.93ms
step:647/1750 train_time:61422ms step_avg:94.93ms
step:648/1750 train_time:61520ms step_avg:94.94ms
step:649/1750 train_time:61618ms step_avg:94.94ms
step:650/1750 train_time:61716ms step_avg:94.95ms
step:651/1750 train_time:61815ms step_avg:94.95ms
step:652/1750 train_time:61913ms step_avg:94.96ms
step:653/1750 train_time:62011ms step_avg:94.96ms
step:654/1750 train_time:62109ms step_avg:94.97ms
step:655/1750 train_time:62206ms step_avg:94.97ms
step:656/1750 train_time:62304ms step_avg:94.98ms
step:657/1750 train_time:62403ms step_avg:94.98ms
step:658/1750 train_time:62502ms step_avg:94.99ms
step:659/1750 train_time:62601ms step_avg:94.99ms
step:660/1750 train_time:62700ms step_avg:95.00ms
step:661/1750 train_time:62800ms step_avg:95.01ms
step:662/1750 train_time:62899ms step_avg:95.01ms
step:663/1750 train_time:62998ms step_avg:95.02ms
step:664/1750 train_time:63097ms step_avg:95.03ms
step:665/1750 train_time:63196ms step_avg:95.03ms
step:666/1750 train_time:63294ms step_avg:95.04ms
step:667/1750 train_time:63393ms step_avg:95.04ms
step:668/1750 train_time:63491ms step_avg:95.05ms
step:669/1750 train_time:63589ms step_avg:95.05ms
step:670/1750 train_time:63687ms step_avg:95.06ms
step:671/1750 train_time:63786ms step_avg:95.06ms
step:672/1750 train_time:63885ms step_avg:95.07ms
step:673/1750 train_time:63984ms step_avg:95.07ms
step:674/1750 train_time:64082ms step_avg:95.08ms
step:675/1750 train_time:64181ms step_avg:95.08ms
step:676/1750 train_time:64280ms step_avg:95.09ms
step:677/1750 train_time:64379ms step_avg:95.09ms
step:678/1750 train_time:64477ms step_avg:95.10ms
step:679/1750 train_time:64575ms step_avg:95.10ms
step:680/1750 train_time:64675ms step_avg:95.11ms
step:681/1750 train_time:64774ms step_avg:95.12ms
step:682/1750 train_time:64873ms step_avg:95.12ms
step:683/1750 train_time:64973ms step_avg:95.13ms
step:684/1750 train_time:65073ms step_avg:95.14ms
step:685/1750 train_time:65173ms step_avg:95.14ms
step:686/1750 train_time:65273ms step_avg:95.15ms
step:687/1750 train_time:65373ms step_avg:95.16ms
step:688/1750 train_time:65471ms step_avg:95.16ms
step:689/1750 train_time:65569ms step_avg:95.17ms
step:690/1750 train_time:65667ms step_avg:95.17ms
step:691/1750 train_time:65766ms step_avg:95.17ms
step:692/1750 train_time:65863ms step_avg:95.18ms
step:693/1750 train_time:65962ms step_avg:95.18ms
step:694/1750 train_time:66061ms step_avg:95.19ms
step:695/1750 train_time:66160ms step_avg:95.19ms
step:696/1750 train_time:66259ms step_avg:95.20ms
step:697/1750 train_time:66359ms step_avg:95.21ms
step:698/1750 train_time:66458ms step_avg:95.21ms
step:699/1750 train_time:66557ms step_avg:95.22ms
step:700/1750 train_time:66655ms step_avg:95.22ms
step:701/1750 train_time:66754ms step_avg:95.23ms
step:702/1750 train_time:66852ms step_avg:95.23ms
step:703/1750 train_time:66950ms step_avg:95.24ms
step:704/1750 train_time:67050ms step_avg:95.24ms
step:705/1750 train_time:67150ms step_avg:95.25ms
step:706/1750 train_time:67249ms step_avg:95.25ms
step:707/1750 train_time:67347ms step_avg:95.26ms
step:708/1750 train_time:67446ms step_avg:95.26ms
step:709/1750 train_time:67546ms step_avg:95.27ms
step:710/1750 train_time:67644ms step_avg:95.27ms
step:711/1750 train_time:67743ms step_avg:95.28ms
step:712/1750 train_time:67841ms step_avg:95.28ms
step:713/1750 train_time:67940ms step_avg:95.29ms
step:714/1750 train_time:68038ms step_avg:95.29ms
step:715/1750 train_time:68137ms step_avg:95.30ms
step:716/1750 train_time:68236ms step_avg:95.30ms
step:717/1750 train_time:68335ms step_avg:95.31ms
step:718/1750 train_time:68434ms step_avg:95.31ms
step:719/1750 train_time:68533ms step_avg:95.32ms
step:720/1750 train_time:68631ms step_avg:95.32ms
step:721/1750 train_time:68730ms step_avg:95.33ms
step:722/1750 train_time:68830ms step_avg:95.33ms
step:723/1750 train_time:68928ms step_avg:95.34ms
step:724/1750 train_time:69027ms step_avg:95.34ms
step:725/1750 train_time:69126ms step_avg:95.35ms
step:726/1750 train_time:69224ms step_avg:95.35ms
step:727/1750 train_time:69322ms step_avg:95.35ms
step:728/1750 train_time:69420ms step_avg:95.36ms
step:729/1750 train_time:69519ms step_avg:95.36ms
step:730/1750 train_time:69618ms step_avg:95.37ms
step:731/1750 train_time:69717ms step_avg:95.37ms
step:732/1750 train_time:69816ms step_avg:95.38ms
step:733/1750 train_time:69914ms step_avg:95.38ms
step:734/1750 train_time:70013ms step_avg:95.39ms
step:735/1750 train_time:70112ms step_avg:95.39ms
step:736/1750 train_time:70213ms step_avg:95.40ms
step:737/1750 train_time:70312ms step_avg:95.40ms
step:738/1750 train_time:70411ms step_avg:95.41ms
step:739/1750 train_time:70509ms step_avg:95.41ms
step:740/1750 train_time:70608ms step_avg:95.42ms
step:741/1750 train_time:70706ms step_avg:95.42ms
step:742/1750 train_time:70804ms step_avg:95.42ms
step:743/1750 train_time:70902ms step_avg:95.43ms
step:744/1750 train_time:71001ms step_avg:95.43ms
step:745/1750 train_time:71099ms step_avg:95.43ms
step:746/1750 train_time:71198ms step_avg:95.44ms
step:747/1750 train_time:71297ms step_avg:95.44ms
step:748/1750 train_time:71397ms step_avg:95.45ms
step:749/1750 train_time:71496ms step_avg:95.46ms
step:750/1750 train_time:71596ms step_avg:95.46ms
step:750/1750 val_loss:3.5945 train_time:71690ms step_avg:95.59ms
step:751/1750 train_time:71717ms step_avg:95.49ms
step:752/1750 train_time:71803ms step_avg:95.48ms
step:753/1750 train_time:71903ms step_avg:95.49ms
step:754/1750 train_time:72003ms step_avg:95.49ms
step:755/1750 train_time:72101ms step_avg:95.50ms
step:756/1750 train_time:72200ms step_avg:95.50ms
step:757/1750 train_time:72298ms step_avg:95.51ms
step:758/1750 train_time:72396ms step_avg:95.51ms
step:759/1750 train_time:72493ms step_avg:95.51ms
step:760/1750 train_time:72591ms step_avg:95.51ms
step:761/1750 train_time:72689ms step_avg:95.52ms
step:762/1750 train_time:72788ms step_avg:95.52ms
step:763/1750 train_time:72887ms step_avg:95.53ms
step:764/1750 train_time:72986ms step_avg:95.53ms
step:765/1750 train_time:73085ms step_avg:95.54ms
step:766/1750 train_time:73184ms step_avg:95.54ms
step:767/1750 train_time:73282ms step_avg:95.54ms
step:768/1750 train_time:73381ms step_avg:95.55ms
step:769/1750 train_time:73479ms step_avg:95.55ms
step:770/1750 train_time:73578ms step_avg:95.56ms
step:771/1750 train_time:73676ms step_avg:95.56ms
step:772/1750 train_time:73775ms step_avg:95.56ms
step:773/1750 train_time:73875ms step_avg:95.57ms
step:774/1750 train_time:73975ms step_avg:95.57ms
step:775/1750 train_time:74074ms step_avg:95.58ms
step:776/1750 train_time:74173ms step_avg:95.58ms
step:777/1750 train_time:74273ms step_avg:95.59ms
step:778/1750 train_time:74372ms step_avg:95.59ms
step:779/1750 train_time:74470ms step_avg:95.60ms
step:780/1750 train_time:74569ms step_avg:95.60ms
step:781/1750 train_time:74668ms step_avg:95.61ms
step:782/1750 train_time:74767ms step_avg:95.61ms
step:783/1750 train_time:74866ms step_avg:95.61ms
step:784/1750 train_time:74966ms step_avg:95.62ms
step:785/1750 train_time:75065ms step_avg:95.62ms
step:786/1750 train_time:75165ms step_avg:95.63ms
step:787/1750 train_time:75263ms step_avg:95.63ms
step:788/1750 train_time:75363ms step_avg:95.64ms
step:789/1750 train_time:75462ms step_avg:95.64ms
step:790/1750 train_time:75560ms step_avg:95.65ms
step:791/1750 train_time:75660ms step_avg:95.65ms
step:792/1750 train_time:75760ms step_avg:95.66ms
step:793/1750 train_time:75859ms step_avg:95.66ms
step:794/1750 train_time:75959ms step_avg:95.67ms
step:795/1750 train_time:76059ms step_avg:95.67ms
step:796/1750 train_time:76159ms step_avg:95.68ms
step:797/1750 train_time:76259ms step_avg:95.68ms
step:798/1750 train_time:76359ms step_avg:95.69ms
step:799/1750 train_time:76457ms step_avg:95.69ms
step:800/1750 train_time:76555ms step_avg:95.69ms
step:801/1750 train_time:76654ms step_avg:95.70ms
step:802/1750 train_time:76753ms step_avg:95.70ms
step:803/1750 train_time:76852ms step_avg:95.71ms
step:804/1750 train_time:76950ms step_avg:95.71ms
step:805/1750 train_time:77049ms step_avg:95.71ms
step:806/1750 train_time:77148ms step_avg:95.72ms
step:807/1750 train_time:77247ms step_avg:95.72ms
step:808/1750 train_time:77347ms step_avg:95.73ms
step:809/1750 train_time:77447ms step_avg:95.73ms
step:810/1750 train_time:77545ms step_avg:95.73ms
step:811/1750 train_time:77644ms step_avg:95.74ms
step:812/1750 train_time:77743ms step_avg:95.74ms
step:813/1750 train_time:77842ms step_avg:95.75ms
step:814/1750 train_time:77941ms step_avg:95.75ms
step:815/1750 train_time:78040ms step_avg:95.75ms
step:816/1750 train_time:78140ms step_avg:95.76ms
step:817/1750 train_time:78240ms step_avg:95.77ms
step:818/1750 train_time:78340ms step_avg:95.77ms
step:819/1750 train_time:78439ms step_avg:95.77ms
step:820/1750 train_time:78538ms step_avg:95.78ms
step:821/1750 train_time:78638ms step_avg:95.78ms
step:822/1750 train_time:78736ms step_avg:95.79ms
step:823/1750 train_time:78836ms step_avg:95.79ms
step:824/1750 train_time:78935ms step_avg:95.80ms
step:825/1750 train_time:79034ms step_avg:95.80ms
step:826/1750 train_time:79134ms step_avg:95.80ms
step:827/1750 train_time:79232ms step_avg:95.81ms
step:828/1750 train_time:79330ms step_avg:95.81ms
step:829/1750 train_time:79429ms step_avg:95.81ms
step:830/1750 train_time:79527ms step_avg:95.82ms
step:831/1750 train_time:79626ms step_avg:95.82ms
step:832/1750 train_time:79726ms step_avg:95.82ms
step:833/1750 train_time:79825ms step_avg:95.83ms
step:834/1750 train_time:79925ms step_avg:95.83ms
step:835/1750 train_time:80025ms step_avg:95.84ms
step:836/1750 train_time:80124ms step_avg:95.84ms
step:837/1750 train_time:80223ms step_avg:95.85ms
step:838/1750 train_time:80322ms step_avg:95.85ms
step:839/1750 train_time:80420ms step_avg:95.85ms
step:840/1750 train_time:80519ms step_avg:95.86ms
step:841/1750 train_time:80618ms step_avg:95.86ms
step:842/1750 train_time:80717ms step_avg:95.86ms
step:843/1750 train_time:80816ms step_avg:95.87ms
step:844/1750 train_time:80915ms step_avg:95.87ms
step:845/1750 train_time:81014ms step_avg:95.87ms
step:846/1750 train_time:81113ms step_avg:95.88ms
step:847/1750 train_time:81211ms step_avg:95.88ms
step:848/1750 train_time:81310ms step_avg:95.88ms
step:849/1750 train_time:81409ms step_avg:95.89ms
step:850/1750 train_time:81508ms step_avg:95.89ms
step:851/1750 train_time:81607ms step_avg:95.90ms
step:852/1750 train_time:81707ms step_avg:95.90ms
step:853/1750 train_time:81806ms step_avg:95.90ms
step:854/1750 train_time:81906ms step_avg:95.91ms
step:855/1750 train_time:82006ms step_avg:95.91ms
step:856/1750 train_time:82106ms step_avg:95.92ms
step:857/1750 train_time:82205ms step_avg:95.92ms
step:858/1750 train_time:82303ms step_avg:95.92ms
step:859/1750 train_time:82402ms step_avg:95.93ms
step:860/1750 train_time:82500ms step_avg:95.93ms
step:861/1750 train_time:82599ms step_avg:95.93ms
step:862/1750 train_time:82698ms step_avg:95.94ms
step:863/1750 train_time:82797ms step_avg:95.94ms
step:864/1750 train_time:82897ms step_avg:95.95ms
step:865/1750 train_time:82997ms step_avg:95.95ms
step:866/1750 train_time:83096ms step_avg:95.95ms
step:867/1750 train_time:83195ms step_avg:95.96ms
step:868/1750 train_time:83295ms step_avg:95.96ms
step:869/1750 train_time:83394ms step_avg:95.97ms
step:870/1750 train_time:83493ms step_avg:95.97ms
step:871/1750 train_time:83592ms step_avg:95.97ms
step:872/1750 train_time:83690ms step_avg:95.97ms
step:873/1750 train_time:83789ms step_avg:95.98ms
step:874/1750 train_time:83888ms step_avg:95.98ms
step:875/1750 train_time:83987ms step_avg:95.99ms
step:875/1750 val_loss:3.5449 train_time:84081ms step_avg:96.09ms
step:876/1750 train_time:84108ms step_avg:96.01ms
step:877/1750 train_time:84195ms step_avg:96.00ms
step:878/1750 train_time:84296ms step_avg:96.01ms
step:879/1750 train_time:84396ms step_avg:96.01ms
step:880/1750 train_time:84494ms step_avg:96.02ms
step:881/1750 train_time:84593ms step_avg:96.02ms
step:882/1750 train_time:84691ms step_avg:96.02ms
step:883/1750 train_time:84790ms step_avg:96.02ms
step:884/1750 train_time:84888ms step_avg:96.03ms
step:885/1750 train_time:84986ms step_avg:96.03ms
step:886/1750 train_time:85086ms step_avg:96.03ms
step:887/1750 train_time:85185ms step_avg:96.04ms
step:888/1750 train_time:85286ms step_avg:96.04ms
step:889/1750 train_time:85387ms step_avg:96.05ms
step:890/1750 train_time:85487ms step_avg:96.05ms
step:891/1750 train_time:85587ms step_avg:96.06ms
step:892/1750 train_time:85687ms step_avg:96.06ms
step:893/1750 train_time:85785ms step_avg:96.06ms
step:894/1750 train_time:85884ms step_avg:96.07ms
step:895/1750 train_time:85983ms step_avg:96.07ms
step:896/1750 train_time:86082ms step_avg:96.07ms
step:897/1750 train_time:86180ms step_avg:96.08ms
step:898/1750 train_time:86279ms step_avg:96.08ms
step:899/1750 train_time:86378ms step_avg:96.08ms
step:900/1750 train_time:86477ms step_avg:96.09ms
step:901/1750 train_time:86576ms step_avg:96.09ms
step:902/1750 train_time:86675ms step_avg:96.09ms
step:903/1750 train_time:86773ms step_avg:96.09ms
step:904/1750 train_time:86872ms step_avg:96.10ms
step:905/1750 train_time:86971ms step_avg:96.10ms
step:906/1750 train_time:87070ms step_avg:96.10ms
step:907/1750 train_time:87169ms step_avg:96.11ms
step:908/1750 train_time:87268ms step_avg:96.11ms
step:909/1750 train_time:87367ms step_avg:96.11ms
step:910/1750 train_time:87468ms step_avg:96.12ms
step:911/1750 train_time:87569ms step_avg:96.12ms
step:912/1750 train_time:87670ms step_avg:96.13ms
step:913/1750 train_time:87771ms step_avg:96.13ms
step:914/1750 train_time:87871ms step_avg:96.14ms
step:915/1750 train_time:87971ms step_avg:96.14ms
step:916/1750 train_time:88071ms step_avg:96.15ms
step:917/1750 train_time:88172ms step_avg:96.15ms
step:918/1750 train_time:88273ms step_avg:96.16ms
step:919/1750 train_time:88374ms step_avg:96.16ms
step:920/1750 train_time:88475ms step_avg:96.17ms
step:921/1750 train_time:88575ms step_avg:96.17ms
step:922/1750 train_time:88676ms step_avg:96.18ms
step:923/1750 train_time:88776ms step_avg:96.18ms
step:924/1750 train_time:88877ms step_avg:96.19ms
step:925/1750 train_time:88977ms step_avg:96.19ms
step:926/1750 train_time:89077ms step_avg:96.20ms
step:927/1750 train_time:89177ms step_avg:96.20ms
step:928/1750 train_time:89277ms step_avg:96.20ms
step:929/1750 train_time:89377ms step_avg:96.21ms
step:930/1750 train_time:89478ms step_avg:96.21ms
step:931/1750 train_time:89578ms step_avg:96.22ms
step:932/1750 train_time:89678ms step_avg:96.22ms
step:933/1750 train_time:89779ms step_avg:96.23ms
step:934/1750 train_time:89879ms step_avg:96.23ms
step:935/1750 train_time:89979ms step_avg:96.23ms
step:936/1750 train_time:90078ms step_avg:96.24ms
step:937/1750 train_time:90178ms step_avg:96.24ms
step:938/1750 train_time:90278ms step_avg:96.24ms
step:939/1750 train_time:90378ms step_avg:96.25ms
step:940/1750 train_time:90478ms step_avg:96.25ms
step:941/1750 train_time:90580ms step_avg:96.26ms
step:942/1750 train_time:90680ms step_avg:96.26ms
step:943/1750 train_time:90780ms step_avg:96.27ms
step:944/1750 train_time:90881ms step_avg:96.27ms
step:945/1750 train_time:90981ms step_avg:96.28ms
step:946/1750 train_time:91081ms step_avg:96.28ms
step:947/1750 train_time:91181ms step_avg:96.28ms
step:948/1750 train_time:91281ms step_avg:96.29ms
step:949/1750 train_time:91381ms step_avg:96.29ms
step:950/1750 train_time:91481ms step_avg:96.30ms
step:951/1750 train_time:91582ms step_avg:96.30ms
step:952/1750 train_time:91682ms step_avg:96.30ms
step:953/1750 train_time:91781ms step_avg:96.31ms
step:954/1750 train_time:91881ms step_avg:96.31ms
step:955/1750 train_time:91982ms step_avg:96.32ms
step:956/1750 train_time:92082ms step_avg:96.32ms
step:957/1750 train_time:92182ms step_avg:96.32ms
step:958/1750 train_time:92282ms step_avg:96.33ms
step:959/1750 train_time:92382ms step_avg:96.33ms
step:960/1750 train_time:92483ms step_avg:96.34ms
step:961/1750 train_time:92583ms step_avg:96.34ms
step:962/1750 train_time:92683ms step_avg:96.34ms
step:963/1750 train_time:92783ms step_avg:96.35ms
step:964/1750 train_time:92884ms step_avg:96.35ms
step:965/1750 train_time:92984ms step_avg:96.36ms
step:966/1750 train_time:93084ms step_avg:96.36ms
step:967/1750 train_time:93184ms step_avg:96.36ms
step:968/1750 train_time:93285ms step_avg:96.37ms
step:969/1750 train_time:93385ms step_avg:96.37ms
step:970/1750 train_time:93485ms step_avg:96.38ms
step:971/1750 train_time:93586ms step_avg:96.38ms
step:972/1750 train_time:93686ms step_avg:96.38ms
step:973/1750 train_time:93787ms step_avg:96.39ms
step:974/1750 train_time:93888ms step_avg:96.39ms
step:975/1750 train_time:93988ms step_avg:96.40ms
step:976/1750 train_time:94088ms step_avg:96.40ms
step:977/1750 train_time:94189ms step_avg:96.41ms
step:978/1750 train_time:94289ms step_avg:96.41ms
step:979/1750 train_time:94390ms step_avg:96.42ms
step:980/1750 train_time:94491ms step_avg:96.42ms
step:981/1750 train_time:94592ms step_avg:96.42ms
step:982/1750 train_time:94693ms step_avg:96.43ms
step:983/1750 train_time:94795ms step_avg:96.43ms
step:984/1750 train_time:94895ms step_avg:96.44ms
step:985/1750 train_time:94996ms step_avg:96.44ms
step:986/1750 train_time:95096ms step_avg:96.45ms
step:987/1750 train_time:95197ms step_avg:96.45ms
step:988/1750 train_time:95297ms step_avg:96.45ms
step:989/1750 train_time:95398ms step_avg:96.46ms
step:990/1750 train_time:95498ms step_avg:96.46ms
step:991/1750 train_time:95598ms step_avg:96.47ms
step:992/1750 train_time:95699ms step_avg:96.47ms
step:993/1750 train_time:95799ms step_avg:96.47ms
step:994/1750 train_time:95900ms step_avg:96.48ms
step:995/1750 train_time:96001ms step_avg:96.48ms
step:996/1750 train_time:96101ms step_avg:96.49ms
step:997/1750 train_time:96201ms step_avg:96.49ms
step:998/1750 train_time:96301ms step_avg:96.49ms
step:999/1750 train_time:96401ms step_avg:96.50ms
step:1000/1750 train_time:96501ms step_avg:96.50ms
step:1000/1750 val_loss:3.5048 train_time:96596ms step_avg:96.60ms
step:1001/1750 train_time:96622ms step_avg:96.53ms
step:1002/1750 train_time:96710ms step_avg:96.52ms
step:1003/1750 train_time:96812ms step_avg:96.52ms
step:1004/1750 train_time:96913ms step_avg:96.53ms
step:1005/1750 train_time:97013ms step_avg:96.53ms
step:1006/1750 train_time:97113ms step_avg:96.53ms
step:1007/1750 train_time:97213ms step_avg:96.54ms
step:1008/1750 train_time:97313ms step_avg:96.54ms
step:1009/1750 train_time:97412ms step_avg:96.54ms
step:1010/1750 train_time:97512ms step_avg:96.55ms
step:1011/1750 train_time:97614ms step_avg:96.55ms
step:1012/1750 train_time:97716ms step_avg:96.56ms
step:1013/1750 train_time:97818ms step_avg:96.56ms
step:1014/1750 train_time:97918ms step_avg:96.57ms
step:1015/1750 train_time:98019ms step_avg:96.57ms
step:1016/1750 train_time:98119ms step_avg:96.57ms
step:1017/1750 train_time:98219ms step_avg:96.58ms
step:1018/1750 train_time:98318ms step_avg:96.58ms
step:1019/1750 train_time:98418ms step_avg:96.58ms
step:1020/1750 train_time:98518ms step_avg:96.59ms
step:1021/1750 train_time:98618ms step_avg:96.59ms
step:1022/1750 train_time:98719ms step_avg:96.59ms
step:1023/1750 train_time:98820ms step_avg:96.60ms
step:1024/1750 train_time:98921ms step_avg:96.60ms
step:1025/1750 train_time:99022ms step_avg:96.61ms
step:1026/1750 train_time:99122ms step_avg:96.61ms
step:1027/1750 train_time:99221ms step_avg:96.61ms
step:1028/1750 train_time:99322ms step_avg:96.62ms
step:1029/1750 train_time:99423ms step_avg:96.62ms
step:1030/1750 train_time:99523ms step_avg:96.62ms
step:1031/1750 train_time:99624ms step_avg:96.63ms
step:1032/1750 train_time:99725ms step_avg:96.63ms
step:1033/1750 train_time:99826ms step_avg:96.64ms
step:1034/1750 train_time:99926ms step_avg:96.64ms
step:1035/1750 train_time:100027ms step_avg:96.64ms
step:1036/1750 train_time:100128ms step_avg:96.65ms
step:1037/1750 train_time:100229ms step_avg:96.65ms
step:1038/1750 train_time:100329ms step_avg:96.66ms
step:1039/1750 train_time:100429ms step_avg:96.66ms
step:1040/1750 train_time:100529ms step_avg:96.66ms
step:1041/1750 train_time:100630ms step_avg:96.67ms
step:1042/1750 train_time:100731ms step_avg:96.67ms
step:1043/1750 train_time:100831ms step_avg:96.67ms
step:1044/1750 train_time:100931ms step_avg:96.68ms
step:1045/1750 train_time:101033ms step_avg:96.68ms
step:1046/1750 train_time:101134ms step_avg:96.69ms
step:1047/1750 train_time:101235ms step_avg:96.69ms
step:1048/1750 train_time:101335ms step_avg:96.69ms
step:1049/1750 train_time:101436ms step_avg:96.70ms
step:1050/1750 train_time:101536ms step_avg:96.70ms
step:1051/1750 train_time:101636ms step_avg:96.70ms
step:1052/1750 train_time:101736ms step_avg:96.71ms
step:1053/1750 train_time:101837ms step_avg:96.71ms
step:1054/1750 train_time:101937ms step_avg:96.71ms
step:1055/1750 train_time:102039ms step_avg:96.72ms
step:1056/1750 train_time:102139ms step_avg:96.72ms
step:1057/1750 train_time:102240ms step_avg:96.73ms
step:1058/1750 train_time:102339ms step_avg:96.73ms
step:1059/1750 train_time:102439ms step_avg:96.73ms
step:1060/1750 train_time:102540ms step_avg:96.74ms
step:1061/1750 train_time:102640ms step_avg:96.74ms
step:1062/1750 train_time:102741ms step_avg:96.74ms
step:1063/1750 train_time:102841ms step_avg:96.75ms
step:1064/1750 train_time:102942ms step_avg:96.75ms
step:1065/1750 train_time:103043ms step_avg:96.75ms
step:1066/1750 train_time:103143ms step_avg:96.76ms
step:1067/1750 train_time:103244ms step_avg:96.76ms
step:1068/1750 train_time:103344ms step_avg:96.76ms
step:1069/1750 train_time:103445ms step_avg:96.77ms
step:1070/1750 train_time:103546ms step_avg:96.77ms
step:1071/1750 train_time:103647ms step_avg:96.78ms
step:1072/1750 train_time:103748ms step_avg:96.78ms
step:1073/1750 train_time:103850ms step_avg:96.78ms
step:1074/1750 train_time:103951ms step_avg:96.79ms
step:1075/1750 train_time:104051ms step_avg:96.79ms
step:1076/1750 train_time:104152ms step_avg:96.80ms
step:1077/1750 train_time:104254ms step_avg:96.80ms
step:1078/1750 train_time:104354ms step_avg:96.80ms
step:1079/1750 train_time:104455ms step_avg:96.81ms
step:1080/1750 train_time:104557ms step_avg:96.81ms
step:1081/1750 train_time:104658ms step_avg:96.82ms
step:1082/1750 train_time:104759ms step_avg:96.82ms
step:1083/1750 train_time:104859ms step_avg:96.82ms
step:1084/1750 train_time:104960ms step_avg:96.83ms
step:1085/1750 train_time:105060ms step_avg:96.83ms
step:1086/1750 train_time:105161ms step_avg:96.83ms
step:1087/1750 train_time:105261ms step_avg:96.84ms
step:1088/1750 train_time:105361ms step_avg:96.84ms
step:1089/1750 train_time:105462ms step_avg:96.84ms
step:1090/1750 train_time:105563ms step_avg:96.85ms
step:1091/1750 train_time:105664ms step_avg:96.85ms
step:1092/1750 train_time:105765ms step_avg:96.85ms
step:1093/1750 train_time:105866ms step_avg:96.86ms
step:1094/1750 train_time:105966ms step_avg:96.86ms
step:1095/1750 train_time:106067ms step_avg:96.86ms
step:1096/1750 train_time:106168ms step_avg:96.87ms
step:1097/1750 train_time:106268ms step_avg:96.87ms
step:1098/1750 train_time:106368ms step_avg:96.87ms
step:1099/1750 train_time:106469ms step_avg:96.88ms
step:1100/1750 train_time:106570ms step_avg:96.88ms
step:1101/1750 train_time:106672ms step_avg:96.89ms
step:1102/1750 train_time:106772ms step_avg:96.89ms
step:1103/1750 train_time:106873ms step_avg:96.89ms
step:1104/1750 train_time:106974ms step_avg:96.90ms
step:1105/1750 train_time:107075ms step_avg:96.90ms
step:1106/1750 train_time:107177ms step_avg:96.91ms
step:1107/1750 train_time:107278ms step_avg:96.91ms
step:1108/1750 train_time:107378ms step_avg:96.91ms
step:1109/1750 train_time:107479ms step_avg:96.92ms
step:1110/1750 train_time:107580ms step_avg:96.92ms
step:1111/1750 train_time:107680ms step_avg:96.92ms
step:1112/1750 train_time:107780ms step_avg:96.92ms
step:1113/1750 train_time:107881ms step_avg:96.93ms
step:1114/1750 train_time:107982ms step_avg:96.93ms
step:1115/1750 train_time:108083ms step_avg:96.94ms
step:1116/1750 train_time:108186ms step_avg:96.94ms
step:1117/1750 train_time:108286ms step_avg:96.94ms
step:1118/1750 train_time:108387ms step_avg:96.95ms
step:1119/1750 train_time:108488ms step_avg:96.95ms
step:1120/1750 train_time:108589ms step_avg:96.95ms
step:1121/1750 train_time:108690ms step_avg:96.96ms
step:1122/1750 train_time:108790ms step_avg:96.96ms
step:1123/1750 train_time:108891ms step_avg:96.96ms
step:1124/1750 train_time:108992ms step_avg:96.97ms
step:1125/1750 train_time:109093ms step_avg:96.97ms
step:1125/1750 val_loss:3.4511 train_time:109188ms step_avg:97.06ms
step:1126/1750 train_time:109215ms step_avg:96.99ms
step:1127/1750 train_time:109303ms step_avg:96.99ms
step:1128/1750 train_time:109403ms step_avg:96.99ms
step:1129/1750 train_time:109505ms step_avg:96.99ms
step:1130/1750 train_time:109607ms step_avg:97.00ms
step:1131/1750 train_time:109708ms step_avg:97.00ms
step:1132/1750 train_time:109808ms step_avg:97.00ms
step:1133/1750 train_time:109907ms step_avg:97.01ms
step:1134/1750 train_time:110008ms step_avg:97.01ms
step:1135/1750 train_time:110107ms step_avg:97.01ms
step:1136/1750 train_time:110209ms step_avg:97.02ms
step:1137/1750 train_time:110311ms step_avg:97.02ms
step:1138/1750 train_time:110412ms step_avg:97.02ms
step:1139/1750 train_time:110514ms step_avg:97.03ms
step:1140/1750 train_time:110616ms step_avg:97.03ms
step:1141/1750 train_time:110716ms step_avg:97.03ms
step:1142/1750 train_time:110816ms step_avg:97.04ms
step:1143/1750 train_time:110916ms step_avg:97.04ms
step:1144/1750 train_time:111016ms step_avg:97.04ms
step:1145/1750 train_time:111116ms step_avg:97.04ms
step:1146/1750 train_time:111216ms step_avg:97.05ms
step:1147/1750 train_time:111317ms step_avg:97.05ms
step:1148/1750 train_time:111419ms step_avg:97.05ms
step:1149/1750 train_time:111519ms step_avg:97.06ms
step:1150/1750 train_time:111621ms step_avg:97.06ms
step:1151/1750 train_time:111721ms step_avg:97.06ms
step:1152/1750 train_time:111822ms step_avg:97.07ms
step:1153/1750 train_time:111922ms step_avg:97.07ms
step:1154/1750 train_time:112022ms step_avg:97.07ms
step:1155/1750 train_time:112123ms step_avg:97.08ms
step:1156/1750 train_time:112224ms step_avg:97.08ms
step:1157/1750 train_time:112326ms step_avg:97.08ms
step:1158/1750 train_time:112427ms step_avg:97.09ms
step:1159/1750 train_time:112530ms step_avg:97.09ms
step:1160/1750 train_time:112631ms step_avg:97.10ms
step:1161/1750 train_time:112731ms step_avg:97.10ms
step:1162/1750 train_time:112832ms step_avg:97.10ms
step:1163/1750 train_time:112932ms step_avg:97.10ms
step:1164/1750 train_time:113033ms step_avg:97.11ms
step:1165/1750 train_time:113134ms step_avg:97.11ms
step:1166/1750 train_time:113235ms step_avg:97.11ms
step:1167/1750 train_time:113336ms step_avg:97.12ms
step:1168/1750 train_time:113437ms step_avg:97.12ms
step:1169/1750 train_time:113538ms step_avg:97.12ms
step:1170/1750 train_time:113639ms step_avg:97.13ms
step:1171/1750 train_time:113742ms step_avg:97.13ms
step:1172/1750 train_time:113843ms step_avg:97.14ms
step:1173/1750 train_time:113945ms step_avg:97.14ms
step:1174/1750 train_time:114047ms step_avg:97.14ms
step:1175/1750 train_time:114150ms step_avg:97.15ms
step:1176/1750 train_time:114251ms step_avg:97.15ms
step:1177/1750 train_time:114353ms step_avg:97.16ms
step:1178/1750 train_time:114455ms step_avg:97.16ms
step:1179/1750 train_time:114558ms step_avg:97.17ms
step:1180/1750 train_time:114659ms step_avg:97.17ms
step:1181/1750 train_time:114760ms step_avg:97.17ms
step:1182/1750 train_time:114862ms step_avg:97.18ms
step:1183/1750 train_time:114963ms step_avg:97.18ms
step:1184/1750 train_time:115066ms step_avg:97.18ms
step:1185/1750 train_time:115170ms step_avg:97.19ms
step:1186/1750 train_time:115272ms step_avg:97.19ms
step:1187/1750 train_time:115373ms step_avg:97.20ms
step:1188/1750 train_time:115475ms step_avg:97.20ms
step:1189/1750 train_time:115576ms step_avg:97.20ms
step:1190/1750 train_time:115678ms step_avg:97.21ms
step:1191/1750 train_time:115780ms step_avg:97.21ms
step:1192/1750 train_time:115880ms step_avg:97.22ms
step:1193/1750 train_time:115981ms step_avg:97.22ms
step:1194/1750 train_time:116082ms step_avg:97.22ms
step:1195/1750 train_time:116184ms step_avg:97.23ms
step:1196/1750 train_time:116286ms step_avg:97.23ms
step:1197/1750 train_time:116388ms step_avg:97.23ms
step:1198/1750 train_time:116491ms step_avg:97.24ms
step:1199/1750 train_time:116593ms step_avg:97.24ms
step:1200/1750 train_time:116695ms step_avg:97.25ms
step:1201/1750 train_time:116796ms step_avg:97.25ms
step:1202/1750 train_time:116899ms step_avg:97.25ms
step:1203/1750 train_time:117001ms step_avg:97.26ms
step:1204/1750 train_time:117101ms step_avg:97.26ms
step:1205/1750 train_time:117203ms step_avg:97.26ms
step:1206/1750 train_time:117304ms step_avg:97.27ms
step:1207/1750 train_time:117407ms step_avg:97.27ms
step:1208/1750 train_time:117509ms step_avg:97.28ms
step:1209/1750 train_time:117611ms step_avg:97.28ms
step:1210/1750 train_time:117714ms step_avg:97.28ms
step:1211/1750 train_time:117816ms step_avg:97.29ms
step:1212/1750 train_time:117917ms step_avg:97.29ms
step:1213/1750 train_time:118019ms step_avg:97.30ms
step:1214/1750 train_time:118120ms step_avg:97.30ms
step:1215/1750 train_time:118222ms step_avg:97.30ms
step:1216/1750 train_time:118326ms step_avg:97.31ms
step:1217/1750 train_time:118428ms step_avg:97.31ms
step:1218/1750 train_time:118532ms step_avg:97.32ms
step:1219/1750 train_time:118634ms step_avg:97.32ms
step:1220/1750 train_time:118736ms step_avg:97.32ms
step:1221/1750 train_time:118838ms step_avg:97.33ms
step:1222/1750 train_time:118940ms step_avg:97.33ms
step:1223/1750 train_time:119042ms step_avg:97.34ms
step:1224/1750 train_time:119143ms step_avg:97.34ms
step:1225/1750 train_time:119244ms step_avg:97.34ms
step:1226/1750 train_time:119346ms step_avg:97.35ms
step:1227/1750 train_time:119449ms step_avg:97.35ms
step:1228/1750 train_time:119551ms step_avg:97.35ms
step:1229/1750 train_time:119653ms step_avg:97.36ms
step:1230/1750 train_time:119755ms step_avg:97.36ms
step:1231/1750 train_time:119857ms step_avg:97.37ms
step:1232/1750 train_time:119959ms step_avg:97.37ms
step:1233/1750 train_time:120060ms step_avg:97.37ms
step:1234/1750 train_time:120162ms step_avg:97.38ms
step:1235/1750 train_time:120263ms step_avg:97.38ms
step:1236/1750 train_time:120365ms step_avg:97.38ms
step:1237/1750 train_time:120468ms step_avg:97.39ms
step:1238/1750 train_time:120569ms step_avg:97.39ms
step:1239/1750 train_time:120671ms step_avg:97.39ms
step:1240/1750 train_time:120772ms step_avg:97.40ms
step:1241/1750 train_time:120875ms step_avg:97.40ms
step:1242/1750 train_time:120978ms step_avg:97.41ms
step:1243/1750 train_time:121079ms step_avg:97.41ms
step:1244/1750 train_time:121181ms step_avg:97.41ms
step:1245/1750 train_time:121281ms step_avg:97.41ms
step:1246/1750 train_time:121384ms step_avg:97.42ms
step:1247/1750 train_time:121486ms step_avg:97.42ms
step:1248/1750 train_time:121589ms step_avg:97.43ms
step:1249/1750 train_time:121692ms step_avg:97.43ms
step:1250/1750 train_time:121793ms step_avg:97.43ms
step:1250/1750 val_loss:3.4058 train_time:121889ms step_avg:97.51ms
step:1251/1750 train_time:121916ms step_avg:97.45ms
step:1252/1750 train_time:122007ms step_avg:97.45ms
step:1253/1750 train_time:122109ms step_avg:97.45ms
step:1254/1750 train_time:122212ms step_avg:97.46ms
step:1255/1750 train_time:122314ms step_avg:97.46ms
step:1256/1750 train_time:122415ms step_avg:97.46ms
step:1257/1750 train_time:122516ms step_avg:97.47ms
step:1258/1750 train_time:122618ms step_avg:97.47ms
step:1259/1750 train_time:122719ms step_avg:97.47ms
step:1260/1750 train_time:122820ms step_avg:97.48ms
step:1261/1750 train_time:122922ms step_avg:97.48ms
step:1262/1750 train_time:123025ms step_avg:97.48ms
step:1263/1750 train_time:123127ms step_avg:97.49ms
step:1264/1750 train_time:123228ms step_avg:97.49ms
step:1265/1750 train_time:123329ms step_avg:97.49ms
step:1266/1750 train_time:123431ms step_avg:97.50ms
step:1267/1750 train_time:123532ms step_avg:97.50ms
step:1268/1750 train_time:123635ms step_avg:97.50ms
step:1269/1750 train_time:123736ms step_avg:97.51ms
step:1270/1750 train_time:123837ms step_avg:97.51ms
step:1271/1750 train_time:123942ms step_avg:97.51ms
step:1272/1750 train_time:124043ms step_avg:97.52ms
step:1273/1750 train_time:124145ms step_avg:97.52ms
step:1274/1750 train_time:124246ms step_avg:97.52ms
step:1275/1750 train_time:124347ms step_avg:97.53ms
step:1276/1750 train_time:124451ms step_avg:97.53ms
step:1277/1750 train_time:124552ms step_avg:97.53ms
step:1278/1750 train_time:124654ms step_avg:97.54ms
step:1279/1750 train_time:124756ms step_avg:97.54ms
step:1280/1750 train_time:124859ms step_avg:97.55ms
step:1281/1750 train_time:124962ms step_avg:97.55ms
step:1282/1750 train_time:125064ms step_avg:97.55ms
step:1283/1750 train_time:125165ms step_avg:97.56ms
step:1284/1750 train_time:125266ms step_avg:97.56ms
step:1285/1750 train_time:125367ms step_avg:97.56ms
step:1286/1750 train_time:125468ms step_avg:97.56ms
step:1287/1750 train_time:125570ms step_avg:97.57ms
step:1288/1750 train_time:125672ms step_avg:97.57ms
step:1289/1750 train_time:125775ms step_avg:97.58ms
step:1290/1750 train_time:125877ms step_avg:97.58ms
step:1291/1750 train_time:125980ms step_avg:97.58ms
step:1292/1750 train_time:126082ms step_avg:97.59ms
step:1293/1750 train_time:126184ms step_avg:97.59ms
step:1294/1750 train_time:126286ms step_avg:97.59ms
step:1295/1750 train_time:126387ms step_avg:97.60ms
step:1296/1750 train_time:126488ms step_avg:97.60ms
step:1297/1750 train_time:126589ms step_avg:97.60ms
step:1298/1750 train_time:126691ms step_avg:97.60ms
step:1299/1750 train_time:126793ms step_avg:97.61ms
step:1300/1750 train_time:126896ms step_avg:97.61ms
step:1301/1750 train_time:126999ms step_avg:97.62ms
step:1302/1750 train_time:127101ms step_avg:97.62ms
step:1303/1750 train_time:127203ms step_avg:97.62ms
step:1304/1750 train_time:127305ms step_avg:97.63ms
step:1305/1750 train_time:127406ms step_avg:97.63ms
step:1306/1750 train_time:127507ms step_avg:97.63ms
step:1307/1750 train_time:127609ms step_avg:97.63ms
step:1308/1750 train_time:127710ms step_avg:97.64ms
step:1309/1750 train_time:127812ms step_avg:97.64ms
step:1310/1750 train_time:127914ms step_avg:97.64ms
step:1311/1750 train_time:128017ms step_avg:97.65ms
step:1312/1750 train_time:128119ms step_avg:97.65ms
step:1313/1750 train_time:128223ms step_avg:97.66ms
step:1314/1750 train_time:128325ms step_avg:97.66ms
step:1315/1750 train_time:128426ms step_avg:97.66ms
step:1316/1750 train_time:128528ms step_avg:97.67ms
step:1317/1750 train_time:128629ms step_avg:97.67ms
step:1318/1750 train_time:128731ms step_avg:97.67ms
step:1319/1750 train_time:128833ms step_avg:97.67ms
step:1320/1750 train_time:128937ms step_avg:97.68ms
step:1321/1750 train_time:129039ms step_avg:97.68ms
step:1322/1750 train_time:129140ms step_avg:97.69ms
step:1323/1750 train_time:129242ms step_avg:97.69ms
step:1324/1750 train_time:129344ms step_avg:97.69ms
step:1325/1750 train_time:129446ms step_avg:97.70ms
step:1326/1750 train_time:129548ms step_avg:97.70ms
step:1327/1750 train_time:129649ms step_avg:97.70ms
step:1328/1750 train_time:129750ms step_avg:97.70ms
step:1329/1750 train_time:129852ms step_avg:97.71ms
step:1330/1750 train_time:129956ms step_avg:97.71ms
step:1331/1750 train_time:130059ms step_avg:97.71ms
step:1332/1750 train_time:130160ms step_avg:97.72ms
step:1333/1750 train_time:130262ms step_avg:97.72ms
step:1334/1750 train_time:130364ms step_avg:97.72ms
step:1335/1750 train_time:130465ms step_avg:97.73ms
step:1336/1750 train_time:130567ms step_avg:97.73ms
step:1337/1750 train_time:130669ms step_avg:97.73ms
step:1338/1750 train_time:130771ms step_avg:97.74ms
step:1339/1750 train_time:130874ms step_avg:97.74ms
step:1340/1750 train_time:130976ms step_avg:97.74ms
step:1341/1750 train_time:131077ms step_avg:97.75ms
step:1342/1750 train_time:131179ms step_avg:97.75ms
step:1343/1750 train_time:131281ms step_avg:97.75ms
step:1344/1750 train_time:131383ms step_avg:97.76ms
step:1345/1750 train_time:131485ms step_avg:97.76ms
step:1346/1750 train_time:131588ms step_avg:97.76ms
step:1347/1750 train_time:131689ms step_avg:97.76ms
step:1348/1750 train_time:131791ms step_avg:97.77ms
step:1349/1750 train_time:131893ms step_avg:97.77ms
step:1350/1750 train_time:131997ms step_avg:97.78ms
step:1351/1750 train_time:132099ms step_avg:97.78ms
step:1352/1750 train_time:132201ms step_avg:97.78ms
step:1353/1750 train_time:132303ms step_avg:97.78ms
step:1354/1750 train_time:132405ms step_avg:97.79ms
step:1355/1750 train_time:132506ms step_avg:97.79ms
step:1356/1750 train_time:132608ms step_avg:97.79ms
step:1357/1750 train_time:132709ms step_avg:97.80ms
step:1358/1750 train_time:132811ms step_avg:97.80ms
step:1359/1750 train_time:132914ms step_avg:97.80ms
step:1360/1750 train_time:133016ms step_avg:97.81ms
step:1361/1750 train_time:133117ms step_avg:97.81ms
step:1362/1750 train_time:133219ms step_avg:97.81ms
step:1363/1750 train_time:133322ms step_avg:97.81ms
step:1364/1750 train_time:133424ms step_avg:97.82ms
step:1365/1750 train_time:133525ms step_avg:97.82ms
step:1366/1750 train_time:133627ms step_avg:97.82ms
step:1367/1750 train_time:133728ms step_avg:97.83ms
step:1368/1750 train_time:133830ms step_avg:97.83ms
step:1369/1750 train_time:133932ms step_avg:97.83ms
step:1370/1750 train_time:134034ms step_avg:97.83ms
step:1371/1750 train_time:134137ms step_avg:97.84ms
step:1372/1750 train_time:134238ms step_avg:97.84ms
step:1373/1750 train_time:134340ms step_avg:97.84ms
step:1374/1750 train_time:134442ms step_avg:97.85ms
step:1375/1750 train_time:134544ms step_avg:97.85ms
step:1375/1750 val_loss:3.3640 train_time:134640ms step_avg:97.92ms
step:1376/1750 train_time:134667ms step_avg:97.87ms
step:1377/1750 train_time:134756ms step_avg:97.86ms
step:1378/1750 train_time:134861ms step_avg:97.87ms
step:1379/1750 train_time:134963ms step_avg:97.87ms
step:1380/1750 train_time:135065ms step_avg:97.87ms
step:1381/1750 train_time:135166ms step_avg:97.88ms
step:1382/1750 train_time:135266ms step_avg:97.88ms
step:1383/1750 train_time:135367ms step_avg:97.88ms
step:1384/1750 train_time:135468ms step_avg:97.88ms
step:1385/1750 train_time:135569ms step_avg:97.88ms
step:1386/1750 train_time:135672ms step_avg:97.89ms
step:1387/1750 train_time:135775ms step_avg:97.89ms
step:1388/1750 train_time:135876ms step_avg:97.89ms
step:1389/1750 train_time:135979ms step_avg:97.90ms
step:1390/1750 train_time:136081ms step_avg:97.90ms
step:1391/1750 train_time:136184ms step_avg:97.90ms
step:1392/1750 train_time:136285ms step_avg:97.91ms
step:1393/1750 train_time:136387ms step_avg:97.91ms
step:1394/1750 train_time:136489ms step_avg:97.91ms
step:1395/1750 train_time:136590ms step_avg:97.91ms
step:1396/1750 train_time:136692ms step_avg:97.92ms
step:1397/1750 train_time:136795ms step_avg:97.92ms
step:1398/1750 train_time:136897ms step_avg:97.92ms
step:1399/1750 train_time:136999ms step_avg:97.93ms
step:1400/1750 train_time:137101ms step_avg:97.93ms
step:1401/1750 train_time:137202ms step_avg:97.93ms
step:1402/1750 train_time:137305ms step_avg:97.94ms
step:1403/1750 train_time:137407ms step_avg:97.94ms
step:1404/1750 train_time:137509ms step_avg:97.94ms
step:1405/1750 train_time:137610ms step_avg:97.94ms
step:1406/1750 train_time:137713ms step_avg:97.95ms
step:1407/1750 train_time:137815ms step_avg:97.95ms
step:1408/1750 train_time:137917ms step_avg:97.95ms
step:1409/1750 train_time:138020ms step_avg:97.96ms
step:1410/1750 train_time:138122ms step_avg:97.96ms
step:1411/1750 train_time:138225ms step_avg:97.96ms
step:1412/1750 train_time:138326ms step_avg:97.96ms
step:1413/1750 train_time:138428ms step_avg:97.97ms
step:1414/1750 train_time:138530ms step_avg:97.97ms
step:1415/1750 train_time:138632ms step_avg:97.97ms
step:1416/1750 train_time:138734ms step_avg:97.98ms
step:1417/1750 train_time:138836ms step_avg:97.98ms
step:1418/1750 train_time:138937ms step_avg:97.98ms
step:1419/1750 train_time:139039ms step_avg:97.98ms
step:1420/1750 train_time:139140ms step_avg:97.99ms
step:1421/1750 train_time:139242ms step_avg:97.99ms
step:1422/1750 train_time:139344ms step_avg:97.99ms
step:1423/1750 train_time:139447ms step_avg:97.99ms
step:1424/1750 train_time:139550ms step_avg:98.00ms
step:1425/1750 train_time:139653ms step_avg:98.00ms
step:1426/1750 train_time:139754ms step_avg:98.00ms
step:1427/1750 train_time:139856ms step_avg:98.01ms
step:1428/1750 train_time:139959ms step_avg:98.01ms
step:1429/1750 train_time:140062ms step_avg:98.01ms
step:1430/1750 train_time:140166ms step_avg:98.02ms
step:1431/1750 train_time:140268ms step_avg:98.02ms
step:1432/1750 train_time:140370ms step_avg:98.02ms
step:1433/1750 train_time:140473ms step_avg:98.03ms
step:1434/1750 train_time:140575ms step_avg:98.03ms
step:1435/1750 train_time:140679ms step_avg:98.03ms
step:1436/1750 train_time:140785ms step_avg:98.04ms
step:1437/1750 train_time:140888ms step_avg:98.04ms
step:1438/1750 train_time:140990ms step_avg:98.05ms
step:1439/1750 train_time:141093ms step_avg:98.05ms
step:1440/1750 train_time:141197ms step_avg:98.05ms
step:1441/1750 train_time:141302ms step_avg:98.06ms
step:1442/1750 train_time:141403ms step_avg:98.06ms
step:1443/1750 train_time:141506ms step_avg:98.06ms
step:1444/1750 train_time:141609ms step_avg:98.07ms
step:1445/1750 train_time:141712ms step_avg:98.07ms
step:1446/1750 train_time:141815ms step_avg:98.07ms
step:1447/1750 train_time:141917ms step_avg:98.08ms
step:1448/1750 train_time:142022ms step_avg:98.08ms
step:1449/1750 train_time:142123ms step_avg:98.08ms
step:1450/1750 train_time:142226ms step_avg:98.09ms
step:1451/1750 train_time:142328ms step_avg:98.09ms
step:1452/1750 train_time:142431ms step_avg:98.09ms
step:1453/1750 train_time:142535ms step_avg:98.10ms
step:1454/1750 train_time:142639ms step_avg:98.10ms
step:1455/1750 train_time:142742ms step_avg:98.10ms
step:1456/1750 train_time:142846ms step_avg:98.11ms
step:1457/1750 train_time:142950ms step_avg:98.11ms
step:1458/1750 train_time:143052ms step_avg:98.12ms
step:1459/1750 train_time:143154ms step_avg:98.12ms
step:1460/1750 train_time:143257ms step_avg:98.12ms
step:1461/1750 train_time:143361ms step_avg:98.13ms
step:1462/1750 train_time:143463ms step_avg:98.13ms
step:1463/1750 train_time:143566ms step_avg:98.13ms
step:1464/1750 train_time:143670ms step_avg:98.13ms
step:1465/1750 train_time:143773ms step_avg:98.14ms
step:1466/1750 train_time:143877ms step_avg:98.14ms
step:1467/1750 train_time:143979ms step_avg:98.15ms
step:1468/1750 train_time:144083ms step_avg:98.15ms
step:1469/1750 train_time:144188ms step_avg:98.15ms
step:1470/1750 train_time:144291ms step_avg:98.16ms
step:1471/1750 train_time:144394ms step_avg:98.16ms
step:1472/1750 train_time:144495ms step_avg:98.16ms
step:1473/1750 train_time:144597ms step_avg:98.17ms
step:1474/1750 train_time:144700ms step_avg:98.17ms
step:1475/1750 train_time:144802ms step_avg:98.17ms
step:1476/1750 train_time:144906ms step_avg:98.17ms
step:1477/1750 train_time:145010ms step_avg:98.18ms
step:1478/1750 train_time:145114ms step_avg:98.18ms
step:1479/1750 train_time:145216ms step_avg:98.19ms
step:1480/1750 train_time:145319ms step_avg:98.19ms
step:1481/1750 train_time:145423ms step_avg:98.19ms
step:1482/1750 train_time:145525ms step_avg:98.20ms
step:1483/1750 train_time:145628ms step_avg:98.20ms
step:1484/1750 train_time:145731ms step_avg:98.20ms
step:1485/1750 train_time:145834ms step_avg:98.20ms
step:1486/1750 train_time:145936ms step_avg:98.21ms
step:1487/1750 train_time:146039ms step_avg:98.21ms
step:1488/1750 train_time:146144ms step_avg:98.22ms
step:1489/1750 train_time:146247ms step_avg:98.22ms
step:1490/1750 train_time:146350ms step_avg:98.22ms
step:1491/1750 train_time:146452ms step_avg:98.22ms
step:1492/1750 train_time:146554ms step_avg:98.23ms
step:1493/1750 train_time:146657ms step_avg:98.23ms
step:1494/1750 train_time:146759ms step_avg:98.23ms
step:1495/1750 train_time:146862ms step_avg:98.24ms
step:1496/1750 train_time:146965ms step_avg:98.24ms
step:1497/1750 train_time:147068ms step_avg:98.24ms
step:1498/1750 train_time:147170ms step_avg:98.24ms
step:1499/1750 train_time:147273ms step_avg:98.25ms
step:1500/1750 train_time:147378ms step_avg:98.25ms
step:1500/1750 val_loss:3.3290 train_time:147474ms step_avg:98.32ms
step:1501/1750 train_time:147501ms step_avg:98.27ms
step:1502/1750 train_time:147590ms step_avg:98.26ms
step:1503/1750 train_time:147692ms step_avg:98.26ms
step:1504/1750 train_time:147794ms step_avg:98.27ms
step:1505/1750 train_time:147896ms step_avg:98.27ms
step:1506/1750 train_time:147998ms step_avg:98.27ms
step:1507/1750 train_time:148102ms step_avg:98.28ms
step:1508/1750 train_time:148204ms step_avg:98.28ms
step:1509/1750 train_time:148307ms step_avg:98.28ms
step:1510/1750 train_time:148409ms step_avg:98.28ms
step:1511/1750 train_time:148514ms step_avg:98.29ms
step:1512/1750 train_time:148617ms step_avg:98.29ms
step:1513/1750 train_time:148720ms step_avg:98.29ms
step:1514/1750 train_time:148824ms step_avg:98.30ms
step:1515/1750 train_time:148929ms step_avg:98.30ms
step:1516/1750 train_time:149033ms step_avg:98.31ms
step:1517/1750 train_time:149134ms step_avg:98.31ms
step:1518/1750 train_time:149236ms step_avg:98.31ms
step:1519/1750 train_time:149340ms step_avg:98.31ms
step:1520/1750 train_time:149444ms step_avg:98.32ms
step:1521/1750 train_time:149547ms step_avg:98.32ms
step:1522/1750 train_time:149651ms step_avg:98.33ms
step:1523/1750 train_time:149753ms step_avg:98.33ms
step:1524/1750 train_time:149858ms step_avg:98.33ms
step:1525/1750 train_time:149961ms step_avg:98.34ms
step:1526/1750 train_time:150063ms step_avg:98.34ms
step:1527/1750 train_time:150167ms step_avg:98.34ms
step:1528/1750 train_time:150272ms step_avg:98.35ms
step:1529/1750 train_time:150374ms step_avg:98.35ms
step:1530/1750 train_time:150478ms step_avg:98.35ms
step:1531/1750 train_time:150581ms step_avg:98.35ms
step:1532/1750 train_time:150684ms step_avg:98.36ms
step:1533/1750 train_time:150787ms step_avg:98.36ms
step:1534/1750 train_time:150889ms step_avg:98.36ms
step:1535/1750 train_time:150993ms step_avg:98.37ms
step:1536/1750 train_time:151094ms step_avg:98.37ms
step:1537/1750 train_time:151197ms step_avg:98.37ms
step:1538/1750 train_time:151299ms step_avg:98.37ms
step:1539/1750 train_time:151402ms step_avg:98.38ms
step:1540/1750 train_time:151505ms step_avg:98.38ms
step:1541/1750 train_time:151609ms step_avg:98.38ms
step:1542/1750 train_time:151714ms step_avg:98.39ms
step:1543/1750 train_time:151817ms step_avg:98.39ms
step:1544/1750 train_time:151921ms step_avg:98.39ms
step:1545/1750 train_time:152023ms step_avg:98.40ms
step:1546/1750 train_time:152125ms step_avg:98.40ms
step:1547/1750 train_time:152230ms step_avg:98.40ms
step:1548/1750 train_time:152334ms step_avg:98.41ms
step:1549/1750 train_time:152437ms step_avg:98.41ms
step:1550/1750 train_time:152540ms step_avg:98.41ms
step:1551/1750 train_time:152644ms step_avg:98.42ms
step:1552/1750 train_time:152747ms step_avg:98.42ms
step:1553/1750 train_time:152851ms step_avg:98.42ms
step:1554/1750 train_time:152954ms step_avg:98.43ms
step:1555/1750 train_time:153056ms step_avg:98.43ms
step:1556/1750 train_time:153160ms step_avg:98.43ms
step:1557/1750 train_time:153264ms step_avg:98.44ms
step:1558/1750 train_time:153369ms step_avg:98.44ms
step:1559/1750 train_time:153472ms step_avg:98.44ms
step:1560/1750 train_time:153575ms step_avg:98.45ms
step:1561/1750 train_time:153677ms step_avg:98.45ms
step:1562/1750 train_time:153782ms step_avg:98.45ms
step:1563/1750 train_time:153887ms step_avg:98.46ms
step:1564/1750 train_time:153990ms step_avg:98.46ms
step:1565/1750 train_time:154093ms step_avg:98.46ms
step:1566/1750 train_time:154197ms step_avg:98.47ms
step:1567/1750 train_time:154299ms step_avg:98.47ms
step:1568/1750 train_time:154402ms step_avg:98.47ms
step:1569/1750 train_time:154504ms step_avg:98.47ms
step:1570/1750 train_time:154609ms step_avg:98.48ms
step:1571/1750 train_time:154712ms step_avg:98.48ms
step:1572/1750 train_time:154814ms step_avg:98.48ms
step:1573/1750 train_time:154918ms step_avg:98.49ms
step:1574/1750 train_time:155021ms step_avg:98.49ms
step:1575/1750 train_time:155124ms step_avg:98.49ms
step:1576/1750 train_time:155228ms step_avg:98.49ms
step:1577/1750 train_time:155332ms step_avg:98.50ms
step:1578/1750 train_time:155434ms step_avg:98.50ms
step:1579/1750 train_time:155537ms step_avg:98.50ms
step:1580/1750 train_time:155640ms step_avg:98.51ms
step:1581/1750 train_time:155744ms step_avg:98.51ms
step:1582/1750 train_time:155848ms step_avg:98.51ms
step:1583/1750 train_time:155954ms step_avg:98.52ms
step:1584/1750 train_time:156058ms step_avg:98.52ms
step:1585/1750 train_time:156161ms step_avg:98.52ms
step:1586/1750 train_time:156265ms step_avg:98.53ms
step:1587/1750 train_time:156368ms step_avg:98.53ms
step:1588/1750 train_time:156471ms step_avg:98.53ms
step:1589/1750 train_time:156573ms step_avg:98.54ms
step:1590/1750 train_time:156676ms step_avg:98.54ms
step:1591/1750 train_time:156779ms step_avg:98.54ms
step:1592/1750 train_time:156883ms step_avg:98.54ms
step:1593/1750 train_time:156986ms step_avg:98.55ms
step:1594/1750 train_time:157092ms step_avg:98.55ms
step:1595/1750 train_time:157194ms step_avg:98.55ms
step:1596/1750 train_time:157296ms step_avg:98.56ms
step:1597/1750 train_time:157399ms step_avg:98.56ms
step:1598/1750 train_time:157505ms step_avg:98.56ms
step:1599/1750 train_time:157607ms step_avg:98.57ms
step:1600/1750 train_time:157712ms step_avg:98.57ms
step:1601/1750 train_time:157815ms step_avg:98.57ms
step:1602/1750 train_time:157919ms step_avg:98.58ms
step:1603/1750 train_time:158023ms step_avg:98.58ms
step:1604/1750 train_time:158125ms step_avg:98.58ms
step:1605/1750 train_time:158229ms step_avg:98.59ms
step:1606/1750 train_time:158333ms step_avg:98.59ms
step:1607/1750 train_time:158436ms step_avg:98.59ms
step:1608/1750 train_time:158539ms step_avg:98.59ms
step:1609/1750 train_time:158642ms step_avg:98.60ms
step:1610/1750 train_time:158747ms step_avg:98.60ms
step:1611/1750 train_time:158852ms step_avg:98.60ms
step:1612/1750 train_time:158956ms step_avg:98.61ms
step:1613/1750 train_time:159058ms step_avg:98.61ms
step:1614/1750 train_time:159161ms step_avg:98.61ms
step:1615/1750 train_time:159263ms step_avg:98.61ms
step:1616/1750 train_time:159366ms step_avg:98.62ms
step:1617/1750 train_time:159469ms step_avg:98.62ms
step:1618/1750 train_time:159573ms step_avg:98.62ms
step:1619/1750 train_time:159675ms step_avg:98.63ms
step:1620/1750 train_time:159778ms step_avg:98.63ms
step:1621/1750 train_time:159881ms step_avg:98.63ms
step:1622/1750 train_time:159984ms step_avg:98.63ms
step:1623/1750 train_time:160086ms step_avg:98.64ms
step:1624/1750 train_time:160190ms step_avg:98.64ms
step:1625/1750 train_time:160294ms step_avg:98.64ms
step:1625/1750 val_loss:3.2984 train_time:160392ms step_avg:98.70ms
step:1626/1750 train_time:160419ms step_avg:98.66ms
step:1627/1750 train_time:160510ms step_avg:98.65ms
step:1628/1750 train_time:160612ms step_avg:98.66ms
step:1629/1750 train_time:160715ms step_avg:98.66ms
step:1630/1750 train_time:160818ms step_avg:98.66ms
step:1631/1750 train_time:160921ms step_avg:98.66ms
step:1632/1750 train_time:161024ms step_avg:98.67ms
step:1633/1750 train_time:161126ms step_avg:98.67ms
step:1634/1750 train_time:161230ms step_avg:98.67ms
step:1635/1750 train_time:161333ms step_avg:98.67ms
step:1636/1750 train_time:161437ms step_avg:98.68ms
step:1637/1750 train_time:161541ms step_avg:98.68ms
step:1638/1750 train_time:161645ms step_avg:98.68ms
step:1639/1750 train_time:161747ms step_avg:98.69ms
step:1640/1750 train_time:161850ms step_avg:98.69ms
step:1641/1750 train_time:161953ms step_avg:98.69ms
step:1642/1750 train_time:162055ms step_avg:98.69ms
step:1643/1750 train_time:162158ms step_avg:98.70ms
step:1644/1750 train_time:162260ms step_avg:98.70ms
step:1645/1750 train_time:162363ms step_avg:98.70ms
step:1646/1750 train_time:162466ms step_avg:98.70ms
step:1647/1750 train_time:162571ms step_avg:98.71ms
step:1648/1750 train_time:162675ms step_avg:98.71ms
step:1649/1750 train_time:162778ms step_avg:98.71ms
step:1650/1750 train_time:162881ms step_avg:98.72ms
step:1651/1750 train_time:162984ms step_avg:98.72ms
step:1652/1750 train_time:163086ms step_avg:98.72ms
step:1653/1750 train_time:163189ms step_avg:98.72ms
step:1654/1750 train_time:163292ms step_avg:98.73ms
step:1655/1750 train_time:163394ms step_avg:98.73ms
step:1656/1750 train_time:163499ms step_avg:98.73ms
step:1657/1750 train_time:163602ms step_avg:98.73ms
step:1658/1750 train_time:163705ms step_avg:98.74ms
step:1659/1750 train_time:163812ms step_avg:98.74ms
step:1660/1750 train_time:163915ms step_avg:98.74ms
step:1661/1750 train_time:164019ms step_avg:98.75ms
step:1662/1750 train_time:164123ms step_avg:98.75ms
step:1663/1750 train_time:164227ms step_avg:98.75ms
step:1664/1750 train_time:164329ms step_avg:98.76ms
step:1665/1750 train_time:164434ms step_avg:98.76ms
step:1666/1750 train_time:164537ms step_avg:98.76ms
step:1667/1750 train_time:164641ms step_avg:98.76ms
step:1668/1750 train_time:164745ms step_avg:98.77ms
step:1669/1750 train_time:164850ms step_avg:98.77ms
step:1670/1750 train_time:164953ms step_avg:98.77ms
step:1671/1750 train_time:165055ms step_avg:98.78ms
step:1672/1750 train_time:165159ms step_avg:98.78ms
step:1673/1750 train_time:165262ms step_avg:98.78ms
step:1674/1750 train_time:165365ms step_avg:98.78ms
step:1675/1750 train_time:165468ms step_avg:98.79ms
step:1676/1750 train_time:165572ms step_avg:98.79ms
step:1677/1750 train_time:165675ms step_avg:98.79ms
step:1678/1750 train_time:165779ms step_avg:98.80ms
step:1679/1750 train_time:165883ms step_avg:98.80ms
step:1680/1750 train_time:165986ms step_avg:98.80ms
step:1681/1750 train_time:166089ms step_avg:98.80ms
step:1682/1750 train_time:166195ms step_avg:98.81ms
step:1683/1750 train_time:166299ms step_avg:98.81ms
step:1684/1750 train_time:166403ms step_avg:98.81ms
step:1685/1750 train_time:166506ms step_avg:98.82ms
step:1686/1750 train_time:166608ms step_avg:98.82ms
step:1687/1750 train_time:166710ms step_avg:98.82ms
step:1688/1750 train_time:166814ms step_avg:98.82ms
step:1689/1750 train_time:166918ms step_avg:98.83ms
step:1690/1750 train_time:167022ms step_avg:98.83ms
step:1691/1750 train_time:167127ms step_avg:98.83ms
step:1692/1750 train_time:167230ms step_avg:98.84ms
step:1693/1750 train_time:167334ms step_avg:98.84ms
step:1694/1750 train_time:167439ms step_avg:98.84ms
step:1695/1750 train_time:167545ms step_avg:98.85ms
step:1696/1750 train_time:167648ms step_avg:98.85ms
step:1697/1750 train_time:167756ms step_avg:98.85ms
step:1698/1750 train_time:167859ms step_avg:98.86ms
step:1699/1750 train_time:167962ms step_avg:98.86ms
step:1700/1750 train_time:168066ms step_avg:98.86ms
step:1701/1750 train_time:168170ms step_avg:98.87ms
step:1702/1750 train_time:168275ms step_avg:98.87ms
step:1703/1750 train_time:168380ms step_avg:98.87ms
step:1704/1750 train_time:168484ms step_avg:98.88ms
step:1705/1750 train_time:168588ms step_avg:98.88ms
step:1706/1750 train_time:168693ms step_avg:98.88ms
step:1707/1750 train_time:168797ms step_avg:98.89ms
step:1708/1750 train_time:168903ms step_avg:98.89ms
step:1709/1750 train_time:169006ms step_avg:98.89ms
step:1710/1750 train_time:169110ms step_avg:98.89ms
step:1711/1750 train_time:169215ms step_avg:98.90ms
step:1712/1750 train_time:169318ms step_avg:98.90ms
step:1713/1750 train_time:169424ms step_avg:98.90ms
step:1714/1750 train_time:169528ms step_avg:98.91ms
step:1715/1750 train_time:169633ms step_avg:98.91ms
step:1716/1750 train_time:169737ms step_avg:98.91ms
step:1717/1750 train_time:169841ms step_avg:98.92ms
step:1718/1750 train_time:169944ms step_avg:98.92ms
step:1719/1750 train_time:170050ms step_avg:98.92ms
step:1720/1750 train_time:170154ms step_avg:98.93ms
step:1721/1750 train_time:170258ms step_avg:98.93ms
step:1722/1750 train_time:170363ms step_avg:98.93ms
step:1723/1750 train_time:170466ms step_avg:98.94ms
step:1724/1750 train_time:170571ms step_avg:98.94ms
step:1725/1750 train_time:170675ms step_avg:98.94ms
step:1726/1750 train_time:170778ms step_avg:98.94ms
step:1727/1750 train_time:170883ms step_avg:98.95ms
step:1728/1750 train_time:170988ms step_avg:98.95ms
step:1729/1750 train_time:171091ms step_avg:98.95ms
step:1730/1750 train_time:171194ms step_avg:98.96ms
step:1731/1750 train_time:171299ms step_avg:98.96ms
step:1732/1750 train_time:171403ms step_avg:98.96ms
step:1733/1750 train_time:171507ms step_avg:98.97ms
step:1734/1750 train_time:171612ms step_avg:98.97ms
step:1735/1750 train_time:171715ms step_avg:98.97ms
step:1736/1750 train_time:171820ms step_avg:98.97ms
step:1737/1750 train_time:171925ms step_avg:98.98ms
step:1738/1750 train_time:172029ms step_avg:98.98ms
step:1739/1750 train_time:172133ms step_avg:98.98ms
step:1740/1750 train_time:172237ms step_avg:98.99ms
step:1741/1750 train_time:172345ms step_avg:98.99ms
step:1742/1750 train_time:172450ms step_avg:99.00ms
step:1743/1750 train_time:172554ms step_avg:99.00ms
step:1744/1750 train_time:172659ms step_avg:99.00ms
step:1745/1750 train_time:172763ms step_avg:99.00ms
step:1746/1750 train_time:172867ms step_avg:99.01ms
step:1747/1750 train_time:172971ms step_avg:99.01ms
step:1748/1750 train_time:173076ms step_avg:99.01ms
step:1749/1750 train_time:173179ms step_avg:99.02ms
step:1750/1750 train_time:173285ms step_avg:99.02ms
step:1750/1750 val_loss:3.2779 train_time:173384ms step_avg:99.08ms
peak memory allocated: 33277 MiB reserved: 48352 MiB
